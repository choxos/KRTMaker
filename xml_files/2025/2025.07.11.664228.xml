<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS207337</article-id><article-id pub-id-type="doi">10.1101/2025.07.11.664228</article-id><article-id pub-id-type="archive">PPR1051951</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>MiroSCOPE: An AI-driven digital pathology platform for annotating functional tissue units</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Fenner</surname><given-names>Madeleine R.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Sevim</surname><given-names>Selim</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Wu</surname><given-names>Guanming</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Beavers</surname><given-names>Deidre</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Guo</surname><given-names>Pengfei</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Tang</surname><given-names>Yucheng</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Eddy</surname><given-names>Christopher Z.</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Ait-Ahmad</surname><given-names>Kaoutar</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Rice-Stitt</surname><given-names>Travis</given-names></name><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Thomas</surname><given-names>George</given-names></name><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Kuykendall</surname><given-names>M.J.</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Stavrinides</surname><given-names>Vasilis</given-names></name><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Emberton</surname><given-names>Mark</given-names></name><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Xu</surname><given-names>Daguang</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Song</surname><given-names>Xubo</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Eksi</surname><given-names>S. Ece</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Demir</surname><given-names>Emek</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>Department of Molecular and Medical Genetics, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/009avj582</institution-id><institution>Oregon Health and Science University</institution></institution-wrap>, <city>Portland</city>, <state>OR</state>, <country country="US">USA</country></aff><aff id="A2"><label>2</label>Cancer Early Detection Advanced Research Center, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/002shna07</institution-id><institution>Knight Cancer Institute</institution></institution-wrap>, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/009avj582</institution-id><institution>Oregon Health and Science University</institution></institution-wrap>, <city>Portland</city>, <state>OR</state>, <country country="US">USA</country></aff><aff id="A3"><label>3</label>Division of Oncological Sciences, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/009avj582</institution-id><institution>Oregon Health and Science University</institution></institution-wrap>, <city>Portland</city>, <state>OR</state>, <country country="US">USA</country></aff><aff id="A4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03jdj4y14</institution-id><institution>NVIDIA</institution></institution-wrap>, <country country="US">USA</country></aff><aff id="A5"><label>5</label>Department of Pathology and Laboratory Medicine, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/009avj582</institution-id><institution>Oregon Health and Science University</institution></institution-wrap>, <city>Portland</city>, <state>OR</state>, <country country="US">USA</country></aff><aff id="A6"><label>6</label>Division of Surgery and Interventional Science, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap>, <city>London</city><postal-code>WC1E 6AP</postal-code>, UK</aff><author-notes><corresp id="CR1">
<label>*</label>Correspondence: <email>demire@ohsu.edu</email>
</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>21</day><month>07</month><year>2025</year></pub-date><pub-date pub-type="preprint"><day>17</day><month>07</month><year>2025</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Cancer tissue analysis in digital pathology is typically conducted across different spatial scales, ranging from high-resolution cell-level modeling to lower-resolution tile-based assessments. However, these perspectives often overlook the structural organization of functional tissue units (FTUs), the small, repeating structures which are crucial to tissue function and key factors during pathological assessment. The incorporation of FTU information is hindered by the need for detailed manual annotations, which are costly and time-consuming to obtain. While artificial intelligence (AI)-based solutions hold great promise to accelerate this process, there is currently no comprehensive workflow for building the large, annotated cohorts required. To remove these roadblocks and advance the development of more interpretable approaches, we developed MiroSCOPE, an end-to-end AI-assisted platform for annotating FTUs at scale, built on QuPath. MiroSCOPE integrates a fine-tunable multiclass segmentation model and curation-specific usability features to enable a human-in-the-loop system that accelerates AI annotation by a pathologist. The system is used to efficiently annotate over 71,900 FTUs on 184 prostate cancer hematoxylin and eosin (H&amp;E)-stained tissue samples and demonstrates ready translation to breast cancer. Furthermore, we publicly release a dataset named Miro-120, consisting of 120 prostate cancer H&amp;E with 30,568 annotations, which can be used by the community as a high-quality resource for FTU-level machine learning aims. In summary, MiroSCOPE provides an adaptable AI-driven platform for annotating functional tissue units, facilitating the use of structural information in digital pathology analyses.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Within an organ, cells organize into functional tissue units (FTUs): small, repeating structures performing essential functions such as nephrons, alveoli, or glands<sup><xref ref-type="bibr" rid="R1">1</xref></sup>. Changes to these structures, visible in hematoxylin and eosin (H&amp;E)-stained tissue, offer critical information regarding the diagnosis and treatment of multiple diseases, including cancer. Driven by the increased digitization of tissue samples and advances in artificial intelligence (AI), there has been a rapid expansion of digital pathology approaches to harness the detailed biological information in tissue images for improved disease characterization and prediction. These approaches vary in their level of granularity, with some viewing the tissue as a collection of image tiles<sup><xref ref-type="bibr" rid="R2">2</xref>–<xref ref-type="bibr" rid="R5">5</xref></sup>, while others zoom in to the single-cell level to examine cellular interactions and neighborhoods<sup><xref ref-type="bibr" rid="R6">6</xref>–<xref ref-type="bibr" rid="R9">9</xref></sup>, often utilizing data from immunofluorescence or omics techniques. At each extreme, context around tissue structure is largely lost; tile-level methods arbitrarily cut through FTUs, disrupting their morphological arrangement, while cell-level methods fail to capture broader spatial organization. This disconnect in granularity makes it challenging to relate model representations to clinical knowledge, limiting interpretability and posing challenges for incorporating feedback from human experts. This, in turn, creates serious barriers to widespread clinical trust and adoption.</p><p id="P3">The limiting factor for incorporating FTU level information is the need for detailed annotation by experts, a costly process that demands significant time and manual effort. Leveraging AI to automate the annotation process is critical for reaching the scale of annotated images needed. There are several existing AI models for segmenting and classifying tissue structures in various organs: Many annotate at the regional level, where groups of FTUs of a similar pattern are segmented together within areas that include the intervening stroma<sup><xref ref-type="bibr" rid="R10">10</xref>–<xref ref-type="bibr" rid="R12">12</xref></sup>. While regional annotations provide an overview of tissue arrangement, they lack the precision required to accurately represent intratumoral heterogeneity and model individual FTU features, distributions, and interactions within the tumor microenvironment. Models that perform true FTU-level annotation are rare, and few attempt segmentation and classification for all FTU structures, covering non-tumoral, tumoral, and stromal components<sup><xref ref-type="bibr" rid="R13">13</xref>–<xref ref-type="bibr" rid="R16">16</xref></sup>. Additionally, applying existing models for custom research aims and new datasets presents many challenges. Even when models are trained with diverse data to enhance generalizability, distribution shifts due to variations in tissue collection or processing methods can lead to drops in performance<sup><xref ref-type="bibr" rid="R17">17</xref></sup>. Most models are offered as static tools without the infrastructure for fine-tuning, limiting adaptability when zero-shot inference underperforms. Furthermore, dataset curation processes are often <italic>ad hoc</italic>, lacking integration with widely adopted annotation viewing and editing platforms and standardization of file types, ontologies, and annotation procedures.</p><p id="P4">To address these challenges, we developed MiroSCOPE, a platform for annotating FTUs, which employs a human-in-the-loop (HITL) approach to enable large-scale curation by an expert (<xref ref-type="fig" rid="F1">Figure 1</xref>). We combined a fine-tunable AI model for segmentation and classification with a QuPath-based user interface that is optimized to rapidly annotate FTUs. We demonstrate the improvements in efficiency and accuracy offered by the platform using 184 prostatic acinar adenocarcinoma H&amp;E images sourced from three distinct cohorts, annotating over 71,900 FTU structures with uniquely detailed labeling of non-tumoral, tumoral (with subpatterns), and stromal components. We publicly release a subset of these annotated images as a high-quality dataset named Miro-120 (120 prostatic acinar adenocarcinoma H&amp;E; 30,568 annotations). The dataset provides a novel level of structural detail for prostate cancer tissue, and may act as a resource for new structurally informed machine learning efforts.</p><p id="P5">Prostate cancer is an ideal use case for the system; despite high rates of early diagnosis, a significant subset progresses to advanced and incurable disease, highlighting the need for improved disease staging and prognostic tools<sup><xref ref-type="bibr" rid="R18">18</xref></sup>. Additionally, prostate cancer is largely characterized by changes in the architectural arrangement of glandular FTUs, but the Gleason scoring system used to categorize this change faces high interobserver variability<sup><xref ref-type="bibr" rid="R19">19</xref>–<xref ref-type="bibr" rid="R21">21</xref></sup>. Employing AI-based approaches to characterize tissue structures holds great promise to reduce variability and increase the prognostic accuracy for prostate cancer and for many other cancer types. We showed the adaptability of the platform to other cancers using a small breast adenocarcinoma dataset in a <italic>few-shot</italic> setting.</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>A controlled vocabulary for annotating prostate cancer FTUs</title><p id="P6">There is no formal ontology for describing the different types and presentations of FTUs in the disease state. We designed a simple, 17-term controlled vocabulary to annotate FTUs of the prostate for tissue samples with prostatic acinar adenocarcinoma, encompassing diverse histopathologic patterns. The FTUs can be divided into two major categories: glandular origin and stromal. The FTUs of glandular origin cover non-tumoral gland structures, defined as either normal or atrophic, as well as gland-derived tumoral structures. The tumoral FTU classes of the vocabulary follow the Gleason scoring system<sup><xref ref-type="bibr" rid="R19">19</xref></sup>, the standard metric for evaluating prostate tumors based on the architectural arrangement of glands, ranging from well-differentiated (Gleason Pattern 3) to poorly differentiated (Gleason Pattern 5). Gleason patterns can be subtyped based on morphological characteristics, and these divisions are captured in the labeling scheme. Prostatic intraepithelial neoplasia (PIN) is also included as a precursor. Stromal components, which may provide valuable insights into tumoral behavior, include nerves, vascular structures, and areas of inflammation. Air bubbles, tissue folding, unfocused scanning areas, color anomalies related to tissue processing, and, most prevalently, sheared pieces of glandular tissue, which cannot be evaluated, were annotated as artifacts. The categories of FTUs are summarized in <xref ref-type="fig" rid="F2">Figure 2</xref>.</p></sec><sec id="S4"><title>MiroSCOPE: A platform for human-in-the-loop annotation at scale</title><p id="P7">To enable the large-scale curation of FTU annotations, we built a HITL system in which a pathologist iteratively uses and improves an AI annotation model. The software stack uses: (i) QuPath<sup><xref ref-type="bibr" rid="R22">22</xref></sup>, a widely used open-source software for bioimage visualization and analysis, as a base platform, (ii) an AI model fine-tuned to segment and classify FTUs (iii) MONAI Label<sup><xref ref-type="bibr" rid="R23">23</xref></sup> to bridge the AI model to QuPath, and (iv) the MiroSCOPE extension, developed by our team to integrate AI inference capabilities and add additional user interface (UI) features specific to the curation task (<xref ref-type="fig" rid="F3">Figure 3</xref>).</p><p id="P8">The MiroSCOPE extension, organized under a tab UI element, allows users to initiate and control FTU segmentation and inference, identify and correct errors, and record the final annotations. Users can use this tab to select an image, and any existing annotations are then loaded into the annotation table. The annotation table serves as the main workspace, and actions are synchronized with the right-side image viewer. Each annotation is assigned a class ID, class name, type, and allows for the input of metadata, all of which can be edited in the table. Annotation type captures the degree of automation in generating the annotation, categorized using four labels: <list list-type="bullet" id="L1"><list-item><p id="P9"><bold>auto</bold>: An annotation created by an AI model.</p></list-item><list-item><p id="P10"><bold>auto_checked</bold>: An annotation identified by an AI model and confirmed by a pathologist.</p></list-item><list-item><p id="P11"><bold>auto_edited</bold>: An annotation identified by an AI model and edited by a pathologist.</p></list-item><list-item><p id="P12"><bold>manual</bold>: An annotation created manually by a pathologist.</p></list-item></list>
</p><p id="P13">To streamline the review of FTU annotations, we also included an auto-play feature that automatically cycles through each FTU annotation. If an annotation is identified as requiring further inspection, it can be marked for later review. Annotations are saved in the GeoJSON format, with each FTU annotation identified by a unique UUID and associated metadata. This format is also used by the Python inference tool to facilitate the persistence and exchange of FTU annotations. All user actions are tracked to enable evaluation of changes in curation efficiency and provide feedback for model enhancements.</p><p id="P14">The QuPath extension-based platform provides a default trained AI model for prostate cancer FTU segmentation and classification. To extend the platform for AI models targeting other cancers or to support models fine-tuned for specific datasets, we have added UI features that allow users to configure the trained model and the associated FTU ID and class file.</p></sec><sec id="S5"><title>Fine-tuned segmentation foundation models accurately segment prostate tissue structures</title><p id="P15">When annotating FTUs, accurately defining the boundaries of each structure demands significant manual effort, making segmentation the most time-consuming step in the curation process. Encouraged by the recent performance gains by using a vision transformer (ViT) architecture in other domains, we applied the ViT-based Segment Anything Model (SAM)<sup><xref ref-type="bibr" rid="R24">24</xref></sup> zero-shot to segment FTU structures in prostate cancer H&amp;E. An evaluation of segmentation performance found an average Dice coefficient of 0.27. While SAM sometimes provided a useful starting point for segmentation correction, detecting boundaries of normal or early-stage well-defined structures reasonably well, it struggled with more disordered, higher-grade tumors.</p><p id="P16">After curating an initial set of annotated H&amp;E images, we adopted a version of SAM that enables prompting by label and is readily fine-tunable. Given the limited number of samples and imbalanced class distributions, the model was first fine-tuned to perform foreground-background segmentation, considering FTU structures as foreground. After fine-tuning, the model reached an average Dice coefficient of 0.86. Following the curation of more images using the latest model checkpoint for assistance, a second fine-tuning was performed, achieving an average Dice coefficient of 0.91. A clear improvement in segmentation performance is observed at each fine-tuning (<xref ref-type="fig" rid="F4">Figure 4</xref>). Since segmentation is the most time-consuming component of annotation, each fine-tuning offers substantial improvements for annotation throughput. Training times varied based on dataset size but were consistently completed within a few hours, a relatively small time investment for the resulting gains in efficiency.</p></sec><sec id="S6"><title>Fine-tuned segmentation foundation models accurately classify prostate tissue structures</title><p id="P17">Following the curation of 120 prostate cancer H&amp;E samples, we fine-tuned a multiclass SAM model using the 17 FTU annotation labels defined in our controlled vocabulary. Model inference produces a pixel-wise classification, resulting in a semantic segmentation mask for the entire analyzed region. Visual observation of the results for unseen data shows that, while the model is more consistent for lower-grade cases, it regardless provides an excellent starting point for annotation correction (<xref ref-type="fig" rid="F5">Figure 5A</xref>).</p><p id="P18">Evaluation of the segmentation performance on the holdout set revealed that the fine-tuned multiclass model achieved an average Dice similarity coefficient of 0.81. While this score is lower than that achieved by the foreground-background model (Dice = 0.91), which is expected given the increased complexity of distinguishing 17 FTU classes plus background, it significantly outperforms the zero-shot SAM baseline (Dice = 0.27). We observed a similar trend regarding cancer grade as seen with original SAM, where lower and medium-grade cases generally yielded higher Dice scores compared to higher-grade cases, likely because tissue architecture is more preserved (<xref ref-type="fig" rid="F5">Figure 5B</xref>)</p><p id="P19">Assessing the classification performance of the fine-tuned multiclass model yielded an overall accuracy of 45.8%. Despite the challenge of classifying 17 distinct FTU categories, the model demonstrated strong discriminatory power for several key classes, far exceeding the 5.9% accuracy expected from random chance (<xref ref-type="fig" rid="F5">Figure 5C</xref>). The model performed well at discriminating normal glands (68%), Gleason pattern 3 glands (66%), artifacts (72%), and stromal components such as nerves (79%) and vascular structures (65%). Performance was moderate for atrophic glands (45%), inflammation (52%), Gleason pattern 4 cribriform (27%), and Gleason pattern 4 poorly formed (38%). Classes corresponding to higher-grade Gleason patterns (other GP4 subtypes, GP5) and prostatic intraepithelial neoplasia (PIN) were largely underrepresented or absent in the training set, and consequently, their classification accuracies were close to a random classifier. Importantly, categories with many annotated samples generally achieved superior performance, and there were no classes which had many instances annotated but low classification accuracy (<xref ref-type="fig" rid="F5">Figure 5D</xref>).</p><p id="P20">Since segmentation and classification performance are not entirely independent in semantic segmentation tasks, the evaluation of one must be understood in the context of the other. The most common source of misclassification across all classes was a failure to segment the structure, resulting in it being classified as background with a rate between 5% to 47% across all classes (<xref ref-type="fig" rid="F5">Figure 5C</xref>). In effect, accuracy as a measure of discrimination between the 17 FTU classes is underestimated by the values presented. Another common source of error was misclassification as Gleason 3, most notably seen in Gleason 4 cribriform (45%) and Gleason 4 fused (36%), with lower rates for normal glands (9%), atrophic glands (5%), nerves (8%), and artifacts (8%). Since Gleason 3 has the highest representation in the training set of all classes, this likely indicates some bias of the model (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 1B</xref>). However, another factor to consider is that diseases typically progress on a continuous trajectory, but grading systems define discrete boundaries along this progression to more easily characterize the disease. There are likely FTUs that exhibit mixed features of normal and Gleason 3, or Gleason 3 and Gleason 4, and misclassification between these groups may not necessarily reflect a failure of the model.</p></sec><sec id="S7"><title>MiroSCOPE greatly improves annotation efficiency</title><p id="P21">To assess gains in annotation efficiency when using MiroSCOPE with AI assistance, the annotation process for a set of 18 UCL AS Cohort images was tracked in detail. Evaluation focused on improvements in segmentation effort and time, as segmentation is the most time-consuming step in the annotation process. For the evaluation set, using AI assistance with the latest multiclass checkpoint was on average 3.18 more efficient than manual annotation alone. On average, 64% of annotations were fully AI generated without needing any manual segmentation creation or editing, saving an estimated 76 minutes of annotation time per image (<xref ref-type="fig" rid="F5">Figure 5E</xref>; <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 4A</xref>). These efficiency measures varied greatly by image, with percentage of fully AI-generated annotations ranging from 42% to 85%, and estimated time saved ranging from 30 to 148 minutes. This is expected, as factors such as tissue size, complexity, and composition of cancer grades, together with model accuracy for different classes, can lead to variability in the annotation process (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 4B</xref>).</p></sec><sec id="S8"><title>Miro-120 dataset: a high-quality FTU-level annotated dataset for the community</title><p id="P22">Using MiroSCOPE we curated Miro-120, a dataset consisting of 120 prostatic acinar adenocarcinoma H&amp;E samples with detailed FTU annotations totaling 30,568 (<xref ref-type="fig" rid="F6">Figure 7</xref>). We are offering Miro-120 as an openly available resource for the community, providing access to uniquely detailed H&amp;E structure labeling to facilitate novel machine learning aims. To the best of our knowledge, this is the first prostate cancer H&amp;E dataset with comprehensive structural annotations that include Gleason grade subpatterns and stromal components to be released. Samples in Miro-120 were sourced from two cohorts: 18 are radical prostatectomy specimens from the OHSU Biolibrary, and 102 are needle biopsy specimens from the CEDAR Biorepository. All patients were treatment-naïve with localized prostate cancer at the time of tissue collection. Annotations were curated using MiroSCOPE with AI assistance, and all annotations went through manual review and correction by an expert, making them highly reliable.</p></sec><sec id="S9"><title>The curation system can be deployed for other cancer types</title><p id="P23">While the curation study was piloted using prostatic acinar adenocarcinoma, the platform itself is generic. To evaluate transfer of the current approach to different cancer types, we used the fine-tuning capabilities offered within MiroSCOPE to adapt the model for annotating breast adenocarcinoma samples. The morphological similarities shared between adenocarcinomas make breast cancer a natural test for transfer learning. Following the structure of the controlled vocabulary defined for prostate FTUs, labels under non-tumoral, precursor, and tumoral categories were modified to reflect terms used for breast FTUs (<xref ref-type="fig" rid="F7">Figure 8A</xref>). As this study on translating the curation system was limited in scope, terms are not exhaustive, and within non-glandular tumoral formations the label “solid” is used to collapse many high-grade patterns into a single term. Additionally, cribriform encompasses complex glandular structures, and benign tumors such as fibroadenoma were absent from the samples and not considered in the vocabulary.</p><p id="P24">Two H&amp;E samples from the TCGA-BRCA cohort with similar subtypes of breast adenocarcinoma were selected, one for fine-tuning a breast-specific annotation model and the other for evaluating model performance. To curate ground truth annotations for the first sample, the latest foreground-background model trained on prostate tissue was applied to accelerate annotation correction. As a baseline, the Fg-bg2 model checkpoint which had only seen prostate images achieved a Dice of 0.68 on the test image. To compare approaches, both a foreground-background model and a multiclass model were trained using the breast H&amp;E sample. The foreground-background model was trained from the Fg-bg2 model checkpoint and reached a Dice of 0.77. The multiclass model was trained from the latest multiclass checkpoint and reached a Dice of 0.68 and overall accuracy of 49.7%.</p><p id="P25">We observed successful classification of ductal carcinoma in-situ (100%), solid tumors (67%), vascular structures (61%), and artifacts (54%) (<xref ref-type="fig" rid="F7">Figure 8B</xref>). Similarly to the multiclass model trained on prostate images, failure to segment the structure and instead labeling it as background was a large source of misclassification, particularly for vascular structures (36%) and inflammation (45%). Additionally, a clear bias is observed for classifying solid tumors, with many false positives in other classes indicating low specificity. This is likely because solid tumors were over-represented in the training image, highlighting the risk of fine-tuning with just one sample - a situation that is hard to avoid at the start of curation (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 2B</xref>).</p></sec></sec><sec id="S10" sec-type="discussion"><title>Discussion</title><p id="P26">In this work, we present MiroSCOPE, an AI-driven, human-in-the-loop annotation platform centered around functional tissue units, designed to address limitations in current digital pathology approaches. MiroSCOPE is a QuPath extension that integrates a fine-tunable AI model for annotation, facilitating large-scale, expert-driven annotation of FTUs and bridging the gap between cellular detail and tissue-level architecture. We demonstrated its utility by annotating over 71,900 FTUs in prostate cancer and showed its ready translation to breast cancer.</p><p id="P27">Tissue FTUs represent the fundamental building blocks of an organ, where intercellular coordination enables critical functions. They offer a biologically meaningful scale for tissue analysis, connecting cellular detail and overall tissue architecture, and offering a window into tumor progression and invasion. In diseases like cancer, the progressive breakdown of this coordination is often most clearly visualized as morphological disorganization at the FTU level<sup><xref ref-type="bibr" rid="R25">25</xref></sup>. Analyzing tissues through the lens of FTUs offers a more biologically grounded perspective, analogous to how modern natural language processing moved beyond word frequencies (“bag-of-words”) to understand contextual relationships across sentences and documents. Integrative analysis of FTU features holds promise for enhancing understanding of tumor progression and informing multi-cellular disease modeling.</p><p id="P28">Analysis at the FTU level is complimentary to tissue analysis approaches that operate at different granularities, such as cell-level or tile-level: While a tile-based model might predict a high-risk score for a region, FTU annotations can reveal if this is due to the presence of a certain type of tumor glands, specific stromal reactions, nerve invasion, or other structurally defined features. This allows for validation and interpretation of tile-based predictions in a language pathologists can interpret. However, unlike cell- or tile-based methods which benefit from existing datasets or established self-supervised techniques respectively, FTU-level analysis has been hindered by the lack of large-scale, detailed annotations. For this reason, methods incorporating FTU information during tissue analysis are sparse<sup><xref ref-type="bibr" rid="R15">15</xref>,<xref ref-type="bibr" rid="R26">26</xref></sup>. MiroSCOPE is designed to overcome this specific challenge by enabling efficient, large-scale curation of FTU annotations.</p><p id="P29">Standardizing the language for annotating FTUs provides a connection between modern AI approaches and clinical literature. A critical component of any annotation system is a standardized ontology with clarity, comprehensiveness and comprehensibility<sup><xref ref-type="bibr" rid="R27">27</xref></sup>. While ontologies for healthy FTUs and cell types are actively developing<sup><xref ref-type="bibr" rid="R28">28</xref>–<xref ref-type="bibr" rid="R30">30</xref></sup>, standardized descriptions for FTUs in disease states are largely lacking, despite extensive documentation in pathology literature. For this study, we developed a practical, 17-term controlled vocabulary for prostatic acinar adenocarcinoma FTUs, grounded in the Gleason system and encompassing key non-tumoral and stromal elements. This vocabulary proved readily adaptable to breast adenocarcinoma by substituting relevant terms.</p><p id="P30">The controlled vocabulary presented in the study is intended as an initial step. Future efforts should focus on developing comprehensive, shared FTU ontologies that formally connect to existing resources like the Cell Ontology (CL)<sup><xref ref-type="bibr" rid="R29">29</xref></sup> and UBERON<sup><xref ref-type="bibr" rid="R30">30</xref></sup>. Such an ontology should extend beyond hierarchical classification to capture cross-cutting morphological features (e.g., “foamy,” “microcystic,” corresponding to unusual but recognized subtypes we excluded initially for clarity) and pathological processes (e.g., “inflammation,” “loss of basal membrane”). This would bridge classical pathology knowledge with modern AI, enhance interpretability, and facilitate comparative studies across organs and diseases. Standardizing terminology is essential for building the large, reusable datasets needed to train robust FTU-level models.</p><p id="P31">To enable large-scale FTU annotation, MiroSCOPE combines a fine-tunable AI annotation model, UI features for curation, and a controlled vocabulary, creating a HITL system within the widely adopted QuPath environment. While general-purpose segmentation models like SAM provide a zero-shot starting point, we found their performance on complex pathology images, especially higher-grade tumors, can be limited. Additionally, existing FTU annotation models cannot reliably generalize to new datasets. Our HITL workflow allows pathologists to efficiently correct initial AI segmentation and classification, generating data to iteratively fine-tune the model. This cycle substantially accelerates the creation of high-quality annotations, and a specialized, high-performing model tailored to the specific task and dataset. In the future, we plan to expand on these features to offer collaboration options, better versioning and tracking, expanded controlled vocabularies and expanding AI models into other tumor types.</p><p id="P32">By using a transformer-based architecture for FTU segmentation and classification, we achieved sufficient accuracy to initiate a HITL workflow. A limited number of automated FTU segmentation algorithms exist, and most fail to cover all FTU presentations, including tumoral and stromal<sup><xref ref-type="bibr" rid="R13">13</xref>–<xref ref-type="bibr" rid="R16">16</xref></sup>. MiroSCOPE’s annotation workflow was employed to capture the entire spectrum of the prostate cancer disease state, in addition to features present in the surrounding stroma. As discussed in the results section, assessing the true performance of the AI model is challenging due to various modes of failure, class representation biases and high interobserver variability and inherent ambiguity in “ground truth” classifications. Importantly, we observed that the initial segmentation and classification performance achieved using MiroSCOPE was sufficient to enable a HITL workflow to iteratively improve the annotation rate and model performance. While it is challenging to measure generalized efficiency gains due to variations in tissue factors like size and cancer grade composition, our assessment on a subset of images found the final multiclass prostate model made the annotation process on average 3.18 more efficient, significantly speeding up the process. Additionally, models trained using prostate cancer images showed translation to breast cancer after fine-tuning with one image, demonstrating the applicability to other cancer types.</p><p id="P33">Using MiroSCOPE, we curated annotated prostate cancer images with segmentations and labeling of all tissue structures. We publicly release Miro-120, a high-quality dataset of 120 prostatic acinar adenocarcinoma H&amp;E with 30,568 annotations that comprehensively cover all non-tumoral, tumoral (with subpatterns), and stromal features of the tissue, offering a unique level of detail which is not present in existing datasets<sup><xref ref-type="bibr" rid="R31">31</xref></sup>. This dataset can be used by the community as a resource for FTU-level machine learning studies that incorporate Gleason subpatterns and stromal features of tissue structure.</p><p id="P34">In the future, MiroSCOPE can act as a common platform for building FTU datasets and analyzing them with modern deep-learning methodologies. Common data marketplaces such as the Protein Data Bank (PDB)<sup><xref ref-type="bibr" rid="R32">32</xref></sup>, the database of Genotypes and Phenotypes (dbGAP)<sup><xref ref-type="bibr" rid="R33">33</xref></sup>, or The Cancer Genome Atlas (TCGA)<sup><xref ref-type="bibr" rid="R34">34</xref></sup> project allow computational biologists to develop algorithms, servers, resources and tools once, but then apply them to a large number of diseases and datasets. Decades after their inception, these common resources remain highly central for their communities and are used every day for testing, evaluating, and applying algorithms. A similar opportunity exists here, where a common resource for well annotated pathology images can spur algorithm development to better understand the disease microenvironment across a large context. Topics we can address include disease prognosis, cell-cell dynamics within FTUs, 3D reconstruction across tissue sections, and higher-level interactions between FTUs and immune and neural processes. MiroSCOPE also holds the potential to facilitate clinically driven aims related to interobserver variability feedback and detection of border cases with high classification error.</p><p id="P35">We have demonstrated that it is possible to accelerate FTU curation with an iterative human-in-the-loop system even with modest datasets and limited expert annotation. There are substantial shared features between different FTUs, such as basal membrane, vasculature, or stroma. As we expand the curated datasets to other tumors and diseases, we expect the AI models to increasingly improve at understanding these common components. Our results indicate that modern ViT architectures can effectively transfer these learned features between disease contexts, further accelerating model training.</p></sec><sec id="S11" sec-type="methods"><title>Methods</title><sec id="S12"><title>Data</title><p id="P36">Whole slide image H&amp;E samples of prostatic acinar adenocarcinoma were sourced from three unique cohorts which are referred to as OHSU Biolibrary, CEDAR Biorepository, and UCL AS Cohort, making 184 images total. The OHSU Biolibrary data refers to 18 radical prostatectomy samples from the Oregon Health and Science University (OHSU) Knight Cancer Institute (KCI) Biolibrary (IRB#4918). The CEDAR Biorepository data refers to 102 needle biopsy samples obtained from the Cancer Early Detection Advanced Research (CEDAR) Specimen and Data Repository (IRB#18048). Samples of the OHSU Biolibrary and CEDAR Biorepository are collected from clinics across OHSU following informed consent and in accordance with OHSU institutional review boards. The UCL AS Cohort refers to 64 needle biopsy images which were collected at University College London (UCL) hospitals for Active Surveillance (AS). For the current study, all samples were de-identified, and work was not considered human subjects research.</p></sec><sec id="S13"><title>Software development</title><p id="P37">We followed the software architecture of the MONAI Label plugin for QuPath (<ext-link ext-link-type="uri" xlink:href="https://github.com/Project-MONAI/MONAILabel/tree/main/plugins/qupath">https://github.com/Project-MONAI/MONAILabel/tree/main/plugins/qupath</ext-link>) to develop our QuPath-based MiroSCOPE platform for AI-assisted annotation curation. To create the Java-based QuPath user interfaces, we forked the main branch of the qupath codebase (as of June 11, 2024), updated the default Java version to 21, and adopted the Gradle-based build scripts in the original QuPath codebase. We developed the Java-based code using IntelliJ IDEA (Community Edition, version 2024.1.4). For offering an AI annotation model for automated FTU segmentation and classification, we utilized MONAI Label’s Python-based architecture. Development was conducted in Python 3.9 within a Conda environment, using VS Code as the primary IDE. The Java code is hosted at <ext-link ext-link-type="uri" xlink:href="https://github.com/ohsu-cedar-comp-hub/qupath_monaillabel_plugin">https://github.com/ohsu-cedar-comp-hub/qupath_monaillabel_plugin</ext-link> the Python code at <ext-link ext-link-type="uri" xlink:href="https://github.com/ohsu-cedar-comp-hub/monailabel_cedar_app">https://github.com/ohsu-cedar-comp-hub/monailabel_cedar_app</ext-link>.</p><p id="P38">Initial iterations of the HITL system used the image viewer napari<sup><xref ref-type="bibr" rid="R35">35</xref></sup> as the interface for annotation correction. A custom napari plugin with UI for managing annotations was developed, and many features were transferred to the current MiroSCOPE QuPath extension.</p></sec><sec id="S14"><title>Training details</title><p id="P39">To demonstrate using MiroSCOPE for HITL curation, we fine-tuned the annotation model as ground truth annotation data was generated. Images were divided into tiles of dimension 1024×1024 as input for training, with 20% randomly assigned to the validation set. All training used AdamW optimization<sup><xref ref-type="bibr" rid="R36">36</xref></sup>, generic data augmentations, and identical parameters including a learning rate of 1×10<sup>-4</sup>, regularization weight of 1×10<sup>-5</sup>, and momentum of 0.99. Final model weights were chosen based on best validation accuracy. Either a Nvidia Tesla V100 32GB DRAM GPU or Nvidia A40 44GB DRAM GPU were used for computation. The curation system was primarily demonstrated with internal prostate cancer samples, but translatability to other cancer types was also piloted with breast cancer data from the TCGA-BRCA dataset. For fine-tuning with prostate cases, the Fg-bg1 checkpoint was trained using 10 OHSU Biolibrary samples, the Fg-bg2 checkpoint was trained using 16 OHSU Biolibrary samples, the multiclass model was trained using 18 OHSU Biolibrary and 91 CEDAR Biorepository samples. For fine-tuning with breast cases, a single TCGA-BRCA sample (TCGA-A2-A259) was used.</p></sec><sec id="S15"><title>Model evaluation</title><p id="P40">A holdout test set of 11 images from the Biorepository cohort which have representation across cancer grades was used for all prostate-specific model evaluations. For running inference for original SAM (vit-h), images were divided into 1200×1200 tiles with 5% overlap before running inference on each, then borders were resolved during restitching. For running inference with class promptable SAM (vit-b), a sliding window approach using patches of size 1024×1024 was used to create a seamless annotation mask. All masks underwent post processing to fill holes and filter objects below a pixel area threshold of 2000. To evaluate segmentation performance, the Dice similarity coefficient<sup><xref ref-type="bibr" rid="R37">37</xref></sup> was computed between the ground truth mask and predicted mask (<xref ref-type="fig" rid="F8">Figure 9A</xref>). For the multiclass model, the Dice was calculated in the same manner without considering class, and accuracy was determined by comparing the class assignment for the center pixel of the ground truth annotation with the same pixel of the predicted mask (<xref ref-type="fig" rid="F8">Figure 9B</xref>). For breast-specific model evaluations, a single TCGA-BRCA sample (TCGA-A2-A0ES) was used, and the segmentation and multiclass model performance analyses were performed in the same way.</p></sec><sec id="S16"><title>Efficiency evaluation</title><p id="P41">To assess gains in efficiency when using the platform, 18 images from the UCL AS Cohort were annotated in MiroSCOPE using AI assistance while all actions were tracked in detail. For each image, the number of manual annotations (<italic>A</italic><sub><italic>m</italic></sub>), which described annotations that were manually segmented or AI-generated annotations that were manually edited, and the total number of annotations (<italic>A</italic><sub><italic>t</italic></sub>), were recorded. The average manual segmentation time (<italic>t</italic>) was estimated at 10s per annotation (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 3</xref>). Calculations of percentage of manual annotations, efficiency gain, percentage of annotations fully generated by AI, and time saved were made for each image, then averaged to estimate more generalized gains of using MiroSCOPE across varied images.</p><p id="P42"><bold>percentage of manual annotations</bold> <inline-formula><mml:math id="M1"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula></p><p id="P43"><bold>efficiency gains</bold> <inline-formula><mml:math id="M2"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula></p><p id="P44"><bold>percentage of annotations fully generated by AI</bold> = 1 − <italic>P</italic><sub><italic>m</italic></sub></p><p id="P45"><bold>time saved</bold> =<italic>t</italic> × (<italic>A</italic><sub><italic>t</italic></sub> − <italic>A</italic><sub><italic>m</italic></sub>)</p></sec><sec id="S17"><title>Statistics</title><p id="P46">To measure the significance of the increase in segmentation performance with each fine-tuning, the Kruskal-Wallis test was performed for the holdout test Dice coefficient scores across the three models (SAM, Fg-bg1, Fg-bg2) and found a p-value of 6.7×10<sup>-6</sup>. To investigate more specifically which models have significant interactions, a pairwise Dunn’s test was done for each grouping, and Bonferroni correction was applied to adjust p values.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Figures</label><media xlink:href="EMS207337-supplement-Supplementary_Figures.pdf" mimetype="application" mime-subtype="pdf" id="d14aAcEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S20"><title>Acknowledgments</title><p>This project was supported by funding from the Cancer Early Detection Advanced Research Center (CEDAR Project ID# Full 2023-1748) at Oregon Health &amp; Science University, Knight Cancer Institute. S. Ece Eksi was supported by funding from the International Alliance for Cancer Early Detection (ACED) (ACED 2022-1508) at Oregon Health &amp; Science University, Knight Cancer Institute. Mark Emberton receives research support from the United Kingdom’s National Institute of Health Research (NIHR) UCLH/UCL Biomedical Research Centre. The research reported in this publication used computational infrastructure supported by the Office of Research Infrastructure Programs, Office of the Director, of the National Institutes of Health under Award Number S10OD034224. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.</p></ack><sec id="S18" sec-type="data-availability"><title>Data Availability</title><p id="P47">The Miro-120 dataset, and all other data and supporting files, are available at <ext-link ext-link-type="uri" xlink:href="https://www.synapse.org/Synapse:syn66298514/wiki/">https://www.synapse.org/Synapse:syn66298514/wiki/</ext-link>.</p></sec><sec id="S19" sec-type="data-availability"><title>Code Availability</title><p id="P48">The MiroSCOPE extension is open-source and available for download at <ext-link ext-link-type="uri" xlink:href="https://github.com/ohsu-cedar-comp-hub/qupath_monaillabel_plugin">https://github.com/ohsu-cedar-comp-hub/qupath_monaillabel_plugin</ext-link>. The MONAI Label back-end to support AI annotation within the platform, as well as scripts for model inference and fine-tuning, are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/ohsu-cedar-comp-hub/monailabel_cedar_app">https://github.com/ohsu-cedar-comp-hub/monailabel_cedar_app</ext-link>.</p></sec><fn-group><fn fn-type="con" id="FN1"><p id="P49"><bold>Author Contributions</bold></p><p id="P50"><bold>M.R. Fenner:</bold> Methodology, investigation, data curation, formal analysis, validation, visualization, project administration, writing – original draft, writing – review and editing. <bold>S. Sevim:</bold> Data curation, resources, writing – review and editing. <bold>G. Wu:</bold> Software, formal analysis, writing – review and editing. <bold>D. Beavers:</bold> Software, writing – review and editing. <bold>P. Guo:</bold> Methodology, investigation, validation. <bold>Y. Tang:</bold> Software. <bold>C.Z. Eddy:</bold> Software, investigation, writing – review and editing. <bold>K. Ait-Ahmad:</bold> Investigation. <bold>T. Rice-Stitt:</bold> Data curation. <bold>G. Thomas:</bold> Resources. <bold>M.J. Kuykendall:</bold> Resources. <bold>V. Stavrinides</bold>: Resources. <bold>M. Emberton:</bold> Resources. <bold>D. Xu:</bold> Supervision. <bold>X. Song:</bold> Conceptualization, supervision, funding acquisition. <bold>S.E. Eksi:</bold> Conceptualization, supervision, funding acquisition, writing – review and editing. <bold>E. Demir:</bold> Conceptualization, supervision, project administration, funding acquisition, writing – original draft, writing – review and editing.</p></fn><fn fn-type="conflict" id="FN2"><p id="P51"><bold>Competing Interests</bold></p><p id="P52">The authors have no competing interests to declare.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Bono</surname><given-names>B</given-names></name><name><surname>Grenon</surname><given-names>P</given-names></name><name><surname>Baldock</surname><given-names>R</given-names></name><name><surname>Hunter</surname><given-names>P</given-names></name></person-group><article-title>Functional tissue units and their primary tissue motifs in multi-scale physiology</article-title><source>J Biomed Semantics</source><year>2013</year><volume>4</volume><fpage>22</fpage><pub-id pub-id-type="doi">10.1186/2041-1480-4-22</pub-id><pub-id pub-id-type="pmcid">PMC4126067</pub-id><pub-id pub-id-type="pmid">24103658</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>H</given-names></name><etal/></person-group><article-title>A whole-slide foundation model for digital pathology from real-world data</article-title><source>Nature</source><year>2024</year><volume>630</volume><fpage>181</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1038/s41586-024-07441-w</pub-id><pub-id pub-id-type="pmcid">PMC11153137</pub-id><pub-id pub-id-type="pmid">38778098</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>RJ</given-names></name><etal/></person-group><article-title>Towards a general-purpose foundation model for computational pathology</article-title><source>Nat Med</source><year>2024</year><volume>30</volume><fpage>850</fpage><lpage>862</lpage><pub-id pub-id-type="doi">10.1038/s41591-024-02857-3</pub-id><pub-id pub-id-type="pmcid">PMC11403354</pub-id><pub-id pub-id-type="pmid">38504018</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>X</given-names></name><etal/></person-group><article-title>End-to-end prognostication in colorectal cancer by deep learning: a retrospective, multicentre study</article-title><source>The Lancet Digital Health</source><year>2024</year><volume>6</volume><fpage>e33</fpage><lpage>e43</lpage><pub-id pub-id-type="pmid">38123254</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saillard</surname><given-names>C</given-names></name><etal/></person-group><article-title>Predicting Survival After Hepatocellular Carcinoma Resection Using Deep Learning on Histological Slides</article-title><source>Hepatology</source><year>2020</year><volume>72</volume><fpage>2000</fpage><pub-id pub-id-type="pmid">32108950</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Unsupervised and supervised discovery of tissue cellular neighborhoods from cell phenotypes</article-title><source>Nat Methods</source><year>2024</year><volume>21</volume><fpage>267</fpage><lpage>278</lpage><pub-id pub-id-type="doi">10.1038/s41592-023-02124-2</pub-id><pub-id pub-id-type="pmcid">PMC10864185</pub-id><pub-id pub-id-type="pmid">38191930</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schürch</surname><given-names>CM</given-names></name><etal/></person-group><article-title>Coordinated Cellular Neighborhoods Orchestrate Antitumoral Immunity at the Colorectal Cancer Invasive Front</article-title><source>Cell</source><year>2020</year><volume>182</volume><fpage>1341</fpage><lpage>1359</lpage><elocation-id>e19</elocation-id><pub-id pub-id-type="doi">10.1016/j.cell.2020.07.005</pub-id><pub-id pub-id-type="pmcid">PMC7479520</pub-id><pub-id pub-id-type="pmid">32763154</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Black</surname><given-names>S</given-names></name><etal/></person-group><article-title>CODEX multiplexed tissue imaging with DNA-conjugated antibodies</article-title><source>Nat Protoc</source><year>2021</year><volume>16</volume><fpage>3802</fpage><lpage>3835</lpage><pub-id pub-id-type="doi">10.1038/s41596-021-00556-8</pub-id><pub-id pub-id-type="pmcid">PMC8647621</pub-id><pub-id pub-id-type="pmid">34215862</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>H</given-names></name><etal/></person-group><article-title>Multiplex immunofluorescence and single-cell transcriptomic profiling reveal the spatial cell interaction networks in the non-small cell lung cancer microenvironment</article-title><source>Clin Transl Med</source><year>2023</year><volume>13</volume><elocation-id>e1155</elocation-id><pub-id pub-id-type="doi">10.1002/ctm2.1155</pub-id><pub-id pub-id-type="pmcid">PMC9806015</pub-id><pub-id pub-id-type="pmid">36588094</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salvi</surname><given-names>M</given-names></name><etal/></person-group><article-title>A hybrid deep learning approach for gland segmentation in prostate histopathological images</article-title><source>Artif Intell Med</source><year>2021</year><volume>115</volume><elocation-id>102076</elocation-id><pub-id pub-id-type="pmid">34001325</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>H</given-names></name><etal/></person-group><article-title>Masked Image Modeling Meets Self-Distillation: A Transformer-Based Prostate Gland Segmentation Framework for Pathology Slides</article-title><source>Cancers (Basel)</source><year>2024</year><volume>16</volume><elocation-id>3897</elocation-id><pub-id pub-id-type="doi">10.3390/cancers16233897</pub-id><pub-id pub-id-type="pmcid">PMC11640496</pub-id><pub-id pub-id-type="pmid">39682085</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qiu</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Automatic Prostate Gleason Grading Using Pyramid Semantic Parsing Network in Digital Histopathology</article-title><source>Front Oncol</source><year>2022</year><volume>12</volume><elocation-id>772403</elocation-id><pub-id pub-id-type="doi">10.3389/fonc.2022.772403</pub-id><pub-id pub-id-type="pmcid">PMC9024330</pub-id><pub-id pub-id-type="pmid">35463378</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jain</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Segmenting functional tissue units across human organs using community-driven development of generalizable machine learning algorithms</article-title><source>Nat Commun</source><year>2023</year><volume>14</volume><elocation-id>4656</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-023-40291-0</pub-id><pub-id pub-id-type="pmcid">PMC10400613</pub-id><pub-id pub-id-type="pmid">37537179</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Godwin</surname><given-names>LL</given-names></name><etal/></person-group><article-title>Robust and generalizable segmentation of human functional tissue units</article-title><year>2021</year><elocation-id>2021.11.09.467810</elocation-id><pub-id pub-id-type="doi">10.1101/2021.11.09.467810</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferrero</surname><given-names>A</given-names></name><etal/></person-group><article-title>HistoEM: A Pathologist-Guided and Explainable Workflow Using Histogram Embedding for Gland Classification</article-title><source>Modern Pathology</source><year>2024</year><volume>37</volume><elocation-id>100447</elocation-id><pub-id pub-id-type="pmid">38369187</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barmpoutis</surname><given-names>P</given-names></name><etal/></person-group><article-title>A digital pathology workflow for the segmentation and classification of gastric glands: Study of gastric atrophy and intestinal metaplasia cases</article-title><source>PLoS One</source><year>2022</year><volume>17</volume><elocation-id>e0275232</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0275232</pub-id><pub-id pub-id-type="pmcid">PMC9803139</pub-id><pub-id pub-id-type="pmid">36584163</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ochi</surname><given-names>M</given-names></name><name><surname>Komura</surname><given-names>D</given-names></name><name><surname>Ishikawa</surname><given-names>S</given-names></name></person-group><article-title>Pathology Foundation Models</article-title><source>JMA J</source><year>2025</year><volume>8</volume><fpage>121</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.31662/jmaj.2024-0206</pub-id><pub-id pub-id-type="pmcid">PMC11799676</pub-id><pub-id pub-id-type="pmid">39926091</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><collab>Prostate Cancer Trialists’ Collaborative Group</collab><article-title>Maximum androgen blockade in advanced prostate cancer: an overview of 22 randomised trials with 3283 deaths in 5710 patients</article-title><source>Lancet</source><year>1995</year><volume>346</volume><fpage>265</fpage><lpage>269</lpage><pub-id pub-id-type="pmid">7630245</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gleason</surname><given-names>DF</given-names></name></person-group><article-title>Classification of prostatic carcinomas</article-title><source>Cancer Chemother Rep</source><year>1966</year><volume>50</volume><fpage>125</fpage><lpage>128</lpage><pub-id pub-id-type="pmid">5948714</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epstein</surname><given-names>JI</given-names></name><name><surname>Allsbrook</surname><given-names>WC</given-names></name><name><surname>Amin</surname><given-names>MB</given-names></name><name><surname>Egevad</surname><given-names>LL</given-names></name></person-group><article-title>The 2005 International Society of Urological Pathology (ISUP) Consensus Conference on Gleason Grading of Prostatic Carcinoma</article-title><year>2005</year><volume>29</volume><pub-id pub-id-type="pmid">16096414</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epstein</surname><given-names>JI</given-names></name><etal/></person-group><article-title>The 2014 International Society of Urological Pathology (ISUP) Consensus Conference on Gleason Grading of Prostatic Carcinoma: Definition of Grading Patterns and Proposal for a New Grading System</article-title><source>Am J Surg Pathol</source><year>2016</year><volume>40</volume><fpage>244</fpage><lpage>252</lpage><pub-id pub-id-type="pmid">26492179</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bankhead</surname><given-names>P</given-names></name><etal/></person-group><article-title>QuPath: Open source software for digital pathology image analysis</article-title><source>Sci Rep</source><year>2017</year><volume>7</volume><elocation-id>16878</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-017-17204-5</pub-id><pub-id pub-id-type="pmcid">PMC5715110</pub-id><pub-id pub-id-type="pmid">29203879</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diaz-Pinto</surname><given-names>A</given-names></name><etal/></person-group><article-title>MONAI Label: A framework for AI-assisted interactive labeling of 3D medical images</article-title><source>Medical Image Analysis</source><year>2024</year><volume>95</volume><elocation-id>103207</elocation-id><pub-id pub-id-type="pmid">38776843</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Kirillov</surname><given-names>A</given-names></name><etal/></person-group><source>Segment Anything</source><year>2023</year><comment>Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2304.02643v1">https://arxiv.org/abs/2304.02643v1</ext-link></comment></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanahan</surname><given-names>D</given-names></name><name><surname>Weinberg</surname><given-names>RA</given-names></name></person-group><article-title>Hallmarks of cancer: the next generation</article-title><source>Cell</source><year>2011</year><volume>144</volume><fpage>646</fpage><lpage>674</lpage><pub-id pub-id-type="pmid">21376230</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sobral-Leite</surname><given-names>M</given-names></name><etal/></person-group><article-title>A morphometric signature to identify ductal carcinoma in situ with a low risk of progression</article-title><source>npj Precis Onc</source><year>2025</year><volume>9</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1038/s41698-024-00769-6</pub-id><pub-id pub-id-type="pmcid">PMC11775207</pub-id><pub-id pub-id-type="pmid">39875514</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Demir</surname><given-names>E</given-names></name><etal/></person-group><article-title>An ontology for collaborative construction and analysis of cellular pathways</article-title><source>Bioinformatics</source><year>2004</year><volume>20</volume><fpage>349</fpage><lpage>356</lpage><pub-id pub-id-type="pmid">14960461</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Osumi-Sutherland</surname><given-names>D</given-names></name><etal/></person-group><article-title>Cell type ontologies of the Human Cell Atlas</article-title><source>Nat Cell Biol</source><year>2021</year><volume>23</volume><fpage>1129</fpage><lpage>1135</lpage><pub-id pub-id-type="pmid">34750578</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diehl</surname><given-names>AD</given-names></name><etal/></person-group><article-title>The Cell Ontology 2016: enhanced content, modularization, and ontology interoperability</article-title><source>J Biomed Semantics</source><year>2016</year><volume>7</volume><fpage>44</fpage><pub-id pub-id-type="doi">10.1186/s13326-016-0088-7</pub-id><pub-id pub-id-type="pmcid">PMC4932724</pub-id><pub-id pub-id-type="pmid">27377652</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mungall</surname><given-names>CJ</given-names></name><name><surname>Torniai</surname><given-names>C</given-names></name><name><surname>Gkoutos</surname><given-names>GV</given-names></name><name><surname>Lewis</surname><given-names>SE</given-names></name><name><surname>Haendel</surname><given-names>MA</given-names></name></person-group><article-title>Uberon, an integrative multi-species anatomy ontology</article-title><source>Genome Biology</source><year>2012</year><volume>13</volume><fpage>R5</fpage><pub-id pub-id-type="doi">10.1186/gb-2012-13-1-r5</pub-id><pub-id pub-id-type="pmcid">PMC3334586</pub-id><pub-id pub-id-type="pmid">22293552</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>L</given-names></name><etal/></person-group><article-title>Harnessing artificial intelligence for prostate cancer management</article-title><source>Cell Reports Medicine</source><year>2024</year><volume>5</volume><elocation-id>101506</elocation-id><pub-id pub-id-type="doi">10.1016/j.xcrm.2024.101506</pub-id><pub-id pub-id-type="pmcid">PMC11031422</pub-id><pub-id pub-id-type="pmid">38593808</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><collab>wwPDB consortium</collab><article-title>Protein Data Bank: the single global archive for 3D macromolecular structure data</article-title><source>Nucleic Acids Research</source><year>2019</year><volume>47</volume><fpage>D520</fpage><lpage>D528</lpage><pub-id pub-id-type="doi">10.1093/nar/gky949</pub-id><pub-id pub-id-type="pmcid">PMC6324056</pub-id><pub-id pub-id-type="pmid">30357364</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mailman</surname><given-names>MD</given-names></name><etal/></person-group><article-title>The NCBI dbGaP database of genotypes and phenotypes</article-title><source>Nat Genet</source><year>2007</year><volume>39</volume><fpage>1181</fpage><lpage>1186</lpage><pub-id pub-id-type="doi">10.1038/ng1007-1181</pub-id><pub-id pub-id-type="pmcid">PMC2031016</pub-id><pub-id pub-id-type="pmid">17898773</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weinstein</surname><given-names>JN</given-names></name><etal/></person-group><article-title>The Cancer Genome Atlas Pan-Cancer analysis project</article-title><source>Nat Genet</source><year>2013</year><volume>45</volume><fpage>1113</fpage><lpage>1120</lpage><pub-id pub-id-type="doi">10.1038/ng.2764</pub-id><pub-id pub-id-type="pmcid">PMC3919969</pub-id><pub-id pub-id-type="pmid">24071849</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chiu</surname><given-names>C-L</given-names></name><name><surname>Clack</surname><given-names>N</given-names></name><collab>the napari community</collab></person-group><article-title>napari: a Python Multi-Dimensional Image Viewer Platform for the Research Community</article-title><source>Microscopy and Microanalysis</source><year>2022</year><volume>28</volume><fpage>1576</fpage><lpage>1577</lpage></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loshchilov</surname><given-names>I</given-names></name><name><surname>Hutter</surname><given-names>F</given-names></name></person-group><article-title>Decoupled Weight Decay Regularization</article-title><year>2019</year><pub-id pub-id-type="doi">10.48550/arXiv.1711.05101</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dice</surname><given-names>LR</given-names></name></person-group><article-title>Measures of the Amount of Ecologic Association Between Species</article-title><source>Ecology</source><year>1945</year><volume>26</volume><fpage>297</fpage><lpage>302</lpage></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Workflow for employing MiroSCOPE.</title><p>MiroSCOPE is a QuPath extension that enables the large-scale annotation of FTUs on H&amp;E images. Using a MONAI Label framework, the extension integrates a SAM-based annotation model for automatic segmentation and classification and also offers user interface features specific to the curation task. By enabling a human-in-the-loop approach with expert corrections by a pathologist, a high-quality annotated dataset and fine-tuned model are generated. These outputs can be valuable assets for downstream applications, including disease characterization modeling and clinical tool development.</p></caption><graphic xlink:href="EMS207337-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Hierarchical organization of prostate FTU categories defined by the controlled vocabulary.</title><p>The controlled vocabulary contains 17 terms covering FTUs observed in prostate tissue, with tumoral labeling focusing on prostatic acinar adenocarcinoma. PIN: prostatic intraepithelial neoplasia.</p></caption><graphic xlink:href="EMS207337-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>The QuPath-based MiroSCOPE platform offers enhanced user interfaces for annotating FTUs.</title><p>Key features within the MiroSCOPE extension tab are highlighted.</p></caption><graphic xlink:href="EMS207337-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Evaluation of foreground-background model segmentation performance.</title><p>Dice coefficient scores were calculated on a holdout dataset of prostate cancer H&amp;Es (n=11) to evaluate the performance of SAM (vit-h) versus fine-tuned foreground-background SAM (vit-b) for segmenting FTUs. Checkpoints Fg-bg1 and Fg-bg2 of the foreground-background model were trained as more annotated H&amp;E data was curated. Dice results are visualized as a box plot, where each image from the holdout set is represented by an overlayed point and coded by cancer grade (low: Gleason score &lt;= 6, medium: Gleason score = 7, high: Gleason score &gt; 7). Mean Dice scores for the SAM, Fg-bg1, and Fg-bg2 models were 0.27, 0.86, and 0.91 respectively. A pairwise Dunn’s test with Bonferroni correction between Dice scores for each model found significant differences between SAM and Fg-bg1 (p=0.0039) and SAM and Fg-bg2 (p= 5×10<sup>-6</sup>), but no significant difference between Fg-bg1 and Fg-bg2 (p=0.352).</p></caption><graphic xlink:href="EMS207337-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Evaluation of multiclass model segmentation and classification performance.</title><p>The fine-tuned multiclass SAM (vit-b) model was trained to classify 17 categories of prostate cancer FTUs and evaluated on a holdout dataset of H&amp;Es with prostatic acinar adenocarcinoma (n=11). To evaluate the practical improvements of using MiroSCOPE with the multiclass model, the annotation process of a separate efficiency evaluation dataset of 18 H&amp;Es with prostatic acinar adenocarcinoma was tracked in detail. <bold>A</bold> Visualization of multiclass model predicted masks on holdout H&amp;Es compared to ground truth masks for different cancer grades (no tumor, low-grade: Gleason score &lt;= 6, medium-grade: Gleason score = 7, high-grade: Gleason score &gt; 7). <bold>B</bold> Boxplot of Dice similarity coefficient scores calculated to evaluate segmentation performance. Each image from the holdout set is represented by an overlayed point and coded by cancer grade. The mean Dice across samples is 0.81. <bold>C</bold> Row-normalized confusion matrix visualizing classification accuracy for FTUs. Gleason pattern 5 labels and Gleason pattern 4 glomeruloids, which were underrepresented or absent from the training set, are not visualized (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 1B</xref>). A full unnormalized confusion matrix can be found in <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 1A</xref>. GP: Gleason pattern; PIN: prostatic intraepithelial neoplasia. <bold>D</bold> Classification accuracy plotted against the number of annotations used for training, with each class represented by a labeled point. <bold>E</bold> Box plot of the percentage of annotations in each image from the efficiency evaluation dataset whose segmentations were fully generated by AI, requiring no manual intervention. The overall mean was 64%.</p></caption><graphic xlink:href="EMS207337-f005"/></fig><fig id="F6" position="float"><label>Figure 7</label><caption><title>Annotation class counts for the Miro-120 dataset.</title><p>The number of FTU annotations for each class label in the controlled vocabulary is visualized. The dataset consists of 120 prostatic acinar adenocarcinoma H&amp;E samples sourced from two cohorts (OHSU Biolibrary (n=18); CEDAR Biorepository (n=102)) which are distinguished by color. The total annotation count across all classes is 30,568.</p></caption><graphic xlink:href="EMS207337-f006"/></fig><fig id="F7" position="float"><label>Figure 8</label><caption><title>FTU categories and multiclass model classification performance for breast cancer.</title><p>The FTU controlled vocabulary structure was adapted for breast cancer, then to assess translatability of the annotation system the AI annotation model was fine-tuned with a single TCGA prostate cancer H&amp;E sample and classification performance was evaluated on another. <bold>A</bold> Hierarchical organization of breast FTU categories defined by the controlled vocabulary. *Solid tumor category encompasses multiple high-grade patterns (e.g. nests, cords, sheets). UDH: usual ductal hyperplasia; DCIS: ductal carcinoma in-situ; LCIS: lobular carcinoma in-situ, ADH: atypical ductal hyperplasia. <bold>B</bold> Row-normalized confusion matrix visualizing classification accuracy on one breast adenocarcinoma H&amp;E sample from the TCGA-BRCA cohort following fine-tuning with another. Classes with less than 50 annotated instances for training are not shown (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 2B</xref>); a full unnormalized confusion matrix can be found in <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 2A</xref>. DCIS: ductal carcinoma in-situ.</p></caption><graphic xlink:href="EMS207337-f007"/></fig><fig id="F8" position="float"><label>Figure 9</label><caption><title>Evaluation metrics for segmentation and classification.</title><p><bold>A</bold> Segmentation performance is measured using the Dice similarity coefficient, which is calculated by taking twice the overlap between the ground truth and predicted masks and dividing it by the total area between both. <bold>B</bold> Classification performance is measured by determining whether the center pixel of a ground truth annotation is a “hit” or “miss” when compared to the same pixel of the predicted mask, then accuracy is calculated to be the number of hits over the total number of annotations.</p></caption><graphic xlink:href="EMS207337-f008"/></fig></floats-group></article>