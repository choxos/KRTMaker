<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS205799</article-id><article-id pub-id-type="doi">10.1101/2025.05.15.654220</article-id><article-id pub-id-type="archive">PPR1023982</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Behavioral Time Scale Synaptic Plasticity (BTSP) endows Hyperdimensional Computing with attractor features</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Yu</surname><given-names>Chengting</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><email>chengting.21@intl.zju.edu.cn</email></contrib><contrib contrib-type="author"><name><surname>Wu</surname><given-names>Yujie</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A3">3</xref><email>wu-yj16@tsinghua.org.cn</email></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Aili</given-names></name><xref ref-type="aff" rid="A2">2</xref><email>ailiwang@intl.zju.edu.cn</email></contrib><contrib contrib-type="author"><name><surname>Maass</surname><given-names>Wolfgang</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">†</xref></contrib></contrib-group><aff id="A1"><label>1</label>Institute of Machine Learning and Neural Computation, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/00d7xrm67</institution-id><institution>Graz University of Technology</institution></institution-wrap>, <city>Graz</city>, <country country="AT">Austria</country></aff><aff id="A2"><label>2</label>College of Information Science and Electronic Engineering, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/00a2xv884</institution-id><institution>Zhejiang University</institution></institution-wrap>, <city>Hangzhou</city>, <country country="CN">China</country></aff><aff id="A3"><label>3</label>Department of Computing, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/0030zas98</institution-id><institution>The Hong Kong Polytechnic University</institution></institution-wrap>, <city>Hong Kong</city> SAR, <country country="CN">China</country></aff><author-notes><corresp id="CR1">
<label>†</label>Corresponding author <email>wolfgang.maass@tugraz.at</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>23</day><month>05</month><year>2025</year></pub-date><pub-date pub-type="preprint"><day>20</day><month>05</month><year>2025</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Hyperdimensional computing (HDC) addresses massively parallel implementations of symbolic computations that are both more transparent than ANNs and LLMs and more suitable for in-memory computing on highly energy-efficient analog hardware. It captures an essential aspects of brain computations: objects, concepts, and their attributes are encoded by very sparse distributed representations. But currently known methods for binding these tokens together suffer from functional deficiencies of the commonly used algebraic methods. We show that a mechanism which the brain employs for binding, Behavioral Time Scale Synaptic Plasticity (BTSP), overcomes these deficiencies by adding attractor features to high-dimensional representations. They drastically improve the capability to recover from composed representations the tokens which have been bound together in them. One arrives in this way at a functionally more powerful HDC paradigm that provides new perspectives both for understanding how brains carry out symbolic computations, and for implementing them in novel energy-efficient and massively parallel neuromorphic hardware.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><label>1</label><title>Introduction</title><p id="P2">Most higher cognitive functions of the human brain rely on some form of compositional or symbolic computation, where entities such as symbols, objects, persons, events, as well as features of them and relations between them, are bound together into composed representations. We will refer to these entities also as tokens or items in the following. Like a sentences in natural language, these composed representations acquire new meanings and functional roles that go beyond those of their individual entities (<xref ref-type="bibr" rid="R12">Frankland and Greene, 2020a</xref>; <xref ref-type="bibr" rid="R26">Kurth-Nelson et al., 2023</xref>; <xref ref-type="bibr" rid="R20">Kazanina and Poeppel, 2023</xref>). Most neural network models and LLMs have problems to reproduce this type of combinatorial coding and compositional computing, which has been argued to contribute to their deficits regarding transparency and trustworthiness.</p><p id="P3">An essential feature of these brain computations is that they are carried out on very sparse neural codes where the firing of just a few among millions of neurons encodes salient objects or concepts, and also structural information such as relations between them, temporal or spatial locations, or semantic roles of words in a sentence. This perspective had inspired models for computation on high-dimensional (high-D) binary vectors that mostly consist of 0’s, such as the sparse distributed memory models of (<xref ref-type="bibr" rid="R19">Kanerva, 1988</xref>) and the holographic reduced representations of (<xref ref-type="bibr" rid="R35">Plate, 1995</xref>). These approaches inspired quite a bit of further work that gave rise to the research areas that are now commonly referred to as hyperdimensional computing (HDC) or vector symbolic architectures (VSAs) (<xref ref-type="bibr" rid="R23">Kleyko et al., 2022</xref>, <xref ref-type="bibr" rid="R22">2023</xref>). Both terms are viewed as synonyms, we use for simplicity only the term HDC. A prominent special case are the semantic pointer architecture that underlie the neural engineering framework of (<xref ref-type="bibr" rid="R9">Eliasmith and Anderson, 2003</xref>; <xref ref-type="bibr" rid="R10">Eliasmith et al., 2012</xref>). The computational operations that have so far been carried out in HDC for binding high-D vectors, such as convolution or permutation, as well as component-wise addition, exclusive OR, or multiplication, are of an algebraic nature. These are hard to interpret as operations in neural networks of the brain, but more importantly, they induce serious functional deficits, such as the incapability to decode the items that are bound together in a composed representation without providing also some of the items as a cue, or the incapability to recover a composed representation by just providing a small fraction of the items as cue. These functional deficits arise from the mathematical fact that one cannot recover from a sum, product, or exclusive OR of two bits each of these two bits. Rather, one of these bits has to given as cue for recovering the other one.</p><p id="P4">We present here a new approach to HDC where algebraic operations are replaced by a mechanism that the brain employs for binding, Behavioral Time Scale Synaptic Plasticity (BTSP) (<xref ref-type="bibr" rid="R2">Bittner et al., 2015</xref>, <xref ref-type="bibr" rid="R3">2017</xref>; <xref ref-type="bibr" rid="R41">Zhao et al., 2022</xref>). BTSP was more recently discovered than Hebbian plasticity or STDP (Spike Timing Dependent Plasticity). Furthermore, it is one of very few synaptic plasticity mechanisms that could be demonstrated in-vivo, even in the awake and operating adult brain, see (<xref ref-type="bibr" rid="R29">Magee and Grienberger, 2020</xref>; <xref ref-type="bibr" rid="R6">Chéreau et al., 2022</xref>) for reviews. In particular, experimental data show that the brain uses BTSP for binding information (<xref ref-type="bibr" rid="R2">Bittner et al., 2015</xref>; <xref ref-type="bibr" rid="R41">Zhao et al., 2022</xref>).</p><p id="P5">In contrast to other synaptic plasticity mechanisms, BTSP is a one-shot mechanism that modifies synaptic weights instantly, after a single trial (<xref ref-type="bibr" rid="R3">Bittner et al., 2017</xref>). Another unique feature of BTSP is that it is triggered in area CA1 of the hippocampus by largely stochastic gating signals from another brain area, the entorhinal cortex (<xref ref-type="bibr" rid="R15">Grienberger and Magee, 2022</xref>). We exploit that this largely stochastic gating enables BTSP to combine functional benefits of random projections and random hashing, i.e., of widely used methods in computer science for distributing information over large memory systems, with additional features that are reminiscent of attractors in dynamical systems. More specifically, we show that this attractor feature of BTSP enables operations on composed representations in HDC, such us top-down unbinding and bottom-up unbinding without providing also the composed representation as cue, that brains can apparently accomplish, but which are not within reach of previously proposed HDC methods for binding. Furthermore, we show that these functional contributions of BTSP can be captured with a simple simple rule for BTSP that only requires binary weights (<xref ref-type="bibr" rid="R40">Wu and Maass, 2025</xref>). This enables implementation of the BTSP-enhanced version of HDC by in-memory computing with arrays of highly energy-efficient memristors even if these can assume only a few different conductance states in a reliable manner. Important examples for that are phase-change memristors as described in (<xref ref-type="bibr" rid="R39">Wang et al., 2020</xref>; <xref ref-type="bibr" rid="R21">Khaddam-Aljameh et al., 2022</xref>).</p><p id="P6">The BTSP-enhanced version of HDC also contributes new mechanisms and functional capabilities to models that aim at reproducing higher cognitive functions of the brain in simplified models that are analytically tractable, see e.g. (<xref ref-type="bibr" rid="R38">Valiant, 2000</xref>; <xref ref-type="bibr" rid="R33">Papadimitriou et al., 2020</xref>). Experimental data suggest that these tokens of meaning are encoded by very sparse distributed assemblies of neurons that fire when a particular concept is invoked. A prominent example are assemblies of concept cells (<xref ref-type="bibr" rid="R36">Quiroga, 2012</xref>; <xref ref-type="bibr" rid="R11">Franch et al., 2025</xref>). Hence, if the current firing activity of all neurons in the brain is represented by a high-D vector, the activation of all neurons in an assembly is represented by a sparse high-D vector. A fundamental operation in these models is the binding of two assemblies, or of an assembly of concept cells with a assembly of neurons that encodes structural information (<xref ref-type="bibr" rid="R31">Müller et al., 2020</xref>; <xref ref-type="bibr" rid="R20">Kazanina and Poeppel, 2023</xref>). Hence binding of high-D vectors is a key issue in these abstract models for brain computation (<xref ref-type="bibr" rid="R34">Piantadosi et al., 2024</xref>). This binding operation has commonly been modeled by an iterative process, where reciprocal connections between neurons of two assemblies are modified by STDP (<xref ref-type="bibr" rid="R38">Valiant, 2000</xref>; <xref ref-type="bibr" rid="R33">Papadimitriou et al., 2020</xref>; <xref ref-type="bibr" rid="R32">Papadimitriou and Friederici, 2022</xref>). But such an iterative process is neither consistent with the capability of brains to form composed representations instantly, nor with the large number of iterations that are needed to modify in experimental studies a synaptic weight by STDP (<xref ref-type="bibr" rid="R14">Froemke et al., 2010</xref>). Our BTSP-enhanced version of HDC provides a simpler, biologically more plausible, and functionally advantageous binding operation for these models. Interestingly, this binding operation does not modify synaptic weights between the neurons of two assemblies in the neocortex, but synaptic weights to and from other large pools of neurons, such as the hippocampus. In other words, the BTSP-enhanced variant of HDC supports a model for brain computation that is consistent with the ”indexing theory”, whereby hippocampal neurons form conjunctive codes that act as pointers to item representations in the neocortex. Recent experimental work has shown that this indexing theory is consistent with the way how the human brain carries out binding operations (<xref ref-type="bibr" rid="R24">Kolibius et al., 2023</xref>).</p><p id="P7">BTSP-enhanced HDC also provides a fresh perspective for models of natural language processing in the human brain. For example, it had been proposed that the grammar of natural languages only requires a single operation, the Merge operation (<xref ref-type="bibr" rid="R7">Chomsky et al., 2023</xref>; <xref ref-type="bibr" rid="R28">Liu et al., 2023</xref>). So far the Merge operation has been modeled as iterative process via STDP (<xref ref-type="bibr" rid="R32">Papadimitriou and Friederici, 2022</xref>). But this appears to be in conflict with the need to carry out Merge instantaneously during language processing. Hence BTSP-enhanced HDC appears to provide a more suitable framework for modeling the implementation of the Merge operation in neural networks of the brain.</p><p id="P8">We will first review fundamental properties of experimental data and models for BTSP that are essential for its binding capability, in particular the attractor features that it contributes. We then demonstrate the capability of BTSP to enhance fundamental operations of compositional computing, such as top-down unbinding and bottom-up unbinding. We then show that bottom-up unbinding with ambiguous cues can be modeled as a dynamic process where possible solutions are generated in a sequential manner. Finally, we examine the capability of BTSP to support hierarchical binding and unbinding. In the last section we discuss possible applications of BTSP-based binding in an assembly-based brain calculus for higher cognitive functions, in particular for language processing.</p></sec><sec id="S2" sec-type="results"><label>2</label><title>Results</title><sec id="S3"><label>2.1</label><title>Fundamental properties of BTSP that are essential for its binding capability</title><p id="P9">Experimental data show that the brain uses BTSP to form conjunctive representations of diverse content that is encoded by sparse activity in areas CA3 (‘input neurons”) through sparse representations in area CA1 (‘memory neurons”) (<xref ref-type="bibr" rid="R2">Bittner et al., 2015</xref>, <xref ref-type="bibr" rid="R3">2017</xref>; <xref ref-type="bibr" rid="R41">Zhao et al., 2022</xref>), see <xref ref-type="fig" rid="F1">Fig. 1A</xref> for a network scheme. We use here the term “memory neurons” for those neurons in area CA1 that are recruited for composed representations in order maintain consistency with the notation in (<xref ref-type="bibr" rid="R40">Wu and Maass, 2025</xref>). BTSP differs in several fundamental properties from plasticity rules that are commonly considered in neural network models. One is, that it does not require repeated representations of the same input or input/output pairing as STDP or other commonly considered plasticity rules. Rather, it creates new representations in a single shot. Obviously, this property is essential for binding and compositional computing in general, since new combinations of familiar items, referred to as words in the following, have to be learned in one shot, for example when we hear or read a sentence.</p><p id="P10">Another fundamental difference between BTSP and previously considered learning rules such as Hebb or STDP is that it does not depend on postsynaptic neuronal activity. A positive influence of postsynaptic firing on LTP would entail that the same neurons are recruited for many composed representations, since their recruitment for some of them would increase their incoming weights through LTP, and hence increase the chance that they also fire during the formation of further composed representations. Instead, BTSP is gated by dendritic plateau potentials in the postsynaptic neuron that are triggered by largely stochastic synaptic inputs from layer 3 of the entorhinal cortex (<xref ref-type="bibr" rid="R15">Grienberger and Magee, 2022</xref>). The stochastic feature of these gating signal induces a more uniform distribution of the recruitment of memory neurons for composed representations. These gating signals open the gate for synaptic plasticity through BTSP for several seconds before and after its onset(<xref ref-type="bibr" rid="R3">Bittner et al., 2017</xref>; <xref ref-type="bibr" rid="R29">Magee and Grienberger, 2020</xref>; <xref ref-type="bibr" rid="R30">Milstein et al., 2021</xref>). We need this long plasticity in our binding model only implicitly, since we work in our subsequent evaluations for simplicity with batch inputs <italic>x</italic> whose bits arrive simultaneously in the model. In the brain the long plasticity window of BTSP allows to collect content information that arrives from several brain areas on a behavioral time scale of seconds, and this long integration time for plasticity would also be useful for applications in artificial devices in order to bind input components from different sensory modalities and other network modules that do not arrive simultaneously, or for binding a sequence of frames together as suggested by recent experimental from the human hippocampus (<xref ref-type="bibr" rid="R18">John et al., 2025</xref>).</p><p id="P11">It was shown in (<xref ref-type="bibr" rid="R40">Wu and Maass, 2025</xref>) that the following simple plasticity rule with binary weights <italic>w<sub>i</sub></italic> captures key features of BTSP in the brain. Synaptic plasticity takes place only when its plasticity window is opened by a stochastic gating signal, and if there is in addition presynaptic activity, i.e., <italic>x<sub>i</sub></italic> = 1, where <italic>x<sub>i</sub></italic> denotes presynaptic activity (encoded by a binary state: <italic>active</italic> = 1, <italic>inactive</italic> = 0). The new value of the weight is then <disp-formula id="FD1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext><mml:mspace width="0.2em"/><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mspace width="0.2em"/><mml:mtext>and</mml:mtext><mml:mspace width="0.2em"/><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext><mml:mspace width="0.2em"/><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mspace width="0.2em"/><mml:mtext>and</mml:mtext><mml:mspace width="0.2em"/><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1.</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="P12">According to this rule, synaptic weights undergo Long-Term Potentiation (LTP) or Long-Term Depression (LTD), in dependence of their preceding value, and of the presynaptic activity. The weight-dependent aspect of BTSP, that had been elucidated in (<xref ref-type="bibr" rid="R30">Milstein et al., 2021</xref>), may look surprising from the functional perspective, since it could in principle weaken the capacity of the network. But if brain-like sparse input patterns and gating signals are used for BTSP, most weights have value 0 even after a large numbers of patterns have been stored <xref ref-type="bibr" rid="R40">Wu and Maass (2025)</xref>. Consequently, LTD is applied only rarely. Nevertheless, it was shown in (<xref ref-type="bibr" rid="R40">Wu and Maass, 2025</xref>) that this rare application of LTD is helpful for maintaining a low fraction of non-zero synaptic weights, and for reproducing the repulsion effect of the human brain.</p><p id="P13">Whereas BTSP looks from this perspective more similar to random hashing, it combines the benefits of random hashing with a key property of Hebbian learning that random hashing cannot provide: It creates attractor properties for stored memories, so that these can also be recalled with partial or perturbed versions of the original content. An explanation on the mechanistic level is provided by <xref ref-type="fig" rid="F1">Fig. 1 B and C</xref>. Its functional benefits become obvious if one trains simultaneously with the feedforward connections through BTSP also backwards connections from memory neurons to input neurons (<xref ref-type="fig" rid="F1">Fig. 1D</xref>). These backwards connections can be viewed as parsimonious model for the well-known big-loop recurrence of the brain between CA1 and CA3 (<xref ref-type="bibr" rid="R25">Koster et al., 2018</xref>). Since sparse gating signals for BTSP induce sparse representations on the memory layer, and since we work with sparse input patterns, it suffices to use just binary weights also for the feedback connections, and to train them with a simple plasticity rule with flips the weight value if both the pre- and postsynaptic neuron are firing (i.e., assume value 1). This plasticity rule is closely related to a variant of BTSP in area CA3 <xref ref-type="bibr" rid="R27">Li et al. (2024)</xref>, which also relies on postsynaptic firing, and can therefore be viewed as a variant of Hebbian learning. If one combines the feedforward synaptic plasticity of <xref ref-type="fig" rid="F1">Fig. 1A</xref> with the plasticity of feedback connections of <xref ref-type="fig" rid="F1">Fig. 1D</xref> one arrives at a functionally powerful model for content addressable memory (CAM), see <xref ref-type="fig" rid="F1">Fig. 1E</xref>: The Hamming distance (HD) between a pattern x that had been learnt through BTSP, and a partially masked variant <italic>x’</italic> of this pattern is reduced when <italic>x’</italic> is replaced by the reconstructed version <italic>r</italic>(<italic>x’</italic>) of <italic>x’</italic> that results from applying the feedforward weights from <xref ref-type="fig" rid="F1">Fig. 1A</xref> in conjunction with the feedback weights from <xref ref-type="fig" rid="F1">Fig. 1D</xref>. The reconstruction <italic>r</italic>(<italic>x’</italic>) arises already if forward and feedback weights are applied just once, i.e., no further recurrent network dynamics has to be examined.</p><p id="P14"><xref ref-type="fig" rid="F1">Fig. 1E</xref>, which is copied from (<xref ref-type="bibr" rid="R40">Wu and Maass, 2025</xref>), shows that a fairly large masking fraction of 50% of the bits of <italic>x</italic> can be tolerated in variants <italic>x</italic>′ of <italic>x</italic>, in the sense that most of the missing bits are correctly reconstructed by <italic>r</italic>(<italic>x’</italic>). This content-addressable-memory (CAM) property that is induced by BTSP will be useful for improving the fidelity of top-down unbinding that we consider in the next section.</p><p id="P15">We will use in the subsequent evaluations of properties of BTSP-based binding largely the same brain-based values of hyperparameters as in <xref ref-type="bibr" rid="R40">Wu and Maass (2025)</xref>. Input patterns are sparse binary vectors with a fraction <italic>f<sub>p</sub></italic> = 0.005 of 1’s. This matches the estimate of the sparsity of input patterns that CA1 receives from area CA3 according to the experimental data from <xref ref-type="bibr" rid="R16">Guzman et al. (2016)</xref>. A memory neuron receives a gating signal with probability <italic>f<sub>q</sub></italic> = 0.0025 for a given input pattern, as suggested by the experimental data of (<xref ref-type="bibr" rid="R15">Grienberger and Magee, 2022</xref>). To be precise, we use here for simplicity the deterministic version (“core BTSP”) of (<xref ref-type="bibr" rid="R40">Wu and Maass, 2025</xref>), which is according to <xref ref-type="supplementary-material" rid="SD1">Fig. S1</xref> of that paper for this value of <italic>f<sub>q</sub></italic> functionally equivalent to the stochastic version with <italic>f<sub>q</sub></italic> = 0.005. Note that <italic>f<sub>q</sub></italic> largely determines the sparsity of composed representations in the memory layer. Results of control experiments for different values of <italic>f<sub>p</sub></italic> and <italic>f<sub>q</sub></italic> can be found in <xref ref-type="supplementary-material" rid="SD1">Fig. S1</xref> of the Supplement.</p></sec><sec id="S4"><label>2.2</label><title>Top-down unbinding</title><p id="P16">We apply in the following BTSP binding to content items (words) that are encoded by sparse binary vectors of length 6000. In view of the chosen brain-like sparsity level of <italic>f<sub>p</sub></italic> = 0.005 this amounts to an average number of 30 bits with value 1 in a word. We focus on binding processes where 8 words are bound together into a composed representation. Each composed representations (represented by neurons in the memory layer that fire) is a bit vectors of length 12,000, see the network scheme in <xref ref-type="fig" rid="F2">Fig. 2A</xref>.</p><p id="P17">A key property of any type of binding scheme is to what extent items that are bound together can be recovered from a composed representation (top-down unbinding). Strangely enough, none of the numerous schemes for HDC enables that; one always needs to provide in addition to the composed representation a substantial fraction of its content items in order to recover the other ones, see the reviews <xref ref-type="bibr" rid="R23">Kleyko et al. (2022</xref>, <xref ref-type="bibr" rid="R22">2023</xref>). In contrast, for binding via BTSP the simultaneously trained feedback connections (see <xref ref-type="fig" rid="F2">Fig. 2B</xref>) enable top-down unbinding without providing any content items, see <xref ref-type="fig" rid="F2">Fig. 2 C, D</xref>. We considered here the case where 8 words where bound into a composed representation in one shot through BTSP, for various numbers of sentences between 100 and 11,000. The recovered noisy versions A’, B’, … of the words A, B, … have in general minor errors, see the left plot of <xref ref-type="fig" rid="F2">Fig. 2F</xref>. But this error can be substantially reduced according to the right plot in <xref ref-type="fig" rid="F2">Fig. 2F</xref> if one applies to each recovered word A’, B’,… a BTSP-based CAM that was trained for the underlying vocabulary of 1000 words. The plot shows that the resulting cleaned-up words A”, B”, .. have even for large numbers of sentences very little errors in comparison with the original words. We used in <xref ref-type="fig" rid="F2">Fig. 2F</xref> the quotient of all incorrect bits in the decoded 8 words divided by the average number of 1’s in a sentence (given in %). Note that just counting the fraction of incorrect bits among all decoded bits would provide a too optimistic measurement: A decoding result consisting of just 0’s would already fair well, due to the fact that the vectors are very sparse (99.5% of them have value 0).</p><p id="P18">There exists also a trivial method for binding content vectors (words) into composed representations (sentences) that humans use in written language: One simply appends the codes for words (with blank spaces as separators). This method perfectly supports top-down decomposition, but it does not support bottom-up decomposition, which is discussed in the next section, because it has no attractor feature. In addition, this trivial method induces a growth in the length of coding vectors when one iterates the binding process. This can be avoided with BTSP-binding (see <xref ref-type="sec" rid="S7">section 2.5</xref>).</p></sec><sec id="S5"><label>2.3</label><title>Bottom-up unbinding</title><p id="P19">We consider here the same process for forming composed representations via BTSP as in the preceding section. But we examine now to what extent their content can be recovered without giving the composed representation as cue. Note that current HDC approaches support unbinding of composed representations only if both the composed representation and at least 1/2 of the content items are provided as cues. The latter is needed because in order to recover an input vector from a component-wise sum, product, or exclusive OR of two input vectors, the other input vector has to be provided as cue (in addition to the composed representation). Surprisingly, even without providing the composed representation as cue, less than 1/2 of the content items suffice as cue. For example, <xref ref-type="fig" rid="F3">Fig. 3 A - C</xref> consider the case where just 2 out of 8 words are provided as cues. <xref ref-type="fig" rid="F3">Fig. 3A</xref> illustrates a generic example, where just the words B and D are provided as cues. The same unbinding process as in <xref ref-type="fig" rid="F2">Fig. 2 C, D</xref> generates from the resulting activity vector M(B, D) on the memory layer an approximation A’, B’, C’, D’, E’, F’, G’, H’ of all 8 words A, …, H that had been bound together. We needed to use here lower thresholds for the memory neurons so that 2 out of 8 words on the input layer can already activate sufficiently many of them.</p><p id="P20">The bottom-up unbinding performance is almost as good as in the previously considered case of top-down unbinding. <xref ref-type="fig" rid="F3">Fig. 3D</xref> shows that an even smaller fraction of words than 1/4 suffices: Even when just 2 out of 12 or 16 words in a sentence are provided as cues, bottom-up unbinding works still well. Obviously, bottom-up unbinding is a direct consequence of the attractor feature of BTSP-based HDC. It has no analogue in previous HDC approaches.</p><p id="P21">We had considered here the case where for each pair of words only a single composed representation (sentence) had been presented that contained these two words at their respective positions. In other words, bottom-up unbinding had a unique solution. In the next section we consider the case where this is no longer guaranteed.</p></sec><sec id="S6"><label>2.4</label><title>Bottom-up unbinding with ambiguous cues</title><p id="P22">We had assumed in the previously discussed bottom-up unbinding tests that just a single sentence had been learnt that was consistent with the given cue. We now drop this uniqueness assumption, and consider the case where there may exist several correct results for bottom-up unbinding. It is obviously not desirable that the network produces a superposition of them. Our brain tends to produce in this case different correct results of bottom-up unbinding in a sequential manner. We show that by adding an additional network module, a recurrently connected neural network, see <xref ref-type="fig" rid="F4">Fig. 4A</xref>, our model can also do that. This recurrent neural network roughly corresponds to area CA3 in the brain, and for simplicity we refer to it as CA3 in our model. Corresponding to the conjecture role of area CA3 in the brain, we enable this recurrent neural network to act as associative memory that can store different correct input completions as attractors. It was shown in (<xref ref-type="bibr" rid="R27">Li et al., 2024</xref>) that recurrent synaptic connections within CA3 are quickly adapted by a Hebbian variant of BTSP. In this variant of the plasticity rule also postsynaptic firing is required, see <xref ref-type="fig" rid="F4">Fig. 4B</xref>. Again, we consider the simplest case where these weights can only assume values 0 or 1. This plasticity rule is activated whenever one of the given set of input patterns (sentences) is encoded by the firing activity of the input neurons. More precisely, this synaptic plasticity within the recurrent network is applied after BTSP has been applied to the synapses from the input neurons. As a result, these activations of the recurrent network become attractors of the dynamics of the recurrent network.</p><p id="P23">One can avoid that the dynamics of this recurrent network module remains in a single attractor by adding an adaptation property to its neurons. This adaptation property makes each neuron reluctant to fire continuously for a large number of time steps (see (<xref ref-type="bibr" rid="R5">Chen et al., 2022</xref>) for models for adapting biological neurons and (<xref ref-type="bibr" rid="R37">Rao et al., 2022</xref>) for applications in neuromorphic hardware). With these neuron models the recurrent network moves sequentially through all attractors that are consistent with the current network input. In the illustration in <xref ref-type="fig" rid="F4">Fig. 4C</xref> the activity of this recurrent module at time t for the cue word B at the 2nd word position is denoted by <italic>S(B, 2)<sub>t</sub>.</italic> Synaptic connections to memory neurons transforms this code into another neural code <italic>R(B, 2)<sub>t</sub>.</italic> When the recurrent network module moves through its attractors, the memory layer moves through corresponding codes <italic>R(B, 2)<sub>t</sub></italic>.</p><p id="P24">Each of them can be decoded by the feedback connections into a possible sentence &lt; <inline-formula><mml:math id="M2"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>B</mml:mi><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:mo>….</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>H</mml:mi><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> &gt; as indicated in <xref ref-type="fig" rid="F4">Fig. 4D</xref>.</p><p id="P25">We demonstrate the functionality of this method by considering the case where one has for each word in a particular position 4 learned sentences that are consistent with this cue. When a single word is given as cue, for example in <xref ref-type="fig" rid="F4">Fig. 4C</xref> the word B in the 2nd word position, neural activity in the recurrent network module cycles through 4 attractors (<xref ref-type="fig" rid="F4">Fig. 4E</xref>), where each of these attractors had been induced during learning of lateral weights by one of the 4 possible sentence completions of this cue. This dynamics in CA3 induces a corresponding cycling through 4 composed representations <italic>R</italic>(<italic>B,</italic> 2)<sub>t</sub> in the memory neurons (<xref ref-type="fig" rid="F4">Fig. 4F</xref>). The correctness values on the y-axis in <xref ref-type="fig" rid="F4">Fig. 4 E and F</xref> (see <xref ref-type="sec" rid="S10">Methods</xref> for details) result by comparing the neural activity over time during the presentation of the single word as cue with the activity induced by presentations of the 4 possible sentence completions as network input. One sees, that the network activity for the single word as cue cycles between those that are induced by the 4 sentences.</p><p id="P26">Each activity pattern on the memory layer induces in the input layer through the feedback connections (<xref ref-type="fig" rid="F4">Fig. 4D</xref>) a proposition for bottom-up unbinding. The quality of this unbinding result is shown in <xref ref-type="fig" rid="F4">Fig. 4G</xref> for two cases where 1000 and 4000 sentences had been learnt. For each of the 4-tuples of sentences that we used in our experiments as correct completions of an input cue we measured the maximal correctness of unbinding over 200 time steps, for each of the 4 possible sentence completions (taking the minimum over them). Hence a value y for the min. of max. correctness over all 4 sentences in a 4-tuple of correct unbinding results indicates that each of the 4 possible sentence completions was reproduced at some time point on the input layer at least with correctness y. The x-axis in <xref ref-type="fig" rid="F4">Fig. 4G</xref> indicates for which fraction of 4-tuples of sentences this value y could be achieved. For example, in the case where one has 1000 4-tuples of sentences, for about 90% of them a value of y close to 100% (i.e., perfect decoding for each of them) could be achieved</p><p id="P27">The yellow curves in <xref ref-type="fig" rid="F4">Fig. 4G</xref> give corresponding measurements if one is satisfied to recover at least 2 of the 4 possible sentence completions at some time point.</p><p id="P28">Although unbinding from ambiguous cues is a natural real-world challenge, which is mastered quite well by our brain, there exist to the best of our knowledge no results for unbinding from ambiguous cues for standard HDC binding.</p></sec><sec id="S7"><label>2.5</label><title>Iterated binding and unbinding</title><p id="P29">A hallmark of any neural network model for compositional computing is the ability to iterate compositional processes, e.g., to form sentences from words and then stories from sentences, as indicated in <xref ref-type="fig" rid="F5">Fig. 5A</xref>. While this scheme only addresses the case of hierarchical binding, it is also desirable to expand a composed representation by adding further content or structural information in an online manner, as indicated in <xref ref-type="fig" rid="F6">Fig. 6A</xref>. The real challenge for iterated binding is of course the capability to be able to fully reverse it through iterated unbinding (see <xref ref-type="fig" rid="F5">Fig. 5B</xref> and <xref ref-type="fig" rid="F6">Fig. 6B</xref>). One concern on the technical level is a likely accumulation of errors at different stages of iterated unbinding. But we show in <xref ref-type="fig" rid="F5">Fig. 5 C,D</xref> that the performance of iterated top-down unbinding is quite good for hierarchical binding through BTSP. Furthermore, it can be significantly improved by using CAM cleanup. The results for iterated online binding are similar, see <xref ref-type="fig" rid="F6">Fig. 6C</xref>. For simplicity we refer to the items that are bound together as words, and to the lists of words that are bound together through iterated binding as a sentence. But in an application to natural language the latter could also represent a story that consists of several sentences.</p><p id="P30">Note that there are no comparison results for iterated top-down unbinding for classical HDC, since top-down unbinding is not possible there.</p></sec><sec id="S8"><label>2.6</label><title>Advancing the assembly calculus for modeling language processing in the brain through BTSP-binding</title><p id="P31">Binding operations play a central role for models of higher-level brain computations, especially for language processing, based on an assembly calculus (<xref ref-type="bibr" rid="R33">Papadimitriou et al., 2020</xref>; <xref ref-type="bibr" rid="R31">Müller et al., 2020</xref>). The idea of these models is that assemblies of neurons, such as for example an assembly of concept cells for a particular concept or structural information, are tokens for brain computations. Complex computations and data structures for semantic content and structural annotations can be formed with these tokens if there is a mechanism for binding two content tokens together to form another assembly. Obviously, a binding mechanism is needed that is able to bind assemblies together very fast, when a particular episode is experienced or a particular sentence is heard. Hence synaptic plasticity mechanisms such as STDP, where dozens of iterations of a learning trial are needed for changing a synaptic weight (<xref ref-type="bibr" rid="R14">Froemke et al., 2010</xref>), are less plausible as biological implementation of binding. For natural language processing, and also for the frequently postulated internal language of thought of the brain (<xref ref-type="bibr" rid="R20">Kazanina and Poeppel, 2023</xref>), one needs in addition the capability to bind an assembly that represents a concept, such as “cat”, to an assembly that indicates a specific role for that concept in a sentence or episode, such as “cat as agent” (<xref ref-type="bibr" rid="R13">Frankland and Greene, 2020b</xref>), see <xref ref-type="fig" rid="F7">Fig. 7A</xref>. Previous models (<xref ref-type="bibr" rid="R33">Papadimitriou et al., 2020</xref>; <xref ref-type="bibr" rid="R31">Müller et al., 2020</xref>) proposed that this is carried out via an instantaneous “pro jection” of an assembly into another cortical area that represents specific semantic roles. Obviously, binding through BTSP offers itself as a suitable synaptic plasticity mechanism for that.</p><p id="P32">Binding of two assemblies by strengthening synaptic connections between neurons in the two assemblies through STDP has been considered in previous work (<xref ref-type="bibr" rid="R33">Papadimitriou et al., 2020</xref>; <xref ref-type="bibr" rid="R31">Müller et al., 2020</xref>). But, the experimental data on instantaneous formation of conjunctive memories in area CA1 (<xref ref-type="bibr" rid="R2">Bittner et al., 2015</xref>; <xref ref-type="bibr" rid="R41">Zhao et al., 2022</xref>) suggest a different solution. There not the synapses between concept cells for different concepts are modified, but direct or indirect synaptic connections between concept cells and neurons in another area, as indicated in the scheme of <xref ref-type="fig" rid="F7">Fig. 7B</xref>. The general compositional computing operations that are supported by BTSP according to <xref ref-type="fig" rid="F2">Fig. 2</xref> and <xref ref-type="fig" rid="F3">3</xref> can readily be used as basic operations of such an alternative brain calculus, see Fig. B and C. Feedback connections and larger loops, such as the big-loop recurrence between CA1 and cortical areas (<xref ref-type="bibr" rid="R25">Koster et al., 2018</xref>), allow to iterate and reverse such binding process, see <xref ref-type="fig" rid="F7">Fig. 7D</xref> for an illustration. Binding through BTSP can handle such iterated binding and unbinding quite well, as shown in <xref ref-type="fig" rid="F5">Fig. 5</xref> and <xref ref-type="fig" rid="F6">6</xref>. Hence, we propose to improve the brain calculus of (<xref ref-type="bibr" rid="R33">Papadimitriou et al., 2020</xref>) by employing binding through BTSP via composed representations that act as pointers to the assembly codes. This model is supported by the recent experimental data of (<xref ref-type="bibr" rid="R24">Kolibius et al., 2023</xref>).</p></sec></sec><sec id="S9" sec-type="discussion"><label>3</label><title>Discussion</title><p id="P33">We have shown that capabilities of HDC can be substantially enhanced if one uses instead of algebraic operations a mechanism that the brain employs for binding, BTSP (<xref ref-type="bibr" rid="R2">Bittner et al., 2015</xref>; <xref ref-type="bibr" rid="R41">Zhao et al., 2022</xref>). This enables HDC to carry out two essential operations for compositional computing that could so far not accomplished. One example is top-down unbinding of all tokens that have been combined in a composed representation (see <xref ref-type="fig" rid="F2">Fig. 2</xref>). This can be carried out with BTSP even in the case of iterated binding operations, where composed representations become tokens for a higher level binding operation (<xref ref-type="fig" rid="F5">Fig. 5</xref> and <xref ref-type="fig" rid="F6">6</xref>). In addition, BTSP-enhanced HDC drastically improves bottom-up unbinding since only a minor fraction of bound tokens have to be provided as cues, and not the composed representation (<xref ref-type="fig" rid="F3">Fig. 3</xref>). Furthermore, in case that the provided cues are ambiguous, different solutions can be provided sequentially (<xref ref-type="fig" rid="F4">Fig. 4</xref>). The attractor features of composed representations that BTSP provides also makes these operations robust against errors that may arise in implementations on highly energy-efficient analog hardware such as memristor crossbars (<xref ref-type="fig" rid="F2">Fig. 2F</xref>).</p><p id="P34">HDC approaches have been created as interface between abstract models for compositional computing and massively parallel implementations of compositional computing in large neural networks of the brain (<xref ref-type="bibr" rid="R12">Frankland and Greene, 2020a</xref>; <xref ref-type="bibr" rid="R26">Kurth-Nelson et al., 2023</xref>; <xref ref-type="bibr" rid="R20">Kazanina and Poeppel, 2023</xref>) and in novel energy-efficient computing hardware such as crossbars of memristors (<xref ref-type="bibr" rid="R39">Wang et al., 2020</xref>; <xref ref-type="bibr" rid="R21">Khaddam-Aljameh et al., 2022</xref>). We refer to the recent reviews of HDC in (<xref ref-type="bibr" rid="R23">Kleyko et al., 2022</xref>, <xref ref-type="bibr" rid="R24">2023</xref>) for details. Obviously, endowing HDC with attractor features enhances both of these application domains. For example, BTSP can instantly bind two assemblies of neurons together, rather than requiring numerous iterations as previous models (<xref ref-type="bibr" rid="R31">Müller et al., 2020</xref>; <xref ref-type="bibr" rid="R33">Papadimitriou et al., 2020</xref>). Hence BTSP-enhanced HDC suggests a new approach for modeling the neural implementation of a language of thought (<xref ref-type="bibr" rid="R20">Kazanina and Poeppel, 2023</xref>) and of natural language processing in the brain (<xref ref-type="fig" rid="F7">Fig. 7</xref>), where binding of tokens into an episode or of words into a sentence have to be carried out instantly. In particular, the Merge operation has been proposed in linguistic theory as universal grammatical binding operation (<xref ref-type="bibr" rid="R7">Chomsky et al., 2023</xref>), and possible implementations in brain networks have been proposed in (<xref ref-type="bibr" rid="R32">Papadimitriou and Friederici, 2022</xref>; <xref ref-type="bibr" rid="R28">Liu et al., 2023</xref>). Instantaneous binding through BTSP provides a biologically more plausible and functional superior method for any neural network implementation of Merge.</p><p id="P35">Binding through BTSP only requires binary weights. This makes our models consistent with the rather limited number of weight values that biological synapses can assume (<xref ref-type="bibr" rid="R1">Bartol Jr et al., 2015</xref>), and with constraints of especially efficient hardware implementations of synaptic weights in memristors that can only assume a small number of conductances (<xref ref-type="bibr" rid="R39">Wang et al., 2020</xref>; <xref ref-type="bibr" rid="R21">Khaddam-Aljameh et al., 2022</xref>). In addition, in contrast to most neural network models, BTSP-enhanced HDC operates with sparse neural codes, both for content items and composed representations. Therefore it can be implemented in a particularly energy efficient sparse activity regime both with spiking and with non-spiking neuromorphic hardware. Hence it points to a way out of the dilemma that implementations of standard neural network algorithms tend to give rise to neuromorphic implementations with dense neural activity that are not more energy efficient than standard hardware implementations (<xref ref-type="bibr" rid="R8">Davies et al., 2021</xref>). Another promising application domain for BTSP-enhanced HDC is the combination of operations on high-D vectors with DNNs (deep neural networks). Already traditional forms of HDC have recently provided in combination with DNNs intriguing advances in analogical reasoning capabilities of novel computing hardware (<xref ref-type="bibr" rid="R17">Hersche et al., 2023</xref>; <xref ref-type="bibr" rid="R4">Camposampiero et al., 2025</xref>).</p></sec><sec id="S10" sec-type="methods"><label>4</label><title>Methods</title><sec id="S11"><label>4.1</label><title>Implementation of BTSP</title><sec id="S12"><label>4.1.1</label><title>Details to the BTSP rule</title><p id="P36">We use throughout the simpler version of the BTSP rule (called core BTSP in (<xref ref-type="bibr" rid="R40">Wu and Maass, 2025</xref>)), in which the explicit probabilistic factor (set as 0.5) governing synaptic weight updates is removed. It was shown in <xref ref-type="supplementary-material" rid="SD1">Fig. S1</xref> of (<xref ref-type="bibr" rid="R40">Wu and Maass, 2025</xref>) that this simpler version produces essentially the same learning results as the rule with probabilistic factors that matches more closely the biological data (this factor arises from the uncertainty whether a synaptic input falls into the LTP or LTD part of the plasticity window, and the fact that LTP can only be applied to a binary weight if it has value 0, and LTD only if it has value 1).</p><p id="P37">Note however, that also this simplified BTSP rule maintains a stochastic element—since the gating signals (plateau potentials) are assumed to occur randomly.</p><p id="P38">In practical simulations, this simplified version enables replacing the original iterative sample-by-sample computation with batch-wise matrix multiplication (detailed in <xref ref-type="sec" rid="S13">Sec. 4.1.2</xref>), thereby significantly enhancing analytical tractability and computational efficiency.</p></sec><sec id="S13"><label>4.1.2</label><title>Details to batch-wise BTSP learning</title><p id="P39">We show here how the BTSP rule can be implemented in parallel computations, enabling batch-wise simulations for accelerated processing. Let <bold>x</bold> = (<bold>x</bold><sub>1</sub>,…, <bold>x</bold><sub><italic>n</italic></sub>) denote binary input vectors representing presynaptic activity for different items, and let <bold>q</bold> = (<bold>q</bold><sub>1</sub>,…, <bold>q</bold><sub><italic>n</italic></sub>) denote synaptic plasticity windows corresponding one-to-one with these inputs, where the memory neurons selected within the synaptic plasticity windows will follow the update formula of <xref ref-type="disp-formula" rid="FD1">Eq. 1</xref>. We consider a single update at iteration <italic>k</italic> for the synaptic weights <bold>w</bold><sup>(<italic>k</italic>)</sup> using the pair (<bold>x</bold><sub><italic>k</italic></sub>, <bold>q</bold><sub><italic>k</italic></sub>) under the BTSP rule described in <xref ref-type="disp-formula" rid="FD1">Eq. 1</xref>. Specifically, focusing on the <italic>i<sup>th</sup></italic> input neuron (<bold>x</bold><sub><italic>k</italic></sub>)<sub><italic>i</italic></sub>, the plasticity of <italic>j<sup>th</sup></italic> memory neuron (<bold>q</bold><sub><italic>k</italic></sub>)<sub><italic>j</italic></sub>, and a single synapse (<bold>w</bold><sup>(<italic>k</italic>)</sup>)<sub><italic>ij</italic></sub>—representing the connection from the <italic>i<sup>th</sup></italic> presynaptic neuron to the <italic>j<sup>th</sup></italic> postsynaptic neuron—the weight update rule can be expressed as: <disp-formula id="FD2"><label>(2)</label><mml:math id="M3"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mstyle><mml:mtext mathvariant="bold">w</mml:mtext></mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mstyle><mml:mtext mathvariant="bold">w</mml:mtext></mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle><mml:mtext mathvariant="bold">x</mml:mtext></mml:mstyle><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle><mml:mtext mathvariant="bold">q</mml:mtext></mml:mstyle><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mstyle><mml:mtext mathvariant="bold">w</mml:mtext></mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>·</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle><mml:mtext mathvariant="bold">x</mml:mtext></mml:mstyle><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle><mml:mtext mathvariant="bold">q</mml:mtext></mml:mstyle><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P40">This equation can be generalized and compactly represented across all synaptic connections as: <disp-formula id="FD3"><label>(3)</label><mml:math id="M4"><mml:mrow><mml:msup><mml:mstyle><mml:mtext mathvariant="bold">w</mml:mtext></mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mstyle><mml:mtext mathvariant="bold">w</mml:mtext></mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>◦</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle><mml:mtext mathvariant="bold">x</mml:mtext></mml:mstyle><mml:mi>k</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mstyle><mml:mtext mathvariant="bold">q</mml:mtext></mml:mstyle><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mstyle><mml:mtext mathvariant="bold">w</mml:mtext></mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>◦</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle><mml:mtext mathvariant="bold">x</mml:mtext></mml:mstyle><mml:mi>k</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mstyle><mml:mtext mathvariant="bold">q</mml:mtext></mml:mstyle><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where ◦ denotes the element-wise product. For further simplification, we introduce modular arithmetic to equivalently represent the binary flipping operation (0 ↔ 1): <disp-formula id="FD4"><label>(4)</label><mml:math id="M5"><mml:mrow><mml:msup><mml:mstyle><mml:mtext mathvariant="bold">w</mml:mtext></mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mstyle><mml:mtext mathvariant="bold">w</mml:mtext></mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mstyle><mml:mtext mathvariant="bold">x</mml:mtext></mml:mstyle><mml:mi>k</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mstyle><mml:mtext mathvariant="bold">q</mml:mtext></mml:mstyle><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.2em"/><mml:mi>mod</mml:mi><mml:mspace width="0.2em"/><mml:mn>2.</mml:mn></mml:mrow></mml:math></disp-formula></p><p id="P41">Thus, after <italic>n</italic> iterations, the weight matrix <bold>w</bold> can be concisely determined as: <disp-formula id="FD5"><label>(5)</label><mml:math id="M6"><mml:mrow><mml:msup><mml:mstyle><mml:mtext mathvariant="bold">w</mml:mtext></mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mstyle><mml:mtext mathvariant="bold">w</mml:mtext></mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mstyle><mml:mtext mathvariant="bold">x</mml:mtext></mml:mstyle><mml:mo>×</mml:mo><mml:mstyle><mml:mtext mathvariant="bold">q</mml:mtext></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mspace width="0.2em"/><mml:mtext>mod</mml:mtext><mml:mspace width="0.2em"/><mml:mn>2.</mml:mn></mml:mrow></mml:math></disp-formula></p></sec></sec><sec id="S14"><label>4.2</label><title>Details of binding and top-down unbinding (<xref ref-type="fig" rid="F2">Fig. 2</xref>)</title><p id="P42">Given a sentence represented by binary atomic vectors <italic>A, B,… ,G, H</italic>, each of length <italic>L,</italic> the binding phase employs one-shot learning via BTSP to train the feedforward synaptic weight matrix <italic>W<sub>feed</sub></italic>, and to produce with these modified weights a composed representation 〈<italic>A, B, …, G, H</italic>〉. Concurrently, direct feedback synaptic connections from memory neurons to input neurons are trained using a one-shot Hebbian-like learning rule to facilitate the unbinding process—recovering the atomic vectors <italic>A′,B′, … ,G′,H′</italic> from the composed representation. Specifically, if both presynaptic and postsynaptic neurons of a feedback connection are simultaneously active (i.e., both assume a binary value of 1), the synaptic weight of this feedback connection is incremented by 1; otherwise, the synaptic weight remains unchanged. If the weight value exceeded 1, it was set to a saturation value of 1. The feedback synaptic connections are adjusted only once for each network input, concurrently with BTSP. Parameter adjustments in the model are performed exclusively during the binding phase. Unless otherwise noted, in all of our experiments, we construct the vocabulary using 1000 words (<italic>N<sub>w</sub></italic> = 1000), initialize both the feedforward (<italic>W</italic><sub>feed</sub>) and backward (<italic>W</italic><sub>back</sub>) weight matrices with 0’s, and use default parameters for modeling: <italic>f<sub>p</sub></italic> = 0.005, <italic>f<sub>q</sub></italic> = 0.0025, and <italic>f<sub>w</sub></italic> = 0.6, consistently applied across both the binding model and the BTSP-based CAM. The CAM model is used to perform cleanup operations on restored words, which involves inputting the restored word into the CAM to activate the corresponding code in the CAM’s memory layer, followed by passing through the CAM’s feedback to restore it to the expected word. The clean-up process serves to filter out noise on restored words.</p><sec id="S15"><label>4.2.1</label><title>Details of top-down experiment (<xref ref-type="fig" rid="F2">Fig. 2F</xref>)</title><p id="P43">In <xref ref-type="fig" rid="F2">Fig. 2F</xref>, top-down unbinding results based on sentences containing 8 words each (<italic>K</italic> = 8) are depicted for a range of sentence counts from 100 to 11,000, with intervals of 100 for each data point. Each word is defined by a vector of 6000 bits (<italic>L</italic> = 6000), consistently containing 30 active bits (<italic>f<sub>p</sub></italic> = 0.005). The size of the input layer is set to <italic>K</italic> times the word length, totaling 48,000, while the memory layer is sized at 12,000. During the binding phase, the threshold for memory neurons is set at 96, meaning neurons with activation levels of 96 or higher will spike, while those below will not. During the unbinding phase, the threshold for the input layer is determined using a grid search for each data point. For the BTSP-based CAM, the size of the input layer is equal to the word size, 6000, and the memory size is set at 12,000. The threshold for memory neurons is set at 12, and the threshold for the input layer for output is similarly obtained through grid search. It is important to note that the grid search process for threshold determination is conducted separately from the binding performance tests; optimal threshold values are determined using a different vocabulary prior to the binding experiments, and these predetermined thresholds are then fixed for use in the experiments. The primary metric employed is the percentage of incorrect bits, calculated as the ratio of the number of error bits to expected number of 1’s.</p></sec></sec><sec id="S16"><label>4.3</label><title>Details of bottom-up unbinding (<xref ref-type="fig" rid="F3">Fig. 3</xref>)</title><p id="P44">The bottom-up unbinding evaluates the capability to recover original inputs when provided with only a subset of cues, while the remaining inputs are masked as zeros. Unless specifically stated otherwise, during testing of bottom-up unbinding, we default to randomly selecting 2 words from the sentence as cues. There are no restrictions on the positions of these cues; they can be chosen randomly. However, we ensure that the selected cues do not lead to multiple viable sentences. Cases involving ambiguous cues are discussed separately in <xref ref-type="sec" rid="S17">Sec. 4.4</xref>. During the bottom-up unbinding process, we reduce the threshold in the memory layer to accommodate the decreased input intensity caused by the mask, ensuring the memory neurons can activate as expected. The reduction of the threshold involves a grid search to select the optimal solution suitable for the length and the number of sentences.</p><p id="P45">In <xref ref-type="fig" rid="F3">Fig. 3C</xref>, bottom-up unbinding results based on 2 cues from 8-word sentences are depicted for a range of sentence counts from 100 to 11,000, with intervals of 100 for each data point. The experimental setup for binding is consistent with that described in <xref ref-type="fig" rid="F2">Fig. 2F</xref>; we employ the same model used in top-down unbinding testing for each data point.</p><p id="P46">In <xref ref-type="fig" rid="F3">Fig. 3D</xref>, bottom-up unbinding results are depicted for varying sentence lengths and counts. In this experiment, the sizes of both the input and memory layers are fixed at 48,000 and 12,000, respectively. The word length (<italic>L</italic>) is scaled according to the number of binding words (<italic>K</italic>), such that <italic>L</italic> = 48000/<italic>K</italic>. Correspondingly, the input dimension of the CAM is adjusted to the corresponding <italic>L</italic>, while the memory size is fixed at 24,000. A grid search for threshold reduction is applied for each data point, where we reselect the most suitable new threshold for the bottom-up process.</p></sec><sec id="S17"><label>4.4</label><title>Details of bottom-up unbinding with ambiguous cues (<xref ref-type="fig" rid="F4">Fig. 4</xref>)</title><p id="P47">We note that during recall via bottom-up unbinding, there can be cases where a set of cues corresponds to multiple valid sentences in memory. We refer to such cues as ambiguous cues, which evoke a superposition of all viable sentences stored in memory. To address this ambiguity, we introduce CA3 as an intermediate layer to temporally decouple the superposition, enabling the network to oscillate sequentially among all valid patterns. Specifically, dense lateral connections and global k-Winners-Take-All (k-WTA) operations are incorporated within CA3, allowing attractors to form before propagating activation to the memory layer. We employ k-WTA to control the number of spikes in CA3, ensuring stability in the recurrent process, selecting the neurons with the highest activation levels to fire spikes while the rest remain inactive. Similar performance can be achieved without WTA competition if the neuron thresholds are properly adjusted to the level of activation that changes with iterations in CA3. One can use a grid search to find the appropriate thresholds for CA3 to achieve the desired number of one bits. The learning of weights for lateral connections within CA3 follows a Hebbian-type one-shot plasticity rule, similar as the one discussed in (<xref ref-type="bibr" rid="R27">Li et al., 2024</xref>). Synaptic weights, represented as <italic>w<sub>ij</sub></italic> connecting neuron <italic>i</italic> to neuron <italic>j</italic>, are binary and updated as follows: if both neuron <italic>i</italic> and <italic>j</italic> are simultaneously active, the synaptic weight is toggled between 0 to 1: <disp-formula id="FD6"><label>(6)</label><mml:math id="M7"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext><mml:mspace width="0.2em"/><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mspace width="0.2em"/><mml:mtext>and</mml:mtext><mml:mspace width="0.2em"/><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>otherwise</mml:mtext><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="P48">These weights were initialized as 0.</p><p id="P49">The schedule of steps where neuron outputs and synaptic weights are updated is as follows (see <xref ref-type="supplementary-material" rid="SD1">Algorithm 1</xref> in the Supplement for further details): <list list-type="simple" id="L1"><list-item><p id="P50">Step 1. A sentence is presented as network input.</p></list-item><list-item><p id="P51">Step 2. BTSP is applied to the connections from input neurons to neurons in CA3.</p></list-item><list-item><p id="P52">Step 3. Neurons in CA3 are activated.</p></list-item><list-item><p id="P53">Step 4. BTSP is applied to weights from neurons in CA3 to neurons in the memory layer.</p></list-item><list-item><p id="P54">Step 5. Neurons in the memory layer are activated.</p></list-item><list-item><p id="P55">Step 6: Weights of recurrent connections in CA3 are subject to synaptic plasticity with the activation pattern from Step 3.</p></list-item></list></p><p id="P56">During testing, we conducted bottom-up unbinding experiments by presenting a single cue (among a total of 8 words per sentence) for the case where this cue appears at the same position in 4 different sentences for which the network was trained. Recurrent synaptic connections are activated in CA3, so that the network input generates a dynamic evolution of its network states, thereby enabling it to switch among different attractors. In order to avoid that the network gets locked in a single one of these attractors, we add an adaptation property to neurons in CA3: If a neuron has fired continuously over 10 time steps, it cannot fire again during the subsequent 50 time steps.</p><p id="P57">We include an algorithmic description (<xref ref-type="supplementary-material" rid="SD1">Algorithm 1</xref>) in the Supplement for further implementation details.</p><p id="P58">For testing bottom-up unbinding with ambiguous cues (see <xref ref-type="fig" rid="F4">Fig. 4</xref>), the experiment is based on binding 1 cue from 8 words, where the word length is <italic>L</italic> = 6000. Consequently, the input layer size is set at 48,000, the CA3 layer size at 24,000, and the memory layer size at 12,000. Synapses from the input to CA3 and from CA3 to the memory layer are subject to the standard BTSP rule with <italic>f<sub>q</sub></italic> = 0.0025 for the probability of a plateau potential in the postsnaptic neuron. Instead of a fixed threshold, a k-Winners-Take-All (k-WTA) mechanism for k = 60 is employed for determining which neurons in the CA3 layer fire (i.e., assume value 1). The threshold for neurons in the memory layer is fixed at 24, with no threshold reduction required during bottom-up unbinding since the input strength from the recurrent activity in CA3 is sufficient. We trained the model on a dataset of 1000 sentences and obtained the results presented in <xref ref-type="fig" rid="F4">Fig. 4E and Fig. 4F</xref>. We measured the correctness of a network state by <italic>max(100% – error%,</italic> 0), where the error percentage was defined as before as ratio of the number of error bits and the number of expected 1’s. Since it is possible for this error measure that there are more error bits than the expected number of 1’s, we imposed a lower bound of 0 for the resulting correctness values. We evaluated bottom-up unbinding performance using datasets of 1000 and 4000 samples, with the correctness distribution shown in <xref ref-type="fig" rid="F4">Fig 4G</xref>. We compared in <xref ref-type="fig" rid="F4">Fig. 4E, F</xref> the network states that occurred in the course of the network dynamics during unbinding with those network states that resulted when each of the 4 possible sentence solutions were initially presented as network inputs during learning, without activating the recurrent connections in the CA3 module.</p></sec><sec id="S18"><label>4.5</label><title>Details of iterated binding</title><sec id="S19"><label>4.5.1</label><title>Details of hierarchical iterated binding (<xref ref-type="fig" rid="F5">Fig. 5</xref>)</title><p id="P59">In hierarchical iterated binding, the encoding utilizes a hierarchical binary tree structure to organize the binding process, until a single top-level composed representation is achieved. For example, with four words, the process involves pairing and binding <italic>A</italic> and <italic>B</italic> to form 〈<italic>A, B</italic>〉 and <italic>C</italic> and <italic>D</italic> to form (<italic>C,D</italic>). These pairs are then bound together to form 〈〈<italic>A, B</italic>〉, 〉<italic>C, D</italic>〉〉, etc. This method can also be used for binding a list of words whose number is not a power of 2. In cases where a path contains only a single word, no binding operation occurs, and the word is directly propagated to the subsequent hierarchical level. We include an algorithmic description of hierarchical binding (<xref ref-type="supplementary-material" rid="SD1">Algorithm 2</xref>) in the <xref ref-type="supplementary-material" rid="SD1">Supplement S1</xref>. for further details.</p><p id="P60">We assume that a single network is used for binding and unbinding at each position in the iterated binding schemes, both for hierarchical binding and online binding. The feedforward weights of this network are trained in an online manner, for each input that is presented to the network. Symmetrically, feedback connections are adjusted together with the feedforwards weights at each of these learning steps. The best unbinding performance is likely to be achieved when it occurs right after the weight adjustments for this particular ensemble of input words. But our results in <xref ref-type="fig" rid="F5">Fig. 5 C, D</xref> and <xref ref-type="fig" rid="F6">Fig. 6 C, D</xref> show that unbinding stills works very well after the weights of the network have been trained for many subsequent ensembles of input words. In other words, learnt performance for a single ensemble of input words is not ruined by subsequent learning processes for other ensembles of input words. The CAM for cleaning up the words that result from top-down unbinding at the input level was trained on the vocabulary of words that we employed.</p><p id="P61">In the experiments of both <xref ref-type="fig" rid="F5">Fig. 5C</xref> and <xref ref-type="fig" rid="F6">Fig. 6C</xref>, the word length is set at 12,000, with each word containing a fixed 60 one-bits (<italic>f<sub>p</sub></italic> = 0.005), forming a vocabulary of 1,000 words (<italic>N<sub>w</sub></italic> = 1000). Under the iterated binding model, the size of the memory layer is set equal to the word length, ensuring that the dimensions of the composed representation match those of the word, which allows for uniform processing of both word representations and composed representations. The dimension of the input layer is set to twice the word length to handle two representations simultaneously. The same model is reused at each binding step, with the size of the input layer set at double <italic>L</italic> (24,000) and the memory size equal to <italic>L</italic> (12,000), with <italic>f<sub>q</sub></italic> = 0.005 for BTSP maintained throughout. At the memory layer, during binding, the k-WTA strategy is used to select the 30 neurons with the highest activations to produce spikes (k=30 for k-WTA), ensuring a consistent number of spikes across all composed representations. During unbinding, activations after the backward pass are split evenly into two parts, each using k-Winners-Take-All (k-WTA) to achieve the expected number of one bits. Specifically, 30 neurons are selected for composed representations (k=30 for k-WTA), and 60 neurons for recalled words (k=60 for k-WTA). Note that similar performance can be achieved without WTA competition if the neuron thresholds are properly adjusted for the activation level at each binding and unbinding step. The CAM trained on the vocabulary is used after the unbinding process concludes. The input size for the CAM is 12,000, and the memory size is 24,000, where k-WTA selects 30 neurons in the memory layer for composed representations, and 60 neurons in the input layer for clean-up words. In both <xref ref-type="fig" rid="F5">Fig. 5C</xref> and <xref ref-type="fig" rid="F6">Fig. 6C</xref>, top-down unbinding results are depicted for varying ensemble lengths and numbers. The range of ensemble counts starts from 100 and extends to 6000, with an interval of 100 between each data point. The error on the z-axis represents the ratio of wrong bits to expected ones.</p></sec><sec id="S20"><label>4.5.2</label><title>Details of online iterated binding (<xref ref-type="fig" rid="F6">Fig. 6</xref>)</title><p id="P62">In the online iterated binding (<xref ref-type="fig" rid="F6">Fig. 6A</xref>), we start by combining two elements to form their composite (e.g., <italic>A</italic> and <italic>B</italic> to 〈<italic>A, B</italic>〉), and the composite 〈<italic>A, B</italic>〉 is then remapped back to the input layer, where it is combined with another element <italic>C</italic> to generate 〈〈<italic>A, B</italic>〉, <italic>C</italic>〉. The process is repeated until the entire sentence is in its complete binding form. Given the number of operations performed by the network as depth, the word quantity contained in the compressed sentence is <italic>K</italic> = <italic>depth</italic> + 1. During the unbinding phase, we utilize the feedback weight that is pre-trained in the forward pass to achieve the mapping from composed representation to inputs recursively (<xref ref-type="fig" rid="F6">Fig. 6B</xref>). We first start from the compressed form of the sentence to obtain the initial unbinding result (e.g., 〈〈〈<italic>A, B</italic>〉<italic>, C</italic>〉, <italic>D</italic>〉 to 〈〈<italic>A, B</italic>〉, <italic>C</italic>〉’ and <italic>D′</italic> 〉. Subsequently, 〈〈<italic>A, B</italic>〉, <italic>C</italic>〉′ is re-introduced as an input in memory neurons, resulting in 〈<italic>A, B</italic>〉′ and <italic>C′</italic>, and so forth, until we obtain the final unbinding results <italic>A′,B′,C′,D′</italic>.</p></sec></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Information</label><media xlink:href="EMS205799-supplement-Supplementary_Information.pdf" mimetype="application" mime-subtype="pdf" id="d46aAcEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S21"><title>Acknowledgments</title><p>The research of WM was partially supported by the National Science Foundation of the USA (EFRI BRAID project 2318152) and the Austrian Science Fund (FWF) (10.55776/COE12). The research of Chengting Yu during his period at TU Graz was supported by the Feiying Program from Zhejiang University. The research of Aili Wang was supported by the National Natural Science Foundation of China under Grant No. 62304203. The authors gratefully acknowledge the Gauss Centre for Super-computing e.V. (<ext-link ext-link-type="uri" xlink:href="http://www.gauss-centre.eu">www.gauss-centre.eu</ext-link>) for funding this project by providing computing time on the GCS Supercomputer JUWELS[1] at Jülich Supercomputing Centre (JSC).</p></ack><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartol</surname><given-names>TM</given-names><suffix>Jr</suffix></name><name><surname>Bromer</surname><given-names>C</given-names></name><name><surname>Kinney</surname><given-names>J</given-names></name><name><surname>Chirillo</surname><given-names>MA</given-names></name><name><surname>Bourne</surname><given-names>JN</given-names></name><name><surname>Harris</surname><given-names>KM</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><article-title>Nanoconnectomic upper bound on the variability of synaptic plasticity</article-title><source>elife</source><year>2015</year><volume>4</volume><elocation-id>e10778</elocation-id><pub-id pub-id-type="pmcid">PMC4737657</pub-id><pub-id pub-id-type="pmid">26618907</pub-id><pub-id pub-id-type="doi">10.7554/eLife.10778</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bittner</surname><given-names>KC</given-names></name><name><surname>Grienberger</surname><given-names>C</given-names></name><name><surname>Vaidya</surname><given-names>SP</given-names></name><name><surname>Milstein</surname><given-names>AD</given-names></name><name><surname>Macklin</surname><given-names>JJ</given-names></name><name><surname>Suh</surname><given-names>J</given-names></name><name><surname>Tonegawa</surname><given-names>S</given-names></name><name><surname>Magee</surname><given-names>JC</given-names></name></person-group><article-title>Conjunctive input processing drives feature selectivity in hippocampal ca1 neurons</article-title><source>Nature neuroscience</source><year>2015</year><volume>18</volume><issue>8</issue><fpage>1133</fpage><lpage>1142</lpage><pub-id pub-id-type="pmcid">PMC4888374</pub-id><pub-id pub-id-type="pmid">26167906</pub-id><pub-id pub-id-type="doi">10.1038/nn.4062</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bittner</surname><given-names>KC</given-names></name><name><surname>Milstein</surname><given-names>AD</given-names></name><name><surname>Grienberger</surname><given-names>C</given-names></name><name><surname>Romani</surname><given-names>S</given-names></name><name><surname>Magee</surname><given-names>JC</given-names></name></person-group><article-title>Behavioral time scale synaptic plasticity underlies ca1 place fields</article-title><source>Science</source><year>2017</year><volume>357</volume><issue>6355</issue><fpage>1033</fpage><lpage>1036</lpage><pub-id pub-id-type="pmcid">PMC7289271</pub-id><pub-id pub-id-type="pmid">28883072</pub-id><pub-id pub-id-type="doi">10.1126/science.aan3846</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Camposampiero</surname><given-names>G</given-names></name><name><surname>Hersche</surname><given-names>M</given-names></name><name><surname>Wattenhofer</surname><given-names>R</given-names></name><name><surname>Sebastian</surname><given-names>A</given-names></name><name><surname>Rahimi</surname><given-names>A</given-names></name></person-group><article-title>Can large reasoning models do analogical reasoning under perceptual uncertainty?</article-title><source>arXiv preprint</source><year>2025</year><elocation-id>arXiv:2503.11207</elocation-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>G</given-names></name><name><surname>Scherr</surname><given-names>F</given-names></name><name><surname>Maass</surname><given-names>W</given-names></name></person-group><article-title>A data-based large-scale model for primary visual cortex enables brain-like robust and versatile visual processing</article-title><source>Science advances</source><year>2022</year><volume>8</volume><issue>44</issue><elocation-id>eabq7592</elocation-id><pub-id pub-id-type="pmcid">PMC9629744</pub-id><pub-id pub-id-type="pmid">36322646</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.abq7592</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chéreau</surname><given-names>R</given-names></name><name><surname>Williams</surname><given-names>LE</given-names></name><name><surname>Bawa</surname><given-names>T</given-names></name><name><surname>Holtmaat</surname><given-names>A</given-names></name></person-group><chapter-title>Circuit mechanisms for cortical plasticity and learning</chapter-title><source>S’eminars in cell &amp; developmental biology</source><publisher-name>Elsevier</publisher-name><year>2022</year><volume>125</volume><fpage>68</fpage><lpage>75</lpage><pub-id pub-id-type="pmid">34332885</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chomsky</surname><given-names>N</given-names></name><name><surname>Seely</surname><given-names>TD</given-names></name><name><surname>Berwick</surname><given-names>RC</given-names></name><name><surname>Fong</surname><given-names>S</given-names></name><name><surname>Huybregts</surname><given-names>M</given-names></name><name><surname>Kitahara</surname><given-names>H</given-names></name><name><surname>McInnerney</surname><given-names>A</given-names></name><name><surname>Sugimoto</surname><given-names>Y</given-names></name></person-group><source>Merge and the strong minimalist thesis</source><publisher-name>Cambridge University Press</publisher-name><year>2023</year></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davies</surname><given-names>M</given-names></name><name><surname>Wild</surname><given-names>A</given-names></name><name><surname>Orchard</surname><given-names>G</given-names></name><name><surname>Sandamirskaya</surname><given-names>Y</given-names></name><name><surname>Guerra</surname><given-names>GAF</given-names></name><name><surname>Joshi</surname><given-names>P</given-names></name><name><surname>Plank</surname><given-names>P</given-names></name><name><surname>Risbud</surname><given-names>SR</given-names></name></person-group><article-title>Advancing neuromorphic computing with loihi: A survey of results and outlook</article-title><source>Proceedings of the IEEE</source><year>2021</year><volume>109</volume><issue>5</issue><fpage>911</fpage><lpage>934</lpage></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eliasmith</surname><given-names>C</given-names></name><name><surname>Anderson</surname><given-names>CH</given-names></name></person-group><article-title>Neural engineering: Computation, representation, and dynamics in neurobiological systems</article-title><year>2003</year></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eliasmith</surname><given-names>C</given-names></name><name><surname>Stewart</surname><given-names>TC</given-names></name><name><surname>Choo</surname><given-names>X</given-names></name><name><surname>Bekolay</surname><given-names>T</given-names></name><name><surname>DeWolf</surname><given-names>T</given-names></name><name><surname>Tang</surname><given-names>Y</given-names></name><name><surname>Rasmussen</surname><given-names>D</given-names></name></person-group><article-title>A large-scale model of the functioning brain</article-title><source>science</source><year>2012</year><volume>338</volume><issue>6111</issue><fpage>1202</fpage><lpage>1205</lpage><pub-id pub-id-type="pmid">23197532</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franch</surname><given-names>M</given-names></name><name><surname>Mickiewicz</surname><given-names>EA</given-names></name><name><surname>Belanger</surname><given-names>JL</given-names></name><name><surname>Chericoni</surname><given-names>A</given-names></name><name><surname>Chavez</surname><given-names>AG</given-names></name><name><surname>Katlowitz</surname><given-names>KA</given-names></name><name><surname>Mathura</surname><given-names>R</given-names></name><name><surname>Bartoli</surname><given-names>E</given-names></name><name><surname>Kemmer</surname><given-names>S</given-names></name><name><surname>Paulo</surname><given-names>D</given-names></name><etal/></person-group><article-title>A vectorial code for semantics in human hippocampus</article-title><source>bioRxiv</source><year>2025</year><comment>2025–02</comment></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frankland</surname><given-names>SM</given-names></name><name><surname>Greene</surname><given-names>JD</given-names></name></person-group><article-title>Concepts and compositionality: in search of the brain’s language of thought</article-title><source>Annual review of psychology</source><year>2020a</year><volume>71</volume><issue>1</issue><fpage>273</fpage><lpage>303</lpage><pub-id pub-id-type="pmid">31550985</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frankland</surname><given-names>SM</given-names></name><name><surname>Greene</surname><given-names>JD</given-names></name></person-group><article-title>Two ways to build a thought: distinct forms of compositional semantic representation across brain regions</article-title><source>Cerebral Cortex</source><year>2020b</year><volume>30</volume><issue>6</issue><fpage>3838</fpage><lpage>3855</lpage><pub-id pub-id-type="pmid">32279078</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Froemke</surname><given-names>RC</given-names></name><name><surname>Debanne</surname><given-names>D</given-names></name><name><surname>Bi</surname><given-names>GQ</given-names></name></person-group><article-title>Temporal modulation of spike-timing-dependent plasticity</article-title><source>Frontiers in synaptic neuroscience</source><year>2010</year><volume>2</volume><elocation-id>1391</elocation-id><pub-id pub-id-type="pmcid">PMC3059714</pub-id><pub-id pub-id-type="pmid">21423505</pub-id><pub-id pub-id-type="doi">10.3389/fnsyn.2010.00019</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grienberger</surname><given-names>C</given-names></name><name><surname>Magee</surname><given-names>JC</given-names></name></person-group><article-title>Entorhinal cortex directs learning-related changes in ca1 representations</article-title><source>Nature</source><year>2022</year><volume>611</volume><issue>7936</issue><fpage>554</fpage><lpage>562</lpage><pub-id pub-id-type="pmcid">PMC9668747</pub-id><pub-id pub-id-type="pmid">36323779</pub-id><pub-id pub-id-type="doi">10.1038/s41586-022-05378-6</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guzman</surname><given-names>SJ</given-names></name><name><surname>Schlögl</surname><given-names>A</given-names></name><name><surname>Frotscher</surname><given-names>M</given-names></name><name><surname>Jonas</surname><given-names>P</given-names></name></person-group><article-title>Synaptic mechanisms of pattern completion in the hippocampal ca3 network</article-title><source>Science</source><year>2016</year><volume>353</volume><issue>6304</issue><fpage>1117</fpage><lpage>1123</lpage><pub-id pub-id-type="pmid">27609885</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hersche</surname><given-names>M</given-names></name><name><surname>Zeqiri</surname><given-names>M</given-names></name><name><surname>Benini</surname><given-names>L</given-names></name><name><surname>Sebastian</surname><given-names>A</given-names></name><name><surname>Rahimi</surname><given-names>A</given-names></name></person-group><article-title>A neuro-vector-symbolic architecture for solving raven’s progressive matrices</article-title><source>Nature Machine Intelligence</source><year>2023</year><volume>5</volume><issue>4</issue><fpage>363</fpage><lpage>375</lpage></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>John</surname><given-names>T</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name><name><surname>Aljishi</surname><given-names>A</given-names></name><name><surname>Rieck</surname><given-names>B</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name><name><surname>Damisah</surname><given-names>EC</given-names></name></person-group><article-title>Representation of visual sequences in the tuning and topology of neuronal activity in the human hippocampus</article-title><source>bioRxiv</source><year>2025</year><fpage>2025</fpage><lpage>03</lpage></element-citation></ref><ref id="R19"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kanerva</surname><given-names>P</given-names></name></person-group><source>Sparse distributed memory</source><publisher-name>MIT press</publisher-name><year>1988</year><pub-id pub-id-type="pmid">18255679</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kazanina</surname><given-names>N</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>The neural ingredients for a language of thought are available</article-title><source>Trends in cognitive sciences</source><year>2023</year><volume>27</volume><issue>11</issue><fpage>996</fpage><lpage>1007</lpage><pub-id pub-id-type="pmid">37625973</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khaddam-Aljameh</surname><given-names>R</given-names></name><name><surname>Stanisavljevic</surname><given-names>M</given-names></name><name><surname>Mas</surname><given-names>JF</given-names></name><name><surname>Karunaratne</surname><given-names>G</given-names></name><name><surname>Brändli</surname><given-names>M</given-names></name><name><surname>Liu</surname><given-names>F</given-names></name><name><surname>Singh</surname><given-names>A</given-names></name><name><surname>Müller</surname><given-names>SM</given-names></name><name><surname>Egger</surname><given-names>U</given-names></name><name><surname>Petropoulos</surname><given-names>A</given-names></name><etal/></person-group><article-title>Hermes-core—a 1.59-tops/mm 2 pcm on 14-nm cmos in-memory compute core using 300-ps/lsb linearized cco-based adcs</article-title><source>IEEE Journal of Solid-State Circuits</source><year>2022</year><volume>57</volume><issue>4</issue><fpage>1027</fpage><lpage>1038</lpage></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleyko</surname><given-names>D</given-names></name><name><surname>Rachkovskij</surname><given-names>D</given-names></name><name><surname>Osipov</surname><given-names>E</given-names></name><name><surname>Rahimi</surname><given-names>A</given-names></name></person-group><article-title>A survey on hyperdimensional computing aka vector symbolic architectures, part ii: Applications, cognitive models, and challenges</article-title><source>ACM Computing Surveys</source><year>2023</year><volume>55</volume><issue>9</issue><fpage>1</fpage><lpage>52</lpage></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleyko</surname><given-names>D</given-names></name><name><surname>Rachkovskij</surname><given-names>DA</given-names></name><name><surname>Osipov</surname><given-names>E</given-names></name><name><surname>Rahimi</surname><given-names>A</given-names></name></person-group><article-title>A survey on hyperdimensional computing aka vector symbolic architectures, part i: Models and data transformations</article-title><source>ACM Computing Surveys</source><year>2022</year><volume>55</volume><issue>6</issue><fpage>1</fpage><lpage>40</lpage></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kolibius</surname><given-names>LD</given-names></name><name><surname>Roux</surname><given-names>F</given-names></name><name><surname>Parish</surname><given-names>G</given-names></name><name><surname>Ter Wal</surname><given-names>M</given-names></name><name><surname>Van Der Plas</surname><given-names>M</given-names></name><name><surname>Chelvarajah</surname><given-names>R</given-names></name><name><surname>Sawlani</surname><given-names>V</given-names></name><name><surname>Rollings</surname><given-names>DT</given-names></name><name><surname>Lang</surname><given-names>JD</given-names></name><name><surname>Gollwitzer</surname><given-names>S</given-names></name><etal/></person-group><article-title>Hippocampal neurons code individual episodic memories in humans</article-title><source>Nature human behaviour</source><year>2023</year><volume>7</volume><issue>11</issue><fpage>1968</fpage><lpage>1979</lpage><pub-id pub-id-type="pmcid">PMC10663153</pub-id><pub-id pub-id-type="pmid">37798368</pub-id><pub-id pub-id-type="doi">10.1038/s41562-023-01706-6</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koster</surname><given-names>R</given-names></name><name><surname>Chadwick</surname><given-names>MJ</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Berron</surname><given-names>D</given-names></name><name><surname>Banino</surname><given-names>A</given-names></name><name><surname>Duözel</surname><given-names>E</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Kumaran</surname><given-names>D</given-names></name></person-group><article-title>Big-loop recurrence within the hippocampal system supports integration of information across episodes</article-title><source>Neuron</source><year>2018</year><volume>99</volume><issue>6</issue><fpage>1342</fpage><lpage>1354</lpage><pub-id pub-id-type="pmid">30236285</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kurth-Nelson</surname><given-names>Z</given-names></name><name><surname>Behrens</surname><given-names>T</given-names></name><name><surname>Wayne</surname><given-names>G</given-names></name><name><surname>Miller</surname><given-names>K</given-names></name><name><surname>Luettgau</surname><given-names>L</given-names></name><name><surname>Dolan</surname><given-names>R</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Schwarten-beck</surname><given-names>P</given-names></name></person-group><article-title>Replay and compositional computation</article-title><source>Neuron</source><year>2023</year><volume>111</volume><issue>4</issue><fpage>454</fpage><lpage>469</lpage><pub-id pub-id-type="pmid">36640765</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Briguglio</surname><given-names>JJ</given-names></name><name><surname>Romani</surname><given-names>S</given-names></name><name><surname>Magee</surname><given-names>JC</given-names></name></person-group><article-title>Mechanisms of memory-supporting neuronal dynamics in hippocampal area ca3</article-title><source>Cell</source><year>2024</year><volume>187</volume><issue>24</issue><fpage>6804</fpage><lpage>6819</lpage><pub-id pub-id-type="pmid">39454575</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Gao</surname><given-names>C</given-names></name><name><surname>Wang</surname><given-names>P</given-names></name><name><surname>Friederici</surname><given-names>AD</given-names></name><name><surname>Zaccarella</surname><given-names>E</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name></person-group><article-title>Exploring the neurobiology of merge at a basic level: insights from a novel artificial grammar paradigm</article-title><source>Frontiers in Psychology</source><year>2023</year><volume>14</volume><elocation-id>1151518</elocation-id><pub-id pub-id-type="pmcid">PMC10242141</pub-id><pub-id pub-id-type="pmid">37287773</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2023.1151518</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Magee</surname><given-names>JC</given-names></name><name><surname>Grienberger</surname><given-names>C</given-names></name></person-group><article-title>Synaptic plasticity forms and functions</article-title><source>Annual review of neuroscience</source><year>2020</year><volume>43</volume><issue>1</issue><fpage>95</fpage><lpage>117</lpage><pub-id pub-id-type="pmid">32075520</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Milstein</surname><given-names>AD</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Bittner</surname><given-names>KC</given-names></name><name><surname>Grienberger</surname><given-names>C</given-names></name><name><surname>Soltesz</surname><given-names>I</given-names></name><name><surname>Magee</surname><given-names>JC</given-names></name><name><surname>Romani</surname><given-names>S</given-names></name></person-group><article-title>Bidirectional synaptic plasticity rapidly modifies hippocampal representations</article-title><source>Elife</source><year>2021</year><volume>10</volume><elocation-id>e73046</elocation-id><pub-id pub-id-type="pmcid">PMC8776257</pub-id><pub-id pub-id-type="pmid">34882093</pub-id><pub-id pub-id-type="doi">10.7554/eLife.73046</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Müller</surname><given-names>MG</given-names></name><name><surname>Papadimitriou</surname><given-names>CH</given-names></name><name><surname>Maass</surname><given-names>W</given-names></name><name><surname>Legenstein</surname><given-names>R</given-names></name></person-group><article-title>A model for structured information representation in neural networks of the brain</article-title><source>eneuro</source><year>2020</year><volume>7</volume><issue>3</issue><pub-id pub-id-type="pmcid">PMC7266140</pub-id><pub-id pub-id-type="pmid">32381648</pub-id><pub-id pub-id-type="doi">10.1523/ENEURO.0533-19.2020</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Papadimitriou</surname><given-names>CH</given-names></name><name><surname>Friederici</surname><given-names>AD</given-names></name></person-group><article-title>Bridging the gap between neurons and cognition through assemblies of neurons</article-title><source>Neural Computation</source><year>2022</year><volume>34</volume><issue>2</issue><fpage>291</fpage><lpage>306</lpage><pub-id pub-id-type="pmid">34915560</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Papadimitriou</surname><given-names>CH</given-names></name><name><surname>Vempala</surname><given-names>SS</given-names></name><name><surname>Mitropolsky</surname><given-names>D</given-names></name><name><surname>Collins</surname><given-names>M</given-names></name><name><surname>Maass</surname><given-names>W</given-names></name></person-group><article-title>Brain computation by assemblies of neurons</article-title><source>Proceedings of the National Academy of Sciences</source><year>2020</year><volume>117</volume><issue>25</issue><fpage>14464</fpage><lpage>14472</lpage><pub-id pub-id-type="pmcid">PMC7322080</pub-id><pub-id pub-id-type="pmid">32518114</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2001893117</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piantadosi</surname><given-names>ST</given-names></name><name><surname>Muller</surname><given-names>DC</given-names></name><name><surname>Rule</surname><given-names>JS</given-names></name><name><surname>Kaushik</surname><given-names>K</given-names></name><name><surname>Gorenstein</surname><given-names>M</given-names></name><name><surname>Leib</surname><given-names>ER</given-names></name><name><surname>Sanford</surname><given-names>E</given-names></name></person-group><article-title>Why concepts are (probably) vectors</article-title><source>Trends in Cognitive Sciences</source><year>2024</year><volume>28</volume><issue>9</issue><fpage>844</fpage><lpage>856</lpage><pub-id pub-id-type="pmid">39112125</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Plate</surname><given-names>TA</given-names></name></person-group><article-title>Holographic reduced representations</article-title><source>IEEE Transactions on Neural networks</source><year>1995</year><volume>6</volume><issue>3</issue><fpage>623</fpage><lpage>641</lpage><pub-id pub-id-type="pmid">18263348</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quiroga</surname><given-names>RQ</given-names></name></person-group><article-title>Concept cells: the building blocks of declarative memory functions</article-title><source>Nature Reviews Neuroscience</source><year>2012</year><volume>13</volume><issue>8</issue><fpage>587</fpage><lpage>597</lpage><pub-id pub-id-type="pmid">22760181</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>A</given-names></name><name><surname>Plank</surname><given-names>P</given-names></name><name><surname>Wild</surname><given-names>A</given-names></name><name><surname>Maass</surname><given-names>W</given-names></name></person-group><article-title>A long short-term memory for ai applications in spike-based neuromorphic hardware</article-title><source>Nature Machine Intelligence</source><year>2022</year><volume>4</volume><issue>5</issue><fpage>467</fpage><lpage>479</lpage></element-citation></ref><ref id="R38"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Valiant</surname><given-names>LG</given-names></name></person-group><source>Circuits of the Mind</source><publisher-name>Oxford University Press</publisher-name><year>2000</year></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name></person-group><article-title>Overview of phase-change materials based photonic devices</article-title><source>IEEE Access</source><year>2020</year><volume>8</volume><fpage>121211</fpage><lpage>121245</lpage></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>Maass</surname><given-names>W</given-names></name></person-group><article-title>A simple model for behavioral time scale synaptic plasticity (btsp) provides content addressable memory with binary synapses and one-shot learning</article-title><source>Nature communications</source><year>2025</year><volume>16</volume><issue>1</issue><fpage>342</fpage><pub-id pub-id-type="pmcid">PMC11695864</pub-id><pub-id pub-id-type="pmid">39747916</pub-id><pub-id pub-id-type="doi">10.1038/s41467-024-55563-6</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>X</given-names></name><name><surname>Hsu</surname><given-names>CL</given-names></name><name><surname>Spruston</surname><given-names>N</given-names></name></person-group><article-title>Rapid synaptic plasticity contributes to a learned conjunctive code of position and choice-related information in the hippocampus</article-title><source>Neuron</source><year>2022</year><volume>110</volume><issue>1</issue><fpage>96</fpage><lpage>108</lpage><pub-id pub-id-type="pmid">34678146</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Fig. 1</label><caption><title>Fundamental properties of BTSP.</title><p><bold>(A)</bold> BTSP transforms an input string of bits, represented by firing or non-firing of input neurons, into a corresponding representation of an output bit string by memory neurons through one-shot learning, with synaptic plasticity restricted to memory neurons that receive a gating signal (plateau potential). <bold>(B)</bold> Distribution of weighted sums at a generic memory neuron that results from applying a fixed random projection (a fixed stochastic binary matrix, whose entries are independent of the input vectors) to 30k input vectors. No matter how one sets the firing threshold of the neuron (see insert for a blow-up), numerous input vectors cause weighted sums that are close to the threshold, and therefore may cross the threshold when the input vector is slightly perturbed. <bold>(C)</bold> If the binary synaptic weights result instead from applications of BTSP to the same list of input patterns, the distribution of weighted sums becomes bi-modal: One mode (green) arises from input patterns for which BTSP was not applied to this neuron, and another mode (yellow) from input patterns for which BTSP was applied to this neuron. This bi-modal distribution enables a choice of the firing threshold that is robust to small changes in the input pattern. This noise-robustness lies at the heart of the attractor features of composed representations in BTSP-based HDC. Panels B and C are reprinted from (<xref ref-type="bibr" rid="R40">Wu and Maass, 2025</xref>). <bold>(D)</bold> In order to be able to reconstruct an input vector from a composed representation, also synaptic weights of connections from the memory neurons to the input neurons have to be assigned. One-shot learning with Hebbian plasticity suffices, due to the sparsity of both bit strings. Note that Hebbian plasticity, rather than BTSP, is suitable for this type of synaptic plasticity because the values in the input pattern can be seen as postsynaptic activity in this case of self-supervised learning. <bold>(E)</bold> The noise robustness of BTSP from panel C endows composed representations with attractor features that, in conjunction with the feedback connections from panel D, create a content addressable memory (CAM) for those input patterns for which BTSP was applied. This panel was copied from (<xref ref-type="bibr" rid="R40">Wu and Maass, 2025</xref>)</p></caption><graphic xlink:href="EMS205799-f001"/></fig><fig id="F2" position="float"><label>Fig. 2</label><caption><title>Top-down unbinding from a composed representation created through BTSP.</title><p>We considered here the case where composed representations were created through BTSP from sequences of 8 words (each represented by sparse binary vectors). BTSP was applied sequentially to a sequence of <italic>K</italic> different combinations of 8 words, where K, the considered number of ”sentences”, varied between 100 and 11,000. <bold>(A,B)</bold> For each of these sentences, BTSP was applied to the forward connections, and a Hebbian rule simultaneously to feedback connections, according to the general scheme of <xref ref-type="fig" rid="F1">Fig. 1 A and D</xref>. <bold>(C,D)</bold> General scheme for binding 8 words into a composed representation, and for decoding these words from the composed representation. <bold>(E)</bold> Scheme for cleaning up decoded words A’, B’, ..by applying a BTSP-based CAM that was trained for the chosen vocabulary of 1000 words. <bold>(F)</bold> The resulting error is shown both without and with a clean-up of word codes via a BTSP-based CAM, as indicated in panel E. Decoding performance is good for up to 3000 sentences without a clean-up, and for about 7000 with the CAM. Furthermore, the dotted lines show that noise that flips the output of a memory neurons with probability 0.0025 affects the performance only slightly, in spite of the fact that this level of noise is on the same scale as the density of 1’s in composed representations.</p></caption><graphic xlink:href="EMS205799-f002"/></fig><fig id="F3" position="float"><label>Fig. 3</label><caption><title>Bottom-up unbinding.</title><p><bold>(A, B)</bold> Scheme for bottom-up unbinding. Note that, unlike unbinding for currently existing HDC methods, the composed representation was not available for unbinding. Only 2 arbitrarily chosen words B and D, here in positions 2 and 4, were given as cues. <bold>(C)</bold> Performance of bottom-up binding with 2 out of 8 words as cues, with and without clean-up of decoded words via a BTSP-based CAM. The error measure is the same as in <xref ref-type="fig" rid="F2">Fig, 2F</xref>. <bold>(D)</bold> We consider here the case where 2 randomly selected words from longer sentences with 12 or 16 words are used as cues. In other words, just 1/6 or 1/8 of the bits in the sentence are used as cue. Good performance can still be achieved in this case, but degrades gracefully when the number of sentences becomes very large.</p></caption><graphic xlink:href="EMS205799-f003"/></fig><fig id="F4" position="float"><label>Fig. 4</label><caption><title>Bottom-up unbinding from single words when several correct unbinding results exist.</title><p><bold>(A)</bold> During the binding process, a newly added recurrent network module is positioned between the input and memory layers to enable a sequential dynamics of unbinding results. <bold>(B)</bold> Lateral connections within the recurrent network module are trained through a Hebbian variant of BTSP that was found in area CA3 of the brain (<xref ref-type="bibr" rid="R27">Li et al., 2024</xref>). <bold>(C)</bold> While a cue is presented on the input layer, the network state <italic>S</italic>(<italic>B</italic>, 2)<italic><sub>t</sub></italic> of the recurrent network module evolves dynamically, where the next network state depends both on the cue and the preceding network state. These network states in the recurrent network module induce a sequences of activity patterns <italic>R(B, 2)<sub>t</sub></italic> of the memory neurons. <bold>(D)</bold> Each activity pattern <italic>R</italic>(<italic>B</italic>, 2)<italic><sub>t</sub></italic> of the memory neurons induces via the feedback connections at each time step a possible result &lt; <inline-formula><mml:math id="M8"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>B</mml:mi><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:mo>..</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>H</mml:mi><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> &gt; of bottom-up unbinding. <bold>(E)</bold> Evaluation of the similarity (measured via cosine) of the dynamically evolving network states <italic>S</italic>(<italic>B,</italic> 2)<sub>t</sub> of the recurrent network module when the word B is presented as cue at the 2nd word position to its first state when a full sentence (one of the 4 possible solutions) is presented on the input layer. One sees that the network state cycles repeatedly through the 4 states (attractors) that result when one of the 4 possible solutions of this bottom-up unbinding test is presented on the input layer. <bold>(F)</bold> The same for activity patterns of the memory neurons. One also sees there repeated cycling through activity patterns that result as first activity pattern when one of the 4 possible unbinding solutions (i.e., full sentences) are presented on the input layer. Note that we have in our model no lateral connections between neurons on the memory layer (reflecting low lateral connectivity of pyramidal cells in area CA1). Hence the memory neurons would not be able to produce a sequence of different possible solutions with the preceding recurrrent network module. <bold>(G)</bold> Evaluation of the sequence of unbinding results that are produced in the input layer through feedback connections as indicated in panel D, both for the case of a total number of 1000 sentences, and for the case of 4000 sentences. The blue curve denotes the fraction of ambiguous single word cues for which each of the 4 possible correct unbinding results (sentences) were reproduced at some time point with the error indicated on the y-axis. The yellow curve shows for which fraction of them at least 2 of the 4 possible solutions were reproduced with the error indicated on the y-axis.</p></caption><graphic xlink:href="EMS205799-f004"/></fig><fig id="F5" position="float"><label>Fig. 5</label><caption><title>Hierarchical iterated binding and unbinding.</title><p><bold>(A)</bold> Scheme for hierarchical iterated binding. <bold>(B)</bold> Scheme for hierarchical top-down unbinding. <bold>(C,D)</bold> Performance of top-down unbinding for iterated hierarchical binding, with and without a BTSP-based CAM for cleaning up the words that are top-down generated at the input layer. The CAM significantly improves the unbinding result.</p></caption><graphic xlink:href="EMS205799-f005"/></fig><fig id="F6" position="float"><label>Fig. 6</label><caption><title>Online (linear) iterated binding and unbinding.</title><p><bold>(A)</bold> Scheme for online iterated binding. <bold>(B)</bold> Scheme for online iterated unbinding. <bold>(C,D)</bold> Performance of top-down unbinding for iterated online binding, with and without a BTSP-based CAM for cleaning up the words that are top-down generated at the input layer.</p></caption><graphic xlink:href="EMS205799-f006"/></fig><fig id="F7" position="float"><label>Fig. 7</label><caption><title>Application of BTSP-based binding in models for language processing in the brain.</title><p><bold>(A)</bold> Scheme for syntactic analysis of a sentence through interaction of several cortical areas (MTL, sSTG, BA44, BA45) proposed by (<xref ref-type="bibr" rid="R32">Papadimitriou and Friederici, 2022</xref>) (reprinted from this publication). <bold>(B)</bold> Binding of the 3 words from panel A and of their syntactic roles into a composed representation through a single application of BTSP, see also <xref ref-type="fig" rid="F2">Fig. 2A</xref>. <bold>(C)</bold> Decoding of constituent words and their syntactic roles from an internal composed representation of the sentence, as generated through BTSP in panel B. As in <xref ref-type="fig" rid="F2">Fig. 2E</xref>, a BTSP-based CAM can be used to clean up the decoded components. <bold>(D)</bold> Hierarchical model for producing a composed representation of the same sentence through BTSP-binding as in panel A, as special case of <xref ref-type="fig" rid="F5">Fig. 5</xref>.</p></caption><graphic xlink:href="EMS205799-f007"/></fig><table-wrap id="T1" position="float" orientation="portrait"><label>Table 1</label><caption><title>Definitions and explanations of key parameters</title></caption><table frame="box" rules="cols"><thead><tr><th align="center" valign="top">Parameters</th><th align="center" valign="top">Category</th><th align="left" valign="top">Explanations</th></tr></thead><tbody><tr style="border-top: solid thin"><td align="center" valign="top"><italic>W<sub>feed</sub></italic></td><td align="center" valign="top">Model learnable weights</td><td align="left" valign="top">The synaptic weight from input neurons to memory neurons, learned by BTSP</td></tr><tr><td align="center" valign="top"><italic>W<sub>back</sub></italic></td><td align="center" valign="top">Model learnable weights</td><td align="left" valign="top">The synaptic weight from memory neurons back to input neurons, learned by Hebbian rule</td></tr><tr><td align="center" valign="top"><italic>f<sub>q</sub></italic></td><td align="center" valign="top">Hyperparameter</td><td align="left" valign="top">Probability of plateau potential</td></tr><tr><td align="center" valign="top"><italic>f<sub>w</sub></italic></td><td align="center" valign="top">Hyperparameter</td><td align="left" valign="top">Probability of connections</td></tr><tr><td align="center" valign="top"><italic>f<sub>p</sub></italic></td><td align="center" valign="top">Hyperparameter</td><td align="left" valign="top">Input density</td></tr><tr><td align="center" valign="top"><italic>L</italic></td><td align="center" valign="top">Binding setting</td><td align="left" valign="top">Number of bits to represent each word</td></tr><tr><td align="center" valign="top"><italic>N<sub>w</sub></italic></td><td align="center" valign="top">Binding setting</td><td align="left" valign="top">Number of words in the vocabulary</td></tr><tr><td align="center" valign="top"><italic>K</italic></td><td align="center" valign="top">Binding setting</td><td align="left" valign="top">Number of words contained in each sentence</td></tr></tbody></table></table-wrap></floats-group></article>