<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS207390</article-id><article-id pub-id-type="doi">10.1101/2025.07.16.665209</article-id><article-id pub-id-type="archive">PPR1052267</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Dual-feature selectivity enables bidirectional coding in visual cortical neurons</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Franke</surname><given-names>Katrin</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref><xref ref-type="corresp" rid="CR1">✉</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Karantzas</surname><given-names>Nikos</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="corresp" rid="CR1">✉</xref></contrib><contrib contrib-type="author"><name><surname>Willeke</surname><given-names>Konstantin</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Diamantaki</surname><given-names>Maria</given-names></name><xref ref-type="aff" rid="A5">5</xref><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Ramakrishnan</surname><given-names>Kandan</given-names></name><xref ref-type="aff" rid="A7">7</xref></contrib><contrib contrib-type="author"><name><surname>Elumalai</surname><given-names>Pavithra</given-names></name><xref ref-type="aff" rid="A8">8</xref></contrib><contrib contrib-type="author"><name><surname>Restivo</surname><given-names>Kelli</given-names></name><xref ref-type="aff" rid="A7">7</xref></contrib><contrib contrib-type="author"><name><surname>Fahey</surname><given-names>Paul</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Nealley</surname><given-names>Cate</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Shinn</surname><given-names>Tori</given-names></name><xref ref-type="aff" rid="A11">11</xref></contrib><contrib contrib-type="author"><name><surname>Garcia</surname><given-names>Gabrielle</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Patel</surname><given-names>Saumil</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Ecker</surname><given-names>Alexander</given-names></name><xref ref-type="aff" rid="A8">8</xref><xref ref-type="aff" rid="A9">9</xref><xref ref-type="aff" rid="A10">10</xref></contrib><contrib contrib-type="author"><name><surname>Walker</surname><given-names>Edgar Y.</given-names></name><xref ref-type="aff" rid="A12">12</xref><xref ref-type="aff" rid="A13">13</xref></contrib><contrib contrib-type="author"><name><surname>Froudarakis</surname><given-names>Emmanouil</given-names></name><xref ref-type="aff" rid="A5">5</xref><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Sanborn</surname><given-names>Sophia</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Sinz</surname><given-names>Fabian H.</given-names></name><xref ref-type="aff" rid="A7">7</xref><xref ref-type="aff" rid="A8">8</xref><xref ref-type="aff" rid="A9">9</xref></contrib><contrib contrib-type="author"><name><surname>Tolias</surname><given-names>Andreas</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib></contrib-group><aff id="A1"><label>1</label>Department of Ophthalmology, Byers Eye Institute, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Stanford University</institution></institution-wrap> School of Medicine, <city>Stanford</city>, <state>CA</state>, <country country="US">US</country></aff><aff id="A2"><label>2</label>Stanford Bio-X, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Stanford University</institution></institution-wrap>, <city>Stanford</city>, <state>CA</state>, <country country="US">US</country></aff><aff id="A3"><label>3</label>Wu Tsai Neurosciences Institute, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Stanford University</institution></institution-wrap>, <city>Stanford</city>, <state>CA</state>, <country country="US">US</country></aff><aff id="A4"><label>4</label>Institute for Ophthalmic Research, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/03a1kwz48</institution-id><institution>Tübingen University</institution></institution-wrap>, <city>Tübingen</city>, <country country="DE">Germany</country></aff><aff id="A5"><label>5</label>Institute of Molecular Biology &amp; Biotechnology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/052rphn09</institution-id><institution>Foundation of Research &amp; Technology - Hellas</institution></institution-wrap>, <city>Heraklion</city>, <state>Crete</state>, <country country="GR">Greece</country></aff><aff id="A6"><label>6</label>Department of Basic Sciences, Faculty of Medicine,  <institution-wrap><institution-id institution-id-type="ror">https://ror.org/00dr28g20</institution-id><institution>University of Crete</institution></institution-wrap>, <city>Heraklion</city>, <state>Crete</state>, <country country="GR">Greece</country></aff><aff id="A7"><label>7</label>Department of Neuroscience &amp; Center for Neuroscience and Artificial Intelligence, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02pttbw34</institution-id><institution>Baylor College of Medicine</institution></institution-wrap>, <city>Houston</city>, <state>Texas</state>, <country country="US">USA</country></aff><aff id="A8"><label>8</label>Institute of Computer Science and Campus Institute Data Science, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01y9bpm73</institution-id><institution>University of Göttingen</institution></institution-wrap>, <country country="DE">Germany</country></aff><aff id="A9"><label>9</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04n901n71</institution-id><institution>Lower Saxony Center for AI and Causal Methods in Medicine</institution></institution-wrap>, <country country="DE">Germany</country></aff><aff id="A10"><label>10</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0087djs12</institution-id><institution>Max Planck Institute for Dynamics and Self-Organization</institution></institution-wrap>, <city>Göttingen</city>, <country country="DE">Germany</country></aff><aff id="A11"><label>11</label>Department of Pediatrics; Allergy &amp; Immunology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02pttbw34</institution-id><institution>Baylor College of Medicine</institution></institution-wrap>, <city>Houston</city>, <state>Texas</state>, <country country="US">USA</country></aff><aff id="A12"><label>12</label>Department of Neurobiology &amp; Biophysics, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cvxb145</institution-id><institution>University of Washington</institution></institution-wrap> School of Medicine, <state>WA</state>, <country country="US">USA</country></aff><aff id="A13"><label>13</label>Computational Neuroscience Center, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cvxb145</institution-id><institution>University of Washington</institution></institution-wrap>, <state>WA</state>, <country country="US">USA</country></aff><author-notes><corresp id="CR1">
<bold>Correspondence:</bold> <email>kafranke@stanford.edu</email>, <email>nikoskar@stanford.edu</email>
</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>22</day><month>07</month><year>2025</year></pub-date><pub-date pub-type="preprint"><day>21</day><month>07</month><year>2025</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nd/4.0/">CC BY-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Sensory neurons are traditionally viewed as feature detectors that respond with an increase in firing rate to preferred stimuli while remaining unresponsive to others. Here, we identify a dual-feature encoding strategy in macaque visual cortex, wherein many neurons in areas V1 and V4 are selectively tuned to two distinct visual features—one that enhances and one that suppresses activity—around an elevated baseline firing rate. By combining neuronal recordings with functional digital twin models—deep learning-based predictive models of biological neurons—we were able to systematically identify each neuron’s preferred and non-preferred features. These feature pairs served as anchors for a continuous, low-dimensional axis in natural image similarity space, along which neuronal activity varied approximately linearly. Within a single visual area, visual features that strongly or weakly activated individual neurons also had a high probability of modulating the activity of other neurons, suggesting a shared feature selectivity across the population that structures stimulus encoding. We show that this encoding strategy is conserved across species, present in both primary and lateral visual areas of mouse cortex. Dual-feature selectivity is consistent with recent anatomical evidence for feature-specific inhibitory connectivity, suggesting a coding strategy in which selective excitation and inhibition increase the representational capacity of the neuronal population.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Early sensory physiology established the picture of the single neuron as a feature detector that remains mostly silent until a stimulus in its receptive field resembles a preferred pattern (e.g. <xref ref-type="bibr" rid="R1">Lettvin et al., 1959</xref>). This view has intuitive appeal because it suggests that each neuron signals the presence of a specific feature in the sensory world, resulting in a sparse (<xref ref-type="bibr" rid="R2">Barlow, 1961</xref>; <xref ref-type="bibr" rid="R3">Field, 1987</xref>; <xref ref-type="bibr" rid="R4">Olshausen and Field, 1996</xref>) and metabolically efficient code (<xref ref-type="bibr" rid="R5">Levy and Baxter, 1996</xref>; <xref ref-type="bibr" rid="R6">Attwell and Laughlin, 2001</xref>). For example, in cat and monkey primary visual cortex (V1), a simple cell’s preferred pattern can be described as a specific Gabor function (<xref ref-type="bibr" rid="R7">Hubel and Wiesel, 1962</xref>, <xref ref-type="bibr" rid="R8">1968</xref>; <xref ref-type="bibr" rid="R9">Bishop et al., 1973</xref>)—a precise combination of orientation, spatial frequency, phase, size, and retinal location. As the similarity of a stimulus to this Gabor increases, the neuron’s firing rate rises, while all dissimilar stimuli evoke little or no activity. Complex cells extend this principle by pooling across spatial phase, yielding phase-invariant orientation tuning (<xref ref-type="bibr" rid="R8">Hubel and Wiesel, 1968</xref>; <xref ref-type="bibr" rid="R10">Schiller et al., 1976</xref>). This feature detector view has been extrapolated to higher visual areas, where, in the idealized model, a neuron might respond exclusively to a specific face, as in the so-called “Jennifer Aniston neuron” (<xref ref-type="bibr" rid="R11">Quiroga et al., 2005</xref>).</p><p id="P3">Under this framework, lifetime sparsity—the proportion of stimuli that elicit strong firing across a neuron’s entire experience (see also <xref ref-type="bibr" rid="R12">Willmore and Tolhurst, 2001</xref>; <xref ref-type="bibr" rid="R13">Willmore et al., 2011</xref>)—is determined by the statistics of the preferred feature: neurons tuned to low spatial frequencies tend to fire more frequently because such content is abundant in natural scenes due to their 1<italic>/f</italic> spectral properties (<xref ref-type="bibr" rid="R3">Field, 1987</xref>), whereas neurons selective for high-frequency edges or specific faces may fire only rarely. Importantly, it is assumed that the underlying code is unidirectional, with each neuron signaling the presence of its preferred feature through increased firing rates.</p><p id="P4">Neuronal responses of real cortical neurons, however, reflect a balance of excitatory (i.e. preferred stimulus) and inhibitory inputs (i.e. non-preferred stimulus) within their receptive field. This inhibition is usually considered to be broadly tuned, providing a form of blanket gain control that normalizes neuronal activity across the population (<xref ref-type="bibr" rid="R14">Heeger, 1992</xref>; <xref ref-type="bibr" rid="R15">Carandini and Heeger, 2011</xref>). A classic example of feature-specific inhibition constitutes cross-orientation inhibition in V1, where responses to a neuron’s preferred orientation are suppressed by the simultaneous presentation of an orthogonal grating (<xref ref-type="bibr" rid="R16">Morrone et al., 1982</xref>; <xref ref-type="bibr" rid="R17">Allison et al., 1995</xref>; <xref ref-type="bibr" rid="R18">Ferster, 1986</xref>). While these examples illustrate well-characterized forms of inhibition, how inhibition shapes neuronal tuning within the space of natural images remains poorly understood. Experimental constraints limit the number of stimuli that can be presented during neuronal recordings, making it challenging to comprehensively map the excitatory and inhibitory influences that determine a neuron’s response across this high-dimensional stimulus space. Resolving this challenge is central to deciphering how neuronal populations transform complex sensory input into meaningful representations.</p><p id="P5">To overcome this challenge, we leveraged functional digital twin models trained on neuronal recordings from macaque primary (V1) and mid-level (V4) visual cortex, as well as mouse visual cortex. A <italic>digital twin</italic> of a brain area (<xref ref-type="bibr" rid="R19">Walker et al., 2019</xref>; <xref ref-type="bibr" rid="R20">Bashivan et al., 2019</xref>; <xref ref-type="bibr" rid="R21">Franke et al., 2022</xref>; <xref ref-type="bibr" rid="R22">Willeke et al., 2023</xref>) can be constructed by training a deep neural network to learn the relationship between stimuli and the resulting neuronal responses. If trained with enough high-quality data, the model can be used to simulate the neuronal response to data never seen by the animal—allowing researchers to scale experiments in silico that would be difficult or impossible in vivo. This approach enabled us to perform a systematic characterization of neuronal selectivity across the full dynamic range of responses to naturalistic images.</p><p id="P6">We found that many neurons in visual cortex maintain non-zero baseline firing rates, enabling them to exhibit dual-feature selectivity: they respond strongly to preferred features while being systematically suppressed by distinct non-preferred features. We found this bidirectional selectivity not only in primate V1 and V4 but also in mouse V1 as well as lateral visual areas. In addition, we showed that the response function of these neurons reflects a continuum between the preferred and non-preferred features. That is, dual-feature selective neurons exhibit graded responses that vary continuously with perceptual stimulus similarity to their preferred and non-preferred features. By lever-aging both excitatory and suppressive selectivity in single cells, this strategy may increase representational capacity by reducing the number of neurons that would be required to encode the equivalent number of features with unidirectional neurons (see section “Balancing sparsity and capacity? A hypothesized role for dual-feature selectivity” in Discussion).</p><p id="P7">Our results, particularly the observation that non-sparse neurons in visual cortex exhibit feature-selective suppression, align with recent connectomic studies in mice and flies showing specific inhibitory connectivity, where distinct interneuron types target defined excitatory populations (<xref ref-type="bibr" rid="R23">Schneider-Mizell et al., 2025</xref>; <xref ref-type="bibr" rid="R24">Matsliah et al., 2024</xref>). This contrasts with blanket inhibition, in which inhibitory neurons provide non-selective input that broadly suppresses local excitatory neurons regardless of their feature selectivity (e.g. <xref ref-type="bibr" rid="R14">Heeger, 1992</xref>). Such structured inhibitory connectivity may support feature-specific normalization (discussed in <xref ref-type="bibr" rid="R25">Sebastian Seung, 2024</xref>), suggesting that dual-feature selectivity may reflect a general principle by which inhibitory circuits shape precise and interpretable neuronal codes.</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>A continuum of response sparseness in macaque V1 and V4</title><p id="P8">We recorded spiking activity from macaque areas V1 and V4 using linear silicon probes while animals viewed a diverse set of naturalistic images. To align the visual stimulus with the recorded neurons’ receptive fields, we first mapped the population receptive field using sparse noise stimuli. For V1 recordings, the monkey maintained central fixation and we positioned grayscale stimuli (6.7 × 6.7 degrees of visual angle) such that we obtained full receptive field coverage (<xref ref-type="fig" rid="F1">Fig. 1a</xref> left). For V4, we located the fixation dot that centered the population receptive field on the display (<xref ref-type="fig" rid="F1">Fig. 1a</xref> right) and showed full-field RGB images (30 × 16.8 degrees of visual angle). Monkeys were trained to maintain fixation during 2.1-second trials while we presented 15 images per trial. Each session included between 7,500 and 15,000 unique naturalistic images, as well as a smaller set of repeated images to assess response reliability. The V1 dataset was obtained from a previously published study (<xref ref-type="bibr" rid="R26">Cadena et al., 2023</xref>), while we collected new data from V4 for this work. After spike sorting, we analyzed data from 453 V1 neurons and 394 V4 neurons recorded across three animals.</p><p id="P9">To characterize each neuron’s stimulus-response function, we trained functional <italic>digital twin</italic> models (<xref ref-type="bibr" rid="R19">Walker et al., 2019</xref>; <xref ref-type="bibr" rid="R20">Bashivan et al., 2019</xref>; <xref ref-type="bibr" rid="R21">Franke et al., 2022</xref>; <xref ref-type="bibr" rid="R22">Willeke et al., 2023</xref>). Each model combined a shared convolutional neural network (CNN) core pretrained on image classification (<xref ref-type="bibr" rid="R27">LeCun et al., 2015</xref>) with neuron-specific readout layers (<xref ref-type="fig" rid="F1">Fig. 1b</xref>). Our modeling approach leveraged prior findings (e.g. <xref ref-type="bibr" rid="R26">Cadena et al., 2023</xref>) that different stages in the visual hierarchy align with different deep neural network layers—early layers with primary visual cortex and deeper layers with intermediate and higher-order areas. Accordingly, we used the first layer of a ConvNeXt architecture for V1 neurons, fine-tuning the convolutional core on the recorded responses and learning neuron-specific readouts (see also <xref ref-type="bibr" rid="R28">Fu et al., 2024</xref>). For V4 neurons, we employed a pretrained ResNet50 model as a fixed core and trained a linear readout for each neuron on top of layer 3 of this representation (<xref ref-type="bibr" rid="R22">Willeke et al., 2023</xref>).</p><p id="P10">Model performance was evaluated on test images not used during training, using the correlation between predicted and observed responses averaged across stimulus repetitions (<xref ref-type="fig" rid="F1">Fig. 1c</xref>). For further analysis, we included only neurons with correlation-to-average above 0.4, yielding high-confidence <italic>digital twins</italic> for 84% of V1 neurons (<italic>n</italic> = 443) and 52% of V4 neurons (<italic>n</italic> = 205). These models enabled us to probe neuronal selectivity across a stimulus space orders of magnitude larger than possible in vivo, where recording time limits the number of stimuli that can be presented per neuron.</p><p id="P11">To assess response selectivity across natural images, we quantified each neuron’s lifetime sparsity (<xref ref-type="bibr" rid="R12">Willmore and Tolhurst, 2001</xref>), which measures how selectively a neuron responds—indicating whether it is activated by many stimuli or only by a few. Using the <italic>digital twin</italic> models, we predicted responses to 1.2 million natural images, generated activation curves by sorting the predicted responses in ascending order, and computed their skewness (<xref ref-type="fig" rid="F2">Fig. 2a,b</xref>). Higher skewness reflects one-sided asymmetry in the data and thus indicates sparser response profiles with few stimuli eliciting strong activity.</p><p id="P12">This analysis revealed that neurons in both V1 and V4 exhibit activation curves spanning a continuum from highly sparse to non-sparse (<xref ref-type="fig" rid="F2">Fig. 2d</xref>). These skewness values obtained from model predictions were validated through strong correlations with skewness values computed from in vivo recorded responses to repeated test images (<xref ref-type="fig" rid="F2">Fig. 2c</xref>), confirming our <italic>digital twins</italic> accurately capture in vivo stimulus selectivity. Interestingly, lifetime sparsity distributions were similar across V1 and V4 (<xref ref-type="fig" rid="F2">Fig. 2d</xref>), with both areas containing substantial proportions of non-sparse neurons (see also <xref ref-type="bibr" rid="R13">Willmore et al., 2011</xref>; <xref ref-type="bibr" rid="R29">Rust and DiCarlo, 2012</xref>).</p><p id="P13">To facilitate our investigations, we classified neurons as non-sparse when their model-derived skewness fell below 2.0 (<xref ref-type="fig" rid="F2">Fig. 2d</xref>). Though inherently arbitrary, this threshold separates neurons with graded responses to most images from sparse neurons that respond strongly to only a small subset of stimuli (<xref ref-type="fig" rid="F2">Fig. 2d</xref>).</p></sec><sec id="S4"><title>Identification of most and least activating stimuli of non-sparse macaque V1 and V4 neurons</title><p id="P14">Traditional approaches to visualizing neuronal selectivity have often focused on the most activating stimuli, providing valuable insights into preferred features. Examining weak or suppressive responses, however, can reveal additional aspects of selectivity, particularly in non-sparse neurons that respond in a graded fashion to most stimuli (see section “The role of suppression in visual cortical tuning: relating our findings to existing work” in Discussion). To test whether such weak responses exhibit systematic structure, we characterized both ends of the activation spectrum by identifying stimuli that maximally and minimally activated each neuron using two complementary methods: gradient-based image synthesis and large-scale image screening. Because weak responses are most informative in neurons with graded activity, the following analyses focused specifically on non-sparse neurons, unless noted otherwise.</p><p id="P15">Based on previous work (<xref ref-type="bibr" rid="R19">Walker et al., 2019</xref>; <xref ref-type="bibr" rid="R20">Bashivan et al., 2019</xref>; <xref ref-type="bibr" rid="R22">Willeke et al., 2023</xref>), the image synthesis approach used gradient ascent in the <italic>digital twin</italic> models to generate images that maximized model-predicted activity. Here, we extended this approach to also minimize neuronal responses. These synthetic stimuli—termed most exciting inputs (MEIs) and least exciting inputs (LEIs)—emerged through iterative modification of noise images to achieve the desired neuronal responses (<xref ref-type="fig" rid="F3">Fig. 3a</xref>). The screening approach computed model activations over more than one million naturalistic images, ranking responses to identify the most activating images (MAIs) and least activating images (LAIs) for each neuron (<xref ref-type="fig" rid="F3">Fig. 3b</xref>). Crucially, all images—both synthesized and screened—were normalized to identical <italic>ℓ</italic><sub><sub>2</sub></sub> norms within each neuron’s receptive field, ensuring that response differences reflected feature selectivity rather than contrast variations.</p><p id="P16">In V1, this approach revealed clear structure at both ends of the response spectrum. MEIs and MAIs consistently featured oriented edges and grating-like stimuli spanning various spatial frequencies (<xref ref-type="fig" rid="F4">Fig. 4</xref>, <xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. 1</xref>), confirming well-established tuning properties of macaque V1 (e.g. <xref ref-type="bibr" rid="R7">Hubel and Wiesel, 1962</xref>; <xref ref-type="bibr" rid="R10">Schiller et al., 1976</xref>; <xref ref-type="bibr" rid="R28">Fu et al., 2024</xref>). Remarkably, LEIs and LAIs exhibited systematic, feature-specific structure: suppression arose not only from orthogonal orientations but also from shifts in orientation, spatial frequency, phase, and texture structure, revealing inhibitory axes that extend beyond classic cross-orientation inhibition (e.g. <xref ref-type="bibr" rid="R16">Morrone et al., 1982</xref>, and see Discussion). As a control analysis, we simulated idealized simple and complex cells and screened for their least and most activating images (<xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. 3</xref>). Simple cells showed phase-shifted versions of preferred stimuli as their least-activating images, while complex cells exhibited no coherent patterns in their lowactivation regime, with diverse unrelated stimuli eliciting uniformly weak responses—reflecting their pooling mechanisms and nonlinear characteristics. Therefore, the empirical structure we observed—where least activating images differ from the most activating along specific features other than phase—cannot be accounted for by classical simple or complex cell models.</p><p id="P17">Non-sparse V4 neurons showed similarly structured selectivity at the activation extremes, but for more complex features. Aligning with previous work, their MEIs and MAIs revealed elaborate patterns—including curved contours, textured surfaces, and distinct color combinations (e.g. <xref ref-type="bibr" rid="R22">Willeke et al., 2023</xref>; <xref ref-type="bibr" rid="R30">Desimone and Schein, 1987</xref>; <xref ref-type="bibr" rid="R31">Pasupathy and Connor, 2002</xref>; <xref ref-type="bibr" rid="R32">Yamane et al., 2008</xref>)—as well as novel motifs such as eye-like configurations and branching structures (<xref ref-type="fig" rid="F5">Fig. 5</xref>, <xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. 2</xref>). Their corresponding LEIs and LAIs depicted equally coherent feature configurations—alternative contour arrangements, different color combinations, or contrasting texture patterns that consistently suppressed neuronal responses.</p><p id="P18">In addition, visual inspection revealed that, in both V1 and V4, the images that most strongly activated a given neuron tended to be perceptually similar to one another, as did the images that elicited the weakest responses. For example, for many V1 neurons, the set of MAIs often shared the same edge orientation but differed in position within the receptive field—consistent with phase invariance in complex cells. In several V4 neurons, the MAIs typically preserved a common global texture or shape, while varying in attributes such as texture phase or color.</p><p id="P19">To quantify this, we examined the organization of MAIs and LAIs within a perceptual similarity space. We used DreamSim, a model of perceptual similarity fine-tuned to align with human visual judgments (<xref ref-type="bibr" rid="R33">Fu et al., 2023a</xref>), and embedded all naturalistic images in this high-dimensional space. For each neuron, we assessed the internal coherence of its preferred (MAIs) and non-preferred (LAIs) image sets by calculating pairwise cosine similarities within the MAIs and separately within the LAIs. We used representations from the penultimate layer of DreamSim, where distances are aligned with human similarity judgments (<xref ref-type="bibr" rid="R33">Fu et al., 2023a</xref>). As a baseline, we computed the similarity of the MAIs and the LAIs to randomly selected sets of naturalistic images (<xref ref-type="fig" rid="F6">Fig. 6a</xref>). We found that, for each neuron, the MAIs were more similar to one another than to random images, and the same held for the LAIs (<xref ref-type="fig" rid="F6">Fig. 6b</xref>), resulting in significantly positive d-prime values (<xref ref-type="fig" rid="F6">Fig. 6c</xref>)—a measure of how well each image set could be discriminated from random images. Notably, d-prime values were similar for MAIs and LAIs, demonstrating that low-activating stimuli were just as structured and perceptually coherent as high-activating ones.</p></sec><sec id="S5"><title>Verification of most and least activating images of V1 and V4 neurons</title><p id="P20">Having identified stimuli predicted to generate maximal and minimal neuronal responses, we next validated these model predictions through multiple approaches. Previous studies have confirmed that model-predicted MEIs reliably drive strong responses in both mouse and macaque visual cortex in vivo (<xref ref-type="bibr" rid="R19">Walker et al., 2019</xref>; <xref ref-type="bibr" rid="R20">Bashivan et al., 2019</xref>; <xref ref-type="bibr" rid="R22">Willeke et al., 2023</xref>; <xref ref-type="bibr" rid="R28">Fu et al., 2024</xref>). In contrast, the question of whether predicted least-activating stimuli suppress neuronal activity has not yet been tested experimentally.</p><p id="P21">For each neuron, we identified the single test-set image predicted by the model to produce the highest response, as well as the one predicted to produce the lowest response, then examined where these images ranked within the neuron’s recorded response distribution over the test set (<xref ref-type="fig" rid="F7">Fig. 7</xref>). For non-sparse V1 and V4 neurons, our models accurately identified both extremes: the most activating image predicted by the model corresponded to a high recorded response, while the predicted least activating image aligned with a consistently low recorded response (<xref ref-type="fig" rid="F7">Fig. 7a-c</xref> top panels). In contrast, for sparse V1 and V4 neurons, our models reliably predicted the strongest response but performed poorly in identifying weakly activating stimuli (<xref ref-type="fig" rid="F7">Fig. 7a-c</xref> bottom panels). Because sparse neurons exhibit near-baseline responses for most stimuli, the bottom tail of the response distribution has minimal dynamic range. As a result, the model cannot accurately predict a low-activating stimulus, and the actual recorded responses to the model’s predicted least-activating images are broadly distributed across the response range, yielding a nearly uniform distribution. These results confirm that, for non-sparse neurons, the <italic>digitial twin</italic> models capture meaningful stimulus selectivity across low and high neuronal activity ranges recorded in vivo.</p><p id="P22">We further verified that the identified most- and least-activating images are not artifacts of specific model implementations. To test this, we evaluated both optimized (i.e. MEIs and LEIs) and screened stimuli (i.e. MAIs and LAIs) using independently trained models with different initializations or architectures. For V1 neurons, we trained an evaluator model with the same ConvNeXt architecture as the original generator model but initialized with independent weights (<xref ref-type="fig" rid="F8">Fig. 8a</xref>). The core was fine-tuned on the same dataset, and neuron-specific readouts were trained from scratch. For V4 neurons, we used a completely different architecture: an attention-based convolutional network trained end-to-end to predict neuronal responses (<xref ref-type="bibr" rid="R34">Pierzchlewicz et al., 2023</xref>) (<xref ref-type="fig" rid="F8">Fig. 8d</xref>).</p><p id="P23">Each evaluator model assessed the MEIs, LEIs, MAIs, and LAIs identified by the generator models. Responses were contextualized against a reference set of 200, 000 naturalistic images that were masked to each neuron’s receptive field and contrast-matched to optimized and screened images. For each neuron, we computed response percentiles—reflecting the proportion of reference images eliciting weaker or stronger model-predicted responses than the identified least- or most-activating images (<xref ref-type="fig" rid="F8">Fig. 8b,e</xref>, left panels). Across both visual areas, MEIs and MAIs consistently ranked in the top percentiles, while LEIs and LAIs fell reliably in the lowest percentiles (<xref ref-type="fig" rid="F8">Fig. 8b,e</xref>, right panels).</p><p id="P24">These results confirm that for non-sparse neurons, <italic>digital twin</italic> models accurately predict both most- and least-activating images, with stimulus selectivity being robust across independently trained models and architectures, indicating they reflect true neuronal selectivity.</p></sec><sec id="S6"><title>Feature-selective excitation and suppression shape non-sparse neuronal responses</title><p id="P25">Our results established that both the most and least activating stimuli of non-sparse macaque V1 and V4 neurons reflect structured selectivity, extending beyond suppression by orthogonal orientations previously described in macaque V1 (<xref ref-type="bibr" rid="R9">Bishop et al., 1973</xref>; <xref ref-type="bibr" rid="R16">Morrone et al., 1982</xref>; <xref ref-type="bibr" rid="R18">Ferster, 1986</xref>; <xref ref-type="bibr" rid="R35">Priebe, 2016</xref>). We next examined potential neuronal mechanisms underlying this behavior. We hypothesized that non-sparse neurons would exhibit elevated baseline firing rates, enabling modulation through both excitation and inhibition, while sparse neurons—responding strongly only to small stimulus subsets—should have near-zero baseline activity.</p><p id="P26">To test this, we quantified each neuron’s baseline firing rate during the 300 ms fixation window immediately preceding stimulus onset, when a uniform gray screen was presented. We then related these baseline rates to two response measures: the median response to natural images predicted by our model, and the skewness of these predicted activation distributions. In both V1 and V4, baseline firing rates positively correlated with median predicted activity (<xref ref-type="fig" rid="F9">Fig. 9a</xref>), and negatively correlated with response skewness (<xref ref-type="fig" rid="F9">Fig. 9b</xref>). In other words, this showed that non-sparse neurons (i.e. low skewness) had a higher probability of exhibited higher spontaneous firing rates—a relationship evident across populations in both cortical areas. Despite this overall trend, some neurons with low baseline activity exhibited high response skewness, indicating non-sparse activity during image presentation. This likely reflects the prevalence of their preferred features in natural scenes—for example, neurons tuned to low spatial frequencies may respond broadly despite low spontaneous firing rates.</p><p id="P27">These results suggest that non-sparse neurons, which tended to exhibit elevated baseline firing rates, may encode information through both excitation and suppression, such that their activity scales in a graded fashion with the similarity of each stimulus to their most and least activating features. To investigate how neuronal activity varies within the high-dimensional perceptual similarity space, we first constructed a low-dimensional embedding defined by each neuron’s most and least activating features. This allowed us to visualize and quantify how responses scale with perceptual similarity to these features, using a space defined independently of neuronal activity. Specifically, we used DreamSim (<xref ref-type="bibr" rid="R33">Fu et al., 2023a</xref>) to embed 200,000 naturalistic images into a 2-dimensional similarity space for each neuron (<xref ref-type="fig" rid="F10">Fig. 10a</xref>), where each image was assigned coordinates based on its similarity to the MAI (<italic>x</italic>-axis) and LAI (<italic>y</italic>-axis; <xref ref-type="fig" rid="F10">Fig. 10b</xref>). In the following, we focus on V4 neurons because DreamSim captures mid-level visual features like color and texture, which are better represented in V4 than in V1. This alignment is important for relating DreamSim’s similarity space to neuronal activity, as both need to encode similar features. Results for macaque V1 neurons are shown in Suppl. <xref ref-type="fig" rid="F4">Fig. 4</xref>.</p><p id="P28">Visualizing predicted V4 responses across this space revealed that many non-sparse neurons exhibited smooth response gradients along the diagonal, extending from high MAI similarity / low LAI similarity to the opposite corner (<xref ref-type="fig" rid="F10">Fig. 10b</xref>). This gradient indicates that neuronal activity increased with similarity to the MAI and decreased with similarity to the LAI. In contrast, most sparse neurons exhibited less structure, with responses primarily varying with similarity to the MAI (<xref ref-type="fig" rid="F10">Fig. 10c</xref>).</p><p id="P29">We performed regression to quantify how well the 2-dimensional space explained V4 neuronal activity. The resulting explained variance <italic>R</italic><sup>2</sup> was significantly higher for non-sparse neurons (mean=0.23, std=0.12) than for sparse neurons (mean=0.13, std=0.08; <xref ref-type="fig" rid="F10">Fig. 10d</xref>), indicating that the similarity space captures a substantial portion of response variance in the non-sparse population. For V1 neurons, we observed the same pattern of higher explained variance for non-sparse compared to sparse neurons (<xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. 4</xref>), but overall the explained variance was lower than in V4, despite the <italic>digital twin</italic> model achieving higher prediction accuracy. This is likely because the DreamSim space is not well aligned with low-level features represented in V1 neurons.</p><p id="P30">When we repeated the analysis for V4 neurons using randomly selected images instead of MAIs and LAIs (<xref ref-type="fig" rid="F10">Fig. 10e</xref>), the <italic>R</italic><sup>2</sup> values were significantly reduced to 0.04 and 0.02 for non-sparse and sparse neurons, respectively (<xref ref-type="fig" rid="F10">Fig. 10f</xref>). This confirms that the observed response gradients depend specifically on the most and least activating images, rather than arising from properties of the similarity space per se. Additionally, for non-sparse neurons, replacing either the MAI or the LAI with random images significantly reduced predictive performance, indicating that both extremes contributed meaningfully to response variation. In contrast, for sparse neurons, only the most activating images carried predictive value; responses remained largely unchanged when the LAIs were replaced by random images.</p><p id="P31">Together, these findings suggest that the responses of non-sparse neurons are shaped by similarity to both preferred and distinct non-preferred features. Despite the complexity of natural stimuli, V4 responses could be well approximated within a low-dimensional subspace defined by these two feature types, with both contributing meaningfully to response variation.</p></sec><sec id="S7"><title>A. Shared feature selectivity across the neuronal population</title><p id="P32">We found that, across neurons within a given visual area, the features that most strongly activate one neuron can resemble those that least activate another (<xref ref-type="fig" rid="F11">Fig. 11a</xref>). Additionally, stimuli that most strongly activate one neuron can also strongly activate others, even when their least-activating images differ (<xref ref-type="fig" rid="F11">Fig. 11c</xref>). These patterns suggest that neurons across the population exhibit shared feature selectivity—that is, they are tuned to a common set of features in perceptual space, which can excite some neurons while suppressing others.</p><p id="P33">To test this prediction, we examined how each neuron’s MAIs and LAIs affected the entire population within the same cortical area (<xref ref-type="fig" rid="F11">Fig. 11b,d</xref>). MAIs and LAIs of one neuron showed high probabilities of driving either strong or weak responses in other neurons, while intermediate responses were less common. MAIs elicited right-skewed distributions—with many neurons strongly activated, relatively few weakly activated. LAIs produced bimodal distributions with increased likelihood of both suppression and excitation across the population. This contrasted sharply with randomly selected control images that evoked uniform distributions. This organization extended across individual animals: MAIs and LAIs identified from neurons recorded in one monkey elicited similar activation patterns in neurons recorded from another monkey <xref ref-type="fig" rid="F11">Fig. 11e-g</xref>).</p><p id="P34">These results support the hypothesis that visual stimuli driving strong or weak responses in individual neurons also modulate responses across the population—exciting some neurons while suppressing others. This pattern reflects a population-level organization of dual-feature selectivity that generalizes across animals.</p></sec><sec id="S8"><title>Dual-feature selectivity is present in the mouse visual cortex</title><p id="P35">To assess whether dual-feature selectivity constitutes a general principle of visual coding across mammals, we extended our analyses to mouse visual cortex. Using Neuropixels probes, we recorded spiking activity from neurons in V1 and two lateral visual areas (lateromedial area (LM) and laterointermediate area (LI)) while head-fixed mice viewed grayscale natural images (<xref ref-type="fig" rid="F12">Fig.12a</xref>). Prior work has shown that LM and LI share functional similarities with the ventral stream in primates (<xref ref-type="bibr" rid="R36">Wang et al., 2012</xref>) and are involved in object representation in mice (<xref ref-type="bibr" rid="R37">Froudarakis et al., 2021</xref>). Using the recorded data, we trained a <italic>digital twin</italic> model using the Sensorium competition model architecture (<xref ref-type="bibr" rid="R38">Willeke et al., 2022</xref>): specifically, a convolutional core shared across neurons combined with neuron-specific readouts, trained end-to-end to predict neuronal responses to natural images (<xref ref-type="fig" rid="F12">Fig.12b</xref>). For further analysis, we only used neurons with a correlation between pre-dicted responses and mean test image responses larger than 0.4.</p><p id="P36">Consistent with the macaque data, neurons across all three mouse visual areas exhibited a continuum of life-time sparsity with substantial non-sparse populations, as measured by the skewness of their predicted responses to 200,000 naturalistic images (<xref ref-type="fig" rid="F12">Fig.12c,d</xref>). The images were masked around the population receptive field and contrast-matched to ensure consistent contrast across images. The skewness of the predicted responses correlated strongly with the skewness of the recorded test responses (<xref ref-type="fig" rid="F12">Fig.12e-g</xref>). Importantly, skewness was negatively correlated with baseline firing rate during gray screen presentation (<xref ref-type="fig" rid="F12">Fig.12h</xref>), suggesting that neurons that were non-sparse during image presentation tended to have higher spontaneous firing rates across areas.</p><p id="P37">For non-sparse neurons, we next identified LAIs and MAIs by screening a large-scale naturalistic image dataset (<xref ref-type="fig" rid="F12">Fig.12i-k</xref>). As in the monkey, both MAIs and LAIs depicted coherent features. The MAIs of single neurons were perceptually coherent, often featuring specific orientations, textures, or particular visual patterns that drove strong excitation. Similarly, the LAIs showed coherent structure, but these patterns differed systematically from the MAIs along specific dimensions, suggesting that images eliciting minimal responses occupied distinct regions of feature space.</p><p id="P38">At the population level, the same principle as in the monkey emerged: LAIs and MAIs of a given neuron were more likely to strongly or weakly activate other neurons within the population, compared to randomly selected images which exhibited an approximately uniform likelihood of eliciting responses across the response range (<xref ref-type="fig" rid="F12">Fig.12l-n</xref>).</p><p id="P39">Together, these findings demonstrate that dual-feature selectivity is present in mouse visual cortex, suggesting that integrating excitatory and suppressive selectivity within single neurons and across populations may constitute a general principle of how mammalian visual systems encode information.</p></sec></sec><sec id="S9" sec-type="discussion"><title>Discussion</title><p id="P40">Visual cortex has long been understood to represent the visual world through neurons that increase their firing rates in response to specific visual features. Here, we identify an additional organizational principle of single neuron selectivity: many neurons exhibit dual-feature selectivity, responding not only to preferred features but also showing systematic suppression to distinct non-preferred features. This bidirectional selectivity enables individual neurons to continuously encode the contrast between two specific feature combinations in high-dimensional input space. Specifically, the neuron’s firing rate is modulated in a graded fashion, with each rate reflecting the relative distance to the most and least activating stimuli, extending beyond simple feature detection. We find that this principle is present across species (macaque and mouse) and cortical areas (from primary visual cortex to higher-order regions), suggesting it may represent a common computational strategy for single neuron and population coding. These findings indicate that cortical representations integrate both specific excitation and suppression within individual neurons, potentially expanding representational capacity while maintaining interpretable single-neuron responses.</p><sec id="S10"><title>The role of suppression in visual cortical tuning: relating our findings to existing work</title><p id="P41">Inhibition plays a fundamental role in sensory processing, with extensive literature documenting its contributions to neuronal encoding (e.g. reviewed in <xref ref-type="bibr" rid="R39">Isaacson and Scanziani, 2011</xref>). Here, we focus specifically on visual cortex and suppression originating within the classical receptive field, excluding surround suppression. In primary visual cortex, a classic example is cross-orientation suppression, where a neuron’s response to its preferred orientation is markedly reduced when an orthogonal (non-preferred) grating is presented simultaneously (<xref ref-type="bibr" rid="R9">Bishop et al., 1973</xref>; <xref ref-type="bibr" rid="R16">Morrone et al., 1982</xref>; <xref ref-type="bibr" rid="R17">Allison et al., 1995</xref>; <xref ref-type="bibr" rid="R18">Ferster, 1986</xref>; <xref ref-type="bibr" rid="R35">Priebe, 2016</xref>; <xref ref-type="bibr" rid="R40">Burr et al., 1981</xref>; <xref ref-type="bibr" rid="R41">Hata et al., 1988</xref>). Theoretical studies suggest that such inhibitory mechanisms sharpen orientation tuning by suppressing responses to non-optimal stimuli (e.g. <xref ref-type="bibr" rid="R42">Ben-Yishai et al., 1995</xref>).</p><p id="P42">Our results reveal that suppression operates through more structured mechanisms than previously recognized. Using unbiased image synthesis and large-scale screening, we find that macaque V1 neurons’ least activating stimuli are not limited to orthogonal orientations. Instead, they comprise specific combinations of orientation, spatial frequency, phase, and size that systematically differ from the features driving maximal activation (<xref ref-type="fig" rid="F4">Figs. 4</xref>, <xref ref-type="fig" rid="F5">5</xref>). This aligns with some previous work demonstrating that suppression extends beyond orthogonal orientations to other non-preferred orientations (<xref ref-type="bibr" rid="R43">Ringach et al., 2002</xref>; <xref ref-type="bibr" rid="R44">DeAngelis et al., 1992</xref>; <xref ref-type="bibr" rid="R45">Burg et al., 2021</xref>) and to spatial frequencies outside a neuron’s preferred range (<xref ref-type="bibr" rid="R46">Bauman and Bonds, 1991</xref>; <xref ref-type="bibr" rid="R47">De Valois and Tootell, 1983</xref>). Our unbiased approach allows revealing the full multidimensional structure of these suppressive interactions.</p><p id="P43">Importantly, this structured suppression is conserved across species and visual cortical areas. In both mouse primary and lateral visual cortex and macaque V4, we observe similar feature-specific suppression. Our results support previous evidence for diverse suppressive mechanisms in visual cortex, including cross-orientation inhibition in V2 (<xref ref-type="bibr" rid="R48">Rowekamp and Sharpee, 2017</xref>), suppressive subfields in V4 receptive fields (<xref ref-type="bibr" rid="R49">Pollen et al., 2002</xref>), suppression by non-optimal stimuli in inferior temporal cortex (<xref ref-type="bibr" rid="R13">Willmore et al., 2011</xref>; <xref ref-type="bibr" rid="R29">Rust and DiCarlo, 2012</xref>; <xref ref-type="bibr" rid="R50">Miller et al., 1993</xref>; <xref ref-type="bibr" rid="R51">Rolls and Tovee, 1995</xref>), and biased competition among multiple stimuli within receptive fields, modulated by attention (<xref ref-type="bibr" rid="R52">Desimone and Duncan, 1995</xref>; <xref ref-type="bibr" rid="R53">Reynolds et al., 1999</xref>). Our work extends prior studies by providing a more general framework for how suppression shapes neuronal tuning across the visual hierarchy and across species. In this framework, neurons are not only excited by specific features—such as oriented edges in V1 or complex shape and texture combinations in V4—but are simultaneously suppressed by distinct, non-preferred combinations drawn from the same underlying feature space (<xref ref-type="fig" rid="F13">Fig. 13</xref>). This organization gives rise to dense and graded responses to naturalistic stimuli, with neuronal activity reflecting the similarity of each stimulus to two distinct regions within a high-dimensional image manifold—one associated with excitatory features and the other with suppressive features.</p></sec><sec id="S11"><title>Balancing sparsity and capacity? A hypothesized role for dual-feature selectivity</title><p id="P44">Dual-feature selectivity arises when neurons exhibit bidirectional modulation to two distinct stimulus features, enabling a coding regime that may balance interpretability with representational capacity. Unlike classical feature detectors, which respond sparsely to a narrow set of stimuli (<xref ref-type="bibr" rid="R2">Barlow, 1961</xref>; <xref ref-type="bibr" rid="R3">Field, 1987</xref>; <xref ref-type="bibr" rid="R4">Olshausen and Field, 1996</xref>), dual-feature selective neurons respond to both excitatory and suppressive inputs, potentially spanning a broader dynamic range and likely supporting higher information-theoretic capacity.</p><p id="P45">By producing distinct firing rates for excitatory, neutral, and suppressive stimuli, such neurons increase the diversity of their response distributions. Mutual information between stimulus and response increases when the response distribution is both diverse—i.e., has high entropy—and reliable—i.e., exhibits low variability given the same stimulus. Formally, mutual information is defined as <italic>I</italic>(<italic>x;r</italic>) = <italic>H</italic>(<italic>r</italic>) − <italic>H</italic>(<italic>r</italic> | <italic>x</italic>), where <italic>H</italic>(<italic>r</italic>) is the entropy of the response distribution and <italic>H</italic>(<italic>r</italic> | <italic>x</italic>) is the conditional entropy reflecting noise or ambiguity in the response. Neurons with bidirectional selectivity can increase <italic>H</italic>(<italic>r</italic>) by uti-lizing a broader dynamic range—responding with different firing rates to excitatory, neutral, and suppressive stimuli—resulting in a more uniform and distributed response profile. If these response patterns are reliable across repeated presentations of the same stimulus (i.e., <italic>H</italic>(<italic>r</italic> | <italic>x</italic>) remains low), then mutual information is increased. This strategy parallels mixed selectivity in higher cognitive areas, where neurons encode nonlinear combinations of features to support high-dimensional and flexible representations (<xref ref-type="bibr" rid="R54">Rigotti et al., 2013</xref>; <xref ref-type="bibr" rid="R55">Fusi et al., 2016</xref>).</p><p id="P46">The benefit of dual-feature coding, however, depends on the alignment between a neuron’s selectivity and the statistics of the input space. If excitatory and suppressive features occur frequently and independently, the neuron can fully exploit its dynamic range, distributing responses more uniformly and maximizing entropy. If the features are strongly correlated or rarely occur in natural scenes, responses become concentrated in a narrow range, limiting entropy and diminishing the coding benefit. Thus, the information-theoretic capacity of a dual-feature neuron is tightly constrained by how well its selectivity structure aligns with the distribution of stimuli it encounters.</p><p id="P47">Neuronal codes with high entropy—where individual neurons respond to many stimuli with variable firing rates—can enhance representational capacity but come with the trade-off of increased metabolic cost. This is because generating action potentials and driving postsynaptic glutamatergic currents are among the most energy-demanding processes in the brain (<xref ref-type="bibr" rid="R5">Levy and Baxter, 1996</xref>; <xref ref-type="bibr" rid="R6">Attwell and Laughlin, 2001</xref>). Yet, structured population responses may help offset this cost. We find that the same stimulus can increase firing in some neurons while suppressing others below baseline, which may reduce net activity and maintain population sparseness. The resulting response variance across the population accounts for a previous finding, where stimuli optimized to elicit high activity in one area simultaneously suppress certain neurons, thereby expanding the dynamic range (<xref ref-type="bibr" rid="R56">Tong et al., 2023</xref>).</p><p id="P48">Importantly, single neuron lifetime sparseness and population sparseness, while often correlated (e.g., <xref ref-type="bibr" rid="R57">Froudarakis et al., 2014</xref>), capture distinct coding properties. As argued by <xref ref-type="bibr" rid="R12">Willmore and Tolhurst (2001)</xref>; <xref ref-type="bibr" rid="R13">Willmore et al. (2011)</xref>, population sparseness can remain high even when individual neurons are broadly tuned—so long as different neurons are active for different stimuli. Therefore, dual-feature selective neurons with high firing rates could be organized as a population to maintain balanced population sparseness, ensuring a broad dynamic range while minimizing metabolic costs.</p></sec><sec id="S12"><title>Technical challenges and limitations</title><p id="P49">This study has several important limitations that merit careful consideration. First and foremost, our conclusions rely partially on in silico analyses employing digital twin models rather than direct experimental validation. Nevertheless, we maintain confidence in our findings for several reasons. Prior research has consistently demonstrated that digital twin approaches can successfully predict neuronal responses to novel stimuli and identify maximally exciting inputs that have been subsequently verified in vivo (<xref ref-type="bibr" rid="R19">Walker et al., 2019</xref>; <xref ref-type="bibr" rid="R20">Bashivan et al., 2019</xref>; <xref ref-type="bibr" rid="R21">Franke et al., 2022</xref>; <xref ref-type="bibr" rid="R22">Willeke et al., 2023</xref>; <xref ref-type="bibr" rid="R28">Fu et al., 2024</xref>; <xref ref-type="bibr" rid="R56">Tong et al., 2023</xref>). Moreover, we restricted our analyses to neurons exhibiting high predictive accuracy on held-out test data, thereby ensuring our models faithfully captured neuronal response functions. Through validation using recorded responses (cf. <xref ref-type="fig" rid="F7">Fig. 7</xref>, we confirmed that model accuracy remained consistent across both high and low neuronal activity regimes—though this held true exclusively for non-sparse neurons. Additionally, we employed an independent evaluator model to verify that inferred selectivity patterns represented genuine neuronal properties rather than optimization artifacts (cf. <xref ref-type="fig" rid="F8">Fig. 8</xref>). The successful replication of these findings across mouse datasets further strengthens our confidence in the robustness of dual-feature selectivity across species (cf. <xref ref-type="fig" rid="F12">Fig. 12</xref>).</p><p id="P50">A second important consideration concerns the scope of our characterization. While we demonstrate dual-feature selectivity in spiking responses, our analyses do not determine the underlying circuit mechanisms, for example whether the observed suppression arises from direct inhibitory synaptic input mediated by specific interneuron subtypes. Elucidating these pathways will prove essential for understanding how feature-selective inhibition integrates into the broader cortical processing hierarchy. In this context, emerging functional connectomics datasets (e.g. <xref ref-type="bibr" rid="R58">Ding et al., 2025</xref>) promise invaluable insights by enabling researchers to bridge functional response profiles—including the dual-feature selectivity described here—with precise anatomical connectivity maps and cell-type-specific wiring motifs.</p><p id="P51">Finally, while we identify images that elicit strong activation or suppression, we have not quantified the underlying features—simple or complex—that drive these responses, for example by extracting tuning curves to latent variables such as color, shape, texture or a combination thereof. Quantifying neuronal tuning in this manner is an important next step toward understanding the relationship between the most and least activating stimuli. The geometric relationships between these stimulus types may follow several organizational principles. One possibility is that they occupy separate regions within high-dimensional feature spaces, with their relative positions reflecting systematic transformations across multiple visual dimensions. For neurons in V1, this might manifest as coordinated shifts in orientation and spatial frequency between excitatory and suppressive stimuli. For higher-order areas, the transformations could involve more complex combinations of shape, color, and texture features. For visual features with cyclic properties—such as orientation or color hue—suppressive features could be positioned at consistent angular distances from excitatory features, creating predictable patterns in circular feature spaces. Addressing these fundamental questions will require future investigations that integrate quantitative feature tuning models with the functional selectivity profiles we have established here.</p></sec><sec id="S13"><title>Normalization revisited: A role for feature-selective inhibition</title><p id="P52">Inhibitory cells in the brain exhibit a diversity of cell types comparable to that of excitatory neurons (<xref ref-type="bibr" rid="R59">Tasic et al., 2018</xref>; <xref ref-type="bibr" rid="R60">Gouwens et al., 2019</xref>; <xref ref-type="bibr" rid="R61">Yao et al., 2021</xref>), despite their much lower density (<xref ref-type="bibr" rid="R62">Keller et al., 2018</xref>). This raises the question: why is such diversity necessary? In the retina, a classic test bed for central brain research, the vast variety of inhibitory amacrine cells provides selective drive to distinct retinal ganglion cell types (e.g. <xref ref-type="bibr" rid="R63">Diamond, 2017</xref>; <xref ref-type="bibr" rid="R64">Matsumoto et al., 2025</xref>), shaping feature encoding and supporting parallel processing streams. Similarly, in the cortex, accumulating evidence indicates that inhibitory neurons target excitatory neurons with high specificity (<xref ref-type="bibr" rid="R65">Muñoz et al., 2017</xref>; <xref ref-type="bibr" rid="R66">Lu et al., 2017</xref>; <xref ref-type="bibr" rid="R67">Wu et al., 2023</xref>).</p><p id="P53">A recent millimetre-scale volumetric EM reconstruction of mouse visual cortex (<xref ref-type="bibr" rid="R23">Schneider-Mizell et al., 2025</xref>) mapped the connectivity of over 1,300 neurons, revealing that inhibitory neurons organize into motif groups with widespread target specificity. These motifs coordinate inhibition onto precise combinations of perisomatic and dendritic compartments across excitatory cell types, enabling compartment- and cell-type-specific modulation of cortical circuits far beyond broad class connectivity.</p><p id="P54">Likewise, dense connectomic reconstructions in the fly revealed that inhibitory interneurons, though few in number, constitute the majority of cell types and form highly specific, feature-selective connections to excitatory neurons (<xref ref-type="bibr" rid="R24">Matsliah et al., 2024</xref>; <xref ref-type="bibr" rid="R25">Sebastian Seung, 2024</xref>). For example, individual inhibitory cell types provide suppression to targeted single excitatory cell types at defined spatial scales, suggesting a division of labor across interneuron types. <xref ref-type="bibr" rid="R25">Sebastian Seung (2024)</xref> argues that such diversity is an inevitable consequence of implementing numerous highly specific normalization operations, paralleling the architectural principles seen in convolutional networks (see below).</p><p id="P55">Our results, together with the anatomical evidence discussed above, suggest that inhibition in sensory cortex is far more structured than traditionally assumed. Classical models treat cortical selectivity as driven by excitation to preferred features, with inhibition providing non-specific gain control via untuned normalization pools that sum across diverse feature preferences (<xref ref-type="bibr" rid="R14">Heeger, 1992</xref>; <xref ref-type="bibr" rid="R15">Carandini and Heeger, 2011</xref>).</p><p id="P56">In contrast, the dual-feature selectivity we describe may represent a biological implementation of specific rather than untuned normalization, where neurons apply suppressive filters to target specific anti-preferred features, rather than uniformly inhibiting all non-preferred inputs, thus enabling functions that extend beyond simple gain control. Indeed, <xref ref-type="bibr" rid="R68">Schwartz and Simoncelli (2001)</xref> demonstrated that non-uniform, feature-dependent normalization naturally emerges when the normalization weights are optimized to reduce statistical dependencies in natural signals, supporting the idea that such structured inhibition serves an efficient coding purpose.</p><p id="P57">Structured suppression and normalization are also integral to modern artificial neural networks, albeit through different mechanisms. For example, batch normalization and attention mechanisms modulate activity based on the relationships between inputs (e.g. <xref ref-type="bibr" rid="R69">Ioffe and Szegedy, 2015</xref>; <xref ref-type="bibr" rid="R70">Ba et al., 2016</xref>; <xref ref-type="bibr" rid="R71">Vaswani et al., 2017</xref>). In our case, suppression is a fixed property of individual neurons: each neuron encodes the input by responding to preferred features and being suppressed by non-preferred ones. In contrast, artificial networks apply input-dependent modulation that is computed dynamically across inputs, such as through attention between tokens. Despite this difference, both artificial and biological networks appear to benefit from selectively attenuating certain features to improve representational efficiency.</p><p id="P58">Overall, our findings highlight feature-selective suppression as a general and underappreciated characteristic of cortical neurons. By defining specific feature dimensions through structured excitation and suppression, inhibition may enhance the representational capacity of individual neurons while preserving structured and interpretable response profiles that support flexible downstream readout—a principle that may be shared across sensory modalities and cognitive functions.</p></sec></sec><sec id="S14" sec-type="materials | methods" specific-use="web-only"><title>Materials and Methods</title><sec id="S15"><title>Ethics and Animal Care</title><p id="P59">Data were collected from three healthy male rhesus macaques with approval from Baylor College of Medicine’s Institutional Animal Care and Use Committee (permit AN-4367). The monkeys were housed individually in a room with approximately ten other monkeys, allowing for rich social interactions on a 12-hour light/dark cycle. Regular veterinary care, balanced nutrition, and environmental enrichment were provided. All surgical procedures were performed under general anesthesia using aseptic techniques, with post-operative analgesics administered for seven days.</p></sec><sec id="S16"><title>Electrophysiological recordings</title><p id="P60">Non-chronic recordings were conducted using a 32-channel linear silicon probe (NeuroNexus V1 ×32-Edge-10mm-60-177). Custom titanium recording chambers and head posts were surgi-cally implanted. Prior to recordings, small trephinations (2 mm) were made over either (i) lateral V4, with eccentricities ranging from 1.7° to 18.3° of visual angle; or Medial V1, with eccentricities ranging from 1.4° to 3.0° of visual angle. A Narishige Microdrive (MO-97) and guide tube were used to carefully position the probes through the dura, taking care to minimize tissue compression.</p><p id="P61">Mice: Eight mice (<italic>Mus musculus</italic>: 4 male, 4 female) aged from 14 to 27 weeks were selected for experiments, with 2 females and 1 male expressing GCaMP6s in excitatory neurons via Slc17a7-Cre and Ai162 transgenic lines (stock nos. 023527 and 031562, respectively; The Jackson Laboratory) and the rest being C57BL/6J wildtype (stock no. 000664; The Jackson Laboratory). We performed acute recordings using Neuropixels probes 1.0 in awake, headfixed mice according to (<xref ref-type="bibr" rid="R72">Jun et al., 2017</xref>). In brief, animals were implanted with a headpost and habituated to the experimental setup (head fixation on a treadmill) after recovery. On the recording day, the animals were briefly anaesthetized with isoflourane and a 1mm craniotomy was made above visual cortex (approximately 2.9 mm lateral to the midline sagittal suture and anterior to the lambda suture) (<xref ref-type="bibr" rid="R57">Froudarakis et al., 2014</xref>). The animals were then transferred to the experimental setup and allowed to recover from anaesthesia. Location of probe insertion was chosen according to stereotaxic coordinates for targeting V1, LM, and LI using Pinpoint (<xref ref-type="bibr" rid="R73">Birman et al., 2023</xref>), with all penetrations ranging from 600–1100 <italic>μm</italic> on the anteroposterior axis, 2900–3500 <italic>μm</italic> on the mediolateral axis, and at an angle of 55° or 60° with respect to the ventrodorsal axis. One probe was smoothly lowered through the craniotomy to the final depth according to the trajectory planning with Pinpoint (<xref ref-type="bibr" rid="R73">Birman et al., 2023</xref>) to cover the whole cortex (covering 1800–2000 <italic>μm</italic> of the probe) and allowed to settle for approximately 20 minutes before any recording.</p></sec><sec id="S17"><title>Data collection and processing</title><p id="P62">Electrophysiological data was recorded as a broadband signal (0.5Hz-16kHz) and digitized at 24 bits. For spike sorting, the 32-channel array was divided into 14 groups of six adjacent channels. Spikes were detected when signals exceeded five times the standard deviation of noise. Principal component analysis was used for feature extraction, and a Kalman filter mixture model tracked waveform drift. Single-unit isolation was manually verified by assessing stability, refractory periods, and principal component plots.</p><p id="P63">For mice, neuronal activity recordings were made with custom-written software in LabView and then automatically spike sorted with the Kilosort3 spike sorting software (<xref ref-type="bibr" rid="R74">Pachitariu et al., 2023</xref>). Neurons automatically classified as “single units” and that showed reliable firing during visual stimuli presentation were used for the model, in total: 598 V1, 350 LM, and 126 LI neurons from 20 recording sessions.</p></sec><sec id="S18"><title>Visual stimulation</title><p id="P64">Stimuli were displayed on a 23.8” LCD monitor (100 Hz refresh rate, 1920 ×1080 resolution) positioned 100 cm from the subjects (≈ 63 pixels/degree). A camera-based eye tracking system verified that monkeys maintained fixation within ≈0.95° of a small red fixation target. After maintaining fixation for 300 ms, visual stimuli were presented. Successful fixation throughout the trial resulted in a juice reward.</p><p id="P65">For mice, natural images were presented 15 cm away from the left eye with a 23.8” LCD monitor (100 Hz refresh rate, 1920 × 1080 resolution). We positioned the monitor so that it was centered on and perpendicular to the surface of the eye at the closest point, corresponding to a visual angle of 2.2°/cm on the monitor.</p></sec><sec id="S19"><title>Receptive field mapping &amp; stimulus placement</title><p id="P66">Receptive fields were mapped at the beginning of each session using a sparse random dot stimulus. A single dot (0.12-1° in size) was displayed on a gray background, changing position and color (black or white) every 30 ms during two-second fixation trials. Multi-unit receptive field profiles were obtained through reverse correlation, and population receptive fields were estimated by fitting a 2D Gaussian to the spike-triggered average. For V1 recordings, the fixation spot was kept at the center of the screen, and grayscale natural image stimuli (6.7° in size) were centered at the mean receptive field location.</p><p id="P67">The remainder of the screen was kept gray. For V4 recordings, color natural image stimuli covered the entire screen, and the fixation spot was positioned to place the mean receptive field as close as possible to the screen’s center. Due to recording site locations, this typically placed the fixation spot near the upper left border of the screen.</p><p id="P68">For mice: Visual area segmentation was performed by mapping the reversals of the retinotopy based on the RF progression along the probe as described previously (<xref ref-type="bibr" rid="R75">Tafazoli et al., 2017</xref>).</p></sec><sec id="S20"><title>Stimulus selection</title><p id="P69">We selected 24,075 images from 964 ImageNet categories, cropped to 420 × 420 pixels with 8-bit intensity resolution. From this set, 75 images were designated as the test set, 20% of the remaining images as the validation set, and the rest (19, 200 images) as the training set. The same image sets were used for both V1 and V4 recordings, with some differences in preprocessing (see below).</p><p id="P70">For a subset of V4 experiments, the dataset was augmented with rendered scenes, resulting in an equal mix of ImageNet and rendered images in the training, validation, and test sets. This synthetic dataset of rendered 3D scenes was created using Kubric (<xref ref-type="bibr" rid="R76">Greff et al., 2022</xref>) and Blender ((<xref ref-type="bibr" rid="R77">Blender Foundation, 2024</xref>)). We first manually created 10 primitive 3D objects in Blender, including basic geometric shapes (spheres, cubes, cylinders, cones, pyramids, etc.) and simple composite forms. Each object was exported as a Wavefront OBJ file with accompanying material (MTL) and texture coordinate information to ensure consistent UV mapping across all renders. For surface textures, we utilized the Describable Textures Dataset (<xref ref-type="bibr" rid="R78">Cimpoi et al., 2014</xref>), which contains 47 texture categories with diverse visual properties ranging from regular patterns (e.g., striped, checkered) to stochastic textures (e.g., marbled, bubbly). We developed an automated rendering pipeline that generated 200, 000 unique scenes at 420 × 236 pixel resolution, matching the aspect ratio used in our V4 experimental stimuli. Each rendered scene consisted of a single 3D object with UV-mapped texture randomly sampled from the DTD dataset, placed against a background with a different randomly selected DTD texture. To ensure comprehensive sampling of the visual parameter space, we systematically varied multiple scene attributes: (1) object identity (uniformly sampled from the 10 primitive shapes), (2) object position (<italic>x, y, z</italic> coordinates sampled within the camera frustum), (3) orientation (random rotation quaternions), (4) scale, and (5) lighting conditions (directional light with varying intensity and angle). Shadow rendering was enabled to introduce natural occlusion patterns and enhance depth cues. This systematic variation ensured that our synthetic dataset captured a broad range of feature combinations while maintaining precise control over individual visual attributes.</p></sec><sec id="S21"><title>Stimulus presentation</title><p id="P71">Each trial involved 2.1 seconds of continuous fixation, including 300 ms of gray screen at the beginning and 15 consecutive images displayed for 120 ms each without gaps. Test images were repeated 20-50 times throughout the session, while training and validation images were shown only once. The animal completed up to 1400 trials per day, resulting in up to 20000 stimulusresponse pairs per neuron.</p><p id="P72">For V1 recordings, grayscale images were displayed at their original resolution covering a 6.7° visual angle, with the fixation spot at the center of the screen and the images centered at the mean receptive field location. The rest of the screen remained gray. Neural responses were analyzed within a 40-160 ms window following stimulus onset. In contrast, for V4 recordings, full-color images were upscaled to match the screen width while maintaining their aspect ratio, with upper and lower bands cropped to fill the entire screen. The fixation spot was positioned to place the mean receptive field as close as possible to the screen’s center, typically near the upper left border due to recording site locations. Spike counts for V4 were collected within a 60-160 ms window after stimulus onset.</p><p id="P73">For mice, 5, 100 natural images from ImageNet (ILSVRC2012) were cropped to fit a 16 : 9 monitor aspect ratio and converted to gray scale. To collect data for training a predictive model of the brain, we showed 5,000 unique images as well as 100 additional images repeated 10 times each. This set of 100 images were shown in every recordings for evaluating cell response reliability within and between recordings. Each image was presented on the monitor for 500 ms followed by a blank screen lasting between 100 and 200 ms, sampled uniformly.</p></sec><sec id="S22"><title>Image pre-processing</title><p id="P74">We employed two distinct image processing pipelines to prepare stimuli for model training and evaluation. In the V4 pipeline, starting with original images of 420 × 420 pixels at a resolution of 14 px/°, we cropped the upper and lower bands to fit the full screen for presentation, resulting in images of 420 236 pixels. We then extracted only the bottom center 200 × 200 pixel region because of the location of the receptive fields (RFs) and subsequently downsampled these images to 100 × 100 pixels (corresponding to either 5.8 px/° or 7 px/° for model training. For the V1 approach, we cropped the central 2.65° (167 pixels) of the original 420 × 420 image at its resolution of 63 px/° and applied bicubic interpolation to downsampled to 93 × 93.</p></sec><sec id="S23"><title>Model architectures &amp; training</title><p id="P75">For macaque V1 data, following <xref ref-type="bibr" rid="R28">Fu et al. (2024)</xref>, we used a ConvNext-v2-tiny (<xref ref-type="bibr" rid="R79">Woo et al., 2023</xref>) architecture as the core, with original weights from the huggingface transformers library. After hyperparameter search, we selected the <italic>stages-1-layers-0</italic> as the optimal output layer. We applied a Gaussian readout approach (<xref ref-type="bibr" rid="R80">Lurz et al., 2022</xref>) to transform the core feature maps into neuronal responses. This readout learns the coordinates of each neuron’s receptive field center on the feature maps and implements a 2D isotropic Gaussian distribution to extract features from this location. During training, the readout samples positions according to this distribution, gradually focusing on the optimal receptive field location, while at inference time it uses the learned fixed positions. The extracted features were then processed through a neuron-specific affine projection with ELU non-linearity to predict the scalar neuronal activity.</p><p id="P76">For V1 model training, following (<xref ref-type="bibr" rid="R28">Fu et al., 2024</xref>), we minimized the Poisson loss between recorded and predicted neuronal activity. We first trained the readout for 20 epochs with frozen core weights, then reduced the learning rate from 0.001 to 0.0001 and optimized both the ConvNext core and readout weights using the AdamW optimizer (<xref ref-type="bibr" rid="R81">Loshchilov and Hutter, 2017</xref>) for 200 epochs. We trained an ensemble of five models with different random seeds and used their averaged predictions for all analyses.</p><p id="P77">Our neural predictive model of primate V4 consisted of a pretrained <italic>core</italic> computing nonlinear features from input images, and a <italic>Gaussian readout</italic> (<xref ref-type="bibr" rid="R80">Lurz et al., 2022</xref>) mapping these features to single neuron responses. Following (<xref ref-type="bibr" rid="R22">Willeke et al., 2023</xref>), we used an adversarially trained ResNet50 (<xref ref-type="bibr" rid="R82">Salman et al., 2019</xref>) as the core, with the first residual block of layer 3 (<monospace>layer3.0</monospace>) providing the feature maps, as this configuration yielded the highest pre-dictive performance. The parameters of this pretrained network remained fixed during training. After batch normalization and ReLU activation, we obtained a nonlinear feature space shared across all neurons. For each V4 neuron, the Gaussian readout learned the receptive field center position on the output tensor to extract a feature vector. The readout implemented a 2D isotropic Gaussian distribution, sampling locations during training and using fixed positions during inference. The extracted features were then processed through a linear-nonlinear model with <italic>L</italic><sub><sub>1</sub></sub>-regularized weights and an ELU+1 nonlinearity (<xref ref-type="bibr" rid="R83">Clevert et al., 2015</xref>) to ensure positive responses.</p><p id="P78">For the V4 model training, as in (<xref ref-type="bibr" rid="R22">Willeke et al., 2023</xref>), we minimized the summed Poisson loss across neurons between observed and predicted spike counts, with added <italic>L</italic><sub><sub>1</sub></sub> regularization on the readout parameters. During training, we zeroed gradients for neurons not shown a particular image. We used a batch size of 64, the Adam optimizer (<xref ref-type="bibr" rid="R84">Kingma and Ba, 2014</xref>) with an initial learning rate of 3 10<sup>−4</sup> and momentum of 0.1. We implemented early stopping based on validation loss, decaying the learning rate by 0.3 after five epochs without improvement, and stopping after four such decay steps. We trained an ensemble of five models with different random seeds and used their averaged predictions for all analyses.</p><p id="P79">For the mouse model, we trained a digital twin model using the Sensorium competition model architecture (<xref ref-type="bibr" rid="R38">Willeke et al., 2022</xref>) with neural data recorded across V1 (<italic>n</italic> = 598 neurons), LM (<italic>n</italic> = 350 neurons), and LI (<italic>n</italic> = 126 neurons). Specifically, a convolutional core shared across all neurons from all areas combined with neuron-specific readouts, trained end-to-end to predict neuronal responses to natural images.</p></sec><sec id="S24"><title>Model evaluation</title><p id="P80">To evaluate both V1 and V4 models, we measured performance using correlation to average (<xref ref-type="bibr" rid="R21">Franke et al., 2022</xref>; <xref ref-type="bibr" rid="R26">Cadena et al., 2023</xref>; <xref ref-type="bibr" rid="R38">Willeke et al., 2022</xref>) on held-out test images. This metric computes the correlation between model predictions and the average neuronal responses across repeated presentations of the same stimuli. By comparing predictions to trialaveraged responses rather than single-trial responses, this approach focuses on how well the models capture the stimulus-driven component of neural activity while accounting for biological trial-to-trial variability. Following (<xref ref-type="bibr" rid="R28">Fu et al., 2024</xref>) for V1 and (<xref ref-type="bibr" rid="R22">Willeke et al., 2023</xref>) for V4, we applied this evaluation consistently across both areas to enable fair comparison of model performance. Models trained exclusively on ImageNet generalized well to rendered images, showing no significant differences in prediction performance (data not shown). Consequently, we refer to both datasets collectively as ‘naturalistic images’ in the Results section. Detailed information regarding which dataset was used for each analysis is provided in the figure legends and the Methods section below.</p></sec><sec id="S25"><title>Identification of most and least activating images</title><p id="P81">To identify naturalistic images that elicited extreme responses from modeled neurons, we conducted a large-scale screening across two complementary sources: 200,000 synthetically rendered scenes with controlled shape and texture variations, and 1,281,167 natural images from the ImageNet-1K training set (<xref ref-type="bibr" rid="R85">Deng et al., 2009</xref>). Prior to neuronal response prediction, all images underwent standardized preprocessing to match the experimental conditions. Images were center-masked using the mean receptive field profile computed from the population of synthesized most and least exciting images for V1 and V4 neurons, respectively. This masking procedure ensured that visual features were evaluated within the approximated retinotopic locations while controlling the influence of peripheral image regions. Additionally, we normalized all images to fixed <italic>ℓ</italic><sub><sub>2</sub></sub> norms (12.0 for V1, 40.0 for V4) to control for overall contrast differences and ensure fair comparison across diverse image content. These normalization values were empirically determined to match the typical contrast range of natural images while preventing saturation artifacts. For response prediction, we employed area-specific models: the ConvNeXt-based architecture for V1 neurons and the adversarially-trained ResNet50 for V4 neurons, as described in the model architecture section. Each model computed predicted firing rates for all neurons across both image datasets, resulting in response matrices of 200,000Ö<italic>N</italic> and 1,281,167Ö<italic>N</italic> for rendered and natural images respectively, where <italic>N</italic> represents the number of recorded neurons. From these combined predictions, ranked in ascending order per neuron, we identified the top and bottom images for each neuron, corresponding to the most and least activating images (MAIs and LAIs). Similarly for mice, screening was conducted across the 200,000 synthetically rendered scenes and images were normalized to a fixed <italic>ℓ</italic><sub><sub>2</sub></sub> norm (10.0 for V1, LM, and LI). For response prediction we employed the one model for all three areas, as described in the model architecture section. The model computed predicted firing rates for all neurons in the dataset, resulting in a response matrix of 200, 000Ö<italic>N</italic>, where <italic>N</italic> represents the total number of recorded neurons recorded across V1, LM, and LI (<italic>N</italic> = 1074).</p></sec><sec id="S26"><title>Optimization of most and least exciting images</title><p id="P82">We employed gradient-based optimization in the pixel space (for V1 neurons) or frequency (for V4 neurons) domain to generate images that optimally drove or suppressed individual neuronal responses. Starting from random noise images, we iteratively modified pixel values to either maximize (for most exciting inputs, MEIs) or minimize (for least exciting inputs, LEIs) the predicted neuronal response. For V4, this optimization operated exclusively on the phase spectrum of the Fourier transform while constraining the amplitude spectrum to match a fixed mean amplitude computed over 10,000 randomly sampled ImageNet images, ensuring that synthesized images maintained realistic spatial frequency content. For V1, the image synthesis was achieved by directly modifying the image pixel values themselves. During optimization, we applied <italic>ℓ</italic><sub><sub>2</sub></sub> norm constraints matching those used in the screening procedure (12.0 for V1, 40.0 for V4) to maintain consistent contrast levels across all analyses. To improve optimization robustness and avoid local minima, we implemented a multi-crop augmentation strategy. This operation generated 4 random crops per image at each optimization step, with crop centers sampled from a Gaussian distribution (<italic>μ</italic> = 0.5, <italic>σ</italic> = 0.15) relative to image dimensions. Importantly, we maintained a fixed box size of 1.0 throughout optimization, meaning crops spanned the full image extent with only positional jittering. This approach effectively provided the optimizer with multiple gradient estimates per iteration by evaluating slightly shifted versions of the image, similar to translation data augmentation but applied during the optimization process itself. The crop sizes included additional Gaussian noise (<italic>σ</italic> = 0.05) clamped between 0.05 and 1.0, though with our box size fixed at 1.0, this primarily introduced minor scale variations around the full image size. Each crop was resized back to the original dimensions (93 × 93 pixels for V1, 100 × 100 pixels for V4) using bilinear interpolation before neural response prediction. By averaging gradients across these multiple crops, the optimization procedure became more robust to small spatial shifts, encouraging the emergence of features that drove consistent neuronal responses. We performed 256 optimization steps using the Adam optimizer ((<xref ref-type="bibr" rid="R86">Kingma and Ba, 2017</xref>)) with a learning rate of 0.05, generating both MEIs and LEIs for each recorded neuron.</p></sec><sec id="S27"><title>Verification of LAIs &amp; MAIs and LEIs &amp; MEIs</title><p id="P83">To verify that model-identified least and most activating stimuli accurately represented neuronal preferences, we implemented two complementary validation approaches. To validate predictions against in vivo recordings, we selected the single images from held-out test sets predicted to elicit the strongest and weakest responses for each neuron. We then determined where these images ranked within each neuron’s recorded response distribution, allowing us to assess whether model predictions aligned with actual neuronal behavior.</p><p id="P84">To test generalization across models, we trained independent evaluator models. For V1 neurons, we used the same ConvNeXt architecture as our generator model but with independent initialization, fine-tuning the core network while training neuron-specific readouts from scratch. For V4 neurons, we employed an entirely different architecture—an attention-based convolutional network trained end-to-end. To evaluate stimulus effectiveness, we presented each evaluator model with the four synthetic images (MEI, LEI, MAI, LAI) derived from the generator model for each neuron. To contextualize response strength, we compared these against a reference set of 200,000 contrast- and size-matched naturalistic images.</p><p id="P85">For each synthetic image, we calculated its response per-centile, defined as the proportion of reference images evoking a weaker model-predicted response. This cross-model validation allowed us to determine whether identified stimuli represented genuine neuronal tuning properties rather than model-specific artifacts.</p></sec><sec id="S28"><title>Baseline firing rate</title><p id="P86">To extract baseline firing rates, we counted spikes during the 300 ms fixation window prior to stimulus onset. We converted these counts into firing rates (Hz) and computed the mean baseline activity across all valid trials. During this fixation period, the screen was set to mean gray level (127 in 8-bit). For mice the baseline firing rates (Hz) were calculated during the blank screen (200ms duration) before the stimulus onset.</p></sec><sec id="S29"><title>DreamSim image similarity</title><p id="P87">To test whether least and most activating rendered images (LAIs and MAIs) contain robust feature combinations and exhibit similarity within categories, we embedded all rendered images into the image similarity model DreamSim (<xref ref-type="bibr" rid="R87">Fu et al., 2023b</xref>). Dream-Sim leverages deep neural network representations to quantify perceptual image similarity in a manner that correlates with human judgments. We used the penultimate layer as an embedding for each image. For V1, we preprocessed images by masking them with the mean receptive field mask of V1 neurons, normalizing them to have the same contrast, and converting them to grayscale. For V4, we masked images with the V1 mask, normalized them, and preserved their color information. To quantify similarity, we computed pairwise cosine similarity between DreamSim vectors for the top and bottom 10 images per neuron (within-MAI and within-LAI comparisons). Additionally, we calculated pairwise distances between MAIs and LAIs, as well as distances from both MAIs and LAIs to 10 randomly selected images. This analysis resulted in cosine similarity distributions per neuron and condition. We used d-prime as a measure of discriminability, with values below 0.5 indicating low discriminability between image categories.</p></sec><sec id="S30"><title>Population-level analysis</title><p id="P88">To assess the extent to which the most activating images (MAIs) and least activating images (LAIs) of a given neuron also activate other neurons within the same area, we performed the following analysis (for both species). For each neuron, we first identified its top 15 MAIs and bottom 15 LAIs. We then evaluated how these images ranked in terms of activation for every other neuron in the population. Specifically, for each other neuron, we computed the percentile rank of each MAI and LAI within its response distribution obtained from the entire set of size- and contrast-matched naturalistic images (&gt; 1 million images). This yielded a distribution of percentile ranks for the MAIs (and LAIs) of each neuron across all other neurons. Percentile ranks were then averaged across the 15 MAIs and 15 LAIs for each neuron. As a control, we randomly sampled 15 images from the full set of naturalistic images (i.e. &gt; 1 million images) and computed their percentile ranks in the same way. Additionally, to examine whether this effect generalizes across individuals, we conducted a cross-monkey analysis. For this, we identified the MAIs and LAIs of neurons recorded in one monkey and calculated the response percentiles of these images for neurons recorded in the other monkey.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Information</label><media xlink:href="EMS207390-supplement-Supplementary_Information.pdf" mimetype="application" mime-subtype="pdf" id="d50aAcEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S31"><title>Acknowledgements</title><p>The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting PE. FHS is supported by the German Federal Ministry of Federal Ministry of Research, Technology and Space (BMFTR) via the Collaborative Research in Computational Neuroscience (CRCNS) (FKZ 01GQ2107), and FHS &amp; KF are supported by the Collaborative Research Center (SFB 1233, Robust Vision, project number: 276693517). FHS and ASE acknowledges the support of the Lower Saxony Ministry of Science and Culture (MWK) with funds from the Volkswagen Foundation’s zukunft.niedersachsen program (project name: CAIMed - Lower Saxony Center for Artificial Intelligence and Causal Methods in Medicine; grant number: ZN4257). EF acknowledges support from a European Research Council (ERC) grant (ERC-2022-STG, NEURACT, Grant agreement No: 101076710), the Hellenic Foundation for Research and Innovation (HFRI) under the 2nd Call for HFRI Research Projects to Support Faculty Members and Researchers with Grant Agreement No. 4049, and the HFRI under the “Funding of Basic Research (Horizontal support of all Sciences)” of the National Recovery and Resilience Plan “Greece 2.0” with funding from the European Union - NextGenerationEU with Grant Agreement No. 016552. MD acknowledges support from the European Union’s Horizon 2020 research and innovation program under the Marie Skłodowska-Curie Actions with Grant Agreement No. 101025482. ASE acknowledges support from the European Research Council (ERC) under the European Union’s Horizon Europe research and innovation programme (Grant agreement No. 101041669). KF acknowledges support from the European Research Council (ERC) under the European Union’s Horizon Europe research and innovation programme (Grant agreement No. 101117156). AST acknowledges support from the NIH (R01EY026927), the NSF (Collaborative Research in Computational Neuroscience, IIS-2113173), and the Defense Advanced Research Projects Agency (DARPA), Contract No. N66001-19-C-4020 and Contract No. DARPA NESD N66001-17-C-4002. The views, opinions and/or findings expressed are those of the author and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government. AST and SS acknowledge support from the Amaranth Foundation (Enigma Project).</p></ack><sec id="S32" sec-type="data-availability"><title>Data and code availability</title><p id="P89">All processed data (i.e. binned responses to natural images), models and analysis code will be made publicly available upon journal submission. Please feel free to contact us in case you would like to get access before that.</p></sec><fn-group><fn id="FN1" fn-type="con"><p id="P90"><bold>Author Contributions KF</bold> Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data Curation, Writing - Original Draft, Visualization, Project Administration, Funding Acquisition <bold>NK</bold>: Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data Curation, Writing - Original Draft, Visualization, Project Administration <bold>KW</bold>: Conceptualization, Methodology, Software, Data Curation <bold>MD</bold>: Investigation, Data Curation, Visualization, Writing - Review &amp; Editing <bold>KRa</bold>: Conceptualization, Investigation, Methodology <bold>PE</bold>: Software <bold>KRe</bold>: Methodology, Investigation, Data Curation <bold>PF</bold>: Conceptualization, Data Curation <bold>CN</bold>: Methodology, Data Curation <bold>TS</bold>: Methodology, Data Curation <bold>GG</bold>: Methodology, Data Curation <bold>SP</bold>: Software, Methodology, Validation <bold>AE</bold>: Writing - Review &amp; Editing <bold>EYW</bold>: Conceptualization, Data Curation <bold>EF</bold>: Writing - Review &amp; Editing <bold>SS</bold>: Conceptualization, Funding Acquisition, Writing - Review &amp; Editing <bold>FHS</bold>: Conceptualization, Supervision, Writing - Review &amp; Editing, Fund- ing Acquisition <bold>AT</bold>: Conceptualization, Supervision, Funding Acquisition, Writing - Review &amp; Editing</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lettvin</surname><given-names>JY</given-names></name><name><surname>Maturana</surname><given-names>HR</given-names></name><name><surname>McCulloch</surname><given-names>WS</given-names></name><name><surname>Pitts</surname><given-names>WH</given-names></name></person-group><article-title>What the frog’s eye tells the frog’s brain</article-title><source>Proceedings of the IRE</source><year>1959</year><month>November</month><volume>47</volume><issue>11</issue><fpage>1940</fpage><lpage>1951</lpage></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barlow</surname></name></person-group><article-title>Possible principles underlying the transformation of sensory messages</article-title><source>Sensory communication</source><year>1961</year></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Field</surname><given-names>DJ</given-names></name></person-group><article-title>Relations between the statistics of natural images and the response properties of cortical cells</article-title><source>J Opt Soc Am A</source><year>1987</year><month>December</month><volume>4</volume><issue>12</issue><fpage>2379</fpage><lpage>2394</lpage><pub-id pub-id-type="pmid">3430225</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname><given-names>BA</given-names></name><name><surname>Field</surname><given-names>DJ</given-names></name></person-group><article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title><source>Nature</source><year>1996</year><month>June</month><volume>381</volume><issue>6583</issue><fpage>607</fpage><lpage>609</lpage><pub-id pub-id-type="pmid">8637596</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levy</surname><given-names>WB</given-names></name><name><surname>Baxter</surname><given-names>RA</given-names></name></person-group><article-title>Energy efficient neural codes</article-title><source>Neural Comput</source><year>1996</year><month>April</month><volume>8</volume><issue>3</issue><fpage>531</fpage><lpage>543</lpage><pub-id pub-id-type="pmid">8868566</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Attwell</surname><given-names>D</given-names></name><name><surname>Laughlin</surname><given-names>SB</given-names></name></person-group><article-title>An energy budget for signaling in the grey matter of the brain</article-title><source>J Cereb Blood Flow Metab</source><year>2001</year><month>October</month><volume>21</volume><issue>10</issue><fpage>1133</fpage><lpage>1145</lpage><pub-id pub-id-type="pmid">11598490</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname><given-names>DH</given-names></name><name><surname>Wiesel</surname><given-names>TN</given-names></name></person-group><article-title>Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex</article-title><source>J Physiol</source><year>1962</year><month>January</month><volume>160</volume><fpage>106</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1962.sp006837</pub-id><pub-id pub-id-type="pmcid">PMC1359523</pub-id><pub-id pub-id-type="pmid">14449617</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname><given-names>DH</given-names></name><name><surname>Wiesel</surname><given-names>TN</given-names></name></person-group><article-title>Receptive fields and functional architecture of monkey striate cortex</article-title><source>J Physiol</source><year>1968</year><month>March</month><volume>195</volume><issue>1</issue><fpage>215</fpage><lpage>243</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1968.sp008455</pub-id><pub-id pub-id-type="pmcid">PMC1557912</pub-id><pub-id pub-id-type="pmid">4966457</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bishop</surname><given-names>PO</given-names></name><name><surname>Coombs</surname><given-names>JS</given-names></name><name><surname>Henry</surname><given-names>GH</given-names></name></person-group><article-title>Receptive fields of simple cells in the cat striate cortex</article-title><source>J Physiol</source><year>1973</year><month>May</month><volume>231</volume><issue>1</issue><fpage>31</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1973.sp010218</pub-id><pub-id pub-id-type="pmcid">PMC1350435</pub-id><pub-id pub-id-type="pmid">4715359</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schiller</surname><given-names>PH</given-names></name><name><surname>Finlay</surname><given-names>BL</given-names></name><name><surname>Volman</surname><given-names>SF</given-names></name></person-group><article-title>Quantitative studies of single-cell properties in monkey striate cortex. I. spatiotemporal organization of receptive fields</article-title><source>J Neurophysiol</source><year>1976</year><month>November</month><volume>39</volume><issue>6</issue><fpage>1288</fpage><lpage>1319</lpage><pub-id pub-id-type="pmid">825621</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quiroga</surname><given-names>RQ</given-names></name><name><surname>Reddy</surname><given-names>L</given-names></name><name><surname>Kreiman</surname><given-names>G</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Fried</surname><given-names>I</given-names></name></person-group><article-title>Invariant visual representation by single neurons in the human brain</article-title><source>Nature</source><year>2005</year><month>June</month><volume>435</volume><issue>7045</issue><fpage>1102</fpage><lpage>1107</lpage><pub-id pub-id-type="pmid">15973409</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willmore</surname><given-names>B</given-names></name><name><surname>Tolhurst</surname><given-names>DJ</given-names></name></person-group><article-title>Characterizing the sparseness of neural codes</article-title><source>Network</source><year>2001</year><month>August</month><volume>12</volume><issue>3</issue><fpage>255</fpage><lpage>270</lpage><pub-id pub-id-type="pmid">11563529</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willmore</surname><given-names>BDB</given-names></name><name><surname>Mazer</surname><given-names>JA</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><article-title>Sparse coding in striate and extrastriate visual cortex</article-title><source>J Neurophysiol</source><year>2011</year><month>June</month><volume>105</volume><issue>6</issue><fpage>2907</fpage><lpage>2919</lpage><pub-id pub-id-type="doi">10.1152/jn.00594.2010</pub-id><pub-id pub-id-type="pmcid">PMC3118756</pub-id><pub-id pub-id-type="pmid">21471391</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><article-title>Normalization of cell responses in cat striate cortex</article-title><source>Vis Neurosci</source><year>1992</year><month>August</month><volume>9</volume><issue>2</issue><fpage>181</fpage><lpage>197</lpage><pub-id pub-id-type="pmid">1504027</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><article-title>Normalization as a canonical neural computation</article-title><source>Nat Rev Neurosci</source><year>2011</year><month>November</month><volume>13</volume><issue>1</issue><fpage>51</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1038/nrn3136</pub-id><pub-id pub-id-type="pmcid">PMC3273486</pub-id><pub-id pub-id-type="pmid">22108672</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morrone</surname><given-names>MC</given-names></name><name><surname>Burr</surname><given-names>DC</given-names></name><name><surname>Maffei</surname><given-names>L</given-names></name></person-group><article-title>Functional implications of cross-orientation inhibition of cortical visual cells. I. neurophysiological evidence</article-title><source>Proc R Soc Lond B Biol Sci</source><year>1982</year><month>October</month><volume>216</volume><issue>1204</issue><fpage>335</fpage><lpage>354</lpage><pub-id pub-id-type="pmid">6129633</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allison</surname><given-names>JD</given-names></name><name><surname>Casagrande</surname><given-names>VA</given-names></name><name><surname>Bonds</surname><given-names>AB</given-names></name></person-group><article-title>The influence of input from the lower cortical layers on the orientation tuning of upper layer V1 cells in a primate</article-title><source>Vis Neurosci</source><year>1995</year><month>March</month><volume>12</volume><issue>2</issue><fpage>309</fpage><lpage>320</lpage><pub-id pub-id-type="pmid">7786852</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferster</surname><given-names>D</given-names></name></person-group><article-title>Orientation selectivity of synaptic potentials in neurons of cat primary visual cortex</article-title><source>J Neurosci</source><year>1986</year><month>May</month><volume>6</volume><issue>5</issue><fpage>1284</fpage><lpage>1301</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.06-05-01284.1986</pub-id><pub-id pub-id-type="pmcid">PMC6568575</pub-id><pub-id pub-id-type="pmid">3711980</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walker</surname><given-names>EY</given-names></name><name><surname>Sinz</surname><given-names>FH</given-names></name><name><surname>Cobos</surname><given-names>E</given-names></name><name><surname>Muhammad</surname><given-names>T</given-names></name><name><surname>Froudarakis</surname><given-names>E</given-names></name><name><surname>Fahey</surname><given-names>PG</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name><name><surname>Reimer</surname><given-names>J</given-names></name><name><surname>Pitkow</surname><given-names>X</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name></person-group><article-title>Inception loops discover what excites neurons most using deep predictive models</article-title><source>Nat Neurosci</source><year>2019</year><month>December</month><volume>22</volume><issue>12</issue><fpage>2060</fpage><lpage>2065</lpage><pub-id pub-id-type="pmid">31686023</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bashivan</surname><given-names>P</given-names></name><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><article-title>Neural population control via deep image synthesis</article-title><source>Science</source><year>2019</year><volume>364</volume><issue>6439</issue><elocation-id>eaav9436</elocation-id><pub-id pub-id-type="pmid">31048462</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franke</surname><given-names>K</given-names></name><name><surname>Willeke</surname><given-names>KF</given-names></name><name><surname>Ponder</surname><given-names>K</given-names></name><name><surname>Galdamez</surname><given-names>M</given-names></name><name><surname>Zhou</surname><given-names>N</given-names></name><name><surname>Muhammad</surname><given-names>T</given-names></name><name><surname>Patel</surname><given-names>S</given-names></name><name><surname>Froudarakis</surname><given-names>E</given-names></name><name><surname>Reimer</surname><given-names>J</given-names></name><name><surname>Sinz</surname><given-names>FH</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name></person-group><article-title>State-dependent pupil dilation rapidly shifts visual feature selectivity</article-title><source>Nature</source><year>2022</year><month>September</month><pub-id pub-id-type="doi">10.1038/s41586-022-05270-3</pub-id><pub-id pub-id-type="pmcid">PMC10635574</pub-id><pub-id pub-id-type="pmid">36171291</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willeke</surname><given-names>KF</given-names></name><name><surname>Restivo</surname><given-names>K</given-names></name><name><surname>Franke</surname><given-names>K</given-names></name><name><surname>Nix</surname><given-names>AF</given-names></name><name><surname>Cadena</surname><given-names>SA</given-names></name><name><surname>Shinn</surname><given-names>T</given-names></name><name><surname>Nealley</surname><given-names>C</given-names></name><name><surname>Rodriguez</surname><given-names>G</given-names></name><name><surname>Patel</surname><given-names>S</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name><name><surname>Sinz</surname><given-names>FH</given-names></name><etal/></person-group><article-title>Deep learning-driven characterization of single cell tuning in primate visual area V4 unveils topological organization</article-title><source>bioRxiv</source><year>2023</year><month>May</month><elocation-id>2023.05.12.540591</elocation-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneider-Mizell</surname><given-names>CM</given-names></name><name><surname>Bodor</surname><given-names>AL</given-names></name><name><surname>Brittain</surname><given-names>D</given-names></name><name><surname>Buchanan</surname><given-names>J</given-names></name><name><surname>Bumbarger</surname><given-names>DJ</given-names></name><name><surname>Elabbady</surname><given-names>L</given-names></name><name><surname>Gamlin</surname><given-names>C</given-names></name><name><surname>Kapner</surname><given-names>D</given-names></name><name><surname>Kinn</surname><given-names>S</given-names></name><name><surname>Mahalingam</surname><given-names>G</given-names></name><name><surname>Seshamani</surname><given-names>S</given-names></name><etal/></person-group><article-title>Inhibitory specificity from a connectomic census of mouse visual cortex</article-title><source>Nature</source><year>2025</year><month>April</month><volume>640</volume><issue>8058</issue><fpage>448</fpage><lpage>458</lpage><pub-id pub-id-type="doi">10.1038/s41586-024-07780-8</pub-id><pub-id pub-id-type="pmcid">PMC11981935</pub-id><pub-id pub-id-type="pmid">40205209</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matsliah</surname><given-names>A</given-names></name><name><surname>Yu</surname><given-names>S-C</given-names></name><name><surname>Kruk</surname><given-names>K</given-names></name><name><surname>Bland</surname><given-names>D</given-names></name><name><surname>Burke</surname><given-names>AT</given-names></name><name><surname>Gager</surname><given-names>J</given-names></name><name><surname>Hebditch</surname><given-names>J</given-names></name><name><surname>Silverman</surname><given-names>B</given-names></name><name><surname>Willie</surname><given-names>KP</given-names></name><name><surname>Willie</surname><given-names>R</given-names></name><name><surname>Sorek</surname><given-names>M</given-names></name><etal/></person-group><article-title>Neuronal parts list and wiring diagram for a visual system</article-title><source>Nature</source><year>2024</year><month>October</month><volume>634</volume><issue>8032</issue><fpage>166</fpage><lpage>180</lpage><pub-id pub-id-type="doi">10.1038/s41586-024-07981-1</pub-id><pub-id pub-id-type="pmcid">PMC11446827</pub-id><pub-id pub-id-type="pmid">39358525</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sebastian Seung</surname><given-names>H</given-names></name></person-group><article-title>Interneuron diversity and normalization specificity in a visual system</article-title><source>bioRxiv</source><year>2024</year><month>April</month><elocation-id>2024.04.03.587837</elocation-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cadena</surname><given-names>SA</given-names></name><name><surname>Willeke</surname><given-names>K</given-names></name><name><surname>Restivo</surname><given-names>K</given-names></name><name><surname>Denfield</surname><given-names>GH</given-names></name><name><surname>Sinz</surname><given-names>FH</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Tolias</surname><given-names>A</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name></person-group><article-title>Diverse task-driven modeling of macaque V4 reveals functional specialization towards semantic tasks</article-title><source>PLoS Comput Biol</source><year>2023</year><month>May</month><volume>20</volume><pub-id pub-id-type="doi">10.1371/journal.pcbi.1012056</pub-id><pub-id pub-id-type="pmcid">PMC11115319</pub-id><pub-id pub-id-type="pmid">38781156</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><article-title>Deep learning</article-title><source>nature</source><year>2015</year><volume>521</volume><issue>7553</issue><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="pmid">26017442</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>J</given-names></name><name><surname>Shrinivasan</surname><given-names>S</given-names></name><name><surname>Baroni</surname><given-names>L</given-names></name><name><surname>Ding</surname><given-names>Z</given-names></name><name><surname>Fahey</surname><given-names>PG</given-names></name><name><surname>Pierzchlewicz</surname><given-names>P</given-names></name><name><surname>Ponder</surname><given-names>K</given-names></name><name><surname>Froebe</surname><given-names>R</given-names></name><name><surname>Ntanavara</surname><given-names>L</given-names></name><name><surname>Muhammad</surname><given-names>T</given-names></name><name><surname>Willeke</surname><given-names>KF</given-names></name><etal/></person-group><article-title>Pattern completion and disruption characterize contextual modulation in the visual cortex</article-title><source>bioRxivorg</source><year>2024</year><month>May</month><elocation-id>2023.03.13.532473</elocation-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rust</surname><given-names>NC</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><article-title>Balanced increases in selectivity and tolerance produce constant sparseness along the ventral visual stream</article-title><source>J Neurosci</source><year>2012</year><month>July</month><volume>32</volume><issue>30</issue><fpage>10170</fpage><lpage>10182</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6125-11.2012</pub-id><pub-id pub-id-type="pmcid">PMC3485085</pub-id><pub-id pub-id-type="pmid">22836252</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desimone</surname><given-names>R</given-names></name><name><surname>Schein</surname><given-names>SJ</given-names></name></person-group><article-title>Visual properties of neurons in area V4 of the macaque: sensitivity to stimulus form</article-title><source>J Neurophysiol</source><year>1987</year><month>March</month><volume>57</volume><issue>3</issue><fpage>835</fpage><lpage>868</lpage><pub-id pub-id-type="pmid">3559704</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pasupathy</surname><given-names>A</given-names></name><name><surname>Connor</surname><given-names>CE</given-names></name></person-group><article-title>Population coding of shape in area V4</article-title><source>Nat Neurosci</source><year>2002</year><month>November</month><volume>5</volume><issue>12</issue><fpage>1332</fpage><lpage>1338</lpage><pub-id pub-id-type="pmid">12426571</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamane</surname><given-names>Y</given-names></name><name><surname>Carlson</surname><given-names>ET</given-names></name><name><surname>Bowman</surname><given-names>KC</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Connor</surname><given-names>CE</given-names></name></person-group><article-title>A neural code for three-dimensional object shape in macaque inferotemporal cortex</article-title><source>Nat Neurosci</source><year>2008</year><month>November</month><volume>11</volume><issue>11</issue><fpage>1352</fpage><lpage>1360</lpage><pub-id pub-id-type="doi">10.1038/nn.2202</pub-id><pub-id pub-id-type="pmcid">PMC2725445</pub-id><pub-id pub-id-type="pmid">18836443</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>S</given-names></name><name><surname>Tamir</surname><given-names>N</given-names></name><name><surname>Sundaram</surname><given-names>S</given-names></name><name><surname>Chai</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>R</given-names></name><name><surname>Dekel</surname><given-names>T</given-names></name><name><surname>Isola</surname><given-names>P</given-names></name></person-group><article-title>DreamSim: Learning new dimensions of human visual similarity using synthetic data</article-title><source>arXiv [csCV]</source><year>2023a</year><month>June</month></element-citation></ref><ref id="R34"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Pierzchlewicz</surname><given-names>PA</given-names></name><name><surname>Willeke</surname><given-names>KF</given-names></name><name><surname>Nix</surname><given-names>A</given-names></name><name><surname>Elumalai</surname><given-names>P</given-names></name><name><surname>Restivo</surname><given-names>K</given-names></name><name><surname>Shinn</surname><given-names>T</given-names></name><name><surname>Nealley</surname><given-names>C</given-names></name><name><surname>Rodriguez</surname><given-names>G</given-names></name><name><surname>Patel</surname><given-names>S</given-names></name><name><surname>Franke</surname><given-names>K</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name><etal/></person-group><source>Energy guided diffusion for generating neurally exciting images</source><conf-name>Thirty-seventh Conference on Neural Information Processing Systems</conf-name><year>2023</year></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Priebe</surname><given-names>NJ</given-names></name></person-group><article-title>Mechanisms of orientation selectivity in the primary visual cortex</article-title><source>Annu Rev Vis Sci</source><year>2016</year><month>October</month><volume>2</volume><fpage>85</fpage><lpage>107</lpage><comment>2(Volume 2, 2016)</comment><pub-id pub-id-type="pmid">28532362</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name><name><surname>Burkhalter</surname><given-names>A</given-names></name></person-group><article-title>Network analysis of corticocortical connections reveals ventral and dorsal processing streams in mouse visual cortex</article-title><source>Journal of Neuroscience</source><year>2012</year><volume>32</volume><issue>13</issue><fpage>4386</fpage><lpage>4399</lpage><comment>ISSN 0270-6474</comment><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6063-11.2012</pub-id><pub-id pub-id-type="pmcid">PMC3328193</pub-id><pub-id pub-id-type="pmid">22457489</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Froudarakis</surname><given-names>E</given-names></name><name><surname>Cohen</surname><given-names>U</given-names></name><name><surname>Diamantaki</surname><given-names>M</given-names></name><name><surname>Walker</surname><given-names>EY</given-names></name><name><surname>Reimer</surname><given-names>J</given-names></name><name><surname>Berens</surname><given-names>P</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name></person-group><article-title>Object manifold geometry across the mouse cortical visual hierarchy</article-title><source>bioRxiv</source><year>2021</year><pub-id pub-id-type="doi">10.1101/2020.08.20.258798</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willeke</surname><given-names>KF</given-names></name><name><surname>Fahey</surname><given-names>PG</given-names></name><name><surname>Bashiri</surname><given-names>M</given-names></name><name><surname>Pede</surname><given-names>L</given-names></name><name><surname>Burg</surname><given-names>MF</given-names></name><name><surname>Blessing</surname><given-names>C</given-names></name><name><surname>Cadena</surname><given-names>SA</given-names></name><name><surname>Ding</surname><given-names>Z</given-names></name><name><surname>Lurz</surname><given-names>K-K</given-names></name><name><surname>Ponder</surname><given-names>K</given-names></name><name><surname>Muhammad</surname><given-names>T</given-names></name><etal/></person-group><article-title>The sensorium competition on predicting large-scale mouse primary visual cortex activity</article-title><source>arXiv [q-bioNC]</source><year>2022</year><month>June</month></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isaacson</surname><given-names>JS</given-names></name><name><surname>Scanziani</surname><given-names>M</given-names></name></person-group><article-title>How inhibition shapes cortical activity</article-title><source>Neuron</source><year>2011</year><month>October</month><volume>72</volume><issue>2</issue><fpage>231</fpage><lpage>243</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.09.027</pub-id><pub-id pub-id-type="pmcid">PMC3236361</pub-id><pub-id pub-id-type="pmid">22017986</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burr</surname><given-names>D</given-names></name><name><surname>Morrone</surname><given-names>C</given-names></name><name><surname>Maffei</surname><given-names>L</given-names></name></person-group><article-title>Intra-cortical inhibition prevents simple cells from responding to textured visual patterns</article-title><source>Exp Brain Res</source><year>1981</year><volume>43</volume><issue>3-4</issue><fpage>455</fpage><lpage>458</lpage><pub-id pub-id-type="pmid">7262240</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hata</surname><given-names>Y</given-names></name><name><surname>Tsumoto</surname><given-names>T</given-names></name><name><surname>Sato</surname><given-names>H</given-names></name><name><surname>Hagihara</surname><given-names>K</given-names></name><name><surname>Tamura</surname><given-names>H</given-names></name></person-group><article-title>Inhibition contributes to orientation selectivity in visual cortex of cat</article-title><source>Nature</source><year>1988</year><month>October</month><volume>335</volume><issue>6193</issue><fpage>815</fpage><lpage>817</lpage><pub-id pub-id-type="pmid">3185710</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ben-Yishai</surname><given-names>R</given-names></name><name><surname>Bar-Or</surname><given-names>RL</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><article-title>Theory of orientation tuning in visual cortex</article-title><source>Proc Natl Acad Sci U S A</source><year>1995</year><month>April</month><volume>92</volume><issue>9</issue><fpage>3844</fpage><lpage>3848</lpage><pub-id pub-id-type="doi">10.1073/pnas.92.9.3844</pub-id><pub-id pub-id-type="pmcid">PMC42058</pub-id><pub-id pub-id-type="pmid">7731993</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ringach</surname><given-names>DL</given-names></name><name><surname>Bredfeldt</surname><given-names>CE</given-names></name><name><surname>Shapley</surname><given-names>RM</given-names></name><name><surname>Hawken</surname><given-names>MJ</given-names></name></person-group><article-title>Suppression of neural responses to nonoptimal stimuli correlates with tuning selectivity in macaque V1</article-title><source>J Neurophysiol</source><year>2002</year><month>February</month><volume>87</volume><issue>2</issue><fpage>1018</fpage><lpage>1027</lpage><pub-id pub-id-type="pmid">11826065</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Robson</surname><given-names>JG</given-names></name><name><surname>Ohzawa</surname><given-names>I</given-names></name><name><surname>Freeman</surname><given-names>RD</given-names></name></person-group><article-title>Organization of suppression in receptive fields of neurons in cat visual cortex</article-title><source>J Neurophysiol</source><year>1992</year><month>July</month><volume>68</volume><issue>1</issue><fpage>144</fpage><lpage>163</lpage><pub-id pub-id-type="pmid">1517820</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burg</surname><given-names>MF</given-names></name><name><surname>Cadena</surname><given-names>SA</given-names></name><name><surname>Denfield</surname><given-names>GH</given-names></name><name><surname>Walker</surname><given-names>EY</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name></person-group><article-title>Learning divisive normalization in primary visual cortex</article-title><source>PLoS Comput Biol</source><year>2021</year><month>June</month><volume>17</volume><issue>6</issue><elocation-id>e1009028</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1009028</pub-id><pub-id pub-id-type="pmcid">PMC8211272</pub-id><pub-id pub-id-type="pmid">34097695</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bauman</surname><given-names>LA</given-names></name><name><surname>Bonds</surname><given-names>AB</given-names></name></person-group><article-title>Inhibitory refinement of spatial frequency selectivity in single cells of the cat striate cortex</article-title><source>Vision Res</source><year>1991</year><volume>31</volume><issue>6</issue><fpage>933</fpage><lpage>944</lpage><pub-id pub-id-type="pmid">1858324</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Valois</surname><given-names>KK</given-names></name><name><surname>Tootell</surname><given-names>RB</given-names></name></person-group><article-title>Spatial-frequency-specific inhibition in cat striate cortex cells</article-title><source>J Physiol</source><year>1983</year><month>March</month><volume>336</volume><issue>1</issue><fpage>359</fpage><lpage>376</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1983.sp014586</pub-id><pub-id pub-id-type="pmcid">PMC1198974</pub-id><pub-id pub-id-type="pmid">6875912</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rowekamp</surname><given-names>RJ</given-names></name><name><surname>Sharpee</surname><given-names>TO</given-names></name></person-group><article-title>Cross-orientation suppression in visual area V2</article-title><source>Nat Commun</source><year>2017</year><month>June</month><volume>8</volume><issue>1</issue><elocation-id>15739</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms15739</pub-id><pub-id pub-id-type="pmcid">PMC5472723</pub-id><pub-id pub-id-type="pmid">28593941</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pollen</surname><given-names>DA</given-names></name><name><surname>Przybyszewski</surname><given-names>AW</given-names></name><name><surname>Rubin</surname><given-names>MA</given-names></name><name><surname>Foote</surname><given-names>W</given-names></name></person-group><article-title>Spatial receptive field organization of macaque V4 neurons</article-title><source>Cereb Cortex</source><year>2002</year><month>June</month><volume>12</volume><issue>6</issue><fpage>601</fpage><lpage>616</lpage><pub-id pub-id-type="pmid">12003860</pub-id></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Gochin</surname><given-names>PM</given-names></name><name><surname>Gross</surname><given-names>CG</given-names></name></person-group><article-title>Suppression of visual responses of neurons in inferior temporal cortex of the awake macaque by addition of a second stimulus</article-title><source>Brain Res</source><year>1993</year><month>July</month><volume>616</volume><issue>1-2</issue><fpage>25</fpage><lpage>29</lpage><pub-id pub-id-type="pmid">8358617</pub-id></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolls</surname><given-names>ET</given-names></name><name><surname>Tovee</surname><given-names>MJ</given-names></name></person-group><article-title>Sparseness of the neuronal representation of stimuli in the primate temporal visual cortex</article-title><source>J Neurophysiol</source><year>1995</year><month>February</month><volume>73</volume><issue>2</issue><fpage>713</fpage><lpage>726</lpage><pub-id pub-id-type="pmid">7760130</pub-id></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desimone</surname><given-names>R</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><article-title>Neural mechanisms of selective visual attention</article-title><source>Annu Rev Neurosci</source><year>1995</year><volume>18</volume><issue>1</issue><fpage>193</fpage><lpage>222</lpage><pub-id pub-id-type="pmid">7605061</pub-id></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reynolds</surname><given-names>JH</given-names></name><name><surname>Chelazzi</surname><given-names>L</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><article-title>Competitive mechanisms subserve attention in macaque areas V2 and V4</article-title><source>J Neurosci</source><year>1999</year><month>March</month><volume>19</volume><issue>5</issue><fpage>1736</fpage><lpage>1753</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.19-05-01736.1999</pub-id><pub-id pub-id-type="pmcid">PMC6782185</pub-id><pub-id pub-id-type="pmid">10024360</pub-id></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rigotti</surname><given-names>M</given-names></name><name><surname>Barak</surname><given-names>O</given-names></name><name><surname>Warden</surname><given-names>MR</given-names></name><name><surname>Wang</surname><given-names>X-J</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Fusi</surname><given-names>S</given-names></name></person-group><article-title>The importance of mixed selectivity in complex cognitive tasks</article-title><source>Nature</source><year>2013</year><month>May</month><volume>497</volume><issue>7451</issue><fpage>585</fpage><lpage>590</lpage><pub-id pub-id-type="doi">10.1038/nature12160</pub-id><pub-id pub-id-type="pmcid">PMC4412347</pub-id><pub-id pub-id-type="pmid">23685452</pub-id></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fusi</surname><given-names>S</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Rigotti</surname><given-names>M</given-names></name></person-group><article-title>Why neurons mix: high dimensionality for higher cognition</article-title><source>Curr Opin Neurobiol</source><year>2016</year><month>April</month><volume>37</volume><fpage>66</fpage><lpage>74</lpage><pub-id pub-id-type="pmid">26851755</pub-id></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tong</surname><given-names>R</given-names></name><name><surname>da Silva</surname><given-names>R</given-names></name><name><surname>Lin</surname><given-names>D</given-names></name><name><surname>Ghosh</surname><given-names>A</given-names></name><name><surname>Wilsenach</surname><given-names>J</given-names></name><name><surname>Cianfarano</surname><given-names>E</given-names></name><name><surname>Bashivan</surname><given-names>P</given-names></name><name><surname>Richards</surname><given-names>B</given-names></name><name><surname>Trenholm</surname><given-names>S</given-names></name></person-group><article-title>The feature landscape of visual cortex</article-title><source>bioRxiv</source><year>2023</year><month>November</month><elocation-id>2023.11.03.565500</elocation-id></element-citation></ref><ref id="R57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Froudarakis</surname><given-names>E</given-names></name><name><surname>Berens</surname><given-names>P</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name><name><surname>Cotton</surname><given-names>RJ</given-names></name><name><surname>Sinz</surname><given-names>FH</given-names></name><name><surname>Yatsenko</surname><given-names>D</given-names></name><name><surname>Saggau</surname><given-names>P</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name></person-group><article-title>Population code in mouse V1 facilitates readout of natural scenes through increased sparseness</article-title><source>Nat Neurosci</source><year>2014</year><month>June</month><volume>17</volume><issue>6</issue><fpage>851</fpage><lpage>857</lpage><pub-id pub-id-type="doi">10.1038/nn.3707</pub-id><pub-id pub-id-type="pmcid">PMC4106281</pub-id><pub-id pub-id-type="pmid">24747577</pub-id></element-citation></ref><ref id="R58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>Z</given-names></name><name><surname>Fahey</surname><given-names>PG</given-names></name><name><surname>Papadopoulos</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>EY</given-names></name><name><surname>Celii</surname><given-names>B</given-names></name><name><surname>Papadopoulos</surname><given-names>C</given-names></name><name><surname>Chang</surname><given-names>A</given-names></name><name><surname>Kunin</surname><given-names>AB</given-names></name><name><surname>Tran</surname><given-names>D</given-names></name><name><surname>Fu</surname><given-names>J</given-names></name><etal/></person-group><article-title>Functional connectomics reveals general wiring rule in mouse visual cortex</article-title><source>Nature</source><year>2025</year><volume>640</volume><issue>8058</issue><fpage>459</fpage><lpage>469</lpage><pub-id pub-id-type="doi">10.1038/s41586-025-08840-3</pub-id><pub-id pub-id-type="pmcid">PMC11981947</pub-id><pub-id pub-id-type="pmid">40205211</pub-id></element-citation></ref><ref id="R59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tasic</surname><given-names>B</given-names></name><name><surname>Yao</surname><given-names>Z</given-names></name><name><surname>Graybuck</surname><given-names>LT</given-names></name><name><surname>Smith</surname><given-names>KA</given-names></name><name><surname>Nguyen</surname><given-names>TN</given-names></name><name><surname>Bertagnolli</surname><given-names>D</given-names></name><name><surname>Goldy</surname><given-names>J</given-names></name><name><surname>Garren</surname><given-names>E</given-names></name><name><surname>Economo</surname><given-names>MN</given-names></name><name><surname>Viswanathan</surname><given-names>S</given-names></name><name><surname>Penn</surname><given-names>O</given-names></name><etal/></person-group><article-title>Shared and distinct transcriptomic cell types across neocortical areas</article-title><source>Nature</source><year>2018</year><month>November</month><volume>563</volume><issue>7729</issue><fpage>72</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1038/s41586-018-0654-5</pub-id><pub-id pub-id-type="pmcid">PMC6456269</pub-id><pub-id pub-id-type="pmid">30382198</pub-id></element-citation></ref><ref id="R60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gouwens</surname><given-names>NW</given-names></name><name><surname>Sorensen</surname><given-names>SA</given-names></name><name><surname>Berg</surname><given-names>J</given-names></name><name><surname>Lee</surname><given-names>C</given-names></name><name><surname>Jarsky</surname><given-names>T</given-names></name><name><surname>Ting</surname><given-names>J</given-names></name><name><surname>Sunkin</surname><given-names>SM</given-names></name><name><surname>Feng</surname><given-names>D</given-names></name><name><surname>Anastassiou</surname><given-names>CA</given-names></name><name><surname>Barkan</surname><given-names>E</given-names></name><name><surname>Bickley</surname><given-names>K</given-names></name><etal/></person-group><article-title>Classification of electrophysiological and morphological neuron types in the mouse visual cortex</article-title><source>Nat Neurosci</source><year>2019</year><month>July</month><volume>22</volume><issue>7</issue><fpage>1182</fpage><lpage>1195</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0417-0</pub-id><pub-id pub-id-type="pmcid">PMC8078853</pub-id><pub-id pub-id-type="pmid">31209381</pub-id></element-citation></ref><ref id="R61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yao</surname><given-names>Z</given-names></name><name><surname>Liu</surname><given-names>H</given-names></name><name><surname>Xie</surname><given-names>F</given-names></name><name><surname>Fischer</surname><given-names>S</given-names></name><name><surname>Adkins</surname><given-names>RS</given-names></name><name><surname>Aldridge</surname><given-names>AI</given-names></name><name><surname>Ament</surname><given-names>SA</given-names></name><name><surname>Bartlett</surname><given-names>A</given-names></name><name><surname>Behrens</surname><given-names>MM</given-names></name><name><surname>Van den Berge</surname><given-names>K</given-names></name><name><surname>Bertagnolli</surname><given-names>D</given-names></name><etal/></person-group><article-title>A transcriptomic and epigenomic cell atlas of the mouse primary motor cortex</article-title><source>Nature</source><year>2021</year><month>October</month><volume>598</volume><issue>7879</issue><fpage>103</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03500-8</pub-id><pub-id pub-id-type="pmcid">PMC8494649</pub-id><pub-id pub-id-type="pmid">34616066</pub-id></element-citation></ref><ref id="R62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keller</surname><given-names>D</given-names></name><name><surname>Erö</surname><given-names>C</given-names></name><name><surname>Markram</surname><given-names>H</given-names></name></person-group><article-title>Cell densities in the mouse brain: A systematic review</article-title><source>Front Neuroanat</source><year>2018</year><month>October</month><volume>12</volume><issue>83</issue><pub-id pub-id-type="doi">10.3389/fnana.2018.00083</pub-id><pub-id pub-id-type="pmcid">PMC6205984</pub-id><pub-id pub-id-type="pmid">30405363</pub-id></element-citation></ref><ref id="R63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diamond</surname><given-names>JS</given-names></name></person-group><article-title>Inhibitory interneurons in the retina: Types, circuitry, and function</article-title><source>Annu Rev Vis Sci</source><year>2017</year><month>September</month><volume>3</volume><fpage>1</fpage><lpage>24</lpage><pub-id pub-id-type="pmid">28617659</pub-id></element-citation></ref><ref id="R64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matsumoto</surname><given-names>A</given-names></name><name><surname>Morris</surname><given-names>J</given-names></name><name><surname>Looger</surname><given-names>LL</given-names></name><name><surname>Yonehara</surname><given-names>K</given-names></name></person-group><article-title>Functionally distinct GABAergic amacrine cell types regulate spatiotemporal encoding in the mouse retina</article-title><source>Nat Neurosci</source><year>2025</year><month>June</month><volume>28</volume><issue>6</issue><fpage>1256</fpage><lpage>1267</lpage><pub-id pub-id-type="doi">10.1038/s41593-025-01935-0</pub-id><pub-id pub-id-type="pmcid">PMC12148929</pub-id><pub-id pub-id-type="pmid">40234708</pub-id></element-citation></ref><ref id="R65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muñoz</surname><given-names>W</given-names></name><name><surname>Tremblay</surname><given-names>R</given-names></name><name><surname>Levenstein</surname><given-names>D</given-names></name><name><surname>Rudy</surname><given-names>B</given-names></name></person-group><article-title>Layer-specific modulation of neocortical dendritic inhibition during active wakefulness</article-title><source>Science</source><year>2017</year><month>March</month><volume>355</volume><issue>6328</issue><fpage>954</fpage><lpage>959</lpage><pub-id pub-id-type="pmid">28254942</pub-id></element-citation></ref><ref id="R66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>J</given-names></name><name><surname>Tucciarone</surname><given-names>J</given-names></name><name><surname>Padilla-Coreano</surname><given-names>N</given-names></name><name><surname>He</surname><given-names>M</given-names></name><name><surname>Gordon</surname><given-names>JA</given-names></name><name><surname>Huang</surname><given-names>ZJ</given-names></name></person-group><article-title>Selective inhibitory control of pyramidal neuron ensembles and cortical subnetworks by chandelier cells</article-title><source>Nat Neurosci</source><year>2017</year><month>October</month><volume>20</volume><issue>10</issue><fpage>1377</fpage><lpage>1383</lpage><pub-id pub-id-type="doi">10.1038/nn.4624</pub-id><pub-id pub-id-type="pmcid">PMC5614838</pub-id><pub-id pub-id-type="pmid">28825718</pub-id></element-citation></ref><ref id="R67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>SJ</given-names></name><name><surname>Sevier</surname><given-names>E</given-names></name><name><surname>Dwivedi</surname><given-names>D</given-names></name><name><surname>Saldi</surname><given-names>G-A</given-names></name><name><surname>Hairston</surname><given-names>A</given-names></name><name><surname>Yu</surname><given-names>S</given-names></name><name><surname>Abbott</surname><given-names>L</given-names></name><name><surname>Choi</surname><given-names>DH</given-names></name><name><surname>Sherer</surname><given-names>M</given-names></name><name><surname>Qiu</surname><given-names>Y</given-names></name><name><surname>Shinde</surname><given-names>A</given-names></name><etal/></person-group><article-title>Cortical somatostatin interneuron subtypes form cell-type-specific circuits</article-title><source>Neuron</source><year>2023</year><month>September</month><volume>111</volume><issue>17</issue><fpage>2675</fpage><lpage>2692</lpage><elocation-id>e9</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuron.2023.05.032</pub-id><pub-id pub-id-type="pmcid">PMC11645782</pub-id><pub-id pub-id-type="pmid">37390821</pub-id></element-citation></ref><ref id="R68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>O</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><article-title>Natural signal statistics and sensory gain control</article-title><source>Nat Neurosci</source><year>2001</year><month>August</month><volume>4</volume><issue>8</issue><fpage>819</fpage><lpage>825</lpage><pub-id pub-id-type="pmid">11477428</pub-id></element-citation></ref><ref id="R69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ioffe</surname><given-names>S</given-names></name><name><surname>Szegedy</surname><given-names>C</given-names></name></person-group><article-title>Batch normalization: Accelerating deep network training by reducing internal covariate shift</article-title><source>arXiv [csLG]</source><year>2015</year><month>February</month></element-citation></ref><ref id="R70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ba</surname><given-names>JL</given-names></name><name><surname>Kiros</surname><given-names>JR</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><article-title>Layer normalization</article-title><source>arXiv [statML]</source><year>2016</year><month>July</month></element-citation></ref><ref id="R71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaswani</surname><given-names>A</given-names></name><name><surname>Shazeer</surname><given-names>N</given-names></name><name><surname>Parmar</surname><given-names>N</given-names></name><name><surname>Uszkoreit</surname><given-names>J</given-names></name><name><surname>Jones</surname><given-names>L</given-names></name><name><surname>Gomez</surname><given-names>AN</given-names></name><name><surname>Kaiser</surname><given-names>L</given-names></name><name><surname>Polosukhin</surname><given-names>I</given-names></name></person-group><article-title>Attention is all you need</article-title><source>arXiv [csCL]</source><year>2017</year><month>June</month></element-citation></ref><ref id="R72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jun</surname><given-names>JJ</given-names></name><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Siegle</surname><given-names>JH</given-names></name><name><surname>Denman</surname><given-names>DJ</given-names></name><name><surname>Bauza</surname><given-names>M</given-names></name><name><surname>Barbarits</surname><given-names>B</given-names></name><name><surname>Lee</surname><given-names>AK</given-names></name><name><surname>Anastassiou</surname><given-names>CA</given-names></name><name><surname>Andrei</surname><given-names>A</given-names></name><name><surname>Aydın</surname></name><name><surname>Barbic</surname><given-names>M</given-names></name><etal/></person-group><article-title>Fully integrated silicon probes for high-density recording of neural activity</article-title><source>Nature</source><year>2017</year><month>November</month><volume>551</volume><issue>7679</issue><fpage>232</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1038/nature24636</pub-id><pub-id pub-id-type="pmcid">PMC5955206</pub-id><pub-id pub-id-type="pmid">29120427</pub-id></element-citation></ref><ref id="R73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Birman</surname><given-names>D</given-names></name><name><surname>Yang</surname><given-names>KJ</given-names></name><name><surname>West</surname><given-names>SJ</given-names></name><name><surname>Karsh</surname><given-names>B</given-names></name><name><surname>Browning</surname><given-names>Y</given-names></name><collab>International Brain Laboratory</collab><name><surname>Siegle</surname><given-names>JH</given-names></name><name><surname>Steinmetz</surname><given-names>NA</given-names></name></person-group><article-title>Pinpoint: trajectory planning for multi-probe electro-physiology and injections in an interactive web-based 3D environment</article-title><source>bioRxivorg</source><year>2023</year><month>July</month><elocation-id>2023.07.14.548952</elocation-id></element-citation></ref><ref id="R74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Sridhar</surname><given-names>S</given-names></name><name><surname>Stringer</surname><given-names>C</given-names></name></person-group><article-title>Solving the spike sorting problem with kilosort</article-title><source>bioRxiv</source><year>2023</year><month>January</month></element-citation></ref><ref id="R75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tafazoli</surname><given-names>S</given-names></name><name><surname>Safaai</surname><given-names>H</given-names></name><name><surname>De Franceschi</surname><given-names>G</given-names></name><name><surname>Rosselli</surname><given-names>FB</given-names></name><name><surname>Vanzella</surname><given-names>W</given-names></name><name><surname>Riggi</surname><given-names>M</given-names></name><name><surname>Buffolo</surname><given-names>F</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name></person-group><article-title>Emergence of transformation-tolerant representations of visual objects in rat lateral extrastriate cortex</article-title><source>Elife</source><year>2017</year><month>April</month><volume>6</volume><pub-id pub-id-type="doi">10.7554/eLife.22794</pub-id><pub-id pub-id-type="pmcid">PMC5388540</pub-id><pub-id pub-id-type="pmid">28395730</pub-id></element-citation></ref><ref id="R76"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Greff</surname><given-names>K</given-names></name><name><surname>Belletti</surname><given-names>F</given-names></name><name><surname>Beyer</surname><given-names>L</given-names></name><name><surname>Doersch</surname><given-names>C</given-names></name><name><surname>Du</surname><given-names>Y</given-names></name><name><surname>Duckworth</surname><given-names>D</given-names></name><name><surname>Fleet</surname><given-names>DJ</given-names></name><name><surname>Gnanapragasam</surname><given-names>D</given-names></name><name><surname>Golemo</surname><given-names>F</given-names></name><name><surname>Herrmann</surname><given-names>C</given-names></name><name><surname>Kipf</surname><given-names>T</given-names></name><etal/></person-group><source>Kubric: a scalable dataset generator</source><year>2022</year></element-citation></ref><ref id="R77"><element-citation publication-type="book"><collab>Blender Foundation</collab><publisher-name>Blender</publisher-name><year>2024</year></element-citation></ref><ref id="R78"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cimpoi</surname><given-names>M</given-names></name><name><surname>Maji</surname><given-names>S</given-names></name><name><surname>Kokkinos</surname><given-names>I</given-names></name><name><surname>Mohamed</surname><given-names>S</given-names></name><name><surname>Vedaldi</surname><given-names>A</given-names></name></person-group><source>Describing textures in the wild</source><conf-name>Proceedings of the IEEE Conf on Computer Vision and Pattern Recognition (CVPR)</conf-name><year>2014</year></element-citation></ref><ref id="R79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woo</surname><given-names>S</given-names></name><name><surname>Debnath</surname><given-names>S</given-names></name><name><surname>Hu</surname><given-names>R</given-names></name><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Kweon</surname><given-names>IS</given-names></name><name><surname>Xie</surname><given-names>S</given-names></name></person-group><article-title>ConvNeXt V2: Co-designing and scaling ConvNets with masked autoencoders</article-title><source>arXiv [csCV]</source><year>2023</year><month>January</month></element-citation></ref><ref id="R80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lurz</surname><given-names>K-K</given-names></name><name><surname>Bashiri</surname><given-names>M</given-names></name><name><surname>Willeke</surname><given-names>K</given-names></name><name><surname>Jagadish</surname><given-names>A</given-names></name><name><surname>Wang</surname><given-names>E</given-names></name><name><surname>Walker</surname><given-names>EY</given-names></name><name><surname>Cadena</surname><given-names>SA</given-names></name><name><surname>Muhammad</surname><given-names>T</given-names></name><name><surname>Cobos</surname><given-names>E</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name><etal/></person-group><article-title>Generalization in data-driven models of primary visual cortex</article-title><year>2022</year><month>February</month></element-citation></ref><ref id="R81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loshchilov</surname><given-names>I</given-names></name><name><surname>Hutter</surname><given-names>F</given-names></name></person-group><article-title>Decoupled weight decay regularization</article-title><source>arXiv [csLG]</source><year>2017</year><month>November</month></element-citation></ref><ref id="R82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salman</surname><given-names>H</given-names></name><name><surname>Yang</surname><given-names>G</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>P</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Razenshteyn</surname><given-names>I</given-names></name><name><surname>Bubeck</surname><given-names>S</given-names></name></person-group><article-title>Provably robust deep learning via adversarially trained smoothed classifiers</article-title><source>arXiv [csLG]</source><year>2019</year><month>June</month></element-citation></ref><ref id="R83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clevert</surname><given-names>D-A</given-names></name><name><surname>Unterthiner</surname><given-names>T</given-names></name><name><surname>Hochreiter</surname><given-names>S</given-names></name></person-group><article-title>Fast and accurate deep network learning by exponential linear units (ELUs</article-title><source>arXiv [csLG]</source><year>2015</year><month>November</month></element-citation></ref><ref id="R84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><article-title>Adam: A method for stochastic optimization</article-title><source>arXiv [csLG]</source><year>2014</year><month>December</month></element-citation></ref><ref id="R85"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>J</given-names></name><name><surname>Dong</surname><given-names>W</given-names></name><name><surname>Socher</surname><given-names>R</given-names></name><name><surname>Li</surname><given-names>L-J</given-names></name><name><surname>Li</surname><given-names>K</given-names></name><name><surname>Fei-Fei</surname><given-names>L</given-names></name></person-group><source>Imagenet: A large-scale hierarchical image database</source><conf-name>2009 IEEE Conference on Computer Vision and Pattern Recognition</conf-name><year>2009</year><fpage>248</fpage><lpage>255</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2009.5206848</pub-id></element-citation></ref><ref id="R86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><article-title>Adam: A method for stochastic optimization</article-title><year>2017</year></element-citation></ref><ref id="R87"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>S</given-names></name><name><surname>Tamir</surname><given-names>NY</given-names></name><name><surname>Sundaram</surname><given-names>S</given-names></name><name><surname>Chai</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>R</given-names></name><name><surname>Dekel</surname><given-names>T</given-names></name><name><surname>Isola</surname><given-names>P</given-names></name></person-group><source>DreamSim: Learning new dimensions of human visual similarity using synthetic data</source><conf-name>Thirty-seventh Conference on Neural Information Processing Systems</conf-name><year>2023b</year><month>November</month></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Fig. 1</label><caption><title>Experimental approach.</title><p><bold>a:</bold> Experimental recordings from macaque visual areas V1 (453 neurons from 2 monkeys) and V4 (394 neurons from 2 monkeys) during presentation of natural images. Monkeys fixated on the screen (fixation spot shown in red). The stimulus was centered on the population receptive field of neurons recorded that day, indicated by the white circle. <bold>b:</bold> Architecture of functional “digital twin” models, consisting of a core shared across neurons (V1: first layer of ConvNext; V4: layer 3 of Robust ResNet50) and a neuron-specific readout. This design creates in silico neurons that model the response properties of individual biological neurons, with example receptive field locations of two neurons illustrated. <bold>c:</bold> Prediction accuracy of the model on test images not used during training, for both V1 (orange) and V4 (blue). Left panel shows distribution of single trial correlations, and right panel displays correlation to the average response across stimulus repeats. The dotted line indicates an inclusion threshold of 0.4. For further analysis, we only included neurons with a correlation to average above that threshold (<italic>n</italic> = 443 neurons for V1, <italic>n</italic> = 205 neurons for V4).</p></caption><graphic xlink:href="EMS207390-f001"/></fig><fig id="F2" position="float"><label>Fig. 2</label><caption><title>Continuum of single neuron response sparseness in early and mid-level macaque visual cortex.</title><p><bold>a</bold>, Response profiles of non-sparse (left) and sparse (right) V1 neurons. Curves display neuronal activity sorted from lowest to highest response, derived from model predictions across 1.2 million ImageNet images (gray, top row) and recorded responses to 75 test images (black, bottom row), averaged over stimulus repeats. We used responses to test images to obtain mean stimulus driven activity and average out stimulus-unrelated signals. Skewness values quantify lifetime sparsity, with higher values indicating neurons that respond selectively to fewer stimuli while remaining silent to most others. <bold>b</bold>, Comparable response profiles for representative V4 neurons, demonstrating similar variations in lifetime sparsity in this higher-order visual area, with some neurons showing broadly tuned responses and others exhibiting highly selective activation patterns. <bold>c</bold>, Correlation analysis between prediction-based and recording-based skewness values for qualifying V1 (orange, <italic>n</italic> = 443) and V4 (blue, <italic>n</italic> = 205) neurons. The strong correlation (<italic>r</italic> = 0.54 for V1 and <italic>r</italic> = 0.66 for V4, both <italic>p &lt;</italic> 0.001) validates that our model accurately captures intrinsic sparsity characteristics across natural scenes, supporting the use of model-predicted responses for systematic in silico analyses across large image datasets. <bold>d</bold>, Population-level distribution of lifetime sparsity across V1 (orange) and V4 (blue) neuronal populations (V1: <italic>n</italic> = 443 neurons, V4: <italic>n</italic> = 205 neurons), revealing a continuous spectrum rather than discrete categories. Representative activation curves above illustrate how response profiles change along this continuum. Neurons with skewness below 2.0 are defined as non-sparse, though this threshold represents a point along a gradual transition. This distribution highlights the functional diversity within each visual area.</p></caption><graphic xlink:href="EMS207390-f002"/></fig><fig id="F3" position="float"><label>Fig. 3</label><caption><title>Two complementary methods to study neuronal selectivity: optimization-based feature visualization and large-scale image screening.</title><p><bold>a</bold>, Schematic of the feature visualization procedure, in which a starting noise image is iteratively optimized using gradient ascent on a neural predictive model to either maximize or minimize the activity of a single neuron. This yields the most exciting input (MEI) or the least exciting input (LEI), respectively. The process begins with a noise image and updates pixel values to achieve the target activation level. Example shows an LEI for a V4 neuron. <bold>b</bold>, Schematic of the image screening procedure, in which a large dataset of 1.2 million ImageNet images is used to probe neuronal responses. Each image elicits a predicted neuronal response from the model, allowing construction of a response profile across the dataset. Sorting these responses identifies the most activating (MAI) and least activating (LAI) images for each neuron.</p></caption><graphic xlink:href="EMS207390-f003"/></fig><fig id="F4" position="float"><label>Fig. 4</label><caption><title>Identification of most and least activating stimuli of macaque V1 neurons.</title><p>Least (left) and most (right) activating inputs for five example V1 neurons. For each neuron, top row show optimized images starting from different initialization (i.e. noise) seeds (LEIs on the left, MEIs on the right) and bottom row shows the most and least activating images identified through screening 1.2 million ImageNet images (LAIs on the left, MAIs on the right). Images are 2.3 × 2.3 degrees visual angle, with each neuron’s receptive field located in the center of the image.</p></caption><graphic xlink:href="EMS207390-f004"/></fig><fig id="F5" position="float"><label>Fig. 5</label><caption><title>Identification of most and least activating stimuli of macaque V4 neurons.</title><p>Least (left) and most (right) activating inputs for five example V4 neurons. For each neuron, top row show optimized images starting from different initialization (i.e. noise) seeds (LEIs on the left, MEIs on the right) and bottom row shows the most and least activating images identified through screening 1.2 million ImageNet images (LAIs on the left, MAIs on the right). Images are 14.81 × 14.81 degrees visual angle, with each neuron’s receptive field located in the center of the image.</p></caption><graphic xlink:href="EMS207390-f005"/></fig><fig id="F6" position="float"><label>Fig. 6</label><caption><title>High and low activity reflect structured and perceptually coherent feature combinations.</title><p><bold>a</bold>, Schematic illustrating the computation of image similarities. All naturalistic rendered images were embedded into DreamSim, a perceptual similarity space fine-tuned on human judgments. Within this high-dimensional space, we computed cosine similarity among the top 10 most activating images (MAIs) and among the least activating images (LAIs), as well as their similarity to random images. <bold>b</bold>, Distributions of cosine similarity among MAIs (top 10 images, red) and between MAIs and random images (gray). These distributions were used to compute discriminability using the d-prime metric. <bold>c</bold>, d-prime values for MAIs and LAIs across all non-sparse V1 and V4 neurons. Gray bars indicate a control condition comparing similarities between random image sets. Across the population, d-prime values for both MAIs and LAIs were significantly higher in V1 and V4 compared to random image sets (two-sample t-test, <italic>p &lt;</italic> 0.001). After applying false discovery rate correction, all V4 neurons (<italic>n</italic> = 168) retained <italic>p</italic>-values below 0.05, indicating that the likelihood of observing such discriminability by chance was consistently low. In the larger population, <italic>n</italic> = 293 out of <italic>n</italic> = 315 neurons showed significant <italic>p</italic>-values for MAIs, and <italic>n</italic> = 281 neurons showed significant <italic>p</italic>-values for LAIs.</p></caption><graphic xlink:href="EMS207390-f006"/></fig><fig id="F7" position="float"><label>Fig. 7</label><caption><title>Model predictions accurately identify extreme stimuli in recorded neuronal responses from V1 and V4.</title><p><bold>a</bold>, Left: Model-predicted responses to 75 test ImageNet images for an example non-sparse (top) and sparse (bottom) V1 neuron. The predicted least and most activating test images are marked in blue and red, respectively. Right: Actual recorded responses of the same neurons to the same test images, averaged across stimulus repeats and sorted by recorded response magnitude. The blue and red dots indicate the images that the model predicted to elicit the lowest and highest responses. <bold>b</bold>, Distribution of response percentiles in recorded data for the predicted least (blue) and most (red) activating images, shown separately for non-sparse (top) and sparse (bottom) V1 neurons. Note that a non-selective ordering would be expected to yield a uniform distribution, similar to the blue distribution observed here. <bold>c</bold>, Same as (b), but for non-sparse and sparse neurons in area V4.</p></caption><graphic xlink:href="EMS207390-f007"/></fig><fig id="F8" position="float"><label>Fig. 8</label><caption><title>Independent evaluator models confirm the identification of optimal stimuli for V1 and V4 neuronal responses.</title><p><bold>a</bold>, Schematic of the verification pipeline. Least and most activating images for V1 neurons were identified using a generator model through both optimization and screening of images. These images were then passed to an independent evaluator model, comprising a separate model ensemble with the same core architecture but a readout trained from scratch. <bold>b</bold>, Example V1 neuron. Left: Distribution of predicted responses to size- and contrast-matched natural images, with the predicted responses to the MEI and LEI (identified by the generator model) highlighted by the evaluator model. Right: Distribution of response percentiles for MEIs, LEIs, MAIs, and LAIs as predicted by the evaluator model, relative to the distribution of natural image responses. MAIs and LAIs were identified based on 200k naturalistic rendered images. <bold>c</bold>,<bold>d</bold>, As in (a,b), but for V4 neurons. The evaluator model in this case had a distinct architecture and training objective and was trained from scratch on the neural data.</p></caption><graphic xlink:href="EMS207390-f008"/></fig><fig id="F9" position="float"><label>Fig. 9</label><caption><title>Non-sparse visual neurons in V1 and V4 tend to exhibit high baseline firing rates.</title><p><bold>a</bold>, Median predicted activity to natural scenes plotted versus baseline firing rate extracted from a 300 ms fixation window before stimulus onset. V1 (orange): <italic>n</italic> = 443 neurons, V4 (blue): <italic>n</italic> = 205 neurons. <italic>R</italic><sup>2</sup> from linear regression. <bold>b</bold>, Baseline firing rate extracted from a 300 ms fixation window before stimulus onset plotted versus skewness of predicted responses. V1 (orange): <italic>n</italic> = 443 neurons, V4 (blue): <italic>n</italic> = 205 neurons. <italic>R</italic><sup>2</sup> from exponential fit. Neurons with low baseline firing rates exhibit variable skewness that likely reflects the prevalence of their preferred features in natural scenes—rare features produce highly skewed responses while common features yield more symmetric distributions.</p></caption><graphic xlink:href="EMS207390-f009"/></fig><fig id="F10" position="float"><label>Fig. 10</label><caption><title>Responses of V4 neurons vary continuously depending on preferred and non-preferred stimuli.</title><p><bold>a</bold>, Schematic illustrating construction of the 2D image similarity space for each neuron. Each of <italic>~</italic>200k naturalistic rendered images was assigned <italic>x</italic>- and <italic>y</italic>-coordinates based on its cosine similarity (computed using DreamSim) to the neuron’s least activating image (LAI, <italic>x</italic>-coordinate) and most activating image (MAI, <italic>y</italic>-coordinate), respectively. MAIs and LAIs were defined from the same image set. <bold>b</bold>, Example 2D similarity space for a non-sparse V4 neuron. Bins are color-coded by the mean predicted neuronal activity of the images within each bin. The arrow denotes the principal activity gradient, and <italic>R</italic><sup>2</sup> indicates the variance explained by a linear fit. The color bar spans the 0.1 to 99.9th percentile of neuronal responses across all images. <bold>c</bold>, Same as (b), but for a sparse V4 neuron. Here, the activity gradient aligns primarily along the <italic>y</italic>-axis, indicating stronger modulation by similarity to the MAI than to the LAI. <bold>d</bold>, Variance explained (<italic>R</italic><sup>2</sup>) by linear regression predicting neuronal activity from the 2D similarity space (as in b,c), shown separately for sparse (dark blue) and non-sparse (light blue) V4 neurons. <bold>e</bold>, Results of three control analyses validating the similarity space for the V4 neuron shown in panel b. Control 1 replaces both MAIs and LAIs with random images. Control 2 replaces MAIs with random images but retains LAIs. Control 3 replaces LAIs with random images but retains MAIs. <bold>f</bold>, Variance explained (<italic>R</italic><sup>2</sup>) by linear regression for the original analysis (similarity to MAIs and LAIs on <italic>x</italic>- and <italic>y</italic>-axes) and for Control 1 (left), Control 2 (middle), and Control 3 (right). Sparse (dark blue) and non-sparse (light blue) neurons are shown separately. Lines indicate linear regression fits, with slopes reported in each panel.</p></caption><graphic xlink:href="EMS207390-f010"/></fig><fig id="F11" position="float"><label>Fig. 11</label><caption><title>Most and least activating stimuli reveal distributed feature tuning across neuronal populations.</title><p><bold>a</bold>, Examples of least activating images (left) and most activating images (right) for two V1 neurons. Arrows indicate the perceptual similarity between MAIs and LAIs shared across both neurons. <bold>b</bold>, Distribution of response percentiles evoked by MAIs, LAIs, and randomly sampled natural images across the V1 neuronal population. The left histogram shows the probability that one neuron’s MAI will elicit a specific response percentile in other neurons. Response percentiles were calculated from each neuron’s responses to 1.2 million ImageNet images. Data represent population means with 99% confidence intervals. <bold>c</bold>,<bold>d</bold>, Same analyses as in (a,b) applied to neurons recorded in visual area V4. <bold>e</bold>, Schematic illustrating how we used the MAIs and LAIs identified from neurons recorded in one monkey to predict the responses of neurons recorded in another monkey. The resulting response percentiles indicate how these images ranked compared to the predicted responses to 1.2 million ImageNet images, as shown in panels (f,g). <bold>f</bold>, Cross-animal generalization: probability that MAIs (left) and LAIs (right) from V1 neurons in monkey A will evoke specific response percentiles in V1 neurons from monkey B. <bold>g</bold>, Same cross-animal analysis as in (e), applied to V4 neurons.</p></caption><graphic xlink:href="EMS207390-f011"/></fig><fig id="F12" position="float"><label>Fig. 12</label><caption><title>Dual-feature selectivity in the mouse visual cortex.</title><p><bold>a</bold>, Experimental recordings from mouse visual areas V1 (598 neurons), LM (350 neurons), and LI (126 neurons) from 8 C57BL-6 mice during presentation of natural images. Mice were head-fixed on a treadmill and passively viewing the stimuli presented on a screen in front of them. <bold>b</bold>, Prediction accuracy of the model on test images not used during training, for V1 (orange), LM (cyan), and LI (blue) as the correlation to the average response across stimulus repeats. The dotted line indicates an inclusion threshold of 0.4. For further analysis, we only included neurons with a correlation to average above that threshold (<italic>n</italic> = 561 neurons for V1, <italic>n</italic> = 325 neurons for LM, <italic>n</italic> = 113 neurons for LI). <bold>c</bold>, Correlation analysis between prediction-based and recording-based skewness values for qualifying V1 (orange, <italic>n</italic> = 561), LM (cyan, <italic>n</italic> = 325), and LI (blue, <italic>n</italic> = 113) neurons. Similar to the primates, there is strong correlation (<italic>r</italic> = 0.834 for V1, <italic>r</italic> = 0.0.828 for LM, and <italic>r</italic> = 0.801 for LI, all <italic>p &lt;</italic> 0.001) indicating that the model accurately captures intrinsic sparsity characteristics across natural scenes. <bold>d</bold>, Population-level distribution of lifetime sparsity across V1 (orange), LM (cyan), and LI (blue) neuronal populations, revealing a continuous spectrum rather than discrete categories. Neurons with skewness below 2.0 are defined as non-sparse. <bold>e</bold>, Response profiles of non-sparse (left) and sparse (right) V1 neurons. Curves display neuronal activity sorted from lowest to highest response, derived from model predictions across 200,000 ImageNet images (gray, top row) and recorded responses to 100 test images (black, bottom row), averaged over stimulus repeats. Skewness values quantify lifetime sparsity, as in <xref ref-type="fig" rid="F2">Fig. 2a,b</xref>. <bold>f</bold>,<bold>g</bold>, Same as (e) for LM neurons (f) and LI neurons (g). <bold>h</bold>, Baseline firing rate extracted from a 200 ms window before stimulus onset plotted versus skewness of predicted responses. V1 (orange, <italic>n</italic> = 561), LM (cyan, <italic>n</italic> = 325), LI (blue, <italic>n</italic> = 113) neurons. R<sup>2</sup> from exponential fit. <bold>i</bold>, Least (blue, LAI) and most (red, MAI) activating images for 4 example non-sparse V1 neurons. LAI and MAI are identified through screening of 200,000 ImageNet images. Images are 84 x 114 degrees visual angle. <bold>j</bold>,<bold>k</bold> Same as (i) for LM neurons (j) and LI neurons (k). <bold>l</bold>, Distribution of response percentiles evoked by MAIs, LAIs, and random images across the V1 neuronal population. The left histogram shows the probability that one neuron’s MAI will elicit a specific response percentile in other neurons. Response percentiles were calculated from each neuron’s responses to 200,000 ImageNet images. Data represent population means with 99% confidence intervals. <bold>m</bold>,<bold>n</bold> Same analysis as (l) for LM neurons (m) and LI neurons (n).</p></caption><graphic xlink:href="EMS207390-f012"/></fig><fig id="F13" position="float"><label>Fig. 13</label><caption><title>Single neuron coding strategies in sparse and non-sparse visual neurons.</title><p>This schematic illustrates how sparse and non-sparse neurons exhibit selectivity for distinct <italic>concepts</italic>, where each concept (e.g., <italic>C</italic><sub>1</sub>, <italic>C</italic><sub>2</sub>) represents a specific combination of latent visual features such as color, shape, and texture—dimensions known to be encoded in area V4. These concepts can be thought of as points in a high-dimensional perceptual space. <bold>Left</bold>: <italic>Sparse neurons</italic> respond selectively and strongly to a single concept (e.g., <italic>C</italic><sub>1</sub> : green dot texture) and remain silent for most other stimuli. These neurons exhibit high lifetime sparseness and encode only a narrow portion of stimulus space. <bold>Right</bold>: <italic>Non-sparse neurons</italic> may exhibit <italic>dual-feature selectivity</italic>, characterized by excitation to one concept (e.g., <italic>C</italic><sub>2</sub> : concave curvature) and suppression to another (e.g., <italic>C</italic><sub>1</sub>). These neurons tend to maintain non-zero baseline activity and modulate their firing rates bidirectionally, enabling graded responses to a broader range of stimuli. This bidirectional modulation likely reflects feature-selective excitation (red arrows) and inhibition (blue arrows), with firing rates encoding the similarity of each stimulus to both excitatory and suppressive features. Sparse and non-sparse neurons are intermingled within the same cortical area, forming a distributed population code in which different neurons anchor their selectivity to partially overlapping sets of concepts. This organizational motif—shared feature selectivity and bidirectional modulation—appears conserved across visual cortical areas, including earlier regions such as V1, though the underlying concepts differ with the feature space represented at each stage.</p></caption><graphic xlink:href="EMS207390-f013"/></fig></floats-group></article>