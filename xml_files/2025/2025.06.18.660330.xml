<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS207018</article-id><article-id pub-id-type="doi">10.1101/2025.06.18.660330</article-id><article-id pub-id-type="archive">PPR1040165</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Tracking visual rhythms: a concert of sensory and motor simulation</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Kaltenmaier</surname><given-names>Aaron</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="fn" rid="FN1">4</xref></contrib><contrib contrib-type="author"><name><surname>Gehmacher</surname><given-names>Quirin</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Kok</surname><given-names>Peter</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Davis</surname><given-names>Matthew H.</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Press</surname><given-names>Clare</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib></contrib-group><aff id="A1"><label>1</label>Department of Experimental Psychology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>UCL</institution></institution-wrap>, <country country="GB">UK</country></aff><aff id="A2"><label>2</label>Functional Imaging Laboratory, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>UCL</institution></institution-wrap>, <country country="GB">UK</country></aff><aff id="A3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/055bpw879</institution-id><institution>MRC Cognition and Brain Sciences Unit</institution></institution-wrap>, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>University of Cambridge</institution></institution-wrap>, <country country="GB">UK</country></aff><author-notes><corresp id="CR1"><email>aaron.kaltenmaier.22@ucl.ac.uk</email></corresp><fn id="FN1"><label>4</label><p id="P1">Lead contact</p></fn></author-notes><pub-date pub-type="nihms-submitted"><day>11</day><month>07</month><year>2025</year></pub-date><pub-date pub-type="preprint"><day>21</day><month>06</month><year>2025</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P2">Neural oscillations have been proposed to model external temporal structure by phase-coupling to environmental rhythms, thereby supporting adaptive perception. However, there is little evidence supporting these theories, particularly in the visual domain, and the underlying mechanisms remain unclear. Using MEG and a new empirical approach we addressed this issue. Participants attended 1.3 and 2 Hz visual displays of rotating Gabors and judged either the timing or content of these events. We show behaviourally-relevant rate-specific phase-coupling in motor structures to – and beyond – the visual rhythm specifically when judging temporal features of the display. We subsequently devised a rate-specific decoding measure to show that visual structures track anticipated, temporally-precise content regardless of task. This sensory simulation predicted the temporal tracking in motor structures. We consequently propose a mechanism by which automatic, temporally-specific sensory simulation yields an information envelope read out by motor areas when estimating temporal characteristics in our environment.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P3">Relevant sensory information is often embedded within predictable temporal structure. Music and speech provide prominent examples of quasi-rhythmic sensory streams that require temporal parsing before we can appreciate their content [<xref ref-type="bibr" rid="R1">1</xref>]. One popular view that has emerged from research into how the brain accomplishes this task is that neural oscillations temporally align with sensory temporal patterns to support perceptual processing [<xref ref-type="bibr" rid="R2">2</xref>–<xref ref-type="bibr" rid="R4">4</xref>]. Given the example of music, spectral peaks in neural activity thereby flexibly shift depending on the beat meter, such that moments of maximal neural sensitivity become aligned with likely peaks in sensory information [<xref ref-type="bibr" rid="R5">5</xref>]. This account has been supported by a range of ‘oscillatory tracking’ evidence - greater neural phase consistency in the frequency band corresponding to the presented information [<xref ref-type="bibr" rid="R6">6</xref>–<xref ref-type="bibr" rid="R8">8</xref>]. Recent advances have additionally been made to dissociate these supposedly oscillatory tracking signatures from evoked responses to the presented events, in part by showing that they are still detectable in periods following the sensory stimulation [<xref ref-type="bibr" rid="R9">9</xref>–<xref ref-type="bibr" rid="R11">11</xref>]. The degree of such neural tracking has in turn been shown to determine our ability to perceive the input stream [<xref ref-type="bibr" rid="R9">9</xref>, <xref ref-type="bibr" rid="R10">10</xref>], supporting the idea that these mechanisms optimize perception of our continuously fluctuating world.</p><p id="P4">The visual domain, in contrast, has typically investigated how theta (and alpha) band oscillations shape visual perception independently of external dynamics [<xref ref-type="bibr" rid="R12">12</xref>–<xref ref-type="bibr" rid="R18">18</xref>]. As a result, the comparatively small literature that has focused on how qualitatively distinct low-frequency visual rhythms are tracked in the brain [<xref ref-type="bibr" rid="R19">19</xref>, <xref ref-type="bibr" rid="R20">20</xref>] is yet to address the supporting mechanisms including whether true oscillatory tracking signatures exist independently of evoked potentials. Notably, the human visual system appears to be tuned more towards spatially – rather than temporally – precise computation, which has led to proposals that oscillatory tracking may indeed operate differently in vision than it does in audition [<xref ref-type="bibr" rid="R21">21</xref>]. However, rhythms, and temporal structure in general, shape our visual environment much like its auditory counterpart, especially as a consequence of interacting with our environment – like when typing on a keyboard or walking along a road [<xref ref-type="bibr" rid="R22">22</xref>]. It is therefore crucial to determine whether insights from auditory research can be extended to the visual domain. Should a similar mechanism appear to support rhythmic tracking across modalities, this would underline the need for accounts that combine intrinsic and externally-based sources of rhythm to determine sampling [<xref ref-type="bibr" rid="R23">23</xref>].</p><p id="P5">As well as asking whether true oscillatory tracking can be found in vision, we further examined the mechanisms that may support it. First, although it appears likely from audition that attention modulates the level of tracking [<xref ref-type="bibr" rid="R3">3</xref>], it remains unclear whether this is a function of the stimulus stream being relevant to the task or explicit prediction of its temporal structure [<xref ref-type="bibr" rid="R9">9</xref>, <xref ref-type="bibr" rid="R24">24</xref>]. For example, in audition, oscillatory tracking enables the resolution of competition between several co-occurring auditory streams – like in so-called ‘cocktail party’ situations [<xref ref-type="bibr" rid="R25">25</xref>] – but it is unclear whether the modulation is driven by attention to the sensory stream of interest as a whole or specifically to its temporal features. A distinction between the two is important in order to show which tracking signatures subserve specifically <italic>temporal</italic> prediction [<xref ref-type="bibr" rid="R3">3</xref>, <xref ref-type="bibr" rid="R9">9</xref>, <xref ref-type="bibr" rid="R25">25</xref>].</p><p id="P6">Second, we were interested in determining the representational content of the tracking response as there is evidence that neural templates of anticipated content appear – and therefore potentially change dynamically over time – without the overall signal amplitude necessarily increasing [<xref ref-type="bibr" rid="R26">26</xref>–<xref ref-type="bibr" rid="R28">28</xref>]. For example, some research suggests that activating one representation may suppress others, perhaps via lateral inhibition [<xref ref-type="bibr" rid="R29">29</xref>, <xref ref-type="bibr" rid="R30">30</xref>]. If oscillatory tracking mechanisms like entrainment encoded temporal predictions in a feature-specific manner, we would therefore not expect whole-scale neural activity to become coupled to the external rhythm but rather activated representations of the changing feature that constitutes the rhythm. If this were the case, examinations of temporal tracking may be informed via content-based, rather than purely amplitude-based, measures, and the literature concerning predictive processing may yield informative insights for mechanisms supporting oscillatory processing.</p><p id="P7">To this end, we conducted a magnetoencephalography (MEG) study in which participants attended to rhythmic visual sequences of rotating Gabor patches and subsequently judged the orientation or timing of a delayed probe. Using a rate-specific phase-locking analysis, we show that the motor system tracks visual rhythms, even when they are no longer concurrently present. Importantly, this maintained tracking response only appears in the timing task and predicts perception of temporal features of the probe stimulus. Using time-resolved decoding analyses, we further show temporally-specific visual simulation of rhythmic predicted sensory outcomes regardless of task, but whose extent predicts motor tracking when deriving time. These results show that true rhythmic tracking extends to the visual modality and provide neural evidence that this capacity is related to internal sensory simulation of perceptual content. On a larger scale, our findings highlight the need for temporal tracking accounts that consider how we combine environmental dynamics with intrinsically-generated rhythmic processes, and that the study of temporal tracking can take logic from the predictive processing literature to move understanding forwards.</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>Participants successfully maintain representations of stimulus orientation and timing</title><p id="P8">Thirty human participants observed a series of 7-8 flashing Gabor patches, whose onsets were separated by 500 or 750ms - creating rhythmic displays at 2 and 1.33Hz respectively. From a random starting point, the orientations progressed in steps of 15, 30 or 45 degrees in either the clockwise or counterclockwise direction. The final Gabor of each trial’s stimulus series was surrounded by a black circle at its perimeter to signal the start of a ‘Maintenance’ period. This Maintenance period lasted three cycles of the stimulus presentation rate, during which participants were shown a fixation cross. At the end of each trial, a final Gabor patch served as the probe for participants’ responses to judge orientation or timing.</p><p id="P9">In orientation blocks, participants judged whether the probe Gabor had the orientation that would have immediately followed the final orientation of the sequence. In timing blocks, they judged whether probes appeared at an ‘on-beat’ timepoint if the preceding rhythm had continued. On both dimensions and orthogonally to each other, probes were ‘off’ on 22% of trials. The stimuli could be either half a frequency cycle too early or too late and rotated either one step size too little or too far relative to the orientation that should have been predicted (described above). Stimuli were identical regardless of block, but what differed was the judgement. The timing task therefore directly required participants to maintain the rhythm and, given the contrast with the orientation task, allowed us to determine what aspect of rhythmic tracking relies on explicit attention to temporal properties of stimulus appearance. Participants were able to perform both tasks at both stimulation rates with above-chance sensitivity (one-sample <italic>t</italic>-test of <italic>d</italic>-<italic>primes</italic> against 0: all p&lt;0.001).</p></sec><sec id="S4"><title>Motor regions support timing judgements through rate-specific tracking beyond ongoing visual stimulation</title><p id="P10">Like similar work in the auditory domain, we first operationalized oscillatory tracking as inter-trial phase consistency (ITPC) [<xref ref-type="bibr" rid="R9">9</xref>]. ITPC leverages that all trials – within each of our stimulation rates – have identically timed stimuli. If neural oscillations became phase-aligned with the sensory rhythm as proposed, this would yield large ITPC in specifically the neural frequency band that matches the stimulation rate. To capture this rate-specificity driven by stimulation rates, we contrasted ITPC between the two rates of stimulation to create a rate-specific response (RSR) that precisely captures how much more ITPC, and therefore tracking, is apparent at the trial’s stimulated compared to non-stimulated frequency (<xref ref-type="fig" rid="F1">Figure 1b</xref>) [<xref ref-type="bibr" rid="R9">9</xref>]. During the Stimulation window, participants showed a scalp-wide RSR in both the timing (cluster-corrected, p &lt; 0.001; summed t = 3118; 272 sensors) and orientation task (cluster-corrected, p &lt; 0.001; summed t = 3086; 272 sensors). Both conditions revealed a similar topography that spanned all 272 planar-transformed MEG sensors and was, as expected, maximal at occipital sensors (<xref ref-type="fig" rid="F2">Figure 2a</xref>, left panel, top and middle). No cluster was found for the difference between the two tasks in the Stimulation window (<xref ref-type="fig" rid="F2">Figure 2a</xref>, left panel, bottom).</p><p id="P11">To distinguish oscillatory tracking from evoked responses during rhythmic stimulation, we tested for an RSR in the absence of visual stimuli during the Maintenance window between the rhythmic sequence and the probe (<xref ref-type="fig" rid="F1">Figure 1a, b</xref>). We found a cluster of significant RSR in the timing task that had a central topography spanning frontal, parietal and temporal sensors bilaterally, cluster-corrected, p &lt; 0.001; summed t = 239.426; 86 sensors (<xref ref-type="fig" rid="F2">Figure 2a</xref>, right panel, top). Meanwhile, no significant cluster was found in the orientation task (<xref ref-type="fig" rid="F2">Figure 2a</xref>, right panel, middle). Comparing the two tasks directly, a central cluster emerged in which maintained RSR values were significantly higher in the timing than in the orientation task, cluster-corrected, p = 0.019; summed t = 44.93; 17 sensors (<xref ref-type="fig" rid="F2">Figure 2a</xref>, right panel, bottom). All sensors belonging to this task difference cluster were also part of the cluster found in the timing task alone. Using an LCMV beamformer in combination with a probabilistic atlas to localise this tracking signal ([<xref ref-type="bibr" rid="R31">31</xref>], see <xref ref-type="sec" rid="S7">Methods</xref>), we found the maintained timing task cluster to be maximal in the right paracentral lobule and supplementary motor area (<xref ref-type="fig" rid="F2">Figure 2b</xref>).</p><p id="P12">To establish the perceptual relevance of this signal, we next extracted participant-specific maintained RSR values from this cluster and averaged over its sensors to create a single value per participant. We did so separately for each task and then correlated participants’ RSRs with their behavioural task performance (d-prime). We found a significant correlation between participants’ maintained RSR responses and their timing task performance, r = 0.385, p = 0.036, while no such correlation was found for the orientation task, r = -0.251, p = 0.182 (<xref ref-type="fig" rid="F2">Figure 2c</xref>) - in line with the absence of a maintained RSR in this condition. Comparing the bootstrapped z-distributions of the two correlation values indeed revealed a large difference between the two, Cohen’s d = 2.884 [<xref ref-type="bibr" rid="R32">32</xref>].</p><p id="P13">In sum, these results suggest that explicit tracking of rhythmic structure engages motor areas that become phase-locked to the tracked rhythm in a frequency-specific manner. This tracking maintains the external temporal structure beyond its presentation in a manner that predicts the detection of timing discrepancies.</p></sec><sec id="S5"><title>Temporally- and feature-specific sensory simulation appears regardless of task but scales rate-specific temporal tracking responses</title><p id="P14">We next investigated the presence of neural representations associated with a continuation of the rhythmically-rotating Gabors (<xref ref-type="fig" rid="F3">Figure 3a</xref>). Should tracking mechanisms align neural sensitivity with external temporal structure in a feature-specific way, we would expect representations of specifically the currently expected stimulus content – rather than overall neural amplitude – to change in a rate-specific manner. To this end, we trained an inverted encoding model (IEM) to convert MEG sensor-level activity into evidence for the orientation spectrum (see <xref ref-type="sec" rid="S7">Methods</xref>) [<xref ref-type="bibr" rid="R26">26</xref>, <xref ref-type="bibr" rid="R33">33</xref>]. We first confirmed that the IEM could decode truly presented Gabor orientations with high fidelity and that overt gaze position, measured using eye-tracking (see <xref ref-type="sec" rid="S7">Methods</xref>), could not account for decoding results (<xref ref-type="supplementary-material" rid="SD1">Figure S1b, d</xref>).</p><p id="P15">Since we were interested in decoding evidence reflective of a mechanism of rhythm maintenance, we again focused our analysis on the Maintenance window between the final sequence Gabor and the probe onset. We focused on specifically those orientations that would have appeared in each of the three ‘empty’ cycles of the maintenance window if the sequence had continued. We formalized our predictions about when those orientations should be represented most strongly in theoretical signed difference matrices (SDMs) and correlated these with empirical counterparts filled using the decoding traces from the IEM (<xref ref-type="fig" rid="F3">Figure 3c</xref>, also see <xref ref-type="sec" rid="S7">Methods</xref>). The resulting correlations thus represent the fit between participants’ data and our predictions about temporally-specific content representation.</p><p id="P16">Comparing participants’ fits to a label-permuted baseline (<xref ref-type="supplementary-material" rid="SD1">Figure S1a</xref>, also see <xref ref-type="sec" rid="S7">Methods</xref>), we found that participants showed such sensory ‘simulation’ in both the orientation, t(29) = 2.186, p = 0.037, and timing task, t(29) = 2.685, p = 0.012, without a significant difference between the two, t(29) = 0.342, p = 0.735 (<xref ref-type="fig" rid="F4">Figure 4a &amp; b</xref>). Source-localising the trained weight matrix of our IEM, we found the origin of this signal in occipital sources and specifically maximal along the left calcarine sulcus (<xref ref-type="fig" rid="F4">Figure 4c</xref>, see S1c for sensor-level). This measure of sensory simulation additionally correlated with the motor-based RSR signature in the timing task, r = 0.43, p = 0.018, but not the orientation task, r = -0.128, p = 0.501 (effect size of difference, d = 3.532, <xref ref-type="fig" rid="F4">Figure 4d</xref>). Further underlining that this represents a direct link with rhythmic tracking and not simply general attentiveness, we did not find an analogous correlation between the simulation and the presumably ERP-dominated occipital RSR during ongoing visual stimulation in the timing task, r = -0.112, p = 0.557, nor in the orientation task, r = 0.075, p = 0.695.</p><p id="P17">RSR was additionally estimated separately for the two windows of interest by averaging over the 86 sensors with the strongest RSR in each respective window. Note that the choice of 86 sensors was based on matching the number of sensors contributing to the significant cluster in the timing task’s Maintenance window cluster. <bold>e</bold> Potential mechanism based on our findings. Activity in sensory processing areas represents predicted stimuli in a temporally and feature-specific manner which yields an envelope of predicted, information-dense timepoints. Motor regions maintain this envelope through covert or overt movement and use it to flexibly coordinate temporally-sensitive processing.</p><p id="P18">These findings suggest an association between behaviourally-relevant temporal tracking in motor regions and ongoing sensory simulation of the tracked sequence in early visual areas. Note that the presence of mental simulation in the orientation task, in addition to the timing task, indicates that this process is not specific to temporal tracking but rather appears regardless of task.</p></sec></sec><sec id="S6" sec-type="discussion"><title>Discussion</title><p id="P19">We demonstrate phase-coupling of motor cortex activity that tracks the temporal structure of low frequency visual rhythms specifically when participants performed explicit temporal judgements. Visual regions meanwhile dynamically represent the concurrently predicted content of the rhythm in a temporally-precise manner across both tasks, but also in a manner that is associated with the motoric tracking response when deriving time. We thus show neural representation of visual temporal structure that consists of two concurrently unfolding components. On the one hand, the visual system automatically simulates the anticipated content of predictable events at their inferred timepoint of occurrence. Elsewhere, the motor system leverages this simulation by effectively maintaining an amplitude readout of the inferred information envelope when time is relevant to the task at hand (<xref ref-type="fig" rid="F4">Figure 4e</xref>).</p><p id="P20">Interestingly, the involvement of motor regions, and specifically the supplementary motor area (SMA), in temporal tracking is in line with recent findings in the auditory domain [<xref ref-type="bibr" rid="R34">34</xref>–<xref ref-type="bibr" rid="R40">40</xref>]. Therefore, the visual domain may similarly capitalize on predictable temporal structure because the underlying mechanism is orchestrated by a centralised motor hub. Our proposed role of the relationship between motor and sensory processing is also consistent with recent work in which monkeys were trained to track a left-to-right moving visual metronome, where neural components reflecting spatial and tempo tracking were dissociated [<xref ref-type="bibr" rid="R41">41</xref>]. Local field potential power fluctuations were predominantly determined by tempo in SMA (beta-band), but by space in more posterior regions like V4 (alpha and beta-bands). Our findings further support this tuned, multi-site internal simulation architecture, and additionally show that rather than simply operating in parallel, sensory simulation and temporal tracking modulate each other in the human brain. Importantly, our findings also directly link internal simulation to delta-band phase locking, which forms a bridge with recent theories of human temporal tracking.</p><p id="P21">The involvement of the motor system in temporal tracking has led to elegant proposals of simulated action as the basis for temporal predictions [<xref ref-type="bibr" rid="R34">34</xref>–<xref ref-type="bibr" rid="R40">40</xref>]. These commonly state that the motor system takes its central role since it orchestrates sensory predictions as the consequence of (simulated) actions through efference copies [<xref ref-type="bibr" rid="R34">34</xref>, <xref ref-type="bibr" rid="R37">37</xref>]. However, our results clearly show that prediction-based sensory simulation appears regardless of motor-based temporal tracking and is therefore unlikely to be its result. Although a true test of directional causality must be the aim of future – potentially interventional – research, our findings are instead more consistent with a new possibility that we propose. That is, the seemingly causally-inverse interpretation that sensory systems continuously represent predictions of future sensory states without relying on the motor system, but that these representations are read out by the motor system only if we must directly extract temporal information (<xref ref-type="fig" rid="F4">Figure 4e</xref>, also see [<xref ref-type="bibr" rid="R41">41</xref>]). This interpretation is consistent with behavioural work showing a dependence of temporal predictions on explicit spatial predictions [<xref ref-type="bibr" rid="R42">42</xref>], as well as recent high-field fMRI work showing that duration and rate-tuned neural responses arise as processing ascends the visual processing hierarchy and are presumably derived from early, feature-tuned visual responses [<xref ref-type="bibr" rid="R43">43</xref>]. At the top of said hierarchy, we suggest that motor regions maintain the predicted information envelope as a potential mental model of external temporal structure. Under this view, temporal predictions are simply derivatives of this simulated information envelope, and sensory predictions are their necessary bedrock.</p><p id="P22">Following this interpretation, the success of temporal tracking scales with the presence of temporally-precise predictions of the stimulus feature that uniquely changes with each cycle of the tracked rhythm. In our case, the temporal prediction of one frequency cycle coincided with the predictable change in orientation angle that came along with it. In the previously mentioned study by de Lafuente and colleagues it was the spatial location of the visual metronome [<xref ref-type="bibr" rid="R44">44</xref>].</p><p id="P23">The sensory predictions upon which the brain potentially bases temporal predictions thus seem to be feature-specific rather than constituting overall stimulus presence [<xref ref-type="bibr" rid="R45">45</xref>]. Whether that is a requirement or not must be addressed in future studies but it raises the question of how temporal predictions are formed in situations that do not allow for feature-specific sensory predictions. Following this notion, the link between temporal tracking and feature-specific sensory predictions could potentially offer new views on certain clinical conditions. For example, it may provide an explanation for the reduced phase-synchronization to speech in those with dyslexia [<xref ref-type="bibr" rid="R46">46</xref>, <xref ref-type="bibr" rid="R47">47</xref>]. The communication of phonological representations from primary auditory to higher-order areas has been found to be impeded and so the motor system may not be able to derive the temporal structure needed for phase-synchronization to occur [<xref ref-type="bibr" rid="R48">48</xref>]. More generally, sufficiently precise sensory predictions being a prerequisite for effective temporal tracking could reveal an underutilized explanation for why clinical conditions associated with aberrant prediction strengths such as schizophrenia are also characterized by altered time perception [<xref ref-type="bibr" rid="R49">49</xref>].</p><p id="P24">The interpretation of temporal predictions as derived from content-based sensory predictions does not conflict with the idea that the motor system influences sensory processing too. This has been the focus of the literature on auditory temporal tracking which has suggested that the motor system’s propensity to synchronize overt and covert action to sensory events leads to improved perception [<xref ref-type="bibr" rid="R37">37</xref>]. Our finding that motor-based temporal tracking supports performance in visual timing judgements is consistent with these effects. Sensory predictions scaffolding motor-based temporal predictions which in turn shape sensory processing instead support the view of an action-perception-loop. Theories like active sensing outline how movement not only passively reflects temporal structure but also reorients sensors to sample the environment in a maximally informative way [<xref ref-type="bibr" rid="R50">50</xref>, <xref ref-type="bibr" rid="R51">51</xref>].</p><p id="P25">In line with these theories, our findings may also suggest that manipulating the time course of sensory predictions should consequently affect the rate at which we overtly sample the environment. For example, focusing on specifically ocular movement, recent accounts have highlighted the overlap between typical saccade rates and visual theta (~ 4Hz) rhythms that are found when stimulation is not itself changing at that particular rate [<xref ref-type="bibr" rid="R52">52</xref>], with most suggesting that this is due to the common, fixed theta rate functioning of the frontoparietal network [<xref ref-type="bibr" rid="R53">53</xref>]. However, as we recently proposed in a theoretical piece [<xref ref-type="bibr" rid="R23">23</xref>], this rate could instead reflect a ‘stubborn’ visual prior, for example resulting from a lifetime of saccading at approximately that rate when exploring our environment. Once reliable temporal structure appears besides this canonical rate, such as through sensory rhythms in our study, the rate of this rhythm and all its corollary effects may flexibly shift to retain a maximally informative sensory sampling routine in the current environment. We therefore suggested that the resulting perceptual rhythm may reflect a dynamically-updated, precision-weighted combination of various estimates of temporal information, some learned over a lifetime and some more recently [<xref ref-type="bibr" rid="R23">23</xref>, <xref ref-type="bibr" rid="R54">54</xref>]. Future research could investigate this proposal by testing whether overwriting the theta default using reliably predictable structure leads to systematic changes in the commonly probed oscillatory profile of behavioural performance.</p><p id="P26">These new ideas show the potential explanatory power of bringing logic from the predictive processing literature to the understanding of how we represent the temporal characteristics of our environment (see also [<xref ref-type="bibr" rid="R55">55</xref>]). The current study’s findings further showcase the strength of this cross-pollination at the empirical level. Much debate has recently focused on distinguishing the oscillatory phenomenon of entrainment from other neural signatures. Quite rightly, many effects attributed to entrainment are argued to provide poor evidence because they are measured during periodic stimulation, and such measures could therefore simply reflect periodic evoked responses. Isolating oscillatory tracking at the rate of stimulation in a period afterwards, as we have done, has been proposed as a solution to revealing the nature of the underlying mechanism. Our findings in visual regions, namely the lack of overall amplitude changes tracking the rhythm but instead activated representations of specifically predicted content, suggest that an exclusive focus on overall sensory neural amplitude changes may be misleading. Notably, this suggestion is in line with past findings that auditory entrainment may be tonotopically-specific [<xref ref-type="bibr" rid="R7">7</xref>, <xref ref-type="bibr" rid="R56">56</xref>]. More broadly, it reflects the value of oscillatory tracking and predictive processing research taking inspiration from each other’s insights – both in terms of mechanistic explanations and empirical methodology – to facilitate progress.</p><p id="P27">In sum, this study provides novel insights into how the human brain tracks temporal structure in its environment. Complementing past research in the auditory domain, we found rate-specific phase coupling between delta oscillations in the motor system and a presented visual rhythm - supporting the interpretation of motor regions as a timing hub when required to extract temporal features from our environment. Importantly, this tracking response was maintained beyond the offset of the visual rhythm and influenced participants’ sensitivity to timing violations. Most interestingly, this tracking response was scaled by the degree to which participants extrapolated the predictable visual sequence by means of feature and temporally-specific visual simulation, which occurred regardless of task. These findings suggest a coordinated interaction of sensory and motor simulation for representing the intricate temporal relations of our environment.</p></sec><sec id="S7" sec-type="materials | methods"><title>Materials and Methods</title><sec id="S8" sec-type="subjects"><title>Participants</title><p id="P28">Thirty-one participants were screened for MEG eligibility and gave informed consent in line with the UCL ethics committee (protocol 3090/004). Participants were recruited through the local SONA subject pool, reimbursed in either credit or cash (£10/hour) and required to have normal or corrected-to-normal vision. One participant was excluded from analyses as they did not complete the experiment, leaving a sample of 30 participants (24 females; mean age = 24.81, SD = 4.39 years; 30 being our preregistered sample size: <ext-link ext-link-type="uri" xlink:href="https://osf.io/8ptsz">https://osf.io/8ptsz</ext-link>).</p></sec><sec id="S9"><title>Stimuli and Task</title><p id="P29">Gabor patches were created using the Psychophysics Toolbox implemented in MATLAB, with 12° of visual angle diameter and a spatial frequency of 0.4 per visual angle [<xref ref-type="bibr" rid="R57">57</xref>, <xref ref-type="bibr" rid="R58">58</xref>]. Annuli were created by overlaying a background-coloured (grey) circle on its centre that spanned 0.43° of visual angle and contained a black fixation cross of 0.35° of visual angle. Each Gabor was presented for 200ms each and the first in the sequence started at one of the 12 possible orientations that result from equally dividing the 0-165° space using steps of 15°.</p><p id="P30">On 78% of trials within each block, this probe appeared in time with the preceding rhythm: exactly four frequency cycles from the onset of the final entrainer of the flashing series. On the remaining 22% of trials, it appeared either half a frequency cycle too early or too late (counterbalanced across trials within each block). Furthermore, and independently of this timing manipulation, on 78% of the trials, the probe showed the result of adding the fixed amount of rotation from the preceding series to its final patch. On the other 22% of trials, the probe showed either the same orientation as the final patch of the series or two steps added to it. Before each of the blocks, participants were told to either judge timing or orientation through text on the screen. In the former, they were asked to press a button when the probe did not appear in line with the preceding rhythm. In the latter, they were asked to press the button when the probe did not show the correct (next following) orientation. They could respond as soon as the probe appeared and had 1500 ms to do so. The next trial started approximately 1250 ms (SD = 300 ms) after the end of the previous trial’s response window. There were three blocks of each stimulation rate and task combination, with presentation order approximately balanced across participants.</p><p id="P31">All combinations of levels of the other trial variables – number of entrainers, step size, direction and starting angle of orientation rotation as well as direction of deviant probes (forward or backward in either rotation or time) – were balanced across trials within a participant and presented in a randomised order.</p><p id="P32">The study consisted of two testing sessions. The first was a behavioural practice session of one hour in which participants learned and practised both tasks. Given sufficient performance in both tasks as well as MEG eligibility, participants were invited for a scanning session on a different day. If, due to scheduling constraints, the scan occurred more than a week after behavioural practice, participants practiced both tasks again for a combined half hour before entering the scanner. In the scanner, participants completed three blocks of 36 trials for each rate and task combination resulting in 108 trials per condition. We instructed participants to stay still citing MEG data quality issues as the reason but also to prevent overt movement along with the stimuli. Participants were monitored during scanning using live video transmission. Following the scan, participants completed a finger tapping task designed to measure their natural motor rates. This measure was however recorded for a different research question and was not analysed for the current study.</p></sec><sec id="S10"><title>MEG acquisition and preprocessing</title><p id="P33">MEG data acquisition occurred in a magnetically shieled room using a 272-channel CTF MEG system with axial gradiometers sampling at 600Hz. A third-order gradient compensation algorithm was applied online for noise reduction. Fiducial coils on the nasion as well as on the left and right preauricular were used to monitor the participant’s head position during data acquisition. A photodiode at the bottom left of the screen was used to temporally align the neural signal with the on-screen stimulus presentation. Alongside MEG acquisition, an EyeLink 1000 infrared tracker (SR Research Ltd.) was used to record eye movements.</p><p id="P34">We further preprocessed the MEG data using the Fieldtrip toolbox as implemented in MATLAB [<xref ref-type="bibr" rid="R59">59</xref>]. Visual artifact rejection was carried out using trial-wise MEG sensor variance as the selection criterion for further manual inspection. Trials were removed when they showed excessive artifacts. The eye-tracking signal was further used to flag trials in which blinks occurred during stimulus presentation. The resulting data were then temporarily downsampled to 200Hz and a 1-40Hz bandpass-filter (two-pass Butterworth) was applied before conducting an independent components analysis [<xref ref-type="bibr" rid="R60">60</xref>]. The first 50 components’ topographies and time courses were visually inspected seeking cardiac and eye-related artifacts, whose components were removed. The resulting projections were applied to the original, not filtered nor downsampled, data.</p></sec><sec id="S11"><title>Behavioural measures</title><p id="P35">Behavioural analyses were conducted in MATLAB and focused on the signal detection theoretic (SDT) measure of d-prime [<xref ref-type="bibr" rid="R61">61</xref>]. It is calculated as the difference between participants’ normalized hit and false alarm rates within a condition and reflects their sensitivity for detecting oddballs in each task.</p></sec><sec id="S12"><title>Rate-specific intertrial phase consistency</title><p id="P36">All MEG-related analyses were conducted in MATLAB using the Fieldtrip toolbox. For the following analyses concerning phase consistency, we used custom scripts adapted from [<xref ref-type="bibr" rid="R9">9</xref>]. We preregistered the analyses mentioned in this section together with the planned sample size, experimental variables and hypothesis on Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/jm6er">https://osf.io/jm6er</ext-link>) before starting data collection. Inspired by past work, we used rate-specific inter-trial phase consistency (ITPC) as a measure of oscillatory tracking. When stimulated at a particular frequency, the neural phase of that frequency should be aligned with the timepoints of stimulus appearance. Making use of the fact that all trials in our design within a frequency follow the same time course, this means that, at a given timepoint, a tracked frequency should show a higher ITPC than the non-tracked frequency. To directly make this comparison, we used the rate-specific response (RSR) proposed by [<xref ref-type="bibr" rid="R9">9</xref>]. It quantifies how much more ITPC there is to a given trial’s stimulation frequency (e.g. 2Hz) compared to the other, non-stimulated frequency (1.3Hz). This approach controls for any non-experimental, spectral differences between the stimulation rates.</p><p id="P37">To calculate ITPC, preprocessed data were low-pass filtered at 20Hz (two-pass Butterworth) to mitigate the influence of movement-related artifacts. The axial gradiometer data were then subjected to a planar transform, yielding two components for each of the 272 sensors. ITPC calculation was carried out on each of the resulting 544 components separately, after which the two components belonging to each sensor were averaged to restore the original gradiometer structure. We used a sliding-window fast fourier transform (FFT) to transform the data into the frequency domain. The window slid in steps of 20ms with its width set to 1500ms to strike a balance between sufficient resolution to distinguish between our stimulation rates 1.33Hz (2 cycles in 1500ms) and 2Hz (3 cycles), and temporal specificity. The phase angle was extracted for both frequencies of interest for each of the timepoints upon which the FFT window was centred. The complex mean of the resulting trial-by-trial phase was then computed separately over all trials of each of the four (rate by task) conditions before taking the absolute value to yield each condition’s ITPC. To further account for non-oscillatory 1/f-components that exist in the frequency spectrum of neural data and that could bias the RSR, we fitted and subtracted a 1/f curve from the ITPC values for each participant, sensor and condition separately before calculating RSR based on the residuals [<xref ref-type="bibr" rid="R9">9</xref>].</p><p id="P38">Since the number of Gabors in each flashing sequence was the same for both stimulation frequencies – 7 or 8 – the trials in the respective conditions were of different lengths. To align the two, we centred trials of both frequencies to the onset of their final Gabor of the sequence. Then, the longer 1.3Hz trial time courses were cut to the length of the 2Hz trials which allowed us to calculate RSR values for each sensor, task, participant and timepoint upon which the FFT was centred. The result of this approach can be seen in the following definition of our windows of interest where the earliest and latest timepoints resemble the bottleneck imposed by the length of the shorter 2Hz trials.</p><p id="P39">To distinguish between oscillatory tracking during and after the offset of external stimulation, we defined two analysis windows of interest. The first window (‘Stimulation’) spanned from 2250ms before each sequence’s final Gabor to its onset, noting that these timepoints represent the centre of the respective FFT window during phase extraction. The second analysis window (‘Maintenance’) took the maximal length of data within the three cycles of the Maintenance period while avoiding contamination from evoked responses stemming from the offset of stimulation sequence and the onset of the probe. The FFT window centres therefore spanned from 750ms to 1250ms after the onset of the final Gabor. Within each analysis window, RSR values were averaged so that task and window-wise results from all participants could be statistically evaluated in the following way.</p><p id="P40">Cluster-based permutation dependent-samples t-tests were used to both compare the two tasks to one another and to evaluate whether each condition’s individual RSR is significantly different (two-sided) from a baseline of 0. Previous work supports this analysis approach by showing that RSR values are normally distributed and that a surrogate distribution based on adding random phase values to the FFT output before calculating ITPC is indeed centred around 0 [<xref ref-type="bibr" rid="R9">9</xref>]. We used a Monte-Carlo algorithm with 5000 permutations for each planar-transformed MEG sensor separately. Clusters were formed by neighbouring (minimum 2) sensors which individually reached significance (alpha = 0.05). The resulting cluster-level statistic represents the sum of the t-values of the sensors within the cluster and is tested for significance (cluster-alpha = 0.05).</p></sec><sec id="S13"><title>Source localisation</title><p id="P41">In order to outline the source of the RSR signal, as well as for visualization purposes, we conducted source localisation by using a template approach as implemented in Fieldtrip. Using participants’ fiducial measurements and the MEG sensor structure, we warped a standard Fieldtrip single shell head model and a standard Fieldtrip source model (8mm dipole spacing) into participant-specific approximations. We estimated separate spatial filters for the Stimulation and Maintenance windows described above using LCMV-beamformers after combining the data from the two stimulation rates [<xref ref-type="bibr" rid="R62">62</xref>]. We applied the spatial filters to FFT-transformed single-trial data before extracting the phase and calculating ITPC and RSR as in the sensor-level data described above. We used the AAL atlas implemented in Fieldtrip for probabilistic anatomical labeling [<xref ref-type="bibr" rid="R31">31</xref>].</p></sec><sec id="S14"><title>Orientation decoding</title><p id="P42">We used an inverted encoding model (IEM) to decode sensory representations, specifically an implementation used in previous work that was able to decode predicted grating orientations in the absence of ongoing stimulus presentation. We will provide a condensed description of the IEM here but for a full implementation, see [<xref ref-type="bibr" rid="R26">26</xref>] and [<xref ref-type="bibr" rid="R33">33</xref>], from which we used custom MATLAB scripts.</p><p id="P43">At its core, the approach includes two steps. First, a forward model is trained to model the MEG sensor-level activity evoked by different Gabor orientations. Specifically, this model consists of 32 equally-spaced channels whose Von Mises tuning curves cover the entire orientation spectrum and can be linearly combined to reproduce any single orientation. Once trained, the transition from orientation to MEG data is described by a weight matrix that captures each MEG sensor’s ‘orientation tuning’ while accounting for the noise covariance of neighbouring sensors. Note that we examined this weight matrix to probe which sensors contribute to the decoder (<xref ref-type="fig" rid="F4">Figure 4c</xref> &amp; <xref ref-type="supplementary-material" rid="SD1">S1c</xref>). In the second step of the IEM, this filter is applied in the opposite direction to create an inverse model that converts MEG data into evidence for the orientation spectrum.</p><p id="P44">In this study, we used the first two Gabors of each trial’s flashing sequence to train the forward model. We chose the first two Gabors because their orientation could not be predicted; after the second Gabor is presented one can anticipate subsequent Gabor orientations because the rotation direction and stepsize for that trial have already been determined. We extracted two 500ms epochs from each preprocessed trial, starting at the respective onsets of the first and second Gabor of the sequence. We also applied a 40Hz low-pass filter with a filter order of 6, in line with past research [<xref ref-type="bibr" rid="R26">26</xref>]. This process yielded an average of around 840 (SD = 34) training examples (~ 70 per orientation class) per participant. The model was trained at each training data timepoint, separated by 5ms. While doing so, the data were averaged within windows of 29.2ms using a sliding window approach to improve the signal-noise-ratio at each training timepoint [<xref ref-type="bibr" rid="R26">26</xref>]. The resulting time-resolved filter matrix was then applied to our test data.</p><p id="P45">We again focused our analysis on the window between the final entrainer and the probe onset. Specifically, we extracted the epoch between each trial sequence’s final Gabor and 1000ms after the probe onset and again applied a 40Hz low-pass filter with a filter order of 6. We estimated the orientation channel responses of these test data at each timepoint, separated by 5ms and averaged within a window of 29.2ms - which yielded a 4D matrix (training time X testing time X model channel X trial) containing the estimated channel responses during the window of interest for each trial. Channel responses were consequently shifted so that the channel centred on the orientation of interest (see below) occupied the position of the 0°-centred channel. This allowed averaging over trials to result in a decoding output of the orientation of interest which was consequently transformed into a scalar projection that was used as the measure of decoding performance. Trial-averaging was done separately for each condition (orientation/timing) and rate (1.3Hz/2Hz).</p><p id="P46">We determined four orientations of interest for every trial. Namely, based on each trial’s orientation sequence parameters (starting orientation, rotation direction and rotation step size), we determined the three orientations that would have appeared in the three cycles of the Maintenance window if the sequence had continued. These could subsequently be used to establish whether participants simulated this continuation. The fourth orientation of interest was the presented probe orientation, which was used to establish the fidelity of the decoding approach (<xref ref-type="supplementary-material" rid="SD1">Figure S1b</xref>). We focused decoding analyses exclusively on non-oddball trials since unexpected presentations or omissions could infringe on the analysis window or lead to divergent responses.</p></sec><sec id="S15"><title>Rate-specific sensory simulation analysis</title><p id="P47">Within the decoding output, we selected a training time window from 120 to 160ms after stimulus onset, based on past research showing optimal decoding sensitivity for similar stimuli and presentation durations in this window, and averaged the decoding output within this window [<xref ref-type="bibr" rid="R26">26</xref>]. We first tested the approach’s fidelity using the probes presented in both tasks to establish that subtle orientation signals can be decoded. Then, the decoding output of each trial was narrowed to the extrapolated orientations and the time between the onsets of the final sequence Gabor and the probe. This resulted in windows of 2000 or 3000ms for decoding output with 2 or 1.3Hz stimulation rate respectively. This yielded three decoding traces for each trial, one for each extrapolated orientation. To focus on delta-rate representation changes and to improve the signal-to-noise ratio, we next smoothed these traces by averaging within a 150ms sliding window. Then, to avoid the analysis being contaminated by temporally-consistent biases towards a particular orientation, for example towards the orientation of the final Gabor of the sequence or the probe, we normalized each decoding trace by z-scoring. This allowed us to examine relative temporal changes in representations of the extrapolated orientations in an unbiased manner.</p><p id="P48">We next defined three timepoints of interest within each decoding trace as the predicted onsets of the extrapolated gratings – one at the beginning of each empty cycle of the Maintenance window. For example, for 2Hz trials, this corresponded to the timepoints 500, 1000 and 1500ms after the onset of the final Gabor of the flashing sequence. We will call these timepoints 2π, 4π and 6π (interval from the final Gabor onset in radians) and their associated orientations A, B and C. For each stimulation rate separately, we extracted the momentary decoding evidence for each combination of timepoint and decoded orientation before averaging over both stimulation rates within each timepoint. Note that the latter is possible since the timepoints of interest were chosen based on the common scale of radians.</p><p id="P49">We assessed whether the decoding evidence resembled the profile of rate and feature-specific mental simulation by using signed difference matrices (SDMs) in which the nine datapoints are related to one another in a pairwise manner. Specifically, we first created a theoretical SDM that captured the predicted differences between the datapoints if participants indeed showed a sensory simulation profile. Within each pair / cell of the matrix, a positive value (1) value was assigned if the row element was predicted to be larger than the column element and a negative value (-1) for the reverse. For example, orientation A should show higher decoding evidence at timepoint 2π than at 4π. Combinations for which we had no prediction, for example orientation A at 2π vs orientation B at 4π, received a 0 but were masked later anyway (see below).</p><p id="P50">Having quantified the model, we filled corresponding empirical SDMs for each participant with the pairwise differences between the momentary decoding evidence as described above. We did so for each task separately. To assess how well each participant’s empirical matrices were captured by the model, we flattened the off-diagonal triangle of each matrix into an array [<xref ref-type="bibr" rid="R63">63</xref>]. Then, like previously mentioned, we masked out the values of the resulting array for which we had no empirical prediction (zeros in the theoretical SDM). This produced two 18-item arrays, one binary and theoretical and the other continuous and empirical. Finally, we calculated the point-biserial correlation coefficient between these two arrays for each participant and task individually. The resulting fit value between -1 and 1 should be interpreted as the extent to which the model predictions of an internal simulation mechanism capture the observed data in each participant.</p><p id="P51">To determine whether such sensory simulation occurred at the sample level, we compared the fit values against a permutation-based baseline using two-sided paired-sample t-tests. This baseline was based on a label shuffling procedure in which, before filling each participant’s empirical SDM, we shuffled the labels of the three decoding traces. Although this only allows for five permutations outside of the true labelling, we elected against also shuffling the measured timepoints since this would artificially destroy temporal structure [<xref ref-type="bibr" rid="R64">64</xref>]. Decoding traces were consequently smoothed and normalized as usual before extracting the timepoints of interest to fill the empirical SDMs. For each participant, we correlated these permutation-filled empirical SDMs with the theoretical SDM before averaging over the permutations. This procedure represents specifically the null hypothesis that sensory simulation is not specific to the predicted feature-sequence.</p></sec><sec id="S16"><title>Orientation decoding source localisation</title><p id="P52">To determine the neural sources of the IEM-based decoding of Gabor orientations, we focused on the weight matrix of the forward model that maps MEG sensor activity onto the model’s hypothetical orientation channels for each training timepoint (see above). We used LCMV beamforming as we did for the ITPC analyses but this time created the spatial filter based on the epochs that served as training data for the IEM. We multiplied each training timepoint’s IEM weight matrix with the resulting source filter before taking the absolute value of the product and averaging over the model’s hypothetical orientation channels. Finally, we averaged each voxel’s model weight within our IEM training window of interest. To account for the centre of head bias that arises during source localisation, we repeated the above steps using a sensor-wise shuffled version of the IEM weight matrix so that the source filter was multiplied with a permuted weight matrix. Specifically, we ran 100 repetitions of this procedure, averaged the result and consequently z-scored the true weight matrix against this shuffled baseline. Since the permuted baseline still contains the centre of head bias after multiplication with the spatial filter but no longer any meaningful weights, this procedure results in a bias-free source-localisation of the IEM weight matrix.</p></sec><sec id="S17"><title>Gaze-based orientation decoding</title><p id="P53">To investigate whether varying Gabor orientation angles are associated with systematic differences in gaze position, we conducted a linear discriminant analysis (LDA) on the time windows that served as training epochs for the neural IEM (see above). Following the logic introduced by [<xref ref-type="bibr" rid="R65">65</xref>], we divided our orientation set (0:15:165) into a near-cardinal (0 degrees +/- 15 and 90 degrees +/- 15) and a near-oblique (45 degrees +/- 15 and 135 degrees +/- 15) subset. This allowed us to conduct two separate two-way classifications between orthogonal classes before averaging the two decoding accuracies into a single performance measure.</p><p id="P54">As mentioned, we focused on the epochs of data that were used to train the neural IEM since the presence of above-chance gaze-based decoding would reveal whether any of the sensory simulation results could, at least in part, be explained by overt gaze position. Eye-tracking data could not be collected for three participants since their head required positioning at a height inside the MEG helmet that did not allow capture of the eyes by the eye-tracker. We preprocessed the raw gaze data of the remaining 27 participants into epochs of 500ms exactly like for the neural IEM besides electing not to demean the gaze data in order to preserve their true location on the screen. We removed epochs in which blinks occurred (M = 29) which, on average, yielded approximately 811 trials (SD = 41) per participant for the following analyses.</p><p id="P55">To balance the training and testing data in terms of instances of our 12 orientation classes, we determined the class with the least instances and downsampled all other classes to that number (M = 67). We then used a leave-one-out cross validation (LOOCV) approach to train and test the LDAs on this data. For each of 20 LOOCV blocks, 95% of each classes’ downsampled instances contributed to the training data and the remaining 5% to the testing data. We then trained and tested the two (ordinal and oblique) classifiers using MATLAB’s built-in ‘classify’ function in a time-resolved manner, training and testing on each pairwise combination of the 95 training timepoints. Within each classifier, accuracy was operationalized as the proportion of correctly classified testing data examples before averaging over the two classifiers as described above. This was done for each of the 20 LOOCV blocks and repeated five times for each participant to avoid spurious results based on which instances of the majority classes were included or excluded during downsampling. Averaging over these repetitions resulted in 95 (training time) x 95 (testing time) generalization matrices of gaze-based decoding accuracy for each participant.</p><p id="P56">We extracted the diagonal of each participant’s generalization matrices where training and testing occurs at the same timepoint and used two-sided cluster-based permutation t-tests (10000 permutations) against the chance baseline of 0.5 to assess whether and when presented orientation could be significantly decoded from gaze data. Like in the RSR analyses, clusters were formed by, here temporally, neighbouring (minimum 2) data points which individually reached significance (alpha = 0.05). The resulting cluster-level statistic represents the sum of the t-values of the timepoints within the cluster and is tested for significance (cluster-alpha = 0.05).</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementaries</label><media xlink:href="EMS207018-suppement-Supplementaries.pdf" mimetype="application" mime-subtype="pdf" id="d29aAcEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S18"><title>Acknowledgments</title><p>The authors would like to thank the FIL’s imaging support team, particularly Dimitra Moraiti and Daniel Bates, as well as Dorottya Hetenyi for MEG scanning assistance. This work was supported by a Leverhulme Trust project grant (RPG-2022-358) and European Research Council (ERC) consolidator grant (101001592) under the European Union’s Horizon 2020 research and innovation programme, both awarded to CP. The Wellcome Centre for Human Neuroimaging was supported by core funding from the Wellcome Trust (203147/Z/16/Z).</p></ack><fn-group><fn fn-type="con" id="FN2"><p id="P57"><bold>Author contributions</bold></p><p id="P58">Conceptualization, A.K., Q.G., P.K., M.H.D, C.P.; data curation, A.K.; formal analysis, A.K.; funding acquisition, C.P.; investigation, A.K.; methodology, A.K., Q.G., P.K., M.H.D, C.P.; project administration, A.K., C.P.; software, A.K.; resources, C.P.; validation, A.K.; visualization, A.K.; writing – original draft, A.K.; writing – review and editing, A.K., Q.G., P.K., M.H.D, C.P.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patel</surname><given-names>AD</given-names></name></person-group><article-title>Language, music, syntax and the brain</article-title><source>Nat Neurosci</source><year>2003</year><volume>6</volume><pub-id pub-id-type="pmid">12830158</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Obleser</surname><given-names>J</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name></person-group><article-title>Neural Entrainment and Attentional Selection in the Listening Brain</article-title><source>Trends Cogn Sci</source><year>2019</year><volume>23</volume><issue>11</issue><fpage>913</fpage><lpage>926</lpage><pub-id pub-id-type="pmid">31606386</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakatos</surname><given-names>P</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Thut</surname><given-names>G</given-names></name></person-group><article-title>A New Unifying Account of the Roles of Neuronal Entrainment</article-title><source>Curr Biol</source><year>2019</year><volume>29</volume><issue>18</issue><fpage>R890</fpage><lpage>R905</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.07.075</pub-id><pub-id pub-id-type="pmcid">PMC6769420</pub-id><pub-id pub-id-type="pmid">31550478</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haegens</surname><given-names>S</given-names></name><name><surname>Zion Golumbic</surname><given-names>E</given-names></name></person-group><article-title>Rhythmic facilitation of sensory processing: A critical review</article-title><source>Neurosci Biobehav Rev</source><year>2018</year><volume>86</volume><fpage>150</fpage><lpage>165</lpage><pub-id pub-id-type="pmid">29223770</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doelling</surname><given-names>KB</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>Cortical entrainment to music and its modulation by expertise</article-title><source>Proc Natl Acad Sci U S A</source><year>2015</year><volume>112</volume><issue>45</issue><fpage>E6233</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1073/pnas.1508431112</pub-id><pub-id pub-id-type="pmcid">PMC4653203</pub-id><pub-id pub-id-type="pmid">26504238</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakatos</surname><given-names>P</given-names></name><etal/></person-group><article-title>Entrainment of Neuronal Oscillations as a Mechanism of Attentional Selection</article-title><source>Science</source><year>2008</year><volume>320</volume><pub-id pub-id-type="pmid">18388295</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakatos</surname><given-names>P</given-names></name><etal/></person-group><article-title>The spectrotemporal filter mechanism of auditory selective attention</article-title><source>Neuron</source><year>2013</year><volume>77</volume><issue>4</issue><fpage>750</fpage><lpage>61</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.11.034</pub-id><pub-id pub-id-type="pmcid">PMC3583016</pub-id><pub-id pub-id-type="pmid">23439126</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname><given-names>H</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>Phase patterns of neuronal responses reliably discriminate speech in human auditory cortex</article-title><source>Neuron</source><year>2007</year><volume>54</volume><issue>6</issue><fpage>1001</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.06.004</pub-id><pub-id pub-id-type="pmcid">PMC2703451</pub-id><pub-id pub-id-type="pmid">17582338</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Bree</surname><given-names>S</given-names></name><etal/></person-group><article-title>Sustained neural rhythms reveal endogenous oscillations supporting speech perception</article-title><source>PLoS Biol</source><year>2021</year><volume>19</volume><issue>2</issue><elocation-id>e3001142</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3001142</pub-id><pub-id pub-id-type="pmcid">PMC7946281</pub-id><pub-id pub-id-type="pmid">33635855</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kosem</surname><given-names>A</given-names></name><etal/></person-group><article-title>Neural Entrainment Determines the Words We Hear</article-title><source>Curr Biol</source><year>2018</year><volume>28</volume><issue>18</issue><fpage>2867</fpage><lpage>2875</lpage><elocation-id>e3</elocation-id><pub-id pub-id-type="pmid">30197083</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bouwer</surname><given-names>FL</given-names></name><etal/></person-group><article-title>A Silent Disco: Differential Effects of Beat-based and Pattern-based Temporal Expectations on Persistent Entrainment of Low-frequency Neural Oscillations</article-title><source>J Cogn Neurosci</source><year>2023</year><volume>35</volume><issue>6</issue><fpage>990</fpage><lpage>1020</lpage><pub-id pub-id-type="pmid">36951583</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gray</surname><given-names>MJ</given-names></name><etal/></person-group><article-title>Oscillatory recruitment of bilateral visual cortex during spatial attention to competing rhythmic inputs</article-title><source>J Neurosci</source><year>2015</year><volume>35</volume><issue>14</issue><fpage>5489</fpage><lpage>503</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2891-14.2015</pub-id><pub-id pub-id-type="pmcid">PMC4388917</pub-id><pub-id pub-id-type="pmid">25855167</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dugue</surname><given-names>L</given-names></name><name><surname>VanRullen</surname><given-names>R</given-names></name></person-group><article-title>Transcranial Magnetic Stimulation Reveals Intrinsic Perceptual and Attentional Rhythms</article-title><source>Front Neurosci</source><year>2017</year><volume>11</volume><fpage>154</fpage><pub-id pub-id-type="doi">10.3389/fnins.2017.00154</pub-id><pub-id pub-id-type="pmcid">PMC5366344</pub-id><pub-id pub-id-type="pmid">28396622</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Michel</surname><given-names>R</given-names></name><name><surname>Dugue</surname><given-names>L</given-names></name><name><surname>Busch</surname><given-names>NA</given-names></name></person-group><article-title>Distinct contributions of alpha and theta rhythms to perceptual and attentional sampling</article-title><source>Eur J Neurosci</source><year>2022</year><volume>55</volume><issue>11-12</issue><fpage>3025</fpage><lpage>3039</lpage><pub-id pub-id-type="pmid">33609313</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiebelkorn</surname><given-names>IC</given-names></name><name><surname>Kastner</surname><given-names>S</given-names></name></person-group><article-title>A Rhythmic Theory of Attention</article-title><source>Trends Cogn Sci</source><year>2019</year><volume>23</volume><issue>2</issue><fpage>87</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2018.11.009</pub-id><pub-id pub-id-type="pmcid">PMC6343831</pub-id><pub-id pub-id-type="pmid">30591373</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Helfrich</surname><given-names>RF</given-names></name><etal/></person-group><article-title>Neural Mechanisms of Sustained Attention Are Rhythmic</article-title><source>Neuron</source><year>2018</year><volume>99</volume><issue>4</issue><fpage>854</fpage><lpage>865</lpage><elocation-id>e5</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuron.2018.07.032</pub-id><pub-id pub-id-type="pmcid">PMC6286091</pub-id><pub-id pub-id-type="pmid">30138591</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Landau</surname><given-names>AN</given-names></name></person-group><article-title>Neuroscience: A Mechanism for Rhythmic Sampling in Vision</article-title><source>Curr Biol</source><year>2018</year><volume>28</volume><issue>15</issue><fpage>R830</fpage><lpage>R832</lpage><pub-id pub-id-type="pmid">30086315</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Re</surname><given-names>D</given-names></name><name><surname>Karvat</surname><given-names>G</given-names></name><name><surname>Landau</surname><given-names>AN</given-names></name></person-group><article-title>Attentional Sampling between Eye Channels</article-title><source>J Cogn Neurosci</source><year>2023</year><volume>35</volume><issue>8</issue><fpage>1350</fpage><lpage>1360</lpage><pub-id pub-id-type="pmid">37315334</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gomez-Ramirez</surname><given-names>M</given-names></name><etal/></person-group><article-title>Oscillatory sensory selection mechanisms during intersensory attention to rhythmic auditory and visual inputs: a human electrocorticographic investigation</article-title><source>J Neurosci</source><year>2011</year><volume>31</volume><issue>50</issue><fpage>18556</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2164-11.2011</pub-id><pub-id pub-id-type="pmcid">PMC3298747</pub-id><pub-id pub-id-type="pmid">22171054</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohenkohl</surname><given-names>G</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name></person-group><article-title>alpha oscillations related to anticipatory attention follow temporal expectations</article-title><source>J Neurosci</source><year>2011</year><volume>31</volume><issue>40</issue><fpage>14076</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3387-11.2011</pub-id><pub-id pub-id-type="pmcid">PMC4235253</pub-id><pub-id pub-id-type="pmid">21976492</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zoefel</surname><given-names>B</given-names></name><name><surname>VanRullen</surname><given-names>R</given-names></name></person-group><article-title>Oscillatory Mechanisms of Stimulus Processing and Selection in the Visual and Auditory Systems: State-of-the-Art, Speculations and Suggestions</article-title><source>Front Neurosci</source><year>2017</year><volume>11</volume><fpage>296</fpage><pub-id pub-id-type="doi">10.3389/fnins.2017.00296</pub-id><pub-id pub-id-type="pmcid">PMC5445505</pub-id><pub-id pub-id-type="pmid">28603483</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davidson</surname><given-names>MJ</given-names></name><name><surname>Verstraten</surname><given-names>FAJ</given-names></name><name><surname>Alais</surname><given-names>D</given-names></name></person-group><article-title>Walking modulates visual detection performance according to stride cycle phase</article-title><source>Nat Commun</source><year>2024</year><volume>15</volume><issue>1</issue><elocation-id>2027</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-024-45780-4</pub-id><pub-id pub-id-type="pmcid">PMC10920920</pub-id><pub-id pub-id-type="pmid">38453900</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaltenmaier</surname><given-names>A</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name><name><surname>Press</surname><given-names>C</given-names></name></person-group><article-title>Fixed and flexible perceptual rhythms</article-title><source>Trends Cogn Sci</source><year>2025</year><pub-id pub-id-type="pmid">40533303</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bouwer</surname><given-names>FL</given-names></name></person-group><article-title>Neural Entrainment to Auditory Rhythms: Automatic or Top-Down Driven?</article-title><source>J Neurosci</source><year>2022</year><volume>42</volume><issue>11</issue><fpage>2146</fpage><lpage>2148</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2305-21.2022</pub-id><pub-id pub-id-type="pmcid">PMC8936579</pub-id><pub-id pub-id-type="pmid">35296536</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morillon</surname><given-names>B</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name></person-group><article-title>Neuronal oscillations as a mechanistic substrate of auditory temporal prediction</article-title><source>Ann N Y Acad Sci</source><year>2015</year><volume>1337</volume><issue>1</issue><fpage>26</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1111/nyas.12629</pub-id><pub-id pub-id-type="pmcid">PMC4363099</pub-id><pub-id pub-id-type="pmid">25773613</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kok</surname><given-names>P</given-names></name><name><surname>Mostert</surname><given-names>P</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><article-title>Prior expectations induce prestimulus sensory templates</article-title><source>Proc Natl Acad Sci U S A</source><year>2017</year><volume>114</volume><issue>39</issue><fpage>10473</fpage><lpage>10478</lpage><pub-id pub-id-type="doi">10.1073/pnas.1705652114</pub-id><pub-id pub-id-type="pmcid">PMC5625909</pub-id><pub-id pub-id-type="pmid">28900010</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Myers</surname><given-names>NE</given-names></name><etal/></person-group><article-title>Testing sensory evidence against mnemonic templates</article-title><source>Elife</source><year>2015</year><volume>4</volume><elocation-id>e09000</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.09000</pub-id><pub-id pub-id-type="pmcid">PMC4755744</pub-id><pub-id pub-id-type="pmid">26653854</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blom</surname><given-names>T</given-names></name><etal/></person-group><article-title>Predictions drive neural representations of visual events ahead of incoming sensory information</article-title><source>Proc Natl Acad Sci U S A</source><year>2020</year><volume>117</volume><issue>13</issue><fpage>7510</fpage><lpage>7515</lpage><pub-id pub-id-type="doi">10.1073/pnas.1917777117</pub-id><pub-id pub-id-type="pmcid">PMC7132318</pub-id><pub-id pub-id-type="pmid">32179666</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kok</surname><given-names>P</given-names></name><name><surname>Jehee</surname><given-names>JF</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><article-title>Less is more: expectation sharpens representations in the primary visual cortex</article-title><source>Neuron</source><year>2012</year><volume>75</volume><issue>2</issue><fpage>265</fpage><lpage>70</lpage><pub-id pub-id-type="pmid">22841311</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yon</surname><given-names>D</given-names></name><etal/></person-group><article-title>Action sharpens sensory representations of expected outcomes</article-title><source>Nat Commun</source><year>2018</year><volume>9</volume><issue>1</issue><elocation-id>4288</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-06752-7</pub-id><pub-id pub-id-type="pmcid">PMC6191413</pub-id><pub-id pub-id-type="pmid">30327503</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tzourio-Mazoyer</surname><given-names>N</given-names></name><etal/></person-group><article-title>Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain</article-title><source>Neuroimage</source><year>2002</year><volume>15</volume><issue>1</issue><fpage>273</fpage><lpage>89</lpage><pub-id pub-id-type="pmid">11771995</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Plinio</surname><given-names>S</given-names></name></person-group><article-title>Testing the Magnitude of Correlations Across Experimental Conditions</article-title><source>Front Psychol</source><year>2022</year><volume>13</volume><elocation-id>860213</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2022.860213</pub-id><pub-id pub-id-type="pmcid">PMC9177411</pub-id><pub-id pub-id-type="pmid">35693490</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mostert</surname><given-names>P</given-names></name><name><surname>Kok</surname><given-names>P</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><article-title>Dissociating sensory from decision processes in human perceptual decision making</article-title><source>Sci Rep</source><year>2015</year><volume>5</volume><elocation-id>18253</elocation-id><pub-id pub-id-type="doi">10.1038/srep18253</pub-id><pub-id pub-id-type="pmcid">PMC4678878</pub-id><pub-id pub-id-type="pmid">26666393</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cannon</surname><given-names>JJ</given-names></name><name><surname>Patel</surname><given-names>AD</given-names></name></person-group><article-title>How Beat Perception Co-opts Motor Neurophysiology</article-title><source>Trends Cogn Sci</source><year>2021</year><volume>25</volume><issue>2</issue><fpage>137</fpage><lpage>150</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2020.11.002</pub-id><pub-id pub-id-type="pmcid">PMC9440376</pub-id><pub-id pub-id-type="pmid">33353800</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coull</surname><given-names>JT</given-names></name></person-group><chapter-title>Discrete Neuroanatomical Substrates for Generating and Updating Temporal Expectations</chapter-title><source>Space, Time and Number in the Brain</source><year>2011</year><fpage>87</fpage><lpage>101</lpage></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arnal</surname><given-names>LH</given-names></name></person-group><article-title>Predicting “When” Using the Motor System’s Beta-Band Oscillations</article-title><source>Front Hum Neurosci</source><year>2012</year><volume>6</volume><fpage>225</fpage><pub-id pub-id-type="doi">10.3389/fnhum.2012.00225</pub-id><pub-id pub-id-type="pmcid">PMC3410664</pub-id><pub-id pub-id-type="pmid">22876228</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morillon</surname><given-names>B</given-names></name><name><surname>Baillet</surname><given-names>S</given-names></name></person-group><article-title>Motor origin of temporal predictions in auditory attention</article-title><source>Proc Natl Acad Sci U S A</source><year>2017</year><volume>114</volume><issue>42</issue><fpage>E8913</fpage><lpage>E8921</lpage><pub-id pub-id-type="doi">10.1073/pnas.1705373114</pub-id><pub-id pub-id-type="pmcid">PMC5651745</pub-id><pub-id pub-id-type="pmid">28973923</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patel</surname><given-names>AD</given-names></name><name><surname>Iversen</surname><given-names>JR</given-names></name></person-group><article-title>The evolutionary neuroscience of musical beat perception: the Action Simulation for Auditory Prediction (ASAP) hypothesis</article-title><source>Front Syst Neurosci</source><year>2014</year><volume>8</volume><fpage>57</fpage><pub-id pub-id-type="doi">10.3389/fnsys.2014.00057</pub-id><pub-id pub-id-type="pmcid">PMC4026735</pub-id><pub-id pub-id-type="pmid">24860439</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ross</surname><given-names>JM</given-names></name><name><surname>Iversen</surname><given-names>JR</given-names></name><name><surname>Balasubramaniam</surname><given-names>R</given-names></name></person-group><article-title>Motor simulation theories of musical beat perception</article-title><source>Neurocase</source><year>2016</year><volume>22</volume><issue>6</issue><fpage>558</fpage><lpage>565</lpage><pub-id pub-id-type="pmid">27726485</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rimmele</surname><given-names>JM</given-names></name><etal/></person-group><article-title>Proactive Sensing of Periodic and Aperiodic Auditory Patterns</article-title><source>Trends Cogn Sci</source><year>2018</year><volume>22</volume><issue>10</issue><fpage>870</fpage><lpage>882</lpage><pub-id pub-id-type="pmid">30266147</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boettcher</surname><given-names>SEP</given-names></name><etal/></person-group><article-title>Output planning at the input stage in visual working memory</article-title><source>Sci Adv</source><year>2021</year><volume>7</volume><pub-id pub-id-type="doi">10.1126/sciadv.abe8212</pub-id><pub-id pub-id-type="pmcid">PMC7990334</pub-id><pub-id pub-id-type="pmid">33762341</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohenkohl</surname><given-names>G</given-names></name><etal/></person-group><article-title>Combining spatial and temporal expectations to improve visual perception</article-title><source>J Vis</source><year>2014</year><volume>14</volume><issue>4</issue><pub-id pub-id-type="doi">10.1167/14.4.8</pub-id><pub-id pub-id-type="pmcid">PMC3983934</pub-id><pub-id pub-id-type="pmid">24722562</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hendrikx</surname><given-names>E</given-names></name><etal/></person-group><article-title>Visual timing-tuned responses in human association cortices and response dynamics in early visual cortex</article-title><source>Nat Commun</source><year>2022</year><volume>13</volume><issue>1</issue><elocation-id>3952</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-022-31675-9</pub-id><pub-id pub-id-type="pmcid">PMC9270326</pub-id><pub-id pub-id-type="pmid">35804026</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Lafuente</surname><given-names>V</given-names></name><etal/></person-group><article-title>Keeping time and rhythm by internal simulation of sensory stimuli and behavioral actions</article-title><source>Sci Adv</source><year>2024</year><volume>10</volume><pub-id pub-id-type="doi">10.1126/sciadv.adh8185</pub-id><pub-id pub-id-type="pmcid">PMC10780886</pub-id><pub-id pub-id-type="pmid">38198556</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haarsma</surname><given-names>J</given-names></name><etal/></person-group><article-title>Expectations about presence enhance the influence of content-specific expectations on low-level orientation judgements</article-title><source>Cognition</source><year>2025</year><volume>254</volume><elocation-id>105961</elocation-id><pub-id pub-id-type="pmid">39305833</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Molinaro</surname><given-names>N</given-names></name><etal/></person-group><article-title>Out-of-synchrony speech entrainment in developmental dyslexia</article-title><source>Hum Brain Mapp</source><year>2016</year><volume>37</volume><issue>8</issue><fpage>2767</fpage><lpage>83</lpage><pub-id pub-id-type="doi">10.1002/hbm.23206</pub-id><pub-id pub-id-type="pmcid">PMC6867425</pub-id><pub-id pub-id-type="pmid">27061643</pub-id></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jimenez-Bravo</surname><given-names>M</given-names></name><name><surname>Marrero</surname><given-names>V</given-names></name><name><surname>Benitez-Burraco</surname><given-names>A</given-names></name></person-group><article-title>An oscillopathic approach to developmental dyslexia: From genes to speech processing</article-title><source>Behav Brain Res</source><year>2017</year><volume>329</volume><fpage>84</fpage><lpage>95</lpage><pub-id pub-id-type="pmid">28442358</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boets</surname><given-names>B</given-names></name><etal/></person-group><article-title>Intact But Less Accessible Phonetic Representations in Adults with Dyslexia</article-title><source>Science</source><year>2013</year><volume>342</volume><pub-id pub-id-type="doi">10.1126/science.1244333</pub-id><pub-id pub-id-type="pmcid">PMC3932003</pub-id><pub-id pub-id-type="pmid">24311693</pub-id></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amadeo</surname><given-names>MB</given-names></name><etal/></person-group><article-title>Time in schizophrenia: a link between psychopathology, psychophysics and technology</article-title><source>Transl Psychiatry</source><year>2022</year><volume>12</volume><issue>1</issue><fpage>331</fpage><pub-id pub-id-type="doi">10.1038/s41398-022-02101-x</pub-id><pub-id pub-id-type="pmcid">PMC9374791</pub-id><pub-id pub-id-type="pmid">35961974</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leszczynski</surname><given-names>M</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name></person-group><article-title>The Role of Neuronal Oscillations in Visual Active Sensing</article-title><source>Front Integr Neurosci</source><year>2019</year><volume>13</volume><fpage>32</fpage><pub-id pub-id-type="doi">10.3389/fnint.2019.00032</pub-id><pub-id pub-id-type="pmcid">PMC6664014</pub-id><pub-id pub-id-type="pmid">31396059</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schroeder</surname><given-names>CE</given-names></name><etal/></person-group><article-title>Dynamics of Active Sensing and perceptual selection</article-title><source>Curr Opin Neurobiol</source><year>2010</year><volume>20</volume><issue>2</issue><fpage>172</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2010.02.010</pub-id><pub-id pub-id-type="pmcid">PMC2963579</pub-id><pub-id pub-id-type="pmid">20307966</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hogendoorn</surname><given-names>H</given-names></name></person-group><article-title>Voluntary Saccadic Eye Movements Ride the Attentional Rhythm</article-title><source>J Cogn Neurosci</source><year>2016</year><volume>28</volume><issue>10</issue><fpage>1625</fpage><lpage>35</lpage><pub-id pub-id-type="pmid">27243615</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anil Meera</surname><given-names>A</given-names></name><etal/></person-group><article-title>Reclaiming saliency: Rhythmic precision-modulated action and perception</article-title><source>Front Neurorobot</source><year>2022</year><volume>16</volume><elocation-id>896229</elocation-id><pub-id pub-id-type="doi">10.3389/fnbot.2022.896229</pub-id><pub-id pub-id-type="pmcid">PMC9368584</pub-id><pub-id pub-id-type="pmid">35966370</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sotiropoulos</surname><given-names>G</given-names></name><name><surname>Seitz</surname><given-names>AR</given-names></name><name><surname>Series</surname><given-names>P</given-names></name></person-group><article-title>Changing expectations about speed alters perceived motion direction</article-title><source>Curr Biol</source><year>2011</year><volume>21</volume><issue>21</issue><fpage>R883</fpage><lpage>4</lpage><pub-id pub-id-type="pmid">22075425</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cannon</surname><given-names>J</given-names></name></person-group><article-title>Expectancy-based rhythmic entrainment as continuous Bayesian inference</article-title><source>PLoS Comput Biol</source><year>2021</year><volume>17</volume><issue>6</issue><elocation-id>e1009025</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1009025</pub-id><pub-id pub-id-type="pmcid">PMC8216548</pub-id><pub-id pub-id-type="pmid">34106918</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>L’Hermite</surname><given-names>S</given-names></name><name><surname>Zoefel</surname><given-names>B</given-names></name></person-group><article-title>Rhythmic Entrainment Echoes in Auditory Perception</article-title><source>J Neurosci</source><year>2023</year><volume>43</volume><issue>39</issue><fpage>6667</fpage><lpage>6678</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0051-23.2023</pub-id><pub-id pub-id-type="pmcid">PMC10538584</pub-id><pub-id pub-id-type="pmid">37604689</pub-id></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleiner</surname><given-names>M</given-names></name><etal/></person-group><article-title>What’s new in Psychtoolbox-3</article-title><source>Perception</source><year>2007</year><volume>36</volume></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="other"><collab>Inc., TM</collab><source>MATLAB</source><year>2024</year></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname><given-names>R</given-names></name><etal/></person-group><article-title>FieldTrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Comput Intell Neurosci</source><year>2011</year><volume>2011</volume><elocation-id>156869</elocation-id><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id><pub-id pub-id-type="pmcid">PMC3021840</pub-id><pub-id pub-id-type="pmid">21253357</pub-id></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferrante</surname><given-names>O</given-names></name><etal/></person-group><article-title>FLUX: A pipeline for MEG analysis</article-title><source>Neuroimage</source><year>2022</year><volume>253</volume><elocation-id>119047</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119047</pub-id><pub-id pub-id-type="pmcid">PMC9127391</pub-id><pub-id pub-id-type="pmid">35276363</pub-id></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stanislaw</surname><given-names>H</given-names></name><name><surname>Todorov</surname><given-names>N</given-names></name></person-group><article-title>Calculation of signal detection theory measures</article-title><source>Behavior Research Methods, Instruments &amp; Computers</source><year>1999</year><volume>31</volume><pub-id pub-id-type="pmid">10495845</pub-id></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Veen</surname><given-names>BD</given-names></name><etal/></person-group><article-title>Localization of Brain Electrical Activity via Linearly Constrained Minimum Variance Spatial Filtering</article-title><source>IEEE Trans Biomed Eng</source><year>1997</year><volume>44</volume><issue>9</issue><pub-id pub-id-type="pmid">9282479</pub-id></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Bandettini</surname><given-names>P</given-names></name></person-group><article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title><source>Front Syst Neurosci</source><year>2008</year><volume>2</volume><fpage>4</fpage><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id><pub-id pub-id-type="pmcid">PMC2605405</pub-id><pub-id pub-id-type="pmid">19104670</pub-id></element-citation></ref><ref id="R64"><label>64</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brookshire</surname><given-names>G</given-names></name></person-group><article-title>Putative rhythms in attentional switching can be explained by aperiodic temporal structure</article-title><source>Nat Hum Behav</source><year>2022</year><volume>6</volume><issue>9</issue><fpage>1280</fpage><lpage>1291</lpage><pub-id pub-id-type="doi">10.1038/s41562-022-01364-0</pub-id><pub-id pub-id-type="pmcid">PMC9489532</pub-id><pub-id pub-id-type="pmid">35680992</pub-id></element-citation></ref><ref id="R65"><label>65</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rademaker</surname><given-names>RL</given-names></name><name><surname>Chunharas</surname><given-names>C</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><article-title>Coexisting representations of sensory and mnemonic information in human visual cortex</article-title><source>Nat Neurosci</source><year>2019</year><volume>22</volume><issue>8</issue><fpage>1336</fpage><lpage>1344</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0428-x</pub-id><pub-id pub-id-type="pmcid">PMC6857532</pub-id><pub-id pub-id-type="pmid">31263205</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Design and conceptual illustration of rate specific response (RSR) analysis.</title><p><bold>a</bold> Participants observed rhythmic sequences of rotating Gabor patches before judging a delayed probe’s orientation or timing. <bold>b</bold> RSR was determined by contrasting intertrial phase consistencies (ITPCs) of stimulated and non-stimulated rates. Stimulation and Maintenance windows were extracted from the RSR timecourse to determine oscillatory tracking both during and beyond ongoing visual stimulation. Note that the x-axis represents time in seconds relative to the onset of the final stimulation Gabor.</p></caption><graphic xlink:href="EMS207018-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>RSR results.</title><p><bold>a</bold> Permutation test t-value topographies for the RSR during the Stimulation and Maintenance windows, separately for the timing task, the orientation task and the difference between the two. Sensors marked with a cross belonged to the significant cluster found for a given condition. <bold>b</bold> Same as <bold>a</bold>’s timing task Maintenance window but source-localised using an LCMV beamformer. <bold>c</bold> Correlation between averaged activity across the sensors belonging to the significant cluster in <bold>b-II</bold> and task performance, separately for the orientation and timing task.</p></caption><graphic xlink:href="EMS207018-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Decoding analysis.</title><p><bold>a</bold> Extrapolating each trial’s rhythmic sequence into the Maintenance window yields one predicted orientation for each of the window’s three ‘empty’ cycles. Relative to the onset of the sequence’s final Gabor, these cycles begin at 2, 4 and 6π rate-specific radians (i.e. if the rate is 2Hz, 2π = 500ms). <bold>b</bold> Neural representation of each of the three predicted orientations should be maximal at their respective predicted timepoint of appearance and therefore temporally-distinct from each other. We use time-resolved decoding (inverted encoding model, IEM) of Gabor orientation to capture these representations. <bold>c</bold> We organise our predictions about the decoding output in signed difference matrices (SDMs) in which each cell contains the binarised (+/- 1) predicted difference between the row and column element. Specifically, each presumed decoding peak (i.e. green orientation @ 2π) is compared to all other ‘non-peak’ decoding timepoints (i.e. green orientation @ 4π, red orientation @ 2π). Comparisons for which we made no prediction (i.e. green orientation @ 2π vs. red orientation @ 4π, white cells in SDM) are masked out when the matrix is flattened into a vector before correlation with SDMs filled with participants’ true decoding output from the IEM.</p></caption><graphic xlink:href="EMS207018-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Sensory simulation results.</title><p><bold>a</bold> Participant-wise SDM fits, separate for each task. <bold>b</bold> Maintenance window decoding traces of the three expected orientations at each cycle of the Maintenance window, averaged over all participants and tasks for illustration purposes. Dotted grey line shows predicted trajectory as quantified in the theoretical SDM. <bold>c</bold> Source-localised inverted encoding model (IEM) weights reveal occipital origin. <bold>d</bold> Correlations between SDM fits and RSR, separately for each task. Each task’s</p></caption><graphic xlink:href="EMS207018-f004"/></fig></floats-group></article>