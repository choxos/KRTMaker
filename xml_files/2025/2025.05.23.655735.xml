<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS206003</article-id><article-id pub-id-type="doi">10.1101/2025.05.23.655735</article-id><article-id pub-id-type="archive">PPR1027368</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Beyond performance: How design choices shape chemical language models</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Fender</surname><given-names>Inken</given-names></name><email>inken.fender@unibe.ch</email><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Gut</surname><given-names>Jannik Adrian</given-names></name><email>jannik.gut@unibe.ch</email><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Lemmin</surname><given-names>Thomas</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>Institute of Biochemistry and Molecular Medicine, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02k7v4d05</institution-id><institution>University of Bern</institution></institution-wrap>, <addr-line>Bühlstrasse 28</addr-line>, Bern, <postal-code>3012</postal-code>, <city>Bern</city>, <country country="CH">Switzerland</country></aff><aff id="A2"><label>2</label>Graduate School for Cellular and Biomedical Sciences (GCB), <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02k7v4d05</institution-id><institution>University of Bern</institution></institution-wrap>, <addr-line>Mittelstrasse 43</addr-line>, Bern, <postal-code>3012</postal-code>, <city>Bern</city>, <country country="CH">Switzerland</country></aff><author-notes><corresp id="CR1"><label>*</label>Corresponding author(s). <email>thomas.lemmin@unibe.ch</email>;</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>30</day><month>05</month><year>2025</year></pub-date><pub-date pub-type="preprint"><day>27</day><month>05</month><year>2025</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Chemical language models (CLMs) have shown strong performance in molecular property prediction and generation tasks. However, the impact of design choices, such as molecular representation format, tokenization strategy, and model architecture, on both performance and chemical interpretability remains underexplored. In this study, we systematically evaluate how these factors influence CLM performance and chemical understanding. We evaluated models through fine-tuning on downstream tasks and probing the structure of their latent spaces using simple classifiers and dimensionality reduction techniques. Despite similar performance on downstream tasks across model configurations, we observed substantial differences in the structure and interpretability of their internal representations. SMILES molecular representation format with atomwise tokenization strategy consistently produced more chemically meaningful embeddings, while models based on BART and RoBERTa architectures yielded comparably interpretable representations. These findings highlight that design choices meaningfully shape how chemical information is represented, even when external metrics appear unchanged. This insight can inform future model development, encouraging more chemically grounded and interpretable CLMs.</p></abstract><abstract abstract-type="graphical"><p id="P2"><fig id="F6" position="anchor"><label>Graphical Abstract</label><graphic xlink:href="EMS206003-f006"/></fig>
</p></abstract><kwd-group><kwd>large language models</kwd><kwd>chemical language models</kwd><kwd>interpretability</kwd><kwd>machine learning for chemistry</kwd><kwd>explainable AI (XAI)</kwd><kwd>SMILES</kwd><kwd>SELFIES</kwd><kwd>RoBERTa</kwd><kwd>BART</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><label>1</label><title>Introduction</title><p id="P3">The design and discovery of novel molecules with desired properties are crucial for advances in medicine, materials science, and agriculture. Traditional experimental methods are often time-consuming and expensive, driving the need for efficient computational approaches. As a result, computational methods have become essential tools for accelerating molecular innovation.</p><p id="P4">Recent advances in deep learning, particularly in natural language processing (NLP), have sparked growing interest in applying language models to molecular data. By representing molecules as sequences, most commonly using SMILES (Simplified Molecular Input Line Entry System) [<xref ref-type="bibr" rid="R1">1</xref>] strings, text-based cheminformatics models have demonstrated strong performance across a range of tasks, including prediction of molecular properties, completion of reactions, planning of retrosynthesis, and generation of de novo molecules [<xref ref-type="bibr" rid="R2">2</xref>–<xref ref-type="bibr" rid="R5">5</xref>]. In several cases, these models have even outperformed earlier cheminformatics methods such as molecular fingerprint-based methods or GNNs [<xref ref-type="bibr" rid="R4">4</xref>, <xref ref-type="bibr" rid="R6">6</xref>, <xref ref-type="bibr" rid="R7">7</xref>].</p><p id="P5">To improve the syntactic robustness of sequence-based molecular representations, SELFIES (Self-Referencing Embedded Strings) [<xref ref-type="bibr" rid="R8">8</xref>] was introduced as an alternative to SMILES. Unlike SMILES, which can produce invalid molecules due to syntax errors, every SELFIES string maps to a valid molecule by design. In addition, tokenization strategies play a crucial role in how models interpret molecular sequences. The most common approaches include atomwise tokenization [<xref ref-type="bibr" rid="R4">4</xref>], which segments the string based on atoms and bonds, and subword tokenization using SentencePiece [<xref ref-type="bibr" rid="R9">9</xref>, <xref ref-type="bibr" rid="R10">10</xref>], which learns data-driven tokens optimized for training efficiency on a finer level.</p><p id="P6">Ultimately, the performance and interpretability of these models are influenced by several design choices, including molecular representation, tokenization strategies and the architectural backbone. These factors collectively influence both the downstream task performance and the internal representation of the chemical knowledge of the model. However, a complete understanding of the individual and synergistic effects of these components is still lacking, motivating increased research in this area [<xref ref-type="bibr" rid="R11">11</xref>, <xref ref-type="bibr" rid="R12">12</xref>]. Although some initial evidence suggests that the choice of molecular representation might not be a primary driver of property prediction performance [<xref ref-type="bibr" rid="R2">2</xref>, <xref ref-type="bibr" rid="R13">13</xref>, <xref ref-type="bibr" rid="R14">14</xref>], the impact of tokenization [<xref ref-type="bibr" rid="R12">12</xref>] and, crucially, the architectural backbone [<xref ref-type="bibr" rid="R15">15</xref>] are still poorly understood and require more in-depth investigation. While non-canonical SMILES have shown benefits in certain settings, prior work recommends canonization when computational resources are limited, as it improves training efficiency without sacrificing performance [<xref ref-type="bibr" rid="R16">16</xref>].</p><p id="P7">In this work, we therefore systematically investigate the impact of three core design choices in large chemical language models (CLMs): molecular representation (SMILES vs. SELFIES), tokenization strategy (atomwise vs. SentencePiece), and model architecture (RoBERTa vs. BART). Our goal is to understand how these choices influence downstream performance, the structure of the latent space, and the chemical interpretability of the learned embeddings. We evaluate models on predictive tasks, probe the organization of their latent representations using simple classifiers and dimensionality reduction, and examine atom-level embeddings in relation to chemical typing schemes. While downstream task performance is often comparable across configurations, we find that certain setups, particularly those using SMILES with atomwise tokenization, yield more chemically structured embeddings, potentially indicating a deeper internalization of chemical context.</p></sec><sec id="S2" sec-type="methods"><label>2</label><title>Methods</title><p id="P8">We investigated the performance of Transformer-based models for chemical structure representation. We trained a series of BART and RoBERTa models, exploring the impact of varying tokenization strategies (atomwise and SentencePiece) and molecular representations (SMILES and SELFIES).</p><sec id="S3"><label>2.1</label><title>Pretraining datasets</title><p id="P9">The initial dataset was derived from the PubChem-10M dataset [<xref ref-type="bibr" rid="R3">3</xref>], a collection of 10 million chemical structures sourced from PubChem [<xref ref-type="bibr" rid="R17">17</xref>]. To ensure the quality and consistency of the SMILES strings, all molecules were canonicalized using the RDKit [<xref ref-type="bibr" rid="R18">18</xref>]. For each molecule, the RDKit was employed to generate potential isomers, with a maximum of ten attempts per molecule. These isomers allow to explicitly model chirality. Subsequently, to generate the corresponding SELFIES representations, the canonicalized SMILES were converted to SELFIES using the SELFIES library [<xref ref-type="bibr" rid="R8">8</xref>]. To validate the accuracy of the SELFIES conversion and ensure reversibility, the generated SELFIES strings were then back-translated to SMILES using the same library. Only molecules for which the back-translated SMILES matched the original canonicalized SMILES were retained. Due to this procedure, 10’818 (0.1%) out of the ten million molecules in the base dataset were filtered out and 5’460’790 isomers were added with explicit chirality.</p></sec><sec id="S4"><label>2.2</label><title>Tokenization</title><p id="P10">Two different tokenization strategies were used: atomwise [<xref ref-type="bibr" rid="R4">4</xref>] and SentencePiece [<xref ref-type="bibr" rid="R9">9</xref>, <xref ref-type="bibr" rid="R10">10</xref>]. The atomwise strategy decomposes SMILES strings into individual atoms and bonds, treating each as a separate token. Alternatively, SentencePiece utilizes a subword tokenization technique, grouping characters within and across atoms based on their frequency of occurrence. To generate the SentencePiece vocabulary, we utilized the Hugging Face Transformers library [<xref ref-type="bibr" rid="R19">19</xref>], creating a vocabulary of 1’000 subword units.</p></sec><sec id="S5"><label>2.3</label><title>Language model description</title><p id="P11">Two Transformer-based large language model architectures were employed: BART [<xref ref-type="bibr" rid="R20">20</xref>] and RoBERTa [<xref ref-type="bibr" rid="R21">21</xref>]. Both models leverage the Transformer architecture [<xref ref-type="bibr" rid="R22">22</xref>] to capture long-range dependencies within sequential data. BART (Bidirectional and Auto-Regressive Transformers) is an encoder-decoder model designed for sequence-to-sequence tasks. It utilizes a denoising autoencoder objective during pretraining, where corrupted input sequences are reconstructed. RoBERTa (Robustly Optimized BERT Pretraining Approach) is an encoder-only model that builds upon the BERT [<xref ref-type="bibr" rid="R23">23</xref>] architecture. It is pretrained using a masked language modelling objective, where the model predicts masked tokens within input sequences.</p><p id="P12">Both BART and RoBERTa models were implemented and trained using the fairseq library [<xref ref-type="bibr" rid="R24">24</xref>]. Detailed training parameters, including hyperparameters and optimization settings, are provided in <xref ref-type="sec" rid="S1">Section S1</xref> in the <xref ref-type="supplementary-material" rid="SD1">Supplementary Information</xref>.</p></sec><sec id="S6"><label>2.4</label><title>Downstream tasks</title><p id="P13">To evaluate the performance of our pretrained models, we conducted fine-tuning experiments on a series of downstream tasks sourced from the MoleculeNet benchmark suite [<xref ref-type="bibr" rid="R25">25</xref>, <xref ref-type="bibr" rid="R26">26</xref>]. The following classification tasks were used: BACE, BBBP, ClinTox (CT TOX), HIV, and Tox21 (SR-p53). For regression tasks, we utilized the BACE, Clearance, Delaney, and Lipo tasks. We employed the default train-validation-test splits provided by MoleculeNet for each dataset. The data processing pipeline mirrored that of the pretraining dataset, with the exception of explicit isomer generation. We note that there are eight samples (0.18%) in the HIV test set that did not pass the SELFIES filters and are omitted in all our tests.</p></sec><sec id="S7"><label>2.5</label><title>Atom type embedding analysis</title><p id="P14">To evaluate the ability of the models to cluster and distinguish different atom types, we created mol2-files for 973 molecules from the BBBP, Clearance, Delaney, BACE classification and Lipo datasets and attempted to assign GAFF2 atom types to each atom of those molecules using Antechamber [<xref ref-type="bibr" rid="R27">27</xref>]. To assign atom types to SELFIES, the SMILES were translated to SELFIES using the selfies library with the flag “attribute=True” to track what SMILES token led to the creation of which SELFIES token. This ensured consistent atom type assignment across both input formats.</p><p id="P15">To ensure the reliability of the atom type assignments, we used parmchk [<xref ref-type="bibr" rid="R27">27</xref>] to filter out molecules with atom assignments that resulted in a penalty score exceeding 300, indicating potential inconsistencies or errors in the parametrization.</p><p id="P16">For each molecule, we then extracted the embedding vector corresponding to each atom from our pretrained models. To facilitate a fair comparison, we only analysed embeddings for atom types that were consistently assigned across both SMILES and SELFIES representations. This led us to a final analysis of 143 molecules with atom assignments and embeddings across different models and representations. Principal Component Analysis (PCA) was subsequently used to visualize the distribution and similarity of these atom type embeddings in the embedding space. The maximum number of depicted atoms per type was limited to 300.</p></sec></sec><sec id="S8" sec-type="results"><label>3</label><title>Results</title><sec id="S9"><label>3.1</label><title>Downstream task performance</title><p id="P17">We first pretrained 16 distinct models, systematically varying key architectural and data representation parameters. Specifically, we explored combinations of two molecular representations (SMILES [<xref ref-type="bibr" rid="R1">1</xref>], SELFIES [<xref ref-type="bibr" rid="R8">8</xref>]), two model architectures (BART [<xref ref-type="bibr" rid="R20">20</xref>], RoBERTa [<xref ref-type="bibr" rid="R21">21</xref>]), two tokenization strategies (atomwise, SentencePiece [<xref ref-type="bibr" rid="R9">9</xref>]), and two chirality representations (implicit, explicit).</p><p id="P18">To evaluate the impact of these parameters on the models’ predictive capabilities, we performed fine-tuning experiments on a series of downstream tasks from the MoleculeNet datasets [<xref ref-type="bibr" rid="R25">25</xref>] of the DeepChem benchmark suite [<xref ref-type="bibr" rid="R26">26</xref>]. Each of the 16 pretrained models was fine-tuned on five classification tasks (BACE, BBBP, ClinTox, HIV, Tox21) and four regression tasks (BACE, Clearance, Delaney, Lipo) and after hyperparameter tuning, fine-tuning was repeated five times using different random seeds to assess model robustness.</p><p id="P19">Given the varying difficulty of these downstream tasks, direct comparison of raw performance metrics is challenging. Therefore, we present and discuss the z-scores [<xref ref-type="bibr" rid="R28">28</xref>] of the models’ performance (<xref ref-type="fig" rid="F2">Figure 2</xref>), which were computed to normalize the performance of each model across the different tasks. For classification tasks, z-scores were calculated based on the area under the precision-recall curve (PR-AUC). For regression tasks, z-scores were calculated for each rectified root mean squared error (RMSE) and then averaged. Raw performance metrics and comparisons to benchmark models for classification and regression tasks are available in <xref ref-type="supplementary-material" rid="SD1">Supplementary Information</xref> <xref ref-type="supplementary-material" rid="SD1">Table S3</xref> and <xref ref-type="supplementary-material" rid="SD1">Table S4</xref> respectively.</p><p id="P20">While no single model configuration consistently outperformed all others, hierarchical clustering of models, based on the cosine similarity of their z-score performance profiles, revealed meaningful groupings. The first-level clustering mainly separated models by tokenization strategy. Notably, models employing atomwise tokenization generally exhibited superior performance compared to those using SentencePiece, with the exception of the Tox21 task.</p><p id="P21">At the next level of clustering, models further grouped according to the molecular representation used. SMILES representations tended to yield better performance on the BBBP and HIV tasks as well as aggregated regression scores, whereas SELFIES representations showed advantages on the ClinTox task. Explicit chirality representation was associated with improved performance on the BACE, ClinTox, HIV, Tox21, and BBBP tasks.</p><p id="P22">The model architecture influenced performance. BART models demonstrated better results on the BACE, HIV, and Tox21 tasks, while RoBERTa models achieved higher performances on the ClinTox and BBBP task.</p><p id="P23">Based on Wilcoxon signed-rank tests [<xref ref-type="bibr" rid="R29">29</xref>] (<xref ref-type="supplementary-material" rid="SD1">Table S5</xref> in the <xref ref-type="supplementary-material" rid="SD1">Supplementary Information</xref>), we prioritized atomwise tokenization due to its statistically significant superior performance (p = 0.020) and implicit chirality representation, which showed no significant detriment (p = 0.123). For subsequent analyses, we further investigated BART and RoBERTa architectures with both SMILES and SELFIES.</p></sec><sec id="S10"><label>3.2</label><title>Latent space analysis</title><p id="P24">To assess whether our pretrained models captured meaningful chemical features within their latent spaces, we investigated the ability of these spaces to encode two non-trivial molecular properties: the presence of heterocycles and the presence of at least one hydrogen bond donor.</p><p id="P25">For this analysis, we sampled 100’000 molecules for each feature class (presence/absence) from our pretraining dataset. These molecules were annotated with the target features using the RDKit [<xref ref-type="bibr" rid="R18">18</xref>]. Subsequently, the latent embeddings of each molecule were generated with pretrained models. These embeddings were then randomly split into equal-sized training and test sets.</p><p id="P26">We trained and evaluated three simple classifiers [<xref ref-type="bibr" rid="R30">30</xref>] on these latent embeddings to probe the structure of the latent space (details in <xref ref-type="sec" rid="S3">Section S2</xref> of the <xref ref-type="supplementary-material" rid="SD1">Supplementary Information</xref>). Each classifier employs a distinct decision boundary, revealing different aspects of how molecules are organized based on the target features. Specifically, k-Nearest Neighbors (k-NN) [<xref ref-type="bibr" rid="R31">31</xref>] assesses local density, classifying molecules based on the majority class among their nearest neighbours, providing insight into local clustering patterns. The Support Vector Classifier (SVC) with Radial Basis Function (RBF) kernel [<xref ref-type="bibr" rid="R31">31</xref>, <xref ref-type="bibr" rid="R32">32</xref>] identifies complex, non-linear relationships, revealing if the latent space allows for intricate separation of molecules based on the target features. Finally, LinearSVC [<xref ref-type="bibr" rid="R33">33</xref>] tests global linear separability, determining if a simple hyperplane can effectively distinguish molecules based on the presence or absence of features.</p><p id="P27">BART embeddings consistently surpassed RoBERTa across all classification tasks, with the most pronounced advantage observed in the LinearSVC tests (<xref ref-type="fig" rid="F3">Figure 3</xref>). Within BART embeddings, SMILES representations slightly outperformed SELFIES. In contrast, RoBERTa embeddings showed comparable performance between SMILES and SELFIES, except for a substantial performance deficit of SMILES in LinearSVC for heterocycle prediction.</p><p id="P28">The RBF-SVC consistently yielded the highest performance across all models and tasks, suggesting the presence of complex, non-linear relationships in the latent space that effectively separate molecules based on target features. The choice between LinearSVC and k-NN as the second-best classifier depends on the underlying embedding model. For BART, LinearSVC exhibited superior performance, whereas for RoBERTa, k-NN and LinearSVC performed comparably.</p><p id="P29">These results suggest that local molecular neighbourhoods, as captured by k-NN, are similarly indicative of heterocycle presence and H-donor capabilities, regardless of the embedding model or molecular representation (SMILES/SELFIES). However, the SVC tests reveal that latent space of BART offers clearer, more distinct boundaries for these properties than RoBERTa. The difference in performance between LinearSVC and RBF-SVC, reflecting the degree of non-linearity required for optimal classification, was relatively small for BART-based models, indicating a more linearly separable latent space, but larger for RoBERTa, highlighting the need for more complex, non-linear decision boundaries.</p></sec><sec id="S11"><label>3.3</label><title>Molecule embeddings</title><p id="P30">For a qualitative assessment of the learned embedding space, we selected 64 molecules from each of the following four chemical classes: steroids, beta-lactams, tropanes, and sulfonamides. These chemical classes were chosen because they all have some common use in pharmacology and yet have distinct structures and chemical features. The selected molecules were embedded using the atomwise tokenizer with implicit chirality representation.</p><p id="P31">BART-based SMILES embeddings exhibited the most distinct clustering in the UMAP [<xref ref-type="bibr" rid="R34">34</xref>] plot (<xref ref-type="fig" rid="F4">Figure 4</xref>), with well-defined chemical families showing different degrees of separation. In contrast, BART-based SELFIES embeddings also demonstrated structured clustering, but the separation was slightly less pronounced, although beta-lactam clusters remained clearly identifiable. RoBERTa-based SMILES embeddings showed less definition than their BART counterparts, whereas RoBERTa-based SELFIES embeddings produced a more scattered distribution, with only weak clustering tendencies.</p><p id="P32">To further validate these observations, we performed a complementary PCA analysis of the selected molecules (<xref ref-type="supplementary-material" rid="SD1">Figure S1</xref> in the <xref ref-type="supplementary-material" rid="SD1">Supplementary Information</xref>). For BART with SMILES, the first principal component was sufficient to separate sulfonamides and steroids, although beta-lactams and tropanes still formed a combined cluster. BART with SELFIES required two principal components to distinguish beta-lactams and steroids, while sulfonamides exhibited only a weak clustering pattern. The PCA results for RoBERTa revealed that its first principal component explains a substantially larger portion of the variance compared to BART, although both models are trained using standard hyperparameters (<xref ref-type="sec" rid="S1">Section S1</xref> in the <xref ref-type="supplementary-material" rid="SD1">Supplementary Information</xref>). The higher explained variance captured by the first principal component in RoBERTa compared to BART persists across all combinations of other design choices, though the difference is less pronounced (<xref ref-type="supplementary-material" rid="SD1">Figure S1</xref> in <xref ref-type="supplementary-material" rid="SD1">Supplementary Information</xref>). RoBERTa with SMILES effectively identifies steroids using the first principal component, while the second component helps to identify sulfonamides. In contrast, RoBERTa with SELFIES only shows weak clustering tendencies without clear separation.</p><p id="P33">To ensure that the observed clustering patterns arise from meaningful learned features rather than artifacts of the embedding process or dimensionality reduction techniques, we evaluated an untrained model as a control (<xref ref-type="supplementary-material" rid="SD1">Figure S2</xref> in the <xref ref-type="supplementary-material" rid="SD1">Supplementary Information</xref>). In the untrained PCA projection, sulfonamides form a distinct cluster, likely due to the presence of sulphur, a feature absent in the other molecules, making it identifiable even without training. However, the remaining molecular classes do not exhibit clear separation, with their embeddings appearing mixed. Similarly, the UMAP projection shows weak clustering tendencies, but no well-defined clusters emerge. These results suggest that the structured organization observed in the learned embedding of the trained models captures meaningful chemical information rather than reflecting artifacts of the embedding or dimensionality reduction techniques.</p></sec><sec id="S12"><label>3.4</label><title>Atom type embeddings</title><p id="P34">To evaluate the degree of chemical understanding acquired by our pretrained models, we analysed the similarity of atom embeddings based on GAFF2 atom types [<xref ref-type="bibr" rid="R27">27</xref>]. The goal was to determine whether atoms sharing the same GAFF2 atom type are represented by similar embedding vectors and whether their clustering differs between the different models. For this analysis, we focussed on the most commonly occurring atom types.</p><p id="P35">For carbon atoms, PCA of the embeddings reveals distinct clustering patterns across molecular representations and architectures (<xref ref-type="fig" rid="F5">Figure 5</xref>). In the RoBERTa model trained on SMILES, aromatic carbons (GAFF2 atom type: ca) are effectively separated from sp<sup>2</sup>-hybridized aliphatic or ketone/thioketone carbons (GAFF2: c2 and c) and sp<sup>3</sup> carbons (GAFF2: c3). A similar, albeit not as strong separation can be seen for BART trained on SMILES as well. When aromatic carbons are excluded and the PCA is rerun, the RoBERTa SMILES model still distinguishes ketone/thioketone carbons from sp<sup>3</sup> carbons along the first principal component, with a similar separation observable along the second component in the RoBERTa SELFIES model (<xref ref-type="supplementary-material" rid="SD1">Figure S5</xref> in the <xref ref-type="supplementary-material" rid="SD1">Supplementary Information</xref>). The BART models, regardless of whether they use SMILES or SELFIES, show less clear separation (<xref ref-type="supplementary-material" rid="SD1">Figure S5</xref> in the <xref ref-type="supplementary-material" rid="SD1">Supplementary Information</xref>).</p><p id="P36">For nitrogen atoms, the embeddings reveal that the sp<sup>2</sup>-hybridized nitrogen in aromatic rings (GAFF2: na) clusters distinctly in the RoBERTa SMILES model along the first principal component (<xref ref-type="supplementary-material" rid="SD1">Figure S3</xref> in the <xref ref-type="supplementary-material" rid="SD1">Supplementary Information</xref>). In this model, sp<sup>2</sup>-hybridized nitrogen in amides (GAFF2: n) and sp<sup>3</sup>-hybridized nitrogen (GAFF2: n3) are mostly separated along the second principal component, indicating a nuanced differentiation of nitrogen environments.</p><p id="P37">Similarly, for oxygen atoms, sp<sup>2</sup>-hybridized oxygen typically found in carbonyl groups (GAFF2: o) is largely separated for all models and molecular representations (<xref ref-type="supplementary-material" rid="SD1">Figure S4</xref> in the <xref ref-type="supplementary-material" rid="SD1">Supplementary Information</xref>). However, in most configurations, the sp<sup>2</sup>-hybridized oxygen in hydroxyl groups (GAFF2: oh) and oxygen in ethers and esters (GAFF2: os) tend to overlap, except in the RoBERTa SMILES model, where these oxygen types are distinctly resolved.</p><p id="P38">We evaluated the impact of case sensitivity by modifying molecules to use only uppercase carbon atoms via the kekulize flag, thereby removing the distinction between aromatic (lowercase) and aliphatic (uppercase) carbons. As expected, this led to less distinct clustering of carbon atom types in the embedding space (<xref ref-type="supplementary-material" rid="SD1">Figure S7</xref>, <xref ref-type="supplementary-material" rid="SD1">Supplementary Information</xref>). Notably, despite the loss of case-based cues, the RoBERTa-based model still differentiated aromatic carbons (ca) from sp<sup>2</sup>-hybridized (c and c2) and sp<sup>3</sup>-hybridized (c3) carbons along the first principal component. In contrast, BART showed no such separation. This trend was consistent across other atom types as well: RoBERTa-based embeddings for nitrogen and oxygen atoms also displayed more structured separation than those from BART-based embeddings (<xref ref-type="supplementary-material" rid="SD1">Figure S8</xref> and <xref ref-type="supplementary-material" rid="SD1">Figure S9</xref> in the <xref ref-type="supplementary-material" rid="SD1">Supplementary Information</xref>).</p><p id="P39">We examined atom type clustering in embeddings produced by untrained models to better understand the extent to which structural information is captured without task-specific training. Using standard SMILES input, the untrained BART model already exhibited notable clustering across atom types (<xref ref-type="supplementary-material" rid="SD1">Figure S6</xref> in the Supplementary Information). For carbon atoms, aromatic carbons (ca) and sp<sup>3</sup>-hybridized carbons (c3) were clearly separated along the first principal component, while sp<sup>2</sup>-hybridized aliphatic carbons (c2) and ketone/thioketone carbons (c) formed a more central, overlapping cluster. Similar, though less pronounced, trends were observed for nitrogen and oxygen atoms. This early clustering likely reflects inherent signals present in SMILES strings, such as lowercase characters denoting aromatic atoms and symbols like “=” indicating bond order, which can implicitly encode hybridization or bonding environments.</p><p id="P40">To test whether such early clustering persists when explicit aromaticity cues are removed, we repeated the analysis using kekulized SMILES input with the untrained BART model. In this setting, the case-based distinction between aromatic and aliphatic atoms is eliminated. Nonetheless, the model continued to show some chemically meaningful separation. For nitrogen atoms, sp<sup>2</sup>-hybridized atoms in amides (n) were distinguishable from sp<sup>3</sup>-hybridized atoms (n3) along the first principal component, while aromatic nitrogen (na) and nitrogen in amines (nh) remained more closely clustered. For oxygen atoms, sp<sup>2</sup>-hybridized oxygen (o) was clearly separated from oh and os (<xref ref-type="supplementary-material" rid="SD1">Figure S8</xref>, <xref ref-type="supplementary-material" rid="SD1">Figure S9</xref> in the <xref ref-type="supplementary-material" rid="SD1">Supplementary Information</xref>).</p><p id="P41">These findings indicate that even without training, the structural information encoded in the input representations can lead to some initial clustering of atom types. This initial clustering is stronger when distinguishing between lowercase and uppercase c as is done in SMILES (<xref ref-type="fig" rid="F5">Figure 5</xref> and <xref ref-type="supplementary-material" rid="SD1">Figure S7</xref> in the <xref ref-type="supplementary-material" rid="SD1">Supplementary Information</xref>). However, training significantly refines these embeddings, allowing models to develop a more nuanced understanding of atomic environments.</p></sec></sec><sec id="S13" sec-type="discussion"><label>4</label><title>Discussion</title><p id="P42">In this study, we systematically investigated how three key design choices influence the performance and representational quality of large chemical language models. These choices include the choice of molecular representation (SMILES [<xref ref-type="bibr" rid="R1">1</xref>] vs. SELFIES [<xref ref-type="bibr" rid="R8">8</xref>]), the tokenisation strategy (atomwise [<xref ref-type="bibr" rid="R4">4</xref>] vs. SentencePiece [<xref ref-type="bibr" rid="R9">9</xref>]), and the underlying model architecture (BART [<xref ref-type="bibr" rid="R20">20</xref>] vs. RoBERTa [<xref ref-type="bibr" rid="R21">21</xref>]). By isolating and evaluating the effects of each of these variables across a range of tasks, we aimed to better understand how these architectural and preprocessing choices shape the ability of chemical language models to learn chemically meaningful representations.</p><p id="P43">Our comparative analysis was structured into four key components, each focusing on a different perspective of model evaluation. First, we evaluated the performance of each model configuration on downstream tasks after fine-tuning. Although certain configurations showed modest advantages, no configuration consistently outperformed the others across all tasks. This indicates that overall, the models learn robust internal representations that generalize well across different input representation and preprocessing strategies, highlighting the flexibility of these architectures in adapting to varied chemical representations. Second, we probed the latent space by training weaker classifiers to predict basic molecular properties using the embeddings generated by each model. The embedding of BART-based models consistently yielded better results than RoBERTa-based models, and SMILES-based models outperformed SELFIES-based models. The third analysis section once again inspected the latent space, but this time evaluating the UMAP and PCA plots of whole molecules. Our results indicate that the SMILES-based BART configuration achieved the best performance, with the molecular representation (SMILES) having a greater positive impact on performance than the specific model architecture. Finally, we examined how well the models captured atomic-level information by visualizing embeddings of atoms labelled by GAFF2 atom types. Here, RoBERTa showed a stronger ability to distinguish between atom types, and SMILES representations led to a more distinct separation than SELFIES, highlighting that the optimal configuration may vary depending on the granularity and nature of the chemical information being encoded.</p><p id="P44">Concerning preprocessing, we found that SentencePiece tokenization reduced training time by using shorter sequence lengths. However, this tokenization also sacrifices interpretability as its subword units do not correspond to tokens as easily interpretable as atoms from the atomwise tokenizer.</p><p id="P45">Overall, across our different analyses, SMILES representations consistently produced more chemically structured latent spaces compared to those derived from SELFIES. However, for certain generative tasks, where ensuring valid molecule generation is critical, the syntactic robustness of SELFIES may offer a distinct advantage [<xref ref-type="bibr" rid="R35">35</xref>]. The differences between BART and RoBERTa architecture were less pronounced in most analyses, where both yielded similarly interpretable latent spaces. Ultimately, the choice between BART and RoBERTa may be based on practical factors. RoBERTa is widely adopted, benefits from a more stable implementation, and has abundant community resources, making it a strong contender in many applications.</p><p id="P46">To further advance this research, future efforts could focus on several key areas. First, exploring the impact of alternative molecular representations, such as t-SMILES [<xref ref-type="bibr" rid="R36">36</xref>] and DeepSMILES [<xref ref-type="bibr" rid="R37">37</xref>], could lead to the development of more robust and generalizable models. Additionally, a direct comparison of BART’s generative capabilities with other large language model architectures, like GPT [<xref ref-type="bibr" rid="R38">38</xref>], would provide valuable insight into their respective strengths and limitations. Parallel to this, investigating the fundamental differences and potential synergies between large language models and graph-based approaches, such as GROVER [<xref ref-type="bibr" rid="R39">39</xref>], offers an exciting opportunity to expand the methodological toolkit for molecular modelling. Finally, utilizing highly curated datasets from specialized domains could reveal subtle chemical relationships and drive significant advancements in targeted applications.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><p id="P47">Supplementary File 1 provides extended figures and tables.</p><supplementary-material content-type="local-data" id="SD1"><label>Supplementary information</label><media xlink:href="EMS206003-supplement-Supplementary_information.pdf" mimetype="application" mime-subtype="pdf" id="d59aAcEcB" position="anchor"/></supplementary-material></sec></body><back><ack id="S14"><title>Acknowledgements</title><p>We thank Noah Kleinschmidt for careful review of the paper and valuable feedback on the project.</p><sec id="S15"><title>Funding</title><p>This work is supported by funds from the FreeNovation 2023 grant and the Swiss National Science Foundation (PCEFP3 194606).</p></sec></ack><sec id="S16" sec-type="data-availability"><title>Availability of data and materials</title><p id="P48">Pretraining data is taken from PubChem dataset [<xref ref-type="bibr" rid="R2">2</xref>] and fine-tuning dataset is taken from MoleculeNet [<xref ref-type="bibr" rid="R25">25</xref>, <xref ref-type="bibr" rid="R26">26</xref>].</p><p id="P49">Implementation code is available on GitHub: <ext-link ext-link-type="uri" xlink:href="https://github.com/ibmm-unibe-ch/SMILES_or_SELFIES">https://github.com/ibmm-unibe-ch/SMILES_or_SELFIES</ext-link>.</p></sec><fn-group><fn fn-type="conflict" id="FN2"><p id="P50"><bold>Competing interests</bold>. None declared.</p></fn><fn fn-type="con" id="FN3"><p id="P51"><bold>Author contributions. Inken Fender</bold>: conceptualisation, data curation, formal analysis, investigation, methodology, software, validation, visualisation, writing <bold>Jannik Adrian Gut</bold>: conceptualisation, data curation, formal analysis, investigation, methodology, software, validation, visualisation, writing <bold>Thomas Lemmin</bold>: conceptualisation, investigation, funding acquisition, methodology, supervision, visualisation, writing</p></fn></fn-group><ref-list><ref id="R1"><label>[1]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weininger</surname><given-names>D</given-names></name></person-group><article-title>Smiles, a chemical language and information system</article-title><source>Journal of chemical information and computer sciences</source><year>1988</year><volume>28</volume><fpage>31</fpage><lpage>36</lpage></element-citation></ref><ref id="R2"><label>[2]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chithrananda</surname><given-names>S</given-names></name><name><surname>Grand</surname><given-names>G</given-names></name><name><surname>Ramsundar</surname><given-names>B</given-names></name></person-group><article-title>Chemberta: large-scale self-supervised pretraining for molecular property prediction</article-title><source>arXiv</source><year>2020</year><elocation-id>arXiv:2010.09885</elocation-id></element-citation></ref><ref id="R3"><label>[3]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahmad</surname><given-names>W</given-names></name><name><surname>Simon</surname><given-names>E</given-names></name><name><surname>Chithrananda</surname><given-names>S</given-names></name><name><surname>Grand</surname><given-names>G</given-names></name><name><surname>Ramsundar</surname><given-names>B</given-names></name></person-group><article-title>Chemberta-2: Towards chemical foundation models</article-title><source>arXiv</source><year>2022</year><elocation-id>arXiv:2209.01712</elocation-id></element-citation></ref><ref id="R4"><label>[4]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwaller</surname><given-names>P</given-names></name><etal/></person-group><article-title>Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction</article-title><source>ACS Central Science</source><year>2019</year><volume>5</volume><fpage>1572</fpage><lpage>1583</lpage><pub-id pub-id-type="pmcid">PMC6764164</pub-id><pub-id pub-id-type="pmid">31572784</pub-id><pub-id pub-id-type="doi">10.1021/acscentsci.9b00576</pub-id></element-citation></ref><ref id="R5"><label>[5]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chilingaryan</surname><given-names>G</given-names></name><etal/></person-group><article-title>Bartsmiles: Generative masked language models for molecular representations</article-title><source>Journal of Chemical Information and Modeling</source><year>2024</year><volume>64</volume><fpage>5832</fpage><lpage>5843</lpage><pub-id pub-id-type="pmid">39054761</pub-id></element-citation></ref><ref id="R6"><label>[6]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sadeghi</surname><given-names>S</given-names></name><name><surname>Bui</surname><given-names>A</given-names></name><name><surname>Forooghi</surname><given-names>A</given-names></name><name><surname>Lu</surname><given-names>J</given-names></name><name><surname>Ngom</surname><given-names>A</given-names></name></person-group><article-title>Can large language models understand molecules?</article-title><source>BMC Bioinformatics</source><year>2024</year><volume>25</volume><fpage>225</fpage><pub-id pub-id-type="pmcid">PMC11552135</pub-id><pub-id pub-id-type="pmid">38926641</pub-id><pub-id pub-id-type="doi">10.1186/s12859-024-05847-x</pub-id></element-citation></ref><ref id="R7"><label>[7]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ross</surname><given-names>J</given-names></name><etal/></person-group><article-title>Large-scale chemical language representations capture molecular structure and properties</article-title><source>Nature Machine Intelligence</source><year>2022</year><volume>4</volume><fpage>1256</fpage><lpage>1264</lpage><comment>URL <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s42256-022-00580-7">https://www.nature.com/articles/s42256-022-00580-7</ext-link></comment></element-citation></ref><ref id="R8"><label>[8]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krenn</surname><given-names>M</given-names></name><name><surname>Häse</surname><given-names>F</given-names></name><name><surname>Nigam</surname><given-names>A</given-names></name><name><surname>Friederich</surname><given-names>P</given-names></name><name><surname>Aspuru-Guzik</surname><given-names>A</given-names></name></person-group><article-title>Self-referencing embedded strings (selfies): A 100% robust molecular string representation</article-title><source>Machine Learning: Science and Technology</source><year>2020</year><volume>1</volume><elocation-id>045024</elocation-id></element-citation></ref><ref id="R9"><label>[9]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kudo</surname><given-names>T</given-names></name></person-group><article-title>Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</article-title><source>arXiv</source><year>2018</year><elocation-id>arXiv:1808.06226</elocation-id></element-citation></ref><ref id="R10"><label>[10]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Fourches</surname><given-names>D</given-names></name></person-group><article-title>Smiles pair encoding: a data-driven substructure tokenization algorithm for deep learning</article-title><source>Journal of chemical information and modeling</source><year>2021</year><volume>61</volume><fpage>1560</fpage><lpage>1569</lpage><pub-id pub-id-type="pmid">33715361</pub-id></element-citation></ref><ref id="R11"><label>[11]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krenn</surname><given-names>M</given-names></name><etal/></person-group><article-title>Selfies and the future of molecular string representations</article-title><source>Patterns</source><year>2022</year><volume>3</volume><pub-id pub-id-type="pmcid">PMC9583042</pub-id><pub-id pub-id-type="pmid">36277819</pub-id><pub-id pub-id-type="doi">10.1016/j.patter.2022.100588</pub-id></element-citation></ref><ref id="R12"><label>[12]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leon</surname><given-names>M</given-names></name><name><surname>Perezhohin</surname><given-names>Y</given-names></name><name><surname>Peres</surname><given-names>F</given-names></name><name><surname>Popovič</surname><given-names>A</given-names></name><name><surname>Castelli</surname><given-names>M</given-names></name></person-group><article-title>Comparing smiles and selfies tokenization for enhanced chemical language modeling</article-title><source>Scientific Reports</source><year>2024</year><volume>14</volume><elocation-id>25016</elocation-id><pub-id pub-id-type="pmcid">PMC11499904</pub-id><pub-id pub-id-type="pmid">39443676</pub-id><pub-id pub-id-type="doi">10.1038/s41598-024-76440-8</pub-id></element-citation></ref><ref id="R13"><label>[13]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yüksel</surname><given-names>A</given-names></name><name><surname>Ulusoy</surname><given-names>E</given-names></name><name><surname>Ünlü</surname><given-names>A</given-names></name><name><surname>Doğan</surname><given-names>T</given-names></name></person-group><article-title>Selformer: molecular representation learning via selfies language models</article-title><source>Machine Learning: Science and Technology</source><year>2023</year><volume>4</volume><elocation-id>025035</elocation-id></element-citation></ref><ref id="R14"><label>[14]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flam-Shepherd</surname><given-names>D</given-names></name><name><surname>Zhu</surname><given-names>K</given-names></name><name><surname>Aspuru-Guzik</surname><given-names>A</given-names></name></person-group><article-title>Language models can learn complex molecular distributions</article-title><source>Nature Communications</source><year>2022</year><volume>13</volume><elocation-id>3293</elocation-id><pub-id pub-id-type="pmcid">PMC9174447</pub-id><pub-id pub-id-type="pmid">35672310</pub-id><pub-id pub-id-type="doi">10.1038/s41467-022-30839-x</pub-id></element-citation></ref><ref id="R15"><label>[15]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sultan</surname><given-names>A</given-names></name><name><surname>Sieg</surname><given-names>J</given-names></name><name><surname>Mathea</surname><given-names>M</given-names></name><name><surname>Volkamer</surname><given-names>A</given-names></name></person-group><article-title>Transformers for Molecular Property Prediction: Lessons Learned from the Past Five Years</article-title><source>Journal of Chemical Information and Modeling</source><year>2024</year><volume>64</volume><fpage>6259</fpage><lpage>6280</lpage><pub-id pub-id-type="pmid">39136669</pub-id></element-citation></ref><ref id="R16"><label>[16]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kimber</surname><given-names>TB</given-names></name><name><surname>Gagnebin</surname><given-names>M</given-names></name><name><surname>Volkamer</surname><given-names>A</given-names></name></person-group><article-title>Maxsmi: maximizing molecular property prediction performance with confidence estimation using smiles augmentation and deep learning</article-title><source>Artificial Intelligence in the Life Sciences</source><year>2021</year><volume>1</volume><elocation-id>100014</elocation-id></element-citation></ref><ref id="R17"><label>[17]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>S</given-names></name><etal/></person-group><article-title>Pubchem 2023 update</article-title><source>Nucleic acids research</source><year>2023</year><volume>51</volume><fpage>D1373</fpage><lpage>D1380</lpage><pub-id pub-id-type="pmcid">PMC9825602</pub-id><pub-id pub-id-type="pmid">36305812</pub-id><pub-id pub-id-type="doi">10.1093/nar/gkac956</pub-id></element-citation></ref><ref id="R18"><label>[18]</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Landrum</surname><given-names>G</given-names></name><etal/></person-group><source>rdkit/rdkit: 2024 09 5 (q3 2024) release</source><year>2025</year><pub-id pub-id-type="doi">10.5281/zenodo.14779836</pub-id></element-citation></ref><ref id="R19"><label>[19]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolf</surname><given-names>T</given-names></name><etal/></person-group><article-title>Huggingface’s transformers: State-of-the-art natural language processing</article-title><source>arXiv</source><year>2019</year><elocation-id>arxiv:1910.03771</elocation-id></element-citation></ref><ref id="R20"><label>[20]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lewis</surname><given-names>M</given-names></name><etal/></person-group><source>BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</source><conf-name>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</conf-name><year>2020</year><fpage>7871</fpage><lpage>7880</lpage></element-citation></ref><ref id="R21"><label>[21]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Roberta: A robustly optimized bert pretraining approach</article-title><source>arXiv</source><year>2019</year><elocation-id>arxiv:1907.11692</elocation-id></element-citation></ref><ref id="R22"><label>[22]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaswani</surname><given-names>A</given-names></name><etal/></person-group><article-title>Attention is all you need</article-title><source>Advances in neural information processing systems</source><year>2017</year><volume>30</volume></element-citation></ref><ref id="R23"><label>[23]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Devlin</surname><given-names>J</given-names></name><name><surname>Chang</surname><given-names>MW</given-names></name><name><surname>Lee</surname><given-names>K</given-names></name><name><surname>Toutanova</surname><given-names>K</given-names></name></person-group><source>Bert: Pre-training of deep bidirectional transformers for language understanding</source><conf-name>Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies</conf-name><year>2019</year><volume>1</volume><fpage>4171</fpage><lpage>4186</lpage><comment>long and short papers</comment></element-citation></ref><ref id="R24"><label>[24]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ott</surname><given-names>M</given-names></name><etal/></person-group><article-title>fairseq: A fast, extensible toolkit for sequence modeling</article-title><source>arXiv</source><year>2019</year><elocation-id>arxiv:1904.01038</elocation-id></element-citation></ref><ref id="R25"><label>[25]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>Z</given-names></name><etal/></person-group><article-title>Moleculenet: A benchmark for molecular machine learning</article-title><source>CoRR</source><year>2017</year><elocation-id>abs/1703.00564</elocation-id><pub-id pub-id-type="pmcid">PMC5868307</pub-id><pub-id pub-id-type="pmid">29629118</pub-id><pub-id pub-id-type="doi">10.1039/c7sc02664a</pub-id></element-citation></ref><ref id="R26"><label>[26]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ramsundar</surname><given-names>B</given-names></name><etal/></person-group><source>Deep Learning for the Life Sciences</source><publisher-name>O’Reilly Media</publisher-name><year>2019</year></element-citation></ref><ref id="R27"><label>[27]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>W</given-names></name><name><surname>Kollman</surname><given-names>PA</given-names></name><name><surname>Case</surname><given-names>DA</given-names></name></person-group><article-title>Automatic atom type and bond type perception in molecular mechanical calculations</article-title><source>Journal of Molecular Graphics and Modelling</source><year>2006</year><volume>25</volume><fpage>247</fpage><lpage>260</lpage><pub-id pub-id-type="pmid">16458552</pub-id></element-citation></ref><ref id="R28"><label>[28]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kreyszig</surname><given-names>E</given-names></name><name><surname>Stroud</surname><given-names>K</given-names></name><name><surname>Stephenson</surname><given-names>G</given-names></name></person-group><article-title>Advanced engineering mathematics</article-title><source>Integration</source><year>2008</year><volume>9</volume><elocation-id>1014</elocation-id></element-citation></ref><ref id="R29"><label>[29]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilcoxon</surname><given-names>F</given-names></name></person-group><article-title>Individual comparisons by ranking methods</article-title><source>Breakthroughs in statistics: Methodology and distribution</source><year>1992</year><fpage>196</fpage><lpage>202</lpage></element-citation></ref><ref id="R30"><label>[30]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><etal/></person-group><article-title>Scikit-learn: Machine learning in Python</article-title><source>Journal of Machine Learning Research</source><year>2011</year><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></element-citation></ref><ref id="R31"><label>[31]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bishop</surname><given-names>CM</given-names></name></person-group><source>Neural networks for pattern recognition</source><publisher-name>Oxford university press</publisher-name><year>1995</year></element-citation></ref><ref id="R32"><label>[32]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cortes</surname><given-names>C</given-names></name><name><surname>Vapnik</surname><given-names>V</given-names></name></person-group><article-title>Support-vector networks</article-title><source>Machine learning</source><year>1995</year><volume>20</volume><fpage>273</fpage><lpage>297</lpage></element-citation></ref><ref id="R33"><label>[33]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fan</surname><given-names>R-E</given-names></name><name><surname>Chang</surname><given-names>K-W</given-names></name><name><surname>Hsieh</surname><given-names>C-J</given-names></name><name><surname>Wang</surname><given-names>X-R</given-names></name><name><surname>Lin</surname><given-names>C-J</given-names></name></person-group><article-title>Liblinear: A library for large linear classification</article-title><source>the Journal of machine Learning research</source><year>2008</year><volume>9</volume><fpage>1871</fpage><lpage>1874</lpage></element-citation></ref><ref id="R34"><label>[34]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McInnes</surname><given-names>L</given-names></name><name><surname>Healy</surname><given-names>J</given-names></name><name><surname>Melville</surname><given-names>J</given-names></name></person-group><article-title>Umap: Uniform manifold approximation and projection for dimension reduction</article-title><source>arXiv</source><year>2018</year><elocation-id>arXiv:1802.03426</elocation-id></element-citation></ref><ref id="R35"><label>[35]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>C</given-names></name><name><surname>Krenn</surname><given-names>M</given-names></name><name><surname>Eppel</surname><given-names>S</given-names></name><name><surname>Aspuru-Guzik</surname><given-names>A</given-names></name></person-group><article-title>Deep molecular dreaming: inverse machine learning for de-novo molecular design and interpretability with surjective representations</article-title><source>Machine Learning: Science and Technology</source><year>2021</year><volume>2</volume><elocation-id>03LT02</elocation-id></element-citation></ref><ref id="R36"><label>[36]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>J-N</given-names></name><etal/></person-group><article-title>t-smiles: a fragment-based molecular representation framework for de novo ligand design</article-title><source>Nature Communications</source><year>2024</year><volume>15</volume><elocation-id>4993</elocation-id><pub-id pub-id-type="pmcid">PMC11167009</pub-id><pub-id pub-id-type="pmid">38862578</pub-id><pub-id pub-id-type="doi">10.1038/s41467-024-49388-6</pub-id></element-citation></ref><ref id="R37"><label>[37]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Boyle</surname><given-names>N</given-names></name><name><surname>Dalke</surname><given-names>A</given-names></name></person-group><article-title>Deepsmiles: An adaptation of smiles for use in machine-learning of chemical structures</article-title><source>chemrxiv</source><year>2018</year><comment>chemrxiv:7097960</comment></element-citation></ref><ref id="R38"><label>[38]</label><element-citation publication-type="report"><person-group person-group-type="author"><name><surname>Achiam</surname><given-names>J</given-names></name><etal/></person-group><source>Gpt-4 technical report. arXiv</source><year>2023</year><elocation-id>arxiv:2303.08774</elocation-id></element-citation></ref><ref id="R39"><label>[39]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rong</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Self-supervised graph transformer on large-scale molecular data</article-title><source>Advances in neural information processing systems</source><year>2020</year><volume>33</volume><fpage>12559</fpage><lpage>12571</lpage></element-citation></ref><ref id="R40"><label>[40]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loshchilov</surname><given-names>I</given-names></name><name><surname>Hutter</surname><given-names>F</given-names></name></person-group><article-title>Fixing weight decay regularization in adam</article-title><source>arXiv</source><year>2017</year><volume>5</volume><fpage>5</fpage><elocation-id>arxiv:1711.05101</elocation-id></element-citation></ref><ref id="R41"><label>[41]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Irwin</surname><given-names>R</given-names></name><name><surname>Dimitriadis</surname><given-names>S</given-names></name><name><surname>He</surname><given-names>J</given-names></name><name><surname>Bjerrum</surname><given-names>EJ</given-names></name></person-group><article-title>Chemformer: a pre-trained transformer for computational chemistry</article-title><source>Machine Learning: Science and Technology</source><year>2022</year><volume>3</volume><elocation-id>015022</elocation-id></element-citation></ref><ref id="R42"><label>[42]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fabian</surname><given-names>B</given-names></name><etal/></person-group><article-title>Molecular representation learning with language models and domain-relevant auxiliary tasks</article-title><source>arXiv</source><year>2020</year><elocation-id>arxiv:2011.13230</elocation-id></element-citation></ref></ref-list></back><floats-group><boxed-text id="BX1" position="float" orientation="portrait"><caption><title>Scientific Contribution</title></caption><p>This study systematically evaluates how core design choices influence chemical language models. Although the performances on downstream tasks were often similar across configurations, we observed substantial differences in internal representations with atomwise tokenized SMILES representations producing more chemically structured latent spaces than representations based on SELFIES. By clarifying the effects of molecular representation format and tokenization strategy, our findings provide actionable guidance for the more informed and interpretable design of future CLMs.</p></boxed-text><fig id="F1" position="float"><label>Fig. 1</label><caption><title>Workflow of the systematic analysis of molecular representation, tokenization strategies, and architecture in chemical language models (CLMs).</title></caption><graphic xlink:href="EMS206003-f001"/></fig><fig id="F2" position="float"><label>Fig. 2</label><caption><title>Z-scores of different tasks of different model configurations grouped by their cosine similarity.</title><p>Z-scores were calculated from PR-AUC, except for the aggregated regression score which is based on RMSE. Model configurations are listed, starting from left to right, by molecule embedding, tokenisation, language model architecture and chirality representation.</p></caption><graphic xlink:href="EMS206003-f002"/></fig><fig id="F3" position="float"><label>Fig. 3</label><caption><title>Accuracy of three simpler scikit-learn classifiers on heterocycle and H-donor prediction of four pretrained models with different language model architectures and molecule representation with the atomwise tokeniser and implicit chirality representation.</title></caption><graphic xlink:href="EMS206003-f003"/></fig><fig id="F4" position="float"><label>Fig. 4</label><caption><p>UMAP visualization of molecular embeddings; Scatter plot depicting the latent space organization of 64 molecules from four distinct chemical families each: steroids (purple diamonds), beta-lactams (green triangles), tropanes (teal squares), and sulfonamides (blue crosses). Embeddings were generated from SELFIES or SMILES representations using RoBERTa or BART models with atomwise tokenization and implicit chirality representation.</p></caption><graphic xlink:href="EMS206003-f004"/></fig><fig id="F5" position="float"><label>Fig. 5</label><caption><title>PCA of carbon atom type embeddings of SMILES or SELFIES-based models BART and RoBERTa with atomwise tokeniser and implicit chirality.</title><p>The GAFF2 atom types have been determined by antechamber[<xref ref-type="bibr" rid="R27">27</xref>] and correspond to the following hybridizations: c: sp<sup>2</sup> in C=O, C=S, c2: sp<sup>2</sup> in aliphatic carbon, c3: sp<sup>3</sup>, ca: sp<sup>2</sup> in aromatic carbon. Amount of examples in brackets.</p></caption><graphic xlink:href="EMS206003-f005"/></fig></floats-group></article>