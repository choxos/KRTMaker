<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS207290</article-id><article-id pub-id-type="doi">10.1101/2025.07.11.664297</article-id><article-id pub-id-type="archive">PPR1051728</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Comparison of state-of-the-art error-correction coding for sequence-based DNA data storage</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Gimpel</surname><given-names>Andreas L.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Remschak</surname><given-names>Alex</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Stark</surname><given-names>Wendelin J.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Heckel</surname><given-names>Reinhard</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Grass</surname><given-names>Robert N.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>Department of Chemistry and Applied Biosciences, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/05a28rw58</institution-id><institution>ETH Zürich</institution></institution-wrap>, <addr-line>Vladimir-Prelog-Weg 1-5</addr-line>, <postal-code>8093</postal-code>, <city>Zürich</city>, <country country="CH">Switzerland</country></aff><aff id="A2"><label>2</label>Department of Electrical and Computer Engineering, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02kkvpp62</institution-id><institution>Technical University of Munich</institution></institution-wrap>, <addr-line>Arcistrasse 21</addr-line>, <postal-code>80333</postal-code>, <city>Munich</city>, <country country="DE">Germany</country></aff><author-notes><corresp id="CR1">
<label>*</label>
<email>robert.grass@chem.ethz.ch</email>
</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>17</day><month>07</month><year>2025</year></pub-date><pub-date pub-type="preprint"><day>16</day><month>07</month><year>2025</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">A wide range of codecs with vastly different error-correction approaches have been proposed and implemented for DNA data storage to date. However, while many codecs claim to provide superior performance, no studies have systematically benchmarked codec implementations to establish the current state-of-the-art in DNA data storage. In this study, we use standardized error scenarios – both <italic>in silico</italic> and <italic>in vitro</italic> – to compare the performance of six representative codecs from the literature. We find synthetic benchmarks commonly used in literature to be unsuitable indicators of codec performance, as our data shows that common experimental benchmarks fail to differentiate codecs under standardized conditions. Instead, we implement a comprehensive benchmark covering the major experimental parameters to assess codec performance under realistic DNA data storage conditions, while establishing important baselines for future codec development. Verifying our results with fair and standardized experiments, we demonstrate data storage at 43 EB g<sup>-1</sup> using synthesis by material deposition and 13 EB g<sup>-1</sup> using the more error prone electrochemical synthesis, employing only existing codecs from the literature. Besides closing in on the physical limits of DNA data storage, this study thus showcases the maturity of error-correction coding and defines its current state-of-the-art.</p></abstract><kwd-group><kwd>Error-correction coding</kwd><kwd>state-of-the-art</kwd><kwd>DNA data storage</kwd><kwd>storage density</kwd><kwd>high-density storage</kwd><kwd>clustering algorithms</kwd><kwd>benchmark</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Since the inception of DNA data storage, a major challenge has been defining the rules for reversibly converting between digital information and DNA sequences. This challenge, which falls within the realm of coding theory for DNA data storage, mainly involves the design and implementation of effective encoders and decoders (“codecs”). For this, early demonstrations of DNA data storage focused on source coding (i.e., compression) to efficiently encode the mostly text-based data used at the time,<sup><xref ref-type="bibr" rid="R1">1</xref></sup> e.g., by run-length encoding<sup><xref ref-type="bibr" rid="R2">2</xref></sup> or Huffmann codes.<sup><xref ref-type="bibr" rid="R3">3</xref>,<xref ref-type="bibr" rid="R4">4</xref></sup> Then, with the advent of array-based DNA synthesis and next-generation sequencing, channel coding came into focus to enable error-free recovery of binary data and overcome the necessity for manual intervention during decoding.<sup><xref ref-type="bibr" rid="R5">5</xref>–<xref ref-type="bibr" rid="R7">7</xref></sup> Still today, the majority of codecs employ linear block codes such as Reed-Solomon<sup><xref ref-type="bibr" rid="R7">7</xref>–<xref ref-type="bibr" rid="R9">9</xref></sup>, Fountain<sup><xref ref-type="bibr" rid="R10">10</xref>,<xref ref-type="bibr" rid="R11">11</xref></sup>, or repetition<sup><xref ref-type="bibr" rid="R6">6</xref>,<xref ref-type="bibr" rid="R12">12</xref></sup> codes with an inner/outer code separation strategy for error correction (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Table 2</xref> for an overview).<sup><xref ref-type="bibr" rid="R1">1</xref></sup></p><p id="P3">For sequence-based DNA data storage with Illumina sequencing, which has been used for the largest demonstrations of DNA data storage to date,<sup><xref ref-type="bibr" rid="R9">9</xref>,<xref ref-type="bibr" rid="R13">13</xref>,<xref ref-type="bibr" rid="R14">14</xref></sup> the use of channel coding is necessitated by the peculiar challenges posed by this workflow. First, array-based DNA synthesis is limited to sequence lengths of only a few hundred nucleotides (nt), requiring data segmentation across many individual sequences in an oligonucleotide pool.<sup><xref ref-type="bibr" rid="R13">13</xref>,<xref ref-type="bibr" rid="R15">15</xref></sup> Secondly, all biochemical steps of the DNA data storage workflow (see <xref ref-type="fig" rid="F1">Fig. 1a</xref>) introduce errors into sequences and affect their distribution, potentially leading to sequence loss.<sup><xref ref-type="bibr" rid="R16">16</xref>,<xref ref-type="bibr" rid="R17">17</xref></sup> Thus, codecs must simultaneously compensate for nucleotide errors and sequence dropout, motivating the widely-used code separation strategy with an inner and outer code.<sup><xref ref-type="bibr" rid="R1">1</xref>,<xref ref-type="bibr" rid="R13">13</xref></sup> Beyond these basic considerations however, the breadth of intended applications and available technologies for DNA data storage each present individual challenges and requirements. Accordingly, many codecs are designed to support specific applications’ error profiles (e.g., photolithographic synthesis,<sup><xref ref-type="bibr" rid="R18">18</xref></sup> aging-induced decay,<sup><xref ref-type="bibr" rid="R19">19</xref>,<xref ref-type="bibr" rid="R20">20</xref></sup> or nanopore sequencing<sup><xref ref-type="bibr" rid="R9">9</xref>,<xref ref-type="bibr" rid="R21">21</xref>,<xref ref-type="bibr" rid="R22">22</xref></sup>) or fulfill specific sequence constraints (e.g., GC content,<sup><xref ref-type="bibr" rid="R6">6</xref>,<xref ref-type="bibr" rid="R10">10</xref></sup> homopolymers,<sup><xref ref-type="bibr" rid="R5">5</xref>,<xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R10">10</xref></sup> k-mer frequency<sup><xref ref-type="bibr" rid="R11">11</xref>,<xref ref-type="bibr" rid="R23">23</xref></sup>, motifs<sup><xref ref-type="bibr" rid="R11">11</xref></sup>, or free energy<sup><xref ref-type="bibr" rid="R24">24</xref></sup>).</p><p id="P4">Besides the logical redundancy introduced by codecs, the inherent presence of many sequence copies during biochemical processing provides an additional layer of redundancy in DNA data storage, termed physical redundancy.<sup><xref ref-type="bibr" rid="R13">13</xref>,<xref ref-type="bibr" rid="R16">16</xref></sup> However, achieving DNA’s extreme storage densities (theoretically up to 227 EB g<sup>-1</sup> for double-stranded DNA, see Supplementary Note 2) requires minimizing both logical and physical redundancy simultaneously, while maintaining sufficient redundancy to facilitate error-free decoding.<sup><xref ref-type="bibr" rid="R25">25</xref></sup> As a result, while logical redundancy is directly related to synthesis cost, only the product of logical and physical redundancy is relevant for the storage system’s data density. Complicating matters further, sequencing also yields multiple reads per sequence, providing another source of redundancy through sequencing depth.<sup><xref ref-type="bibr" rid="R13">13</xref>,<xref ref-type="bibr" rid="R16">16</xref></sup> Evidently, there exist a trade-off between these forms of redundancy which is not reflected in the encoder’s code rate (i.e., data bits stored per nucleotide). Thus, any attempt at isolating the performance of a codec from these other sources of redundancy is futile without sufficient standardization of experimental conditions.</p><p id="P5">Nonetheless, codecs are still commonly evaluated across studies by simply comparing the level of logical redundancy used (i.e., the encoder’s code rate), despite the often vastly different experimental conditions (e.g., synthesis provider, physical redundancy, sequencing depth). <sup><xref ref-type="bibr" rid="R11">11</xref>,<xref ref-type="bibr" rid="R24">24</xref>,<xref ref-type="bibr" rid="R26">26</xref></sup> In recent years, codecs have also been increasingly compared through <italic>in silico</italic> simulations that artificially vary the error rate in their input data.<sup><xref ref-type="bibr" rid="R11">11</xref>,<xref ref-type="bibr" rid="R23">23</xref>,<xref ref-type="bibr" rid="R24">24</xref></sup> However, these comparisons lack an established baseline and the extent of their standardization (e.g., code rate, sequence length, file size) often remains unclear. As a notable exception, Ping et al.<sup><xref ref-type="bibr" rid="R24">24</xref></sup> were the first to evaluate two codecs in a standardized experiment, using a serial dilution as a benchmark. Nonetheless, the absence of a generally accepted state-of-the-art and standardized benchmarks with experimental relevance currently impedes objective assessment of codec performance.</p><p id="P6">In this study, we systematically benchmark codecs for DNA data storage both <italic>in silico</italic> and <italic>in vitro</italic> to establish the current state-of-the-art. For this, six representative codecs selected from the literature in October 2023 (see <xref ref-type="table" rid="T1">Table 1</xref>) were tested in multiple standardized scenarios across a range of experimental conditions centered around the most common, sequence-based data storage workflow with Illumina sequencing.<sup><xref ref-type="bibr" rid="R13">13</xref>,<xref ref-type="bibr" rid="R14">14</xref></sup> In doing so, we demonstrate the benefits of read clustering on codec performance, assess the transferability of synthetic benchmarks to realistic scenarios, and evaluate the limits of common experimental benchmarks. We establish and verify an experimental benchmark for codec performance, demonstrating the capabilities of existing codecs to achieve record-breaking storage densities. Our work presents an unbiased and standardized assessment of the current state-of-the-art in error-correction coding for DNA data storage, thereby providing both a suitable baseline and a framework for benchmarking for future studies.</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>Selecting and standardizing codecs for benchmarking</title><p id="P7">For this study, we limited our benchmarking of codec performance to representative examples, using the availability of an open-source implementation, prominence in literature, and broad coverage of error-correction approaches as selection criteria (see <xref ref-type="sec" rid="S10">Methods</xref>, cut-off date October 2023). As a result, we selected DNA-Aeon by Welzel et al.<sup><xref ref-type="bibr" rid="R11">11</xref></sup>, DNA Fountain by Erlich and Zielinski<sup><xref ref-type="bibr" rid="R10">10</xref>,<xref ref-type="bibr" rid="R27">27</xref></sup>, DNA-RS by Heckel<sup><xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R28">28</xref>,<xref ref-type="bibr" rid="R29">29</xref></sup>, an implementation of the codec used by Goldman et al.<sup><xref ref-type="bibr" rid="R6">6</xref></sup> (“Goldman”), HEDGES by Press et al.<sup><xref ref-type="bibr" rid="R30">30</xref></sup>, and Yin-Yang by Ping et al.<sup><xref ref-type="bibr" rid="R24">24</xref></sup> (see <xref ref-type="table" rid="T1">Table 1</xref> for an overview, and <xref ref-type="supplementary-material" rid="SD1">Supplementary Table 2</xref> for a list of candidate codecs). All codecs were standardized to facilitate impartial performance comparisons by choosing their parameters – as far as possible – such that they yield similar code rates (0.50, 1.00, and 1.50 bit nt<sup>-1</sup>) and sequence lengths (around 150 nt). These selected parameters are listed in <xref ref-type="supplementary-material" rid="SD1">Supplementary Tables 4-8</xref>. Minor changes to some codec implementations were also required to facilitate automated testing and ensure representative performance (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Note 1</xref>).</p><p id="P8">For an initial comparison, a basic error scenario was implemented, analogous to the most common synthetic performance benchmarks in the literature.<sup><xref ref-type="bibr" rid="R11">11</xref>,<xref ref-type="bibr" rid="R23">23</xref>,<xref ref-type="bibr" rid="R24">24</xref>,<xref ref-type="bibr" rid="R30">30</xref></sup> This scenario randomly introduces single-nucleotide errors to create 30 erroneous copies of each sequence generated by a codec, at a variable rate and specified composition (53% substitutions, 45% deletions, and 2% insertions, resembling the error pattern in Ref. <sup><xref ref-type="bibr" rid="R17">17</xref></sup>). Using this approach, a broad range of error rates were iteratively sampled in order to identify the error rate at which decoding started to fail (for a total of 30 decoding attempts per condition, see <xref ref-type="fig" rid="F1">Fig. 1b</xref> and <xref ref-type="sec" rid="S10">Methods</xref>). Then, as our standard metric of error tolerance, we report the error rate at which decoding still succeeded with 95% probability, based on a logistic regression of all 30 decoding outcomes (solid lines in <xref ref-type="fig" rid="F1">Fig. 1b</xref>). Notably, decoding was further constrained to one hour, 8 GB of memory, and one CPU core per attempt, using an input file of 19 kB. This enforced a decoding speed of at least 5.4 bytes per second, and negated any undue disadvantage due to a lack of parallelization in a codec’s implementation.</p><p id="P9"><xref ref-type="fig" rid="F1">Fig. 1d</xref> shows the error tolerance of all codecs in this basic error scenario, without clustering and consensus generation by alignment (“Naïve”, grey bars). Unsurprisingly, lower code rates (i.e., higher redundancy) led to increased error tolerances for all codecs. However, the time constraint limited decoding performance in several runs, especially for the DNA-Aeon codec (symbolized by in <xref ref-type="fig" rid="F1">Fig. 1d</xref>, see also <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 1</xref>). As a result, DNA-Aeon failed to decode the data within the time limit at error rates above 0.3%, irrespective of code rate. In contrast, the HEDGES codec at 0.63 bit nt<sup>-1</sup> code rate exhibited the highest error tolerance at 7.7%, more than double that of the runner-up, DNA-RS with 3.3% at 0.50 bit nt<sup>-1</sup> at the given decoding time constraint Thus, based only on this commonly used synthetic performance benchmark, the HEDGES codec would be considered the best-performing codec of our selection from the literature.</p></sec><sec id="S4"><title>Testing the benefits of read clustering on codecs’ error tolerance</title><p id="P10">A codec’s decoding capability is directly related to the balance between available redundancy and error frequency in the data. Contrary to most traditional coding channels however, the sequencing data used for decoding in DNA data storage is inherently replicated, i.e., multiple erroneous reads of each sequence are available. This inherent repetition code can be exploited by generating a less erroneous consensus sequence from the individual reads via clustering (see <xref ref-type="fig" rid="F1">Fig. 1a</xref>). Therefore, read clustering promises to increase a codec’s error tolerance without the need for additional logical redundancy, which is surprisingly seldomly exploited in the literature (see <xref ref-type="table" rid="T1">Table 1</xref>).</p><p id="P11">To assess the benefits of clustering on codec performance, we selected both established clustering algorithms from bioinformatics (CD-HIT, MMseqs2, Starcode)<sup><xref ref-type="bibr" rid="R31">31</xref>–<xref ref-type="bibr" rid="R33">33</xref></sup> as well as specialized DNA data storage clustering algorithms (LSH, Clover)<sup><xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R34">34</xref></sup> from the literature. First, parameters for all clustering algorithms were selected which maximized their sensitivity, accuracy, and specificity (see <xref ref-type="sec" rid="S10">Methods</xref> and <xref ref-type="supplementary-material" rid="SD1">Supplementary Table 1</xref>). Then, each codec was paired with each clustering algorithm to assess compatibility and quantify improvements in error-correction capabilities. For this, the basic error scenario – the introduction of random errors into 30 sequence copies – was reused.</p><p id="P12">Comparing the previously established error tolerances without clustering (“Naïve”, grey bars in <xref ref-type="fig" rid="F1">Fig. 1d</xref>) to the best-performing clustering algorithm for each codec (colored bars in <xref ref-type="fig" rid="F1">Fig. 1d</xref>) revealed that clustering and consensus generation improved codec performance in all cases. On average, tolerated error rates increased by 6.5±2.5% in absolute terms, effectively more than doubling the error tolerance of most codecs. This matches previous results on the error-correction capability of consensus generation,<sup><xref ref-type="bibr" rid="R35">35</xref></sup> and highlights the benefits of exploiting the inherent redundancy in sequencing data. The latter is best illustrated by the performance of the Yin-Yang codec, an optimal bit-to-base coding scheme without error-correction capabilities:<sup><xref ref-type="bibr" rid="R24">24</xref></sup> it exhibited an error tolerance of 4.2% while relying solely on the indirect error correction conveyed by clustering. Thereby, its performance represents the baseline for every codec with additional error-correction capabilities in this basic error scenario.</p><p id="P13">Across all codecs, the tolerated error rate with clustering often exceeded 5%, well above the error rates commonly encountered after commercial synthesis and in common workflows.<sup><xref ref-type="bibr" rid="R16">16</xref>,<xref ref-type="bibr" rid="R17">17</xref>,<xref ref-type="bibr" rid="R25">25</xref></sup> Additional tests considering each error type in isolation (see <xref ref-type="fig" rid="F1">Fig. 1c</xref>) also did not expose any major differences in error tolerance between error types. Notably, the performance of DNA-Aeon improved most drastically with clustering, achieving an error tolerance of 7.7% at 1.50 bit nt<sup>-1</sup>. Evidently, the reduced workload conveyed by clustering lifted its limitation by the time constraint (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 1</xref>). This highlights another benefit to read clustering besides the exploitation of sequencing data’s inherent redundancy: the acceleration of decoding pipelines through the reduction of codec workload by 1-2 orders of magnitude (depending on sequencing depth).</p><p id="P14">Interestingly, nine of the thirteen tested codecs and code rates performed best with the established general-purpose clustering algorithm CD-HIT (red bars in <xref ref-type="fig" rid="F1">Fig. 1d</xref>). In addition, several combinations of clustering algorithms and codecs failed completely (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Table 3</xref> for full results). This suggests incompatibilities exist between these clustering algorithms and the sequence features generated by some codecs (e.g., indexing regions, overlapping sections in the Goldman codec). Given the universal benefits of clustering identified above, the pairing between codecs and clustering algorithms established in <xref ref-type="fig" rid="F1">Fig. 1d</xref> were used for all further <italic>in silico</italic> studies and <italic>in vitro</italic> experiments.</p></sec><sec id="S5"><title>Evaluating codecs’ tolerance to errors and sequence dropout simultaneously</title><p id="P15">Errors within the DNA sequence are not the only type of fault occurring in the DNA data storage channel. Also sequence dropout, i.e., the absence of reads from some sequences in the sequencing data, requires correction by a codec.<sup><xref ref-type="bibr" rid="R13">13</xref>,<xref ref-type="bibr" rid="R16">16</xref></sup> However, codecs’ tolerance to sequence dropout is rarely quantified in the literature, especially in combination with variable error rates. Thus, we extended the aforementioned basic error scenario with another variable, the fraction of sequences lost. This enabled the simultaneous quantification of codecs’ tolerance towards errors and sequence dropout, uncovering any considerable tradeoffs. <xref ref-type="fig" rid="F1">Fig. 1e</xref> shows the resulting Pareto fronts, delimiting each codec’s feasible region for the simultaneous correction of errors and sequence dropout.</p><p id="P16">In accordance with the previous results, lower code rates (i.e., higher redundancy) considerably extended each codec’s feasible regions in <xref ref-type="fig" rid="F1">Fig. 1e</xref>, by increasing their tolerances to both errors and sequence dropout. However, the extent to which each codec tolerated sequence dropout, even at minimal error rates, differed considerably. Especially HEDGES, previously identified as best-performing codec based on raw error-correction capability, only tolerated up to 7.8% and 10.5% sequence dropout at 1.07 bit nt<sup>-1</sup> and 0.63 bit nt<sup>-1</sup>, respectively. This improves only slightly on the 7.9% sequence dropout tolerated by the Goldman codec, with its basic repetition code at 0.34 bit nt<sup>-1</sup>. In contrast, DNA-Fountain – including only a small RS code within each sequence to detect rather than correct an erroneous sequence<sup><xref ref-type="bibr" rid="R10">10</xref></sup> – previously exhibited a low error-correction capability (up to around 5% nt<sup>-1</sup>) across all code rates. However, its tolerance to sequence dropout increased drastically from 3% at 1.50 bit nt<sup>-1</sup> up to 63% at 0.50 bit nt<sup>-1</sup>. Evidently, each codec’s tolerances to errors and sequence dropout are balanced differently, such that neither metric in isolation can be considered as a robust performance benchmark.</p><p id="P17">In showcasing each codec’s balance between correction of errors and compensation for sequence loss, <xref ref-type="fig" rid="F1">Fig. 1e</xref> also highlights the benefits of a balanced approach to redundancy. The DNA-RS codec – and DNA-Aeon to a lesser extent – dominated the feasible regions of DNA Fountain and HEDGES, unless extreme sequence dropout (&gt; 50%) or error rates (&gt; 12%) occurred. Considering additionally that especially DNA-Aeon (at all code rates), but also DNA-RS (at 0.50 bit nt<sup>-1</sup>), were partially limited by the time constraint (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 3</xref>) at these extremes, these two codecs emerge as broadly superior in this synthetic analysis of errors and sequence dropout.</p></sec><sec id="S6"><title>Benchmarking codecs with common literature experiments</title><p id="P18">While the previous scenarios clearly showcased each codec’s theoretical capabilities under synthetic conditions, they are only a simplified representation of DNA data storage’s true error channel. Most notably, error rates and sequence dropout are exclusively the result of workflow choices, rather than an independently controllable experimental variable. Moreover, the non-ideal error patterns and biases in experimental sequencing data – from error runs to skewed coverage distributions – must be considered to draw informative conclusions about codec performance. Thus, three benchmarking experiments used by Erlich and Zielinski<sup><xref ref-type="bibr" rid="R10">10</xref></sup>, Organick et al.<sup><xref ref-type="bibr" rid="R9">9</xref>,<xref ref-type="bibr" rid="R25">25</xref></sup>, and Ping et al.<sup><xref ref-type="bibr" rid="R24">24</xref></sup> were replicated <italic>in silico</italic> using the models implemented in the simulation software DT4DDS<sup><xref ref-type="bibr" rid="R17">17</xref></sup> (see <xref ref-type="sec" rid="S10">Methods</xref>). This simulation software takes into account the aforementioned non-idealities, such as poor pool homogeneity, deletion runs, and the bias introduced by PCR, which are missing from conventional synthetic tests. The three chosen scenarios covered the most common experiments in DNA data storage: a serial dilution to assess maximum storage density, a serial amplification to demonstrate copyability, and read down-sampling to quantify minimum sequencing depth (see <xref ref-type="fig" rid="F2">Fig. 2</xref>).</p><p id="P19">In accordance with the original studies’ results (gray lines in <xref ref-type="fig" rid="F2">Fig. 2</xref>), our standardized, in silico replications of these literature benchmarks showcased the possibility of data recovery at high storage density and after deep replication. However, the serial dilution and serial amplification benchmarks exposed only minor performance differences between codecs at best (blue and orange bars in <xref ref-type="fig" rid="F2">Fig. 2</xref>). In the serial dilution benchmark, the iterative 10-fold dilutions only partially resolved any differences between code rates of the same codec. This is expected, given that this benchmark uses oligo pools synthesized by material deposition (i.e., Twist Bioscience), whose high homogeneity and low error rate pose few challenges to codecs even at physical redundancies as low as 10x. <sup><xref ref-type="bibr" rid="R25">25</xref></sup> In the serial amplification benchmark, deep replication of these oligo pools by a high-fidelity (i.e., low-error) polymerase thus expectedly also did not lead to any discernable difference between codecs. Evidently, all codec implementations – with the exception of the Goldman codec – are similarly performant in these two literature benchmarks under equal and controlled conditions. This renders these two literature benchmarks ill-suited for comparing codec performance experimentally.</p><p id="P20">In contrast to the other two literature benchmarks, the down-sampling of sequencing data did expose considerable differences between codecs (green bars in <xref ref-type="fig" rid="F2">Fig. 2</xref>). Given the low error rates associated with synthesis by material deposition,<sup><xref ref-type="bibr" rid="R17">17</xref></sup> this benchmark relies mostly on tolerance to sequence dropout rather than error correction, thereby it is expected to favor codecs such as DNA Fountain. However, both DNA-Aeon and DNA-RS outperform DNA Fountain considerably across all code rates, requiring as few as only one sequencing read per sequence on average at 0.50 bit nt<sup>-1</sup> (i.e., a sequencing depth of 1). Ostensibly, the low sequencing depth in this benchmark negated the benefits from clustering (as shown by the poor performance of the Yin-Yang codec) and thus enforced efficient use of the available read data. These results suggest read down-sampling is the only experimental benchmark sufficiently informative for codec comparisons out of the three literature experiments tested.</p></sec><sec id="S7"><title>Benchmarking codecs across common experimental workflow choices</title><p id="P21">From an experimental perspective, three parameters matter most for any DNA data storage workflow: the synthesis provider, the number of oligos per sequence during storage (“physical redundancy”), and the number of sequencing reads per sequence (“sequencing depth”). <sup><xref ref-type="bibr" rid="R13">13</xref>,<xref ref-type="bibr" rid="R16">16</xref>,<xref ref-type="bibr" rid="R17">17</xref></sup> While the literature benchmarks presented above offer straightforward workflows, they still consider only the latter two parameters in isolation. Thus, we set out to compare the selected codecs across all three experimental parameters directly. First, we implemented two scenarios in DT4DDS (see <xref ref-type="sec" rid="S10">Methods</xref> and <xref ref-type="fig" rid="F3">Fig. 3b</xref>), centered around the two most-commonly used array-based synthesis technologies: material deposition / printing (i.e., Agilent, Twist Biosciences) and electrochemical synthesis (i.e., Genscript/CustomArray).<sup><xref ref-type="bibr" rid="R13">13</xref></sup> The former is included in the high-uniformity, low-error scenario (“high-fidelity”) using a high-fidelity polymerase for amplification and yielding an error rate of around 0.1% with minor sequence loss (&lt;1% without dilution).<sup><xref ref-type="bibr" rid="R17">17</xref></sup> In contrast, the low-uniformity, high-error scenario (“low-fidelity”) includes electrochemical synthesis and amplification by an error-prone polymerase, yielding an error rate of around 1.5% and considerable sequence dropout (&gt;2% without dilution).<sup><xref ref-type="bibr" rid="R17">17</xref></sup> Within these two scenarios, we then varied the physical redundancy and sequencing depth akin to the synthetic scenario in <xref ref-type="fig" rid="F1">Fig. 1e</xref>, thereby simultaneously optimizing for storage density and reading cost.</p><p id="P22">The Pareto fronts illustrating the tradeoff between physical redundancy and sequencing depth in the two scenarios are shown in <xref ref-type="fig" rid="F3">Fig. 3a</xref>. Focusing first on the high-fidelity scenario (top half of <xref ref-type="fig" rid="F3">Fig. 3a</xref>, average error rate 0.1%), a considerable separation of codecs was observed across all code rates. The Yin-Yang codec, as the baseline without additional error-correction capabilities, showcases how clustering and the inherent repetition in sequencing reads suffice to yield a storage density of 6.6 EB g<sup>-1</sup> with a sequencing depth of 30x (32x physical redundancy at 1.85 bit nt<sup>-1</sup> code rate, see <xref ref-type="fig" rid="F3">Fig. 3c</xref>). Both DNA Fountain (15 EB g<sup>-1</sup> at 7.6x and 1.00 bit nt<sup>-1</sup>) as well as HEDGES (38 EB g<sup>-1</sup> at 3.2x and 1.07 bit nt<sup>-1</sup>) considerably improved upon this baseline, despite their limited ability to tolerate considerable sequence dropout and large error rates simultaneously. In contrast, both DNA-Aeon and DNA-RS tolerated physical redundancies and sequencing depths below 10x even at a code rate of 1.50 bit nt<sup>-1</sup>. At a code rate of 0.50 bit nt<sup>-1</sup>, these codecs required only around one physical copy and one sequencing read per reference sequence on average, in-line with the results in <xref ref-type="fig" rid="F2">Fig. 2</xref>. As a result, both DNA-Aeon and DNA-RS achieved storage densities well above the current state-of-the-art (28 EB g<sup>-1</sup>, see <xref ref-type="supplementary-material" rid="SD1">Supplementary Note 2</xref> and Organick et al.<sup><xref ref-type="bibr" rid="R25">25</xref></sup>) in this scenario (about 117 EB g<sup>-1</sup> at 30x sequencing depth, see <xref ref-type="fig" rid="F3">Fig. 3c</xref>).</p><p id="P23">Expectedly, the low-fidelity scenario (bottom half of <xref ref-type="fig" rid="F3">Fig. 3a</xref>, average error rate 1.5%) and the significantly increased error load under these conditions, challenged codecs considerably more. Thus, required sequencing depths and physical redundancies were about 1-2 order of magnitude larger than in the high-fidelity scenario. Surprisingly, the DNA Fountain, Goldman, and Yin-Yang codecs were unable to decode the data in the low-fidelity scenario at all. As these codecs tolerated much larger average error rates in the synthetic benchmarks (see <xref ref-type="fig" rid="F1">Fig. 1e</xref>, dotted line), this observation further questions the transferability of synthetic results to experimental workflows. The other codecs – DNA-Aeon, DNA-RS, and HEDGES – achieved generally similar performance, albeit with slight advantages for DNA-Aeon at 1.00 bit nt<sup>-1</sup> and DNA-RS at 0.50 bit nt<sup>-1</sup>. Nonetheless, the overall performance falls drastically short compared to the storage densities obtained in the high-fidelity scenario, peaking at 17 EB g<sup>-1</sup> for DNA-Aeon at 1.00 bit nt<sup>-1</sup> (6.7x physical coverage, see <xref ref-type="fig" rid="F3">Fig. 3c</xref>). Additionally, both DNA-Aeon and DNA-RS were partially limited by the time constraint, as shown in <xref ref-type="supplementary-material" rid="SD1">Supplementary Figs. 6+7</xref>.</p><p id="P24">Notably, most Pareto fronts in both scenarios feature a clear symmetry with respect to the extremes of physical redundancy and sequencing depth. This suggests down-sampling sequencing data to assess minimum sequencing depth – one of the literature benchmarks tested above – is a good proxy for the minimum physical redundancy required during storage. However, the Pareto fronts in <xref ref-type="fig" rid="F3">Fig. 3a</xref> demonstrate clearly that the validity of this approach relies upon the other parameter being non-limiting. For example, while DNA-RS at 1.00 bit nt<sup>-1</sup> exhibited a minimum sequencing depth of 1x in the read down-sampling (see <xref ref-type="fig" rid="F2">Fig. 2</xref>), recovering data with this codec after storage at a 1x physical redundancy would require a sequencing depth exceeding 20x.</p><p id="P25">Interestingly, the highest storage densities of DNA-Aeon (140 EB g<sup>-1</sup>, 0.50 bit nt<sup>-1</sup>, physical redundancy 0.49x) and DNA-RS (125 EB g<sup>-1</sup>, 1.00 bit nt<sup>-1</sup>, physical redundancy 0.97x) at 30x sequencing depth were not achieved at the highest code rate tested (i.e., 1.50 bit nt <sup>-1</sup>). This suggests the major benefit of additional logical redundancy – enabling further reduction in physical redundancy – considerably outweighs the loss in code rate. On the other hand, these results also indicate that current state-of-the-art codecs are sufficient for high-density, high-fidelity data storage well beyond currently demonstrated limits.</p></sec><sec id="S8"><title>Experimental replications demonstrating state-of-the-art storage densities</title><p id="P26">Experimental replications of the high- and low-fidelity scenarios <italic>in vitro</italic> were performed to establish the accuracy of our <italic>in silico</italic> analysis and confirm the validity of our observations regarding storage density. For this, adjusted codec parameters were used to store files with 5 kB, 17 kB, and 19 kB into a total of 11 293 sequences using all six selected codecs. These parameters (detailed in <xref ref-type="supplementary-material" rid="SD1">Supplementary Tables 4-8</xref>) were chosen to target code rates of 1.00 bit nt<sup>-1</sup>, 1.50 bit nt<sup>-1</sup>, and the highest code rate supported by each codec, while adhering to a length limit of 126 nt (a constraint of electrochemical synthesis, see <xref ref-type="sec" rid="S10">Methods</xref>). After synthesis by Genscript and Twist Biosciences, both oligo pools were amplified, diluted, re-amplified, and sequenced as outlined in <xref ref-type="fig" rid="F3">Fig. 3b</xref> for the synthetic scenarios. <xref ref-type="fig" rid="F4">Figure 4</xref> shows the decoding results of these <italic>in vitro</italic> replications of the low- and high-fidelity workflows, using five different physical redundancies at a fixed sequencing depth of 30x. The decoding results predicted from our <italic>in silico</italic> analysis are also shown for comparison (green shading).</p><p id="P27">Overall, the experimental results (see <xref ref-type="fig" rid="F4">Fig. 4</xref>) closely follow the trends observed in our <italic>in silico</italic> analysis of the workflows (see <xref ref-type="fig" rid="F3">Fig. 3a</xref>). Expectedly, the low-fidelity scenario proved more challenging than the high-fidelity scenarios across all codecs. Nonetheless, DNA-Aeon and DNA-RS performed best, both capable of recovering the data stored at physical redundancies as low as 2x in the high-, and 5x in the low-fidelity scenario (at a code rate of 1.0 bit nt<sup>-1</sup>). Taking into account the physical redundancies measured by qPCR (see <xref ref-type="fig" rid="F4">Fig. 4</xref> and <xref ref-type="supplementary-material" rid="SD1">Supplementary Tables 12+13</xref>), this equals storage densities of 57 EB g<sup>-1</sup> and 17 EB g<sup>-1</sup>, respectively (43 EB g<sup>-1</sup> and 13 EB g<sup>-1</sup> including adapters, see <xref ref-type="supplementary-material" rid="SD1">Supplementary Table 9</xref>). These results not only validate the accuracy of our benchmarks in <xref ref-type="fig" rid="F3">Fig. 3</xref>, but also considerably improve upon previously demonstrated achievable data densities for DNA data storage (26 EB g<sup>-1</sup> using high-fidelity workflow by Organick et al.<sup><xref ref-type="bibr" rid="R25">25</xref></sup>, 0.033 EB g<sup>-1</sup> using low-fidelity workflow by Grass et al.<sup><xref ref-type="bibr" rid="R7">7</xref></sup>, see <xref ref-type="supplementary-material" rid="SD1">Supplementary Table 9</xref>).</p><p id="P28">High-density data storage in the high-fidelity scenario was also achieved with HEDGES (physical redundancy of 2x, equivalent to 56 EB g<sup>-1</sup>). However, the HEDGES codec was less capable in the low-fidelity scenario, maxing out at a physical redundancy of 50x (52x by qPCR, equivalent to 2.2 EB g<sup>-1</sup>). More generally, most codecs failed to decode the data in the low-fidelity scenario at code rates above 1.0 bit nt<sup>-1</sup>, even at 1000x physical redundancy (see <xref ref-type="fig" rid="F4">Fig.4</xref>, right). The DNA-RS codec is the only exception in our test, achieving successful decoding with 1.50 bit nt<sup>-1</sup> down to 10x physical redundancy (12.5x by qPCR, 14 EB g<sup>-1</sup>). This highlights the ability of the low-fidelity scenario to differentiate codecs by testing their tolerance to both errors and sequence dropout. In contrast, all codecs with error-correction capabilities worked reliably at 1.5 bit nt<sup>-1</sup> and 1000x physical redundancy in the high-fidelity scenario.</p><p id="P29"><xref ref-type="fig" rid="F4">Figure 4</xref> also shows the probability of successful decoding derived for the experimental conditions from <italic>in silico</italic> simulations. These decoding probabilities show good agreement with the experimental results, highlighting the overall accuracy of the <italic>in silico</italic> simulations. However, systematic deviations to experimental results were present in some cases. In the high-fidelity scenario (<xref ref-type="fig" rid="F4">Fig. 4</xref>, left), the capabilities of the DNA-Aeon, DNA Fountain and Yin-Yang codecs were overestimated. In contrast, these simulations were too pessimistic with respect to the HEDGES and Goldman codecs. The predictions in the low-fidelity scenario (<xref ref-type="fig" rid="F4">Fig. 4</xref>, right) – while generally more accurate – tended to slightly overestimate the capabilities of the DNA-Aeon and HEDGES codecs.</p><p id="P30">To elucidate the source of these systematic deviations, the homogeneity of error rates and sequence dropout in the experimental datasets was assessed. While the rates of errors were similar across codecs, the rate of sequence dropout varied drastically between codecs at the same physical redundancy (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 9</xref>). Upon closer inspection, these variations were caused by systematic differences in the coverage homogeneity of each codec’s sequences (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Figs. 10+11</xref>). Evidently, the sequences generated by some codecs were less homogeneously represented in the oligo pools than others, with coefficients of variation at 1000x physical coverage ranging from 0.38 to 1.15. As all sequences were synthesized on the same chips in a randomized order, this inhomogeneity does not impact the fairness of the experiments. Instead, the differences in pool homogeneity likely resulted from biases during amplification, potentially caused by certain sequence features.<sup><xref ref-type="bibr" rid="R36">36</xref>,<xref ref-type="bibr" rid="R37">37</xref></sup> For example, the sequences generated by the Goldman codec were least abundant in both scenarios, at half the mean sequencing depth (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 9</xref>). Here, the repetitive elements introduced by the codec could inhibit amplification by enabling secondary structures.<sup><xref ref-type="bibr" rid="R38">38</xref></sup> In-line with this reasoning, its sequences are underrepresented in the sequencing data (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 9</xref>), and thus less likely to be sampled during dilution. This explains the increased rates of sequence dropout from these codecs, which cause the systematic deviations to the simulation results (additional discussion is provided in <xref ref-type="supplementary-material" rid="SD1">Supplementary Note 3</xref>). However, these deviations in copy numbers in the physical pool do not explain the much larger differences between codec performances across physical redundancies in <xref ref-type="fig" rid="F4">Figure 4</xref>.</p></sec></sec><sec id="S9" sec-type="discussion"><title>Discussion</title><p id="P31">The comparison of error-correction coding for DNA data storage in this work highlights the maturity of the field while closing in on DNA’s physical limits. To do so, this study systematically harmonized six established codecs across three code rates to perform fair performance comparisons in standardized scenarios. This study thereby comprehensively established and experimentally verified the current state-of-the-art in error-correction coding for DNA data storage under unbiased and standardized conditions.</p><p id="P32">Using synthetic error benchmarks, this study first isolates codec performance from experimental factors by assessing codecs’ tolerance to errors and sequence dropout both individually and simultaneously. Then, using literature experiments replicated <italic>in silico</italic>, we identify shortcomings in commonly performed experimental benchmarks. Taken together, our results thus challenge common practice in the DNA data storage literature, as both synthetic error benchmarks and serial dilution/amplification experiments are widely-used standards to showcase codec performance.<sup><xref ref-type="bibr" rid="R10">10</xref>,<xref ref-type="bibr" rid="R11">11</xref>,<xref ref-type="bibr" rid="R23">23</xref>,<xref ref-type="bibr" rid="R24">24</xref></sup></p><p id="P33">As an alternative, rigorous benchmark, we then systematically assessed codecs across three critical experimental factors often confounded with codec performance: synthesis technology, physical redundancy, and sequencing depth. Importantly, we show that only little logical redundancy is required for high-density storage (i.e., up to 7 EB g<sup>-1</sup>) using the common high-fidelity scenario (i.e., synthesis by Twist Biosciences and amplification with high-fidelity polymerases). In contrast, enabling data storage at low physical redundancy and low sequencing depth required high levels of balanced logical redundancy (i.e., DNA-Aeon and DNA-RS), theoretically enabling storage densities as high as 117 EB g<sup>-1</sup> (see <xref ref-type="fig" rid="F3">Fig. 3c</xref>).</p><p id="P34">An experimental replication of our rigorous benchmark constitutes an objective, experimental benchmark of standardized codecs from the literature, using multiple code rates and both a high- and low-fidelity scenario. We experimentally demonstrate data storage at 43 EB g<sup>-1</sup> and 13 EB g<sup>-1</sup> in the high- and low-fidelity scenarios respectively, using only these established codecs from the literature. These results mark a major step towards the theoretical limit for data storage in DNA, at 227 EB g<sup>-1</sup> (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Note 2</xref>), first postulated by Church et al.<sup><xref ref-type="bibr" rid="R5">5</xref></sup> in 2012. Moreover, they improve upon both the first experimental investigation of storage density by Erlich and Zielinski <sup><xref ref-type="bibr" rid="R10">10</xref></sup> in 2017, at 215 PB g<sup>-1</sup>, and the subsequent improvement by Organick et al.<sup><xref ref-type="bibr" rid="R25">25</xref></sup> to 17 EB g<sup>-1</sup> in 2020. Additionally, our experimental demonstration of data storage at 13 EB g<sup>-1</sup> in the low-fidelity scenario represents the first experimental investigation of maximum storage density using error-prone electrochemical DNA synthesis. In doing so, our approach to experimental benchmarking ensured an unbiased comparison and precluded any discrimination against a codec by the experimenter.</p><p id="P35">All in all, this work provides strong evidence against the use of non-standardized, synthetic error benchmarks as the foremost performance indicators for codecs. Instead, codec comparisons in the future should harmonize codec parameters, test for tolerance against errors and sequence dropout simultaneously, and include a comparison with the state-of-the-art. Based on our results, this would include at least a standardized read down-sampling experiment (either <italic>in silico</italic>, e.g., with DT4DDS<sup><xref ref-type="bibr" rid="R17">17</xref></sup>, or ideally <italic>in vitro</italic>, if possible), comparing against either DNA-Aeon<sup><xref ref-type="bibr" rid="R11">11</xref></sup> or DNA-RS<sup><xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R28">28</xref>,<xref ref-type="bibr" rid="R29">29</xref></sup> as the state-of-the-art. Beyond raw codec performance, decoding speed also emerged as another major limitation throughout this study, while considerable untapped potential from read clustering was identified.</p><p id="P36">A major limitation of this work is the small selection of investigated codecs and clustering algorithms, as well as the lack of a systematic optimization of their parameters. Moreover, the investigated scenarios omit less common workflows, such as photolithographic<sup><xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R39">39</xref></sup> or enzymatic<sup><xref ref-type="bibr" rid="R40">40</xref>,<xref ref-type="bibr" rid="R41">41</xref></sup> synthesis, nanopore sequencing,<sup><xref ref-type="bibr" rid="R21">21</xref>,<xref ref-type="bibr" rid="R22">22</xref></sup> aging-induced decay,<sup><xref ref-type="bibr" rid="R19">19</xref>,<xref ref-type="bibr" rid="R42">42</xref></sup> or the use of degenerate bases.<sup><xref ref-type="bibr" rid="R23">23</xref>,<xref ref-type="bibr" rid="R43">43</xref>,<xref ref-type="bibr" rid="R44">44</xref></sup> In these workflows, certain codec features (e.g., constraints, error type specificity) could convey advantages which were not evident in the common workflows used in this study. In addition, as DNA data storage might gravitate towards even lower-fidelity processes to decrease costs, new challenges for error-correction coding might open up in the future. In all cases, our standardized approach to benchmarking presented in this work – ensuring fairness by eliminating biases from experimental errors and experimentalists’ preferences – establishes best practices for codec comparisons in DNA data storage.</p></sec><sec id="S10" sec-type="methods"><title>Methods</title><sec id="S11"><title>Selection of codecs and clustering algorithms</title><p id="P37">The selection of codecs for this study was based on the availability of an open-source implementation with sufficient documentation, the presence of in vitro experiments in the original publication, the prominence in literature, and considerations for covering a broad range of approaches. We therefore selected DNA-Aeon by Welzel et al.<sup><xref ref-type="bibr" rid="R11">11</xref></sup>, DNA Fountain by Erlich and Zielinski<sup><xref ref-type="bibr" rid="R10">10</xref>,<xref ref-type="bibr" rid="R27">27</xref></sup>, DNA-RS by Heckel<sup><xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R28">28</xref>,<xref ref-type="bibr" rid="R29">29</xref></sup>, an implementation of the codec used by Goldman et al.<sup><xref ref-type="bibr" rid="R6">6</xref></sup> (“Goldman”), HEDGES by Press et al.<sup><xref ref-type="bibr" rid="R30">30</xref></sup>, and Yin-Yang by Ping et al.<sup><xref ref-type="bibr" rid="R24">24</xref></sup> (without additional error-correction elements). The installation and usage for encoding and decoding of each codec followed the documentation as far as possible. Nonetheless, some minor changes to the implementations of the DNA-Aeon, Goldman, HEDGES, and Yin-Yang codecs were required to either facilitate automated testing or ensure representative performance. These changes are described in <xref ref-type="supplementary-material" rid="SD1">Supplementary Note 1</xref> and are available in the code associated with this study.</p><p id="P38">The selection of clustering algorithms for this study was based on the same criteria as for the codecs. However, due to the sparse availability of suitable clustering algorithms specific to DNA data storage, we mainly considered general purpose clustering algorithms. We therefore selected CD-HIT by Li et al.<sup><xref ref-type="bibr" rid="R31">31</xref>,<xref ref-type="bibr" rid="R45">45</xref>,<xref ref-type="bibr" rid="R46">46</xref></sup>, Clover by Qu et al.<sup><xref ref-type="bibr" rid="R34">34</xref></sup>, clustering based on Locality-Sensititve Hashing by Darestani and Heckel<sup><xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R47">47</xref></sup> (“LSH”), MMseqs2 by Steinegger et al.<sup><xref ref-type="bibr" rid="R32">32</xref>,<xref ref-type="bibr" rid="R48">48</xref></sup>, and Starcode by Zorita et al.<sup><xref ref-type="bibr" rid="R33">33</xref>,<xref ref-type="bibr" rid="R49">49</xref></sup>. In addition, we implement the naïve clustering approach used for DNA Fountain<sup><xref ref-type="bibr" rid="R10">10</xref></sup> which simply uses the unique sequencing reads sorted by their abundance.</p><sec id="S12"><title>Selection of codec parameters</title><p id="P39">All of the selected codecs which support adjusting the sequence design and/or the level of error-correction were harmonized with respect to code rate, sequence length, and constraint choice. If supported, three sets of parameters yielding code rates of 1.50 bit nt <sup>-1</sup>, 1.00 bit nt<sup>-1</sup>, and 0.50 bit nt<sup>-1</sup> at a sequence length of around 150 nt were created for the <italic>in silico</italic> studies. For the <italic>in vitro</italic> experiment, due to the constraint on sequence length imposed by one of the synthesis providers (Genscript, 170 nt including adapters), three sets of parameters yielding code rates of 1.50 bit nt<sup>-1</sup>, 1.00 bit nt<sup>-1</sup>, and the highest code rate possible, using at most 126 nt, were selected. A detailed list of codec parameters used in this study is provided in <xref ref-type="supplementary-material" rid="SD1">Supplementary Tables 4-8</xref>.</p></sec></sec><sec id="S13"><title>Benchmarking of clustering algorithms</title><p id="P40">To compare the selected clustering algorithms individually and select their optimal parameters, two experimental sequencing datasets from a previous study<sup><xref ref-type="bibr" rid="R17">17</xref></sup> (PRJEB65931) were used. To preclude any impact of sequence design on clustering performance, these sequencing datasets were obtained from randomly generated sequences. To test both a high- and a low-fidelity scenario, both an experiment with synthesis by material deposition (ERR12033821) and one by electrochemical synthesis (ERR12033820) were used. For more details, see Ref. <sup><xref ref-type="bibr" rid="R17">17</xref></sup>. Prior to clustering, the paired sequencing reads were merged with NGmerge<sup><xref ref-type="bibr" rid="R50">50</xref></sup> (v0.3) and subsampled to a sequencing coverage of 20 sequencing reads per reference sequence using seqtk<sup><xref ref-type="bibr" rid="R51">51</xref></sup> (v1.4). After clustering, individual clusters were aligned using Kalign<sup><xref ref-type="bibr" rid="R52">52</xref></sup> (v3.4.0) to yield consensus sequences.</p><p id="P41">Besides clustering speed, three other performance metrics were calculated by comparing the consensus sequences after clustering to the reference sequences. To do so, each consensus sequence was assigned to a reference sequence by minimizing their Levenshtein distance. First, sensitivity was defined as the fraction of reference sequences which still had at least one associated consensus sequence after clustering. Second, accuracy was defined as the mean Levenshtein similarity of the closest match to each reference sequence. Third, specificity was defined as the ratio of the number of reference sequences with at least one associated consensus sequence relative to the total number of consensus sequences.</p></sec><sec id="S14"><title>Setup of the simulation pipelines for codec evaluation</title><p id="P42">Each simulation pipeline consisted of the steps outlined in <xref ref-type="fig" rid="F1">Fig. 1a</xref>: encoding, workflow, clustering, and decoding. For encoding, a set of fixed parameters (see below) for each codec was used to encode a 19 kB binary file with random content into DNA sequences (“reference sequences”). The workflow, either a script to introduce random errors at fixed rates or a workflow implemented in DT4DDS<sup><xref ref-type="bibr" rid="R17">17</xref>,<xref ref-type="bibr" rid="R53">53</xref></sup> (v1.1, see below), then generated simulated sequencing reads from the reference sequences. These sequencing reads were then clustered by a specified clustering algorithm (see above), and the clusters were aligned individually using Kalign<sup><xref ref-type="bibr" rid="R52">52</xref></sup> (v3.4.0) to yield consensus sequences. Finally, the consensus sequences were provided to the codec together with any supplementary data if needed, in order to attempt decoding. Decoding success was assessed by byte-by-byte comparison to the original input file, with only complete recovery of the data being considered as successful decoding.</p><p id="P43">In all cases, the consensus sequences generated after clustering were padded or trimmed to the length of the reference sequences. This was necessary as several codec implementations were incompatible with sequences shorter or longer than those originally designed. Moreover, workflows using DT4DDS<sup><xref ref-type="bibr" rid="R17">17</xref></sup> (see below) – thus yielding paired sequencing reads – employed NGmerge<sup><xref ref-type="bibr" rid="R50">50</xref></sup> (v0.3) for read merging prior to clustering.</p><p id="P44">To automate the process of preparing and running the simulation pipelines, management tools and wrapper scripts were written in Python (v3.11) using BioPython (v1.84), scipy (v1.14.1), statsmodels (v0.14.1), numpy (v2.0.2), pandas (v2.2.3), plotly (v5.24.0), psutil (v6.0.0), RapidFuzz (v3.10.0), bamboost (v0.8.0), and h5py (v3.12.1) under Ubuntu 22.04 LTS.</p><sec id="S15"><title>Computational constraints</title><p id="P45">All simulation pipelines were run on the Euler cluster operated by the High-Performance Computing group at ETH Zürich, using the Slurm workload manager. Each pipeline was constrained to one core of an AMD EPYC 7763 CPU (2.45 GHz nominal, 3.50 GHz peak), 8 GB RAM (DDR4, 3200 MHz), and 2 GB of temporary disk space. Each individual step of a pipeline was further limited to one hour of runtime programmatically. If any constraint was violated throughout the pipeline, decoding was considered unsuccessful.</p></sec><sec id="S16"><title>Definition and estimation of decoding probability</title><p id="P46">In order to assess the resilience of a chosen selection of codec, parameters, and clustering algorithm towards any of the workflow’s experimental parameters, a one-dimensional sensitivity analysis was performed (see below). Using the binary outcomes of this analysis (i.e., decoding success at each parameter choice) as dependent variables, a logit model was fitted to estimate the parameter value at which the probability of successful decoding would equal 95%. This value was then considered the performance threshold for the chosen selection of codec, parameters, and clustering algorithm.</p></sec><sec id="S17"><title>One-dimensional sensitivity analysis</title><p id="P47">Each one-dimensional sensitivity analysis was performed for a single parameter of a workflow and over a specified parameter range. To increase the accuracy of the estimated performance threshold, the sensitivity analysis was performed in three stages. In the first stage, ten logarithmically-spaced points were chosen across the full parameter range and tested. In the second stage, the ten outcomes of the first stage were used for a rough estimation of the performance threshold (see above), and ten additional logarithmically-spaced points were selected from the range <inline-formula><mml:math id="M1"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mfrac><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> and tested. In the third stage, the procedure of the second stage was repeated using the points from both the first and the second stage. The complete set of thirty binary outcomes were then used to generate the final estimate of the performance threshold (see above).</p></sec><sec id="S18"><title>Generation of Pareto fronts</title><p id="P48">In order to enable sensitivity analyses across two parameters of a workflow, two sets of one-dimensional sensitivity analyses were performed. In each set, one of the workflow parameters was fixed to one of ten logarithmically-spaced values selected from the specified parameter range, while the other was varied according to the three-stage process outlined above. From the resulting combinations of feasible parameter thresholds, only the Pareto efficient points were used to generate Pareto fronts.</p></sec></sec><sec id="S19"><title>In silico experiments</title><sec id="S20"><title>Pairing of codecs with clustering algorithms</title><p id="P49">For the pairing with codecs, only the best-performing parameter sets of each clustering algorithm from the individual benchmarking (see above) were considered. Each clustering algorithm was then tested with each codec in the basic error scenario, as described in detail in the following section. The clustering algorithm with the best performance threshold for each codec was then used throughout the rest of the study. The pairings are provided in <xref ref-type="supplementary-material" rid="SD1">Supplementary Table 3</xref>.</p></sec><sec id="S21"><title>Evaluation of codec performance in basic error scenarios</title><p id="P50">A workflow was implemented which introduced specific error types at specified rates randomly throughout the reference sequences, generating 30 erroneous reads per sequence. Considered error types were substitutions (with equal probability for all substituting nucleobases), insertions (with equal probability for all inserted nucleobases), deletions, mixed-errors (at a fixed ratio of 53:45:2 substitutions:deletions:insertions, resembling the error pattern in Ref. <sup><xref ref-type="bibr" rid="R17">17</xref></sup>), and sequence dropout (i.e., the removal of a fixed proportion of reference sequences from the reads). All error types were considered in a range of 0.1-40% nt<sup>-1</sup>, and sequence dropout used a range of 0.5-99%. This workflow was used for the one-dimensional sensitivity analyses and the Pareto fronts in <xref ref-type="fig" rid="F1">Fig. 1</xref>.</p></sec><sec id="S22"><title>Evaluation of codec performance in literature experiments</title><p id="P51">The serial dilution and serial PCR experiments used in the study by Erlich and Zielinski<sup><xref ref-type="bibr" rid="R10">10</xref></sup>, as well as the read down-sampling experiment by Organick et al.<sup><xref ref-type="bibr" rid="R9">9</xref></sup>, were replicated as workflows using DT4DDS<sup><xref ref-type="bibr" rid="R17">17</xref></sup>. All workflows assumed high-fidelity synthesis by material deposition, and the use of a high-fidelity polymerase. Sequencing depths were adapted to reflect the average sequencing depths reported in the original studies. Contrary to the other workflows, the comparison of codecs in these replicated literature experiments did not use the sensitivity analysis approach described above. Instead, each iteration of the workflow was tested ten times and the highest iteration count with which all ten tests were successfully decoded was reported.</p></sec><sec id="S23"><title>Evaluation of codec performance in high- and low-fidelity scenarios</title><p id="P52">Both a high- and a low-fidelity version of a generic data storage workflow, following the definitions for the best- and worst-case in Gimpel et al.<sup><xref ref-type="bibr" rid="R17">17</xref></sup>, were implemented using DT4DDS<sup><xref ref-type="bibr" rid="R17">17</xref></sup>. These scenarios are outlined in <xref ref-type="fig" rid="F3">Fig. 3b</xref>. In short, the simulated workflows include synthesis by material deposition or electrochemical synthesis, amplification for 15 cycles with either a high- (Q5) or low-fidelity (Taq) polymerase, dilution to a specified mean physical coverage, re-amplification for 25 cycles, and sequencing at a specified sequencing depth using paired-end, 150 nt reads of an iSeq 100. For both the high- and low-fidelity scenario, the mean physical redundancy after dilution and the sequencing depth were varied (see above) to yield the Pareto fronts in <xref ref-type="fig" rid="F3">Fig. 3a</xref>.</p></sec><sec id="S24"><title>Replication of the in vitro workflows</title><p id="P53">To compare the experimental data (see below) with the simulated results from the <italic>in silico</italic> analysis, the high- and low-fidelity scenarios used previously (see above) were adapted to use the adjusted codec parameters (see above) and match the workflow employed in the <italic>in vitro</italic> experiment. Specifically, the deletion rate during synthesis in the low-fidelity scenario was decreased to 0.0044 nt<sup>-1</sup>, the insertion rate during synthesis increased to 0.0010 nt<sup>-1</sup>, and the number of PCR cycles increased to 23 and 29 in the first and second round of amplification, respectively. These adapted workflows were used as described above in a one-dimensional sensitivity analysis, varying the physical redundancy after dilution to yield the decoding probability as a function of physical redundancy.</p></sec></sec><sec id="S25"><title>Experimental replication of high- and low-fidelity scenarios</title><p id="P54">The high- and low-fidelity scenarios outlined above were recreated experimentally with identically composed oligo pools synthesized by Twist Biosciences and Genscript, using established protocols for DNA data storage.<sup><xref ref-type="bibr" rid="R28">28</xref></sup> As Genscript only supported a total sequence length of 170 nt including PCR adapters, the parameters of all codecs were adjusted (see above) to limit the reference sequences to 126 nt, leaving 41 nt for truncated Illumina TruSeq adapters and 3 nt for a codec-specific suffix (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 8</xref>). In addition, to limit the number of sequences to be synthesized, the parameter sets yielding code rates of 0.50 bit nt<sup>-1</sup> were replaced with parameter sets yielding the maximum code rate supported by each codec. Multiple versions of a compressed image of ETH Zürich’s main building (created by ETH Zürich / Gian Marco Castelberg) were used as input files, with either 5 kB (Goldman codec), 17 kB (all codecs at 1.00 bit nt<sup>-1</sup>), or 19 kB (all others). As the sequence length varied between codecs, shorter reference sequences were padded with random nucleotides (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Table 10</xref> and <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 8</xref>). In total, 11 293 sequences across all codecs were created, padded and indexed (up to 129 nt), supplied with PCR adapters (for a total of 170 nt), and their order randomized prior to being ordered for synthesis.</p><sec id="S26"><title>Pool preparation</title><p id="P55">The oligo pools ordered from Twist Biosciences and Genscript were handled and amplified individually. The oligo pool by Twist Biosciences, received dry, was resuspended to 10 ng μL<sup>-1</sup> with ultrapure water. To create a master pool, 1 μL of a 5000x dilution of the oligo pool was then amplified with 10 μL Q5 High-Fidelity polymerase master mix (New England Biolabs, M0492S), 1 μL of 10 μM 0F and 0R primers each (Microsynth, see <xref ref-type="supplementary-material" rid="SD1">Supplementary Table 11</xref>), and 7 μL ultrapure water, replicated in a total of 96 wells. Thermocycling followed established protocols,<sup><xref ref-type="bibr" rid="R28">28</xref></sup> using an initial denaturation at 95⍰°C for 3⍰min, followed by 15 cycles at 95⍰°C for 15⍰s, 54⍰°C for 30⍰s, and 72⍰°C for 30⍰s.</p><p id="P56">The oligo pool by Genscript, received as a solution, was diluted to 5 ng μL<sup>-1</sup> with ultrapure water. To create a master pool, 1 μL of the diluted oligo pool was then amplified with 10 μL KAPA SYBR FAST polymerase master mix (Sigma-Aldrich), 1 μL of 10 μM 0F and 0R primers each (Microsynth, see <xref ref-type="supplementary-material" rid="SD1">Supplementary Table 11</xref>), and 7 μL ultrapure water, replicated in a total of 96 wells. Thermocycling followed the aforementioned protocol for 23 cycles.</p><p id="P57">For both oligo pools, each pool’s wells were then combined and purified (DNA Clean &amp; Concentrator-5, ZymoResearch). To increase purity further, each pool was then run on an agarose gel (E-Gel EX Agarose Gels 2%, Invitrogen) and the appropriate bands excised and purified (ZymoClean Gel DNA Recovery Kit, ZymoResearch). Finally, the pools were dialyzed (0.025 μm, 25 mm VSWP membrane, MF-Millipore) for 4 hours against ultrapure water. Concentration was measured by fluorescence (Qubit dsDNA HS Kit, Invitrogen) and by spectrophotometry (NanoDrop, Thermo Scientific).</p><p id="P58">The concentration of the master pool prepared from Twist Biosciences was measured as 49.4 ng μL<sup>-1</sup> (Qubit) and 51.5 ng μL<sup>-1</sup> (NanoDrop) respectively. Its concentration was therefore averaged to 50.45 ng μL<sup>-1</sup> for all further experiments. The concentration of the master pool prepared from Genscript was measured as 20.6 ng μL<sup>-1</sup> (Qubit) and 18.2 ng μL<sup>-1</sup> (NanoDrop) respectively. Its concentration was therefore averaged to 19.40 ng μL<sup>-1</sup> for all further experiments.</p></sec><sec id="S27"><title>Dilution and quantification by qPCR</title><p id="P59">Dilution to specified physical coverages (1000x, 50x, 25x, 10x, 5x, and 2x) was performed starting from the master pool. To convert from specified physical redundancy to required concentration, 11 293 sequences of dsDNA with 170 nt were assumed, yielding a physical redundancy of 509074x per ng. All dilutions were prepared such that 5 μL contained the required mass corresponding to the specified physical redundancy <italic>r</italic>, e.g., the concentration <italic>c</italic> was selected as <inline-formula><mml:math id="M2"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mi>r</mml:mi><mml:mrow><mml:mn>5</mml:mn><mml:mo>μ</mml:mo><mml:mtext>L</mml:mtext><mml:mo>⋅</mml:mo><mml:mspace width="0.2em"/><mml:mn>509074</mml:mn><mml:mspace width="0.2em"/><mml:msup><mml:mrow><mml:mtext>ng</mml:mtext></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>. Dilutions were performed serially while limiting the dilution factor to below 100x in each step, thereby maximizing dilution accuracy.</p><p id="P60">Calibration curves for qPCR were set up for each master pool individually, using serial dilutions of the master pools spanning a range from 0.05 ng μL<sup>-1</sup> (equivalent to a coverage of around 130 000 per 5 μL) to 5·10<sup>-8</sup> ng μL<sup>-1</sup> (coverage of 0.13 per 5 μL). qPCR used the same thermocycling settings as above, using 5 μL sample with 10 μL KAPA SYBR FAST polymerase master mix (Sigma-Aldrich), 1 μL of 10 μM 0F and 0R primers each (Microsynth, see <xref ref-type="supplementary-material" rid="SD1">Supplementary Table 11</xref>), and 3 μL ultrapure water, measured in duplicates. The calibration curves are shown in <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 12</xref>. The qPCR results of all prepared dilutions, calibrated against a standard from the calibration curve measured in parallel, are given in <xref ref-type="supplementary-material" rid="SD1">Supplementary Tables 12 and 13</xref>.</p></sec><sec id="S28"><title>Sequencing</title><p id="P61">Sequencing preparation followed established protocols.<sup><xref ref-type="bibr" rid="R17">17</xref>,<xref ref-type="bibr" rid="R28">28</xref></sup> In short, 5 μL of each dilution was amplified for 20 (samples of Genscript and Twist pool with 1000x physical redundancy), 26 cycles (only Genscript pool, 50x and 25x physical redundancy), or 29 cycles (all others) with 10 μL KAPA SYBR FAST polymerase master mix (only Genscript pool, Sigma-Aldrich) or 10 μL Q5 High-Fidelity polymerase master mix (only Twist pool, New England Biolabs, M0492S), 1 μL of 10 μM 2FUF and indexed 2RIF primers each (Microsynth, see <xref ref-type="supplementary-material" rid="SD1">Supplementary Table 11</xref>), and 3 μL ultrapure water. Amplified samples were run on an agarose gel (E-Gel EX Agarose Gels 2%, Invitrogen) and the appropriate band excised and purified (ZymoClean Gel DNA Recovery Kit, ZymoResearch) prior to quantification by fluorescence (Qubit dsDNA HS Kit, Invitrogen). All purified samples were individually diluted to 1⍰nM and pooled. The combined samples, diluted to 50 pM, were added to an Illumina iSeq 100 i1 Reagent v2 cartridge for 150 nt paired-end sequencing.</p></sec><sec id="S29"><title>Analysis of error rates and coverage biases</title><p id="P62">Sequence coverage in all sequencing datasets was assessed by read mapping with BBMap<sup><xref ref-type="bibr" rid="R54">54</xref></sup> (v39.01). Error analysis of the sample with 1000x coverage was performed as outlined in Gimpel et al. <sup><xref ref-type="bibr" rid="R17">17</xref></sup> using the tools implemented in DT4DDS<sup><xref ref-type="bibr" rid="R17">17</xref></sup>. Error analysis of the sets of sequences belonging to a single codec at a specified code rate were performed by using only the reference sequences of that codec for mapping with BBMap.</p></sec><sec id="S30"><title>Evaluation of codec performance in the in vitro experiment</title><p id="P63">The sequencing reads corresponding to each sample with a specified physical coverage were separated into subsets for each codec and code rate by filtering with BBMap<sup><xref ref-type="bibr" rid="R54">54</xref></sup>. After separation, each set of sequencing reads was randomly downsampled to a sequencing depth of 30 reads per reference sequence ten times. These sampled sequencing reads were used as input to the decoding pipeline consisting of a codec-specific clustering algorithm (see above) and the decoding step of the codec itself. Decoding success was assessed by byte-by-byte comparison to the original input file used for encoding.</p></sec></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Information</label><media xlink:href="EMS207290-supplement-Supplementary_Information.pdf" mimetype="application" mime-subtype="pdf" id="d35aAcEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S31"><title>Acknowledgments</title><p>This project was financed by the European Union’s Horizon 2020 Program, FET-Open: DNA-FAIRYLIGHTS, grant agreement no. 964995, and the European Union’s Horizon EIC Pathfinder Challenge Program: DiDAX, Grant Agreement No. 101115134 (Swiss Participants supported by the Swiss Secretariat for Education, Research and Innovation (SERI) under contract number 23.00330). Views and opinions expressed are however those of the authors only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them. Data analysis and simulations were performed on the Euler cluster operated by the High-Performance Computing group at ETH Zürich. Figures were partially created with BioRender.com.</p></ack><sec id="S32" sec-type="data-availability"><title>Data availability</title><p id="P64">The sequencing data generated in this study has been deposited in the European Nucleotide Archive under accession code PRJEB90546.</p></sec><sec id="S33" sec-type="data-availability"><title>Code availability</title><p id="P65">The code for benchmarking simulations and the Jupyter Notebooks for data analysis are deposited in the public GitHub repositories at github.com/fml-ethz/dt4dds-benchmark and github.com/fml-ethz/dt4dds-benchmark_notebooks.</p></sec><fn-group><fn id="FN1" fn-type="con"><p id="P66"><bold>Author contributions</bold></p><p id="P67">R.N.G. and A.G. initiated and supervised the project with input from W.J.S. and R.H. A.R. and A.L.G. performed the experiments. A.R. and A.L.G. developed the code, and performed simulations as well as data analysis. A.G. prepared the illustrations, and wrote the manuscript with input and approval from all authors.</p></fn><fn id="FN2" fn-type="conflict"><p id="P68"><bold>Competing interests</bold></p><p id="P69">W.J.S., R.H., and R.N.G. are authors of the studies demonstrating the use of the DNA-RS codec and the LSH clustering algorithm.<sup><xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R28">28</xref></sup> The other authors declare no competing interests.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heinis</surname><given-names>T</given-names></name><name><surname>Sokolovskii</surname><given-names>R</given-names></name><name><surname>Alnasir</surname><given-names>JJ</given-names></name></person-group><article-title>Survey of Information Encoding Techniques for DNA</article-title><source>ACM Comput Surv</source><year>2024</year><volume>56</volume><fpage>1</fpage><lpage>30</lpage></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davis</surname><given-names>J</given-names></name></person-group><article-title>Microvenus</article-title><source>Art J</source><year>1996</year><volume>55</volume><fpage>70</fpage><lpage>74</lpage></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>GC</given-names></name><name><surname>Fiddes</surname><given-names>CC</given-names></name><name><surname>Hawkins</surname><given-names>JP</given-names></name><name><surname>Cox</surname><given-names>JPL</given-names></name></person-group><article-title>Some possible codes for encrypting data in DNA</article-title><source>Biotechnol Lett</source><year>2003</year><volume>25</volume><fpage>1125</fpage><lpage>1130</lpage><pub-id pub-id-type="pmid">12966998</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ailenberg</surname><given-names>M</given-names></name><name><surname>Rotstein</surname><given-names>OD</given-names></name></person-group><article-title>An improved Huffman coding method for archiving text, images, and music characters in DNA</article-title><source>BioTechniques</source><year>2009</year><volume>47</volume><fpage>747</fpage><lpage>754</lpage><pub-id pub-id-type="pmid">19852760</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Church</surname><given-names>GM</given-names></name><name><surname>Gao</surname><given-names>Y</given-names></name><name><surname>Kosuri</surname><given-names>S</given-names></name></person-group><article-title>Next-generation digital information storage in DNA</article-title><source>Science</source><year>2012</year><volume>337</volume><elocation-id>1628</elocation-id><pub-id pub-id-type="pmid">22903519</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldman</surname><given-names>N</given-names></name><etal/></person-group><article-title>Towards practical, high-capacity, low-maintenance information storage in synthesized DNA</article-title><source>Nature</source><year>2013</year><volume>494</volume><fpage>77</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1038/nature11875</pub-id><pub-id pub-id-type="pmcid">PMC3672958</pub-id><pub-id pub-id-type="pmid">23354052</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grass</surname><given-names>RN</given-names></name><name><surname>Heckel</surname><given-names>R</given-names></name><name><surname>Puddu</surname><given-names>M</given-names></name><name><surname>Paunescu</surname><given-names>D</given-names></name><name><surname>Stark</surname><given-names>WJ</given-names></name></person-group><article-title>Robust Chemical Preservation of Digital Information on DNA in Silica with Error-Correcting Codes</article-title><source>Angew Chem Int Ed</source><year>2015</year><volume>54</volume><fpage>2552</fpage><lpage>2555</lpage><pub-id pub-id-type="pmid">25650567</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blawat</surname><given-names>M</given-names></name><etal/></person-group><article-title>Forward Error Correction for DNA Data Storage</article-title><source>Procedia Comput Sci</source><year>2016</year><volume>80</volume><fpage>1011</fpage><lpage>1022</lpage></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Organick</surname><given-names>L</given-names></name><etal/></person-group><article-title>Random access in large-scale DNA data storage</article-title><source>Nat Biotechnol</source><year>2018</year><volume>36</volume><fpage>242</fpage><lpage>248</lpage><pub-id pub-id-type="pmid">29457795</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erlich</surname><given-names>Y</given-names></name><name><surname>Zielinski</surname><given-names>D</given-names></name></person-group><article-title>DNA Fountain enables a robust and efficient storage architecture</article-title><source>Science</source><year>2017</year><volume>355</volume><fpage>950</fpage><lpage>954</lpage><pub-id pub-id-type="pmid">28254941</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Welzel</surname><given-names>M</given-names></name><etal/></person-group><article-title>DNA-Aeon provides flexible arithmetic coding for constraint adherence and error correction in DNA storage</article-title><source>Nat Commun</source><year>2023</year><volume>14</volume><fpage>628</fpage><pub-id pub-id-type="doi">10.1038/s41467-023-36297-3</pub-id><pub-id pub-id-type="pmcid">PMC9902613</pub-id><pub-id pub-id-type="pmid">36746948</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bornholt</surname><given-names>J</given-names></name><etal/></person-group><source>A DNA-based archival storage system</source><conf-name>Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</conf-name><conf-sponsor>Association for Computing Machinery</conf-sponsor><year>2016</year><fpage>637</fpage><lpage>649</lpage></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ceze</surname><given-names>L</given-names></name><name><surname>Nivala</surname><given-names>J</given-names></name><name><surname>Strauss</surname><given-names>K</given-names></name></person-group><article-title>Molecular digital data storage using DNA</article-title><source>Nat Rev Genet</source><year>2019</year><volume>20</volume><fpage>456</fpage><lpage>466</lpage><comment>2019 208</comment><pub-id pub-id-type="pmid">31068682</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doricchi</surname><given-names>A</given-names></name><etal/></person-group><article-title>Emerging Approaches to DNA Data Storage: Challenges and Prospects</article-title><source>ACS Nano</source><year>2022</year><volume>16</volume><fpage>17552</fpage><lpage>17571</lpage><pub-id pub-id-type="doi">10.1021/acsnano.2c06748</pub-id><pub-id pub-id-type="pmcid">PMC9706676</pub-id><pub-id pub-id-type="pmid">36256971</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kosuri</surname><given-names>S</given-names></name><name><surname>Church</surname><given-names>GM</given-names></name></person-group><article-title>Large-scale de novo DNA synthesis: technologies and applications</article-title><source>Nat Methods</source><year>2014</year><volume>11</volume><fpage>499</fpage><lpage>507</lpage><pub-id pub-id-type="doi">10.1038/nmeth.2918</pub-id><pub-id pub-id-type="pmcid">PMC7098426</pub-id><pub-id pub-id-type="pmid">24781323</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heckel</surname><given-names>R</given-names></name><name><surname>Mikutis</surname><given-names>G</given-names></name><name><surname>Grass</surname><given-names>RN</given-names></name></person-group><article-title>A Characterization of the DNA Data Storage Channel</article-title><source>Sci Rep</source><year>2019</year><volume>9</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1038/s41598-019-45832-6</pub-id><pub-id pub-id-type="pmcid">PMC6609604</pub-id><pub-id pub-id-type="pmid">31273225</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gimpel</surname><given-names>AL</given-names></name><name><surname>Stark</surname><given-names>WJ</given-names></name><name><surname>Heckel</surname><given-names>R</given-names></name><name><surname>Grass</surname><given-names>RN</given-names></name></person-group><article-title>A digital twin for DNA data storage based on comprehensive quantification of errors and biases</article-title><source>Nat Commun</source><year>2023</year><volume>14</volume><elocation-id>6026</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-023-41729-1</pub-id><pub-id pub-id-type="pmcid">PMC10533828</pub-id><pub-id pub-id-type="pmid">37758710</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Antkowiak</surname><given-names>PL</given-names></name><etal/></person-group><article-title>Low cost DNA data storage using photolithographic synthesis and advanced information reconstruction and error correction</article-title><source>Nat Commun</source><year>2020</year><volume>11</volume><elocation-id>5345</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-19148-3</pub-id><pub-id pub-id-type="pmcid">PMC7582880</pub-id><pub-id pub-id-type="pmid">33093494</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>L</given-names></name><etal/></person-group><article-title>Robust data storage in DNA by de Bruijn graph-based de novo strand assembly</article-title><source>Nat Commun</source><year>2022</year><volume>13</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1038/s41467-022-33046-w</pub-id><pub-id pub-id-type="pmcid">PMC9468002</pub-id><pub-id pub-id-type="pmid">36097016</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar-Lev</surname><given-names>D</given-names></name><name><surname>Marcovich</surname><given-names>S</given-names></name><name><surname>Yaakobi</surname><given-names>E</given-names></name><name><surname>Yehezkeally</surname><given-names>Y</given-names></name></person-group><article-title>Adversarial Torn-Paper Codes</article-title><source>IEEE Trans Inf Theory</source><year>2023</year><volume>69</volume><fpage>6414</fpage><lpage>6427</lpage></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yazdi</surname><given-names>SMHT</given-names></name><name><surname>Gabrys</surname><given-names>R</given-names></name><name><surname>Milenkovic</surname><given-names>O</given-names></name></person-group><article-title>Portable and Error-Free DNA-Based Data Storage</article-title><source>Sci Rep</source><year>2017</year><volume>7</volume><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1038/s41598-017-05188-1</pub-id><pub-id pub-id-type="pmcid">PMC5503945</pub-id><pub-id pub-id-type="pmid">28694453</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lopez</surname><given-names>R</given-names></name><etal/></person-group><article-title>DNA assembly for nanopore data storage readout</article-title><source>Nat Commun</source><year>2019</year><volume>10</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1038/s41467-019-10978-4</pub-id><pub-id pub-id-type="pmcid">PMC6610119</pub-id><pub-id pub-id-type="pmid">31270330</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>X</given-names></name><etal/></person-group><article-title>Composite Hedges Nanopores codec system for rapid and portable DNA data readout with high INDEL-Correction</article-title><source>Nat Commun</source><year>2024</year><volume>15</volume><elocation-id>9395</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-024-53455-3</pub-id><pub-id pub-id-type="pmcid">PMC11525716</pub-id><pub-id pub-id-type="pmid">39477940</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ping</surname><given-names>Z</given-names></name><etal/></person-group><article-title>Towards practical and robust DNA-based data archiving using the yin–yang codec system</article-title><source>Nat Comput Sci</source><year>2022</year><volume>2</volume><fpage>234</fpage><lpage>242</lpage><pub-id pub-id-type="doi">10.1038/s43588-022-00231-2</pub-id><pub-id pub-id-type="pmcid">PMC10766522</pub-id><pub-id pub-id-type="pmid">38177542</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Organick</surname><given-names>L</given-names></name><etal/></person-group><article-title>Probing the physical limits of reliable DNA data retrieval</article-title><source>Nat Commun</source><year>2020</year><volume>11</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1038/s41467-020-14319-8</pub-id><pub-id pub-id-type="pmcid">PMC6992699</pub-id><pub-id pub-id-type="pmid">32001691</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar-Lev</surname><given-names>D</given-names></name><name><surname>Orr</surname><given-names>I</given-names></name><name><surname>Sabary</surname><given-names>O</given-names></name><name><surname>Etzion</surname><given-names>T</given-names></name><name><surname>Yaakobi</surname><given-names>E</given-names></name></person-group><article-title>Scalable and robust DNA-based storage via coding theory and deep learning</article-title><source>Nat Mach Intell</source><year>2025</year><volume>7</volume><fpage>639</fpage><lpage>649</lpage></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Erlich</surname><given-names>Y</given-names></name><name><surname>Zielinski</surname><given-names>D</given-names></name></person-group><source>TeamErlich/dna-fountain</source><publisher-name>GitHub</publisher-name><year>2024</year><comment><ext-link ext-link-type="uri" xlink:href="https://github.com/TeamErlich/dna-fountain">https://github.com/TeamErlich/dna-fountain</ext-link></comment></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meiser</surname><given-names>LC</given-names></name><etal/></person-group><article-title>Reading and writing digital data in DNA</article-title><source>Nat Protoc</source><year>2019</year><volume>15</volume><fpage>86</fpage><lpage>101</lpage><pub-id pub-id-type="pmid">31784718</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Heckel</surname><given-names>R</given-names></name></person-group><source>reinhardh/dna_rs_coding: Error correction scheme for storing information on DNA using Reed Solomon codes</source><publisher-name>GitHub</publisher-name><year>2021</year><comment><ext-link ext-link-type="uri" xlink:href="https://github.com/reinhardh/dna_rs_coding">https://github.com/reinhardh/dna_rs_coding</ext-link></comment></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Press</surname><given-names>WH</given-names></name><name><surname>Hawkins</surname><given-names>JA</given-names></name><name><surname>Jones</surname><given-names>SK</given-names></name><name><surname>Schaub</surname><given-names>JM</given-names></name><name><surname>Finkelstein</surname><given-names>IJ</given-names></name></person-group><article-title>HEDGES error-correcting code for DNA storage corrects indels and allows sequence constraints</article-title><source>Proc Natl Acad Sci</source><year>2020</year><volume>117</volume><fpage>18489</fpage><lpage>18496</lpage><pub-id pub-id-type="doi">10.1073/pnas.2004821117</pub-id><pub-id pub-id-type="pmcid">PMC7414044</pub-id><pub-id pub-id-type="pmid">32675237</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>L</given-names></name><name><surname>Niu</surname><given-names>B</given-names></name><name><surname>Zhu</surname><given-names>Z</given-names></name><name><surname>Wu</surname><given-names>S</given-names></name><name><surname>Li</surname><given-names>W</given-names></name></person-group><article-title>CD-HIT: accelerated for clustering the next-generation sequencing data</article-title><source>Bioinformatics</source><year>2012</year><volume>28</volume><fpage>3150</fpage><lpage>3152</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/bts565</pub-id><pub-id pub-id-type="pmcid">PMC3516142</pub-id><pub-id pub-id-type="pmid">23060610</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinegger</surname><given-names>M</given-names></name><name><surname>Söding</surname><given-names>J</given-names></name></person-group><article-title>MMseqs2 enables sensitive protein sequence searching for the analysis of massive data sets</article-title><source>Nat Biotechnol</source><year>2017</year><volume>35</volume><fpage>1026</fpage><lpage>1028</lpage><pub-id pub-id-type="pmid">29035372</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zorita</surname><given-names>E</given-names></name><name><surname>Cuscó</surname><given-names>P</given-names></name><name><surname>Filion</surname><given-names>GJ</given-names></name></person-group><article-title>Starcode: sequence clustering based on all-pairs search</article-title><source>Bioinformatics</source><year>2015</year><volume>31</volume><fpage>1913</fpage><lpage>1919</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btv053</pub-id><pub-id pub-id-type="pmcid">PMC4765884</pub-id><pub-id pub-id-type="pmid">25638815</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qu</surname><given-names>G</given-names></name><name><surname>Yan</surname><given-names>Z</given-names></name><name><surname>Wu</surname><given-names>H</given-names></name></person-group><article-title>Clover: tree structure-based efficient DNA clustering for DNA-based data storage</article-title><source>Brief Bioinform</source><year>2022</year><volume>23</volume><elocation-id>bbac336</elocation-id><pub-id pub-id-type="pmid">35975958</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>R</given-names></name><etal/></person-group><article-title>Study of the error correction capability of multiple sequence alignment algorithm (MAFFT) in DNA storage</article-title><source>BMC Bioinformatics</source><year>2023</year><volume>24</volume><fpage>111</fpage><pub-id pub-id-type="doi">10.1186/s12859-023-05237-9</pub-id><pub-id pub-id-type="pmcid">PMC10037887</pub-id><pub-id pub-id-type="pmid">36959531</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gimpel</surname><given-names>AL</given-names></name><etal/></person-group><article-title>Deep learning uncovers sequence-specific amplification bias in multi-template PCR</article-title><source>bioRxiv</source><year>2024</year><elocation-id>2024.09.20.614030</elocation-id><pub-id pub-id-type="doi">10.1101/2024.09.20.614030</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ben-Dov</surname><given-names>E</given-names></name><name><surname>Shapiro</surname><given-names>OH</given-names></name><name><surname>Kushmaro</surname><given-names>A</given-names></name></person-group><article-title>‘Next-base’ effect on PCR amplification</article-title><source>Environ Microbiol Rep</source><year>2012</year><volume>4</volume><fpage>183</fpage><lpage>188</lpage><pub-id pub-id-type="pmid">23757271</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qiao</surname><given-names>H</given-names></name><etal/></person-group><article-title>Oligo replication advantage driven by GC content and Gibbs free energy</article-title><source>Biotechnol Lett</source><year>2022</year><volume>44</volume><fpage>1189</fpage><lpage>1199</lpage><pub-id pub-id-type="pmid">36029395</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lietard</surname><given-names>J</given-names></name><etal/></person-group><article-title>Chemical and photochemical error rates in light-directed synthesis of complex DNA libraries</article-title><source>Nucleic Acids Res</source><year>2021</year><volume>49</volume><fpage>6687</fpage><lpage>6701</lpage><pub-id pub-id-type="doi">10.1093/nar/gkab505</pub-id><pub-id pub-id-type="pmcid">PMC8266620</pub-id><pub-id pub-id-type="pmid">34157124</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>HH</given-names></name><name><surname>Kalhor</surname><given-names>R</given-names></name><name><surname>Goela</surname><given-names>N</given-names></name><name><surname>Bolot</surname><given-names>J</given-names></name><name><surname>Church</surname><given-names>GM</given-names></name></person-group><article-title>Terminator-free template-independent enzymatic DNA synthesis for digital information storage</article-title><source>Nat Commun</source><year>2019</year><volume>10</volume><elocation-id>2383</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-10258-1</pub-id><pub-id pub-id-type="pmcid">PMC6546792</pub-id><pub-id pub-id-type="pmid">31160595</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>H</given-names></name><etal/></person-group><article-title>Photon-directed multiplexed enzymatic DNA synthesis for molecular digital data storage</article-title><source>Nat Commun</source><year>2020</year><volume>11</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1038/s41467-020-18681-5</pub-id><pub-id pub-id-type="pmcid">PMC7567835</pub-id><pub-id pub-id-type="pmid">33067441</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meiser</surname><given-names>LC</given-names></name><etal/></person-group><article-title>Information decay and enzymatic information recovery for DNA data storage</article-title><source>Commun Biol</source><year>2022</year><volume>5</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1038/s42003-022-04062-9</pub-id><pub-id pub-id-type="pmcid">PMC9584896</pub-id><pub-id pub-id-type="pmid">36266439</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Choi</surname><given-names>Y</given-names></name><etal/></person-group><article-title>High information capacity DNA-based data storage with augmented encoding characters using degenerate bases</article-title><source>Sci Rep</source><year>2019</year><volume>9</volume><elocation-id>6582</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-43105-w</pub-id><pub-id pub-id-type="pmcid">PMC6488701</pub-id><pub-id pub-id-type="pmid">31036920</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anavy</surname><given-names>L</given-names></name><name><surname>Vaknin</surname><given-names>I</given-names></name><name><surname>Atar</surname><given-names>O</given-names></name><name><surname>Amit</surname><given-names>R</given-names></name><name><surname>Yakhini</surname><given-names>Z</given-names></name></person-group><article-title>Data storage in DNA with fewer synthesis cycles using composite DNA letters</article-title><source>Nat Biotechnol</source><year>2019</year><volume>37</volume><fpage>1229</fpage><lpage>1236</lpage><pub-id pub-id-type="pmid">31501560</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>W</given-names></name><name><surname>Godzik</surname><given-names>A</given-names></name></person-group><article-title>CD-HIT: a fast program for clustering and comparing large sets of protein or nucleotide sequences</article-title><source>Bioinformatics</source><year>2006</year><volume>22</volume><fpage>1658</fpage><lpage>1659</lpage><pub-id pub-id-type="pmid">16731699</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weizhong</surname><given-names>Li</given-names></name></person-group><article-title>weizhongli/cdhit: CD-HIT</article-title><source>GitHub</source><comment><ext-link ext-link-type="uri" xlink:href="https://github.com/weizhongli/cdhit">https://github.com/weizhongli/cdhit</ext-link></comment></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Darestani</surname><given-names>MZ</given-names></name><name><surname>Heckel</surname><given-names>R</given-names></name></person-group><article-title>MLI-lab/noisy_dna_data_storage: Data recovery from millions of noisy reads</article-title><source>Zenodo</source><year>2020</year><pub-id pub-id-type="doi">10.5281/zenodo.4044459</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Soeding</surname><given-names>J</given-names></name></person-group><source>soedinglab/MMseqs2: MMseqs2, ultra fast and sensitive search and clustering suite</source><publisher-name>GitHub</publisher-name><year>2024</year><comment><ext-link ext-link-type="uri" xlink:href="https://github.com/soedinglab/MMseqs2">https://github.com/soedinglab/MMseqs2</ext-link></comment></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Filion</surname><given-names>G</given-names></name></person-group><source>gui11aume/starcode</source><publisher-name>GitHub</publisher-name><year>2024</year><comment><ext-link ext-link-type="uri" xlink:href="https://github.com/gui11aume/starcode">https://github.com/gui11aume/starcode</ext-link></comment></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gaspar</surname><given-names>JM</given-names></name></person-group><article-title>NGmerge: merging paired-end reads via novel empirically-derived models of sequencing errors</article-title><source>BMC Bioinformatics</source><year>2018</year><volume>19</volume><fpage>536</fpage><pub-id pub-id-type="doi">10.1186/s12859-018-2579-2</pub-id><pub-id pub-id-type="pmcid">PMC6302405</pub-id><pub-id pub-id-type="pmid">30572828</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Li</surname><given-names>H</given-names></name></person-group><source>lh3/seqtk: Toolkit for processing sequences in FASTA/Q formats</source><publisher-name>GitHub</publisher-name><year>2024</year><comment><ext-link ext-link-type="uri" xlink:href="https://github.com/lh3/seqtk">https://github.com/lh3/seqtk</ext-link></comment></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lassmann</surname><given-names>T</given-names></name><name><surname>Sonnhammer</surname><given-names>EL</given-names></name></person-group><article-title>Kalign – an accurate and fast multiple sequence alignment algorithm</article-title><source>BMC Bioinformatics</source><year>2005</year><volume>6</volume><fpage>298</fpage><pub-id pub-id-type="doi">10.1186/1471-2105-6-298</pub-id><pub-id pub-id-type="pmcid">PMC1325270</pub-id><pub-id pub-id-type="pmid">16343337</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gimpel</surname><given-names>AL</given-names></name><name><surname>Stark</surname><given-names>WJ</given-names></name><name><surname>Heckel</surname><given-names>R</given-names></name><name><surname>Grass</surname><given-names>RN</given-names></name></person-group><source>A digital twin for DNA data storage based on comprehensive quantification of errors and biases</source><publisher-name>githubcom/fml-ethz/dt4dds</publisher-name><year>2023</year><pub-id pub-id-type="doi">10.5281/zenodo.8329043</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bushnell</surname><given-names>Brian</given-names></name></person-group><source>BBMap: A Fast, Accurate, Splice-Aware Aligner</source><conf-name>9th Annual Genomics of Energy &amp; Environment Meeting</conf-name><conf-sponsor>Lawrence Berkeley National Laboratory</conf-sponsor><conf-loc>Berkeley, CA, USA</conf-loc><year>2014</year></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Overview of the scope of this work and the evaluations of clustering algorithms and codecs.</title><p>(<bold>a</bold>) Overview of the data storage workflow considered in this work, including encoding of the data with a codec, an <italic>in silico</italic> or <italic>in vitro</italic> data storage workflow, post-processing by read clustering and generation of consensus sequences, and decoding of the data with a codec. (<bold>b</bold>) Exemplary outcome of a basic error scenario with naïve (grey) or CD-HIT clustering (red) upon variation of the overall error rate (see <xref ref-type="sec" rid="S10">Methods</xref>). Individual points denote the outcomes of 30 individual iterations of the scenario. The solid lines represent the logistic regression performed to estimate the error rate at which data recovery succeeds with 95% probability (dashed grey line). The corresponding error rate is then used as performance metric in this work. (<bold>c</bold>) Performance of all codecs in the basic error scenario, introducing only deletions (blue), insertions (green), or substitutions (orange). All codecs use the best-performing clustering algorithm denoted in panel d. (<bold>d</bold>) Performance of each codec at its supported code rates in the basic error scenario, using both naïve clustering (grey bars) and the best-performing clustering algorithm for each codec (colored bars, see <xref ref-type="supplementary-material" rid="SD1">Supplementary Table 3</xref> for full data). Conditions indicated with an hourglass (⍰) were limited by the time constraint (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Figs. 1-4</xref>). (<bold>e</bold>) Pareto fronts of codec performance in a scenario combining errors at a fixed ratio of 53% substitutions, 45% deletions, and 2% insertions with sequence dropout. Feasible regions lie below the indicated pareto fronts, with the mean error rates of the high-fidelity (dashed line) and low-fidelity scenario (dotted line) indicated. The Yin-Yang codec, which did not include independent error-correction capabilities, was unable to decode the data in this scenario at all, given that neither it nor the clustering step was capable of compensating for any sequence dropout.</p></caption><graphic xlink:href="EMS207290-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Comparison with literature experiments replicated <italic>in silico</italic>.</title><p>All codecs were tested in three replicated literature experiments as reported by Erlich and Zielinski<sup><xref ref-type="bibr" rid="R10">10</xref></sup>, Organick et al.<sup><xref ref-type="bibr" rid="R9">9</xref>,<xref ref-type="bibr" rid="R25">25</xref></sup>, and Ping et al.<sup><xref ref-type="bibr" rid="R24">24</xref></sup>: serial dilution (blue), deep amplification (orange), and down-sampling (green). The individual workflows are illustrated on the right. In all cases, the best iteration at which all five repetitions succeeded are reported (see <xref ref-type="sec" rid="S10">Methods</xref>). For comparison, the performance reported in the corresponding literature experiments by Organick et al.<sup><xref ref-type="bibr" rid="R9">9</xref>,<xref ref-type="bibr" rid="R25">25</xref></sup> (solid lines), Ping et al.<sup><xref ref-type="bibr" rid="R24">24</xref></sup> (dashed lines), and Erlich and Zielinski<sup><xref ref-type="bibr" rid="R10">10</xref></sup> (dotted lines) are shown.</p></caption><graphic xlink:href="EMS207290-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Codec performance in high- and low-fidelity scenarios.</title><p>(<bold>a</bold>) Pareto fronts for each codec and code rate in the high-(top) and low-fidelity scenario (bottom). The physical redundancy during storage (e.g., average number of oligos per sequence) and the sequencing depth (e.g., average number of reads per sequence) were varied to identify feasible and infeasible regions. Only Pareto efficient points are shown, and connected for illustration, with the feasible region lying above the curves. The dashed line highlights a sequencing depth of 30x, as used in the <italic>in vitro</italic> experiment. (<bold>b</bold>) Illustration of the high- and low-fidelity scenarios, resembling common workflows in DNA data storage. The physical redundancy and sequencing depth were systematically varied to obtain the Pareto fronts in Panel a). (<bold>c</bold>) Highest feasible storage density in the high- (blue) and low-fidelity (red) scenarios by codec and code rate at a sequencing depth of 30x (dashed lines in Panel a). Storage densities only consider the payload, and assume a molecular weight of 662 g mol<sup>-1</sup> bp<sup>-1</sup>, see <xref ref-type="supplementary-material" rid="SD1">Supplementary Note 2</xref>.</p></caption><graphic xlink:href="EMS207290-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Codec performance in the in vitro experiments.</title><p>The <italic>in vitro</italic> experiments followed the high- (left) and low-fidelity (right) workflows outlined in <xref ref-type="fig" rid="F3">Fig. 3b</xref> with slight changes to the amount of PCR cycles (see <xref ref-type="sec" rid="S10">Methods</xref>). Five nominal physical redundancies ranging from 2x to 1000x were implemented by dilution and validated by qPCR. For each codec and code rate, decoding success in both experimental workflows was assessed by each codec’s ability to decode the experimental sequencing data after down-sampling to a sequencing depth of 30x (check marks). Robustness of decoding was assessed by performing a total of ten downsampling repetitions, with the fraction of successful repetitions reported (intensity of check marks). For comparison, the probabilities of decoding success estimated from replicating the amended high- and low-fidelity workflows <italic>in silico</italic> are also shown (see <xref ref-type="sec" rid="S10">Methods</xref>, green shading).</p></caption><graphic xlink:href="EMS207290-f004"/></fig><table-wrap id="T1" orientation="portrait" position="float"><label>Table 1</label><caption><title>Selected codecs for this study and their properties.</title><p>Clustering denotes the clustering approach used in the original study, if any. The pairing of codecs with clustering algorithms used in this study is detailed in <xref ref-type="supplementary-material" rid="SD1">Supplementary Table 3</xref>.</p></caption><table frame="box" rules="all"><thead><tr><th valign="middle" align="left" rowspan="2">Name</th><th valign="middle" align="right" colspan="2">Error correction</th><th valign="middle" align="right" rowspan="2">Constraints</th><th valign="middle" align="right" rowspan="2">Clustering</th><th valign="middle" align="right" rowspan="2">Customizability</th><th valign="middle" align="right" rowspan="2">Code rates</th><th valign="middle" align="right" rowspan="2">Refs.</th></tr><tr><th valign="middle" align="right">inner</th><th valign="middle" align="right">outer</th></tr></thead><tbody><tr><td valign="middle" align="left">DNA-Aeon</td><td valign="middle" align="right">Arithmetic</td><td valign="middle" align="right">Fountain</td><td valign="middle" align="right">GC, HP, motifs</td><td valign="middle" align="right">CD-Hit</td><td valign="middle" align="right">Broad</td><td valign="middle" align="right">1.50, 1.00, 0.50</td><td valign="middle" align="right"><sup><xref ref-type="bibr" rid="R11">11</xref></sup></td></tr><tr><td valign="middle" align="left">DNA Fountain</td><td valign="middle" align="right">Reed-Solomon</td><td valign="middle" align="right">Fountain</td><td valign="middle" align="right">GC, HP</td><td valign="middle" align="right">Naïve<sup><xref ref-type="fn" rid="TFN1">a</xref></sup></td><td valign="middle" align="right">Broad</td><td valign="middle" align="right">1.50, 1.00, 0.50</td><td valign="middle" align="right"><sup><sup><xref ref-type="bibr" rid="R10">10</xref></sup>,<sup><xref ref-type="bibr" rid="R27">27</xref></sup></sup></td></tr><tr><td valign="middle" align="left">DNA-RS</td><td valign="middle" align="right">Reed-Solomon</td><td valign="middle" align="right">Reed-Solomon</td><td valign="middle" align="right">None</td><td valign="middle" align="right">None or LSH</td><td valign="middle" align="right">Broad</td><td valign="middle" align="right">1.50,1.00, 0.50</td><td valign="middle" align="right"><sup><xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R28">28</xref>,<xref ref-type="bibr" rid="R29">29</xref></sup></td></tr><tr><td valign="middle" align="left">Goldman</td><td valign="middle" align="right">Parity</td><td valign="middle" align="right">Repetition</td><td valign="middle" align="right">HP</td><td valign="middle" align="right">None</td><td valign="middle" align="right">None</td><td valign="middle" align="right">0.34</td><td valign="middle" align="right"><sup><xref ref-type="bibr" rid="R6">6</xref></sup></td></tr><tr><td valign="middle" align="left">HEDGES</td><td valign="middle" align="right">Convolutional</td><td valign="middle" align="right">Reed-<break/>Solomon</td><td valign="middle" align="right">GC, HP</td><td valign="middle" align="right">None</td><td valign="middle" align="right">Constrained</td><td valign="middle" align="right">1.07, 0.63</td><td valign="middle" align="right"><sup><xref ref-type="bibr" rid="R30">30</xref></sup></td></tr><tr><td valign="middle" align="left">Yin-Yang</td><td valign="middle" align="right" colspan="2">None<sup><xref ref-type="fn" rid="TFN2">b</xref></sup></td><td valign="middle" align="right">GC, HP, ΔE</td><td valign="middle" align="right">None</td><td valign="middle" align="right">Only length<sup><xref ref-type="fn" rid="TFN2">b</xref></sup></td><td valign="middle" align="right">1.85</td><td valign="middle" align="right"><sup><xref ref-type="bibr" rid="R24">24</xref></sup></td></tr></tbody></table><table-wrap-foot><fn id="TFN1"><label>a</label><p id="P70">Naïve clustering reduces sequencing data to its set of unique reads, i.e., removing duplicates without consensus generation.</p></fn><fn id="TFN2"><label>b</label><p id="P71">The Yin-Yang codec is a bit-to-base coding scheme, and does not include any error correction in itself.</p></fn></table-wrap-foot></table-wrap></floats-group></article>