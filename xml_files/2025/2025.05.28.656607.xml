<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS206053</article-id><article-id pub-id-type="doi">10.1101/2025.05.28.656607</article-id><article-id pub-id-type="archive">PPR1027996</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Contextual inference through flexible integration of environmental features and behavioural outcomes</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Passlack</surname><given-names>Jessica</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>MacAskill</surname><given-names>Andrew F</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>Department of Neuroscience, Physiology and Pharmacology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap>; <addr-line>Gower St</addr-line>, <city>London</city>, <postal-code>WC1E 6BT</postal-code></aff><aff id="A2"><label>2</label>Centre for Discovery Brain Sciences, Edinburgh Medical School: Biomedical Sciences, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01nrxwf90</institution-id><institution>University of Edinburgh</institution></institution-wrap>, <city>Edinburgh</city><postal-code>EH8 9XD</postal-code></aff><author-notes><corresp id="CR1">
<label>*</label>For correspondence: <email>jpasslac@ed.ac.uk</email>; <email>a.macaskill@ucl.ac.uk</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>31</day><month>05</month><year>2025</year></pub-date><pub-date pub-type="preprint"><day>28</day><month>05</month><year>2025</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nd/4.0/">CC BY-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">The ability to use the context we are in to flexibly adjust our decision-making is vital for navigating a complex world. To do this, the brain must i) use environmental features and behavioural outcomes to distinguish between different, often hidden contexts; and ii) learn how to use these inferred contexts to guide behaviour. However, how these two interacting processes can be performed simultaneously remains unclear. Within the brain it is thought that interaction between the prefrontal cortex (PFC) and hippocampus (HPC) supports contextual inference. We show that models of the HPC using environmental features readily support context-specific behaviour, but struggle to differentiate ambiguous contexts during learning. In contrast, models of the PFC using behavioural outcomes can stably differentiate contexts during periods of learning, but struggle to guide context-specific behaviour. Supporting feature-based with outcome-based strategies during learning overcomes the limitations of both approaches, allowing for the formation of distinct contextual representations that support contextual inference. Moreover, agents using this joint approach reproduce both behavioural- and cellular-level phenomena associated with the interaction between PFC and HPC. Together, these results provide insight into how the brain uses contextual information to guide flexible behaviour.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Humans and animals have a remarkable ability to learn and recall multiple optimal behaviours dependent on the current context [<xref ref-type="bibr" rid="R1">1</xref>, <xref ref-type="bibr" rid="R2">2</xref>, <xref ref-type="bibr" rid="R3">3</xref>, <xref ref-type="bibr" rid="R4">4</xref>, <xref ref-type="bibr" rid="R5">5</xref>]. While such contexts are often distinct environments that are easily distinguishable from one another, distinct contexts are also commonly found within the same environment where they can be differentiated by the presence of distinct features (such as smells, objects or sounds), or the distinct distribution of outcomes (such as rewards). This ability to identify distinct contexts and differentiate between them - often termed contextual inference - is crucial, as different behaviours are needed to achieve the optimal outcome in each context. Importantly, while the brain can effortlessly perform contextual inference, how this is achieved remains unclear, apart from knowing that it requires complex interplay between multiple brain regions, in particular the prefrontal cortex and hippocampus [<xref ref-type="bibr" rid="R6">6</xref>, <xref ref-type="bibr" rid="R7">7</xref>]. Similarly, computational approaches are still limited in their ability to simultaneously learn to identify contexts and drive context-dependent behaviour [<xref ref-type="bibr" rid="R8">8</xref>, <xref ref-type="bibr" rid="R9">9</xref>].</p><p id="P3">Computationally, two main approaches have had success with learning to perform contextual inference. One approach is to use recurrence to maintain information related to the hidden context over time in the form of recurrent neural networks (RNNs) [<xref ref-type="bibr" rid="R10">10</xref>, <xref ref-type="bibr" rid="R11">11</xref>, <xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R13">13</xref>]. The other approach uses approximate Bayesian inference over model-based representations that learn explicitly how observations are linked to each hidden context [<xref ref-type="bibr" rid="R14">14</xref>, <xref ref-type="bibr" rid="R15">15</xref>, <xref ref-type="bibr" rid="R1">1</xref>, <xref ref-type="bibr" rid="R16">16</xref>, <xref ref-type="bibr" rid="R17">17</xref>]. However, despite computational approaches being able to successfully maintain multiple context-dependent representations [<xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R16">16</xref>, <xref ref-type="bibr" rid="R18">18</xref>], during learning these approaches often struggle to identify the correct context upon which to learn, especially during more complex tasks. Specifically, models struggle to learn when the signal-to-noise ratio (SNR) of context-dependent to non-context-dependent observations decreases [<xref ref-type="bibr" rid="R13">13</xref>, <xref ref-type="bibr" rid="R10">10</xref>, <xref ref-type="bibr" rid="R19">19</xref>, <xref ref-type="bibr" rid="R20">20</xref>]. This inability to cope with low contextual SNR is a major limitation to applying these techniques successfully to complex environments as, given the noise and uncertainty of the world, often proportionately small changes signal a change in the current optimal behaviour. Recent evidence shows that this issue can be overcome by using externally provided context signals during learning, however how such supporting context signals can be generated and how they support learning remains unclear [<xref ref-type="bibr" rid="R21">21</xref>, <xref ref-type="bibr" rid="R22">22</xref>, <xref ref-type="bibr" rid="R23">23</xref>, <xref ref-type="bibr" rid="R24">24</xref>].</p><p id="P4">Interestingly, in the brain, regions that support contextual inference also seem to rely on support from other brain regions specifically during learning. In particular, prefrontal cortex (PFC) input to the hippocampus (HPC) is important during learning [<xref ref-type="bibr" rid="R25">25</xref>, <xref ref-type="bibr" rid="R26">26</xref>, <xref ref-type="bibr" rid="R27">27</xref>, <xref ref-type="bibr" rid="R28">28</xref>]. The HPC is thought to learn context-dependent, detailoriented maps of features like sounds, location, time, and odours that allow for the prediction of upcoming features based on currently observed features [<xref ref-type="bibr" rid="R29">29</xref>, <xref ref-type="bibr" rid="R30">30</xref>, <xref ref-type="bibr" rid="R31">31</xref>, <xref ref-type="bibr" rid="R32">32</xref>, <xref ref-type="bibr" rid="R33">33</xref>, <xref ref-type="bibr" rid="R1">1</xref>], similar to RNNs [<xref ref-type="bibr" rid="R34">34</xref>] and Bayesian model-based representations [<xref ref-type="bibr" rid="R35">35</xref>, <xref ref-type="bibr" rid="R36">36</xref>]. In contrast, the PFC is often proposed to classify different contexts based on their distinct rules. More specifically, PFC is thought to differentiate between contexts based on the distinct outcomes that result from following each rule in each context [<xref ref-type="bibr" rid="R37">37</xref>, <xref ref-type="bibr" rid="R28">28</xref>, <xref ref-type="bibr" rid="R38">38</xref>, <xref ref-type="bibr" rid="R39">39</xref>, <xref ref-type="bibr" rid="R6">6</xref>, <xref ref-type="bibr" rid="R39">39</xref>, <xref ref-type="bibr" rid="R40">40</xref>, <xref ref-type="bibr" rid="R2">2</xref>]. As a result, within the brain it seems that outcome-based approaches in PFC may support the generation of a context signal that can support the learning of more complex, feature-based approaches in HPC.</p><p id="P5">Computationally, both feature- and outcome-based approaches can predict the current context based on their experience within the environment. An example of using features to infer the current context is adjusting the amount of flour you add to bread dough on the fly depending on the appearance and texture of the dough. However, if it is your first time making that specific bread and you do not know what the dough should look like, you can instead use the outcome of whether the resulting loaf is too dry or too wet to adjust the amount of flour you add next time. As is evident in this example, feature- and outcome-based approaches trade-off in their ability to predict contextual changes within a trial and in the complexity of the representations they use. Feature-based approaches learn complex, detailed representations of the environment, whereas outcome-based approaches focus on learning only about one parameter - outcomes associated with behaviour.</p><p id="P6">The greatly reduced complexity of learnt representations in outcome-based approaches compared to feature-based approaches allows outcome-based approaches to rapidly learn to distinguish different contexts [<xref ref-type="bibr" rid="R41">41</xref>, <xref ref-type="bibr" rid="R42">42</xref>, <xref ref-type="bibr" rid="R43">43</xref>]. However, by design outcome-based approaches can only react to changes in behavioural outcomes once they have been experienced. In contrast, although they are more complex, feature-based approaches are able to predict changes in outcome in advance of them being experienced, by identifying changes in the environment using the detailed representations they have created. On this basis, in this study we hypothesised that during learning - especially where distinct contexts have increasingly overlapping features - the initial differentiation of contexts by feature-based models may benefit from the addition of a stable ‘teacher’ in the form of an outcome-based algorithm.</p><p id="P7">Therefore in this study we first characterise the performance of agents utilising each of these strategies in a simple, context-dependent task. We then go on to probe how these agents cope with tasks with decreasing contextual SNR, and ask to what extent combining both strategies mitigates these impairments. Finally, we test the generalisability of our findings by investigating how these algorithms perform on a series of common behavioural tasks used in the neuroscience community.</p></sec><sec id="S2" sec-type="results"><title>Results</title><p id="P8">Our main goals were to investigate i) the mechanism underlying deficits in learning to perform contextual inference that occur as contextual SNR decreases and ii) whether such deficits could be alleviated by incorporating outcome-based inference during learning. To test the performance of outcome- and feature-based approaches, we trained and probed the learning of these algorithms on a simple cued T-maze paradigm often used in both neuroscience and computational experiments investigating contextual inference (<xref ref-type="fig" rid="F1">Figure 1A</xref>). In this task a distinct cue at the beginning of a T-shaped maze indicates whether the left or right arm is rewarded [<xref ref-type="bibr" rid="R10">10</xref>, <xref ref-type="bibr" rid="R23">23</xref>, <xref ref-type="bibr" rid="R44">44</xref>, <xref ref-type="bibr" rid="R45">45</xref>, <xref ref-type="bibr" rid="R46">46</xref>]. This choice of task facilitated comparison of the performance and workings of our algorithms with neuroscience data (see also <xref ref-type="fig" rid="F7">Figure 7</xref> for comparison with other common neuroscience tasks).</p><p id="P9">We modelled the T-maze task by representing each arm of the maze as 3 discrete locations - leading to 9 total discrete locations. We then represented the cue as an additional discrete feature at one single location in the starting arm (<xref ref-type="fig" rid="F1">Figure 1A</xref>). To perform the task, an agent must choose actions to transition between discrete locations. After every step, the agent observes its current location and whether there is a cue present. Each ‘attempt’ ends when the agent reaches the end of the left or right arm, after which the agent is teleported back to the starting state. The agent has an unlimited number of attempts to make the correct choice and once the correct choice is made it moves into the next trial.</p><p id="P10">We focused on a Bayesian model-based approach, to allow us to easily probe how learnt representations support behaviour. Specifically, to model feature-based strategies, we used a Bayesian successor feature algorithm, which we refer to as ‘feature inference’ (FI). This model infers and predicts optimal behaviour based on differing features of the environment using context-specific successor representations (SRs) [<xref ref-type="bibr" rid="R14">14</xref>]. To model outcome-based strategies, and to investigate whether these can be used to support learning of feature inference, we used a similar model-based Bayesian approach, which we refer to as ‘outcome inference’ (OI). In contrast to feature inference, this model learns context-specific diffuse state-outcome maps to identify contexts [<xref ref-type="bibr" rid="R42">42</xref>]. As outcome inference focuses its attention solely on state-outcomes, which define the optimal behaviour, we hypothesised that it would be well suited to support initial learning of context-specific SRs for predictive inference.</p><p id="P11">To solve this task, both FI and OI learn a set of context-dependent maps of the world, which they use to identify the current context. FI learns a set of SRs <italic>M</italic> and OI learns a set of convolved reward maps <italic>C</italic>. The SRs learn the predicted future occupancy of all features of the environment given the currently observed features (see methods) (<xref ref-type="fig" rid="F1">Figure 1B</xref>). In contrast, the convolved reward maps <italic>C</italic> learn an average estimate of reward in each location using a filter that weights any rewards located either side of the current location (see methods) (<xref ref-type="fig" rid="F1">Figure 1C</xref>). Importantly, at the same time as they are learning these maps, both algorithms also use Bayesian inference to compare the current observations from the environment with their current maps of the world to infer which context <italic>z</italic> they are most likely in. Specifically, FI compares the features <italic>ϕ</italic> observed by the agent prior to and following an action and compares how well the observed transition fits with the predictions encoded in each successor map <italic>M</italic>. In contrast, OI compares the observed convolved reward <italic>cr</italic> with the predicted <italic>cr</italic> from each convolved reward map <italic>C</italic>. Based on their current observations, the agents can either infer whether the current observations align well with an existing map; or if not, the agent can create a new map.</p><p id="P12">FI or OI cannot on their own be used to select actions, as FI does not learn about rewards and OI learns only a local representation of reward. As a result, in addition to this overall architecture, for each context a corresponding temporal difference <italic>T D</italic> map is learnt for action selection within each context (<xref ref-type="fig" rid="F1">Figure 1B,C</xref>). For FI specifically, an alternative would be to learn a reward vector indicating which features are rewarded to allow for predicting the best current action. Here, we used <italic>T D</italic> maps for both strategies to (1) allow both FI and OI to use the same mechanisms for decision making to facilitate comparison of the two inference strategies and (2) isolate contextual inference from action selection. The <italic>T D</italic> maps learn value gradients across the locations and features of the environment, thereby allowing the agent to evaluate which action brings them closer to the reward (see methods for more detail). Once a context has been inferred, the context identity is passed to the corresponding context specific <italic>T D</italic> map to select the next action. Finally, the corresponding <italic>T D</italic> and <italic>M</italic> or <italic>T D</italic> and <italic>C</italic> maps are updated.</p><p id="P13">Importantly, the agents are not provided with any representations of the environment and do not receive any external context signals, aside from the environmental observations that indirectly indicate the hidden context. As a result, the main challenge for the agents is that they must simultaneously learn context-dependent representations, and use these representations to infer the current context and guide behaviour. As a simple comparative metric, if the algorithms learn accurate representations of both contexts, within the cued T-maze FI should be able to predict the current context based on the cue, whereas OI should be able to react to the absence of reward on the previously rewarded arm to infer the current context (<xref ref-type="fig" rid="F1">Figure 1D</xref>).</p><sec id="S3"><title>Feature and outcome inference outperform alternative strategies during cued switches in a T-maze</title><p id="P14">We first wanted to understand whether FI and OI can solve contextual inference tasks in principle, when contextual SNR is high. To do so, we tested whether they can solve the simple cued T-maze task, that has been readily shown to be solvable by other contextual inference algorithms [<xref ref-type="bibr" rid="R10">10</xref>, <xref ref-type="bibr" rid="R19">19</xref>, <xref ref-type="bibr" rid="R20">20</xref>]. Importantly, using the simple cued-T maze configuration described above provides a baseline, upon which we can then investigate how each model type is influenced by the amount of overlap between environments in different contexts by decreasing the SNR of the contextual signal. We will do so either by i) adding distractor cues around the predictive cue or ii) increasing the distance between the predictive cue and the choice.</p><p id="P15">In order to test whether agents and animals can perform predictive, cue-based inference in the cued T-maze we present trial types from each context in a random order, as in the computational and rodent literature [<xref ref-type="bibr" rid="R10">10</xref>, <xref ref-type="bibr" rid="R19">19</xref>, <xref ref-type="bibr" rid="R20">20</xref>, <xref ref-type="bibr" rid="R46">46</xref>, <xref ref-type="bibr" rid="R45">45</xref>, <xref ref-type="bibr" rid="R47">47</xref>]. However, in laboratory settings an initial learning phase is required for animals to learn this task [<xref ref-type="bibr" rid="R46">46</xref>, <xref ref-type="bibr" rid="R45">45</xref>, <xref ref-type="bibr" rid="R47">47</xref>]. A common training technique to teach contextual inference tasks consists of using a blocked learning phase, where trials are arranged in blocks of the same trial type. This technique has been used in both the rodent and human literature in similar tasks [<xref ref-type="bibr" rid="R37">37</xref>, <xref ref-type="bibr" rid="R48">48</xref>, <xref ref-type="bibr" rid="R37">37</xref>, <xref ref-type="bibr" rid="R49">49</xref>, <xref ref-type="bibr" rid="R50">50</xref>], as well as in the computational literature, specifically for training Bayesian SR algorithms [<xref ref-type="bibr" rid="R14">14</xref>, <xref ref-type="bibr" rid="R1">1</xref>]. Guided by this literature, we first trained FI and OI agents to perform the cued T-maze in a blocked learning phase, where trials were arranged as blocks of the same trial type, before moving agents on to the full task with randomised trials.</p><p id="P16">Once agents had completed 50 correct trials in one context, the context was switched such that the cue was different and the reward was located on the opposite arm. We trained agents on this version of the task for 10 blocks (500 correct trials). Next, we allowed these agents to continue to perform the task, and tested their performance on a further 10 blocks to see how quickly each algorithm reacted to changes in the trial type (<xref ref-type="fig" rid="F2">Figure 2A,B</xref>). We found that both FI and OI models learnt to perform the block switches well, with OI requiring 2-3 attempts to adapt its behaviour following a trial type switch and FI requiring 1-2 attempts (<xref ref-type="fig" rid="F2">Figure 2C,D</xref>).</p><p id="P17">To confirm that Bayesian inference is necessary to perform contextual inference in the cued T-maze, we next compared the performance of these models with the performance of their individual components. We specifically compared the algorithms to an SR algorithm and TD algorithm that both learnt only one map of the environment. Within the SR algorithm, we compared two different versions i) where the reward vector was either updated incrementally (SR) or ii) where the learning rate was set to 1 such that rewards were learnt and unlearnt within one experience (SR1). We found that both feature and outcome inference performed better than the algorithms that do not use Bayesian inference on block switches (<xref ref-type="fig" rid="F2">Figure 2C,D</xref>). This increase in performance of inference agents is due to the SR and TD algorithm both needing to gradually unlearn the reward location and relearn the new reward location following a trial switch. Even in SR1, following the absence of reward the agent must re-explore the whole environment to find the new reward location, as it has no memory of previous reward locations.</p></sec><sec id="S4"><title>Predictive inference is required to solve a cued T-maze</title><p id="P18">We next wanted to test the extent to which each algorithm could support predictive inference. To do this, we exposed each agent that had been trained on the block switch task described above, to a version of the task that required predictive cue-based inference within each trial, where the context (and hence appropriate behaviour) on each trial was chosen at random. Therefore in this instance, past outcome is not informative for behaviour, and so we reasoned that OI agents would not be ale to perform this version of the task. We let each agent perform an additional 400 trials before testing their performance over 100 further random trials (<xref ref-type="fig" rid="F2">Figure 2B</xref>).</p><p id="P19">Consistent with this reasoning, FI agents could accurately predict the reward location based on the cue around 80 percent of the time, while OI agents could not. Indeed when we compared performance to all other model types (SR, SR1, TD), only FI algorithms were able to perform predictive inference (<xref ref-type="fig" rid="F2">Figure 2C,D</xref>).</p><p id="P20">We next investigated what specific representation allowed FI agents to perform the task. We found that FI agents formed a distinct successor map for each context, including distinct representations of the cue identity and the associated behaviour that the agent follows. This can be seen by comparing the average predicted future occupancy of each feature when the agent is in the starting location across each context (<xref ref-type="fig" rid="F2">Figure 2E</xref>).</p><p id="P21">Together these initial results confirm previous reports [<xref ref-type="bibr" rid="R42">42</xref>] that OI can learn to use an outcome-based strategy that is effective at solving tasks that contain blocks of repeating trials, but cannot solve random trial presentations, where the current best behaviour is independent of past outcomes. In contrast, FI can learn to use a predictive strategy, offering an improved performance on blocks of trials, as well as the ability to solve random trials.</p></sec><sec id="S5"><title>Performance of feature inference degrades as contexts become more similar</title><p id="P22">Our results so far suggest that in principle, FI is well-suited to perform predictive inference in the cued T-maze task. We next looked at the effect of decreasing the contextual SNR, by increasing environmental overlap between contexts. We tested this with two complementary manipulations to the task structure. First, we created a series of different mazes, where we incrementally added an increasing number of ‘distractor’ features that are present in both contexts around the cue (<xref ref-type="fig" rid="F3">Figure 3A</xref>) [<xref ref-type="bibr" rid="R51">51</xref>, <xref ref-type="bibr" rid="R52">52</xref>, <xref ref-type="bibr" rid="R53">53</xref>]. In other words, while in the base task the cues may have been A vs B, a version with 3 distractor cues has 4 cues in each maze in total, three of which overlap: XYZA vs XYZB. To compliment this, we also created a series of mazes where instead we incrementally increased the distance between the cue and the choice-point, without changing the number or identity of the cues (<xref ref-type="fig" rid="F3">Figure 3D</xref>) [<xref ref-type="bibr" rid="R44">44</xref>, <xref ref-type="bibr" rid="R46">46</xref>, <xref ref-type="bibr" rid="R45">45</xref>]. In this task the increased overlap between the two contexts arises from the increasing number of locations in the centre arm, that due to their lack of distinguishing features are highly similar across contexts.</p><p id="P23">Using this approach, we found distinct effects of distractors both across stages of the task, and across FI and OI agents. These differences were independent of the means by which the overlap increased. During the block switching stage of the task, the number of attempts required by FI agents to reverse increased consistently with increasing environmental overlap between contexts (<xref ref-type="fig" rid="F3">Figure 3B,E</xref>). In contrast, we found that the number of attempts to reverse on block trials using OI was extremely consistent independent of the amount of environmental overlap between contexts (<xref ref-type="fig" rid="F3">Figure 3B,E</xref>).</p><p id="P24">Similar to the block switching stage, the performance of FI on random trials decreased steadily to chance with increasing contextual overlap (<xref ref-type="fig" rid="F3">Figure 3C,F</xref>). This was in contrast to OI agents remaining unable to perform above chance for any of the distractor combinations. Together, these results indicate that during learning, OI is robust to decreases in contextual SNR, resulting from increasing overlapping features. This suggests that OI may be well-suited to provide a context signal to FI agents that otherwise struggle during learning with low SNR.</p></sec><sec id="S6"><title>Supporting feature inference with outcome inference during learning rescues performance at low SNR</title><p id="P25">Our data suggest that FI agents increasingly struggled to perform the T-maze task as the two contexts became more similar. In contrast, OI agents were able to stably learn to separate the two contexts, independent of their overlap. Therefore we reasoned that supporting FI with OI during learning may overcome the limitations of FI, by supporting more accurate initial learning.</p><p id="P26">To test this, we built a joint inference architecture where OI supports FI during learning (<xref ref-type="fig" rid="F4">Figure 4A</xref>). To implement this, we began by generating joint agents containing both FI and OI algorithms, and allowing the algorithms to interact with one another only during the initial learning phase - for the first 200 trials of the task. During these trials the agent created a ‘joint estimate’ of which context is most likely by calculating the average of the probability according to the FI and the OI algorithms (<xref ref-type="fig" rid="F4">Figure 4B</xref>). We chose the average over the first 200 trials as a proof of principle experiment, as it allows us to investigate whether the simple metric of averaging implemented for very few trials at the start of learning is sufficient for improving performance, and leaves room for more complex methods in the future. This jointly inferred context identity is then used to select the optimal choice and to update the corresponding outcome and feature inference maps. After the first 200 trials, the agents then revert to a solely FI strategy, and no longer incorporate information from OI.</p><p id="P27">When we tested this joint inference model, in contrast to FI alone, we found this algorithm had stable performance across all contextual SNRs, both in the case where SNR decreased with increases in distractor features and in the case where SNR decreased with increases in distance between the cue and the choice (<xref ref-type="fig" rid="F4">Figure 4C-F</xref>. Crucially, although we only allowed the interaction of the two agents for from first 200 trials of the block switches, this stable performance lasted throughout the remaining block switches, and also resulted in stable performance during random trials. Thus, supporting FI during learning with OI rescues deficits in performance with low SNR, even after input from the OI algorithm is no longer present.</p><p id="P28">Importantly, the ability to incorporate OI into FI estimates can occur in multiple ways - not just via real-time access to both estimates to drive contextual inference. Therefore, we also tested whether this ability to support feature inference could be achieved using multiple alternative implementations of the interaction. In particular we focused on a model that uses replay of contexts inferred by outcome inference after each trial (<xref ref-type="fig" rid="F4">Figure 4</xref>-<xref ref-type="supplementary-material" rid="SD1">S1A</xref>), and a model that uses ideal observer estimates of the current context (<xref ref-type="fig" rid="F4">Figure 4</xref>-<xref ref-type="supplementary-material" rid="SD1">S1B</xref>). In both cases we found that such approaches did not consistently overcome the deficits of feature-based learning. Therefore, while multiple mechanisms could support this joint estimate, it seems that real time access to both outcome and feature based representations is particularly beneficial for accurate contextual inference at low SNR.</p></sec><sec id="S7"><title>Access to outcome inference stabilises the learning of feature-based representations</title><p id="P29">Our data confirm that the presence of OI information during learning greatly facilitates the use of FI, even long after this OI information is no longer available. We next wanted to investigate <italic>how</italic> the addition of OI to the FI algorithm supported this facilitation of learning. We found that in the block learning phase, FI, but not OI or joint inference, struggled to maintain performance as environmental overlap between contexts increased. We reasoned that the distinction between algorithms is that in FI it is necessary to learn to predict the future occupancy of all of the features in the environment, whereas OI only learns to predict one feature - the reward. As overlap between contexts increases, the features of the environment become more similar between contexts, but the observed rewards do not. Therefore, when contexts are highly overlapping, early in learning FI agents will be likely to assign observations to the incorrect context, thereby reducing their ability to distinguish between the different contexts. As a result, specifically the first contextual switch could be impacted by increasing contextual overlap, which could influence the long term representations of the FI model, by degrading the difference between the representations.</p><p id="P30">To test this, we investigated the first time the agent experienced a switch in trial type (the first switch from context A to context B in the block switch stage). We reasoned that even on this first switch, when overlap between contexts is high, the FI algorithm would incorrectly incorporate new observations in the original context, rather than create a new context from these new observations. Consistent with this, we found that agents using FI displayed a large increase in the amount of incorrect updates to the original context representation as overlap increased. In contrast, and consistent with our previous data, this incorrect updating did not occur when using OI or joint inference, and these differences between FI, OI and joint inference were independent of whether overlap was increased by increasing the number of distractor features, or increasing the distance form the cue to the choice point (<xref ref-type="fig" rid="F5">Figure 5A</xref>).</p><p id="P31">We next investigated what influence this incorrect updating had on the representations of context. To do this, we plotted the predicted future occupancy of each environmental feature when the agents are at the start of the maze both before and after the first trial type switch (<xref ref-type="fig" rid="F5">Figure 5B</xref>). We found that at the end of the first block, preceding the first trial type switch, there was a clear distinction between the expectation of cue 1 and cue 2, despite multiple distractors (<xref ref-type="fig" rid="F5">Figure 5B</xref>). We then compared the effect of the first trial type switch on the representation of both cues in the FI and joint inference algorithms. Interestingly, when using FI, the incorrect cue became incorporated in the original context representation (note high expected cue 2, <xref ref-type="fig" rid="F5">Figure 5B</xref>). However, this was not the case when FI was combined with OI in the joint algorithm (note that cue 1 continues to be the most expected, <xref ref-type="fig" rid="F5">Figure 5B</xref>). Therefore consistent with our prediction, FI agents incorporate new observations into the wrong representation after the first switch in context.</p><p id="P32">Interestingly, we found that this degradation of context-specific maps when using FI was long-lasting: on random trials the FI agents had a decreased representation of the correct cue and an increased representation of the incorrect cue in both context representations as the amount of environmental overlap increased. This was despite these trials occurring many hundreds of trials after the first trial type switch. This long lasting degradation was also mitigated by use of a joint agent, despite again the access of this agent to OI information being removed many hundreds of trials earlier (<xref ref-type="fig" rid="F5">Figure 5C</xref>).</p><p id="P33">This degradation of context-specific maps in FI agents had a marked behavioural effect - it resulted in a decrease in the inferred probability of the context following the cue on correct trials, indicating a larger chance of inferring the incorrect context (<xref ref-type="fig" rid="F5">Figure 5D</xref>). Therefore, we found that increasing environmental overlap between contexts impairs the ability of FI to learn distinct context representations. This deficit can be compensated for by integrating outcome inference during learning.</p><p id="P34">Importantly, this specific implementation of the FI model makes a number of mechanistic assumptions, and so we also tested multiple alternate methods for making inference of a new map easier for FI when there is a large amount of overlap between context features. First, we gave the FI agent access to an additional map filled with a random-exploration SR (learnt through random exploration of the environment - see methods) when each new context was first introduced [<xref ref-type="bibr" rid="R54">54</xref>]. This removes the requirement to create a new, unexplored contextual map upon inference of a contextual switch, which we hypothesised could in some cases decrease the likelihood of switching representations. However, this approach did not improve performance of the FI agent, as the lack of directionality in the random-exploration SR led to a poorer fit with the new experiences than the previous context map (<xref ref-type="fig" rid="F5">Figure 5</xref>-<xref ref-type="supplementary-material" rid="SD1">S1A</xref>). Second, we tested the influence of adding the reward as a feature, which could intuitively be thought to further distinguish the two contexts based on reward location [<xref ref-type="bibr" rid="R14">14</xref>]. Interestingly, this approach further increased the amount of environmental overlap between different contexts during learning and thereby unreliably impacted performance (<xref ref-type="fig" rid="F5">Figure 5</xref>-<xref ref-type="supplementary-material" rid="SD1">S1B</xref>). Lastly, we found that blocking access to the incorrect arm during training, forcing the choice of the agent towards the rewarded arm, a strategy often employed during rodent experiments [<xref ref-type="bibr" rid="R44">44</xref>, <xref ref-type="bibr" rid="R46">46</xref>, <xref ref-type="bibr" rid="R45">45</xref>], also did not consistently improve performance of the feature inference algorithm (<xref ref-type="fig" rid="F5">Figure 5</xref>-<xref ref-type="supplementary-material" rid="SD1">S1C</xref>). To summarise, the most effective means we found to support the formation of distinct context maps in FI is by integrating outcome inference during learning.</p><p id="P35">Together, our results indicate that as overlap increases between contexts, the ability of FI agents to perform predictive inference decreases. This is similar to previous observations of decreasing performance of RNNs as overlap between contexts increases in similar tasks [<xref ref-type="bibr" rid="R10">10</xref>, <xref ref-type="bibr" rid="R19">19</xref>, <xref ref-type="bibr" rid="R20">20</xref>]. Interestingly, we found that the deficits in FI could be rescued by using OI to guide initial learning of feature representations. Specifically, we found that OI could be used to stably identify changes in context during learning, allowing for the FI algorithm to learn distinct representations of each context, resulting in long-lasting improvements in behaviour.</p></sec><sec id="S8"><title>Joint agents produce internal variables similar to hippocampal splitter cells</title><p id="P36">A core goal of our study was to frame the interaction between these algorithms in a way that was interpretable from a neuroscience perspective. Neural activity in the CA1 area of the hippocampus (HPC) has been proposed to exhibit an implementation of SRs, where the activity of individual neurons represents the predicted future occupancy of a specific location [<xref ref-type="bibr" rid="R35">35</xref>, <xref ref-type="bibr" rid="R55">55</xref>, <xref ref-type="bibr" rid="R56">56</xref>]. Multiple neuroscience studies have shown that a signature of HPC activity in context-dependent tasks is the presence of so called ‘splitter cells’: cells that fire differently in the same location in an environment depending on the inferred context [<xref ref-type="bibr" rid="R57">57</xref>, <xref ref-type="bibr" rid="R58">58</xref>, <xref ref-type="bibr" rid="R59">59</xref>, <xref ref-type="bibr" rid="R60">60</xref>]. These cells are thought to allow for similar locations in different contexts to be represented with distinct neural activity [<xref ref-type="bibr" rid="R25">25</xref>, <xref ref-type="bibr" rid="R37">37</xref>, <xref ref-type="bibr" rid="R1">1</xref>]. We hypothesised that agents utilising FI (therefore both FI and joint inference agents) would produce activity reminiscent of splitter cells, as in these models different SRs are learnt and used to guide behaviour in each of the two contexts of the task [<xref ref-type="bibr" rid="R42">42</xref>, <xref ref-type="bibr" rid="R1">1</xref>]. To investigate this directly, we estimated simulated ‘cell firing’ in each of our agents, using the future predicted occupancy of a chosen location, weighed by the probability of the context map being inferred. We then calculated this firing for each context map at every location in the environment to create a set of ‘place cells’ [<xref ref-type="bibr" rid="R35">35</xref>] for each agent.</p><p id="P37">We then used this metric to compare simulated cell firing along the shared central arm of the maze - similar to classic ‘splitter cell’ papers [<xref ref-type="bibr" rid="R57">57</xref>, <xref ref-type="bibr" rid="R58">58</xref>, <xref ref-type="bibr" rid="R59">59</xref>]. For each simulated cell with a firing field in the central arm, we found the location and context in which its ‘activity’ was highest, and compared this ‘activity’ to the same location in the other context. When we did this, we found that both joint and FI models produced patterns highly reminiscent of ‘splitter cells’ (<xref ref-type="fig" rid="F6">Figure 6A,B</xref>). Therefore, similar to previous work [<xref ref-type="bibr" rid="R42">42</xref>, <xref ref-type="bibr" rid="R1">1</xref>, <xref ref-type="bibr" rid="R34">34</xref>, <xref ref-type="bibr" rid="R11">11</xref>] in our task, FI models can reproduce classic HPC splitter cell experiments.</p></sec><sec id="S9"><title>Removal of outcome inference mimics loss of HPC splitter cells after PFC lesions</title><p id="P38">We next wanted to investigate whether the interaction between FI and OI may behave in a similar way to the interaction between PFC and HPC. Consistent with this idea, PFC is often thought of as performing outcome inference [<xref ref-type="bibr" rid="R6">6</xref>, <xref ref-type="bibr" rid="R39">39</xref>, <xref ref-type="bibr" rid="R37">37</xref>] and its input to HPC is crucial for accurate learning [<xref ref-type="bibr" rid="R25">25</xref>, <xref ref-type="bibr" rid="R26">26</xref>, <xref ref-type="bibr" rid="R27">27</xref>, <xref ref-type="bibr" rid="R28">28</xref>]. Moreover, if PFC input to the HPC is eliminated, there is a dramatic loss of splitter cells in the HPC [<xref ref-type="bibr" rid="R25">25</xref>, <xref ref-type="bibr" rid="R37">37</xref>], suggesting that loss of PFC input degrades the representation of context in HPC. In our framework, during learning the joint inference algorithm could be described as using contextual estimates based on OI calculated in PFC, and integrating these with contextual estimates based on feature inference in HPC. We therefore wanted to investigate if removing OI from the joint algorithm during learning could replicate the loss of splitter cells in HPC caused by removal of PFC input.</p><p id="P39">To investigate this, we returned to our simulated splitter cell analysis described in <xref ref-type="fig" rid="F6">Figure 6A,B</xref>. We compared the properties of simulated cell activity from the FI algorithm with or without the addition of OI. As before, we found that in the full joint inference algorithm cells showed a large difference in firing between their preferred and non-preferred contexts (<xref ref-type="fig" rid="F6">Figure 6A,B</xref>). In contrast, in the algorithm without outcome inference, this difference in firing was greatly reduced (<xref ref-type="fig" rid="F6">Figure 6A,B</xref>).</p><p id="P40">We next quantified this loss of splitter cell-like activity more generally by looking at the difference in the probability of firing of each cell in its preferred versus non-preferred context. Using this metric, we found large differences in activity in each context along the entire stem of the maze in agents utilising joint inference. Consistent with our previous findings, we also found that these differences were consistently lower in agents without access to OI (<xref ref-type="fig" rid="F6">Figure 6C</xref>). Further, we found that while increasing environmental overlap between contexts had minimal effect on splitter-like activity in joint inference agents, increased overlap led to a large reduction in this difference in activity in agents without OI (<xref ref-type="fig" rid="F6">Figure 6D,E</xref>). Again, this degradation was similar irrespective of whether overlap was increased using distractors or distance between the cue and the choice point. Consistent with this, we found that agent-by-agent, decreased behavioural performance without OI was strongly correlated with decreased differences in splitter-like activity (<xref ref-type="fig" rid="F6">Figure 6D,E</xref>).</p><p id="P41">These experiments suggest that the joint inference model provides a framework describing HPC activity during behaviour requiring use of hidden contexts, and also the contribution of PFC to the learning of this behaviour. Using this framework, we found evidence to suggest that the strength of splitter cell specificity indicates how different context representations are from one another. In turn, this also indicates how well the agent will perform on tasks involving predictive inference.</p></sec><sec id="S10"><title>Outcome inference supports learning of a delayed non-match to sample task</title><p id="P42">In our results so far, we have shown that supporting FI with OI during learning generates a robust and long lasting basis for performing predictive inference. Until now however, our experiments have been limited to a relatively simple T-maze task. Although this task is widely used in the neuroscience literature, we next wanted to investigate if the interaction of FI and OI may be a more general mechanism to facilitate performance on contextual inference tasks. To do this, we focused on two alternate tasks - first a classic delayed non match to sample task, which has been used for decades to study PFC and HPC function [<xref ref-type="bibr" rid="R61">61</xref>, <xref ref-type="bibr" rid="R62">62</xref>, <xref ref-type="bibr" rid="R63">63</xref>, <xref ref-type="bibr" rid="R64">64</xref>], and second a more complex structural learning task, that has been proposed to isolate a core function of the HPC in associative learning [<xref ref-type="bibr" rid="R65">65</xref>, <xref ref-type="bibr" rid="R66">66</xref>, <xref ref-type="bibr" rid="R67">67</xref>, <xref ref-type="bibr" rid="R68">68</xref>].</p><p id="P43">First, we investigated a delayed non-match to sample task - a behavioural task strongly dependent upon activity in both PFC [<xref ref-type="bibr" rid="R26">26</xref>, <xref ref-type="bibr" rid="R69">69</xref>, <xref ref-type="bibr" rid="R70">70</xref>, <xref ref-type="bibr" rid="R71">71</xref>, <xref ref-type="bibr" rid="R64">64</xref>] and HPC [<xref ref-type="bibr" rid="R72">72</xref>, <xref ref-type="bibr" rid="R61">61</xref>, <xref ref-type="bibr" rid="R73">73</xref>, <xref ref-type="bibr" rid="R62">62</xref>, <xref ref-type="bibr" rid="R63">63</xref>]. In this task, two cues are presented one after the other in time. If the cues do not match (i.e. the first and second cue differ from one another) the agent must make one choice, for example, ‘go’, or choose right, whereas if the cues match the agent must make a different choice, for example, ‘no-go’ or choose left (<xref ref-type="fig" rid="F7">Figure 7A</xref>) [<xref ref-type="bibr" rid="R69">69</xref>, <xref ref-type="bibr" rid="R70">70</xref>]. Therefore, we implemented this task using a similar layout as our T-maze, where two cues were presented one after the other as an agent traversed a central arm. Subsequently, the agent had to make a choice to turn left or right, based on the preceding cues: matching cues (A-A or B-B) cued a right choice, while non matching cues (A-B, or B-A) signalled a left choice. Based on the rodent literature, learning requires that animals are pretrained first on trials associated with only one outcome, before adding in trials associated with the other outcome [<xref ref-type="bibr" rid="R26">26</xref>]. To be able to directly compare with this literature, we therefore pretrained agents first on trials where the reward was located on the right arm using blocks of trials of each trial type (i.e. A-A or B-B) (6 blocks each, consisting of 50 trials), followed by training the agent on blocks of all trial types (A-A, B-B, A-B or B-A) (5 blocks each, consisting of 50 trials). Finally, we tested whether agents learnt to predict the reward location by testing agents on 500 randomly selected trials.</p><p id="P44">Based on our previous results, our prediction was that the addition of OI to FI during initial learning would facilitate the performance of FI during and following learning. Therefore, we built a joint algorithm where OI supported FI during the block training phase. We first compared the performance during the block training phase of the joint inference algorithm, where OI supported FI, to the FI algorithm without OI support. Similar to our previous results in the T-maze, we observed that during the delayed non-match to sample task the use of joint inference dramatically improved performance during training (<xref ref-type="fig" rid="F7">Figure 7B</xref>). This improvement also led to improved performance when tested on random trials. Interestingly however, in contrast to the cued T-maze, the effect was not long lasting, and only apparent for the first 200 trials (<xref ref-type="fig" rid="F7">Figure 7C</xref>). These data suggest that supporting FI with OI during learning also improves the formation of distinct context-representations for context-dependent decision making during a delayed non-match to sample task. Additionally, the overlap between tasks leading to degradation of performance over time when OI was no longer present, indicates that additional support may be necessary to ensure stability of representations after learning. Notably, this result is highly consistent with data observed in recent animal work, where PFC activity is required for accurate learning of such tasks [<xref ref-type="bibr" rid="R26">26</xref>], and further suggests a role of PFC activity may be to provide stable contextual estimates based on outcome inference to HPC as feature representations are being constructed.</p></sec><sec id="S11"><title>Outcome inference supports learning of a structured paired-associates task</title><p id="P45">While delayed non-match to sample tasks have been widely historically used, a number of alternate ways to solve such tasks, have resulted in it falling out of favour as a means to study contextual behaviour. Recently developed structured paired-associate discriminations have been designed with the aim to mitigate such issues [<xref ref-type="bibr" rid="R67">67</xref>, <xref ref-type="bibr" rid="R68">68</xref>]. Much like a delayed non-match to sample task, these tasks consist of trials each with two cues separated in time. However, to prevent the use of alternative strategies, each trial type is formed from a pool of 3 cues (A,B and C) that are combined in a specific way. Firstly, they are ordered such that the meaning of the second cue is dependent on the first (e.g. A-B is different from C-B). Secondly, the identity of the first cue is also uninformative without the second cue (A-B is different from A-C). Finally, the order the cues are experienced in is also critical (A-B is different from B-A). These trial types are arranged into 6 combinations such that the agent must use a combination of the two features in order to solve the task (<xref ref-type="fig" rid="F7">Figure 7D</xref>).</p><p id="P46">We next investigated if FI agents could solve this complex task, and whether this performance was facilitated by the addition of OI during learning. To test this we trained agents using the same stepwise approach as the non-match to sample task, where we first pretrained in blocks of only ‘turn right’ cue combinations (3 blocks each, consisting of 50 trials), followed by blocks of both ‘turn right’ and ‘turn left’ cue combinations (3 blocks each, consisting of 50 trials) (<xref ref-type="fig" rid="F7">Figure 7D</xref>). In this complex task, we found that agents with FI alone struggled to perform well - these agents struggled to solve the block stage and plateaued at only slightly above chance performance when presented random trials. However, we again found that supporting FI with OI during training dramatically improved performance. This was apparent both during training (<xref ref-type="fig" rid="F7">Figure 7E</xref>), and during random trials (<xref ref-type="fig" rid="F7">Figure 7F</xref>). Interestingly, contrasting with the non-match to sample task, but consistent with the T-maze task, the support provided by OI during learning was essential and led to long lasting improvements on random trials, where agents lacking the support of OI performed poorly, with performance remaining near chance. Together, these results indicate that during learning OI can support the formation of stable FI representations across a range of neuroscience tasks.</p></sec></sec><sec id="S12" sec-type="discussion"><title>Discussion</title><p id="P47">Most of the decisions that we make on an everyday basis are context-dependent decisions. However, we still do not understand how context-dependent learning is supported by the brain, nor do we have algorithms that can perform context-dependent learning without a constant, externally provided context signal. To address this issue, we first investigated how well current predictive algorithms can learn to perform context-dependent decision tasks. We found that as contexts become harder to distinguish from one another, predictive algorithms in general struggle to learn distinct contextual representations and thereby struggle to achieve good task performance. Focusing on a Bayesian model-based predictive algorithm, we found that supporting this predictive algorithm with outcome inference during learning stabilises learning across a wide range of contexts with increasing similarity, and across a range of distinct behavioural tasks used in neuroscience studies. Finally, we found that these algorithms reproduce observations from neuroscience experiments. Specifically, our results suggest that OI supporting FI during learning may be viewed as representing a model where context signals inferred using outcome inference are provided by PFC to HPC during learning, to support the formation of accurate predictive feature representations. Our results indicate that this framework replicates the behavioural- and cellular-level results of circuit-manipulations in animal studies across multiple behavioural paradigms, where inhibiting PFC input to HPC during learning prevents learning and produces signature loss of context-specific ‘splitter cell’ activity. Our results indicate that predictive approaches when supported with outcome inference during learning, make up a resilient system for predicting the optimal behaviour in context-dependent decision tasks. Together, our results offer a framework for interpreting behavioural strategies and cellular activity from neuroscience experiments during learning of context-dependent behavioural tasks. Our future work will aim to use this framework to investigate how the brain supports context-dependent learning.</p><sec id="S13"><title>Implications for understanding how the brain performs contextual inference</title><p id="P48">A key caveat of a number of the most promising models of contextual inference, is that such models require an explicit context signal to be provided in order to successfully to support learning [<xref ref-type="bibr" rid="R23">23</xref>, <xref ref-type="bibr" rid="R22">22</xref>, <xref ref-type="bibr" rid="R21">21</xref>, <xref ref-type="bibr" rid="R24">24</xref>]. Here we propose how such a context-signal might be simultaneously learnt during experience, removing the need for this provision. Moreover, we show that this approach can learn to solve multiple behavioural tasks commonly used in animals in neuroscience studies. In the future, using these models may allow us to tease apart the extent to which different circuits or brain regions carry distinct functions to support context-dependent learning. For example, our model predicts that for learning about abstract concepts, PFC may provide outcome-based support to HPC during learning [<xref ref-type="bibr" rid="R27">27</xref>, <xref ref-type="bibr" rid="R25">25</xref>, <xref ref-type="bibr" rid="R26">26</xref>, <xref ref-type="bibr" rid="R28">28</xref>]. On a more conceptual level, our models provide a framework that can begin to help us understand how the interplay of different brain regions that each focus on different details of our experiences, can together guide learning.</p></sec><sec id="S14"><title>Limitations of model architecture</title><p id="P49">It is important to point out that because our algorithms are designed based on first principles, we make a number of assumptions about how the agents explore and learn about the environment.</p><p id="P50">Firstly, we assumed that the agents learn using a random walk, however a more targeted strategy, for example Lévy flight, could make learning of representations easier [<xref ref-type="bibr" rid="R74">74</xref>]. Secondly, we focused on <italic>ϵ</italic>-greedy explore-exploit strategies in this study, and it is important to note that alternative strategies could also aid in improving learning speed [<xref ref-type="bibr" rid="R42">42</xref>, <xref ref-type="bibr" rid="R75">75</xref>]. This is because the way that the agents learn about the environment is extremely important to how quickly they learn and how well they perform on the paradigm. Even the differences in the random-walk each agent uses to explore here lead to large differences in how well the task is learnt, visible in the variance around average performance. As a result, improving the speed of learning individual context representations may reduce interference between context maps.</p><p id="P51">Additionally, the way in which the outcome and feature inference agents identify different contexts depends on the initialization of parameters for the algorithms. As a result, different behavioural tasks will require the use of distinct parameters. For outcome inference, the filter length determines how distinct different reward locations need to be to be recognised as belonging to distinct contexts. For both algorithms, the optimal prior will depend on the number of distinct contexts. Despite this, it is worth noting that these parameters are all interpretable from a computational and neuroscience perspective, and remain far fewer than those needed to fit an equivalent neural network approach.</p><p id="P52">Furthermore, the current design of our algorithms require the trials during learning to be presented in blocks, where multiple trials are presented in a row from each context. If tasks did not have this structure the facilitation by outcome inference would not be possible within our current joint inference framework. We chose this design as it is common in neuroscience studies, where often both animals and humans have been shown to learn best when initially presented with different contexts in blocks of trials. As a result this design is highly applicable for a wide range of tasks currently used within neuroscience [<xref ref-type="bibr" rid="R48">48</xref>, <xref ref-type="bibr" rid="R37">37</xref>, <xref ref-type="bibr" rid="R49">49</xref>, <xref ref-type="bibr" rid="R50">50</xref>]. Additionally, alternative methods that do not require this structure will be interesting to examine in the future. For example, replay of experience at the end of each trial has been suggested to be able to support learning without such block structure - by essentially allowing for a compressed repeat of the experience to facilitate learning [<xref ref-type="bibr" rid="R42">42</xref>, <xref ref-type="bibr" rid="R76">76</xref>, <xref ref-type="bibr" rid="R77">77</xref>, <xref ref-type="bibr" rid="R78">78</xref>, <xref ref-type="bibr" rid="R79">79</xref>]. Although our implementation of replay here did not provide consistent improvements in learning within the block structure used here, it remains to be seen whether it could be used in situations where such a block structure is not present.</p><p id="P53">Finally, in the current implementation of our model, we manually determine when outcome inference should be used to support feature inference - by manually defining this interaction for the first 200 trials. In the future, it would be interesting to see how this can be automated, for example, by weighing the current utility of outcome versus feature inference. One way to achieve this could be to use the discrepancies in predictions between each model, as recently proposed for combining model-based and temporal difference learning to solve contextual inference problems [<xref ref-type="bibr" rid="R43">43</xref>]. This addition will also allow for investigation of how the brain may determine when it is necessary to provide support to predictive approaches using outcome-based information.</p></sec></sec><sec id="S15" sec-type="conclusions"><title>Conclusion</title><p id="P54">Overall, our results indicate that models that support feature-based inference with outcome-based inference during learning provide a powerful framework for understanding how we learn to make decisions to support contextual inference.</p></sec><sec id="S16" sec-type="methods | materials"><title>Methods and Materials</title><sec id="S17"><title>Behavioural task design</title><sec id="S18"><title>Cued T-maze</title><p id="P55">We modelled the T-maze as consisting of discrete spatial locations in the form of one-hot vectors, on top of which we added cues as distinct features. Each arm of the maze is made up of three distinct locations resulting in a state-space <italic>S</italic> ∈ [<italic>s</italic><sub>1</sub>, <italic>s</italic><sub>2</sub>… <italic>s</italic><sub>9</sub>]. Observations emitted at the start of the stem of the maze then take the form of <italic>ϕ</italic>(<italic>s</italic><sub>1</sub>) = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]. The predictive cues are overrepresented 4x in comparison to the locations and are present at the second location on the stem resulting in the observation <italic>ϕ</italic>(<italic>s</italic><sub>2</sub>) = [0, 1, 0, 0…, 4, 0] when in one context/trial type and <italic>ϕ</italic>(<italic>s</italic><sub>2</sub>) = [0, 1, 0, 0, …, 4] when in the other context/trial type. The agent moves between states by selecting an action from the set of actions available to them <italic>A</italic> ∈ [<italic>up, down, left, right</italic>]. If the selected action involves running into a wall, the agent remains in its previous state. When the agent reaches the end of the correct arm, it receives a reward.</p></sec></sec><sec id="S19"><title>Non-match to sample task and structural discrimination task</title><p id="P56">For the non-match to sample task and structural discrimination task we used a modified version of the T-maze design. Behaviourally these tasks are usually implemented as head-fixed tasks, where animals either must choose to lick from distinct left or right spouts [<xref ref-type="bibr" rid="R69">69</xref>, <xref ref-type="bibr" rid="R70">70</xref>] or preemptive licking on a set of rewarded trials and no preemptive licking on a set of non-rewarded trials is used as the read out for whether animals have understood the task [<xref ref-type="bibr" rid="R26">26</xref>, <xref ref-type="bibr" rid="R80">80</xref>, <xref ref-type="bibr" rid="R81">81</xref>, <xref ref-type="bibr" rid="R82">82</xref>]. Within our model the second case can be interpreted as rewarded trial types being equivalent to receiving reward at the end of the right arm of the maze and non-rewarded trial types being equivalent to receiving reward at the end of the left arm of the maze. As a result actions taken that move the agent to the right can be thought of as preemptive licking, whereas actions taken that move the agent to the left can be thought of as not licking preemptively. Additionally, we only allowed the action <italic>A</italic> ∈ [<italic>up</italic>] on the central stem of the maze to mimic the agent being presented with cues rather than physically moving between them. For the non-match to sample task, the predictive cues are overrepresented 2x and for the structural discrimination the predictive cues are overrepresented 4x.</p></sec><sec id="S20"><title>Training of algorithms</title><p id="P57">For the cued T-maze task, we trained agents on 20 block switches consisting of 50 trials each, such that the agents see each trial type 10 times. To test predictive inference, we then trained the agents on 500 random trials. For the joint inference algorithm, we used the joint prior and joint task estimate for the first 200 trials, which is equivalent to the first 4 block switches.</p><p id="P58">For the non-match to sample task, we pretrained agents alternating between blocks of the rewarded trial types until each trial type had been seen 6 times, with each block consisting of 50 trials. We then trained the agents on alternating blocks of all trial types until each trial type had been seen 5 times. To test predictive inference, we then trained the agents on 500 random trials. For the joint inference algorithm, we used the joint prior and joint task estimate for all of the block trials during training, but not during pretraining or random trials.</p><p id="P59">For the structural discrimination task, we pretrained agents alternating between blocks of the rewarded trial types until each trial type had been seen 3 times, with each block consisting of 50 trials. We then trained the agents on alternating blocks of all trial types until each trial type had been seen 3 times. To test predictive inference, we then trained the agents on 500 random trials. For the joint inference algorithm, we used the joint prior and joint task estimate for all of the block trials during training, but not during pretraining or random trials.</p></sec><sec id="S21"><title>Reinforcement learning problem statement</title><p id="P60">Reinforcement learning agents aim to maximise reward by interacting with an unknown environment. Here, they aim to solve the randomised cued T-maze, which represents a partially observable Markov-decision process (POMDP) defined by <italic>D</italic> = (<italic>L, A, p, R</italic>, Φ, <italic>γ</italic>) where <italic>L</italic> is the hidden state space, <italic>A</italic> is the action space and Φ is the observation space. For <italic>l</italic> ∈ <italic>L</italic> and <italic>a</italic> ∈ <italic>A</italic> the next hidden state <italic>l</italic>′ is given by <italic>p</italic>(<italic>l</italic>′|<italic>l, a</italic>). The hidden state <italic>l</italic> is defined by the history of past observations for <italic>ϕ</italic> ∈ Φ. The reward is defined as <italic>R</italic>(<italic>l, a, l</italic>′) and <italic>γ</italic> ∈ [0, 1] is the discount factor which reduces reward values with distance into the future. The aim of the agent is to find a policy <italic>π</italic>, a set of mappings from hidden states to actions, that maximises expected discounted total future reward, known as value <italic>V</italic>, based on reward <italic>R</italic>(<italic>s</italic>): <disp-formula id="FD1"><mml:math id="M1"><mml:mrow><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mi>π</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∣</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="P61">The cued T-maze represents a specific instance of this problem where hidden states factorise into partially observable contexts <italic>Z</italic> and fully observable states <italic>S</italic> which maintain the same transition dynamics <italic>p</italic>(<italic>s</italic>′|<italic>s, a</italic>) for all <italic>s</italic> ∈ <italic>S</italic>. The problem then reduces to a set of Markov-decision process (MDPs) <italic>G</italic><sup><italic>z</italic></sup> = (<italic>S, A, p, R, γ</italic>) for <italic>z</italic> ∈ <italic>Z</italic>, where each context is defined by a specific instantiation of the reward function <italic>r</italic>(<italic>s, a, s</italic>′) ∈ <italic>R</italic>(<italic>s, a, s</italic>′).</p><p id="P62">The optimal way to solve this problem is to infer z to predict the current relevant behaviour. However, if <italic>z</italic> cannot be predictively inferred based off of <italic>ϕ</italic> the problem reduces to a transfer learning problem as defined in [<xref ref-type="bibr" rid="R41">41</xref>] and [<xref ref-type="bibr" rid="R42">42</xref>] where the optimal behaviour is reactive to changes in <italic>R</italic>(<italic>s, a, s</italic>′) [<xref ref-type="bibr" rid="R43">43</xref>]. To model the behaviour associated with these two distinct strategies, we use an existing hippocampal model that attempts to predict the context and associated best behaviour, and an existing prefrontal cortex model that is reactive to behavioural outcomes.</p></sec><sec id="S22"><title>Feature inference model</title><p id="P63">To estimate value in this set of MDPs, first, the current relevant context must be identified. To infer <italic>z</italic> we implemented an existing model of the hippocampus from [<xref ref-type="bibr" rid="R14">14</xref>] (<xref ref-type="fig" rid="F8">Figure 8A</xref>). This model estimates its belief about the current context in the form of a belief state <italic>b</italic>, which is a distribution of probabilities over all possible contexts <italic>b</italic>(<italic>z</italic>) = <italic>P</italic> (<italic>z</italic>|<italic>ϕ</italic>). To do so, this model learns a set of trial-type specific successor feature maps <italic>M</italic><sup><italic>z</italic></sup> where <italic>M</italic> ∈ <italic>M</italic><sup><italic>z</italic></sup> is a predictive map of how observed features in <italic>ϕ</italic> are related to each other. Concurrently to learning these maps the model uses Bayesian inference over <italic>M</italic><sup><italic>z</italic></sup> to determine <italic>b</italic>(<italic>z</italic>) based on differing features between contexts.</p><p id="P64">Each successor feature map <italic>M</italic> learns the predicted future occupancy of all other features based on the current column vector of observed features <italic>ϕ. M</italic> is an <italic>ixj</italic> matrix that linearly transforms the current feature observations to predict the future occupancy of all features given the current features, called successor features <italic>ψ</italic>(<italic>s</italic>) = <italic>Mϕ</italic>(<italic>s</italic>) for <italic>ϕ</italic> ∈ Φ and <italic>ψ</italic> ∈ Ψ. We use subscript <italic>i</italic> to denote the rows and <italic>j</italic> to denote the columns, such that <italic>M</italic><sub><italic>i</italic>,<italic>j</italic></sub> = <italic>M</italic>[<italic>i, j</italic>]. Intuitively, <italic>M</italic><sub><italic>i</italic>,<italic>j</italic></sub> is the predicted future occupancy of feature <italic>i</italic> given feature <italic>j. M</italic> is learnt using a simple delta rule <italic>M</italic><sub><italic>t</italic>+1</sub> = <italic>M</italic><sub><italic>t</italic></sub> + <italic>αδg</italic> after every observation for <italic>t</italic> ∈ <italic>T</italic> with the prediction error: <disp-formula id="FD2"><mml:math id="M2"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="P65">Here, <italic>g</italic><sub><italic>i</italic></sub> = <italic>ϕ</italic><sub><italic>i</italic>,<italic>t</italic></sub>/<italic>x</italic><sub><italic>t</italic></sub> normalises the update depending on feature overrepresentation and the number of features observed x such that entries in <italic>M</italic><sub>:,<italic>j</italic></sub> represent predicted future occupancies for <italic>ϕ</italic><sub><italic>j</italic></sub> = 1.</p><p id="P66">From a probabilistic perspective the values <italic>M</italic><sub><italic>i</italic>,<italic>j</italic></sub> are point estimates of the future predicted occupancies. We assume their distribution is Gaussian with variance <inline-formula><mml:math id="M3"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>, giving a multivariate normal of the form <italic>M</italic><sub><italic>i</italic>,<italic>j</italic></sub> ∝ <inline-formula><mml:math id="M4"><mml:mrow><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to anneal the. This allows us to use a flexible learning rate known as the Kalman gain <italic>α</italic> = <italic>k</italic><sub><italic>t</italic></sub> to anneal the learning rate as confidence in <italic>M</italic> increases. The Kalman gain is calculated using a Kalman filter that tracks the uncertainty in model estimates Σ<sub><italic>t</italic>+1</sub> = Σ<sub><italic>t</italic></sub> − <italic>k</italic><sub><italic>t</italic></sub><italic>h</italic><sub><italic>t</italic></sub>Σ<sub><italic>t</italic></sub> versus the uncertainty in observations defined by the observation noise <inline-formula><mml:math id="M5"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>: <disp-formula id="FD3"><mml:math id="M6"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mo>Σ</mml:mo><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mo>Σ</mml:mo><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p id="P67">where <italic>h</italic><sub><italic>t</italic></sub> = <italic>γϕ</italic><sub><italic>t</italic>+1</sub> − <italic>ϕ</italic><sub><italic>t</italic></sub> is the discounted feature derivative. The Kalman gain decreases as the confidence in feature predictions increases, which is proportional to how often they have been observed.</p><p id="P68">To find the context map <italic>z</italic> with the best fit to the observed features, Bayesian inference is used: <italic>p</italic>(<italic>z</italic>|<italic>ϕ</italic><sub><italic>t</italic></sub>) ∝ <italic>p</italic>(<italic>ϕ</italic>|<italic>z</italic>)<italic>p</italic>(<italic>z, t</italic>)<sub><italic>t</italic></sub>. The map that can best predict <italic>ϕ</italic> is the map where the future predicted occupancies of the observed state transition are best aligned, and thereby is the map that has the smallest prediction error. If <italic>δ</italic> = 0, <italic>ϕ</italic><sub><italic>t</italic></sub> = <italic>ψ</italic>(<italic>s</italic><sub><italic>t</italic></sub>) − <italic>γψ</italic>(<italic>s</italic><sub><italic>t</italic>+1</sub>), so the probability of observing <italic>ϕ</italic> for each context map is estimated with: <disp-formula id="FD4"><mml:math id="M7"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="P69">For inference in the feature-based algorithm we assume the variance is a constant predefined hyperparameter <inline-formula><mml:math id="M8"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> to limit the impact of exploratory actions on context inference, as it allows for an improved tolerance for deviations from the current context map after learning.</p><p id="P70">For the prior a sticky Chinese restaurant process (sCRP) prior is used. Based on <italic>N</italic><sub><italic>z</italic></sub>, the number of previous observations assigned to each context <italic>z</italic> over the last <italic>y</italic> timesteps, a new context identity is generated according to the sCRP. The sCRP gives the probability of being in any of the previously experienced contexts or being in a new context: <disp-formula id="FD5"><mml:math id="M9"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mi>δ</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mo>+</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="0.5em"/><mml:mtext>i</mml:mtext><mml:mi>f</mml:mi><mml:mspace width="0.2em"/><mml:mi>z</mml:mi><mml:mspace width="0.2em"/><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.2em"/><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>l</mml:mi><mml:mi>y</mml:mi><mml:mspace width="0.2em"/><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mi>α</mml:mi><mml:mrow><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:mo>+</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="0.5em"/><mml:mtext>o</mml:mtext><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="P71">with the concentration <italic>α</italic>, the prior belief as to how likely context switching is, and stickiness <italic>β</italic>, the prior belief as to how likely the context is to stay the same as on the previous step. The sCRP is based on the belief that although there is an infinite number of possible contexts, the agent is most likely to be in the context it has seen the most in the past weighted by recency. Since the prior is intractable, a particle filtering method from [<xref ref-type="bibr" rid="R42">42</xref>] is used for its estimation.</p><p id="P72">The inferred context is thus based on how well the features observed fit the features predicted by the learnt feature map for each context and is given by <italic>z</italic>* = argmax <italic>p</italic>(<italic>z</italic>|<italic>ϕ</italic><sub><italic>t</italic></sub>).</p></sec><sec id="S23"><title>Outcome inference model</title><p id="P73">As described above, if <italic>M</italic><sup><italic>z</italic></sup> has not been learnt well or cannot be used to infer the current context the problem nears a transfer learning problem, as inference based on <italic>ϕ</italic> is poor. We used an existing model of PFC from [<xref ref-type="bibr" rid="R42">42</xref>] that was designed to solve transfer learning problems to simulate this strategy (<xref ref-type="fig" rid="F8">Figure 8B</xref>). The PFC model uses the same framework as the HPC model described above: it learns distinct maps of different contexts and uses Bayesian inference over them to determine the current context, making use of Kalman filtering to have a flexible learning rate and estimate the variance during Bayesian inference. The difference is in what information the maps contain. Instead of learning about observed features of the environment, the PFC model utilises a map of diffuse state-values to separate contexts where rewards are in sufficiently different states, meaning that different policies are necessary to reach rewards. The PFC model estimates <italic>b</italic>(<italic>z</italic>) = <italic>P</italic>(<italic>z</italic>|<italic>cr</italic>) using Bayesian inference over a set of convolved reward value maps <italic>C</italic> ∈ <italic>C</italic><sup><italic>z</italic></sup> that learn a local <italic>b</italic>(<italic>z</italic>) = <italic>P</italic> (<italic>z</italic>|<italic>cr</italic>) value estimate <italic>cr</italic> for each <italic>s</italic> based on reward locations. The PFC model learns about this environment as a tabular environment, where each state can be represented by a one-hot vector defined by current location. Briefly, outcome-based agents use a kernel of the form <italic>F</italic><sub><italic>γ</italic></sub> = [<italic>γ</italic><sup>−<italic>len</italic></sup>, …, <italic>γ</italic><sup><italic>len</italic></sup>] with a filter length <italic>len</italic> either side of the current state to calculate the convolved reward estimate <italic>cr</italic><sub><italic>t</italic></sub> = <italic>F</italic><sub><italic>γ</italic></sub> <italic>r</italic><sub><italic>t</italic>−2<italic>len</italic>+1:<italic>t</italic></sub>. Due to the filter, there is a temporal delay of len states preceding inference. The agent updates its predictions for the currently inferred context map using a simple delta rule <italic>C</italic>(<italic>s</italic>)<sub><italic>t</italic>+1</sub> = <italic>C</italic>(<italic>s</italic>)<sub><italic>t</italic></sub> + <italic>α</italic>(<italic>cr</italic><sub><italic>t</italic></sub> − <italic>C</italic>(<italic>s</italic>)<sub><italic>t</italic></sub>), where <italic>α</italic> = <italic>α</italic><sub><italic>o</italic></sub><italic>k</italic>(<italic>s</italic>) and <italic>k</italic> is calculated as above replacing <italic>h</italic><sub><italic>t</italic></sub> with the current state <italic>s</italic><sub><italic>t</italic></sub>. The estimates the model learns again assume a Gaussian of the form <italic>C</italic>(<italic>s</italic>) ∝ <italic>N</italic>(<italic>C</italic>(<italic>s</italic>), <italic>σ</italic><sup>2</sup>(<italic>s</italic>)). The same Bayesian inference mechanism is used as in feature inference and <italic>p</italic>(<italic>z</italic>|<italic>cr</italic><sub><italic>t</italic></sub>) is determined with <italic>p</italic>(<italic>cr</italic><sub><italic>t</italic></sub>|<italic>z</italic>) = <italic>p</italic>(<italic>cr</italic><sub><italic>t</italic></sub>|<italic>C</italic>(<italic>s</italic>)). As a result, outcome maps have diffuse state value estimates allowing for clustering of different contexts based on different optimal policies.</p></sec><sec id="S24"><title>Choice via temporal difference learning</title><p id="P74">We decided to separate the choice component from the inference component, to compare the ability of feature and outcome inference algorithms specifically in inference. For estimating state-action values and making choices we learn a set of temporal difference maps <italic>T D</italic><sup><italic>z</italic></sup>. Once a context identity has been selected a corresponding temporal difference model is used to pick the optimal action. The temporal difference TD value for each state gives its discounted expected future reward and is updated with <italic>δ</italic><sub><italic>t</italic></sub> = <italic>r</italic><sub><italic>t</italic>+1</sub> − <italic>T D</italic><sub><italic>t</italic></sub><italic>ϕ</italic><sub><italic>t</italic></sub> + <italic>γT D</italic><sub><italic>t</italic></sub><italic>ϕ</italic><sub><italic>t</italic>+1</sub> and the Kalman gain. The Kalman filter is used as above replacing <italic>h</italic><sub><italic>t</italic></sub> with the current state features <italic>ϕ</italic><sub><italic>t</italic></sub>. Future predicted reward is given by <italic>Q</italic>(<italic>a</italic>|<italic>ϕ</italic><sub><italic>t</italic></sub>) = <italic>T D</italic>(<italic>ϕ</italic><sub><italic>t</italic>+1</sub>) where <italic>ϕ</italic><sub><italic>t</italic>+1</sub> is determined using a one-step lookahead under the assumption that the agent has access to the transition dynamics of the environment <italic>p</italic>(<italic>ϕ</italic><sub><italic>t</italic>+1</sub>|<italic>phi</italic><sub><italic>t</italic></sub>, <italic>a</italic>). The action taken is chosen at random <italic>ϵ</italic> proportion of the time to allow for continuous exploration and with <italic>a</italic><sup>*</sup> = argmax <italic>Q</italic>(<italic>a</italic>|<italic>ϕ</italic><sub><italic>t</italic></sub>) the remainder of the time. argmax <italic>Q</italic>(<italic>a</italic>|<italic>ϕ</italic><sub><italic>t</italic></sub>)</p></sec><sec id="S25"><title>Joint inference</title><p id="P75">During learning, the joint inference algorithm determines the current most likely task based on how well the observations <italic>o</italic><sub><italic>t</italic></sub> fit both the successor feature maps and convolved reward value maps learnt for each context: <disp-formula id="FD6"><mml:math id="M10"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>∣</mml:mo><mml:mi>c</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p id="P76">Additionally, a joint prior is used where at each step the prior evolves once according to the feature dynamics and once according to the outcome dynamics. In contrast to maintaining a separate prior for each algorithm, a joint prior aids in ensuring that task estimates converge onto the equivalent maps within each algorithm and allows balancing of the different optimal hyperparameters across different algorithms. The joint inferred context is then used for making choices and updating corresponding feature and outcome maps (<xref ref-type="fig" rid="F8">Figure 8C</xref>). After learning, the agent switches to using only feature inference.</p></sec><sec id="S26"><title>Other algorithms used for comparison</title><p id="P77">Here we provide a brief description of the models used for comparison.</p></sec><sec id="S27"><title>SR and SR1</title><p id="P78">The SR algorithms learn one successor representation map M, in the same way as the HPC algorithm does. In contrast to above, we do not learn a separate TD map, but instead learn a reward vector for the SR to make choices. The SR allows decomposition of reward into the reward associated with each feature w and the state features <italic>R</italic>(<italic>s</italic>) = <italic>ϕ</italic>(<italic>s</italic>)<italic>w</italic>. The value function decomposes into future predicted occupancy of all features given the current features and the reward associated with each feature <italic>V</italic> (<italic>s</italic>) = <italic>Mϕ</italic>(<italic>s</italic>)<italic>w. w</italic> is learnt using a simple delta rule with learning rate <italic>α</italic><sub><italic>r</italic></sub>. For one-shot SR <italic>α</italic><sub><italic>r</italic></sub> is set to 1.</p></sec><sec id="S28"><title>TD</title><p id="P79">The TD algorithm learns one temporal difference map TD as described above.</p></sec><sec id="S29"><title>Other ways to combine algorithms explored</title><sec id="S30"><title>Replay</title><p id="P80">During each attempt on a trial, the agent keeps track of the feature-transitions it observes. If the agent receives a reward at the end of the attempt, the task identity inferred by the outcome-based model is accepted as ground truth and the feature-transitions are replayed within said task in the feature-based model.</p></sec><sec id="S31"><title>Ideal observer</title><p id="P81">In the ideal observer model the outcome inference algorithm is told the actual task identity, which is assigned a probability of 95 percent. The identity of the other task is assigned a probability of 5 percent.</p></sec><sec id="S32"><title>Exploration</title><p id="P82">Each agent takes 10,000 steps via random walk in the environment without the predictive cues and rewards. The learnt SR and associated covariance make up the random-policy map.</p></sec><sec id="S33"><title>Parameters and hyperparameters</title><p id="P83">Outcome inference and feature inference model parameters were optimised using the initial task with the short maze stem, starting from the original parameters in [<xref ref-type="bibr" rid="R42">42</xref>] with an initial manual search for a reasonable range of values followed by a grid search within the found range. Parameters remained the same for all other experiments, apart from the structural discrimination task. As talked about in the discussion, parameters for structural discrimination task were readjusted following the same procedure as above, as there were 6 total contexts instead of 4.</p></sec><sec id="S34"><title>Statistics</title><p id="P84">Values plotted are the mean ±95% confidence interval, unless there are outliers that skew the mean in which case the median is plotted (only in <xref ref-type="fig" rid="F5">Figure 5b</xref>: cue-choice distance). Time-series data across algorithms is analysed using mixed ANOVA and bargraph data is analysed using ANOVA. For post-hoc testing, for ANOVAs we used pairwise Tukey tests and for mixed ANOVAs we used multiple T-tests with the Benjamini/Hochberg FDR correction. * indicates p&lt;0.05. Statistical results are detailed in <xref ref-type="table" rid="T6">Table 6</xref>.</p></sec><sec id="S35"><title>Compute power</title><p id="P85">The compute power needed to generate the data in this paper from these algorithms is around 300 hours using 200 cores each with 5GB RAM.</p></sec></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Material</label><media xlink:href="EMS206053-supplement-Supplementary_Material.pdf" mimetype="application" mime-subtype="pdf" id="d35aAcFbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S37"><title>Acknowledgments</title><p>We thank Daniel Bush, Jesse Geerts, Neil Burgess and other members of the MacAskill and Burgess labs for helpful discussions and feedback. A.F.M. was supported by a Sir Henry Dale Fellowship jointly funded by the Wellcome Trust and the Royal Society 109360/Z/15/Z; and by an MRC project grant MR/W02005X/1. J.P. was supported by the Wellcome Trust 4-year PhD in Neuroscience at UCL 222292/Z/20/Z.</p></ack><sec id="S36" sec-type="data-availability"><title>Code and data availability</title><p id="P86">Model code, analysis code and data generated by the models and used for analysis will be made available publicly.</p></sec><ref-list><ref id="R1"><label>[1]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanders</surname><given-names>Honi</given-names></name><name><surname>Wilson</surname><given-names>Matthew A</given-names></name><name><surname>Gershman</surname><given-names>Samuel J</given-names></name></person-group><article-title>Hippocampal remapping as hidden state inference</article-title><source>eLife</source><year>2020</year><volume>9</volume><fpage>1</fpage><lpage>31</lpage><pub-id pub-id-type="pmcid">PMC7282808</pub-id><pub-id pub-id-type="pmid">32515352</pub-id><pub-id pub-id-type="doi">10.7554/eLife.51140</pub-id></element-citation></ref><ref id="R2"><label>[2]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wikenheiser</surname><given-names>Andrew M</given-names></name><name><surname>Schoenbaum</surname><given-names>Geoffrey</given-names></name></person-group><article-title>Over the river, through the woods: Cognitive maps in the hippocampus and orbitofrontal cortex</article-title><source>Nature Reviews Neuroscience</source><year>2016</year><day>7</day><volume>17</volume><issue>8</issue><fpage>513</fpage><lpage>523</lpage><pub-id pub-id-type="pmcid">PMC5541258</pub-id><pub-id pub-id-type="pmid">27256552</pub-id><pub-id pub-id-type="doi">10.1038/nrn.2016.56</pub-id></element-citation></ref><ref id="R3"><label>[3]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maurer</surname><given-names>Andrew P</given-names></name><name><surname>Nadel</surname><given-names>Lynn</given-names></name></person-group><article-title>The Continuity of Context: A Role for the Hippocampus</article-title><source>Trends in Cognitive Sciences</source><year>2021</year><day>3</day><volume>25</volume><issue>3</issue><fpage>187</fpage><lpage>199</lpage><pub-id pub-id-type="pmcid">PMC9617208</pub-id><pub-id pub-id-type="pmid">33431287</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2020.12.007</pub-id></element-citation></ref><ref id="R4"><label>[4]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>Samuel J</given-names></name></person-group><article-title>Context-dependent learning and causal structure</article-title><source>Psychonomic Bulletin and Review</source><year>2017</year><day>4</day><volume>24</volume><issue>2</issue><fpage>557</fpage><lpage>565</lpage><pub-id pub-id-type="pmid">27418259</pub-id></element-citation></ref><ref id="R5"><label>[5]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maren</surname><given-names>Stephen</given-names></name><name><surname>Phan</surname><given-names>K Luan</given-names></name><name><surname>Liberzon</surname><given-names>Israel</given-names></name></person-group><article-title>The contextual brain: Implications for fear conditioning, extinction and psychopathology</article-title><source>Nature Reviews Neuroscience</source><year>2013</year><day>6</day><volume>14</volume><issue>6</issue><fpage>417</fpage><lpage>428</lpage><pub-id pub-id-type="pmcid">PMC5072129</pub-id><pub-id pub-id-type="pmid">23635870</pub-id><pub-id pub-id-type="doi">10.1038/nrn3492</pub-id></element-citation></ref><ref id="R6"><label>[6]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eichenbaum</surname><given-names>Howard</given-names></name></person-group><article-title>Prefrontal-hippocampal interactions in episodic memory</article-title><source>Nature Reviews Neuroscience</source><year>2017</year><volume>18</volume><issue>9</issue><fpage>547</fpage><lpage>558</lpage><pub-id pub-id-type="pmid">28655882</pub-id></element-citation></ref><ref id="R7"><label>[7]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Okazawa</surname><given-names>Gouki</given-names></name><name><surname>Kiani</surname><given-names>Roozbeh</given-names></name></person-group><article-title>Neural Mechanisms That Make Perceptual Decisions Flexible</article-title><source>Annual Review of Physiology</source><year>2023</year><pub-id pub-id-type="pmcid">PMC10308708</pub-id><pub-id pub-id-type="pmid">36343603</pub-id><pub-id pub-id-type="doi">10.1146/annurev-physiol-031722-024731</pub-id></element-citation></ref><ref id="R8"><label>[8]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heald</surname><given-names>James B</given-names></name><name><surname>Lengyel</surname><given-names>Máté</given-names></name><name><surname>Wolpert</surname><given-names>Daniel M</given-names></name></person-group><article-title>Contextual inference underlies the learning of sensorimotor repertoires</article-title><source>Nature</source><year>2021</year><volume>600</volume><fpage>489</fpage><lpage>493</lpage><pub-id pub-id-type="pmcid">PMC8809113</pub-id><pub-id pub-id-type="pmid">34819674</pub-id><pub-id pub-id-type="doi">10.1038/s41586-021-04129-3</pub-id></element-citation></ref><ref id="R9"><label>[9]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flesch</surname><given-names>Timo</given-names></name><name><surname>Saxe</surname><given-names>Andrew</given-names></name><name><surname>Summerfield</surname><given-names>Christopher</given-names></name></person-group><article-title>Continual task learning in natural and artificial agents</article-title><source>Trends in Neurosciences</source><year>2023</year><volume>46</volume><issue>3</issue><fpage>199</fpage><lpage>210</lpage><pub-id pub-id-type="pmcid">PMC10914671</pub-id><pub-id pub-id-type="pmid">36682991</pub-id><pub-id pub-id-type="doi">10.1016/j.tins.2022.12.006</pub-id></element-citation></ref><ref id="R10"><label>[10]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bakker</surname><given-names>Bram</given-names></name></person-group><article-title>Reinforcement Learning with Long Short-Term Memory</article-title><source>Advances in Neural Information Processing Systems</source><year>2001</year><volume>14</volume></element-citation></ref><ref id="R11"><label>[11]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hennig</surname><given-names>Jay A</given-names></name><name><surname>Pinto</surname><given-names>Sandra A Romero</given-names></name><name><surname>Yamaguchi</surname><given-names>Takahiro</given-names></name><name><surname>Linderman</surname><given-names>Scott W</given-names></name><name><surname>Uchida</surname><given-names>Naoshige</given-names></name><name><surname>Gershman</surname><given-names>Samuel J</given-names></name></person-group><article-title>Emergence of belief-like representations through reinforcement learning</article-title><source>PLOS Computational Biology</source><year>2023</year><day>9</day><volume>19</volume><issue>9</issue><elocation-id>e1011067</elocation-id><pub-id pub-id-type="pmcid">PMC10513382</pub-id><pub-id pub-id-type="pmid">37695776</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1011067</pub-id></element-citation></ref><ref id="R12"><label>[12]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>Francis</given-names></name><name><surname>Yang</surname><given-names>Guangyu R</given-names></name><name><surname>Wang</surname><given-names>Xiao-Jing</given-names></name></person-group><article-title>Reward-based training of recurrent neural networks for cognitive and value-based tasks</article-title><source>eLife</source><year>2017</year><volume>6</volume><elocation-id>e21492</elocation-id><pub-id pub-id-type="pmcid">PMC5293493</pub-id><pub-id pub-id-type="pmid">28084991</pub-id><pub-id pub-id-type="doi">10.7554/eLife.21492</pub-id></element-citation></ref><ref id="R13"><label>[13]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Sheng Jun</given-names></name><name><surname>Yang</surname><given-names>Zhou</given-names></name></person-group><article-title>Effect of similarity between patterns in associative memory</article-title><source>Physical Review E</source><year>2017</year><day>1</day><volume>95</volume><issue>1</issue><pub-id pub-id-type="pmid">28208341</pub-id></element-citation></ref><ref id="R14"><label>[14]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geerts</surname><given-names>Jesse P</given-names></name><name><surname>Gershman</surname><given-names>Samuel J</given-names></name><name><surname>Burgess</surname><given-names>Neil</given-names></name><name><surname>Stachenfeld</surname><given-names>Kimberly L</given-names></name></person-group><article-title>A Probabilistic Successor Representation for Context-Dependent Learning</article-title><source>Psychological Review</source><year>2024</year><volume>131</volume><issue>2</issue><fpage>578</fpage><lpage>597</lpage><pub-id pub-id-type="pmid">37166847</pub-id></element-citation></ref><ref id="R15"><label>[15]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>Samuel J</given-names></name><name><surname>Niv</surname><given-names>Yael</given-names></name></person-group><article-title>Exploring a latent cause theory of classical conditioning</article-title><source>Learning and Behavior</source><year>2012</year><volume>40</volume><fpage>255</fpage><lpage>268</lpage><pub-id pub-id-type="pmid">22927000</pub-id></element-citation></ref><ref id="R16"><label>[16]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guntupalli</surname><given-names>J Swaroop</given-names></name><name><surname>Raju</surname><given-names>Rajkumar Vasudeva</given-names></name><name><surname>Kushagra</surname><given-names>Shrinu</given-names></name><name><surname>Wendelken</surname><given-names>Carter</given-names></name><name><surname>Sawyer</surname><given-names>Danny</given-names></name><name><surname>Deshpande</surname><given-names>Ishan</given-names></name><name><surname>Zhou</surname><given-names>Guangyao</given-names></name><name><surname>Lázaro-Gredilla</surname><given-names>Miguel</given-names></name><name><surname>George</surname><given-names>Dileep</given-names></name></person-group><article-title>Graph schemas as abstractions for transfer learning, inference, and planning</article-title><source>ArXiv</source><year>2023</year></element-citation></ref><ref id="R17"><label>[17]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>Yuhang</given-names></name><name><surname>Millidge</surname><given-names>Beren</given-names></name><name><surname>Salvatori</surname><given-names>Tommaso</given-names></name><name><surname>Lukasiewicz</surname><given-names>Thomas</given-names></name><name><surname>Xu</surname><given-names>Zhenghua</given-names></name><name><surname>Bogacz</surname><given-names>Rafal</given-names></name></person-group><article-title>Inferring neural activity before plasticity as a foundation for learning beyond backpropagation</article-title><source>Nature Neuroscience</source><year>2024</year><volume>27</volume><fpage>348</fpage><lpage>358</lpage><pub-id pub-id-type="pmcid">PMC7615830</pub-id><pub-id pub-id-type="pmid">38172438</pub-id><pub-id pub-id-type="doi">10.1038/s41593-023-01514-1</pub-id></element-citation></ref><ref id="R18"><label>[18]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arcieri</surname><given-names>Giacomo</given-names></name><name><surname>Hoelzl</surname><given-names>Cyprien</given-names></name><name><surname>Schwery</surname><given-names>Oliver</given-names></name><name><surname>Straub</surname><given-names>Daniel</given-names></name><name><surname>Papakonstantinou</surname><given-names>Konstantinos G</given-names></name><name><surname>Chatzi</surname><given-names>Eleni</given-names></name></person-group><article-title>POMDP inference and robust solution via deep reinforcement learning: an application to railway optimal maintenance</article-title><source>Machine Learning</source><year>2024</year><issue>113</issue></element-citation></ref><ref id="R19"><label>[19]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lambrechts</surname><given-names>Gaspard</given-names></name><name><surname>De Geeter</surname><given-names>Florent</given-names></name><name><surname>Vecoven</surname><given-names>Nicolas</given-names></name><name><surname>Ernst</surname><given-names>Damien</given-names></name><name><surname>Drion</surname><given-names>Guillaume</given-names></name></person-group><article-title>Warming up recurrent neural networks to maximise reachable multistability greatly improves learning</article-title><source>Neural Networks</source><year>2023</year><day>9</day><volume>166</volume><fpage>645</fpage><lpage>669</lpage><pub-id pub-id-type="pmid">37604075</pub-id></element-citation></ref><ref id="R20"><label>[20]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hess</surname><given-names>Florian</given-names></name><name><surname>Monfared</surname><given-names>Zahra</given-names></name><name><surname>Brenner</surname><given-names>Manuel</given-names></name><name><surname>Durstewitz</surname><given-names>Daniel</given-names></name></person-group><source>Generalized Teacher Forcing for Learning Chaotic Dynamics</source><conf-name>Proceedings of the 40th International Conference on Machine Learning</conf-name><year>2023</year><volume>202</volume><fpage>13017</fpage><lpage>13049</lpage></element-citation></ref><ref id="R21"><label>[21]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boven</surname><given-names>Ellen</given-names></name><name><surname>Pemberton</surname><given-names>Joseph</given-names></name><name><surname>Chadderton</surname><given-names>Paul</given-names></name><name><surname>Apps</surname><given-names>Richard</given-names></name><name><surname>Costa</surname><given-names>Rui Ponte</given-names></name></person-group><article-title>Cerebro-cerebellar networks facilitate learning through feedback decoupling</article-title><source>Nature Communications</source><year>2023</year><volume>14</volume><pub-id pub-id-type="pmcid">PMC9813152</pub-id><pub-id pub-id-type="pmid">36599827</pub-id><pub-id pub-id-type="doi">10.1038/s41467-022-35658-8</pub-id></element-citation></ref><ref id="R22"><label>[22]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flesch</surname><given-names>Timo</given-names></name><name><surname>Nagy</surname><given-names>David G</given-names></name><name><surname>Saxe</surname><given-names>Andrew</given-names></name><name><surname>Summerfield</surname><given-names>Christopher</given-names></name></person-group><article-title>Modelling continual learning in humans with Hebbian context gating and exponentially decaying task signals</article-title><source>PLoS Computational Biology</source><year>2023</year><volume>19</volume><pub-id pub-id-type="pmcid">PMC9851563</pub-id><pub-id pub-id-type="pmid">36656823</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1010808</pub-id></element-citation></ref><ref id="R23"><label>[23]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cone</surname><given-names>Ian</given-names></name><name><surname>Clopath</surname><given-names>Claudia</given-names></name></person-group><article-title>Latent representations in hippocampal network model co-evolve with behavioral exploration of task structure</article-title><source>Nature Communications</source><year>2024</year><volume>15</volume><pub-id pub-id-type="pmcid">PMC10806076</pub-id><pub-id pub-id-type="pmid">38263408</pub-id><pub-id pub-id-type="doi">10.1038/s41467-024-44871-6</pub-id></element-citation></ref><ref id="R24"><label>[24]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>Qihong</given-names></name><name><surname>Nguyen</surname><given-names>Tan T</given-names></name><name><surname>Zhang</surname><given-names>Qiong</given-names></name><name><surname>Hasson</surname><given-names>Uri</given-names></name><name><surname>Griffiths</surname><given-names>Thomas L</given-names></name><name><surname>Zacks</surname><given-names>Jeffrey M</given-names></name><name><surname>Gershman</surname><given-names>Samuel J</given-names></name><name><surname>Norman</surname><given-names>Kenneth A</given-names></name></person-group><article-title>Reconciling shared versus context-specific information in a neural network model of latent causes</article-title><source>Scientific Reports</source><year>2024</year><day>12</day><volume>14</volume><issue>1</issue><pub-id pub-id-type="pmcid">PMC11263346</pub-id><pub-id pub-id-type="pmid">39039131</pub-id><pub-id pub-id-type="doi">10.1038/s41598-024-64272-5</pub-id></element-citation></ref><ref id="R25"><label>[25]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ito</surname><given-names>Hiroshi T</given-names></name><name><surname>Zhang</surname><given-names>Sheng Jia</given-names></name><name><surname>Witter</surname><given-names>Menno P</given-names></name><name><surname>Moser</surname><given-names>Edvard I</given-names></name><name><surname>Moser</surname><given-names>May Britt</given-names></name></person-group><article-title>A prefrontal-thalamo-hippocampal circuit for goal-directed spatial navigation</article-title><source>Nature</source><year>2015</year><volume>522</volume><issue>7554</issue><fpage>50</fpage><lpage>55</lpage><pub-id pub-id-type="pmid">26017312</pub-id></element-citation></ref><ref id="R26"><label>[26]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Ding</given-names></name><name><surname>Gu</surname><given-names>Xiaowei</given-names></name><name><surname>Zhu</surname><given-names>Jia</given-names></name><name><surname>Zhang</surname><given-names>Xiaoxing</given-names></name><name><surname>Han</surname><given-names>Zhe</given-names></name><name><surname>Yan</surname><given-names>Wenjun</given-names></name><name><surname>Cheng</surname><given-names>Qi</given-names></name><name><surname>Hao</surname><given-names>Jiang</given-names></name><name><surname>Fan</surname><given-names>Hongmei</given-names></name><name><surname>Hou</surname><given-names>Ruiqing</given-names></name><name><surname>Chen</surname><given-names>Zhaoqin</given-names></name><etal/></person-group><article-title>Medial prefrontal activity during delay period contributes to learning of a working memory task</article-title><source>Science</source><year>2014</year><volume>346</volume><issue>6208</issue><fpage>458</fpage><lpage>463</lpage><pub-id pub-id-type="pmid">25342800</pub-id></element-citation></ref><ref id="R27"><label>[27]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eleore</surname><given-names>Lyndell</given-names></name><name><surname>López-Ramos</surname><given-names>Juan Carlos</given-names></name><name><surname>Guerra-Narbona</surname><given-names>Rafael</given-names></name><name><surname>José M</surname><given-names>Delgado-García</given-names></name></person-group><article-title>Role of reuniens nucleus projections to the medial prefrontal cortex and to the hippocampal pyramidal CA1 area in associative learning</article-title><source>PLoS ONE</source><year>2011</year><volume>6</volume><issue>8</issue><pub-id pub-id-type="pmcid">PMC3156136</pub-id><pub-id pub-id-type="pmid">21858159</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0023538</pub-id></element-citation></ref><ref id="R28"><label>[28]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Wei</given-names></name><name><surname>Thomas C</surname><given-names>Südhof</given-names></name></person-group><article-title>A neural circuit for memory specificity and generalization</article-title><source>Science</source><year>2013</year><volume>339</volume><fpage>1290</fpage><lpage>1295</lpage><pub-id pub-id-type="pmcid">PMC3651700</pub-id><pub-id pub-id-type="pmid">23493706</pub-id><pub-id pub-id-type="doi">10.1126/science.1229534</pub-id></element-citation></ref><ref id="R29"><label>[29]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aronov</surname><given-names>Dmitriy</given-names></name><name><surname>Nevers</surname><given-names>Rhino</given-names></name><name><surname>Tank</surname><given-names>David W</given-names></name></person-group><article-title>Mapping of a non-spatial dimension by the hippocampal-entorhinal circuit</article-title><source>Nature</source><year>2017</year><volume>543</volume><fpage>719</fpage><lpage>722</lpage><pub-id pub-id-type="pmcid">PMC5492514</pub-id><pub-id pub-id-type="pmid">28358077</pub-id><pub-id pub-id-type="doi">10.1038/nature21692</pub-id></element-citation></ref><ref id="R30"><label>[30]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrens</surname><given-names>Timothy EJ</given-names></name><name><surname>Muller</surname><given-names>Timothy H</given-names></name><name><surname>Whittington</surname><given-names>James CR</given-names></name><name><surname>Mark</surname><given-names>Shirley</given-names></name><name><surname>Baram</surname><given-names>Alon B</given-names></name><name><surname>Stachenfeld</surname><given-names>Kimberly L</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Zeb</given-names></name></person-group><article-title>What Is a Cognitive Map? Organizing Knowledge for Flexible Behavior</article-title><source>Neuron</source><year>2018</year><volume>100</volume><fpage>490</fpage><lpage>509</lpage><pub-id pub-id-type="pmid">30359611</pub-id></element-citation></ref><ref id="R31"><label>[31]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eichenbaum</surname><given-names>Howard</given-names></name></person-group><article-title>Time cells in the hippocampus: A new dimension for mapping memories</article-title><source>Nature Reviews Neuroscience</source><year>2014</year><volume>15</volume><fpage>732</fpage><lpage>744</lpage><pub-id pub-id-type="pmcid">PMC4348090</pub-id><pub-id pub-id-type="pmid">25269553</pub-id><pub-id pub-id-type="doi">10.1038/nrn3827</pub-id></element-citation></ref><ref id="R32"><label>[32]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Keefe</surname><given-names>John</given-names></name></person-group><article-title>Place units in the hippocampus of the freely moving rat</article-title><source>Experimental Neurology</source><year>1976</year><volume>51</volume><issue>1</issue><fpage>78</fpage><lpage>109</lpage><pub-id pub-id-type="pmid">1261644</pub-id></element-citation></ref><ref id="R33"><label>[33]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wood</surname><given-names>Emma R</given-names></name><name><surname>Dudchenko</surname><given-names>Paul A</given-names></name><name><surname>Eichenbaum</surname><given-names>Howard</given-names></name></person-group><article-title>The global record of memory in hippocampal neuronal activity</article-title><source>Nature</source><year>1999</year><volume>397</volume><fpage>613</fpage><lpage>616</lpage><pub-id pub-id-type="pmid">10050854</pub-id></element-citation></ref><ref id="R34"><label>[34]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Low</surname><given-names>Isabel Ic</given-names></name><name><surname>Giocomo</surname><given-names>Lisa M</given-names></name><name><surname>Williams</surname><given-names>Alex H</given-names></name></person-group><article-title>Remapping in a recurrent neural network model of navigation and context inference</article-title><source>eLife</source><year>2023</year><volume>12</volume><elocation-id>86943</elocation-id><pub-id pub-id-type="pmcid">PMC10328512</pub-id><pub-id pub-id-type="pmid">37410093</pub-id><pub-id pub-id-type="doi">10.7554/eLife.86943</pub-id></element-citation></ref><ref id="R35"><label>[35]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stachenfeld</surname><given-names>Kimberly L</given-names></name><name><surname>Botvinick</surname><given-names>Matthew M</given-names></name><name><surname>Gershman</surname><given-names>Samuel J</given-names></name></person-group><article-title>The hippocampus as a predictive map</article-title><source>Nature Neuroscience</source><year>2017</year><volume>20</volume><fpage>1643</fpage><lpage>1653</lpage><pub-id pub-id-type="pmid">28967910</pub-id></element-citation></ref><ref id="R36"><label>[36]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wirtshafter</surname><given-names>Hannah S</given-names></name><name><surname>Wilson</surname><given-names>Matthew A</given-names></name></person-group><article-title>Artificial intelligence insights into hippocampal processing</article-title><source>Frontiers in Computational Neuroscience</source><year>2022</year><volume>16</volume><pub-id pub-id-type="pmcid">PMC9676980</pub-id><pub-id pub-id-type="pmid">36419939</pub-id><pub-id pub-id-type="doi">10.3389/fncom.2022.1044659</pub-id></element-citation></ref><ref id="R37"><label>[37]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guise</surname><given-names>Kevin G</given-names></name><name><surname>Shapiro</surname><given-names>Matthew L</given-names></name></person-group><article-title>Medial Prefrontal Cortex Reduces Memory Interference by Modifying Hippocampal Encoding</article-title><source>Neuron</source><year>2017</year><volume>94</volume><issue>1</issue><fpage>183</fpage><lpage>192</lpage><pub-id pub-id-type="pmcid">PMC5398284</pub-id><pub-id pub-id-type="pmid">28343868</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2017.03.011</pub-id></element-citation></ref><ref id="R38"><label>[38]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meder</surname><given-names>David</given-names></name><name><surname>Kolling</surname><given-names>Nils</given-names></name><name><surname>Verhagen</surname><given-names>Lennart</given-names></name><name><surname>Wittmann</surname><given-names>Marco K</given-names></name><name><surname>Scholl</surname><given-names>Jacqueline</given-names></name><name><surname>Madsen</surname><given-names>Kristoffer H</given-names></name><name><surname>Hulme</surname><given-names>Oliver J</given-names></name><name><surname>Behrens</surname><given-names>Timothy EJ</given-names></name><name><surname>Rushworth</surname><given-names>Matthew FS</given-names></name></person-group><article-title>Simultaneous representation of a spectrum of dynamically changing value estimates during decision making</article-title><source>Nature Communications</source><year>2017</year><volume>8</volume><issue>1942</issue><pub-id pub-id-type="pmcid">PMC5717172</pub-id><pub-id pub-id-type="pmid">29208968</pub-id><pub-id pub-id-type="doi">10.1038/s41467-017-02169-w</pub-id></element-citation></ref><ref id="R39"><label>[39]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spellman</surname><given-names>Timothy</given-names></name><name><surname>Svei</surname><given-names>Malka</given-names></name><name><surname>Kaminsky</surname><given-names>Jesse</given-names></name><name><surname>Manzano-nieves</surname><given-names>Gabriela</given-names></name><name><surname>Liston</surname><given-names>Conor</given-names></name></person-group><article-title>Prefrontal deep projection neurons enable cognitive flexibility via persistent feedback monitoring</article-title><source>Cell</source><year>2021</year><volume>184</volume><issue>10</issue><fpage>2750</fpage><lpage>2766</lpage><pub-id pub-id-type="pmcid">PMC8684294</pub-id><pub-id pub-id-type="pmid">33861951</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2021.03.047</pub-id></element-citation></ref><ref id="R40"><label>[40]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Jane X</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Zeb</given-names></name><name><surname>Kumaran</surname><given-names>Dharshan</given-names></name><name><surname>Tirumala</surname><given-names>Dhruva</given-names></name><name><surname>Soyer</surname><given-names>Hubert</given-names></name><name><surname>Leibo</surname><given-names>Joel Z</given-names></name><name><surname>Hassabis</surname><given-names>Demis</given-names></name><name><surname>Botvinick</surname><given-names>Matthew</given-names></name></person-group><article-title>Prefrontal cortex as a meta-reinforcement learning system</article-title><source>Nature Neuroscience</source><year>2018</year><volume>21</volume><issue>6</issue><fpage>860</fpage><lpage>868</lpage><pub-id pub-id-type="pmid">29760527</pub-id></element-citation></ref><ref id="R41"><label>[41]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Barreto</surname><given-names>André</given-names></name><name><surname>Dabney</surname><given-names>Will</given-names></name><name><surname>Munos</surname><given-names>Rémi</given-names></name><name><surname>Hunt</surname><given-names>Jonathan J</given-names></name><name><surname>Schaul</surname><given-names>Tom</given-names></name><name><surname>Van Hasselt</surname><given-names>Hado</given-names></name><name><surname>Silver</surname><given-names>David</given-names></name></person-group><source>Successor features for transfer in reinforcement learning</source><conf-name>31st Conference on Neural Information Processing Systems</conf-name><year>2018</year></element-citation></ref><ref id="R42"><label>[42]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Madarasz</surname><given-names>Tamas J</given-names></name><name><surname>Behrens</surname><given-names>Timothy E</given-names></name></person-group><article-title>Inferred successor maps for better transfer learning</article-title><source>NeurIPS</source><year>2019</year><volume>33</volume><fpage>1</fpage><lpage>12</lpage></element-citation></ref><ref id="R43"><label>[43]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>Cameron</given-names></name><name><surname>Kirtland</surname><given-names>Aaron</given-names></name><name><surname>Tao</surname><given-names>Ruo Yu</given-names></name><name><surname>Lobel</surname><given-names>Sam</given-names></name><name><surname>Scott</surname><given-names>Daniel</given-names></name><name><surname>Petrocelli</surname><given-names>Nicholas</given-names></name><name><surname>Gottesman</surname><given-names>Omer</given-names></name><name><surname>Parr</surname><given-names>Ronald</given-names></name><name><surname>Littman</surname><given-names>Michael L</given-names></name><name><surname>Konidaris</surname><given-names>George</given-names></name></person-group><article-title>Mitigating Partial Observability in Sequential Decision Processes via the Lambda Discrepancy</article-title><source>arxiv</source><year>2024</year><day>7</day></element-citation></ref><ref id="R44"><label>[44]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hallock</surname><given-names>Henry L</given-names></name><name><surname>Wang</surname><given-names>Arick</given-names></name><name><surname>Shaw</surname><given-names>Crystal L</given-names></name><name><surname>Griffin</surname><given-names>Amy L</given-names></name></person-group><article-title>Transient Inactivation of the Thalamic Nucleus Reuniens and Rhomboid Nucleus Produces Deficits of a Working-Memory Dependent Tactile-Visual Conditional Discrimination Task</article-title><source>Behavioral Neuroscience</source><year>2013</year><day>12</day><volume>127</volume><issue>6</issue><fpage>860</fpage><lpage>866</lpage><pub-id pub-id-type="pmcid">PMC4009727</pub-id><pub-id pub-id-type="pmid">24341710</pub-id><pub-id pub-id-type="doi">10.1037/a0034653</pub-id></element-citation></ref><ref id="R45"><label>[45]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>Xinyu</given-names></name><name><surname>Hsu</surname><given-names>Ching Lung</given-names></name><name><surname>Spruston</surname><given-names>Nelson</given-names></name></person-group><article-title>Rapid synaptic plasticity contributes to a learned conjunctive code of position and choice-related information in the hippocampus</article-title><source>Neuron</source><year>2022</year><day>1</day><volume>110</volume><issue>1</issue><fpage>96</fpage><lpage>108</lpage><pub-id pub-id-type="pmid">34678146</pub-id></element-citation></ref><ref id="R46"><label>[46]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harvey</surname><given-names>Christopher D</given-names></name><name><surname>Coen</surname><given-names>Philip</given-names></name><name><surname>Tank</surname><given-names>David W</given-names></name></person-group><article-title>Choice-specific sequences in parietal cortex during a virtual-navigation decision task</article-title><source>Nature</source><year>2012</year><volume>484</volume><issue>7392</issue><fpage>62</fpage><lpage>68</lpage><pub-id pub-id-type="pmcid">PMC3321074</pub-id><pub-id pub-id-type="pmid">22419153</pub-id><pub-id pub-id-type="doi">10.1038/nature10918</pub-id></element-citation></ref><ref id="R47"><label>[47]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hallock</surname><given-names>Henry L</given-names></name><name><surname>Arreola</surname><given-names>Adrian C</given-names></name><name><surname>Shaw</surname><given-names>Crystal L</given-names></name><name><surname>Griffin</surname><given-names>Amy L</given-names></name></person-group><article-title>Dissociable roles of the dorsal striatum and dorsal hippocampus in conditional discrimination and spatial alternation T-maze tasks</article-title><source>Neurobiology of Learning and Memory</source><year>2013</year><day>2</day><volume>100</volume><fpage>108</fpage><lpage>116</lpage><pub-id pub-id-type="pmid">23261856</pub-id></element-citation></ref><ref id="R48"><label>[48]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flesch</surname><given-names>Timo</given-names></name><name><surname>Balaguer</surname><given-names>Jan</given-names></name><name><surname>Dekker</surname><given-names>Ronald</given-names></name><name><surname>Nili</surname><given-names>Hamed</given-names></name><name><surname>Summerfield</surname><given-names>Christopher</given-names></name></person-group><article-title>Comparing continual task learning in minds and machines</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2018</year><day>10</day><volume>115</volume><issue>44</issue><fpage>E10313</fpage><lpage>E10322</lpage><pub-id pub-id-type="pmcid">PMC6217400</pub-id><pub-id pub-id-type="pmid">30322916</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1800755115</pub-id></element-citation></ref><ref id="R49"><label>[49]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carvalho</surname><given-names>Paulo F</given-names></name><name><surname>Goldstone</surname><given-names>Robert L</given-names></name></person-group><article-title>Putting category learning in order: Category structure and temporal arrangement affect the benefit of interleaved over blocked study</article-title><source>Memory and Cognition</source><year>2014</year><volume>42</volume><issue>3</issue><fpage>481</fpage><lpage>495</lpage><pub-id pub-id-type="pmid">24092426</pub-id></element-citation></ref><ref id="R50"><label>[50]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carvalho</surname><given-names>Paulo F</given-names></name><name><surname>Goldstone</surname><given-names>Robert L</given-names></name></person-group><article-title>What you learn is more than what you see: What can sequencing effects tell us about inductive category learning?</article-title><source>Frontiers in Psychology</source><year>2015</year><volume>6</volume><pub-id pub-id-type="pmcid">PMC4415402</pub-id><pub-id pub-id-type="pmid">25983699</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2015.00505</pub-id></element-citation></ref><ref id="R51"><label>[51]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nieh</surname><given-names>Edward H</given-names></name><name><surname>Schottdorf</surname><given-names>Manuel</given-names></name><name><surname>Freeman</surname><given-names>Nicolas W</given-names></name><name><surname>Low</surname><given-names>Ryan J</given-names></name><name><surname>Lewallen</surname><given-names>Sam</given-names></name><name><surname>Koay</surname><given-names>Sue Ann</given-names></name><name><surname>Pinto</surname><given-names>Lucas</given-names></name><name><surname>Gauthier</surname><given-names>Jeffrey L</given-names></name><name><surname>Brody</surname><given-names>Carlos D</given-names></name><name><surname>Tank</surname><given-names>David W</given-names></name></person-group><article-title>Geometry of abstract learned knowledge in the hippocampus</article-title><source>Nature</source><year>2021</year><day>7</day><volume>595</volume><issue>7865</issue><fpage>80</fpage><lpage>84</lpage><pub-id pub-id-type="pmcid">PMC9549979</pub-id><pub-id pub-id-type="pmid">34135512</pub-id><pub-id pub-id-type="doi">10.1038/s41586-021-03652-7</pub-id></element-citation></ref><ref id="R52"><label>[52]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pinto</surname><given-names>Lucas</given-names></name><name><surname>Koay</surname><given-names>Sue A</given-names></name><name><surname>Engelhard</surname><given-names>Ben</given-names></name><name><surname>Yoon</surname><given-names>Alice M</given-names></name><name><surname>Deverett</surname><given-names>Ben</given-names></name><name><surname>Thiberge</surname><given-names>Stephan Y</given-names></name><name><surname>Witten</surname><given-names>Ilana B</given-names></name><name><surname>Tank</surname><given-names>David W</given-names></name><name><surname>Brody</surname><given-names>Carlos D</given-names></name></person-group><article-title>An accumulation-of-evidence task using visual pulses for mice navigating in virtual reality</article-title><source>Frontiers in Behavioral Neuroscience</source><year>2018</year><day>3</day><volume>12</volume><pub-id pub-id-type="pmcid">PMC5845651</pub-id><pub-id pub-id-type="pmid">29559900</pub-id><pub-id pub-id-type="doi">10.3389/fnbeh.2018.00036</pub-id></element-citation></ref><ref id="R53"><label>[53]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bolkan</surname><given-names>Scott S</given-names></name><name><surname>Stone</surname><given-names>Iris R</given-names></name><name><surname>Pinto</surname><given-names>Lucas</given-names></name><name><surname>Ashwood</surname><given-names>Zoe C</given-names></name><name><surname>Iravedra Garcia</surname><given-names>Jorge M</given-names></name><name><surname>Herman</surname><given-names>Alison L</given-names></name><name><surname>Singh</surname><given-names>Priyanka</given-names></name><name><surname>Bandi</surname><given-names>Akhil</given-names></name><name><surname>Cox</surname><given-names>Julia</given-names></name><name><surname>Zimmerman</surname><given-names>Christopher A</given-names></name><name><surname>Cho</surname><given-names>Jounhong Ryan</given-names></name><etal/></person-group><article-title>Opponent control of behavior by dorsomedial striatal pathways depends on task demands and internal state</article-title><source>Nature Neuroscience</source><year>2022</year><day>3</day><volume>25</volume><issue>3</issue><fpage>345</fpage><lpage>357</lpage><pub-id pub-id-type="pmcid">PMC8915388</pub-id><pub-id pub-id-type="pmid">35260863</pub-id><pub-id pub-id-type="doi">10.1038/s41593-022-01021-9</pub-id></element-citation></ref><ref id="R54"><label>[54]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piray</surname><given-names>Payam</given-names></name><name><surname>Daw</surname><given-names>Nathaniel D</given-names></name></person-group><article-title>Linear reinforcement learning in planning, grid fields, and cognitive control</article-title><source>Nature Communications</source><year>2021</year><day>12</day><volume>12</volume><issue>1</issue><pub-id pub-id-type="pmcid">PMC8368103</pub-id><pub-id pub-id-type="pmid">34400622</pub-id><pub-id pub-id-type="doi">10.1038/s41467-021-25123-3</pub-id></element-citation></ref><ref id="R55"><label>[55]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raju</surname><given-names>Rajkumar Vasudeva</given-names></name><name><surname>Guntupalli</surname><given-names>J Swaroop</given-names></name><name><surname>Zhou</surname><given-names>Guangyao</given-names></name><name><surname>Lázaro-Gredilla</surname><given-names>Miguel</given-names></name><name><surname>George</surname><given-names>Dileep</given-names></name></person-group><article-title>Space is a latent sequence: Structured sequence learning as a unified theory of representation in the hippocampus</article-title><source>arXiv</source><year>2022</year></element-citation></ref><ref id="R56"><label>[56]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ekman</surname><given-names>Matthias</given-names></name><name><surname>Kusch</surname><given-names>Sarah</given-names></name><name><surname>de Lange</surname><given-names>Floris P</given-names></name></person-group><article-title>Successor-like representation guides the prediction of future events in human visual cortex and hippocampus</article-title><source>eLife</source><year>2023</year><volume>12</volume><pub-id pub-id-type="pmcid">PMC9894584</pub-id><pub-id pub-id-type="pmid">36729024</pub-id><pub-id pub-id-type="doi">10.7554/eLife.78904</pub-id></element-citation></ref><ref id="R57"><label>[57]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duvelle</surname><given-names>Eléonore</given-names></name><name><surname>Grieves</surname><given-names>Roddy M</given-names></name><name><surname>van der Meer</surname><given-names>Matthijs AA</given-names></name></person-group><article-title>Temporal context and latent state inference in the hippocampal splitter signal</article-title><source>eLife</source><year>2023</year><volume>12</volume><elocation-id>e82357</elocation-id><pub-id pub-id-type="pmcid">PMC9829411</pub-id><pub-id pub-id-type="pmid">36622350</pub-id><pub-id pub-id-type="doi">10.7554/eLife.82357</pub-id></element-citation></ref><ref id="R58"><label>[58]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wood</surname><given-names>Emma R</given-names></name><name><surname>Dudchenko</surname><given-names>Paul A</given-names></name><name><surname>Robitsek</surname><given-names>R Jonathan</given-names></name><name><surname>Eichenbaum</surname><given-names>Howard</given-names></name></person-group><article-title>Hippocampal Neurons Encode Information about Different Types of Memory Episodes Occurring in the Same Location</article-title><source>Neuron</source><year>2000</year><volume>27</volume><fpage>623</fpage><lpage>633</lpage><pub-id pub-id-type="pmid">11055443</pub-id></element-citation></ref><ref id="R59"><label>[59]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grieves</surname><given-names>Roddy M</given-names></name><name><surname>Wood</surname><given-names>Emma R</given-names></name><name><surname>Dudchenko</surname><given-names>Paul A</given-names></name></person-group><article-title>Place cells on a maze encode routes rather than destinations</article-title><source>eLife</source><year>2016</year><volume>5</volume><fpage>1</fpage><lpage>24</lpage><pub-id pub-id-type="pmcid">PMC4942257</pub-id><pub-id pub-id-type="pmid">27282386</pub-id><pub-id pub-id-type="doi">10.7554/eLife.15986</pub-id></element-citation></ref><ref id="R60"><label>[60]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mishchanchuk</surname><given-names>Karyna</given-names></name><name><surname>Gregoriou</surname><given-names>Gabrielle</given-names></name><name><surname>Qü</surname><given-names>Albert</given-names></name><name><surname>Kastler</surname><given-names>Alizée</given-names></name><name><surname>Huys</surname><given-names>Quentin JM</given-names></name><name><surname>Wilbrecht</surname><given-names>Linda</given-names></name><name><surname>MacAskill</surname><given-names>Andrew F</given-names></name></person-group><article-title>Hidden state inference requires abstract contextual representations in the ventral hippocampus</article-title><source>Science</source><year>2024</year><fpage>926</fpage><lpage>932</lpage><pub-id pub-id-type="pmid">39571013</pub-id></element-citation></ref><ref id="R61"><label>[61]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cahusac</surname><given-names>PMB</given-names></name><name><surname>Miyashita</surname><given-names>Y</given-names></name><name><surname>Rolls</surname><given-names>ET</given-names></name></person-group><article-title>Responses of hippocampal formation neurons in the monkey related to delayed spatial response and object-place memory tasks</article-title><source>Behavioural Brain Research</source><year>1989</year><volume>33</volume><fpage>229</fpage><lpage>240</lpage><pub-id pub-id-type="pmid">2757782</pub-id></element-citation></ref><ref id="R62"><label>[62]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deadwyler</surname><given-names>Sam A</given-names></name><name><surname>Bunn</surname><given-names>Terence</given-names></name><name><surname>Hampson</surname><given-names>Robert E</given-names></name></person-group><article-title>Hippocampal Ensemble Activity during Spatial Delayed-Nonmatch-to-Sample Performance in Rats</article-title><source>The Journal of Neuroscience</source><year>1996</year><volume>16</volume><issue>1</issue><fpage>354</fpage><lpage>372</lpage><pub-id pub-id-type="pmcid">PMC6578714</pub-id><pub-id pub-id-type="pmid">8613802</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.16-01-00354.1996</pub-id></element-citation></ref><ref id="R63"><label>[63]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Otto</surname><given-names>Tim</given-names></name><name><surname>Eichenbaum</surname><given-names>Howard</given-names></name></person-group><article-title>Neuronal activity in the hippocampus during delayed non-match to sample performance in rats: Evidence for hippocampal processing in recognition memory</article-title><source>Hippocampus</source><year>1992</year><volume>2</volume><issue>3</issue><fpage>323</fpage><lpage>334</lpage><pub-id pub-id-type="pmid">1308191</pub-id></element-citation></ref><ref id="R64"><label>[64]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Otto</surname><given-names>Tim</given-names></name><name><surname>Eichenbaum</surname><given-names>Howard</given-names></name></person-group><article-title>Complementary Roles of the Orbital Prefrontal Cortex and the Perirhinal-Entorhinal Cortices in an Odor-Guided Delayed-Nonmatching-to-SampleTask</article-title><source>Behavioral Neuroscience</source><year>1992</year><volume>106</volume><issue>5</issue><fpage>762</fpage><lpage>775</lpage><pub-id pub-id-type="pmid">1445656</pub-id></element-citation></ref><ref id="R65"><label>[65]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sutherland</surname><given-names>RJ</given-names></name><name><surname>Rudy</surname><given-names>JW</given-names></name></person-group><article-title>Configural association theory: The role of the hippocampal formation in learning, memory, and amnesia</article-title><source>Psychobiology</source><year>1989</year><volume>17</volume><issue>2</issue><fpage>129</fpage><lpage>144</lpage></element-citation></ref><ref id="R66"><label>[66]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Good</surname><given-names>Mark</given-names></name><name><surname>Honey</surname><given-names>RC</given-names></name></person-group><article-title>Conditioning and Contextual Retrieval in Hippocampal Rats</article-title><source>Behavioral Neuroscieace</source><year>1991</year><volume>105</volume><issue>4</issue><fpage>499</fpage><lpage>509</lpage><pub-id pub-id-type="pmid">1930720</pub-id></element-citation></ref><ref id="R67"><label>[67]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aggleton</surname><given-names>John P</given-names></name><name><surname>Sanderson</surname><given-names>David J</given-names></name><name><surname>Pearce</surname><given-names>John M</given-names></name></person-group><article-title>Structural learning and the hippocampus</article-title><year>2007</year><pub-id pub-id-type="pmid">17598160</pub-id></element-citation></ref><ref id="R68"><label>[68]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanderson</surname><given-names>David J</given-names></name><name><surname>Pearce</surname><given-names>John M</given-names></name><name><surname>Kyd</surname><given-names>Rachel J</given-names></name><name><surname>Aggleton</surname><given-names>John P</given-names></name></person-group><article-title>The importance of the rat hippocampus for learning the structure of visual arrays</article-title><source>European Journal of Neuroscience</source><year>2006</year><day>9</day><volume>24</volume><issue>6</issue><fpage>1781</fpage><lpage>1788</lpage><pub-id pub-id-type="pmid">17004941</pub-id></element-citation></ref><ref id="R69"><label>[69]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>Jong Chan</given-names></name><name><surname>Bae</surname><given-names>Jung Won</given-names></name><name><surname>Kim</surname><given-names>Jieun</given-names></name><name><surname>Jung</surname><given-names>Min Whan</given-names></name></person-group><article-title>Dynamically changing neuronal activity supporting working memory for predictable and unpredictable durations</article-title><source>Scientific Reports</source><year>2019</year><volume>9</volume><pub-id pub-id-type="pmcid">PMC6820562</pub-id><pub-id pub-id-type="pmid">31664169</pub-id><pub-id pub-id-type="doi">10.1038/s41598-019-52017-8</pub-id></element-citation></ref><ref id="R70"><label>[70]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bae</surname><given-names>Jung Won</given-names></name><name><surname>Jeong</surname><given-names>Huijeong</given-names></name><name><surname>Yoon</surname><given-names>Young Ju</given-names></name><name><surname>Bae</surname><given-names>Chan Mee</given-names></name><name><surname>Lee</surname><given-names>Hyeonsu</given-names></name><name><surname>Paik</surname><given-names>Se Bum</given-names></name><name><surname>Jung</surname><given-names>Min Whan</given-names></name></person-group><article-title>Parallel processing of working memory and temporal information by distinct types of cortical projection neurons</article-title><source>Nature Communications</source><year>2021</year><volume>12</volume><issue>1</issue><pub-id pub-id-type="pmcid">PMC8285375</pub-id><pub-id pub-id-type="pmid">34272368</pub-id><pub-id pub-id-type="doi">10.1038/s41467-021-24565-z</pub-id></element-citation></ref><ref id="R71"><label>[71]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gobin</surname><given-names>Christina</given-names></name><name><surname>Wu</surname><given-names>Lizhen</given-names></name><name><surname>Schwendt</surname><given-names>Marek</given-names></name></person-group><article-title>Using rat operant delayed match-to-sample task to identify neural substrates recruited with increased working memory load</article-title><source>Learning and Memory</source><year>2020</year><volume>27</volume><issue>11</issue><fpage>467</fpage><lpage>476</lpage><pub-id pub-id-type="pmcid">PMC7571269</pub-id><pub-id pub-id-type="pmid">33060284</pub-id><pub-id pub-id-type="doi">10.1101/lm.052134.120</pub-id></element-citation></ref><ref id="R72"><label>[72]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sloan</surname><given-names>Hazel L</given-names></name><name><surname>Döbrössy</surname><given-names>Màtè</given-names></name><name><surname>Dunnett</surname><given-names>Stephen B</given-names></name></person-group><article-title>Hippocampal lesions impair performance on a conditional delayed matching and non-matching to position task in the rat</article-title><source>Behavioural Brain Research</source><year>2006</year><volume>171</volume><issue>2</issue><fpage>240</fpage><lpage>250</lpage><pub-id pub-id-type="pmid">16697059</pub-id></element-citation></ref><ref id="R73"><label>[73]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>Robert E</given-names></name><name><surname>West</surname><given-names>Alisha N</given-names></name><name><surname>Zola</surname><given-names>Stuart M</given-names></name><name><surname>Squire</surname><given-names>Larry R</given-names></name></person-group><article-title>Rats with lesions of the hippocampus are impaired on the delayed nonmatching-to-sample task</article-title><source>Hippocampus</source><year>2001</year><volume>11</volume><issue>2</issue><fpage>176</fpage><lpage>186</lpage><pub-id pub-id-type="pmid">11345124</pub-id></element-citation></ref><ref id="R74"><label>[74]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McNamee</surname><given-names>Daniel C</given-names></name><name><surname>Stachenfeld</surname><given-names>Kimberly L</given-names></name><name><surname>Botvinick</surname><given-names>Matthew M</given-names></name><name><surname>Gershman</surname><given-names>Samuel J</given-names></name></person-group><article-title>Flexible modulation of sequence generation in the entorhinal–hippocampal system</article-title><source>Nature Neuroscience</source><year>2021</year><volume>24</volume><fpage>851</fpage><lpage>862</lpage><pub-id pub-id-type="pmcid">PMC7610914</pub-id><pub-id pub-id-type="pmid">33846626</pub-id><pub-id pub-id-type="doi">10.1038/s41593-021-00831-7</pub-id></element-citation></ref><ref id="R75"><label>[75]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Machado</surname><given-names>Marlos C</given-names></name><name><surname>Bellemare</surname><given-names>Marc G</given-names></name><name><surname>Bowling</surname><given-names>Michael</given-names></name></person-group><article-title>Count-Based Exploration with the Successor Representation</article-title><source>arXiv</source><year>2019</year></element-citation></ref><ref id="R76"><label>[76]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Evans</surname><given-names>Talfan</given-names></name><name><surname>Pathak</surname><given-names>Shreya</given-names></name><name><surname>Merzic</surname><given-names>Hamza</given-names></name><name><surname>Schwarz</surname><given-names>Jonathan</given-names></name><name><surname>Tanno</surname><given-names>Ryutaro</given-names></name><name><surname>Henaff</surname><given-names>Olivier J</given-names></name></person-group><article-title>Bad Students Make Great Teachers: Active Learning Accelerates Large-Scale Visual Understanding</article-title><source>arXiv</source><year>2024</year></element-citation></ref><ref id="R77"><label>[77]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jensen</surname><given-names>Kristopher T</given-names></name><name><surname>Hennequin</surname><given-names>Guillaume</given-names></name><name><surname>Mattar</surname><given-names>Marcelo G</given-names></name></person-group><article-title>A recurrent network model of planning explains hippocampal replay and human behavior</article-title><source>Nature Neuroscience</source><year>2024</year><day>7</day><volume>27</volume><issue>7</issue><fpage>1340</fpage><lpage>1348</lpage><pub-id pub-id-type="pmcid">PMC11239510</pub-id><pub-id pub-id-type="pmid">38849521</pub-id><pub-id pub-id-type="doi">10.1038/s41593-024-01675-7</pub-id></element-citation></ref><ref id="R78"><label>[78]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kurth-Nelson</surname><given-names>Zeb</given-names></name><name><surname>Behrens</surname><given-names>Timothy</given-names></name><name><surname>Wayne</surname><given-names>Greg</given-names></name><name><surname>Miller</surname><given-names>Kevin</given-names></name><name><surname>Luettgau</surname><given-names>Lennart</given-names></name><name><surname>Dolan</surname><given-names>Ray</given-names></name><name><surname>Liu</surname><given-names>Yunzhe</given-names></name><name><surname>Schwartenbeck</surname><given-names>Philipp</given-names></name></person-group><source>Replay and compositional computation</source><year>2023</year><day>2</day><pub-id pub-id-type="pmid">36640765</pub-id></element-citation></ref><ref id="R79"><label>[79]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Momennejad</surname><given-names>I</given-names></name><name><surname>Russek</surname><given-names>EM</given-names></name><name><surname>Cheong</surname><given-names>JH</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><article-title>The successor representation in human reinforcement learning</article-title><source>Nature Human Behaviour</source><year>2017</year><volume>1</volume><issue>9</issue><fpage>680</fpage><lpage>692</lpage><pub-id pub-id-type="pmcid">PMC6941356</pub-id><pub-id pub-id-type="pmid">31024137</pub-id><pub-id pub-id-type="doi">10.1038/s41562-017-0180-8</pub-id></element-citation></ref><ref id="R80"><label>[80]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reuschenbach</surname><given-names>Josefine</given-names></name><name><surname>Reinert</surname><given-names>Janine K</given-names></name><name><surname>Fu</surname><given-names>Xiaochen</given-names></name><name><surname>Fukunaga</surname><given-names>Izumi</given-names></name></person-group><article-title>Effects of Stimulus Timing on the Acquisition of an Olfactory Working Memory Task in Head-Fixed Mice</article-title><source>Journal of Neuroscience</source><year>2023</year><volume>43</volume><issue>17</issue><fpage>3120</fpage><lpage>3130</lpage><pub-id pub-id-type="pmcid">PMC10146466</pub-id><pub-id pub-id-type="pmid">36927573</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1636-22.2023</pub-id></element-citation></ref><ref id="R81"><label>[81]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Xiaoxing</given-names></name><name><surname>Yan</surname><given-names>Wenjun</given-names></name><name><surname>Wang</surname><given-names>Wenliang</given-names></name><name><surname>Fan</surname><given-names>Hongmei</given-names></name><name><surname>Hou</surname><given-names>Ruiqing</given-names></name><name><surname>Chen</surname><given-names>Yulei</given-names></name><name><surname>Chen</surname><given-names>Zhaoqin</given-names></name><name><surname>Ge</surname><given-names>Chaofan</given-names></name><name><surname>Duan</surname><given-names>Shumin</given-names></name><name><surname>Compte</surname><given-names>Albert</given-names></name><name><surname>Li</surname><given-names>Chengyu T</given-names></name></person-group><article-title>Active information maintenance in working memory by a sensory cortex</article-title><source>eLife</source><year>2019</year><volume>8</volume><pub-id pub-id-type="pmcid">PMC6634975</pub-id><pub-id pub-id-type="pmid">31232695</pub-id><pub-id pub-id-type="doi">10.7554/eLife.43191</pub-id></element-citation></ref><ref id="R82"><label>[82]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gmaz</surname><given-names>Jimmie M</given-names></name><name><surname>Van Der Meer</surname><given-names>Matthijs AA</given-names></name></person-group><article-title>Context coding in the mouse nucleus accumbens modulates motivationally relevant information</article-title><source>PLoS Biology</source><year>2022</year><volume>20</volume><pub-id pub-id-type="pmcid">PMC9094556</pub-id><pub-id pub-id-type="pmid">35486662</pub-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3001338</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Design of feature and outcome inference models.</title><p>(A) Setup of the cued T-maze task, showing different location and cue-based features, (B) Algorithm underlying feature inference showing how observed feature transitions <italic>ϕ</italic><sub><italic>t</italic></sub> to <italic>ϕ</italic><sub><italic>t</italic>+1</sub> are compared with learnt successor feature maps <italic>M</italic> using Bayesian inference to determine which context <italic>z</italic><sup>*</sup> is currently most likely, followed by using the corresponding temporal difference map <italic>T D</italic> to choose the current best action <italic>a</italic><sub><italic>t</italic></sub>, (C) Algorithm underlying outcome inference showing how observed outcomes <italic>cr</italic><sub><italic>t</italic></sub> are compared with learnt convolved reward maps <italic>C</italic> using Bayesian inference to determine which context is currently most likely, (D) Schematic of how the feature and outcome inference models react to a change in trial type, showing selection of the relevant map for feature inference following the cue and for outcome inference following the lack of reward, and how the inferred context allows for action selection using separate temporal difference maps.</p></caption><graphic xlink:href="EMS206053-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Feature and outcome inference learn distinct strategies to solve a cued T-maze.</title><p>(A) Setup of the cued T-maze, showing distinct cues and reward locations for distinct trial types, (B) Training and test setup for the paradigm, showing trial identity for 1000 trials during block switches followed by 500 random trials, (C) Performance of an example feature inference agent on the last 10 block switches (left) and last 100 random trials (right) used to quantify their performance, (D) Performance of feature inference and outcome inference in comparison to other RL agents on block switches (left) and random trials (right), (E) Trial type specific SRs learnt by the feature inference algorithm showing distinct predicted future occupancy when the agent is in the starting state, averaged over all agents on the last 100 random trials (log-scale). Predicted future occupancy of locations is indicated within the T-maze and predicted future occupancy of cue 1 associated with trial type L is indicated to the left of the location it occurs in and cue 2 associated with trial type R is indicated to the right of the location it occurs in.</p></caption><graphic xlink:href="EMS206053-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Increasing overlap between contexts reduces performance of feature inference, but not outcome inference.</title><p>(A) Decreasing the contextual SNR by adding multiple overlapping features around the cue in each context or (D) extending the distance between the cue and the choice point, (B, E) Performance of feature and outcome inference as contextual overlap increases on block switches and (C, F) random trials</p></caption><graphic xlink:href="EMS206053-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Supporting feature inference with outcome inference during learning rescues performance with increasing contextual overlap.</title><p>(A) Schematic of joint algorithm showing that during learning outcome inference is used to generate a joint context estimate, (B) Algorithmic implementation showing how the joint estimate is determined during learning, (C) Performance of joint inference on random trials as contextual overlap increases via distractor features around the cue or (D) increasing cue-choice distance, (E) Performance of joint inference on block switches as contextual overlap increases via distractor features around the cue or (F) increasing cue-choice distance.</p><p><xref ref-type="supplementary-material" rid="SD1">Figure 4—figure supplement 1</xref>. Inconsistent performance of algorithms using other ways of combining outcome inference and feature inference during learning.</p></caption><graphic xlink:href="EMS206053-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Supporting learning with outcome inference improves initial formation of context-dependent representations leading to long-lasting improvement in performance.</title><p>(A) Number of incorrect updates of the context map during the first trial type switch, (B) Evolution of the average context map’s predicted future occupancy when the agent is in the starting state (log scale), at the end of the first block and the end of the second block of trials for joint and feature inference agents, (C) Underrepresentation of the correct cue (left) and overrepresentation of the incorrect cue (right) on the average context maps used on random trials. (e) Confidence in inferred context identity following the cue on random trials.</p><p><xref ref-type="supplementary-material" rid="SD1">Figure 5—figure supplement 1</xref>. Inconsistent performance of algorithms using methods other than outcome-inference to support learning.</p></caption><graphic xlink:href="EMS206053-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><title>Removing support from outcome inference during learning mimics experimentally observed splitter cell loss.</title><p>(A) Simulated cell firing of a cell representing future predicted occupancy of states along the overlapping central arm on correct random trials across all agents for cue-choice distance 20, (B) Quantification of simulated cell firing along the central arm in its preferred and non-preferred contexts, (C) Evolution of the difference in splitter probabilities along the central arm, (D) Impact of increasing cue-choice distance on the difference in splitter probabilities (left) and correlation between performance on random trials and the difference in splitter probabilities (right). (E) Same as (D) but for distractor features.</p></caption><graphic xlink:href="EMS206053-f006"/></fig><fig id="F7" position="float"><label>Figure 7</label><caption><title>Supporting feature inference with outcome inference during learning improves performance on cue-discrimination tasks.</title><p>(A) Schematic of the non-match to sample task, where trials with the same cue repeated twice require a different response than trials where two differing cues are presented (left) and training protocol showing that outcome inference is used to support feature inference during the training phase (right), (B) Performance of feature inference and joint inference on the last 10 block switches, (C) Performance of feature inference and joint inference on random trials, (D) Schematic of the biconditional discrimination task, where the meaning of the second cue depends on the identity of the first cue, (E) Performance of feature inference and joint inference on the last 10 block switches, (F) Performance of feature inference and joint inference on random trials.</p></caption><graphic xlink:href="EMS206053-f007"/></fig><fig id="F8" position="float"><label>Figure 8</label><caption><title>Schematics of model designs.</title><p>(A) Schematic of HPC model, showing how observed feature transitions are compared with existing successor feature maps using Bayesian inference to determine which context is currently most likely. Schematic of PFC model, (B) showing how observed convolved reward is compared with existing maps of convolved reward using Bayesian inference to determine which context is currently most likely, (C) Schematic of joint model architecture, showing how the context likelihoods generated by both HPC and PFC models are combined for contextual inference and action selection.</p></caption><graphic xlink:href="EMS206053-f008"/></fig><table-wrap id="T1" orientation="portrait" position="float"><label>Table 1</label><caption><title>Parameters across all models</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" align="left" style="border-top:solid 1px #000000">Parameter</th><th valign="top" align="left" style="border-top:solid 1px #000000">Value</th><th valign="top" align="left" style="border-top:solid 1px #000000">Description</th></tr></thead><tbody><tr><td valign="top" align="left" style="border-top: 1px solid #000000"><italic>ϵ</italic></td><td valign="top" align="left" style="border-top: 1px solid #000000">0.2</td><td valign="top" align="left" style="border-top: 1px solid #000000">Explore-exploit ratio</td></tr><tr><td valign="top" align="left"><italic>γ</italic></td><td valign="top" align="left">0.9</td><td valign="top" align="left">Discount factor</td></tr><tr><td valign="top" align="left"><inline-formula><mml:math id="M11"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula></td><td valign="top" align="left">1</td><td valign="top" align="left">Outcome noise variance</td></tr><tr><td valign="top" align="left"><italic>p</italic>(<italic>z</italic><sub>0</sub>)</td><td valign="top" align="left">[1,0.4,0.1,0,0,0,0,0,0,0]</td><td valign="top" align="left">Initial context probabilities</td></tr><tr><td valign="top" align="left"><italic>P</italic><sub>0</sub></td><td valign="top" align="left"><italic>multinomial</italic>(<italic>p</italic>(<italic>z</italic><sub>0</sub>))</td><td valign="top" align="left">Initial particles</td></tr><tr><td valign="top" align="left"><italic>n</italic></td><td valign="top" align="left">100</td><td valign="top" align="left">Number of particles</td></tr><tr><td valign="top" align="left"><italic>y</italic></td><td valign="top" align="left">10</td><td valign="top" align="left">Particle context history</td></tr><tr><td valign="top" align="left"><italic>z</italic><sub><italic>max</italic></sub></td><td valign="top" align="left">10</td><td valign="top" align="left">Maximum task number</td></tr><tr><td valign="top" align="left" style="border-bottom:solid 1px #000000"><italic>α</italic><sub><italic>r</italic></sub></td><td valign="top" align="left" style="border-bottom:solid 1px #000000">0.7</td><td valign="top" align="left" style="border-bottom:solid 1px #000000">Reward learning rate</td></tr></tbody></table></table-wrap><table-wrap id="T2" orientation="portrait" position="float"><label>Table 2</label><caption><title>Parameters for feature inference</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" align="left" style="border-top:solid 1px #000000">Parameter</th><th valign="top" align="left" style="border-top:solid 1px #000000">Value</th><th valign="top" align="left" style="border-top:solid 1px #000000">Description</th></tr></thead><tbody><tr><td valign="top" align="left" style="border-top: 1px solid #000000"><italic>α</italic></td><td valign="top" align="left" style="border-top: 1px solid #000000">0.5</td><td valign="top" align="left" style="border-top: 1px solid #000000">Concentration</td></tr><tr><td valign="top" align="left"><italic>β</italic></td><td valign="top" align="left">5</td><td valign="top" align="left">Stickiness</td></tr><tr><td valign="top" align="left"><inline-formula><mml:math id="M12"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula></td><td valign="top" align="left">1.6</td><td valign="top" align="left">Initial residual variance</td></tr><tr><td valign="top" align="left" style="border-bottom: 1px solid"><inline-formula><mml:math id="M13"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula></td><td valign="top" align="left" style="border-bottom: 1px solid">1.6</td><td valign="top" align="left" style="border-bottom: 1px solid">Residual variance for inference</td></tr></tbody></table></table-wrap><table-wrap id="T3" orientation="portrait" position="float"><label>Table 3</label><caption><title>Parameters for feature inference in biconditional discrimination</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" align="left" style="border-top:solid 1px #000000">Parameter</th><th valign="top" align="left" style="border-top:solid 1px #000000">Value</th><th valign="top" align="left" style="border-top:solid 1px #000000">Description</th></tr></thead><tbody><tr><td valign="top" align="left" style="border-top: 1px solid #000000"><italic>α</italic></td><td valign="top" align="left" style="border-top: 1px solid #000000">0.5</td><td valign="top" align="left" style="border-top: 1px solid #000000">Concentration</td></tr><tr><td valign="top" align="left"><italic>β</italic></td><td valign="top" align="left">8</td><td valign="top" align="left">Stickiness</td></tr><tr><td valign="top" align="left"><inline-formula><mml:math id="M14"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula></td><td valign="top" align="left">1.6</td><td valign="top" align="left">Initial residual variance</td></tr><tr><td valign="top" align="left" style="border-bottom: 1px solid"><inline-formula><mml:math id="M15"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula></td><td valign="top" align="left" style="border-bottom: 1px solid">1.6</td><td valign="top" align="left" style="border-bottom: 1px solid">Residual variance for inference</td></tr></tbody></table></table-wrap><table-wrap id="T4" orientation="portrait" position="float"><label>Table 4</label><caption><title>Parameters for outcome inference</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" align="left" style="border-top:solid 1px #000000">Parameter</th><th valign="top" align="left" style="border-top:solid 1px #000000">Value</th><th valign="top" align="left" style="border-top:solid 1px #000000">Description</th></tr></thead><tbody><tr><td valign="top" align="left" style="border-top: 1px solid #000000"><italic>α</italic></td><td valign="top" align="left" style="border-top: 1px solid #000000">0.1</td><td valign="top" align="left" style="border-top: 1px solid #000000">Concentration</td></tr><tr><td valign="top" align="left"><italic>β</italic></td><td valign="top" align="left">0.7</td><td valign="top" align="left">Stickiness</td></tr><tr><td valign="top" align="left"><inline-formula><mml:math id="M16"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula></td><td valign="top" align="left">1.6</td><td valign="top" align="left">Initial residual variance</td></tr><tr><td valign="top" align="left"><italic>α<sub>o</sub></italic></td><td valign="top" align="left"><inline-formula><mml:math id="M17"><mml:mrow><mml:mfrac><mml:mn>2</mml:mn><mml:mn>3</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula></td><td valign="top" align="left">Learning rate weighting</td></tr><tr><td valign="top" align="left" style="border-bottom: 1px solid"><italic>len</italic></td><td valign="top" align="left" style="border-bottom: 1px solid">2</td><td valign="top" align="left" style="border-bottom: 1px solid">Filter length</td></tr></tbody></table></table-wrap><table-wrap id="T5" orientation="portrait" position="float"><label>Table 5</label><caption><title>Parameters for outcome inference in biconditional discrimination</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" align="left" style="border-top:solid 1px #000000">Parameter</th><th valign="top" align="left" style="border-top:solid 1px #000000">Value</th><th valign="top" align="left" style="border-top:solid 1px #000000">Description</th></tr></thead><tbody><tr><td valign="top" align="left" style="border-top: 1px solid #000000"><italic>α</italic></td><td valign="top" align="left" style="border-top: 1px solid #000000">0.8</td><td valign="top" align="left" style="border-top: 1px solid #000000">Concentration</td></tr><tr><td valign="top" align="left"><italic>β</italic></td><td valign="top" align="left">0.7</td><td valign="top" align="left">Stickiness</td></tr><tr><td valign="top" align="left"><inline-formula><mml:math id="M18"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula></td><td valign="top" align="left">1.6</td><td valign="top" align="left">Initial residual variance</td></tr><tr><td valign="top" align="left"><italic>α<sub>o</sub></italic></td><td valign="top" align="left"><inline-formula><mml:math id="M19"><mml:mrow><mml:mfrac><mml:mn>2</mml:mn><mml:mn>3</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula></td><td valign="top" align="left">Learning rate weighting</td></tr><tr><td valign="top" align="left" style="border-bottom: 1px solid"><italic>len</italic></td><td valign="top" align="left" style="border-bottom: 1px solid">2</td><td valign="top" align="left" style="border-bottom: 1px solid">Filter length</td></tr></tbody></table></table-wrap><table-wrap id="T6" orientation="portrait" position="float"><label>Table 6</label><caption><title>Statistical results</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" align="left" style="border-top: 1px solid">Figure</th><th valign="top" align="left" style="border-top: 1px solid">Test</th><th valign="top" align="left" style="border-top: 1px solid">Result</th><th valign="top" align="left" style="border-top:solid 1px #000000">Post-hoc test</th><th valign="top" align="left" style="border-top: 1px solid">Result</th></tr></thead><tbody><tr><td valign="top" align="left" style="border-top: 1px solid">2D left</td><td valign="top" align="left" style="border-top: 1px solid">Welch<break/>ANOVA</td><td valign="top" align="left" style="border-top: 1px solid"><bold><italic>F</italic></bold><sub>(4,92)</sub> = 104, p&lt; 0.001</td><td valign="top" align="left" style="border-top: 1px solid">Games-Howell</td><td valign="top" align="left" style="border-top:solid 1px #000000">FI-OI: <italic>t</italic><sub>(68)</sub>=-17, p &lt; 0.001;FI-SR1: <italic>t</italic><sub>(77)</sub> = -14, p&lt; 0.001;FI-SR: <italic>t</italic><sub>(39)</sub> = -9, p &lt; 0.001;OI-SR: <italic>t</italic><sub>(39)</sub> = -10, p &lt; 0.001</td></tr><tr><td valign="top" align="left">2D right</td><td valign="top" align="left">ANOVA</td><td valign="top" align="left"><bold><italic>F</italic></bold><sub>(4,195)</sub> = 330, p &lt; 0.001</td><td valign="top" align="left">Tukey</td><td valign="top" align="left">FI-OI <italic>t</italic><sub>(39)</sub> = 18, p &lt; 0.001;FI-SR <italic>t</italic><sub>(39)</sub> = 18, p &lt; 0.001;FI-SR1 <italic>t</italic><sub>(39)</sub> = 18, p &lt; 0.001;FI-TD <italic>t</italic><sub>(39)</sub> = 17, p &lt; 0.001</td></tr><tr><td valign="top" align="left" rowspan="2">3B</td><td valign="top" align="left" rowspan="2">Mixed<break/>ANOVA</td><td valign="top" align="left" rowspan="2">interaction <bold><italic>F</italic></bold><sub>(7,546)</sub> = 11, p &lt; 0.001 algorithm <bold><italic>F</italic></bold><sub>(1,78)</sub> = 13, p &lt; 0.001</td><td valign="top" align="left">post-hoc t-tests with BHF</td><td valign="top" align="left">For interaction: p &lt; 0.05 for all except distractors = 10</td></tr><tr><td valign="top" align="left">Linear regression</td><td valign="top" align="left">FI interaction distractors*attempts: <bold><italic>R</italic></bold><italic><sup>2</sup></italic> = 0.15, slope = 0.3, x-intercept = 0.6, <italic>p</italic><sub>(318)</sub> &lt; 0.001</td></tr><tr><td valign="top" align="left" rowspan="2">3E</td><td valign="top" align="left" rowspan="2">Mixed<break/>ANOVA</td><td valign="top" align="left" rowspan="2">interaction <bold><italic>F</italic></bold><sub>(7,546)</sub> = 5, p &lt; 0.001 algorithm <bold><italic>F</italic></bold><sub>(1,78)</sub> = 15, p &lt; 0.001</td><td valign="top" align="left">post-hoc t-tests BHF</td><td valign="top" align="left">For interaction: p &lt; 0.05 for all except distance = 10</td></tr><tr><td valign="top" align="left">Linear regression</td><td valign="top" align="left">FI interaction cue-choice distance*attempts, <bold><italic>R</italic></bold><sup>2</sup> = 0.05, slope = 0.4, x-intercept = 0.02, <italic>p</italic><sub>(318)</sub> &lt; 0.001</td></tr><tr><td valign="top" align="left" rowspan="2">3C</td><td valign="top" align="left" rowspan="2">Mixed<break/>ANOVA</td><td valign="top" align="left" rowspan="2">interaction <bold><italic>F</italic></bold><sub>(7,546)</sub> = 55, p &lt; 0.001 algorithm <bold><italic>F</italic></bold><sub>(1,78)</sub> = 309, p &lt; 0.001</td><td valign="top" align="left">post-hoc t-tests with BHF</td><td valign="top" align="left">For interaction: p &lt; 0.05 for all except distractors = 20</td></tr><tr><td valign="top" align="left">Linear regression</td><td valign="top" align="left">FI interaction distractors*performance, <bold><italic>R</italic></bold><italic><sup>2</sup></italic> = 0.5, slope = -2, x-intercept = 89, <italic>p</italic><sub>(318)</sub>&lt; 0.001</td></tr><tr><td valign="top" align="left" rowspan="2">3F</td><td valign="top" align="left" rowspan="2">Mixed<break/>ANOVA</td><td valign="top" align="left" rowspan="2">interaction <bold><italic>F</italic></bold><sub>(7,546)</sub> = 42, p &lt; 0.001, algorithm <bold><italic>F</italic></bold><sub>(1,78)</sub> = 524, p &lt; 0.001</td><td valign="top" align="left">post-hoc t-tests BHF</td><td valign="top" align="left">Interaction: p &lt; 0.05 for all</td></tr><tr><td valign="top" align="left">Linear regression</td><td valign="top" align="left">FI interaction cue-choice distance*attempts, <bold><italic>R</italic></bold><sup>2</sup> = 0.4, slope = -1, x-intercept = 90, <italic>p</italic><sub>(318)</sub> &lt; 0.001</td></tr><tr><td valign="top" align="left">4C</td><td valign="top" align="left">Mixed<break/>ANOVA</td><td valign="top" align="left">interaction <bold><italic>F</italic></bold><sub>(14,819)</sub> = 47, p &lt; 0.001, algorithm <bold><italic>F</italic></bold><sub>(2,117)</sub> = 544, p &lt; 0.001</td><td valign="top" align="left">post-hoc t-tests with BHF</td><td valign="top" align="left">Algorithms: p &lt; 0.05 for all</td></tr><tr><td valign="top" align="left">4D</td><td valign="top" align="left">Mixed<break/>ANOVA</td><td valign="top" align="left">interaction <bold><italic>F</italic></bold><sub>(14,819)</sub> = 30, p &lt; 0.001, algorithm <bold><italic>F</italic></bold><sub>(2,117)</sub> = 711, p &lt; 0.001</td><td valign="top" align="left">post-hoc t-tests BHF</td><td valign="top" align="left">Algorithms: p &lt; 0.05 for all</td></tr><tr><td valign="top" align="left">4E</td><td valign="top" align="left">Mixed<break/>ANOVA</td><td valign="top" align="left">interaction <bold><italic>F</italic></bold><sub>(14,819)</sub> = 10, p &lt; 0.001, algorithm <bold><italic>F</italic></bold><sub>(2,117)</sub> = 47, p &lt; 0.001</td><td valign="top" align="left">post-hoc t-tests with BHF</td><td valign="top" align="left">Algorithms: p &lt; 0.05 for all</td></tr><tr><td valign="top" align="left">4F</td><td valign="top" align="left">Mixed<break/>ANOVA</td><td valign="top" align="left">interaction <bold><italic>F</italic></bold><sub>(14,819)</sub> = 5, p &lt; 0.001, algorithm <bold><italic>F</italic></bold><sub>(2,117)</sub> =23, p &lt; 0.001</td><td valign="top" align="left">post-hoc t-tests BHF</td><td valign="top" align="left">Algorithms: p &lt; 0.05 for all</td></tr><tr><td valign="top" align="left">5A top</td><td valign="top" align="left">Mixed<break/>ANOVA</td><td valign="top" align="left">interaction distractors*algorithm: <bold><italic>F</italic></bold><sub>(14,819</sub>) = 58, p &lt; 0.001</td><td valign="top" align="left"/><td valign="top" align="left"/></tr><tr><td valign="top" align="left">5A bottom</td><td valign="top" align="left">Mixed<break/>ANOVA</td><td valign="top" align="left">interaction distance*algorithm: <bold><italic>F</italic></bold><sub>(14,819)</sub> = 4, p &lt; 0.001</td><td valign="top" align="left"/><td valign="top" align="left"/></tr><tr><td valign="top" align="left">5C top</td><td valign="top" align="left">Mixed<break/>ANOVA</td><td valign="top" align="left">distractors*algorithm: correct cue <bold><italic>F</italic></bold>(<sub>7,546</sub>) = 57, p &lt; 0.001, incorrect cue <bold><italic>F</italic></bold>(<sub>7,546</sub>) = 47, p &lt; 0.001</td><td valign="top" align="left"/><td valign="top" align="left"/></tr><tr><td valign="top" align="left">5C bottom</td><td valign="top" align="left">Mixed<break/>ANOVA</td><td valign="top" align="left">interaction distance*algorithm: correct cue <bold><italic>F</italic></bold>(<sub>7,546</sub>) = 9, p &lt; 0.001, incorrect cue <bold><italic>F</italic></bold>(<sub>7,546</sub>) = 6, p &lt; 0.001</td><td valign="top" align="left"/><td valign="top" align="left"/></tr><tr><td valign="top" align="left">5D top<break/>5D bottom</td><td valign="top" align="left">t-test<break/>Mixed<break/>ANOVA</td><td valign="top" align="left"><italic>t</italic><sub>(40)</sub> = -12, p &lt; 0.001 interaction distance*algorithm: <bold><italic>F</italic></bold><sub>(18,1224)</sub> = 7, p &lt; 0.001</td><td valign="top" align="left"/><td valign="top" align="left"/></tr><tr><td valign="top" align="left">Supplement 5A top</td><td valign="top" align="left">Mixed<break/>ANOVA</td><td valign="top" align="left">algorithm <bold><italic>F</italic></bold><sub>(2,117)</sub> = 204, p &lt; 0.001</td><td valign="top" align="left">post-hoc t-tests with BHF</td><td valign="top" align="left">Algorithms: p &lt; 0.05 for explore-joint</td></tr><tr><td valign="top" align="left">Supplement<break/>5A bottom</td><td valign="top" align="left">Mixed<break/>ANOVA</td><td valign="top" align="left">algorithm <bold><italic>F</italic></bold><sub>(2,117)</sub> = 90, p &lt; 0.001</td><td valign="top" align="left">post-hoc t-tests with BHF</td><td valign="top" align="left">Algorithms: p &lt; 0.05 for all</td></tr><tr><td valign="top" align="left">Supplement 5B top</td><td valign="top" align="left">Mixed<break/>ANOVA</td><td valign="top" align="left">algorithm <bold><italic>F</italic></bold><sub>(2,117)</sub> = 63, p &lt; 0.001</td><td valign="top" align="left">post-hoc t-tests with BHF</td><td valign="top" align="left">Algorithms: p &lt; 0.05 for all</td></tr><tr><td valign="top" align="left">Supplement 5C top</td><td valign="top" align="left">Mixed<break/>ANOVA</td><td valign="top" align="left">algorithm <bold><italic>F</italic></bold><sub>(2,117)</sub> = 97, p &lt; 0.001</td><td valign="top" align="left">post-hoc t-tests with BHF</td><td valign="top" align="left">Algorithms: p &lt; 0.05 for all</td></tr><tr><td valign="top" align="left">Supplement<break/>5C bottom</td><td valign="top" align="left">Mixed<break/>ANOVA</td><td valign="top" align="left">algorithm <bold><italic>F</italic></bold><sub>(2,117)</sub> = 72, p &lt; 0.001</td><td valign="top" align="left">post-hoc t-tests with BHF</td><td valign="top" align="left">Algorithms: p &lt; 0.05 for all</td></tr><tr><td valign="top" align="left">6B left</td><td valign="top" align="left">Paired<break/>t-test</td><td valign="top" align="left">area under the curve of simulated firing rate on preferred vs non-preferred trials, <italic>t</italic><sub>(39)</sub> = 25, p &lt;0.001</td><td valign="top" align="left"/><td valign="top" align="left"/></tr><tr><td valign="top" align="left">6B right</td><td valign="top" align="left">Paired<break/>t-test</td><td valign="top" align="left">area under the curve of simulated firing rate on preferred vs non-preferred trials, <italic>t</italic><sub>(39)</sub> = 8, p &lt; 0.001</td><td valign="top" align="left"/><td valign="top" align="left"/></tr><tr><td valign="top" align="left">6B</td><td valign="top" align="left">Paired<break/>t-test</td><td valign="top" align="left">Joint vs feature: difference in area under the curve of simulated firing rate on preferred vs non-preferred trials, <italic>t</italic><sub>(78)</sub> = 7, p &lt; 0.001</td><td valign="top" align="left"/><td valign="top" align="left"/></tr><tr><td valign="top" align="left">6C</td><td valign="top" align="left">Mixed<break/>ANOVA</td><td valign="top" align="left">interaction distance from cue*difference in splitter probabilities: <bold><italic>F</italic></bold><sub>(17,1326)</sub> = 8, p &lt; 0.001</td><td valign="top" align="left"/><td valign="top" align="left"/></tr><tr><td valign="top" align="left">6D left</td><td valign="top" align="left">Mixed<break/>ANOVA</td><td valign="top" align="left">interaction difference in splitter probabilities*algorithm: <bold><italic>F</italic></bold><sub>(7,546)</sub> = 61, p &lt; 0.001</td><td valign="top" align="left"/><td valign="top" align="left"/></tr><tr><td valign="top" align="left">6D right</td><td valign="top" align="left">Linear regression</td><td valign="top" align="left">interaction performance on random trials*difference in splitter probabilities, <bold><italic>R</italic></bold><italic><sup>2</sup></italic> = 0.78, slope = 0.01, x-intercept = -0.5, <italic>p</italic><sub>(318)</sub> &lt; 0.001</td><td valign="top" align="left"/><td valign="top" align="left"/></tr><tr><td valign="top" align="left">6E left</td><td valign="top" align="left">Mixed<break/>ANOVA</td><td valign="top" align="left">interaction difference in splitter probabilities*algorithm: <bold><italic>F</italic></bold><sub>(7,546)</sub> = 22, p &lt; 0.001</td><td valign="top" align="left"/><td valign="top" align="left"/></tr><tr><td valign="top" align="left">6E right</td><td valign="top" align="left">Linear regression</td><td valign="top" align="left">interaction performance on random trials*difference in splitter probabilities, <bold><italic>R</italic></bold><sup>2</sup> = 0.25, slope = 0.006, x-intercept = 0.13, <italic>p</italic><sub>(318)</sub> &lt; 0.001</td><td valign="top" align="left"/><td valign="top" align="left"/></tr><tr><td valign="top" align="left">7B</td><td valign="top" align="left">T-test</td><td valign="top" align="left">t<sub>(78)</sub> = 5, p &lt; 0.001</td><td valign="top" align="left"/><td valign="top" align="left"/></tr><tr><td valign="top" align="left">7C</td><td valign="top" align="left">Mixed<break/>ANOVA</td><td valign="top" align="left">algorithm <italic>F</italic><sub>(1,78)</sub> = 10, p = 0.002; interaction <italic>F</italic><sub>(4,312)</sub> = 4, p = 0.006</td><td valign="top" align="left">post-hoc t-tests with BHF</td><td valign="top" align="left">Interaction: p &lt; 0.05 for 0-100 and 100-200</td></tr><tr><td valign="top" align="left">7E</td><td valign="top" align="left">Mann-Whitney U</td><td valign="top" align="left">U = 1558, p&lt; 0.001</td><td valign="top" align="left"/><td valign="top" align="left"/></tr><tr><td valign="top" align="left" style="border-bottom: 1px solid">7F</td><td valign="top" align="left" style="border-bottom: 1px solid #000000">Mixed<break/>ANOVA</td><td valign="top" align="left" style="border-bottom: 1px solid #000000">algorithm <bold><italic>F</italic></bold><sub>(1,78)</sub> = 73, p &lt; 0.001; interaction <bold><italic>F</italic></bold><sub>(4,312)</sub> = 0.5, p = 0.8</td><td valign="top" align="left" style="border-bottom: 1px solid"/><td valign="top" align="left" style="border-bottom: 1px solid"/></tr></tbody></table></table-wrap></floats-group></article>