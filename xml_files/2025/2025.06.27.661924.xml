<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS207044</article-id><article-id pub-id-type="doi">10.1101/2025.06.27.661924</article-id><article-id pub-id-type="archive">PPR1043733</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Optimization of regulatory DNA with active learning</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Shen</surname><given-names>Yuxin</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Kudla</surname><given-names>Grzegorz</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Oyarzún</surname><given-names>Diego A.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="corresp" rid="CR1">†</xref></contrib></contrib-group><aff id="A1"><label>1</label>School of Biological Sciences, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01nrxwf90</institution-id><institution>University of Edinburgh</institution></institution-wrap>, <country country="GB">UK</country></aff><aff id="A2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05hygey35</institution-id><institution>Institute for Genetics and Cancer</institution></institution-wrap>, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01nrxwf90</institution-id><institution>University of Edinburgh</institution></institution-wrap>, <country country="GB">UK</country></aff><aff id="A3"><label>3</label>School of Informatics, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01nrxwf90</institution-id><institution>University of Edinburgh</institution></institution-wrap>, <country country="GB">UK</country></aff><author-notes><corresp id="CR1">
<label>†</label>Corresponding author: <email>d.oyarzun@ed.ac.uk</email>
</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>11</day><month>07</month><year>2025</year></pub-date><pub-date pub-type="preprint"><day>27</day><month>06</month><year>2025</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Many biotechnology applications rely on microbial strains engineered to express heterologous proteins at maximal yield. A common strategy for improving protein output is to design expression systems with optimized regulatory DNA elements. Recent advances in high-throughput experimentation have enabled the use of machine learning predictors in tandem with sequence optimizers to find regulatory sequences with improved phenotypes. Yet the narrow coverage of training data, limited model generalization, and highly non-convex nature of genotype-phenotype landscapes can limit the use of traditional sequence optimization algorithms. Here, we explore the use of active learning as a strategy to improve expression levels through iterative rounds of measurements, model training, and sequence sampling-and-selection. We explore convergence and performance of the active learning loop using synthetic data and an experimentally characterized genotype-phenotype landscape of yeast promoter sequences. Our results show that active learning can outperform one-shot optimization approaches in complex landscapes with a high degree of epistasis. We demonstrate the ability of active learning to effectively optimize sequences using datasets from different experimental conditions, with potential for leveraging data across laboratories, strains or growth conditions. Our findings highlight active learning as an effective framework for DNA sequence design, offering a powerful strategy for phenotype optimization in biotechnology.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><label>1</label><title>Introduction</title><p id="P2">The design of DNA sequences to achieve a desired phenotype is a key challenge in biotechnology. Regulatory elements, in particular, are often employed to control protein expression across many use cases, including industrial strain design [<xref ref-type="bibr" rid="R1">1</xref>, <xref ref-type="bibr" rid="R2">2</xref>], gene therapy [<xref ref-type="bibr" rid="R3">3</xref>], and mRNA therapeutics [<xref ref-type="bibr" rid="R4">4</xref>]. Designing regulatory sequences that maximize expression often requires many iterations and domain knowledge to discover mutations that improve the desired phenotype. Moreover, the high dimensionality of genotype space together with higher order interactions between mutations can produce complex and highly nonconvex genotype–phenotype landscapes. Such landscapes are challenging to navigate experimentally and computational models are increasingly being adopted for <italic>in silico</italic> exploration of the genotype space [<xref ref-type="bibr" rid="R5">5</xref>].</p><p id="P3">Advances in massively parallel reporter assays are generating extensive sequencing and phenotypic data [<xref ref-type="bibr" rid="R6">6</xref>, <xref ref-type="bibr" rid="R7">7</xref>, <xref ref-type="bibr" rid="R8">8</xref>, <xref ref-type="bibr" rid="R9">9</xref>], which enables the use of machine learning algorithms to model protein expression landscapes [<xref ref-type="bibr" rid="R10">10</xref>]. Such sequence-to-expression (STE) models have been developed for various regulatory elements, including ribosome binding sites [<xref ref-type="bibr" rid="R11">11</xref>], promoters [<xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R13">13</xref>], transcriptional enhancers [<xref ref-type="bibr" rid="R14">14</xref>, <xref ref-type="bibr" rid="R15">15</xref>], and 5’ untranslated regions [<xref ref-type="bibr" rid="R16">16</xref>, <xref ref-type="bibr" rid="R17">17</xref>]. A common approach to sequence design is one-shot optimization, whereby STE models are first trained on sequencing and screening data, and then looped into a sequence optimization algorithm (<xref ref-type="fig" rid="F1">Figure 1A</xref>) [<xref ref-type="bibr" rid="R18">18</xref>, <xref ref-type="bibr" rid="R19">19</xref>]. The optimizer navigates the input space towards sequences with improved predicted expression, which can then screened <italic>in vivo</italic>. This approach has led to the discovery of improved sequences in model microbes [<xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R13">13</xref>, <xref ref-type="bibr" rid="R20">20</xref>, <xref ref-type="bibr" rid="R21">21</xref>] as well as complex multicellular organisms such as drosophila, zebrafish and mice [<xref ref-type="bibr" rid="R15">15</xref>, <xref ref-type="bibr" rid="R22">22</xref>, <xref ref-type="bibr" rid="R23">23</xref>].</p><p id="P4">There are many computational techniques to traverse the STE predictions toward maxima, including gradient ascent [<xref ref-type="bibr" rid="R21">21</xref>], global optimization [<xref ref-type="bibr" rid="R24">24</xref>] and generative modelling [<xref ref-type="bibr" rid="R13">13</xref>, <xref ref-type="bibr" rid="R25">25</xref>, <xref ref-type="bibr" rid="R26">26</xref>]. However, one-shot optimization can face limitations due to the sparse and limited coverage of the sequence space employed for training—particularly as sequence length increases. In one-shot optimization, the STE model remains fixed through the sequence search, and thus optimizers can divert far away from the sequence space where the model was originally trained. This can lead to low-confidence predictions, decrease the success rate of experimental testing, and increase discovery costs. While this can be mitigated by training on larger data, the cost and complexity of acquiring large data can be a barrier in many real-world use cases [<xref ref-type="bibr" rid="R10">10</xref>].</p><p id="P5">Here, we explore the use of active learning for DNA sequence optimization. Active learning is a paradigm whereby models are iteratively re-trained with batches of newly acquired data (<xref ref-type="fig" rid="F1">Figure 1A</xref>). This enables the adaptive selection of new sequences to measure and maximize the information gained from each round of experiments [<xref ref-type="bibr" rid="R27">27</xref>]. Active learning has found applications in many biological design tasks, including protein engineering [<xref ref-type="bibr" rid="R28">28</xref>, <xref ref-type="bibr" rid="R29">29</xref>], drug discovery [<xref ref-type="bibr" rid="R30">30</xref>, <xref ref-type="bibr" rid="R31">31</xref>], media optimization [<xref ref-type="bibr" rid="R32">32</xref>, <xref ref-type="bibr" rid="R33">33</xref>, <xref ref-type="bibr" rid="R34">34</xref>], and metabolic engineering [<xref ref-type="bibr" rid="R35">35</xref>, <xref ref-type="bibr" rid="R36">36</xref>, <xref ref-type="bibr" rid="R37">37</xref>]. We first explore the performance of active learning on a synthetic phenotype landscape produced with the classic NK fitness model [<xref ref-type="bibr" rid="R38">38</xref>] adapted to nucleotide mutations. The results suggest that active learning can effectively traverse the sequence space toward increased expression even in increasingly rugged landscapes with many local optima. We then apply the methodology to an experimentally measured expression landscape containing more than 20,000,000 promoter sequences in <italic>Saccharomyces cerevisiae</italic>, using a highly accurate Transformer-based deep learning model as a surrogate to extrapolate expression across the whole sequence space. We demonstrate the ability of active learning to robustly find sequences with improved expression, and its ability to utilize data acquired in different growth conditions. We finally explore several performance improvements by embedding biological knowledge into the strategy for sequence sampling and selection. Our results demonstrate the utility of active learning in DNA sequence design in sparsely sampled and highly non-convex protein expression landscapes.</p></sec><sec id="S2" sec-type="results"><label>2</label><title>Results</title><sec id="S3"><label>2.1</label><title>Active learning on a synthetic genotype-phenotype landscape</title><p id="P6">We focus on optimization of regulatory DNA sequences using sequence-to-expression (STE) machine learning models as predictors. In a typical use case, an STE model is trained on <italic>n</italic> measured pairs (<bold>x</bold>, <italic>y</italic>) of genotype-phenotype associations, where <bold>x</bold> is a DNA sequence of length <italic>N</italic> and <italic>y</italic> is a readout of expression, typically quantified via fluorescence reporters or suitably designed screening assays that couple expression to fitness. In one-shot optimization, the STE model remains fixed and is iteratively queried by a sequence searching algorithm, typically using hill climbing or global optimization heuristics. We focus on an active learning approach (<xref ref-type="fig" rid="F1">Figure 1A</xref>), whereby the STE model is re-trained during <italic>M</italic> learning loops, where new batches of data with <italic>q</italic> new samples (<bold>x</bold>, <italic>y</italic>) that are iteratively selected through the optimization loop. By careful design of a sampling-and-selection routine that balances exploration and exploitation of the sequence space, active learning can continuously improve model accuracy and navigate the predicted landscape in high-confidence regions of the STE model.</p><p id="P7">To first explore the ability of active learning to traverse highly non-convex expression land-scapes, we focussed on synthetic data generated with a theoretical fitness model [<xref ref-type="bibr" rid="R39">39</xref>]. This allows sampling the landscape across the entire sequence space and computing global maxima as a ground truth baseline. We focussed on the NK model for fitness landscapes, because it has few parameters and produces landscapes with tuneable ruggedness. The NK model was originally developed for gene-to-gene interactions [<xref ref-type="bibr" rid="R38">38</xref>], and we adapted it to model epistatic interactions between positions in a nucleotide sequence (<xref ref-type="fig" rid="F1">Figure 1B</xref>). In the adapted model, <italic>N</italic> is the number of positions in a sequence, and <italic>K</italic> models the number of other positions each nucleotide interacts with epistatically (i.e., the order of interaction). The parameter <italic>K</italic> can be tuned between <italic>K</italic> = 0 (no epistasis, only additive effects) and <italic>K</italic> = <italic>N</italic> − 1 to control the ruggedness, with larger <italic>K</italic> leading to increasingly complex landscapes with many local maxima and minima; more details on our implementation of the NK landscape can be found in the Methods.</p><p id="P8">We generated several NK landscapes for sequences of length <italic>N</italic> = 10, resulting in a sequence space with a total of 4<sup>10</sup> = 1, 048, 576 variants. To visualize the ruggedness of the landscape, we employed the t-SNE dimensionality reduction algorithm (<xref ref-type="fig" rid="F1">Figure 1C</xref>). As <italic>K</italic> increases, variants with more extreme phenotypes tend to appear more dispersed across sequence space, reflecting the increased ruggedness and non-convexity of the landscape. To first assess the challenge of modelling such complex landscapes, we trained feedforward neural networks on NK fitness values (<xref ref-type="fig" rid="F1">Figure 1D</xref>). Models were trained on 2,000 sequences with Latin Hypercube Sampling (LHS) (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S1</xref>), which represents a coverage of ~0.2% of the sequence space; this is substantially larger than some of the largest datasets employed in the literature. Prediction results on a held-out test set show that in the absence of epistatic interactions (<italic>K</italic> = 0, <xref ref-type="fig" rid="F1">Figure 1D</xref>), the landscape can be regressed with reasonable accuracy. However, for <italic>K</italic> = 1 and above, the introduction of higher-order interactions makes the landscape significantly more challenging to regress, even after landscape-specific optimization of the neural architecture.</p><p id="P9">We built an active learning loop designed to find the global optimum of the NK fitness landscape (<xref ref-type="fig" rid="F1">Figure 1A</xref>), assuming an initial set of <italic>n</italic> = 1, 000 variants for training that were randomly selected from the whole input space. We employed an ensemble of <italic>r</italic> = 10 feedforward neural networks as a machine learning regressor of the fitness landscape. At each active learning loop, sequences are sampled and selected based on the model predictions and a reward function. First, the ensemble is queried with 10, 000 sampled sequences, and the Upper Confidence Bound reward function <italic>J</italic><sub><italic>i</italic></sub> is calculated for each sequence <italic>X</italic><sub><italic>i</italic></sub>: <disp-formula id="FD1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>×</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>r</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>r</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mo>×</mml:mo></mml:mrow></mml:mstyle><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>r</mml:mi></mml:mfrac><mml:msup><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>r</mml:mi></mml:munderover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>r</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>r</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>y</italic><sub><italic>ij</italic></sub> is the predicted value of the the <italic>i</italic>th sequence with the <italic>j</italic>th model in the ensemble. The terms in <xref ref-type="disp-formula" rid="FD1">Eq. (1)</xref> correspond to the mean and standard deviation of the predicted expression levels across an ensemble of <italic>r</italic> neural for each sequence. The parameter <italic>α</italic> controls the balance between sequence exploration and exploration. For sequence selection, to emulate scenarios with limited data acquisition capability, we fixed the batch size to the top <italic>q</italic> = 100 sequences ranked by their reward function value. At each learning loop, the selected batch then goes into the evaluation step, and is added to the existing data for model re-training in the next loop.</p><p id="P10">To test the impact of the strategy employed for sequence sampling in active learning, we compared random sampling with a biologically inspired sampling based on directed evolution (DE) [<xref ref-type="bibr" rid="R40">40</xref>], whereby new sequences are generated by introducing mutations at <italic>j</italic> positions starting from sequences in the previous active learning batch. The results (<xref ref-type="fig" rid="F2">Figure 2A</xref>) suggest that after four active learning loops, directed evolution sampling reaches better optima than random sampling across all levels NK ruggedness. The final-batch genotype distribution on the NK0 landscape is shown in <xref ref-type="fig" rid="F2">Figure 2B</xref>, showing that active learning in tandem with directed evolution can effectively find optimal DNA sequences, in agreement with previous studies on protein fitness optimization [<xref ref-type="bibr" rid="R29">29</xref>].</p><p id="P11">We also tested the impact of exploration and exploitation in the reward function, as their balance is crucial to ensure thorough landscape search while maintaining strong optimization performance [<xref ref-type="bibr" rid="R41">41</xref>]. We compared the phenotype distribution along the active learning loops in two exploration/exploitation ratio regimes using directed evolution for sequence sampling (<xref ref-type="fig" rid="F2">Figure 2C</xref>). A lower ratio resulted in a higher mean phenotype distribution and with lower variance, indicating a tendency to refine predictions around already identified high expression regions while limiting broader landscape exploration.</p><p id="P12">To compare active learning with traditional optimization approaches, we implemented three one-shot optimization strategies based on random screening (RS), strong-selection weak-mutation (SSWM) and gradient descent (GD). The results (<xref ref-type="fig" rid="F2">Figure 2D</xref>) across four rounds of active learning optimization suggest that active learning can outperform one-shot methods, particularly in complex landscapes with strong epistasis. We observed comparable optimization performance with SSWM in smooth landscapes (<italic>K</italic> = 0), but under higher-order interactions, active learning can produce better optima than one-shot optimizers. Notably, sequences selected through active learning also exhibit a clear trend of performance improvement across training loops, likely due to refinements to the STE model through re-training at each loop. We also observed large batch-to-batch improvements expression levels with active learning (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S2</xref>). The success of active learning with directed evolution sampling can be attributed to its mutation from sequences that already exhibit satisfactory performance in each round of selection, which also produces STE models with improved accuracy (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S3-S4</xref>).</p></sec><sec id="S4"><label>2.2</label><title>Optimization of promoter sequences in yeast</title><p id="P13">To test the utility of active learning in an experimentally characterized expression landscape, we explored the optimization of promoter sequences using a large STE dataset acquired in Saccharomyces cerevisiae [<xref ref-type="bibr" rid="R12">12</xref>]. These data include two sets of approximately <italic>n</italic> = 20M and <italic>n</italic> = 30M promoter sequences of fixed length (<italic>L</italic> = 80nt), alongside expression readouts of a yfp fluorescent reporter in two different growth media. This dataset is ideal for our study because we can subsample the sequence variants to mimic different real world use cases, and test the ability of active learning to learn and transfer information across different experimental conditions (<xref ref-type="fig" rid="F3">Figure 3A</xref>). To query the landscape with sequences that were not screened in the original work, we employed a transformer-based STE regressor as a surrogate for the expression landscape; this regressor was developed in the original work [<xref ref-type="bibr" rid="R12">12</xref>] and achieved exceptionally high accuracy on independent test sequences (Pearson <italic>r&gt;</italic> 95% in both growth media).</p><p id="P14">To mimic data scenarios encountered in applications, we considered a small initial batch of promoters in two relevant data scenarios: a case with initial promoters cover a broad range of protein expression levels, and a case in which the initial promoters are enriched for low expression (<xref ref-type="fig" rid="F3">Figure 3B</xref>). This allowed testing the ability of active learning to traverse the landscape from substantially different initial starting points. We compared active learning to one-shot methods using the same total number of evaluated sequences. We employed an initial batch size <italic>n</italic> = 1000 promoters, with <italic>M</italic> = 4 active learning loops, and a batch size of <italic>q</italic> = 1000 top performers according to the same reward function in <xref ref-type="disp-formula" rid="FD1">Eq. (1)</xref> for an ensemble of 10 feedforward neural networks evaluated at 10,000 DE-sampled sequences; we adjusted the exploration/exploitation ratio in the reward function to account for the longer sequence length compared to the previous NK landscape. In the case of uniform initial data, we observed (<xref ref-type="fig" rid="F3">Figure 3B</xref>) clear batch-to-batch improvement with active learning, which outperformed one-shot methods after two active learning loops. In the case of low expression initial data, one-shot methods struggled to escape local optima at intermediate expression levels, while active learning was able to traverse expression levels close to the measured maximum. Expression distribution in <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S5</xref> also shows batch-to-batch improvements in expression across the active learning loops. Despite the comparable sequence diversity in both initial batches of sequences, we observed pronounced differences in the space of sequences explored by active learning (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S6</xref>).</p><p id="P15">Examination of neural network predictions against ground truth along the optimization rounds suggests that in SSWM on one-shot optimization, the fixed STE regressor tends to over-shoot predictions as compared to the ground truth measurements computed with the surrogate transformer model (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S7</xref>). In contrast, the STE regressor in the active learning loop progressively improves accuracy with each batch. The ability of active learning to iteratively improve model accuracy is particularly noticeable in the low expression scenario, whereby the narrow distribution of training labels leads to an initial regressor with poor generalization (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S8</xref>). By iteratively supplementing the model with improved sequences, active learning can improve predictions and traverse the landscape toward better phenotypes.</p><p id="P16">To test the robustness of the active learning approach, we introduced noise to the measured expression levels for the initial batch of promoters, and thus forced the loop to start from more uncertain data. The results suggest that the active learning loop retains its ability to traverse expression towards a maximum (<xref ref-type="fig" rid="F3">Figure 3C</xref>).</p><p id="P17">We sought to examine this robustness in more detail by simulating a data transfer scenario, whereby the loop is initialized with data acquired under different experimental conditions. This mimics use cases in which data from other laboratories is used to start an active learning pipeline. We initialized the sequence optimizer with promoters measured in growth medium B, and run an active learning loop using the Transformer network pre-trained on medium A as surrogate for new measurements. The results (<xref ref-type="fig" rid="F3">Figure 3D</xref>) show that the active learning loop achieves similar expression levels for both initializations. Furthermore, when initializing with sequences preoptimized in medium B, the active learning loop was able to reach a further 7.4% improvement in expression in the final batch in medium A. Overall these results suggest that active learning can effectively leverage datasets from different conditions.</p></sec><sec id="S5"><label>2.3</label><title>Performance improvement with alternative sampling and selection</title><p id="P18">In this section, we explore further performance improvements via other strategies for sequence sampling and selection on the yeast promoter dataset. We first trialed alternatives to directed evolution sampling that aim to mimic other aspects of natural evolution. Specifically, we explored two alternative ways to introduce mutations in the sampled sequences (<xref ref-type="fig" rid="F4">Figure 4A</xref>, see Methods for details): <list list-type="bullet" id="L1"><list-item><p id="P19"><bold>Genetic drift</bold> In this approach new query sequences are designed by assigning a probability of mutation to each site from a randomly picked sequence in the sequences from previous active learning loop. Each sequence was generated with 10% probability of mutation from a random sequence of the batch in the previous loop.</p></list-item><list-item><p id="P20"><bold>Recombination</bold> We break down the sequences from the querying data of the latest learning loop, and recombine them randomly. In each loop, 10 breakpoints were randomly introduced, and the sequences from the previous loop were broken at these points and recombined.</p></list-item></list>
</p><p id="P21">To evaluate these sampling strategies, 10,000 sequences are sampled in each learning loop with the 3 biological sampling methods: DE, genetic drift and recombination, and the top 1000 are selected based on the reward function in Eq. (1). <xref ref-type="fig" rid="F4">Figure 4B</xref> shows the performance of sampling methods across four active learning loops. The result shows the sampling methods have a substantial impact on the active learning loop. While genetic drift performs slightly worse than DE, recombination outperform DE, highlighting further potential of improving the active learning pipeline. In all three methods we observed a decreased but reasonable sequence diversity along the active learning loops (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S6</xref>, <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S10</xref>).</p><p id="P22">We also observed motif enrichment in the batches of sequences sampled by directed evolution, genetic drift and recombination in the active learning pipeline. These motifs are known to bind to specific transcription factors [<xref ref-type="bibr" rid="R42">42</xref>]. For each AL loop, we calculated the average motif scores across the selected batch of 1000 sequences for the 244 relevant transcription factor motifs, and compared them with the average motif score in the initial batch, which has no bias towards any motif (<xref ref-type="fig" rid="F4">Figure 4C</xref>). Certain motifs were consistently enriched or depleted across all four batches and sampling methods, which suggests that active learning can capture biologically-relevant sequence features.</p><p id="P23">As another strategy to improve the active learning loop, we explored the use of introducing task-specific domain knowledge into the sequence selection step. To this end, we modified the reward function with a score that weights the presence of sequence motifs known to bind to specific transcription factors. Previous work has shown that such mechanistic information has strong correlations with expression levels [<xref ref-type="bibr" rid="R43">43</xref>] and helps with model generalization to new regions of the sequence space [<xref ref-type="bibr" rid="R44">44</xref>]. To this end, we modified the reward function by including a weight on a motif score (see (2) in Methods) mined from the YeTFaSCo database [<xref ref-type="bibr" rid="R42">42</xref>]. This strategy is motivated by previous work showing that most of the motifs are activators rather than repressors [<xref ref-type="bibr" rid="R43">43</xref>], so that maximization of the modified reward function can steer the search towards enriched with relevant motifs and hence improved expression. The results (<xref ref-type="fig" rid="F4">Figure 4D</xref>) show that motif sampling improves active learning performance, indicating the effectiveness of including task-specific biological knowledge into the learning loop.</p></sec></sec><sec id="S6" sec-type="discussion"><label>3</label><title>Discussion</title><p id="P24">Many applications in biotechnology and biomedicine require optimization of heterologous protein expression. One strategy is to design an expression system with highly performant regulatory DNA elements, such as promoters, enhancers or terminators, without modification to the coding sequence. Here, we demonstrated the use of active learning as a computational strategy to find regulatory sequences that improve protein expression. Using both synthetic and experimentally determined expression landscapes, our results suggest that active learning can find optimal sequences in complex expression landscapes, thanks to its ability to iteratively refine phenotypic predictions.</p><p id="P25">Active learning has been widely applied to diverse biological tasks [<xref ref-type="bibr" rid="R30">30</xref>, <xref ref-type="bibr" rid="R31">31</xref>, <xref ref-type="bibr" rid="R32">32</xref>, <xref ref-type="bibr" rid="R33">33</xref>, <xref ref-type="bibr" rid="R34">34</xref>, <xref ref-type="bibr" rid="R35">35</xref>, <xref ref-type="bibr" rid="R36">36</xref>, <xref ref-type="bibr" rid="R37">37</xref>]. In the case of protein sequence design, recent studies have demonstrated that directed evolution paired with active learning can efficiently navigate protein fitness landscapes to improve enzyme function [<xref ref-type="bibr" rid="R28">28</xref>, <xref ref-type="bibr" rid="R29">29</xref>]. Such approaches rely on iterative mutation of specific residues that are expected to impact function, such as enzymatic active sites or specific transcription factor binding motifs. In the case of regulatory DNA designed to improve expression levels, however, current experimental approaches increasingly rely on large libraries of fully randomized sequences using massively parallel reporter assays [<xref ref-type="bibr" rid="R10">10</xref>, <xref ref-type="bibr" rid="R45">45</xref>, <xref ref-type="bibr" rid="R46">46</xref>]. While this enables broader coverage of the sequence space [<xref ref-type="bibr" rid="R47">47</xref>], it also introduces additional challenges resulting from the ruggedness of the phenotypic landscape. Such expression fitness landscapes can have many local maxima; for example, in the case of promoter sequences such maxima may cluster around sequence regions enriched for specific transcriptional motifs.</p><p id="P26">To explore the performance of active learning on highly non-convex landscapes, we first focussed on synthetic data generated via the NK model of epistatic interactions. We show that active learning can effectively navigate such complex landscapes, which would otherwise be a substantial challenge with traditional hill climbing algorithms. We further trialed active learning on a large promoter expression dataset, using a pretrained deep learning model [<xref ref-type="bibr" rid="R12">12</xref>] as a surrogate for unmeasured sequences. Previous studies have explored the transfer of data across sequence-to-expression predictors models by pre-training on different experimental conditions [<xref ref-type="bibr" rid="R48">48</xref>], indicating that cross-condition data can provide useful information. By swapping initial data from two growth conditions, we show that active learning can robustly find optimal sequences when initialized with expression data from different experimental conditions. Notably, we found that initializing the active learning loop with pre-optimized sequences from another experimental condition led to better expression than starting from non-optimized samples from the same condition. This suggests that active learning optimization can make effective use of data acquired in different growth conditions or laboratories.</p><p id="P27">In our implementations we compared various strategies for sampling and selection of DNA sequences along the active learning loop. However, other alternatives including Bayesian optimization [<xref ref-type="bibr" rid="R49">49</xref>] and generative models [<xref ref-type="bibr" rid="R25">25</xref>, <xref ref-type="bibr" rid="R50">50</xref>] could further improve performance. Moreover, in our analysis we focussed exclusively on feedforward neural networks as sequence-to-expression predictors, and further studies are needed to explore the advantages of various deep learning architectures that have shown high predictive power [<xref ref-type="bibr" rid="R51">51</xref>]. These and other extensions offer substantial promise for the use of active learning in DNA sequence optimization.</p></sec><sec id="S7" sec-type="methods"><label>4</label><title>Methods</title><sec id="S8"><label>4.1</label><title>Protein expression datasets</title><sec id="S9"><title>Synthetic expression landscapes</title><p id="P28">To simulate synthetic genotype–phenotype landscapes with a controllable level of ruggedness, we adapted the classic NK fitness model [<xref ref-type="bibr" rid="R38">38</xref>] to nucleotide sequences; <italic>N</italic> represents the length of the DNA sequence and <italic>K</italic> defines the order of epistatic interactions among positions. The parameter <italic>K</italic> ranges from 0 to <italic>N</italic> − 1 and controls the ruggedness of the landscape: higher <italic>K</italic> values introduce higher-order epistatic interactions, increasing the ruggedness and number of local optima in the fitness landscape. We employed a previous implementation of [<xref ref-type="bibr" rid="R52">52</xref>] based on the code available at <ext-link ext-link-type="uri" xlink:href="https://github.com/acmater/NK_Benchmarking">https://github.com/acmater/NK_Benchmarking</ext-link>. The generation process begins by constructing a random epistatic interaction network, such that each nucleotide position interacts with other <italic>K</italic> positions. Then a random fitness value is assigned to the <italic>i</italic>th nucleotide in the sequence based on the epistatic network. The phenotype is calculated as the average fitness values of all nucleotide positions.</p></sec><sec id="S10"><title>Yeast promoter expression data</title><p id="P29">The promoter sequence ground truth models were taken from previous work by Vaishnav <italic>et al</italic> [<xref ref-type="bibr" rid="R12">12</xref>]. The original data contain yellow fluorescent protein (YFP) expression levels of 80-nt promoter sequences under two different experimental conditions: defined medium (<italic>n</italic> = 20M sequences) and complex medium (<italic>n</italic> = 30M sequences). Fluorescence measurements are in the range of [0, <xref ref-type="bibr" rid="R18">18</xref>] across the whole dataset. A transformer-based predictor was trained in the original work for each experimental condition respectively, achieving high prediction accuracy (Pearson <italic>r &gt;</italic> 95% in both cases). We employed these unmodified pretrained models as surrogates for experimental measurements in our active learning loops.</p></sec></sec><sec id="S11"><label>4.2</label><title>Active learning loop</title><sec id="S12"><title>Supervised learning of expression landscapes</title><p id="P30">We employed an ensemble of feedforward neural networks (multilayer perceptron, MLP) to regress expression levels. The ensemble enables to compute expression and estimate uncertainty of predictions for sequence sampling and selection. The ensemble consists of the top 10 performing models from a pool of 40 MLPs spanning four architectures of variable complexity and 10 random weight initializations. Model architectures are detailed in <xref ref-type="supplementary-material" rid="SD1">Table S1</xref>, trained with Adam optimizer and a constant learning rate of 0.001. The mean and standard deviation of predicted expressions from the 10 MLP models were used in the reward function to balance the exploration and exploitation for sequence selection.</p></sec><sec id="S13"><title>Sequence sampling and selection via directed evolution</title><p id="P31">In each active learning loop, we picked a random sequence from the previous batch, and generated 10,000 randomly mutated sequences at 4 positions (NK landscape) or 10 positions (promoter landscape). The sampled sequences were then evaluated with the reward function, and the top 100 sequences on the NK landscapes or the top 1000 sequences on promoter sequence landscapes were selected for the active learning loop.</p></sec><sec id="S14"><title>Reward function</title><p id="P32">To score and select sequences for the next active learning loop, we employed the Upper Confidence Bound (UCB) reward function [<xref ref-type="bibr" rid="R41">41</xref>] specified in <xref ref-type="disp-formula" rid="FD1">Eq. (1)</xref>. To determine an appropriate value for the exploration/exploitation parameter <italic>α</italic>, we performed a grid search with a step size of 0.1 on the NK0 landscape (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S9</xref>). The optimal value was found to be <italic>α</italic> = 0.7 which was used consistently across the four NK landscapes. For the promoter datasets, we scaled <italic>α</italic> by a factor based on the sequence length difference <inline-formula><mml:math id="M2"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msqrt><mml:mrow><mml:mn>80</mml:mn><mml:mo>/</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:msqrt><mml:mo>≈</mml:mo><mml:mn>2.8</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. <boxed-text id="BX1" position="anchor" content-type="below"><p id="P33"><bold>Implementation</bold> Our implementation follows the following pseudocode:</p><p id="P34">  <bold>Input</bold> Oracle (ground truth model or real experiments)</p><p id="P35">  <bold>Input</bold> Initial samples (<italic>X</italic><sub><italic>ini</italic></sub>, <italic>y</italic><sub><italic>ini</italic></sub>)</p><p id="P36">  <italic>X, y</italic> ← <italic>X</italic><sup>0</sup>, <italic>y</italic><sup>0</sup></p><p id="P37">  <bold>for</bold> 1 ≤ <italic>i</italic>≤ 4 <bold>do</bold></p><p id="P38">       <italic>f</italic> <sup>*</sup> = <italic>fit</italic>(<italic>X, y</italic>)</p><p id="P39">       <inline-formula><mml:math id="M3"><mml:mrow><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.2em"/></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mspace width="0.2em"/><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.2em"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mspace width="0.2em"/><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mspace width="0.2em"/><mml:mo>=</mml:mo><mml:mspace width="0.2em"/><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mspace width="0.2em"/><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mspace width="0.2em"/><mml:mo>=</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p><p id="P40">       <italic>Reward</italic> = <italic>α</italic> × <italic>std</italic> + (1 − <italic>α</italic>) × (<italic>mean</italic> + <italic>k</italic> * <italic>motifscore</italic>)</p><p id="P41">       <inline-formula><mml:math id="M4"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.2em"/></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mi>k</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.2em"/><mml:mo>=</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mspace width="0.2em"/><mml:mo>=</mml:mo><mml:mspace width="0.2em"/><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mspace width="0.2em"/></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p><p id="P42">       <italic>y</italic><sup><italic>i</italic></sup> = <italic>F</italic> (<italic>X</italic><sup><italic>i</italic></sup>)</p><p id="P43">       <italic>X, y</italic> ←<italic>X</italic> ∪ <italic>X</italic><sup><italic>i</italic></sup>,<italic>y</italic> ∪ <italic>y</italic><sup><italic>i</italic></sup></p><p id="P44">end for</p></boxed-text>
</p></sec></sec><sec id="S15"><label>4.3</label><title>One-shot optimization</title><p id="P45">For fair comparisons with active learning in <xref ref-type="fig" rid="F2">Figure 2</xref>–<xref ref-type="fig" rid="F3">3</xref>, one-shot optimizations were computed using MLP regressors trained on the same number of sequences as in each active learning loop. The MLP architecture was selected with 5-fold cross-validation across the same architectures in <xref ref-type="supplementary-material" rid="SD1">Table S1</xref>, trained with Adam optimizer and a constant learning rate of 0.001. Three methods were applied consistently across the NK landscapes and the promoter sequence landscapes, detailed next.</p><sec id="S16"><title>Random sampling (RS)</title><p id="P46">For RS, 100,000 sequences were randomly generated and scored using the MLP model. The top 100 sequences based on predicted expression were selected.</p></sec><sec id="S17"><title>Gradient descent (GD)</title><p id="P47">For GD, sequences were iterated over 200 steps to reach a higher predicted expression. At each step, MLP model gradients were estimated via finite differences and used to update the sequence in the direction that improved the model output, and the final sequence after 200 iterations was selected. The full procedure was repeated independently for 100 random starting sequences to generate 100 optimized sequences.</p></sec><sec id="S18"><title>Strong-selection weak-mutation (SSWM)</title><p id="P48">For SSWM, sequences were generated following 200 generations for the NK landscapes (100 generations for the promoter landscapes) of evolution and selection to reach a higher predicted expression. In each generation, mutations were introduced at a low rate on a small population of 10 sequences, and the mutated sequences were evaluated using the MLP model. The population of next generation consisted of the top 5 sequences and 5 single-mutant variants of the current best sequence. As with GD, the full procedure was repeated for 100 times to generate 100 optimized sequences.</p></sec></sec><sec id="S19"><label>4.4</label><title>Motif enrichment and motif sampling</title><p id="P49">For the results in <xref ref-type="fig" rid="F4">Figure 4D</xref>, we included additional motif score to the reward function: <disp-formula id="FD2"><label>(2)</label><mml:math id="M5"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>r</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>r</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>×</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>r</mml:mi></mml:mfrac><mml:msup><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>r</mml:mi></mml:munderover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>r</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>r</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>r</italic> is the number of MLP predictors in the ensemble, the parameter <italic>α</italic> controls the balance between sequence exploration and exploration, <italic>P</italic> is the sum of the motif score, and the <italic>k</italic> is a penalty on the motif score; we employed parameters <italic>α</italic> = 0.3 and <italic>k</italic> = 0.66. The motif score <italic>P</italic> was computed from predicted binding probabilities between transcription factors (TF) and sequence motifs based on earlier work[<xref ref-type="bibr" rid="R44">44</xref>]. The 244 position frequency matrices (PFM) were obtained from the YeTFaSCo database [<xref ref-type="bibr" rid="R42">42</xref>], which provides the occurrences of each nucleotide at each position of 244 TF-specific motifs. Each PFM has dimensions of 4 × <italic>𝓁</italic><sub><italic>i</italic></sub>, where <italic>𝓁</italic><sub><italic>i</italic></sub> is the motif length. For each promoter sequence, we compute the binding probability for each motif <italic>P</italic><sub><italic>i</italic></sub> by integrating over all sequence positions: <disp-formula id="FD3"><label>(3)</label><mml:math id="M6"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>81</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>ℓ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mn>244</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula> with <italic>P</italic><sub><italic>i</italic>,<italic>j</italic></sub> being the probability of TF binding starting from the <italic>j</italic><sup>th</sup> nucleotide in the 80-nucleotide sequence, derived from the corresponding PFM. The final value <italic>P</italic><sub><italic>i</italic></sub> is the probability that the <italic>i</italic><sup>th</sup> TF binds to at least one site within the entire sequence.</p></sec><sec id="S20"><label>4.5</label><title>Data transfer across growth conditions</title><p id="P50">For our data transfer analysis (<xref ref-type="fig" rid="F3">Figure 3D</xref>), we employed the two pre-trained transformer models (medium A, medium B) from the original data source [<xref ref-type="bibr" rid="R12">12</xref>] as surrogetes for ground truth data. For the data transfer across media, we first initialized the active learning loop with randomly sampled <italic>n</italic> =1,000 sequences measured in medium B. We then ran the active learning optimization on medium A using iterative collection of data from measurements in that medium. For data transfer with pre-optimization, we followed the same process but pre-optimized the <italic>n</italic> =1,000 initial sequences for medium B, and then ran the active learning loop for medium A. The pre-optimization process is a separate rounds of four active learning loops on medium B, and the final batch of 1000 sequences with highest expression in medium B was emlpoyed to initialize the loop in medium A. The benchmark (<xref ref-type="fig" rid="F3">Figure 3D</xref>, blue) is the same as <xref ref-type="fig" rid="F3">Figure 3B</xref> (top).</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Figures</label><media xlink:href="EMS207044-supplement-Supplementary_Figures.pdf" mimetype="application" mime-subtype="pdf" id="d68aAcEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S22"><title>Acknowledgments</title><p>Y.S. was supported by the UKRI Biotechnology and Biological Sciences Research Council (BB-SRC) grant number BB/T00875X/1. D.A.O. was supported by the UKRI Centre for Doctoral Training in Biomedical AI (EP/S02431X/1), and the Engineering Biology Mission Award CY-BER (BB/Y007638/1).</p></ack><sec id="S21" sec-type="data-availability"><title>Code availability</title><p id="P51">Python code for model training and evaluation will be made available in an open repository.</p></sec><ref-list><ref id="R1"><label>[1]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clomburg</surname><given-names>James M</given-names></name><name><surname>Crumbley</surname><given-names>Anna M</given-names></name><name><surname>Gonzalez</surname><given-names>Ramon</given-names></name></person-group><article-title>Industrial biomanufacturing: the future of chemical production</article-title><source>Science</source><year>2017</year><volume>355</volume><issue>6320</issue><elocation-id>aag0804</elocation-id><pub-id pub-id-type="pmid">28059717</pub-id></element-citation></ref><ref id="R2"><label>[2]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cazier</surname><given-names>Andrew P</given-names></name><name><surname>Blazeck</surname><given-names>John</given-names></name></person-group><article-title>Advances in promoter engineering: novel applications and predefined transcriptional control</article-title><source>Biotechnology Journal</source><year>2021</year><volume>16</volume><issue>10</issue><elocation-id>2100239</elocation-id><pub-id pub-id-type="pmid">34351706</pub-id></element-citation></ref><ref id="R3"><label>[3]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Konkle</surname><given-names>Barbara A</given-names></name><name><surname>Walsh</surname><given-names>Christopher E</given-names></name><name><surname>Escobar</surname><given-names>Miguel A</given-names></name><etal/></person-group><article-title>Bax 335 hemophilia b gene therapy clinical trial results: potential impact of cpg sequences on gene expression</article-title><source>Blood, The Journal of the American Society of Hematology</source><year>2021</year><volume>137</volume><issue>6</issue><fpage>763</fpage><lpage>774</lpage><pub-id pub-id-type="doi">10.1182/blood.2019004625</pub-id><pub-id pub-id-type="pmcid">PMC7885820</pub-id><pub-id pub-id-type="pmid">33067633</pub-id></element-citation></ref><ref id="R4"><label>[4]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Castillo-Hair</surname><given-names>Sebastian</given-names></name><name><surname>Fedak</surname><given-names>Stephen</given-names></name><name><surname>Wang</surname><given-names>Ban</given-names></name><etal/></person-group><chapter-title>Optimizing 5’UTRs for mRNA-delivered gene editing using deep learning</chapter-title><source>Nature Communications</source><publisher-name>Nature Publishing Group</publisher-name><year>2024</year><month>June</month><volume>15</volume><issue>1</issue><elocation-id>5284</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-024-49508-2</pub-id><pub-id pub-id-type="pmcid">PMC11189900</pub-id><pub-id pub-id-type="pmid">38902240</pub-id></element-citation></ref><ref id="R5"><label>[5]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greenbury</surname><given-names>Sam F</given-names></name><name><surname>Louis</surname><given-names>Ard A</given-names></name><name><surname>Ahnert</surname><given-names>Sebastian E</given-names></name></person-group><article-title>The structure of genotype-phenotype maps makes fitness landscapes navigable</article-title><source>Nature Ecology &amp; Evolution</source><year>2022</year><month>November</month><volume>6</volume><issue>11</issue><fpage>1742</fpage><lpage>1752</lpage><pub-id pub-id-type="pmid">36175543</pub-id></element-citation></ref><ref id="R6"><label>[6]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grace Gordon</surname><given-names>M</given-names></name><name><surname>Inoue</surname><given-names>Fumitaka</given-names></name><name><surname>Martin</surname><given-names>Beth</given-names></name><etal/></person-group><article-title>lentiMPRA and MPRAflow for high-throughput functional characterization of gene regulatory elements</article-title><source>Nature protocols</source><year>2020</year><volume>15</volume><issue>8</issue><fpage>2387</fpage><lpage>2412</lpage><pub-id pub-id-type="doi">10.1038/s41596-020-0333-5</pub-id><pub-id pub-id-type="pmcid">PMC7550205</pub-id><pub-id pub-id-type="pmid">32641802</pub-id></element-citation></ref><ref id="R7"><label>[7]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kreimer</surname><given-names>Anat</given-names></name><name><surname>Zeng</surname><given-names>Haoyang</given-names></name><name><surname>Edwards</surname><given-names>Matthew D</given-names></name><etal/></person-group><article-title>Predicting gene expression in massively parallel reporter assays: A comparative study</article-title><source>Human mutation</source><year>2017</year><volume>38</volume><issue>9</issue><fpage>1240</fpage><lpage>1250</lpage><pub-id pub-id-type="doi">10.1002/humu.23197</pub-id><pub-id pub-id-type="pmcid">PMC5560998</pub-id><pub-id pub-id-type="pmid">28220625</pub-id></element-citation></ref><ref id="R8"><label>[8]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilliot</surname><given-names>Pierre-Aurélien</given-names></name><name><surname>Gorochowski</surname><given-names>Thomas E</given-names></name></person-group><article-title>Sequencing enabling design and learning in synthetic biology</article-title><source>Current Opinion in Chemical Biology</source><year>2020</year><volume>58</volume><fpage>54</fpage><lpage>62</lpage><pub-id pub-id-type="pmid">32771905</pub-id></element-citation></ref><ref id="R9"><label>[9]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Baranowski</surname><given-names>Catherine</given-names></name><name><surname>Martin</surname><given-names>Hector Garcia</given-names></name><name><surname>Diego</surname><given-names>A Oyarzún</given-names></name><etal/></person-group><chapter-title>Can protein expression be ‘solved’?</chapter-title><source>Trends in Biotechnology</source><publisher-name>Elsevier</publisher-name><year>2025</year><month>June</month><pub-id pub-id-type="pmid">40461315</pub-id></element-citation></ref><ref id="R10"><label>[10]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nikolados</surname><given-names>Evangelos-Marios</given-names></name><name><surname>Diego</surname><given-names>A Oyarzún</given-names></name></person-group><article-title>Deep learning for optimization of protein expression</article-title><source>Current Opinion in Biotechnology</source><year>2023</year><pub-id pub-id-type="pmid">37087839</pub-id></element-citation></ref><ref id="R11"><label>[11]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simon</surname><given-names>Höllerer</given-names></name><name><surname>Papaxanthos</surname><given-names>Laetitia</given-names></name><name><surname>Gumpinger</surname><given-names>Anja Cathrin</given-names></name><etal/></person-group><article-title>Large-scale DNA-based phenotypic recording and deep learning enable highly accurate sequence-function mapping</article-title><source>Nature Communications</source><year>2020</year><month>December</month><volume>11</volume><issue>1</issue><fpage>3551</fpage><pub-id pub-id-type="doi">10.1038/s41467-020-17222-4</pub-id><pub-id pub-id-type="pmcid">PMC7363850</pub-id><pub-id pub-id-type="pmid">32669542</pub-id></element-citation></ref><ref id="R12"><label>[12]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaishnav</surname><given-names>Eeshit Dhaval</given-names></name><name><surname>de Boer</surname><given-names>Carl G</given-names></name><name><surname>Molinet</surname><given-names>Jennifer</given-names></name><etal/></person-group><article-title>The evolution, evolvability and engineering of gene regulatory DNA</article-title><source>Nature</source><year>2022</year><month>March</month><volume>603</volume><issue>7901</issue><fpage>455</fpage><lpage>463</lpage><pub-id pub-id-type="doi">10.1038/s41586-022-04506-6</pub-id><pub-id pub-id-type="pmcid">PMC8934302</pub-id><pub-id pub-id-type="pmid">35264797</pub-id></element-citation></ref><ref id="R13"><label>[13]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seo</surname><given-names>Euijin</given-names></name><name><surname>Choi</surname><given-names>Yun-Nam</given-names></name><name><surname>Shin</surname><given-names>Ye Rim</given-names></name><etal/></person-group><article-title>Design of synthetic promoters for cyanobacteria with generative deep-learning model</article-title><source>Nucleic Acids Research</source><year>2023</year><volume>51</volume><issue>13</issue><fpage>7071</fpage><lpage>7082</lpage><pub-id pub-id-type="doi">10.1093/nar/gkad451</pub-id><pub-id pub-id-type="pmcid">PMC10359606</pub-id><pub-id pub-id-type="pmid">37246641</pub-id></element-citation></ref><ref id="R14"><label>[14]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Almeida</surname><given-names>Bernardo P</given-names></name><name><surname>Reiter</surname><given-names>Franziska</given-names></name><name><surname>Pagani</surname><given-names>Michaela</given-names></name><name><surname>Stark</surname><given-names>Alexander</given-names></name></person-group><article-title>Deep-STARR predicts enhancer activity from DNA sequence and enables the de novo design of synthetic enhancers</article-title><source>Nature Genetics</source><year>2022</year><volume>54</volume><issue>5</issue><fpage>613</fpage><lpage>624</lpage><pub-id pub-id-type="pmid">35551305</pub-id></element-citation></ref><ref id="R15"><label>[15]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gosai</surname><given-names>Sager J</given-names></name><name><surname>Castro</surname><given-names>Rodrigo I</given-names></name><name><surname>Fuentes</surname><given-names>Natalia</given-names></name><etal/></person-group><article-title>Machine-guided design of cell-type-targeting cis-regulatory elements</article-title><source>Nature</source><year>2024</year><volume>634</volume><issue>8036</issue><fpage>1211</fpage><lpage>1220</lpage><pub-id pub-id-type="doi">10.1038/s41586-024-08070-z</pub-id><pub-id pub-id-type="pmcid">PMC11525185</pub-id><pub-id pub-id-type="pmid">39443793</pub-id></element-citation></ref><ref id="R16"><label>[16]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sample</surname><given-names>Paul J</given-names></name><name><surname>Wang</surname><given-names>Ban</given-names></name><name><surname>Reid</surname><given-names>David W</given-names></name><etal/></person-group><article-title>Human 5’ UTR design and variant effect prediction from a massively parallel translation assay</article-title><source>Nature Biotechnology</source><year>2019</year><volume>37</volume><issue>7</issue><fpage>803</fpage><lpage>809</lpage><pub-id pub-id-type="doi">10.1038/s41587-019-0164-5</pub-id><pub-id pub-id-type="pmcid">PMC7100133</pub-id><pub-id pub-id-type="pmid">31267113</pub-id></element-citation></ref><ref id="R17"><label>[17]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nikolados</surname><given-names>Evangelos-Marios</given-names></name><name><surname>Wongprommoon</surname><given-names>Arin</given-names></name><name><surname>Aodha</surname><given-names>Oisin Mac</given-names></name><etal/></person-group><article-title>Accuracy and data efficiency in deep learning models of protein expression</article-title><source>Nature Communications</source><year>2022</year><month>December</month><volume>13</volume><issue>1</issue><fpage>7755</fpage><pub-id pub-id-type="doi">10.1038/s41467-022-34902-5</pub-id><pub-id pub-id-type="pmcid">PMC9751117</pub-id><pub-id pub-id-type="pmid">36517468</pub-id></element-citation></ref><ref id="R18"><label>[18]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Linder</surname><given-names>Johannes</given-names></name><name><surname>Seelig</surname><given-names>Georg</given-names></name></person-group><article-title>Fast activation maximization for molecular sequence design</article-title><source>BMC Bioinformatics</source><year>2021</year><month>December</month><volume>22</volume><issue>1</issue><fpage>510</fpage><pub-id pub-id-type="doi">10.1186/s12859-021-04437-5</pub-id><pub-id pub-id-type="pmcid">PMC8527647</pub-id><pub-id pub-id-type="pmid">34670493</pub-id></element-citation></ref><ref id="R19"><label>[19]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sinai</surname><given-names>Sam</given-names></name><name><surname>Kelsic</surname><given-names>Eric D</given-names></name></person-group><article-title>A primer on model-guided exploration of fitness landscapes for biological sequence design</article-title><year>2020</year><month>October</month></element-citation></ref><ref id="R20"><label>[20]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Angenent-Mari</surname><given-names>Nicolaas M</given-names></name><name><surname>Garruss</surname><given-names>Alexander S</given-names></name><name><surname>Soenksen</surname><given-names>Luis R</given-names></name><etal/></person-group><article-title>A deep learning approach to programmable RNA switches</article-title><source>Nature Communications</source><year>2020</year><volume>11</volume><issue>1</issue><fpage>5057</fpage><pub-id pub-id-type="doi">10.1038/s41467-020-18677-1</pub-id><pub-id pub-id-type="pmcid">PMC7541447</pub-id><pub-id pub-id-type="pmid">33028812</pub-id></element-citation></ref><ref id="R21"><label>[21]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kotopka</surname><given-names>Benjamin J</given-names></name><name><surname>Smolke</surname><given-names>Christina D</given-names></name></person-group><article-title>Model-driven generation of artificial yeast promoters</article-title><source>Nature Communications</source><year>2020</year><month>December</month><volume>11</volume><issue>1</issue><fpage>2113</fpage><pub-id pub-id-type="doi">10.1038/s41467-020-15977-4</pub-id><pub-id pub-id-type="pmcid">PMC7192914</pub-id><pub-id pub-id-type="pmid">32355169</pub-id></element-citation></ref><ref id="R22"><label>[22]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Almeida</surname><given-names>Bernardo P</given-names></name><name><surname>Schaub</surname><given-names>Christoph</given-names></name><name><surname>Pagani</surname><given-names>Michaela</given-names></name><etal/></person-group><article-title>Targeted design of synthetic enhancers for selected tissues in the Drosophila embryo</article-title><source>Nature</source><year>2024</year><volume>626</volume><issue>7997</issue><fpage>207</fpage><lpage>211</lpage><pub-id pub-id-type="doi">10.1038/s41586-023-06905-9</pub-id><pub-id pub-id-type="pmcid">PMC10830412</pub-id><pub-id pub-id-type="pmid">38086418</pub-id></element-citation></ref><ref id="R23"><label>[23]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taskiran</surname><given-names>Ibrahim I</given-names></name><name><surname>Spanier</surname><given-names>Katina I</given-names></name><name><surname>Hannah</surname><given-names>Dickmänken</given-names></name><etal/></person-group><article-title>Cell-type-directed design of synthetic enhancers</article-title><source>Nature</source><year>2024</year><volume>626</volume><issue>7997</issue><fpage>212</fpage><lpage>220</lpage><pub-id pub-id-type="doi">10.1038/s41586-023-06936-2</pub-id><pub-id pub-id-type="pmcid">PMC10830415</pub-id><pub-id pub-id-type="pmid">38086419</pub-id></element-citation></ref><ref id="R24"><label>[24]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Angermueller</surname><given-names>Christof</given-names></name><name><surname>Belanger</surname><given-names>David</given-names></name><name><surname>Gane</surname><given-names>Andreea</given-names></name><etal/></person-group><chapter-title>Population-Based Black-Box Optimization for Biological Sequence Design</chapter-title><source>Proceedings of the 37th International Conference on Machine Learning</source><conf-name>PMLR</conf-name><year>2020</year><month>November</month><fpage>324</fpage><lpage>334</lpage></element-citation></ref><ref id="R25"><label>[25]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Linder</surname><given-names>Johannes</given-names></name><name><surname>Bogard</surname><given-names>Nicholas</given-names></name><name><surname>Rosenberg</surname><given-names>Alexander B</given-names></name><name><surname>Seelig</surname><given-names>Georg</given-names></name></person-group><article-title>A Generative Neural Network for Maximizing Fitness and Diversity of Synthetic DNA and Protein Sequences</article-title><source>Cell Systems</source><year>2020</year><month>July</month><volume>11</volume><issue>1</issue><fpage>49</fpage><lpage>62</lpage><elocation-id>e16</elocation-id><pub-id pub-id-type="doi">10.1016/j.cels.2020.05.007</pub-id><pub-id pub-id-type="pmcid">PMC8694568</pub-id><pub-id pub-id-type="pmid">32711843</pub-id></element-citation></ref><ref id="R26"><label>[26]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Killoran</surname><given-names>Nathan</given-names></name><name><surname>Lee</surname><given-names>Leo J</given-names></name><name><surname>Delong</surname><given-names>Andrew</given-names></name><etal/></person-group><article-title>Generating and designing DNA with deep generative models</article-title><year>2017</year><month>December</month></element-citation></ref><ref id="R27"><label>[27]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Settles</surname><given-names>Burr</given-names></name></person-group><article-title>Active Learning Literature Survey</article-title><source>University of Wisconsin-Madison Department of Computer Sciences</source><year>2009</year></element-citation></ref><ref id="R28"><label>[28]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thornton</surname><given-names>Ella Lucille</given-names></name><name><surname>Boyle</surname><given-names>Jeremy T</given-names></name><name><surname>Laohakunakorn</surname><given-names>Nadanai</given-names></name><name><surname>Regan</surname><given-names>Lynne</given-names></name></person-group><article-title>Cell-free protein synthesis as a method to rapidly screen machine learning-generated protease variants</article-title><source>ACS Synthetic Biology</source><year>2025</year><volume>14</volume><issue>25</issue><pub-id pub-id-type="doi">10.1021/acssynbio.5c00062</pub-id><pub-id pub-id-type="pmcid">PMC12090339</pub-id><pub-id pub-id-type="pmid">40304425</pub-id></element-citation></ref><ref id="R29"><label>[29]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>Jason</given-names></name><name><surname>Lal</surname><given-names>Ravi G</given-names></name><name><surname>Bowden</surname><given-names>James C</given-names></name><etal/></person-group><article-title>Active learning-assisted directed evolution</article-title><source>Nature Communications</source><year>2025</year><volume>16</volume><issue>1</issue><fpage>714</fpage><pub-id pub-id-type="doi">10.1038/s41467-025-55987-8</pub-id><pub-id pub-id-type="pmcid">PMC11739421</pub-id><pub-id pub-id-type="pmid">39821082</pub-id></element-citation></ref><ref id="R30"><label>[30]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reker</surname><given-names>Daniel</given-names></name><name><surname>Schneider</surname><given-names>Gisbert</given-names></name></person-group><article-title>Active-learning strategies in computer-assisted drug discovery</article-title><source>Drug Discovery Today</source><year>2015</year><month>April</month><volume>20</volume><issue>4</issue><fpage>458</fpage><lpage>465</lpage><pub-id pub-id-type="pmid">25499665</pub-id></element-citation></ref><ref id="R31"><label>[31]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raquel</surname><given-names>Rodríguez-Pérez</given-names></name><name><surname>Filip</surname><given-names>Miljković</given-names></name><name><surname>Bajorath</surname><given-names>Jürgen</given-names></name></person-group><article-title>Assessing the information content of structural and protein–ligand interaction representations for the classification of kinase inhibitor binding modes via machine learning and active learning</article-title><source>Journal of Cheminformatics</source><year>2020</year><month>May</month><volume>12</volume><issue>1</issue><fpage>36</fpage><pub-id pub-id-type="doi">10.1186/s13321-020-00434-7</pub-id><pub-id pub-id-type="pmcid">PMC7245824</pub-id><pub-id pub-id-type="pmid">33431025</pub-id></element-citation></ref><ref id="R32"><label>[32]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borkowski</surname><given-names>Olivier</given-names></name><name><surname>Koch</surname><given-names>Mathilde</given-names></name><name><surname>Zettor</surname><given-names>Agèes</given-names></name><etal/></person-group><article-title>Large scale active-learning-guided exploration for in vitro protein production optimization</article-title><source>Nature Communications</source><year>2020</year><month>April</month><volume>11</volume><issue>1</issue><fpage>1872</fpage><pub-id pub-id-type="doi">10.1038/s41467-020-15798-5</pub-id><pub-id pub-id-type="pmcid">PMC7170859</pub-id><pub-id pub-id-type="pmid">32312991</pub-id></element-citation></ref><ref id="R33"><label>[33]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albornoz</surname><given-names>Ricardo Valencia</given-names></name><name><surname>Diego</surname><given-names>A Oyarzún</given-names></name><name><surname>Burgess</surname><given-names>Karl</given-names></name></person-group><article-title>Optimisation of surfactin yield in Bacillus using data-efficient active learning and high-throughput mass spectrometry</article-title><source>Computational and Structural Biotechnology Journal</source><year>2024</year><month>December</month><volume>23</volume><fpage>1226</fpage><lpage>1233</lpage><pub-id pub-id-type="doi">10.1016/j.csbj.2024.02.012</pub-id><pub-id pub-id-type="pmcid">PMC10973723</pub-id><pub-id pub-id-type="pmid">38550972</pub-id></element-citation></ref><ref id="R34"><label>[34]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zournas</surname><given-names>Apostolos</given-names></name><name><surname>Incha</surname><given-names>Matthew R</given-names></name><name><surname>Radivojevic</surname><given-names>Tijana</given-names></name><etal/></person-group><article-title>Machine learning-led semi-automated medium optimization reveals salt as key for flaviolin production in Pseudomonas putida</article-title><source>Communications Biology</source><publisher-name>Nature Publishing Group</publisher-name><year>2025</year><month>April</month><volume>8</volume><issue>1</issue><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1038/s42003-025-08039-2</pub-id><pub-id pub-id-type="pmcid">PMC12008372</pub-id><pub-id pub-id-type="pmid">40251395</pub-id></element-citation></ref><ref id="R35"><label>[35]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>HamediRad</surname><given-names>Mohammad</given-names></name><name><surname>Chao</surname><given-names>Ran</given-names></name><name><surname>Weisberg</surname><given-names>Scott</given-names></name><etal/></person-group><article-title>Towards a fully automated algorithm driven platform for biosystems design</article-title><source>Nature Communications</source><year>2019</year><month>November</month><volume>10</volume><issue>1</issue><fpage>5150</fpage><pub-id pub-id-type="doi">10.1038/s41467-019-13189-z</pub-id><pub-id pub-id-type="pmcid">PMC6853954</pub-id><pub-id pub-id-type="pmid">31723141</pub-id></element-citation></ref><ref id="R36"><label>[36]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumar</surname><given-names>Prashant</given-names></name><name><surname>Adamczyk</surname><given-names>Paul A</given-names></name><name><surname>Zhang</surname><given-names>Xiaolin</given-names></name><etal/></person-group><article-title>Active and machine learning-based approaches to rapidly enhance microbial chemical production</article-title><source>Metabolic Engineering</source><year>2021</year><month>September</month><volume>67</volume><fpage>216</fpage><lpage>226</lpage><pub-id pub-id-type="pmid">34229079</pub-id></element-citation></ref><ref id="R37"><label>[37]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pandi</surname><given-names>Amir</given-names></name><name><surname>Diehl</surname><given-names>Christoph</given-names></name><name><surname>Kharrazi</surname><given-names>Ali Yazdizadeh</given-names></name><etal/></person-group><article-title>A versatile active learning workflow for optimization of genetic and metabolic networks</article-title><source>Nature Communications</source><year>2022</year><month>July</month><volume>13</volume><issue>1</issue><fpage>3876</fpage><pub-id pub-id-type="doi">10.1038/s41467-022-31245-z</pub-id><pub-id pub-id-type="pmcid">PMC9256728</pub-id><pub-id pub-id-type="pmid">35790733</pub-id></element-citation></ref><ref id="R38"><label>[38]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kauffman</surname><given-names>Stuart A</given-names></name><name><surname>Weinberger</surname><given-names>Edward D</given-names></name></person-group><article-title>The nk model of rugged fitness landscapes and its application to maturation of the immune response</article-title><source>Journal of theoretical biology</source><year>1989</year><volume>141</volume><issue>2</issue><fpage>211</fpage><lpage>245</lpage><pub-id pub-id-type="pmid">2632988</pub-id></element-citation></ref><ref id="R39"><label>[39]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Obolski</surname><given-names>Uri</given-names></name><name><surname>Ram</surname><given-names>Yoav</given-names></name><name><surname>Hadany</surname><given-names>Lilach</given-names></name></person-group><article-title>Key issues review: evolution on rugged adaptive landscapes</article-title><source>Reports on Progress in Physics</source><year>2017</year><volume>81</volume><issue>1</issue><elocation-id>012602</elocation-id><pub-id pub-id-type="pmid">29051394</pub-id></element-citation></ref><ref id="R40"><label>[40]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Yajie</given-names></name><name><surname>Xue</surname><given-names>Pu</given-names></name><name><surname>Cao</surname><given-names>Mingfeng</given-names></name><etal/></person-group><article-title>Directed evolution: methodologies and applications</article-title><source>Chemical reviews</source><year>2021</year><volume>121</volume><issue>20</issue><fpage>12384</fpage><lpage>12444</lpage><pub-id pub-id-type="pmid">34297541</pub-id></element-citation></ref><ref id="R41"><label>[41]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>Richard S</given-names></name><name><surname>Barto</surname><given-names>Andrew G</given-names></name></person-group><chapter-title>Reinforcement Learning: An Introduction</chapter-title><source>Adaptive Computation and Machine Learning</source><publisher-name>MIT Press</publisher-name><publisher-loc>Cambridge, Mass</publisher-loc><year>1998</year></element-citation></ref><ref id="R42"><label>[42]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Boer</surname><given-names>Carl G</given-names></name><name><surname>Hughes</surname><given-names>Timothy R</given-names></name></person-group><article-title>Yetfasco: a database of evaluated yeast transcription factor sequence specificities</article-title><source>Nucleic acids research</source><year>2012</year><volume>40</volume><issue>D1</issue><fpage>D169</fpage><lpage>D179</lpage><pub-id pub-id-type="doi">10.1093/nar/gkr993</pub-id><pub-id pub-id-type="pmcid">PMC3245003</pub-id><pub-id pub-id-type="pmid">22102575</pub-id></element-citation></ref><ref id="R43"><label>[43]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Boer</surname><given-names>Carl G</given-names></name><name><surname>Vaishnav</surname><given-names>Eeshit Dhaval</given-names></name><name><surname>Sadeh</surname><given-names>Ronen</given-names></name><etal/></person-group><article-title>Deciphering eukaryotic gene-regulatory logic with 100 million random promoters</article-title><source>Nature biotechnology</source><year>2020</year><volume>38</volume><issue>1</issue><fpage>56</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1038/s41587-019-0315-8</pub-id><pub-id pub-id-type="pmcid">PMC6954276</pub-id><pub-id pub-id-type="pmid">31792407</pub-id></element-citation></ref><ref id="R44"><label>[44]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>Yuxin</given-names></name><name><surname>Kudla</surname><given-names>Grzegorz</given-names></name><name><surname>Diego</surname><given-names>A Oyarzún</given-names></name></person-group><article-title>Improving the generalization of protein expression models with mechanistic sequence information</article-title><source>Nucleic Acids Research</source><year>2025</year><volume>53</volume><issue>3</issue><elocation-id>gkaf020</elocation-id><pub-id pub-id-type="doi">10.1093/nar/gkaf020</pub-id><pub-id pub-id-type="pmcid">PMC11773361</pub-id><pub-id pub-id-type="pmid">39873269</pub-id></element-citation></ref><ref id="R45"><label>[45]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fleur</surname><given-names>Alyssa La</given-names></name><name><surname>Shi</surname><given-names>Yongsheng</given-names></name><name><surname>Seelig</surname><given-names>Georg</given-names></name></person-group><article-title>Decoding biology with massively parallel reporter assays and machine learning</article-title><source>Genes &amp; Development</source><year>2024</year><volume>38</volume><issue>17-20</issue><fpage>843</fpage><lpage>865</lpage><pub-id pub-id-type="doi">10.1101/gad.351800.124</pub-id><pub-id pub-id-type="pmcid">PMC11535156</pub-id><pub-id pub-id-type="pmid">39362779</pub-id></element-citation></ref><ref id="R46"><label>[46]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zahm</surname><given-names>Adam M</given-names></name><name><surname>Owens</surname><given-names>William S</given-names></name><name><surname>Himes</surname><given-names>Samuel R</given-names></name><etal/></person-group><article-title>A massively parallel reporter assay library to screen short synthetic promoters in mammalian cells</article-title><source>Nature Communications</source><publisher-name>Nature Publishing Group</publisher-name><year>2024</year><month>November</month><volume>15</volume><issue>1</issue><elocation-id>10353</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-024-54502-9</pub-id><pub-id pub-id-type="pmcid">PMC11604768</pub-id><pub-id pub-id-type="pmid">39609378</pub-id></element-citation></ref><ref id="R47"><label>[47]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Boer</surname><given-names>Carl G</given-names></name><name><surname>Vaishnav</surname><given-names>Eeshit Dhaval</given-names></name><name><surname>Sadeh</surname><given-names>Ronen</given-names></name><etal/></person-group><article-title>Deciphering eukaryotic gene-regulatory logic with 100 million random promoters</article-title><source>Nature Biotechnology</source><year>2020</year><month>January</month><volume>38</volume><issue>1</issue><fpage>56</fpage><lpage>65</lpage></element-citation></ref><ref id="R48"><label>[48]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilliot</surname><given-names>Pierre-Aurélien</given-names></name><name><surname>Gorochowski</surname><given-names>Thomas E</given-names></name></person-group><article-title>Transfer learning for cross-context prediction of protein expression from 5’UTR sequence</article-title><source>Nucleic Acids Research</source><year>2024</year><elocation-id>gkae491</elocation-id><pub-id pub-id-type="doi">10.1093/nar/gkae491</pub-id><pub-id pub-id-type="pmcid">PMC11260469</pub-id><pub-id pub-id-type="pmid">38864396</pub-id></element-citation></ref><ref id="R49"><label>[49]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Merzbacher</surname><given-names>Charlotte</given-names></name><name><surname>Aodha</surname><given-names>Oisin Mac</given-names></name><name><surname>Diego</surname><given-names>A Oyarzún</given-names></name></person-group><article-title>Bayesian Optimization for Design of Multiscale Biological Circuits</article-title><source>ACS Synthetic Biology</source><publisher-name>American Chemical Society</publisher-name><year>2023</year><month>July</month><volume>12</volume><issue>7</issue><fpage>2073</fpage><lpage>2082</lpage><pub-id pub-id-type="doi">10.1021/acssynbio.3c00120</pub-id><pub-id pub-id-type="pmcid">PMC10367132</pub-id><pub-id pub-id-type="pmid">37339382</pub-id></element-citation></ref><ref id="R50"><label>[50]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barazandeh</surname><given-names>Sina</given-names></name><name><surname>Ozden</surname><given-names>Furkan</given-names></name><name><surname>Hincer</surname><given-names>Ahmet</given-names></name><etal/></person-group><article-title>Learning to Generate 5’ UTR Sequences for Optimized Ribosome Load and Gene Expression</article-title><year>2023</year><month>February</month><pub-id pub-id-type="doi">10.1093/bioadv/vbaf134</pub-id><pub-id pub-id-type="pmcid">PMC12228966</pub-id><pub-id pub-id-type="pmid">40621603</pub-id></element-citation></ref><ref id="R51"><label>[51]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rafi</surname><given-names>Abdul Muntakim</given-names></name><name><surname>Nogina</surname><given-names>Daria</given-names></name><name><surname>Penzar</surname><given-names>Dmitry</given-names></name><etal/></person-group><article-title>A community effort to optimize sequence-based deep learning models of gene regulation</article-title><source>Nature Biotechnology</source><publisher-name>Nature Publishing Group</publisher-name><year>2024</year><month>October</month><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="pmid">39394483</pub-id></element-citation></ref><ref id="R52"><label>[52]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mater</surname><given-names>Adam C</given-names></name><name><surname>Sandhu</surname><given-names>Mahakaran</given-names></name><name><surname>Jackson</surname><given-names>Colin</given-names></name></person-group><article-title>The nk landscape as a versatile benchmark for machine learning driven protein engineering</article-title><source>bioRxiv</source><year>2020</year><elocation-id>2020-09</elocation-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Active learning and modelling of a synthetic genotype-phenotype landscape.</title><p>(<bold>A</bold>) Schematic of traditional one-shot sequence optimization and active learning. In active learning, experimental screening of optimized sequences is built into the optimization loop and employed to iteratively augment the training data. (<bold>B</bold>) NK model for fitness landscapes; <italic>N</italic> represents the DNA sequence length and <italic>K</italic> defines the number of positions that have epistatic interactions with a given position in the sequence. The value of <italic>K</italic> controls the ruggedness of the expression landscape. (<bold>C</bold>) Two dimensional t-SNE projections of the full 10nt DNA sequence space represented via one-hot encodings (1,048,576 sequences), labeled by their NK-predicted expression; t-SNE parameters no. neighbors=15 and min. distance=0.1. (<bold>D</bold>) Regression performance of sequence-to-expression MLP models trained on the NK synthetic data, before and after hyperparameter optimization of hidden layer size with grid search. Plots show the coefficient of determination (<italic>R</italic><sup>2</sup>) between model predictions and simulated ground truth on a held out test set (2,000 sequences); the predictor is a feedforward neural network (MLP) trained on 2,000 sequences obtained via Latin hypercube sampling of the full sequence space. Error bars denoted the standard deviation of <italic>R</italic><sup>2</sup> across five random test sets of 2,000 sequences each.</p></caption><graphic xlink:href="EMS207044-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Optimization of NK genotype-phenotype landscape with active learning.</title><p>(<bold>A</bold>) Optimized expression levels using active learning with two sampling strategies (random, directed evolution). Bars show average expression of final batch of 100 optimized sequences after four active learning loops, across NK landscapes of increasing ruggedness. (<bold>B</bold>) Two-dimensional t-SNE representation of the final batch of optimized sequences (black) against the ground truth expression levels (as in <xref ref-type="fig" rid="F1">Figure 1C</xref>) for the NK0 landscape using directed evolution for sampling. (<bold>C</bold>) Expression levels for each batch of 100 optimized sequences across the active learning loops with active learning for sampling, and using different values of the exploration–exploitation parameter <italic>α</italic> in the Upper Confidence Bound reward function in Eq. (1). (<bold>D</bold>) Comparison between and active learning and one-shot optimization in NK landscapes of increased ruggedness. Plots show average optimized expression per batch in each active learning loop using directed evolution for sampling, against several one-shot optimizers (strong-selection weak-mutation, SSWM; random sampling, RS; gradient descent, GD) ran on a multilayer perceptron (MLP) regressor. For fair comparison, the MLP was retrained on the same number of sequences as those employed for the active learning loop (<italic>N</italic>, one-shot). In all plots, dots and whiskers represent the mean and standard error across three replicates with resampled initial training set of 1,000 sequences, one drawn from Latin Hypercube Sampling and two from uniform sampling.</p></caption><graphic xlink:href="EMS207044-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Active learning of promoter sequences in <italic>Saccharomyces cerevisiae</italic>.</title><p>(<bold>A</bold>) We employed a large promoter dataset from Vaishnav <italic>et al</italic>, including more than 20M fully randomized promoter sequences measured in two growth media. Transformer-based models can regress the expression landscape with high accuracy (Person <italic>r</italic>&gt; 0.95 in both growth media [<xref ref-type="bibr" rid="R12">12</xref>]). (<bold>B</bold>) Promoter optimization with different batches of initial data; inset shows two subsamples of <italic>n</italic> = 1, 000 promoters with uniform (blue) and low expression (orange); as in <xref ref-type="fig" rid="F2">Figure 2D</xref>, plots show optimized expression levels across the active learning loops, compared to three one-shot optimizers on MLP regressors trained on the same number of sequences. Low expression samples were obtained by first sampling 100,000 sequences, followed by selection of sequences with lowest expression (see Methods). Dots represent the mean and whiskers show the standard error across three replicates with resampled initial training sets as in <xref ref-type="fig" rid="F2">Figure 2D</xref>. (<bold>C</bold>) Impact of noise on active learning performance. We introduced Gaussian noise into the starting batch of measured expression levels, with zero mean and a standard deviation of 5% (orange, small noise) and 10% (green, large noise) of the ground truth value of each sequence for modelling heteroscedasticity. Dots represent the mean and whiskers show the standard error across three replicates with resampled initial training sets, as in <xref ref-type="fig" rid="F2">Figure 2D</xref>. (<bold>D</bold>) Active learning of promoter sequences across growth conditions. We initialized the active learning loop with <italic>n</italic>=1,000 sequences measured in one medium, and ran the active learning optimization using iterative collection of data in a different medium. The use of sequence pre-optimization in the original medium can lead to substantial performance improvement. All active learning results employ directed evolution for sampling promoter sequences.</p></caption><graphic xlink:href="EMS207044-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Improving active learning performance with alternative methods for sequence sampling and selection.</title><p>(<bold>A</bold>) Two additional sequence sampling methods: genetic drift and recombination. (<bold>B</bold>) Comparison of active learning performance for different sequence sampling methods, initialized with a Latin Hypercube Sampling batch. (<bold>C</bold>) Enrichment of promoter motifs across the active learning loops for different sequence sampling strategies. Shown are average position frequency matrices (PFM) scores of 244 motifs [<xref ref-type="bibr" rid="R42">42</xref>] for each batch across the active learning loops; values were normalized to the PFM scores of the initial batch. (<bold>D</bold>) Comparison of active learning with directed evolution for sampling and selection based on a reward function with and without motif weighting, as in Eq. (2); error bars denote repeats across three initial batch of promoters.</p></caption><graphic xlink:href="EMS207044-f004"/></fig></floats-group></article>