<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS206052</article-id><article-id pub-id-type="doi">10.1101/2025.05.28.656584</article-id><article-id pub-id-type="archive">PPR1027934</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">2</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Memory by a thousand rules: Automated discovery of multi-type plasticity rules reveals variety &amp; degeneracy at the heart of learning</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Confavreux</surname><given-names>Basile</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Harrington</surname><given-names>Zoe P. M.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Kania</surname><given-names>Maciej</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Ramesh</surname><given-names>Poornima</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Krouglova</surname><given-names>Anastasia N.</given-names></name><xref ref-type="aff" rid="A5">5</xref><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Bozelos</surname><given-names>Panos A.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Macke</surname><given-names>Jakob H.</given-names></name><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Saxe</surname><given-names>Andrew M.</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Gonçalves</surname><given-names>Pedro J.</given-names></name><xref ref-type="aff" rid="A5">5</xref><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Vogels</surname><given-names>Tim P.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib></contrib-group><aff id="A1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03gnh5541</institution-id><institution>Institute of Science and Technology Austria</institution></institution-wrap>, <city>Klosterneuburg</city>, <country country="AT">Austria</country></aff><aff id="A2"><label>2</label>Gatsby Computational Neuroscience Unit, London, UK</aff><aff id="A3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03a1kwz48</institution-id><institution>University of Tübingen</institution></institution-wrap>, <city>Tübingen</city>, <country country="DE">Germany</country></aff><aff id="A4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04fq9j139</institution-id><institution>Max Planck Institute for Intelligent Systems</institution></institution-wrap>, <city>Tübingen</city>, <country country="DE">Germany</country></aff><aff id="A5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03xrhmk39</institution-id><institution>VIB</institution></institution-wrap>-<institution-wrap><institution-id institution-id-type="ror">https://ror.org/04sc67422</institution-id><institution>Neuroelectronics Research Flanders (NERF)</institution></institution-wrap>, <country country="BE">Belgium</country></aff><aff id="A6"><label>6</label>Departments of Computer Science and Electrical Engineering, KU Leuven, Belgium</aff><pub-date pub-type="nihms-submitted"><day>31</day><month>05</month><year>2025</year></pub-date><pub-date pub-type="preprint"><day>29</day><month>05</month><year>2025</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Synaptic plasticity is the basis of learning and memory, but the link between synaptic changes and neural function remains elusive. Here, we used automated search algorithms to obtain thousands of strikingly diverse quadruplets of excitatory(E)-to-E, E-to-inhibitory(I), IE, and II plasticity rules, cooperating to stabilize recurrent spiking networks. Despite the fact that quadruplets were selected for homeostasis, more than 90% of them performed well in simple and more difficult memory tasks such as novelty detection, contextual novelty and sequence replay. Co-activity was crucial, i.e., most rules failed in isolation. Our purely local, unsupervised plasticity rules could also help solve computer games such as pong. Our work showcases automated discovery augmenting human intuition to find en masse solutions for high dimensional problems.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><label>I</label><title>Introduction</title><p id="P2">How do millions of heterogeneous synapses coordinate to form memories? Direct measurements of multiple synapses remain most difficult<sup><xref ref-type="bibr" rid="R1">1</xref>,<xref ref-type="bibr" rid="R2">2</xref></sup>, and theoretical work has focused on single-rule models<sup><xref ref-type="bibr" rid="R3">3</xref>–<xref ref-type="bibr" rid="R8">8</xref>, c.f.<xref ref-type="bibr" rid="R9">9</xref>–<xref ref-type="bibr" rid="R12">12</xref></sup>, leaving the joint impact of multiple concurrent rules largely uncharted. Indeed, synaptic plasticity rules—typically expressed as weight changes dependent on pre- and post-synaptic activity—are difficult to tune by hand<sup><xref ref-type="bibr" rid="R9">9</xref>,<xref ref-type="bibr" rid="R10">10</xref>,<xref ref-type="bibr" rid="R13">13</xref></sup>, as they comprise many parameters and their solution spaces become unruly very quickly. Meta-learning approaches, in which plasticity rules are generated and evaluated by automated methods, have been gaining traction to explore more systematically multiple coactive rules<sup><xref ref-type="bibr" rid="R12">12</xref>,<xref ref-type="bibr" rid="R14">14</xref>–<xref ref-type="bibr" rid="R26">26</xref></sup>, but only for small networks or simple functions. Here, we used simulation-based inference<sup><xref ref-type="bibr" rid="R12">12</xref>,<xref ref-type="bibr" rid="R27">27</xref></sup> to meta-learn thousands of co-active quadruplets of plasticity rules that produce stable and biologically plausible network activity. Strikingly, most of these rule quadruplets also supported various forms of memory acquisition when stimulated accordingly. Without fine-tuning, we could identify families of rule quadruplets for various experimentally observed behaviors such as familiarity—or novelty—detection, contextual novelty, and replay, with memory lifetimes from seconds to hours. In some, but not all cases, rule quadruplets with similar network functions had notable trends in their plasticity parameters, hinting at what features of each learning rule bestowed a particular function. We also found that most individual rules were unstable in isolation, and only functioned successfully as part of their quadruplet, providing a possible explanation for why plasticity is so difficult to probe experimentally. Finally, we showed how these unsupervised, co-active rules could be a stepping stone towards more complex computations, such as helping a computer model play the game of pong.</p></sec><sec id="S2" sec-type="results"><label>II</label><title>Results</title><sec id="S3"><title>Meta-learning co-active plasticity rules at scale</title><p id="P3">Recent work uncovered a space of co-active EE, EI, IE and II plasticity rule quadruplets which could produce stable and robust activity for a few minutes in recurrent spiking networks of excitatory and inhibitory neurons<sup><xref ref-type="bibr" rid="R12">12</xref></sup> (<xref ref-type="fig" rid="F1">Fig. 1A</xref>). We wondered if these rules could support neural functions beyond stability. To investigate, we considered such networks in which synapses from neuron <italic>i</italic> to <italic>j</italic> underwent spike-timing dependent plasticity, STDP<sup><xref ref-type="bibr" rid="R1">1</xref>,<xref ref-type="bibr" rid="R3">3</xref></sup>, with additional pre- or post-only updates<sup><xref ref-type="bibr" rid="R12">12</xref></sup>: <disp-formula id="FD1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mtext>d</mml:mtext><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mtext>XY</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mtext>XY</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mtext>XY</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mspace width="14.0em"/><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mtext>XY</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula> where <italic>η</italic> is the learning rate and variables <italic>x</italic><sub><italic>i</italic></sub>(<italic>t</italic>) and <italic>x</italic><sub><italic>j</italic></sub>(<italic>t</italic>) are low-pass filters of the pre- and post-synaptic spike trains with time constants <inline-formula><mml:math id="M2"><mml:mrow><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>XY</mml:mtext></mml:mrow><mml:mrow><mml:mtext>pre</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>XY</mml:mtext></mml:mrow><mml:mrow><mml:mtext>post</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, with (X, Y) ∈ {E, I}. Plasticity updates at each synapse type depended on 6 parameters <inline-formula><mml:math id="M3"><mml:mrow><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>XY</mml:mtext></mml:mrow><mml:mrow><mml:mtext>pre</mml:mtext></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>XY</mml:mtext></mml:mrow><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, resulting in 24 plasticity parameters across the four synapse types. We visualized plasticity rule quadruplets using a classic representation of weight changes as a function of the time lag between a pre- and a post-synaptic spike<sup><xref ref-type="bibr" rid="R1">1</xref>–<xref ref-type="bibr" rid="R3">3</xref></sup> (<xref ref-type="fig" rid="F1">Fig. 1B</xref>).</p><p id="P4">To meta-learn plasticity rules, we combined filter-based simulation-based inference, fSBI<sup><xref ref-type="bibr" rid="R12">12</xref></sup>, with a multi-fidelity simulation-based inference approach<sup><xref ref-type="bibr" rid="R27">27</xref></sup>. This combination of methods dramatically reduced the compute required to explore the 24-dimensional plasticity parameter space and allowed us to numerically infer a manifold of varied co-active learning rules with hours-long stability, the stability manifold (<xref ref-type="fig" rid="F1">Fig. 1C</xref> and <xref ref-type="supplementary-material" rid="SD1">Fig. Supp. 1</xref>).</p></sec><sec id="S4"><title>Memory as a byproduct of network stabilization</title><p id="P5">We sampled 2,500 rule quadruplets from the stability manifold and embedded them in networks performing a range of memory tasks. We began with a familiarity detection task that consisted of a single 10s high-frequency input stimulus to a subset of excitatory neurons, followed by a break period—during which the network received only background inputs—ranging from 1s to 4h (<xref ref-type="fig" rid="F1">Fig. 1D</xref>, left). After the break, we compared the network response to the familiar stimulus and to a novel stimulus (<xref ref-type="fig" rid="F1">Fig. 1D</xref>, middle).</p><p id="P6">Immediately after the initial stimulus, we observed significant differences (Student t-test, <italic>p &lt;</italic> 0.05) between the response to novel and familiar stimuli for 94% of the sampled quadruplets, i.e., a memory of the initial stimulus was expressed. Memory lifetime, the time until a memory could not be recalled with significant firing rate differences, varied across rule quadruplets. After a 4h break, the fraction of networks still expressing a memory dropped to 7% (<xref ref-type="fig" rid="F1">Fig. 1D</xref>).</p><p id="P7">When considering only transient responses to stimulus onset and offset (so-called ”ON” or ”OFF” responses<sup><xref ref-type="bibr" rid="R28">28</xref></sup>), we found that some quadruplets created networks with rich transient dynamics that were absent from the naïve networks (<xref ref-type="supplementary-material" rid="SD1">Fig. Supp. 5</xref>). In some cases, these transient responses were significantly different for novel and familiar stimuli, and could serve as a readout signal for familiarity or novelty (<xref ref-type="fig" rid="F1">Fig. 1E</xref>).</p><p id="P8">We also played looping sequences of 5 different inputs that stimulated 5 non-overlapping groups of excitatory neurons in the network (<xref ref-type="fig" rid="F1">Fig. 1F</xref>). After the break period, we tested if the presentation of any one stimulus would elicit ”successor representations” in the other, non-stimulated groups. 98% of meta-learned rules produced various signatures of sequence learning after 10s, gradually dropping to 3% after 4h (<xref ref-type="fig" rid="F1">Fig. 1F</xref>).</p><p id="P9">We tested for contextual novelty—also called omission novelty<sup><xref ref-type="bibr" rid="R29">29</xref></sup>, or error prediction in predictive coding frameworks<sup><xref ref-type="bibr" rid="R30">30</xref></sup>—by presenting a set of stimuli out of order compared to the training sequence (<xref ref-type="fig" rid="F1">Fig. 1G</xref>). This task was more difficult than regular familiarity detection as all test stimuli were encountered during training, only their ordering during the test phase was novel or familiar. 23% of meta-learned rules produced networks that could detect this more subtle form of novelty after 10s, and memories were more short-lived across the board (<xref ref-type="fig" rid="F1">Fig. 1G</xref>).</p><p id="P10">Our meta-learning approach generated thousands of co-active, unsupervised rule quadruplets producing complex network functions despite having been selected solely for network stability. Only one of the 2,500 quadruplets sampled from the stability manifold did not respond to any memory task according to our predefined metrics (<xref ref-type="sec" rid="S9">Methods</xref>, <xref ref-type="fig" rid="F1">Fig. 1H</xref>).</p></sec><sec id="S5"><title>Experimental predictions</title><p id="P11">To discover plasticity rules consistent with more specific experimental observations, we zoomed in on sub-regions of the stability manifold by performing additional rounds of inference (see <xref ref-type="sec" rid="S9">Methods</xref>). We chose several published datasets for which the underlying plasticity rules are unknown, or have been subject to previous theoretical predictions<sup><xref ref-type="bibr" rid="R2">2</xref>,<xref ref-type="bibr" rid="R29">29</xref>,<xref ref-type="bibr" rid="R31">31</xref>–<xref ref-type="bibr" rid="R35">35</xref></sup>.</p><p id="P12">We first focused on two subfamilies of rule quadruplets that reproduced the trends observed in two experimental studies on novelty<sup><xref ref-type="bibr" rid="R32">32</xref></sup>, and familiarity detection<sup><xref ref-type="bibr" rid="R31">31</xref></sup> (<xref ref-type="fig" rid="F2">Fig. 2B-D</xref>). For novelty detection, we considered rule quadruplets as plausible if they robustly produced stronger network responses for novel stimuli than for familiar stimuli, with a memory lifetime of at least 4h. Plausible novelty-detecting quadruplets revealed a striking diversity of shapes, i.e. a number of degenerate solutions (<xref ref-type="fig" rid="F2">Fig. 2B</xref>). In contrast, familiarity detection was much less common within the stability manifold (as suggested by <xref ref-type="fig" rid="F1">Fig. 1D</xref>). Interestingly, all quadruplets which satisfied the criteria of familiarity detection switched from novelty detection early in the simulation (<xref ref-type="fig" rid="F2">Fig. 2B</xref>). No obvious commonalities could be observed in the shapes of all plausible familiar-detecting quadruplets.</p><p id="P13">Plausible quadruplets for contextual novelty<sup><xref ref-type="bibr" rid="R29">29</xref></sup> appeared to constrain the shape of the EE rule to symmetric anti-Hebbian rules, similar to what has been observed experimentally<sup><xref ref-type="bibr" rid="R36">36</xref>,<xref ref-type="bibr" rid="R37">37</xref></sup> (<xref ref-type="fig" rid="F2">Fig. 2F</xref>). Note however, that many stable quadruplets with symmetric anti-Hebbian EE rule did not elicit contextual novelty detection.</p><p id="P14">A few, rare quadruplets (&lt; 0.1%, <xref ref-type="fig" rid="F2">Fig. 2K</xref>) could imprint sequential memories into a network. We expanded on this subregion of the stability manifold to obtain two classes of solutions. Some quadruplets created networks with spontaneous or triggered, forward or backward sequential dynamics during the recall phase (<xref ref-type="fig" rid="F2">Fig. 2G</xref>), reminiscent of hippocampal replay<sup><xref ref-type="bibr" rid="R33">33</xref>,<xref ref-type="bibr" rid="R35">35</xref></sup>. Other quadruplets could recall the training sequence without any sequential, replay information (<xref ref-type="fig" rid="F2">Fig. 2H</xref>, ”coincidental” replay). Quadruplets from each class could be distinguished by their shapes: coincidental quadruplets comprised mostly asymmetric EE rules—reminiscent of STDP<sup><xref ref-type="bibr" rid="R2">2</xref></sup>—and asymmetric IE rules with pre-before-post potentiation (<xref ref-type="fig" rid="F2">Fig. 2H</xref>). Conversely, replay rules consisted mostly of symmetric anti-Hebbian EE and asymmetric IE rules, in this case with post-before-pre potentiation (<xref ref-type="fig" rid="F2">Fig. 2G</xref>).</p><p id="P15">Meta-learned rules could also be compared directly with published data from <italic>ex vivo</italic> patch clamp recordings. For example, the oft-assumed unstable<sup><xref ref-type="bibr" rid="R13">13</xref>,<xref ref-type="bibr" rid="R38">38</xref></sup> STDP EE rule observed in cortex<sup><xref ref-type="bibr" rid="R1">1</xref>,<xref ref-type="bibr" rid="R2">2</xref></sup> appeared stable in many quadruplet configurations (<xref ref-type="fig" rid="F2">Fig. 2I</xref>), suggesting that stable Hebbian learning may have to depend on co-activity of plasticity rules across different synapse types.</p></sec><sec id="S6"><title>Illuminating plasticity-function relationships</title><p id="P16">We wanted to further investigate the inter-dependencies between co-active plasticity rules. We had observed that stable rule quadruplets could take a plethora of shapes (<xref ref-type="fig" rid="F3">Fig. 3A</xref>), with no clear, linear, low-dimensional macro structure in the parameter space (<xref ref-type="fig" rid="F3">Fig. 3B</xref>, and Supplementary Material).</p><p id="P17">To visualize the micro structure of the stability manifold, we sampled rules from a plane with two stable quadruplets of dissimilar shape and functions, one quadruplet elicited transient replay (<xref ref-type="fig" rid="F3">Fig. 3G</xref>), the other detected novelty for at least 4 hours (<xref ref-type="fig" rid="F3">Fig. 3B</xref>). Our analysis revealed a likely-continuous, non-linear manifold surface of stable rule quadruplets, with smooth functional variations (<xref ref-type="fig" rid="F3">Fig. 3C</xref>).</p><p id="P18">Next, we studied how individual rules contributed to the stability of a quadruplet. We compared the network dynamics and function of single rules in isolation (“triple KO”), or rule triplets with one deactivated rule in the quadruplet (“single KO”, see <xref ref-type="sec" rid="S9">Methods</xref> and <xref ref-type="fig" rid="F3">Fig. 3D</xref>). With the exception of many isolated EE rules, the majority of isolated rules proved unstable (<xref ref-type="fig" rid="F1">Fig. 1D</xref>). Curiously, removing an unstable rule from a quadruplet often destabilized the remaining triplet of rules. For instance, many rule triplets without active (intrinsically unstable) IE or II plasticity failed to support stable network dynamics (<xref ref-type="fig" rid="F3">Fig. 3D</xref>). Further, stable triplets and singlets exhibited altered and typically diminished function (<xref ref-type="supplementary-material" rid="SD1">Fig. Supp. 10</xref>). Our findings suggest that unstable rules can be “rescued”, and even utilized by co-activity, thereby broadening the space of stable parameter combinations compared to single rule cases.</p><p id="P19">Having established that the stability of rule quadruplets relied on complex parameter inter-dependencies across and within synapse types, we wondered whether mean-field theory—a rate-based description of spiking plasticity rules between independent neurons<sup><xref ref-type="bibr" rid="R39">39</xref></sup>— was able to capture these observations. We devised a perturbation to the rule quadruplets that would not affect their mean-field description: swapping the Hebbian parameters of one individual rule at a time: <inline-formula><mml:math id="M4"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mtext>XY</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>XY</mml:mtext></mml:mrow><mml:mrow><mml:mtext>post</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>↔</mml:mo><mml:mspace width="0.2em"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mtext>XY</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>XY</mml:mtext></mml:mrow><mml:mrow><mml:mtext>pre</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (see <xref ref-type="supplementary-material" rid="SD1">Fig. Supp. 12</xref>). This corresponds to swapping the response to pre-post vs. post-pre spike pairs. We found that such swaps destabilized the quadruplets around 40% of the time, typically because the weights of the affected rule would grow out of physiologically reasonable bounds within the 4h of simulation (<xref ref-type="fig" rid="F3">Fig. 3E</xref>). We concluded that the simplified rate description of our quadruplets was not sufficient to predict stability.</p><p id="P20">Similarly to swapping terms that did not change a rule’s mean-field description, we could also swap non-Hebbian parameters <italic>α</italic> and <italic>β</italic> to explicitly change the mean-field description, but not the shape of the rule with regards to the pre-post protocol. Such swaps destabilized over 95% of tested quadruplets (<xref ref-type="fig" rid="F3">Fig. 3F</xref>), highlighting the insufficiency of the pre-post protocol representation, as well.</p><p id="P21">We also tested the robustness of our quadruplets with regard to the parameters of the networks in which they acted (<xref ref-type="fig" rid="F3">Fig. 3G</xref>). Most simulation parameters, such as stimulus strength and weight initialization (<xref ref-type="fig" rid="F2">Fig. 2G</xref>) did not affect stability or function. Some, e.g., background drive, impacted the performance of most rules, sometimes in interesting ways (<xref ref-type="fig" rid="F4">Fig. 4A</xref>). For example, one of our quadruplets only memorized familiar stimuli that were presented with low background rates, but not with high rates (<xref ref-type="fig" rid="F4">Fig. 4A</xref>), reminiscent of previously reported effects of neuro-modulation on memory acquisition<sup><xref ref-type="bibr" rid="R40">40</xref>,<xref ref-type="bibr" rid="R41">41</xref></sup>.</p></sec><sec id="S7"><title>More complex tasks and rules</title><p id="P22">The rules quadruplets we have meta-learned act purely locally. We wondered whether such local plasticity could help with learning supervised tasks. Towards this goal, we added readout units to the spiking network, whose inputs were trained using supervised learning<sup><xref ref-type="bibr" rid="R42">42</xref>,<xref ref-type="bibr" rid="R43">43</xref></sup>. We then assessed if the readout units performed better in combination with plastic (vs. static) upstream networks. First, we trained the readouts to report on familiar stimuli with a specified firing rate (<xref ref-type="supplementary-material" rid="SD1">Fig. Supp. 18</xref>). Plastic networks outperformed a static control network, as long as familiar and novel stimuli elicited different responses that could be mapped onto the desired output signal (<xref ref-type="supplementary-material" rid="SD1">Fig. Supp. 18</xref>). We then turned to a more difficult task, i.e., the Atari game of pong (<xref ref-type="fig" rid="F4">Fig. 4B</xref>) that has been used to demonstrate learning capabilities of biological and artificial systems<sup><xref ref-type="bibr" rid="R44">44</xref>,<xref ref-type="bibr" rid="R45">45</xref></sup>. We tested if some quadruplets could produce networks from which the location and direction-of-motion of the ball could be inferred. Two quadruplets—those which had displayed sequential learning capabilities (<xref ref-type="fig" rid="F2">Fig. 2G,H</xref>)—outperformed static networks substantially (<xref ref-type="fig" rid="F2">Fig. 2G,H</xref>). Interestingly, these quadruplets created place-field-like plasticity (<xref ref-type="fig" rid="F4">Fig. 4E</xref>,<sup><xref ref-type="bibr" rid="R46">46</xref>,<xref ref-type="bibr" rid="R47">47</xref></sup>), as well as directional, distance-dependent connectivity (i.e. aligned vs. against ball direction, <xref ref-type="fig" rid="F4">Fig. 4F</xref>). Quadruplets that exhibited no significant responses in any memory task performed similar to, or worse than a static network (<xref ref-type="fig" rid="F4">Fig. 4C,D</xref>).</p><p id="P23">Finally, we tested a higher-dimensional rule parameterization, in which synaptic changes in the spiking network were computed by a feedforward neural network, based on additional variables beyond spike pairs: post-synaptic membrane potential, activity at neighboring synapses, spike triplets and the synaptic weight<sup><xref ref-type="bibr" rid="R12">12</xref></sup>. Stable quadruplets from this manifold also performed successfully many of the above mentioned tasks, again as a byproduct of network stabilization (<xref ref-type="fig" rid="F4">Fig. 4G</xref>). Unlike for the polynomial implementation, the most frequent function was transient familiarity detection (<xref ref-type="fig" rid="F4">Fig. 4G</xref>) instead of novelty detection (<xref ref-type="fig" rid="F1">Fig. 1D</xref>).</p></sec></sec><sec id="S8" sec-type="discussion"><label>III</label><title>Discussion</title><p id="P24">Our findings challenge the prevailing view that complex cognitive functions require intricately designed plasticity rules. Instead, we demonstrate that memory in its many forms may emerge spontaneously when simple plasticity rules work together to maintain network stability. Our discovery has three major implications: First, it explains why experimental studies of single plasticity rules often yield contradictory results. Second, it suggests that evolution may have discovered memory through the simpler route of ensuring stable neural dynamics. Third, it provides a new framework for understanding learning disorders as failures of plasticity coordination rather than single-rule deficits.</p><p id="P25">A key ingredient in stable and functional dynamics was co-activity (<xref ref-type="fig" rid="F3">Fig. 3</xref>), by producing negative feedback cycles that counteracted the destabilizing tendencies of Hebbian learning and thus increased the size of the solution manifold. Such interactions might spell doom for the success of studying single rules in isolation, as has been the standard in the field. Our finding on co-activity also provides a new perspective on the computational advantages of the cell type diversity seen in the brain<sup><xref ref-type="bibr" rid="R48">48</xref></sup> that may broaden the repertoire of possible network computations.</p><p id="P26">The plasticity rules we considered are likely an over-simplification of the mechanisms taking place in the brain. We did not consider slower mechanisms such as behavioral time scale plasticity<sup><xref ref-type="bibr" rid="R8">8</xref></sup>, homeostatic plasticity, consolidation and sleep. Different memory functions may appear more naturally for some types of plasticity rules than others (<xref ref-type="fig" rid="F4">Fig. 4G</xref>). Moreover, plasticity rules themselves have been shown to change across time, for instance as a result of neuro-modulation<sup><xref ref-type="bibr" rid="R49">49</xref>,<xref ref-type="bibr" rid="R50">50</xref></sup>. The stable rule quadruplets discovered in this study can be thought as basic building blocks towards making more complex and realistic plasticity processes (<xref ref-type="fig" rid="F4">Fig. 4A</xref>).</p><p id="P27">Though it is clear that the brain implements some form of reward signal for learning<sup><xref ref-type="bibr" rid="R51">51</xref></sup>, a good part of the brain’s ongoing plasticity may be unsupervised<sup><xref ref-type="bibr" rid="R52">52</xref></sup>. We provide <italic>in silico</italic> backing to this hypothesis, with complex network functions pertaining to memory emerging from simple, co-active and unsupervised rules. As our approach was limited to unsupervised learning—our plasticity rules did not incorporate error signals or top-down feedback—we added a readout trained with supervised learning and showed that we could use the network for reservoir computing<sup><xref ref-type="bibr" rid="R53">53</xref>,<xref ref-type="bibr" rid="R54">54</xref></sup> to solve real-world tasks, such as pong (<xref ref-type="fig" rid="F4">Fig. 4</xref>).</p><p id="P28">In summary, we have demonstrated the viability of algorithms for automated discovery of synaptic plasticity rules, and show that relatively simple, unsupervised rules can elicit complex network functions through cooperation. Our manifold may serve as an open resource for designing closed-loop plasticity experiments. More generally, our work suggests that simple parts can work together and interact with each other to produce something greater than the sum of their parts.</p></sec><sec id="S9" sec-type="methods"><label>IV</label><title>Methods</title><p id="P29">Unless mentioned otherwise, all networks in this study were simulated using Auryn<sup><xref ref-type="bibr" rid="R55">55</xref></sup>. All network simulations were run on the ISTA HPC cluster (913 cpu cores). The simulations for pong were run in python (Brian2 and Brian2CUDA<sup><xref ref-type="bibr" rid="R56">56</xref>,<xref ref-type="bibr" rid="R57">57</xref></sup>). All the code for running the networks and reproducing the analysis is available on Github <ext-link ext-link-type="uri" xlink:href="https://github.com/VogelsLab/dataSBI">https://github.com/VogelsLab/dataSBI</ext-link>. Numpy<sup><xref ref-type="bibr" rid="R58">58</xref></sup>, Pytorch<sup><xref ref-type="bibr" rid="R59">59</xref></sup>, Pandas<sup><xref ref-type="bibr" rid="R60">60</xref></sup>, Matplotlib<sup><xref ref-type="bibr" rid="R61">61</xref></sup>, Scipy<sup><xref ref-type="bibr" rid="R62">62</xref></sup>, sklearn<sup><xref ref-type="bibr" rid="R63">63</xref></sup> and the sbi libraries<sup><xref ref-type="bibr" rid="R64">64</xref></sup> were used for the analysis.</p><sec id="S10"><title>Network model</title><p id="P30">Throughout the study, we considered recurrent spiking networks with <italic>N</italic><sub>E</sub> = 4096 excitatory neurons and <italic>N</italic><sub>I</sub> = 1024 inhibitory neurons (leaky-integrate and fire point neurons with variable threshold, AMPA and NMDA currents, and conductance-based synapses). These networks were similar to previous work<sup><xref ref-type="bibr" rid="R10">10</xref>,<xref ref-type="bibr" rid="R12">12</xref></sup>, except for the inputs that depended on the task considered. The membrane potential dynamics of neuron <italic>j</italic> (excitatory or inhibitory) followed: <disp-formula id="FD2"><label>(2)</label><mml:math id="M5"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mfrac><mml:mtext>d</mml:mtext><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mi>V</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mtext>rest</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mi>j</mml:mi><mml:mtext>E</mml:mtext></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mtext>E</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mi>j</mml:mi><mml:mtext>I</mml:mtext></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mtext>I</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula> where E stands for excitation and I for inhibition, <italic>τ</italic><sub><italic>m</italic></sub> = 20 ms, <italic>V</italic><sub>rest</sub> = −70 mV, <italic>E</italic><sub>E</sub> = 0 mV and <italic>E</italic><sub>I</sub> = −80 mV.</p><p id="P31">A post-synaptic spike was emitted whenever the membrane potential <italic>V</italic><sub><italic>j</italic></sub>(<italic>t</italic>) crossed a threshold <inline-formula><mml:math id="M6"><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mtext>th</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, with an instantaneous reset to <italic>V</italic><sub>reset</sub> = −70 mV. This threshold <inline-formula><mml:math id="M7"><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mtext>th</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> was incremented by <inline-formula><mml:math id="M8"><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mtext>spike</mml:mtext><mml:mspace width="0.2em"/></mml:mrow><mml:mrow><mml:mtext>th</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>100</mml:mn><mml:mspace width="0.2em"/><mml:mtext>mV</mml:mtext></mml:mrow></mml:math></inline-formula> every time neuron <italic>j</italic> spiked and otherwise decayed following: <disp-formula id="FD3"><label>(3)</label><mml:math id="M9"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>th</mml:mtext></mml:mrow></mml:msub><mml:mfrac><mml:mtext>d</mml:mtext><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>V</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mtext>th</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mtext>base</mml:mtext></mml:mrow><mml:mrow><mml:mtext>th</mml:mtext></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mtext>th</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula> with <inline-formula><mml:math id="M10"><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mtext>base</mml:mtext><mml:mspace width="0.2em"/></mml:mrow><mml:mrow><mml:mtext>th</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>50</mml:mn><mml:mspace width="0.2em"/><mml:mtext>mV</mml:mtext></mml:mrow></mml:math></inline-formula>. The excitatory and inhibitory conductances, <italic>g</italic><sup>E</sup> and <italic>g</italic><sup>I</sup> evolved such that <disp-formula id="FD4"><label>(4)</label><mml:math id="M11"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>g</mml:mi><mml:mi>j</mml:mi><mml:mtext>E</mml:mtext></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:msubsup><mml:mi>g</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mtext>AMPA</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mtext>NMDA</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mtext>d</mml:mtext><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>g</mml:mi><mml:mi>j</mml:mi><mml:mtext>I</mml:mtext></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>g</mml:mi><mml:mi>j</mml:mi><mml:mtext>I</mml:mtext></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>GABA</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:mstyle displaystyle="true"><mml:mi>∑</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mtext>Inh</mml:mtext></mml:mrow></mml:munder><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mtext>d</mml:mtext><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>g</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mtext>AMPA</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>g</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mtext>AMPA</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>AMPA</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:mstyle displaystyle="true"><mml:mi>∑</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mtext>Exc</mml:mtext></mml:mrow></mml:munder><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mtext>d</mml:mtext><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>g</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mtext>NMDA</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>g</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mtext>AMPA</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mtext>NMDA</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>NMDA</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula> with <italic>w</italic><sub><italic>ij</italic></sub>(<italic>t</italic>) the connection strength between neurons <italic>i</italic> and <italic>j</italic> (unitless), <italic>a</italic> = 0.23 (unitless), <italic>τ</italic><sub>GABA</sub> = 10 ms, <italic>τ</italic><sub>AMPA</sub> = 5 ms, <italic>τ</italic><sub>NMDA</sub> = 100 ms, <inline-formula><mml:math id="M12"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mi>∑</mml:mi></mml:mstyle><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mo>∗</mml:mo></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> the spike train of pre-synaptic neuron <italic>i</italic>, where <inline-formula><mml:math id="M13"><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mo>∗</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> denotes the spike times of neuron <italic>k</italic>, and <italic>δ</italic> the Dirac delta.</p><p id="P32">Network initializations: Unless mentioned otherwise, all networks were initialized with random sparse connectivity (10%), with <inline-formula><mml:math id="M14"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mtext>EE</mml:mtext></mml:mrow><mml:mrow><mml:mtext>init</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mtext>EI</mml:mtext></mml:mrow><mml:mrow><mml:mtext>init</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M15"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mtext>IE</mml:mtext></mml:mrow><mml:mrow><mml:mtext>init</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mtext>II</mml:mtext></mml:mrow><mml:mrow><mml:mtext>init</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mspace width="0.2em"/><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p id="P33">The inputs received by the network depended on the exact task, which are described in subsequent sections.</p></sec><sec id="S11"><title>Rule quadruplets parameterization</title><p id="P34">The parameterization of synaptic plasticity, initially defined in<sup><xref ref-type="bibr" rid="R21">21</xref></sup>, captured first order Hebbian spike-triggered updates, the weight from neuron <italic>i</italic> of type X ∈ (E, I) to neuron <italic>j</italic> of type Y ∈ (E, I) evolved such that: <disp-formula id="FD5"><label>(5)</label><mml:math id="M16"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mtext>d</mml:mtext><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mtext>XY</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mtext>XY</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mtext>XY</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mtext>XY</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula> with <italic>η</italic> = 0.01 a fixed learning rate, <inline-formula><mml:math id="M17"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:msub><mml:mi>∑</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> the spike train of neuron <italic>i, δ</italic> the Dirac delta function to denote the presence of a pre (post)-synaptic spike at time <italic>t</italic>. The synaptic traces <italic>x</italic><sub><italic>i</italic></sub> and <italic>x</italic><sub><italic>j</italic></sub> are low-pass filters of the activity of pre-synaptic neuron <italic>i</italic> and post-synaptic neuron <italic>j</italic>, with time constants <italic>τ</italic><sub>pre</sub> and <italic>τ</italic><sub>post</sub>, such that: <disp-formula id="FD6"><label>(6)</label><mml:math id="M18"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mtext>d</mml:mtext><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>pre</mml:mtext></mml:mrow><mml:mrow><mml:mtext>XY</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mspace width="0.2em"/><mml:mtext>and</mml:mtext><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mfrac><mml:mtext>d</mml:mtext><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow><mml:mrow><mml:mtext>XY</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p id="P35">Overall, this search space comprised 6 tunable plasticity parameters per synapse type XY ∈ (X, Y (E, I)): <italic>θ</italic><sub>XY</sub> = [<italic>α</italic><sub>XY</sub>, <italic>β</italic><sub>XY</sub>, <italic>γ</italic><sub>XY</sub>, <italic>κ</italic><sub>XY</sub>, <inline-formula><mml:math id="M19"><mml:mrow><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>pre</mml:mtext><mml:mspace width="0.2em"/></mml:mrow><mml:mrow><mml:mtext>XY</mml:mtext></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>post</mml:mtext><mml:mspace width="0.2em"/></mml:mrow><mml:mrow><mml:mtext>XY</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>], for a total of 24 plasticity parameters across all four synapse types.</p><p id="P36">Note that all weights in the network were capped at all times, in the [0, <italic>w</italic><sub>max</sub>] range, with <italic>w</italic><sub>max</sub> = 20.</p><p id="P37">The MLP rules used in <xref ref-type="fig" rid="F4">Fig. 4</xref> were defined as in previous work<sup><xref ref-type="bibr" rid="R12">12</xref></sup>.</p><p id="P38">Note that there were no other plasticity mechanisms in these networks such as synaptic normalization, homeostatic plasticity or short-term plasticity.</p></sec><sec id="S12"><title>Inferring rule quadruplets: fSBI and MF-NPE</title><p id="P39">We got posterior distributions over plasticity rule quadruplets (parameterized with 24 parameters <italic>θ</italic> = [<italic>θ</italic><sub>EE</sub>, <italic>θ</italic><sub>EI</sub>, <italic>θ</italic><sub>IE</sub>, <italic>θ</italic><sub>II</sub>]; more details in section above) that created stable network dynamics for at least 4h, by combining two simulation-based inference (SBI) methods: filter Simulation-Based Inference (fSBI)<sup><xref ref-type="bibr" rid="R12">12</xref></sup> and Multi-Fidelity Neural Posterior Estimation (MF-NPE)<sup><xref ref-type="bibr" rid="R27">27</xref></sup>. We proceeded in the following way:</p><list list-type="order" id="L1"><list-item><p id="P40">Define the prior over rule quadruplets: In line with previous work<sup><xref ref-type="bibr" rid="R12">12</xref></sup>, the prior was a uniform distribution, such that <italic>α</italic><sub>XY</sub>, <italic>β</italic><sub>XY</sub>, <italic>γ</italic><sub>XY</sub>, <italic>κ</italic><sub>XY</sub> ~ 𝒰[−2, 2] and <inline-formula><mml:math id="M20"><mml:mrow><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>pre</mml:mtext><mml:mspace width="0.2em"/></mml:mrow><mml:mrow><mml:mtext>XY</mml:mtext></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>post</mml:mtext><mml:mspace width="0.2em"/></mml:mrow><mml:mrow><mml:mtext>XY</mml:mtext></mml:mrow></mml:msubsup><mml:mo>~</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>100</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mtext>ms</mml:mtext></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p id="P41">Obtain 2min-stable quadruplets with fSBI: This step was performed in prior work<sup><xref ref-type="bibr" rid="R12">12</xref></sup>. In that study, fSBI was used to gradually filter out rule quadruplets that created unstable network dynamics after 2 minutes (rather than 4h). On this shorter task, rule quadruplets were deemed stable if they passed a number of conditions on the activity and weight dynamics<sup><xref ref-type="bibr" rid="R21">21</xref></sup>. The result was a neural density estimator that approximated a posterior over plasticity parameters that generated stable quadruplets with high probability (≈ 50%). In total, 110k stable quadruplets were obtained with this method. The stability of each quadruplet in practice was verified with numerical simulations.</p></list-item><list-item><p id="P42">Obtain 4h-stable quadruplets with MF-NPE: We used the 110k 2min-stable quadruplets to pre-train a posterior density estimator <italic>p</italic>(<italic>θ</italic>|<italic>r</italic><sub>exc</sub>(2min)), with <italic>r</italic><sub>exc</sub>(2min) the firing rate of the excitatory population after 2 minutes. We then fine-tuned the density estimator on a subset of these 110k quadruplets that led to stable networks for at least 4h, <italic>p</italic>(<italic>θ</italic>|<italic>r</italic><sub>exc</sub>(4h)) (2,500 quadruplets, see Fig. Supp. 1). The 4h-stability criteria are defined below. Finally, we sampled 1,000 rule quadruplets from this posterior, conditioning on low firing rates (<italic>r</italic><sub>exc</sub>(4h) ~ 𝒰[2 − 10]Hz). 49.2% of the posterior samples led to stable dynamics of the spiking networks when simulated for 4h.</p></list-item></list><sec id="S13"><title>Criteria for 4h-stability</title><p id="P43">In this study, a network was classified as stable in the 4h simulations if: <list list-type="bullet" id="L2"><list-item><p id="P44">Firing rate condition: <italic>r</italic><sub>exc</sub>(<italic>t</italic>) ∈ [0.1, 100] for all <italic>t</italic> &lt; 4h, with <italic>r</italic><sub>exc</sub>(<italic>t</italic>) the firing rate of the excitatory population computed over a 10s window.</p></list-item><list-item><p id="P45">Irregularity condition: CV<sub>ISI</sub>(<italic>t</italic>) &gt; 0.7 for all <italic>t</italic> &lt; 4h, with CV<sub>ISI</sub>(<italic>t</italic>) the coefficient of variation of the inter spike interval computed over a 10s window averaged across all excitatory neurons.</p></list-item><list-item><p id="P46">Final weight values: ⟨<italic>w</italic><sub>EE</sub>(4h)⟩ &lt; 0.5, ⟨<italic>w</italic><sub>EI</sub>(4h)⟩ &lt; 0.5, ⟨<italic>w</italic><sub>IE</sub>(4h)⟩ &lt; 5, ⟨<italic>w</italic><sub>II</sub>(4h)⟩ &lt; 5 with <italic>w</italic><sub>EE</sub>(4h) (resp. <italic>w</italic><sub>EI</sub>(4h), <italic>w</italic><sub>IE</sub>(4h), <italic>w</italic><sub>II</sub>(4h) the average EE weight (resp. EI, IE, II weight) after the 4h simulation.</p></list-item><list-item><p id="P47">Final weight distribution: <italic>w</italic><sub>blow</sub>(4h) &lt; 0.1, with <italic>w</italic><sub>blow</sub>(4<italic>h</italic>) the fraction of synaptic weights that reached 0 or <italic>w</italic><sub>max</sub> (maximum over all 4 synapse types). This condition effectively enforced that no more than 10% of the weights of each synapse types had ”blown up”, i.e., had reached extreme values.</p></list-item></list>
</p><p id="P48">Note that these criteria were less stringent than for 2 minutes, as many metrics on the 2min-task were designed to detect networks that would be unstable in the future. In our hands, these conditions which were added on top of the pre-screening performed in previous work<sup><xref ref-type="bibr" rid="R12">12</xref></sup> were enough to constrain the network dynamics to plausible regimes. Note that these criteria for stability were established with a focus on cortex, and that in the future one may want to impose different conditions to better fit other brain regions.</p></sec><sec id="S14"><title>Posteriors for specific network functions</title><p id="P49">To infer plasticity rules that created networks able to solve specific memory tasks, we applied a similar procedure as for the 4h-stability posterior. However, we did not have any indication of memory performance for the samples obtained from the 2min pre-screening. However, we had access to the memory performance of the 2,500 4h-stable quadruplets, and used these values to assign memory performance to the 110k quadruplets, using a nearest-neighbor algorithm. Several posteriors were obtained this way: <italic>p</italic>(<italic>θ</italic>|Δ<italic>r</italic><sub>mem</sub>(10min)), <italic>p</italic>(<italic>θ</italic>|ON<sub>nov</sub>(10min)), <inline-formula><mml:math id="M21"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>∣</mml:mo><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow><mml:mrow><mml:mtext>rep</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mi>min</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M22"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>∣</mml:mo><mml:mtext>Δ</mml:mtext><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mtext>seq</mml:mtext></mml:mrow><mml:mrow><mml:mtext>cnov</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mi>min</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (see <xref ref-type="supplementary-material" rid="SD1">Fig. Supp. 13</xref>). Refer to the following sections for the definition of these metrics of network responses. We then sampled rule quadruplets from these posteriors conditioned on metric values reminiscent of experimental observations. In practice, this step increased the probability of sampling quadruplets with desired properties compared to the 4h-stable posterior (<xref ref-type="supplementary-material" rid="SD1">Fig. Supp. 13E</xref>). However, all quadruplets were verified numerically before being used in further analyses.</p></sec></sec><sec id="S15"><title>Familiarity detection task</title><sec id="S16"><title>Input connectivity</title><p id="P50">The connectivity from <italic>N</italic><sub>inp→E</sub> = 10000 input neurons to excitatory recurrent neurons was receptive-field-like, similar to previous work<sup><xref ref-type="bibr" rid="R10">10</xref></sup> (<xref ref-type="supplementary-material" rid="SD1">Fig. Supp. 2</xref>). For each recurrent excitatory neuron, we selected a random input neuron as the center of the circular receptive field of radius 8. The connections from neurons of this circular patch of input neurons to the considered recurrent neuron was <italic>w</italic><sub>inp</sub> = 0.075, and 0 to all other input neurons. The inhibitory neurons received inputs from <italic>N</italic><sub>inp→I</sub> = 4096 Poisson neurons with weight <italic>w</italic><sub>inp</sub> and similar receptive field connectivity than for the excitatory population. However, inhibitory neurons only received background inputs and no specific stimulus patterns. All input neurons fired at a baseline firing rate of <inline-formula><mml:math id="M23"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mtext>inp</mml:mtext><mml:mspace width="0.2em"/></mml:mrow><mml:mrow><mml:mtext>bg</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:mtext>Hz</mml:mtext></mml:mrow></mml:math></inline-formula> at all time, unless the input was part of an active input pattern (e.g. an active familiar stimulus).</p></sec><sec id="S17"><title>Task</title><p id="P51">After a burn-out period with only background inputs of duration <italic>l</italic><sub>pre-train</sub> = 30s, the network was presented with one training stimulus (the familiar stimulus) for <italic>l</italic><sub>train</sub> = 10s. A break period with only background inputs followed, which could last <italic>t</italic><sub>b</sub> ∈ [1s, 10s, 20s, 1min, 2min, 5min, 10min, 30min, 1h, 4h]. A test session then began, during which a novel stimulus was presented to the network for <inline-formula><mml:math id="M24"><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow><mml:mtext>test</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mtext>s</mml:mtext></mml:mrow></mml:math></inline-formula> followed by <inline-formula><mml:math id="M25"><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>off</mml:mtext></mml:mrow><mml:mrow><mml:mtext>test</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>3</mml:mn><mml:mtext>s</mml:mtext></mml:mrow></mml:math></inline-formula> of background inputs. The same was then done with the familiar stimulus. Plasticity was turned on throughout the task. To avoid any biases due to the ordering of stimulus presentations (novel before familiar), the network state was saved at the beginning of each test session and loaded before presenting the familiar stimuli.</p></sec><sec id="S18"><title>Familiar and novel stimulus design</title><p id="P52">For each stimulus pattern (familiar or novel), 500 adjacent input neurons were selected. While the pattern was active, the participating input neurons fired at <inline-formula><mml:math id="M26"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mtext>inp</mml:mtext><mml:mspace width="0.2em"/></mml:mrow><mml:mrow><mml:mtext>active</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>60</mml:mn><mml:mtext>Hz</mml:mtext></mml:mrow></mml:math></inline-formula> instead of <inline-formula><mml:math id="M27"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mtext>inp</mml:mtext><mml:mspace width="0.2em"/></mml:mrow><mml:mrow><mml:mtext>bgg</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>. There was no overlap between the stimulus patterns.</p></sec><sec id="S19"><title>Evaluation of network response</title><p id="P53">The firing rate of the excitatory population was computed during the <inline-formula><mml:math id="M28"><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow><mml:mtext>test</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> presentation of the familiar (<italic>r</italic><sub>fam</sub>(<italic>t</italic><sub>b</sub>)) and novel (<italic>r</italic><sub>nov</sub>(<italic>t</italic><sub>b</sub>)) stimuli. We then computed the relative preference of the network for the novel stimulus: <disp-formula id="FD7"><label>(7)</label><mml:math id="M29"><mml:mrow><mml:mtext>Δ</mml:mtext><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mtext>mem</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mfrac><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mtext>nov</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mtext>fam</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mtext>nov</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mtext>fam</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p id="P54">In <xref ref-type="fig" rid="F1">Fig. 1</xref>, each rule quadruplet was tested <italic>N</italic><sub>trials</sub> = 5 times. Each trial involved a different draw of the connectivity matrices and the input Poisson spike trains. A rule quadruplet was deemed responsive at break time <italic>t</italic><sub>b</sub> if it passed student T-test comparing <inline-formula><mml:math id="M30"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mtext>fam</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>trials</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M31"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mtext>nov</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>trials</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</p><p id="P55">Other metrics to evaluate memory recall on this task can be designed, for instance the transient dynamics (see next section for ON and OFF responses), or by taking into account the standard deviation of the neuron activities (see <xref ref-type="supplementary-material" rid="SD1">Fig. Supp. 4E</xref>): <disp-formula id="FD8"><label>(8)</label><mml:math id="M32"><mml:mrow><mml:mtext>Δ</mml:mtext><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mtext>mem</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mfrac><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mtext>nov</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mtext>fam</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mtext>nov</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mtext>fam</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> with <italic>σ</italic><sub>nov</sub>(<italic>t</italic><sub>b</sub>) = <italic>σ</italic>({<italic>r</italic><sub>nov,<italic>i</italic></sub>(<italic>t</italic><sub>b</sub>)}<sub><italic>i</italic>∈E</sub>) (resp. <italic>σ</italic><sub>fam</sub>(<italic>t</italic><sub>b</sub>) = <italic>σ</italic>({<italic>r</italic><sub>fam,<italic>i</italic></sub>(<italic>t</italic><sub>b</sub>) }<sub><italic>i</italic>∈E</sub>)) the standard deviation of the firing rates of excitatory neurons computed during presentation of the novel (resp. familiar) stimulus.</p><p id="P56">Note that we also checked that the simulation respected the stability criteria detailed in the above paragraphs.</p></sec></sec><sec id="S20"><title>Transient dynamics task</title><p id="P57">This task was similar to the familiarity detection task described above, only the scoring metrics differed. We computed ON and OFF responses for familiar and novel stimuli presentations: <disp-formula id="FD9"><label>(9)</label><mml:math id="M33"><mml:mrow><mml:msub><mml:mrow><mml:mtext>ON</mml:mtext></mml:mrow><mml:mrow><mml:mtext>nov</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mrow><mml:mi>max</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mtext>exc</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow><mml:mtext>nov</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow><mml:mtext>nov</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>200</mml:mn><mml:mtext>ms</mml:mtext></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>〈</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mtext>exc</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>〉</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow><mml:mtext>nov</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>on</mml:mtext></mml:mrow><mml:mrow><mml:mtext>nov</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>200</mml:mn><mml:mtext>ms</mml:mtext></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula> <disp-formula id="FD10"><label>(10)</label><mml:math id="M34"><mml:mrow><mml:msub><mml:mrow><mml:mtext>OFF</mml:mtext></mml:mrow><mml:mrow><mml:mtext>nov</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mrow><mml:mi>max</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mtext>exc</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>off</mml:mtext></mml:mrow><mml:mrow><mml:mtext>nov</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>off</mml:mtext></mml:mrow><mml:mrow><mml:mtext>nov</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>200</mml:mn><mml:mtext>ms</mml:mtext></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>c</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>〉</mml:mo><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>off</mml:mtext></mml:mrow><mml:mrow><mml:mtext>nov</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>off</mml:mtext><mml:mspace width="0.2em"/></mml:mrow><mml:mrow><mml:mtext>nov</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>200</mml:mn><mml:mtext>ms</mml:mtext></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula> with <italic>t</italic><sub>on</sub>(<italic>t</italic><sub>b</sub>) (resp. <italic>t</italic><sub>off</sub>(<italic>t</italic><sub>b</sub>)) are the time of stimulus onset (resp. offset) of the novel stimulus after break period <italic>t</italic><sub>b</sub>.</p><p id="P58">We computed the relative preference for novel stimuli when reading out memories from ON responses: <disp-formula id="FD11"><label>(11)</label><mml:math id="M35"><mml:mrow><mml:mtext>ΔON</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mtext>ON</mml:mtext></mml:mrow><mml:mrow><mml:mtext>nov</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mtext>ON</mml:mtext></mml:mrow><mml:mrow><mml:mtext>fam</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mtext>ON</mml:mtext></mml:mrow><mml:mrow><mml:mtext>nov</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mtext>ON</mml:mtext></mml:mrow><mml:mrow><mml:mtext>fam</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p id="P59">Other metrics were designed to quantify the quadruplet responses on this task. For instance, memory could also be read out from the OFF responses (see <xref ref-type="supplementary-material" rid="SD1">Fig. Supp. 5</xref>): <disp-formula id="FD12"><label>(12)</label><mml:math id="M36"><mml:mrow><mml:mtext>ΔOFF</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mtext>OFF</mml:mtext></mml:mrow><mml:mrow><mml:mtext>nov</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mtext>OFF</mml:mtext></mml:mrow><mml:mrow><mml:mtext>fam</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mtext>OFF</mml:mtext></mml:mrow><mml:mrow><mml:mtext>nov</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mtext>OFF</mml:mtext></mml:mrow><mml:mrow><mml:mtext>fam</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p id="P60">We observed that ON responses had less variance than the OFF responses (<xref ref-type="supplementary-material" rid="SD1">Fig. Supp. 5C</xref>), and thus favored the ON response metric.</p></sec><sec id="S21"><title>Sequential task</title><sec id="S22"><title>Input connectivity</title><p id="P61">The excitatory neurons in the network received <italic>N</italic><sub>inp→E</sub> = 11025 inputs from Poisson neurons firing at <inline-formula><mml:math id="M37"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mtext>bg</mml:mtext></mml:mrow><mml:mrow><mml:mtext>inp</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:mtext>Hz</mml:mtext><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula>. When a stimulus was active, a subset of the input neurons increased their firing rate to <inline-formula><mml:math id="M38"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mtext>active</mml:mtext><mml:mspace width="0.2em"/></mml:mrow><mml:mrow><mml:mtext>seq</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>100</mml:mn><mml:mtext>Hz</mml:mtext></mml:mrow></mml:math></inline-formula> (see <xref ref-type="supplementary-material" rid="SD1">Fig. Supp. 3</xref>). The connectivity from input neurons to excitatory and inhibitory neurons was receptive-field-like (similar to the familiarity task, see <xref ref-type="supplementary-material" rid="SD1">Fig. Supp. 2</xref>). The inhibitory neurons received inputs from <italic>N</italic><sub>inp→I</sub> = 4096 Poisson neurons with <italic>w</italic><sub>inp</sub> and similar receptive field connectivity than for the excitatory population. However, inhibitory neurons only received background inputs <inline-formula><mml:math id="M39"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mtext>bg</mml:mtext><mml:mspace width="0.2em"/></mml:mrow><mml:mrow><mml:mtext>input</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and no specific stimulus patterns.</p></sec><sec id="S23"><title>Task</title><p id="P62">After a burn-out period <inline-formula><mml:math id="M40"><mml:mrow><mml:msubsup><mml:mi>l</mml:mi><mml:mrow><mml:mtext>pre-train</mml:mtext><mml:mspace width="0.2em"/></mml:mrow><mml:mrow><mml:mtext>seq</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>30</mml:mn><mml:mtext>s</mml:mtext></mml:mrow></mml:math></inline-formula> with only background inputs, the network was trained for <inline-formula><mml:math id="M41"><mml:mrow><mml:msubsup><mml:mi>l</mml:mi><mml:mrow><mml:mtext>train</mml:mtext><mml:mspace width="0.2em"/></mml:mrow><mml:mrow><mml:mtext>seq</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>50</mml:mn><mml:mtext>s</mml:mtext></mml:mrow></mml:math></inline-formula> on a loop of 5 stimuli (the ”familiar” stimuli). After a break period ranging from 1s to 4h with only background inputs, each training stimulus was presented independently for <inline-formula><mml:math id="M42"><mml:mrow><mml:msubsup><mml:mi>l</mml:mi><mml:mrow><mml:mtext>test-on</mml:mtext><mml:mspace width="0.2em"/></mml:mrow><mml:mrow><mml:mtext>seq</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>200</mml:mn><mml:mtext>ms</mml:mtext></mml:mrow></mml:math></inline-formula>, as well as two novel stimuli.</p></sec><sec id="S24"><title>Familiar and novel stimuli design</title><p id="P63">All stimuli were non-overlapping (<xref ref-type="supplementary-material" rid="SD1">Fig. Supp. 3</xref>). To isolate as best as possible the effects of plasticity, the stimuli were designed such that a static network did not create any memories on this task (<xref ref-type="supplementary-material" rid="SD1">Fig. Supp. 3</xref>).</p></sec><sec id="S25"><title>Engram neurons definition</title><p id="P64">Engram neurons were defined for every stimulus pattern (novel and familiar), at the beginning of the task. With plasticity turned off, we presented every stimulus to the network for 1s and collected the firing rates of each neuron during the whole stimulus presentation. The top 1/7th most active neurons were defined to be engram neurons for that stimulus.</p></sec><sec id="S26"><title>Successor representation metric</title><p id="P65">This metric is used in <xref ref-type="fig" rid="F1">Fig. 1F</xref>. We define <inline-formula><mml:math id="M43"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>engr</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> the firing rate of engram <italic>i</italic> during presentation of the stimulus <italic>j</italic> as the population firing rate computed over engram neurons for pattern <italic>i</italic> during <inline-formula><mml:math id="M44"><mml:mrow><mml:msubsup><mml:mi>l</mml:mi><mml:mrow><mml:mtext>test-on</mml:mtext><mml:mspace width="0.2em"/></mml:mrow><mml:mrow><mml:mtext>seq</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> from stimulus onset. Then, <inline-formula><mml:math id="M45"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>〈</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mtext>engr</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>. In particular, <italic>m</italic><sub>+1</sub> is plotted in <xref ref-type="fig" rid="F1">Fig. 1F</xref>.</p><p id="P66">In <xref ref-type="fig" rid="F1">Fig. 1</xref>, rule quadruplets were deemed responsive at a break duration <italic>t</italic><sub>b</sub> for the <italic>m</italic> + 1 metric if they passed a Student T-test comparing <inline-formula><mml:math id="M46"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mtext>engr</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>≤</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>fam</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M47"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mtext>nov</mml:mtext><mml:mspace width="0.2em"/></mml:mrow><mml:mrow><mml:mtext>engr</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>≤</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>fam</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="S27"><title>Metrics to evaluate replay</title><list list-type="bullet" id="L3"><list-item><p id="P67"><inline-formula><mml:math id="M48"><mml:mrow><mml:msubsup><mml:mi>ρ</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>rep</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mtext>fam</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mtext>nov</mml:mtext></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mtext>nov</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>: During the presentation of stimulus <italic>i</italic>, ratio of the firing rate of the engram of next stimulus <italic>i</italic> + 1 in the sequence, compared to the firing rate of the engram associated to a novel stimulus.</p></list-item><list-item><p id="P68"><inline-formula><mml:math id="M49"><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mo>∗</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>argmax</mml:mtext></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>onset</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>onset</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>3</mml:mn><mml:mtext>s</mml:mtext></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:msubsup><mml:mi>ρ</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>rep</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> the time at which the ratio above was maximal, i.e. when the next stimulus was maximally activated following presentation of a given stimulus.</p></list-item><list-item><p id="P69"><inline-formula><mml:math id="M50"><mml:mrow><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow><mml:mrow><mml:mtext>rep</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>〈</mml:mo><mml:msubsup><mml:mi>ρ</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>rep</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mo>∗</mml:mo></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mtext>fam</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> the maximum ratios averaged across all familiar stimuli.</p></list-item><list-item><p id="P70"><inline-formula><mml:math id="M51"><mml:mrow><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mtext>spec</mml:mtext></mml:mrow><mml:mrow><mml:mtext>rep</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>〈</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>ρ</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>rep</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mo>∗</mml:mo></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mtext>rep</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mo>∗</mml:mo></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>〉</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mtext>fam</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, a metric to check if the maximal activation of the next stimulus is concurrent with activation of 2 stimuli in the future.</p></list-item><list-item><p id="P71"><inline-formula><mml:math id="M52"><mml:mrow><mml:mtext>Δ</mml:mtext><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mtext>rep</mml:mtext></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>〈</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mo>∗</mml:mo></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>onset</mml:mtext></mml:mrow></mml:msubsup><mml:mo>〉</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mtext>fam</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> the time at which the maximal reactivation takes place.</p></list-item></list><p id="P72">In our hands, it was important for some metrics to compare engram firing rates to other engram firing rates, even if they were engrams associated to novel stimuli, as the process of selecting neurons to be part of an engram induced a bias on their overall activity.</p></sec></sec><sec id="S28"><title>Contextual novelty task</title><p id="P73">This task was similar to the sequential task described above, only the test periods differed. After each break duration, we presented three groups of sequences of four stimuli to the network (see <xref ref-type="supplementary-material" rid="SD1">Fig. Supp. 9</xref>). The first group of sequences was fully familiar: Fam1-Fam2-Fam3-Fam4; Fam2-Fam3-Fam4-Fam5; Fam3-Fam4-Fam5-Fam1 etc. with each stimulus being on for <inline-formula><mml:math id="M53"><mml:mrow><mml:msubsup><mml:mi>l</mml:mi><mml:mrow><mml:mtext>test-on</mml:mtext><mml:mspace width="0.2em"/></mml:mrow><mml:mrow><mml:mtext>seq</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>200</mml:mn><mml:mtext>ms</mml:mtext></mml:mrow></mml:math></inline-formula> and no down time between stimuli presentations. The second group was composed of sequences ending with a novel stimulus: Fam1-Fam2-Fam3-Nov1; Fam2-Fam3-Fam4-Nov2; Fam3-Fam4-Fam5-Nov1 etc. The third group was composed of sequences ending in a out-of-order familiar stimulus: a contextual novel stimulus: Fam1-Fam2-Fam3-Fam5; Fam2-Fam3-Fam4-Fam1; Fam3-Fam4-Fam5-Fam2 etc. To assess the network responses to these stimuli, we define <inline-formula><mml:math id="M54"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mtext>fam</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> the firing rate of the excitatory population computed during the fourth stimulus in the <italic>n</italic><sup>th</sup> familiar sequence, and similarly <inline-formula><mml:math id="M55"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mtext>nov</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mtext>cnov</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. From these, we defined two metrics for this memory task: <disp-formula id="FD13"><label>(13)</label><mml:math id="M56"><mml:mrow><mml:mtext>Δ</mml:mtext><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mtext>seq</mml:mtext></mml:mrow><mml:mrow><mml:mtext>nov</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>〈</mml:mo><mml:mn>2</mml:mn><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mtext>nov</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mtext>fam</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mtext>nov</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mtext>fam</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:mo>〉</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>fam</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula> and <disp-formula id="FD14"><label>(14)</label><mml:math id="M57"><mml:mrow><mml:mtext>Δ</mml:mtext><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mtext>seq</mml:mtext></mml:mrow><mml:mrow><mml:mtext>cnov</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>〈</mml:mo><mml:mn>2</mml:mn><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mtext>cnov</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mtext>fam</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mtext>cnov</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mtext>fam</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:mo>〉</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>fam</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula> <inline-formula><mml:math id="M58"><mml:mrow><mml:mtext>Δ</mml:mtext><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mtext>seq</mml:mtext></mml:mrow><mml:mrow><mml:mtext>nov</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> can be seen as a generalization of Δ<italic>r</italic><sub>mem</sub> used in the familiarity task, but for the case of multiple familiar inputs (see <xref ref-type="supplementary-material" rid="SD1">Fig. Supp. 9</xref>). <inline-formula><mml:math id="M59"><mml:mrow><mml:mtext>Δ</mml:mtext><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mtext>seq</mml:mtext></mml:mrow><mml:mrow><mml:mtext>cnov</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is the relative preference of the network to the contextually novel stimulus compared to the familiar one.</p></sec><sec id="S29"><title>Criteria for quadruplets to be responsive to a task</title><p id="P74">In this study, we used two criteria for each network function.</p><sec id="S30"><title>Loose criteria</title><p id="P75">In <xref ref-type="fig" rid="F1">Fig. 1</xref>, we use permissive criteria to label a rule quadruplet as responsive to a task. These criteria involves averaging over several trials.</p><list list-type="bullet" id="L4"><list-item><p id="P76">Novelty detection: A rule was deemed able to perform novelty detection if at any of the break times a significant difference between <italic>r</italic><sub>nov</sub>(<italic>t</italic><sub>b</sub> and <italic>r</italic><sub>fam</sub>(<italic>t</italic><sub>b</sub> with <italic>r</italic><sub>nov</sub>(<italic>t</italic><sub>b</sub>) ≥ <italic>r</italic><sub>fam</sub>(<italic>t</italic><sub>b</sub>) (see section on familiarity task for the statistical test).</p></list-item><list-item><p id="P77">Familiarity detection: Same as above but with <italic>r</italic><sub>nov</sub>(<italic>t</italic><sub>b</sub>) ≤ <italic>r</italic><sub>fam</sub>(<italic>t</italic><sub>b</sub>)</p></list-item><list-item><p id="P78">Transient dynamics: The condition was ON<sub>nov</sub>(<italic>t</italic><sub>b</sub>) &gt; 1.5 for all break after 5min. Note that here, we are only assessing whether a quadruplets generates notable transient dynamics and not whether it read out any memories.</p></list-item><list-item><p id="P79">Successor representation: A rule was deemed able to be responsive to the sequential task if at any of the break times a significant difference between <inline-formula><mml:math id="M60"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mtext>engr</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>≤</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>fam</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M61"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mtext>nov</mml:mtext><mml:mspace width="0.2em"/></mml:mrow><mml:mrow><mml:mtext>engr</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>≤</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>fam</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> could be detected (see section on sequential task for the statistical test).</p></list-item><list-item><p id="P80">Contextual novelty: A rule was deemed responsive if at any of the break times there was a significant difference between <inline-formula><mml:math id="M62"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>cnov</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>fam</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M63"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>fam</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>fam</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>.</p></list-item></list><p id="P81">These criteria only applied to the 2,500 quadruplets initially sampled from the stability manifold. For the other quadruplets sampled from more specialized posteriors, we employed the criteria below.</p></sec><sec id="S31"><title>Tight criteria</title><p id="P82">In <xref ref-type="fig" rid="F2">Fig. 2</xref>, we use more stringent criteria to qualify rule quadruplets as consistent with experimental observations:</p><list list-type="bullet" id="L5"><list-item><p id="P83">Novelty detection: ∀<italic>t</italic><sub>b</sub> ≥ 2min, Δ<italic>r</italic><sub>mem</sub>.(<italic>t</italic><sub>b</sub>) &gt; 0.1.</p></list-item><list-item><p id="P84">Familiarity detection: Δ<italic>r</italic><sub>mem</sub>(<italic>t</italic><sub>b</sub>) &lt; −0.1 for 3 consecutive break durations.</p></list-item><list-item><p id="P85">Transient dynamics: ∀<italic>t</italic><sub>b</sub> ≥ 2min, ON<sub>nov</sub>(<italic>t</italic><sub>b</sub>) &gt; 1.8.</p></list-item><list-item><p id="P86">Flat replay: ∃<italic>t</italic><sub>b</sub> ∈ <italic>T</italic><sub>breaks</sub>, <inline-formula><mml:math id="M64"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mtext>rep</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mstyle displaystyle="true"><mml:mo>∩</mml:mo></mml:mstyle><mml:mspace width="0.2em"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>Δ</mml:mtext><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext>rep</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mn>0.3</mml:mn><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p id="P87">Sequential replay: ∃<italic>t</italic><sub>b</sub> ∈ <italic>T</italic><sub>breaks</sub>, <inline-formula><mml:math id="M65"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow><mml:mrow><mml:mtext>rep</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mstyle displaystyle="true"><mml:mo>∩</mml:mo></mml:mstyle><mml:mspace width="0.2em"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>ρ</mml:mi><mml:mrow><mml:mtext>spec</mml:mtext></mml:mrow><mml:mrow><mml:mtext>rep</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>1.5</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mstyle displaystyle="true"><mml:mo>∩</mml:mo></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>Δ</mml:mtext><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext>rep</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≥</mml:mo><mml:mn>0.4</mml:mn><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p id="P88">Contextual novelty: ∀<italic>t</italic><sub>b</sub> ≤ 10min, <inline-formula><mml:math id="M66"><mml:mrow><mml:mtext>Δ</mml:mtext><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mtext>seq</mml:mtext></mml:mrow><mml:mrow><mml:mtext>cnov</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≥</mml:mo><mml:mn>0.15</mml:mn></mml:mrow></mml:math></inline-formula>.</p></list-item></list><p id="P89">Note that these criteria above are relatively arbitrary and were chosen to ensure that the network responses were qualitatively like experimental data and significant. The exact stringency of the criteria was tuned to select the best ≈ 100 rule quadruplets in the dataset. Due to the compute constraint, in this second part, we only tested rule quadruplets on a single trial (i.e. only the 2,500 base quadruplets were tested on all tasks on several trials).</p></sec></sec><sec id="S32"><title>Knock-outs</title><p id="P90">We conducted two types of knock-out (KO) experiments: single KOs, where plasticity was removed from one connection type at a time, and triple KOs, where plasticity was removed from three connection types simultaneously, leaving only one connection type plastic. For both KO types, we tested all possible combinations across the four connection types, for all 2,500 stable rule quadruplets tested in <xref ref-type="fig" rid="F1">Fig. 1</xref>. Knocking out the plasticity rule of type XY was defined as setting <italic>η</italic><sub>XY</sub> = 0 in equation (1) after a burn-in period of <italic>l</italic><sub>pre-train</sub> = 30s, during which only background inputs were present. This burn-in allowed synaptic weights to evolve from their default values, ensuring that KO weights were not overly dependent on our choice of initialization weights. The networks then underwent the familiarity detection task as described previously. Simulations were performed across all <italic>t</italic><sub>break</sub> values up to 4h, with all break times listed. Networks were then classified as stable or unstable using the stability criteria defined previously.</p></sec><sec id="S33"><title>Reservoir computing and pong</title><sec id="S34"><title>Familiarity detection task</title><p id="P91">A dataset of 2,500 simulations with the stable rule quadruplets used in <xref ref-type="fig" rid="F1">Fig. 1</xref> undergoing the familiarity task was prepared. During each simulation, spiking activity was recorded from all 5120 recurrent neurons. Simulations were conducted using the Brian2 simulator<sup><xref ref-type="bibr" rid="R56">56</xref></sup>. One readout unit (leaky-integrate-and-fire dynamics) was trained to perform the familiarity task receiving 55s of spike trains from the dataset as input. To show that we could obtain a readout of the network that was not bound to the preference induced by the quadruplet, the task was either to increase (resp. decrease) activity when a novel (resp. familiar) stimulus had been shown (<xref ref-type="supplementary-material" rid="SD1">Fig. Supp. 18</xref>). In a similar fashion to previous work<sup><xref ref-type="bibr" rid="R53">53</xref></sup>, the readout neuron received spike trains from 5,120 inputs and underwent training using the surrogate gradient descent approach<sup><xref ref-type="bibr" rid="R42">42</xref></sup>. The loss function was defined as the mean squared error between the firing rate of the output unit and the expected firing rate (in the case of novelty detection: 50Hz for novel and 0Hz for familiar; and vice versa for familiarity detection). 80% of the dataset was used for training and the remaining 20% for the validation phase. Accuracy was calculated as: <inline-formula><mml:math id="M67"><mml:mrow><mml:mtext>acc</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mi>N</mml:mi></mml:mfrac></mml:mrow></mml:math></inline-formula>, where <italic>C</italic><sub><italic>f</italic></sub> is the number of correct familiar responses, <italic>C</italic><sub><italic>n</italic></sub> is the number of correct novel responses, and <italic>N</italic> is the total number of validation trials. Maximum accuracy was calculated as the highest achievable accuracy for a threshold separating familiar and novel responses between 0 and 50Hz. To avoid an input location bias, during the training session, locations of familiar and novel inputs were regularly swapped between two different populations of neurons. During the validation phase, novel and familiar inputs were provided to the neurons that had never been exposed to them during training.</p></sec><sec id="S35"><title>Pong - Offline</title><p id="P92">To simulate the game environment, we started from existing code training electro-active polymer hydrogels to play pong<sup><xref ref-type="bibr" rid="R65">65</xref></sup>. The game field was a 640x640 pixels square field and the ball was a 90x90 pixel square (<xref ref-type="supplementary-material" rid="SD1">Fig. Supp. 19</xref>). The ball was initialized on the right side of the field in a random direction (an angle between 109° and 253°) and traveled across the field at a constant speed. No paddle was included in these simulations.</p><p id="P93">Following the idea of training a neural culture to play pong<sup><xref ref-type="bibr" rid="R45">45</xref></sup>, 4096 excitatory neurons were arranged spatially onto a 64 x 64 grid. Neurons received external inputs that modeled the position of a ball during the simulated games, i.e. neurons at the location of the ball would receive increased excitatory inputs compared to baseline. Each ball position excited a 9x9 square of recurrent neurons for 200 ms. Simulations were conducted using the Brian2CUDA simulator<sup><xref ref-type="bibr" rid="R57">57</xref></sup>. Every 200ms, the firing rate of each neuron was calculated to match separate stimulation periods and ball movement. To facilitate the position readout from the network activity, the firing rate was normalized to the highest firing rate during one simulation, and the ball position coordinates were normalized to the range (-1,1).</p><p id="P94">To decode the current and previous positions of the ball from the network activity, we trained readout weights from the recurrent network with a dataset composed of 2,000 pong games for training and 1,000 for validation. Since trials were independent, no position prediction was allowed using network activity from more than one trial. The readout comprised one linear layer with input size <italic>N</italic><sub>E</sub> = 4096 and 4 linear output units. These four output neurons modeled the decoded x and y coordinates of the current and previous ball positions. The loss function was defined as follows: <disp-formula id="FD15"><label>(15)</label><mml:math id="M68"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mtext>MSE</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula> with <italic>n</italic> is the number of output units, <italic>x</italic><sub><italic>t</italic></sub>, <italic>y</italic><sub><italic>t</italic></sub> are the true x and y coordinates of the ball at the current time step <italic>t</italic>, <inline-formula><mml:math id="M69"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <italic>ŷ</italic><sub><italic>t</italic></sub> are the predicted x and y coordinates at time <italic>t, x</italic><sub><italic>t</italic>−3</sub>, <italic>y</italic><sub><italic>t</italic>−3</sub> are the true x and y coordinates of the ball at the previous time step <italic>t</italic> − 3, <inline-formula><mml:math id="M70"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <italic>ŷ</italic><sub><italic>t</italic>−3</sub> are the predicted x and y coordinates at time <italic>t</italic> − 3. We did not train the recurrent network to take actions from these decoded values. Instead, using the decoded position and direction of the ball, a straight line was fitted to the direction of the ball, and the paddle movement was simulated accordingly, to intersect with the ball. The paddle size was equal to the size of the ball. The accuracy was implemented by comparing the ball position with the paddle. If both overlapped, the agent scored a point. The score was calculated as the number of successful trials over the total number of trials.</p></sec><sec id="S36"><title>Place field calculation</title><p id="P95">Place field experiment involved the same game environment as pong (offline version). The simulation of the network activity consisted of showing solely one ball trajectory 200 times during one simulation. A single ball position that was part of the training trajectory was shown for 2s, first at the beginning of the simulation (”naive network” in <xref ref-type="fig" rid="F4">Fig. 4E</xref>) and 30s after the last trajectory was shown (”after pong” in <xref ref-type="fig" rid="F4">Fig. 4E</xref>). The weights were extracted for each connection type and sorted by their direction and distance along the x-axis of the 64x64 grid network. For each neuron in the grid, we calculated the distribution of weights of the connections sent to the neurons on the left side (right-to-left direction) vs the weights of the connections sent to the neurons on the right side (left-to-right direction). The summed weights were plotted against the distance on the x-axis of the grid.</p></sec><sec id="S37"><title>Pong - Real-time</title><p id="P96">This game version extends the offline pong to the real-time prediction of the ball direction and control of the paddle. The game used the same environment as described in the section above. To decrease the computations needed to train the readout model, a model trained offline was used to predict the ball position in real-time. Real-time pong followed the same protocol as its offline version: ball position was tracked during the game and immediately provided as a sensory input to the network. Next, the activity of the network was normalized and sent to the readout which predicted the direction of the ball by fitting a linear equation to two predicted positions. As a result, the paddle was controlled. Real-time pong allowed for changing the velocity of the ball and initializing the ball in different directions than during the offline game. Two different versions of readout models for the network with static or plastic connections were verified (Table III). First, the mean score per trial and the maximum score for the games were measured with models that were trained offline and validated in real-time (no fine-tuning). Second, the same statistics were measured for models that underwent offline training and a small real-time fine-tuning for 50 trials before validating them. This was done to leverage any possible differences related to a real-time setup such as different ball trajectories or speed.</p></sec></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary material</label><media xlink:href="EMS206052-supplement-Supplementary_material.zip" mimetype="application" mime-subtype="zip" id="d8aAcEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S38"><title>Acknowledgments</title><p>We would like to thank Tim Behrens, Henning Sprekeler, Nicolas Brunel, David Scheinberg, Andrew Peters, Matteo Carandini, Maneesh Sahani, Nicoleta Condruz, Chaitanya Chintaluri and Douglas Feitosa Tomé for useful discussions, as well as Stefano Elefante and Alois Schlögl for their help with deploying simulations on the ISTA cluster.</p><sec id="S39"><title>Funding</title><p>This project has received funding from the HORIZON EUROPE European Research Council (ERC) consolidator grant (SYNAPSEEK, awarded to T.V.), a Wellcome Trust Sir Henry Dale Research Fellowship (WT100000, awarded to T.V.), a Wellcome Trust Senior Research Fellowship (214316/Z/18/Z, awarded to T.V.). AS was supported by a Schmidt Science Polymath Award, the Sainsbury Wellcome Centre Core Grant from Wellcome (219627/Z/19/Z) and the Gatsby Charitable Foundation (GAT3850). P.R. and J.H.M. were supported by the German Research Foundation (DFG; Germany’s Excellence Strategy MLCoE – EXC number 2064/1 PN 390727645), the German Federal Ministry of Education and Research (BMBF; Tübingen AI Center, FKZ: 01IS18039A) and the ERC through a consolidator grant (DeepCoMechTome, awarded to J.H.M.). A.N.K. was supported by an FWO grant (G097022N). This research was supported by the Scientific Service Units (SSU) of IST Austria through resources provided by Scientific Computing (SciComp).</p></sec></ack><sec id="S40" sec-type="data-availability"><title>Data and materials availability</title><p id="P97">All the code for running the networks and reproducing the analysis is available on Github <ext-link ext-link-type="uri" xlink:href="https://github.com/VogelsLab/dataSBI">https://github.com/VogelsLab/dataSBI</ext-link>.</p></sec><fn-group><fn fn-type="con" id="FN1"><p id="P98"><bold>Author contributions</bold></p><p id="P99">BC, PR, PG, JHM and TV designed the study, BC, MK and ZH ran and analyzed the simulations. BC and TV wrote the manuscript, with help from all coauthors. PB created the online supplementary material.</p></fn><fn fn-type="conflict" id="FN2"><p id="P100"><bold>Competing interests</bold></p><p id="P101">There are no competing interests to declare.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markram</surname><given-names>H</given-names></name><name><surname>Lübke</surname><given-names>J</given-names></name><name><surname>Frotscher</surname><given-names>M</given-names></name><name><surname>Sakmann</surname><given-names>B</given-names></name></person-group><article-title>Regulation of synaptic efficacy by coincidence of postsynaptic aps and epsps</article-title><source>Science</source><year>1997</year><volume>275</volume><fpage>213</fpage><lpage>215</lpage><pub-id pub-id-type="pmid">8985014</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bi</surname><given-names>G-q</given-names></name><name><surname>Poo</surname><given-names>M-m</given-names></name></person-group><article-title>Synaptic modifications in cultured hippocampal neurons: dependence on spike timing, synaptic strength, and postsynaptic cell type</article-title><source>Journal of neuroscience</source><year>1998</year><volume>18</volume><fpage>10464</fpage><lpage>10472</lpage><pub-id pub-id-type="pmcid">PMC6793365</pub-id><pub-id pub-id-type="pmid">9852584</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.18-24-10464.1998</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gerstner</surname><given-names>W</given-names></name><name><surname>Kempter</surname><given-names>R</given-names></name><name><surname>Van Hemmen</surname><given-names>JL</given-names></name><name><surname>Wagner</surname><given-names>H</given-names></name></person-group><article-title>A neuronal learning rule for sub-millisecond temporal coding</article-title><source>Nature</source><year>1996</year><volume>383</volume><fpage>76</fpage><lpage>78</lpage><pub-id pub-id-type="pmid">8779718</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfister</surname><given-names>J-P</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><article-title>Triplets of spikes in a model of spike timing-dependent plasticity</article-title><source>Journal of Neuroscience</source><year>2006</year><volume>26</volume><fpage>9673</fpage><lpage>9682</lpage><pub-id pub-id-type="pmcid">PMC6674434</pub-id><pub-id pub-id-type="pmid">16988038</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1425-06.2006</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clopath</surname><given-names>C</given-names></name><name><surname>Büsing</surname><given-names>L</given-names></name><name><surname>Vasilaki</surname><given-names>E</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><article-title>Connectivity reflects coding: a model of voltage-based stdp with homeostasis</article-title><source>Nature Neuroscience</source><year>2010</year><volume>13</volume><fpage>344</fpage><lpage>352</lpage><pub-id pub-id-type="pmid">20098420</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Payeur</surname><given-names>A</given-names></name><name><surname>Guerguiev</surname><given-names>J</given-names></name><name><surname>Zenke</surname><given-names>F</given-names></name><name><surname>Richards</surname><given-names>BA</given-names></name><name><surname>Naud</surname><given-names>R</given-names></name></person-group><article-title>Burst-dependent synaptic plasticity can coordinate learning in hierarchical circuits</article-title><source>Nature neuroscience</source><year>2021</year><volume>24</volume><fpage>1010</fpage><lpage>1019</lpage><pub-id pub-id-type="pmid">33986551</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Halvagal</surname><given-names>MS</given-names></name><name><surname>Zenke</surname><given-names>F</given-names></name></person-group><article-title>The combination of hebbian and predictive plasticity learns invariant object representations in deep sensory networks</article-title><source>Nature Neuroscience</source><year>2023</year><volume>26</volume><fpage>1906</fpage><lpage>1915</lpage><pub-id pub-id-type="pmcid">PMC10620089</pub-id><pub-id pub-id-type="pmid">37828226</pub-id><pub-id pub-id-type="doi">10.1038/s41593-023-01460-y</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bittner</surname><given-names>KC</given-names></name><name><surname>Milstein</surname><given-names>AD</given-names></name><name><surname>Grienberger</surname><given-names>C</given-names></name><name><surname>Romani</surname><given-names>S</given-names></name><name><surname>Magee</surname><given-names>JC</given-names></name></person-group><article-title>Behavioral time scale synaptic plasticity underlies ca1 place fields</article-title><source>Science</source><year>2017</year><volume>357</volume><fpage>1033</fpage><lpage>1036</lpage><pub-id pub-id-type="pmcid">PMC7289271</pub-id><pub-id pub-id-type="pmid">28883072</pub-id><pub-id pub-id-type="doi">10.1126/science.aan3846</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litwin-Kumar</surname><given-names>A</given-names></name><name><surname>Doiron</surname><given-names>B</given-names></name></person-group><article-title>Formation and maintenance of neuronal assemblies through synaptic plasticity</article-title><source>Nature Communications</source><year>2014</year><volume>5</volume><pub-id pub-id-type="pmid">25395015</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zenke</surname><given-names>F</given-names></name><name><surname>Agnes</surname><given-names>EJ</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><article-title>Diverse synaptic plasticity mechanisms orchestrated to form and retrieve memories in spiking neural networks</article-title><source>Nature communications</source><year>2015</year><volume>6</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="pmcid">PMC4411307</pub-id><pub-id pub-id-type="pmid">25897632</pub-id><pub-id pub-id-type="doi">10.1038/ncomms7922</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soldado-Magraner</surname><given-names>S</given-names></name><name><surname>Seay</surname><given-names>MJ</given-names></name><name><surname>Laje</surname><given-names>R</given-names></name><name><surname>Buonomano</surname><given-names>DV</given-names></name></person-group><article-title>Paradoxical self-sustained dynamics emerge from orchestrated excitatory and inhibitory homeostatic plasticity rules</article-title><source>Proceedings of the National Academy of Sciences</source><year>2022</year><volume>119</volume><pub-id pub-id-type="pmcid">PMC9618084</pub-id><pub-id pub-id-type="pmid">36251988</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2200621119</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Confavreux</surname><given-names>B</given-names></name><name><surname>Ramesh</surname><given-names>P</given-names></name><name><surname>Goncalves</surname><given-names>PJ</given-names></name><name><surname>Macke</surname><given-names>JH</given-names></name><name><surname>Vogels</surname><given-names>TP</given-names></name></person-group><source>Meta-learning families of plasticity rules in recurrent spiking networks using simulation-based inference</source><conf-name>Thirty-seventh Conference on Neural Information Processing Systems</conf-name><year>2023</year></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zenke</surname><given-names>F</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name></person-group><article-title>The temporal paradox of hebbian learning and homeostatic plasticity</article-title><source>Current opinion in neurobiology</source><year>2017</year><volume>43</volume><fpage>166</fpage><lpage>176</lpage><pub-id pub-id-type="pmid">28431369</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Bengio</surname><given-names>S</given-names></name><name><surname>Cloutier</surname><given-names>J</given-names></name></person-group><source>Learning a synaptic learning rule</source><conf-name>IJCNN-91-Seattle International Joint Conference on Neural Networks</conf-name><year>1991</year><volume>2</volume><fpage>969</fpage></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Stevenson</surname><given-names>IH</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name></person-group><source>Inferring spike-timing-dependent plasticity from spike train data</source><conf-name>Advances in neural information processing systems</conf-name><conf-sponsor>NeurIPS</conf-sponsor><year>2014</year><volume>27</volume></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Linderman</surname><given-names>S</given-names></name><name><surname>Stock</surname><given-names>C</given-names></name><name><surname>Adams</surname><given-names>R</given-names></name></person-group><source>A framework for studying synaptic plasticity with neural spike train data</source><conf-name>Advances in neural information processing systems</conf-name><conf-sponsor>NeurIPS</conf-sponsor><year>2014</year><volume>27</volume></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robinson</surname><given-names>BS</given-names></name><name><surname>Berger</surname><given-names>TW</given-names></name><name><surname>Song</surname><given-names>D</given-names></name></person-group><article-title>Identification of stable spike-timing-dependent plasticity from spiking activity with generalized multilinear modeling</article-title><source>Neural Computation</source><year>2016</year><volume>28</volume><fpage>2320</fpage><lpage>2351</lpage><pub-id pub-id-type="pmid">27557101</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Metz</surname><given-names>L</given-names></name><name><surname>Maheswaranathan</surname><given-names>N</given-names></name><name><surname>Cheung</surname><given-names>B</given-names></name><name><surname>Sohl-Dickstein</surname><given-names>J</given-names></name></person-group><article-title>Learning unsupervised learning rules</article-title><source>arXiv preprint</source><year>2018</year><elocation-id>1804.00222</elocation-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lappalainen</surname><given-names>J</given-names></name><name><surname>Herpich</surname><given-names>J</given-names></name><name><surname>Tetzlaff</surname><given-names>C</given-names></name></person-group><article-title>A theoretical framework to derive simple, firing-rate-dependent mathematical models of synaptic plasticity</article-title><source>Frontiers in computational neuroscience</source><year>2019</year><volume>13</volume><fpage>26</fpage><pub-id pub-id-type="pmcid">PMC6517541</pub-id><pub-id pub-id-type="pmid">31133837</pub-id><pub-id pub-id-type="doi">10.3389/fncom.2019.00026</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lindsey</surname><given-names>J</given-names></name><name><surname>Litwin-Kumar</surname><given-names>A</given-names></name></person-group><article-title>Learning to learn with feedback and local plasticity</article-title><source>arXiv preprint</source><year>2020</year><elocation-id>2006.09549</elocation-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Confavreux</surname><given-names>B</given-names></name><name><surname>Zenke</surname><given-names>F</given-names></name><name><surname>Agnes</surname><given-names>EJ</given-names></name><name><surname>Lillicrap</surname><given-names>T</given-names></name><name><surname>Vogels</surname><given-names>TP</given-names></name></person-group><source>A meta-learning approach to (re) discover plasticity rules that carve a desired function into a neural network</source><conf-name>Advances in Neural Information Processing Systems</conf-name><conf-sponsor>NeurIPS</conf-sponsor><year>2020</year><volume>34</volume></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jordan</surname><given-names>J</given-names></name><name><surname>Schmidt</surname><given-names>M</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name><name><surname>Petrovici</surname><given-names>MA</given-names></name></person-group><article-title>Evolving interpretable plasticity for spiking networks</article-title><source>eLife</source><year>2021</year><volume>10</volume><elocation-id>e66273</elocation-id><pub-id pub-id-type="pmcid">PMC8553337</pub-id><pub-id pub-id-type="pmid">34709176</pub-id><pub-id pub-id-type="doi">10.7554/eLife.66273</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tyulmankov</surname><given-names>D</given-names></name><name><surname>Yang</surname><given-names>GR</given-names></name><name><surname>Abbott</surname><given-names>L</given-names></name></person-group><article-title>Meta-learning synaptic plasticity and memory addressing for continual familiarity detection</article-title><source>Neuron</source><year>2022</year><volume>110</volume><fpage>544</fpage><lpage>557</lpage><pub-id pub-id-type="pmcid">PMC8813911</pub-id><pub-id pub-id-type="pmid">34861149</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2021.11.009</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shervani-Tabar</surname><given-names>N</given-names></name><name><surname>Rosenbaum</surname><given-names>R</given-names></name></person-group><article-title>Meta-learning biologically plausible plasticity rules with random feedback pathways</article-title><source>Nature Communications</source><year>2023</year><volume>14</volume><elocation-id>1805</elocation-id><pub-id pub-id-type="pmcid">PMC10066328</pub-id><pub-id pub-id-type="pmid">37002222</pub-id><pub-id pub-id-type="doi">10.1038/s41467-023-37562-1</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Miconi</surname><given-names>T</given-names></name></person-group><source>Learning to acquire novel cognitive tasks with evolution, plasticity and meta-meta-learning</source><conf-name>International Conference on Machine Learning</conf-name><year>2023</year><fpage>24756</fpage><lpage>24774</lpage></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miconi</surname><given-names>T</given-names></name><name><surname>Kay</surname><given-names>K</given-names></name></person-group><article-title>Neural mechanisms of relational learning and fast knowledge reassembly in plastic neural networks</article-title><source>Nature Neuroscience</source><year>2025</year><pub-id pub-id-type="pmid">39814949</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krouglova</surname><given-names>AN</given-names></name><name><surname>Johnson</surname><given-names>HR</given-names></name><name><surname>Confavreux</surname><given-names>B</given-names></name><name><surname>Deistler</surname><given-names>M</given-names></name><name><surname>Gonçalves</surname><given-names>PJ</given-names></name></person-group><article-title>Multifidelity simulation-based inference for computationally expensive simulators</article-title><source>aRxiv</source><year>2025</year><elocation-id>2502.08416</elocation-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adrian</surname><given-names>ED</given-names></name><name><surname>Matthews</surname><given-names>R</given-names></name></person-group><article-title>The action of light on the eye: Part i. the discharge of impulses in the optic nerve and its relation to the electric changes in the retina</article-title><source>The Journal of Physiology</source><year>1927</year><volume>63</volume><fpage>378</fpage><pub-id pub-id-type="pmcid">PMC1514941</pub-id><pub-id pub-id-type="pmid">16993896</pub-id><pub-id pub-id-type="doi">10.1113/jphysiol.1927.sp002410</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aitken</surname><given-names>K</given-names></name><name><surname>Campagnola</surname><given-names>L</given-names></name><name><surname>Garrett</surname><given-names>ME</given-names></name><name><surname>Olsen</surname><given-names>SR</given-names></name><name><surname>Mihalas</surname><given-names>S</given-names></name></person-group><article-title>Simple synaptic modulations implement diverse novelty computations</article-title><source>Cell Reports</source><year>2024</year><volume>43</volume><pub-id pub-id-type="pmcid">PMC12054332</pub-id><pub-id pub-id-type="pmid">38713584</pub-id><pub-id pub-id-type="doi">10.1016/j.celrep.2024.114188</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Asabuki</surname><given-names>T</given-names></name><name><surname>Gillon</surname><given-names>CJ</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name></person-group><article-title>Learning predictive signals within a local recurrent circuit</article-title><source>bioRxiv</source><year>2023</year><fpage>2023</fpage><lpage>06</lpage></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peters</surname><given-names>AJ</given-names></name><name><surname>Marica</surname><given-names>A-M</given-names></name><name><surname>Fabre</surname><given-names>JM</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name></person-group><article-title>Visuomotor learning promotes visually evoked activity in the medial prefrontal cortex</article-title><source>Cell Reports</source><year>2022</year><volume>41</volume><pub-id pub-id-type="pmcid">PMC9631115</pub-id><pub-id pub-id-type="pmid">36261004</pub-id><pub-id pub-id-type="doi">10.1016/j.celrep.2022.111487</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lim</surname><given-names>S</given-names></name><name><surname>McKee</surname><given-names>JL</given-names></name><name><surname>Woloszyn</surname><given-names>L</given-names></name><name><surname>Amit</surname><given-names>Y</given-names></name><name><surname>Freedman</surname><given-names>DJ</given-names></name><name><surname>Sheinberg</surname><given-names>DL</given-names></name><name><surname>Brunel</surname><given-names>N</given-names></name></person-group><article-title>Inferring learning rules from distributions of firing rates in cortical neurons</article-title><source>Nature Neuroscience</source><year>2015</year><volume>18</volume><fpage>1804</fpage><lpage>1810</lpage><pub-id pub-id-type="pmcid">PMC4666720</pub-id><pub-id pub-id-type="pmid">26523643</pub-id><pub-id pub-id-type="doi">10.1038/nn.4158</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foster</surname><given-names>DJ</given-names></name><collab>W. M. A</collab></person-group><article-title>Reverse replay of behavioural sequences in hippocampal place cells during the awake state</article-title><source>Nature</source><year>2006</year><volume>440</volume><fpage>680</fpage><lpage>683</lpage><pub-id pub-id-type="pmid">16474382</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>MA</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name></person-group><article-title>Reactivation of hippocampal ensemble memories during sleep</article-title><source>Science</source><year>1994</year><volume>265</volume><fpage>676</fpage><lpage>679</lpage><pub-id pub-id-type="pmid">8036517</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>AK</given-names></name><name><surname>Wilson</surname><given-names>MA</given-names></name></person-group><article-title>Memory of sequential experience in the hippocampus during slow wave sleep</article-title><source>Neuron</source><year>2002</year><volume>36</volume><fpage>1183</fpage><lpage>1194</lpage><pub-id pub-id-type="pmid">12495631</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harvey-Girard</surname><given-names>E</given-names></name><name><surname>Lewis</surname><given-names>J</given-names></name><name><surname>Maler</surname><given-names>L</given-names></name></person-group><article-title>Burst-induced anti-hebbian depression acts through short-term synaptic dynamics to cancel redundant sensory signals</article-title><source>Journal of Neuroscience</source><year>2010</year><volume>30</volume><fpage>6152</fpage><lpage>6169</lpage><pub-id pub-id-type="pmcid">PMC6632600</pub-id><pub-id pub-id-type="pmid">20427673</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0303-10.2010</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Egger</surname><given-names>V</given-names></name><name><surname>Feldmeyer</surname><given-names>D</given-names></name><name><surname>Sakmann</surname><given-names>B</given-names></name></person-group><article-title>Coincidence detection and changes of synaptic efficacy in spiny stellate neurons in rat barrel cortex</article-title><source>Nature neuroscience</source><year>1999</year><volume>2</volume><fpage>1098</fpage><lpage>1105</lpage><pub-id pub-id-type="pmid">10570487</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Nelson</surname><given-names>SB</given-names></name></person-group><article-title>Synaptic plasticity: taming the beast</article-title><source>Nature neuroscience</source><year>2000</year><volume>3</volume><fpage>1178</fpage><lpage>1183</lpage><pub-id pub-id-type="pmid">11127835</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogels</surname><given-names>TP</given-names></name><name><surname>Sprekeler</surname><given-names>H</given-names></name><name><surname>Zenke</surname><given-names>F</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><article-title>Inhibitory plasticity balances excitation and inhibition in sensory pathways and memory networks</article-title><source>Science</source><year>2011</year><volume>334</volume><fpage>1569</fpage><lpage>1573</lpage><pub-id pub-id-type="pmid">22075724</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chance</surname><given-names>FS</given-names></name><name><surname>Abbott</surname><given-names>L</given-names></name><name><surname>Reyes</surname><given-names>AD</given-names></name></person-group><article-title>Gain modulation from background synaptic input</article-title><source>Neuron</source><year>2002</year><volume>35</volume><fpage>773</fpage><lpage>782</lpage><pub-id pub-id-type="pmid">12194875</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Froemke</surname><given-names>RC</given-names></name><name><surname>Merzenich</surname><given-names>MM</given-names></name><name><surname>Schreiner</surname><given-names>CE</given-names></name></person-group><article-title>A synaptic memory trace for cortical receptive field plasticity</article-title><source>Nature</source><year>2007</year><volume>450</volume><fpage>425</fpage><lpage>429</lpage><pub-id pub-id-type="pmid">18004384</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neftci</surname><given-names>EO</given-names></name><name><surname>Mostafa</surname><given-names>H</given-names></name><name><surname>Zenke</surname><given-names>F</given-names></name></person-group><article-title>Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks</article-title><source>IEEE Signal Processing Magazine</source><year>2019</year><volume>36</volume></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zenke</surname><given-names>F</given-names></name><name><surname>Vogels</surname><given-names>TP</given-names></name></person-group><article-title>The remarkable robustness of surrogate gradient learning for instilling complex function in spiking neural networks</article-title><source>Neural computation</source><year>2021</year><volume>33</volume><fpage>899</fpage><lpage>925</lpage><pub-id pub-id-type="pmid">33513328</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mnih</surname><given-names>V</given-names></name><name><surname>Kavukcuoglu</surname><given-names>K</given-names></name><name><surname>Silver</surname><given-names>D</given-names></name><name><surname>Graves</surname><given-names>A</given-names></name><name><surname>Antonoglou</surname><given-names>I</given-names></name><name><surname>Wierstra</surname><given-names>D</given-names></name><name><surname>Riedmiller</surname><given-names>M</given-names></name></person-group><article-title>Playing atari with deep reinforcement learning</article-title><source>arXiv preprint</source><year>2013</year><elocation-id>arXiv:1312.5602</elocation-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kagan</surname><given-names>BJ</given-names></name><name><surname>Kitchen</surname><given-names>AC</given-names></name><name><surname>Tran</surname><given-names>NT</given-names></name><name><surname>Habibollahi</surname><given-names>F</given-names></name><name><surname>Khajehnejad</surname><given-names>M</given-names></name><name><surname>Parker</surname><given-names>BJ</given-names></name><name><surname>Bhat</surname><given-names>A</given-names></name><name><surname>Rollo</surname><given-names>B</given-names></name><name><surname>Razi</surname><given-names>A</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><article-title>In vitro neurons learn and exhibit sentience when embodied in a simulated game-world</article-title><source>Neuron</source><year>2022</year><volume>110</volume><fpage>3952</fpage><lpage>3969</lpage><pub-id pub-id-type="pmcid">PMC9747182</pub-id><pub-id pub-id-type="pmid">36228614</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2022.09.001</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gerstner</surname><given-names>W</given-names></name><name><surname>Abbott</surname><given-names>L</given-names></name></person-group><article-title>Learning navigational maps through potentiation and modulation of hippocampal place cells</article-title><source>Journal of computational neuroscience</source><year>1997</year><volume>4</volume><fpage>79</fpage><lpage>94</lpage><pub-id pub-id-type="pmid">9046453</pub-id></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robinson</surname><given-names>NT</given-names></name><name><surname>Descamps</surname><given-names>LA</given-names></name><name><surname>Russell</surname><given-names>LE</given-names></name><name><surname>Buchholz</surname><given-names>MO</given-names></name><name><surname>Bicknell</surname><given-names>BA</given-names></name><name><surname>Antonov</surname><given-names>GK</given-names></name><name><surname>Lau</surname><given-names>JY</given-names></name><name><surname>Nutbrown</surname><given-names>R</given-names></name><name><surname>Schmidt-Hieber</surname><given-names>C</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name></person-group><article-title>Targeted activation of hippocampal place cells drives memory-guided spatial behavior</article-title><source>Cell</source><year>2020</year><volume>183</volume><fpage>1586</fpage><lpage>1599</lpage><pub-id pub-id-type="pmcid">PMC7773032</pub-id><pub-id pub-id-type="pmid">33357402</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2020.12.010</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yao</surname><given-names>Z</given-names></name><name><surname>van Velthoven</surname><given-names>CT</given-names></name><name><surname>Kunst</surname><given-names>M</given-names></name><name><surname>Zhang</surname><given-names>M</given-names></name><name><surname>McMillen</surname><given-names>D</given-names></name><name><surname>Lee</surname><given-names>C</given-names></name><name><surname>Jung</surname><given-names>W</given-names></name><name><surname>Goldy</surname><given-names>J</given-names></name><name><surname>Abdelhak</surname><given-names>A</given-names></name><name><surname>Aitken</surname><given-names>M</given-names></name><etal/></person-group><article-title>A high-resolution transcriptomic and spatial atlas of cell types in the whole mouse brain</article-title><source>Nature</source><year>2023</year><volume>624</volume><fpage>317</fpage><lpage>332</lpage><pub-id pub-id-type="pmcid">PMC10719114</pub-id><pub-id pub-id-type="pmid">38092916</pub-id><pub-id pub-id-type="doi">10.1038/s41586-023-06812-z</pub-id></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abraham</surname><given-names>WC</given-names></name><name><surname>Bear</surname><given-names>MF</given-names></name></person-group><article-title>Metaplasticity: the plasticity of synaptic plasticity</article-title><source>Trends in Neurosciences</source><year>1996</year><volume>19</volume><pub-id pub-id-type="pmid">8658594</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abraham</surname><given-names>WC</given-names></name></person-group><article-title>Metaplasticity: tuning synapses and networks for plasticity</article-title><source>Nature Reviews Neuroscience</source><year>2008</year><volume>9</volume><pub-id pub-id-type="pmid">18401345</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schultz</surname><given-names>W</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Montague</surname><given-names>PR</given-names></name></person-group><article-title>A neural substrate of prediction and reward</article-title><source>Science</source><year>1997</year><volume>275</volume><fpage>1593</fpage><lpage>1599</lpage><pub-id pub-id-type="pmid">9054347</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhong</surname><given-names>L</given-names></name><name><surname>Baptista</surname><given-names>S</given-names></name><name><surname>Gattoni</surname><given-names>R</given-names></name><name><surname>Arnold</surname><given-names>J</given-names></name><name><surname>Flickinger</surname><given-names>D</given-names></name><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name></person-group><article-title>Distinct streams for supervised and unsupervised learning in the visual cortex</article-title><source>bioRxiv</source><year>2024</year><pub-id pub-id-type="doi">10.1101/2024.02.25.581990</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maass</surname><given-names>W</given-names></name><name><surname>Natschläger</surname><given-names>T</given-names></name><name><surname>Markram</surname><given-names>H</given-names></name></person-group><article-title>Real-time computing without stable states: a new framework for neural computation based on perturbations</article-title><source>Neural computation</source><year>2002</year><volume>14</volume><pub-id pub-id-type="pmid">12433288</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="report"><person-group person-group-type="author"><name><surname>Jaeger</surname><given-names>H</given-names></name></person-group><source>The” echo state” approach to analysing and training recurrent neural networks-with an erratum note’ Bonn, Germany: German National Research Center for Information Technology GMD Technical Report</source><year>2001</year><volume>148</volume></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zenke</surname><given-names>F</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><article-title>Limits to high-speed simulations of spiking neural networks using general-purpose computers</article-title><source>Frontiers in neuroinformatics</source><year>2014</year><volume>8</volume><fpage>76</fpage><pub-id pub-id-type="pmcid">PMC4160969</pub-id><pub-id pub-id-type="pmid">25309418</pub-id><pub-id pub-id-type="doi">10.3389/fninf.2014.00076</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stimberg</surname><given-names>M</given-names></name><name><surname>Brette</surname><given-names>R</given-names></name><name><surname>Goodman</surname><given-names>DF</given-names></name></person-group><article-title>Brian 2, an intuitive and efficient neural simulator</article-title><source>elife</source><year>2019</year><volume>8</volume><elocation-id>e47314</elocation-id><pub-id pub-id-type="pmcid">PMC6786860</pub-id><pub-id pub-id-type="pmid">31429824</pub-id><pub-id pub-id-type="doi">10.7554/eLife.47314</pub-id></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alevi</surname><given-names>D</given-names></name><name><surname>Stimberg</surname><given-names>M</given-names></name><name><surname>Sprekeler</surname><given-names>H</given-names></name><name><surname>Obermayer</surname><given-names>K</given-names></name><name><surname>Augustin</surname><given-names>M</given-names></name></person-group><article-title>Brian2cuda: flexible and efficient simulation of spiking neural network models on gpus</article-title><source>Frontiers in Neuroinformatics</source><year>2022</year><volume>16</volume><pub-id pub-id-type="pmcid">PMC9660315</pub-id><pub-id pub-id-type="pmid">36387586</pub-id><pub-id pub-id-type="doi">10.3389/fninf.2022.883700</pub-id></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Wieser</surname><given-names>E</given-names></name><name><surname>Taylor</surname><given-names>J</given-names></name><name><surname>Berg</surname><given-names>S</given-names></name><name><surname>Smith</surname><given-names>NJ</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><etal/></person-group><article-title>Array programming with NumPy</article-title><source>Nature</source><year>2020</year><volume>585</volume><fpage>357</fpage><lpage>362</lpage><pub-id pub-id-type="pmcid">PMC7759461</pub-id><pub-id pub-id-type="pmid">32939066</pub-id><pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A</given-names></name><name><surname>Gross</surname><given-names>S</given-names></name><name><surname>Massa</surname><given-names>F</given-names></name><name><surname>Lerer</surname><given-names>A</given-names></name><name><surname>Bradbury</surname><given-names>J</given-names></name><name><surname>Chanan</surname><given-names>G</given-names></name><name><surname>Killeen</surname><given-names>T</given-names></name><name><surname>Lin</surname><given-names>Z</given-names></name><name><surname>Gimelshein</surname><given-names>N</given-names></name><name><surname>Antiga</surname><given-names>L</given-names></name><name><surname>Desmaison</surname><given-names>A</given-names></name><etal/></person-group><article-title>Pytorch: An imperative style, high-performance deep learning library</article-title><source>CoRR</source><year>2019</year><elocation-id>abs/1912.01703</elocation-id><comment>1912.01703</comment></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McKinney</surname><given-names>W</given-names></name><etal/></person-group><article-title>pandas: a foundational python library for data analysis and statistics</article-title><source>Python for high performance and scientific computing</source><year>2011</year><volume>14</volume><fpage>1</fpage><lpage>9</lpage></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunter</surname><given-names>JD</given-names></name></person-group><article-title>Matplotlib: A 2d graphics environment</article-title><source>Computing in Science &amp; Engineering</source><year>2007</year><volume>9</volume><fpage>90</fpage><lpage>95</lpage></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name><name><surname>Haberland</surname><given-names>M</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Burovski</surname><given-names>E</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Bright</surname><given-names>J</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><etal/></person-group><article-title>SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python</article-title><source>Nature Methods</source><year>2020</year><volume>17</volume><fpage>261</fpage><lpage>272</lpage><pub-id pub-id-type="pmcid">PMC7056644</pub-id><pub-id pub-id-type="pmid">32015543</pub-id><pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Michel</surname><given-names>V</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Grisel</surname><given-names>O</given-names></name><name><surname>Blondel</surname><given-names>M</given-names></name><name><surname>Prettenhofer</surname><given-names>P</given-names></name><name><surname>Weiss</surname><given-names>R</given-names></name><name><surname>Dubourg</surname><given-names>V</given-names></name><name><surname>Vanderplas</surname><given-names>J</given-names></name><etal/></person-group><article-title>Scikit-learn: Machine learning in Python</article-title><source>Journal of Machine Learning Research</source><year>2011</year><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></element-citation></ref><ref id="R64"><label>64</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tejero-Cantero</surname><given-names>A</given-names></name><name><surname>Boelts</surname><given-names>J</given-names></name><name><surname>Deistler</surname><given-names>M</given-names></name><name><surname>Lueckmann</surname><given-names>J-M</given-names></name><name><surname>Durkan</surname><given-names>C</given-names></name><name><surname>Gonçalves</surname><given-names>PJ</given-names></name><name><surname>Greenberg</surname><given-names>DS</given-names></name><name><surname>Macke</surname><given-names>JH</given-names></name></person-group><article-title>sbi: A toolkit for simulation-based inference</article-title><source>Journal of Open Source Software</source><year>2020</year><volume>5</volume><elocation-id>2505</elocation-id></element-citation></ref><ref id="R65"><label>65</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strong</surname><given-names>V</given-names></name><name><surname>Holderbaum</surname><given-names>W</given-names></name><name><surname>Hayashi</surname><given-names>Y</given-names></name></person-group><article-title>Electro-active polymer hydrogels exhibit emergent memory when embodied in a simulated game environment</article-title><source>Cell Reports Physical Science</source><year>2024</year><volume>5</volume></element-citation></ref><ref id="R66"><label>66</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maass</surname><given-names>W</given-names></name><name><surname>Natschlager</surname><given-names>T</given-names></name><name><surname>Markram</surname><given-names>H</given-names></name></person-group><article-title>Real-time computing without stable states: A new framework for neural computation based on perturbations</article-title><source>Neural computation</source><year>2002</year><volume>314</volume><fpage>2531</fpage><lpage>2560</lpage></element-citation></ref><ref id="R67"><label>67</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woloszyn</surname><given-names>L</given-names></name><name><surname>Sheinberg</surname><given-names>DL</given-names></name></person-group><article-title>Effects of long-term visual experience on responses of distinct classes of single units in inferior temporal cortex</article-title><source>Neuron</source><year>2012</year><volume>74</volume><fpage>193</fpage><lpage>205</lpage><pub-id pub-id-type="pmcid">PMC3329224</pub-id><pub-id pub-id-type="pmid">22500640</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2012.01.032</pub-id></element-citation></ref><ref id="R68"><label>68</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Astrid A Prinz</surname><given-names>DB</given-names></name><name><surname>Marder</surname><given-names>E</given-names></name></person-group><article-title>Similar network activity from disparate circuit parameters</article-title><source>Nature Neuroscience</source><year>2004</year><volume>7</volume><fpage>1345</fpage><lpage>1352</lpage><pub-id pub-id-type="pmid">15558066</pub-id></element-citation></ref><ref id="R69"><label>69</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramesh</surname><given-names>P</given-names></name><name><surname>Confavreux</surname><given-names>B</given-names></name><name><surname>Goncalves</surname><given-names>PJ</given-names></name><name><surname>Vogels</surname><given-names>TP</given-names></name><name><surname>Macke</surname><given-names>JH</given-names></name></person-group><article-title>Indistinguishable network dynamics can emerge from unalike plasticity rules</article-title><source>eLife</source><year>2024</year></element-citation></ref><ref id="R70"><label>70</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Agnes</surname><given-names>EJ</given-names></name><name><surname>Vogels</surname><given-names>TP</given-names></name></person-group><article-title>Co-dependent excitatory and inhibitory plasticity accounts for quick, stable and long-lasting memories in biological networks</article-title><source>Nature Neuroscience</source><year>2024</year><volume>27</volume><fpage>964</fpage><lpage>974</lpage><pub-id pub-id-type="pmcid">PMC11089004</pub-id><pub-id pub-id-type="pmid">38509348</pub-id><pub-id pub-id-type="doi">10.1038/s41593-024-01597-4</pub-id></element-citation></ref><ref id="R71"><label>71</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maass</surname><given-names>W</given-names></name><name><surname>Natschläger</surname><given-names>T</given-names></name><name><surname>Markram</surname><given-names>H</given-names></name></person-group><article-title>Real-time computing without stable states: A new framework for neural computation based on perturbations</article-title><source>Neural computation</source><year>2002</year><volume>14</volume><fpage>2531</fpage><lpage>2560</lpage></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Fig. 1</label><caption><title>Rule quadruplets meta-learned for stability elicit memories.</title><p><bold>A</bold>: Recurrent spiking network with 4096 excitatory and 1024 inhibitory neurons. <bold>B</bold>: Each recurrent synapse type (EE, EI, IE and II) has its own, STDP-like, plasticity rule. Weight change as a function of the time-lag between a pre- and a post-synaptic spike. <bold>C</bold>: Obtaining the stability manifold starting from a uniform prior. <bold>D</bold>: Familiarity detection task. Middle: example rule quadruplet in this task. Top: pre-post protocol of the rule quadruplet, like in B. Bottom: raster plot of 250 excitatory neurons, and population firing rate. Dotted vertical lines denote onset and offset of the stimuli presentations. Right: evaluation of network function in the familiarity task for each meta-learned rule tested. The black line denotes the last timestep at which the rule quadruplet displayed a significant difference between novel and familiar. <bold>E</bold>: Same as D but now reading out memories from ON responses. <bold>F</bold>: Sequence learning task: a network is trained for 50s on a loop of 5 stimuli. During testing, each training stimulus is presented independently, as well as two novel stimuli. All stimuli are non-overlapping. <bold>G</bold>: Contextual novelty task, comparing network population responses when familiar stimuli are presented in vs out of order. <bold>H</bold>: Left: percentage of rule quadruplets from the 2,500 sampled from the stability manifold that have significant responses for least at one break time in the task shown above. Right: distribution of the number of functions per rule quadruplet.</p></caption><graphic xlink:href="EMS206052-f001"/></fig><fig id="F2" position="float"><label>Fig. 2</label><caption><title>Isolating subfamilies of rule quadruplets consistent with data.</title><p><bold>A</bold>: Using the stability manifold as a prior, new posteriors with specific network functions are obtained with the same inference pipeline as in <xref ref-type="fig" rid="F1">Fig. 1</xref> (see <xref ref-type="sec" rid="S9">methods</xref>). <bold>B</bold>: Left: Overlay of the shapes of quadruplets that robustly elicit long-term novelty detection, tested on 4h-simulations; Middle: Firing rate of an example quadruplet undergoing the familiarity task; Right: Response of the 2,500 meta-learned quadruplets used in <xref ref-type="fig" rid="F1">Fig. 1</xref> on the familiarity task, with difference in mean (x-axis) and variance (y-axis) in the network response to familiar and novel stimuli, static networks (with no plasticity), data from<sup><xref ref-type="bibr" rid="R31">31</xref>,<xref ref-type="bibr" rid="R32">32</xref></sup>. <bold>C</bold>-<bold>H</bold>: Same as B, but selecting for robust transient familiarity/transient contextual novelty/replay-like events/sequential memory without temporal unfolding of the memory at recall/robust ON responses (see <xref ref-type="sec" rid="S9">methods</xref> for the criteria to define the function of quadruplets). Example quadruplets are shown, as well as the network dynamics that they elicit (similar format as <xref ref-type="fig" rid="F1">Fig. 1F</xref>). Bottom right: EE matrix of a quadruplet exhibiting spontaneous replay. Weights are grouped by which engram their pre- and post-synaptic (excitatory) neurons belong to (see <xref ref-type="sec" rid="S9">methods</xref> for engram definition). ’rs’ stands for the rest of neurons not participating in any of the 5 familiar engrams. <bold>I</bold>: Selecting quadruplets based on the network function as well as their shape (condition on the plasticity parameter values). Here, selecting stable quadruplets with EE rules that are similar to the STDP rule<sup><xref ref-type="bibr" rid="R1">1</xref>–<xref ref-type="bibr" rid="R3">3</xref></sup>. <bold>K</bold>: Considering only the 2,500 quadruplets sampled from the stability manifold (and not from the subsequent posteriors with specific network functions), proportion of rules that exhibit each function shown above (see <xref ref-type="sec" rid="S9">methods</xref> for selection criteria).</p></caption><graphic xlink:href="EMS206052-f002"/></fig><fig id="F3" position="float"><label>Fig. 3</label><caption><title>Analysis of the stability manifold.</title><p><bold>A</bold>: Histogram of the shapes of the 2,500 rule quadruplets stable for at least 4h used in <xref ref-type="fig" rid="F1">Fig. 1</xref>. <bold>B</bold>: Left: Principal Component Analysis (PCA) performed on the plasticity parameters from the quadruplets shown in A, variance explained by each dimension. Right: PCA performed on the network responses elicited by the quadruplets in the tasks shown in <xref ref-type="fig" rid="F1">Fig. 1</xref>. <bold>C</bold>: Two quadruplets from the stability manifold (shown in <xref ref-type="fig" rid="F1">Fig. 1E</xref> and <xref ref-type="fig" rid="F2">Fig. 2G</xref>) in plasticity parameter space shown with strong black borders. A chosen random plane containing the two quadruplets and simulated quadruplets sampled across this plane. Small black dots denote unstable quadruplets. Larger dots denote stable quadruplets and the color represents their response after 10 minutes on the familiarity task (same color bar as <xref ref-type="fig" rid="F1">Fig. 1D</xref>). <bold>D</bold>: Left: Considering the 2,500 stable quadruplets used in Fig.1, simulating networks with only one of the 4 rules active (”triple KO”), and computing the fraction of these networks that were stable. Right: Same when removing a single rule from the stable quadruplets (“single KO”). Black connections are static (KO), colored connections are plastic. <bold>E</bold>: Raster plot and evolution of 100 weights of each synapse type and their means in bold, for two rule quadruplets. The left one is stable (see <xref ref-type="fig" rid="F1">Fig. 1G</xref>), the right one has the Hebbian terms of the II rule swapped <inline-formula><mml:math id="M71"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>pre</mml:mtext><mml:mspace width="0.2em"/></mml:mrow><mml:mrow><mml:mtext>II</mml:mtext></mml:mrow></mml:msubsup><mml:mo>↔</mml:mo><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>post</mml:mtext><mml:mspace width="0.2em"/></mml:mrow><mml:mrow><mml:mtext>II</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M72"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mtext>II</mml:mtext></mml:mrow></mml:msub><mml:mo>↔</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mtext>II</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and has unrealistically high inhibitory weights. <bold>F</bold>: Same as B for two other quadruplets. the left one is stable (see <xref ref-type="fig" rid="F3">Fig. 3</xref>), the right one has the non-Hebbian parameters of the EE rule swapped (<italic>α</italic><sub>EE</sub> ↔ <italic>β</italic><sub>EE</sub>, note this cannot be seen on the pre-post protocol plots). <bold>G</bold>: For each of the 2,500 stable quadruplets used in Fig.1, comparisons of two simulations with changes to one task parameter at a time. From left to right: Firing rate of input neurons that are part of a stimulus pattern while that pattern is active, initial network connectivity (strength of initialization and random initialization), initial network connectivity (random initialization seed only) and general tonic input firing rate received by all recurrent neurons in the network. The circles denote 50% of the mass, the color indicates different break times.</p></caption><graphic xlink:href="EMS206052-f003"/></fig><fig id="F4" position="float"><label>Fig. 4</label><caption><title>Beyond unsupervised learning in isolation: additional rules and network functions.</title><p><bold>A</bold>: Example quadruplet from the stability manifold with different responses on the familiarity task depending on the tonic input background firing rate (see <xref ref-type="fig" rid="F3">Fig. 3G</xref>). <bold>B</bold>: Learning linear readouts on top of a plastic recurrent network playing pong. <bold>C</bold>: Validation set accuracy at pong for three rule quadruplets. Quadruplet 1 is shown in <xref ref-type="fig" rid="F2">Fig. 2G</xref> and elicits sequential replay, quadruplet 2 corresponds to no plasticity (static network), quadruplet 3 elicits no function on any of the memory tasks. <bold>D</bold>: Given the network activity at time <italic>t</italic> (with the ball at position (<italic>x, y</italic>)), accuracy of three networks with the three rule quadruplets described above at predicting current and previous ball positions. <bold>E</bold>: Firing rates of recurrent excitatory neurons of the network evolving with quadruplet 1, during presentation of the same ball position before and after the network as been exposed to several pong sequences. <bold>F</bold>: Weight distribution of the network evolving with quadruplet 1 after minutes of exposure to pong. Synapses are sorted by their distance and direction along the x-axis (axis along which the ball moves). <bold>G</bold>: Same tasks as in <xref ref-type="fig" rid="F1">Fig. 1</xref>, but considering different plasticity rule parameterization, MLP-based (see <xref ref-type="sec" rid="S9">methods</xref>). Note that only the EE and IE synapses are plastic in this case. Bottom: Three example doublets, two undergoing the familiarity task and one the sequential task.</p></caption><graphic xlink:href="EMS206052-f004"/></fig></floats-group></article>