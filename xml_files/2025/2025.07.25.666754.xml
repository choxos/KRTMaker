<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS207596</article-id><article-id pub-id-type="doi">10.1101/2025.07.25.666754</article-id><article-id pub-id-type="archive">PPR1055829</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Under which circumstances do genomic neural networks learn motifs and their interactions?</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Thompson</surname><given-names>Mike</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Lehner</surname><given-names>Ben</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03wyzt892</institution-id><institution>Center for Genomic Regulation (CRG)</institution></institution-wrap>, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/03kpps236</institution-id><institution>The Barcelona Institute of Science and Technology</institution></institution-wrap>, <addr-line>Dr. Aiguader 88</addr-line>, <city>Barcelona</city><postal-code>08003</postal-code>, <country country="ES">Spain</country></aff><aff id="A2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04n0g0b29</institution-id><institution>Universitat Pompeu Fabra (UPF)</institution></institution-wrap>, <city>Barcelona</city><postal-code>08002</postal-code>, <country country="ES">Spain</country></aff><aff id="A3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0371hy230</institution-id><institution>ICREA</institution></institution-wrap>, <addr-line>Pg. Lluis Companys 23</addr-line>, <city>Barcelona</city><postal-code>08010</postal-code>, <country country="ES">Spain</country></aff><aff id="A4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05cy4wa09</institution-id><institution>Wellcome Sanger Institute</institution></institution-wrap>, <addr-line>Wellcome Genome Campus</addr-line>, <city>Hinxton</city><postal-code>CB10 1RQ</postal-code>, <country country="GB">UK</country></aff><author-notes><corresp id="CR1">
<label>*</label>Corresponding authors <email>mikejthomps@gmail.com</email>; <email>ben.lehner@crg.eu</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>28</day><month>07</month><year>2025</year></pub-date><pub-date pub-type="preprint"><day>27</day><month>07</month><year>2025</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">The use of neural networks to model genomic data in sequence-to-function scenarios has soared over the last decade. There remains much debate about whether these models are interpretable, either inherently, or when using downstream interpretability or explainability techniques (xAI). Conclusions are further complicated by the steady publication of novel models, each with their own architectures, evaluations, and xAI experimental designs. Here, we posit that many of these complications arise due to a lack of explicit specification of a generative model, baseline comparators, and thorough evaluation. Consequently, we attempt to reconcile concerns of interpretability under a motif-based generative model by simulating at scale over 1000 motif-based genetic architectures and evaluating the ability of different model architectures to predict an outcome given a sequence as input. We first show that a single convolutional layer is sufficient to discover motifs in a sequence-to-function model due to the way in which it shares the gradient locally amongst nucleotides. We next build upon this by showing that across genetic and network architectures—including attention, LSTMs, and stacked convolutions—most models are capable of modeling motifs and their interactions, with certain models outperforming others across genetic contexts and sample sizes. Distinguishing between shallow-level interpretations of motifs and deeper, gradient-based interpretations of motifs, we show that these approaches discover separate but overlapping sets of motifs, depending on motif characteristics. Finally, we validate our findings on an experimental dataset, and conclude that while attention is accurate, there are genetic contexts in which other neural networks complement findings from attention-based models and produce higher correlations between predictive performance and interpretability. The work here suggests that when a generative model is correctly specified, most models are to an extent interpretable, whether their architectures are inherently so or not. Moreover, our work highlights opportunities for methods development in motif discovery and also implies that employing a mixture of model architectures may be best for biological discovery.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><label>1</label><title>Introduction</title><p id="P2">Neural networks have become a foundational tool for modeling a wide variety of regulatory processes[<xref ref-type="bibr" rid="R1">1</xref>, <xref ref-type="bibr" rid="R2">2</xref>, <xref ref-type="bibr" rid="R3">3</xref>, <xref ref-type="bibr" rid="R4">4</xref>]. Despite the success of these complex models achieving state-of-the-art accuracy on a variety of prediction tasks, there is currently no consensus about which architecture and post-hoc interpretability analyses best capture motifs—the functional unit underlying many of these biological processes—and their interactions. As interpretability becomes an increasing concern for deep learning models of biological processes, understanding when their explanations are fiable, and when they are not, is crucial[<xref ref-type="bibr" rid="R4">4</xref>, <xref ref-type="bibr" rid="R5">5</xref>].</p><p id="P3">Since their introduction into computational genetics, neural networks have leveraged convolutions as the near-universal first-layer architectual component[<xref ref-type="bibr" rid="R1">1</xref>, <xref ref-type="bibr" rid="R2">2</xref>, <xref ref-type="bibr" rid="R3">3</xref>, <xref ref-type="bibr" rid="R4">4</xref>, <xref ref-type="bibr" rid="R6">6</xref>]. This is primarily due to two reasons: first, allowing a model to learn its own features for prediction tasks is a rational design choice, and second, convolutions have an immediately reifiable meaning as position weight matrices (PWM), the representative data structure of biological motifs. Indeed, the authors of [<xref ref-type="bibr" rid="R1">1</xref>, <xref ref-type="bibr" rid="R3">3</xref>] were able to show that by either via visualizing the first-layer convolutional filters (i.e., PWMs) or by running <italic>in silico</italic> experiments (called “mutation maps”), these models learn biological motifs underlying the biological processes that they model. Whether the fact that these models can quite simply be interpreted to reveal their learned motifs is serendipitous or not, the tangential question of why convolutions enable extraction of these motifs (i.e., what is the exact mechanism by which convolutions directly enable the identification of such motifs), remains unanswered.</p><p id="P4">Genomic neural networks have enjoyed many of the advances in interpretability methods (or explainable AI “xAI”) for models of image-recognition, or other fields in which neural networks dominate[<xref ref-type="bibr" rid="R4">4</xref>, <xref ref-type="bibr" rid="R7">7</xref>, <xref ref-type="bibr" rid="R8">8</xref>]. Most prevalent in xAI interpretations of genomic neural networks has been the use of attribution methods, in which per-feature importance of an input sample is calculated either by taking the gradient of the trained model with respect to the input features, or via more involved methods, such as DeepLIFT[<xref ref-type="bibr" rid="R9">9</xref>]. Leveraging the regulatory information embedded in attribution maps across sequences, tf-MoDIsCO uses these importances to construct motifs that affect the biological mechanism represented by the trained model[<xref ref-type="bibr" rid="R10">10</xref>]. When interpreting a model, researchers have the choice of using some combination of tf-MoDIsCO, first-layer filter visualizations, or additional methods [<xref ref-type="bibr" rid="R2">2</xref>], yet evaluation of the complementarity of these methods, and for which genomic architectures or motifs they reveal different insights, is understudied.</p><p id="P5">Attention (or its use in transformer blocks) has more recently become the dominating paradigm for genomic network architectures[<xref ref-type="bibr" rid="R11">11</xref>, <xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R13">13</xref>]. This is not only due to the predictive performance of models that use attention, but also due to the understanding that the attention mechanism and its values are to some extent interpretable—the scores of attention may represent interactions between positions, more specifically, interactions which are correlated to known biological phenonema, such as contact maps in proteins[<xref ref-type="bibr" rid="R14">14</xref>] or motif cooperativity in genomics[<xref ref-type="bibr" rid="R15">15</xref>, <xref ref-type="bibr" rid="R16">16</xref>, <xref ref-type="bibr" rid="R17">17</xref>, <xref ref-type="bibr" rid="R18">18</xref>]. For example, SATORI directly used the learned attention matrices to show that interactions among motifs are enriched in attention matrices with higher values[<xref ref-type="bibr" rid="R15">15</xref>]. SATORI has since been extended to leverage sparsity in its attention maps[<xref ref-type="bibr" rid="R16">16</xref>], or to include first-layer convolution activations in calculating its interactions[<xref ref-type="bibr" rid="R17">17</xref>]. All of these methods, however, compare the power to direct motif interaction effects only across other models that include attention (perhaps due to the fact that there is no obvious component of other models to use in such evaluations). Nonetheless, other architectures are indeed capable of learning motif interactions. For example, SpliceAI[<xref ref-type="bibr" rid="R19">19</xref>], which models splice site usage of a genomic sequence, uses only convolutional layers with residual connections and dilations, yet it achieves state-of-the-art accuracy over a modeling task that requires learning representations and mechanisms across a substantial number of motifs and contexts.</p><p id="P6">Given the above limited comparisons, and the fact that the aforementioned techniques generally focus on classification tasks[<xref ref-type="bibr" rid="R20">20</xref>, <xref ref-type="bibr" rid="R21">21</xref>] (which can be biased by interactions that are present in cases but not in controls [<xref ref-type="bibr" rid="R22">22</xref>]) or overly-simplistic deterministic simulations (wherein labels are hard-assigned by presence or absence of motifs or motif combinations[<xref ref-type="bibr" rid="R16">16</xref>, <xref ref-type="bibr" rid="R18">18</xref>]), we propose a more comprehensive analysis of these models. We posit that genomic neural networks have been considered uninterpretable black-box models in part because there is rarely a specific generative model from which the data is believed to arise. Here, we argue that when a generative model is specified (and assumed correct), predictive performance can be a measurement of model ability to capture motifs and their interactions. We explore these arguments and set out to answer the question <italic>Under which specific genomic architectures do attention modules or additional context-specifying modules (e.g. LSTM, dilated convolutions) lead to increased network expressivity?</italic> by first proposing a novel simulation framework for genomic sequences. Using this framework, we then rigorously examine a wide range of neural network architectures and post-hoc interpretability analyses to (1) evaluate current practices for constructing learned motifs of networks and (2) evaluate whether certain architectures or post-hoc explainability methods better capture motif interactions than others. We finally replicate our conclusions in experimental data, and summarize our contributions as follows: <list list-type="bullet" id="L1"><list-item><p id="P7">Convolutions enable learning and xAI extraction of motifs due to the manner in which they regularize the local gradient over a sequence.</p></list-item><list-item><p id="P8">Networks utilizing attention are most powerful when phenotypic variability is entirely explained by motifs and interactions, however they are surpassed by stacked convolutions and LSTMs in the presence of non-motif, sequence-based effects.</p></list-item><list-item><p id="P9">Motif discovery power is highest for strong effect and high information-content motifs.</p></list-item><list-item><p id="P10">Independent of architecture, motif length is negatively associated with discovery power and filter length, past a certain threshold, has a slight negative association with discovery power.</p></list-item><list-item><p id="P11">Correlations between motif discovery power and predictive performance are modest, and worsen with greater sample sizes, and under simpler architectures, highlighting a gap in extraction of learned-motifs, or benign overfitting.</p></list-item></list></p></sec><sec id="S2" sec-type="methods"><label>2</label><title>Methods</title><p id="P12">To evaluate neural network architectures and motif discovery methods, we simulated ground truth data with a variety of tunable parameters. Importantly, we set our basis of simulations upon random sequences. Random sequences enable researchers to overcome problems of overly optimistic predictive performance due to homology, as well as to learn more generalizable sequence and motif grammars as random sequences comprise a much wider range of contexts than the genome[<xref ref-type="bibr" rid="R23">23</xref>]. Additionally, as researchers continue to perform experiments using synthetic sequences and random sequences[<xref ref-type="bibr" rid="R24">24</xref>, <xref ref-type="bibr" rid="R25">25</xref>, <xref ref-type="bibr" rid="R26">26</xref>, <xref ref-type="bibr" rid="R27">27</xref>, <xref ref-type="bibr" rid="R28">28</xref>, <xref ref-type="bibr" rid="R29">29</xref>], it is crucial to understand the circumstances in which neural networks produce predictions that are faithful to the underlying biology in order to maximize the insight gained from these experiments.</p><p id="P13">Accordingly, the background sequences comprised 200 base-pairs with a uniform distribution of nucleotides at each position. Into these sequences, we injected up to 10 motifs downloaded from JASPAR[<xref ref-type="bibr" rid="R30">30</xref>], each with their own probability drawn from a Uniform distribution (half of the motifs came from Unif[0.1, 0.4] and the other half Unif[0.4, 0.8], distributions were shuffled per simulation run). To increase motif variability, the motifs per sequence were injected as a realization from the collected position-weight matrices (PWM). After drawing and potentially injecting motifs into a sequence (into non-overlapping positions drawn from Unif[1, 200 –|motif|] per motif and sequence), a phenotype was generated following a basic variance components model: <disp-formula id="FD1"><label>(1)</label><mml:math id="M1"><mml:mi>y</mml:mi><mml:mspace width="0.5em"/><mml:mo>=</mml:mo><mml:mspace width="0.5em"/><mml:mi>M</mml:mi><mml:mi>β</mml:mi><mml:mo>+</mml:mo><mml:mi>I</mml:mi><mml:mi>γ</mml:mi><mml:mo>+</mml:mo><mml:mi>ε</mml:mi></mml:math></disp-formula></p><p id="P14">Where <italic>y</italic> is a length-<italic>n</italic> vector of the phenotypes across the <italic>n</italic> sequences, <italic>M</italic> is a scaled <italic>n</italic> × <italic>p</italic> matrix of sequences’ presence of <italic>p</italic> motifs, <italic>β</italic> is length-p vector representing the effects of the motifs, <italic>I</italic> is a scaled <italic>n</italic> × <italic>q</italic> matrix across sequences representing the presence of multiple motifs in the arrangement required for them to interact (specified below), <italic>γ</italic> is a length-q vector representing the interaction effects, and <italic>ɛ</italic> is a length-<italic>n</italic> vector of normally distributed noise. Importantly, we define a quantity <italic>h</italic><sup>2</sup> which we term “motif-explainability,” that represents the variance in phenotype that can be explained by motifs and their interactions. This quantity is analogous to heritability in the statistical genetics field[<xref ref-type="bibr" rid="R31">31</xref>]. We partition the motif-explainability into one component of simple additive effects <italic>β</italic>, which act on <italic>M</italic>, and another that is explained by motif interactions <italic>γ</italic>, which act on <italic>I</italic>. To introduce sparsity of motif effects, we sample per simulation run a subset of <italic>π</italic> proportion of motifs to have an additive effect. We split the proportion of motif-explainability across motifs and their interactions via a parameter <italic>α</italic>, which when set to 0 generates phenotypes whose variability is explained exclusively by interactive effects, and only additive effects when set to 1. More rigorously, we simulate motif effects <italic>β</italic> as: <disp-formula id="FD2"><label>(2)</label><mml:math id="M2"><mml:mi>β</mml:mi><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mi>α</mml:mi><mml:msup><mml:mi>h</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>π</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula> interaction effects as: <disp-formula id="FD3"><label>(3)</label><mml:math id="M3"><mml:mi>γ</mml:mi><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mi>q</mml:mi></mml:mfrac></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula> and finally draw random noise <italic>ε</italic> from: <disp-formula id="FD4"><label>(4)</label><mml:math id="M4"><mml:mi>ε</mml:mi><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P15">For the purpose of simplifying the simulations, we limit our evaluations to situations in which the motif explainability is set to 80%, there are only 10 motifs (6 of which have an additive effect) and a set of 4 interaction classes among at most 3 hypothetical motifs A, B and C (which are randomly selected from all 10 motifs per interaction and per simulation run) as follows: <list list-type="simple" id="L2"><list-item><p id="P16"><bold>Simple interactions</bold> The presence of both motif A and motif B at any position in the sequence is sufficient for them to interact.</p></list-item><list-item><p id="P17"><bold>Order interactions</bold> Motifs A and B only interact if motif A is upstream of motif B.</p></list-item><list-item><p id="P18"><bold>Distance interactions</bold> Motifs A and B only interact if the distance between them is less than 4 basepairs.</p></list-item><list-item><p id="P19"><bold>High-order interactions</bold> The presence of motifs A, B, and C produces an interaction.</p></list-item></list>
</p><p id="P20">In the text, a given simulation run refers to a specific realization of background sequences, motif frequencies, motif presence, motif positions, motifs with non-zero effect sizes, sets of motifs involved in the interactions, beta and gamma effect sizes, and noise all drawn under one specific combination of <italic>α</italic>, 𝜓, and sample size parameters. We performed 50 simulation runs for each distinct combination of parameters. We also evaluated scenarios in which there are non-motif, sequence-based effects on the phenotype (<xref ref-type="supplementary-material" rid="SD1">Appendix</xref>). We include the simulations script, Probabilistic Evaluation of genomic NEUral networks (PRENEU) and all accompanying code under <ext-link ext-link-type="uri" xlink:href="https://github.com/mj-thompson/PRENEU">https://github.com/mj-thompson/PRENEU</ext-link>.</p></sec><sec id="S3" sec-type="results"><label>3</label><title>Results</title><sec id="S4"><title>Convolutions regularize the local gradient of a sequence</title><p id="P21">We begin our analyses with a simple simulations experiment to elucidate the manner by which convolutional layers bias the model toward learning motifs. Here, we simulated 100,000 length-200 random sequences, injected two length-7 motifs each with a probability 0.5 into a position within 3-nucleotide windows at the beginning and ends of each sequence, and assigned sequences with both motifs a label of 1, and all others, 0. Here, the model must learn a basic AND function that is slightly noisy due to the slight variance in motif locations between sequences. We train two models, the first is a multilayer perception (MLP) consisting of 3 dense layers, comprising roughly 52,000 parameters, and the other is a model first with a convolutional layer (and exponential activation[<xref ref-type="bibr" rid="R21">21</xref>]), followed by two additional dense layers, also made up of roughly 52,000 parameters (52,177 and 52,257 parameters respectively).</p><p id="P22">Both models predicted the held-out 10,000 sequences’ class with near-perfect accuracy (area under receiver-operating characteristic curve (AUC) &gt; 0.99 and area under precision-recall curve (AUPR) &gt; 0.97 for both models; <xref ref-type="fig" rid="F1">Fig. 1B</xref>). To make sense of the models’ learned function in a manner amenable to both architectures, we generated for each sequence an attribution map containing the adjusted[<xref ref-type="bibr" rid="R8">8</xref>] gradient of the sequence with respect to the trained models’ predictions. The attribution maps of the MLP suggested benign overfitting—the model seemed to place emphasis on the most-common nucleotide within the length-3 nucleotide windows of the motifs, rather than across the entire injected motif (or motif window) itself[<xref ref-type="bibr" rid="R32">32</xref>, <xref ref-type="bibr" rid="R33">33</xref>]. Contrastingly, the attribution maps of the model with a first-layer convolution had near-zero attribution across the sequence, except for the motifs, where there was high attribution shared across the entire motif (<xref ref-type="fig" rid="F1">Fig. 1A</xref>). Following standard procedure within the field, we ran TF-MoDISco [<xref ref-type="bibr" rid="R10">10</xref>], a method which parses repeated substrings with high attribution for motif discovery in sequence-to-function models, to establish whether the inserted motifs were recoverable from the models. The attribution maps from the MLP led to no significant discoveries, whereas the attribution maps of the model with a convolutional layer captured both motifs with statistical significance (false discovery rate FDR q&lt; 0.05).</p><p id="P23">To make sense of the between-model differences in attribution at motifs, we collected the attribution scores in all sequences with an injected motif, then calculated Kruskall-Wallace (Uniform) and Kurtosis test statistics over the learned attribution scores. Intuitively, we tested for smoothness (uniformity) and absence of single-nucleotide concentrated attribution, or peaks (heavy tails, low kurtosis). Within motifs, the gradient of the MLP was both significantly (p&lt; 10<sup>−16</sup>) and substantially less uniform than that of the model with a convolutional layer (<xref ref-type="fig" rid="F1">Fig 1C</xref>). The kurtosis per sequence followed the same trend (p&lt; 10<sup>−16</sup>; <xref ref-type="fig" rid="F1">Fig. 1D</xref>). Moreover, this trend was consistent throughout 9 additional replicates of generating sequences and training both models per generated dataset (<xref ref-type="fig" rid="F1">Fig. 1E</xref>), as well as other experimental designs that used two other pairs of motifs (<xref ref-type="supplementary-material" rid="SD1">Appendix</xref>).</p><p id="P24">We therefore posit that a single convolutional layer (here, with exponential activation[<xref ref-type="bibr" rid="R21">21</xref>]) is sufficient for a model to learn motifs due to the way it regularizes the gradient. Qualitatively, the inclusion of a convolutional layer led to shrinkage (collapse to 0) of the gradient throughout the sequence, except for within motifs, where the gradient was shared across the motif quite uniformly, rather than in the MLP in which most of the gradient was concentrated at a single nucleotide or two. This is akin to effect sizes learned over correlated variables in linear regression regularized by an elastic net—groups of correlated variables (here, nucleotides within a motif), are selected, then the effect size (gradient) is partitioned among them[<xref ref-type="bibr" rid="R34">34</xref>, <xref ref-type="bibr" rid="R35">35</xref>].</p></sec><sec id="S5"><title>Deeper-layer modules outperform others under specific genomic architectures and dataset sizes</title><p id="P25">Having established a baseline model (a single convolution followed by dense layers, <xref ref-type="table" rid="T1">Table 1</xref>) for motif discovery, we next performed more complex simulations to evaluate the ability of a range of commonly used neural network designs to capture motifs and their interactions in sequence-to-phenotype prediction tasks (<xref ref-type="table" rid="T1">Table 1</xref>). Here, we simulated phenotypes in which the sequence and its motifs explained nearly all of the phenotypic variance (Methods). We varied the simulations to include a variety of dataset sizes (few or many sequences) and genetic architectures, ranging from scenarios in which motif effects are exclusively additive to exclusively interactive, as well as scenarios in which there exist non-motif sequence effects that explain varying levels of phenotypic variance (Methods). We employed a collection of architectures that are prevalent in the genomic field, including stacked convolutions[<xref ref-type="bibr" rid="R3">3</xref>], stacked dilated convolutions[<xref ref-type="bibr" rid="R19">19</xref>, <xref ref-type="bibr" rid="R36">36</xref>], attention layers[<xref ref-type="bibr" rid="R15">15</xref>, <xref ref-type="bibr" rid="R37">37</xref>], and long short-term memory (LSTM) units[<xref ref-type="bibr" rid="R38">38</xref>, <xref ref-type="bibr" rid="R39">39</xref>]. We also evaluated whether certain components of these architectures (e.g. skip/residual connections[<xref ref-type="bibr" rid="R40">40</xref>]) had significant effects on the predictions made by the models. All models shared the same first layer, comprising a convolution with exponential activation, as was shown in the more simplistic simulations setting above.</p><p id="P26">Across most genetic architectures, nearly all tested models could significantly predict the phenotype (<xref ref-type="fig" rid="F2">Fig. 2A</xref>). Nonetheless, in the low data, highly over-parameterized regime (20,000 sequences with over ~200,000 parameters), the baseline, CNN (“Stacked convolutions”), and LSTM models almost always substantially under-performed compared to all other methods. For the baseline and LSTMs, this was ameliorated in the scenarios with 100,000 sequences, however, the CNN only became competitive in the high sample size (&gt;number parameters) scenarios.</p><p id="P27">Under genetic architectures in which motif effects were entirely additive, models with attention performed most consistently accurate across sample sizes. Interestingly, the baseline model performed worse than the other models, despite having the same convolutional layer as the others and motif effects being simply additive. When motif effects on the phenotype were exclusively due to interactions, most models performed worse than in the analogous exclusively additive scenarios. Attention again was the most efficient mechanism to model these effects, followed by dilated convolutions, likely due to these models’ fields-of-view encompassing the entire sequence simultaneously. Interestingly, dilated convolutions outperformed attention in the 100,000 and 300,000 sequence contexts. When there was a mixture of both additive and interaction motif effects, most models followed trends similar to the previous scenarios. Attention continued to perform consistently accurate and dilated convolutions outperformed attention once trained over a sufficient amount of data.</p><p id="P28">In the presence of non-motif, sequence-based effects (<xref ref-type="supplementary-material" rid="SD1">Appendix</xref>), dilated convolutions and LSTMs began to more drastically outcompete attention models. Moreover, these models were increasingly competitive as a function of the variance in the phenotype explained by the sequence-level, non-motif effect, exemplifying their preference toward learning sequential effects, and perhaps highlighting a deficit in the attention module’s ability to properly contextualize motif effects in these contexts.</p><p id="P29">Finally, in an abalation-style analysis, we evaluated the effect of several design choices on model performance. Interestingly, including residual connections in the dilated convolutions led to a loss of power across several tested genetic architectures. Nonetheless, these connections were proposed to stablize gradient flow when training dilated CNNs[<xref ref-type="bibr" rid="R19">19</xref>, <xref ref-type="bibr" rid="R40">40</xref>], rather than optimize their predictive power. Additionally, we assumed that including the annotated hidden states of the LSTM per position would assist the models in learning interaction effects (e.g., by retaining positional information). However, using only the final cell state of the LSTM systematically outperformed using only the learned hidden states of the whole sequence (while keeping parameter counts similar in both models, <xref ref-type="table" rid="T1">Table 1</xref>). Lastly, we trained convolution-attention networks with and without pooling after the convolutional layer and before the attention layers, hypothesizing that the pooling may lead to better motif recognition and construction[<xref ref-type="bibr" rid="R6">6</xref>], as well as consequential effect-size modeling. We found that attention in the absence of pooling consistently underperformed than when including pooling, with more pronounced differences in the interactive genetic architectures. However, this difference in prediction accuracy does not validate our hypothesis about motif construction, for which we turned to model explanation methods (i.e., explainable AI, xAI).</p></sec><sec id="S6"><title>Recovered motifs vary by motif characteristics, network architecture, and interpretability strategy</title><p id="P30">We next performed a series of model interpretation strategies for all trained models and genetic architectures. We note that there exists a multitude of ways in which to interpret a trained model. Here, we focus on two of the most prevalent methods—visualizing first-layer convolutional filters (used interchangably with “Shallow”) and running TF-MoDISco over model attribution maps (“Deep”, <xref ref-type="supplementary-material" rid="SD1">Appendix</xref>). Intuitively, the shallow approach should reveal features that are recognized and either directly used to predict the phenotype, or that are non-trivially combined in deeper layers to predict the phenotype[<xref ref-type="bibr" rid="R6">6</xref>]. The deep approach should capture motifs that are not just recognized but used by the model, as the attribution is calculated with respect to all other parameters and model prediction score[<xref ref-type="bibr" rid="R10">10</xref>]. Additionally, we highlight that our models all included 100 filters with a fixed width of 7. We did this intentionally to evaluate the effect of mis-specifying the model—in other words, evaluating what happens when motifs are either shorter or longer than the filter width.</p><p id="P31">Across all model architectures and interpretability strategies, each motif could be detected, reinforcing that a single convolutional layer with exponential activation is sufficient for non-zero power (<xref ref-type="fig" rid="F3">Fig. 3A</xref>)[<xref ref-type="bibr" rid="R21">21</xref>]. Nonetheless, the baseline method was consistently the most under-power method, which conveys the benefit of more complicated motif-embedding strategies in deeper layers. The effect of model mis-specification on motif discovery power was less straightforward. Longer motifs (motifs 5 and 6) did not appear to suffer from reduced power, despite being mis-specified by the model. Conversely, the shorter motifs 7, 8, and 9 were all relatively underpowered when compared to the correctly specified motifs (<xref ref-type="fig" rid="F3">Fig. 3A</xref>). This effect was most pronounced for motif7, though, we note that motif7 is a repetitive, low information content motif, which likely complicates its detection among random sequence backgrounds (we further explore this hypothesis below). Analogously to regression-based association studies, motifs were much better powered for discovery when their effect size was stronger, though power was less affected by motif frequencies (see below; <xref ref-type="supplementary-material" rid="SD1">Appendix</xref>).</p><p id="P32">While LSTMs, dilated convolutions, and models with attention performed similarly well in terms of prediction accuracy, models with attention were generally the highest-powered in terms of motif discovery. The differences in motif discovery were higher when using first-layer visualization as the explainability technique compared to TF-MoDISco (<xref ref-type="fig" rid="F3">Fig. 3A</xref>). Like previous reports, we found that stacked convolutions were relatively low-powered when using filter visualization as the interpretability strategy[<xref ref-type="bibr" rid="R6">6</xref>, <xref ref-type="bibr" rid="R21">21</xref>]. These works, and our results here, suggest that deeper CNNs create more complex features in deeper layers that enable them to to either reconstruct motifs or latent representations that can then be used to model motif effects or interactions. This is further evidenced by the necessity of larger amounts of data for increased performance in these models—building more complex representations likely requires larger sample sizes.</p><p id="P33">In terms of accuracy across xAI strategies, TF-MoDISco was generally much more conservative, but precise, than visualizing the first-layer filters (<xref ref-type="fig" rid="F3">Fig. 3 A,D</xref>). This is likely due to the fact that the filters capture general features and variability within the sequences, and the deeper layers are able to contextualize and learn the effects (including effect sizes of 0) of these motifs. We also scored discovered motifs in terms of redundancy—the number of times the motif was represented and called a significant hit (q&lt; 0.05) divided by the number of uniquely called motifs—and found that shallow motifs are substantially more redundant than deep motifs (<xref ref-type="fig" rid="F3">Fig. 3E</xref>). In its analysis of the attribution maps of sequences, TF-MoDISco is likely able to better cluster the motif occurrences and refine its representation, whereas perhaps first-layer filters are noisier, requiring more filters to accurately represent the motif, or simply that they benignly-overfit motifs to surrounding nucleotides. The latter is perhaps evidenced by the fact that removing pooling after the attention layer or using all hidden states at each nucleotide in the LSTM increased motif redundancy.</p><p id="P34">Notably, the predictive performance of each method was only moderately correlated with its motif discovery power (<xref ref-type="fig" rid="F3">Fig. 3C</xref>). A similar result has been shown previously [<xref ref-type="bibr" rid="R21">21</xref>, <xref ref-type="bibr" rid="R41">41</xref>], however, we show here that this phenomenon is general across model architectures and genomic grammars, to degrees that vary depending on the choice of deeper layer embedding strategies. LSTMs, for example, had the highest correlation of predictive performance and motif discovery power, whereas models with attention or dilated convolutions had the lowest correlation between the two measurements. The mechanisms by which this occurs are unclear. Perhaps since LSTMs generate their predictions based on directional recurrence over estimated motif presence, they are forced to learn accurate motifs to generate meaningful predictions, as they cannot “view” and adjust for earlier partial mismatches. Attention on the other hand may be able to examine motif matches or partial mismatches, and contextualize them across the entire sequence at once, potentially enabling it to generate powerful predictions with noisy motif reconstructions. Whatever the mechanism, we observed evidence for benign overfitting in many cases—the correlation of test performance and motif discovery power was generally worse under simple genetic architectures (purely additive motifs, no sequence-based effects), as well as when training with larger sample sizes (<xref ref-type="fig" rid="F4">Fig. 4</xref>, <xref ref-type="supplementary-material" rid="SD1">Appendix</xref>). Additionally, despite several models having limited performance under low training data regimes (N=20,000), their correlation with discovery power and test prediction performance was relatively high (Baseline, Dilated convolutions, LSTMs). This suggests that under lower-data regimes, the model may be more dependent on accurately reconstructing the motifs to make its predictions, whereas given enough data, the model may be able to construct more complicated features that are either less faithful to the generative model, or less amenable to discovery via the xAI approaches tested here.</p><p id="P35">We next turned our focus to motif interactions. For this analysis, we scored an interaction as “discovered” if all motifs involved with the interaction were captured. The simplest interaction, in which there is an interaction effect only if both involved motifs are present, was generally highly powered across architecture-xAI pairs (<xref ref-type="fig" rid="F3">Fig. 3B</xref>). Attention was most powerful using the shallow approach, whereas dilated convolutions were highest-powered when paired with MoDISco. The LSTM that used sequence hidden states also performed quite well for this interaction with both xAI methods. We next examined the upstream effect, in which there is only an interactive effect if motifs follow an order where one motif occurs earlier in the sequence than the other (Methods). The performance was similar to the simple effect, though dilated CNNs paired with MoDISco had reduced power, bringing their performance to that of the attention networks. The distance effect, where there is only an interaction effect if motifs have at most 3 basepairs in between them, was strikingly the most under-powered across network-xAI pairs. This is likely due to the fact that the filters are width 7, potentially leading the first-layer filters to attempt to include components of both motifs in a single filter and lowering the shallow score (<xref ref-type="supplementary-material" rid="SD1">Appendix</xref>). Moreover, the noisy filters in combination with the complex spacings between the motifs could lead to noisier attribution maps, which would affect subsequence-clustering ability in MoDISco. To conclude our analysis, we examined a higher-order effect, in which there was only an interaction effect when all 3 motifs were present in a sequence. Compared to the simple or upstream effects, the power was similar but reduced. In terms of ablation or within-architecture designs, pooling before the attention layer typically increased interaction discovery power, whereas residual connections decreased interaction discovery power in all but the simple interaction when using MoDISco. LSTMs including all hidden states generally outperformed LSTMs that used only the final cell state, potentially because they maintain more of the sequence-level or positional information. For simplicity’s sake, we have described results for the scenario in which there were 300,000 sequences, motif effects are only interactive, and there are no sequence-based non-motif effects. Nonetheless, we emphasize that similarly to the predictive performance, in the presence of sequence-based non-motif effects, dilated convolutions and LSTMs generally had increased motif discovery power over attention (<xref ref-type="supplementary-material" rid="SD1">Appendix</xref>).</p></sec><sec id="S7"><title>Evaluation on real data validates simulation findings and elucidates the effects of model design and motif characteristics on motif discovery power</title><p id="P36">To validate our findings, we turned to an experimental dataset in which the authors synthesized random sequences with varying combinations of transcription factor motifs and measured their downstream transcriptional activity [<xref ref-type="bibr" rid="R25">25</xref>]. The dataset comprises 57,342 length-246 sequences, spanning a total of 44 transcription factor motifs (whose lengths range from 9-22, with a median of 11.5 and mode of 10). Each sequence contained potentially no motifs, a single motif, a single motif with multiple replicates, or multiple motifs each with multiple replicates.</p><p id="P37">Unlike in the simulations setting, this experimental setting is within only a single genomic grammar. Accordingly, to characterize model tendencies, we trained each model with a given set of hyperparameters 10 times. We also varied the filter width from 3 to 31 (with a step size of 2) to probe downstream effects on predictive performance and motif discovery power. In terms of test predictive performance, the models followed a similar trend as compared to the simulations—the baseline and stacked CNNs performed worst, dilated convolutions were comparable with attention models, and LSTMs in some cases outperformed the rest (<xref ref-type="fig" rid="F5">Fig5A</xref>). Interestingly, dilated CNNs and LSTMs were able to converge to within 10% of their maximum predictive power using first-layer filters of length 3, whereas the baseline, stacked CNNs, and attention models required filters of at least 7 to perform comparably.</p><p id="P38">As model-motif discovery power in the experimental context was also consistent with the simulations context (<xref ref-type="fig" rid="F5">Fig. 5B</xref>), we sought to evaluate the effect of filter widths and motif characteristics on motif discovery power using regression models. Explicitly, we used linear mixed models with a binomial link to model out of the 10 times a model was trained, how many times a motif was discovered as a function of filter width, motif length, information content, and frequency, and finally, the motif-transcription activity association statistic (i.e., a proxy for the motif effect size, which here, is unknown; <xref ref-type="supplementary-material" rid="SD1">Appendix</xref>). As we were interested in learning general associations of discovery power across models, we treated model architecture as a random effect in the model.</p><p id="P39">As expected, motif effect size had the greatest influence on xAI discovery, and this influence was stronger with TF-MoDISco than the first-layer filters approach (<xref ref-type="table" rid="T2">Table 2</xref>). Moreover, motif information content was more important for the shallow approach relative to the deep approach. These results also replicated across our simulated data. Unexpectedly, longer filter widths were associated with higher discovery power across xAI approaches, even when accounting for motif lengths and effect sizes. However, in line with previous works that suggest a modest or insignificant effect of filter length[<xref ref-type="bibr" rid="R6">6</xref>], the association became negative and much weaker when limiting the regression to models with filter width 9 and above (shallow <italic>t</italic>=-3.59, p=3.4e-4, deep <italic>t</italic>=-2.78, p=5.4e-03). We excluded motif frequency from the regression model on the experimental data as most of the TFs had, by design, equal frequency, except for several motifs that caused ascertainment issues over motif effect sizes with their inclusion (<xref ref-type="supplementary-material" rid="SD1">Appendix</xref>). Nonetheless, on our simulated data where frequency was a random variable, we observed a very slight association of frequency to discovery power, but note there was difference in magnitude of its association relative to the other variables (<xref ref-type="table" rid="T2">Table 2</xref>).</p><p id="P40">Finally, we again examined the relationship between the number of unique motifs discovered and prediction performance on the test set. We limited the models to those with filter widths 9 and above to focus on models with significant test prediction accuracy and motif recovery. As in the simulation settings, LSTMs had the highest correlation (hidden=0.81, final=0.51), followed by the baseline (0.65), dilated convolutions (0.54, without residual connections=0.45), then attention (0.19, without pooling=0.08), and finally stacked CNNs (0.14). As many state-of-the-art methods leverage attention and dilated convolutions, understanding why this discrepancy is worse for these models than LSTMs or baseline models warrants further study. We also note that the differences between dilated convolutions with and without residual connections that we observed throughout this study are not at odds with previous literature—most ablation studies focus on different types of residual connections, rather than the absence thereof[<xref ref-type="bibr" rid="R42">42</xref>], and other studies have found evidence that residual connections may harm interpretability[<xref ref-type="bibr" rid="R43">43</xref>].</p></sec></sec><sec id="S8" sec-type="discussion"><label>4</label><title>Discussion</title><p id="P41">Neural networks enable biological discovery owing to the way that they can automatically construct features and use them to predict a variety of mechanisms. Here, we evaluated current abilities to extract some of this learned biology by using a simulations framework in which biological motifs make up most of the variance in a sequence-phenotype relationship. We found that existing models are well-powered, particularly in the case of large-effect, information-rich motifs. We also found that dilated convolutions and LSTMs can improve over attention models for discovery of motifs and their interactions, especially in the context of non-motif, sequence-based effects, and when using deep xAI approaches.</p><p id="P42">Importantly, our evaluation has several limitations. Primarily, we set out to establish general rules for motif discovery across random sequences, as we believe experimental data (rather than natural sequences) will become increasingly important for elucidating complex genetic grammar[<xref ref-type="bibr" rid="R24">24</xref>, <xref ref-type="bibr" rid="R25">25</xref>, <xref ref-type="bibr" rid="R26">26</xref>, <xref ref-type="bibr" rid="R27">27</xref>, <xref ref-type="bibr" rid="R28">28</xref>, <xref ref-type="bibr" rid="R29">29</xref>]. Future work could perform a similar evaluation using natural sequences as the background. Second, our models and dataset sizes were only roughly ~250,000 parameters and at most 300,000 samples. Experimental data is increasing in availability, and it is common for models to reach many millions of parameters[<xref ref-type="bibr" rid="R11">11</xref>]. We attempted to show what happens in different sample-size-to-parameter ratios, but perhaps a worthy next direction is expanding these ratios and scenarios. Moreover, we limited our analysis to 8 fixed architectures. Generally, model building may encompass more architecture choices, but almost always includes hyperparameter optimization—which we only conducted over filter widths. Of course, further hyperparameter searches may affect the results, but our goal was to discuss the relationship of the generative model and predictive performance as well as to explore the effect of architectural inductive bias on a fixed parameter budget. Nonetheless, by running the evaluation over 1,500 scenarios, and replicating our findings on an experimental dataset, we are hopeful that our findings are indeed generalizable.</p><p id="P43">Given that the generative model is known, predictive performance should indeed serve as a measure of the model’s ability to discover motifs and their interactions. The modest correlation between predictive performance and motif discovery power, however, suggests either a gap in our ability to extract model-learned motifs, benign overfitting, or some combination thereof. Of particular interest is that this correlation declined as a function of simplicity in the genetic architecture and training sample size. Future work may address this gap, or focus on improving motif-discovery power, via, for example, regularizing filter redundancy, exploring extensions of attention, or perhaps even decaying residual connections[<xref ref-type="bibr" rid="R44">44</xref>].</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Appendix</label><media xlink:href="EMS207596-supplement-Appendix.pdf" mimetype="application" mime-subtype="pdf" id="d46aAcEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S9"><title>Acknowledgments</title><p>We would first like to thank Rosa Martinez Corral for initial discussions and push to turn this into a paper, without her motivation this manuscript would not have happened. We appreciate useful discussions with Lars Velten about the Frommel et al. dataset. We are also grateful to the UCLA hoffman2 server and support.</p><sec id="S10"><title>Funding</title><p>This work received support from the following: “La Caixa” Foundation (ID 100010434) under grant agreement LCF/PR/HR21/52410004 (B.L.); European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme grant agreement 883742 (B.L.); AXA Research Fund AXA Chair in Risk prediction in age-related diseases (B.L.); Secretariat of Universities and Research, Ministry of Enterprise and Knowledge of the Government of Catalonia and the European Social Funds 2017 SGR 1322 (B.L.); Bettencourt Schueller Foundation (B.L.); PID2023-146685NB-I00 funded by MCIN/AEI/10.13039/501100011033/FEDER, UE; Wellcome 220540/Z/20/A, “Wellcome Sanger Institute Quinquennial Review 2021-2026” (B.L.); EMBO Fellowship ALTF 266-2023 (M.T.).</p></sec></ack><fn-group><fn id="FN1" fn-type="con"><p id="P44"><bold>Author contributions</bold> Conceptualization: M.T. Computational analyses: M.T. Software: M.T. Manuscript writing: M.T. with input from B.L. Visualization: M.T. Funding acquisition: M.T. and B.L.</p></fn></fn-group><ref-list><title>References</title><ref id="R1"><label>[1]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>Jian</given-names></name><name><surname>Troyanskaya</surname><given-names>Olga G</given-names></name></person-group><article-title>Predicting effects of noncoding variants with deep learning-based sequence model</article-title><source>Nat Methods</source><year>2015</year><month>October</month><volume>12</volume><issue>10</issue><fpage>931</fpage><lpage>934</lpage><pub-id pub-id-type="doi">10.1038/nmeth.3547</pub-id><pub-id pub-id-type="pmcid">PMC4768299</pub-id><pub-id pub-id-type="pmid">26301843</pub-id></element-citation></ref><ref id="R2"><label>[2]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alipanahi</surname><given-names>Babak</given-names></name><name><surname>Delong</surname><given-names>Andrew</given-names></name><name><surname>Weirauch</surname><given-names>Matthew T</given-names></name><name><surname>Frey</surname><given-names>Brendan J</given-names></name></person-group><article-title>Predicting the sequence specificities of DNA- and RNA-binding proteins by deep learning</article-title><source>Nat Biotechnol</source><year>2015</year><month>August</month><volume>33</volume><issue>8</issue><fpage>831</fpage><lpage>838</lpage><pub-id pub-id-type="pmid">26213851</pub-id></element-citation></ref><ref id="R3"><label>[3]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kelley</surname><given-names>David R</given-names></name><name><surname>Snoek</surname><given-names>Jasper</given-names></name><name><surname>John</surname><given-names>L</given-names></name></person-group><article-title>Rinn. Basset: learning the regulatory code of the accessible genome with deep convolutional neural networks</article-title><source>Genome Research</source><year>2016</year><volume>26</volume><issue>7</issue><fpage>990</fpage><lpage>999</lpage><pub-id pub-id-type="doi">10.1101/gr.200535.115</pub-id><pub-id pub-id-type="pmcid">PMC4937568</pub-id><pub-id pub-id-type="pmid">27197224</pub-id></element-citation></ref><ref id="R4"><label>[4]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Novakovsky</surname><given-names>Gherman</given-names></name><name><surname>Dexter</surname><given-names>Nick</given-names></name><name><surname>Libbrecht</surname><given-names>Maxwell W</given-names></name><name><surname>Wasserman</surname><given-names>Wyeth W</given-names></name><name><surname>Mostafavi</surname><given-names>Sara</given-names></name></person-group><article-title>Obtaining genetics insights from deep learning via explainable artificial intelligence</article-title><source>Nature Reviews Genetics</source><year>2022</year><month>October</month><volume>24</volume><issue>2</issue><fpage>125</fpage><lpage>137</lpage><pub-id pub-id-type="pmid">36192604</pub-id></element-citation></ref><ref id="R5"><label>[5]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koo</surname><given-names>Peter K</given-names></name><name><surname>Ploenzke</surname><given-names>Matt</given-names></name></person-group><article-title>Deep learning for inferring transcription factor binding sites</article-title><source>Current Opinion in Systems Biology</source><year>2020</year><volume>19</volume><fpage>16</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1016/j.coisb.2020.04.001</pub-id><pub-id pub-id-type="pmcid">PMC7469942</pub-id><pub-id pub-id-type="pmid">32905524</pub-id></element-citation></ref><ref id="R6"><label>[6]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koo</surname><given-names>Peter K</given-names></name><name><surname>Eddy</surname><given-names>SeanR</given-names></name></person-group><article-title>Representation learning of genomic sequence motifs with convolutional neural networks</article-title><source>PLOS Computational Biology</source><year>2019</year><month>12</month><volume>15</volume><issue>12</issue><fpage>1</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007560</pub-id><pub-id pub-id-type="pmcid">PMC6941814</pub-id><pub-id pub-id-type="pmid">31856220</pub-id></element-citation></ref><ref id="R7"><label>[7]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>Karen</given-names></name><name><surname>Vedaldi</surname><given-names>Andrea</given-names></name><name><surname>Zisserman</surname><given-names>Andrew</given-names></name></person-group><article-title>Deep inside convolutional networks: Visualising image classification models and saliency maps</article-title><year>2013</year></element-citation></ref><ref id="R8"><label>[8]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Majdandzic</surname><given-names>Antonio</given-names></name><name><surname>Rajesh</surname><given-names>Chandana</given-names></name><name><surname>Koo</surname><given-names>Peter K</given-names></name></person-group><article-title>Correcting gradient-based interpretations of deep neural networks for genomics</article-title><source>Genome Biology</source><year>2023</year><month>May</month><volume>24</volume><issue>1</issue><pub-id pub-id-type="doi">10.1186/s13059-023-02956-3</pub-id><pub-id pub-id-type="pmcid">PMC10169356</pub-id><pub-id pub-id-type="pmid">37161475</pub-id></element-citation></ref><ref id="R9"><label>[9]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shrikumar</surname><given-names>Avanti</given-names></name><name><surname>Greenside</surname><given-names>Peyton</given-names></name><name><surname>Kundaje</surname><given-names>Anshul</given-names></name></person-group><article-title>Learning important features through propagating activation differences</article-title><year>2017</year></element-citation></ref><ref id="R10"><label>[10]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shrikumar</surname><given-names>Avanti</given-names></name><name><surname>Tian</surname><given-names>Katherine</given-names></name><name><surname>Avsec</surname><given-names>Žiga</given-names></name><name><surname>Shcherbina</surname><given-names>Anna</given-names></name><name><surname>Banerjee</surname><given-names>Abhimanyu</given-names></name><name><surname>Sharmin</surname><given-names>Mahfuza</given-names></name><name><surname>Nair</surname><given-names>Surag</given-names></name><name><surname>Kundaje</surname><given-names>Anshul</given-names></name></person-group><article-title>Technical note on transcription factor motif discovery from importance scores (tf-modisco) version 0.5.6.5</article-title><year>2018</year></element-citation></ref><ref id="R11"><label>[11]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avsec</surname><given-names>Žiga</given-names></name><name><surname>Agarwal</surname><given-names>Vikram</given-names></name><name><surname>Visentin</surname><given-names>Daniel</given-names></name><name><surname>Ledsam</surname><given-names>Joseph R</given-names></name><name><surname>Grabska-Barwinska</surname><given-names>Agnieszka</given-names></name><name><surname>Taylor</surname><given-names>KyleR</given-names></name><name><surname>Assael</surname><given-names>Yannis</given-names></name><name><surname>Jumper</surname><given-names>John</given-names></name><name><surname>Kohli</surname><given-names>Pushmeet</given-names></name><name><surname>Kelley</surname><given-names>DavidR</given-names></name></person-group><article-title>Effective gene expression prediction from sequence by integrating long-range interactions</article-title><source>Nature Methods</source><year>2021</year><month>October</month><volume>18</volume><issue>10</issue><fpage>1196</fpage><lpage>1203</lpage><pub-id pub-id-type="doi">10.1038/s41592-021-01252-x</pub-id><pub-id pub-id-type="pmcid">PMC8490152</pub-id><pub-id pub-id-type="pmid">34608324</pub-id></element-citation></ref><ref id="R12"><label>[12]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lal</surname><given-names>Avantika</given-names></name><name><surname>Karollus</surname><given-names>Alexander</given-names></name><name><surname>Gunsalus</surname><given-names>Laura</given-names></name><name><surname>Garfield</surname><given-names>David</given-names></name><name><surname>Nair</surname><given-names>Surag</given-names></name><name><surname>Tseng</surname><given-names>Alex M</given-names></name><name><surname>Gordon</surname><given-names>MGrace</given-names></name><name><surname>Collier</surname><given-names>JennaL</given-names></name><name><surname>Diamant</surname><given-names>Nathaniel</given-names></name><name><surname>Biancalani</surname><given-names>Tommaso</given-names></name><name><surname>Bravo</surname><given-names>HectorCorrada</given-names></name><etal/></person-group><article-title>Decoding sequence determinants of gene expression in diverse cellular and disease states</article-title><source>bioRxiv</source><year>2024</year></element-citation></ref><ref id="R13"><label>[13]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Linder</surname><given-names>Johannes</given-names></name><name><surname>Srivastava</surname><given-names>Divyanshi</given-names></name><name><surname>Yuan</surname><given-names>Han</given-names></name><name><surname>Agarwal</surname><given-names>Vikram</given-names></name><name><surname>Kelley</surname><given-names>David R</given-names></name></person-group><article-title>Predicting rna-seq coverage from dna sequence as a unifying model of gene regulation</article-title><source>Nature Genetics</source><year>2025</year><month>January</month><volume>57</volume><issue>4</issue><fpage>949</fpage><lpage>961</lpage><pub-id pub-id-type="doi">10.1038/s41588-024-02053-6</pub-id><pub-id pub-id-type="pmcid">PMC11985352</pub-id><pub-id pub-id-type="pmid">39779956</pub-id></element-citation></ref><ref id="R14"><label>[14]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bhattacharya</surname><given-names>Nick</given-names></name><name><surname>Thomas</surname><given-names>Neil</given-names></name><name><surname>Rao</surname><given-names>Roshan</given-names></name><name><surname>Daupras</surname><given-names>Justas</given-names></name><name><surname>Koo</surname><given-names>Peter K</given-names></name><name><surname>Baker</surname><given-names>David</given-names></name><name><surname>Song</surname><given-names>YunS</given-names></name><name><surname>Ovchinnikov</surname><given-names>Sergey</given-names></name></person-group><article-title>Single layers of attention suffice to predict protein contacts</article-title><year>2021</year></element-citation></ref><ref id="R15"><label>[15]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ullah</surname><given-names>Fahad</given-names></name><name><surname>Ben-Hur</surname><given-names>Asa</given-names></name></person-group><article-title>A self-attention model for inferring cooperativity between regulatory features</article-title><source>Nucleic Acids Research</source><year>2021</year><day>05</day><volume>49</volume><issue>13</issue><fpage>e77</fpage><pub-id pub-id-type="doi">10.1093/nar/gkab349</pub-id><pub-id pub-id-type="pmcid">PMC8287919</pub-id><pub-id pub-id-type="pmid">33950192</pub-id></element-citation></ref><ref id="R16"><label>[16]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jabeen</surname><given-names>Saira</given-names></name><name><surname>Ben-Hur</surname><given-names>Asa</given-names></name></person-group><article-title>A comprehensive evaluation of self attention for detecting feature interactions</article-title><source>bioRxiv</source><year>2024</year></element-citation></ref><ref id="R17"><label>[17]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ghotra</surname><given-names>RohanSingh</given-names></name><name><surname>Lee</surname><given-names>Nicholas Keone</given-names></name><name><surname>Koo</surname><given-names>Peter K</given-names></name></person-group><source>Uncovering motif interactions from convolutional-attention networks for genomics</source><conf-name>NeurIPS 2021 AI for Science Workshop</conf-name><year>2021</year></element-citation></ref><ref id="R18"><label>[18]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tseng</surname><given-names>AlexM</given-names></name><name><surname>Eraslan</surname><given-names>Gokcen</given-names></name><name><surname>Biancalani</surname><given-names>Tommaso</given-names></name><name><surname>Scalia</surname><given-names>Gabriele</given-names></name></person-group><article-title>A mechanistically interpretable neural network for regulatory genomics</article-title><year>2024</year></element-citation></ref><ref id="R19"><label>[19]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaganathan</surname><given-names>Kishore</given-names></name><name><surname>Panagiotopoulou</surname><given-names>Sofia Kyriazopoulou</given-names></name><name><surname>McRae</surname><given-names>Jeremy F</given-names></name><name><surname>Darbandi</surname><given-names>Siavash Fazel</given-names></name><name><surname>Knowles</surname><given-names>David</given-names></name><name><surname>Li</surname><given-names>YangI</given-names></name><name><surname>Kosmicki</surname><given-names>JackA</given-names></name><name><surname>Arbelaez</surname><given-names>Juan</given-names></name><name><surname>Cui</surname><given-names>Wenwu</given-names></name><name><surname>Schwartz</surname><given-names>Grace B</given-names></name><name><surname>Chow</surname><given-names>Eric D</given-names></name><etal/></person-group><article-title>Predicting splicing from primary sequence with deep learning</article-title><source>Cell</source><year>2019</year><month>January</month><volume>176</volume><issue>3</issue><fpage>535</fpage><lpage>548</lpage><elocation-id>e24</elocation-id><pub-id pub-id-type="pmid">30661751</pub-id></element-citation></ref><ref id="R20"><label>[20]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Prakash</surname><given-names>Eva I</given-names></name><name><surname>Shrikumar</surname><given-names>Avanti</given-names></name><name><surname>Kundaje</surname><given-names>Anshul</given-names></name></person-group><source>Towards more realistic simulated datasets for benchmarking deep learning models in regulatory genomics</source><person-group person-group-type="editor"><name><surname>Knowles</surname><given-names>David A</given-names></name><name><surname>Mostafavi</surname><given-names>Sara</given-names></name><name><surname>Lee</surname><given-names>Su-In</given-names></name></person-group><conf-name>Proceedings of the 16th Machine Learning in Computational Biology meeting, volume 165 of Proceedings of Machine Learning Research</conf-name><conf-sponsor>PMLR</conf-sponsor><conf-date>22–23 Nov 2022</conf-date><fpage>58</fpage><lpage>77</lpage></element-citation></ref><ref id="R21"><label>[21]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koo</surname><given-names>Peter K</given-names></name><name><surname>Ploenzke</surname><given-names>Matt</given-names></name></person-group><article-title>Improving representations of genomic sequence motifs in convolutional networks with exponential activations</article-title><source>Nature Machine Intelligence</source><year>2021</year><month>February</month><volume>3</volume><issue>3</issue><fpage>258</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.1038/s42256-020-00291-x</pub-id><pub-id pub-id-type="pmcid">PMC8315445</pub-id><pub-id pub-id-type="pmid">34322657</pub-id></element-citation></ref><ref id="R22"><label>[22]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lemanczyk</surname><given-names>MartaS</given-names></name><name><surname>Bartoszewicz</surname><given-names>Jakub M</given-names></name><name><surname>Renard</surname><given-names>Bernhard Y</given-names></name></person-group><article-title>Motif interactions affect post-hoc interpretability of genomic convolutional neural networks</article-title><source>bioRxiv</source><year>2024</year></element-citation></ref><ref id="R23"><label>[23]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Boer</surname><given-names>Carl G</given-names></name><name><surname>Taipale</surname><given-names>Jussi</given-names></name></person-group><article-title>Hold out the genome: a roadmap to solving the cis-regulatory code</article-title><source>Nature</source><year>2024</year><month>January</month><volume>625</volume><issue>7993</issue><fpage>41</fpage><lpage>50</lpage><pub-id pub-id-type="pmid">38093018</pub-id></element-citation></ref><ref id="R24"><label>[24]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liao</surname><given-names>Susan E</given-names></name><name><surname>Sudarshan</surname><given-names>Mukund</given-names></name><name><surname>Regev</surname><given-names>Oded</given-names></name></person-group><article-title>Deciphering rna splicing logic with interpretable machine learning</article-title><source>Proceedings of the National Academy of Sciences</source><year>2023</year><volume>120</volume><issue>41</issue><elocation-id>e2221165120</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2221165120</pub-id><pub-id pub-id-type="pmcid">PMC10576025</pub-id><pub-id pub-id-type="pmid">37796983</pub-id></element-citation></ref><ref id="R25"><label>[25]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Frömel</surname><given-names>Robert</given-names></name><name><surname>Rühle</surname><given-names>Julia</given-names></name><name><surname>Martinez</surname><given-names>Aina Bernal</given-names></name><name><surname>Szu-Tu</surname><given-names>Chelsea</given-names></name><name><surname>Pastor</surname><given-names>Felix Pacheco</given-names></name><name><surname>Martinez-Corral</surname><given-names>Rosa</given-names></name><name><surname>Velten</surname><given-names>Lars</given-names></name></person-group><article-title>Design principles of cell-state-specific enhancers in hematopoiesis</article-title><source>Cell</source><publisher-name>Elsevier</publisher-name><year>2025</year><month>June</month><volume>188</volume><comment>12</comment><fpage>3202</fpage><lpage>3218</lpage><elocation-id>e21</elocation-id><pub-id pub-id-type="doi">10.1016/j.cell.2025.04.017</pub-id><pub-id pub-id-type="pmcid">PMC12173716</pub-id><pub-id pub-id-type="pmid">40345201</pub-id></element-citation></ref><ref id="R26"><label>[26]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castillo-Hair</surname><given-names>Sebastian M</given-names></name><name><surname>Seelig</surname><given-names>Georg</given-names></name></person-group><article-title>Machine learning for designing next-generation mrna therapeutics</article-title><source>Accounts of Chemical Research</source><year>2022</year><volume>55</volume><issue>1</issue><fpage>24</fpage><lpage>34</lpage><pub-id pub-id-type="pmid">34905691</pub-id></element-citation></ref><ref id="R27"><label>[27]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenberg</surname><given-names>Alexander B</given-names></name><name><surname>Patwardhan</surname><given-names>Rupali P</given-names></name><name><surname>Shendure</surname><given-names>Jay</given-names></name><name><surname>Seelig</surname><given-names>Georg</given-names></name></person-group><article-title>Learning the sequence determinants of alternative splicing from millions of random sequences</article-title><source>Cell</source><year>2015</year><month>October</month><volume>163</volume><issue>3</issue><fpage>698</fpage><lpage>711</lpage><pub-id pub-id-type="pmid">26496609</pub-id></element-citation></ref><ref id="R28"><label>[28]</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Rafi</surname><given-names>Abdul Muntakim</given-names></name><name><surname>Nogina</surname><given-names>Daria</given-names></name><name><surname>Penzar</surname><given-names>Dmitry</given-names></name><name><surname>Lee</surname><given-names>Dohoon</given-names></name><name><surname>Lee</surname><given-names>Danyeong</given-names></name><name><surname>Kim</surname><given-names>Nayeon</given-names></name><name><surname>Kim</surname><given-names>Sangyeup</given-names></name><name><surname>Kim</surname><given-names>Dohyeon</given-names></name><name><surname>Shin</surname><given-names>Yeojin</given-names></name><name><surname>Kwak</surname><given-names>Il-Youp</given-names></name><name><surname>Meshcheryakov</surname><given-names>Georgy</given-names></name><etal/></person-group><article-title>Evaluation and optimization of sequence-based gene regulatory deep learning models</article-title><year>2023</year><month>April</month></element-citation></ref><ref id="R29"><label>[29]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thompson</surname><given-names>Mike</given-names></name><name><surname>Martín</surname><given-names>Mariano</given-names></name><name><surname>Olmo</surname><given-names>Trinidad Sanmartín</given-names></name><name><surname>Rajesh</surname><given-names>Chandana</given-names></name><name><surname>Koo</surname><given-names>Peter K</given-names></name><name><surname>Bolognesi</surname><given-names>Benedetta</given-names></name><name><surname>Lehner</surname><given-names>Ben</given-names></name></person-group><article-title>Massive experimental quantification allows interpretable deep learning of protein aggregation</article-title><source>Science Advances</source><year>2025</year><volume>11</volume><issue>18</issue><elocation-id>eadt5111</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.adt5111</pub-id><pub-id pub-id-type="pmcid">PMC12042874</pub-id><pub-id pub-id-type="pmid">40305601</pub-id></element-citation></ref><ref id="R30"><label>[30]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fornes</surname><given-names>Oriol</given-names></name><name><surname>Castro-Mondragon</surname><given-names>Jaime A</given-names></name><name><surname>Khan</surname><given-names>Aziz</given-names></name><name><surname>van der Lee</surname><given-names>Robin</given-names></name><name><surname>Zhang</surname><given-names>Xi</given-names></name><name><surname>Richmond</surname><given-names>Phillip A</given-names></name><name><surname>Modi</surname><given-names>Bhavi P</given-names></name><name><surname>Correard</surname><given-names>Solenne</given-names></name><name><surname>Gheorghe</surname><given-names>Marius</given-names></name><name><surname>Baranašić</surname><given-names>Damir</given-names></name><name><surname>Santana-Garcia</surname><given-names>Walter</given-names></name><etal/></person-group><article-title>Jaspar 2020: update of the open-access database of transcription factor binding profiles</article-title><source>Nucleic Acids Research</source><year>2019</year><day>11</day><volume>48</volume><issue>D1</issue><fpage>D87</fpage><lpage>D92</lpage><pub-id pub-id-type="doi">10.1093/nar/gkz1001</pub-id><pub-id pub-id-type="pmcid">PMC7145627</pub-id><pub-id pub-id-type="pmid">31701148</pub-id></element-citation></ref><ref id="R31"><label>[31]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Visscher</surname><given-names>Peter M</given-names></name><name><surname>Hill</surname><given-names>William G</given-names></name><name><surname>Wray</surname><given-names>Naomi R</given-names></name></person-group><article-title>Heritability in the genomics era — concepts and misconceptions</article-title><source>Nature Reviews Genetics</source><year>2008</year><month>March</month><volume>9</volume><issue>4</issue><fpage>255</fpage><lpage>266</lpage><pub-id pub-id-type="pmid">18319743</pub-id></element-citation></ref><ref id="R32"><label>[32]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartlett</surname><given-names>Peter L</given-names></name><name><surname>Long</surname><given-names>Philip M</given-names></name><name><surname>Lugosi</surname><given-names>Gábor</given-names></name><name><surname>Tsigler</surname><given-names>Alexander</given-names></name></person-group><article-title>Benign overfitting in linear regression</article-title><source>Proceedings of the National Academy of Sciences</source><year>2020</year><volume>117</volume><issue>48</issue><fpage>30063</fpage><lpage>30070</lpage><pub-id pub-id-type="doi">10.1073/pnas.1907378117</pub-id><pub-id pub-id-type="pmcid">PMC7720150</pub-id><pub-id pub-id-type="pmid">32332161</pub-id></element-citation></ref><ref id="R33"><label>[33]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seitz</surname><given-names>Evan E</given-names></name><name><surname>McCandlish</surname><given-names>David M</given-names></name><name><surname>Kinney</surname><given-names>Justin B</given-names></name><name><surname>Koo</surname><given-names>Peter K</given-names></name></person-group><article-title>Interpreting cis-regulatory mechanisms from genomic deep neural networks using surrogate models</article-title><source>bioRxiv</source><year>2024</year><pub-id pub-id-type="doi">10.1038/s42256-024-00851-5</pub-id><pub-id pub-id-type="pmcid">PMC11823438</pub-id><pub-id pub-id-type="pmid">39950082</pub-id></element-citation></ref><ref id="R34"><label>[34]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zou</surname><given-names>Hui</given-names></name><name><surname>Hastie</surname><given-names>Trevor</given-names></name></person-group><article-title>Regularization and variable selection via the elastic net</article-title><source>Journal of the Royal Statistical Society Series B: Statistical Methodology</source><year>2005</year><day>03</day><volume>67</volume><issue>2</issue><fpage>301</fpage><lpage>320</lpage></element-citation></ref><ref id="R35"><label>[35]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ogutu</surname><given-names>Joseph O</given-names></name><name><surname>Schulz-Streeck</surname><given-names>Torben</given-names></name><name><surname>Piepho</surname><given-names>Hans-Peter</given-names></name></person-group><article-title>Genomic selection using regularized linear regression models: ridge regression, lasso, elastic net and their extensions</article-title><source>BMC Proceedings</source><year>2012</year><month>May</month><volume>6</volume><issue>S2</issue><pub-id pub-id-type="doi">10.1186/1753-6561-6-S2-S10</pub-id><pub-id pub-id-type="pmcid">PMC3363152</pub-id><pub-id pub-id-type="pmid">22640436</pub-id></element-citation></ref><ref id="R36"><label>[36]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeng</surname><given-names>Tony</given-names></name><name><surname>Li</surname><given-names>Yang I</given-names></name></person-group><article-title>Predicting rna splicing from dna sequence using pangolin</article-title><source>Genome Biology</source><year>2022</year><month>April</month><volume>23</volume><issue>1</issue><pub-id pub-id-type="doi">10.1186/s13059-022-02664-4</pub-id><pub-id pub-id-type="pmcid">PMC9022248</pub-id><pub-id pub-id-type="pmid">35449021</pub-id></element-citation></ref><ref id="R37"><label>[37]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Jiawei</given-names></name><name><surname>Pu</surname><given-names>Yuqian</given-names></name><name><surname>Tang</surname><given-names>Jijun</given-names></name><name><surname>Zou</surname><given-names>Quan</given-names></name><name><surname>Guo</surname><given-names>Fei</given-names></name></person-group><article-title>Deepatt: a hybrid category attention neural network for identifying functional effects of dna sequences</article-title><source>Briefings in Bioinformatics</source><year>2020</year><month>August</month><volume>22</volume><issue>3</issue><pub-id pub-id-type="pmid">32778871</pub-id></element-citation></ref><ref id="R38"><label>[38]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>Xin</given-names></name><name><surname>Wang</surname><given-names>Jun</given-names></name><name><surname>Li</surname><given-names>Qianyue</given-names></name><name><surname>Liu</surname><given-names>Taigang</given-names></name></person-group><article-title>Bilstm-5mc: A bidirectional long short-term memory-based approach for predicting 5-methylcytosine sites in genome-wide dna promoters</article-title><source>Molecules</source><year>2021</year><month>December</month><volume>26</volume><issue>24</issue><elocation-id>7414</elocation-id><pub-id pub-id-type="doi">10.3390/molecules26247414</pub-id><pub-id pub-id-type="pmcid">PMC8704614</pub-id><pub-id pub-id-type="pmid">34946497</pub-id></element-citation></ref><ref id="R39"><label>[39]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dsouza</surname><given-names>Kevin B</given-names></name><name><surname>Maslova</surname><given-names>Alexandra</given-names></name><name><surname>Al-Jibury</surname><given-names>Ediem</given-names></name><name><surname>Merkenschlager</surname><given-names>Matthias</given-names></name><name><surname>Bhargava</surname><given-names>VijayK</given-names></name><name><surname>Libbrecht</surname><given-names>MaxwellW</given-names></name></person-group><article-title>Learning representations of chromatin contacts using a recurrent neural network identifies genomic drivers of conformation</article-title><source>Nature Communications</source><year>2022</year><month>June</month><volume>13</volume><issue>1</issue><pub-id pub-id-type="doi">10.1038/s41467-022-31337-w</pub-id><pub-id pub-id-type="pmcid">PMC9240038</pub-id><pub-id pub-id-type="pmid">35764630</pub-id></element-citation></ref><ref id="R40"><label>[40]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>Kaiming</given-names></name><name><surname>Zhang</surname><given-names>Xiangyu</given-names></name><name><surname>Ren</surname><given-names>Shaoqing</given-names></name><name><surname>Sun</surname><given-names>Jian</given-names></name></person-group><source>Deep residual learning for image recognition</source><conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><year>2016</year><fpage>770</fpage><lpage>778</lpage></element-citation></ref><ref id="R41"><label>[41]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Majdandzic</surname><given-names>Antonio</given-names></name><name><surname>Rajesh</surname><given-names>Chandana</given-names></name><name><surname>Tang</surname><given-names>Ziqi</given-names></name><name><surname>Toneyan</surname><given-names>Shushan</given-names></name><name><surname>Labelson</surname><given-names>Ethan L</given-names></name><name><surname>Tripathy</surname><given-names>RohitK</given-names></name><name><surname>Koo</surname><given-names>Peter K</given-names></name></person-group><source>Selecting deep neural networks that yield consistent attribution-based interpretations for genomics</source><person-group person-group-type="editor"><name><surname>Knowles</surname><given-names>DavidA</given-names></name><name><surname>Mostafavi</surname><given-names>Sara</given-names></name><name><surname>Lee</surname><given-names>Su-In</given-names></name></person-group><conf-name>Proceedings of the 17th Machine Learning in Computational Biology meeting, volume 200 of Proceedings of Machine Learning Research</conf-name><conf-sponsor>PMLR</conf-sponsor><conf-date>21–22 Nov 2022</conf-date><fpage>131</fpage><lpage>149</lpage></element-citation></ref><ref id="R42"><label>[42]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Penzar</surname><given-names>Dmitry</given-names></name><name><surname>Nogina</surname><given-names>Daria</given-names></name><name><surname>Noskova</surname><given-names>Elizaveta</given-names></name><name><surname>Zinkevich</surname><given-names>Arsenii</given-names></name><name><surname>Meshcheryakov</surname><given-names>Georgy</given-names></name><name><surname>Lando</surname><given-names>Andrey</given-names></name><name><surname>Rafi</surname><given-names>Abdul Muntakim</given-names></name><name><surname>de Boer</surname><given-names>Carl</given-names></name><name><surname>Kulakovskiy</surname><given-names>IvanV</given-names></name></person-group><article-title>Legnet: a best-in-class deep learning model for short dna regulatory regions</article-title><source>Bioinformatics</source><year>2023</year><month>07</month><volume>39</volume><issue>8</issue><elocation-id>btad457</elocation-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btad457</pub-id><pub-id pub-id-type="pmcid">PMC10400376</pub-id><pub-id pub-id-type="pmid">37490428</pub-id></element-citation></ref><ref id="R43"><label>[43]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Xiao</given-names></name><name><surname>Jiang</surname><given-names>Ruoxi</given-names></name><name><surname>Gao</surname><given-names>William</given-names></name><name><surname>Willett</surname><given-names>Rebecca</given-names></name><name><surname>Maire</surname><given-names>Michael</given-names></name></person-group><article-title>Residual connections harm generative representation learning</article-title><year>2024</year></element-citation></ref><ref id="R44"><label>[44]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Xiao</given-names></name><name><surname>Jiang</surname><given-names>Ruoxi</given-names></name><name><surname>Gao</surname><given-names>William</given-names></name><name><surname>Willett</surname><given-names>Rebecca</given-names></name><name><surname>Maire</surname><given-names>Michael</given-names></name></person-group><article-title>Residual connections harm generative representation learning</article-title><year>2024</year></element-citation></ref><ref id="R45"><label>[45]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gupta</surname><given-names>Shobhit</given-names></name><name><surname>Stamatoyannopoulos</surname><given-names>John A</given-names></name><name><surname>Bailey</surname><given-names>Timothy L</given-names></name><name><surname>Noble</surname><given-names>William Stafford</given-names></name></person-group><article-title>Quantifying similarity between motifs</article-title><source>Genome Biology</source><year>2007</year><month>February</month><volume>8</volume><issue>2</issue><pub-id pub-id-type="doi">10.1186/gb-2007-8-2-r24</pub-id><pub-id pub-id-type="pmcid">PMC1852410</pub-id><pub-id pub-id-type="pmid">17324271</pub-id></element-citation></ref><ref id="R46"><label>[46]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lundberg</surname><given-names>Scott</given-names></name><name><surname>Lee</surname><given-names>Su-In</given-names></name></person-group><article-title>A unified approach to interpreting model predictions</article-title><year>2017</year></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Convolutions regularize the gradient and enable motif discovery.</title><p>(A) An example of the gradient calculated over the multi-layer perceptron (MLP) or the network with a convolutional layer with respect to a sequence with both injected motifs. The gradient at the injected motif is closer examined adjacent to the fully saliency maps, and the result of running TF-MoDISco over the gradient maps of all sequences is shown to the right. (B) The predictive performance of each model on 10,000 heldout sequences. (C) Kruskal-Wallace test statistics of the gradient at the injected motif positions calculated for all sequences with the injected motif. (D) Kurtosis test statistics of the gradient at the injected motif positions calculated for all sequences with the injected motif. (E) We calculated a t-statistic over the model differences in sequences’ Kruskall-Wallace (or Kurtosis) test statistics (e.g. the tan distributions in (C) or (D)) for ten replicates of the simulation. The dashed line indicates a t-statistic that would yield significance at p= 0.001.</p></caption><graphic xlink:href="EMS207596-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Genetic architectures and data availability modulate network performance.</title><p>(A) Pearson correlation between the predicted and true phenotype across the 10% heldout data. Boxplots represent a distribution of 50 results from different realized simulations under one specific combination of genetic architecture configurations. (B) The performance of models in the presence of sequence-based, non-motif effects on the phenotypic variability.</p></caption><graphic xlink:href="EMS207596-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Interpretability methods capture a variety of motifs and motif interactions.</title><p>(A) The power (ability to detect a motif given it has a non-zero effect) across all 1500 simulations, using a combination of either the trained model and the shallow or deep interpretation strategies. Motifs are represented on the same scale by their information content. (B) The power (same as (A)) but capturing all motifs involved in an interaction. (C) The correlation of model prediction accuracy and motif discovery power. (D) Model-interpretability strategy false positive rates (reporting a significant motif discovery for motifs that do not have effects, weighted by those that do). (E) The redundancy of true positive motifs, calculated as the ratio of the mean number of times a single motif was reported by one model-interpretability strategy, in one simulation. Error bars represent 95% confidence intervals obtained via non-parametric bootstrapping.</p></caption><graphic xlink:href="EMS207596-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Correlation of test performance and TF-MoDISco motif discovery power in the absence of non-motif, sequence-based effects.</title><p>Despite all models sharing the first-layer architecture, their correlation of predictive performance and motif discovery power changes across sample sizes and genomic grammars. Many models have worse correlations when provided large sample sizes, or under simpler genetic contexts, which may indicate benign overfitting. Correlations are consistent when using the shallow motif discovery approach, but worsen in the presence of sequence-based effects (<xref ref-type="supplementary-material" rid="SD1">Appendix</xref>).</p></caption><graphic xlink:href="EMS207596-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Model performance on experimental data as a function of filter width</title><p>(A) Pearson correlation of the true and predicted test set transcriptional activity. (B) Number of motif discoveries made by each model using TF-MoDISco.</p></caption><graphic xlink:href="EMS207596-f005"/></fig><table-wrap id="T1" position="float" orientation="portrait"><label>Table 1</label><caption><title>Architectures evaluated in the simulations. Additional details are in the <xref ref-type="supplementary-material" rid="SD1">Appendix</xref>.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle">Name</th><th align="left" valign="middle">Components</th><th align="left" valign="middle">Parameters</th></tr></thead><tbody><tr><td align="left" valign="middle">Baseline</td><td align="left" valign="middle">Convolution, dense, flatten, dense, out</td><td align="left" valign="middle">238,385</td></tr><tr><td align="left" valign="middle">Attention[<xref ref-type="bibr" rid="R15">15</xref>, <xref ref-type="bibr" rid="R37">37</xref>]</td><td align="left" valign="middle">Convolution, pooling, attention, flatten, dense, out</td><td align="left" valign="middle">225,089</td></tr><tr><td align="left" valign="middle">Attention no pool</td><td align="left" valign="middle">Convolution, attention, flatten, dense, out</td><td align="left" valign="middle">241,913</td></tr><tr><td align="left" valign="middle">Stacked convolutions [<xref ref-type="bibr" rid="R3">3</xref>]</td><td align="left" valign="middle">(Convolution, pooling)x4, flatten, dense, dense, out</td><td align="left" valign="middle">222,749</td></tr><tr><td align="left" valign="middle">Dilated convolutions</td><td align="left" valign="middle">Convolution, (Dilated Conv.)x5, flatten, dense, out</td><td align="left" valign="middle">212,801</td></tr><tr><td align="left" valign="middle">Dilated convolutions w/ residual [<xref ref-type="bibr" rid="R19">19</xref>, <xref ref-type="bibr" rid="R36">36</xref>]</td><td align="left" valign="middle">Convolution, (Dilated Conv. + residual)x5, flatten, dense, out</td><td align="left" valign="middle">225,667</td></tr><tr><td align="left" valign="middle">LSTM final cell state[<xref ref-type="bibr" rid="R38">38</xref>]</td><td align="left" valign="middle">Convolution, (BiLSTM)x2, dense, out</td><td align="left" valign="middle">203,249</td></tr><tr><td align="left" valign="middle">LSTM all hidden states</td><td align="left" valign="middle">Convolution, (BiLSTM)x2, dense, out</td><td align="left" valign="middle">271,361</td></tr></tbody></table></table-wrap><table-wrap id="T2" position="float" orientation="portrait"><label>Table 2</label><caption><p>Regression association statistics (and p-value) for motif discovery across experimental and simulated data. IC stands for information content, effect sizes in the experimental setting were proxied by their χ<sup>2</sup> association test statistic to the reporter activity (<xref ref-type="supplementary-material" rid="SD1">Appendix</xref>).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom" rowspan="2">Motif feature</th><th align="center" valign="middle" colspan="2">Experimental Data</th><th align="center" valign="middle" colspan="2">Simulations</th></tr><tr><th align="center" valign="middle" style="border-top: solid thin">Shallow</th><th align="center" valign="middle" style="border-top: solid thin">MoDISco</th><th align="center" valign="middle" style="border-top: solid thin">MoDISco</th><th align="center" valign="middle" style="border-top: solid thin">Shallow</th></tr></thead><tbody><tr><td align="left" valign="middle">Length</td><td align="left" valign="middle">-24.70 (&lt;2e-16)</td><td align="left" valign="middle">-27.16 (&lt;2e-16)</td><td align="left" valign="middle">-38.56 (&lt;2e-16)</td><td align="left" valign="middle">-20.35 (&lt;2e-16)</td></tr><tr><td align="left" valign="middle">IC</td><td align="left" valign="middle">24.70 (&lt;2e-16)</td><td align="left" valign="middle">13.15 (&lt;2e-16)</td><td align="left" valign="middle">70.42 (&lt;2e-16)</td><td align="left" valign="middle">58.82 (&lt;2e-16)</td></tr><tr><td align="left" valign="middle">Filter width</td><td align="left" valign="middle">22.84 (&lt;2e-16)</td><td align="left" valign="middle">15.504 (&lt;2e-16)</td><td align="center" valign="middle">–</td><td align="center" valign="middle">–</td></tr><tr><td align="left" valign="middle">Frequency</td><td align="center" valign="middle">–</td><td align="center" valign="middle">–</td><td align="left" valign="middle">5.11 (3.2e-7)</td><td align="left" valign="middle">4.56 (5.2e-6)</td></tr><tr><td align="left" valign="middle">(Effect size)<sup>2</sup></td><td align="left" valign="middle">76.24 (&lt;2e-16)</td><td align="left" valign="middle">110.58 (&lt;2e-16)</td><td align="left" valign="middle">50.13 (&lt;2e-16)</td><td align="left" valign="middle">67.12 (&lt;2e-16)</td></tr></tbody></table></table-wrap></floats-group></article>