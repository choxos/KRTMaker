<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS206054</article-id><article-id pub-id-type="doi">10.1101/2025.05.28.656283</article-id><article-id pub-id-type="archive">PPR1027968</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Determinants of visual ambiguity resolution</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Linde-Domingo</surname><given-names>Juan</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="corresp" rid="CR1">✉</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Ortiz-Tudela</surname><given-names>Javier</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Völler</surname><given-names>Johannah</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Hebart</surname><given-names>Martin N.</given-names></name><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>González-García</surname><given-names>Carlos</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="corresp" rid="CR1">✉</xref></contrib></contrib-group><aff id="A1"><label>1</label>Mind, Brain and Behavior Research Center, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/04njjy449</institution-id><institution>University of Granada</institution></institution-wrap>, <city>Granada</city>, <country country="ES">Spain</country></aff><aff id="A2"><label>2</label>Department of Experimental Psychology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/04njjy449</institution-id><institution>University of Granada</institution></institution-wrap>, <city>Granada</city>, <country country="ES">Spain</country></aff><aff id="A3"><label>3</label>Department of Medicine, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/033eqas34</institution-id><institution>Justus Liebig University Giessen</institution></institution-wrap>, <city>Giessen</city>, <country country="DE">Germany</country></aff><aff id="A4"><label>4</label>Vision and Computational Cognition Group, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/0387jng26</institution-id><institution>Max Planck Institute for Human Cognitive and Brain Sciences</institution></institution-wrap>, <city>Leipzig</city>, <country country="DE">Germany</country></aff><aff id="A5"><label>5</label>Center for Mind, Brain, and Behavior, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01rdrb571</institution-id><institution>Universities of Marburg</institution></institution-wrap>, Giessen, and Darmstadt, <city>Marburg</city>, <country country="DE">Germany</country></aff><author-notes><corresp id="CR1">
<label>✉</label>Correspondence to Juan Linde-Domingo (<email>lindedomingo@ugr.es</email>) or Carlos González-García (<email>cgonzalez@ugr.es</email>)</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>31</day><month>05</month><year>2025</year></pub-date><pub-date pub-type="preprint"><day>29</day><month>05</month><year>2025</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Visual inputs during natural perception are highly ambiguous: objects are frequently occluded, lighting conditions vary, and object identification depends significantly on prior experiences. However, why do certain images remain unidentifiable while others can be recognized immediately, and what visual features drive subjective clarification? To address these critical questions, we developed a unique dataset of 1,854 ambiguous images and collected more than 100,000 participant ratings evaluating their identifiability before and after seeing undistorted versions of the images. Relating the representations of a brain-inspired neural network model in response to our images with human ratings, we show that subjective identification depends largely on the extent to which higher-level visual features from the original images are preserved in their ambiguous counterparts. Notably, the predominance of higher-level features over lower-level ones softens after participants disambiguate the images. In line with these results, an image-level regression analysis showed that the subjective identification of ambiguous images was best explained by high-level visual dimensions. Moreover, we found that the process of ambiguity resolution was accompanied by a notable decrease in semantic distance and a greater consistency in object naming among participants. However, the relationship between information gained after disambiguation and subjective identification was non-linear, indicating that acquiring more information does not necessarily enhance subjective clarity. Instead, we observed a U-shaped relationship, suggesting that subjective identification improves when the acquired information either strongly matches or mismatches prior predictions. Collectively, these findings highlight fundamental principles underlying the mapping between human visual perception and memory, advancing our understanding on how we resolve ambiguity and extract meaning from incomplete visual information.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">The natural world is inherently ambiguous, yet humans can in most cases make sense of what they see. This remarkable feature of human perception is a core principle of the predictive processing framework, which casts perception as a purely inferential process of hypothesis testing (<xref ref-type="bibr" rid="R7">Clark, 2013</xref>; <xref ref-type="bibr" rid="R10">Friston, 2005</xref>; <xref ref-type="bibr" rid="R21">Knill &amp; Pouget, 2004</xref>; <xref ref-type="bibr" rid="R30">Rao &amp; Ballard, 1999</xref>). Specifically, to resolve ambiguity, this framework proposes that the brain generates predictions about the most probable causes underlying the sensory input from real-world experience. These initial predictions are then compared to feedforward sensory input, and any mismatch – or prediction error – drives an iterative refinement process. Through repeated prediction error minimization, perception stabilizes, and ambiguity is resolved (<xref ref-type="bibr" rid="R10">Friston, 2005</xref>; <xref ref-type="bibr" rid="R35">Walsh et al., 2020</xref>).</p><p id="P3">Despite the central role of ambiguity in predictive processing, experimental research has rarely tested how iterative prediction error minimization unfolds when encountering truly ambiguous stimuli. Instead, studies on visual object recognition typically rely on clear and unambiguous images (<xref ref-type="bibr" rid="R4">Carlson et al., 2013</xref>; <xref ref-type="bibr" rid="R5">Cichy et al., 2014</xref>, <xref ref-type="bibr" rid="R6">2016</xref>) or more complex, “challenging” natural images (<xref ref-type="bibr" rid="R20">Kar et al., 2019</xref>). These studies robustly demonstrate that top-down prior knowledge influences perception, even at early processing stages (i.e., areas V1, V2 of the primate visual cortex; <xref ref-type="bibr" rid="R8">De Lange et al., 2018</xref>; <xref ref-type="bibr" rid="R35">Walsh et al., 2020</xref>). Despite the significance of these studies, we still lack an understanding of the cognitive and input-related determinants of visual ambiguity and how we can resolve it. This is the case primarily because it remains difficult to experimentally capture the ambiguity inherent in real-world perception, which arises mostly from two sources: (1) the biological limitations of the sensory system, such as structural limitations or inherent neural noise, and (2) a sensory input that is often incomplete or unclear. While the former can be studied even with clear stimuli, the latter requires the use of impoverished materials mimicking real-world scenarios (<xref ref-type="bibr" rid="R3">Brascamp &amp; Shevell, 2021</xref>).</p><p id="P4">Importantly, in real-world perception, ambiguity is rarely passively endured; instead, we actively seek additional information to resolve it. For example, imagine walking down a road and noticing a shadowy shape in the distance. This shape is ambiguous not only because of the limitations of our sensory organs, but also because it happens to be partially occluded by a tree, which, additionally, casts shadows over some part of the shape, thus altering the input that actually reaches the eyes. Your brain, based on prior experience, predicts it is a road sign. However, the incomplete visual information makes it difficult to verify your hypothesis. To resolve this ambiguity, you might change your viewing angle, move closer, or squint – actively gathering new inputs to refine your prediction and minimize the error. Several questions arise at this point: what makes the sign ambiguous in the first place? What information was missing or distorted to prevent identification? (e.g. <xref ref-type="bibr" rid="R3">Brascamp &amp; Shevell, 2021</xref>). Also, what is the effect of this information-seeking behaviour on your perception?</p><p id="P5">Here, we build upon these ideas to (i) determine why certain stimuli induce ambiguity and others do not, and (ii) reveal the relationship between gaining new information and subjective disambiguation. To address these research questions, we use ambiguous black-and-white Mooney images (<xref ref-type="bibr" rid="R26">Mooney, 1957</xref>), which are difficult to identify without additional information, but become effortlessly identifiable afterwards. These stimuli allow for real-time investigation of ambiguity resolution in a controlled setting, allowing for a direct test of how perception evolves when new information is gained (<xref ref-type="bibr" rid="R9">Flounders et al., 2019</xref>; <xref ref-type="bibr" rid="R11">González-García et al., 2018</xref>; <xref ref-type="bibr" rid="R13">Gorlin et al., 2012</xref>; <xref ref-type="bibr" rid="R19">Hsieh et al., 2010</xref>), mirroring natural perceptual processes.</p><p id="P6">Answering these questions requires a large amount of data but, to date, no exhaustive collection of Mooney images is publicly available. Therefore, we generated a large-scale, openly available dataset of Mooney images of 1,854 objects. To do so, we created the Mooney version of the images from the THINGSplus database (<xref ref-type="bibr" rid="R32">Stoinski et al., 2023</xref>) and paired it with over 100,000 behavioural ratings from more than 1,000 participants. We first verified that the curated images worked as expected, in that they were poorly identified unless an external, unambiguous input was provided. Next, we examined what makes stimuli ambiguous in the first place by deriving feature representations from text-based embeddings and human behaviour, in combination with deep neural networks (DNNs) that emulate hierarchical processing in the ventral stream (<xref ref-type="bibr" rid="R24">Kubilius et al., 2018</xref>, <xref ref-type="bibr" rid="R23">2019</xref>). We then determined what information helps to resolve ambiguity and how this newly gained information shapes subsequent subjective identification. To this end, we first explored which specific combination of visual features relates to subjective identification before and after disambiguation. Then, we implemented two new metrics that identify the type of semantic information that can be learned during disambiguation and their relationship with subjective identification.</p><p id="P7">To anticipate our results, we observed that ambiguous stimuli are prevented from subjective identification when higher-level, rather than low-level visual features are distorted. However, after disambiguation, lower-level features become more related to subjective identification while higher-level ones play a reduced role. Moreover, disambiguation leads to significant changes in the semantic representations induced by Mooney images, reducing their distance in semantic space to their original, undistorted counterpart while also decreasing uncertainty. Finally, our results reveal that the relationship between information gain and later subjective identification is not strictly linear. Instead, we observe a U-shaped pattern, suggesting that</p><p id="P8">knowledge acquisition does not directly translate into improvements in subjective identification. Rather, later identification improves whenever the acquired information either strongly confirms or violates previous predictions.</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>Large stimulus set validates established disambiguation effects</title><p id="P9">A total of 1,065 participants (final n after exclusions = 947; see <xref ref-type="sec" rid="S9">Methods</xref> section) performed a visual identification task (<xref ref-type="fig" rid="F1">Fig. 1b</xref>). On each trial, an image was presented on the screen. The images were taken from the THINGSplus database (<xref ref-type="bibr" rid="R32">Stoinski et al., 2023</xref>) and reflected a broad set of 1,854 images of natural objects. The images were either presented as an unambiguous, greyscale version of these objects, or as an ambiguous two-tone Mooney image that was created by adding Gaussian blur and binarization (<xref ref-type="fig" rid="F1">Fig. 1a</xref>, see <xref ref-type="sec" rid="S9">Methods</xref>), making it a total of 3,708 images, 3,628 of which were included in further analyses (see <xref ref-type="sec" rid="S9">Methods</xref> for image exclusion details). Participants were first asked to respond whether they could identify the main object on the image. Subsequently, to assess the extent to which participants’ subjective identification matched the true content of the images, they were asked to provide a verbal label or to guess a label if they were unsure about the correct one. The Mooney version of each image was presented in two different trials, always interleaved by a trial where its unambiguous version was displayed (maximum distance between a Mooney and its unambiguous greyscale version = 4 trials, see <xref ref-type="sec" rid="S9">Methods</xref>), allowing us to experimentally induce visual disambiguation. In the following, we will refer to these as pre-disambiguation trials (Mooney images), disambiguation trials (unambiguous, greyscale images), and post-disambiguation trials (Mooney images). This nomenclature reflects our experimental approach, manipulating the timing of ambiguity resolution, and follows the terminology used in previous research (<xref ref-type="bibr" rid="R11">González-García et al., 2018</xref>; <xref ref-type="bibr" rid="R12">González-García &amp; He, 2021</xref>). Each of the 3,628 images was rated by participants an average of 17.8 times (±3.51; range = [11-33]).</p><p id="P10">To evaluate whether the manipulation worked as intended, we first investigated whether subjective identification was affected by exposure conditions. As expected (<xref ref-type="bibr" rid="R1">Albright, 2012</xref>; <xref ref-type="bibr" rid="R9">Flounders et al., 2019</xref>; <xref ref-type="bibr" rid="R11">González-García et al., 2018</xref>; <xref ref-type="bibr" rid="R25">Ludmer et al., 2011</xref>; <xref ref-type="bibr" rid="R33">Van de Cruys et al., 2021</xref>), a repeated-measures ANOVA revealed that subjective identification increased significantly (F<sub>3626,2</sub> = 4101, p &lt; 0.001, ηp2 = 0.48; <xref ref-type="fig" rid="F1">Fig.1C</xref>, left panel) after exposure to the unambiguous version of the image (M = 85.9%, SD = 15.1%), compared to pre-disambiguation trials (M = 47%, SD = 27.9%). Similarly, reaction times were significantly faster after (M = 1692 ms, SD = 366) compared to pre-disambiguation (M = 2537 ms, SD = 410; F<sub>3626,2</sub> = 2902, p &lt; 0.001, ηp2 = 0.38). To assess correct identification, we measured performance in the subsequent naming task, considering both the provided label for each image and the true label plus a list of predefined synonyms as correct responses. Participants’ precision in naming increased significantly after disambiguation trials (pre-disambiguation, M = 16.7%, SD = 22.7%; post-disambiguation, M = 38.5%, SD = 26.9%; F<sub>3626,2</sub> = 2092, p &lt; 0.001, ηp2 = 0.16; see <xref ref-type="fig" rid="F1">Fig. 1C</xref>, right panel). Altogether, these analyses confirm that the material used robustly produced the expected behavioural enhancement in subjective and objective measures in identification of ambiguous images after disambiguation, in line with previous results (<xref ref-type="bibr" rid="R9">Flounders et al., 2019</xref>; <xref ref-type="bibr" rid="R11">González-García et al., 2018</xref>; <xref ref-type="bibr" rid="R33">Van de Cruys et al., 2021</xref>).</p></sec><sec id="S4"><title>Feature preservation and subjective identification</title><p id="P11">Resolving visual ambiguity likely depends on a combination of the visual attributes of the stimuli at different levels of complexity, ranging from lower-level features (e.g., line orientations) to higher-level ones (e.g., object identity) (<xref ref-type="bibr" rid="R29">Rajalingham &amp; DiCarlo, 2019</xref>). To explore the relative contribution of these types of features to subjective identification, we used a computational approach inspired by the hierarchical architecture of the human ventral visual stream. This analysis involved two main steps: feature extraction across hierarchical layers of a neural network model, and estimation of the degree to which feature representations of the unambiguous images are maintained in the Mooney images via a similarity-based assessment (<xref ref-type="fig" rid="F2">Fig. 2a</xref>).</p><p id="P12">First, unambiguous greyscale and Mooney versions of the same image were processed through CORNet-S (<xref ref-type="bibr" rid="R23">Kubilius et al., 2019</xref>). This deep convolutional neural network is a computational model that emulates hierarchical processing in the primate ventral stream with separate modules for V1, V2, V4, and IT and has been shown to yield a good correspondence of individual layers to different cortical processing stages (<xref ref-type="bibr" rid="R23">Kubilius et al., 2019</xref>). For each input image, we extracted the feature representations at each module within the network using the Net2Brain toolbox (<xref ref-type="bibr" rid="R2">Bersch et al., 2022</xref>). In the second step, we quantified the similarity between the features extracted from the unambiguous grayscale and Mooney versions of each image. This similarity, measured as a Pearson correlation between the two feature sets at each layer, yielded a preservation index for each image at each level (<xref ref-type="fig" rid="F2">Fig. 2a</xref>). The preservation index reflects the degree to which properties of the original image were maintained in its ambiguous counterpart on a specific level of the processing hierarchy of the visual system. In other words, this index provides insights into how much of the original image’s representational content survives the abstraction process of creating Mooney images. In all layers, the preservation index was significantly greater than zero (all p-values &lt; 0.001; <xref ref-type="fig" rid="F2">Fig. 2b</xref>), suggesting that Mooney images always kept certain features of their unambiguous greyscale counterpart. The mean preservation index was higher for V1 (M = 0.718, SD = 0.084) and decreased in V2 (M = 0.581, SD = 0.082), V4 (M = 0.524, SD = 0.08), and IT (M = 0.236, SD = 0.072). Accordingly, a one-way ANOVA revealed significant differences in preservation across layers (F<sub>7388,3</sub> = 12068, p &lt; 0.001, ηp2 = 0.322), indicating that original features of the unambiguous image were particularly impaired in higher-level layers after Mooney transformation. To better assess whether the preservation index pattern across layers was driven by our image manipulation, we conducted control analyses using alternative high-pass and low-pass filtered transformations (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Materials</xref>). The results showed that the high-pass transformation produced a distinct preservation pattern, with equivalent preservation in early and late layers. In contrast, the low-pass transformation yielded a preservation profile similar to that of the Mooney transformation, with reduced preservation in higher-level layers. This result is consistent with the construction process of the Mooney images, which included a low-pass Gaussian filter as one of its steps.</p><p id="P13">We then explored the impact of this particular pattern of feature retention on behavior. In particular, we examined 1) the extent to which the degree of preservation of individual images correlated with their subjective identification during Mooney image presentation, and 2) whether such identification was more strongly associated with the preservation of lower-level features or higher-level information. To do so, for each image, we calculated the image-level correlation (see <xref ref-type="sec" rid="S9">Methods</xref>) between the preservation index and subjective identification. This correlation was computed for each DNN layer and exposure condition (pre- and post-disambiguation). In both conditions, correlations between preservation index and subjective identification were significantly above baseline (see <xref ref-type="sec" rid="S9">Methods</xref>; p &lt; 0.01, Bonferroni corrected; <xref ref-type="fig" rid="F2">Fig. 2c</xref>). Furthermore, a two-way ANOVA with factors of layer and exposure condition revealed a significant interaction (F<sub>11082,6</sub> = 1220.46, p &lt; 0.001, ηp2 = 0.248), together with main effects of layer (F<sub>3694,2</sub> = 56316.01, p &lt; 0.001, ηp2 = 0.838) and exposure condition (F<sub>5541,3</sub> = 10547.78, p &lt; 0.001, ηp2 = 0.584). These main effects reveal that the relation between subjective identification and preservation 1) becomes stronger in higher-order layers, indicating an overall greater reliance on higher-level features for subjective identification; and 2) is generally higher after disambiguation. Importantly, further analyses of the interaction effect suggest that this general pattern is reversed in higher-order layers.</p><p id="P14">More specifically, t-test planned comparisons revealed a significant increase in correlation after disambiguation in the V1 and V2 layers (all p &lt; 0.01, Bonferroni corrected), while a significant decrease was observed in the IT layer (p &lt; 0.01, Bonferroni corrected). These results suggest that the preservation of lower-level features (as captured in V1, V2 and V4) increases its relative effect on subjective identification after disambiguation, compared to higher-level features (e.g., IT), whose influence diminishes once the unambiguous greyscale image is presented.</p></sec><sec id="S5"><title>Contribution of visual dimensions to identification</title><p id="P15">Building on these findings, we next determined which specific combination of visual dimensions enables successful subjective identification (pre- and post-disambiguation) using the metadata of the THINGS database (<xref ref-type="bibr" rid="R16">Hebart et al., 2019</xref>, <xref ref-type="bibr" rid="R15">2023</xref>; <xref ref-type="bibr" rid="R32">Stoinski et al., 2023</xref>). The THINGS database provides a 66-dimensional behavioural similarity embedding for the 1,854 object concepts used in this study, constructed based on human similarity judgments (<xref ref-type="bibr" rid="R17">Hebart et al., 2020</xref>, <xref ref-type="bibr" rid="R15">2023</xref>). These dimensions have been shown to capture visual (e.g., round, fine-grained) and semantic (e.g., animal-related, valuable) properties of objects. In our study, three independent raters classified this original set of dimensions as either visual, semantic or mixed. Then, we discarded the ones labeled as mixed and all color-related dimensions (e.g., “sand-colored”) as our stimuli were greyscale images. This left us with 47 dimensions. We next applied a multiple regression approach which allowed us to simultaneously evaluate the unique and shared predictive power of these dimensions. To contextualize this result, we calculated a noise ceiling over 1,000 participant splits using Spearman-Brown corrected split-half reliability, representing the maximum explainable variance, given the inherent noise in the data.</p><p id="P16">In the pre-disambiguation condition, the regression model explained a total variance (R<sup>2</sup>) of 0.153 in subjective identification ratings. The average noise ceiling, representing the maximum explainable variance, was 0.925, resulting in a model-to-ceiling ratio of 0.18. This indicates that 18% of the explainable variance was captured by the model, suggesting that while the model may leverage meaningful predictive information from the dimensional embeddings, a substantial portion of the variance remains unexplained, likely reflecting other sources of variability (e.g., trial-by-trial changes within-participants). Variance partitioning revealed that 62.4% of the total explained variance was uniquely attributable to semantic dimensions, 21.8% to perceptual dimensions, and the remaining portion to shared contributions between the two. These results highlight the dominant role of semantic information in shaping identification judgments prior to disambiguation, with perceptual dimensions providing minimal additional predictive value.</p><p id="P17">In the post-disambiguation condition, the regression model explained a total variance (R<sup>2</sup>) of 0.099 in subjective identification ratings. The average noise ceiling was 0.849, with a model-to-ceiling ratio of 0.137, indicating that the model captured approximately 14% of the maximum explainable variance. Despite this decrease in overall predictive performance, the relative contributions of semantic and perceptual dimensions remained consistent, with semantic dimensions maintaining a substantially larger influence (64.4%) on subjective identification than perceptual dimensions (28%).</p><p id="P18">These results suggest that while the overall predictability of subjective identification ratings diminishes post-disambiguation, the underlying reliance on semantic features remains robust. The minimal contribution of perceptual dimensions across conditions further reinforces the idea that subjective identification judgments are predominantly driven by higher-order semantic information, regardless of disambiguation.</p></sec><sec id="S6"><title>Semantic distance and entropy</title><p id="P19">Verbal accuracy of correct identification (<xref ref-type="fig" rid="F1">Fig. 1c</xref>) provides valuable yet limited insights into the type of information gained during disambiguation. Here, we leveraged two complementary metrics to provide a more fine-grained characterisation of this process: semantic distance and semantic entropy. Semantic distance (see <xref ref-type="fig" rid="F3">Fig. 3a</xref>) measures how far in the semantic space a provided label is concerning the target words or their synonyms. For instance, for a given image, participants may have typed a related but incorrect label (“donkey” for “horse”). A low semantic distance would then capture how closely related the answer is to the correct label. Quantifying these distances provides a more nuanced measure of how participants’ representations shift in semantic space after acquiring relevant information (i.e., during grey trials). Orthogonal to this measure, semantic entropy (see <xref ref-type="fig" rid="F3">Fig. 3a</xref>) quantifies the degree of heterogeneity across participants in the labels provided for a given image. Lower semantic entropy reflects that participants are consistent in their responses to an image, using a small and constant set of labels. However, higher levels of semantic entropy indicate that participants use a broader set of labels when naming an object. A reduction in semantic entropy after being presented with the unambiguous grayscale image would suggest decreased variability in the labels provided by participants. Importantly, as mentioned before, the computation of these two metrics is independent (see <xref ref-type="fig" rid="F3">Fig. 3a</xref>): a low semantic entropy does not imply a reduced semantic space, as a small set of responses (e.g. branch and tree; low entropy) can still be far from the target label (e.g. arm). For both metrics, our manipulation induced strong effects. For semantic distance (F = 3038, p &lt; 0.001, η<sub>p</sub><sup>2</sup> = 0.26), the separation between the provided and true labels in the semantic space decreased after disambiguation (pre-disambiguation: M = 0.25, SD = 0.18; post-disambiguation: M = 0.46, SD = 0.22). Similarly, semantic entropy was significantly higher before than after disambiguation (pre-disambiguation: M = 2.52, SD = 0.92; post-disambiguation: M = 1.72, SD = 0.92; F = 2085, p &lt; 0.001, ηp2 = 0.74). Finally, we tested the extent to which semantic distance tracks subjective identification. First, we labelled trials corresponding to individual images and participants based on their subjective identification pattern (e.g. “always identified” if a participant answered “yes” to the subjective identification question in all three versions of one image; pre, unambiguous grayscale, and post). We then assessed semantic distance for images following each of 4 a priori patterns: never identified, only identified during unambiguous grayscale trials, only identified in post and unambiguous grayscale trials (i.e. disambiguation pattern), and, finally, always identified. A pattern x exposure condition ANOVA showed a significant interaction of both factors (F = 36.58, p &lt; 0.001, η<sub>p</sub><sup>2</sup> = 0.054), revealing a differential effect of the condition on the semantic distance scores of images following different patterns of subjective identification (see <xref ref-type="fig" rid="F3">Fig. 3b</xref>). More specifically, exposure had the greatest effect on semantic distance for those images only identified in their unambiguous version and post (t = 19.8, p &lt; 0.001), followed by images always identified (t = 11, p &lt; 0.001), then images identified only in unambiguous grayscale (t = 6.5, p &lt; 0.001), and finally, images that were never identified (t = 6.13, p &lt; 0.001). In summary, these results demonstrate that the disambiguation of visual information leads to a reduced semantic distance and lower variability in participants’ responses. Altogether, this suggests that disambiguation not only sharpens semantic representation but increases the consistency in how visual information is interpreted across individuals.</p></sec><sec id="S7"><title>The complex relationship between information gained and later identification</title><p id="P20">A reduction in semantic distance and entropy between pre- and post-disambiguation can be understood as the consequence of gaining certain information during unambiguous visual presentation. However, the impact of this gain on later identification remains unclear. For instance, gaining more information could linearly improve identification (e.g., large increments of information would provide a richer context and thus lead to better identification performance than smaller increments). Yet, small increments of information can still entail profound changes in later identification albeit via different mechanisms (e.g., by confirming initial guesses; <xref ref-type="bibr" rid="R28">Quent et al., 2022</xref>). Hence, we employed a regression model (Ordinary Least Square; OLS) to examine the effect of information gained either in semantic distance or entropy on subjective identification after disambiguation. To account for a non-linear (quadratic) relation between predictors and subjective identification, we also included squared terms for both independent variables.</p><p id="P21">When including all images, the model was statistically significant (F(4,1804) = 54.52, p &lt; 0.001) and explained 10.8% of the variance in subjective identification (R<sup>2</sup> = 0.108, adjusted R<sup>2</sup> = 0.106, p &lt; 0.001). Information gained in semantic distance and entropy showed a significant monotonic contribution for subjective identification, being such relation negative in the case of semantic distance (β = -0.7285, p &lt; 0.001) and positive for entropy (β = 0.0695, p &lt; 0.001). Regarding a squared relationship between predictors and subjective identification, information gained on semantic distance had a significant non-linear effect (β = 0.7608, p &lt; 0.001) but not information gained in entropy (β = -0.0018, p = 0.679). This pattern of results was also found when applying a jackknife approach, excluding data points where information gain in entropy or semantic distance fell below the 5th percentile or above the 95th percentile. This confirms that the observed relationship is not merely driven by a small number of extreme values.</p><p id="P22">To specifically examine the role of gaining both types of semantic information after successful disambiguation, we focused this analysis on a subset of images exhibiting the most relevant pattern. Specifically, we selected images with high semantic distance and entropy values before disambiguation and lower values for both measures afterwards, corresponding to the bottom-right quadrant in both plots of <xref ref-type="fig" rid="F3">Fig. 3c</xref>. To identify these images in a data-driven manner, we applied k-means clustering (k = 3) using pre- and post-disambiguation values for both variables (semantic distance and entropy). From this analysis, we selected Cluster 2 (see <xref ref-type="fig" rid="F3">Fig. 3c</xref>), which displayed the desired pattern of interest: high values before disambiguation that dropped to lower values afterwards. Regression models were statistically significant for Cluster 2 both in semantic distance (F(2, 746) = 17.17, p &lt; 0.001; R<sup>2</sup> = 0.044, adjusted R<sup>2</sup> = 0.041) and entropy (F(2, 705) = 12.27, p &lt; 0.001; R<sup>2</sup> = 0.034, adjusted R<sup>2</sup> = 0.031). For semantic distance, results showed a significant negative linear relationship (β = −0.4932, p &lt; 0.001) and a positive quadratic relationship (β = 0.6512, p &lt; 0.001). Similarly, for entropy, a significant negative linear relationship (β = −0.0573, p &lt; 0.001) and a positive quadratic relationship (β = 0.0160, p = 0.001) were observed. The same pattern of results was found when applying a jackknife approach for outliers (excluding data in information gained below or above 5th and 95th percentile respectively), except a lack of significant linear contribution of information gained in entropy (β = -0.0163, p = 0.122). Together, these findings suggest that, while information gain initially relates negatively to subjective identification, a quadratic effect indicates that this relationship reverses at higher levels of semantic distance reduction and, to a lesser extent, entropy. In other words, small information gain might introduce uncertainty or challenge initial interpretation, which could lead to lower subjective identification. However, when information gain is low, confirming prior predictions, or large enough to violate previous expectations, it improves identification performance.</p></sec></sec><sec id="S8" sec-type="discussion"><title>Discussion</title><p id="P23">Despite the fact the world is inherently ambiguous, how humans deal with such ambiguity in natural vision remains unclear. What are the visual features underlying perceptual ambiguity and how do these influence its later resolution? How can we make use of new information to then shape the identification of stimuli that were perceptually uncertain? The present work provides new insights into these questions by leveraging a combination of behavioural and computational modeling approaches to characterize ratings from over 1000 participants of a large-scale dataset of ambiguous Mooney images. In summary, we observed that (i) distortion of high- rather than low-level visual features induces initial ambiguity; (ii) after disambiguation, while low-level features increase their contribution to subjective identification, high-level ones decrease in relevance; (iii) disambiguation is characterised by information gain, i.e., a systematic reduction in semantic distance between ambiguous and unambiguous images, with a concurrent decrease in uncertainty; yet, (iv) the relationship between information gain and subjective identification follows a non-linear, U-shaped trajectory.</p><p id="P24">Previous research has explored the visual features that contribute to ambiguity, including spatial characteristics such as contours and edges, occlusion, and figure-ground relationships (for a review, see <xref ref-type="bibr" rid="R3">Brascamp &amp; Shevell, 2021</xref>), as well as colour, luminance, motion, and depth. Our study extends previous attempts by capitalizing on DNNs that mimic the primate visual system to investigate how different visual features influence perception along the processing gradient of the ventral visual stream. Specifically, we examine this in a context where a series of low-pass filters are applied to unambiguous, clear images to generate Mooney versions of them (see <xref ref-type="sec" rid="S9">Methods</xref> section for details). We found that in such manipulation, higher-level (vs. lower-level) features are particularly impaired. In other words, converting a clear image to a Mooney image mostly impairs higher-level visual features. But how does high-level visual information (or lack thereof) relate to resolving ambiguity? Pre-disambiguation (that is, when participants encounter a Mooney image for the first time before having seen its clear counterpart), the poorer preservation of high-level visual features in the Mooney image explains the observed low rate of identification. In contrast, after being able to reconstruct the semantic meaning of the Mooney image from the greyscale clear image (post-disambiguation phase), lower-level features become more strongly correlated with subjective identification. This indicates that lower-level features may play a more critical role for a successful match between the unambiguous and Mooney image versions. Therefore, even with identical sensory input, different representational content appears to be prioritized depending on the availability of a recently learned clarifying percept. Altogether, our results suggest that the resolution of visual ambiguity is not only a function of low-level visual feature properties but instead is highly dependent on transformations in higher-level visual components.</p><p id="P25">Traditional perceptual learning metrics of ambiguity resolution (e.g., subjective identification, reaction times) have largely been agnostic to the type of information learned. Here, we found disambiguation to be characterised by a systematic reduction in semantic distance between an ambiguous image and its unambiguous greyscale version, with a concurrent decrease in entropy (i.e., less variance in the given labels for the same perceptual input). These two complementary metrics provide deeper insight into the information extracted from unambiguous images and its relationship with subjective identification during the post-disambiguation stage. Crucially, although both types of information track perceptual learning, the content captured by semantic distance and entropy differentially helps subjective identification. Across all trials, information gain in semantic entropy (i.e., reducing semantic entropy) relates positively to subjective identification, suggesting that reduced perceived ambiguity strengthens the subjective sense of identification. In contrast to this linear relationship, information gain in semantic distance relates positively to subjective identification only for high levels of information gain, with lower levels displaying a negative relationship. This distinction highlights the different roles played by these two metrics in perceptual learning: while semantic entropy tracks how well participants can perceive the object as an entity (e.g., “Now I clearly see the object”), semantic distance reflects the precision with which participants can access relational features of the stimuli (e.g., “The object is a horse-like animal”). The dataset that we are releasing with the current manuscript is thus well poised to further explore these metrics and their contributions to perceptual learning and other related phenomena.</p><p id="P26">Interestingly, when restricting analyses to images with reduced semantic distance/entropy after disambiguation (i.e., the frequently observed Mooney pattern), both metrics exhibit a non-linear relationship with subjective identification. Specifically, both minimal and maximal information gain in these metrics correspond to higher levels of subjective identification than moderate information gain. For semantic distance, maximal information gain occurs when the initial guess was semantically far from the correct label and was drastically reduced post-disambiguation. In contrast, minimal information gain describes trials where distance was similar during the initial guess and the post-disambiguation response. Whereas the former scenario arguably results in a large prediction error, which has been linked to knowledge updating (<xref ref-type="bibr" rid="R18">Henson &amp; Gagnepain, 2010</xref>), the latter one could reflect a situation where the initial guess was close enough to the actual stimulus and the clear image acted as a confirmation, enhancing subjective identification. Such duality finds parallels in accounts of the relationship between prediction error and episodic encoding (<xref ref-type="bibr" rid="R34">Van Kesteren et al., 2012</xref>), where the interplay of two opposing encoding mechanisms renders non-linear patterns in a common output variable (<xref ref-type="bibr" rid="R14">Greve et al., 2019</xref>; <xref ref-type="bibr" rid="R27">Ortiz-Tudela et al., 2024</xref>; <xref ref-type="bibr" rid="R28">Quent et al., 2022</xref>). It is worth noting that these minimal-gain trials may reflect cases where participants had subjectively identified a percept (albeit with intermediate semantic distance) before disambiguation and maintained the same percept after disambiguation. In such cases, low semantic entropy may have additionally boosted the subjective feeling of identification and promoted a rigid internal model - that would only be updated in response to a substantial prediction error. Accordingly, only substantial reductions in semantic entropy lead to better subjective identification, again arguably via model updating. This speculative idea deserves future testing.</p><p id="P27">Altogether, the reported non-linear, U-shaped trajectory between information gain and subjective identification challenges the intuition that information gain linearly facilitates perception and suggests a more complex interplay where the dynamics between predictive mechanisms and prior knowledge must be taken into account. Relatedly, recent evidence has shown that post-disambiguation, ambiguous images elicit a neural representation more similar to its undistorted version than to the exact same image before disambiguation (<xref ref-type="bibr" rid="R9">Flounders et al., 2019</xref>; <xref ref-type="bibr" rid="R11">González-García et al., 2018</xref>). This drastic representational change given the same sensory input suggests that encountering the distorted image after disambiguation brings back to mind the previously perceived clear version. This raises the interesting possibility that visual identification after gaining new information can be understood as a pattern completion process (<xref ref-type="bibr" rid="R31">Rolls, 2015</xref>), in which a partial stimulus serves as a cue for a previously encoded unambiguous stimulus. However, to the best of our knowledge, the underlying processes behind the (re)activation of a partial (ambiguous) stimulus linked to prior non-ambiguous percepts have not been explored before. Although our results provide initial insights into this question, future research should ascertain the extent to which the association between ambiguous and unambiguous stimuli depends on connections between lower- and/or higher-level features.</p><p id="P28">In conclusion, we found that ambiguous, distorted stimuli primarily suffer from a loss of high-level visual features, while retaining lower-level features from their original, undistorted counterparts. Interestingly, while high-level information is initially crucial to subjective identification, lower-level features gain in relevance after successful disambiguation. This suggests that the successful reactivation of recently perceived prior knowledge relies more on matching low-level characteristics, whereas abstract and invariant object features play a greater role in identification before disambiguation. Finally, our results reveal a non-linear relationship between information gain and subsequent subjective identification. Altogether, these findings open new avenues for studying how information gain and predictive mechanisms interact in visual perception.</p></sec><sec id="S9" sec-type="methods"><title>Methods</title><sec id="S10" sec-type="subjects"><title>Participants</title><p id="P29">A total of 1065 participants were recruited online via Prolific. Participants who did not finish the task or presented mean reaction times longer than 5 s in the identification task were excluded, remaining 1002 of them (384 female, 608 male, 7 answered “other”, and 3 participants preferred not to answer; mean age 28.67, SD ±4.60; 831 from the United Kingdom, 153 from the United States of America, and 13 from other countries). The eligibility criteria were that participants had to be between 18 and 35 years old and have English as a first language. Additionally, only participants with an approval rate between 95-100% and more than 10 previous submissions in other experiments in Prolific were eligible. Regarding exclusion criteria, participants from previous pilot experiments with the same images, as well as participants using smartphones or tablets, were not eligible. For the remaining participants, we applied the 1.5 interquartile range (IQR) rule in the identification task for unambiguous images to exclude outliers (55 outliers in the lower bound). After this, 947 participants remained for further analyses. All participants were reimbursed at a rate of £6.00 per hour for completing the task (~ 20 minutes); participation was entirely voluntary, and all participants provided informed consent. The experiment was approved by the Ethics Committee of the University of Granada.</p></sec><sec id="S11"><title>Stimuli, task, and procedure</title><p id="P30">A total of 1854 greyscale-transformed images from the THINGSplus object database (<xref ref-type="bibr" rid="R32">Stoinski et al., 2023</xref>) were used in the experiment. Three experimenters, who had been previously trained, transformed all greyscale images into Mooney images by manually adjusting a Gaussian filter and selecting an intensity threshold for each item individually. This threshold operated so that pixels above it are set to white and pixels below are set to black, rendering two-tone images.</p><p id="P31">In the main task, each trial started with a fixation cross (1 s) in the centre of the screen, followed by either a Mooney or a greyscale unambiguous image. Participants were asked to respond as quickly as possible whether they identified or not the object on the displayed image by pressing the “E” or “I” key respectively (response mappings were counterbalanced across participants). After response or 5 s, participants were asked to type the name of the object using the keyboard. This naming task was intended to verify participants’ identification. When participants started typing, they were prompted with a drop-down menu that contained a series of words matching the typed letters. Participants were instructed to select the desired word by clicking on it. These words included the THINGS image names plus their WordNet synonyms, 3434 words in total. The next trial started after choosing the name of the object or after a maximum of 20 s. The Mooney version of each image was presented in two different trials (from now on, pre- and post-disambiguation), always interleaved by a trial where its unambiguous, greyscale version was displayed (disambiguation trial). The maximum distance between a Mooney image and its unambiguous version was 4 trials. Due to design constraints reasons, the Mooney and unambiguous version of the same image were presented sequentially in one case per participant. The experiment lasted approximately 20 minutes.</p><p id="P32">Participants were collected following a batch fashion. To sample the entire image space, the 1854 images were randomly divided in 46 subgroups of 40 images each. Then, we proceeded to collect data from 3 batches (~150 subjects, to account for dropouts). Since participants were randomly allocated to one of the 47 subgroups, some images were oversampled while others were not presented at all. To account for this, every 3 batches, we collected an additional fine-tuned batch that contained those subgroups of images that were not presented initially. Additionally, due to a technical issue, participants from the first 9 batches had some words (394 out of 3434) missing from the drop-down menu of the naming task. To avoid any potential influence of this issue, we removed these trials from these participants. Moreover, to compensate for these missing trials, we ran one additional batch of 100 participants that were presented 40 of the 394 images each, before continuing with the 10th batch of participants. On average, each participant rated 34.10 images (±5.28; range = [10 - 40]). Across all participants, images rated by participants a number of times below 2 SD of the mean number of ratings were excluded from further analysis (40 out of a total of 1854 images). After applying this exclusion criteria, each of the remaining 1814 images was rated an average of 17.80 times (±3.51; range = [11 - 33]). An additional five images had to be excluded from semantic distance analyses because their semantic embedding was not available in the original THINGSplus object database (<xref ref-type="bibr" rid="R32">Stoinski et al., 2023</xref>).</p></sec><sec id="S12"><title>Data preprocessing</title><p id="P33">After exclusion of participants and images (see above), we processed the provided verbal labels for the remaining trials to check for the use of synonyms (considering those included in WordNet for each of our true labels) and computed semantic distance as the cosine dissimilarity between the embeddings of the provided and true label. Embeddings were retrieved from the THINGS metadata. Specifically, we used a word2vec implementation of sense vector (developed to distinguish between different meanings of words) augmented to account for missing entries. We then computed semantic entropy to quantify response diversity for each image under different conditions. For each image-condition pair, response probabilities were derived by normalizing the frequency of each unique verbal descriptor. Entropy was then calculated applying Shannon’s formula to the normalized frequencies. No further preprocessing was done except for reaction time analysis, where trials with responses below 200 milliseconds or above 5 seconds were excluded.</p></sec><sec id="S13"><title>Image-level correlation estimation</title><p id="P34">To obtain an image-level correlation between their preservation index and subjective identification values, we implemented a bootstrapped random sampling approach inspired by approaches to obtain local correlations (<xref ref-type="bibr" rid="R36">Zhang et al., 2013</xref>). This method calculates the correlation coefficients for subsets of data associated with each image, while introducing randomness to enhance robustness. Image-level correlations were calculated per image, using subsets of other images of a given size (n = 3). These subsets were defined randomly without replacement (even in other iterations of the bootstrapping approach), ensuring variability in the selection across repetitions. Pearson correlation coefficient was calculated between the two variables for each randomly selected subset. To obtain a more stable estimate, the random selection and local correlation were repeated 400 times (bootstrapping), keeping in mind the limitation of defining each dataset without replacement. For each image, this resulted in 400 local correlation values which were then average to calculate the image-level correlation.</p><p id="P35">A null correlation baseline was generated following the same approach but shuffling the two variables independently within each subset. This preserved the statistical properties of the data while removing any inherent relationship between variables. The same bootstrapping and aggregation procedures were applied to the shuffled data.</p></sec><sec id="S14"><title>Regression on dimensional embedding</title><p id="P36">Out of the total 49 available object space dimensions for each image obtained in the THING database (<xref ref-type="bibr" rid="R22">Kramer et al., 2023</xref>), a subset of 36 were labeled as mostly semantic or mostly visual based on previous study using a similar approach (<xref ref-type="bibr" rid="R22">Kramer et al., 2023</xref>). Each dimensional embedding was treated as a predictor in a regression model for subjective identification in post, enabling the decomposition of the total variance explained (\(R^2 \)) into components attributable to semantic features, visual features, and their shared interactions.</p><p id="P37">To contextualize the model’s performance within the limits of the data’s inherent predictability, we computed a noise ceiling representing the maximum explainable variance. This was computed using a split-half reliability approach repeated 1,000 times to ensure robustness. In each iteration, participants were randomly split into two non-overlapping groups, and average subjective identification scores were computed for each image within each group. The Pearson correlation between the two groups’ image-level scores was calculated and corrected using the Spearman-Brown prediction formula to estimate the reliability of the full dataset. The average Spearman-Brown–corrected value across iterations was used as the noise ceiling. Model performance was then expressed as a proportion of this ceiling, indicating how much of the explainable variance was accounted for by the dimensions.</p><p id="P38">In addition to this global measure, variance partitioning was performed to disentangle the unique contributions of semantic and visual dimensions, as well as their shared variance. Variance partitioning involved fitting regression models for semantic dimensions alone, visual dimensions alone, and both combined. From these models, we computed the unique variance explained by semantic dimensions and the unique variance explained by visual dimensions. These components were normalized relative to the model-explained variance to obtain a reliable estimate of the relative contribution of each feature type under the constraints of the data’s inherent noise.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Materials</label><media xlink:href="EMS206054-supplement-Supplementary_Materials.pdf" mimetype="application" mime-subtype="pdf" id="d47aAcEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S16"><title>Acknowledgments</title><p>J.L.D. was supported by Project PID2023-151104NA-I00 funded by MCIN/AEI/10.13039/501100011033 and by FEDER, EU, and Grant RYC2021-033940-I funded by MCIN/AEI/10.13039/501100011033 and by the European Union NextGeneration EU/PRTR. J.O.T. was supported by an European Research Council Starting Grant (ERC-2024-StG-CONNECTS-101161992), and Grant RYC2023-045452-I funded by MICIU/AEI/10.13039/501100011033 and by the FSE+. J.V. was supported by an FPI grant (CEX2021-001161-M-20-5) by the Spanish Ministry of Science and Innovation. M.H. was supported by a research group grant from the Max Planck Society, the ERC Starting Grant project COREDIM (ERC-StG-2021-101039712), and funding from the Hessian Ministry of Higher Education, Science, Research, and the Arts through a LOEWE Start Professorship and the Research Cluster “The Adaptive Mind” via its Excellence Program. C.G.G. was supported by Project PID2023-149428NB-I00 funded by MCIN/AEI/10.13039/501100011033 and by FEDER, EU, and Grant RYC2021-033536-I funded by MCIN/AEI/10.13039/501100011033 and by the European Union NextGeneration EU/PRTR. The Mind, Brain and Behavior Research Center receives funding from grants CEX2023-001312-M by MICIU/AEI/10.13039/501100011033 and UCE-PP2023-11 by the University of Granada. The funders had no role in the study design, data collection and analysis, decision to publish or preparation of the manuscript. This manuscript is part of the PhD thesis of J.V.</p></ack><sec id="S15" sec-type="data-availability"><title>Data and code availability</title><p id="P39">The data and code of this study are available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/mzp23/">https://osf.io/mzp23/</ext-link>. All the materials plus a Mooney image database derived from the study can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/wobc/things-mooney">https://github.com/wobc/things-mooney</ext-link>. This repository further contains the toolbox that was used by the experimenters to create the Mooney images as well as an interface to specify various criteria (e.g., semantic distances, recognition rates) and generate your own Mooney-style image set.</p></sec><fn-group><fn fn-type="con" id="FN1"><p id="P40"><bold>Author contributions</bold></p><p id="P41">J.L.D., J.O.T, and C.G.G. were responsible for conceptualization, data curation, formal analysis, funding acquisition, investigation, methodology, project administration, resources, software, visualization, and writing of the original draft and further edits. J.V. was involved in data curation, software, visualization and writing – review and editing. M.H. was involved in conceptualization, methodology and writing – review and editing.</p></fn><fn fn-type="conflict" id="FN2"><p id="P42"><bold>Competing interests</bold></p><p id="P43">The authors declare no competing interests.</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albright</surname><given-names>TD</given-names></name></person-group><article-title>On the Perception of Probable Things: Neural Substrates of Associative Memory, Imagery, and Perception</article-title><source>Neuron</source><year>2012</year><volume>74</volume><issue>2</issue><fpage>227</fpage><lpage>245</lpage><pub-id pub-id-type="pmcid">PMC3361508</pub-id><pub-id pub-id-type="pmid">22542178</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2012.04.001</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bersch</surname><given-names>D</given-names></name><name><surname>Dwivedi</surname><given-names>K</given-names></name><name><surname>Vilas</surname><given-names>M</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Roig</surname><given-names>G</given-names></name></person-group><article-title>Net2Brain: A Toolbox to compare artificial vision models with human brain responses (Version 2)</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type="pmcid">PMC12089098</pub-id><pub-id pub-id-type="pmid">40395367</pub-id><pub-id pub-id-type="doi">10.3389/fninf.2025.1515873</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brascamp</surname><given-names>JW</given-names></name><name><surname>Shevell</surname><given-names>SK</given-names></name></person-group><article-title>The Certainty of Ambiguity in Visual Neural Representations</article-title><source>Annual Review of Vision Science</source><year>2021</year><volume>7</volume><issue>1</issue><fpage>465</fpage><lpage>486</lpage><pub-id pub-id-type="pmcid">PMC8687672</pub-id><pub-id pub-id-type="pmid">34524881</pub-id><pub-id pub-id-type="doi">10.1146/annurev-vision-100419-125929</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlson</surname><given-names>T</given-names></name><name><surname>Tovar</surname><given-names>D</given-names></name><name><surname>Alink</surname><given-names>A</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Representational dynamics of object vision: The first 1000 ms</article-title><source>Journal of Vision</source><year>2013</year><volume>13</volume><issue>10</issue><fpage>1</fpage><lpage>19</lpage><pub-id pub-id-type="pmid">23908380</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><article-title>Resolving human object recognition in space and time</article-title><source>Nature Publishing Group</source><year>2014</year><volume>17</volume><issue>3</issue><fpage>455</fpage><lpage>462</lpage><pub-id pub-id-type="pmcid">PMC4261693</pub-id><pub-id pub-id-type="pmid">24464044</pub-id><pub-id pub-id-type="doi">10.1038/nn.3635</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><article-title>Similarity-Based Fusion of MEG and fMRI Reveals Spatio-Temporal Dynamics in Human Cortex During Visual Object Recognition</article-title><source>Cerebral Cortex</source><year>2016</year><volume>26</volume><issue>8</issue><fpage>3563</fpage><lpage>3579</lpage><pub-id pub-id-type="pmcid">PMC4961022</pub-id><pub-id pub-id-type="pmid">27235099</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhw135</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>A</given-names></name></person-group><article-title>Whatever next? Predictive brains, situated agents, and the future of cognitive science</article-title><source>Behavioral and Brain Sciences</source><year>2013</year><volume>36</volume><issue>3</issue><fpage>181</fpage><lpage>204</lpage><pub-id pub-id-type="pmid">23663408</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Lange</surname><given-names>FP</given-names></name><name><surname>Heilbron</surname><given-names>M</given-names></name><name><surname>Kok</surname><given-names>P</given-names></name></person-group><article-title>How Do Expectations Shape Perception?</article-title><source>Trends in Cognitive Sciences</source><year>2018</year><volume>22</volume><issue>9</issue><fpage>764</fpage><lpage>779</lpage><pub-id pub-id-type="pmid">30122170</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flounders</surname><given-names>MW</given-names></name><name><surname>González-García</surname><given-names>C</given-names></name><name><surname>Hardstone</surname><given-names>R</given-names></name><name><surname>He</surname><given-names>BJ</given-names></name></person-group><article-title>Neural dynamics of visual ambiguity resolution by perceptual prior</article-title><source>eLife</source><year>2019</year><volume>8</volume><elocation-id>e41861</elocation-id><pub-id pub-id-type="pmcid">PMC6415935</pub-id><pub-id pub-id-type="pmid">30843519</pub-id><pub-id pub-id-type="doi">10.7554/eLife.41861</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K</given-names></name></person-group><article-title>A theory of cortical responses</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><year>2005</year><volume>360</volume><issue>1456</issue><fpage>815</fpage><lpage>836</lpage><pub-id pub-id-type="pmcid">PMC1569488</pub-id><pub-id pub-id-type="pmid">15937014</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2005.1622</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>González-García</surname><given-names>C</given-names></name><name><surname>Flounders</surname><given-names>MW</given-names></name><name><surname>Chang</surname><given-names>R</given-names></name><name><surname>Baria</surname><given-names>AT</given-names></name><name><surname>He</surname><given-names>BJ</given-names></name></person-group><article-title>Content-specific activity in frontoparietal and default-mode networks during prior-guided visual perception</article-title><source>eLife</source><year>2018</year><volume>7</volume><elocation-id>e36068</elocation-id><pub-id pub-id-type="pmcid">PMC6067880</pub-id><pub-id pub-id-type="pmid">30063006</pub-id><pub-id pub-id-type="doi">10.7554/eLife.36068</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>González-García</surname><given-names>C</given-names></name><name><surname>He</surname><given-names>BJ</given-names></name></person-group><article-title>A Gradient of Sharpening Effects by Perceptual Prior across the Human Cortical Hierarchy</article-title><source>The Journal of Neuroscience</source><year>2021</year><volume>41</volume><issue>1</issue><fpage>167</fpage><lpage>178</lpage><pub-id pub-id-type="pmcid">PMC7786209</pub-id><pub-id pub-id-type="pmid">33208472</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2023-20.2020</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorlin</surname><given-names>S</given-names></name><name><surname>Meng</surname><given-names>M</given-names></name><name><surname>Sharma</surname><given-names>J</given-names></name><name><surname>Sugihara</surname><given-names>H</given-names></name><name><surname>Sur</surname><given-names>M</given-names></name><name><surname>Sinha</surname><given-names>P</given-names></name></person-group><article-title>Imaging prior information in the brain</article-title><source>Proceedings of the National Academy of Sciences</source><year>2012</year><volume>109</volume><issue>20</issue><fpage>7935</fpage><lpage>7940</lpage><pub-id pub-id-type="pmcid">PMC3356663</pub-id><pub-id pub-id-type="pmid">22538820</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1111224109</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greve</surname><given-names>A</given-names></name><name><surname>Cooper</surname><given-names>E</given-names></name><name><surname>Tibon</surname><given-names>R</given-names></name><name><surname>Henson</surname><given-names>RN</given-names></name></person-group><article-title>Knowledge is power: Prior knowledge aids memory for both congruent and incongruent events, but in different ways</article-title><source>Journal of Experimental Psychology: General</source><year>2019</year><volume>148</volume><issue>2</issue><fpage>325</fpage><lpage>341</lpage><pub-id pub-id-type="pmcid">PMC6390882</pub-id><pub-id pub-id-type="pmid">30394766</pub-id><pub-id pub-id-type="doi">10.1037/xge0000498</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Contier</surname><given-names>O</given-names></name><name><surname>Teichmann</surname><given-names>L</given-names></name><name><surname>Rockter</surname><given-names>AH</given-names></name><name><surname>Zheng</surname><given-names>CY</given-names></name><name><surname>Kidder</surname><given-names>A</given-names></name><name><surname>Corriveau</surname><given-names>A</given-names></name><name><surname>Vaziri-Pashkam</surname><given-names>M</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><article-title>THINGS-data, a multimodal collection of large-scale datasets for investigating object representations in human brain and behavior</article-title><source>eLife</source><year>2023</year><volume>12</volume><elocation-id>e82580</elocation-id><pub-id pub-id-type="pmcid">PMC10038662</pub-id><pub-id pub-id-type="pmid">36847339</pub-id><pub-id pub-id-type="doi">10.7554/eLife.82580</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Dickter</surname><given-names>AH</given-names></name><name><surname>Kidder</surname><given-names>A</given-names></name><name><surname>Kwok</surname><given-names>WY</given-names></name><name><surname>Corriveau</surname><given-names>A</given-names></name><name><surname>Van Wicklin</surname><given-names>C</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><article-title>THINGS: A database of 1,854 object concepts and more than 26,000 naturalistic object images</article-title><source>PLOS ONE</source><year>2019</year><volume>14</volume><issue>10</issue><elocation-id>e0223792</elocation-id><pub-id pub-id-type="pmcid">PMC6793944</pub-id><pub-id pub-id-type="pmid">31613926</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0223792</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Zheng</surname><given-names>CY</given-names></name><name><surname>Pereira</surname><given-names>F</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><article-title>Revealing the multidimensional mental representations of natural objects underlying human similarity judgements</article-title><source>Nature Human Behaviour</source><year>2020</year><volume>4</volume><issue>11</issue><fpage>1173</fpage><lpage>1185</lpage><pub-id pub-id-type="pmcid">PMC7666026</pub-id><pub-id pub-id-type="pmid">33046861</pub-id><pub-id pub-id-type="doi">10.1038/s41562-020-00951-3</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henson</surname><given-names>RN</given-names></name><name><surname>Gagnepain</surname><given-names>P</given-names></name></person-group><article-title>Predictive, interactive multiple memory systems</article-title><source>Hippocampus</source><year>2010</year><volume>20</volume><issue>11</issue><fpage>1315</fpage><lpage>1326</lpage><pub-id pub-id-type="pmid">20928831</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hsieh</surname><given-names>P-J</given-names></name><name><surname>Vul</surname><given-names>E</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><article-title>Recognition Alters the Spatial Pattern of fMRI Activation in Early Retinotopic Cortex</article-title><source>Journal of Neurophysiology</source><year>2010</year><volume>103</volume><issue>3</issue><fpage>1501</fpage><lpage>1507</lpage><pub-id pub-id-type="pmcid">PMC3257064</pub-id><pub-id pub-id-type="pmid">20071627</pub-id><pub-id pub-id-type="doi">10.1152/jn.00812.2009</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Schmidt</surname><given-names>K</given-names></name><name><surname>Issa</surname><given-names>EB</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><article-title>Evidence that recurrent circuits are critical to the ventral stream’s execution of core object recognition behavior</article-title><source>Nature Neuroscience</source><year>2019</year><volume>22</volume><issue>6</issue><fpage>974</fpage><lpage>983</lpage><pub-id pub-id-type="pmcid">PMC8785116</pub-id><pub-id pub-id-type="pmid">31036945</pub-id><pub-id pub-id-type="doi">10.1038/s41593-019-0392-5</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knill</surname><given-names>DC</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><article-title>The Bayesian brain: The role of uncertainty in neural coding and computation</article-title><source>Trends in Neurosciences</source><year>2004</year><volume>27</volume><issue>12</issue><fpage>712</fpage><lpage>719</lpage><pub-id pub-id-type="pmid">15541511</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kramer</surname><given-names>MA</given-names></name><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name><name><surname>Bainbridge</surname><given-names>WA</given-names></name></person-group><article-title>The features underlying the memorability of objects</article-title><source>Science Advances</source><year>2023</year><volume>9</volume><issue>17</issue><elocation-id>eadd2981</elocation-id><pub-id pub-id-type="pmcid">PMC10132746</pub-id><pub-id pub-id-type="pmid">37126552</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.add2981</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>Rajalingham</surname><given-names>R</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Majaj</surname><given-names>N</given-names></name><name><surname>Issa</surname><given-names>E</given-names></name><name><surname>Bashivan</surname><given-names>P</given-names></name><name><surname>Prescott-Roy</surname><given-names>J</given-names></name><name><surname>Schmidt</surname><given-names>K</given-names></name><name><surname>Nayebi</surname><given-names>A</given-names></name><etal/></person-group><chapter-title>Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs</chapter-title><person-group person-group-type="editor"><name><surname>Wallach</surname><given-names>H</given-names></name><name><surname>Larochelle</surname><given-names>H</given-names></name><name><surname>Beygelzimer</surname><given-names>A</given-names></name><name><surname>d’Alché-Buc</surname><given-names>F</given-names></name><name><surname>Fox</surname><given-names>E</given-names></name><name><surname>Garnett</surname><given-names>R</given-names></name></person-group><source>Advances in Neural Information Processing Systems</source><publisher-name>Curran Associates, Inc</publisher-name><year>2019</year><volume>32</volume><comment><ext-link ext-link-type="uri" xlink:href="https://proceedings.neurips.cc/paper_files/paper/2019/file/7813d1590d28a7dd372ad54b5d29d033-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2019/file/7813d1590d28a7dd372ad54b5d29d033-Paper.pdf</ext-link></comment></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Nayebi</surname><given-names>A</given-names></name><name><surname>Bear</surname><given-names>D</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><article-title>CORnet: Modeling the Neural Mechanisms of Core Object Recognition</article-title><source>Neuroscience</source><year>2018</year><pub-id pub-id-type="doi">10.1101/408385</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ludmer</surname><given-names>R</given-names></name><name><surname>Dudai</surname><given-names>Y</given-names></name><name><surname>Rubin</surname><given-names>N</given-names></name></person-group><article-title>Uncovering Camouflage: Amygdala Activation Predicts Long-Term Memory of Induced Perceptual Insight</article-title><source>Neuron</source><year>2011</year><volume>69</volume><issue>5</issue><fpage>1002</fpage><lpage>1014</lpage><pub-id pub-id-type="pmcid">PMC3281502</pub-id><pub-id pub-id-type="pmid">21382558</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2011.02.013</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mooney</surname><given-names>CM</given-names></name></person-group><article-title>Age in the development of closure ability in children</article-title><source>Canadian Journal of Psychology / Revue Canadienne de Psychologie</source><year>1957</year><volume>11</volume><issue>4</issue><fpage>219</fpage><lpage>226</lpage><pub-id pub-id-type="pmid">13489559</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ortiz-Tudela</surname><given-names>J</given-names></name><name><surname>Turan</surname><given-names>G</given-names></name><name><surname>Vilas</surname><given-names>M</given-names></name><name><surname>Melloni</surname><given-names>L</given-names></name><name><surname>Shing</surname><given-names>YL</given-names></name></person-group><article-title>Schema-driven prediction effects on episodic memory across the lifespan</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><year>2024</year><volume>379</volume><issue>1913</issue><elocation-id>20230401</elocation-id><pub-id pub-id-type="pmcid">PMC11449153</pub-id><pub-id pub-id-type="pmid">39278241</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2023.0401</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quent</surname><given-names>JA</given-names></name><name><surname>Greve</surname><given-names>A</given-names></name><name><surname>Henson</surname><given-names>RN</given-names></name></person-group><article-title>Shape of U: The Nonmonotonic Relationship Between Object–Location Memory and Expectedness</article-title><source>Psychological Science</source><year>2022</year><volume>33</volume><issue>12</issue><fpage>2084</fpage><lpage>2097</lpage><pub-id pub-id-type="pmid">36221196</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajalingham</surname><given-names>R</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><article-title>Reversible Inactivation of Different Millimeter-Scale Regions of Primate IT Results in Different Patterns of Core Object Recognition Deficits</article-title><source>Neuron</source><year>2019</year><volume>102</volume><issue>2</issue><fpage>493</fpage><lpage>505</lpage><elocation-id>e5</elocation-id><pub-id pub-id-type="pmid">30878289</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>RPN</given-names></name><name><surname>Ballard</surname><given-names>DH</given-names></name></person-group><article-title>Predictive coding in the visual cortex: A functional interpretation of some extra-classical receptive-field effects</article-title><source>Nature Neuroscience</source><year>1999</year><volume>2</volume><issue>1</issue><fpage>79</fpage><lpage>87</lpage><pub-id pub-id-type="pmid">10195184</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolls</surname><given-names>ET</given-names></name></person-group><article-title>Pattern separation, completion, and categorisation in the hippocampus and neocortex</article-title><source>Neurobiology of Learning and Memory</source><year>2015</year><volume>2015</volume><pub-id pub-id-type="pmid">26190832</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stoinski</surname><given-names>LM</given-names></name><name><surname>Perkuhn</surname><given-names>J</given-names></name><name><surname>Hebart</surname><given-names>MN</given-names></name></person-group><article-title>THINGSplus: New norms and metadata for the THINGS database of 1854 object concepts and 26,107 natural object images</article-title><source>Behavior Research Methods</source><year>2023</year><volume>56</volume><issue>3</issue><fpage>1583</fpage><lpage>1603</lpage><pub-id pub-id-type="pmcid">PMC10991023</pub-id><pub-id pub-id-type="pmid">37095326</pub-id><pub-id pub-id-type="doi">10.3758/s13428-023-02110-8</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van de Cruys</surname><given-names>S</given-names></name><name><surname>Damiano</surname><given-names>C</given-names></name><name><surname>Boddez</surname><given-names>Y</given-names></name><name><surname>Król</surname><given-names>M</given-names></name><name><surname>Goetschalckx</surname><given-names>L</given-names></name><name><surname>Wagemans</surname><given-names>J</given-names></name></person-group><article-title>Visual affects: Linking curiosity, Aha-Erlebnis, and memory through information gain</article-title><source>Cognition</source><year>2021</year><volume>212</volume><pub-id pub-id-type="pmid">33798948</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Kesteren</surname><given-names>MTR</given-names></name><name><surname>Ruiter</surname><given-names>DJ</given-names></name><name><surname>Fernández</surname><given-names>G</given-names></name><name><surname>Henson</surname><given-names>RN</given-names></name></person-group><article-title>How schema and novelty augment memory formation</article-title><source>Trends in Neurosciences</source><year>2012</year><volume>35</volume><issue>4</issue><fpage>211</fpage><lpage>219</lpage><pub-id pub-id-type="pmid">22398180</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walsh</surname><given-names>KS</given-names></name><name><surname>McGovern</surname><given-names>DP</given-names></name><name><surname>Clark</surname><given-names>A</given-names></name><name><surname>O’Connell</surname><given-names>RG</given-names></name></person-group><article-title>Evaluating the neurophysiological evidence for predictive processing as a model of perception</article-title><source>Annals of the New York Academy of Sciences</source><year>2020</year><volume>1464</volume><issue>1</issue><fpage>242</fpage><lpage>268</lpage><pub-id pub-id-type="pmcid">PMC7187369</pub-id><pub-id pub-id-type="pmid">32147856</pub-id><pub-id pub-id-type="doi">10.1111/nyas.14321</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>R</given-names></name><name><surname>Xu</surname><given-names>P</given-names></name><name><surname>Liu</surname><given-names>T</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Guo</surname><given-names>L</given-names></name><name><surname>Li</surname><given-names>P</given-names></name><name><surname>Yao</surname><given-names>D</given-names></name></person-group><article-title>Local Temporal Correlation Common Spatial Patterns for Single Trial EEG Classification during Motor Imagery</article-title><source>Computational and Mathematical Methods in Medicine</source><year>2013</year><volume>2013</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="pmcid">PMC3853213</pub-id><pub-id pub-id-type="pmid">24348740</pub-id><pub-id pub-id-type="doi">10.1155/2013/591216</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Fig. 1</label><caption><title>Experimental design and behavioural performance.</title><p><bold>a</bold>, A total of 1,854 images obtained from the THINGS database were transformed into Mooney images. This was achieved converting these images into greyscale and applying a Gaussian blur and binarization filter (see <xref ref-type="sec" rid="S9">Methods</xref> section). <bold>b</bold>, 1,065 participants were presented with images in three conditions: pre-disambiguation (initial exposure to Mooney images), disambiguation (presenting the unambiguous greyscale image), and post-disambiguation (re-exposure to Mooney images after seeing their unambiguous version). In all conditions, participants responded whether they identified the image (yes/no) and typed a label. <bold>c</bold>, Subjective identification rates (left) and verbal accuracy (right) across all three conditions. Black squares represent medians, black lines denote the interquartile range, and green shaded areas indicate the data distribution.</p></caption><graphic xlink:href="EMS206054-f001"/></fig><fig id="F2" position="float"><label>Fig. 2</label><caption><title>Visual and verbal stimulus features and their link to subjective identification.</title><p><bold>a</bold>, Mooney and unambiguous greyscale versions of the same image were processed through CORNet-S, a DNN that mimics the hierarchical architecture of the ventral visual stream. For each layer, feature representations were extracted, and the similarity (Pearson) between Mooney and unambiguous features was calculated as the preservation index. <bold>b</bold>, The preservation index decreased progressively from early visual layers (V1) to higher-level layers (IT), indicating that higher-level representations are more impaired by the Mooney transformation. <bold>c</bold>, Differences in preservation index and subjective identification correlations across DNN layers and exposure conditions (pre- and post-disambiguation) are shown. A significant increase in correlation was observed in V1 and V2 after disambiguation (red asterisks), while a decrease occurred in IT, suggesting that lower-level features play a stronger relationship after disambiguation. d, Venn diagrams illustrate the contribution of verbal semantic and visual dimensions to variance in subjective identification pre- and post-disambiguation.</p></caption><graphic xlink:href="EMS206054-f002"/></fig><fig id="F3" position="float"><label>Fig. 3</label><caption><title>Semantic distance, entropy and their relation with subjective identification.</title><p><bold>a</bold>, Illustration of semantic distance and entropy. Targets (green dots) and responses (black dots) are shown in semantic space. High semantic distance indicates labels far from the target (e.g., “lettuce” for “drain”), while low semantic distance reflects closer labels (e.g., “raspberry” for “blackberry”). High entropy occurs when participants provide a broad range of labels, whereas low entropy reflects consistent responses. <bold>b</bold>, Effect of disambiguation on semantic measures. Top: Violin plots show reductions in semantic distance (left) and entropy (right) across exposure conditions (pre-, disambiguation [grey], and post-disambiguation). Black square represents medians, black lines denote interquartile range and green shaded areas indicate the data distribution. Bottom: Semantic distance across subjective identification patterns (always identified, never identified, only identified in disambiguation [grey], and only identified post) demonstrates the greatest effect of disambiguation on the images following the classic Mooney effect (“identified in post and grey” group). Markers indicate median semantic distance for each condition and error bars denote 95% confidence interval <bold>c</bold>, Hexbin plots display semantic distance (left) and entropy (right) from pre- to post-disambiguation. Red lines represent k-means clusters (k = 3), of which Cluster 2 shows stimuli of interest where values transition from high to low, reflecting significant information gain. <bold>d</bold>, Relationship between information gain and subjective recognition. Scatterplots show subjective identification post-disambiguation as a function of information gain in semantic distance (left) and entropy (right). Regression lines highlight a U-shaped relationship for semantic distance, suggesting that large initial gains reverse the trend at higher levels of semantic distance. For entropy, a similar but weaker quadratic pattern emerges. For illustrative purposes, data where subjective identifications were equal to 1 was excluded. The full set of images was used for the analyses reported in the main text.</p></caption><graphic xlink:href="EMS206054-f003"/></fig></floats-group></article>