<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS207125</article-id><article-id pub-id-type="doi">10.1101/2025.07.07.662360</article-id><article-id pub-id-type="archive">PPR1047186</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Brain-Informed Fine-Tuning for Improved Multilingual Understanding in Language Models</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Negi</surname><given-names>Anuja</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Oota</surname><given-names>Subba Reddy</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Gupta</surname><given-names>Manish</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Deniz</surname><given-names>Fatma</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib></contrib-group><aff id="A1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03v4gjf40</institution-id><institution>Technical University of Berlin</institution></institution-wrap>, <country country="DE">Germany</country></aff><aff id="A2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05ewdps05</institution-id><institution>Bernstein Center for Computational Neuroscience Berlin</institution></institution-wrap>, <country country="DE">Germany</country></aff><aff id="A3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04ww0w091</institution-id><institution>Microsoft</institution></institution-wrap></aff><pub-date pub-type="nihms-submitted"><day>14</day><month>07</month><year>2025</year></pub-date><pub-date pub-type="preprint"><day>10</day><month>07</month><year>2025</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Recent studies have demonstrated that fine-tuning language models with brain data can improve their semantic understanding, although these findings have so far been limited to English. Interestingly, similar to the shared multilingual embedding space of pretrained multilingual language models, human studies provide strong evidence for a shared semantic system in bilingual individuals. Here, we investigate whether fine-tuning language models with bilingual brain data changes model representations in a way that improves them across multiple languages. To test this, we fine-tune monolingual and multilingual language models using brain activity recorded while bilingual participants read stories in English and Chinese. We then evaluate how well these representations generalize to the bilingual participants’ first language, their second language, and several other languages that the participant is not fluent in. We assess the fine-tuned language models on brain encoding performance and downstream NLP tasks. Our results show that bilingual brain-informed fine-tuned language models outperform their vanilla (pretrained) counterparts in both brain encoding performance and most downstream NLP tasks across multiple languages. These findings suggest that brain-informed fine-tuning improves multilingual understanding in language models, offering a bridge between cognitive neuroscience and NLP research.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><label>1</label><title>Introduction</title><p id="P2">Recent research has demonstrated that representations extracted from text-based Transformer language models can be used to accurately predict human brain activity evoked during language processing, suggesting parallels between artificial and brain language representations (<xref ref-type="bibr" rid="R41">Wehbe et al., 2014b</xref>; <xref ref-type="bibr" rid="R18">Jain &amp; Huth, 2018</xref>; <xref ref-type="bibr" rid="R35">Toneva &amp; Wehbe, 2019</xref>; <xref ref-type="bibr" rid="R31">Schrimpf et al., 2021</xref>; <xref ref-type="bibr" rid="R5">Caucheteux &amp; King, 2022</xref>; <xref ref-type="bibr" rid="R14">Goldstein et al., 2022</xref>; <xref ref-type="bibr" rid="R19">Karamolegkou et al., 2023</xref>). Although these models accurately predict patterns of brain activity, they have not been originally pretrained to do so. Several previous studies hypothesized that fine-tuning language models with brain data can lead to representations that are better aligned with brain activity (<xref ref-type="bibr" rid="R33">Schwartz et al., 2019</xref>; <xref ref-type="bibr" rid="R24">Moussa et al., 2025</xref>; <xref ref-type="bibr" rid="R38">Vattikonda et al., 2025</xref>). However, these efforts have largely focused on monolingual language models and monolingual brain data, usually trained and evaluated only in English. This overlooks the widespread prevalence of bilingualism and multilingualism in the human population (<xref ref-type="bibr" rid="R15">Grosjean, 2024</xref>). The limitation is particularly significant given recent neuroscientific evidence revealing that bilingual individuals have shared semantic representations across languages (<xref ref-type="bibr" rid="R7">Chen et al., 2024b</xref>; <xref ref-type="bibr" rid="R13">Francis, 2005</xref>), hinting at a shared component for the processing of different languages in the human brain (<xref ref-type="bibr" rid="R9">de Varda et al., 2025</xref>). Complementary NLP research has shown that multilingual language models operate in a shared, language-agnostic conceptual space (<xref ref-type="bibr" rid="R42">Wendler et al., 2024</xref>; <xref ref-type="bibr" rid="R32">Schut et al., 2025</xref>). This raises the question of <italic>whether fine-tuning language models with bilingual brain data can elicit multilingual capabilities in language models</italic>.</p><p id="P3">In this study, we use brain recordings of bilingual participants reading the same naturalistic stories in English and Chinese and investigate whether fine-tuning language models with brain data from bilingual participants can elicit multilingual capabilities in language models. For this, we first introduce a novel, full brain-informed fine-tuning pipeline (as shown in <xref ref-type="fig" rid="F1">Fig. 1</xref>), which can be trained using data from the whole-brain, or from language- or semantically-selective brain regions. Second, we use brain recordings of bilingual participants to fine-tune two monolingual language models (BERT for English and Chinese (<xref ref-type="bibr" rid="R11">Devlin et al., 2019</xref>)) and four multilingual language models (mBERT (<xref ref-type="bibr" rid="R11">Devlin et al., 2019</xref>), XLM-R (<xref ref-type="bibr" rid="R8">Conneau et al., 2020</xref>), XGLM (<xref ref-type="bibr" rid="R22">Lin et al., 2022</xref>), LLaMA-3.2 (<xref ref-type="bibr" rid="R37">Touvron et al., 2023</xref>)). Third, we investigate how monolingual and multilingual language models change when fine-tuned with bilingual brain data. We evaluate this along two axes: brain encoding and downstream NLP tasks performance. To assess generalization across languages (zero-shot cross-lingual transfer), we evaluate the brain-informed fine-tuned language models on downstream NLP benchmarks not only in the models’ fine-tuned language but also in the bilingual participants’ other language (cross-language transfer between known languages) and several other languages that the participant is not fluent in (and also not used in fine-tuning; referred to as ‘unseen languages’), e.g., German and French. Fine-tuning is done separately for each participant, and we assess encoding performance both within the same participant and across the other participants.</p><p id="P4">Brain-informed fine-tuning with bilingual brain data reveals several key conclusions: (1) Brain encoding performance improves after fine-tuning, even when evaluated on participants not used in training. This suggests that the brain bias introduced is not specific to a participant but reflects some shared bilingual representations. (2) Both monolingual and multilingual language models show improved performance on downstream NLP tasks in the fine-tuned language, the participant’s other language, and in several other unseen languages, indicating that brain-informed fine-tuning elicits generalizable semantic structure not tied to any one language. (3) To ascertain whether our results are specific to fine-tuning with bilingual brain data, and not a general outcome of fine-tuning language models with brain data, we performed the same analyses using brain data of a monolingual individual. Results suggest that the observed effects are indeed driven by bilingual brain representations. These findings suggest that bilingual brain-informed fine-tuning improves multilingual understanding in text-based language models. Our results contribute to the alignment between brain and artificial multilingual language representations, offering insights into the development of brain-inspired multilingual NLP systems.</p><p id="P5">We make the following contributions: (1) To the best of our knowledge, this is the first study to perform brain-informed fine-tuning using bilingual brain data, applying it to both monolingual and multilingual language models. (2) We introduce a novel brain-informed fine-tuning pipeline in which downsampling and temporal-delay modeling are integrated within the pipeline itself. This contrasts with previous brain-based fine-tuning studies, where these were modeled as preprocessing steps before fine-tuning the model. (3) We evaluate the performance of brain-informed fine-tuned monolingual and multilingual language models on downstream NLP tasks in both English and Chinese. We will release the code upon publication of this paper.</p></sec><sec id="S2"><label>2</label><title>Related Work</title><sec id="S3"><title>Fine-tuning of language models with naturalistic brain data</title><p id="P6">Our work builds on the brain-tuning approach introduced by <xref ref-type="bibr" rid="R33">Schwartz et al. (2019)</xref>; <xref ref-type="bibr" rid="R24">Moussa et al. (2025)</xref>; <xref ref-type="bibr" rid="R38">Vattikonda et al. (2025)</xref>, which fine-tunes pretrained Transformer-based language models using brain data to integrate brain-relevant information. <xref ref-type="bibr" rid="R33">Schwartz et al. (2019)</xref> demonstrated improved brain encoding and NLP task performance using brain data from monolingual English readers, while <xref ref-type="bibr" rid="R24">Moussa et al. (2025)</xref>; <xref ref-type="bibr" rid="R38">Vattikonda et al. (2025)</xref> extended this to speech-based models to enhance semantic representations. Our study complements these by exploring bilingual brain-informed fine-tuning and analyzing how monolingual and multilingual models change when trained with bilingual brain data.</p></sec><sec id="S4"><title>Multilingual language models and brain alignment</title><p id="P7">Our work aligns with a growing body of research examining the alignment between human brain activity and language models. Several studies have shown that text-based models can predict brain responses to written and spoken stimuli with high accuracy (<xref ref-type="bibr" rid="R40">Wehbe et al., 2014a</xref>; <xref ref-type="bibr" rid="R18">Jain &amp; Huth, 2018</xref>; <xref ref-type="bibr" rid="R35">Toneva &amp; Wehbe, 2019</xref>; <xref ref-type="bibr" rid="R10">Deniz et al., 2019</xref>; <xref ref-type="bibr" rid="R1">Abdou et al., 2021</xref>; <xref ref-type="bibr" rid="R36">Toneva et al., 2022</xref>; <xref ref-type="bibr" rid="R2">Antonello et al., 2021</xref>; <xref ref-type="bibr" rid="R27">Oota et al., 2022</xref>; <xref ref-type="bibr" rid="R3">Aw &amp; Toneva, 2023</xref>; <xref ref-type="bibr" rid="R30">Oota et al., 2024b</xref>; <xref ref-type="bibr" rid="R20">Lamarre et al., 2022</xref>; <xref ref-type="bibr" rid="R6">Chen et al., 2024a</xref>; <xref ref-type="bibr" rid="R29">Oota et al., 2024a</xref>). Recent efforts have extended this to multilingual Transformer-based models using brain data from tasks in multiple languages (<xref ref-type="bibr" rid="R9">de Varda et al., 2025</xref>), though most studies remain monolingual in design, with the exception of <xref ref-type="bibr" rid="R6">Chen et al. (2024a)</xref>, who explored bilingual processing using similar English and Chinese stimuli. Our study supplements this by investigating bilingual brain alignment through brain-informed fine-tuning and its impact on downstream NLP tasks in both languages. Detailed related work is in <xref ref-type="supplementary-material" rid="SD1">Appendix A</xref>.</p></sec></sec><sec id="S5" sec-type="methods"><label>3</label><title>Methodology</title><sec id="S6"><label>3.1</label><title>Naturalistic Brain Imaging Dataset</title><sec id="S7"><title>Bilingual fMRI dataset</title><p id="P8">Blood oxygen level-dependent (BOLD) responses were collected using fMRI from six bilingual participants (three males, three females), all native speakers of Mandarin Chinese and fluent in English as a second language. Participants read 11 narrative stories from The Moth Radio Hour (<xref ref-type="bibr" rid="R17">Huth et al., 2016</xref>), presented word-by-word in separate scanning sessions for English (en) and Chinese (zh). Each story is 10-15 minutes long. Of the 11 stories, seven were used for fine-tuning the language models (covering 2756 TRs), three were used for training voxelwise encoding models (1117 TRs), and one story was reserved for testing (291 TRs). The same set of stories are used in each language to ensure comparability across languages.</p></sec><sec id="S8"><title>Monolingual fMRI dataset</title><p id="P9">To test whether the effects of brain-informed fine-tuning are specific to bilingual brains, we also use fMRI data from one English-monolingual participant (male). The participant was presented with the same English narrative stories under the same experimental design and scanning parameters. This allows direct comparison between models fine-tuned with bilingual and monolingual brain data. If similar improvements are seen with monolingual brain fine-tuning, it suggests that the effects are driven by general brain representations rather than shared semantic representations in bilinguals. More details about datasets and preprocessing are in <xref ref-type="supplementary-material" rid="SD1">Appendix B</xref>.</p></sec></sec><sec id="S9"><label>3.2</label><title>Text-based Language Models</title><p id="P10">We fine-tuned monolingual and multilingual Transformer-based text-based models to investigate the effects of brain-informed fine-tuning. All pretrained model checkpoints were obtained from HuggingFace (<xref ref-type="bibr" rid="R43">Wolf et al., 2020</xref>).</p><sec id="S10"><title>Monolingual pretrained language models</title><p id="P11">We fine-tuned two monolingual models: English BERT (BERT-en) and Chinese BERT (BERT-zh) (<xref ref-type="bibr" rid="R11">Devlin et al., 2019</xref>). Both models share the same architecture, comprising 12 Transformer layers with a hidden dimension size of 768, differing only in the language of their pretraining corpora.</p></sec><sec id="S11"><title>Multilingual pretrained language models</title><p id="P12">We fine-tuned four multilingual Transformer-based models: mBERT (<xref ref-type="bibr" rid="R11">Devlin et al., 2019</xref>), XLM-R (<xref ref-type="bibr" rid="R8">Conneau et al., 2020</xref>), XGLM (<xref ref-type="bibr" rid="R22">Lin et al., 2022</xref>), and LLaMA-3.2 (<xref ref-type="bibr" rid="R37">Touvron et al., 2023</xref>). These models represent three distinct architecture types: encoder-based (mBERT, XLM-R), cross-lingual pretrained (XGLM), and decoder-based (LLaMA-3.2). Representations were extracted from the base versions of mBERT, XLM-R, and XGLM, and from the 1B parameter version of LLaMA-3.2. mBERT, XLM-R, and XGLM consist of 12 layers with a hidden dimension size of 768, while LLaMA-3.2 has 16 layers with a hidden dimension size of 2048. Results and analyses using XLM-R, XGLM and LLaMA-3.2 are reported in the Supplementary.</p></sec></sec><sec id="S12"><label>3.3</label><title>Brain-informed Fine-Tuning Approach</title><p id="P13">We performed supervised full fine-tuning of pretrained language models using fMRI BOLD responses as targets. Models were fine-tuned separately for each participant and language. An overview of the fine-tuning pipeline is shown in <xref ref-type="fig" rid="F1">Fig. 1</xref>. We fine-tune language models using fMRI responses from the whole-brain, language-selective (from (<xref ref-type="bibr" rid="R12">Fedorenko et al., 2010</xref>)), or semantically-selective brain regions. Details on fine-tuning with language-selective and semantically-selective brain regions are described in <xref ref-type="supplementary-material" rid="SD1">Appendix C</xref>.</p><sec id="S13"><title>Model architecture</title><p id="P14">Text transcripts of the narrative stimuli were provided as inputs to the pretrained language model, using a sequence length of 20 tokens. We then extract the representations of the last token from the last hidden layer for each word. This representation is given as input to a dropout layer. This is followed by differentiable 3-lobe Lanczos (<xref ref-type="bibr" rid="R17">Huth et al., 2016</xref>) interpolation to downsample to the fMRI sampling rate. To model the delayed hemodynamic response, we apply finite impulse response (FIR) filtering by concatenating representations delayed by 2, 4, 6, and 8 seconds. The resulting representations are then passed through a linear projection layer to predict voxelwise BOLD responses. Previous brain-based fine-tuning studies (<xref ref-type="bibr" rid="R24">Moussa et al., 2025</xref>; <xref ref-type="bibr" rid="R38">Vattikonda et al., 2025</xref>) typically treated downsampling and temporal-delay modeling as separate preprocessing steps before model fine-tuning. In contrast, our method integrates these components directly into the model architecture, enabling end-to-end optimization.</p></sec><sec id="S14"><title>Training protocol</title><p id="P15">We fine-tuned the models using the AdamW optimizer (<xref ref-type="bibr" rid="R23">Loshchilov &amp; Hutter, 2017</xref>) with a learning rate of 1e-4 and weight decay of 1e-3 for 30 epochs with a batch size of 32. A ReduceLROnPlateau scheduler was used to adjust the learning rate based on validation loss. We used mixed-precision training for computational efficiency and applied early stopping with a patience of 5 epochs based on validation performance. Implementation details are in <xref ref-type="supplementary-material" rid="SD1">Appendix G</xref>.</p></sec><sec id="S15"><title>Training objective</title><p id="P16">Our training objective minimized the NT-Xent (Normalized Temperature-Scaled Cross-entropy) loss (<xref ref-type="bibr" rid="R34">Sohn, 2016</xref>) between the predicted and actual BOLD responses. We also experimented with alternative loss functions such as MSE, ridge loss, spatial loss, and a combination of all (referred to as hybrid), and found NT-Xent to perform best across models and experiments. Results from other losses are reported in Supplementary. We fine-tuned all layers of the language models and propagated the loss backward to update both the projection and transformer layers.</p><p id="P17">In addition to comparing our brain-informed fine-tuned models to their pretrained counterparts in both monolingual and multilingual settings, we include baselines for comprehensive comparison: TR-shuffled fMRI, multilingual model representations, and monolingual brain data as fine-tuning targets. This experiment was done to clarify the impact of bilingual brain data on the resulting fine-tuned model representations. Details are reported in <xref ref-type="supplementary-material" rid="SD1">Appendix C.3</xref>.</p></sec></sec><sec id="S16"><label>3.4</label><title>Voxelwise Encoding Modeling</title><p id="P18">To evaluate whether brain-informed fine-tuning alters the alignment between language model representations and brain activity, we assess voxelwise encoding performance before and after fine-tuning. Voxelwise encoding models (VEM) estimate, for each voxel, a linear mapping from an embedding to the observed fMRI responses (<xref ref-type="bibr" rid="R17">Huth et al., 2016</xref>; <xref ref-type="bibr" rid="R10">Deniz et al., 2019</xref>). If fine-tuning with bilingual brain data improves encoding performance, we interpret this as evidence that the language model’s internal representations have become more brain-like, indicating successful alignment. If encoding performance remains unchanged, this suggests that fine-tuning preserves the existing brain alignment of the representations without introducing degradation. A decrease in performance would imply that fine-tuning introduces distortions that reduce the language model’s ability to predict brain responses, thus reducing its alignment with the brain.</p><p id="P19">We used representations extracted from layer 7 (found to yield the highest encoding performance based on validation data) as the embedding. Embeddings and brain responses were z-scored across time separately for each of the three stories not used for fine-tuning. Embeddings were downsampled to match the fMRI acquisition rate using 3-lobe Lanczos interpolation. Next, to account for the delayed hemodynamic response, each embedding was passed through a finite impulse response (FIR) filter with four delays. Specifically, delayed copies of each feature at 1, 2, 3, and 4 TRs (2, 4, 6, and 8 seconds) were concatenated. Banded ridge regression (<xref ref-type="bibr" rid="R26">Nunez-Elizalde et al., 2019</xref>) was used to determine how each embedding is represented in each voxel (<xref ref-type="bibr" rid="R44">Wu et al., 2006</xref>; <xref ref-type="bibr" rid="R25">Naselaris et al., 2011</xref>). We performed 5-fold cross-validation to find optimal regularization parameters. Encoding performance was quantified by calculating the Pearson correlation coefficient (<italic>r</italic>) between predicted and recorded BOLD responses. The held-out story was used to estimate the encoding performance. A separate VEM was fit for each voxel, participant, and language model variant (before and after brain-informed fine-tuning). No sample size calculations were performed, as each subject serves as a full replication of the results.</p><sec id="S17"><title>Cross-participant transfer of encoding performance</title><p id="P20">We evaluated whether brain-informed fine-tuned model representations generalize across participants. We fine-tuned the language model using brain data from a one bilingual participant and evaluated its encoding performance using VEMs on others. Improved transfer would suggest that changes introduced by brain-informed fine-tuning are not participant-specific but reflect shared representations across bilingual individuals.</p></sec></sec><sec id="S18"><label>3.5</label><title>Downstream NLP tasks</title><p id="P21">To evaluate how brain-informed fine-tuning influences language model behavior, we assessed its performance on standard NLP benchmarks before and after fine-tuning. For English, we used 9 tasks from GLUE benchmark (<xref ref-type="bibr" rid="R39">Wang et al., 2018</xref>), and for Chinese, we used 7 tasks from CLUE benchmark (<xref ref-type="bibr" rid="R45">Xu et al., 2020</xref>). Both monolingual and multilingual models were evaluated on these benchmarks. To investigate cross-linguistic generalization to unseen languages, we additionally evaluated multilingual models on 3 tasks from XGLUE benchmark (<xref ref-type="bibr" rid="R21">Liang et al., 2020</xref>) and 3 tasks from XTREME benchmark (<xref ref-type="bibr" rid="R16">Hu et al., 2020</xref>). Details about these benchmarks are in <xref ref-type="supplementary-material" rid="SD1">Appendix D</xref>.</p><sec id="S19"><title>Fine-tuning and evaluation in the same language</title><p id="P22">To assess how brain-informed fine-tuning of language models affects performance on NLP tasks within the same language, we fine-tuned language models using fMRI data in English or Chinese. We then evaluated these language models on NLP benchmarks in the corresponding language. English and Chinese brain data are used to fine-tune monolingual (BERT-en, BERT-zh) and multilingual (mBERT) language models, which are then evaluated on GLUE and CLUE, respectively. Any performance increase observed in this setting would imply that brain-informed fine-tuning not only changes language model representations to better align with the brain, but that these modified representations are better for NLP tasks within the trained language.</p></sec><sec id="S20"><title>Cross-language transfer between known languages</title><p id="P23">To test whether brain-informed fine-tuning transfers to a bilingual participant’s other language, we evaluate models on the participant’s other language after fine-tuning on the first. For example, weights from BERT-en fine-tuned with English brain data are transferred to BERT-zh and evaluated on Chinese tasks, and vice versa. Improved performance in this setting would suggest that brain-informed fine-tuning introduces the bilingual brain’s shared semantic representations into the model, enabling cross-language transfer.</p></sec><sec id="S21"><title>Zero-shot transfer to unseen languages</title><p id="P24">To evaluate the broader generalizability of brain-informed fine-tuned models, we tested multilingual models on five additional languages (German, French, Spanish, Japanese, and Korean) that were neither used for fine-tuning nor known to the participant. Specifically, we fine-tuned mBERT on English brain data and evaluated its performance on the XGLUE (<xref ref-type="bibr" rid="R21">Liang et al., 2020</xref>) and XTREME (<xref ref-type="bibr" rid="R16">Hu et al., 2020</xref>) benchmarks. This setup allows us to assess whether brain-informed fine-tuning can introduce language-agnostic representations that support broader cross-linguistic transfer.</p></sec></sec></sec><sec id="S22" sec-type="results"><label>4</label><title>Results</title><sec id="S23"><label>4.1</label><title>Effects on Brain Encoding Performance and Generalization</title><p id="P25">We tested whether brain-informed fine-tuning improves brain encoding performance and if these possible improvements are generalizable across participants. We compared the VEM performance of the vanilla language model against the three fine-tuned variants, trained using whole-brain or language-selective, or subject-specific semantically-selective voxels. <xref ref-type="fig" rid="F2">Fig. 2</xref> shows flattened cortical surfaces for participant 1, visualizing which language model variant achieved the best encoding performance across well-predicted voxels (Pearson correlation <italic>r</italic>&gt;0.1) for (a) monolingual and (b) multilingual models within the same language. Across both languages and model types, we found that fine-tuned language models outperform their vanilla counterparts, with the best-performing variant explaining more variance in a majority of voxels (76-84% of well-predicted voxels). This was consistent across all six participants (see Fig. 5 in <xref ref-type="supplementary-material" rid="SD1">Appendix E.2</xref> for VEM results on other participants). Our results suggest that a language model’s ability to predict brain activity benefits from brain-informed fine-tuning. Further, as shown in <xref ref-type="supplementary-material" rid="SD1">Appendix E.1</xref>, brain-informed fine-tuning outperforms control baselines (like TR-shuffled fMRI responses and mBERT representations as fine-tuning targets).</p><p id="P26">We do not observe any regional preference for any particular fine-tuning approach. No specific brain region consistently preferred one fine-tuned variant over the others across participants. Although fine-tuning improved encoding performance, the overall increase was modest (maximum Δ<italic>r ≈</italic> 0.15), which aligns with prior studies. For example, previous work has shown that current representations from language models already serve as one of the best predictors of brain responses (<xref ref-type="bibr" rid="R31">Schrimpf et al., 2021</xref>), and even in speech models that lack brain-relevant semantics (<xref ref-type="bibr" rid="R28">Oota et al., 2023</xref>), fine-tuning yielded small improvements (Δ<italic>r ≈</italic> 0.1) (<xref ref-type="bibr" rid="R38">Vattikonda et al., 2025</xref>).</p><p id="P27">To test whether these improvements were participant-specific, we checked for cross-participant generalization. We fine-tuned a BERT-en model on one participant’s English brain data and evaluated its VEM performance on the other participants. As shown in Fig. 6 in <xref ref-type="supplementary-material" rid="SD1">Appendix E.3</xref>, we observe small but consistent improvements (Δ<italic>r ≈</italic> 0.03 <italic>−</italic> 0.05) in high-level semantic areas. These findings suggest that the improvements from brain-informed fine-tuning of language model representations are not specific to individual participants but reflect shared representations across bilingual individuals.</p></sec><sec id="S24"><label>4.2</label><title>Performance on Downstream NLP tasks</title><p id="P28">We tested the effect of brain-informed fine-tuning on language models by evaluating model performance on standard NLP benchmarks. We assess this for three settings:</p><sec id="S25"><title>Fine-tuning and evaluation in the same language</title><p id="P29">We show in <xref ref-type="table" rid="T1">Table 1(a)</xref> performance on downstream NLP tasks when brain-informed fine-tuning and evaluation are computed in the same language. For English, models were fine-tuned using English brain data and evaluated on the GLUE benchmark. We observe that the monolingual model (BERT-ft-en) outperforms its vanilla counterpart (BERT-en) on 6/9 tasks, with an average improvement of +0.80 percentage points, and a maximum gain of +7.04 on the WNLI task. The multilingual model (mBERT-ft-en) improves on 7/9 tasks, with an average gain of +0.18 percentage points, and max gain of +2.82 also on WNLI. For Chinese, models were fine-tuned using Chinese brain data and evaluated on the CLUE benchmark. Here, BERT-ft-zh achieves slight improvements on 5/7 tasks relative to BERT-zh, with an average gain of +0.09 percentage point and max gain of +0.55 on the CSL task. The multilingual model (mBERT-ft-zh) also improves on 5/7 tasks, achieving an average improvement of +0.29 percentage point, with the largest improvement of +0.81 observed on the AFQMC task. These results indicate that brain-informed fine-tuning consistently improves downstream task performance for both monolingual and multilingual models when the fine-tuning and evaluation languages match.</p></sec><sec id="S26"><title>Cross-language transfer between known languages</title><p id="P30"><xref ref-type="table" rid="T1">Table 1(b)</xref> reports performance when brain-informed fine-tuning is done in one language and evaluation is performed in the participant’s other language. On the GLUE benchmark, we evaluated BERT-zh and mBERT models fine-tuned using Chinese brain data. Both models outperform their vanilla counterparts on all 9 tasks: BERT-ft-zh achieves an average improvement of +0.81 percentage points (max gain: +2.82 on WNLI), and mBERT-ft-zh shows an average improvement of +0.68 points (max gain: +1.41 on WNLI). On the CLUE benchmark, we evaluated BERT-en and mBERT models fine-tuned using English brain data. BERT-ft-en shows improvement on 5/7 tasks, with a slight decline in 2 tasks (-1.29 points on average). mBERT-ft-en improves across all 7 tasks, with an average gain of +0.27 points and a maximum improvement of +0.88 on the AFQMC task.</p><p id="P31">Overall, bilingual brain-informed fine-tuning enables cross-lingual generalization in language models, improving downstream task performance even when the fine-tuning and evaluation languages differ. This suggests that bilingual brain-informed fine-tuning of language models introduces changes that reflect the bilingual brain’s shared semantic representations, enabling cross-language transfer.</p></sec><sec id="S27"><title>Zero-shot transfer to unseen languages</title><p id="P32"><xref ref-type="table" rid="T1">Table 1(c)</xref> reports performance on downstream NLP tasks when brain-informed fine-tuning is performed with English brain data and evaluated on five unseen languages (German, French, Spanish, Japanese, and Korean), that were neither used for fine-tuning nor known to the participant. We evaluate the multilingual model (mBERT-ft-en) using tasks from the XGLUE and XTREME benchmarks. We observe that mBERT-ft-en improves performance on 2/3 tasks in German, French, Spanish, and Japanese, with average gains of +0.26, +1.61, +1.00, and +0.46 percentage points, respectively. In Korean, performance slightly declines with an average drop of -0.34 points. These results demonstrate that bilingual brain-informed fine-tuning improves language-agnostic representations in multilingual models, enabling generalization beyond the language of the brain data to entirely unseen languages (zero-shot).</p></sec><sec id="S28"><title>Performance on downstream tasks with language- or semantically-selective voxels</title><p id="P33">We evaluated downstream task performance before and after brain-informed fine-tuning using only language- or semantically-selective voxels. Table 5 in <xref ref-type="supplementary-material" rid="SD1">Appendix F</xref> compares results for mBERT-en (fine-tuned with English brain data) and mBERT-zh (fine-tuned with Chinese brain data) when using whole-brain, language-selective, or semantic-selective regions for fine-tuning. Fine-tuning with either variant improves performance across all downstream tasks compared to vanilla models. Detailed analysis is provided in the Appendix. Results and analyses for other multilingual language models (XLM-R, XGLM, and LLaMA) are reported in the Supplementary.</p></sec></sec><sec id="S29"><label>4.3</label><title>Monolingual Brain data as Fine-Tuning Targets</title><p id="P34"><xref ref-type="table" rid="T2">Table 2</xref> compares downstream task performance after brain-informed fine-tuning with English brain data from either bilingual or monolingual participant. On GLUE benchmark (within-language evaluation), bilingual tuning outperforms monolingual tuning on 5/7 tasks, especially in all inference-related tasks (MNLI, QNLI, WNLI, and RTE). On CLUE benchmark (cross-language evaluation), bilingual brain-informed fine-tuning leads to higher performance on 5/7 tasks, while monolingual tuning leads on 2. These results suggest that while monolingual brain-informed fine-tuning also benefits downstream tasks within-language, fine-tuning with bilingual brain data enhances cross-linguistic generalization.</p></sec></sec><sec id="S30" sec-type="discussion | Conclusions"><label>5</label><title>Discussion and Conclusion</title><p id="P35">In this work, we perform brain-informed fine-tuning of monolingual and multilingual language models using brain data from bilingual participants reading the same naturalistic stories in English and Chinese. We show that bilingual brain-informed fine-tuned language models outperform their vanilla (pretrained) counterparts in both brain encoding performance and most downstream tasks. Our analysis of brain-informed fine-tuning reveals several key conclusions.</p><p id="P36">First, representations from brain-informed fine-tuned language models are shared across bilingual individuals. Both monolingual and multilingual language models show improved encoding performance after brain-informed fine-tuning, in both English and Chinese (<xref ref-type="fig" rid="F2">Fig.2</xref>). These models outperform baseline controls (<xref ref-type="supplementary-material" rid="SD1">Appendix Fig.4</xref>) and generalize across participants (<xref ref-type="supplementary-material" rid="SD1">Appendix Fig.6</xref>). This highlights the potential of bilingual brain data as a rich supervisory signal for grounding language model representations with biologically meaningful representations. Second, brain-informed fine-tuning enables cross-language transfer in language models. Downstream performance improves across within-language, cross-language, and unseen language settings (<xref ref-type="table" rid="T1">Table 1</xref>). This holds for fine-tuning with whole-brain or language-selective or semantically-selective regions (<xref ref-type="supplementary-material" rid="SD1">Appendix Table 5</xref>). This effect is likely driven by shared semantic representations in bilingual individuals (<xref ref-type="bibr" rid="R7">Chen et al., 2024b</xref>). This highlights the potential of leveraging bilingual brain representations for developing language-agnostic language models. Lastly, the observed improvements are driven specifically by brain-informed fine-tuning with bilingual brain data, not brain data in general. Fine-tuning with monolingual brain data does not yield similar cross-linguistic transfer (<xref ref-type="table" rid="T2">Table 2</xref>).</p><p id="P37">Our results contribute to a growing body of work exploring the representational alignment between brain and artificial language models. We demonstrate that bilingual brain-informed fine-tuning improves multilingual understanding in language models. To the best of our knowledge, this is the first work to do so. This study offers a bridge between neuroscience research on cross-linguistic semantic representation and NLP efforts to interpret and improve multilingual language models.</p></sec><sec id="S31"><label>6</label><title>Limitations and Future Work</title><p id="P38">A key limitation of this study is the relatively small number of data samples per subject, due to limited bilingual brain recordings with naturalistic stimuli. This constrains the observed performance gains. Additionally, our analysis is restricted to only two languages (English and Chinese). Expanding to larger and more diverse multilingual brain datasets would allow a broader evaluation of cross-linguistic transfer. Future work could also examine what linguistic properties are captures by brain-informed fine-tuning (such as syntax, morphology, or discourse-level structure). This can potentially be tested by targeting functionally selective brain regions and analyzing fine-tuned model representations across layers.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Material</label><media xlink:href="EMS207125-supplement-Supplementary_Material.pdf" mimetype="application" mime-subtype="pdf" id="d47aAcGbB" position="anchor"/></supplementary-material><supplementary-material content-type="local-data" id="SD2"><label>Appendix</label><media xlink:href="EMS207125-supplement-Appendix.pdf" mimetype="application" mime-subtype="pdf" id="d47aAcGcB" position="anchor"/></supplementary-material></sec></body><back><ack id="S32"><title>Acknowledgment</title><p>We thank Lily Gong for preprocessing the fMRI data, and Mathis Lamarre for valuable discussions. This work was funded by grants from the German Federal Ministry of Education and Research (BMBF; Grant no. 01GQ1906) and the European Research Council (ERC; Grant no. 101042567).</p></ack><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abdou</surname><given-names>Mostafa</given-names></name><name><surname>Valeria</surname><given-names>Ana</given-names></name><name><surname>Toneva</surname><given-names>Mariya</given-names></name><name><surname>Hershcovich</surname><given-names>Daniel</given-names></name><name><surname>Søgaard</surname><given-names>Anders</given-names></name></person-group><article-title>Does injecting linguistic structure into language models lead to better alignment with brain recordings?</article-title><source>arXiv preprint preprint</source><year>2021</year><elocation-id>arXiv:2101.12608</elocation-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Antonello</surname><given-names>Richard</given-names></name><name><surname>Turek</surname><given-names>Javier S</given-names></name><name><surname>Vo</surname><given-names>Vy</given-names></name><name><surname>Huth</surname><given-names>Alexander</given-names></name></person-group><article-title>Low-dimensional structure in the space of language representations is reflected in brain responses</article-title><source>Advances in Neural Information Processing Systems</source><year>2021</year><volume>34</volume><fpage>8332</fpage><lpage>8344</lpage></element-citation></ref><ref id="R3"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Aw</surname><given-names>Khai Loong</given-names></name><name><surname>Toneva</surname><given-names>Mariya</given-names></name></person-group><source>Training language models to summarize narratives improves brain alignment</source><conf-name>The Eleventh International Conference on Learning Representations</conf-name><year>2023</year></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bojanowski</surname><given-names>Piotr</given-names></name><name><surname>Grave</surname><given-names>Edouard</given-names></name><name><surname>Joulin</surname><given-names>Armand</given-names></name><name><surname>Mikolov</surname><given-names>Tomas</given-names></name></person-group><article-title>Enriching word vectors with subword information</article-title><source>arXiv preprint</source><year>2016</year><elocation-id>arXiv:1607.04606</elocation-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caucheteux</surname><given-names>Charlotte</given-names></name><name><surname>King</surname><given-names>Jean-Rémi</given-names></name></person-group><article-title>Brains and algorithms partially converge in natural language processing</article-title><source>Communications biology</source><year>2022</year><volume>5</volume><issue>1</issue><fpage>134</fpage><pub-id pub-id-type="doi">10.1038/s42003-022-03036-1</pub-id><pub-id pub-id-type="pmcid">PMC8850612</pub-id><pub-id pub-id-type="pmid">35173264</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Catherine</given-names></name><name><surname>Tour</surname><given-names>Tom Dupré la</given-names></name><name><surname>Gallant</surname><given-names>Jack L</given-names></name><name><surname>Klein</surname><given-names>Daniel</given-names></name><name><surname>Deniz</surname><given-names>Fatma</given-names></name></person-group><article-title>The cortical representation of language timescales is shared between reading and listening</article-title><source>Communications Biology</source><year>2024a</year><volume>7</volume><issue>1</issue><fpage>284</fpage><pub-id pub-id-type="doi">10.1038/s42003-024-05909-z</pub-id><pub-id pub-id-type="pmcid">PMC11245628</pub-id><pub-id pub-id-type="pmid">38454134</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Catherine</given-names></name><name><surname>Gong</surname><given-names>Xue L</given-names></name><name><surname>Tseng</surname><given-names>Christine</given-names></name><name><surname>Klein</surname><given-names>Daniel L</given-names></name><name><surname>Gallant</surname><given-names>Jack L</given-names></name><name><surname>Deniz</surname><given-names>Fatma</given-names></name></person-group><article-title>Bilingual language processing relies on shared semantic representations that are modulated by each language</article-title><source>bioRxiv</source><year>2024b</year><elocation-id>2024–06</elocation-id></element-citation></ref><ref id="R8"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Conneau</surname><given-names>Alexis</given-names></name><name><surname>Khandelwal</surname><given-names>Kartikay</given-names></name><name><surname>Goyal</surname><given-names>Naman</given-names></name><name><surname>Chaudhary</surname><given-names>Vishrav</given-names></name><name><surname>Wenzek</surname><given-names>Guillaume</given-names></name><name><surname>Guzmán</surname><given-names>Francisco</given-names></name><name><surname>Grave</surname><given-names>Édouard</given-names></name><name><surname>Ott</surname><given-names>Myle</given-names></name><name><surname>Zettlemoyer</surname><given-names>Luke</given-names></name><name><surname>Stoyanov</surname><given-names>Veselin</given-names></name></person-group><source>Unsupervised cross-lingual representation learning at scale</source><conf-name>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</conf-name><year>2020</year><fpage>8440</fpage><lpage>8451</lpage></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Varda</surname><given-names>Andrea Gregor</given-names></name><name><surname>Malik-Moraleda</surname><given-names>Saima</given-names></name><name><surname>Tuckute</surname><given-names>Greta</given-names></name><name><surname>Fedorenko</surname><given-names>Evelina</given-names></name></person-group><article-title>Multilingual computational models reveal shared brain responses to 21 languages</article-title><source>bioRxiv</source><year>2025</year><elocation-id>2025–02</elocation-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deniz</surname><given-names>Fatma</given-names></name><name><surname>Nunez-Elizalde</surname><given-names>Anwar O</given-names></name><name><surname>Huth</surname><given-names>Alexander G</given-names></name><name><surname>Gallant</surname><given-names>Jack L</given-names></name></person-group><article-title>The representation of semantic information across human cerebral cortex during listening versus reading is invariant to stimulus modality</article-title><source>Journal of Neuroscience</source><year>2019</year><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0675-19.2019</pub-id><pub-id pub-id-type="pmcid">PMC6764208</pub-id><pub-id pub-id-type="pmid">31427396</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Devlin</surname><given-names>Jacob</given-names></name><name><surname>Chang</surname><given-names>Ming-Wei</given-names></name><name><surname>Lee</surname><given-names>Kenton</given-names></name><name><surname>Toutanova</surname><given-names>Kristina</given-names></name></person-group><source>Bert: Pre-training of deep bidirectional transformers for language understanding</source><conf-name>Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers)</conf-name><year>2019</year><fpage>4171</fpage><lpage>4186</lpage></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fedorenko</surname><given-names>Evelina</given-names></name><name><surname>Hsieh</surname><given-names>Po-Jang</given-names></name><name><surname>Alfonso</surname><given-names>Nieto-Castañón</given-names></name><name><surname>Whitfield-Gabrieli</surname><given-names>Susan</given-names></name><name><surname>Kanwisher</surname><given-names>Nancy</given-names></name></person-group><article-title>New method for fmri investigations of language: defining rois functionally in individual subjects</article-title><source>Journal of neurophysiology</source><year>2010</year><volume>104</volume><issue>2</issue><fpage>1177</fpage><lpage>1194</lpage><pub-id pub-id-type="doi">10.1152/jn.00032.2010</pub-id><pub-id pub-id-type="pmcid">PMC2934923</pub-id><pub-id pub-id-type="pmid">20410363</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Francis</surname><given-names>Wendy S</given-names></name></person-group><article-title>Bilingual semantic and conceptual representation</article-title><source>Handbook of bilingualism: Psycholinguistic approaches</source><year>2005</year><fpage>251</fpage><lpage>267</lpage></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldstein</surname><given-names>Ariel</given-names></name><name><surname>Zada</surname><given-names>Zaid</given-names></name><name><surname>Buchnik</surname><given-names>Eliav</given-names></name><name><surname>Schain</surname><given-names>Mariano</given-names></name><name><surname>Price</surname><given-names>Amy</given-names></name><name><surname>Aubrey</surname><given-names>Bobbi</given-names></name><name><surname>Nastase</surname><given-names>Samuel A</given-names></name><name><surname>Feder</surname><given-names>Amir</given-names></name><name><surname>Emanuel</surname><given-names>Dotan</given-names></name><name><surname>Cohen</surname><given-names>Alon</given-names></name><etal/></person-group><article-title>Shared computational principles for language processing in humans and deep language models</article-title><source>Nature neuroscience</source><year>2022</year><volume>25</volume><issue>3</issue><fpage>369</fpage><lpage>380</lpage><pub-id pub-id-type="doi">10.1038/s41593-022-01026-4</pub-id><pub-id pub-id-type="pmcid">PMC8904253</pub-id><pub-id pub-id-type="pmid">35260860</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Grosjean</surname><given-names>François</given-names></name></person-group><source>On bilinguals and bilingualism</source><publisher-name>Cambridge University Press</publisher-name><year>2024</year></element-citation></ref><ref id="R16"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>Junjie</given-names></name><name><surname>Ruder</surname><given-names>Sebastian</given-names></name><name><surname>Siddhant</surname><given-names>Aditya</given-names></name><name><surname>Neubig</surname><given-names>Graham</given-names></name><name><surname>Firat</surname><given-names>Orhan</given-names></name><name><surname>Johnson</surname><given-names>Melvin</given-names></name></person-group><source>Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation</source><conf-name>International conference on machine learning</conf-name><conf-sponsor>PMLR</conf-sponsor><year>2020</year><fpage>4411</fpage><lpage>4421</lpage></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huth</surname><given-names>Alexander G</given-names></name><name><surname>De Heer</surname><given-names>Wendy A</given-names></name><name><surname>Griffiths</surname><given-names>Thomas L</given-names></name><name><surname>Theunissen</surname><given-names>Frédéric E</given-names></name><name><surname>Gallant</surname><given-names>Jack L</given-names></name></person-group><article-title>Natural speech reveals the semantic maps that tile human cerebral cortex</article-title><source>Nature</source><year>2016</year><volume>532</volume><issue>7600</issue><fpage>453</fpage><lpage>458</lpage><pub-id pub-id-type="doi">10.1038/nature17637</pub-id><pub-id pub-id-type="pmcid">PMC4852309</pub-id><pub-id pub-id-type="pmid">27121839</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jain</surname><given-names>Shailee</given-names></name><name><surname>Huth</surname><given-names>Alexander</given-names></name></person-group><article-title>Incorporating context into language encoding models for fmri</article-title><source>Advances in Neural Information Processing Systems</source><year>2018</year><volume>31</volume><pub-id pub-id-type="pmcid">PMC11258918</pub-id><pub-id pub-id-type="pmid">39035676</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karamolegkou</surname><given-names>Antonia</given-names></name><name><surname>Abdou</surname><given-names>Mostafa</given-names></name><name><surname>Anders</surname><given-names>Søgaard</given-names></name></person-group><article-title>Mapping brains with language models: A survey</article-title><source>arXiv preprint</source><year>2023</year><elocation-id>arXiv:2306.05126</elocation-id></element-citation></ref><ref id="R20"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lamarre</surname><given-names>Mathis</given-names></name><name><surname>Chen</surname><given-names>Catherine</given-names></name><name><surname>Deniz</surname><given-names>Fatma</given-names></name></person-group><source>Attention weights accurately predict language representations in the brain</source><conf-name>Findings of the Association for Computational Linguistics: EMNLP 2022</conf-name><year>2022</year><fpage>4513</fpage><lpage>4529</lpage></element-citation></ref><ref id="R21"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Liang</surname><given-names>Yaobo</given-names></name><name><surname>Duan</surname><given-names>Nan</given-names></name><name><surname>Gong</surname><given-names>Yeyun</given-names></name><name><surname>Wu</surname><given-names>Ning</given-names></name><name><surname>Guo</surname><given-names>Fenfei</given-names></name><name><surname>Qi</surname><given-names>Weizhen</given-names></name><name><surname>Gong</surname><given-names>Ming</given-names></name><name><surname>Shou</surname><given-names>Linjun</given-names></name><name><surname>Jiang</surname><given-names>Daxin</given-names></name><name><surname>Cao</surname><given-names>Guihong</given-names></name><etal/></person-group><source>Xglue: A new benchmark datasetfor cross-lingual pre-training, understanding and generation</source><conf-name>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</conf-name><conf-sponsor>Association for Computational Linguistics</conf-sponsor><year>2020</year><fpage>6008</fpage><lpage>6018</lpage></element-citation></ref><ref id="R22"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>Xi Victoria</given-names></name><name><surname>Mihaylov</surname><given-names>Todor</given-names></name><name><surname>Artetxe</surname><given-names>Mikel</given-names></name><name><surname>Wang</surname><given-names>Tianlu</given-names></name><name><surname>Chen</surname><given-names>Shuohui</given-names></name><name><surname>Simig</surname><given-names>Daniel</given-names></name><name><surname>Ott</surname><given-names>Myle</given-names></name><name><surname>Goyal</surname><given-names>Naman</given-names></name><name><surname>Bhosale</surname><given-names>Shruti</given-names></name><name><surname>Du</surname><given-names>Jingfei</given-names></name><name><surname>Pasunuru</surname><given-names>Ramakanth</given-names></name><etal/></person-group><source>Few-shot learning with multilingual generative language models</source><conf-name>Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</conf-name><year>2022</year><fpage>9019</fpage><lpage>9052</lpage></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loshchilov</surname><given-names>Ilya</given-names></name><name><surname>Hutter</surname><given-names>Frank</given-names></name></person-group><article-title>Decoupled weight decay regularization</article-title><source>arXiv preprint</source><year>2017</year><elocation-id>arXiv:1711.05101</elocation-id></element-citation></ref><ref id="R24"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Moussa</surname><given-names>Omer</given-names></name><name><surname>Klakow</surname><given-names>Dietrich</given-names></name><name><surname>Toneva</surname><given-names>Mariya</given-names></name></person-group><source>Improving semantic understanding in speech language models via brain-tuning</source><conf-name>The Thirteenth International Conference on Learning Representations</conf-name><year>2025</year><comment>URL <ext-link ext-link-type="uri" xlink:href="https://openreview.net/forum?id=KL8Sm4xRn7">https://openreview.net/forum?id=KL8Sm4xRn7</ext-link></comment></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naselaris</surname><given-names>Thomas</given-names></name><name><surname>Kay</surname><given-names>Kendrick N</given-names></name><name><surname>Nishimoto</surname><given-names>Shinji</given-names></name><name><surname>Gallant</surname><given-names>Jack L</given-names></name></person-group><article-title>Encoding and decoding in fmri</article-title><source>Neuroimage</source><year>2011</year><volume>56</volume><issue>2</issue><fpage>400</fpage><lpage>410</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.07.073</pub-id><pub-id pub-id-type="pmcid">PMC3037423</pub-id><pub-id pub-id-type="pmid">20691790</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nunez-Elizalde</surname><given-names>Anwar O</given-names></name><name><surname>Huth</surname><given-names>Alexander G</given-names></name><name><surname>Gallant</surname><given-names>Jack L</given-names></name></person-group><article-title>Voxelwise encoding models with non-spherical multivariate normal priors</article-title><source>Neuroimage</source><year>2019</year><volume>197</volume><fpage>482</fpage><lpage>492</lpage><pub-id pub-id-type="pmid">31075394</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Oota</surname><given-names>Subba Reddy</given-names></name><name><surname>Arora</surname><given-names>Jashn</given-names></name><name><surname>Agarwal</surname><given-names>Veeral</given-names></name><name><surname>Marreddy</surname><given-names>Mounika</given-names></name><name><surname>Gupta</surname><given-names>Manish</given-names></name><name><surname>Surampudi</surname><given-names>Bapi</given-names></name></person-group><source>Neural language taskonomy: Which nlp tasks are the most predictive of fmri brain activity?</source><conf-name>Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</conf-name><year>2022</year><fpage>3220</fpage><lpage>3237</lpage></element-citation></ref><ref id="R28"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Oota</surname><given-names>Subba Reddy</given-names></name><name><surname>Veeral</surname><given-names>Agarwal</given-names></name><name><surname>Mounika</surname><given-names>Marreddy</given-names></name><name><surname>Manish</surname><given-names>Gupta</given-names></name><name><surname>Bapi</surname><given-names>Raju Surampudi</given-names></name></person-group><source>Speech taskonomy: Which speech tasks are the most predictive of fmri brain activity?</source><conf-name>24th INTERSPEECH Conference</conf-name><year>2023</year></element-citation></ref><ref id="R29"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Oota</surname><given-names>Subba Reddy</given-names></name><name><surname>Emin</surname><given-names>Çelik</given-names></name><name><surname>Deniz</surname><given-names>Fatma</given-names></name><name><surname>Toneva</surname><given-names>Mariya</given-names></name></person-group><source>Speech language models lack important brain-relevant semantics</source><conf-name>Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</conf-name><conf-sponsor>Association for Computational Linguistics</conf-sponsor><year>2024a</year><fpage>8503</fpage><lpage>8528</lpage></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oota</surname><given-names>Subba Reddy</given-names></name><name><surname>Gupta</surname><given-names>Manish</given-names></name><name><surname>Toneva</surname><given-names>Mariya</given-names></name></person-group><article-title>Joint processing of linguistic properties in brains and language models</article-title><source>Advances in Neural Information Processing Systems</source><year>2024b</year><volume>36</volume></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schrimpf</surname><given-names>Martin</given-names></name><name><surname>Blank</surname><given-names>Idan Asher</given-names></name><name><surname>Tuckute</surname><given-names>Greta</given-names></name><name><surname>Kauf</surname><given-names>Carina</given-names></name><name><surname>Hosseini</surname><given-names>Eghbal A</given-names></name><name><surname>Kanwisher</surname><given-names>Nancy</given-names></name><name><surname>Tenenbaum</surname><given-names>Joshua B</given-names></name><name><surname>Fedorenko</surname><given-names>Evelina</given-names></name></person-group><article-title>The neural architecture of language: Integrative modeling converges on predictive processing</article-title><source>Proceedings of the National Academy of Sciences</source><year>2021</year><pub-id pub-id-type="doi">10.1073/pnas.2105646118</pub-id><pub-id pub-id-type="pmcid">PMC8694052</pub-id><pub-id pub-id-type="pmid">34737231</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schut</surname><given-names>Lisa</given-names></name><name><surname>Gal</surname><given-names>Yarin</given-names></name><name><surname>Farquhar</surname><given-names>Sebastian</given-names></name></person-group><article-title>Do multilingual llms think in english?</article-title><source>arXiv preprint</source><year>2025</year><elocation-id>arXiv:2502.15603</elocation-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>Dan</given-names></name><name><surname>Toneva</surname><given-names>Mariya</given-names></name><name><surname>Wehbe</surname><given-names>Leila</given-names></name></person-group><article-title>Inducing brain-relevant bias in natural language processing models</article-title><source>Advances in Neural Information Processing Systems</source><year>2019</year><volume>32</volume></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sohn</surname><given-names>Kihyuk</given-names></name></person-group><article-title>Improved deep metric learning with multi-class n-pair loss objective</article-title><source>Advances in Neural Information Processing Systems</source><year>2016</year><volume>29</volume></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Toneva</surname><given-names>Mariya</given-names></name><name><surname>Wehbe</surname><given-names>Leila</given-names></name></person-group><article-title>Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain)</article-title><source>Advances in Neural Information Processing Systems</source><year>2019</year><volume>32</volume></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Toneva</surname><given-names>Mariya</given-names></name><name><surname>Williams</surname><given-names>Jennifer</given-names></name><name><surname>Bollu</surname><given-names>Anand</given-names></name><name><surname>Dann</surname><given-names>Christoph</given-names></name><name><surname>Wehbe</surname><given-names>Leila</given-names></name></person-group><article-title>Same cause; different effects in the brain</article-title><source>Causal Learning and Reasoning</source><year>2022</year><pub-id pub-id-type="pmcid">PMC12090893</pub-id><pub-id pub-id-type="pmid">40395838</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Touvron</surname><given-names>Hugo</given-names></name><name><surname>Martin</surname><given-names>Louis</given-names></name><name><surname>Stone</surname><given-names>Kevin</given-names></name><name><surname>Albert</surname><given-names>Peter</given-names></name><name><surname>Almahairi</surname><given-names>Amjad</given-names></name><name><surname>Babaei</surname><given-names>Yasmine</given-names></name><name><surname>Bashlykov</surname><given-names>Nikolay</given-names></name><name><surname>Batra</surname><given-names>Soumya</given-names></name><name><surname>Bhargava</surname><given-names>Prajjwal</given-names></name><name><surname>Bhosale</surname><given-names>Shruti</given-names></name><etal/></person-group><article-title>Llama 2: Open foundation and fine-tuned chat models</article-title><source>arXiv preprint</source><year>2023</year><elocation-id>arXiv:2307.09288</elocation-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vattikonda</surname><given-names>Nishitha</given-names></name><name><surname>Vaidya</surname><given-names>Aditya R</given-names></name><name><surname>Antonello</surname><given-names>Richard J</given-names></name><name><surname>Huth</surname><given-names>Alexander G</given-names></name></person-group><article-title>Brain-wavlm: Fine-tuning speech representations with brain responses to language</article-title><source>arXiv preprint</source><year>2025</year><elocation-id>arXiv:2502.08866</elocation-id></element-citation></ref><ref id="R39"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Alex</given-names></name><name><surname>Singh</surname><given-names>Amanpreet</given-names></name><name><surname>Michael</surname><given-names>Julian</given-names></name><name><surname>Hill</surname><given-names>Felix</given-names></name><name><surname>Levy</surname><given-names>Omer</given-names></name><name><surname>Bowman</surname><given-names>Samuel R</given-names></name></person-group><source>Glue: A multi-task benchmark and analysis platform for natural language understanding</source><conf-name>International Conference on Learning Representations</conf-name><year>2018</year></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wehbe</surname><given-names>Leila</given-names></name><name><surname>Murphy</surname><given-names>Brian</given-names></name><name><surname>Talukdar</surname><given-names>Partha</given-names></name><name><surname>Fyshe</surname><given-names>Alona</given-names></name><name><surname>Ramdas</surname><given-names>Aaditya</given-names></name><name><surname>Mitchell</surname><given-names>Tom</given-names></name></person-group><article-title>Simultaneously uncovering the patterns of brain regions involved in different story reading subprocesses</article-title><source>PloS One</source><year>2014a</year><volume>11</volume><pub-id pub-id-type="doi">10.1371/journal.pone.0112575</pub-id><pub-id pub-id-type="pmcid">PMC4245107</pub-id><pub-id pub-id-type="pmid">25426840</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wehbe</surname><given-names>Leila</given-names></name><name><surname>Vaswani</surname><given-names>Ashish</given-names></name><name><surname>Knight</surname><given-names>Kevin</given-names></name><name><surname>Mitchell</surname><given-names>Tom</given-names></name></person-group><source>Aligning context-based statistical models of language with brain activity during reading</source><conf-name>Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</conf-name><year>2014b</year><fpage>233</fpage><lpage>243</lpage></element-citation></ref><ref id="R42"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wendler</surname><given-names>Chris</given-names></name><name><surname>Veselovsky</surname><given-names>Veniamin</given-names></name><name><surname>Monea</surname><given-names>Giovanni</given-names></name><name><surname>West</surname><given-names>Robert</given-names></name></person-group><source>Do llamas work in english? on the latent language of multilingual transformers</source><conf-name>Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</conf-name><year>2024</year><fpage>15366</fpage><lpage>15394</lpage></element-citation></ref><ref id="R43"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wolf</surname><given-names>Thomas</given-names></name><name><surname>Debut</surname><given-names>Lysandre</given-names></name><name><surname>Sanh</surname><given-names>Victor</given-names></name><name><surname>Chaumond</surname><given-names>Julien</given-names></name><name><surname>Delangue</surname><given-names>Clement</given-names></name><name><surname>Moi</surname><given-names>Anthony</given-names></name><name><surname>Cistac</surname><given-names>Pierric</given-names></name><name><surname>Rault</surname><given-names>Tim</given-names></name><name><surname>Louf</surname><given-names>Rémi</given-names></name><name><surname>Funtowicz</surname><given-names>Morgan</given-names></name><etal/></person-group><source>Transformers: State-of-the-art natural language processing</source><conf-name>Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations</conf-name><year>2020</year><fpage>38</fpage><lpage>45</lpage></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>Michael C-K</given-names></name><name><surname>David</surname><given-names>Stephen V</given-names></name><name><surname>Gallant</surname><given-names>Jack L</given-names></name></person-group><article-title>Complete functional characterization of sensory neurons by system identification</article-title><source>Annual Review of Neuroscience</source><year>2006</year><volume>29</volume><issue>1</issue><fpage>477</fpage><lpage>505</lpage><pub-id pub-id-type="pmid">16776594</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Liang</given-names></name><name><surname>Hu</surname><given-names>Hai</given-names></name><name><surname>Zhang</surname><given-names>Xuanwei</given-names></name><name><surname>Li</surname><given-names>Lu</given-names></name><name><surname>Cao</surname><given-names>Chenjie</given-names></name><name><surname>Li</surname><given-names>Yudong</given-names></name><name><surname>Xu</surname><given-names>Yechen</given-names></name><name><surname>Sun</surname><given-names>Kai</given-names></name><name><surname>Yu</surname><given-names>Dian</given-names></name><name><surname>Yu</surname><given-names>Cong</given-names></name><etal/></person-group><source>Clue: A chinese language understanding evaluation benchmark</source><conf-name>Proceedings of the 28th International Conference on Computational Linguistics</conf-name><year>2020</year><fpage>4762</fpage><lpage>4772</lpage></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Brain-informed fine-tuning pipeline.</title><p>Participants read naturalistic stories in English or Chinese while fMRI responses were recorded. The corresponding transcripts were fed into a pretrained language model. Representations from the final token of the last hidden layer were downsampled and temporally delayed to align with the fMRI acquisition. These were then projected to voxel space via a linear layer to predict brain activity. The loss between predicted and recorded responses was backpropagated through all layers for full model fine-tuning.</p></caption><graphic xlink:href="EMS207125-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Brain encoding performance before and after brain-informed fine-tuning.</title><p>We evaluated the effect of bilingual brain-informed fine-tuning using voxelwise encoding models (VEMs; see Methods). VEMs were trained using representations from both the vanilla (pretrained) model and their bilingual brain-informed fine-tuned variants (using whole-brain or using language, or semantically-selective brain regions). Flattened cortical surface for one participant is shown. Each voxel on the cortical surface is colored according to which model variant achieved the best encoding performance (<italic>r</italic>) for (a) BERT and (b) mBERT with English (left) and Chinese (right) brain data, respectively. Each point corresponds to a voxel that was consistently well predicted (<italic>r</italic>&gt;0.1) across models. Voxel colors reflect the best-performing model: black for vanilla, blue for fine-tuned with whole-brain, magenta for fine-tuned with language and yellow for semantically-selective brain regions. Across both languages and model types, bilingual brain-informed fine-tuning consistently yields better encoding performance than its vanilla counterpart. Results for other participants are in <xref ref-type="supplementary-material" rid="SD1">Appendix D</xref>.</p></caption><graphic xlink:href="EMS207125-f002"/></fig><table-wrap-group id="T1" position="float" orientation="portrait"><label>Table 1</label><caption><title>Downstream task performance before and after bilingual brain-informed fine-tuning (with whole-brain).</title><p>We compared vanilla (pretrained) models, English BERT (BERT-en), Chinese BERT (BERT-zh), and mBERT, with their bilingual brain-informed fine-tuned counterparts (e.g., BERT-ft-en: BERT-en fine-tuned using English brain data). To assess whether brain-informed fine-tuning elicits multilingual capabilities, we evaluate downstream task performance in three settings: (a) Fine-tuning and evaluation in the same language: models are fine-tuned with brain data in one language (en or zh) and evaluated on NLP tasks in the same language (GLUE benchmark for en, CLUE benchmark for zh). This tests within-language improvements due to brain-informed fine-tuning. (b) Cross-language transfer between known languages: model is fine-tuned on brain data in one language and evaluated on tasks in the participants’s second (not used in fine-tuning) language (e.g., fine-tuned with en brain data and evaluated on CLUE (zh benchmark) tasks). This tests whether bilingual brain-informed fine-tuning elicits the participants’ shared semantic representations. (c) Zero-shot transfer to unseen languages: to assess broader multilingual transfer, mBERT-ft-en is evaluated on downstream tasks in additional languages not seen during fine-tuning (German, French, Spanish, Japanese, and Korean) using XGLUE and XTREME benchmarks. Bolded values indicate performance equal to or better than the corresponding vanilla model. We observe that bilingual brain-informed fine-tuning improves performance on several NLP tasks, with mBERT showing greater benefits than BERT across all three settings. Results for fine-tuning using language/semantically-selective regions are reported in <xref ref-type="supplementary-material" rid="SD1">Appendix F.1</xref>.</p></caption><table-wrap id="T01" position="float" orientation="portrait"><label>a</label><caption><title>Fine-tuning and Evaluation in the Same Language</title></caption><table frame="box" rules="groups"><thead><tr><th valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000">GLUE Task</th><th valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000">BERT-en</th><th valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000">BERT-ft-en</th><th valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000">mBERT</th><th valign="top" align="center" style="border-top: 1px solid #000000;border-right: 1px solid #000000;border-left: 1px solid #000000">mBERT-ft-en</th><th valign="top" align="center" style="border-top: 1px solid #ffffff;border-right: 1px solid #ffffff"/><th valign="top" align="center" style="border-top: 1px solid #ffffff;border-right: 1px solid #ffffff"/><th valign="top" align="center" style="border-top: 1px solid #ffffff;border-right: 1px solid #ffffff"/><th valign="top" align="center" style="border-top: 1px solid #ffffff;border-right: 1px solid #ffffff"/><th valign="top" align="center" style="border-top: 1px solid #ffffff;border-right: 1px solid #ffffff"/><th valign="top" align="center" style="border-top: 1px solid #ffffff;border-right: 1px solid #ffffff"/></tr></thead><tbody><tr><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000">CoLA</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000">53.38</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000"><bold>55.75</bold></td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000"><bold>42.68</bold></td><td valign="top" align="center" style="border-top: 1px solid #000000;border-right: 1px solid #000000;border-left: 1px solid #000000">40.05</td><td valign="top" align="center" style="border-top: 1px solid #ffffff;border-right: 1px solid #ffffff"/><td valign="top" align="center" style="border-top: 1px solid #ffffff;border-right: 1px solid #ffffff"/><td valign="top" align="center" style="border-top: 1px solid #ffffff;border-right: 1px solid #ffffff"/><td valign="top" align="center" style="border-top: 1px solid #ffffff;border-right: 1px solid #ffffff"/><td valign="top" align="center" style="border-top: 1px solid #ffffff;border-right: 1px solid #ffffff"/><td valign="top" align="center" style="border-top: 1px solid #ffffff;border-right: 1px solid #ffffff"/></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000">SST-2</td><td valign="top" align="center" style="border-left: 1px solid #000000">92.08</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>93.12</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000">89.68</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid #000000"><bold>90.14</bold></td><td valign="top" align="center" style="border-top: 1px solid #ffffff;border-right: 1px solid #ffffff"/><td valign="top" align="center" style="border-top: 1px solid #ffffff;border-right: 1px solid #ffffff"/><td valign="top" align="center" style="border-top: 1px solid #ffffff;border-right: 1px solid #ffffff"/><td valign="top" align="center" style="border-top: 1px solid #ffffff;border-right: 1px solid #ffffff"/><td valign="top" align="center" style="border-top: 1px solid #ffffff;border-right: 1px solid #ffffff"/><td valign="top" align="center" style="border-top: 1px solid #0ffffff00000;border-right: 1px solid #ffffff"/></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000">MRPC (Acc.)</td><td valign="top" align="center" style="border-left: 1px solid #000000">79.41</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>79.66</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>84.80</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid #000000"><bold>84.80</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000"/><td valign="top" align="center" style="border-top: 1px solid #000000;border-bottom: 1px solid #000000;border-right: 1px solid #000000"><bold>CLUE Task</bold></td><td valign="top" align="center" style="border-top: 1px solid #000000;border-bottom: 1px solid #000000;border-right: 1px solid #000000"><bold>BERT-zh</bold></td><td valign="top" align="center" style="border-top: 1px solid #000000;border-bottom: 1px solid #000000;border-right: 1px solid #000000"><bold>BERT-ft-zh</bold></td><td valign="top" align="center" style="border-top: 1px solid #000000;border-bottom: 1px solid #000000;border-right: 1px solid #000000"><bold>mBERT</bold></td><td valign="top" align="center" style="border-top: 1px solid #000000;border-bottom: 1px solid #000000;border-right: 1px solid #000000"><bold>mBERT-ft-zh</bold></td></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000">MRPC (F1)</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>86.27</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000">86.19</td><td valign="top" align="center" style="border-left: 1px solid #000000">88.56</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid #000000"><bold>89.01</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000"/><td valign="top" align="center" style="border-right: 1px solid #000000">AFQMC</td><td valign="top" align="center" style="border-right: 1px solid #000000"><bold>75.25</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000">74.01</td><td valign="top" align="center" style="border-right: 1px solid #000000">69.74</td><td valign="top" align="center" style="border-right: 1px solid #000000"><bold>70.55</bold></td></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000">STS-B (Pears.)</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>88.06</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000">88.01</td><td valign="top" align="center" style="border-left: 1px solid #000000">88.06</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid #000000"><bold>88.42</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000"/><td valign="top" align="center" style="border-right: 1px solid #000000">CMNLI</td><td valign="top" align="center" style="border-right: 1px solid #000000">80.50</td><td valign="top" align="center" style="border-right: 1px solid #000000"><bold>80.89</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000">78.66</td><td valign="top" align="center" style="border-right: 1px solid #000000"><bold>79.02</bold></td></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000">STS-B (Spear.)</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>87.65</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000">87.63</td><td valign="top" align="center" style="border-left: 1px solid #000000">87.76</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid #000000"><bold>88.22</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000"/><td valign="top" align="center" style="border-right: 1px solid #000000">CSL (F1.)</td><td valign="top" align="center" style="border-right: 1px solid #000000">80.18</td><td valign="top" align="center" style="border-right: 1px solid #000000"><bold>80.73</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000"><bold>81.10</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000">80.71</td></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000">QQP (Acc.)</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>90.84</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000">90.77</td><td valign="top" align="center" style="border-left: 1px solid #000000">90.22</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid #000000"><bold>90.47</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000"/><td valign="top" align="center" style="border-right: 1px solid #000000">IFLYTEK</td><td valign="top" align="center" style="border-right: 1px solid #000000">60.25</td><td valign="top" align="center" style="border-right: 1px solid #000000"><bold>60.29</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000">56.52</td><td valign="top" align="center" style="border-right: 1px solid #000000"><bold>56.83</bold></td></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000">QQP (F1)</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>87.70</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000">87.60</td><td valign="top" align="center" style="border-left: 1px solid #000000">86.70</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid #000000"><bold>87.07</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000"/><td valign="top" align="center" style="border-right: 1px solid #000000">TNEWS (Acc.)</td><td valign="top" align="center" style="border-right: 1px solid #000000">56.24</td><td valign="top" align="center" style="border-right: 1px solid #000000"><bold>56.61</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000">54.77</td><td valign="top" align="center" style="border-right: 1px solid #000000"><bold>55.12</bold></td></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000">MNLI-m</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>84.38</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000">84.28</td><td valign="top" align="center" style="border-left: 1px solid #000000">82.09</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid #000000"><bold>82.66</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000"/><td valign="top" align="center" style="border-right: 1px solid #000000">TNEWS (F1.)</td><td valign="top" align="center" style="border-right: 1px solid #000000">55.17</td><td valign="top" align="center" style="border-right: 1px solid #000000"><bold>55.52</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000">53.69</td><td valign="top" align="center" style="border-right: 1px solid #000000"><bold>53.74</bold></td></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000">MNLI-mm</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>84.64</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000">84.26</td><td valign="top" align="center" style="border-left: 1px solid #000000">82.38</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid #000000"><bold>82.97</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000"/><td valign="top" align="center" style="border-right: 1px solid #000000">ChID</td><td valign="top" align="center" style="border-right: 1px solid #000000">10.66</td><td valign="top" align="center" style="border-right: 1px solid #000000"><bold>10.66</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000">10.66</td><td valign="top" align="center" style="border-right: 1px solid #000000"><bold>10.66</bold></td></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000">QNLI</td><td valign="top" align="center" style="border-left: 1px solid #000000">91.45</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>91.56</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000">91.14</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid #000000"><bold>91.18</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000"/><td valign="top" align="center" style="border-right: 1px solid #000000">C<sup>3</sup></td><td valign="top" align="center" style="border-right: 1px solid #000000">49.74</td><td valign="top" align="center" style="border-right: 1px solid #000000"><bold>49.97</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000">49.42</td><td valign="top" align="center" style="border-right: 1px solid #000000"><bold>50.21</bold></td></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000">RTE</td><td valign="top" align="center" style="border-left: 1px solid #000000">67.15</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>67.51</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>67.15</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid #000000">65.70</td><td valign="top" align="center" style="border-bottom: 1px solid #ffffff;border-right: 1px solid #000000"/><td valign="top" align="center" style="border-bottom: 1px solid #000000;border-right: 1px solid #000000"/><td valign="top" align="center" style="border-bottom: 1px solid #000000;border-right: 1px solid #000000"/><td valign="top" align="center" style="border-bottom: 1px solid #000000;border-right: 1px solid #000000"/><td valign="top" align="center" style="border-bottom: 1px solid #000000;border-right: 1px solid #000000"/><td valign="top" align="center" style="border-bottom: 1px solid #000000;border-right: 1px solid #000000"/></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000;border-bottom: 1px solid #000000">WNLI</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-bottom: 1px solid #000000">49.30</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-bottom: 1px solid #000000"><bold>56.34</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000;border-bottom: 1px solid #000000">53.52</td><td valign="top" align="center" style="border-right: 1px solid #000000;border-bottom: 1px solid #000000;border-left: 1px solid #000000"><bold>56.34</bold></td><td valign="top" align="center" style="border-bottom: 1px solid #ffffff;border-right: 1px solid #ffffff"/><td valign="top" align="center" style="border-bottom: 1px solid #ffffff;border-right: 1px solid #ffffff"/><td valign="top" align="center" style="border-bottom: 1px solid #ffffff;border-right: 1px solid #ffffff"/><td valign="top" align="center" style="border-bottom: 1px solid #ffffff;border-right: 1px solid #ffffff"/><td valign="top" align="center" style="border-bottom: 1px solid #ffffff;border-right: 1px solid #ffffff"/><td valign="top" align="center" style="border-bottom: 1px solid #ffffff;border-right: 1px solid #ffffff"/></tr></tbody></table></table-wrap><table-wrap id="T02" position="float" orientation="portrait"><label>b</label><caption><title>Cross-Language Transfer Between Known Languages</title></caption><table frame="box" rules="groups"><thead><tr><th valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000">GLUE Task</th><th valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000">BERT-zh</th><th valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000">BERT-ft-zh</th><th valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000">mBERT</th><th valign="top" align="center" style="border-top: 1px solid #000000;border-right: 1px solid #000000;border-left: 1px solid #000000">mBERT-ft-zh</th><th valign="top" align="center" style="border-top: 1px solid #ffffff;border-right: 1px solid #ffffff"/><th valign="top" align="center" style="border-top: 1px solid #ffffff;border-right: 1px solid #ffffff"/><th valign="top" align="center" style="border-top: 1px solid #ffffff;border-right: 1px solid #ffffff"/><th valign="top" align="center" style="border-top: 1px solid #ffffff;border-right: 1px solid #ffffff"/><th valign="top" align="center" style="border-top: 1px solid #ffffff;border-right: 1px solid #ffffff"/><th valign="top" align="center" style="border-top: 1px solid #ffffff;border-right: 1px solid #ffffff"/></tr></thead><tbody><tr><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000">CoLA</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000">51.13</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000"><bold>53.82</bold></td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000">40.96</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-right: 1px solid #000000;border-left: 1px solid #000000"><bold>41.22</bold></td><td valign="top" align="center" style="border-top: 1px solid #ffffff;border-right: 1px solid #ffffff"/><td valign="top" align="center" style="border-top: 1px solid #ffffff;border-right: 1px solid #ffffff"/><td valign="top" align="center" style="border-top: 1px solid #ffffff;border-right: 1px solid #ffffff"/><td valign="top" align="center" style="border-top: 1px solid #ffffff;border-right: 1px solid #ffffff"/><td valign="top" align="center" style="border-top: 1px solid #ffffff;border-right: 1px solid #ffffff"/><td valign="top" align="center" style="border-top: 1px solid #ffffff;border-right: 1px solid #ffffff"/></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000">SST-2</td><td valign="top" align="center" style="border-left: 1px solid #000000">92.26</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>92.85</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000">88.27</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid #000000"><bold>89.25</bold></td><td valign="top" align="center" style="border-right: 1px solid #ffffff"/><td valign="top" align="center" style="border-right: 1px solid #ffffff"/><td valign="top" align="center" style="border-right: 1px solid #ffffff"/><td valign="top" align="center" style="border-right: 1px solid #ffffff"/><td valign="top" align="center" style="border-right: 1px solid #ffffff"/><td valign="top" align="center" style="border-right: 1px solid #ffffff"/></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000">MRPC (Acc.)</td><td valign="top" align="center" style="border-left: 1px solid #000000">76.96</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>77.45</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000">82.84</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid #000000"><bold>83.82</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000"/><td valign="top" align="center" style="border-bottom: 1px solid #000000;border-top: 1px solid #000000;border-right: 1px solid #000000"><bold>CLUE Task</bold></td><td valign="top" align="center" style="border-bottom: 1px solid #000000;border-top: 1px solid #000000;border-right: 1px solid #000000"><bold>BERT-en</bold></td><td valign="top" align="center" style="border-bottom: 1px solid #000000;border-top: 1px solid #000000;border-right: 1px solid #000000"><bold>BERT-ft-en</bold></td><td valign="top" align="center" style="border-bottom: 1px solid #000000;border-top: 1px solid #000000;border-right: 1px solid #000000"><bold>mBERT</bold></td><td valign="top" align="center" style="border-bottom: 1px solid #000000;border-top: 1px solid #000000;border-right: 1px solid #000000"><bold>mBERT-ft-en</bold></td></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000">MRPC (F1)</td><td valign="top" align="center" style="border-left: 1px solid #000000">83.38</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>84.20</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000">87.15</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid #000000"><bold>87.88</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000"/><td valign="top" align="center" style="border-right: 1px solid #000000">AFQMC</td><td valign="top" align="center" style="border-right: 1px solid #000000">69.00</td><td valign="top" align="center" style="border-right: 1px solid #000000"><bold>69.00</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000">69.74</td><td valign="top" align="center" style="border-right: 1px solid #000000"><bold>70.62</bold></td></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000">STS-B (Pears.)</td><td valign="top" align="center" style="border-left: 1px solid #000000">87.21</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>87.95</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000">87.09</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid #000000"><bold>88.11</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000"/><td valign="top" align="center" style="border-right: 1px solid #000000">CMNLI</td><td valign="top" align="center" style="border-right: 1px solid #000000">68.34</td><td valign="top" align="center" style="border-right: 1px solid #000000"><bold>68.63</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000">78.66</td><td valign="top" align="center" style="border-right: 1px solid #000000"><bold>78.83</bold></td></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000">STS-B (Spear.)</td><td valign="top" align="center" style="border-left: 1px solid #000000">86.73</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>87.65</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000">86.97</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid #000000"><bold>87.83</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000"/><td valign="top" align="center" style="border-right: 1px solid #000000">CSL (F1.)</td><td valign="top" align="center" style="border-right: 1px solid #000000">71.20</td><td valign="top" align="center" style="border-right: 1px solid #000000"><bold>71.43</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000">81.10</td><td valign="top" align="center" style="border-right: 1px solid #000000"><bold>81.30</bold></td></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000">QQP (Acc.)</td><td valign="top" align="center" style="border-left: 1px solid #000000">90.65</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>90.76</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000">89.91</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid #000000"><bold>90.16</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000"/><td valign="top" align="center" style="border-right: 1px solid #000000">IFLYTEK</td><td valign="top" align="center" style="border-right: 1px solid #000000"><bold>47.86</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000">46.79</td><td valign="top" align="center" style="border-right: 1px solid #000000">56.52</td><td valign="top" align="center" style="border-right: 1px solid #000000"><bold>57.14</bold></td></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000">QQP (F1)</td><td valign="top" align="center" style="border-left: 1px solid #000000">87.53</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>87.72</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000">86.27</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid #000000"><bold>86.89</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000"/><td valign="top" align="center" style="border-right: 1px solid #000000">TNEWS (Acc.)</td><td valign="top" align="center" style="border-right: 1px solid #000000">50.92</td><td valign="top" align="center" style="border-right: 1px solid #000000"><bold>51.14</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000">54.77</td><td valign="top" align="center" style="border-right: 1px solid #000000"><bold>54.81</bold></td></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000">MNLI-m</td><td valign="top" align="center" style="border-left: 1px solid #000000">84.00</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>84.12</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000">81.52</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid #000000"><bold>82.13</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000"/><td valign="top" align="center" style="border-right: 1px solid #000000">TNEWS (F1.)</td><td valign="top" align="center" style="border-right: 1px solid #000000">50.15</td><td valign="top" align="center" style="border-right: 1px solid #000000"><bold>50.60</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000">53.69</td><td valign="top" align="center" style="border-right: 1px solid #000000"><bold>53.84</bold></td></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000">MNLI-mm</td><td valign="top" align="center" style="border-left: 1px solid #000000">83.91</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>84.11</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000">81.94</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid #000000"><bold>82.36</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000"/><td valign="top" align="center" style="border-right: 1px solid #000000">ChID</td><td valign="top" align="center" style="border-right: 1px solid #000000">10.66</td><td valign="top" align="center" style="border-right: 1px solid #000000"><bold>10.66</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000">10.66</td><td valign="top" align="center" style="border-right: 1px solid #000000"><bold>10.66</bold></td></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000">QNLI</td><td valign="top" align="center" style="border-left: 1px solid #000000">91.27</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>91.38</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000">90.55</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid #000000"><bold>90.92</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000"/><td valign="top" align="center" style="border-right: 1px solid #000000">C<sup>3</sup></td><td valign="top" align="center" style="border-right: 1px solid #000000"><bold>42.64</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000">41.12</td><td valign="top" align="center" style="border-right: 1px solid #000000">49.42</td><td valign="top" align="center" style="border-right: 1px solid #000000"><bold>49.48</bold></td></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000">RTE</td><td valign="top" align="center" style="border-left: 1px solid #000000">67.15</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>67.87</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000">66.06</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid #000000"><bold>66.43</bold></td><td valign="top" align="center" style="border-right: 1px solid #ffffff;border-top: 1px solid #ffffff"/><td valign="top" align="center" style="border-right: 1px solid #ffffff;border-top: 1px solid #000000"/><td valign="top" align="center" style="border-right: 1px solid #ffffff;border-top: 1px solid #000000"/><td valign="top" align="center" style="border-right: 1px solid #ffffff;border-top: 1px solid #000000"/><td valign="top" align="center" style="border-right: 1px solid #ffffff;border-top: 1px solid #000000"/><td valign="top" align="center" style="border-right: 1px solid #ffffff;border-top: 1px solid #000000"/></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000;border-bottom: 1px solid #000000">WNLI</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-bottom: 1px solid #000000">53.52</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-bottom: 1px solid #000000"><bold>56.34</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000;border-bottom: 1px solid #000000">54.93</td><td valign="top" align="center" style="border-right: 1px solid #000000;border-bottom: 1px solid #000000;border-left: 1px solid #000000"><bold>56.34</bold></td><td valign="top" align="center" style="border-bottom: 1px solid #ffffff;border-right: 1px solid #ffffff"/><td valign="top" align="center" style="border-bottom: 1px solid #ffffff;border-right: 1px solid #ffffff"/><td valign="top" align="center" style="border-bottom: 1px solid #ffffff;border-right: 1px solid #ffffff"/><td valign="top" align="center" style="border-bottom: 1px solid #ffffff;border-right: 1px solid #ffffff"/><td valign="top" align="center" style="border-bottom: 1px solid #ffffff;border-right: 1px solid #ffffff"/><td valign="top" align="center" style="border-bottom: 1px solid #ffffff;border-right: 1px solid #ffffff"/></tr></tbody></table></table-wrap><table-wrap id="T03" position="float" orientation="portrait"><label>c</label><caption><title>Zero-Shot Transfer to Unseen Languages</title></caption><table frame="box" rules="groups"><thead><tr><th valign="top" align="left" rowspan="2" style="border-top: 1px solid #000000;border-left: 1px solid #000000">Task</th><th valign="top" align="center" colspan="2" style="border-top: 1px solid #000000;border-left: 1px solid #000000">German (de)</th><th valign="top" align="center" colspan="2" style="border-top: 1px solid #000000;border-left: 1px solid #000000">French (fr)</th><th valign="top" align="center" colspan="2" style="border-top: 1px solid #000000;border-left: 1px solid #000000">Spanish (es)</th><th valign="top" align="center" colspan="2" style="border-top: 1px solid #000000;border-left: 1px solid #000000">Japanese (ja)</th><th valign="top" align="center" colspan="2" style="border-top: 1px solid #000000;border-right: 1px solid #000000;border-left: 1px solid #000000">Korean (ko)</th></tr><tr><th valign="top" align="center" style="border-top: 1px solid #ffffff;border-left: 1px solid #000000">mBERT</th><th valign="top" align="center" style="border-top: 1px solid #ffffff;border-left: 1px solid #000000">mBERT-ft-en</th><th valign="top" align="center" style="border-top: 1px solid #ffffff;border-left: 1px solid #000000">mBERT</th><th valign="top" align="center" style="border-top: 1px solid #ffffff;border-left: 1px solid #000000">mBERT-ft-en</th><th valign="top" align="center" style="border-top: 1px solid #ffffff;border-left: 1px solid #000000">mBERT</th><th valign="top" align="center" style="border-top: 1px solid #ffffff;border-left: 1px solid #000000">mBERT-ft-en</th><th valign="top" align="center" style="border-top: 1px solid #ffffff;border-left: 1px solid #000000">mBERT</th><th valign="top" align="center" style="border-top: 1px solid #ffffff;border-left: 1px solid #000000">mBERT-ft-en</th><th valign="top" align="center" style="border-top: 1px solid #ffffff;border-left: 1px solid #000000">mBERT</th><th valign="top" align="center" style="border-top: 1px solid #ffffff;border-right: 1px solid #000000;border-left: 1px solid #000000">mBERT-ft-en</th></tr></thead><tbody><tr><td valign="top" align="center" colspan="11" style="border-top: 1px solid #000000;border-right: 1px solid #000000;border-left: 1px solid #000000"><bold>XGLUE</bold></td></tr><tr><td valign="top" align="left" style="border-top: 1px solid #000000;border-left: 1px solid #000000">PAWS-X</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000"><bold>84.00</bold></td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000">82.85</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000"><bold>88.10</bold></td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000">87.75</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000"><bold>87.05</bold></td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000">86.80</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000"><bold>78.70</bold></td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000">78.05</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000"><bold>78.75</bold></td><td valign="top" align="center" style="border-top: 1px solid #000000;border-right: 1px solid #000000;border-left: 1px solid #000000">77.15</td></tr><tr><td valign="top" align="left" style="border-left: 1px solid #000000">XNLI</td><td valign="top" align="center" style="border-left: 1px solid #000000">70.60</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>71.04</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000">72.69</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>72.73</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000">74.10</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>74.58</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000">86.00</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>88.00</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000">86.00</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid #000000"><bold>88.00</bold></td></tr><tr><td valign="top" align="left" style="border-left: 1px solid #000000">NER</td><td valign="top" align="center" style="border-left: 1px solid #000000">10.67</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>13.00</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000">06.67</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>12.33</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000">07.33</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>12.67</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>13.00</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000">10.67</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>12.00</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid #000000">11.67</td></tr><tr><td valign="top" align="center" colspan="11" style="border-top: 1px solid #000000;border-right: 1px solid #000000;border-left: 1px solid #000000"><bold>XTREME</bold></td></tr><tr><td valign="top" align="left" style="border-top: 1px solid #000000;border-left: 1px solid #000000">PAWS-X</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000"><bold>85.65</bold></td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000">83.90</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000"><bold>88.95</bold></td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000">87.30</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000"><bold>87.90</bold></td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000">87.25</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000"><bold>79.55</bold></td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000">77.95</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000"><bold>79.35</bold></td><td valign="top" align="center" style="border-top: 1px solid #000000;border-right: 1px solid #000000;border-left: 1px solid #000000">78.25</td></tr><tr><td valign="top" align="left" style="border-left: 1px solid #000000">XNLI</td><td valign="top" align="center" style="border-left: 1px solid #000000">70.06</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>71.08</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>72.97</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000">72.25</td><td valign="top" align="center" style="border-left: 1px solid #000000">74.50</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>74.60</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000">86.00</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>87.00</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000">86.00</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid #000000"><bold>87.00</bold></td></tr><tr><td valign="top" align="left" style="border-left: 1px solid #000000;border-bottom: 1px solid #000000">NER</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-bottom: 1px solid #000000">12.33</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-bottom: 1px solid #000000"><bold>13.00</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000;border-bottom: 1px solid #000000">09.33</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-bottom: 1px solid #000000"><bold>14.00</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000;border-bottom: 1px solid #000000">10.33</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-bottom: 1px solid #000000"><bold>11.33</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000;border-bottom: 1px solid #000000">08.67</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-bottom: 1px solid #000000"><bold>13.00</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000;border-bottom: 1px solid #000000"><bold>12.33</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000;border-bottom: 1px solid #000000;border-left: 1px solid #000000">10.33</td></tr></tbody></table></table-wrap></table-wrap-group><table-wrap-group id="T2" position="float" orientation="portrait"><label>Table 2</label><caption><title>Downstream task performance before and after bilingual or monolingual brain-informed fine-tuning.</title><p>We perform brain-informed fine-tuning of mBERT with English whole-brain brain data (mBERT-ft-en) from either a bilingual (participant 1) or a monolingual participant. We evaluate downstream task performance in two settings: (a) Fine-tuning and evaluation in the same language: the model is evaluated in English with GLUE tasks. (b) Cross-language transfer between known languages: the model is evaluated in Chinese with CLUE tasks.</p></caption><table-wrap id="T04" position="float" orientation="portrait"><label>a</label><caption><title>Fine-tuning and Evaluation in the Same Language</title></caption><table frame="box" rules="groups"><thead><tr><th valign="top" align="left" style="border-top: 1px solid #000000;border-left: 1px solid"/><th valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid"/><th valign="top" align="center" colspan="2" style="border-top: 1px solid #000000;border-left: 1px solid #000000;border-right: 1px solid">mBERT-ft-en</th></tr><tr><th valign="top" align="left" style="border-top: 1px solid #000000;border-left: 1px solid">Task</th><th valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid">mBERT</th><th valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid">Bilingual</th><th valign="top" align="center" style="border-top: 1px solid #000000;border-right: 1px solid #000000;border-left: 1px solid">Monolingual</th></tr></thead><tbody><tr><td valign="top" align="left" style="border-top: 1px solid #000000;border-left: 1px solid">CoLA</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid"><bold>42.68</bold></td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid">40.05</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-right: 1px solid #000000;border-left: 1px solid">35.74</td></tr><tr><td valign="top" align="left" style="border-left: 1px solid">SST-2</td><td valign="top" align="center" style="border-left: 1px solid">89.68</td><td valign="top" align="center" style="border-left: 1px solid">90.14</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid"><bold>90.48</bold></td></tr><tr><td valign="top" align="left" style="border-left: 1px solid #000000">MRPC (Acc.)</td><td valign="top" align="center" style="border-left: 1px solid #000000">84.80</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>84.80</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid #000000">78.85</td></tr><tr><td valign="top" align="left" style="border-left: 1px solid">MRPC (F1)</td><td valign="top" align="center" style="border-left: 1px solid">88.56</td><td valign="top" align="center" style="border-left: 1px solid"><bold>89.01</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid">85.78</td></tr><tr><td valign="top" align="left" style="border-left: 1px solid">STS-B (Pearson)</td><td valign="top" align="center" style="border-left: 1px solid">88.06</td><td valign="top" align="center" style="border-left: 1px solid"><bold>88.42</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid"><bold>88.42</bold></td></tr><tr><td valign="top" align="left" style="border-left: 1px solid">STS-B (Spearman)</td><td valign="top" align="center" style="border-left: 1px solid">87.76</td><td valign="top" align="center" style="border-left: 1px solid">88.22</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid"><bold>88.34</bold></td></tr><tr><td valign="top" align="left" style="border-left: 1px solid">QQP (Acc.)</td><td valign="top" align="center" style="border-left: 1px solid">90.22</td><td valign="top" align="center" style="border-left: 1px solid">90.47</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid"><bold>90.54</bold></td></tr><tr><td valign="top" align="left" style="border-left: 1px solid">QQP (F1)</td><td valign="top" align="center" style="border-left: 1px solid">86.70</td><td valign="top" align="center" style="border-left: 1px solid">87.07</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid"><bold>87.19</bold></td></tr><tr><td valign="top" align="left" style="border-left: 1px solid">MNLI-m</td><td valign="top" align="center" style="border-left: 1px solid">82.09</td><td valign="top" align="center" style="border-left: 1px solid"><bold>82.66</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid">82.41</td></tr><tr><td valign="top" align="left" style="border-left: 1px solid #000000">MNLI-mm</td><td valign="top" align="center" style="border-left: 1px solid #000000">82.38</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>82.97</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid #000000">82.78</td></tr><tr><td valign="top" align="left" style="border-left: 1px solid #000000">QNLI</td><td valign="top" align="center" style="border-left: 1px solid #000000">91.14</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>91.18</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid #000000">91.12</td></tr><tr><td valign="top" align="left" style="border-left: 1px solid">RTE</td><td valign="top" align="center" style="border-left: 1px solid">67.15</td><td valign="top" align="center" style="border-left: 1px solid"><bold>65.70</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid">65.34</td></tr><tr><td valign="top" align="left" style="border-left: 1px solid #000000;border-bottom: 1px solid">WNLI</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-bottom: 1px solid">53.52</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-bottom: 1px solid"><bold>56.34</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000;border-bottom: 1px solid #000000;border-left: 1px solid"><bold>56.34</bold></td></tr></tbody></table></table-wrap><table-wrap id="T05" position="float" orientation="portrait"><label>b</label><caption><title>Cross-Language Transfer Between Known Languages</title></caption><table frame="box" rules="groups"><thead><tr><th valign="top" align="center" colspan="2" style="border-top: 1px solid #000000;border-left: 1px solid"/><th valign="top" align="center" colspan="2" style="border-top: 1px solid #000000;border-left: 1px solid #000000;border-right: 1px solid">mBERT-ft-en</th></tr><tr><th valign="top" align="left" style="border-top: 1px solid #000000;border-left: 1px solid">CLUE Task</th><th valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid">mBERT</th><th valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid">Bilingual</th><th valign="top" align="center" style="border-top: 1px solid #000000;border-right: 1px solid #000000;border-left: 1px solid">Monolingual</th></tr></thead><tbody><tr><td valign="top" align="left" style="border-top: 1px solid #000000;border-left: 1px solid">AFQMC</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid">69.74</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid">70.62</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-right: 1px solid #000000;border-left: 1px solid"><bold>70.64</bold></td></tr><tr><td valign="top" align="left" style="border-left: 1px solid">CMNLI</td><td valign="top" align="center" style="border-left: 1px solid">78.66</td><td valign="top" align="center" style="border-left: 1px solid"><bold>78.83</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid">78.48</td></tr><tr><td valign="top" align="left" style="border-left: 1px solid #000000">CSL (F1)</td><td valign="top" align="center" style="border-left: 1px solid #000000">81.10</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>81.30</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid #000000">81.18</td></tr><tr><td valign="top" align="left" style="border-left: 1px solid">IFLYTEK</td><td valign="top" align="center" style="border-left: 1px solid">46.79</td><td valign="top" align="center" style="border-left: 1px solid">56.52</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid"><bold>57.06</bold></td></tr><tr><td valign="top" align="left" style="border-left: 1px solid #000000">TNEWS (Acc.)</td><td valign="top" align="center" style="border-left: 1px solid #000000">54.77</td><td valign="top" align="center" style="border-left: 1px solid #000000"><bold>54.81</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid #000000">54.80</td></tr><tr><td valign="top" align="left" style="border-left: 1px solid">TNEWS (F1)</td><td valign="top" align="center" style="border-left: 1px solid">53.69</td><td valign="top" align="center" style="border-left: 1px solid"><bold>53.84</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid">53.50</td></tr><tr><td valign="top" align="left" style="border-left: 1px solid">ChID</td><td valign="top" align="center" style="border-left: 1px solid">10.66</td><td valign="top" align="center" style="border-left: 1px solid"><bold>10.66</bold></td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid"><bold>10.66</bold></td></tr><tr><td valign="top" align="left" style="border-left: 1px solid #000000;border-bottom: 1px solid">C<sup>3</sup></td><td valign="top" align="center" style="border-left: 1px solid #000000;border-bottom: 1px solid">49.42</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-bottom: 1px solid"><bold>49.48</bold></td><td valign="top" align="center" style="border-right: 1px solid #000000;border-bottom: 1px solid #000000;border-left: 1px solid">49.46</td></tr></tbody></table></table-wrap></table-wrap-group></floats-group></article>