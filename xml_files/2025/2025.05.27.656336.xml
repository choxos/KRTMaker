<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS206028</article-id><article-id pub-id-type="doi">10.1101/2025.05.27.656336</article-id><article-id pub-id-type="archive">PPR1027548</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Parallel and dynamic attention allocation during natural reading</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Pan</surname><given-names>Yali</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Frisson</surname><given-names>Steven</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Snell</surname><given-names>Joshua</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Federmeier</surname><given-names>Kara D</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Jensen</surname><given-names>Ole</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A5">5</xref></contrib></contrib-group><aff id="A1"><label>1</label>Centre for Human Brain Health, School of Psychology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/03angcq70</institution-id><institution>University of Birmingham</institution></institution-wrap>, <city>Birmingham</city>, <country country="GB">UK</country></aff><aff id="A2"><label>2</label>Department of Psychiatry, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap>, <city>Oxford</city>, <country country="GB">UK</country></aff><aff id="A3"><label>3</label>Department of Experimental and Applied Psychology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/008xxew50</institution-id><institution>Vrije Universiteit Amsterdam</institution></institution-wrap>, <city>Amsterdam</city>, <country country="NL">The Netherlands</country></aff><aff id="A4"><label>4</label>Department of Psychology, Program in Neuroscience, and the Beckman Institute for Advanced Science and Technology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/047426m28</institution-id><institution>University of Illinois</institution></institution-wrap>, <city>Champaign</city>, <country country="US">USA</country></aff><aff id="A5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0172mzb45</institution-id><institution>Oxford Centre for Human Brain Activity</institution></institution-wrap>, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/0172mzb45</institution-id><institution>Wellcome Centre for Integrative Neuroimaging</institution></institution-wrap>, Department of Experimental Psychology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap>, <city>Oxford</city>, <country country="GB">UK</country></aff><author-notes><corresp id="CR1">
<label>*</label><bold>Correspondence</bold>: <email>yalipan666@gmail.com</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>30</day><month>05</month><year>2025</year></pub-date><pub-date pub-type="preprint"><day>27</day><month>05</month><year>2025</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">During natural reading, attention constantly shifts across words, yet how linguistic properties (e.g., lexical frequency) impact the allocation of attention remains unclear. In this study, we co-registered MEG data and eye movements while participants read one-line sentences containing target words of either low or high lexical frequency. Using rapid invisible frequency tagging (RIFT), we simultaneously tracked attention to target and post-target words by flickering them at different frequencies. First, we provide neural evidence that attention was allocated simultaneously to both foveal target and parafoveal post-target words. Second, we found an early parafoveal lexical effect, whereby lower frequency targets demanded more attention prior to fixation, and, additionally, a foveal load effect whereby lower frequency targets reduced the amount of attention allocated to post-targets. Furthermore, flexibility in attention shifts between foveal and parafoveal processing correlated with individual reading speed. These results suggest attention is distributed across multiple words and is flexibly adjusted during reading.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">What you are doing now, reading, involves moving your eyes to bring the word of interest into foveal vision, then with focused attention, you crack its linguistic code. While reading feels effortless, it requires seamless interaction of oculomotor control, attention shifts, and linguistic processing<sup><xref ref-type="bibr" rid="R1">1</xref></sup>. There is no doubt that linguistic information in the text impacts how we allocate our attention; for example, when processing a difficult, low lexical-frequency word, we allocate more attention by fixating it longer, demonstrating the classic word frequency effect<sup><xref ref-type="bibr" rid="R2">2</xref>,<xref ref-type="bibr" rid="R3">3</xref></sup>. However, how a word modulates attention across multiple saccades - before, during, and after fixating on it- remains poorly understood. Investigating the dynamics of attention allocation can also provide key insights for shaping theories of natural reading<sup><xref ref-type="bibr" rid="R4">4</xref></sup>.</p><p id="P3">To capture the dynamics of attention allocation during natural reading, it is essential to track not only attention to the fixated word but also to adjacent, yet-to-be-fixated parafoveal words. However, conventional techniques in reading research typically do not directly measure attention allocated to parafoveal vision. Behaviourally, eye tracking informs how long our eyes stay on the fixated word, making it difficult to directly measure the processing of parafoveal words. Instead, parafoveal processing is inferred through two key effects: the preview benefit, where parafoveal preview of a word’s linguistic information shortens the fixation duration when it is subsequently processed foveally<sup><xref ref-type="bibr" rid="R5">5</xref>–<xref ref-type="bibr" rid="R14">14</xref></sup>; and parafoveal-on-foveal (PoF) effects, whereby certain properties of a word in the parafovea impact the fixation duration of the currently fixated word<sup><xref ref-type="bibr" rid="R3">3</xref>,<xref ref-type="bibr" rid="R15">15</xref>–<xref ref-type="bibr" rid="R19">19</xref></sup>. Thus, eye tracking based evidence relies on indirect inferences based on how parafoveal characteristics affect either the subsequent fixation or the currently fixated word, rather than providing direct measurements of parafoveal processing. Measurements of neural activity through fixation/event-related-potentials (FRPs/ERPs) have also provided evidence for parafoveal processing by aligning brain activity with fixation/word onsets<sup><xref ref-type="bibr" rid="R20">20</xref>–<xref ref-type="bibr" rid="R29">29</xref></sup> (for a review see<sup><xref ref-type="bibr" rid="R30">30</xref></sup>). For instance, the amplitude of the N400 has been shown to be modulated by the predictability of the parafoveal word <sup><xref ref-type="bibr" rid="R24">24</xref>,<xref ref-type="bibr" rid="R25">25</xref>,<xref ref-type="bibr" rid="R31">31</xref></sup>. However, the N400 does not directly measure attention allocation; instead, it reflects the retrieval of a parafoveal word’s representation from long-term memory, with less predictable words eliciting more new semantic activation in long-term memory, thus resulting in larger N400 amplitudes<sup><xref ref-type="bibr" rid="R32">32</xref></sup>. Despite substantial evidence for parafoveal processing, techniques that allow a direct measurement of the attention allocated to both foveal and parafoveal words have been lacking. With the present study we will tackle this longstanding issue.</p><p id="P4">A recently developed technique, rapid invisible frequency tagging (RIFT), enables the direct and precise measurement of attention during reading, particularly for words in parafoveal vision. RIFT is derived from steady-state-visually-evoked-potentials elicited by stimuli flickering at high frequencies (&gt;= 60 Hz; visually invisible, to minimize interference with ongoing tasks). Neurons in the visual system respond to the visual flicker by generating a signal at the same tagging frequency band that can be measured with EEG or MEG, a phenomenon termed “frequency tagging”<sup><xref ref-type="bibr" rid="R33">33</xref>–<xref ref-type="bibr" rid="R39">39</xref></sup>. These tagging responses increase with attention, making frequency tagging an ideal technique for measuring attention allocation. In a natural reading task, when a word flickers in a sentence, the attention allocated to this word can be directly tracked by the frequency tagged responses. Our previous studies using RIFT demonstrated that lexical<sup><xref ref-type="bibr" rid="R40">40</xref></sup> and semantic<sup><xref ref-type="bibr" rid="R41">41</xref></sup> properties of a target word modulate attention allocation even before it is fixated. These effects were observed within 100 ms after fixating the pre-target word, when the target word was still in the parafoveal vision. Furthermore, since tagging responses are yoked to a specific flickering frequency, we can track the allocation of attention to multiple words simultaneously simply by flickering them at different frequencies.</p><p id="P5">In the current study, we applied RIFT to investigate attentional dynamics during natural reading (<xref ref-type="fig" rid="F1">Figure 1</xref>). Specifically, we aimed to address three questions (<xref ref-type="fig" rid="F2">Figure 2</xref>): 1) Can attention actually be allocated to two words simultaneously? 2) How does linguistic information, such as lexical frequency, modulate attention allocation <italic>before</italic> we fixate on that word? 3) and <italic>while</italic> we fixate on that word? This co-registered eye-tracking and MEG study involved participants (<italic>n</italic> = 42) silently reading 188 one-line sentences. Embedded in each sentence were one or two target words with either high or low lexical frequency (<xref ref-type="table" rid="T1">Table 1</xref>). The target words were unpredictable from the prior sentence context, and all sentences were plausible (see Stimuli and Behavioural pre-tests in <xref ref-type="sec" rid="S9">Methods</xref>). We frequency-tagged both the target and post-target words by simultaneously flickering their respective underlying patches at <italic>f<sub>t</sub></italic> and <italic>f<sub>p</sub></italic> (60 and 65 Hz sinusoids, balanced across participants; <xref ref-type="fig" rid="F1">Figure 1</xref>). We also applied Gaussian masks over the flickering patches to minimize their visibility across saccades. Since the flickering frequencies are near or above the critical flicker fusion frequency, participants perceived the flickers as grey, the same colour as the background, rendering them invisible.</p><p id="P6">RIFT responses provide a direct and precise measure of attention allocated to the target and post-target words throughout the reading process. We hypothesized that: 1) attention would be allocated simultaneously to the target and post-target words, evidenced by significantly-above-baseline RIFT responses at <italic>f<sub>t</sub></italic> and <italic>f<sub>p</sub></italic> during the target fixation intervals. 2) The lexical parafoveal effect would result in stronger RIFT responses at <italic>f<sub>t</sub></italic> when previewing low frequency versus high frequency target words, replicating findings from our previous study<sup><xref ref-type="bibr" rid="R40">40</xref></sup>. 3) The (word-frequency-based) foveal load effect would manifest as weaker RIFT responses at <italic>f<sub>p</sub></italic> during fixation on low frequency target words, as greater attention retention in the foveal vision would reduce parafoveal attention available for processing post-target words.</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>No eye movement evidence for parallel word processing in reading</title><p id="P7">We first examined eye movement measures to test whether we replicated classic effects in the literature — namely, no evidence for lexical frequency effects on eye movement behaviours driven by words in parafoveal vision<sup><xref ref-type="bibr" rid="R43">43</xref></sup>, but, once those words are fixated, shorter first fixations to higher frequency words. We analysed first fixation durations (i.e., the time spent on a word upon the first landing), an early eye movement measure. No lexical frequency effect was observed for pre-target words (<italic>t</italic><sub>(41)</sub> = 0.575, <italic>p</italic> = 0.569; <xref ref-type="fig" rid="F3">Figure 3a</xref>), indicating the lexical frequency of the target word did not affect fixation durations on the pre-target word, providing no evidence for lexical parafoveal processing. As expected, we observed the classic word frequency effect, as evidenced by a significant lexical frequency effect on the first fixation durations of target words, with higher frequency target words associated with shorter durations (<italic>t</italic><sub>(41)</sub> = 5.042, <italic>p</italic> = 9.801×10<sup>-6</sup>, two-tailed pairwise <italic>t</italic>-test; <xref ref-type="fig" rid="F3">Figure 3b</xref>). Interestingly, this lexical frequency effect extended to post-target words (<italic>t</italic><sub>(41)</sub> = 2.146, <italic>p</italic> = 0.038; <xref ref-type="fig" rid="F3">Figure 3c</xref>), indicating “spill over” of the frequency effect from the target words.</p></sec><sec id="S4"><title>Parallel attention allocation in reading measured by RIFT</title><p id="P8">MEG data were segmented into a 1 s epoch centred around the first fixation onset of the target words. Coherence between the tagging signal (recorded by a photodiode) and MEG sensors was calculated to measure RIFT responses. Sensors were identified as RIFT response sensors if coherence at the tagging frequency was significantly stronger during target fixation intervals (target word was flickering) compared with baseline intervals (no flickering). This sensor selection procedure included all the trials during each fixation type, irrespective of the experimental conditions (see <xref ref-type="sec" rid="S9">Methods</xref>). Sensor selection was conducted separately for the frequencies of <italic>f<sub>t</sub></italic> and <italic>f<sub>p</sub></italic> to ensure precise identification of sensors responding to the foveal and parafoveal tagging (for RIFT sensor topographies see <xref ref-type="supplementary-material" rid="SD1">Supplementary figure 1</xref>; for <italic>f<sub>t</sub></italic>, 5.2 ± 5.7 sensors per participant (mean ± STD), range from 1 to 26 sensors; for <italic>f<sub>p</sub></italic>, 6.3 ± 5.6 sensors, range from 1 to 27 sensors). Further coherence analysis was based on participants with RIFT sensors for both the foveal and parafoveal tagging (29 out of 42 participants).</p><p id="P9">Target words were frequency tagged at <italic>f<sub>t</sub></italic> in foveal vision, while post-target words were frequency tagged at <italic>f<sub>p</sub></italic> in parafoveal vision. Therefore, tagging responses at <italic>f<sub>t</sub></italic> indicated foveal attention, while tagging responses at <italic>f<sub>p</sub></italic> indicated parafoveal attention. Using the corresponding RIFT sensors, we calculated coherence as a function of time for the frequencies <italic>f<sub>t</sub></italic> and <italic>f<sub>p</sub></italic> (<xref ref-type="fig" rid="F4">Figure 4</xref>). Additionally, we calculated the coherence during the baseline intervals before the sentence presentation to estimate the noise level of coherence. The tagging signal was from the flickering disks on the corners that were covered by photodiodes. We then averaged the coherence values over the 0.2 s target fixations for foveal attention and parafoveal attention, as well as the coherence during the baseline intervals. Dependent sample <italic>t</italic>-tests (pairwise, two-sided) revealed significantly stronger coherence during both foveal (<italic>t</italic><sub>(28)</sub> = 4.544, <italic>p</italic> = 9.636×10<sup>-5</sup>) and parafoveal processing (<italic>t</italic><sub>(28)</sub> = 5.198, <italic>p</italic> = 1.617×10<sup>-5</sup>) compared with the baseline. Additionally, no significant difference was found between foveal and parafoveal coherence during the target fixation intervals (<italic>t</italic><sub>(28)</sub> = 1.415, <italic>p</italic> = 0.168), meaning that the amount of attention allocation to foveal and parafoveal words was comparable. We also questioned whether the parallel allocation of attention as reflected in <xref ref-type="fig" rid="F4">Figure 4</xref> may have been caused by exclusive coherence at <italic>f<sub>t</sub></italic> on one half of trials and exclusive coherence at <italic>f<sub>p</sub></italic> on the other half of trials (which would mean that attention was actually only at one word at any given timepoint). In the Supplementary Notes we provide the single-trial power analysis that rules out this scenario.</p><p id="P10">In addition to coherence curves at <italic>f<sub>t</sub></italic> and <italic>f<sub>p</sub></italic> over all trials during target fixation intervals (<xref ref-type="fig" rid="F4">Figure 4</xref>), we also estimated coherence curves separately by condition (<xref ref-type="supplementary-material" rid="SD1">Supplementary figure 2</xref>) for pre-target and post-target fixation intervals. For the corresponding RIFT response sensors used in this coherence analysis, see <xref ref-type="supplementary-material" rid="SD1">Supplementary figure 1</xref>.</p></sec><sec id="S5"><title>Neural evidence for lexical parafoveal processing in natural reading</title><p id="P11">Eye movement data showed no evidence of a parafoveal-on-foveal (PoF) lexical effect (<xref ref-type="fig" rid="F3">Figure 3</xref>). The neural data measured as RIFT responses, however, revealed significant differences (<xref ref-type="fig" rid="F5">Figure 5</xref>, column <bold>a</bold>). First, RIFT response sensors were selected based on a stronger coherence at <italic>f<sub>t</sub></italic> during pre-target fixation intervals (using all trials) compared with the baseline intervals (for RIFT sensors topography see <xref ref-type="fig" rid="F5">Figure 5</xref> row <bold>i</bold>). Then, for participants with RIFT sensors (30 out of 42 participants), the time-resolved coherence spectra were calculated for the low and high lexical target conditions (<xref ref-type="fig" rid="F5">Figure 5</xref>, row <bold>iii</bold> and <bold>iv</bold>). Afterwards, coherence spectra were averaged over all sensors-of-interest for each RIFT-response-participant. To compare the coherence differences between these two experimental conditions, a group-level cluster-based permutation test was performed during pre-target fixations (0 – 0.2 s) at the frequency of <italic>f<sub>t</sub></italic> (± 3Hz) (<xref ref-type="fig" rid="F5">Figure 5</xref>, row <bold>v</bold>). We found a significant positive cluster indicating a stronger coherence during pre-target fixations when previewing a low lexical frequency target word (low – high, <italic>p</italic><sub>cluster</sub> = 0.036, pairwise two-sided <italic>t</italic>-test during permutations; see <xref ref-type="sec" rid="S9">Methods</xref> for details).</p></sec><sec id="S6"><title>Foveal load determined attention allocated to parafoveal words</title><p id="P12">We then asked whether the lexical frequency of the foveal word determined how much attention was allocated to parafoveal words. During the fixation intervals of the target word, the post-target word was frequency tagged at <italic>f<sub>p</sub></italic> in the parafovea (<xref ref-type="fig" rid="F5">Figure 5</xref>, column <bold>b</bold>). First, RIFT sensors were selected based on stronger coherence at <italic>f<sub>p</sub></italic> during target fixation intervals compared with baseline intervals (for RIFT sensors topography see <xref ref-type="fig" rid="F5">Figure 5</xref> row <bold>i)</bold>. Then, for the RIFT response participants (33 out of 42 participants), an averaged coherence spectrum over all RIFT sensors was calculated. A group-level cluster-based permutation test during target fixations (0 – 0.2 s) at <italic>f<sub>p</sub></italic> (± 3Hz) revealed a significant negative cluster (low – high, <italic>p</italic><sub>cluster</sub> = 0.025, <xref ref-type="fig" rid="F5">Figure 5</xref> row <bold>v</bold>). This result demonstrated the foveal load effect: when readers fixated on low lexical frequency words in the foveal vision, less attention was allocated to the parafoveal vision, resulting in weaker tagging responses.</p><p id="P13">To explore whether attention would regress back covertly to the more difficult, low frequency words, we also did a conditional contrast during the post-target fixation intervals (<xref ref-type="fig" rid="F5">Figure 5</xref>, column <bold>c</bold>). We first selected the RIFT sensors-of-interest during the post-target fixation intervals when the target word was tagged at <italic>f<sub>t</sub></italic>. Then a cluster-based permutation test was performed, revealing no significant positive or negative clusters (low – high, <italic>p</italic><sub>cluster</sub> = 0.290 for the positive clusters; no negative clusters were found), indicating that covert attention did not shift further back when the just-processed word was of low frequency.</p></sec><sec id="S7"><title>Correlation between attention shifts and reading speed</title><p id="P14">To examine the behavioural significance of the dynamics of attention shifts, we correlated attentional flexibility with reading speed. Here, flexibility was defined as the difference between the lexical parafoveal and the foveal load effects. The lexical parafoveal effect was quantified as the averaged coherence difference between conditions (low – high parafoveal lexicality) within the significant clusters during pre-target fixation intervals (<xref ref-type="fig" rid="F5">Figure 5</xref>, column <bold>a</bold> row <bold>v</bold>). Similarly, the foveal load effect was quantified as the averaged coherence difference within the significant clusters during target fixation intervals (<xref ref-type="fig" rid="F5">Figure 5</xref>, column <bold>b</bold> row <bold>v</bold>). Individual reading speed was estimated by averaging the first fixation durations over all words in the sentences, i.e., a lower value indicated a faster reading speed (27 out of 42 participants who responded to both the target and post-target flickers). A Pearson’s correlation analysis revealed a significant negative correlation between the flexibility of attention shift and reading speed (<italic>r</italic> = -0.468, <italic>p</italic> = 0.014, <xref ref-type="fig" rid="F6">Figure 6</xref>).</p></sec></sec><sec id="S8" sec-type="discussion"><title>Discussion</title><p id="P15">How is attention distributed during reading? Here we applied rapid invisible frequency tagging (RIFT) in a natural reading task, simultaneously flickering both the target and post-target words as sinusoids at frequency <italic>f<sub>t</sub></italic> and <italic>f<sub>p</sub></italic> (<xref ref-type="fig" rid="F1">Figure 1</xref>) while recording MEG and eye-tracking data. By calculating the RIFT responses, we directly and precisely tracked attention allocated to multiple words (i.e., the target and post-target words) across multiple saccades (i.e., before, during, and after target fixation). First, when fixating on a target word while pre-processing the post-target word in the parafovea, we observed significant RIFT responses to both words (<xref ref-type="fig" rid="F4">Figure 4</xref>), indicating attention being allocated to two words simultaneously. Turning to the effects of lexical frequency, before fixating on a low frequency target word, we observed stronger RIFT responses at <italic>f<sub>t</sub></italic>, indicating increased parafoveal attention (<xref ref-type="fig" rid="F5">Figure 5</xref>, column <bold>a</bold>). However, when fixating on this low frequency target word, less attention was allocated to the parafovea, as evidenced by weaker RIFT responses at <italic>f<sub>p</sub></italic> (<xref ref-type="fig" rid="F5">Figure 5</xref>, column <bold>b</bold>). This flexibility in shifting attention between parafoveal and foveal vision was positively correlated with reading speed, with a higher degree of flexibility linked to faster reading speeds (<xref ref-type="fig" rid="F6">Figure 6</xref>).</p><p id="P16">First, we provide direct electrophysiological evidence that attention can be allocated to two words simultaneously (<xref ref-type="fig" rid="F4">Figure 4</xref>). Whether attention can be split between foveal and parafoveal words lies at the heart of the debate between serial and parallel processing models. Serial models<sup><xref ref-type="bibr" rid="R44">44</xref>–<xref ref-type="bibr" rid="R48">48</xref></sup>, such as E-Z Reader, assume that attention moves across words in a serial fashion, with only one word under the spotlight of attention at any given time. However, by simultaneously tracking attention to both the foveal and parafoveal words, we found that the attention spotlight can subsume at least two words during a single fixation. Importantly, the simultaneous high tagging responses for foveal and parafoveal words cannot be attributed to a trial-level confound in which attention is focused on the fovea in some trials and on the parafovea in others (see Supplementary Notes and <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 3 and 4</xref> for details). The absence of a time shift between the attentional distribution of the two words, as indicated by the overlapping tagging responses curves in <xref ref-type="fig" rid="F4">Figure 4</xref>, suggests that attention can be allocated simultaneously to foveal and parafoveal words, rather than simply shifting to the parafoveal word while the eye remain on the foveal one (<xref ref-type="fig" rid="F4">Figure 4</xref>). When considered alongside the lexical parafoveal effect shown in <xref ref-type="fig" rid="F5">Figure 5</xref> panel <bold>a</bold> (discussed further in the paragraph below), our results indicate that the amount of attention allocated to the parafoveal word is sufficient for decoding the lexical information. These findings thus point to the recognition of two words during a single fixation, providing compelling neural evidence in support of parallel processing models<sup><xref ref-type="bibr" rid="R49">49</xref>–<xref ref-type="bibr" rid="R54">54</xref></sup>. In the future, theoretical frameworks of natural reading should give greater weight to the flexible, distributed nature of attention and its capacity to support simultaneous processing across multiple words.</p><p id="P17">If the neural data evidence parallel word processing, why does the oculomotor data not follow suit? In our RIFT-based neural data, stronger RIFT responses were observed for low frequency target words during parafoveal processing (<xref ref-type="fig" rid="F5">Figure 5</xref>, column <bold>a</bold>); in contrast, lexical information from parafoveal words did not influence the fixation durations on the foveal words (<xref ref-type="fig" rid="F3">Figure 3<bold>a</bold></xref>). This pattern, also observed in our previous study<sup><xref ref-type="bibr" rid="R40">40</xref></sup>, suggests a dissociation between attention (measured by RIFT responses) and eye movements (measured by fixation durations) during natural reading. That is, attention acts “ahead of” the eyes to pre-process the parafoveal word, but this parafoveal processing need not be reflected in changes in fixation patterns. It is possible that the parafoveal information is either too subtle or too slow to affect saccade programming before the point-of-no-return. Alternatively, it may be that the attentional allocation to the parafoveal word is sufficient for the extraction of key information, such that additional fixation time is not needed. In either case, eye movement patterns do not need to be adjusted word by word, which may serve to optimize cognitive resources for processing words efficiently during reading. This is supported by our correlation results: readers who can flexibly shift attention, without slowing down the eyes (as reflected in the absence of a lexical parafoveal effect in the eye tracking data), tend to process words more quickly (<xref ref-type="fig" rid="F6">Figure 6</xref>). This observation aligns with Yang and McConkie’s model, which proposes that most eye movements are not directly controlled by cognitive processes<sup><xref ref-type="bibr" rid="R55">55</xref>–<xref ref-type="bibr" rid="R57">57</xref></sup>. Furthermore, this notion underpins the principle of autonomous saccade generation in the SWIFT model<sup><xref ref-type="bibr" rid="R50">50</xref></sup>. In sum, our findings highlight that eye movements do not inform the serial-versus-parallel debate.</p><p id="P18">The RIFT responses also revealed a foveal load effect: foveal processing of a low compared to high frequency target word reduced attention allocation to parafoveal words (<xref ref-type="fig" rid="F5">Figure 5</xref>, column <bold>b</bold>). This reduction in parafoveal processing due to the increased demands of foveal processing has been repeatedly demonstrated in eye movement studies<sup><xref ref-type="bibr" rid="R58">58</xref>–<xref ref-type="bibr" rid="R61">61</xref></sup> and event-related-potentials studies<sup><xref ref-type="bibr" rid="R62">62</xref>,<xref ref-type="bibr" rid="R63">63</xref></sup>. In our study, we provide neural evidence for this effect and specify its precise timing. The reduction in parafoveal attention, reflected by reduced RIFT responses, occurred immediately after the fixation onset on the low frequency target word. This immediate modulation of parafoveal attention does not align with the timeline predicted by serial processing models, which assume that attention shifts to the parafovea only after some level of foveal processing has been completed<sup><xref ref-type="bibr" rid="R44">44</xref>–<xref ref-type="bibr" rid="R48">48</xref></sup>. Instead, this rapid modulation of attention suggests that lexical information of the target word was already extracted before fixating on it, as supported by the lexical parafoveal effect discussed earlier. Our previous study even found a modulation of saccade timing based on parafoveal lexical information<sup><xref ref-type="bibr" rid="R64">64</xref></sup>.</p><p id="P19">Combining the lexical parafoveal effect and the foveal load effect, we observed a dynamic and flexible shift of attention during natural reading. When the parafoveal word is more difficult to process, more attention is allocated to it; however, while fixating on this difficult word, more attention is withheld to prioritize foveal processing. This flexibility in shifting attention was correlated with reading speed (<xref ref-type="fig" rid="F6">Figure 6</xref>). This correlation also suggests that fast readers do not necessarily always allocate more attention to pre-process parafoveal words; they also retain attention foveally when the currently fixated word is more demanding. Therefore, flexibility, rather than constant forward allocation, may be key to efficient reading.</p><p id="P20">In summary, by applying RIFT in a natural reading task, we provide the neural evidence that attention can be split between foveal and parafoveal words in a single fixation. Furthermore, we observed that lexical frequency dynamically modulates attention allocation: increased parafoveal attention facilitates previewing of difficult words, while foveal attention retention supports further processing of difficult words. This dynamic flexibility in attention shifts underlies efficient reading. Our study contributes to parallel word processing theories and demonstrates the utility of combining eye-tracking with RIFT to obtain comprehensive insights into the neural underpinnings of reading.</p></sec><sec id="S9" sec-type="materials | methods"><title>Materials and Methods</title><sec id="S10" sec-type="subjects"><title>Participants</title><p id="P21">Forty-two participants (32 females; age: 22.3 ± 3.3 years, mean ± SD) were recruited for the current study. All participants were native English speakers with normal or corrected-to-normal vision (via contact lenses, not glasses). Participants were right-handed and had no history of neurological conditions or language disorders such as dyslexia. Ethical approval for the study was obtained from the University of Birmingham Ethics Committee (Ethics approval number: ERN-18-0226AP27). Informed consent was obtained from all participants following a full explanation of the study. Compensation was provided as either £30 or 2 course credits. The study lasted approximately 2 hours, including preparation and MEG scanning time.</p></sec><sec id="S11"><title>Stimuli</title><p id="P22">In the current study we had 188 sentences, consisting of two sentence sets.</p><sec id="S12"><title>First sentence set</title><p id="P23">The first sentence set consisted of 80 sentences, selected from our previous study<sup><xref ref-type="bibr" rid="R40">40</xref></sup>. Sentences that were embedded with a long (i.e., more than 8 letters) pre-target word or target word were not selected, as prior findings demonstrated that long pre-target and target words reduce parafoveal processing (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 5 and Supplementary Notes</xref> in <sup><xref ref-type="bibr" rid="R40">40</xref></sup>). For the selected sentences, pre-target word lengths range from 4 to 8 letters, while target word lengths ranged from 5 to 8 letters. For a given sentence, the combined word length of pre-target and target words was between 9 to 15 letters, ensuring that the target word fell within the perceptual span and could undergo parafoveal processing. The same word length standard was applied in the second sentence set as well.</p><p id="P24">Each sentence was embedded with one target word, which was never placed within the first or last three words in the sentence. Target words were always nouns and appeared in grammatical structures comprising adjectives, nouns, and verbs in the pre-target, target, and post-target positions, respectively. These target words varied in lexical frequency (low and high) and were matched for word length (for instance, “ <italic>waltz</italic>” and ”<italic>music</italic>”). Each pair of target words was embedded in two different sentence frames. The surrounding pre-target and post-target words were of the same length across the two sentences frames. The target words within each pair were interchangeable between the two sentence frames, yielding two versions of sentences that were balanced across participants. An example pair of target words for version A and B are shown below (note that target words in the example are bolded and in italics for illustration but were not set apart in the actual experiment). Words were presented in equal-spaced Courier New font.</p><preformat>
<monospace>A. Mike thought this difficult <bold><italic>waltz</italic></bold> received lots of criticism.</monospace>
<monospace>   It was obvious that the beautiful <bold><italic>music</italic></bold> captured her attention.</monospace>
<monospace>B. Mike thought this difficult <bold><italic>music</italic></bold> received lots of criticism.</monospace>
<monospace>   It was obvious that the beautiful <bold><italic>waltz</italic></bold> captured her attention.</monospace>
</preformat><p id="P29">In total, the first sentence set contained 80 sentences, embedded with 40 pairs of low and high frequency target words.</p></sec><sec id="S13"><title>Second sentence set</title><p id="P30">The second sentence set included all of the 108 sentences from Degno and colleagues (2019)<sup><xref ref-type="bibr" rid="R18">18</xref></sup>. These sentences were embedded with target words from various word classes, such as nouns, adjectives, adverbs, and verbs. Each sentence was embedded with two target words of the same lexical frequency condition; for instance, “bleak” and “risky” in the example sentence below are both low frequency words. These two target words were paired with another two target words of the same word length but from the opposite lexical frequency condition; for instance, “weird” and “nasty” are both high frequency words. These two pairs of target words were interchangeable within the same sentence frame, resulting in two versions per sentence (A and B), balanced across participants; see example below (again, targets are in bold and italic in the example only for illustration). Words were presented in equal-spaced Courier New font.</p><preformat>
<monospace>A. I felt quite <bold>bleak</bold> after discussing that really <bold>risky</bold> subject with Paul.</monospace>
<monospace>B. I felt quite <bold>weird</bold> after discussing that really <bold>nasty</bold> subject with Paul.</monospace>
</preformat><p id="P33">In sum, the second sentence set contained 108 sentences, embedded with 108 pairs of low and high frequency target words. Putting the first and second sentence sets together, we had 188 sentences, with 148 pairs of target words, evenly distributed across low and high lexical frequency conditions. Participants read either sentence version A or version B for both sentence sets.</p></sec></sec><sec id="S14"><title>Behavioural pre-tests</title><p id="P34">All sentences were plausible and all target words were unpredictable, as established by prior behavioural norming<sup><xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R40">40</xref></sup>, which we summarize here. For detailed information on the first sentence set, please refer to Pan et. Al.<sup><xref ref-type="bibr" rid="R40">40</xref></sup>; for the second sentence set, please refer to Degno et. Al.<sup><xref ref-type="bibr" rid="R18">18</xref></sup>.</p><p id="P35">Plausibility of the sentences was rated on a 7-point scale, with scores ranging from 1 (least plausible) to 7 (most plausible). The experimental sentences were designed to be highly plausible; therefore, filler sentences with low and medium plausibility were included in the norming to occupy the full range of the scale. All experimental sentences achieved plausibility ratings of 5 and above, confirming that the sentences were plausible.</p><p id="P36">Predictability of the target words were assessed using a cloze test. Participants read sentence fragments consisting of the experimental materials up to, but not including the target words. They were then asked to write down the first word that came to mind that could continue the sentence fragment (without needing to complete the entire sentence). Predictability of a given word was estimated as the percentage of participants who wrote down exactly this word during the cloze test. The predictability of all target words was below 10%, indicating that the target words could not be predicted based on the prior sentential context. Additionally, the highest predictability of any word at the target word location was below 25%, indicating that the sentential contexts at that point were low constraint.</p></sec><sec id="S15" sec-type="methods"><title>Experimental procedure</title><p id="P37">First, participants were required to remove all metal items on the face and body and change into scrubs. They then read and signed the consent form of this study and reviewed the instructions for the reading task. Following preparations for MEG data acquisition (details below), participants were seated in a dimly lit magnetically shielded room (MSR). The MEG gantry was set at a 60° upright angle, fully covering the participant’s head.</p><p id="P38">All visual stimuli were projected from the stimulus computer, located outside the MSR, onto the participant screen, located inside the MSR, using a PROPixx DLP LED projector (Vpixx Technologies Inc., Canada; for details see Projection of the sentence stimuli). The experimental stimuli presentation scripts were programmed in Psychophysics Toolbox -3 65. The background colour of the screen was middle-grey (RGB [128 128 128]), and words were displayed in black (RGB [0 0 0]). The words were drawn in an equal-spaced Courier New font, at font size 20, with the font style of bold. In the present set up, each letter and space occupied 0.43° visual angle, with the entire sentence spanning no more than 37° of visual angle in the horizontal direction. Sentences were presented as a single line, vertically centred on the screen, starting 2° from the left edge of the screen (<xref ref-type="fig" rid="F1">Figure 1</xref>).</p><p id="P39">Each trial began with a fixation cross displayed at the centre of the middle-grey screen for 1.2 – 1.6 seconds (randomly selected from a uniform distribution). This was followed by a black square (the “starting square”) with a radius of 1° visual angle, placed at the left vertical centre, 2° from the left edge of the screen. A gaze at the starting square for 0.2 seconds triggered the onset of the sentence. The entire sentence was displayed on the screen at once, with its first letter placed at the position of the starting square. An “ending square” (the same size as the starting square but grey, RGB [64 64 64]), was presented below the sentence.</p><p id="P40">Participants were instructed to gaze at the ending square after reading the sentence. A gaze at the ending square for 0.1seconds triggered the offset of the sentence presentation. The trial ended with a blank middle-grey screen that lasted 0.5 seconds. Randomly, in 25% of trials, a comprehension statement about the content of the immediately preceding sentence appeared. Participants were required to answer “True or “False” by pressing a button. The behavioural performance on this comprehension task was high (94.1% ± 5.4%, mean ± SD), indicating that participants read the sentences attentively.</p><p id="P41">The experiment was divided into four blocks, each lasting approximately 6 minutes. Participants had at least a one-minute break between blocks and could resume the experiment at their convenience by pressing any button. Participants were instructed to read sentences silently at their own pace. In total, the entire experiment took no longer than 40 minutes. Eye movements and brain activity were recorded simultaneously throughout the session, and eye data were co-registered with MEG data in later analysis.</p></sec><sec id="S16"><title>Rapid invisible frequency tagging (RIFT)</title><sec id="S17"><title>Projection of the sentence stimuli</title><p id="P42">The refresh rate of the PROPixx projector was 1440 Hz (VPixx Technologies Inc., Canada), while the refresh rate of the stimulus computer monitor was 120 Hz (1920 × 1080 pixels resolution). To achieve the high refresh rate of the PROPixx projector at 1440 Hz, the sentence was displayed repeatedly in four quadrants of the stimulus computer monitor. The projector then interpreted these 12 colour channels (3 RGB channels × 4 quadrants) as 12 individual grayscale frames, which were projected onto the participant screen in rapid succession. Consequently, one frame from the stimulus computer became 12 frames for the PROPixx projector, resulting in displaying sentences at 12 × 120 Hz, which was 1440 Hz.</p></sec><sec id="S18"><title>Frequency tagging of target and post-target words simultaneously</title><p id="P43">To frequency-tag a word, a rectangular patch was added underneath it. The side length of the patch was the width of this word plus the spaces on both sides, spanning approximately 3° to 4.5° visual angle. We flickered the patch by changing the luminance of each pixel inside of it from black to white in a sinusoid pattern. Using this method, we tagged both the target and post-target words simultaneously by flickering the patches underneath at two different frequencies. For half of the participants, target words were tagged at 60 Hz, and post-target words at 65 Hz; for the other half, target words were tagged at 65 Hz and post-target words at 60 Hz. These two tagging frequencies were balanced across participants rather than trials to ensure maximum number of trials for an optimal signal-to-noise ratio for detecting the tagging responses. For illustration purposes, the tagging frequency of the target words is denoted as <italic>f<sub>t</sub></italic>, and that of the post-target words was denoted as <italic>f<sub>p</sub></italic> (<xref ref-type="fig" rid="F1">Figure 1</xref>). The flickering patches were perceived as middle-grey (matching the screen background) due to their sinusoidal luminance modulation, rendering them invisible to participants. Note that both the target and the post-target words were displayed in black and were not flickering, which were the same as the other words on the screen.</p><p id="P44">The two rectangular patches underneath the target and post-target words were flickering throughout the entire presentation of the sentence, with participants making continuous saccades across words. To minimize sharp luminance transitions and prevent visibility of patch edges during saccades from the non-tagged to the tagged area, we applied a Gaussian-smoothed transparent mask over the flickering patch. The mask was fully transparent at the centre and gradually became opaque towards the edges. The mask was generated using a two-dimensional Gaussian function (<xref ref-type="disp-formula" rid="FD1">Equation 1</xref>): <disp-formula id="FD1"><mml:math id="M1"><mml:mrow><mml:mi>mask</mml:mi><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="P45">where <italic>x</italic> and <italic>y</italic> are the mesh grid coordinates for the flickering patch, and <italic>σ</italic> is the <italic>x</italic> and <italic>y</italic> spread of the mask with <italic>σ</italic> = 0.02° visual angle.</p><p id="P46">To record the tagging signal for later coherence analysis, we used two custom-made photodiodes (Aalto NeuroImaging Centre, Finland) to record the luminance changes of the flickering patches. These photodiodes, placed to the bottom-left and bottom-right corners of the participant screen, monitored disks that flickered identically to the flickering patches underneath the target and post-target words. These photodiodes converted luminance/light changes into voltage signals, which were recorded as two external channels in the MEG system, sampled at 1,000 Hz, the same rate as the MEG sensors.</p></sec></sec><sec id="S19"><title>Data acquisition</title><sec id="S20"><title>MEG data</title><p id="P47">During the whole session of the reading experiment, MEG data were collected using a 306-sensor TRIUX Elekta Neuromag system, which consisted of 204 orthogonal planar gradiometers and 102 magnetometers (Elekta, Finland). To prepare for MEG data acquisition, we first attached four head-position indicator (HPI) coils to each participant’s head — two on the left and right mastoid bone and two on the forehead, with a minimum separation of 3 cm between them. Afterwards, a Polhemus Fastrack electromagnetic digitizer system (Polhemus Inc, USA) was used to digitize the individual head. We first digitized the locations of three bony fiducial points — the nasion, and left and right preauricular points. Then we digitized locations of the four HPI coils. Finally, we digitized the whole head by sampling at least 200 points that covered the whole scalp and were distributed evenly. These points from the digitization procedure were later used in spatially co-registering the MEG head model with individual structural MRI images for the source analysis. During this preparation, we also attached three pairs of electrodes on each participant’s face; one pair was placed above and below the right eye to record the vertical electrooculogram (EOG), another pair was placed 1 cm away from the left and right of the eyes to record the horizontal EOG, and the last pair was placed on the left and right collarbone to record the heart’s electrical activity (i.e., electrocardiogram, EKG or ECG).</p><p id="P48">After the preparation, participants were seated upright under the MEG gantry at a 60° angle within the magnetically shielded room. The MEG data were sampled at 1,000 Hz, with an online band-pass filtering from 0.1 to 330 Hz to minimize aliasing effects.</p></sec><sec id="S21"><title>Eye movement data</title><p id="P49">Eye movements were tracked using an EyeLink 1000 Plus eye-tracker (long-range mount, SR Research Ltd, Canada) throughout the whole MEG session. The eye tracker was placed on a wooden table in front of the participant. The centre of the horizontal bar of the eye tracker was aligned to the middle of the participant screen, and the top of the eye tracker was at the same level as the screen bottom edge. The distance between the eye-tracker camera and the participant’s eyes was 90 cm. We recorded the horizontal and vertical positions as well as the pupil size from the left eye, at a sampling rate of 1,000 Hz. Each block began with a five-point calibration and validation test. The test was accepted if the validation error was below 1° visual angle both horizontally and vertically. During the block, we performed a one-point drift checking test every three trials to ensure a good enough precision of the eye tracking. If the drift check failed or the sentence presentation was unable to be triggered by participant gaze at the starting box, the five-point calibration and validation test was conducted again.</p></sec></sec><sec id="S22"><title>Eye movement data analysis</title><p id="P50">We used the EyeLink built-in algorithms to parse eye movement events based on the online detection of saccade onset using the following parameters: the motion threshold as 0.1°, the velocity threshold as 30°/second, and the acceleration threshold as 8,000°/second<sup><xref ref-type="bibr" rid="R2">2</xref></sup>. This conservative setting is suggested by the EyeLink user manual for reading studies where saccades are prominent, as this parameter setup can prevent false saccade detections and reduce the number of micro-saccades.</p><p id="P51">Fixation onset events were extracted from the EyeLink output file using custom-made scripts, which were parsed using the parameters described above. A fixation event was assigned to a given word if the averaged x and y positions of this fixation landed in the area of this word and the space to the left of it, because the optimal landing position is slightly left within a word. For further eye movement data analysis, only the fixation that first landed on a given word was selected; i.e., the first fixations. First fixation durations that were shorter than 0.08 second or longer than 1 second were viewed as outliers and discarded from further analysis. For each participant, the averaged first fixation durations for the pre-target, target, and post-target words were obtained separately for the low and high-lexical frequency condition of the target words. Afterwards, dependent sample <italic>t</italic>-tests (pairwise, two-sided) were conducted separately on the first fixation durations of pre-target, target, and post-target words.</p></sec><sec id="S23"><title>MEG data analysis</title><p id="P52">The MEG data analysis was conducted using MATLAB R2020a (Mathworks Inc, USA), incorporating the FieldTrip toolbox (version 20200220; Oostenveld et al., 2011), the FLUX MEG analysis pipeline 67, and custom-made scripts.</p><sec id="S24"><title>Pre-processing</title><p id="P53">First, malfunctioning sensors were identified through visual inspection, based on excessive high frequency noise compared to other sensors, and then excluded from data analysis (0 to 2 sensors excluded per participant). The MEG data were then band-pass filtered from 0.5 to 100 Hz using phase-preserving, two-pass Butterworth filters. To factor out slow drifts, the data were detrended. In order to remove oculomotor and heartbeat related artefacts, the MEG data were decomposed using an independent component analysis (ICA)<sup><xref ref-type="bibr" rid="R68">68</xref></sup>. The number of components matched the number of valid MEG sensors (up to 306). Since independent components were ranked by their contribution to explaining the data, we visually inspected only the first 100 components for each participant. We removed only components related to blinks, eye movements, and heartbeat, based on visual inspection of their topographies, power spectrum, and time course (for tutorial see Fieldtrip<sup><xref ref-type="bibr" rid="R66">66</xref></sup>; we excluded 2 to 5 components per participant).</p><p id="P54">MEG and eye movement data were co-registered by aligning triggers from the MEG system and the EyeLink device. Afterwards, eye movement events were used to segment the MEG data. MEG data were segmented from -0.5 to 0.5 seconds relative to the onset of the first fixations on the pre-target, target, and post-target words. Segments with extreme fixation durations, i.e., shorter than 0.08 seconds or longer than 1 second, were deemed outliers and excluded from further analysis. Additionally, we segmented 1 second long baseline data, spanning from 0 to 1 second relative to the onset of the fixation cross before the sentence presentation. Through visual inspection of all segments in a condition blind fashion, segments that were contaminated by muscle or movement artefacts were identified and removed from further analysis.</p></sec><sec id="S25"><title>Coherence calculation</title><p id="P55">The brain responses to flickering target and post-target words were quantified separately by calculating coherence between the MEG sensors and the corresponding photodiode channel (the tagging signal). Amplitudes of the photodiode channel were normalized within each segment to standardise photodiode values across all segments and participants. To estimate the time-resolved coherence spectrum for a given frequency of interest, we first filtered the segments using Hamming-tapered Butterworth bandpass filters (4<sup>th</sup> order, phase preserving, two-pass), with a bandwidth of ± 5 Hz around the centre frequency point. Then, the Hilbert transform was applied to obtain analytic signals of the filtered narrow band data. Afterwards, the analytic signals were used to estimate the coherence at the frequency of interest (<xref ref-type="disp-formula" rid="FD2">Equation 2</xref>): <disp-formula id="FD2"><mml:math id="M2"><mml:mrow><mml:mi>coh</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:msub><mml:mo>∅</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p id="P56">where <italic>n</italic> is the number of segments. For the time point <italic>t</italic> in the segment <italic>j, m<sub>x</sub>(t)</italic> and <italic>m<sub>y</sub>(t)</italic> are the time-varying magnitude of the analytic signals from a MEG sensor (<italic>x</italic>) and the photodiode (<italic>y</italic>) respectively, ∅<sub><italic>xy</italic></sub>(<italic>t</italic>) is the phase difference as a function of time (for detailed description, please see <sup><xref ref-type="bibr" rid="R69">69</xref></sup>.</p><p id="P57">This coherence analysis was repeated for frequencies of interest from 50 to 75 Hz in a step of 1 Hz to obtain a complete spectrum of time-resolved coherence.</p></sec><sec id="S26"><title>Selection for RIFT response sensors</title><p id="P58">Not all MEG sensors respond to visual flickers, so as the first step of analysing MEG data, RIFT response sensors were selected to enhance analysis sensitivity. This sensor selection procedure was carried out separately for fixation intervals of pre-target, target, and post-target words (for topographies of all three types of sensors see <xref ref-type="fig" rid="F5">Figure 5</xref> panel i). MEG sensors that showed significantly stronger coherence at <italic>f<sub>t</sub></italic> during pre-target fixation intervals (when target words were frequency-tagged at <italic>f<sub>t</sub></italic>) compared with baseline intervals (when nothing was frequency-tagged) were selected as the pre-target RIFT response sensors. Similarly, sensors that showed significantly stronger coherence at <italic>f<sub>p</sub></italic> during target fixation intervals (when post-target words were frequency-tagged at <italic>f<sub>p</sub></italic>) compared with baseline intervals were selected as the target RIFT response sensors. Finally, in order to investigate whether attention would regress back covertly to the previous difficult word (i.e., the low frequency target words), we also selected post-target RIFT response sensors, which showed significantly stronger coherence at <italic>f<sub>t</sub></italic> during post-target fixation intervals (when target words were frequency-tagged at <italic>f<sub>t</sub></italic>) compared with baseline intervals.</p><p id="P59">To estimate statistical significance during the RIFT response sensors selection procedure, we used a non-parametric Monte-Carlo method <sup><xref ref-type="bibr" rid="R70">70</xref></sup> to compare the coherence difference between the frequency tagging intervals versus the no-tagging intervals (the baseline segments). The statistical comparison was conducted separately for the three types of frequency tagging intervals — the pre-target, target and post-target fixation intervals. Because previous RIFT studies with a natural reading task only observed robust tagging responses to visual flicker from the visual cortex <sup><xref ref-type="bibr" rid="R40">40</xref>,<xref ref-type="bibr" rid="R41">41</xref></sup>, this sensor selection procedure was restricted to MEG sensors in the visual cortex (52 planar sensors).</p><p id="P60">Frequency tagging intervals include all trials, regardless of the lexical conditions of the target words in the experimental design. In the non-parametric statistical analysis here, frequency tagging intervals (the pre-target or target or post-target fixation intervals) and baseline intervals were treated as two conditions. For each combination of the MEG sensor and the photodiode channel, coherence at the frequency of interest was estimated over trials for the frequency tagging and baseline conditions separately. Afterwards, we calculated the z-statistic value for the coherence difference between these two conditions using the following equation (for details please see <sup><xref ref-type="bibr" rid="R70">70</xref></sup>) (<xref ref-type="disp-formula" rid="FD3">Equation 3</xref>): <disp-formula id="FD3"><mml:math id="M3"><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>tanh</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>coh</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mtext>bias</mml:mtext></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>tanh</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>coh</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mtext>bias</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msub><mml:mrow><mml:mtext>bias</mml:mtext></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mtext>bias</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mrow><mml:mtext>bias</mml:mtext></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mtext>bias</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p id="P61">where coh<sub>1</sub> and coh<sub>2</sub> denote the coherence value for the frequency tagging and baseline conditions, bias<sub>1</sub> and bias<sub>2</sub> is the term used to correct for the bias from trial numbers of the frequency tagging (<italic>n</italic><sub>1</sub>) and baseline conditions (<italic>n</italic><sub>2</sub>).</p><p id="P62">After obtaining the z statistic value for the empirical coherence difference (i.e., without shuffling trial labels), we ran a permutation procedure to estimate the statistical significance of this comparison. We randomly shuffled trial labels between the frequency tagging and baseline conditions for 2,000 times. During each permutation, coherence was computed for both conditions using trials with the shuffled labels, then entered into <xref ref-type="disp-formula" rid="FD3">Equation 3</xref> to obtain a z score for the coherence difference. After the 2,000 iterations of permutations were performed, all the z-values were sorted from the minimum value to the maximum value, establishing the null distribution for statistical analysis. Because we had a prior direction of the comparison, i.e., a RIFT response sensor was supposed to have stronger coherence for the frequency tagging condition compared with the baseline condition, the statistical test was one-sided rather than two-sided. If the z-value of the empirical coherence difference exceeding the 99<sup>th</sup> percentile of the null distribution, this sensor was selected as a RIFT response sensor (right-sided, <italic>p</italic> = .01).</p></sec><sec id="S27"><title>Coherence comparison between experimental conditions</title><p id="P63">The coherence comparison between the two experimental conditions (the low and high-lexical frequency of target words) were only conducted for participants with RIFT response sensors. For each participant, this coherence comparison was performed separately for the fixation intervals of pre-target, target, and post-target words using their corresponding RIFT response sensors. The time-resolved coherence spectrum, derived from <xref ref-type="disp-formula" rid="FD2">Equation 2</xref>, were averaged across the RIFT responses sensors, so that each participant had one coherence spectrum for each condition during each frequency tagging interval. Please note that to avoid any bias from unequal trial numbers, we randomly discarded the redundant trials from the condition with more trials, ensuring an equal number of trials per condition were entered the coherence analysis.</p><p id="P64">To compare the time-resolved coherence between two conditions on the group level, we conducted a cluster-based Monte-Carlo permutation test 70. This non-parametric permutation test was performed separately for the three tagging intervals — the fixation intervals of pretarget, target and post-target words. We pre-defined the following frequency window and time window for the permutation test. A frequency window of ± 3 Hz around the frequency of interest was selected because the difference between these two tagging frequencies was 5 Hz; a time window of 0 to 0.2 seconds around the fixation onset of the word of interest was selected because the averaged fixation duration of a given word was around 0.2 seconds.</p><p id="P65">During each permutation iteration, trial labels between the two experimental conditions were randomly shuffled across participants. This label shuffling could break down the corresponding relation between the coherence values and the conditions, which was essential in creating the null distribution on the group level. Dependent samples <italic>t</italic>-tests were conducted between the coherence values from the two conditions (within the pre-defined frequency and time window, see above) to obtain an array of <italic>t</italic>-values. Then, a significance threshold of 0.05 (two-sided) was applied to cluster the <italic>t</italic>-values. The <italic>t</italic>-values within each cluster were added up to get a summed <italic>t</italic>-value, and then the maximum summed <italic>t</italic>-value was used to create the null distribution (the “maxsum” method of cluster-based permutation test). After 5,000 iterations of permutation, a null distribution of maximum summed <italic>t</italic>-values was created. We then sorted the distribution from the smallest value to the biggest value, and the summed <italic>t</italic>-values at the position of 2.5% and 97.5% were selected as the critical values at the significance level of 0.05 (two-sided). Next, we calculated the dependent samples <italic>t</italic>-test using the empirical data and applied a significance threshold of 0.05 (two-sided) to cluster the <italic>t</italic>-values. By comparing the <italic>t</italic>-values clusters against the critical values from the null distribution, only the significant clusters remained, as outlined in <xref ref-type="fig" rid="F5">Figure 5</xref> row <bold>v</bold>.</p></sec><sec id="S28"><title>Simultaneous coherence analysis during the foveal and parafoveal reading</title><p id="P66">In order to estimate the dynamics of attention allocated to process the target (frequency tagged at <italic>f<sub>t</sub></italic>) and post-target words (frequency tagged at <italic>f<sub>p</sub></italic>) during reading, we calculated tagging responses with respect to <italic>f<sub>t</sub></italic> and <italic>f<sub>p</sub></italic> in three time-windows: the fixation intervals of pre-target, target, and the post-target words.</p><p id="P67">First, using the procedure described in the section of “Selection for the RIFT response sensors”, we selected RIFT response sensors at two tagging frequency bands (<italic>f<sub>t</sub></italic> and <italic>f<sub>p</sub></italic>) during three fixation intervals (pre-target, target, and post-target fixations). This sensor selection procedure was conducted over all trials, irrespective of the experimental conditions. Topographies for these six types of RIFT response sensors are provided in <xref ref-type="supplementary-material" rid="SD1">Supplementary figure 1</xref>. Notably, the topographies of pre-target RIFT sensors at <italic>f<sub>t</sub></italic>, target RIFT sensors at <italic>f<sub>p</sub></italic>, and post-target RIFT sensors at <italic>f<sub>t</sub></italic> were the same as displayed in <xref ref-type="fig" rid="F5">Figure 5</xref>. The selection accounted for frequency-specific interests during each fixation interval to ensure a precise and accurate tagging response detection.</p><p id="P68">Then, using these selected RIFT sensors, we calculated tagging response curves for each experimental condition following <xref ref-type="disp-formula" rid="FD2">Equation 2</xref>. For each type of fixation interval and each condition, we obtained a coherence curve at the frequency of interest (with a bandwidth of 5Hz in the Hilbert filtering). Coherence curves were averaged across RIFT sensors to derive participant-specific curves, which were then averaged across participants. <xref ref-type="supplementary-material" rid="SD1">Supplementary figure 2</xref> shows the averaged coherence curves for each fixation interval and each experimental condition. As a no-tagging baseline, we calculated tagging responses during the fixation-cross intervals before the sentence presentation, using post-target RIFT sensors. Since no flickering occurred during baseline intervals, the choice of RIFT sensors from any fixation intervals did not affect the coherence values, which reflected only chance or noise levels.</p><p id="P69">During target fixation intervals, target words were frequency tagged at <italic>f<sub>t</sub></italic> while post-target words were frequency tagged at <italic>f<sub>p</sub></italic>. Thus, tagging responses from all trials at <italic>f<sub>t</sub></italic> and <italic>f<sub>p</sub></italic> were considered as measures for attention allocated to the foveal and parafoveal vision. To assess the significance of tagging responses during foveal and parafoveal processing, we conducted group-level paired <italic>t</italic>-tests, comparing coherence values for target fixation intervals against baseline intervals. The coherence values were averaged over a 0.2 second time window aligned with the target word fixation onset or the baseline presentation onset.</p></sec></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Materials</label><media xlink:href="EMS206028-supplement-Supplementary_Materials.pdf" mimetype="application" mime-subtype="pdf" id="d107aAcEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S29"><title>Acknowledgements</title><p>This work was supported by a Leverhulme Early Career Fellow to Y.P. (grant number ECF-2023-626) and a Wellcome Trust Discovery Award to O.J. (grant number 207550).</p></ack><sec id="S30" sec-type="data-availability"><title>Data availability</title><p id="P70">We have deposited the following data in the current study on figshare (<ext-link ext-link-type="uri" xlink:href="https://figshare.com/projects/Attention_in_Reading/250316">https://figshare.com/projects/Attention_in_Reading/250316</ext-link>): the raw MEG data, the epoch data after pre-processing, the raw EyeLink files, the Psychotoolbox data.</p></sec><sec id="S31" sec-type="data-availability"><title>Code availability</title><p id="P71">The experiment presentation scripts (Psychtoolbox-3), statistics scripts (R), data analysis scripts (MATLAB) are available on GitHub</p><p id="P72">(<ext-link ext-link-type="uri" xlink:href="https://github.com/yalipan666/Attention_in_Reading">https://github.com/yalipan666/Attention_in_Reading</ext-link>).</p></sec><fn-group><fn id="FN1" fn-type="con"><p id="P73"><bold>Author Contributions</bold></p><p id="P74">Y.P. devised and designed the study, programmed and conducted the experiment, Y.P. carried out the analyses with assistance from O.J., S.F., J.S., and K.D.F. Y.P. wrote the first draft. Y.P., S.F., J.S., K.D.F., and O.J. edited the manuscript together.</p></fn><fn id="FN2" fn-type="conflict"><p id="P75"><bold>Competing interests</bold></p><p id="P76">The authors declare no competing interests.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rayner</surname><given-names>K</given-names></name></person-group><article-title>Eye movements and attention in reading, scene perception, and visual search</article-title><source>Q J Exp Psychol</source><year>2009</year><volume>62</volume><fpage>1457</fpage><lpage>1506</lpage><pub-id pub-id-type="pmid">19449261</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rayner</surname><given-names>K</given-names></name><name><surname>Duffy</surname><given-names>SA</given-names></name></person-group><article-title>Lexical complexity and fixation times in reading: Effects of word frequency, verb complexity, and lexical ambiguity</article-title><source>Mem Cognit</source><year>1986</year><volume>14</volume><fpage>191</fpage><lpage>201</lpage><pub-id pub-id-type="pmid">3736392</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname><given-names>SJ</given-names></name></person-group><article-title>Eye movement control during reading: Effects of word frequency and orthographic familiarity</article-title><source>J Exp Psychol Hum Percept Perform</source><year>2008</year><volume>34</volume><fpage>205</fpage><lpage>223</lpage><pub-id pub-id-type="pmid">18248149</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Starr</surname><given-names>MS</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name></person-group><article-title>Eye movements during reading: some current controversies</article-title><source>Trends Cogn Sci</source><year>2001</year><volume>5</volume><fpage>156</fpage><lpage>163</lpage><pub-id pub-id-type="pmid">11287269</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname><given-names>SJ</given-names></name><name><surname>Johnson</surname><given-names>RL</given-names></name><name><surname>Liversedge</surname><given-names>SP</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name></person-group><article-title>Eye movements when reading transposed text: The importance of word-beginning letters</article-title><source>J Exp Psychol Hum Percept Perform</source><year>2008</year><volume>34</volume><fpage>1261</fpage><lpage>1276</lpage><pub-id pub-id-type="pmcid">PMC2662926</pub-id><pub-id pub-id-type="pmid">18823209</pub-id><pub-id pub-id-type="doi">10.1037/0096-1523.34.5.1261</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>CC</given-names></name><name><surname>Perea</surname><given-names>M</given-names></name><name><surname>Pollatsek</surname><given-names>A</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name></person-group><article-title>Previewing the neighborhood: The role of orthographic neighbors as parafoveal previews in reading</article-title><source>J Exp Psychol Hum Percept Perform</source><year>2006</year><volume>32</volume><fpage>1072</fpage><lpage>1082</lpage><pub-id pub-id-type="pmid">16846298</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashby</surname><given-names>J</given-names></name><name><surname>Treiman</surname><given-names>R</given-names></name><name><surname>Kessler</surname><given-names>B</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name></person-group><article-title>Vowel processing during silent reading: Evidence from eye movements</article-title><source>J Exp Psychol Learn Mem Cogn</source><year>2006</year><volume>32</volume><fpage>416</fpage><lpage>424</lpage><pub-id pub-id-type="pmid">16569156</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chace</surname><given-names>KH</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name><name><surname>Well</surname><given-names>AD</given-names></name></person-group><article-title>Eye movements and phonological parafoveal preview: Effects of reading skill</article-title><source>Can J Exp Psychol</source><year>2005</year><volume>59</volume><fpage>209</fpage><lpage>217</lpage><pub-id pub-id-type="pmid">16248500</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miellet</surname><given-names>S</given-names></name><name><surname>Sparrow</surname><given-names>L</given-names></name></person-group><article-title>Phonological codes are assembled before word fixation: Evidence from boundary paradigm in sentence reading</article-title><source>Brain Lang</source><year>2004</year><volume>90</volume><fpage>299</fpage><lpage>310</lpage><pub-id pub-id-type="pmid">15172547</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rayner</surname><given-names>K</given-names></name><name><surname>Sereno</surname><given-names>SC</given-names></name><name><surname>Lesch</surname><given-names>MF</given-names></name><name><surname>Pollatsek</surname><given-names>A</given-names></name></person-group><article-title>Phonological Codes Are Automatically Activated During Reading: Evidence From an Eye Movement Priming Paradigm</article-title><source>Psychol Sci</source><year>1995</year><volume>6</volume><fpage>26</fpage><lpage>32</lpage></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schotter</surname><given-names>ER</given-names></name></person-group><article-title>Synonyms provide semantic preview benefit in English</article-title><source>J Mem Lang</source><year>2013</year><volume>69</volume><fpage>619</fpage><lpage>633</lpage><pub-id pub-id-type="pmcid">PMC3859233</pub-id><pub-id pub-id-type="pmid">24347813</pub-id><pub-id pub-id-type="doi">10.1016/j.jml.2013.09.002</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schotter</surname><given-names>ER</given-names></name><name><surname>Jia</surname><given-names>A</given-names></name></person-group><article-title>Semantic and plausibility preview benefit effects in English: Evidence from eye movements</article-title><source>J Exp Psychol Learn Mem Cogn</source><year>2016</year><volume>42</volume><fpage>1839</fpage><lpage>1866</lpage><pub-id pub-id-type="pmcid">PMC5085893</pub-id><pub-id pub-id-type="pmid">27123754</pub-id><pub-id pub-id-type="doi">10.1037/xlm0000281</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yan</surname><given-names>M</given-names></name><name><surname>Richter</surname><given-names>EM</given-names></name><name><surname>Shu</surname><given-names>H</given-names></name><name><surname>Kliegl</surname><given-names>R</given-names></name></person-group><article-title>Readers of Chinese extract semantic information from parafoveal words</article-title><source>Psychon Bull Rev</source><year>2009</year><volume>16</volume><fpage>561</fpage><lpage>566</lpage><pub-id pub-id-type="pmid">19451385</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hohenstein</surname><given-names>S</given-names></name><name><surname>Klieg</surname><given-names>R</given-names></name></person-group><article-title>Semantic preview benefit during reading</article-title><source>J Exp Psychol Learn Mem Cogn</source><year>2014</year><volume>40</volume><fpage>166</fpage><lpage>190</lpage><pub-id pub-id-type="pmid">23895448</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Angele</surname><given-names>B</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name></person-group><article-title>Parafoveal Processing of Word n + 2 During Reading: Do the Preceding Words Matter?</article-title><source>J Exp Psychol Hum Percept Perform</source><year>2011</year><volume>37</volume><fpage>1210</fpage><lpage>1220</lpage><pub-id pub-id-type="pmid">21553996</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Angele</surname><given-names>B</given-names></name><name><surname>Slattery</surname><given-names>TJ</given-names></name><name><surname>Yang</surname><given-names>J</given-names></name><name><surname>Kliegl</surname><given-names>R</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name></person-group><article-title>Parafoveal processing in reading: Manipulating n+1 and n+2 previews simultaneously</article-title><source>Vis Cogn</source><year>2008</year><volume>16</volume><fpage>697</fpage><lpage>707</lpage><pub-id pub-id-type="pmcid">PMC2677831</pub-id><pub-id pub-id-type="pmid">19424452</pub-id><pub-id pub-id-type="doi">10.1080/13506280802009704</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henderson</surname><given-names>JM</given-names></name><name><surname>Ferreira</surname><given-names>F</given-names></name></person-group><article-title>Eye movement control during reading: fixation measures reflect foveal but not parafoveal processing difficulty</article-title><source>Can J Exp Psychol</source><year>1993</year><volume>47</volume><fpage>201</fpage><lpage>221</lpage><pub-id pub-id-type="pmid">8364530</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Degno</surname><given-names>F</given-names></name><etal/></person-group><article-title>Parafoveal previews and lexical frequency in natural reading : Evidence from eye movements and Fixation-related potentials</article-title><source>J Exp Psychol Gen</source><year>2019</year><volume>148</volume><fpage>453</fpage><lpage>474</lpage><pub-id pub-id-type="pmcid">PMC6388670</pub-id><pub-id pub-id-type="pmid">30335444</pub-id><pub-id pub-id-type="doi">10.1037/xge0000494</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naveed</surname><given-names>T</given-names></name><name><surname>Evans</surname><given-names>A</given-names></name><name><surname>Ferrante</surname><given-names>O</given-names></name><name><surname>Pan</surname><given-names>Y</given-names></name></person-group><article-title>Emotional Content Interacts with Word Frequency During Parafoveal Reading</article-title><year>2024</year><comment>Available SSRN 5002588</comment><pub-id pub-id-type="doi">10.2139/SSRN.5002588</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schotter</surname><given-names>ER</given-names></name><name><surname>Milligan</surname><given-names>S</given-names></name><name><surname>Estevez</surname><given-names>VM</given-names></name></person-group><article-title>Event-related potentials show that parafoveal vision is insufficient for semantic integration</article-title><source>Psychophysiology</source><year>2023</year><elocation-id>e14246</elocation-id><pub-id pub-id-type="pmid">36811523</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Milligan</surname><given-names>S</given-names></name><name><surname>Nestor</surname><given-names>B</given-names></name><name><surname>Antúnez</surname><given-names>M</given-names></name><name><surname>Schotter</surname><given-names>ER</given-names></name></person-group><article-title>Out of Sight, Out of Mind: Foveal Processing is Necessary for Semantic Integration of Words into Sentence Context</article-title><source>J Exp Psychol Hum Percept Perform</source><year>2023</year><pub-id pub-id-type="pmid">37261774</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Antúnez</surname><given-names>M</given-names></name><name><surname>Milligan</surname><given-names>S</given-names></name><name><surname>Andrés Hernández-Cabrera</surname><given-names>J</given-names></name><name><surname>Barber</surname><given-names>HA</given-names></name><name><surname>Schotter</surname><given-names>ER</given-names></name></person-group><article-title>Semantic parafoveal processing in natural reading: Insight from fixation-related potentials &amp; eye movements</article-title><source>Psychophysiology</source><year>2022</year><volume>59</volume><elocation-id>e13986</elocation-id><pub-id pub-id-type="pmid">34942021</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barber</surname><given-names>HA</given-names></name><name><surname>Doñamayor</surname><given-names>N</given-names></name><name><surname>Kutas</surname><given-names>M</given-names></name><name><surname>Münte</surname><given-names>T</given-names></name></person-group><article-title>Parafoveal N400 effect during sentence reading</article-title><source>Neurosci Lett</source><year>2010</year><volume>479</volume><fpage>152</fpage><lpage>156</lpage><pub-id pub-id-type="pmcid">PMC4096702</pub-id><pub-id pub-id-type="pmid">20580772</pub-id><pub-id pub-id-type="doi">10.1016/j.neulet.2010.05.053</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Payne</surname><given-names>BR</given-names></name><name><surname>Stites</surname><given-names>MC</given-names></name><name><surname>Federmeier</surname><given-names>KD</given-names></name></person-group><article-title>Event-related brain potentials reveal how multiple aspects of semantic processing unfold across parafoveal and foveal vision during sentence reading</article-title><source>Psychophysiology</source><year>2019</year><volume>56</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="pmcid">PMC6879358</pub-id><pub-id pub-id-type="pmid">31274200</pub-id><pub-id pub-id-type="doi">10.1111/psyp.13432</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stites</surname><given-names>MC</given-names></name><name><surname>Payne</surname><given-names>BR</given-names></name><name><surname>Federmeier</surname><given-names>KD</given-names></name></person-group><article-title>Getting ahead of yourself: Parafoveal word expectancy modulates the N400 during sentence reading</article-title><source>Cogn Affect Behav Neurosci</source><year>2017</year><volume>17</volume><fpage>475</fpage><lpage>490</lpage><pub-id pub-id-type="pmcid">PMC5603229</pub-id><pub-id pub-id-type="pmid">28101830</pub-id><pub-id pub-id-type="doi">10.3758/s13415-016-0492-6</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barber</surname><given-names>HA</given-names></name><name><surname>van der Meij</surname><given-names>M</given-names></name><name><surname>Kutas</surname><given-names>M</given-names></name></person-group><article-title>An electrophysiological analysis of contextual and temporal constraints on parafoveal word processing</article-title><source>Psychophysiology</source><year>2013</year><volume>50</volume><fpage>48</fpage><lpage>59</lpage><pub-id pub-id-type="pmcid">PMC4096715</pub-id><pub-id pub-id-type="pmid">23153323</pub-id><pub-id pub-id-type="doi">10.1111/j.1469-8986.2012.01489.x</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>N</given-names></name><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Kornrumpf</surname><given-names>F</given-names></name><name><surname>Sommer</surname><given-names>W</given-names></name><name><surname>Dimigen</surname><given-names>O</given-names></name></person-group><article-title>Parafoveal and foveal N400 effects in natural reading: A timeline of semantic processing from fixation-related potentials</article-title><source>Psychophysiology</source><year>2024</year><elocation-id>e14524</elocation-id><pub-id pub-id-type="pmid">38297818</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>C</given-names></name><name><surname>Midgley</surname><given-names>KJ</given-names></name><name><surname>Holcomb</surname><given-names>PJ</given-names></name></person-group><article-title>ERPs reveal how semantic and syntactic processing unfold across parafoveal and foveal vision during sentence comprehension</article-title><source>Lang Cogn Neurosci</source><year>2023</year><volume>38</volume><fpage>88</fpage><lpage>104</lpage><pub-id pub-id-type="pmcid">PMC9916175</pub-id><pub-id pub-id-type="pmid">36776698</pub-id><pub-id pub-id-type="doi">10.1080/23273798.2022.2091150</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snell</surname><given-names>J</given-names></name><name><surname>Yeaton</surname><given-names>J</given-names></name><name><surname>Mirault</surname><given-names>J</given-names></name><name><surname>Grainger</surname><given-names>J</given-names></name></person-group><article-title>Parallel word reading revealed by fixation-related brain potentials</article-title><source>Cortex</source><year>2023</year><volume>162</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="pmid">36948090</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Degno</surname><given-names>F</given-names></name><name><surname>Liversedge</surname><given-names>SP</given-names></name></person-group><article-title>Eye movements and fixation-related potentials in reading: A review</article-title><source>Vision</source><year>2020</year><volume>4</volume><fpage>11</fpage><pub-id pub-id-type="pmcid">PMC7157570</pub-id><pub-id pub-id-type="pmid">32028566</pub-id><pub-id pub-id-type="doi">10.3390/vision4010011</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Payne</surname><given-names>BR</given-names></name><name><surname>Federmeier</surname><given-names>KD</given-names></name></person-group><article-title>Event-related brain potentials reveal age-related changes in parafoveal-foveal integration during sentence processing</article-title><source>Neuropsychologia</source><year>2017</year><volume>106</volume><fpage>358</fpage><lpage>370</lpage><pub-id pub-id-type="pmcid">PMC5720378</pub-id><pub-id pub-id-type="pmid">28987909</pub-id><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2017.10.002</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Federmeier</surname><given-names>KD</given-names></name></person-group><article-title>Connecting and considering: Electrophysiology provides insights into comprehension</article-title><source>Psychophysiology</source><year>2022</year><volume>59</volume><elocation-id>e13940</elocation-id><pub-id pub-id-type="pmcid">PMC9009268</pub-id><pub-id pub-id-type="pmid">34520568</pub-id><pub-id pub-id-type="doi">10.1111/psyp.13940</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duecker</surname><given-names>K</given-names></name><name><surname>Gutteling</surname><given-names>TP</given-names></name><name><surname>Herrmann</surname><given-names>CS</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name></person-group><article-title>No Evidence for Entrainment: Endogenous Gamma Oscillations and Rhythmic Flicker Responses Coexist in Visual Cortex</article-title><source>J Neurosci</source><year>2021</year><volume>41</volume><fpage>6684</fpage><lpage>6698</lpage><pub-id pub-id-type="pmcid">PMC8336697</pub-id><pub-id pub-id-type="pmid">34230106</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3134-20.2021</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhigalov</surname><given-names>A</given-names></name><name><surname>Herring</surname><given-names>JD</given-names></name><name><surname>Herpers</surname><given-names>J</given-names></name><name><surname>Bergmann</surname><given-names>TO</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name></person-group><article-title>Probing cortical excitability using rapid frequency tagging</article-title><source>Neuroimage</source><year>2019</year><volume>195</volume><fpage>59</fpage><lpage>66</lpage><pub-id pub-id-type="pmcid">PMC6547046</pub-id><pub-id pub-id-type="pmid">30930309</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.03.056</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferrante</surname><given-names>O</given-names></name><name><surname>Zhigalov</surname><given-names>A</given-names></name><name><surname>Hickey</surname><given-names>C</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name></person-group><article-title>Statistical Learning of Distractor Suppression Down-regulates Pre-Stimulus Neural Excitability in Early Visual Cortex</article-title><source>J Neurosci</source><year>2023</year><volume>43</volume><fpage>2190</fpage><lpage>2198</lpage><pub-id pub-id-type="pmcid">PMC10039740</pub-id><pub-id pub-id-type="pmid">36801825</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1703-22.2022</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gutteling</surname><given-names>TP</given-names></name><name><surname>Sillekens</surname><given-names>L</given-names></name><name><surname>Lavie</surname><given-names>N</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name></person-group><article-title>Alpha oscillations reflect suppression of distractors with increased perceptual load</article-title><source>Prog Neurobiol</source><year>2022</year><volume>214</volume><elocation-id>102285</elocation-id><pub-id pub-id-type="pmcid">PMC7615060</pub-id><pub-id pub-id-type="pmid">35533812</pub-id><pub-id pub-id-type="doi">10.1101/2021.04.13.439637</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brickwedde</surname><given-names>M</given-names></name><name><surname>Bezsudnova</surname><given-names>Y</given-names></name><name><surname>Kowalczyk</surname><given-names>A</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name><name><surname>Zhigalov</surname><given-names>A</given-names></name></person-group><article-title>Application of rapid invisible frequency tagging for brain computer interfaces</article-title><source>J Neurosci Methods</source><year>2022</year><volume>382</volume><elocation-id>109726</elocation-id><pub-id pub-id-type="pmcid">PMC7615063</pub-id><pub-id pub-id-type="pmid">36228894</pub-id><pub-id pub-id-type="doi">10.1016/j.jneumeth.2022.109726</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marshall</surname><given-names>TR</given-names></name><name><surname>Ruesseler</surname><given-names>M</given-names></name><name><surname>Hunt</surname><given-names>LT</given-names></name><name><surname>O’Reilly</surname><given-names>JX</given-names></name></person-group><article-title>The representation of priors and decisions in the human parietal cortex</article-title><source>PLOS Biol</source><year>2024</year><volume>22</volume><elocation-id>e3002383</elocation-id><pub-id pub-id-type="pmcid">PMC10824454</pub-id><pub-id pub-id-type="pmid">38285671</pub-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3002383</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arora</surname><given-names>K</given-names></name><name><surname>Gayet</surname><given-names>S</given-names></name><name><surname>Kenemans</surname><given-names>JL</given-names></name><name><surname>Van Der Stigchel</surname><given-names>S</given-names></name><name><surname>Chota</surname><given-names>S</given-names></name></person-group><article-title>Rapid Invisible Frequency Tagging (RIFT) in a novel setup with EEG</article-title><source>bioRxiv</source><year>2024</year></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>Y</given-names></name><name><surname>Frisson</surname><given-names>S</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name></person-group><article-title>Neural evidence for lexical parafoveal processing</article-title><source>Nat Commun</source><year>2021</year><volume>12</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="pmcid">PMC8413448</pub-id><pub-id pub-id-type="pmid">34475391</pub-id><pub-id pub-id-type="doi">10.1038/s41467-021-25571-x</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>Y</given-names></name><name><surname>Frisson</surname><given-names>S</given-names></name><name><surname>Federmeier</surname><given-names>KD</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name></person-group><article-title>Early parafoveal semantic integration in natural reading</article-title><source>Elife</source><year>2024</year><volume>12</volume><pub-id pub-id-type="pmcid">PMC11226228</pub-id><pub-id pub-id-type="pmid">38968325</pub-id><pub-id pub-id-type="doi">10.7554/eLife.91327</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davis</surname><given-names>CJ</given-names></name></person-group><article-title>N-watch: A program for deriving neighborhood size and other psycholinguistic statistics</article-title><source>Behav Res Methods</source><year>2005</year><volume>37</volume><fpage>65</fpage><lpage>70</lpage><pub-id pub-id-type="pmid">16097345</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brothers</surname><given-names>T</given-names></name><name><surname>Hoversten</surname><given-names>LJ</given-names></name><name><surname>Traxler</surname><given-names>MJ</given-names></name></person-group><article-title>Looking back on reading ahead: No evidence for lexical parafoveal-on-foveal effects</article-title><source>J Mem Lang</source><year>2017</year><volume>96</volume><fpage>9</fpage><lpage>22</lpage></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reichle</surname><given-names>ED</given-names></name><name><surname>Pollatsek</surname><given-names>A</given-names></name><name><surname>Fisher</surname><given-names>DL</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name></person-group><article-title>Toward a model of eye movement control in reading</article-title><source>Psychol Rev</source><year>1998</year><volume>105</volume><fpage>125</fpage><lpage>157</lpage><pub-id pub-id-type="pmid">9450374</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reichle</surname><given-names>ED</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name><name><surname>Pollatsek</surname><given-names>A</given-names></name></person-group><article-title>The E-Z reader model of eye-movement control in reading: Comparisons to other models</article-title><source>Behav Brain Sci</source><year>2003</year><volume>26</volume><fpage>445</fpage><lpage>476</lpage><pub-id pub-id-type="pmid">15067951</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reichle</surname><given-names>ED</given-names></name><name><surname>Warren</surname><given-names>T</given-names></name><name><surname>McConnell</surname><given-names>K</given-names></name></person-group><article-title>Using E-Z reader to model the effects of higher level language processing on eye movements during reading</article-title><source>Psychonomic Bulletin and Review</source><year>2009</year><volume>16</volume><fpage>1</fpage><lpage>21</lpage><pub-id pub-id-type="pmcid">PMC2629133</pub-id><pub-id pub-id-type="pmid">19145006</pub-id><pub-id pub-id-type="doi">10.3758/PBR.16.1.1</pub-id></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reichle</surname><given-names>ED</given-names></name><name><surname>Tokowicz</surname><given-names>N</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Perfetti</surname><given-names>CA</given-names></name></person-group><article-title>Testing an assumption of the E-Z Reader model of eye-movement control during reading: Using event-related potentials to examine the familiarity check</article-title><source>Psychophysiology</source><year>2011</year><volume>48</volume><fpage>993</fpage><lpage>1003</lpage><pub-id pub-id-type="pmid">21261631</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reichle</surname><given-names>ED</given-names></name><name><surname>Pollatsek</surname><given-names>A</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name></person-group><article-title>E-Z Reader: A cognitive-control, serial-attention model of eye-movement behavior during reading</article-title><source>Cogn Syst Res</source><year>2006</year><volume>7</volume><fpage>4</fpage><lpage>22</lpage></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engbert</surname><given-names>R</given-names></name><name><surname>Longtin</surname><given-names>A</given-names></name><name><surname>Kliegl</surname><given-names>R</given-names></name></person-group><article-title>A dynamical model of saccade generation in reading based on spatially distributed lexical processing</article-title><source>Vision Res</source><year>2002</year><volume>42</volume><fpage>621</fpage><lpage>636</lpage><pub-id pub-id-type="pmid">11853779</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engbert</surname><given-names>R</given-names></name><name><surname>Nuthmann</surname><given-names>A</given-names></name><name><surname>Richter</surname><given-names>EM</given-names></name><name><surname>Kliegl</surname><given-names>R</given-names></name></person-group><article-title>Swift: A dynamical model of saccade generation during reading</article-title><source>Psychol Rev</source><year>2005</year><volume>112</volume><fpage>777</fpage><lpage>813</lpage><pub-id pub-id-type="pmid">16262468</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snell</surname><given-names>J</given-names></name><name><surname>Grainger</surname><given-names>J</given-names></name></person-group><article-title>Readers are parallel processors</article-title><source>Trends Cogn Sci</source><year>2019</year><volume>23</volume><fpage>537</fpage><lpage>546</lpage><pub-id pub-id-type="pmid">31138515</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snell</surname><given-names>J</given-names></name><name><surname>van Leipsig</surname><given-names>S</given-names></name><name><surname>Grainger</surname><given-names>J</given-names></name><name><surname>Meeter</surname><given-names>M</given-names></name></person-group><article-title>OB1-reader: A model of word recognition and eye movements in text reading</article-title><source>Psychol Rev</source><year>2018</year><volume>125</volume><fpage>969</fpage><lpage>984</lpage><pub-id pub-id-type="pmid">30080066</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snell</surname><given-names>J</given-names></name></person-group><article-title>PONG: A Computational Model of Visual Word Recognition Through Bihemispheric Activation</article-title><source>Psychol Rev</source><year>2025</year><volume>132</volume><fpage>505</fpage><lpage>527</lpage><pub-id pub-id-type="pmid">38407322</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jensen</surname><given-names>O</given-names></name><name><surname>Pan</surname><given-names>Y</given-names></name><name><surname>Frisson</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name></person-group><article-title>An oscillatory pipelining mechanism supporting previewing during visual exploration and reading</article-title><source>Trends Cogn Sci</source><year>2021</year><volume>25</volume><fpage>1033</fpage><lpage>1044</lpage><pub-id pub-id-type="pmcid">PMC7615059</pub-id><pub-id pub-id-type="pmid">34544653</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2021.08.008</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>SN</given-names></name><name><surname>McConkie</surname><given-names>GW</given-names></name></person-group><article-title>Eye movements during reading: a theory of saccade initiation times</article-title><source>Vision Res</source><year>2001</year><volume>41</volume><fpage>3567</fpage><lpage>3585</lpage><pub-id pub-id-type="pmid">11718796</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>McConkie</surname><given-names>GW</given-names></name><name><surname>Yang</surname><given-names>SN</given-names></name></person-group><chapter-title>How Cognition Affects Eye Movements During Reading</chapter-title><source>The Mind’s Eye: Cognitive and Applied Aspects of Eye Movement Research</source><publisher-loc>North-Holland</publisher-loc><year>2003</year><fpage>413</fpage><lpage>427</lpage><pub-id pub-id-type="doi">10.1016/B978-044451020-4/50023-2</pub-id></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>SN</given-names></name><name><surname>McConkie</surname><given-names>GW</given-names></name></person-group><article-title>Saccade generation during reading: Are words necessary?</article-title><source>Eur J Cogn Psychol</source><year>2004</year><volume>16</volume><fpage>226</fpage><lpage>261</lpage></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henderson</surname><given-names>JM</given-names></name><name><surname>Ferreira</surname><given-names>F</given-names></name></person-group><article-title>Effects of Foveal Processing Difficulty on the Perceptual Span in Reading: Implications for Attention and Eye Movement Control</article-title><source>J Exp Psychol Learn Mem Cogn</source><year>1990</year><volume>16</volume><fpage>417</fpage><lpage>429</lpage><pub-id pub-id-type="pmid">2140401</pub-id></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schroyens</surname><given-names>W</given-names></name><name><surname>Vitu</surname><given-names>F</given-names></name><name><surname>Brysbaert</surname><given-names>M</given-names></name><name><surname>D’Ydewalle</surname><given-names>G</given-names></name></person-group><article-title>Eye Movement Control during Reading: Foveal Load and Parafoveal Processing</article-title><source>Q J Exp Psychol Sect A</source><year>1999</year><volume>52</volume><fpage>1021</fpage><lpage>1046</lpage><pub-id pub-id-type="pmid">10605397</pub-id></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname><given-names>SJ</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name><name><surname>Liversedge</surname><given-names>SP</given-names></name></person-group><article-title>Eye movements and the modulation of parafoveal processing by foveal processing difficulty: A reexamination</article-title><source>Psychon Bull Rev</source><year>2005</year><volume>12</volume><fpage>891</fpage><lpage>896</lpage><pub-id pub-id-type="pmid">16524007</pub-id></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Veldre</surname><given-names>A</given-names></name><name><surname>Andrews</surname><given-names>S</given-names></name></person-group><article-title>How does foveal processing difficulty affect parafoveal processing during reading?</article-title><source>J Mem Lang</source><year>2018</year><volume>103</volume><fpage>74</fpage><lpage>90</lpage></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Antúnez</surname><given-names>M</given-names></name><name><surname>López-Pérez</surname><given-names>PJ</given-names></name><name><surname>Dampuré</surname><given-names>J</given-names></name><name><surname>Barber</surname><given-names>HA</given-names></name></person-group><article-title>Frequency-based foveal load modulates semantic parafoveal-on-foveal effects</article-title><source>J Neurolinguistics</source><year>2022</year><volume>63</volume><elocation-id>101071</elocation-id></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Payne</surname><given-names>BR</given-names></name><name><surname>Stites</surname><given-names>MC</given-names></name><name><surname>Federmeier</surname><given-names>KD</given-names></name></person-group><article-title>Out of the corner of my eye: Foveal semantic load modulates parafoveal processing in reading</article-title><source>J Exp Psychol Hum Percept Perform</source><year>2016</year><volume>42</volume><fpage>1839</fpage><lpage>1857</lpage><pub-id pub-id-type="pmcid">PMC5083148</pub-id><pub-id pub-id-type="pmid">27428778</pub-id><pub-id pub-id-type="doi">10.1037/xhp0000253</pub-id></element-citation></ref><ref id="R64"><label>64</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>Y</given-names></name><name><surname>Popov</surname><given-names>T</given-names></name><name><surname>Frisson</surname><given-names>S</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name></person-group><article-title>Saccades are locked to the phase of alpha oscillations during natural reading</article-title><source>PLOS Biol</source><year>2023</year><volume>21</volume><elocation-id>e3001968</elocation-id><pub-id pub-id-type="pmcid">PMC9882905</pub-id><pub-id pub-id-type="pmid">36649331</pub-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3001968</pub-id></element-citation></ref><ref id="R65"><label>65</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleiner</surname><given-names>M</given-names></name><name><surname>Brainard</surname><given-names>D</given-names></name><name><surname>Pelli</surname><given-names>D</given-names></name></person-group><source>What’s new in Psychtoolbox-3?</source><year>2007</year><volume>14</volume></element-citation></ref><ref id="R66"><label>66</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Schoffelen</surname><given-names>J-M</given-names></name></person-group><article-title>FieldTrip: Open Source Software for Advanced Analysis of MEG, EEG, and Invasive Electrophysiological Data</article-title><source>Comput Intell Neurosci</source><year>2011</year><volume>2011</volume><pub-id pub-id-type="pmcid">PMC3021840</pub-id><pub-id pub-id-type="pmid">21253357</pub-id><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id></element-citation></ref><ref id="R67"><label>67</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferrante</surname><given-names>O</given-names></name><etal/></person-group><article-title>FLUX: A pipeline for MEG analysis</article-title><source>Neuroimage</source><year>2022</year><volume>253</volume><elocation-id>119047</elocation-id><pub-id pub-id-type="pmcid">PMC9127391</pub-id><pub-id pub-id-type="pmid">35276363</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119047</pub-id></element-citation></ref><ref id="R68"><label>68</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ikeda</surname><given-names>S</given-names></name><name><surname>Toyama</surname><given-names>K</given-names></name></person-group><article-title>Independent component analysis for noisy data - MEG data analysis</article-title><source>Neural Networks</source><year>2000</year><volume>13</volume><fpage>1063</fpage><lpage>1074</lpage><pub-id pub-id-type="pmid">11156188</pub-id></element-citation></ref><ref id="R69"><label>69</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>MX</given-names></name></person-group><source>Analyzing neural time series data: Theory and practice</source><publisher-name>MIT Press</publisher-name><year>2014</year></element-citation></ref><ref id="R70"><label>70</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name></person-group><article-title>Nonparametric statistical testing of coherence differences</article-title><source>J Neurosci Methods</source><year>2007</year><volume>163</volume><fpage>161</fpage><lpage>175</lpage><pub-id pub-id-type="pmid">17395267</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Rapid invisible frequency tagging (RIFT) in a natural reading task.</title><p>Each trial began with a fixation-cross at the screen centre, followed by a “starting box” on the left. Fixating on the starting box would trigger the sentence onset. Participants (<italic>n</italic> = 42) were instructed to read 188 sentences silently and then gaze at the “ending box” at the screen bottom to trigger the sentence offset. Randomly, 25% of trails included a simple comprehension question requiring a button response. Each sentence contained one or two target words, either of low or high lexical frequency. The target words were unpredictable from the prior context and all sentences were plausible. RIFT was applied by continuously flickering rectangle patches underneath the target and post-target words at frequencies <italic>f<sub>t</sub></italic> and <italic>f<sub>p</sub></italic> respectively (60 and 65 Hz sine waves, balanced across participants). A Gaussian mask was applied over the patch to smooth the sharp luminance changes around the edges to reduce their visibility across saccades. Two discs were positioned at the bottom corners of the screen, with their luminance oscillating at sine waves of <italic>f<sub>t</sub></italic> and <italic>f<sub>p</sub></italic> separately throughout the entire trial. These discs were covered by two photodiodes, and, during sentence presentation, their luminance changes mirrored those of the patches beneath the flickering words. Eye tracker and MEG data were acquired simultaneously. ITI, inter-trial interval.</p></caption><graphic xlink:href="EMS206028-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>The research questions.</title><p><bold>(a)</bold>. When fixating on the target word, the foveal target word was frequency-tagged at <italic>f<sub>t</sub></italic> and the parafoveal post-target word was frequency-tagged at <italic>f<sub>p</sub></italic>. By measuring tagging responses associated with <italic>f<sub>t</sub></italic> and <italic>f<sub>p</sub></italic>, we could quantify the attention allocated to foveal and parafoveal words, which was used to investigate the parallel processing of multiple words in reading. (<bold>b)</bold>. When fixating on the pre-target word, tagging responses at <italic>f<sub>t</sub></italic> measured the parafoveal processing of the target word. By comparing the conditional difference, we could probe for lexical parafoveal processing. (<bold>c)</bold>. When fixating on the target word, tagging responses at <italic>f<sub>p</sub></italic> indicate the amount of attention allocated to the parafoveal post-target word. Conditional contrast of these tagging responses indicates the foveal load effect.</p></caption><graphic xlink:href="EMS206028-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Eye movement results.</title><p>The first fixation durations of pre-target (<bold>a</bold>), target (<bold>b</bold>), and post-target words (<bold>c</bold>) for conditions of low and high-lexical frequency target words. Each dot denotes the mean of one participant, and the bold horizontal line indicates the overall mean fixation duration over all participants (<italic>n</italic> = 42). Pairwise two-sided <italic>t</italic>-tests were conducted separately for first fixation durations of pre-target, target, and post-target words. ***<italic>p</italic> &lt; 0.001; *<italic>p</italic> &lt; 0.05; n.s., not statistically significant.</p></caption><graphic xlink:href="EMS206028-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Coherence curves measuring the parallel attention allocated to foveal and parafoveal words.</title><p><bold>(a).</bold> Target words were frequency tagged at <italic>f<sub>t</sub></italic> and post-target words were frequency tagged at <italic>f<sub>p</sub></italic> (zero time point denotes the first fixation onset of target words). Therefore, RIFT measures (coherence) at <italic>f<sub>t</sub></italic> (in blue) over all trials measured the foveal attention to the target word, while coherence at <italic>f<sub>p</sub></italic> (in orange) measured the parafoveal attention to the post-target word. The noise level of the coherence estimation was assessed by calculating the coherence (in grey) during baseline intervals, when only the photodiode discs were flickering, and no flickering words were presented on the screen. Shaded areas around the curve show the standard error around the mean value across participants. (<bold>b)</bold>. Pairwise two-sided <italic>t</italic>-tests were conducted between these three coherence curves (averaged over 0 – 0.2 s). Violin plots display participant-level mean coherence, with bold lines indicating overall averages. ***<italic>p</italic> &lt; 0.001; n.s., not statistically significant.</p></caption><graphic xlink:href="EMS206028-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Time-resolved coherence spectra during fixation intervals.</title><p>First, we selected RIFT response sensors separately for pre-target, target, and post-target fixation intervals by calculating the coherence at their corresponding frequency of interest (<italic>f<sub>t</sub></italic> for pre-target, <italic>f<sub>p</sub></italic> for target, and <italic>f<sub>t</sub></italic> for post-target; columns <bold>a</bold>, <bold>b</bold>, <bold>c</bold>). Topographies for RIFT response sensors summarized over all participants are shown in row <bold>i</bold>. Then, using these RIFT response sensors, time-resolved coherence spectra were estimated for all trials (row <bold>ii</bold>) and for low and high lexical frequency conditions (row <bold>iii</bold>, <bold>iv</bold>). Finally, cluster-based permutation tests were performed to test the conditional difference (row <bold>v</bold>). Significant clusters are highlighted in the coherence spectra by masking non-significant areas (multiply coherence values by 0.4).</p></caption><graphic xlink:href="EMS206028-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><title>Flexible allocation of attention correlates with reading speed.</title><p>Flexibility in shifting attention was quantified by contrasting the lexical parafoveal effect with the foveal load effect. The lexical parafoveal effect was derived from the coherence difference averaged over the significant cluster during pre-target fixation intervals (<xref ref-type="fig" rid="F5">Figure 5</xref>, column <bold>a</bold> row <bold>v</bold>), while the foveal load effect was derived from the coherence difference averaged over the significant cluster during target fixation intervals (<xref ref-type="fig" rid="F5">Figure 5</xref>, column <bold>b</bold> row <bold>v</bold>). Individual reading speed was estimated by averaging the first fixation durations over all words in the sentences. We found a significant correlation between flexibility of attentional shifts and reading speed (Pearson’s <italic>r</italic> = -0.468, <italic>p</italic> = 0.014).</p></caption><graphic xlink:href="EMS206028-f006"/></fig><table-wrap id="T1" position="float" orientation="portrait"><label>Table 1</label><caption><title>Word characteristics of pre-target, target, and post-target words.</title></caption><table frame="box" rules="all"><thead><tr><th align="left" valign="middle"/><th align="left" valign="middle">Pre-target</th><th align="left" valign="middle" colspan="2">Target</th><th align="left" valign="middle">Post-target</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="2">Word frequency</td><td align="left" valign="middle" rowspan="2">513.4 (1474.9)</td><td align="left" valign="middle">Low</td><td align="left" valign="middle">High</td><td align="left" valign="middle" rowspan="2">857.5 (2300.4)</td></tr><tr><td align="left" valign="middle">5.5 (4.6)</td><td align="left" valign="middle">91.2 (124.3)</td></tr><tr><td align="left" valign="middle" rowspan="2">Word length</td><td align="left" valign="middle" rowspan="2">5.7 (1.2)</td><td align="left" valign="middle">Low</td><td align="left" valign="middle">High</td><td align="left" valign="middle" rowspan="2">6.3 (1.8)</td></tr><tr><td align="left" valign="middle">5.7 (0.7)</td><td align="left" valign="middle">5.7 (0.7)</td></tr></tbody></table><table-wrap-foot><fn id="TFN1"><p id="P77">Note. All values here are mean values, with standard deviations in parentheses. Low (&lt;10) and high lexical frequency (&gt;30) target words are reported in terms of the written CELEX frequency per million 42. Word length refers to the number of letters in a word. The word length of pre-target words ranged from 4 to 8 letters, target words from 5 to 8 letters, and post-target words from 2 to 10 letters. Each low and high-lexical frequency target word pair shared the same pre- and post-target words, balanced across participants.</p></fn></table-wrap-foot></table-wrap></floats-group></article>