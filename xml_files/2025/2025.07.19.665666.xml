<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS207531</article-id><article-id pub-id-type="doi">10.1101/2025.07.19.665666</article-id><article-id pub-id-type="archive">PPR1054490</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Mapping the visual cortex with Zebra noise and wavelets</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Skriabine</surname><given-names>Sophie</given-names></name></contrib><contrib contrib-type="author"><name><surname>Shinn</surname><given-names>Maxwell</given-names></name></contrib><contrib contrib-type="author"><name><surname>Picard</surname><given-names>Samuel</given-names></name></contrib><contrib contrib-type="author"><name><surname>Harris</surname><given-names>Kenneth D</given-names></name></contrib><contrib contrib-type="author"><name><surname>Carandini</surname><given-names>Matteo</given-names></name></contrib><aff id="A1"><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap></aff></contrib-group><pub-date pub-type="nihms-submitted"><day>25</day><month>07</month><year>2025</year></pub-date><pub-date pub-type="preprint"><day>23</day><month>07</month><year>2025</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Studies of the early visual system often require characterizing the visual preferences of large populations of neurons. This task typically requires multiple stimuli such as sparse noise and drifting gratings, each of which probe only a limited set of visual features. Here we introduce a new dynamic stimulus with sharp-edged stripes called Zebra noise and a new analysis model based on wavelets, and show that in combination they are highly efficient for mapping multiple aspects of the visual preferences of thousands of neurons. We used two-photon calcium imaging to record the activity of neurons in the mouse visual cortex. Zebra noise elicited strong responses that were more repeatable than those evoked by traditional stimuli. The wavelet-based model captured the repeatable aspects of the resulting responses, providing measures of neuronal tuning for multiple stimulus features: position, orientation, size, spatial frequency, drift rate, and direction. The method proved efficient, requiring only 3 minutes of stimulation (repeated 3 times) to characterize the tuning of thousands of neurons across visual areas. In combination, the Zebra noise stimulus and the wavelet-based model provide a broadly applicable toolkit for the rapid characterization of visual representations, promising to accelerate future studies of visual function.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Neurons in the early visual cortex are selective for a wide range of visual attributes, and this selectivity is currently probed with multiple distinct synthetic stimuli. For instance, neurons in the primary visual cortex are selective for properties such as position, size, orientation, spatial frequency, speed and direction. Initial efforts to measure their selectivity were aimed at single neurons and focused on a few visual properties at a time, with stimuli such as bars (<xref ref-type="bibr" rid="R11">Hubel &amp; Wiesel, 1959</xref>), squares (<xref ref-type="bibr" rid="R13">Jones &amp; Palmer, 1987b</xref>), and gratings (<xref ref-type="bibr" rid="R27">Ringach et al., 1997</xref>, <xref ref-type="bibr" rid="R17">Movshon et al., 1978b</xref>). In later efforts, the emphasis shifted towards stimuli that can simultaneously characterize many aspects of selectivity in multiple neurons. These include contrast-modulated pink noise (<xref ref-type="bibr" rid="R18">Niell &amp; Stryker, 2008</xref>) sums of spatiotemporal wavelets (<xref ref-type="bibr" rid="R2">Bonin et al., 2011</xref>), and structured noise stimuli such as “Monet” and “Trippy” (MICrONS Consortium et al., 2025).</p><p id="P3">An efficient stimulus for characterizing populations of visual neurons should elicit strong and repeatable responses while varying along multiple visual dimensions. The activity of neurons in the visual system is determined not only by their visual preferences but also by non-visual factors (<xref ref-type="bibr" rid="R19">Niell &amp; Stryker, 2010</xref>, <xref ref-type="bibr" rid="R9">Erisken et al., 2014</xref>, <xref ref-type="bibr" rid="R28">Schröder et al., 2020</xref>). An ideal stimulus for mapping visual preferences would evoke responses that are stronger than this non-visual activity, and thus repeatable. Moreover, an ideal stimulus would minimize biases, to avoid compensatory adaptations that are typical of the visual system when stimuli are unbalanced (<xref ref-type="bibr" rid="R1">Benucci et al., 2009</xref>, <xref ref-type="bibr" rid="R5">Dhruv &amp; Carandini, 2014</xref>). Finally, an ideal stimulus should allow rapid mapping, so that recording sessions can be brief or mostly devoted to other purposes.</p><p id="P4">To meet these criteria, we designed a dynamic visual stimulus we call “Zebra noise”. The stimulus is made of stripes that vary randomly in orientation, width, eccentricity, direction, and speed. The stripes have high contrast and sharp edges. Because edges have power at multiple spatial frequencies, they can drive neurons with diverse spatial frequency preferences. Moreover, the sharp edges cause the spatial frequency components to be in phase, leveraging nonlinear effects that increase the responses of neurons in visual cortex (<xref ref-type="bibr" rid="R10">Felsen et al., 2005</xref>).</p><p id="P5">To analyze the responses of visual cortical neurons to Zebra noise, we designed a simple nonlinear analysis based on Gabor wavelets (“WaveEn”). The analysis is inspired by models of simple and complex cells in the primary visual cortex (<xref ref-type="bibr" rid="R16">Movshon et al., 1978a</xref>, <xref ref-type="bibr" rid="R17">Movshon et al., 1978b</xref>) and by their subsequent elaborations (<xref ref-type="bibr" rid="R32">Touryan et al., 2005</xref>, <xref ref-type="bibr" rid="R33">Vintch et al., 2015</xref>, <xref ref-type="bibr" rid="R4">de Vries et al., 2020</xref>). It is a basic approximation of known visual tuning properties, designed to characterize the visual preferences that are probed by Zebra noise.</p><p id="P6">In combination, the Zebra noise stimulus and the wavelet-based analysis proved highly effective in characterizing the preferences of neurons in the visual cortex. We used two-photon imaging to record the responses of thousands of neurons in the mouse visual cortex. Zebra noise elicited strong and repeatable responses both in the primary visual area and in higher visual areas. These responses were well described by the wavelet-based model. Moreover, the stimulus and the analysis were highly efficient: even 3 minutes of Zebra noise, repeated 3 times, was sufficient to characterize the responses of thousands of neurons. These results suggest that Zebra noise and a wavelet-based analysis are useful tools to characterize the visual system.</p></sec><sec id="S2" sec-type="results"><title>Results</title><p id="P7">Zebra noise contains black and white stripes of varying size, orientation, and velocity (<xref ref-type="supplementary-material" rid="SD1">Supplementary Movie 1</xref>). To generate it, we start from 3-dimensional fractal Perlin noise (<xref ref-type="bibr" rid="R24">Perlin, 1985</xref>), which is computed rapidly on a pixel-by-pixel basis (<xref ref-type="fig" rid="F1">Figure 1a</xref>). We then apply a “comb threshold” to the gray values, discretizing it into bins of equal size, and setting the pixels to white if their value falls into an even bin, and to black otherwise (<xref ref-type="fig" rid="F1">Figure 1b</xref>). Decreasing the number of teeth in the comb (increasing the bin size) creates patterns with thicker stripes (<xref ref-type="fig" rid="F1">Figure 1b,c</xref>). In addition to the bin size, Zebra noise has two other tunable parameters: the fractal exponent of the Perlin noise, which controls the jaggedness of the contours, and the zoom magnitude, which controls the overall scale (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 1</xref>). The movement of the stripes creates a constantly changing motion field (<xref ref-type="fig" rid="F1">Figure 1d</xref>). The resulting movie is largely uncorrelated over space (<xref ref-type="fig" rid="F1">Figure 1e</xref>). It contains power in multiple spatial frequencies and is approximately uniform across orientations (<xref ref-type="fig" rid="F1">Figure 1f,g</xref>).</p><p id="P8">We tested the Zebra noise stimulus in mouse visual cortex and found that it elicited strong and repeatable responses. We performed recordings with a two-photon mesoscope (<xref ref-type="bibr" rid="R30">Sofroniew et al., 2016</xref>) in the visual cortex of transgenic mice expressing the calcium indicator GCaMP6s in excitatory neurons (<xref ref-type="bibr" rid="R39">Wekselblatt et al., 2016</xref>) (<xref ref-type="fig" rid="F2">Figure 2a-c</xref>). When we presented Zebra noise (10 min, repeated 3 times), the neurons gave strong responses (e.g. <xref ref-type="fig" rid="F2">Figure 2d</xref>). To assess response repeatability, we measured the average correlation across repeats. The responses of many neurons were highly repeatable, both in primary visual cortex and in higher areas (<xref ref-type="fig" rid="F2">Figure 2e</xref>).</p><p id="P9">These responses were more repeatable than those elicited by visual stimuli that are commonly used to map selectivity for stimulus position (sparse noise) or for stimulus orientation and direction (drifting gratings). In the same imaging session, we presented sparse noise made of black and white flashed squares, a visual stimulus commonly used to map preferences for stimulus position (10 min, repeated 3 times). Responses elicited by sparse noise were generally less repeatable (e.g. <xref ref-type="fig" rid="F2">Figure 2f</xref>). Among the neurons that showed repeatable visual responses (n = 2,026 / 3,907 neurons with repeatability &gt; 0.2 with any stimulus), 70% had higher repeatability in response to Zebra noise than in response to sparse noise (<xref ref-type="fig" rid="F2">Figure 2g-h</xref>). We also showed drifting gratings (18 directions in 5 min, repeated twice), the visual stimulus typically used to measure preferences for stimulus orientation and direction. Once again, we observed responses that were substantially less repeatable throughout the visual cortex and especially in higher visual areas (<xref ref-type="fig" rid="F2">Figure 2i-j</xref>). Among the neurons with repeatable visual responses, 75% had higher re-peatability in response to Zebra noise than to drifting gratings (<xref ref-type="fig" rid="F2">Figure 2k</xref>). Therefore, Zebra noise elicited more repeatable visual responses than sparse noise and drifting gratings.</p><p id="P10">Zebra noise also elicited more repeatable responses than “Trippy”, a similar dynamic stimulus that appears blurrier. Like Zebra noise, Trippy has irregular and dynamic bands, but it lacks sharp edges (MICrONS Consortium et al., 2025; <xref ref-type="bibr" rid="R42">Yatsenko et al., 2018</xref>). We compared the responses of V1 neurons in to Zebra noise and to Trippy and found the latter to be less repeatable (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 2</xref>), suggesting that the sharp edges of Zebra noise enhance the repeatability of the neural responses.</p><p id="P11">Zebra noise triggered a repeatable response also in neurons that fired rarely. Some neurons fired only a few times during 10 minutes of Zebra noise and thus had a distribution of firing rates that was highly skewed. The responses of these neurons were highly repeatable (repeatability &gt; 0.5, <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 3</xref>). By contrast, the same neurons gave much less repeatable responses to drifting gratings and sparse noise, with a correlation across trials typically &lt;0.2. This suggests that by sampling a rich parametric space, Zebra noise can reliably drive neurons that are highly selective and thus typically silent.</p><p id="P12">To analyze responses of neurons in primary visual cortex to Zebra noise, we started by filtering the stimulus with Gabor wavelets and pooling the results across phases. Gabor wavelets are a reasonable description of the receptive fields of V1 simple cells (<xref ref-type="bibr" rid="R12">Jones &amp; Palmer, 1987a</xref>), and are thus a robust starting point for more elaborate models of cortical responses (<xref ref-type="bibr" rid="R4">de Vries et al., 2020</xref>; <xref ref-type="bibr" rid="R17">Movshon et al., 1978b</xref>, <xref ref-type="bibr" rid="R16">1978a</xref>; <xref ref-type="bibr" rid="R32">Touryan et al., 2005</xref>; <xref ref-type="bibr" rid="R33">Vintch et al., 2015</xref>; <xref ref-type="bibr" rid="R43">Yoshida &amp; Ohki, 2020</xref>). They are parametrized by their position x and y, orientation θ, size s, and spatial frequency f (<xref ref-type="fig" rid="F3">Figure 3a,b</xref>). In addition, their phase φ can range from 0 to 2π. Filtering a frame of the Zebra noise stimulus with a Gabor wavelet highlights its edges along a given orientation (<xref ref-type="fig" rid="F3">Figure 3c-d</xref>). We then combine the output of two wavelets in cosine phase (φ =0) and sine phase (φ = π/2) into a complex number and take its amplitude <italic>α</italic>, obtaining an image that highlights the edges regardless of their polarity (<xref ref-type="fig" rid="F3">Figure 3e</xref>).</p><p id="P13">For each neuron, we next selected the wavelet whose phase-pooled output best correlates with the neuron’s responses. Having obtained for each wavelet a filtered amplitude <italic>α</italic>(<italic>t</italic>) of the stimulus as a function of time <italic>t</italic>, we find for each neuron the wavelet whose <italic>α</italic>(<italic>t</italic>) best correlates with the activity of the neuron. The parameters of this best-fitting wavelet estimate the neuron’s preferred azimuth x, elevation y, and orientation θ (<xref ref-type="fig" rid="F4">Figure 4a</xref>) as well as its preferred size s and spatial frequency f. For the example neuron (<xref ref-type="fig" rid="F4">Figure 4b</xref>), the correlation between the neuron’s response and the amplitude <italic>α</italic>(<italic>t</italic>) of the best-fitting wavelet is 0.26 (<xref ref-type="fig" rid="F4">Figure 4c</xref>).</p><p id="P14">Finally, we refined the predictions by identifying for each neuron any nonlinear dependence on amplitude, phase, and phase derivative (drift rate) (<xref ref-type="fig" rid="F4">Figure 4d</xref>). Having identified a best-fitting wavelet for each neuron, we go back to the complex-valued output of the sine-phase and cosine-phase wavelets and consider not only the amplitude <italic>α</italic>(<italic>t</italic>), but also the phase <italic>φ</italic>(<italic>t</italic>) and its time derivative <italic>φ′</italic>(<italic>t</italic>). We term the latter the drift rate; it represents both the direction and the speed of movement. At each moment <italic>t,</italic> these three quantities describe a point in a three-dimensional space, and the neuron’s response provides a value <italic>R</italic>(<italic>α, φ, φ′</italic>) in that point (<xref ref-type="fig" rid="F4">Figure 4d</xref>). As Zebra noise samples much of this three-dimensional space, we can obtain robust estimates for the value <italic>R</italic> for the entire space (Methods). This allows the model to predict responses to stimuli that were not strictly part of the training set. The resulting prediction is quite close to the actual activity of the neuron (<xref ref-type="fig" rid="F4">Figure 4e</xref>). For instance, in the example neuron the correlation between responses and model prediction is 0.68 (<xref ref-type="fig" rid="F4">Figure 4e</xref>).</p><p id="P15">This approach yields approximate tuning curves for multiple stimulus parameters. In the first stage in the model (<xref ref-type="fig" rid="F4">Figure 4a</xref>), we estimate preferred azimuth x, elevation y, orientation θ, size s, and spatial frequency f by finding the wavelet that provides the highest correlation. We can then vary one parameter at a time and obtain a “tuning curve” showing how the correlation decreases when the parameter deviates from optimal (<xref ref-type="fig" rid="F4">Figure 4f</xref>). In the second stage of the model (<xref ref-type="fig" rid="F4">Figure 4d</xref>) we estimate a joint function <italic>R</italic>(<italic>α, φ, φ′</italic>) indicating how the response varies with amplitude <italic>α</italic>, phase <italic>φ,</italic> and drift rate <italic>φ′</italic>. To estimate a neuron’s selectivity for these attributes we take averages over two of the three parameters to obtain a (generally nonlinear) input/output function <italic>A</italic>(<italic>α</italic>); a tuning for spatial phase <italic>P</italic>(<italic>φ</italic>) and a tuning for direction and drift rate <italic>S</italic>(<italic>φ′</italic>) (<xref ref-type="fig" rid="F4">Figure 4g</xref>). This analysis provides insight into how the stimulus attributes contribute to each neuron’s response.</p><p id="P16">Similar results were obtained across visually responsive neurons with different types of selectivity. In ~80% of neurons, the tuning curves displayed a single well-defined peak, enabling straightforward estimation of position x and y, orientation θ, scale s, and spatial frequency f (similar to <xref ref-type="fig" rid="F4">Figure 4a</xref>). However, in the remaining ~20%, the curves exhibited multiple peaks in x or y (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 4</xref>), complicating receptive field localization. To address this ambiguity, we developed an optional correction step, where the user can opt to rely on the smooth organization of retinotopic maps. This allows the preferred position of the ambiguous neurons to be determined from the tuning preferences of adjacent neurons. However, this assumption may obscure the local scatter of preferred positions where this scatter is present (<xref ref-type="bibr" rid="R2">Bonin et al., 2011</xref>).</p><p id="P17">Having characterized the tuning of individual neurons with Zebra noise, we can now create maps of those tuning properties. The maps of retinotopy displayed the expected orthogonal representation of visual field azimuth and elevation (<xref ref-type="bibr" rid="R6">Dräger, 1975</xref>) with some degree of scatter (<xref ref-type="bibr" rid="R2">Bonin et al., 2011</xref>) (<xref ref-type="fig" rid="F5">Figure 5a-b</xref>). These maps were similar to those obtained with traditional stimuli such as sparse noise and drifting gratings (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 5</xref>). The maps of preferred spatial frequency and size, in turn, displayed slow variations across space (<xref ref-type="fig" rid="F5">Figure 5c-d</xref>), with V1 neurons showing, on average, smaller receptive fields. The map of preferred orientation exhibited the typical salt-and-pepper organization (<xref ref-type="bibr" rid="R20">Ohki &amp; Reid, 2007</xref>) (<xref ref-type="fig" rid="F5">Figure 5e</xref>). As is customary, from the retinotopy maps, we extracted the sign map, which encodes the sign of retinotopic gradients (<xref ref-type="fig" rid="F5">Figure 5f</xref>). We used the sign map to delineate visual areas (<xref ref-type="bibr" rid="R45">Zhuang et al., 2017</xref>) (curves in <xref ref-type="fig" rid="F5">Figure 5a-g</xref>).</p><p id="P18">The model performed well both in primary visual cortex and in higher visual areas. To validate the model, we measured the correlation between the model’s predictions and neural activity (predictability). Predictability was generally higher for V1 neurons, with an average of 0.44, and slightly lower for higher visual areas (HVA) neurons, with an average of 0.36 (<xref ref-type="fig" rid="F5">Figure 5g</xref>). However, this difference could be due to the higher repeatability (inter-trial correlation) of V1 responses, which we have already documented (<xref ref-type="fig" rid="F2">Figure 2e</xref>). To assess this possibility, we computed a prediction score, defined as the ratio between predictability and repeatability. This score estimates the ability of the model to explain repeatable aspects of the responses. The model achieved a prediction score of 0.82 (<xref ref-type="fig" rid="F5">Figure 5h</xref>) over all visually responsive neurons, indicating that it captured a large fraction of the visual properties of the neurons. This score (0.82) was on average the same for neurons in V1 and in HVAs, suggesting that the difference in predictability between V1 and HVA neurons is entirely explained by the difference in repeatability.</p><p id="P19">The Zebra noise stimulus proved highly efficient, such that 3 minutes of it were sufficient to provide a reasonable characterization of the visual properties of the neurons. To evaluate how well our model predicts neuronal responses to unseen stimuli, we trained it on increasing durations of the stimulus (1 to 8 minutes) and tested it on the final two minutes (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 6</xref>). We found that with less than 3 minutes of training, the model fails to generalize. However, from 3 minutes onward, its prediction score surpasses 0.5, without much improvement with longer training. This results suggests that a 3 minute Zebra noise session is sufficient to characterize individual neurons.</p><p id="P20">Similar results were obtained in other mice and across calcium indicators and expression strategies. The experiments described up to here were performed in a mouse from a transgenic line (TetO) expressing the calcium indicator GCaMP6s in excitatory neurons (<xref ref-type="bibr" rid="R39">Wekselblatt et al., 2016</xref>). To validate the approach in other preparations, we replicated our experiments in two additional mice: one from the same TetO line and one from a mouse line (RiboL) characterized by a soma localized expression of a ribosome-tethered GCaMP8m. (<xref ref-type="bibr" rid="R38">Wang et al., 2023</xref>). Thanks to the soma-localized expression, this mouse line has minimal neuropil contamination. Moreover, GCaMP8m has faster calcium dynamics than GCaMP6s (<xref ref-type="bibr" rid="R44">Zhang et al., 2023</xref>). Zebra noise and our analysis method provided an efficient characterization of the visual cortex in both mice, with the model achieving a prediction score of 0.72 in the RiboL mouse and 0.67 in the TetO mouse (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 7</xref>). These results indicate that in combination, the Zebra noise stimulus and our analysis method are effective for multiple calcium indicators and expression strategies.</p></sec><sec id="S3" sec-type="discussion"><title>Discussion</title><p id="P21">Characterizing the tuning of large neuronal populations is a central challenge in visual neuroscience, which is hampered by stimuli that are inefficient or probe a limited feature space. Here, we have introduced a solution: a combination of a novel dynamic stimulus, Zebra noise, and an interpretable wavelet-based model. We demonstrated that the sharp, high-contrast edges of Zebra noise drive highly repeatable and robust responses across visual cortex, enabling the efficient mapping of multiple feature preferences in thousands of neurons from only a few minutes of data.</p><p id="P22">The success of Zebra noise is likely due not only to its high-contrast moving patterns of appropriate spatial and temporal scales, but also its sharp edges, which exert a powerful influence on neurons in the visual cortex (<xref ref-type="bibr" rid="R10">Felsen et al., 2005</xref>). The benefit of sharp edges appeared to be confirmed by our observation that the ‘Trippy’ stimulus (MICrONS Consortium et al., 2025), which has similar dynamic bands but lacks sharp edges, elicited responses that were less repeatable. Furthermore, by systematically sampling a rich parametric space (orientation, speed, etc.), Zebra noise reliably drove even neurons that were highly selective and fired sparsely, which are often missed by less comprehensive stimuli.</p><p id="P23">Analyzing responses to a rich, dynamic stimulus like Zebra noise requires a quantitative model. To this end, we developed a nonlinear wavelet-based model that occupies a middle ground between simple filters and complex deep neural networks. While advanced neural network models may achieve higher predictive accuracy (e.g., <xref ref-type="bibr" rid="R3">Cadena et al., 2019</xref>; <xref ref-type="bibr" rid="R7">Du et al., 2025</xref>; <xref ref-type="bibr" rid="R31">Tong et al., 2023</xref>; <xref ref-type="bibr" rid="R35">Wang et al., 2025</xref>; <xref ref-type="bibr" rid="R40">Willeke et al., 2023</xref>; <xref ref-type="bibr" rid="R41">Yamins et al., 2014</xref>), our approach prioritizes computational efficiency and interpretability. We showed that despite its simplicity, this model successfully provided good estimates of neuronal visual preferences across eight dimensions, defined by the azimuth and elevation of the stimulus position, and by stimulus orientation, size, spatial frequency, phase, magnitude, and drift.</p><p id="P24">Remarkably, the model performed equally well in mouse primary visual cortex (V1) and in the surrounding higher visual areas (HVA). Indeed, while predictability was slightly lower in HVA than in V1, this decrease in predictability was fully explained by a decrease in repeatability. Indeed, the ratio of predictability to repeatability (the prediction score) was the same in the two regions.</p><p id="P25">In future work, the wavelet-based model might be further refined by including multiple wavelets and accounting for eye movements. For instance, the current model assigns a single optimal wavelet (of all possible phases) to each neuron, potentially oversimplifying the diversity of receptive field structures. A better model might involve combinations of driving and suppressive Gabor wavelets (e.g., <xref ref-type="bibr" rid="R4">de Vries et al., 2020</xref>; <xref ref-type="bibr" rid="R33">Vintch et al., 2015</xref>). Another limitation is the absence of pupil tracking in our model, which could partly account for reduced repeatability, as saccades or undetected eye movements shift the visual input. A refinement of the model could account for the eye movements to help improve the reliability of feature estimation (<xref ref-type="bibr" rid="R23">Parker et al., 2023</xref>; <xref ref-type="bibr" rid="R34">Walker et al., 2019</xref>).</p><p id="P26">Furthermore, while the Zebra noise and analysis method that we used were optimized for mice, both stimulus and analysis can be readily adapted for species with higher spatial and temporal acuity, potentially including humans. We provide tools to generate custom libraries of Gabor wavelets, enabling finer feature descriptions, for instance in non-human primates. Likewise, our analytical framework is tailored for two-photon imaging but could be potentially adjusted to other modalities such as electrophysiological recordings, broadening its applicability to diverse experimental settings. Zebra noise could also be useful to map visual cortex with coarser methods, such as widefield imaging, functional ultrasound imaging, or fMRI. However, the analysis that we used assumes selectivity for orientation and spatial frequency, which is typically a poor assumption for the measurements obtained with coarse methods. The analysis would likely need to be modified by assuming the pooling of activity of multiple model neu-rons (<xref ref-type="bibr" rid="R8">Dumoulin &amp; Wandell, 2008</xref>; <xref ref-type="bibr" rid="R14">Larsson &amp; Heeger, 2006</xref>; <xref ref-type="bibr" rid="R26">Ribeiro et al., 2025</xref>).</p><p id="P27">In combination, the Zebra noise stimulus and the wavelet-based model provide a powerful means for rapid and robust ‘fingerprinting’ of neurons. This is highly valuable for tracking neuronal populations over time in studies of learning and plasticity. Furthermore, the model’s interpretability allows for direct comparison of tuning properties across visual regions, offering insights into the functional specialization of visual areas. This toolkit thus promises to accelerate progress in understanding how visual representations are organized, transformed by experience, and altered in disease.</p></sec><sec id="S4" sec-type="methods"><title>Methods</title><p id="P28">All procedures were conducted in accordance with the UK Animals Scientific Procedures Act (1986) under personal and project licenses released by the Home Office following appropriate ethics review.</p><sec id="S5"><title>Zebra noise</title><p id="P29">Zebra noise is a comb-thresholded 3D Perlin noise. Below we describe the steps in generating it, and the analyses that we performed.</p><p id="P30">The first step is to generate 3D Perlin noise. Perlin noise is a type of gradient based noise widely used in computer graphics to generate texture or naturalistic terrains, and is attractive because it is fast to compute, and can be layered at different spatial and temporal scales to form a fractal 1/f-like spectrum. Notably, Perlin noise is computed on a pixel-by-pixel basis, allowing frames of the stimulus to be generated one-by-one with low memory requirements and processing power, permitting memory-efficient generation and parallel processing. By comparison, generating true 1/f pink noise (noise with a logarithmic frequency spectrum, from which Perlin Noise can be seen as a variation (<xref ref-type="bibr" rid="R18">Niell &amp; Stryker, 2008</xref>)) quickly becomes computationally intractable due to the need to perform an inverse Fourier transform of a very large tensor.</p><p id="P31">To generate Perlin noise, an N-dimensional grid is generated. At each grid node, a random gradient vector is assigned. Then for each grid node we computed the dot product between the gradient vector and the node-point distance, for each point in the N-dimensional cells defined by the grid. The value is then interpolated across each value, resulting in a smooth random noise. This is performed at multiple spatial and temporal scales to produce a fractal spectrum. The process is performed in 3D so as to generate a movie.</p><p id="P32">The second step of Zebra noise generation is what we call a comb threshold, binning sub-intervals of the noise signal to generate sharp edge stripes. The Perlin noise values are rescaled to have a minimum value of 0 and a maximum value of 255 and then binned into k bins. We then assign the value of 1 to the odd bins and -1 to the even bins. This process creates a distinct high-contrast pattern of alternating sharp-edge stripes, from which Zebra noise derives its name. This texture exhibits both randomness and structured periodicity.</p><p id="P33">To measure the spatial autocorrelation of Zebra noise we computed the pairwise Pearson correlation of each pixel with a predetermined seed pixel at the center of the screen (<xref ref-type="fig" rid="F1">Figure 1e</xref>). To analyze the spatial frequency and orientation content of Zebra noise (<xref ref-type="fig" rid="F1">Figure 1f</xref>), we computed the power spectral density (PSD) of the stimulus images using the Fast Fourier Transform. We then transformed the PSD into polar coordinates to extract its radial (frequency) and angular (orientation) components. Finally, we computed the mean and standard deviation of the power at each frequency and orientation.</p></sec><sec id="S6"><title>Experiments</title><p id="P34">Experiments were conducted on 3 adult mice aged between ~3 weeks and 6 months (2 females - tetO-RiboL1-jGCaMP8m-Cdh23 x Camk-Cdh23, TetO-Cdh23 x Camk-Cdh23 and one male - TetO-Cdh23 x Camk-Cdh23)</p><p id="P35">We used two-photon imaging to record thousands of neurons in V1 and higher visual areas of head-fixed transgenic mice expressing GCaMP6s or GCaMP8m in all excitatory neurons. The mouse was head fixed but free to run on a wheel, while Zebra noise was played on a screen covering its full visual field.</p><sec id="S7"><title>Surgical procedure</title><p id="P36">For the implant surgery, mice were anesthetized with isoflurane (5% for induction and 0.5–1% during the procedure) and secured in a stereotaxic frame. An analgesic, Rimadyl (5 mg/kg), was administered subcutaneously prior to the procedure and orally for the three following days. To prevent brain edema Dexamethasone (0.5 mg/kg, IM) was administered intramuscularly 30 minutes prior to surgery. Throughout the procedure, the exposed brain was continuously perfused with artificial cerebrospinal fluid (150 mM NaCl, 2.5 mM KCl, 10 mM HEPES, 2 mM CaCl2, 1 mM MgCl2; pH 7.3 adjusted with NaOH, 300 mOsm). A headplate was implanted for subsequent head-fixation, and a craniotomy (3 to 4 mm in diameter) was performed to implant a cranial window for optical access, made of three-layer sandwiched borosilicate glass. After they recovered (minimum of 3 days after the surgery, in practice, 1 week), mice were habituated to the rig and trained to be head fixed.</p></sec><sec id="S8"><title>Two-photon imaging</title><p id="P37">We performed two-photon imaging using a standard resonant mesoscope (<xref ref-type="bibr" rid="R30">Sofroniew et al., 2016</xref>) (Multiphoton Mesoscope, ThorLabs Inc) controlled by ScanImage 4.2 (<xref ref-type="bibr" rid="R25">Pologruto et al., 2003</xref>). Excitation light was delivered by a 920 nm ultrafast laser (FemtoFibre ultra 920, Toptica Photonics AG, Graefelfing, Germany). Imaging depth was adjusted using a remote focusing mirror with 1 mm travel range. Fluorescent signal was collected in the green channel (Semrock FF01-520/70 emission filter) by a GaAsP PMT (PMT2103, Thorlabs, H10770PA-40, Hamamatsu Photonics), amplified by a fast transimpedance preamplifier (400 MHz bandwidth, HCA-400M-5K-C, FEMTO Messtechnik GmbH, Berlin, Germany), and digitized by a vDAQ acquisition board (Vidrio Technologies, MBF Bioscience, Williston, VT, USA). Acquisition sampling was synchronized to the laser pulses.</p><p id="P38">To prevent visual stimulation light from contaminating the fluorescence signal, the monitor backlight was rapidly switched on/off by a custom circuit implemented on Teensy 4.0, such that the monitor light was on only when the resonant mirror was making a U-turn and no image-building fluorescence was collected. In addition, to prevent the PMT from saturating or tripping, the imaging objective was light-shielded using a custom-made plastic cone.</p><p id="P39">Fields of view (FOV), roughly centered on V1, were imaged with a resolution of 2000 × 1536 pixels at 3-4 Hz. FOVs typically spanned 3 stripes of size 2000 × 500 pixels covering a window of 2734 × 2050 μm for the imaging of neurons in a single plane in V1 L2/3. The acquisition resolution was 1.3671 μm per pixel.</p><p id="P40">The raw neural images were processed with Suite2p (<xref ref-type="bibr" rid="R22">Pachitariu et al., 2017</xref>) to extract the firing rate of the neuron as a time series. The firing rate vectors were upsampled to 30 Hz to match the stimulus frame rate.</p></sec><sec id="S9"><title>Traditional visual stimuli</title><p id="P41">Visual stimuli were displayed at 30 Hz on two screens placed at 90 degrees in front and on the left side of the head-fixed animal, covering the range of -135 to 45 deg of azimuth and -35 to 35 deg of elevation.</p><p id="P42">Experiments were 3 hours long, and each session included 10 min of spontaneous activity, 3 × 10 min of sparse noise, 3 × 18 directions drifting gratings, and 3 × 10 min of Zebra noise.</p><p id="P43">To map the early retinotopic organization of the mouse visual cortex, we used a sparse noise stimulus consisting of black and white squares presented on a gray background. The stimulus was displayed on a screen covering an azimuthal visual field of 135° to -45° and an elevation range of -35° to 35°. Individual squares, each measuring 9 × 9 degrees of visual angle, appeared randomly at different positions within this field. The stimulus was presented for a total duration of 10 minutes, ensuring sufficient sampling of receptive fields across the visual space.</p><p id="P44">To assess neuronal tuning to orientation and direction, we presented full-field drifting gratings spanning both monitors. The gratings were shown in 18 evenly spaced orientations (10° increments) with a randomly assigned forward or backward drift. Each grating was presented five times, with a duration of 1 s, followed by a 1-s inter-stimulus interval with a gray screen. The gratings had a spatial frequency of 0.05 cycles per degree and a drift rate of 2 Hz.</p><p id="P45">We also performed a separate experiment to measure responses to the Trippy stimulus (MICrONS Consortium et al., 2025). The associated code (<xref ref-type="bibr" rid="R42">Yatsenko et al., 2018</xref>) produced up to a few minutes of stimulus. We thus ran the stimulus for 1 minute (3 repeats). For fairness, we compared those responses to Zebra noise that was run for the same duration.</p></sec></sec><sec id="S10"><title>Wavelet-based model</title><p id="P46">To characterize neuronal selectivity with high spatiotemporal precision, we developed a wavelet-based approach that decomposes any visual stimuli into a structured library of localized wavelets. The code for this analysis is contained in a package called WavEn, which contains a graphical user interface (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 8</xref>). The package is available on GitHub, at <ext-link ext-link-type="uri" xlink:href="https://github.com/skriabineSop/waven">https://github.com/skriabineSop/waven</ext-link>.</p><sec id="S11"><title>Gabor library and filtering</title><p id="P47">To extract spatial features from the visual stimulus, we constructed two libraries of Gabor wavelets. Each wavelet is defined by a center location (azimuth: [-45°, [–35°], elevation: [-35°, 35°]), an orientation (8 evenly spaced angles from 0° to 180°), a spatial frequency, a size (defined as Gaussian envelope radius at half maximum), and a phase (sine or cosine). We assembled these wavelets in two libraries, one coarse and one fine. The <italic>coarse</italic> library contains 28,512 wavelets (27 azimuths × 11 elevations × 8 orientations × 6 size/frequency combinations × 2 phases). In this library, spatial frequency and size are linearly coupled, enabling rapid estimation of neuronal receptive field position and orientation with a compact set of wavelets. The <italic>fine</italic> library contains 2,332,800 wavelets (135 azimuths × 54 elevations × 8 orientations × 5 sizes × 4 frequencies × 2 phases), with spatial frequency and size sampled independently. This library allows more refined modeling of visual properties, including precise receptive field location and tuning to scale and frequency.</p><p id="P48">The raw stimulus displayed on the screen during the experiment had high resolution (1200 × 600 pixels). For processing time, it was downsampled to 135 × 54 pixels before applying the wavelet transform, which allows to reduce the number of Gabor wavelets. Each frame of the movie was then filtered using the dot product with every wavelet in the library.</p></sec><sec id="S12"><title>Step 1: Coarse feature estimation</title><p id="P49">We first obtained a rough estimate of a neuron’s receptive field properties using the coarse Gabor wavelets. For each wavelet, we filtered the Zebra noise video with the wavelet, as in <xref ref-type="fig" rid="F3">Figure 3</xref>. Because each wavelet is defined at two phases (0° and 90°), we applied the filtering separately for each phase and then summed the squared outputs. This yielded for each wavelet, one vector that can be interpreted as the variation of the wavelet amplitude. We then computed the Pearson’s correlation these timeseries and each neuron’s deconvolved and z-scored activity. This procedure provided one correlation value per wavelet, serving as a measure of similarity between the neuron, and the expected activity of the neuron and a model complex neuron described by that particular wavelet.</p></sec><sec id="S13"><title>Step 2: Refined feature estimation</title><p id="P50">Step 2 aims to refine the parameters (x, y, θ, s, f) of a neuron (<xref ref-type="fig" rid="F4">Figure 4a</xref>). We refined our initial coarse estimates of neuron features using our fine feature Gabor wavelets. Due to the large number of parameters of the fine Gabor wavelets, computational runtime prevented us from running the same procedure as with the coarse Gabor wavelets. The fine Gabor wavelets provide essential information about neurons, including the phase, the ratio between spatial frequency and receptive field size, and a high-resolution estimate of receptive field location. Therefore, we designed an algorithm to efficiently estimate these properties using the fine Gabor wavelet library. By iteratively fixing one parameter at a time (e.g., position) and refining the others (e.g., orientation, frequency, scale), we progressively improve each neuron’s receptive field position estimate as much as possible, within the limits of the filter library. The procedure is detailed below: <list list-type="order" id="L2"><list-item><p id="P51">Initialize position (x0, y0) based on the earlier coarse estimation</p></list-item><list-item><p id="P52">Compute Pearson correlation of all fine wavelets within (x0±dx, y0±dy) with neuron activity for both phases separately. Select the wavelet with the highest correlation score (across both phases) in absolute value. Fix a new optimal position x, y accordingly.</p></list-item><list-item><p id="P53">Estimate initial orientation (o), frequency (f), and scale (s)</p></list-item><list-item><p id="P54">While not converged <list list-type="bullet" id="L3"><list-item><p id="P55">Fix orientation (o), frequency (f), and scale (s)</p></list-item><list-item><p id="P56">Choose the wavelet with position (x, y) which maximizes Pearson correlation</p></list-item><list-item><p id="P57">Fix position (x, y)</p></list-item><list-item><p id="P58">Choose the orientation (o), frequency (f), and scale (s) which maximize Pearson correlation</p></list-item></list>
</p></list-item></list>
</p><p id="P59">Using the resulting correlations, we compute the tuning curve for each of these parameters. We define the tuning curve for each model parameter x, y, θ, s, f to be the correlation of the neuron’s activity with the wavelets where that specific parameter is varied, and all other parameters are optimal (<xref ref-type="fig" rid="F4">Figure 4f</xref>). This is analogous to predicting the response to experimental stimuli which probe a single parameter at a time, such as varying the direction of drifting gratings.</p></sec><sec id="S14"><title>Step 3: Model fitting</title><p id="P60">Step 3 aims to refine the estimate of each neuron’s receptive field by considering its preferences for phase (<italic>φ</italic>), drift (<italic>φ′</italic>) and amplitude (<italic>α</italic>)</p><p id="P61">Let us call [<italic>w</italic>
<sub>1</sub>, <italic>w</italic>
<sub>2</sub>, …, <italic>w<sub>w</sub></italic>] the wavelets in a library. Let’s assume our neuron’s best-matching wavelet (as found in Step 2) is <italic>w<sub>j</sub></italic>. This wavelet <italic>w<sub>j</sub></italic> is a matrix of complex numbers whose real part corresponds to the 0° phase an imaginary part corresponds to the 90° phase. Let <italic>p<sub>j</sub></italic>(<italic>t</italic>) be the projection of stimulus frame <italic>t</italic> on the wavelet <italic>w<sub>j</sub></italic>. It is a complex vector whose real and imaginary components indicate projections on the wavelets with 0° and 90° phase. We then compute the magnitude <italic>α</italic>(<italic>t</italic>) = |<italic>p<sub>j</sub></italic>(<italic>t</italic>)| and phase <italic>φ</italic>(<italic>t</italic>) = arg(<italic>p<sub>j</sub></italic>(<italic>t</italic>)) of the wavelet-transformed stimulus, as well as the derivative of the phase <italic>φ</italic>(<italic>t</italic>)′ = <italic>dφ</italic>(<italic>t</italic>)/<italic>dt</italic>. Based on these three properties, <italic>α</italic>(<italic>t</italic>), <italic>φ</italic>(<italic>t</italic>), and <italic>φ′</italic>(<italic>t</italic>), we build a 3D weighted histogram of the firing rate of the neuron, and its 3D occupancy matrix for these three properties (<xref ref-type="fig" rid="F4">Figure 4d</xref>). (20 bins per dimension). To improve robustness and to extrapolate to regions not covered by the stimulus, we blurred the 3D matrix of firing rates of the neuron and the 3D occupancy matrix separately before dividing the two. A similar procedure is typically used to compute two-dimensional place fields (<xref ref-type="bibr" rid="R21">O’ Keefe &amp; Burgess, 1996</xref>). To evaluate the model, we used the resulting 3D matrix as a lookup table: for any combination of phase, amplitude and drift: <italic>φ</italic>(<italic>t</italic>), <italic>α</italic>(<italic>t</italic>), and <italic>φ′</italic>(<italic>t</italic>), we estimated the predicted firing rate using nearest-neighbor interpolation into the 3D matrix. This step allows us to approximate the expected response for arbitrary tuples, even if they do not fall exactly on bin centers. To extract phase, drift, and amplitude tuning curves (<xref ref-type="fig" rid="F4">Figure 4g</xref>), we average the response matrix along two of its three dimensions, each time isolating the dimension of interest.</p></sec><sec id="S15"><title>Cross-validation and performance metrics</title><p id="P62">The model predictions are cross-validated as follows: the model estimates the different tuning from the repeats number 1 and 3, and the correlation of the prediction is computed from the remaining repetitions (<xref ref-type="fig" rid="F4">Figure 4</xref> and <xref ref-type="fig" rid="F5">5</xref>).</p><p id="P63">To assess the model’s ability to generalize to previously unseen stimuli, we reserved the last two minutes of the recording as a test set, while training was performed only on the first one to eight minutes of the stimulus presentation.</p><p id="P64">We define repeatability as the Pearson correlation between the mean responses computed from two subsets of stimulus repetitions of equal size.</p><p id="P65">We define predictability as the Pearson correlation between the predicted firing rate and the neuron’s actual firing rate, computed as the average activity on the test set.</p><p id="P66">We define the prediction score as the ratio of predictability to repeatability.</p></sec></sec><sec id="S16"><title>Additional analyses</title><sec id="S17"><title>Smooth estimates of preferred position</title><p id="P67">When generating sign maps, it is necessary to compute retinotopic gradients (<xref ref-type="bibr" rid="R29">Sereno et al., 1995</xref>). To do so, we must impose spatial smoothness on the retinotopic representation and disregard the local scatter observed at the single-neuron scale. For this purpose, when computing sign maps, we smoothed the estimated receptive field positions of the neurons by taking the median of the estimated position for neighboring neurons within a 25 μm radius. This step is optional and can be useful to generate retinotopic maps and segment visual areas. To avoid bias, however, we advise not to use it when analyzing the properties of individual neurons.</p></sec><sec id="S18"><title>Sign maps and visual areas</title><p id="P68">Using each neuron’s preferred retinotopic position and anatomical coordinates, we interpolated these values to estimate the preferred azimuth and elevation at every pixel across the imaging plane, thereby generating continuous retinotopic maps. These maps were then smoothed using a Gaussian kernel to ensure gradient continuity and suppress local noise. We then extract the visual field sign at each pixel, computed as the sine of the angle between the local gradients in azimuth and elevation (<xref ref-type="bibr" rid="R29">Sereno et al., 1995</xref>, <xref ref-type="bibr" rid="R45">Zhuang et al., 2017</xref>).</p><p id="P69">We use the sign map’s regions to segment the cortical space into distinct zones, allowing us to categorize neurons based on the location of their receptive fields in the visual field.: From the sign map, we performed image segmentation using a modified watershed algorithm to identify distinct regions in the visual cortex from their signed values. First, a binary mask was generated based on the signed map, where the threshold was determined by the sign of the values. A morphological opening operation was applied to remove small noise in the binary map. Each pixel was assigned to be “sure back-ground”, “sure foreground”, or “unknown”. To define the regions, the sure background was obtained by dilating the binary map, while the sure foreground was determined using the distance transform followed by a thresholding. The unknown region was then calculated by subtracting the sure foreground from the sure background. Connected components in the sure foreground were labeled to form markers for the watershed algorithm. The markers were adjusted by incrementing the labels of the sure background and setting the unknown regions to zero. The watershed algorithm was then applied to segment the regions, with the signed map serving as the input for the merging process.</p><p id="P70">Finally, for each neuron we identified its corresponding visual area by extracting the watershed labels at its position on the imaging plane, allowing us to segment and classify neuronal regions in the image. V1 was nicely segmented. We define as ‘visually responsive neurons’ all neurons with a repeatability higher than 0.2. We define as Higher visual areas (HVA) any neurons that do not belong to V1 but are visually responsive.</p></sec></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Material</label><media xlink:href="EMS207531-supplement-Supplementary_Material.pdf" mimetype="application" mime-subtype="pdf" id="d2aAcEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S19"><title>Acknowledgments</title><p>We thank Michael Krumin for help with the mesoscope, Charu Reddy for help with surgery, Ali Haydaroğlu for advice on data processing, and Anyi Liu and Tinya Chang for testing our software. This work was supported by EMBO (ALTF 712-2021 to MS), the BRAIN Initiative (grant U01NS126057 to MC) and the Wellcome Trust (grant 223144/Z/21/Z to MC and KDH). MC holds the GlaxoSmithKline / Fight for Sight Chair in Visual Neuroscience.</p></ack><fn-group><fn id="FN1" fn-type="con"><p id="P71">
<bold>Author contributions</bold>
</p><p id="P72">Contributions according to the <italic>CRediT</italic> taxonomy (<ext-link ext-link-type="uri" xlink:href="https://credit.niso.org/">https://credit.niso.org/</ext-link>).</p><p id="P73">
<table-wrap id="T1" position="anchor" orientation="portrait"><table frame="box" rules="all"><thead><tr><th align="center" valign="top"/><th align="center" valign="middle" style="writing-mode:tb-rl;writing-mode:vertical-rl;">Carandini</th><th align="center" valign="middle" style="writing-mode:tb-rl;writing-mode:vertical-rl;">Harris</th><th align="center" valign="middle" style="writing-mode:tb-rl;writing-mode:vertical-rl;">Picard</th><th align="center" valign="middle" style="writing-mode:tb-rl;writing-mode:vertical-rl;">Shinn</th><th align="center" valign="middle" style="writing-mode:tb-rl;writing-mode:vertical-rl;">Skriabine</th></tr></thead><tbody><tr><td align="center" valign="top">Conceptualization</td><td align="center" valign="top">•</td><td align="center" valign="top">•</td><td align="center" valign="top"/><td align="center" valign="top">•</td><td align="center" valign="top">•</td></tr><tr><td align="center" valign="top">Data curation</td><td align="center" valign="top"/><td align="center" valign="top"/><td align="center" valign="top">•</td><td align="center" valign="top"/><td align="center" valign="top">•</td></tr><tr><td align="center" valign="top">Formal analysis</td><td align="center" valign="top"/><td align="center" valign="top"/><td align="center" valign="top"/><td align="center" valign="top">•</td><td align="center" valign="top">•</td></tr><tr><td align="center" valign="top">Funding acquisition</td><td align="center" valign="top">•</td><td align="center" valign="top">•</td><td align="center" valign="top"/><td align="center" valign="top">•</td><td align="center" valign="top"/></tr><tr><td align="center" valign="top">Investigation</td><td align="center" valign="top"/><td align="center" valign="top"/><td align="center" valign="top">•</td><td align="center" valign="top"/><td align="center" valign="top">•</td></tr><tr><td align="center" valign="top">Methodology</td><td align="center" valign="top">•</td><td align="center" valign="top">•</td><td align="center" valign="top"/><td align="center" valign="top">•</td><td align="center" valign="top">•</td></tr><tr><td align="center" valign="top">Project administration</td><td align="center" valign="top">•</td><td align="center" valign="top"/><td align="center" valign="top"/><td align="center" valign="top"/><td align="center" valign="top"/></tr><tr><td align="center" valign="top">Software</td><td align="center" valign="top"/><td align="center" valign="top"/><td align="center" valign="top"/><td align="center" valign="top">•</td><td align="center" valign="top">•</td></tr><tr><td align="center" valign="top">Resources</td><td align="center" valign="top">•</td><td align="center" valign="top">•</td><td align="center" valign="top"/><td align="center" valign="top"/><td align="center" valign="top"/></tr><tr><td align="center" valign="top">Supervision</td><td align="center" valign="top">•</td><td align="center" valign="top">•</td><td align="center" valign="top"/><td align="center" valign="top"/><td align="center" valign="top"/></tr><tr><td align="center" valign="top">Validation</td><td align="center" valign="top"/><td align="center" valign="top"/><td align="center" valign="top"/><td align="center" valign="top"/><td align="center" valign="top">•</td></tr><tr><td align="center" valign="top">Visualization</td><td align="center" valign="top">•</td><td align="center" valign="top"/><td align="center" valign="top"/><td align="center" valign="top">•</td><td align="center" valign="top">•</td></tr><tr><td align="center" valign="top">Original draft</td><td align="center" valign="top">•</td><td align="center" valign="top"/><td align="center" valign="top"/><td align="center" valign="top">•</td><td align="center" valign="top">•</td></tr><tr><td align="center" valign="top">Review &amp; editing</td><td align="center" valign="top">•</td><td align="center" valign="top">•</td><td align="center" valign="top"/><td align="center" valign="top">•</td><td align="center" valign="top">•</td></tr></tbody></table></table-wrap>
</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benucci</surname><given-names>A</given-names></name><name><surname>Ringach</surname><given-names>DL</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name></person-group><article-title>Coding of stimulus sequences by population responses in visual cortex</article-title><source>Nature Neuroscience</source><year>2009</year><volume>12</volume><issue>10</issue><fpage>1317</fpage><lpage>1324</lpage><pub-id pub-id-type="doi">10.1038/nn.2398</pub-id><pub-id pub-id-type="pmcid">PMC2847499</pub-id><pub-id pub-id-type="pmid">19749748</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonin</surname><given-names>V</given-names></name><name><surname>Histed</surname><given-names>MH</given-names></name><name><surname>Yurgenson</surname><given-names>S</given-names></name><name><surname>Reid</surname><given-names>RC</given-names></name></person-group><article-title>Local Diversity and Fine-Scale Organization of Receptive Fields in Mouse Visual Cortex</article-title><source>Journal of Neuroscience</source><year>2011</year><volume>31</volume><issue>50</issue><fpage>18506</fpage><lpage>18521</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2974-11.2011</pub-id><pub-id pub-id-type="pmcid">PMC3758577</pub-id><pub-id pub-id-type="pmid">22171051</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cadena</surname><given-names>SA</given-names></name><name><surname>Denfield</surname><given-names>GH</given-names></name><name><surname>Walker</surname><given-names>EY</given-names></name><name><surname>Gatys</surname><given-names>LA</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name></person-group><article-title>Deep convolutional models improve predictions of macaque V1 responses to natural images</article-title><source>PLOS Computational Biology</source><year>2019</year><volume>15</volume><issue>4</issue><elocation-id>e1006897</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006897</pub-id><pub-id pub-id-type="pmcid">PMC6499433</pub-id><pub-id pub-id-type="pmid">31013278</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Vries</surname><given-names>SEJ</given-names></name><name><surname>Lecoq</surname><given-names>JA</given-names></name><name><surname>Buice</surname><given-names>MA</given-names></name><name><surname>Groblewski</surname><given-names>PA</given-names></name><name><surname>Ocker</surname><given-names>GK</given-names></name><name><surname>Oliver</surname><given-names>M</given-names></name><name><surname>Feng</surname><given-names>D</given-names></name><name><surname>Cain</surname><given-names>N</given-names></name><name><surname>Ledochowitsch</surname><given-names>P</given-names></name><name><surname>Millman</surname><given-names>D</given-names></name><name><surname>Roll</surname><given-names>K</given-names></name><etal/></person-group><article-title>A large-scale standardized physiological survey reveals functional organization of the mouse visual cortex</article-title><source>Nature Neuroscience</source><year>2020</year><volume>23</volume><issue>1</issue><fpage>138</fpage><lpage>151</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0550-9</pub-id><pub-id pub-id-type="pmcid">PMC6948932</pub-id><pub-id pub-id-type="pmid">31844315</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dhruv</surname><given-names>NT</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name></person-group><article-title>Cascaded effects of spatial adaptation in the early visual system</article-title><source>Neuron</source><year>2014</year><volume>81</volume><issue>3</issue><fpage>529</fpage><lpage>535</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.11.025</pub-id><pub-id pub-id-type="pmcid">PMC3969249</pub-id><pub-id pub-id-type="pmid">24507190</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dräger</surname><given-names>UC</given-names></name></person-group><article-title>Receptive fields of single cells and topography in mouse visual cortex</article-title><source>The Journal of Comparative Neurology</source><year>1975</year><volume>160</volume><issue>3</issue><fpage>269</fpage><lpage>289</lpage><pub-id pub-id-type="pmid">1112925</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Du</surname><given-names>F</given-names></name><name><surname>Angel Núñez-Ochoa</surname><given-names>M</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Stringer</surname><given-names>C</given-names></name></person-group><article-title>A simplified minimodel of visual cortical neurons</article-title><source>Nature Communications</source><year>2025</year><volume>16</volume><issue>1</issue><fpage>1</fpage><lpage>13</lpage><comment>2025 16:1</comment><pub-id pub-id-type="doi">10.1038/s41467-025-61171-9</pub-id><pub-id pub-id-type="pmcid">PMC12219398</pub-id><pub-id pub-id-type="pmid">40593666</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dumoulin</surname><given-names>SO</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><article-title>Population receptive field estimates in human visual cortex</article-title><source>NeuroImage</source><year>2008</year><volume>39</volume><issue>2</issue><fpage>647</fpage><lpage>660</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.09.034</pub-id><pub-id pub-id-type="pmcid">PMC3073038</pub-id><pub-id pub-id-type="pmid">17977024</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erisken</surname><given-names>S</given-names></name><name><surname>Vaiceliunaite</surname><given-names>A</given-names></name><name><surname>Jurjut</surname><given-names>O</given-names></name><name><surname>Fiorini</surname><given-names>M</given-names></name><name><surname>Katzner</surname><given-names>S</given-names></name><name><surname>Busse</surname><given-names>L</given-names></name></person-group><article-title>Effects of locomotion extend throughout the mouse early visual system</article-title><source>Current Biology: CB</source><year>2014</year><volume>24</volume><issue>24</issue><fpage>2899</fpage><lpage>2907</lpage><pub-id pub-id-type="pmid">25484299</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Felsen</surname><given-names>G</given-names></name><name><surname>Touryan</surname><given-names>J</given-names></name><name><surname>Han</surname><given-names>F</given-names></name><name><surname>Dan</surname><given-names>Y</given-names></name></person-group><article-title>Cortical Sensitivity to Visual Features in Natural Scenes</article-title><source>PLOS Biology</source><year>2005</year><volume>3</volume><issue>10</issue><fpage>e342</fpage><pub-id pub-id-type="doi">10.1371/journal.pbio.0030342</pub-id><pub-id pub-id-type="pmcid">PMC1233414</pub-id><pub-id pub-id-type="pmid">16171408</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname><given-names>DH</given-names></name><name><surname>Wiesel</surname><given-names>TN</given-names></name></person-group><article-title>Receptive fields of single neurones in the cat’s striate cortex</article-title><source>The Journal of Physiology</source><year>1959</year><volume>148</volume><issue>3</issue><fpage>574</fpage><lpage>591</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1959.sp006308</pub-id><pub-id pub-id-type="pmcid">PMC1363130</pub-id><pub-id pub-id-type="pmid">14403679</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>JP</given-names></name><name><surname>Palmer</surname><given-names>LA</given-names></name></person-group><article-title>An evaluation of the two-dimensional Gabor filter model of simple receptive fields in cat striate cortex</article-title><source>Journal of Neurophysiology</source><year>1987a</year><volume>58</volume><issue>6</issue><fpage>1233</fpage><lpage>1258</lpage><pub-id pub-id-type="pmid">3437332</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>JP</given-names></name><name><surname>Palmer</surname><given-names>LA</given-names></name></person-group><article-title>The two-dimensional spatial structure of simple receptive fields in cat striate cortex</article-title><year>1987b</year><volume>58</volume><issue>6</issue><fpage>1187</fpage><lpage>1211</lpage><pub-id pub-id-type="pmid">3437330</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Larsson</surname><given-names>J</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><article-title>Two Retinotopic Visual Areas in Human Lateral Occipital Cortex</article-title><source>Journal of Neuroscience</source><year>2006</year><volume>26</volume><issue>51</issue><fpage>13128</fpage><lpage>13142</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1657-06.2006</pub-id><pub-id pub-id-type="pmcid">PMC1904390</pub-id><pub-id pub-id-type="pmid">17182764</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><collab>MICrONS Consortium</collab><person-group person-group-type="author"><name><surname>Bae</surname><given-names>JA</given-names></name><name><surname>Baptiste</surname><given-names>M</given-names></name><name><surname>Bishop</surname><given-names>CA</given-names></name><name><surname>Bodor</surname><given-names>AL</given-names></name><name><surname>Brittain</surname><given-names>D</given-names></name><name><surname>Buchanan</surname><given-names>J</given-names></name><name><surname>Bumbarger</surname><given-names>DJ</given-names></name><name><surname>Castro</surname><given-names>MA</given-names></name><name><surname>Celii</surname><given-names>B</given-names></name><name><surname>Cobos</surname><given-names>E</given-names></name><name><surname>Collman</surname><given-names>F</given-names></name><etal/></person-group><article-title>Functional connectomics spanning multiple areas of mouse visual cortex</article-title><year>2023</year><pub-id pub-id-type="doi">10.1038/s41586-025-08790-w</pub-id><pub-id pub-id-type="pmcid">PMC11981939</pub-id><pub-id pub-id-type="pmid">40205214</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Movshon</surname><given-names>JA</given-names></name><name><surname>Thompson</surname><given-names>ID</given-names></name><name><surname>Tolhurst</surname><given-names>DJ</given-names></name></person-group><article-title>Receptive field organization of complex cells in the cat’s striate cortex</article-title><source>The Journal of Physiology</source><year>1978a</year><volume>283</volume><issue>1</issue><fpage>79</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1978.sp012489</pub-id><pub-id pub-id-type="pmcid">PMC1282766</pub-id><pub-id pub-id-type="pmid">722592</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Movshon</surname><given-names>JA</given-names></name><name><surname>Thompson</surname><given-names>ID</given-names></name><name><surname>Tolhurst</surname><given-names>DJ</given-names></name></person-group><article-title>Spatial summation in the receptive fields of simple cells in the cat’s striate cortex</article-title><source>The Journal of Physiology</source><year>1978b</year><volume>283</volume><issue>1</issue><fpage>53</fpage><lpage>77</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1978.sp012488</pub-id><pub-id pub-id-type="pmcid">PMC1282765</pub-id><pub-id pub-id-type="pmid">722589</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niell</surname><given-names>CM</given-names></name><name><surname>Stryker</surname><given-names>MP</given-names></name></person-group><article-title>Highly Selective Receptive Fields in Mouse Visual Cortex</article-title><source>Journal of Neuroscience</source><year>2008</year><volume>28</volume><issue>30</issue><fpage>7520</fpage><lpage>7536</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0623-08.2008</pub-id><pub-id pub-id-type="pmcid">PMC3040721</pub-id><pub-id pub-id-type="pmid">18650330</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niell</surname><given-names>CM</given-names></name><name><surname>Stryker</surname><given-names>MP</given-names></name></person-group><article-title>Modulation of visual responses by behavioral state in mouse visual cortex</article-title><source>Neuron</source><year>2010</year><volume>65</volume><issue>4</issue><fpage>472</fpage><lpage>479</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.01.033</pub-id><pub-id pub-id-type="pmcid">PMC3184003</pub-id><pub-id pub-id-type="pmid">20188652</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ohki</surname><given-names>K</given-names></name><name><surname>Reid</surname><given-names>RC</given-names></name></person-group><article-title>Specificity and randomness in the visual cortex</article-title><source>Current Opinion in Neurobiology</source><year>2007</year><volume>17</volume><issue>4</issue><fpage>401</fpage><lpage>407</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2007.07.007</pub-id><pub-id pub-id-type="pmcid">PMC2951601</pub-id><pub-id pub-id-type="pmid">17720489</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’ Keefe</surname><given-names>J</given-names></name><name><surname>Burgess</surname><given-names>N</given-names></name></person-group><article-title>Geometric determinants of the place fields of hippocampal neurons</article-title><source>Nature</source><year>1996</year><volume>381</volume><issue>6581</issue><fpage>425</fpage><lpage>428</lpage><comment>1996 381:6581</comment><pub-id pub-id-type="pmid">8632799</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Dipoppa</surname><given-names>M</given-names></name><name><surname>Schröder</surname><given-names>S</given-names></name><name><surname>Rossi</surname><given-names>LF</given-names></name><name><surname>Dalgleish</surname><given-names>H</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><article-title>Suite2p: beyond 10,000 neurons with standard two-photon microscopy</article-title><source>BioRxiv</source><year>2017</year><elocation-id>061507</elocation-id><pub-id pub-id-type="doi">10.1101/061507</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parker</surname><given-names>PRL</given-names></name><name><surname>Martins</surname><given-names>DM</given-names></name><name><surname>Leonard</surname><given-names>ESP</given-names></name><name><surname>Casey</surname><given-names>NM</given-names></name><name><surname>Sharp</surname><given-names>SL</given-names></name><name><surname>Abe</surname><given-names>ETT</given-names></name><name><surname>Smear</surname><given-names>MC</given-names></name><name><surname>Yates</surname><given-names>JL</given-names></name><name><surname>Mitchell</surname><given-names>JF</given-names></name><name><surname>Niell</surname><given-names>CM</given-names></name></person-group><article-title>A dynamic sequence of visual processing initiated by gaze shifts</article-title><source>Nature Neuroscience</source><year>2023</year><volume>26</volume><issue>12</issue><fpage>2192</fpage><lpage>2202</lpage><pub-id pub-id-type="doi">10.1038/s41593-023-01481-7</pub-id><pub-id pub-id-type="pmcid">PMC11270614</pub-id><pub-id pub-id-type="pmid">37996524</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perlin</surname><given-names>K</given-names></name></person-group><article-title>An image synthesizer</article-title><source>SIGGRAPH Comput Graph</source><year>1985</year><volume>19</volume><issue>3</issue><fpage>287</fpage><lpage>296</lpage><pub-id pub-id-type="doi">10.1145/325165.325247</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pologruto</surname><given-names>TA</given-names></name><name><surname>Sabatini</surname><given-names>BL</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><article-title>Scanlmage: Flexible software for operating laser scanning microscopes</article-title><source>BioMedical Engineering OnLine</source><year>2003</year><volume>2</volume><fpage>13</fpage><pub-id pub-id-type="doi">10.1186/1475-925X-2-13</pub-id><pub-id pub-id-type="pmcid">PMC161784</pub-id><pub-id pub-id-type="pmid">12801419</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ribeiro</surname><given-names>FL</given-names></name><name><surname>Benson</surname><given-names>NC</given-names></name><name><surname>Puckett</surname><given-names>AM</given-names></name></person-group><article-title>Human retinotopic mapping: From empirical to computational models of retinotopy</article-title><source>Journal of Vision</source><year>2025</year><volume>25</volume><issue>8</issue><fpage>14</fpage><pub-id pub-id-type="doi">10.1167/jov.25.8.14</pub-id><pub-id pub-id-type="pmcid">PMC12279071</pub-id><pub-id pub-id-type="pmid">40668063</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ringach</surname><given-names>DL</given-names></name><name><surname>Hawken</surname><given-names>MJ</given-names></name><name><surname>Shapley</surname><given-names>R</given-names></name></person-group><article-title>Dynamics of orientation tuning in macaque primary visual cortex</article-title><source>Nature</source><year>1997</year><volume>387</volume><issue>6630</issue><fpage>281</fpage><lpage>284</lpage><pub-id pub-id-type="pmid">9153392</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schröder</surname><given-names>S</given-names></name><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Krumin</surname><given-names>M</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Rizzi</surname><given-names>M</given-names></name><name><surname>Lagnado</surname><given-names>L</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name></person-group><article-title>Arousal Modulates Retinal Output</article-title><source>Neuron</source><year>2020</year><volume>107</volume><issue>3</issue><fpage>487</fpage><lpage>495</lpage><elocation-id>e9</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuron.2020.04.026</pub-id><pub-id pub-id-type="pmcid">PMC7427318</pub-id><pub-id pub-id-type="pmid">32445624</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sereno</surname><given-names>MI</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Reppas</surname><given-names>JB</given-names></name><name><surname>Kwong</surname><given-names>KK</given-names></name><name><surname>Belliveau</surname><given-names>JW</given-names></name><name><surname>Brady</surname><given-names>TJ</given-names></name><name><surname>Rosen</surname><given-names>BR</given-names></name><name><surname>Tootell</surname><given-names>RBH</given-names></name></person-group><article-title>Borders of multiple visual areas in humans revealed by functional magnetic resonance imaging</article-title><source>Science (New York, N Y)</source><year>1995</year><volume>268</volume><issue>5212</issue><fpage>889</fpage><lpage>893</lpage><pub-id pub-id-type="pmid">7754376</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sofroniew</surname><given-names>NJ</given-names></name><name><surname>Flickinger</surname><given-names>D</given-names></name><name><surname>King</surname><given-names>J</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><article-title>A large field of view two-photon mesoscope with subcellular resolution for in vivo imaging</article-title><source>ELife</source><year>2016</year><volume>5</volume><comment>JUN2016</comment><pub-id pub-id-type="doi">10.7554/eLife.14472</pub-id><pub-id pub-id-type="pmcid">PMC4951199</pub-id><pub-id pub-id-type="pmid">27300105</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tong</surname><given-names>R</given-names></name><name><surname>da Silva</surname><given-names>R</given-names></name><name><surname>Lin</surname><given-names>D</given-names></name><name><surname>Ghosh</surname><given-names>A</given-names></name><name><surname>Wilsenach</surname><given-names>J</given-names></name><name><surname>Cianfarano</surname><given-names>E</given-names></name><name><surname>Bashivan</surname><given-names>P</given-names></name><name><surname>Richards</surname><given-names>B</given-names></name><name><surname>Trenholm</surname><given-names>S</given-names></name></person-group><article-title>The feature landscape of visual cortex</article-title><source>BioRxiv</source><year>2023</year><elocation-id>2023.11.03.565500</elocation-id><pub-id pub-id-type="doi">10.1101/2023.11.03.565500</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Touryan</surname><given-names>J</given-names></name><name><surname>Felsen</surname><given-names>G</given-names></name><name><surname>Dan</surname><given-names>Y</given-names></name></person-group><article-title>Spatial structure of complex cell receptive fields measured with natural images</article-title><source>Neuron</source><year>2005</year><volume>45</volume><issue>5</issue><fpage>781</fpage><lpage>791</lpage><pub-id pub-id-type="pmid">15748852</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vintch</surname><given-names>B</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><article-title>A Convolutional Subunit Model for Neuronal Responses in Macaque V1</article-title><source>Journal of Neuroscience</source><year>2015</year><volume>35</volume><issue>44</issue><fpage>14829</fpage><lpage>14841</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2815-13.2015</pub-id><pub-id pub-id-type="pmcid">PMC4635132</pub-id><pub-id pub-id-type="pmid">26538653</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walker</surname><given-names>EY</given-names></name><name><surname>Sinz</surname><given-names>FH</given-names></name><name><surname>Cobos</surname><given-names>E</given-names></name><name><surname>Muhammad</surname><given-names>T</given-names></name><name><surname>Froudarakis</surname><given-names>E</given-names></name><name><surname>Fahey</surname><given-names>PG</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name><name><surname>Reimer</surname><given-names>J</given-names></name><name><surname>Pitkow</surname><given-names>X</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name></person-group><article-title>Inception loops discover what excites neurons most using deep predictive models</article-title><source>Nature Neuroscience</source><year>2019</year><volume>22</volume><issue>12</issue><fpage>2060</fpage><lpage>2065</lpage><pub-id pub-id-type="pmid">31686023</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>EY</given-names></name><name><surname>Fahey</surname><given-names>PG</given-names></name><name><surname>Ding</surname><given-names>Z</given-names></name><name><surname>Papadopoulos</surname><given-names>S</given-names></name><name><surname>Ponder</surname><given-names>K</given-names></name><name><surname>Weis</surname><given-names>MA</given-names></name><name><surname>Chang</surname><given-names>A</given-names></name><name><surname>Muhammad</surname><given-names>T</given-names></name><name><surname>Patel</surname><given-names>S</given-names></name><name><surname>Ding</surname><given-names>Z</given-names></name><name><surname>Tran</surname><given-names>D</given-names></name><etal/></person-group><article-title>Foundation model of neural activity predicts response to new stimulus types</article-title><source>Nature</source><year>2025</year><volume>640</volume><issue>8058</issue><fpage>470</fpage><lpage>477</lpage><comment>2025 640:8058</comment><pub-id pub-id-type="doi">10.1038/s41586-025-08829-y</pub-id><pub-id pub-id-type="pmcid">PMC11981942</pub-id><pub-id pub-id-type="pmid">40205215</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>EY</given-names></name><name><surname>Fahey</surname><given-names>PG</given-names></name><name><surname>Ponder</surname><given-names>K</given-names></name><name><surname>Ding</surname><given-names>Z</given-names></name><name><surname>Chang</surname><given-names>A</given-names></name><name><surname>Muhammad</surname><given-names>T</given-names></name><name><surname>Patel</surname><given-names>S</given-names></name><name><surname>Ding</surname><given-names>Z</given-names></name><name><surname>Tran</surname><given-names>D</given-names></name><name><surname>Fu</surname><given-names>J</given-names></name><name><surname>Papadopoulos</surname><given-names>S</given-names></name><etal/></person-group><article-title>Towards a Foundation Model of the Mouse Visual Cortex</article-title><comment>n.d</comment><pub-id pub-id-type="doi">10.1101/2023.03.21.533548</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Ding</surname><given-names>SL</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Royall</surname><given-names>J</given-names></name><name><surname>Feng</surname><given-names>D</given-names></name><name><surname>Lesnar</surname><given-names>P</given-names></name><name><surname>Graddis</surname><given-names>N</given-names></name><name><surname>Naeemi</surname><given-names>M</given-names></name><name><surname>Facer</surname><given-names>B</given-names></name><name><surname>Ho</surname><given-names>A</given-names></name><name><surname>Dolbeare</surname><given-names>T</given-names></name><etal/></person-group><article-title>The Allen Mouse Brain Common Coordinate Framework: A 3D Reference Atlas</article-title><source>Cell</source><year>2020</year><volume>181</volume><issue>4</issue><fpage>936</fpage><lpage>953</lpage><elocation-id>e20</elocation-id><pub-id pub-id-type="doi">10.1016/j.cell.2020.04.007</pub-id><pub-id pub-id-type="pmcid">PMC8152789</pub-id><pub-id pub-id-type="pmid">32386544</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>T</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Guo</surname><given-names>C</given-names></name><name><surname>Lee</surname><given-names>A</given-names></name><name><surname>Lee</surname><given-names>JS</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name></person-group><article-title>jGCaMP8 transgenic mice</article-title><source>Janelia Research Campus Dataset</source><year>2023</year><pub-id pub-id-type="doi">10.25378/janelia.24565837.v3</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wekselblatt</surname><given-names>JB</given-names></name><name><surname>Flister</surname><given-names>ED</given-names></name><name><surname>Piscopo</surname><given-names>DM</given-names></name><name><surname>Niell</surname><given-names>CM</given-names></name></person-group><article-title>Large-scale imaging of cortical dynamics during sensory perception and behavior</article-title><source>Journal of Neurophysiology</source><year>2016</year><volume>115</volume><issue>6</issue><fpage>2852</fpage><lpage>2866</lpage><pub-id pub-id-type="doi">10.1152/jn.01056.2015</pub-id><pub-id pub-id-type="pmcid">PMC4922607</pub-id><pub-id pub-id-type="pmid">26912600</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willeke</surname><given-names>KF</given-names></name><name><surname>Fahey</surname><given-names>PG</given-names></name><name><surname>Bashiri</surname><given-names>M</given-names></name><name><surname>Hansel</surname><given-names>L</given-names></name><name><surname>Blessing</surname><given-names>C</given-names></name><name><surname>Lurz</surname><given-names>K-K</given-names></name><name><surname>Burg</surname><given-names>MF</given-names></name><name><surname>Cadena</surname><given-names>SA</given-names></name><name><surname>Ding</surname><given-names>Z</given-names></name><name><surname>Ponder</surname><given-names>K</given-names></name><name><surname>Muhammad</surname><given-names>T</given-names></name><etal/></person-group><article-title>Retrospective on the SENSORIUM 2022 competition</article-title><source>Proceedings of Machine Learning Research</source><year>2023</year><volume>220</volume><fpage>314</fpage><lpage>333</lpage></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Cadieu</surname><given-names>CF</given-names></name><name><surname>Solomon</surname><given-names>EA</given-names></name><name><surname>Seibert</surname><given-names>D</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2014</year><volume>111</volume><issue>23</issue><fpage>8619</fpage><lpage>8624</lpage><comment>/-/DCSUPPLEMENTAL/PNAS.201403112SI.PDF</comment><pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id><pub-id pub-id-type="pmcid">PMC4060707</pub-id><pub-id pub-id-type="pmid">24812127</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Yatsenko</surname><given-names>D</given-names></name><name><surname>Fahey</surname></name><name><surname>Froudarakis</surname></name><name><surname>Reimer</surname></name><name><surname>Walker</surname></name><name><surname>Sinz</surname></name><name><surname>Cobos</surname></name><name><surname>Tolias</surname></name></person-group><source>trippy-monet/yatsenko-SfN2018-lowres.pdf at master · dimitri-yatsenko/trippy-monet</source><year>2018</year><comment><ext-link ext-link-type="uri" xlink:href="https://github.com/dimitri-yatsenko/trippy-monet/blob/master/yatsenko-SfN2018-lowres.pdf">https://github.com/dimitri-yatsenko/trippy-monet/blob/master/yatsenko-SfN2018-lowres.pdf</ext-link></comment></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoshida</surname><given-names>T</given-names></name><name><surname>Ohki</surname><given-names>K</given-names></name></person-group><article-title>Natural images are reliably represented by sparse and variable populations of neurons in visual cortex</article-title><source>Nature Communications</source><year>2020</year><volume>11</volume><issue>1</issue><fpage>1</fpage><lpage>19</lpage><comment>2020 11:1</comment><pub-id pub-id-type="doi">10.1038/s41467-020-14645-x</pub-id><pub-id pub-id-type="pmcid">PMC7018721</pub-id><pub-id pub-id-type="pmid">32054847</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Ró</surname><given-names>M</given-names></name><name><surname>Liang</surname><given-names>Y</given-names></name><name><surname>Bushey</surname><given-names>D</given-names></name><name><surname>Wei</surname><given-names>Z</given-names></name><name><surname>Zheng</surname><given-names>J</given-names></name><name><surname>Reep</surname><given-names>D</given-names></name><name><surname>Joey Broussard</surname><given-names>G</given-names></name><name><surname>Tsang</surname><given-names>A</given-names></name><name><surname>Tsegaye</surname><given-names>G</given-names></name><name><surname>Narayan</surname><given-names>S</given-names></name><etal/></person-group><article-title>Fast and sensitive GCaMP calcium indicators for imaging neural populations</article-title><source>Nature</source><year>2023</year><volume>615</volume><fpage>884</fpage><pub-id pub-id-type="doi">10.1038/s41586-023-05828-9</pub-id><pub-id pub-id-type="pmcid">PMC10060165</pub-id><pub-id pub-id-type="pmid">36922596</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhuang</surname><given-names>J</given-names></name><name><surname>Ng</surname><given-names>L</given-names></name><name><surname>Williams</surname><given-names>D</given-names></name><name><surname>Valley</surname><given-names>M</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Garrett</surname><given-names>M</given-names></name><name><surname>Waters</surname><given-names>J</given-names></name></person-group><article-title>An extended retinotopic map of mouse cortex</article-title><source>ELife</source><year>2017</year><volume>6</volume><fpage>1</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.7554/eLife.18372</pub-id><pub-id pub-id-type="pmcid">PMC5218535</pub-id><pub-id pub-id-type="pmid">28059700</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>The Zebra noise stimulus.</title><p><bold>a</bold>: A frame of fractal Perlin noise, with contours at 8 gray levels (yellow curves). <bold>b</bold>: The resulting frame of Zebra noise, obtained by applying an 8-tooth comb threshold to the intensity scale. <bold>c</bold>: A coarser version of the Zebra noise obtained by applying a 4-tooth comb threshold to the same frame of 1/f noise. <bold>d</bold>: Vector field showing the movement of the edges in the Zebra noise in b. For clarity, the spatial pattern is shown at reduced contrast. <bold>e</bold>: Spatial autocorrelation of the Zebra noise movie. <bold>f</bold>: Spatial frequency content of the Zebra noise stimuli in b,c. <bold>g</bold>: Orientation content of the two Zebra noise stimuli. The quantities in e-g were computed over 1,500 stimulus frames.</p></caption><graphic xlink:href="EMS207531-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Zebra noise elicits repeatable responses in the mouse visual cortex.</title><p><bold>a</bold>: Head-fixed mice were free to run on a wheel and viewed images on two screens, in front and on the left, while a 2-photon microscope imaged the right visual cortex (<italic>green rectangle</italic>). <bold>b</bold>, Mean fluorescence image from the 2-photon microscope. Curves outline primary visual cortex (V1) and higher visual areas (HVA), as defined by the Allen Mouse Brain Common Coordinate Framework (<xref ref-type="bibr" rid="R37">Q. Wang et al., 2020</xref>). <bold>c</bold>: Neurons detected in the field of view (with random colors). <bold>d</bold>: Responses of an example neuron to three repeats of Zebra noise. <italic>Scale bar</italic> shows 5 arbitrary units of deconvolved firing rate. <bold>e</bold>: Repeatability across repeats of the responses of the neurons in the field of view for Zebra noise. <bold>f</bold>: Responses of the same neuron as d to sparse noise stimuli. <bold>g</bold>, Repeatability of neuronal responses for sparse noise. <bold>h</bold>: Comparison of repeatability of responses to sparse noise compared to Zebra noise in neurons that gave repeatable visual responses (<italic>green</italic>) and remaining neurons (<italic>black</italic>). <bold>i-k</bold>: Same as f-h, for drifting grating stimuli.</p></caption><graphic xlink:href="EMS207531-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Wavelet transform of a Zebra noise video.</title><p><bold>a</bold>. Each Gabor wavelet has a position, orientation, size, and frequency. One example wavelet is shown. <bold>b</bold>. Examples of Gabor wavelets differing in size and frequency. <bold>c</bold>. A frame of Zebra noise. <bold>d</bold>. The result of filtering the stimulus frame with a sine-phase Gabor wavelet of intermediate scale and frequency, at three orientations. Red indicates positive values, blue negative values. <bold>e</bold>. The amplitude of the filtered image, obtained by combining the output of the sine-phase and cosine-phase Gabor wavelets.</p></caption><graphic xlink:href="EMS207531-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Fitting the wavelet-based model and obtaining tuning curves.</title><p><bold>a</bold>: Correlation between the responses of an example neuron and the wavelet transform of the stimulus, as a function of wavelet position, for 3 example wavelet orientations (<italic>insets</italic>). Correlation is strongest for the 45 deg orientation at a particular position (<italic>dashed lines</italic>). <bold>b</bold>: Firing rate of the example neuron during an interval of Zebra noise. <bold>c</bold>: Amplitude, phase, and phase derivative of the response of the best-fitting Gabor wavelet as a function of time, compared to the neuron’s firing rate (<italic>dashed</italic>). <bold>d</bold>: Cartoon of the model. The image is filtered with two versions of the best-fitting wavelet, in sine and cosine phase. The two outputs are then projected into a three-dimensional space with coordinates amplitude <italic>A,</italic> phase <italic>φ,</italic> and drift rate <italic>φ′</italic> (the derivative of phase), providing a predicted response for each combination of these 3 values. <bold>e</bold>: Output of the model for the example neuron, compared to the measured firing rate (<italic>dashed curve</italic>). <bold>f</bold>: Tuning of the example neuron for azimuth, elevation, orientation, size (radius), and frequency. <bold>g</bold>: Tuning of the example neuron for wavelet amplitude, phase, and drift rate, showing that this neuron was highly selective for stimulus direction.</p></caption><graphic xlink:href="EMS207531-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Mapping of neural tuning parameters and model predictive quality</title><p>
<bold>a</bold>: Map of preferred azimuth in the visual cortex of an example mouse. Each dot is a neuron. Its color is the preferred azimuth in visual degrees. The different visual areas are delineated by the black contours. <bold>b-e</bold>: Same for preferred elevation (b), size (radius) (c), spatial frequency (d), and orientation (e). <bold>f</bold>: Sign map, showing areas where a clockwise rotation in cortex marks a clockwise (<italic>red</italic>) or counter-clockwise (<italic>blue</italic>) rotation in the visual field. <bold>g</bold>: Map of the predictability of the neuron. The color shows the cross-validated correlation between the model prediction and the neuron’s firing rate. <bold>h</bold>: Relation between predictability by the model and repeatability of a neuron’s firing rate across repeats of the same stimulus. Each dot is a neuron, colored according to whether it had high repeatability and was in V1 (<italic>black</italic>) or higher visual areas (<italic>green</italic>) or it had low repeatability (&lt;0.2) (<italic>gray</italic>). The marginal distribution for repeatability and predictability are shown on the sides. The red line represents the linear fit. Its slope (0.82) indicates that the model captures the aspects of the responses that are repeatable across trials.</p></caption><graphic xlink:href="EMS207531-f005"/></fig></floats-group></article>