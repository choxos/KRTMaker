<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS206304</article-id><article-id pub-id-type="doi">10.1101/2025.06.05.657930</article-id><article-id pub-id-type="archive">PPR1032485</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">2</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Neural coding of spectrotemporal modulations in the auditory cortex supports speech and music categorization</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ginzburg</surname><given-names>Jérémie</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Debaque</surname><given-names>Émilie Cloutier</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Borderie</surname><given-names>Arthur</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Morillon</surname><given-names>Benjamin</given-names></name><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Martineau</surname><given-names>Laurence</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Bonaventure</surname><given-names>Paule Lessard</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Zatorre</surname><given-names>Robert J</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="fn" rid="FN1">6</xref></contrib><contrib contrib-type="author"><name><surname>Albouy</surname><given-names>Philippe</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref><xref ref-type="fn" rid="FN1">6</xref></contrib></contrib-group><aff id="A1"><label>1</label>Montreal Neurological Institute, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01pxwe438</institution-id><institution>McGill University</institution></institution-wrap>, <city>Montreal</city>, <country country="CA">Canada</country></aff><aff id="A2"><label>2</label>Centre for Research in Brain, Language and Music (CRBLM), and <institution-wrap><institution-id institution-id-type="ror">https://ror.org/05yfz9t60</institution-id><institution>International Laboratory for Brain Music and Sound Research (BRAMS)</institution></institution-wrap>, <city>Montreal</city>, <country country="CA">Canada</country></aff><aff id="A3"><label>3</label>CERVO Brain research center, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/04sjchr03</institution-id><institution>Laval University</institution></institution-wrap>, <city>Québec</city>, <country country="CA">Canada</country></aff><aff id="A4"><label>4</label>Centre Hospitalier Universitaire de Québec, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/04sjchr03</institution-id><institution>Laval University</institution></institution-wrap>, <city>Québec</city>, <country country="CA">Canada</country></aff><aff id="A5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/035xkbk20</institution-id><institution>Aix-Marseille Université</institution></institution-wrap>, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02vjkv261</institution-id><institution>INSERM</institution></institution-wrap>, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/019kqby73</institution-id><institution>Institut de Neurosciences des Systèmes</institution></institution-wrap>, <city>Marseille</city>, <country country="FR">France</country></aff><author-notes><corresp id="CR1">
<label>*</label>Lead and corresponding author: <bold>Contact Information:</bold> Correspondance: <email>jeremie.ginzburg@mcgill.ca</email></corresp><fn id="FN1"><label>6</label><p id="P1">Senior authors</p></fn></author-notes><pub-date pub-type="nihms-submitted"><day>08</day><month>06</month><year>2025</year></pub-date><pub-date pub-type="preprint"><day>07</day><month>06</month><year>2025</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 International license</ext-link>.</license-p></license></permissions><abstract><title>Summary</title><p id="P2">Humans effortlessly distinguish speech from music, but the neural basis of this categorization remains debated. Here, we combined intracranial recordings in humans with behavioral categorization of speech or music taken from a naturalistic movie soundtrack to test whether the encoding of spectrotemporal modulation (STM) acoustic features is sufficient to categorize the two classes. We show that primarily temporal and primarily spectral modulation patterns characterize speech and music, respectively, and that non-primary auditory regions robustly track these acoustic features over time. Critically, cortical representations of STMs predicted perceptual judgments gathered in an independent sample. Finally, speech- and music-related STM representations showed stronger tracking of category-specific acoustical features in the left versus right non-primary auditory regions, respectively. These findings support a domain-general framework for auditory categorization, such that perceptual categories emerge from efficient neural coding of low-level acoustical cues, highlighting the computational relevance of STMs in shaping auditory representations that support behavior.</p></abstract><kwd-group><kwd>auditory cortex</kwd><kwd>intracranial EEG</kwd><kwd>spectrotemporal modulations</kwd><kwd>speech perception</kwd><kwd>music perception</kwd><kwd>efficient neural coding</kwd><kwd>humans</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P3">Understanding how the brain represents perceptual categories remains a central question in cognitive neuroscience. A prominent example of this problem is provided by arguments about the neural systems underlying categorization of the two most important forms of human auditory communication: speech and music (<xref ref-type="bibr" rid="R56">Mehr et al., 2021</xref>). This debate specifically centers on whether speech and music categorization rely on domain-specific, modular representations in distinct neural populations that cannot be accounted for by acoustic features (<xref ref-type="bibr" rid="R4">Angulo-Perkins et al., 2014</xref>; <xref ref-type="bibr" rid="R9">Boebinger et al., 2021</xref>; <xref ref-type="bibr" rid="R61">Norman-Haignere et al., 2015</xref>, <xref ref-type="bibr" rid="R60">2022</xref>) or whether their categorization can be accounted for by domain-general models through the hierarchical organization of lower-level neural resources capable of processing acoustical cues relevant for a wide range of auditory inputs (<xref ref-type="bibr" rid="R1">Albouy et al., 2020</xref>; <xref ref-type="bibr" rid="R26">Flinker et al., 2019</xref>; <xref ref-type="bibr" rid="R98">Zatorre, 2022</xref>).</p><p id="P4">The nature of these neural representations carries significant implications for our understanding of brain function, including the question of whether representations are localized or distributed across broader networks (<xref ref-type="bibr" rid="R8">Bizley &amp; Cohen, 2013</xref>; <xref ref-type="bibr" rid="R69">Rissman &amp; Wagner, 2012</xref>; <xref ref-type="bibr" rid="R85">Staeren et al., 2009</xref>). While some neuroimaging studies argue for clear functional specialization, identifying distinct bilateral brain regions dedicated to speech versus song or instrumental music (<xref ref-type="bibr" rid="R15">Chen et al., 2023</xref>; <xref ref-type="bibr" rid="R27">Friederici, 2020</xref>; <xref ref-type="bibr" rid="R61">Norman-Haignere et al., 2015</xref>, <xref ref-type="bibr" rid="R60">2022</xref>), others propose that the representation of speech and music is based upon consistent differences in the acoustical cues that characterize these two classes of sounds, which also have different hemispheric weighting (<xref ref-type="bibr" rid="R1">Albouy et al., 2020</xref>; <xref ref-type="bibr" rid="R25">Fadiga et al., 2009</xref>; <xref ref-type="bibr" rid="R47">Koelsch, 2011</xref>; <xref ref-type="bibr" rid="R70">Robert et al., 2024</xref>; <xref ref-type="bibr" rid="R75">Schön et al., 2010</xref>; <xref ref-type="bibr" rid="R87">Te Rietmolen et al., 2024</xref>).</p><p id="P5">The domain-specific view posits that the human auditory cortex is organized to support specialized processing pathways for behaviorally meaningful sound categories (i.e. speech, music). While early stages of the auditory pathway are largely explained by tuning to basic acoustic features, this framework suggests that category-specific representations emerge just beyond these early areas, where neural processing becomes increasingly non-linear (<xref ref-type="bibr" rid="R46">Kell et al., 2018</xref>). These transformations are thought to give rise to representations in non-primary auditory regions that encode auditory objects in ways that are more invariant to low-level acoustic variability (<xref ref-type="bibr" rid="R55">McDermott &amp; Simoncelli, 2011</xref>). Supporting this view, several studies have reported distinct neural populations in bilateral non-primary auditory areas that respond selectively and non-linearly to speech, music, or even specifically to songs (<xref ref-type="bibr" rid="R9">Boebinger et al., 2021</xref>; <xref ref-type="bibr" rid="R46">Kell et al., 2018</xref>; <xref ref-type="bibr" rid="R61">Norman-Haignere et al., 2015</xref>, <xref ref-type="bibr" rid="R60">2022</xref>; <xref ref-type="bibr" rid="R100">Zuk et al., 2020</xref>), concluding that these responses cannot be explained by acoustic models (<xref ref-type="bibr" rid="R61">Norman-Haignere et al., 2015</xref>, <xref ref-type="bibr" rid="R60">2022</xref>).</p><p id="P6">The domain-general perspective, in contrast, posits that neural populations must be capable of encoding a wide range of acoustic features while at the same time being capable of more abstract categorical representations of stimulus classes, which raises the need for a biologically plausible model that explains how activity patterns within these populations can develop representations of sound categories. Such representations must be rich enough to capture the complexity of the signal yet structured enough to allow the extraction of meaningful invariants from them. The concept of spectrotemporal receptive fields (<xref ref-type="bibr" rid="R16">Chi et al., 2005</xref>) offers a computationally grounded and neurophysiologically plausible framework for understanding how auditory neurons decompose incoming acoustic signals (<xref ref-type="bibr" rid="R78">Shamma, 2001</xref>; <xref ref-type="bibr" rid="R83">Singh &amp; Theunissen, 2003</xref>; <xref ref-type="bibr" rid="R97">Woolley et al., 2005</xref>) without making any unnecessary assumptions about domain specificity. This model suggests that auditory neurons function as spectrotemporal modulation (STM) rate filters, a proposal supported by single-cell recordings in animals (<xref ref-type="bibr" rid="R18">Depireux et al., 2001</xref>; <xref ref-type="bibr" rid="R53">Massoudi et al., 2015</xref>; <xref ref-type="bibr" rid="R71">Rodríguez et al., 2010</xref>; <xref ref-type="bibr" rid="R91">Theunissen et al., 2000</xref>; <xref ref-type="bibr" rid="R97">Woolley et al., 2005</xref>), and human neuroimaging studies (<xref ref-type="bibr" rid="R40">Hullett et al., 2016</xref>; <xref ref-type="bibr" rid="R73">Santoro et al., 2014</xref>; <xref ref-type="bibr" rid="R77">Schönwiesner &amp; Zatorre, 2009</xref>; <xref ref-type="bibr" rid="R94">Venezia et al., 2019</xref>). A recent study has shown that STM-based representations are sufficient to capture spectrotemporal features that can robustly discriminate song from speech across diverse cultures, outperforming a wide range of other acoustic descriptors (<xref ref-type="bibr" rid="R3">Albouy et al., 2024</xref>). Furthermore, this STM framework has been shown to account for the functional hemispheric lateralization of speech and music processing (<xref ref-type="bibr" rid="R1">Albouy et al., 2020</xref>; <xref ref-type="bibr" rid="R37">Haiduk et al., 2024</xref>), and that this lateralization, rooted in the STM encoding scheme, extends beyond speech and music (<xref ref-type="bibr" rid="R10">Boemio et al., 2005</xref>; <xref ref-type="bibr" rid="R42">Jamison et al., 2006</xref>; <xref ref-type="bibr" rid="R70">Robert et al., 2024</xref>; <xref ref-type="bibr" rid="R76">Schönwiesner et al., 2005</xref>; <xref ref-type="bibr" rid="R99">Zatorre &amp; Belin, 2001</xref>) emphasizing its domain-general nature.</p><p id="P7">The present study aimed to address the domain-general versus domain-specific debate by investigating whether STM features of naturalistic sounds are sufficient to explain categorical responses to speech and music, or whether such higher-order representations cannot be accounted for by low-level acoustic features, as proposed by the domain-specific view. To address this issue, we recorded stereo-electroencephalography (sEEG) activity via implanted electrodes from patients with intractable epilepsy while they passively listened to a naturalistic 7-minute excerpt from the audio track of a movie. The excerpt contained speech and music presented both separately and simultaneously, allowing us to approach natural listening conditions with neural measures of high spatial and temporal precision. Separately, behavioral categorization data were collected from healthy participants listening to the same excerpt to identify segments of speech and/or music. These datasets allowed us to test whether the STM features of the audio track could be predicted from oscillatory magnitude recorded intracranially, and whether these features, as predicted by a domain-general account are in turn sufficient to predict behavioral categorization in an independent group (out-of-sample prediction).</p><p id="P8">We formulated five key predictions: (i) If the behavioral categorization of the continuous naturalistic stimulus is based on STM features, we should observe distinct STM patterns in the acoustics of the waveform for segments classified as speech and music, replicating findings from studies using shorter and more controlled stimuli (<xref ref-type="bibr" rid="R1">Albouy et al., 2020</xref>, <xref ref-type="bibr" rid="R3">2024</xref>). (ii) If the brain tracks these STM patterns in naturalistic settings, they should be robustly encoded in the auditory cortex, particularly in non-primary regions (<xref ref-type="bibr" rid="R1">Albouy et al., 2020</xref>; <xref ref-type="bibr" rid="R26">Flinker et al., 2019</xref>). (iii) If STM features alone are sufficient to support perceptual categorization, then brain-reconstructed STMs (i.e., STMs predicted from neural signals) should be able to account for behavioral categorization judgments obtained from an independent group of participants. (iv) Under the same assumption, the brain-reconstructed STMs should closely match the acoustically derived STMs that are most relevant for categorization in the healthy listeners, as determined in the previous analysis step. (v) Finally, the similarity between brain-reconstructed and acoustically derived STM patterns should exhibit hemispheric asymmetries, stronger in the left hemisphere for speech, and in the right hemisphere for music.</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>STM features predict perceptual categorization: pure spectral modulations for music, pure temporal modulations for speech</title><p id="P9">We collected behavioral categorization data from 19 healthy participants listening to the first 7 minutes and 50 seconds of the French-dubbed audio track of <italic>Harry Potter and the Philosopher’s Stone</italic>. Participants continuously indicated whether they perceived speech and/or music by pressing corresponding keys (see <xref ref-type="fig" rid="F1">Figure 1a</xref> and <xref ref-type="sec" rid="S20">STAR method</xref>), resulting in binary vectors for each participant reflecting perceived categories over time. To relate these judgments to acoustic properties, we decomposed the audio signal using the STM framework (<xref ref-type="bibr" rid="R22">Elliott &amp; Theunissen, 2009</xref>), extracting STM patterns at each time point with a 1.5-second sliding window (see <xref ref-type="fig" rid="F1">Figure 1b</xref> and <xref ref-type="sec" rid="S20">STAR method</xref>). This yielded time-resolved 2D matrices capturing spectral (0–9 cycles/kHz, sampled over 109 points) and temporal (0–15 Hz, sampled over 20 points) modulation power.</p><p id="P10">Our first goal was to investigate if STM features are sufficient to explain categorization of speech and music, and if so which specific STM patterns contribute most to each category. We applied stratified 5-fold cross-validated logistic regression models to predict behavioral categorization based on the STM decomposition of the audio track. Each model used the STM data of all audio track samples as features to predict the binary behavioral judgment (presence/absence) outcome, for each category (speech or music). Subsequently, we applied the Haufe transformation (<xref ref-type="bibr" rid="R38">Haufe et al., 2014</xref>) to derive interpretable feature patterns from the feature weights, which were averaged for each judgement category across behavioral subjects (<xref ref-type="fig" rid="F1">Figure 1c</xref>). For the speech category, the STM features most important for the binary classification contained pure temporal modulations. Specifically, the 90th percentile of highest values spanned from 0.8 to 15 Hz in temporal modulations, but only from 0 to 1 cyc/kHz in spectral modulations (dotted line in <xref ref-type="fig" rid="F1">Fig 1c</xref>, left). Peak activity (99th percentile; solid line in <xref ref-type="fig" rid="F1">Fig 1c</xref>, left) was concentrated within 5.6 to 8.8 Hz in temporal modulations but 0 to 0.4 cyc/kHz in spectral modulations. For the music category, pure spectral modulations were instead the primary distinguishing features. High activity in the 90th percentile was observed across most of the spectral modulation range (except for a narrow gap between 3.3 and 3.7 cyc/kHz), and within a range of 0 to 1.6 Hz in temporal modulations (dotted line in <xref ref-type="fig" rid="F1">Fig 1c</xref>, right). Peaks (99th percentile) were localized in a wide range of spectral modulation values, around 0.3, 1.2, 5.7, 6.5, 7.4, and 8.6 cyc/kHz, and temporal modulation range of 0 to 0.8 Hz (solid lines in <xref ref-type="fig" rid="F1">Fig 1c</xref>, right). These findings align with the expected dynamics of high temporal/low spectral modulation for speech signals and low temporal/high spectral modulation for musical signals (<xref ref-type="bibr" rid="R1">Albouy et al., 2020</xref>, <xref ref-type="bibr" rid="R3">2024</xref>; <xref ref-type="bibr" rid="R22">Elliott &amp; Theunissen, 2009</xref>; <xref ref-type="bibr" rid="R26">Flinker et al., 2019</xref>; <xref ref-type="bibr" rid="R37">Haiduk et al., 2024</xref>). Importantly, these results validate the use of time-resolved tracking in STM space to differentiate auditory categories. Building on this outcome, we next applied the STM tracking approach to assess which sEEG contacts in our patient sample could effectively track these STM features in the audio tracks that they listened to.</p></sec><sec id="S4"><title>Bilateral non-primary auditory areas track the spectrotemporal modulations of a naturalistic sounds</title><p id="P11">Intracranial EEG recordings were collected from eleven patients with focal drug-resistant epilepsy as they passively listened to the audio track. After bipolar re-referencing, this resulted in a total of 846 sEEG contacts, with 51.4% located in the left hemisphere and 48.6% in the right hemisphere (<xref ref-type="fig" rid="F2">Figure 2a</xref>, see <xref ref-type="supplementary-material" rid="SD1">supplementary Figure S1</xref> for detail). For each sEEG contact, the magnitude of delta (0.5-3 Hz), theta (4-8 Hz), alpha (9-13 Hz), beta (14-25 Hz), low gamma (26-50 Hz), and high gamma (50-150 Hz) oscillations were extracted from the raw signal during the audio track listening, using Hilbert transform (z-score-normalized over time).</p><p id="P12">To uncover brain regions responsive to STMs, we applied 10-fold cross-validated ridge regression models that linked oscillatory magnitude across these six frequency bands, recorded from the 846 sEEG contacts, to the STM features of the audio track. For each sEEG contact, the model predicted the time course of STM amplitude for each of the 2,180 spectrotemporal combinations— corresponding to 109 spectral (0–9 cycles/kHz) × 20 temporal (0–15 Hz) modulation points extracted from the 2D STM matrices —producing an accuracy map of fisher-transformed correlation coefficients within the STM space. sEEG contacts that significantly predicted any portion of the STM space (t-tests vs 0, Bonferroni-corrected for number of STM points, sEEG contacts and cross-validation folds) were then spatially clustered in the MNI space, where clusters with overlapping contacts from at least three patients were considered significant (see <xref ref-type="bibr" rid="R11">Borderie et al., 2024</xref> and <xref ref-type="sec" rid="S20">STAR methods</xref>). Using this method, we identified four spatial clusters comprising a total of 39 sEEG contacts, bilaterally distributed in non-primary auditory cortical areas (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Table S1</xref> for MNI coordinates and atlas labels). Two clusters were located in each hemisphere. In both hemispheres, one cluster included sEEG contacts primarily positioned in the anterior portion of the superior temporal region, with 11 sEEG contacts in the left hemisphere and 13 in the right. These anterior clusters included sEEG contacts located in the medial belt, in area A5, and the anterior distal superior temporal sulcus (STS), with the right cluster additionally containing an sEEG contact in area A4 according to the HCP atlas (<xref ref-type="bibr" rid="R33">Glasser et al., 2016</xref>). The second cluster in the left hemisphere was located more posteriorly, encompassing sEEG contacts in the posterior distal STS and area A5. In the right hemisphere, the remaining cluster included sEEG contacts in the medial and lateral belt, and in area A5, corresponding to mid-to-posterior portions of the non-primary auditory cortex. Each cluster included sEEG contacts spanning at least three patients, ensuring the generalizability of these findings (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Table 1</xref> for the MNI coordinates and atlas labels of the sEEG contacts). Overall, we observed that sEEG contacts located in non-primary auditory areas demonstrate the ability to track STM features of complex, naturalistic sounds. We then used the signal from these sEEG contacts for all subsequent analyses.</p></sec><sec id="S5"><title>Behavioral categorization judgments can be decoded from brain-reconstructed STMs in bilateral non-primary auditory areas</title><p id="P13">We next address the key question of whether brain-reconstructed STMs (i.e., STMs predicted from neural signal) are sufficient to decode behavioral categorization judgments of speech and music. Our objective was to define how brain-reconstructed STMs can effectively link neural activity to behavioral judgment obtained in healthy individuals through acoustic features. At each time point, reconstructed STMs from STM-sensitive sEEG contacts (see <xref ref-type="sec" rid="S20">STAR methods</xref>) were used as features to classify participants’ binary judgments of whether the track contained speech or music. Reconstructed STMs from 36 of the 39 STM-sensitive sEEG contacts significantly predicted whether the audio track contained speech (all corrected p &lt; .046), and all 39 contacts significantly predicted the presence of music (all corrected p &lt; .001). These findings highlight that the category (speech or music) in audio tracks can be accurately predicted from brain activity, when represented in the STM space.</p><p id="P14">To identify the specific regions of the STM space contributing to category decoding across sEEG contacts, we performed, within each contact, a statistical test against zero on feature patterns at each point in the STM space across behavioral subjects (see <xref ref-type="sec" rid="S20">STAR methods</xref>). We then counted, for each STM point, the number of contacts showing a significant feature pattern (<xref ref-type="fig" rid="F3">Figure 3e</xref>). For the speech category, the STM regions with the highest number of sEEG contacts showing significant feature patterns correspond to pure temporal modulations. Specifically, the top 10% of sEEG contact counts (90th percentile) ranged from 0.6 to 15 Hz in temporal modulations and from 0 to 1 cyc/kHz in spectral modulations (dotted line in <xref ref-type="fig" rid="F3">Fig 3e</xref>, left). The peak sEEG contact count (99th percentile; solid line in <xref ref-type="fig" rid="F3">Fig 3e</xref>, left) was concentrated in the range of 4.8 to 8 Hz for temporal modulations and 0 to 0.25 cyc/kHz for spectral modulations. For the music category, the STM regions with the highest number of sEEG contacts showing significant feature patterns corresponded to pure spectral modulations. A high sEEG contact count in the 90th percentile was observed across most of the spectral modulation range, except for two narrow gaps between 3.4 and 3.7 cyc/kHz (consistent with the analysis using the actual audio track) and between 4.5 and 4.8 cyc/kHz (dotted lines in <xref ref-type="fig" rid="F3">Fig 3e</xref>, right). The peak sEEG contact count (99th percentile; solid lines in <xref ref-type="fig" rid="F3">Fig 3e</xref>, right) was localized within spectral modulation values around 0.2, 6, 6.6, 7.7, and 8.7 cyc/kHz and temporal modulation of 0 to 0.8 Hz. These results demonstrate that brain-reconstructed STMs can accurately predict behavioral categorization even though behavioral data were derived from different samples of participants (out of sample prediction). To assess the behavioral relevance of these brain-derived features, we will next test whether the STM patterns driving categorization are similar when derived from brain data (<xref ref-type="fig" rid="F3">Figure 3e</xref>) and when derived directly from the acoustics (<xref ref-type="fig" rid="F1">Figure 1c</xref>).</p></sec><sec id="S6"><title>Brain-reconstructed and acoustic STM representations are highly similar</title><p id="P15">To assess whether the brain-reconstructed STM representations truly reflect the acoustic features relevant for perceptual categorization, we evaluated the similarity between the feature patterns (<xref ref-type="fig" rid="F4">Figure 4a</xref>) of: (1) the STM of the actual audio track (<xref ref-type="fig" rid="F1">Fig 1c</xref>), and (2) the brain-reconstructed STM (<xref ref-type="fig" rid="F3">Fig 3d</xref>) averaged across hemispheres. For each category and each hemisphere, nineteen comparisons were performed between STM patterns derived from brain-reconstructed STMs and from acoustic STMs, both based on the prediction of each behavioral judgment vector (n = 19 behavioral participants). STM patterns were highly similar, showing significant correlation coefficients (Spearman correlation; p &lt; .001, Bonferroni-corrected adjusted for number of subjects, categories, and hemispheres) for all comparisons, confirming the strong alignment between the STM features of the audio and those derived from brain activity (<xref ref-type="fig" rid="F4">Figure 4b</xref>). We confirmed this result at the group-level, by comparing the distribution of correlation coefficients across behavioral participants to shuffled data, (see <xref ref-type="sec" rid="S20">STAR methods</xref>; <xref ref-type="fig" rid="F4">Figure 4b</xref>) revealing highly significant differences for each category (both p = 2.9x10<sup>-22</sup>). These key results demonstrate that the STM patterns in behaviorally relevant acoustic features are represented in a continuous and sufficiently similar manner in the brain in non-primary auditory areas. Next, we examined whether the representation of these STM patterns exhibits hemispheric asymmetries that have been previously associated with differential STM patterns processing.</p></sec><sec id="S7"><title>Similarity between brain-reconstructed and acoustic STM representations reveals hemispheric lateralization</title><p id="P16">Finally, we tested whether the similarity between the two representations reflected the previously described hemispheric specialization of the auditory cortices (<xref ref-type="fig" rid="F4">Figure 4c</xref>), with the left hemisphere being more sensitive to temporal modulations and the right to spectral modulations (<xref ref-type="bibr" rid="R1">Albouy et al., 2020</xref>; <xref ref-type="bibr" rid="R26">Flinker et al., 2019</xref>; <xref ref-type="bibr" rid="R99">Zatorre &amp; Belin, 2001</xref>). To do so, we carried out a 2x2 aligned rank transform test on the correlation coefficients used in the previous similarity analysis, with category (speech/music) and hemisphere (left/right) as factors. The main effect of category was significant (F(1, 55.65) = 72.20, <italic>p</italic> &lt; .001), with higher correlation coefficients for the music category compared to the speech category. This result was expected, as models involving speech categorization consistently performed worse than those for music, likely due to the more fragmented and less continuous nature of speech categorization vectors, reflecting the conversational structure of the audio tracks (<xref ref-type="fig" rid="F1">Figure 1a</xref>, right panel). No main effect of hemisphere was observed (F(1, 55.65) = 0.68, <italic>p</italic> = 0.412), indicating that both hemispheres similarly encode auditory categorical information. Finally, the category-by-hemisphere interaction was significant (F(1, 55.65) = 23.64, <italic>p</italic> &lt; .001). Post-hoc analysis showed that for the speech category, correlation coefficients were significantly higher in the left hemisphere (FDR-corrected <italic>p</italic> = .027) compared to the right, whereas for the music category, correlation coefficients were significantly higher in the right hemisphere (FDR-corrected <italic>p</italic> &lt; .001) compared to the left. Additional post-hoc analyses of music vs speech comparisons in each hemisphere revealed significantly lower correlation coefficients for speech than for music in both the left and right hemispheres (both FDR-corrected <italic>p</italic> &lt; .001), consistent with the previously noted lower accuracy for models involving speech categorization. Overall, we found that the high similarity between feature patterns derived from acoustical-STM and brain-reconstructed STM showed hemispheric asymmetry with greater similarity in the left hemisphere for speech categorization and in the right hemisphere for music categorization.</p></sec></sec><sec id="S8" sec-type="discussion"><title>Discussion</title><p id="P17">In the present study, we investigated how the human brain encodes and categorizes naturalistic speech and music, integrating continuous behavioral categorization judgments from healthy participants with intracranial recordings from the auditory cortex of epileptic patients. We specifically tested whether perceptual categories depend on the encoding of acoustical features, as predicted by a domain-general model, or whether such features are insufficient for categorization, as predicted by a domain-specific model. Our results provide converging evidence favoring the domain-general hypothesis, demonstrating that spectro-temporal modulation (STM) features of naturalistic sounds are sufficient for speech and music categorization to emerge from non-primary auditory cortical activity. Specifically, we found that (i) distinct STM patterns are associated with the perception of naturalistic speech and music categories in healthy individuals (<xref ref-type="fig" rid="F1">Figure 1c</xref>), validating our analysis approach to naturalistic auditory input; (ii) STM features are robustly tracked in non-primary auditory areas (<xref ref-type="fig" rid="F2">Figure 2d</xref>); (iii) STMs reconstructed from brain signals in these regions are sufficient to predict behavioral categorization judgments from a separate group of participants (<xref ref-type="fig" rid="F3">Figure 3e</xref>), showing that brain-derived representations align closely with perceptual decisions; (iv) STM patterns from the brain-reconstructed STMs are highly similar to those obtained from the actual audio track (<xref ref-type="fig" rid="F4">Figure 4b</xref>), and (v), this effect is lateralized (<xref ref-type="fig" rid="F4">Figure 4c</xref>) with stronger acoustic/brain similarity in the right hemisphere for music and in the left hemisphere for speech, consistent with well-known hemispheric asymmetries in auditory processing. We next discuss each of these points in turn.</p><sec id="S9"><title>Distinct STM patterns are associated with the perception of naturalistic speech and music categories</title><p id="P18">Using a time-resolved tracking approach, we found that human judgments of perceiving speech and/or music at any given moment within a soundtrack can be well differentiated based on their STM content, with speech associated with high temporal (peak 5.6–8.8 Hz) and low spectral modulations (0–1 cyc/kHz), and music with high spectral (peaks between 5.7 and 8.6 cyc/kHz) and low temporal modulations (0–1 Hz, <xref ref-type="fig" rid="F1">Figure 1c</xref>). This finding is consistent with prior literature demonstrating that speech and music rely on distinct STM features, which not only support their perceptual differentiation but also provide an optimal representational framework for capturing their complex structures. Specifically, temporal modulations in the range of 4–7 Hz are known to be critical for speech intelligibility, as this range aligns with the syllabic rate of spoken language (<xref ref-type="bibr" rid="R3">Albouy et al., 2024</xref>; <xref ref-type="bibr" rid="R19">Ding et al., 2017</xref>; <xref ref-type="bibr" rid="R22">Elliott &amp; Theunissen, 2009</xref>; <xref ref-type="bibr" rid="R30">Giroud et al., 2023</xref>, <xref ref-type="bibr" rid="R31">2024</xref>; <xref ref-type="bibr" rid="R80">R. V. Shannon et al., 1995</xref>; <xref ref-type="bibr" rid="R92">Varnet et al., 2017</xref>). In the spectral domain, speech recognition has been shown to rely primarily on low spectral modulations (<xref ref-type="bibr" rid="R3">Albouy et al., 2024</xref>), with critical modulations for speech comprehension typically falling below 4 cyc/kHz (<xref ref-type="bibr" rid="R22">Elliott &amp; Theunissen, 2009</xref>). In contrast, music perception operates on a different timescale, with key temporal modulations typically falling below 2 Hz (<xref ref-type="bibr" rid="R14">Chang et al., 2024</xref>), reflecting the slower amplitude modulations of musical phrases across genres (<xref ref-type="bibr" rid="R19">Ding et al., 2017</xref>) and cultures (<xref ref-type="bibr" rid="R3">Albouy et al., 2024</xref>). Regarding spectral modulations, previous studies have shown that for songs that contain only vocal elements, important spectral modulations range from approximately 3 to 8 cyc/kHz (<xref ref-type="bibr" rid="R1">Albouy et al., 2020</xref>, <xref ref-type="bibr" rid="R3">2024</xref>). Our present results and previous literature thus confirm that STM representations can effectively capture the statistical properties of speech and music that are most relevant for perceptual categorization. These observations raise the question of whether such behaviorally relevant acoustic representations are also reflected in the tuning properties of the human auditory cortex.</p></sec><sec id="S10"><title>The auditory cortex robustly tracks STMs</title><p id="P19">Our second finding demonstrated that the auditory cortex is tuned to track the STM regularities present in natural speech and music. We identified sEEG contacts that significantly tracked STM patterns, using a robust ridge regression approach combined with the temporal response function method (<xref ref-type="fig" rid="F2">Figure 2a</xref>). We demonstrated that STM patterns are dynamically tracked in non-primary auditory regions (<xref ref-type="fig" rid="F2">Figure 2d</xref>). Specifically, STM tracking was observed in the left hemisphere within the medial belt, in area A5, and in both anterior and posterior portions of the distal superior temporal sulcus (STS); in the right hemisphere, tracking was found in the medial and lateral belt regions, area A5, and the anterior distal STS. No such tracking was observed in sEEG contacts outside these auditory regions. Importantly, none of our participants had sEEG contact coverage over the primary auditory cortex, a common limitation of intracranial studies, so we cannot exclude the possibility that STM tracking may also occur in primary or adjacent auditory areas.</p><p id="P20">Our findings align with a broad body of evidence from both animal and human studies showing that STM sensitivity is a fundamental property of the auditory system. In animal models, single-unit recordings across species such as zebra finches, ferrets, cats, and monkeys have consistently revealed tuning to STM throughout the auditory pathway. Tuning to STM has been reported from the inferior colliculus and medial geniculate body to primary and secondary auditory cortex (primarily using methods that estimate spectrotemporal receptive fields (STRFs) of individual neurons) in response to controlled synthetic ripple stimuli as well as more ecologically valid sounds like birdsong and vocalizations (<xref ref-type="bibr" rid="R12">Calhoun &amp; Schreiner, 1998</xref>; <xref ref-type="bibr" rid="R18">Depireux et al., 2001</xref>; <xref ref-type="bibr" rid="R20">Eggermont, 2002</xref>; <xref ref-type="bibr" rid="R23">Escabí et al., 2003</xref>; <xref ref-type="bibr" rid="R24">Escabí &amp; Schreiner, 2002</xref>; <xref ref-type="bibr" rid="R39">Hsu et al., 2004</xref>; <xref ref-type="bibr" rid="R48">Kowalski et al., 1996</xref>; <xref ref-type="bibr" rid="R53">Massoudi et al., 2015</xref>; <xref ref-type="bibr" rid="R57">Miller et al., 2002</xref>; <xref ref-type="bibr" rid="R71">Rodríguez et al., 2010</xref>; <xref ref-type="bibr" rid="R91">Theunissen et al., 2000</xref>; <xref ref-type="bibr" rid="R97">Woolley et al., 2005</xref>). Similarly, human neuroimaging research has confirmed STM sensitivity across primary, secondary, and even higher-order auditory regions. This has been demonstrated through multiple approaches: spectrotemporal modulation transfer functions measured with synthetic ripple stimuli (<xref ref-type="bibr" rid="R77">Schönwiesner &amp; Zatorre, 2009</xref>), STRFs estimated with naturalistic sounds (<xref ref-type="bibr" rid="R94">Venezia et al., 2019</xref>, <xref ref-type="bibr" rid="R93">2021</xref>), classic activation-based fMRI mapping using dynamic ripple stimuli (<xref ref-type="bibr" rid="R49">Langers et al., 2003</xref>), and encoding/decoding models that explicitly linked neural responses to the STM content of speech, music, vocalizations, and environmental sounds (<xref ref-type="bibr" rid="R73">Santoro et al., 2014</xref>).</p><p id="P21">Further evidence for STM tuning has been shown recently in three human studies conducted within the STM framework. These studies demonstrated that degrading the STM of speech and tonal sounds leads to reduced cortical discriminability, with effects localized in the same regions as our STM-sensitive contacts. <xref ref-type="bibr" rid="R1">Albouy et al. (2020)</xref> showed that STM degradation of sentences and melodies impaired category classification in left A4 and medial belt for temporal modulations, and right medial belt for spectral ones. <xref ref-type="bibr" rid="R70">Robert et al. (2024)</xref> used synthetic sounds mimicking simple actions (containing mainly temporal modulations) and object properties (containing mainly spectral modulations) and found that STM degradation disrupted classification in left A4/A5 and IFG for actions and right parabelt, A4/A5 and pSTS for objects. <xref ref-type="bibr" rid="R26">Flinker et al. (2019)</xref>, using electrocorticography and degraded speech, reported cortical tracking of STM features in left A5 (temporal) and right medial belt (spectral) for speech- and vocal timbre-related features respectively. Across these studies, despite differences in stimuli and methods, sensitivity to STM structure consistently emerged in areas overlapping with the STM-sensitive contacts identified in the present study.</p><p id="P22">In studies adopting a domain-specific perspective, <xref ref-type="bibr" rid="R61">Norman-Haignere et al. (2015</xref>, <xref ref-type="bibr" rid="R60">2022)</xref> proposed distinct neural components for speech and music categories and distinct from other components sensitive to acoustic features based on fMRI voxel decomposition method. Yet, the topographies they report largely overlap with the STM-sensitive regions identified in the present and previous studies. In their fMRI work (<xref ref-type="bibr" rid="R61">Norman-Haignere et al., 2015</xref>), independent components tuned to STMs were located near the borders of primary auditory cortex, both anteriorly and posteriorly, while speech- and music-selective category components emerged in lateral STG, planum polare, and planum temporale. Similarly, in their electrocorticography (ECOG) study (<xref ref-type="bibr" rid="R60">Norman-Haignere et al., 2022</xref>), they reported STM sensitivity in both primary auditory cortex and more anterior non-primary auditory areas and revealed speech- and music-selective components in middle and anterior STG. Thus, the findings across studies converge regarding both the existence and the location of higher-order representations for speech and music, a point we shall return to in the final discussion.</p></sec><sec id="S11"><title>Brain-reconstructed STMs are sufficient to decode behavioral categorization judgments</title><p id="P23">Having established that STMs provide an efficient shared acoustic representation for distinguishing speech and music, and that the auditory cortex is finely tuned to these features, we reconstructed the STM features of the audio track from intracranial signals recorded at STM-sensitive sEEG contacts (<xref ref-type="fig" rid="F3">Figure 3a-c</xref>) and found that these predicted STMs were highly effective in decoding continuous categorization judgments. This crucial result shows that behavioral categorical decisions can be predicted from brain-reconstructed acoustic representations. In addition, a closer look at the feature patterns of the brain-reconstructed STMs that were most predictive of behavioral categorization (<xref ref-type="fig" rid="F3">Figure 3d</xref>) revealed a match with those identified using the STM patterns derived directly from the real audio track (<xref ref-type="fig" rid="F3">Figure 3e</xref>). This observation critically suggests a functional convergence between acoustically and neurally derived features. The next important step was therefore to formally test the similarity between these two sets of STM patterns to determine whether the same representational structure underlies both acoustically and neurally derived data.</p></sec><sec id="S12"><title>High similarity between brain-reconstructed and acoustically-derived STMs</title><p id="P24">A similarity analysis (<xref ref-type="fig" rid="F4">Figure 4a</xref>) revealed a strong correspondence between the acoustically and neurally derived STM patterns that are critical for successful behavioral categorization (<xref ref-type="fig" rid="F4">Figure 4b</xref>). Notably, because our neural and behavioral data were collected from separate participant groups, the observed neural–perceptual mapping reflects a robust out-of-sample prediction, a rigorous validation approach in cognitive neuroscience (<xref ref-type="bibr" rid="R28">Gabrieli et al., 2015</xref>; <xref ref-type="bibr" rid="R74">Scheinost et al., 2019</xref>; <xref ref-type="bibr" rid="R96">Walters et al., 2022</xref>), indicating that the underlying coding principles generalize across individuals rather than overfitting to subject-specific variability or noise. These findings offer our most compelling demonstration that brain-reconstructed STM representations alone are sufficient to accurately predict behavioral categorization judgments of speech and music, establishing a direct link between neural encoding of STM features in the auditory cortex and high-order perceptual outcomes. The final analytical step focused on whether this acoustic-neural coupling reflects meaningful organizational principles of auditory processing, by testing for hemispheric specialization.</p></sec><sec id="S13"><title>Similarity between brain-reconstructed and acoustic STM representations reveals hemispheric lateralization</title><p id="P25">Hemispheric asymmetries (favoring the left auditory cortex for speech and the right for music) have been robustly documented over decades of research, including lesion studies, electrophysiological recordings, and neuroimaging investigations in both neurotypical individuals and those with neurodevelopmental disorders such as amusia (<xref ref-type="bibr" rid="R2">Albouy et al., 2013</xref>; <xref ref-type="bibr" rid="R44">Johnsrude et al., 2000</xref>; <xref ref-type="bibr" rid="R51">Loui et al., 2009</xref>; <xref ref-type="bibr" rid="R58">Milner, 1962</xref>; <xref ref-type="bibr" rid="R63">Okamoto &amp; Kakigi, 2015</xref>; <xref ref-type="bibr" rid="R72">Samson &amp; Zatorre, 1988</xref>; <xref ref-type="bibr" rid="R81">Sihvonen et al., 2019</xref>; <xref ref-type="bibr" rid="R98">Zatorre, 2022</xref>). Our data revealed a hemispheric lateralization in the similarity between brain-reconstructed STM patterns (extracted from STM-sensitive sEEG contacts) and the STM patterns of the actual audio stimuli, when both were used to predict perceptual categorization outcomes (<xref ref-type="fig" rid="F4">Figure 4c</xref>). Specifically, speech categorization showed higher similarity in sEEG contacts implanted in the left-hemisphere, while music categorization showed higher similarity in sEEG contacts implanted in the right-hemisphere. In other words, the observed hemispheric lateralization of the similarity patterns between brain-reconstructed and stimulus-derived STMs reveals that left auditory regions are more sensitive to the pure temporal modulations characteristic of speech, while right auditory regions are more sensitive to the pure spectral modulations typical of music. Our results thus show that the brain leverages the statistical structure of STM in a category- and hemisphere-specific manner to support auditory categorization.</p><p id="P26">In line with our results, hemispheric specialization for STM processing has been well-documented: the left auditory cortex operates at faster temporal integration windows than the right (<xref ref-type="bibr" rid="R10">Boemio et al., 2005</xref>; <xref ref-type="bibr" rid="R32">Giroud et al., 2020</xref>; <xref ref-type="bibr" rid="R52">Luo &amp; Poeppel, 2012</xref>; <xref ref-type="bibr" rid="R88">Teng et al., 2016</xref>, <xref ref-type="bibr" rid="R89">2017</xref>), making it better suited for tracking rapid temporal modulations, such as those prominent in speech. Conversely, the right auditory cortex exhibits sharper spectral tuning and heightened sensitivity to pitch-related information (<xref ref-type="bibr" rid="R13">Cha et al., 2016</xref>; <xref ref-type="bibr" rid="R17">Coffey et al., 2016</xref>; <xref ref-type="bibr" rid="R41">Hyde et al., 2008</xref>; <xref ref-type="bibr" rid="R42">Jamison et al., 2006</xref>; <xref ref-type="bibr" rid="R98">Zatorre, 2022</xref>; <xref ref-type="bibr" rid="R99">Zatorre &amp; Belin, 2001</xref>), aligning with its preferential processing of fine spectral cues relevant for music. More recently, this hypothesis has been directly tested using the STM framework across fMRI, MEG, and sEEG/ECOG, showing that degradation of speech, songs, or environmental sounds in the temporal or spectral modulation domains selectively impact brain activity in the left STG for temporal degradation and in the right STG for spectral degradation (<xref ref-type="bibr" rid="R1">Albouy et al., 2020</xref>; <xref ref-type="bibr" rid="R26">Flinker et al., 2019</xref>; <xref ref-type="bibr" rid="R70">Robert et al., 2024</xref>). The present findings and recent literature reinforce the view that auditory perception relies on a shared STM encoding framework, with category-related hemispheric biases emerging as a consequence of distinct sensitivity in the processing of temporal and spectral modulations rather than as a function of intrinsic domain specificity.</p></sec><sec id="S14"><title>Efficient neural coding in the auditory system</title><p id="P27">Our results can be understood within the broader efficient neural coding framework which posits that perceptual systems have evolved to efficiently encode behaviorally relevant environmental stimuli for any given species (<xref ref-type="bibr" rid="R5">Attneave, 1954</xref>; <xref ref-type="bibr" rid="R6">Barlow, 1961</xref>). Computationally, efficient coding can be achieved if neural representations match the statistical regularities of the signals they represent (<xref ref-type="bibr" rid="R79">C. E. Shannon, 1948</xref>). While this principle has been extensively documented in the visual domain (<xref ref-type="bibr" rid="R35">Graham &amp; Field, 2007</xref>; <xref ref-type="bibr" rid="R82">Simoncelli &amp; Olshausen, 2001</xref>), it has also been increasingly supported in the auditory modality (<xref ref-type="bibr" rid="R29">Gervain &amp; Geffen, 2019</xref>; <xref ref-type="bibr" rid="R36">Guevara Erra &amp; Gervain, 2016</xref>; <xref ref-type="bibr" rid="R50">Lewicki, 2002</xref>; <xref ref-type="bibr" rid="R59">Ming &amp; Holt, 2009</xref>; <xref ref-type="bibr" rid="R68">Rieke et al., 1995</xref>; <xref ref-type="bibr" rid="R84">Smith &amp; Lewicki, 2006</xref>). Among various ways to represent natural sounds, STMs are particularly well-suited for capturing the statistical structure of complex sounds (<xref ref-type="fig" rid="F1">Figure 1c</xref>), including vocalizations, as they reflect joint dependencies between temporal and spectral modulations that are characteristic of natural stimuli, dependencies that are not captured by the raw spectrum or spectrogram alone (<xref ref-type="bibr" rid="R16">Chi et al., 2005</xref>; <xref ref-type="bibr" rid="R78">Shamma, 2001</xref>; <xref ref-type="bibr" rid="R83">Singh &amp; Theunissen, 2003</xref>; <xref ref-type="bibr" rid="R90">Theunissen &amp; Elie, 2014</xref>).</p><p id="P28">At the neural level, STMs have been shown to capture response properties of auditory neurons that are not accounted for by traditional frequency tuning curves across multiple species and auditory regions, including the midbrain, thalamus, and both primary and secondary auditory cortices in birds and non-human mammals (<xref ref-type="bibr" rid="R18">Depireux et al., 2001</xref>; <xref ref-type="bibr" rid="R39">Hsu et al., 2004</xref>; <xref ref-type="bibr" rid="R53">Massoudi et al., 2015</xref>; <xref ref-type="bibr" rid="R91">Theunissen et al., 2000</xref>; <xref ref-type="bibr" rid="R97">Woolley et al., 2005</xref>). In humans, such specific tuning to STMs in the auditory cortex, including non-primary auditory regions, has been demonstrated in neuroimaging studies using synthetic stimuli like dynamic ripples (<xref ref-type="bibr" rid="R49">Langers et al., 2003</xref>; <xref ref-type="bibr" rid="R77">Schönwiesner &amp; Zatorre, 2009</xref>) and natural stimuli such as speech (<xref ref-type="bibr" rid="R73">Santoro et al., 2014</xref>; <xref ref-type="bibr" rid="R94">Venezia et al., 2019</xref>, <xref ref-type="bibr" rid="R93">2021</xref>), as well as in an intracranial recording study using speech stimuli (<xref ref-type="bibr" rid="R40">Hullett et al., 2016</xref>). In addition, speech signals can be reconstructed from patterns of activity in the STG using STM-based STRF models (<xref ref-type="bibr" rid="R64">Pasley et al., 2012</xref>). Accordingly, under the assumption of efficient neural coding and given the known tuning of central auditory neurons to STMs, we propose that category-specific neural representations in the auditory cortex, such as those distinguishing speech from music that we have studied here, can be largely explained by differential sensitivity to STM features.</p><p id="P29">While we argue that sensitivity to STM features provides a domain-general and low-dimensional framework for explaining auditory cortical representations of complex sounds, we do not dispute the likely existence of subsequent non-linear transformations that may support higher-level categorical processing. Indeed, abstract categorical representations must exist to account for humans’ ability to effortlessly categorize complex sounds. A critical consideration in this context is the hierarchical nature of auditory cortical organization (cf. <xref ref-type="bibr" rid="R46">Kell et al., 2018</xref>), as supported by extensive evidence in both human and non-human primates (<xref ref-type="bibr" rid="R45">Kaas &amp; Hackett, 1999</xref>; <xref ref-type="bibr" rid="R62">Okada et al., 2010</xref>; <xref ref-type="bibr" rid="R65">Patterson et al., 2002</xref>; <xref ref-type="bibr" rid="R67">Rauschecker &amp; Scott, 2009</xref>), paralleling the functional architecture observed in the visual system (<xref ref-type="bibr" rid="R34">Goodale &amp; Milner, 1992</xref>; <xref ref-type="bibr" rid="R54">Maunsell &amp; Newsome, 1987</xref>; <xref ref-type="bibr" rid="R95">Vogels &amp; Orban, 1996</xref>). However, our findings, consistent with a large body of literature demonstrating STM tuning throughout the auditory cortex, show in a direct manner that neural representations of continuous, naturalistic sounds in auditory areas are fundamentally structured by sensitivity to STM features. These representations are sufficient to support category-level distinctions, such as between speech and music, without the need to invoke categorical abstraction at this processing stage.</p><p id="P30">This interpretation does not preclude additional, potentially non-linear transformations at later stages of the auditory object recognition hierarchy. Rather, it highlights that the input to such stages already carries rich, domain-relevant information structured by the statistical properties of natural sounds. From this perspective, category-selective responses observed in non-primary auditory cortex through non-linear voxel decomposition methods (e.g., <xref ref-type="bibr" rid="R61">Norman-Haignere et al., 2015</xref>, <xref ref-type="bibr" rid="R60">2022</xref>) may reflect higher-order transformations that build on, rather than replace, STM-based representations. Crucially, the brain regions identified in those studies as selectively responsive to speech or music largely overlap with the STM-sensitive regions revealed in our present work and in previous reports, with the additional property that they are lateralized. This overlap suggests that the emergence of categorical selectivity in these areas may be a consequence of their differential sensitivity to the statistical structure of specific sound categories, as captured by STMs. In this view, the observed selectivity is not at odds with a domain-general encoding scheme but rather emerges naturally from it.</p></sec><sec id="S15"><title>Conclusions and future perspectives</title><p id="P31">Speech and music can arguably be considered the two most important and species-characteristic human auditory communication signals (<xref ref-type="bibr" rid="R56">Mehr et al., 2021</xref>). Our findings that neural STM representations efficiently capture the statistical properties of naturalistic speech and music thus can be viewed in the context of functional adaptation of the human brain to these important classes of sounds. Importantly, our study bridges a longstanding theoretical debate by showing that domain-general mechanisms, grounded in the efficient neural encoding of STMs, can provide a biologically plausible framework that fully supports category-level distinctions, both at the neural and behavioral levels. While our findings do not exclude domain-specific mechanisms at later stages of processing in higher-order associative brain regions, we argue that the foundation for these distinctions lies in the acoustic domain.</p><p id="P32">Finally, it is important to recall that the auditory processing hierarchy does not start at the cortical level. Subcortical structures such as the inferior colliculus and the medial geniculate body exhibit tuning to spectrotemporal features in animal models (e.g., <xref ref-type="bibr" rid="R97">Woolley et al., 2005</xref>, Escabi &amp; Schreiner, 2002). Investigating these subcortical contributions in humans could provide valuable insights into the early stages of hierarchical auditory encoding and help refine our understanding of how STM representations emerge and propagate along the auditory pathway.</p></sec></sec><sec id="S16"><title>Resource Availability</title><sec id="S17"><title>Lead contact</title><p id="P33">For further information or resource requests, please contact the lead investigator, Jérémie Ginzburg (<email>jeremie.ginzburg@mcgill.ca</email>).</p></sec><sec id="S18"><title>Materials availability</title><p id="P34">This study did not generate new materials.</p></sec></sec><sec sec-type="methods" id="S19" specific-use="web-only"><title>Star Methods</title><sec id="S20"><title>Experimental model and subject details</title><sec id="S21" sec-type="subjects"><title>Patients</title><p id="P35">sEEG data from 11 patients suffering from drug-resistant focal epilepsy were collected (self-reported biological sex, 4 females, mean age = 29.6 years, SD = 3.2 years) following the Research Opportunities in Humans Consortium guidelines (Feinsinger et al., 2022). All patients were native French speakers from Canada and they all completed the task as instructed. The study protocol was explained verbally by both the research and clinical teams, and participation was discussed with each patient prior to hospitalization. All patients provided verbal assent and signed written informed consent forms. This research complies with all relevant ethical regulations; the experimental procedures were approved by the Ethics Review Board of the CHU de Québec - Université Laval (2022-5890, Québec, QC, Canada).</p></sec><sec id="S22"><title>Healthy Participants</title><p id="P36">20 healthy adults participated in the behavioral experiment online. No statistical method was used to predetermine sample size. All participants were native French speakers from France or Canada (self-reported biological sex, 11 females, mean age = 36.4 years, SD = 14.7 years). Participants reported normal hearing and no history of neurological or psychiatric disease. Data from one participant was excluded due to a problem with data collection on the online server. All participants provided written informed consent, and the experimental procedures were approved by the Ethics Review Board of the CIUSSS de la Capitale Nationale (2022–2476). The study has been conducted according to the principles expressed in the Declaration of Helsinki.</p></sec></sec><sec id="S23"><title>Method details</title><sec id="S24"><title>Stimuli and procedure</title><sec id="S25"><title>Stimuli</title><p id="P37">The stimuli used in both the sEEG and behavioral experiments were identical and consisted of the original audio track from the first 7 minutes and 50 seconds of the French-dubbed version of <italic>Harry Potter and the Philosopher’s Stone</italic> (titled <italic>Harry Potter à l’école des sorciers</italic> in French). This audio track included the complete soundtrack from the movie, including speech, music, and environmental sounds as they appear in the cinematic experience.</p></sec><sec id="S26"><title>Procedure: sEEG experiment</title><p id="P38">sEEG patients, who had electrodes implanted based on clinical requirements, were individually tested in a dedicated sEEG monitoring room at the <italic>Hôpital de l’Enfant Jésus</italic>, CHU de Québec (QC, Canada). sEEG data were recorded continuously throughout the session. Stimuli were presented using Presentation software (Neurobehavioral Systems, Albany, CA, USA). Patients were given the option to use their own earphones or those provided by the research team. After ensuring a comfortable sound level with a separate audio file, patients were instructed to listen attentively to the entire audio track.</p></sec><sec id="S27"><title>Procedure: Behavioral experiment</title><p id="P39">Participants completed a judgment task to determine whether music or speech was present in an audio track. The experiment was designed using PsychoPy (<xref ref-type="bibr" rid="R66">Peirce et al., 2019</xref>) and hosted on Pavlovia for online data collection (available here <ext-link ext-link-type="uri" xlink:href="https://pavlovia.org/palbouy/hp_categories_task_2">https://pavlovia.org/palbouy/hp_categories_task_2</ext-link>). Upon accessing the experiment, participants provided demographic information, including age and biological sex. They were instructed to ensure they would be undisturbed for at least 15 minutes, wear personal headphones, and adjust the volume to a comfortable level using an audio excerpt. The task instructions explained that participants were to press the “Q” key whenever they heard music and the “L” key whenever they heard speech, holding the key as long as the respective category was present and releasing it when it was absent. A visual feedback screen displayed two rectangles labeled “Q = Music” and “L = Language” (see <xref ref-type="fig" rid="F1">Figure 1a</xref>) which turned red when the corresponding key was pressed and reverted to white when released. Prior to starting, participants were asked to try pressing both keys to familiarize themselves with the feedback. They then completed a 2-minute training session using an unrelated audio track with both music and conversation before proceeding to the main task, which involved the 7-minute-and-50-second experimental audio stimuli described earlier.</p></sec></sec><sec id="S28"><title>Extraction of spectro-temporal modulations</title><p id="P40">The acoustic signal was analyzed over its time course using the STM framework (see <xref ref-type="bibr" rid="R1">Albouy et al., 2020</xref>, <xref ref-type="bibr" rid="R3">2024</xref>; <xref ref-type="bibr" rid="R22">Elliott &amp; Theunissen, 2009</xref> for similar procedures). STM patterns (i.e., modulograms) were extracted to obtain 512 STM per second, a rate corresponding to the sEEG sampling frequency using the ModFilter algorithm (<xref ref-type="bibr" rid="R22">Elliott &amp; Theunissen, 2009</xref>) with a 1.5-second window starting from the current time sample (see <xref ref-type="fig" rid="F1">Figure 1b</xref>). For each window, this method applies a 2D fast Fourier transform to the autocorrelation matrix of the sound stimulus in its spectrographic representation. This process generates, at every time sample, a 2D matrix capturing the power of the audio signal across spectral and temporal combinations. The STM pattern extraction was performed separately for each channel of the stereo audio track, and the resulting patterns were averaged together in the STM space to obtain a unified representation of the audio stimulus. Because speech and music STM patterns are typically symmetric between positive and negative temporal modulations (<xref ref-type="bibr" rid="R22">Elliott &amp; Theunissen, 2009</xref>) and to reduce data size for faster computation, the negative temporal modulation values were averaged with the corresponding positive values. This resulted in STM patterns at each time sample, with temporal modulations ranging from 0 to 15 Hz (20 points) and spectral modulations spanning 0 to 9 cycles/kHz (109 points), corresponding to a total of 2180 spectrotemporal combinations.</p></sec><sec id="S29"><title>Behavioral data</title><p id="P41">For each participant and each category (speech/music), a binary judgement vector was created, with 1 indicating that the participant perceived the category in the audio track, and 0 indicating its absence.</p></sec><sec id="S30"><title>sEEG recording and preprocessing</title><sec id="S31"><title>sEEG contacts localisation</title><p id="P42">Patients were stereotactically implanted with 16 to 18 semi-rigid multi-lead electrodes (AdTech; diameter: 0.8 mm). Brain activity was recorded from sEEG contacts, which were 2.0 mm wide and spaced 3, 4, 5, or 6 mm apart, targeting clinically defined brain structures. A 3D T1-weighted anatomical MRI (MPRAGE, 3T Siemens Trio, Siemens AG, Erlangen, Germany) was acquired pre-implantation, consisting of 160 sagittal slices with 1 mm<sup>3</sup> isotropic voxels covering the entire brain. Cortical surfaces were reconstructed from the T1-weighted MRI, and the precise locations of the sEEG contact were identified using a post-implantation CT scan co-registered to the pre-implantation MRI (ImaGIN toolbox, <ext-link ext-link-type="uri" xlink:href="https://f-tract.eu/software/imagin/">https://f-tract.eu/software/imagin/</ext-link>). The MNI coordinates of the sEEG contacts were determined using the SPM toolbox (<ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm/">http://www.fil.ion.ucl.ac.uk/spm/</ext-link>). Finally, atlas labels were extracted from the extended Human Connectome Project (HCPex) multimodal parcellation atlas (Huang et al., 2022), a modified and expanded version of the HCP-MMP v1.0 atlas (<xref ref-type="bibr" rid="R33">Glasser et al., 2016</xref>).</p></sec><sec id="S32"><title>Recording</title><p id="P43">Intracranial brain activity was recorded simultaneously from sEEG contacts (sampling rate: 512 Hz) using a Natus video-sEEG monitoring system. Online referencing was performed using a contact located in white matter. During acquisition, signals were bandpass filtered between 0.1 and 200 Hz. sEEG contacts contaminated by pathological epileptic activity or environmental artifacts were rejected in collaboration with the clinical team</p></sec><sec id="S33"><title>Preprocessing</title><p id="P44">The Brainstorm software with default parameters (<xref ref-type="bibr" rid="R86">Tadel et al., 2011</xref>) was used to preprocess and Hilbert transform sEEG data.</p><p id="P45">A notch filter was applied to reduce powerline interference (60 Hz and its harmonics, 120 and 160 Hz). The sEEG signal was segmented from -10 seconds to 490 seconds relative to the audio track onset, (-10 to 0 seconds baseline period). Each sEEG signal was subsequently re-referenced to its immediate neighbor using bipolar derivations. This bipolar montage enhances local specificity by removing artifacts common to adjacent contacts, such as powerline noise (e.g., 60 Hz) and distant physiological artifacts, while also reducing the effects of distant sources propagating through volume conduction. The resulting spatial resolution of each sEEG bipolar contact is estimated at 3–4 mm (<xref ref-type="bibr" rid="R43">Jerbi et al., 2009</xref>). Brain activity was recorded from a total of 846 bipolar contacts (ranging from 47 to 88 across patients).</p></sec><sec id="S34"><title>Hilbert transform</title><p id="P46">To investigate the relationship between oscillatory rhythm fluctuations and STM in the audio track, we applied the Hilbert transform to extract the amplitude envelope (i.e., magnitude) of oscillatory rhythms across the entire neurophysiological range. This analysis was performed for each sEEG contact during the audio track listening period. Oscillatory rhythms were segmented into the following frequency bands: delta (0.5–3 Hz), theta (4–8 Hz), alpha (9–13 Hz), beta (14–25 Hz), low gamma (26–50 Hz), and high gamma (50–150 Hz).</p></sec></sec></sec><sec id="S35"><title>Quantification and statistical analysis</title><p id="P47">The logistic regression analyses were conducted with the <italic>MNE-Python</italic> (Gramfort et al., 2013) and <italic>Scikit-learn</italic> (Pedregosa et al., 2011) libraries.</p><p id="P48">The ridge regression analyses were conducted with the <italic>spyeeg</italic> library (Guilleminot et al., 2021; latest version: <ext-link ext-link-type="uri" xlink:href="https://github.com/phg17/sPyEEG">https://github.com/phg17/sPyEEG</ext-link>).</p><p id="P49">Statistical analyses were conducted with MATLAB Version: 9.12.0 (R2022a).</p><p id="P50">Spatial clustering was conducted with custom-made MATLAB scripts (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/FT289">https://doi.org/10.17605/OSF.IO/FT289</ext-link>).</p><sec id="S36"><title>STM features used in perceptual categorisation</title><p id="P51">For each behavioral subject and each category (speech/music), a logistic regression model was applied, where the STM of all samples of the audio track served as features, and the binary behavioral judgment of the category acted as the target variable. Both the features and target variables were temporally downsampled to 50 Hz to optimize processing efficiency. The data were shuffled before splitting into folds to ensure representative and unbiased partitions, and the model was evaluated using stratified 5-fold cross-validation. This combined approach ensured robust generalization and mitigated potential biases from the data’s original ordering. To verify that all models were successfully trained, we computed the classifier’s performance as the mean accuracy across the 5 stratified cross-validation folds. Model accuracies for each category were significantly above chance across subjects (all adjusted p &lt; .001), as determined by a Wilcoxon signed-rank test against chance level (0.5, Bonferroni-corrected to account for the number of subjects with an alpha level of 0.05). A non-parametric test was used because it is better suited to handle the non-parametric nature of accuracy values. Then, feature patterns were derived by applying the Haufe transformation to the feature weights (<xref ref-type="bibr" rid="R38">Haufe et al., 2014</xref>). The Haufe transformation makes feature weights interpretable by converting them into feature patterns that directly reflect the contributions of features’ spatial sources to the model’s predictions. While raw feature weights often mix statistical relevance with noise and are influenced by the properties of the data representation, the transformation projects these weights back to the data space, disentangling the predictive components from the data structure. A feature pattern of zero indicates no contribution from the corresponding neural source, while positive patterns indicate that increased activity in those sources enhances the predicted outcome. Conversely, negative patterns suggest that increased activity in those sources suppresses or inversely relates to the predicted outcome, providing interpretable results. In other words, this process provided, for each behavioral subject and category, interpretable feature importance within the STM space, highlighting the specific regions of the modulation space used to decode each category. For each category, feature patterns were then averaged across behavioral subjects (<xref ref-type="fig" rid="F1">Figure 1c</xref>).</p></sec><sec id="S37"><title>Identification of STM-sensitive sEEG contacts</title><sec id="S38"><title>Ridge regressions</title><p id="P52">The goal of the analysis was to identify sEEG contacts that exhibited significant responses to the spectrotemporal modulations of the audio track. To achieve this, a ridge regression model was implemented, using the brain signals recorded from each sEEG contact (n=846)—decomposed into six frequency bands (as described above)—as features, and the time course of each spectrotemporal combination of the STM-decomposed audio track (n=2180) as the target variable. To enhance computational efficiency, both the feature and target variables were temporally downsampled to 50 Hz. To account for potential delays between brain responses and audio features, the ridge regression model was applied using a temporal response function (TRF), incorporating time lags ranging from 0 to 1.5 seconds. A backward-modeling approach was used, mapping brain signals onto audio features, as opposed to the more conventional forward-modeling approach. Data were shuffled prior to splitting into folds to ensure robust generalization, and the model was evaluated using 10-fold cross-validation. The optimal regularization parameter (α) was selected by testing values sampled logarithmically across 20 points, spanning from 10<sup>-10</sup> to 10<sup>10</sup>. Model performance for each fold was assessed using Spearman’s correlation between the predicted (ŷ) and actual (y) time courses of the spectrotemporal combinations. The final accuracy was calculated as the average correlation across all folds.</p><p id="P53">For subsequent analyses involving model weight extraction, only the models trained with the optimal regularization parameter—determined based on the highest average accuracy across folds—were used. The extracted weights were then averaged across all time lags tested with the TRF, providing a summary measure of feature importance that integrated responses over the entire lag period.</p></sec><sec id="S39"><title>Accuracy map and thresholding</title><p id="P54">The analysis described earlier resulted in, for each sEEG contact, a correlation coefficient (rho) from a ridge regression model predicting the time course of STM amplitude based on brain signal for each spectrotemporal combination. In other words, an accuracy map of rhos in the STM space was obtained for each sEEG contact (see <xref ref-type="fig" rid="F2">Figure 2b</xref> for an example). After applying a Fisher z-transformation to the rhos, statistical significance was evaluated using Bonferroni-corrected one-tailed t-tests for correlation coefficients on each rho of each accuracy map at an alpha level of 10<sup>-30</sup> and adjusting for the number of combinations in the STM space, sEEG contacts, and folds (as used in a similar procedure in <xref ref-type="bibr" rid="R7">Berezutskaya et al., 2022</xref>). Only sEEG contacts showing at least six adjacent significant rhos within the 2D STM space were retained.</p></sec><sec id="S40"><title>Spatial clustering</title><p id="P55">To identify brain areas responsive to STM, the MNI coordinates of STM-sensitive sEEG contacts were extracted. For each significant sEEG contact, a 5-mm radius 3D sphere was projected onto an MNI-standardized MRI volume using the MarsBar toolbox (<ext-link ext-link-type="uri" xlink:href="https://marsbar-toolbox.github.io/">https://marsbar-toolbox.github.io/</ext-link>). The radius was chosen based on the estimated spatial resolution of the bipolar montage (<xref ref-type="bibr" rid="R43">Jerbi et al., 2009</xref>). Only clusters consisting of at least two overlapping sEEG contacts from at least three different patients (to ensure generalizability) were considered significant, while isolated contacts were excluded to minimize Type I error rate.</p></sec></sec><sec id="S41"><title>Using brain-reconstructed STMs to decode behavioral categorization</title><p id="P56">In this analysis, we extracted from STM-sensitive sEEG contacts the prediction outputs from the previously described ridge regression model. For each sEEG contact, a predicted time-course of the fluctuation of amplitude of each spectrotemporal combination was generated from the corresponding ridge regression model, thus obtaining a predicted audio track in the STM space (i.e., brain-reconstructed STMs). Then, we used the brain-reconstructed STMs as features in a logistic regression model and the binary behavioral judgment of the category (speech/music) as the target variable. The data were shuffled before splitting into folds and the model was evaluated using stratified 5-fold cross-validation. For each category, only sEEG contacts showing significant model accuracy across behavioral subjects were retained, as determined by a Wilcoxon signed-rank test against chance level (0.5) for each category and sEEG contact across behavioral subjects (Bonferroni-corrected to account for the number of sEEG contacts with an alpha level of 0.05). Feature patterns were then derived from significant models by applying the Haufe transformation to the feature weights (<xref ref-type="bibr" rid="R38">Haufe et al., 2014</xref>). This resulted in, for each sEEG contact, behavioral subject and category, interpretable feature importance within the STM space. To precisely identify the specific regions of the STM space used to decode each category, one-tailed t-tests (right-tailed) against 0 on z-scored feature patterns were conducted for each combination within the STM space and each sEEG contact across behavioral subjects (Bonferroni-corrected adjusting for the number of sEEG contacts, and combinations, with an alpha level of 0.05). The one-tailed approach was used because only positive feature patterns indicate that increased activity in the feature space enhances prediction outcomes (<xref ref-type="bibr" rid="R38">Haufe et al., 2014</xref>). This analysis resulted in a representation that indicated, for each point in the STM space, the number of sEEG contacts exhibiting a significant feature pattern (<xref ref-type="fig" rid="F3">Figure 3e</xref>).</p></sec><sec id="S42"><title>Similarity between feature patterns using brain-reconstructed vs. actual audio track for behavioral categorization</title><p id="P57">For this analysis, we used two sets of feature patterns. First, we used the feature patterns from the previous logistic regression, where the STMs of all audio track samples served as the features, and the binary behavioral judgment of the category acted as the target variable for each category and participant. Second, we used the feature patterns from the logistic regression where the brain-reconstructed audio track in the STM space served as the features, and the behavioral categorization served as the target variable for each STM-sensitive sEEG contact (for which model accuracy was significantly greater than chance, as described in the previous section), category, and behavioral participant. For the second set, feature patterns from STM-sensitive sEEG contacts were averaged for each hemisphere of the brain. We used spearman correlations to quantify similarity between feature patterns, and their significance was tested using Bonferroni correction to account for the number of participants, categories, and hemispheres, with an alpha level of 0.05. This process resulted in a correlation coefficient for each behavioral participant, category, and hemisphere. In addition to the main analysis described above, we conducted a control analysis to assess the statistical significance of the observed similarity patterns. Specifically, for the first model, in which a logistic regression was applied to binary behavioral judgments using the STM representation of the actual audio track, we disrupted the relationship between predictors and data by shuffling the binary judgments independently for each model. We then performed the same similarity analysis on these shuffled datasets, allowing us to obtain a random distribution of correlation coefficients. To evaluate whether the observed correlation coefficients were significantly different from those obtained with shuffled predictors, we conducted a Wilcoxon signed-rank test for each category, comparing the true distribution to the shuffled distribution of correlation coefficients. Finally, to assess hemisphere preference for each category, we conducted a 2x2 non-parametric factorial ANOVA using the aligned rank transform (ART) method as implemented in the ARTool package in R (<xref ref-type="bibr" rid="R21">Elkin et al., 2021</xref>) on correlation coefficients, which served as similarity scores. The design included category (speech/music) as a within-subject factor and hemisphere (left/right) as a between-subject factor. Post-hoc comparisons were performed using pairwise Wilcoxon signed-rank tests, with false discovery rate (FDR) correction applied to control for multiple comparisons.</p></sec><sec id="S43"><title>Declaration of generative AI and AI-assisted technologies in the writing process</title><p id="P58">During the preparation of this work the lead author used ChatGPT 4 for purposes of language and grammar clean-up exclusively. After using this tool/service, the lead author reviewed and edited the content as needed and take(s) full responsibility for the content of the publication.</p></sec></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary material</label><media xlink:href="EMS206304-supplement-Supplementary_material.pdf" mimetype="application" mime-subtype="pdf" id="d74aAcFbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S44"><title>Acknowledgements</title><p>This work was supported via a project grant (486895) from the Canadian Institutes of Health Research to R.J.Z. and P.A., by the Fonds de Recherche du Québec via funding to the Center for Research in Brain, Language and Music (RSMA-340954), and by an NSERC Discovery grant to P.A. (RGPIN-2019-06162). R.J.Z. is funded via the Canada Research Chair program, and by the Scientific Grand Prize (FPA RD-2021-6) from the Fondation pour l’Audition (Paris, France). P.A. is supported by FRQS Junior 2 (329968) grant, and a Brain Canada Future leaders Grant (<ext-link ext-link-type="uri" xlink:href="https://braincanada.ca/">https://braincanada.ca/</ext-link>). B.M. is supported by the European Union (ERC, SPEEDY, ERC-CoG-101043344), Fondation Pour l’Audition (FPA RD-2022-09), the ANR (ANR-20-CE28-0007; ANR-16-CONV-0002) and the Excellence Initiative of Aix-Marseille University (A*MIDEX).</p><p>We thank the patients for their participation, and Romain Quentin and Oscar Bedford for their valuable discussions and insights.</p></ack><sec sec-type="data-availability" id="S45"><title>Data and code availability</title><p id="P59">De-identified and pre-processed patients and behavioral data have been deposited at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/FT289">https://doi.org/10.17605/OSF.IO/FT289</ext-link>. They are publicly available as of the date of publication. Note that raw SEEG and neuroimaging (T1-MPRAGE) data are protected and cannot be shared (2022-5890).</p><p id="P60">All original code has been deposited on OSF at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/FT289">https://doi.org/10.17605/OSF.IO/FT289</ext-link> and is publicly available as of the date of publication.</p><p id="P61">Any additional information required to reanalyze the data reported in this paper is available from the lead contact upon request.</p></sec><fn-group><fn id="FN2" fn-type="con"><p id="P62"><bold>Author Contributions</bold></p><p id="P63">JG: Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Software, Validation, Visualization, Writing – original draft, Writing – review &amp; editing. ECD: Data curation, Formal analysis, Software; AB: Data curation, Software; BM: Writing – review &amp; editing. LM, PLB: Data curation; RJZ: Conceptualization, Funding acquisition, Project administration, Resources, Investigation, Methodology, Supervision, Writing – review &amp; editing, PA: Conceptualization, Data curation, Funding acquisition, Project administration, Resources, Formal analysis, Investigation, Methodology, Validation, Supervision, Writing – review &amp; editing. The manuscript’s contents have been approved by all the co-authors, and they have provided consent for its publication.</p></fn><fn id="FN3" fn-type="conflict"><p id="P64"><bold>Declaration Of Interests</bold></p><p id="P65">The authors declare no competing interests.</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albouy</surname><given-names>P</given-names></name><name><surname>Benjamin</surname><given-names>L</given-names></name><name><surname>Morillon</surname><given-names>B</given-names></name><name><surname>Zatorre</surname><given-names>RJ</given-names></name></person-group><article-title>Distinct sensitivity to spectrotemporal modulation supports brain asymmetry for speech and melody</article-title><source>Science</source><year>2020</year><volume>367</volume><issue>6481</issue><fpage>1043</fpage><lpage>1047</lpage><pub-id pub-id-type="pmid">32108113</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albouy</surname><given-names>P</given-names></name><name><surname>Mattout</surname><given-names>J</given-names></name><name><surname>Bouet</surname><given-names>R</given-names></name><name><surname>Maby</surname><given-names>E</given-names></name><name><surname>Sanchez</surname><given-names>G</given-names></name><name><surname>Aguera</surname><given-names>P-E</given-names></name><name><surname>Daligault</surname><given-names>S</given-names></name><name><surname>Delpuech</surname><given-names>C</given-names></name><name><surname>Bertrand</surname><given-names>O</given-names></name><name><surname>Caclin</surname><given-names>A</given-names></name><name><surname>Tillmann</surname><given-names>B</given-names></name></person-group><article-title>Impaired pitch perception and memory in congenital amusia: The deficit starts in the auditory cortex</article-title><source>Brain</source><year>2013</year><volume>136</volume><issue>5</issue><fpage>1639</fpage><lpage>1661</lpage><pub-id pub-id-type="pmid">23616587</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albouy</surname><given-names>P</given-names></name><name><surname>Mehr</surname><given-names>SA</given-names></name><name><surname>Hoyer</surname><given-names>RS</given-names></name><name><surname>Ginzburg</surname><given-names>J</given-names></name><name><surname>Du</surname><given-names>Y</given-names></name><name><surname>Zatorre</surname><given-names>RJ</given-names></name></person-group><article-title>Spectro-temporal acoustical markers differentiate speech from song across cultures</article-title><source>Nature Communications</source><year>2024</year><volume>15</volume><issue>1</issue><fpage>4835</fpage><pub-id pub-id-type="pmcid">PMC11156671</pub-id><pub-id pub-id-type="pmid">38844457</pub-id><pub-id pub-id-type="doi">10.1038/s41467-024-49040-3</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Angulo-Perkins</surname><given-names>A</given-names></name><name><surname>Aubé</surname><given-names>W</given-names></name><name><surname>Peretz</surname><given-names>I</given-names></name><name><surname>Barrios</surname><given-names>FA</given-names></name><name><surname>Armony</surname><given-names>JL</given-names></name><name><surname>Concha</surname><given-names>L</given-names></name></person-group><article-title>Music listening engages specific cortical regions within the temporal lobes: Differences between musicians and non-musicians</article-title><source>Cortex</source><year>2014</year><volume>59</volume><fpage>126</fpage><lpage>137</lpage><pub-id pub-id-type="pmid">25173956</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Attneave</surname><given-names>F</given-names></name></person-group><article-title>Some informational aspects of visual perception</article-title><source>Psychological Review</source><year>1954</year><volume>61</volume><issue>3</issue><fpage>183</fpage><pub-id pub-id-type="pmid">13167245</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barlow</surname><given-names>HB</given-names></name></person-group><article-title>Possible principles underlying the transformation of sensory messages</article-title><source>Sensory Communication</source><year>1961</year><volume>1</volume><issue>01</issue><fpage>217</fpage><lpage>233</lpage></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berezutskaya</surname><given-names>J</given-names></name><name><surname>Vansteensel</surname><given-names>MJ</given-names></name><name><surname>Aarnoutse</surname><given-names>EJ</given-names></name><name><surname>Freudenburg</surname><given-names>ZV</given-names></name><name><surname>Piantoni</surname><given-names>G</given-names></name><name><surname>Branco</surname><given-names>MP</given-names></name><name><surname>Ramsey</surname><given-names>NF</given-names></name></person-group><article-title>Open multimodal iEEG-fMRI dataset from naturalistic stimulation with a short audiovisual film</article-title><source>Scientific Data</source><year>2022</year><volume>9</volume><issue>1</issue><fpage>91</fpage><pub-id pub-id-type="pmcid">PMC8938409</pub-id><pub-id pub-id-type="pmid">35314718</pub-id><pub-id pub-id-type="doi">10.1038/s41597-022-01173-0</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bizley</surname><given-names>JK</given-names></name><name><surname>Cohen</surname><given-names>YE</given-names></name></person-group><article-title>The what, where and how of auditory-object perception</article-title><source>Nature Reviews Neuroscience</source><year>2013</year><volume>14</volume><issue>10</issue><fpage>693</fpage><lpage>707</lpage><pub-id pub-id-type="pmcid">PMC4082027</pub-id><pub-id pub-id-type="pmid">24052177</pub-id><pub-id pub-id-type="doi">10.1038/nrn3565</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boebinger</surname><given-names>D</given-names></name><name><surname>Norman-Haignere</surname><given-names>SV</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><article-title>Music-selective neural populations arise without musical training</article-title><source>Journal of Neurophysiology</source><year>2021</year><volume>125</volume><issue>6</issue><fpage>2237</fpage><lpage>2263</lpage><pub-id pub-id-type="pmcid">PMC8285655</pub-id><pub-id pub-id-type="pmid">33596723</pub-id><pub-id pub-id-type="doi">10.1152/jn.00588.2020</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boemio</surname><given-names>A</given-names></name><name><surname>Fromm</surname><given-names>S</given-names></name><name><surname>Braun</surname><given-names>A</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>Hierarchical and asymmetric temporal sensitivity in human auditory cortices</article-title><source>Nature Neuroscience</source><year>2005</year><volume>8</volume><issue>3</issue><fpage>389</fpage><lpage>395</lpage><pub-id pub-id-type="pmid">15723061</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borderie</surname><given-names>A</given-names></name><name><surname>Caclin</surname><given-names>A</given-names></name><name><surname>Lachaux</surname><given-names>J-P</given-names></name><name><surname>Perrone-Bertollotti</surname><given-names>M</given-names></name><name><surname>Hoyer</surname><given-names>RS</given-names></name><name><surname>Kahane</surname><given-names>P</given-names></name><name><surname>Catenoix</surname><given-names>H</given-names></name><name><surname>Tillmann</surname><given-names>B</given-names></name><name><surname>Albouy</surname><given-names>P</given-names></name></person-group><article-title>Cross-frequency coupling in cortico-hippocampal networks supports the maintenance of sequential auditory information in short-term memory</article-title><source>PLOS Biology</source><year>2024</year><volume>22</volume><issue>3</issue><elocation-id>e3002512</elocation-id><pub-id pub-id-type="pmcid">PMC10914261</pub-id><pub-id pub-id-type="pmid">38442128</pub-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3002512</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calhoun</surname><given-names>BM</given-names></name><name><surname>Schreiner</surname><given-names>CE</given-names></name></person-group><article-title>Spectral envelope coding in cat primary auditory cortex: Linear and non-linear effects of stimulus characteristics</article-title><source>European Journal of Neuroscience</source><year>1998</year><volume>10</volume><issue>3</issue><fpage>926</fpage><lpage>940</lpage><pub-id pub-id-type="pmid">9753160</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cha</surname><given-names>K</given-names></name><name><surname>Zatorre</surname><given-names>RJ</given-names></name><name><surname>Schönwiesner</surname><given-names>M</given-names></name></person-group><article-title>Frequency Selectivity of Voxel-by-Voxel Functional Connectivity in Human Auditory Cortex</article-title><source>Cerebral Cortex</source><year>2016</year><volume>26</volume><issue>1</issue><fpage>211</fpage><lpage>224</lpage><pub-id pub-id-type="pmcid">PMC4677975</pub-id><pub-id pub-id-type="pmid">25183885</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhu193</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>A</given-names></name><name><surname>Teng</surname><given-names>X</given-names></name><name><surname>Assaneo</surname><given-names>MF</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>The human auditory system uses amplitude modulation to distinguish music from speech</article-title><source>PLOS Biology</source><year>2024</year><volume>22</volume><issue>5</issue><elocation-id>e3002631</elocation-id><pub-id pub-id-type="pmcid">PMC11132470</pub-id><pub-id pub-id-type="pmid">38805517</pub-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3002631</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Affourtit</surname><given-names>J</given-names></name><name><surname>Ryskin</surname><given-names>R</given-names></name><name><surname>Regev</surname><given-names>TI</given-names></name><name><surname>Norman-Haignere</surname><given-names>S</given-names></name><name><surname>Jouravlev</surname><given-names>O</given-names></name><name><surname>Malik-Moraleda</surname><given-names>S</given-names></name><name><surname>Kean</surname><given-names>H</given-names></name><name><surname>Varley</surname><given-names>R</given-names></name><name><surname>Fedorenko</surname><given-names>E</given-names></name></person-group><article-title>The human language system, including its inferior frontal component in “Broca’s area,” does not support music perception</article-title><source>Cerebral Cortex</source><year>2023</year><volume>33</volume><issue>12</issue><fpage>7904</fpage><lpage>7929</lpage><pub-id pub-id-type="pmcid">PMC10505454</pub-id><pub-id pub-id-type="pmid">37005063</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhad087</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chi</surname><given-names>T</given-names></name><name><surname>Ru</surname><given-names>P</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><article-title>Multiresolution spectrotemporal analysis of complex sounds</article-title><source>The Journal of the Acoustical Society of America</source><year>2005</year><volume>118</volume><issue>2</issue><fpage>887</fpage><lpage>906</lpage><pub-id pub-id-type="pmid">16158645</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coffey</surname><given-names>EBJ</given-names></name><name><surname>Herholz</surname><given-names>SC</given-names></name><name><surname>Chepesiuk</surname><given-names>AMP</given-names></name><name><surname>Baillet</surname><given-names>S</given-names></name><name><surname>Zatorre</surname><given-names>RJ</given-names></name></person-group><article-title>Cortical contributions to the auditory frequency-following response revealed by MEG</article-title><source>Nature Communications</source><year>2016</year><volume>7</volume><issue>1</issue><elocation-id>11070</elocation-id><pub-id pub-id-type="pmcid">PMC4820836</pub-id><pub-id pub-id-type="pmid">27009409</pub-id><pub-id pub-id-type="doi">10.1038/ncomms11070</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Depireux</surname><given-names>DA</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name><name><surname>Klein</surname><given-names>DJ</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><article-title>Spectro-Temporal Response Field Characterization With Dynamic Ripples in Ferret Primary Auditory Cortex</article-title><source>Journal of Neurophysiology</source><year>2001</year><volume>85</volume><issue>3</issue><fpage>1220</fpage><lpage>1234</lpage><pub-id pub-id-type="pmid">11247991</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>N</given-names></name><name><surname>Patel</surname><given-names>AD</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Butler</surname><given-names>H</given-names></name><name><surname>Luo</surname><given-names>C</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>Temporal modulations in speech and music</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><year>2017</year><volume>81</volume><fpage>181</fpage><lpage>187</lpage><pub-id pub-id-type="pmid">28212857</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eggermont</surname><given-names>JJ</given-names></name></person-group><article-title>Temporal modulation transfer functions in cat primary auditory cortex: Separating stimulus effects from neural mechanisms</article-title><source>Journal of Neurophysiology</source><year>2002</year><volume>87</volume><issue>1</issue><fpage>305</fpage><lpage>321</lpage><pub-id pub-id-type="pmid">11784752</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Elkin</surname><given-names>LA</given-names></name><name><surname>Kay</surname><given-names>M</given-names></name><name><surname>Higgins</surname><given-names>JJ</given-names></name><name><surname>Wobbrock</surname><given-names>JO</given-names></name></person-group><source>An Aligned Rank Transform Procedure for Multifactor Contrast Tests</source><conf-name>The 34th Annual ACM Symposium on User Interface Software and Technology</conf-name><year>2021</year><fpage>754</fpage><lpage>768</lpage><pub-id pub-id-type="doi">10.1145/3472749.3474784</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elliott</surname><given-names>TM</given-names></name><name><surname>Theunissen</surname><given-names>FE</given-names></name></person-group><article-title>The Modulation Transfer Function for Speech Intelligibility</article-title><source>PLoS Computational Biology</source><year>2009</year><volume>5</volume><issue>3</issue><elocation-id>e1000302</elocation-id><pub-id pub-id-type="pmcid">PMC2639724</pub-id><pub-id pub-id-type="pmid">19266016</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000302</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Escabí</surname><given-names>MA</given-names></name><name><surname>Miller</surname><given-names>LM</given-names></name><name><surname>Read</surname><given-names>HL</given-names></name><name><surname>Schreiner</surname><given-names>CE</given-names></name></person-group><article-title>Naturalistic auditory contrast improves spectrotemporal coding in the cat inferior colliculus</article-title><source>Journal of Neuroscience</source><year>2003</year><volume>23</volume><issue>37</issue><fpage>11489</fpage><lpage>11504</lpage><pub-id pub-id-type="pmcid">PMC6740949</pub-id><pub-id pub-id-type="pmid">14684853</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.23-37-11489.2003</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Escabí</surname><given-names>MA</given-names></name><name><surname>Schreiner</surname><given-names>CE</given-names></name></person-group><article-title>Nonlinear Spectrotemporal Sound Analysis by Neurons in the Auditory Midbrain</article-title><source>The Journal of Neuroscience</source><year>2002</year><volume>22</volume><issue>10</issue><fpage>4114</fpage><lpage>4131</lpage><pub-id pub-id-type="pmcid">PMC6757655</pub-id><pub-id pub-id-type="pmid">12019330</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.22-10-04114.2002</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fadiga</surname><given-names>L</given-names></name><name><surname>Craighero</surname><given-names>L</given-names></name><name><surname>D’Ausilio</surname><given-names>A</given-names></name></person-group><article-title>Broca’s Area in Language, Action, and Music</article-title><source>Annals of the New York Academy of Sciences</source><year>2009</year><volume>1169</volume><issue>1</issue><pub-id pub-id-type="pmid">19673823</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flinker</surname><given-names>A</given-names></name><name><surname>Doyle</surname><given-names>WK</given-names></name><name><surname>Mehta</surname><given-names>AD</given-names></name><name><surname>Devinsky</surname><given-names>O</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>Spectrotemporal modulation provides a unifying framework for auditory cortical asymmetries</article-title><source>Nature Human Behaviour</source><year>2019</year><volume>3</volume><issue>4</issue><fpage>393</fpage><lpage>405</lpage><pub-id pub-id-type="pmcid">PMC6650286</pub-id><pub-id pub-id-type="pmid">30971792</pub-id><pub-id pub-id-type="doi">10.1038/s41562-019-0548-z</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friederici</surname><given-names>AD</given-names></name></person-group><article-title>Hierarchy processing in human neurobiology: How specific is it?</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><year>2020</year><volume>375</volume><issue>1789</issue><elocation-id>20180391</elocation-id><pub-id pub-id-type="pmcid">PMC6895560</pub-id><pub-id pub-id-type="pmid">31735144</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2018.0391</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gabrieli</surname><given-names>JDE</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name><name><surname>Whitfield-Gabrieli</surname><given-names>S</given-names></name></person-group><article-title>Prediction as a Humanitarian and Pragmatic Contribution from Human Cognitive Neuroscience</article-title><source>Neuron</source><year>2015</year><volume>85</volume><issue>1</issue><fpage>11</fpage><lpage>26</lpage><pub-id pub-id-type="pmcid">PMC4287988</pub-id><pub-id pub-id-type="pmid">25569345</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2014.10.047</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gervain</surname><given-names>J</given-names></name><name><surname>Geffen</surname><given-names>MN</given-names></name></person-group><article-title>Efficient Neural Coding in Auditory and Speech Perception</article-title><source>Trends in Neurosciences</source><year>2019</year><volume>42</volume><issue>1</issue><fpage>56</fpage><lpage>65</lpage><pub-id pub-id-type="pmcid">PMC6542557</pub-id><pub-id pub-id-type="pmid">30297085</pub-id><pub-id pub-id-type="doi">10.1016/j.tins.2018.09.004</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giroud</surname><given-names>J</given-names></name><name><surname>Lerousseau</surname><given-names>JP</given-names></name><name><surname>Pellegrino</surname><given-names>F</given-names></name><name><surname>Morillon</surname><given-names>B</given-names></name></person-group><article-title>The channel capacity of multilevel linguistic features constrains speech comprehension</article-title><source>Cognition</source><year>2023</year><volume>232</volume><elocation-id>105345</elocation-id><pub-id pub-id-type="pmid">36462227</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giroud</surname><given-names>J</given-names></name><name><surname>Trébuchon</surname><given-names>A</given-names></name><name><surname>Mercier</surname><given-names>M</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name><name><surname>Morillon</surname><given-names>B</given-names></name></person-group><article-title>The human auditory cortex concurrently tracks syllabic and phonemic timescales via acoustic spectral flux</article-title><source>Science Advances</source><year>2024</year><volume>10</volume><issue>51</issue><elocation-id>eado8915</elocation-id><pub-id pub-id-type="pmcid">PMC11661434</pub-id><pub-id pub-id-type="pmid">39705351</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.ado8915</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giroud</surname><given-names>J</given-names></name><name><surname>Trébuchon</surname><given-names>A</given-names></name><name><surname>Schön</surname><given-names>D</given-names></name><name><surname>Marquis</surname><given-names>P</given-names></name><name><surname>Liegeois-Chauvel</surname><given-names>C</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name><name><surname>Morillon</surname><given-names>B</given-names></name></person-group><article-title>Asymmetric sampling in human auditory cortex reveals spectral processing hierarchy</article-title><source>PLOS Biology</source><year>2020</year><volume>18</volume><issue>3</issue><elocation-id>e3000207</elocation-id><pub-id pub-id-type="pmcid">PMC7067489</pub-id><pub-id pub-id-type="pmid">32119667</pub-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3000207</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glasser</surname><given-names>MF</given-names></name><name><surname>Coalson</surname><given-names>TS</given-names></name><name><surname>Robinson</surname><given-names>EC</given-names></name><name><surname>Hacker</surname><given-names>CD</given-names></name><name><surname>Harwell</surname><given-names>J</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name><name><surname>Andersson</surname><given-names>J</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><etal/></person-group><article-title>A multi-modal parcellation of human cerebral cortex</article-title><source>Nature</source><year>2016</year><volume>536</volume><issue>7615</issue><fpage>171</fpage><lpage>178</lpage><pub-id pub-id-type="pmcid">PMC4990127</pub-id><pub-id pub-id-type="pmid">27437579</pub-id><pub-id pub-id-type="doi">10.1038/nature18933</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodale</surname><given-names>MA</given-names></name><name><surname>Milner</surname><given-names>AD</given-names></name></person-group><article-title>Separate visual pathways for perception and action</article-title><source>Trends in Neurosciences</source><year>1992</year><volume>15</volume><issue>1</issue><fpage>20</fpage><lpage>25</lpage><pub-id pub-id-type="pmid">1374953</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graham</surname><given-names>DJ</given-names></name><name><surname>Field</surname><given-names>DJ</given-names></name></person-group><article-title>Statistical regularities of art images and natural scenes: Spectra, sparseness and nonlinearities</article-title><source>Spatial Vision</source><year>2007</year><volume>21</volume><pub-id pub-id-type="pmid">18073056</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guevara Erra</surname><given-names>R</given-names></name><name><surname>Gervain</surname><given-names>J</given-names></name></person-group><article-title>The Efficient Coding of Speech: Cross-Linguistic Differences</article-title><source>PLOS ONE</source><year>2016</year><volume>11</volume><issue>2</issue><elocation-id>e0148861</elocation-id><pub-id pub-id-type="pmcid">PMC4763999</pub-id><pub-id pub-id-type="pmid">26901527</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0148861</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haiduk</surname><given-names>F</given-names></name><name><surname>Zatorre</surname><given-names>RJ</given-names></name><name><surname>Benjamin</surname><given-names>L</given-names></name><name><surname>Morillon</surname><given-names>B</given-names></name><name><surname>Albouy</surname><given-names>P</given-names></name></person-group><article-title>Spectrotemporal cues and attention jointly modulate fMRI network topology for sentence and melody perception</article-title><source>Scientific Reports</source><year>2024</year><volume>14</volume><issue>1</issue><fpage>5501</fpage><pub-id pub-id-type="pmcid">PMC10917817</pub-id><pub-id pub-id-type="pmid">38448636</pub-id><pub-id pub-id-type="doi">10.1038/s41598-024-56139-6</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haufe</surname><given-names>S</given-names></name><name><surname>Meinecke</surname><given-names>F</given-names></name><name><surname>Görgen</surname><given-names>K</given-names></name><name><surname>Dähne</surname><given-names>S</given-names></name><name><surname>Haynes</surname><given-names>J-D</given-names></name><name><surname>Blankertz</surname><given-names>B</given-names></name><name><surname>Bießmann</surname><given-names>F</given-names></name></person-group><article-title>On the interpretation of weight vectors of linear models in multivariate neuroimaging</article-title><source>NeuroImage</source><year>2014</year><volume>87</volume><fpage>96</fpage><lpage>110</lpage><pub-id pub-id-type="pmid">24239590</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hsu</surname><given-names>A</given-names></name><name><surname>Woolley</surname><given-names>SMN</given-names></name><name><surname>Fremouw</surname><given-names>TE</given-names></name><name><surname>Theunissen</surname><given-names>FE</given-names></name></person-group><article-title>Modulation Power and Phase Spectrum of Natural Sounds Enhance Neural Encoding Performed by Single Auditory Neurons</article-title><source>The Journal of Neuroscience</source><year>2004</year><volume>24</volume><issue>41</issue><fpage>9201</fpage><lpage>9211</lpage><pub-id pub-id-type="pmcid">PMC6730078</pub-id><pub-id pub-id-type="pmid">15483139</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2449-04.2004</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hullett</surname><given-names>PW</given-names></name><name><surname>Hamilton</surname><given-names>LS</given-names></name><name><surname>Mesgarani</surname><given-names>N</given-names></name><name><surname>Schreiner</surname><given-names>CE</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name></person-group><article-title>Human Superior Temporal Gyrus Organization of Spectrotemporal Modulation Tuning Derived from Speech Stimuli</article-title><source>The Journal of Neuroscience</source><year>2016</year><volume>36</volume><issue>6</issue><fpage>2014</fpage><lpage>2026</lpage><pub-id pub-id-type="pmcid">PMC4748082</pub-id><pub-id pub-id-type="pmid">26865624</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1779-15.2016</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hyde</surname><given-names>KL</given-names></name><name><surname>Peretz</surname><given-names>I</given-names></name><name><surname>Zatorre</surname><given-names>RJ</given-names></name></person-group><article-title>Evidence for the role of the right auditory cortex in fine pitch resolution</article-title><source>Neuropsychologia</source><year>2008</year><volume>46</volume><issue>2</issue><fpage>632</fpage><lpage>639</lpage><pub-id pub-id-type="pmid">17959204</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jamison</surname><given-names>HL</given-names></name><name><surname>Watkins</surname><given-names>KE</given-names></name><name><surname>Bishop</surname><given-names>DVM</given-names></name><name><surname>Matthews</surname><given-names>PM</given-names></name></person-group><article-title>Hemispheric Specialization for Processing Auditory Nonspeech Stimuli</article-title><source>Cerebral Cortex</source><year>2006</year><volume>16</volume><issue>9</issue><fpage>1266</fpage><lpage>1275</lpage><pub-id pub-id-type="pmid">16280465</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jerbi</surname><given-names>K</given-names></name><name><surname>Ossandón</surname><given-names>T</given-names></name><name><surname>Hamamé</surname><given-names>CM</given-names></name><name><surname>Senova</surname><given-names>S</given-names></name><name><surname>Dalal</surname><given-names>SS</given-names></name><name><surname>Jung</surname><given-names>J</given-names></name><name><surname>Minotti</surname><given-names>L</given-names></name><name><surname>Bertrand</surname><given-names>O</given-names></name><name><surname>Berthoz</surname><given-names>A</given-names></name><name><surname>Kahane</surname><given-names>P</given-names></name><name><surname>Lachaux</surname><given-names>J</given-names></name></person-group><article-title>Task-related gamma-band dynamics from an intracerebral perspective: Review and implications for surface EEG and MEG</article-title><source>Human Brain Mapping</source><year>2009</year><volume>30</volume><issue>6</issue><fpage>1758</fpage><lpage>1771</lpage><pub-id pub-id-type="pmcid">PMC6870589</pub-id><pub-id pub-id-type="pmid">19343801</pub-id><pub-id pub-id-type="doi">10.1002/hbm.20750</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnsrude</surname><given-names>IS</given-names></name><name><surname>Penhune</surname><given-names>VB</given-names></name><name><surname>Zatorre</surname><given-names>RJ</given-names></name></person-group><article-title>Functional specificity in the right human auditory cortex for perceiving pitch direction</article-title><source>Brain</source><year>2000</year><volume>123</volume><issue>1</issue><fpage>155</fpage><lpage>163</lpage><pub-id pub-id-type="pmid">10611129</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaas</surname><given-names>JH</given-names></name><name><surname>Hackett</surname><given-names>TA</given-names></name></person-group><article-title>“What” and “where” processing in auditory cortex</article-title><source>Nature Neuroscience</source><year>1999</year><volume>2</volume><issue>12</issue><fpage>1045</fpage><lpage>1047</lpage><pub-id pub-id-type="pmid">10570476</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kell</surname><given-names>AJE</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Shook</surname><given-names>EN</given-names></name><name><surname>Norman-Haignere</surname><given-names>SV</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name></person-group><article-title>A Task-Optimized Neural Network Replicates Human Auditory Behavior, Predicts Brain Responses, and Reveals a Cortical Processing Hierarchy</article-title><source>Neuron</source><year>2018</year><volume>98</volume><issue>3</issue><fpage>630</fpage><lpage>644</lpage><elocation-id>e16</elocation-id><pub-id pub-id-type="pmid">29681533</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koelsch</surname><given-names>S</given-names></name></person-group><article-title>Toward a Neural Basis of Music Perception – A Review and Updated Model</article-title><source>Frontier in Psychology</source><year>2011</year><volume>2</volume><pub-id pub-id-type="pmcid">PMC3114071</pub-id><pub-id pub-id-type="pmid">21713060</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2011.00110</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kowalski</surname><given-names>N</given-names></name><name><surname>Depireux</surname><given-names>DA</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><article-title>Analysis of dynamic spectra in ferret primary auditory cortex. I. Characteristics of single-unit responses to moving ripple spectra</article-title><source>Journal of Neurophysiology</source><year>1996</year><volume>76</volume><issue>5</issue><fpage>3503</fpage><lpage>3523</lpage><pub-id pub-id-type="pmid">8930289</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langers</surname><given-names>DR</given-names></name><name><surname>Backes</surname><given-names>WH</given-names></name><name><surname>Dijk</surname><given-names>PV</given-names></name></person-group><article-title>Spectrotemporal features of the auditory cortex: The activation in response to dynamic ripples</article-title><source>NeuroImage</source><year>2003</year><volume>20</volume><issue>1</issue><fpage>265</fpage><lpage>275</lpage><pub-id pub-id-type="pmid">14527587</pub-id></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewicki</surname><given-names>MS</given-names></name></person-group><article-title>Efficient coding of natural sounds</article-title><source>Nature Neuroscience</source><year>2002</year><volume>5</volume><issue>4</issue><fpage>356</fpage><lpage>363</lpage><pub-id pub-id-type="pmid">11896400</pub-id></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loui</surname><given-names>P</given-names></name><name><surname>Alsop</surname><given-names>D</given-names></name><name><surname>Schlaug</surname><given-names>G</given-names></name></person-group><article-title>Tone Deafness: A New Disconnection Syndrome?</article-title><source>The Journal of Neuroscience</source><year>2009</year><volume>29</volume><issue>33</issue><fpage>10215</fpage><lpage>10220</lpage><pub-id pub-id-type="pmcid">PMC2747525</pub-id><pub-id pub-id-type="pmid">19692596</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1701-09.2009</pub-id></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname><given-names>H</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>Cortical Oscillations in Auditory Perception and Speech: Evidence for Two Temporal Windows in Human Auditory Cortex</article-title><source>Frontiers in Psychology</source><year>2012</year><volume>3</volume><pub-id pub-id-type="pmcid">PMC3364513</pub-id><pub-id pub-id-type="pmid">22666214</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2012.00170</pub-id></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Massoudi</surname><given-names>R</given-names></name><name><surname>Van Wanrooij</surname><given-names>MM</given-names></name><name><surname>Versnel</surname><given-names>H</given-names></name><name><surname>Van Opstal</surname><given-names>AJ</given-names></name></person-group><article-title>Spectrotemporal response properties of core auditory cortex neurons in awake monkey</article-title><source>PLoS One</source><year>2015</year><volume>10</volume><issue>2</issue><elocation-id>e0116118</elocation-id><pub-id pub-id-type="pmcid">PMC4332665</pub-id><pub-id pub-id-type="pmid">25680187</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0116118</pub-id></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maunsell</surname><given-names>JHR</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><article-title>Visual Processing in Monkey Extrastriate Cortex</article-title><source>Annual Review of Neuroscience</source><year>1987</year><volume>10</volume><issue>1</issue><fpage>363</fpage><lpage>401</lpage><pub-id pub-id-type="pmid">3105414</pub-id></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McDermott</surname><given-names>JH</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><article-title>Sound Texture Perception via Statistics of the Auditory Periphery: Evidence from Sound Synthesis</article-title><source>Neuron</source><year>2011</year><volume>71</volume><issue>5</issue><fpage>926</fpage><lpage>940</lpage><pub-id pub-id-type="pmcid">PMC4143345</pub-id><pub-id pub-id-type="pmid">21903084</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2011.06.032</pub-id></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehr</surname><given-names>SA</given-names></name><name><surname>Krasnow</surname><given-names>MM</given-names></name><name><surname>Bryant</surname><given-names>GA</given-names></name><name><surname>Hagen</surname><given-names>EH</given-names></name></person-group><article-title>Origins of music in credible signaling</article-title><source>Behavioral and Brain Sciences</source><year>2021</year><volume>44</volume><fpage>e60</fpage><pub-id pub-id-type="pmcid">PMC7907251</pub-id><pub-id pub-id-type="pmid">32843107</pub-id><pub-id pub-id-type="doi">10.1017/S0140525X20000345</pub-id></element-citation></ref><ref id="R57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>LM</given-names></name><name><surname>Escabí</surname><given-names>MA</given-names></name><name><surname>Read</surname><given-names>HL</given-names></name><name><surname>Schreiner</surname><given-names>CE</given-names></name></person-group><article-title>Spectrotemporal Receptive Fields in the Lemniscal Auditory Thalamus and Cortex</article-title><source>Journal of Neurophysiology</source><year>2002</year><volume>87</volume><issue>1</issue><fpage>516</fpage><lpage>527</lpage><pub-id pub-id-type="pmid">11784767</pub-id></element-citation></ref><ref id="R58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Milner</surname><given-names>B</given-names></name></person-group><article-title>Laterality effects in audition</article-title><source>Interhemispheric Relations and Cerebral Dominance</source><year>1962</year><fpage>177</fpage><lpage>195</lpage></element-citation></ref><ref id="R59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ming</surname><given-names>VL</given-names></name><name><surname>Holt</surname><given-names>LL</given-names></name></person-group><article-title>Efficient coding in human auditory perception</article-title><source>The Journal of the Acoustical Society of America</source><year>2009</year><volume>126</volume><issue>3</issue><fpage>1312</fpage><lpage>1320</lpage><pub-id pub-id-type="pmcid">PMC2809690</pub-id><pub-id pub-id-type="pmid">19739745</pub-id><pub-id pub-id-type="doi">10.1121/1.3158939</pub-id></element-citation></ref><ref id="R60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norman-Haignere</surname><given-names>S</given-names></name><name><surname>Feather</surname><given-names>J</given-names></name><name><surname>Boebinger</surname><given-names>D</given-names></name><name><surname>Brunner</surname><given-names>P</given-names></name><name><surname>Ritaccio</surname><given-names>A</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name><name><surname>Schalk</surname><given-names>G</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><article-title>A neural population selective for song in human auditory cortex</article-title><source>Current Biology</source><year>2022</year><volume>32</volume><issue>7</issue><fpage>1470</fpage><lpage>1484</lpage><elocation-id>e12</elocation-id><pub-id pub-id-type="pmcid">PMC9092957</pub-id><pub-id pub-id-type="pmid">35196507</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2022.01.069</pub-id></element-citation></ref><ref id="R61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norman-Haignere</surname><given-names>S</given-names></name><name><surname>Kanwisher</surname><given-names>NG</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name></person-group><article-title>Distinct Cortical Pathways for Music and Speech Revealed by Hypothesis-Free Voxel Decomposition</article-title><source>Neuron</source><year>2015</year><volume>88</volume><issue>6</issue><fpage>1281</fpage><lpage>1296</lpage><pub-id pub-id-type="pmcid">PMC4740977</pub-id><pub-id pub-id-type="pmid">26687225</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2015.11.035</pub-id></element-citation></ref><ref id="R62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Okada</surname><given-names>K</given-names></name><name><surname>Rong</surname><given-names>F</given-names></name><name><surname>Venezia</surname><given-names>J</given-names></name><name><surname>Matchin</surname><given-names>W</given-names></name><name><surname>Hsieh</surname><given-names>I-H</given-names></name><name><surname>Saberi</surname><given-names>K</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name><name><surname>Hickok</surname><given-names>G</given-names></name></person-group><article-title>Hierarchical Organization of Human Auditory Cortex: Evidence from Acoustic Invariance in the Response to Intelligible Speech</article-title><source>Cerebral Cortex</source><year>2010</year><volume>20</volume><issue>10</issue><fpage>2486</fpage><lpage>2495</lpage><pub-id pub-id-type="pmcid">PMC2936804</pub-id><pub-id pub-id-type="pmid">20100898</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhp318</pub-id></element-citation></ref><ref id="R63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Okamoto</surname><given-names>H</given-names></name><name><surname>Kakigi</surname><given-names>R</given-names></name></person-group><article-title>Hemispheric Asymmetry of Auditory Mismatch Negativity Elicited by Spectral and Temporal Deviants: A Magnetoencephalographic Study</article-title><source>Brain Topography</source><year>2015</year><volume>28</volume><issue>3</issue><fpage>471</fpage><lpage>478</lpage><pub-id pub-id-type="pmcid">PMC4408358</pub-id><pub-id pub-id-type="pmid">24366694</pub-id><pub-id pub-id-type="doi">10.1007/s10548-013-0347-1</pub-id></element-citation></ref><ref id="R64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pasley</surname><given-names>BN</given-names></name><name><surname>David</surname><given-names>SV</given-names></name><name><surname>Mesgarani</surname><given-names>N</given-names></name><name><surname>Flinker</surname><given-names>A</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name><name><surname>Crone</surname><given-names>NE</given-names></name><name><surname>Knight</surname><given-names>RT</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name></person-group><article-title>Reconstructing Speech from Human Auditory Cortex</article-title><source>PLoS Biology</source><year>2012</year><volume>10</volume><issue>1</issue><elocation-id>e1001251</elocation-id><pub-id pub-id-type="pmcid">PMC3269422</pub-id><pub-id pub-id-type="pmid">22303281</pub-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1001251</pub-id></element-citation></ref><ref id="R65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patterson</surname><given-names>RD</given-names></name><name><surname>Uppenkamp</surname><given-names>S</given-names></name><name><surname>Johnsrude</surname><given-names>IS</given-names></name><name><surname>Griffiths</surname><given-names>TD</given-names></name></person-group><article-title>The Processing of Temporal Pitch and Melody Information in Auditory Cortex</article-title><source>Neuron</source><year>2002</year><volume>36</volume><issue>4</issue><fpage>767</fpage><lpage>776</lpage><pub-id pub-id-type="pmid">12441063</pub-id></element-citation></ref><ref id="R66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peirce</surname><given-names>J</given-names></name><name><surname>Gray</surname><given-names>JR</given-names></name><name><surname>Simpson</surname><given-names>S</given-names></name><name><surname>MacAskill</surname><given-names>M</given-names></name><name><surname>Höchenberger</surname><given-names>R</given-names></name><name><surname>Sogo</surname><given-names>H</given-names></name><name><surname>Kastman</surname><given-names>E</given-names></name><name><surname>Lindeløv</surname><given-names>JK</given-names></name></person-group><article-title>PsychoPy2: Experiments in behavior made easy</article-title><source>Behavior Research Methods</source><year>2019</year><volume>51</volume><issue>1</issue><fpage>195</fpage><lpage>203</lpage><pub-id pub-id-type="pmcid">PMC6420413</pub-id><pub-id pub-id-type="pmid">30734206</pub-id><pub-id pub-id-type="doi">10.3758/s13428-018-01193-y</pub-id></element-citation></ref><ref id="R67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rauschecker</surname><given-names>JP</given-names></name><name><surname>Scott</surname><given-names>SK</given-names></name></person-group><article-title>Maps and streams in the auditory cortex: Nonhuman primates illuminate human speech processing</article-title><source>Nature Neuroscience</source><year>2009</year><volume>12</volume><issue>6</issue><fpage>718</fpage><lpage>724</lpage><pub-id pub-id-type="pmcid">PMC2846110</pub-id><pub-id pub-id-type="pmid">19471271</pub-id><pub-id pub-id-type="doi">10.1038/nn.2331</pub-id></element-citation></ref><ref id="R68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rieke</surname><given-names>F</given-names></name><name><surname>Bodnar</surname><given-names>D</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name></person-group><article-title>Naturalistic stimuli increase the rate and efficiency of information transmission by primary auditory afferents</article-title><source>Proceedings of the Royal Society of London. Series B: Biological Sciences</source><year>1995</year><volume>262</volume><issue>1365</issue><fpage>259</fpage><lpage>265</lpage><pub-id pub-id-type="pmid">8587884</pub-id></element-citation></ref><ref id="R69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rissman</surname><given-names>J</given-names></name><name><surname>Wagner</surname><given-names>AD</given-names></name></person-group><article-title>Distributed Representations in Memory: Insights from Functional Brain Imaging</article-title><source>Annual Review of Psychology</source><year>2012</year><volume>63</volume><issue>1</issue><fpage>101</fpage><lpage>128</lpage><pub-id pub-id-type="pmcid">PMC4533899</pub-id><pub-id pub-id-type="pmid">21943171</pub-id><pub-id pub-id-type="doi">10.1146/annurev-psych-120710-100344</pub-id></element-citation></ref><ref id="R70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robert</surname><given-names>P</given-names></name><name><surname>Zatorre</surname><given-names>RJ</given-names></name><name><surname>Gupta</surname><given-names>A</given-names></name><name><surname>Sein</surname><given-names>J</given-names></name><name><surname>Anton</surname><given-names>J-L</given-names></name><name><surname>Belin</surname><given-names>P</given-names></name><name><surname>Thoret</surname><given-names>E</given-names></name><name><surname>Morillon</surname><given-names>B</given-names></name></person-group><article-title>Auditory hemispheric asymmetry for actions and objects</article-title><source>Cerebral Cortex</source><year>2024</year><volume>34</volume><issue>7</issue><elocation-id>bhae292</elocation-id><pub-id pub-id-type="pmid">39051660</pub-id></element-citation></ref><ref id="R71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rodríguez</surname><given-names>FA</given-names></name><name><surname>Read</surname><given-names>HL</given-names></name><name><surname>Escabí</surname><given-names>MA</given-names></name></person-group><article-title>Spectral and temporal modulation tradeoff in the inferior colliculus</article-title><source>Journal of Neurophysiology</source><year>2010</year><volume>103</volume><issue>2</issue><fpage>887</fpage><lpage>903</lpage><pub-id pub-id-type="pmcid">PMC2822687</pub-id><pub-id pub-id-type="pmid">20018831</pub-id><pub-id pub-id-type="doi">10.1152/jn.00813.2009</pub-id></element-citation></ref><ref id="R72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Samson</surname><given-names>S</given-names></name><name><surname>Zatorre</surname><given-names>RJ</given-names></name></person-group><article-title>Melodic and harmonic discrimination following unilateral cerebral excision</article-title><source>Brain and Cognition</source><year>1988</year><volume>7</volume><issue>3</issue><fpage>348</fpage><lpage>360</lpage><pub-id pub-id-type="pmid">3401387</pub-id></element-citation></ref><ref id="R73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Santoro</surname><given-names>R</given-names></name><name><surname>Moerel</surname><given-names>M</given-names></name><name><surname>De Martino</surname><given-names>F</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Formisano</surname><given-names>E</given-names></name></person-group><article-title>Encoding of Natural Sounds at Multiple Spectral and Temporal Resolutions in the Human Auditory Cortex</article-title><source>PLoS Computational Biology</source><year>2014</year><volume>10</volume><issue>1</issue><elocation-id>e1003412</elocation-id><pub-id pub-id-type="pmcid">PMC3879146</pub-id><pub-id pub-id-type="pmid">24391486</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003412</pub-id></element-citation></ref><ref id="R74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scheinost</surname><given-names>D</given-names></name><name><surname>Noble</surname><given-names>S</given-names></name><name><surname>Horien</surname><given-names>C</given-names></name><name><surname>Greene</surname><given-names>AS</given-names></name><name><surname>Lake</surname><given-names>E</given-names><prefix>Mr</prefix></name><name><surname>Salehi</surname><given-names>M</given-names></name><name><surname>Gao</surname><given-names>S</given-names></name><name><surname>Shen</surname><given-names>X</given-names></name><name><surname>O’Connor</surname><given-names>D</given-names></name><name><surname>Barron</surname><given-names>DS</given-names></name><name><surname>Yip</surname><given-names>SW</given-names></name><name><surname>Rosenberg</surname><given-names>MD</given-names></name><name><surname>Constable</surname><given-names>RT</given-names></name></person-group><article-title>Ten simple rules for predictive modeling of individual differences in neuroimaging</article-title><source>NeuroImage</source><year>2019</year><volume>193</volume><fpage>35</fpage><lpage>45</lpage><pub-id pub-id-type="pmcid">PMC6521850</pub-id><pub-id pub-id-type="pmid">30831310</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.02.057</pub-id></element-citation></ref><ref id="R75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schön</surname><given-names>D</given-names></name><name><surname>Gordon</surname><given-names>R</given-names></name><name><surname>Campagne</surname><given-names>A</given-names></name><name><surname>Magne</surname><given-names>C</given-names></name><name><surname>Astésano</surname><given-names>C</given-names></name><name><surname>Anton</surname><given-names>J-L</given-names></name><name><surname>Besson</surname><given-names>M</given-names></name></person-group><article-title>Similar cerebral networks in language, music and song perception</article-title><source>NeuroImage</source><year>2010</year><volume>51</volume><issue>1</issue><fpage>450</fpage><lpage>461</lpage><pub-id pub-id-type="pmid">20156575</pub-id></element-citation></ref><ref id="R76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schönwiesner</surname><given-names>M</given-names></name><name><surname>Rübsamen</surname><given-names>R</given-names></name><name><surname>Von Cramon</surname><given-names>DY</given-names></name></person-group><article-title>Hemispheric asymmetry for spectral and temporal processing in the human antero-lateral auditory belt cortex</article-title><source>European Journal of Neuroscience</source><year>2005</year><volume>22</volume><issue>6</issue><fpage>1521</fpage><lpage>1528</lpage><pub-id pub-id-type="pmid">16190905</pub-id></element-citation></ref><ref id="R77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schönwiesner</surname><given-names>M</given-names></name><name><surname>Zatorre</surname><given-names>RJ</given-names></name></person-group><article-title>Spectro-temporal modulation transfer function of single voxels in the human auditory cortex measured with high-resolution fMRI</article-title><source>Proceedings of the National Academy of Sciences</source><year>2009</year><volume>106</volume><issue>34</issue><fpage>14611</fpage><lpage>14616</lpage><pub-id pub-id-type="pmcid">PMC2732853</pub-id><pub-id pub-id-type="pmid">19667199</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0907682106</pub-id></element-citation></ref><ref id="R78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shamma</surname><given-names>S</given-names></name></person-group><article-title>On the role of space and time in auditory processing</article-title><source>Trends in Cognitive Sciences</source><year>2001</year><volume>5</volume><issue>8</issue><fpage>340</fpage><lpage>348</lpage><pub-id pub-id-type="pmid">11477003</pub-id></element-citation></ref><ref id="R79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shannon</surname><given-names>CE</given-names></name></person-group><article-title>A mathematical theory of communication</article-title><source>The Bell System Technical Journal</source><year>1948</year><volume>27</volume><issue>3</issue><fpage>379</fpage><lpage>423</lpage></element-citation></ref><ref id="R80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shannon</surname><given-names>RV</given-names></name><name><surname>Zeng</surname><given-names>F-G</given-names></name><name><surname>Kamath</surname><given-names>V</given-names></name><name><surname>Wygonski</surname><given-names>J</given-names></name><name><surname>Ekelid</surname><given-names>M</given-names></name></person-group><article-title>Speech Recognition with Primarily Temporal Cues</article-title><source>Science</source><year>1995</year><volume>270</volume><issue>5234</issue><fpage>303</fpage><lpage>304</lpage><pub-id pub-id-type="pmid">7569981</pub-id></element-citation></ref><ref id="R81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sihvonen</surname><given-names>AJ</given-names></name><name><surname>Särkämö</surname><given-names>T</given-names></name><name><surname>Rodríguez-Fornells</surname><given-names>A</given-names></name><name><surname>Ripollés</surname><given-names>P</given-names></name><name><surname>Münte</surname><given-names>TF</given-names></name><name><surname>Soinila</surname><given-names>S</given-names></name></person-group><article-title>Neural architectures of music – Insights from acquired amusia</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><year>2019</year><volume>107</volume><fpage>104</fpage><lpage>114</lpage><pub-id pub-id-type="pmid">31479663</pub-id></element-citation></ref><ref id="R82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simoncelli</surname><given-names>EP</given-names></name><name><surname>Olshausen</surname><given-names>BA</given-names></name></person-group><article-title>Natural Image Statistics and Neural Representation</article-title><source>Annual Review of Neuroscience</source><year>2001</year><volume>24</volume><issue>1</issue><fpage>1193</fpage><lpage>1216</lpage><pub-id pub-id-type="pmid">11520932</pub-id></element-citation></ref><ref id="R83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singh</surname><given-names>NC</given-names></name><name><surname>Theunissen</surname><given-names>FE</given-names></name></person-group><article-title>Modulation spectra of natural sounds and ethological theories of auditory processing</article-title><source>The Journal of the Acoustical Society of America</source><year>2003</year><volume>114</volume><issue>6</issue><fpage>3394</fpage><lpage>3411</lpage><pub-id pub-id-type="pmid">14714819</pub-id></element-citation></ref><ref id="R84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>EC</given-names></name><name><surname>Lewicki</surname><given-names>MS</given-names></name></person-group><article-title>Efficient auditory coding</article-title><source>Nature</source><year>2006</year><volume>439</volume><issue>7079</issue><fpage>978</fpage><lpage>982</lpage><pub-id pub-id-type="pmid">16495999</pub-id></element-citation></ref><ref id="R85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Staeren</surname><given-names>N</given-names></name><name><surname>Renvall</surname><given-names>H</given-names></name><name><surname>De Martino</surname><given-names>F</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>Formisano</surname><given-names>E</given-names></name></person-group><article-title>Sound Categories Are Represented as Distributed Patterns in the Human Auditory Cortex</article-title><source>Current Biology</source><year>2009</year><volume>19</volume><issue>6</issue><fpage>498</fpage><lpage>502</lpage><pub-id pub-id-type="pmid">19268594</pub-id></element-citation></ref><ref id="R86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tadel</surname><given-names>F</given-names></name><name><surname>Baillet</surname><given-names>S</given-names></name><name><surname>Mosher</surname><given-names>JC</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Leahy</surname><given-names>RM</given-names></name></person-group><article-title>Brainstorm: A UserFriendly Application for MEG/EEG Analysis</article-title><source>Computational Intelligence and Neuroscience</source><year>2011</year><volume>2011</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="pmcid">PMC3090754</pub-id><pub-id pub-id-type="pmid">21584256</pub-id><pub-id pub-id-type="doi">10.1155/2011/879716</pub-id></element-citation></ref><ref id="R87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Te Rietmolen</surname><given-names>N</given-names></name><name><surname>Mercier</surname><given-names>M</given-names></name><name><surname>Trébuchon</surname><given-names>A</given-names></name><name><surname>Morillon</surname><given-names>B</given-names></name><name><surname>Schön</surname><given-names>D</given-names></name></person-group><source>Speech and music recruit frequency-specific distributed and overlapping cortical networks</source><year>2024</year><pub-id pub-id-type="pmcid">PMC11262799</pub-id><pub-id pub-id-type="pmid">39038076</pub-id><pub-id pub-id-type="doi">10.7554/eLife.94509</pub-id></element-citation></ref><ref id="R88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teng</surname><given-names>X</given-names></name><name><surname>Tian</surname><given-names>X</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>Testing multi-scale processing in the auditory system</article-title><source>Scientific Reports</source><year>2016</year><volume>6</volume><issue>1</issue><elocation-id>34390</elocation-id><pub-id pub-id-type="pmcid">PMC5054370</pub-id><pub-id pub-id-type="pmid">27713546</pub-id><pub-id pub-id-type="doi">10.1038/srep34390</pub-id></element-citation></ref><ref id="R89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teng</surname><given-names>X</given-names></name><name><surname>Tian</surname><given-names>X</given-names></name><name><surname>Rowland</surname><given-names>J</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>Concurrent temporal channels for auditory processing: Oscillatory neural entrainment reveals segregation of function at different scales</article-title><source>PLOS Biology</source><year>2017</year><volume>15</volume><issue>11</issue><elocation-id>e2000812</elocation-id><pub-id pub-id-type="pmcid">PMC5667736</pub-id><pub-id pub-id-type="pmid">29095816</pub-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2000812</pub-id></element-citation></ref><ref id="R90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Theunissen</surname><given-names>FE</given-names></name><name><surname>Elie</surname><given-names>JE</given-names></name></person-group><article-title>Neural processing of natural sounds</article-title><source>Nature Reviews Neuroscience</source><year>2014</year><volume>15</volume><issue>6</issue><fpage>355</fpage><lpage>366</lpage><pub-id pub-id-type="pmid">24840800</pub-id></element-citation></ref><ref id="R91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Theunissen</surname><given-names>FE</given-names></name><name><surname>Sen</surname><given-names>K</given-names></name><name><surname>Doupe</surname><given-names>AJ</given-names></name></person-group><article-title>Spectral-Temporal Receptive Fields of Nonlinear Auditory Neurons Obtained Using Natural Sounds</article-title><source>The Journal of Neuroscience</source><year>2000</year><volume>20</volume><issue>6</issue><fpage>2315</fpage><lpage>2331</lpage><pub-id pub-id-type="pmcid">PMC6772498</pub-id><pub-id pub-id-type="pmid">10704507</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.20-06-02315.2000</pub-id></element-citation></ref><ref id="R92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Varnet</surname><given-names>L</given-names></name><name><surname>Ortiz-Barajas</surname><given-names>MC</given-names></name><name><surname>Erra</surname><given-names>RG</given-names></name><name><surname>Gervain</surname><given-names>J</given-names></name><name><surname>Lorenzi</surname><given-names>C</given-names></name></person-group><article-title>A cross-linguistic study of speech modulation spectra</article-title><source>The Journal of the Acoustical Society of America</source><year>2017</year><volume>142</volume><issue>4</issue><fpage>1976</fpage><lpage>1989</lpage><pub-id pub-id-type="pmid">29092595</pub-id></element-citation></ref><ref id="R93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Venezia</surname><given-names>JH</given-names></name><name><surname>Richards</surname><given-names>VM</given-names></name><name><surname>Hickok</surname><given-names>G</given-names></name></person-group><article-title>Speech-Driven Spectrotemporal Receptive Fields Beyond the Auditory Cortex</article-title><source>Hearing Research</source><year>2021</year><volume>408</volume><elocation-id>108307</elocation-id><pub-id pub-id-type="pmcid">PMC8378265</pub-id><pub-id pub-id-type="pmid">34311190</pub-id><pub-id pub-id-type="doi">10.1016/j.heares.2021.108307</pub-id></element-citation></ref><ref id="R94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Venezia</surname><given-names>JH</given-names></name><name><surname>Thurman</surname><given-names>SM</given-names></name><name><surname>Richards</surname><given-names>VM</given-names></name><name><surname>Hickok</surname><given-names>G</given-names></name></person-group><article-title>Hierarchy of speech-driven spectrotemporal receptive fields in human auditory cortex</article-title><source>NeuroImage</source><year>2019</year><volume>186</volume><fpage>647</fpage><lpage>666</lpage><pub-id pub-id-type="pmcid">PMC6338500</pub-id><pub-id pub-id-type="pmid">30500424</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.11.049</pub-id></element-citation></ref><ref id="R95"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Vogels</surname><given-names>R</given-names></name><name><surname>Orban</surname><given-names>GA</given-names></name></person-group><chapter-title>Chapter 14 Coding of stimulus invariances by inferior temporal neurons</chapter-title><source>Progress in Brain Research</source><publisher-name>Elsevier</publisher-name><year>1996</year><volume>112</volume><fpage>195</fpage><lpage>211</lpage><pub-id pub-id-type="pmid">8979830</pub-id></element-citation></ref><ref id="R96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walters</surname><given-names>J</given-names></name><name><surname>King</surname><given-names>M</given-names></name><name><surname>Bissett</surname><given-names>PG</given-names></name><name><surname>Ivry</surname><given-names>RB</given-names></name><name><surname>Diedrichsen</surname><given-names>J</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name></person-group><article-title>Predicting brain activation maps for arbitrary tasks with cognitive encoding models</article-title><source>NeuroImage</source><year>2022</year><volume>263</volume><elocation-id>119610</elocation-id><pub-id pub-id-type="pmcid">PMC9981816</pub-id><pub-id pub-id-type="pmid">36064138</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119610</pub-id></element-citation></ref><ref id="R97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woolley</surname><given-names>SMN</given-names></name><name><surname>Fremouw</surname><given-names>TE</given-names></name><name><surname>Hsu</surname><given-names>A</given-names></name><name><surname>Theunissen</surname><given-names>FE</given-names></name></person-group><article-title>Tuning for spectro-temporal modulations as a mechanism for auditory discrimination of natural sounds</article-title><source>Nature Neuroscience</source><year>2005</year><volume>8</volume><issue>10</issue><fpage>1371</fpage><lpage>1379</lpage><pub-id pub-id-type="pmid">16136039</pub-id></element-citation></ref><ref id="R98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zatorre</surname><given-names>RJ</given-names></name></person-group><article-title>Hemispheric asymmetries for music and speech: Spectrotemporal modulations and top-down influences</article-title><source>Frontiers in Neuroscience</source><year>2022</year><volume>16</volume><elocation-id>1075511</elocation-id><pub-id pub-id-type="pmcid">PMC9809288</pub-id><pub-id pub-id-type="pmid">36605556</pub-id><pub-id pub-id-type="doi">10.3389/fnins.2022.1075511</pub-id></element-citation></ref><ref id="R99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zatorre</surname><given-names>RJ</given-names></name><name><surname>Belin</surname><given-names>P</given-names></name></person-group><article-title>Spectral and temporal processing in human auditory cortex</article-title><source>Cerebral Cortex</source><year>2001</year><volume>11</volume><issue>10</issue><fpage>946</fpage><lpage>953</lpage><pub-id pub-id-type="pmid">11549617</pub-id></element-citation></ref><ref id="R100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zuk</surname><given-names>NJ</given-names></name><name><surname>Teoh</surname><given-names>ES</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><article-title>EEG-based classification of natural sounds reveals specialized responses to speech and music</article-title><source>NeuroImage</source><year>2020</year><volume>210</volume><elocation-id>116558</elocation-id><pub-id pub-id-type="pmid">31962174</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>STM features predict perceptual categorization: pure spectral modulations for music, pure temporal modulations for speech</title><p>(a): Online behavioral task. Participants (n = 19) judged whether music and/or speech were present during the time-course of the audio track. The right panel illustrates judgement vectors obtained for speech and music categorization from one subject. Two judgement vectors were obtained for each participant.</p><p>(b): Analysis pipeline. The audio track was decomposed into the STM space using a 1.5-second sliding window, generating a STM matrix for each time sample. For each category (speech/music), a binary logistic regression model was trained on the audio track in the STM space to classify whether the corresponding category was present or absent along the time-course of the audio track. The feature weights of the model were then extracted and transformed into interpretable feature patterns using the Haufe transformation (<xref ref-type="bibr" rid="R38">Haufe et al., 2014</xref>).</p><p>(c): Results. Averaged feature patterns across participants in the STM space for each categorical judgement. Threshold-based contours highlight regions of high feature patterns amplitude: the 90th percentile of the distribution is outlined with a dotted line, and the 99th percentile is outlined with a solid line.</p></caption><graphic xlink:href="EMS206304-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Bilateral non-primary auditory areas track the spectrotemporal modulations of a naturalistic sound</title><p>(a): Schematics of the regression model. Each sEEG contact (top part of the figure, showing all sEEG contacts from the eleven patients in standard MNI space on the left and right sides of a glass half-brain visualization). Colors on the scale correspond to each patient’s implantation scheme. Signal was decomposed into six frequency bands and used as features in a ridge regression model (ridge regression panel: middle part of panel a). The audio track that the patients listened to (bottom part of panel a) was decomposed into the STM space as in <xref ref-type="fig" rid="F1">Figure 1b</xref>. Each spectral/temporal combination time course from the matrix was then used as the target variable (<italic>y</italic>) in the regression model. Model predictions <inline-formula><mml:math id="M1"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> were evaluated (model evaluation panel) against the target (<italic>y</italic>) using Spearman’s correlations.</p><p>(b): Example accuracy maps. An accuracy map was generated for each contact by running the model (depicted in (a)) at each spectral and temporal combination/coordinate in the STM space, yielding a map of correlation coefficient (ρ).</p><p>(c): Thresholding. Each accuracy map was thresholded by performing one-tailed one-sample t-tests against zero on Fisher z-transformed correlation coefficients for each STM point (Bonferroni corrected adjusted for number of STM points, sEEG contacts and cross-validation folds, α = 1x10<sup>-30</sup>). The heatmap represents the thresholded accuracy map from (b).</p><p>(d): STM-sensitive contacts. Main panel: contacts that survived statistical thresholding and spatial clustering (see <xref ref-type="sec" rid="S20">STAR methods</xref>). Four clusters were identified in bilateral non-primary auditory areas (see <xref ref-type="supplementary-material" rid="SD1">supplementary Table S1 for coordinates and Figure S1</xref> for detailed visualization).</p></caption><graphic xlink:href="EMS206304-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Behavioral categorization judgments can be decoded from brain-reconstructed STMs in bilateral non-primary auditory areas.</title><p>(a) STM-sensitive sEEG contacts (same as <xref ref-type="fig" rid="F2">Figure 2d</xref>).</p><p>(b) Schematics of the regression model. The model used brain signals as predictors and the time courses of each spectral/temporal combination of the audio track as targets, as depicted in <xref ref-type="fig" rid="F2">Figure 2a</xref>. The predicted time courses for each spectral/temporal combination were extracted, providing the audio track in the STM space predicted from brain signals.</p><p>(c) Schematics of the logistic regression model. Based on the predicted audio track in the STM space obtained in (b), binary logistic regression models were trained to classify whether music or speech was present or absent. The feature weights of the model were then extracted and transformed into feature patterns using the Haufe transformation (<xref ref-type="bibr" rid="R38">Haufe et al., 2014</xref>). Feature patterns were obtained for each STM-sensitive sEEG contact, behavioral participant and category.</p><p>(d) Feature patterns in the STM space averaged over all sEEG contacts and all behavioral subjects.</p><p>(e) Heatmaps showing, for each category, the percentage of STM-sensitive sEEG contacts that exhibited a significant feature pattern across behavioral subjects at each spectral and temporal combination/coordinate in the STM space. Threshold-based contours highlight regions of high sEEG contact count with significant feature patterns: the 90th percentile of the distribution is outlined with a dotted line, and the 99th percentile is outlined with a solid line.</p></caption><graphic xlink:href="EMS206304-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Similarity between brain-reconstructed and acoustic STM representations reveals hemispheric lateralization</title><p>(a) Schematic of the analysis pipeline. Feature patterns were extracted from the logistic regression models described in <xref ref-type="fig" rid="F3">Figure 3c</xref>, where brain-reconstructed STMs served as predictors and behavioral categorization responses as target variables (top panel). This was done separately for each STM-sensitive sEEG contact, category, and behavioral participant, and the resulting feature patterns were then averaged across sEEG contacts within each hemisphere. In parallel, feature patterns were obtained from the logistic regression models described in <xref ref-type="fig" rid="F1">Figure 1b</xref>, where the actual acoustic STMs were used as predictors of behavioral categorization (bottom panel), again for each category and behavioral participant. To evaluate similarity between brain-reconstructed and acoustic feature representations, Spearman correlations were computed between the two sets of feature patterns. For each category and each hemisphere, this resulted in 19 comparisons (one per behavioral participant) between the hemisphere-averaged brain-reconstructed STM patterns and the corresponding acoustic STM patterns.</p><p>(b) Feature pattern similarity distribution. Density curves represent the distribution of correlation coefficients between feature patterns derived from the two models depicted in (a). The top (aqua) distribution corresponds to the speech category, and the bottom (orange) distribution corresponds to the music category. For comparison, a separate distribution (gray) shows the correlation coefficients obtained when feature patterns from the first model (top panel of (a)) were compared with those from the second model after shuffling the behavioral categorization labels. All Spearman’s correlation coefficients calculated from the two original models were significant at p &lt; .001 after Bonferroni correction adjusted for number of subjects, categories, and hemispheres.</p><p>(c) Similarity lateralization. Correlation coefficients are displayed for each behavioral participant in the speech category (left plot) and the music category (right plot), with data from sEEG contacts implanted in the left and right hemisphere (x-axis). *p &lt; .01, with pairwise Wilcoxon signed-rank tests, FDR-corrected.</p></caption><graphic xlink:href="EMS206304-f004"/></fig></floats-group></article>