<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS206299</article-id><article-id pub-id-type="doi">10.1101/2025.06.03.657632</article-id><article-id pub-id-type="archive">PPR1032341</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>High-dimensional neuronal activity from low-dimensional latent dynamics: a solvable model</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Schmutz</surname><given-names>Valentin</given-names></name><email>v.schmutz@ucl.ac.uk</email></contrib><contrib contrib-type="author"><name><surname>Haydaroglu</surname><given-names>Ali</given-names></name><email>ali.haydaroglu.20@ucl.ac.uk</email></contrib><aff id="A1"><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap><postal-code>WC1E 6BT</postal-code><city>London</city>, <country country="GB">UK</country></aff></contrib-group><contrib-group><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Shuqi</given-names></name><email>shuqi.wang@epfl.ch</email><aff id="A2"><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02s376052</institution-id><institution>École Polytechnique Fédérale de Lausanne</institution></institution-wrap><postal-code>1015</postal-code><city>Lausanne</city>, <country country="CH">Switzerland</country></aff></contrib><contrib contrib-type="author"><name><surname>Feng</surname><given-names>Yixiao</given-names></name><email>China yf2887@nyu.edu</email><aff id="A3"><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0220qvk04</institution-id><institution>Shanghai Jiao Tong University</institution></institution-wrap><postal-code>200240</postal-code><city>Shanghai</city></aff></contrib></contrib-group><contrib-group><contrib contrib-type="author"><name><surname>Carandini</surname><given-names>Matteo</given-names></name><email>m.carandini@ucl.ac.uk</email></contrib><contrib contrib-type="author"><name><surname>Harris</surname><given-names>Kenneth D.</given-names></name><email>kenneth.harris@ucl.ac.uk</email></contrib><aff id="A4"><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap><postal-code>WC1E 6BT</postal-code><city>London</city>, <country country="GB">UK</country></aff></contrib-group><pub-date pub-type="nihms-submitted"><day>08</day><month>06</month><year>2025</year></pub-date><pub-date pub-type="preprint"><day>06</day><month>06</month><year>2025</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Computation in recurrent networks of neurons has been hypothesized to occur at the level of low-dimensional latent dynamics, both in artificial systems and in the brain. This hypothesis seems at odds with evidence from large-scale neuronal recordings in mice showing that neuronal population activity is high-dimensional. To demonstrate that low-dimensional latent dynamics and high-dimensional activity can be two sides of the same coin, we present an analytically solvable recurrent neural network (RNN) model whose dynamics can be exactly reduced to a low-dimensional dynamical system, but generates an activity manifold that has a high linear embedding dimension. This raises the question: Do low-dimensional latents explain the high-dimensional activity observed in mouse visual cortex? Spectral theory tells us that the covariance eigenspectrum alone does not allow us to recover the dimensionality of the latents, which can be low or high, when neurons are nonlinear. To address this indeterminacy, we develop Neural Cross-Encoder (NCE), an interpretable, nonlinear latent variable modeling method for neuronal recordings, and find that high-dimensional neuronal responses to drifting gratings and spontaneous activity in visual cortex can be reduced to low-dimensional latents, while the responses to natural images cannot. We conclude that the high-dimensional activity measured in certain conditions, such as in the absence of a stimulus, is explained by low-dimensional latents that are nonlinearly processed by individual neurons.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><label>1</label><title>Introduction</title><p id="P2">The mammalian cortex comprises a large number of neurons, which, in principle, should allow it to use a high-dimensional neural code to represent sensory, motor, and cognitive information. Nevertheless, multi-neuronal recordings in nonhuman primates [<xref ref-type="bibr" rid="R1">1</xref>–<xref ref-type="bibr" rid="R4">4</xref>] have suggested that cortical populations perform computations by approximating low-dimensional dynamical systems [<xref ref-type="bibr" rid="R5">5</xref>, <xref ref-type="bibr" rid="R6">6</xref>], with neuronal firing rates lying on a low-dimensional “neural manifold” [<xref ref-type="bibr" rid="R7">7</xref>]. In support of this hypothesis, low-dimensional dynamics have been inferred from multi-neuronal recordings through a wide variety of methods [<xref ref-type="bibr" rid="R8">8</xref>–<xref ref-type="bibr" rid="R23">23</xref>]; they spontaneously emerge in recurrent neural networks (RNNs) trained to solve behavioral tasks [<xref ref-type="bibr" rid="R2">2</xref>, <xref ref-type="bibr" rid="R24">24</xref>–<xref ref-type="bibr" rid="R34">34</xref>]; and they appear in several theoretical models of noise-robust neuronal population dynamics [<xref ref-type="bibr" rid="R35">35</xref>–<xref ref-type="bibr" rid="R38">38</xref>]. A result that might at first sight challenge the low-dimensional dynamical systems hypothesis is that visual cortical population activity in mice has high linear dimension [<xref ref-type="bibr" rid="R39">39</xref>, <xref ref-type="bibr" rid="R40">40</xref>] with shared neuronal covariance having a heavy-tailed eigenspectrum (see also [<xref ref-type="bibr" rid="R41">41</xref>] and [<xref ref-type="bibr" rid="R42">42</xref>] for recordings in cerebellum and across cortex, respectively). In particular, the shared covariance eigenspectrum has a power-law tail with an exponent close to 1 (<italic>α</italic> ≈ 1.04) [<xref ref-type="bibr" rid="R39">39</xref>] for responses to natural images and an exponent of <italic>α</italic> ≈ 1.14 for spontaneous activity [<xref ref-type="bibr" rid="R40">40</xref>]. Are these two views on the dimensionality of population activity compatible? Namely, can a low-dimensional dynamical system produce a neural manifold that has a high linear embedding dimension?</p><p id="P3">Here, we first construct a solvable RNN model that reconciles the low- and high-dimensional perspectives on population activity by carefully disambiguating the <italic>linear</italic> dimension of the system <italic>before</italic> and <italic>after</italic> the neurons’ nonlinearity, which we refer to as the pre- and post-activation dimension, respectively. This dichotomy refines the usual distinction between linear and “intrinsic” dimension [<xref ref-type="bibr" rid="R39">39</xref>, <xref ref-type="bibr" rid="R43">43</xref>, <xref ref-type="bibr" rid="R44">44</xref>], since the intrinsic dimension of a system is the same before and after any continuous, injective nonlinearity. Using the notions of pre- and post-activation linear dimensions, we show that our RNN can be exactly reduced to a low-dimensional dynamical system in the space of pre-activations, making the pre-activations low-dimensional. Then, we show that these latent dynamics generate high-dimensional post-activation activity that has a power-law covariance eigenspectrum. (In this work, dimension will always refer to linear dimension, unless stated otherwise.)</p><p id="P4">Before analyzing experimental recordings, we revisit the spectral theory of infinite-width neural networks (random feature kernels) [<xref ref-type="bibr" rid="R45">45</xref>–<xref ref-type="bibr" rid="R47">47</xref>] to quantitatively relate the pre-activation dimension, the neuronal activation function, and the post-activation covariance eigenspectrum. This three-way relationship tells us that high-dimensional activity is consistent with both low- and high-dimensional pre-activations. To uncover the pre-activation dimension of high-dimensional activity in visual cortex, we perform two-photon calcium recordings of tens of thousands of neurons from mouse visual cortex, and infer the pre-activation dimension using the Neural Cross-Encoder (NCE), an interpretable, nonlinear latent variable modeling method which models the activity of each neuron as a simple linear-nonlinear readout of low-dimensional latents. NCE reveals that both the responses to drifting gratings and spontaneous activity can be well approximated by low-dimensional pre-activations, but that responses to natural images cannot. This suggests that the encoding of natural images in visual cortex is already high-dimensional in the space of pre-activations.</p></sec><sec id="S2"><label>2</label><title>Solvable RNN Model</title><p id="P5">To demonstrate how high-dimensional post-activations can arise from low-dimensional pre-activation dynamics, we first present a solvable RNN model whose autonomous dynamics is low-dimensional in the space of pre-activations, but high-dimensional in the space of post-activations, with the post-activations producing a power-law covariance eigenspectrum.</p><p id="P6">We consider an RNN consisting of <italic>N</italic> rate-units (neurons). The pre-activation <italic>x<sub>i</sub></italic> of neuron <italic>i</italic> evolves according to <disp-formula id="FD1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>W<sub>ij</sub></italic> denotes the synaptic weight from neuron <italic>j</italic> to neuron <italic>i</italic>, and <italic>ϕ</italic>: ℝ → ℝ<sub>≥0</sub> is a nonlinear activation function converting the pre-activations into post-activations (firing rates). To define the weights <italic>W<sub>ij</sub></italic>, we randomly place neurons on a ring [<xref ref-type="bibr" rid="R48">48</xref>–<xref ref-type="bibr" rid="R50">50</xref>] by assigning to each neuron <italic>i</italic> an independent and uniformly distributed angle <italic>θ<sub>i</sub></italic> ∈ [0, 2π) (<xref ref-type="fig" rid="F1">Fig. 1A</xref>). The weights <italic>W<sub>ij</sub></italic> are then given by the following shifted cosine function: <disp-formula id="FD2"><label>(2)</label><mml:math id="M2"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>:=</mml:mo><mml:mi>J</mml:mi><mml:mi>cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mo>Δ)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P7">The shift Δ in <xref ref-type="disp-formula" rid="FD2">Eq. (2)</xref> makes the weights asymmetric, with neurons sending their strongest excitatory output to neurons located at an angle Δ counter-clockwise (<xref ref-type="fig" rid="F1">Fig. 1B</xref>). To make the model solvable, we assume that the activation function <italic>ϕ</italic> is the Heaviside step function Θ, i.e., <disp-formula id="FD3"><label>(3)</label><mml:math id="M3"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>Θ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext><mml:mspace width="0.2em"/><mml:mi>x</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext><mml:mspace width="0.2em"/><mml:mi>x</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><sec id="S3"><label>2.1</label><title>Low-dimensional pre-activation dynamics</title><p id="P8">The RNN model defined above has low-dimensional pre-activation dynamics because the weight matrix <bold>W</bold> has rank 2. Indeed, using an elementary trigonometric identity,<sup><xref ref-type="fn" rid="FN1">2</xref></sup> <bold>W</bold> can be factorized as the outer product <bold>W</bold> = <bold>UV</bold><sup>T</sup> of two <italic>N</italic> × 2-matrices (<xref ref-type="fig" rid="F1">Fig. 1C</xref>), with <disp-formula id="FD4"><mml:math id="M4"><mml:mrow><mml:mstyle><mml:mtext mathvariant="bold">U</mml:mtext></mml:mstyle><mml:mo>:=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mtext>cos</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext>sin</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtext>cos</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext>sin</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="1em"/><mml:mtext>and</mml:mtext><mml:mspace width="1em"/><mml:mstyle><mml:mtext mathvariant="bold">V</mml:mtext></mml:mstyle><mml:mo>:=</mml:mo><mml:mi>J</mml:mi><mml:mspace width="0.2em"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mtext>cos</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mo>Δ</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext>sin</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mo>Δ</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtext>cos</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mo>Δ</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext>sin</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mo>Δ</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P9">Then, following Beiran et al. [<xref ref-type="bibr" rid="R51">51</xref>], we can reduce the <italic>N</italic>-dimensional system, <xref ref-type="disp-formula" rid="FD1">Eq. (1)</xref>, to a 2-dimensional system describing the dynamics of the latent variables <italic>κ</italic> := <bold>U</bold><sup>†</sup><bold>x</bold>, where <sup>†</sup> denotes the pseudoinverse and <bold>x</bold> the <italic>N</italic>-dimensional vector of pre-activations (<italic>x</italic><sub>1</sub>,…,<italic>x<sub>N</sub></italic>)<sup>T</sup>. The dynamics of the latent variables follows <disp-formula id="FD5"><mml:math id="M5"><mml:mrow><mml:mover><mml:mstyle><mml:mo mathvariant="bold">κ</mml:mo></mml:mstyle><mml:mo>.</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mstyle><mml:mo mathvariant="bold">κ</mml:mo></mml:mstyle><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:msup><mml:mstyle><mml:mtext mathvariant="bold">v</mml:mtext></mml:mstyle><mml:mtext>T</mml:mtext></mml:msup><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle><mml:mtext mathvariant="bold">U</mml:mtext><mml:mi>κ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>ϕ</italic> is applied element-wise to the <italic>N</italic>-dimensional vector of pre-activations <bold>U<sub>κ</sub></bold> = x. In the equation above, the vector <italic>ϕ</italic>(<bold>U<sub>κ</sub></bold>) = <italic>ϕ</italic>(x) represents the joint post-activations (firing rates) of the <italic>N</italic> neurons.</p><p id="P10">Taking the number of neurons <italic>N</italic> → ∞ yields a neural field limit [<xref ref-type="bibr" rid="R52">52</xref>] where the sum over neurons becomes an integral over the ring, <disp-formula id="FD6"><label>(4)</label><mml:math id="M6"><mml:mrow><mml:mover><mml:mstyle><mml:mo mathvariant="bold">κ</mml:mo></mml:mstyle><mml:mo>.</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mstyle><mml:mo mathvariant="bold">κ</mml:mo></mml:mstyle><mml:mo>+</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:msubsup><mml:mstyle><mml:mtext mathvariant="bold">v</mml:mtext></mml:mstyle></mml:mrow></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle><mml:mtext mathvariant="bold">u</mml:mtext></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>·</mml:mo><mml:mstyle><mml:mo mathvariant="bold">κ</mml:mo></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> with <bold>u</bold>(<italic>θ</italic>) := (cos(<italic>θ</italic>), sin(<italic>θ</italic>))<sup>T</sup> and <bold>v</bold>(<italic>θ</italic>) := J (cos(<italic>θ</italic> + Δ), sin(<italic>θ</italic> + Δ))<sup>T</sup>. <xref ref-type="disp-formula" rid="FD6">Equation (4)</xref> describes the dynamics of the latent variables <bold>κ</bold> as the solution to a 2-dimensional dynamical system whose vector field involves an integral over the “circuit structure” [<xref ref-type="bibr" rid="R52">52</xref>] (the ring).</p><p id="P11">Since <italic>ϕ</italic> is the step function, the integral over the ring in <xref ref-type="disp-formula" rid="FD6">Eq. (4)</xref> can be solved, and we obtain the solvable 2-dimensional dynamical system, <disp-formula id="FD7"><label>(5)</label><mml:math id="M7"><mml:mrow><mml:mover><mml:mstyle><mml:mo mathvariant="bold">κ</mml:mo></mml:mstyle><mml:mo>.</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mstyle><mml:mo mathvariant="bold">κ</mml:mo></mml:mstyle><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mfrac><mml:mstyle><mml:mo mathvariant="bold">κ</mml:mo></mml:mstyle><mml:mrow><mml:mo>‖</mml:mo><mml:mstyle><mml:mo mathvariant="bold">κ</mml:mo></mml:mstyle><mml:mo>‖</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> when <inline-formula><mml:math id="M8"><mml:mrow><mml:mi>J</mml:mi><mml:mspace width="0.2em"/><mml:mo>:=</mml:mo><mml:mi>π</mml:mi><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mrow></mml:math></inline-formula> and Δ := <italic>π</italic>/4 (derivation presented in <xref ref-type="supplementary-material" rid="SD1">Appendix A</xref>). <xref ref-type="disp-formula" rid="FD7">Equation (5)</xref> generates a stable limit cycle over the unit circle (<xref ref-type="fig" rid="F1">Fig. 1D</xref>), which implies that the latent variables <italic>κ</italic> will eventually rotate on the unit circle indefinitely. In <xref ref-type="supplementary-material" rid="SD1">Appendix C</xref>, we provide other examples of low-rank RNNs for which the latent dynamics can be expressed in a tractable form similar to <xref ref-type="disp-formula" rid="FD7">Eq. (5)</xref>.</p><p id="P12">Neuronal activity, modeled here as the post-activations <italic>ϕ</italic>(<bold>U<sub><italic>κ</italic></sub></bold>), are simple <italic>linear-nonlinear</italic> readouts of latent variables <italic>κ</italic> (<xref ref-type="fig" rid="F1">Fig. 1E</xref>). Hence, we have effectively reduced the dynamics of the RNN, <xref ref-type="disp-formula" rid="FD1">Eq. (1)</xref>, to a 2-dimensional latent dynamical system. We will say that neuronal activity is a <italic>linear-nonlinear</italic> function of latent variables if it is given by the composition of a linear mapping (<bold>U</bold>) and an element-wise, nondecreasing nonlinear mapping (<italic>ϕ</italic>).</p></sec><sec id="S4"><label>2.2</label><title>Post-activations produce a power-law eigenspectrum</title><p id="P13">Since the dynamics of the latent variables is solvable in the large-network limit, and rotates on the unit circle, we can compute the correlation between the post-activations of two neurons. For any pair of neurons <italic>i</italic> and <italic>j</italic>, with positions <italic>θ<sub>i</sub></italic> and <italic>θ<sub>j</sub></italic> on the ring, respectively, the correlation of their post-activations <italic>C<sub>ij</sub></italic> is, in the long-recording limit, given by <disp-formula id="FD8"><label>(6)</label><mml:math id="M9"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>π</mml:mi><mml:mo>−</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where |<italic>θ<sub>i</sub>-θ<sub>j</sub></italic>| := cos<sup>−1</sup>(cos(<italic>θ<sub>i</sub>−θ<sub>j</sub></italic>)) is the absolute angle difference <italic>θ<sub>i</sub></italic> and <italic>θ<sub>j</sub></italic> (derivation presented in <xref ref-type="supplementary-material" rid="SD1">Appendix B</xref>).</p><p id="P14">We can find the eigenvalue spectrum of post-activations by noting that as the number of recorded neurons <italic>M</italic> → ∞, the eigenvalues of the <italic>M</italic> × <italic>M</italic> correlation matrix <bold>C</bold><sup>(<italic>M</italic>)</sup> defined by <xref ref-type="disp-formula" rid="FD8">Eq. (6)</xref> converge to the eigenvalues of an integral operator. Since the angles <italic>θ<sub>i</sub></italic> are independently and uniformly sampled on circle [0, 2π), the correlation matrix <bold>C</bold><sup>(<italic>M</italic>)</sup> is a so-called Euclidean random matrix [<xref ref-type="bibr" rid="R53">53</xref>], that is, a matrix whose entries are given by the pairwise distances between randomly sampled points in a given space. Writing <inline-formula><mml:math id="M10"><mml:mrow><mml:msubsup><mml:mo>λ</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>≥</mml:mo><mml:msubsup><mml:mo>λ</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>≥</mml:mo><mml:mo>⋯</mml:mo><mml:mo>≥</mml:mo><mml:msubsup><mml:mo>λ</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> for the ranked eigenvalues of the matrix <bold>C</bold><sup>(<italic>M</italic>)</sup>, random matrix theory [<xref ref-type="bibr" rid="R54">54</xref>, <xref ref-type="bibr" rid="R55">55</xref>] tells us that, as <italic>M</italic> → ∞, the scaled eigenvalues <inline-formula><mml:math id="M11"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mi>λ</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>|</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, converge (in a <italic>ℓ</italic><sub>2</sub> sense) to the eigenvalues of the integral operator, <disp-formula id="FD9"><label>(7)</label><mml:math id="M12"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>↦</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mfrac><mml:munder><mml:munder><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>π</mml:mi><mml:mo>−</mml:mo><mml:mo>|</mml:mo><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>|</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">︸</mml:mo></mml:munder><mml:mrow><mml:mn>0</mml:mn><mml:mo>-</mml:mo><mml:mtext>th</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>arc</mml:mtext><mml:mo>−</mml:mo><mml:mtext>cosine</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>kernel</mml:mtext></mml:mrow></mml:munder><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mstyle><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P15">The eigenvalues of this integral operator can be computed analytically. In <xref ref-type="disp-formula" rid="FD9">Eq. (7)</xref>, we have highlighted the presence of the 0-th arc-cosine kernel <italic>k</italic><sub>0</sub>(<italic>θ, θ</italic>′) := π – |<italic>θ - θ</italic>′| of Cho and Saul [<xref ref-type="bibr" rid="R45">45</xref>], which is well-known in machine learning, and whose eigenvalues have been computed in [<xref ref-type="bibr" rid="R46">46</xref>]. In short, by the rotational invariance of <italic>k</italic><sub>0</sub>, we have that, for any positive integer <italic>m</italic>, the functions <italic>θ</italic> → cos(<italic>mθ</italic>) and <italic>θ</italic> → sin(<italic>mθ</italic>) are orthogonal eigenfunctions of the operator <xref ref-type="disp-formula" rid="FD9">Eq. (7)</xref> sharing the same eigenvalue, <disp-formula id="FD10"><label>(4)</label><mml:math id="M13"><mml:mrow><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:msup><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>π</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>π</mml:mi><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.2em"/><mml:mtext>cos</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mtext>d</mml:mtext><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:msup><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>m</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P16">Using this result, we obtain that the ranked eigenvalues λ<sub>1</sub> ≥ λ<sub>2</sub> ≥ … of the operator <xref ref-type="disp-formula" rid="FD9">Eq. (7)</xref> are given by <disp-formula id="FD11"><label>(8)</label><mml:math id="M14"><mml:mrow><mml:msub><mml:mo>λ</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>m</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mo>λ</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>4</mml:mn><mml:mrow><mml:msup><mml:mi>π</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mspace width="0.7em"/><mml:mo>∀</mml:mo><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℕ</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> that is, eigenvalues come in identical pairs that decay exactly as a power law with decay exponent <italic>α</italic> = 2 (<xref ref-type="fig" rid="F1">Fig. 1F</xref>). Hence, post-activations are high-dimensional in the sense that their covariance eigenspectrum has a heavy tail [<xref ref-type="bibr" rid="R39">39</xref>].</p><p id="P17">In summary, this solvable model shows that low-dimensional dynamics in the space of pre-activations can generate high-dimensional post-activations. The heavy tail of the covariance eigenspectrum implies that post-activations are not confined to any finite-dimensional linear subspace. Formally, the smallest vector space containing the post-activations generated by our model has the same size as the infinite-dimensional reproducing kernel Hilbert space associated with the kernel <italic>k</italic><sub>0</sub>. We stress that, in this model, the heavy tail of the post-activation eigenspectrum is not due to noise, since we used a deterministic, non-chaotic RNN. Also, all the results presented above remain exact if the rate-units in <xref ref-type="disp-formula" rid="FD1">Eq. (1)</xref> are replaced by linear-nonlinear-Poisson neurons, as spike noise cancels out in the limits we consider [<xref ref-type="bibr" rid="R38">38</xref>, <xref ref-type="bibr" rid="R52">52</xref>].</p></sec></sec><sec id="S5"><label>3</label><title>Post-activation eigenspectrum depends on pre-activation dimension and activation function</title><p id="P18">To shed light on the relationship between the post-activation eigenspectrum, pre-activation dimension, and the activation function <italic>ϕ</italic>, we now turn to a more general setup, which allows us to relax some of the strong assumptions of the solvable model (<xref ref-type="fig" rid="F1">Fig. 1E</xref>). First, we allow the number of latent variables <italic>d</italic> to be greater than 2, assuming that the latent variables, denoted by <bold>z</bold>, are uniformly distributed on the unit sphere <inline-formula><mml:math id="M15"><mml:mrow><mml:msup><mml:mi mathvariant="double-struck">S</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> in ℝ<sup><italic>d</italic></sup>. We assume that the pre-activations of the network are determined by passing the latent activity through a <italic>N</italic> × <italic>d</italic> feedforward weight matrix <bold>U</bold> with i.i.d. standard normal entries. In this setup (<xref ref-type="fig" rid="F2">Fig. 2A</xref>), we call <italic>d</italic> the <italic>pre-activation dimension</italic>, as it sets the linear dimensionality of the pre-activations. In the solvable model of <xref ref-type="sec" rid="S4">Sec. 2.2</xref>, for example, the pre-activation dimension was <italic>d</italic> = 2 (<xref ref-type="fig" rid="F1">Fig. 1E</xref>). Finally, we replace the step function, <xref ref-type="disp-formula" rid="FD3">Eq. (3)</xref>, by the general rectified power activation function <disp-formula id="FD12"><label>(9)</label><mml:math id="M16"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>:=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>max</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>p</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where the activation parameter <italic>p</italic> ∈ ℝ<sub>≥0</sub> is a nonnegative real value and the bias <italic>c</italic> ∈ ℝ. (By convention, <italic>ϕ</italic><sub>0,c</sub> (<italic>x</italic>) := Θ(<italic>x + c</italic>).)</p><p id="P19">This setup can be analyzed within the framework of random feature kernels (see [56, Sec. 9.5]). Denoting <italic>μ<sub>d</sub></italic><sub>−1</sub> the uniform probability measure on the sphere <inline-formula><mml:math id="M17"><mml:mrow><mml:msup><mml:mi mathvariant="double-struck">S</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, let us take <italic>T</italic> independent latent variable samples <bold>z</bold><sub>1</sub>,…, <bold>z</bold><sub><italic>T</italic></sub> from <italic>μ<sub>d-1</sub>,</italic> and define the <italic>N</italic> × <italic>T</italic> post-activation matrix <inline-formula><mml:math id="M18"><mml:mrow><mml:msup><mml:mstyle><mml:mtext mathvariant="bold">A</mml:mtext></mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>:=</mml:mo><mml:mspace width="0.2em"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle><mml:mtext mathvariant="bold">U</mml:mtext></mml:mstyle><mml:msub><mml:mstyle><mml:mtext mathvariant="bold">z</mml:mtext></mml:mstyle><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle><mml:mtext mathvariant="bold">U</mml:mtext></mml:mstyle><mml:msub><mml:mstyle><mml:mtext mathvariant="bold">z</mml:mtext></mml:mstyle><mml:mi>T</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In the limits <italic>N</italic> → ∞ and <italic>T</italic> → ∞ taken successively, the covariance eigenspectrum of <bold>A</bold><sup>(<italic>N,T</italic>)</sup> converges (when properly scaled) to the eigenvalue spectrum of the integral operator <disp-formula id="FD13"><label>(10)</label><mml:math id="M19"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle><mml:mtext mathvariant="bold">z</mml:mtext></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>↦</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:msub><mml:mo>∫</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="double-struck">S</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle><mml:mtext mathvariant="bold">z</mml:mtext></mml:mstyle><mml:mo>,</mml:mo><mml:msup><mml:mstyle><mml:mtext mathvariant="bold">z</mml:mtext></mml:mstyle><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mstyle><mml:mtext mathvariant="bold">z</mml:mtext></mml:mstyle><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext>d</mml:mtext><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mstyle><mml:mtext mathvariant="bold">z</mml:mtext></mml:mstyle><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <inline-formula><mml:math id="M20"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:msup><mml:mi mathvariant="double-struck">S</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mi mathvariant="double-struck">S</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>→</mml:mo><mml:mi>ℝ</mml:mi></mml:mrow></mml:math></inline-formula> is the <italic>random feature kernel</italic> <disp-formula id="FD14"><label>(11)</label><mml:math id="M21"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle><mml:mtext mathvariant="bold">z</mml:mtext></mml:mstyle><mml:mo>,</mml:mo><mml:msup><mml:mstyle><mml:mtext mathvariant="bold">z</mml:mtext></mml:mstyle><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>:=</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mstyle><mml:mo mathvariant="bold">ξ</mml:mo></mml:mstyle><mml:mo>~</mml:mo><mml:mi>𝒩</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle><mml:mn mathvariant="bold">0</mml:mn></mml:mstyle><mml:mo>,</mml:mo><mml:msub><mml:mstyle><mml:mtext mathvariant="bold">I</mml:mtext></mml:mstyle><mml:mi>d</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mstyle><mml:mo mathvariant="bold">ξ</mml:mo></mml:mstyle><mml:mo>·</mml:mo><mml:mstyle><mml:mtext mathvariant="bold">z</mml:mtext></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle><mml:mo mathvariant="bold">ξ</mml:mo></mml:mstyle><mml:mo>·</mml:mo><mml:msup><mml:mstyle><mml:mtext mathvariant="bold">z</mml:mtext></mml:mstyle><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula> (see [<xref ref-type="bibr" rid="R46">46</xref>, <xref ref-type="bibr" rid="R56">56</xref>] or <xref ref-type="supplementary-material" rid="SD1">Appendix D</xref> for more details).</p><p id="P20">Known theoretical results [<xref ref-type="bibr" rid="R46">46</xref>] and simulations suggest the following conjecture:</p><p id="P21"><bold>Conjecture 1</bold>. <italic>For any p</italic> ∈ ℝ<sub>≥0</sub>, <italic>c</italic> ∈ ℝ, <italic>and any integer d</italic> ≥ 2, <italic>the ranked eigenvalues</italic> λ<sub>1</sub> ≥ λ<sub>2</sub> ≥ … <italic>of the integral operator</italic> (<xref ref-type="disp-formula" rid="FD13">10</xref>) <italic>obey the following power-law decay</italic>: <disp-formula id="FD15"><label>(12)</label><mml:math id="M22"><mml:mrow><mml:msub><mml:mo>λ</mml:mo><mml:mi>n</mml:mi></mml:msub><mml:mo>≍</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:msup><mml:mspace width="0.8em"/><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mspace width="0.7em"/><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> <italic>where a<sub>n</sub> ≍ b<sub>n</sub> means</italic> lim<sub><italic>n→+∞</italic></sub> <italic>a<sub>n</sub>/b<sub>n</sub></italic> = <italic>C</italic> ∈ (0, +∞).</p><p id="P22">This conjecture goes beyond, but is consistent with, known theoretical results for the special cases <italic>p</italic> = 0 (step function) and <italic>p</italic> = 1 (ReLU) with <italic>c</italic> = 0 [<xref ref-type="bibr" rid="R46">46</xref>]. To the best of our knowledge, <xref ref-type="other" rid="P30">Conjecture 1</xref> is not a straightforward consequence of any existing result in theoretical machine learning [<xref ref-type="bibr" rid="R47">47</xref>, <xref ref-type="bibr" rid="R57">57</xref>, <xref ref-type="bibr" rid="R58">58</xref>] or harmonic analysis [<xref ref-type="bibr" rid="R59">59</xref>–<xref ref-type="bibr" rid="R62">62</xref>], hence our presentation of <xref ref-type="disp-formula" rid="FD15">Eq. (12)</xref> as a conjecture. Note that when the activation parameter <italic>p</italic> is an integer, <italic>ϕ<sub>p,c</sub></italic> is <italic>p</italic>-times weakly differentiable, that is, the first <italic>p</italic> weak derivatives<sup><xref ref-type="fn" rid="FN2">3</xref></sup> of <italic>ϕ<sub>p,c</sub></italic> are all locally integrable. This, and the fact that the bias <italic>c</italic> does not affect the decay rate, suggest a further extension of the conjecture to more general activation functions, with <italic>p</italic> replaced by the weak differentiability of the activation function.</p><p id="P23">We tested <xref ref-type="other" rid="R30">Conjecture 1</xref> numerically by performing PCA on large post-activation matrices <bold>A</bold><sup>(<italic>N,T</italic>)</sup>. The linear and continuous dependence of the decay exponent <italic>α</italic> on the activation parameter <italic>p</italic> predicted by <xref ref-type="disp-formula" rid="FD15">Eq. (12)</xref> was confirmed in simulations (<xref ref-type="fig" rid="F2">Fig. 2B</xref>). Simulations also confirmed that the bias <italic>c</italic> of the activation function does not affect the decay rate (<xref ref-type="fig" rid="F2">Fig. 2C</xref>), a fact already mentioned in [<xref ref-type="bibr" rid="R63">63</xref>, <xref ref-type="bibr" rid="R64">64</xref>].</p><p id="P24">To summarize, the spectral theory of random feature kernels suggests a three-way relationship between the power-law tail exponent of the post-activation eigenspectrum, the pre-activation dimension, and the activation function. This relationship should hold when we can consider the neurons as linear-nonlinear functions of the latent vector, with weights that vary randomly and independently between neurons. The relationship suggests that, when high-dimensional neuronal activity (modeled here as post-activations) is observed [<xref ref-type="bibr" rid="R39">39</xref>, <xref ref-type="bibr" rid="R40">40</xref>], two scenarios are possible: high-dimensional activity could arise from nonlinear transformation of low-dimensional latent states, or it could reflect pre-activations that are already high-dimensional. To distinguish these two scenarios, we propose, in what follows, a method for inferring the pre-activation dimension of neuronal activity in experimental recordings.</p></sec><sec id="S6"><label>4</label><title>Latent Variable Modeling of Neuronal Recordings</title><sec id="S7"><label>4.1</label><title>Experimental Data</title><p id="P25">We conducted large-scale volumetric two-photon microscopy on awake, adult mice during visual stimulation and spontaneous activity. We targeted primary and higher visual cortices with a Light Beads Microscope [<xref ref-type="bibr" rid="R65">65</xref>], and extracted deconvolved activity traces for 19,223 ± 2,948 neurons using Suite3D [<xref ref-type="bibr" rid="R66">66</xref>] as described in <xref ref-type="supplementary-material" rid="SD1">Appendix F</xref>. Recordings were performed in three stimulus conditions: (1) responses to 320 full-field drifting grating stimuli with 2-14 repeats each; (2) responses to 1866 natural images with 2 repeats each; (3) spontaneous activity in the absence of stimuli for 10-15 minutes.</p></sec><sec id="S8"><label>4.2</label><title>Neural Cross-Encoder (NCE)</title><p id="P26">The Neural Cross-Encoder (NCE) divides neurons randomly into two sets: a source set and a target set. It predicts the activity <bold>b</bold><sub><italic>t</italic></sub> of the target set from the source set <bold>a</bold><sub><italic>t</italic></sub> via a non-linear readout of a set of latents, <bold>z</bold><sub><italic>t</italic></sub> (<xref ref-type="fig" rid="F3">Fig. 3A</xref>). NCE uses a multi-layer feedforward encoder ℰ that ends in a bottleneck layer whose activity <bold>z</bold><italic>t</italic> = ℰ (<bold>a</bold><italic><sub>t</sub></italic>) represents a low-dimensional latent state estimated from the source neurons. The reason to use this rather than an autoencoder, which predicts one set of neurons from themselves, is to discard variability that is not shared across neurons. The NCE we used here has a single power-ReLU output layer, matching the setup of section 3, and a 3-layer encoder allowing flexible estimation of latent variables, so that the number of latent variables can be readily interpreted as the pre-activation dimension. We train NCE with stochastic gradient descent on source-target activity pairs as described in <xref ref-type="supplementary-material" rid="SD1">Appendix G</xref>. When all nonlinearities are removed, NCE becomes equivalent to Reduced Rank Regression [<xref ref-type="bibr" rid="R67">67</xref>]. When predicting stimulus-driven activity, we pair the activity of source and target neurons on different repeats of the same stimulus, to also discard shared variability that is not related to the stimulus [<xref ref-type="bibr" rid="R39">39</xref>].</p><sec id="S9"><title>Linear-nonlinear readout</title><p id="P27">The predicted activity of a set of target neurons at time <italic>t</italic>, <bold>b</bold><sub>t</sub> ∈ ℝ<sup><italic>B</italic></sup>, is a weighted sum of the latent variables, <bold>z</bold><sub><italic>t</italic></sub> ∈ ℝ<sup><italic>d</italic></sup>, passed through a nondecreasing nonlinearity: <disp-formula id="FD16"><label>(13)</label><mml:math id="M23"><mml:mrow><mml:msub><mml:mstyle><mml:mtext mathvariant="bold">b</mml:mtext></mml:mstyle><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mstyle><mml:mtext mathvariant="bold">c</mml:mtext></mml:mstyle></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle><mml:mtext mathvariant="bold">U</mml:mtext></mml:mstyle><mml:msub><mml:mstyle><mml:mtext mathvariant="bold">z</mml:mtext></mml:mstyle><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mstyle><mml:mtext mathvariant="bold">r</mml:mtext></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P28">Here, <italic>ϕ<sub>p,c</sub></italic> is the rectified power activation function, defined in <xref ref-type="disp-formula" rid="FD12">Eq. 9</xref>, with a power parameter <italic>p</italic> that is constant across neurons, a pre-activation bias that varies across neurons encoded by an <italic>N</italic>-dimensional vector <bold>c</bold>, and a post-activation added bias <bold>r</bold> to account for non-zero baseline firing rates. The decoder parameters {<italic>p</italic>,<bold>c</bold>,<bold>U</bold>,<bold>r</bold>} are learned alongside the encoder parameters of ℰ. The fact that the decoder, <xref ref-type="disp-formula" rid="FD16">Eq. (13)</xref>, has a single-layer is crucial as it allows us to interpret the latents (<bold>z</bold><italic><sub>t</sub></italic>) as linear factors of the observed neurons’ pre-activations (<bold>U</bold><sub><bold>z</bold><italic>t</italic></sub>). It is this constrained decoder that allows us to infer the pre-activation dimension of neuronal activity; in comparison, a multi-layer decoder as in used in [<xref ref-type="bibr" rid="R13">13</xref>, <xref ref-type="bibr" rid="R23">23</xref>] would infer something closer to the intrinsic dimension of neuronal activity, which is not our goal.</p></sec></sec></sec><sec id="S10" sec-type="results"><label>4.3</label><title>Results</title><sec id="S11"><title>NCE identifies the latent dimensionality of simulated data</title><p id="P29">To validate that NCE can identify the pre-activation latent variables, we test it on simulated data generated from the toy model in <xref ref-type="sec" rid="S2">Sec. 2</xref> with a ReLU readout (<italic>p</italic> = 1) and <italic>d</italic> = 2. NCE recovers the true latents up to a scaling and a shift (<xref ref-type="fig" rid="F3">Fig 3B</xref>). Moreover, NCE can explain all of the variance in the population with only two pre-activation dimensions, while the corresponding linear model (Reduced Rank Regression) requires more dimensions (<xref ref-type="fig" rid="F3">Fig 3C</xref>).</p></sec><sec id="S12"><title>Pre-activation dimension is low for grating responses, high for natural image responses</title><p id="P30">We first consider the pre-activation dimensional of visual stimulus responses. To ensure that the NCE focused on the stimulus responses, and not correlated ongoing activity such as spontaneous activity or encoding of movements, the the activity of the target cells and the source cells were taken from different repeats of the same stimuli. In the case of drifting gratings, for which we know there is a low-dimensional latent variable (the grating orientation), an NCE model with low-dimensional pre-activations accurately predicts neuronal responses (<xref ref-type="fig" rid="F4">Fig. 4A,B</xref>). NCE requires fewer dimensions (5.5 ± 1.2, mean ± std) than the corresponding linear model (13.9 ± 3.0) to predict 95% of the explainable variance (defined as the maximum variance explained across all <italic>d</italic> for a given model type). On the other hand, NCE models with low pre-activation dimension are not sufficient to predict responses to natural images (<xref ref-type="fig" rid="F4">Fig. 4C,D</xref>), requiring 93.9 ± 6.0 dimensions to reach the threshold, suggesting that natural images produce high-dimensional representations in the space of pre-activations. Linear models only account for a smaller fraction of the total variance (<xref ref-type="fig" rid="F4">Fig. 4D</xref>), and therefore underestimate the dimensionality of natural image responses (48.0 ± 13.1).</p></sec><sec id="S13"><title>Spontaneous activity has low pre-activation dimension</title><p id="P31">Spontaneous activity is well predicted by NCE with low pre-activation dimension (<xref ref-type="fig" rid="F4">Fig. 4E,F</xref>). Across all recordings, spontaneous activity of 1000 target neurons has an estimated pre-activation dimension <italic>d</italic> of 7.0 ± 1.0 (mean ± std.), somewhat larger than grating responses but substantially lower than natural images responses (<xref ref-type="fig" rid="F4">Fig. 4G,H</xref>). The linear model finds a similar dimension (7.5 ± 2.0), though its performance deteriorates at high pre-activation dimensions due to overfitting, while the NCE performance remains consistent (<xref ref-type="fig" rid="F4">Fig. 4F</xref>).</p><p id="P32">These results indicate that visual cortex activity can be modeled as a linear-nonlinear transformation of a latent vector, which is low-dimensional for grating responses and spontaneous activity, but high-dimensional for natural image responses. In the case of grating responses, we find latents that resemble the sine and cosine of the stimulus angle (<xref ref-type="fig" rid="F4">Fig. 4A</xref>)–this is what one would expect to find if neurons follow the canonical model of simple cells in visual cortex [<xref ref-type="bibr" rid="R68">68</xref>]. On the other hand, during spontaneous activity, the dynamics of the latent variables are correlated with the running speed of the mouse, and are perhaps related to its arousal state (see <xref ref-type="supplementary-material" rid="SD1">Appendix G.3</xref>).</p></sec></sec><sec id="S14"><label>5</label><title>Summary of Technical Contributions and Previous Works</title><sec id="S15"><title>Latent dynamics of low-rank RNNs</title><p id="P33">Low-rank RNNs are tractable models of how the brain can perform computations through low-dimensional population dynamics [<xref ref-type="bibr" rid="R36">36</xref>, <xref ref-type="bibr" rid="R51">51</xref>, <xref ref-type="bibr" rid="R69">69</xref>–<xref ref-type="bibr" rid="R72">72</xref>]. In particular, the dynamics of certain low-rank RNNs, in the large-network limit, reduce to that of “effective circuits”, i.e., dynamical systems describing the evolution of the latent variables [<xref ref-type="bibr" rid="R51">51</xref>, <xref ref-type="bibr" rid="R52">52</xref>, <xref ref-type="bibr" rid="R73">73</xref>]. A limitation of these effective dynamical systems is that the expression of the vector field involves an integral over the distribution of weights (the “circuit structure” [<xref ref-type="bibr" rid="R52">52</xref>]), making them somewhat opaque and costly to solve numerically. In this work (<xref ref-type="sec" rid="S3">Sec. 2.1</xref> and <xref ref-type="supplementary-material" rid="SD1">Appendix C</xref>), we prove that, in several special cases, the integral over the weight distribution can be solved, yielding simple exact equations for the latent dynamics.</p></sec><sec id="S16"><title>Eigenvalue decay of random feature kernels</title><p id="P34">In the infinite-width limit, two-layer neural networks with random input weights behave like <italic>random feature kernels</italic> that depend on the distribution of input weights and the activation function of the neurons in the hidden layer [<xref ref-type="bibr" rid="R56">56</xref>, Sec. 9.5]. This functional perspective can be generalized to deep networks [<xref ref-type="bibr" rid="R45">45</xref>, <xref ref-type="bibr" rid="R74">74</xref>] and constitute the basis of the Neural Tangent Kernel formalism for studying learning dynamics [<xref ref-type="bibr" rid="R75">75</xref>]. When the activation function is the ReLU, the decay rate of the eigenvalues has been proven to be polynomial [<xref ref-type="bibr" rid="R46">46</xref>, <xref ref-type="bibr" rid="R47">47</xref>, <xref ref-type="bibr" rid="R58">58</xref>, <xref ref-type="bibr" rid="R64">64</xref>, <xref ref-type="bibr" rid="R76">76</xref>, <xref ref-type="bibr" rid="R77">77</xref>] (see [<xref ref-type="bibr" rid="R57">57</xref>, <xref ref-type="bibr" rid="R61">61</xref>, <xref ref-type="bibr" rid="R63">63</xref>, <xref ref-type="bibr" rid="R78">78</xref>, <xref ref-type="bibr" rid="R79">79</xref>] for general results on dot-product kernels). In this work, we propose a simple formula that links the power-law exponent of the eigenvalue decay rate, the power of the rectified-power activation function, and the input dimensionality. This formula, which goes beyond known results [<xref ref-type="bibr" rid="R46">46</xref>, <xref ref-type="bibr" rid="R47">47</xref>, <xref ref-type="bibr" rid="R64">64</xref>], is presented as a conjecture that we test in simulations.</p></sec><sec id="S17"><title>Latent variable modeling of neuronal activity</title><p id="P35">While most latent variable models of neuronal activity were originally developed for electrophysiological recordings [<xref ref-type="bibr" rid="R8">8</xref>–<xref ref-type="bibr" rid="R23">23</xref>], some are tailored for calcium recordings [<xref ref-type="bibr" rid="R80">80</xref>–<xref ref-type="bibr" rid="R82">82</xref>]. These models vary in their mechanistic interpretability: The inferred latents are either abstract variables, for example when the model’s mapping from latents to neuronal activity involves a multi-layer neural network [<xref ref-type="bibr" rid="R13">13</xref>, <xref ref-type="bibr" rid="R23">23</xref>], or they can be interpreted as linear factors of the neurons’ pre-activations, as in [<xref ref-type="bibr" rid="R9">9</xref>, <xref ref-type="bibr" rid="R10">10</xref>]. With nonlinear dimensionality reduction methods such as CEBRA [<xref ref-type="bibr" rid="R83">83</xref>] or Rastermap [<xref ref-type="bibr" rid="R84">84</xref>], latent variables are also abstract as there is no explicit mapping going from the latents to neuronal activity. In this work (<xref ref-type="sec" rid="S6">Sec. 4</xref>), we developed NCE, a latent variable model for calcium recordings that models neuronal activity as an interpretable linear-nonlinear readout of latent variables. NCE also uses a cross-encoding scheme, which allows it to discard variability not shared across neurons. We demonstrate that NCE is capable of identifying a low-dimensional pre-activation space even when the recorded neuronal activity has high linear dimension.</p></sec></sec><sec id="S18" sec-type="discussion"><label>6</label><title>Discussion</title><sec id="S19"><title>Dimensionality of neural systems</title><p id="P36">The solvable RNN model we proposed produces cyclic population dynamics that is low-dimensional in the space of pre-activations, and high-dimensional in the space of post-activations (firing rates). Thus, an RNN can produce trajectories that are simultaneously low-dimensional and high-dimensional, depending on the variables being considered. In this work, we focused on the notion of <italic>linear</italic> dimensionality and adopted an infinite-dimensional Hilbert space formalism borrowed from kernel methods to characterize the linear dimensionality of firing rate trajectories in the large-network limit. Of course, the “intrinsic” dimension of neuronal activity is equal to 1, since the dynamics is periodic; this highlights the important distinction between intrinsic and linear (or “embedding”) dimension of neuronal activity (see also [<xref ref-type="bibr" rid="R43">43</xref>, <xref ref-type="bibr" rid="R44">44</xref>]). The type of high-dimensional activity our model produces is computationally relevant: It can be exploited by a downstream readout neuron to represent arbitrary periodic functions (see also [<xref ref-type="bibr" rid="R85">85</xref>]), or, following the random readout approach of [<xref ref-type="bibr" rid="R86">86</xref>, <xref ref-type="bibr" rid="R87">87</xref>], it can be used to represent a Gaussian process prior over periodic functions (see <xref ref-type="supplementary-material" rid="SD1">Appendix E</xref>).</p><p id="P37">Two definitions of high-dimensional neuronal activity have been studied in neuroscience. Throughout this work, we define high-dimensional neuronal activity to mean a covariance eigenspectrum with a heavy tail that decays strictly faster that 1/<italic>n</italic> [<xref ref-type="bibr" rid="R39">39</xref>]. This definition is well-suited for systems generating activity whose pairwise correlations do not converge to zero in the limit of large network size. In contrast, random chaotic RNNs––solvable models whose pairwise correlations do converge to zero––produce a different form of high-dimensional activity, where the eigenspectrum decays slower than 1/<italic>n</italic> [<xref ref-type="bibr" rid="R88">88</xref>], reflecting noise that is not shared between neurons. While a comprehensive comparison of these two types of high-dimensional activity is beyond the scope of this work, we mention that the latter type is relevant when one wants to study the noisiness of neuronal responses [<xref ref-type="bibr" rid="R89">89</xref>].</p></sec><sec id="S20"><title>Pre-activations and subthreshold membrane potentials</title><p id="P38">We developed NCE to disentangle the linear dimension of neuronal activity and of the neurons’ pre-activations. We show that even when activity is high-dimensional, it can be well-explained with low-dimensional pre-activations in the case of grating responses and spontaneous activity. From a biological point of view, how should we interpret the pre-activations inferred by NCE? If one assumes that the link between synaptic integration and neuronal firing is well approximated by a simple nonlinear activation function in cortical neurons, as NCE does, one could argue that pre-activations represent estimates of the neurons’ synaptic inputs or subthreshold membrane potentials. This is an experimental prediction that large-scale voltage imaging of neuronal populations [<xref ref-type="bibr" rid="R90">90</xref>] may make testable in the near future.</p></sec><sec id="S21"><title>Limitations</title><p id="P39">Training NCE, which is a non-convex optimization problem, can be challenging when neural data is limited. Due to experimental constraints, the duration of imaging experiments is limited to 3.5 h, which yields only a few thousand training examples per session for tens of thousands of neurons; thus, optimization can get stuck in local minima. To facilitate training, we limit to fitting only 1000 highly responsive neurons at a time, and use tools such as data augmentation and pretraining as described in <xref ref-type="supplementary-material" rid="SD1">Appendix G</xref>. The fact that we were not able to accurately predict neuronal responses to natural images with a low-dimensional NCE model does not necessarily exclude the possibility for such a model to exist, and one could possibly find it with a larger training set. Note that this work did not revisit the problem of how to estimate the tail of the shared covariance eigenspectrum from neuronal recordings, which is discussed in [<xref ref-type="bibr" rid="R39">39</xref>, <xref ref-type="bibr" rid="R40">40</xref>, <xref ref-type="bibr" rid="R91">91</xref>, <xref ref-type="bibr" rid="R92">92</xref>].</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Appendix</label><media xlink:href="EMS206299-supplement-Appendix.pdf" mimetype="application" mime-subtype="pdf" id="d59aAcHbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S22"><title>Acknowledgments and Disclosure of Funding</title><p>The authors thank Louis Pezon for useful discussions; Michael Krumin, Bex Terry and Charu B. Reddy for experimental support; Kimberly Ren for feedback on the manuscript. This work was funded by UKRI (Frontier Award EP/X022366/1 to MC), BBSRC (grant BB/W019884/1 to MC), the National Institutes of Health BRAIN initiative (grant U01NS126057 to MC), the Wellcome Trust (Investigator Award 223144/Z/21/Z to MC and KDH), and the ERC (101097874 to KDH). MC holds the GlaxoSmithKline / Fight for Sight Chair in Visual Neuroscience. AH is supported by a studentship from the Gatsby Charitable Foundation (GAT3755) and the Wellcome Trust (219627/Z/19/Z). VS is supported by a Royal Society Newton International Fellowship (NIF\R1\231927) and a fellowship from the Swiss National Science Foundation (grant no. 222150).</p></ack><fn-group><fn id="FN1"><label>2</label><p id="P40">cos(<italic>α - β</italic>) = cos(<italic>α</italic>) cos(<italic>β</italic>) + sin(<italic>α</italic>) sin(<italic>β</italic>)</p></fn><fn id="FN2"><label>3</label><p id="P41">The <italic>k</italic>-th weak derivative of a function f: ℝ → ℝ is the defined as the function <inline-formula><mml:math id="M24"><mml:mrow><mml:mi>g</mml:mi><mml:mo>∈</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mtext>loc</mml:mtext></mml:mrow><mml:mn>1</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> that satisfies <inline-formula><mml:math id="M25"><mml:mrow><mml:mstyle displaystyle="true"><mml:mrow><mml:msub><mml:mo>∫</mml:mo><mml:mi>ℝ</mml:mi></mml:msub><mml:mi>φ</mml:mi></mml:mrow></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext>d</mml:mtext><mml:mi>x</mml:mi><mml:mspace width="0.2em"/><mml:mo>=</mml:mo><mml:mspace width="0.2em"/><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>k</mml:mi></mml:msup><mml:mstyle displaystyle="true"><mml:mrow><mml:msub><mml:mo>∫</mml:mo><mml:mi>ℝ</mml:mi></mml:msub><mml:mrow><mml:msup><mml:mi>φ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext>d</mml:mtext><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula>, for all <inline-formula><mml:math id="M26"><mml:mrow><mml:mi>φ</mml:mi><mml:mo>∈</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mi>c</mml:mi><mml:mi>∞</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>ℝ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</p></fn></fn-group><ref-list><ref id="R1"><label>[1]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname><given-names>Mark M</given-names></name><name><surname>Cunningham</surname><given-names>John P</given-names></name><name><surname>Kaufman</surname><given-names>Matthew T</given-names></name><name><surname>Foster</surname><given-names>Justin D</given-names></name><name><surname>Nuyujukian</surname><given-names>Paul</given-names></name><name><surname>Ryu</surname><given-names>Stephen I</given-names></name><name><surname>Shenoy</surname><given-names>Krishna V</given-names></name></person-group><article-title>Neural population dynamics during reaching</article-title><source>Nature</source><year>2012</year><volume>487</volume><issue>7405</issue><fpage>51</fpage><lpage>56</lpage><pub-id pub-id-type="pmcid">PMC3393826</pub-id><pub-id pub-id-type="pmid">22722855</pub-id><pub-id pub-id-type="doi">10.1038/nature11129</pub-id></element-citation></ref><ref id="R2"><label>[2]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mante</surname><given-names>Valerio</given-names></name><name><surname>Sussillo</surname><given-names>David</given-names></name><name><surname>Shenoy</surname><given-names>Krishna V</given-names></name><name><surname>Newsome</surname><given-names>William T</given-names></name></person-group><article-title>Context-dependent computation by recurrent dynamics in prefrontal cortex</article-title><source>Nature</source><year>2013</year><volume>503</volume><issue>7474</issue><fpage>78</fpage><lpage>84</lpage><pub-id pub-id-type="pmcid">PMC4121670</pub-id><pub-id pub-id-type="pmid">24201281</pub-id><pub-id pub-id-type="doi">10.1038/nature12742</pub-id></element-citation></ref><ref id="R3"><label>[3]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russo</surname><given-names>Abigail A</given-names></name><name><surname>Bittner</surname><given-names>Sean R</given-names></name><name><surname>Perkins</surname><given-names>Sean M</given-names></name><name><surname>Seely</surname><given-names>Jeffrey S</given-names></name><name><surname>London</surname><given-names>Brian M</given-names></name><name><surname>Lara</surname><given-names>Antonio H</given-names></name><name><surname>Miri</surname><given-names>Andrew</given-names></name><name><surname>Marshall</surname><given-names>Najja J</given-names></name><name><surname>Kohn</surname><given-names>Adam</given-names></name><name><surname>Jessell</surname><given-names>Thomas M</given-names></name><etal/></person-group><article-title>Motor cortex embeds muscle-like commands in an untangled population response</article-title><source>Neuron</source><year>2018</year><volume>97</volume><issue>4</issue><fpage>953</fpage><lpage>966</lpage><pub-id pub-id-type="pmcid">PMC5823788</pub-id><pub-id pub-id-type="pmid">29398358</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2018.01.004</pub-id></element-citation></ref><ref id="R4"><label>[4]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Remington</surname><given-names>Evan D</given-names></name><name><surname>Narain</surname><given-names>Devika</given-names></name><name><surname>Hosseini</surname><given-names>Eghbal A</given-names></name><name><surname>Jazayeri</surname><given-names>Mehrdad</given-names></name></person-group><article-title>Flexible sensorimotor computations through rapid reconfiguration of cortical dynamics</article-title><source>Neuron</source><year>2018</year><volume>98</volume><issue>5</issue><fpage>1005</fpage><lpage>1019</lpage><pub-id pub-id-type="pmcid">PMC6009852</pub-id><pub-id pub-id-type="pmid">29879384</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2018.05.020</pub-id></element-citation></ref><ref id="R5"><label>[5]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shenoy</surname><given-names>Krishna V</given-names></name><name><surname>Sahani</surname><given-names>Maneesh</given-names></name><name><surname>Churchland</surname><given-names>Mark M</given-names></name></person-group><article-title>Cortical control of arm movements: a dynamical systems perspective</article-title><source>Annual Review of Neuroscience</source><year>2013</year><volume>36</volume><issue>1</issue><fpage>337</fpage><lpage>359</lpage><pub-id pub-id-type="pmid">23725001</pub-id></element-citation></ref><ref id="R6"><label>[6]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vyas</surname><given-names>Saurabh</given-names></name><name><surname>Golub</surname><given-names>Matthew D</given-names></name><name><surname>Sussillo</surname><given-names>David</given-names></name><name><surname>Shenoy</surname><given-names>Krishna V</given-names></name></person-group><article-title>Computation through neural population dynamics</article-title><source>Annual Review of Neuroscience</source><year>2020</year><volume>43</volume><issue>1</issue><fpage>249</fpage><lpage>275</lpage><pub-id pub-id-type="pmcid">PMC7402639</pub-id><pub-id pub-id-type="pmid">32640928</pub-id><pub-id pub-id-type="doi">10.1146/annurev-neuro-092619-094115</pub-id></element-citation></ref><ref id="R7"><label>[7]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallego</surname><given-names>Juan A</given-names></name><name><surname>Perich</surname><given-names>Matthew G</given-names></name><name><surname>Miller</surname><given-names>Lee E</given-names></name><name><surname>Solla</surname><given-names>Sara A</given-names></name></person-group><article-title>Neural manifolds for the control of movement</article-title><source>Neuron</source><year>2017</year><volume>94</volume><issue>5</issue><fpage>978</fpage><lpage>984</lpage><pub-id pub-id-type="pmcid">PMC6122849</pub-id><pub-id pub-id-type="pmid">28595054</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2017.05.025</pub-id></element-citation></ref><ref id="R8"><label>[8]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Byron</surname><given-names>M Yu</given-names></name><name><surname>Cunningham</surname><given-names>John P</given-names></name><name><surname>Santhanam</surname><given-names>Gopal</given-names></name><name><surname>Ryu</surname><given-names>Stephen I</given-names></name><name><surname>Shenoy</surname><given-names>Krishna V</given-names></name><name><surname>Sahani</surname><given-names>Maneesh</given-names></name></person-group><article-title>Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</article-title><source>Journal of Neurophysiology</source><year>2009</year><volume>102</volume><issue>1</issue><fpage>614</fpage><lpage>635</lpage><pub-id pub-id-type="pmcid">PMC2712272</pub-id><pub-id pub-id-type="pmid">19357332</pub-id><pub-id pub-id-type="doi">10.1152/jn.90941.2008</pub-id></element-citation></ref><ref id="R9"><label>[9]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Macke</surname><given-names>Jakob H</given-names></name><name><surname>Buesing</surname><given-names>Lars</given-names></name><name><surname>Cunningham</surname><given-names>John P</given-names></name><name><surname>Yu</surname><given-names>Byron M</given-names></name><name><surname>Shenoy</surname><given-names>Krishna V</given-names></name><name><surname>Sahani</surname><given-names>Maneesh</given-names></name></person-group><chapter-title>Empirical models of spiking in neural populations</chapter-title><source>Advances in Neural Information Processing Systems</source><year>2011</year><volume>24</volume></element-citation></ref><ref id="R10"><label>[10]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pandarinath</surname><given-names>Chethan</given-names></name><name><surname>O’Shea</surname><given-names>Daniel J</given-names></name><name><surname>Collins</surname><given-names>Jasmine</given-names></name><name><surname>Jozefowicz</surname><given-names>Rafal</given-names></name><name><surname>Stavisky</surname><given-names>Sergey D</given-names></name><name><surname>Kao</surname><given-names>Jonathan C</given-names></name><name><surname>Trautmann</surname><given-names>Eric M</given-names></name><name><surname>Kaufman</surname><given-names>Matthew T</given-names></name><name><surname>Ryu</surname><given-names>Stephen I</given-names></name><name><surname>Hochberg</surname><given-names>Leigh R</given-names></name><name><surname>Henderson</surname><given-names>Jaimie M</given-names></name><etal/></person-group><article-title>Inferring single-trial neural population dynamics using sequential auto-encoders</article-title><source>Nature Methods</source><year>2018</year><volume>15</volume><issue>10</issue><fpage>805</fpage><lpage>815</lpage><pub-id pub-id-type="pmcid">PMC6380887</pub-id><pub-id pub-id-type="pmid">30224673</pub-id><pub-id pub-id-type="doi">10.1038/s41592-018-0109-9</pub-id></element-citation></ref><ref id="R11"><label>[11]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Duncker</surname><given-names>Lea</given-names></name><name><surname>Bohner</surname><given-names>Gergo</given-names></name><name><surname>Boussard</surname><given-names>Julien</given-names></name><name><surname>Sahani</surname><given-names>Maneesh</given-names></name></person-group><source>Learning interpretable continuous-time models of latent stochastic dynamical systems</source><conf-name>International conference on machine learning</conf-name><conf-sponsor>PMLR</conf-sponsor><year>2019</year><fpage>1726</fpage><lpage>1734</lpage></element-citation></ref><ref id="R12"><label>[12]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Glaser</surname><given-names>Joshua</given-names></name><name><surname>Whiteway</surname><given-names>Matthew</given-names></name><name><surname>Cunningham</surname><given-names>John P</given-names></name><name><surname>Paninski</surname><given-names>Liam</given-names></name><name><surname>Linderman</surname><given-names>Scott</given-names></name></person-group><chapter-title>Recurrent switching dynamical systems models for multiple interacting neural populations</chapter-title><source>Advances in Neural Information Processing Systems</source><year>2020</year><volume>33</volume><fpage>14867</fpage><lpage>14878</lpage></element-citation></ref><ref id="R13"><label>[13]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>Ding</given-names></name><name><surname>Wei</surname><given-names>Xue-Xin</given-names></name></person-group><chapter-title>Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-VAE</chapter-title><source>Advances in neural information processing systems</source><year>2020</year><volume>33</volume><fpage>7234</fpage><lpage>7247</lpage></element-citation></ref><ref id="R14"><label>[14]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>Timothy D</given-names></name><name><surname>Luo</surname><given-names>Thomas Z</given-names></name><name><surname>Pillow</surname><given-names>Jonathan W</given-names></name><name><surname>Brody</surname><given-names>Carlos D</given-names></name></person-group><source>Inferring latent dynamics underlying neural population activity via neural differential equations</source><conf-name>International Conference on Machine Learning</conf-name><conf-sponsor>PMLR</conf-sponsor><year>2021</year><fpage>5551</fpage><lpage>5561</lpage></element-citation></ref><ref id="R15"><label>[15]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Schimel</surname><given-names>Marine</given-names></name><name><surname>Kao</surname><given-names>Ta-Chu</given-names></name><name><surname>Jensen</surname><given-names>Kristopher T</given-names></name><name><surname>Hennequin</surname><given-names>Guillaume</given-names></name></person-group><source>iLQR-VAE : control-based learning of input-driven dynamics with applications to neural data</source><conf-name>International Conference on Learning Representations</conf-name><year>2022</year></element-citation></ref><ref id="R16"><label>[16]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keshtkaran</surname><given-names>Mohammad Reza</given-names></name><name><surname>Sedler</surname><given-names>Andrew R</given-names></name><name><surname>Chowdhury</surname><given-names>Raeed H</given-names></name><name><surname>Tandon</surname><given-names>Raghav</given-names></name><name><surname>Basrai</surname><given-names>Diya</given-names></name><name><surname>Nguyen</surname><given-names>Sarah L</given-names></name><name><surname>Sohn</surname><given-names>Hansem</given-names></name><name><surname>Jazayeri</surname><given-names>Mehrdad</given-names></name><name><surname>Miller</surname><given-names>Lee E</given-names></name><name><surname>Pandarinath</surname><given-names>Chethan</given-names></name></person-group><article-title>A large-scale neural network training framework for generalized estimation of single-trial population dynamics</article-title><source>Nature Methods</source><year>2022</year><volume>19</volume><issue>12</issue><fpage>1572</fpage><lpage>1577</lpage><pub-id pub-id-type="pmcid">PMC9825111</pub-id><pub-id pub-id-type="pmid">36443486</pub-id><pub-id pub-id-type="doi">10.1038/s41592-022-01675-0</pub-id></element-citation></ref><ref id="R17"><label>[17]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Valente</surname><given-names>Adrian</given-names></name><name><surname>Pillow</surname><given-names>Jonathan W</given-names></name><name><surname>Ostojic</surname><given-names>Srdjan</given-names></name></person-group><chapter-title>Extracting computational mechanisms from neural data using low-rank RNNs</chapter-title><source>Advances in Neural Information Processing Systems</source><year>2022</year><volume>35</volume><fpage>24072</fpage><lpage>24086</lpage></element-citation></ref><ref id="R18"><label>[18]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Shuqi</given-names></name><name><surname>Schmutz</surname><given-names>Valentin</given-names></name><name><surname>Bellec</surname><given-names>Guillaume</given-names></name><name><surname>Gerstner</surname><given-names>Wulfram</given-names></name></person-group><chapter-title>Mesoscopic modeling of hidden spiking neurons</chapter-title><source>Advances in Neural Information Processing Systems</source><year>2022</year><volume>35</volume><fpage>23566</fpage><lpage>23579</lpage></element-citation></ref><ref id="R19"><label>[19]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Genkin</surname><given-names>Mikhail</given-names></name><name><surname>Shenoy</surname><given-names>Krishna V</given-names></name><name><surname>Chandrasekaran</surname><given-names>Chandramouli</given-names></name><name><surname>Engel</surname><given-names>Tatiana A</given-names></name></person-group><article-title>The dynamics and geometry of choice in premotor cortex</article-title><source>bioRxiv</source><year>2023</year></element-citation></ref><ref id="R20"><label>[20]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>Timothy Doyeon</given-names></name><name><surname>Luo</surname><given-names>Thomas Zhihao</given-names></name><name><surname>Can</surname><given-names>Tankut</given-names></name><name><surname>Krishnamurthy</surname><given-names>Kamesh</given-names></name><name><surname>Pillow</surname><given-names>Jonathan W</given-names></name><name><surname>Brody</surname><given-names>Carlos D</given-names></name></person-group><article-title>Flow-field inference from neural data using deep recurrent networks</article-title><source>bioRxiv</source><year>2023</year><pub-id pub-id-type="doi">10.1101/2023.11.14.567136</pub-id></element-citation></ref><ref id="R21"><label>[21]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jha</surname><given-names>Aditi</given-names></name><name><surname>Gupta</surname><given-names>Diksha</given-names></name><name><surname>Brody</surname><given-names>Carlos</given-names></name><name><surname>Pillow</surname><given-names>Jonathan W</given-names></name></person-group><chapter-title>Disentangling the roles of distinct cell classes with cell-type dynamical systems</chapter-title><source>Advances in Neural Information Processing Systems</source><year>2024</year><volume>37</volume><fpage>33668</fpage><lpage>33690</lpage></element-citation></ref><ref id="R22"><label>[22]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langdon</surname><given-names>Christopher</given-names></name><name><surname>Engel</surname><given-names>Tatiana A</given-names></name></person-group><article-title>Latent circuit inference from heterogeneous neural responses during cognitive tasks</article-title><source>Nature Neuroscience</source><year>2025</year><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="pmcid">PMC11893458</pub-id><pub-id pub-id-type="pmid">39930096</pub-id><pub-id pub-id-type="doi">10.1038/s41593-025-01869-7</pub-id></element-citation></ref><ref id="R23"><label>[23]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gosztolai</surname><given-names>Adam</given-names></name><name><surname>Peach</surname><given-names>Robert L</given-names></name><name><surname>Arnaudon</surname><given-names>Alexis</given-names></name><name><surname>Barahona</surname><given-names>Mauricio</given-names></name><name><surname>Vandergheynst</surname><given-names>Pierre</given-names></name></person-group><article-title>MARBLE: interpretable representations of neural population dynamics using geometric deep learning</article-title><source>Nature Methods</source><year>2025</year><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="pmcid">PMC11903309</pub-id><pub-id pub-id-type="pmid">39962310</pub-id><pub-id pub-id-type="doi">10.1038/s41592-024-02582-2</pub-id></element-citation></ref><ref id="R24"><label>[24]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname><given-names>David</given-names></name><name><surname>Barak</surname><given-names>Omri</given-names></name></person-group><article-title>Opening the black box: low-dimensional dynamics in high-dimensional recurrent neural networks</article-title><source>Neural Computation</source><year>2013</year><volume>25</volume><issue>3</issue><fpage>626</fpage><lpage>649</lpage><pub-id pub-id-type="pmid">23272922</pub-id></element-citation></ref><ref id="R25"><label>[25]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname><given-names>David</given-names></name><name><surname>Churchland</surname><given-names>Mark M</given-names></name><name><surname>Kaufman</surname><given-names>Matthew T</given-names></name><name><surname>Shenoy</surname><given-names>Krishna V</given-names></name></person-group><article-title>A neural network that finds a naturalistic solution for the production of muscle activity</article-title><source>Nature Neuroscience</source><year>2015</year><volume>18</volume><issue>7</issue><fpage>1025</fpage><lpage>1033</lpage><pub-id pub-id-type="pmcid">PMC5113297</pub-id><pub-id pub-id-type="pmid">26075643</pub-id><pub-id pub-id-type="doi">10.1038/nn.4042</pub-id></element-citation></ref><ref id="R26"><label>[26]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hennequin</surname><given-names>Guillaume</given-names></name><name><surname>Vogels</surname><given-names>Tim P</given-names></name><name><surname>Gerstner</surname><given-names>Wulfram</given-names></name></person-group><article-title>Optimal control of transient dynamics in balanced networks supports generation of complex movements</article-title><source>Neuron</source><year>2014</year><volume>82</volume><issue>6</issue><fpage>1394</fpage><lpage>1406</lpage><pub-id pub-id-type="pmcid">PMC6364799</pub-id><pub-id pub-id-type="pmid">24945778</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2014.04.045</pub-id></element-citation></ref><ref id="R27"><label>[27]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carnevale</surname><given-names>Federico</given-names></name><name><surname>de Lafuente</surname><given-names>Victor</given-names></name><name><surname>Romo</surname><given-names>Ranulfo</given-names></name><name><surname>Barak</surname><given-names>Omri</given-names></name><name><surname>Parga</surname><given-names>Néstor</given-names></name></person-group><article-title>Dynamic control of response criterion in premotor cortex during perceptual detection under temporal uncertainty</article-title><source>Neuron</source><year>2015</year><volume>86</volume><issue>4</issue><fpage>1067</fpage><lpage>1077</lpage><pub-id pub-id-type="pmid">25959731</pub-id></element-citation></ref><ref id="R28"><label>[28]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Michaels</surname><given-names>Jonathan A</given-names></name><name><surname>Dann</surname><given-names>Benjamin</given-names></name><name><surname>Scherberger</surname><given-names>Hansjörg</given-names></name></person-group><article-title>Neural population dynamics during reaching are better explained by a dynamical system than representational tuning</article-title><source>PLoS Computational Biology</source><year>2016</year><volume>12</volume><issue>11</issue><elocation-id>e1005175</elocation-id><pub-id pub-id-type="pmcid">PMC5096671</pub-id><pub-id pub-id-type="pmid">27814352</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005175</pub-id></element-citation></ref><ref id="R29"><label>[29]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rivkind</surname><given-names>Alexander</given-names></name><name><surname>Barak</surname><given-names>Omri</given-names></name></person-group><article-title>Local dynamics in trained recurrent neural networks</article-title><source>Physical Review Letters</source><year>2017</year><volume>118</volume><issue>25</issue><elocation-id>258101</elocation-id><pub-id pub-id-type="pmid">28696758</pub-id></element-citation></ref><ref id="R30"><label>[30]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barbosa</surname><given-names>Joao</given-names></name><name><surname>Proville</surname><given-names>Rémi</given-names></name><name><surname>Rodgers</surname><given-names>Chris C</given-names></name><name><surname>DeWeese</surname><given-names>Michael R</given-names></name><name><surname>Ostojic</surname><given-names>Srdjan</given-names></name><name><surname>Boubenec</surname><given-names>Yves</given-names></name></person-group><article-title>Early selection of task-relevant features through population gating</article-title><source>Nature Communications</source><year>2023</year><volume>14</volume><issue>1</issue><elocation-id>6837</elocation-id><pub-id pub-id-type="pmcid">PMC10603060</pub-id><pub-id pub-id-type="pmid">37884507</pub-id><pub-id pub-id-type="doi">10.1038/s41467-023-42519-5</pub-id></element-citation></ref><ref id="R31"><label>[31]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langdon</surname><given-names>Christopher</given-names></name><name><surname>Genkin</surname><given-names>Mikhail</given-names></name><name><surname>Engel</surname><given-names>Tatiana A</given-names></name></person-group><article-title>A unifying perspective on neural manifolds and circuits for cognition</article-title><source>Nature Reviews Neuroscience</source><year>2023</year><volume>24</volume><issue>6</issue><fpage>363</fpage><lpage>377</lpage><pub-id pub-id-type="pmcid">PMC11058347</pub-id><pub-id pub-id-type="pmid">37055616</pub-id><pub-id pub-id-type="doi">10.1038/s41583-023-00693-x</pub-id></element-citation></ref><ref id="R32"><label>[32]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Durstewitz</surname><given-names>Daniel</given-names></name><name><surname>Koppe</surname><given-names>Georgia</given-names></name><name><surname>Thurm</surname><given-names>Max Ingo</given-names></name></person-group><article-title>Reconstructing computational system dynamics from neural data with recurrent neural networks</article-title><source>Nature Reviews Neuroscience</source><year>2023</year><volume>24</volume><issue>11</issue><fpage>693</fpage><lpage>710</lpage><pub-id pub-id-type="pmid">37794121</pub-id></element-citation></ref><ref id="R33"><label>[33]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Driscoll</surname><given-names>Laura N</given-names></name><name><surname>Shenoy</surname><given-names>Krishna</given-names></name><name><surname>Sussillo</surname><given-names>David</given-names></name></person-group><article-title>Flexible multitask computation in recurrent networks utilizes shared dynamical motifs</article-title><source>Nature Neuroscience</source><year>2024</year><volume>27</volume><issue>7</issue><fpage>1349</fpage><lpage>1363</lpage><pub-id pub-id-type="pmcid">PMC11239504</pub-id><pub-id pub-id-type="pmid">38982201</pub-id><pub-id pub-id-type="doi">10.1038/s41593-024-01668-6</pub-id></element-citation></ref><ref id="R34"><label>[34]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gurnani</surname><given-names>Harsha</given-names></name><name><surname>Liu</surname><given-names>Weixuan</given-names></name><name><surname>Brunton</surname><given-names>Bingni W</given-names></name></person-group><article-title>Feedback control of recurrent dynamics constrains learning timescales during motor adaptation</article-title><source>bioRxiv</source><year>2024</year><month>05</month><comment>2024</comment></element-citation></ref><ref id="R35"><label>[35]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litwin-Kumar</surname><given-names>Ashok</given-names></name><name><surname>Doiron</surname><given-names>Brent</given-names></name></person-group><article-title>Slow dynamics and high variability in balanced cortical networks with clustered connections</article-title><source>Nature Neuroscience</source><year>2012</year><volume>15</volume><issue>11</issue><fpage>1498</fpage><lpage>1505</lpage><pub-id pub-id-type="pmcid">PMC4106684</pub-id><pub-id pub-id-type="pmid">23001062</pub-id><pub-id pub-id-type="doi">10.1038/nn.3220</pub-id></element-citation></ref><ref id="R36"><label>[36]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mastrogiuseppe</surname><given-names>Francesca</given-names></name><name><surname>Ostojic</surname><given-names>Srdjan</given-names></name></person-group><article-title>Linking connectivity, dynamics, and computations in low-rank recurrent neural networks</article-title><source>Neuron</source><year>2018</year><volume>99</volume><issue>3</issue><fpage>609</fpage><lpage>623</lpage><pub-id pub-id-type="pmid">30057201</pub-id></element-citation></ref><ref id="R37"><label>[37]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DePasquale</surname><given-names>Brian</given-names></name><name><surname>Sussillo</surname><given-names>David</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Churchland</surname><given-names>Mark M</given-names></name></person-group><article-title>The centrality of population-level factors to network computation is demonstrated by a versatile approach for training spiking networks</article-title><source>Neuron</source><year>2023</year><volume>111</volume><issue>5</issue><fpage>631</fpage><lpage>649</lpage><pub-id pub-id-type="pmcid">PMC10118067</pub-id><pub-id pub-id-type="pmid">36630961</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2022.12.007</pub-id></element-citation></ref><ref id="R38"><label>[38]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmutz</surname><given-names>Valentin</given-names></name><name><surname>Brea</surname><given-names>Johanni</given-names></name><name><surname>Gerstner</surname><given-names>Wulfram</given-names></name></person-group><article-title>Emergent rate-based dynamics in duplicate-free populations of spiking neurons</article-title><source>Physical Review Letters</source><year>2025</year><volume>134</volume><issue>1</issue><elocation-id>018401</elocation-id><pub-id pub-id-type="pmid">39913719</pub-id></element-citation></ref><ref id="R39"><label>[39]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>Carsen</given-names></name><name><surname>Pachitariu</surname><given-names>Marius</given-names></name><name><surname>Steinmetz</surname><given-names>Nicholas</given-names></name><name><surname>Carandini</surname><given-names>Matteo</given-names></name><name><surname>Harris</surname><given-names>Kenneth D</given-names></name></person-group><article-title>High-dimensional geometry of population responses in visual cortex</article-title><source>Nature</source><year>2019</year><volume>571</volume><issue>7765</issue><fpage>361</fpage><lpage>365</lpage><pub-id pub-id-type="pmcid">PMC6642054</pub-id><pub-id pub-id-type="pmid">31243367</pub-id><pub-id pub-id-type="doi">10.1038/s41586-019-1346-5</pub-id></element-citation></ref><ref id="R40"><label>[40]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>Carsen</given-names></name><name><surname>Pachitariu</surname><given-names>Marius</given-names></name><name><surname>Steinmetz</surname><given-names>Nicholas</given-names></name><name><surname>Reddy</surname><given-names>Charu Bai</given-names></name><name><surname>Carandini</surname><given-names>Matteo</given-names></name><name><surname>Harris</surname><given-names>Kenneth D</given-names></name></person-group><article-title>Spontaneous behaviors drive multidimensional, brainwide activity</article-title><source>Science</source><year>2019</year><volume>364</volume><issue>6437</issue><elocation-id>eaav7893</elocation-id><pub-id pub-id-type="pmcid">PMC6525101</pub-id><pub-id pub-id-type="pmid">31000656</pub-id><pub-id pub-id-type="doi">10.1126/science.aav7893</pub-id></element-citation></ref><ref id="R41"><label>[41]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lanore</surname><given-names>Frederic</given-names></name><name><surname>Cayco-Gajic</surname><given-names>N Alex</given-names></name><name><surname>Gurnani</surname><given-names>Harsha</given-names></name><name><surname>Coyle</surname><given-names>Diccon</given-names></name><name><surname>Silver</surname><given-names>R Angus</given-names></name></person-group><article-title>Cerebellar granule cell axons support high-dimensional representations</article-title><source>Nature Neuroscience</source><year>2021</year><volume>24</volume><issue>8</issue><fpage>1142</fpage><lpage>1150</lpage><pub-id pub-id-type="pmcid">PMC7611462</pub-id><pub-id pub-id-type="pmid">34168340</pub-id><pub-id pub-id-type="doi">10.1038/s41593-021-00873-x</pub-id></element-citation></ref><ref id="R42"><label>[42]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manley</surname><given-names>Jason</given-names></name><name><surname>Lu</surname><given-names>Sihao</given-names></name><name><surname>Barber</surname><given-names>Kevin</given-names></name><name><surname>Demas</surname><given-names>Jeffrey</given-names></name><name><surname>Kim</surname><given-names>Hyewon</given-names></name><name><surname>Meyer</surname><given-names>David</given-names></name><name><surname>Traub</surname><given-names>Francisca Martínez</given-names></name><name><surname>Vaziri</surname><given-names>Alipasha</given-names></name></person-group><article-title>Simultaneous, cortex-wide dynamics of up to 1 million neurons reveal unbounded scaling of dimensionality with neuron number</article-title><source>Neuron</source><year>2024</year><volume>112</volume><issue>10</issue><fpage>1694</fpage><lpage>1709</lpage><pub-id pub-id-type="pmcid">PMC11098699</pub-id><pub-id pub-id-type="pmid">38452763</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2024.02.011</pub-id></element-citation></ref><ref id="R43"><label>[43]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jazayeri</surname><given-names>Mehrdad</given-names></name><name><surname>Ostojic</surname><given-names>Srdjan</given-names></name></person-group><article-title>Interpreting neural computations by examining intrinsic and embedding dimensionality of neural activity</article-title><source>Current Opinion in Neurobiology</source><year>2021</year><volume>70</volume><fpage>113</fpage><lpage>120</lpage><pub-id pub-id-type="pmcid">PMC8688220</pub-id><pub-id pub-id-type="pmid">34537579</pub-id><pub-id pub-id-type="doi">10.1016/j.conb.2021.08.002</pub-id></element-citation></ref><ref id="R44"><label>[44]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Humphries</surname><given-names>Mark D</given-names></name></person-group><article-title>Strong and weak principles of neural dimension reduction</article-title><source>Neurons, Behavior, Data analysis, and Theory</source><year>2021</year><volume>5</volume><issue>2</issue><fpage>1</fpage><lpage>28</lpage></element-citation></ref><ref id="R45"><label>[45]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cho</surname><given-names>Youngmin</given-names></name><name><surname>Saul</surname><given-names>Lawrence</given-names></name></person-group><article-title>Kernel methods for deep learning</article-title><source>Advances in Neural Information Processing Systems</source><year>2009</year><volume>22</volume></element-citation></ref><ref id="R46"><label>[46]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bach</surname><given-names>Francis</given-names></name></person-group><article-title>Breaking the curse of dimensionality with convex neural networks</article-title><source>Journal of Machine Learning Research</source><year>2017</year><volume>18</volume><issue>19</issue><fpage>1</fpage><lpage>53</lpage></element-citation></ref><ref id="R47"><label>[47]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bietti</surname><given-names>Alberto</given-names></name><name><surname>Bach</surname><given-names>Francis</given-names></name></person-group><source>Deep equals shallow for ReLU networks in kernel regimes</source><conf-name>International Conference on Learning Representations</conf-name><year>2021</year></element-citation></ref><ref id="R48"><label>[48]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Skaggs</surname><given-names>William</given-names></name><name><surname>Knierim</surname><given-names>James</given-names></name><name><surname>Kudrimoti</surname><given-names>Hemant</given-names></name><name><surname>McNaughton</surname><given-names>Bruce</given-names></name></person-group><chapter-title>A model of the neural basis of the rat’s sense of direction</chapter-title><source>Advances in Neural Information Processing Systems</source><year>1994</year><volume>7</volume><pub-id pub-id-type="pmid">11539168</pub-id></element-citation></ref><ref id="R49"><label>[49]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ben-Yishai</surname><given-names>Rani</given-names></name><name><surname>Bar-Or</surname><given-names>R Lev</given-names></name><name><surname>Sompolinsky</surname><given-names>Haim</given-names></name></person-group><article-title>Theory of orientation tuning in visual cortex</article-title><source>Proceedings of the National Academy of Sciences</source><year>1995</year><volume>92</volume><issue>9</issue><fpage>3844</fpage><lpage>3848</lpage><pub-id pub-id-type="pmcid">PMC42058</pub-id><pub-id pub-id-type="pmid">7731993</pub-id><pub-id pub-id-type="doi">10.1073/pnas.92.9.3844</pub-id></element-citation></ref><ref id="R50"><label>[50]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Kechen</given-names></name></person-group><article-title>Representation of spatial orientation by the intrinsic dynamics of the headdirection cell ensemble: a theory</article-title><source>Journal of Neuroscience</source><year>1996</year><volume>16</volume><issue>6</issue><fpage>2112</fpage><lpage>2126</lpage><pub-id pub-id-type="pmcid">PMC6578512</pub-id><pub-id pub-id-type="pmid">8604055</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.16-06-02112.1996</pub-id></element-citation></ref><ref id="R51"><label>[51]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beiran</surname><given-names>Manuel</given-names></name><name><surname>Dubreuil</surname><given-names>Alexis</given-names></name><name><surname>Valente</surname><given-names>Adrian</given-names></name><name><surname>Mastrogiuseppe</surname><given-names>Francesca</given-names></name><name><surname>Ostojic</surname><given-names>Srdjan</given-names></name></person-group><article-title>Shaping dynamics with multiple populations in low-rank recurrent networks</article-title><source>Neural Computation</source><year>2021</year><volume>33</volume><issue>6</issue><fpage>1572</fpage><lpage>1615</lpage><pub-id pub-id-type="pmid">34496384</pub-id></element-citation></ref><ref id="R52"><label>[52]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pezon</surname><given-names>Louis</given-names></name><name><surname>Schmutz</surname><given-names>Valentin</given-names></name><name><surname>Gerstner</surname><given-names>Wulfram</given-names></name></person-group><article-title>Linking neural manifolds to circuit structure in recurrent networks</article-title><source>bioRxiv</source><year>2024</year><comment>2024–02</comment></element-citation></ref><ref id="R53"><label>[53]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mézard</surname><given-names>Marc</given-names></name><name><surname>Parisi</surname><given-names>Giorgio</given-names></name><name><surname>Zee</surname><given-names>Anthony</given-names></name></person-group><article-title>Spectra of euclidean random matrices</article-title><source>Nuclear Physics B</source><year>1999</year><volume>559</volume><issue>3</issue><fpage>689</fpage><lpage>701</lpage></element-citation></ref><ref id="R54"><label>[54]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koltchinskii</surname><given-names>Vladimir</given-names></name><name><surname>Giné</surname><given-names>Evarist</given-names></name></person-group><article-title>Random matrix approximation of spectra of integral operators</article-title><source>Bernoulli</source><year>2000</year><volume>6</volume><issue>1</issue><fpage>113</fpage><lpage>167</lpage><comment>ISSN 1350-7265,1573-9759</comment><pub-id pub-id-type="doi">10.2307/3318636</pub-id></element-citation></ref><ref id="R55"><label>[55]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bordenave</surname><given-names>Charles</given-names></name></person-group><article-title>Eigenvalues of euclidean random matrices</article-title><source>Random Structures &amp; Algorithms</source><year>2008</year><volume>33</volume><issue>4</issue><fpage>515</fpage><lpage>532</lpage><pub-id pub-id-type="doi">10.1002/rsa.20228</pub-id></element-citation></ref><ref id="R56"><label>[56]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bach</surname><given-names>Francis</given-names></name></person-group><source>Learning theory from first principles</source><publisher-name>MIT press</publisher-name><year>2024</year></element-citation></ref><ref id="R57"><label>[57]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Scetbon</surname><given-names>Meyer</given-names></name><name><surname>Harchaoui</surname><given-names>Zaid</given-names></name></person-group><source>A spectral analysis of dot-product kernels</source><conf-name>International Conference on Artificial Intelligence and Statistics</conf-name><conf-sponsor>PMLR</conf-sponsor><year>2021</year><fpage>3394</fpage><lpage>3402</lpage></element-citation></ref><ref id="R58"><label>[58]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Yicheng</given-names></name><name><surname>Yu</surname><given-names>Zixiong</given-names></name><name><surname>Chen</surname><given-names>Guhan</given-names></name><name><surname>Lin</surname><given-names>Qian</given-names></name></person-group><article-title>On the eigenvalue decay rates of a class of neural-network related kernel functions defined on general domains</article-title><source>Journal of Machine Learning Research</source><year>2024</year><volume>25</volume><issue>82</issue><fpage>1</fpage><lpage>47</lpage></element-citation></ref><ref id="R59"><label>[59]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kühn</surname><given-names>Thomas</given-names></name></person-group><article-title>Eigenvalues of integral operators with smooth positive definite kernels</article-title><source>Archiv der Mathematik</source><year>1987</year><volume>49</volume><fpage>525</fpage><lpage>534</lpage></element-citation></ref><ref id="R60"><label>[60]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castro</surname><given-names>M</given-names></name><name><surname>Menegatto</surname><given-names>V</given-names></name></person-group><article-title>Eigenvalue decay of positive integral operators on the sphere</article-title><source>Mathematics of Computation</source><year>2012</year><volume>81</volume><issue>280</issue><fpage>2303</fpage><lpage>2317</lpage></element-citation></ref><ref id="R61"><label>[61]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Azevedo</surname><given-names>Douglas</given-names></name><name><surname>Menegatto</surname><given-names>Valdir Antonio</given-names></name></person-group><article-title>Sharp estimates for eigenvalues of integral operators generated by dot product kernels on the sphere</article-title><source>Journal of Approximation Theory</source><year>2014</year><volume>177</volume><fpage>57</fpage><lpage>68</lpage></element-citation></ref><ref id="R62"><label>[62]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jordão</surname><given-names>Thaís</given-names></name><name><surname>Menegatto</surname><given-names>V</given-names></name></person-group><article-title>Estimates for fourier sums and eigenvalues of integral operators via multipliers on the sphere</article-title><source>Proceedings of the American Mathematical Society</source><year>2016</year><volume>144</volume><issue>1</issue><fpage>269</fpage><lpage>283</lpage></element-citation></ref><ref id="R63"><label>[63]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bordelon</surname><given-names>Blake</given-names></name><name><surname>Pehlevan</surname><given-names>Cengiz</given-names></name></person-group><article-title>Population codes enable learning from few examples by shaping inductive bias</article-title><source>Elife</source><year>2022</year><volume>11</volume><elocation-id>e78606</elocation-id><pub-id pub-id-type="pmcid">PMC9839349</pub-id><pub-id pub-id-type="pmid">36524716</pub-id><pub-id pub-id-type="doi">10.7554/eLife.78606</pub-id></element-citation></ref><ref id="R64"><label>[64]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>Marjorie</given-names></name><name><surname>Muscinelli</surname><given-names>Samuel P</given-names></name><name><surname>Harris</surname><given-names>Kameron Decker</given-names></name><name><surname>Litwin-Kumar</surname><given-names>Ashok</given-names></name></person-group><article-title>Taskdependent optimal representations for cerebellar learning</article-title><source>Elife</source><year>2023</year><volume>12</volume><elocation-id>e82914</elocation-id><pub-id pub-id-type="pmcid">PMC10541175</pub-id><pub-id pub-id-type="pmid">37671785</pub-id><pub-id pub-id-type="doi">10.7554/eLife.82914</pub-id></element-citation></ref><ref id="R65"><label>[65]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Demas</surname><given-names>Jeffrey</given-names></name><name><surname>Manley</surname><given-names>Jason</given-names></name><name><surname>Tejera</surname><given-names>Frank</given-names></name><name><surname>Barber</surname><given-names>Kevin</given-names></name><name><surname>Kim</surname><given-names>Hyewon</given-names></name><name><surname>Traub</surname><given-names>Francisca Martínez</given-names></name><name><surname>Chen</surname><given-names>Brandon</given-names></name><name><surname>Vaziri</surname><given-names>Alipasha</given-names></name></person-group><article-title>High-speed, cortex-wide volumetric recording of neuroactivity at cellular resolution using light beads microscopy</article-title><source>Nature Methods</source><year>2021</year><volume>18</volume><issue>9</issue><fpage>1103</fpage><lpage>1111</lpage><pub-id pub-id-type="pmcid">PMC8958902</pub-id><pub-id pub-id-type="pmid">34462592</pub-id><pub-id pub-id-type="doi">10.1038/s41592-021-01239-8</pub-id></element-citation></ref><ref id="R66"><label>[66]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haydaroglu</surname><given-names>Ali</given-names></name><name><surname>Chang</surname><given-names>Tinya</given-names></name><name><surname>Landau</surname><given-names>Andrew</given-names></name><name><surname>Krumin</surname><given-names>Michael</given-names></name><name><surname>Dodgson</surname><given-names>Sam</given-names></name><name><surname>Baruchin</surname><given-names>Liad J</given-names></name><name><surname>Cozan</surname><given-names>Maria</given-names></name><name><surname>Guo</surname><given-names>Jingkun</given-names></name><name><surname>Meyer</surname><given-names>David</given-names></name><name><surname>Reddy</surname><given-names>Charu Bai</given-names></name><name><surname>Zhong</surname><given-names>Jian</given-names></name><etal/></person-group><article-title>Suite3D: Volumetric cell detection for two-photon microscopy</article-title><source>bioRxiv</source><year>2025</year><elocation-id>2025.03.26.645628</elocation-id></element-citation></ref><ref id="R67"><label>[67]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Izenman</surname><given-names>Alan Julian</given-names></name></person-group><article-title>Reduced-rank regression for the multivariate linear model</article-title><source>Journal of Multivariate Analysis</source><year>1975</year><volume>5</volume><issue>2</issue><fpage>248</fpage><lpage>264</lpage></element-citation></ref><ref id="R68"><label>[68]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carandini</surname><given-names>Matteo</given-names></name></person-group><article-title>What simple and complex cells compute</article-title><source>The Journal of Physiology</source><year>2006</year><month>December</month><volume>577</volume><issue>Pt 2</issue><fpage>463</fpage><lpage>466</lpage><comment>ISSN 0022-3751</comment><pub-id pub-id-type="pmcid">PMC1890437</pub-id><pub-id pub-id-type="pmid">16973710</pub-id><pub-id pub-id-type="doi">10.1113/jphysiol.2006.118976</pub-id></element-citation></ref><ref id="R69"><label>[69]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schuessler</surname><given-names>Friedrich</given-names></name><name><surname>Dubreuil</surname><given-names>Alexis</given-names></name><name><surname>Mastrogiuseppe</surname><given-names>Francesca</given-names></name><name><surname>Ostojic</surname><given-names>Srdjan</given-names></name><name><surname>Barak</surname><given-names>Omri</given-names></name></person-group><article-title>Dynamics of random recurrent networks with correlated low-rank structure</article-title><source>Physical Review Research</source><year>2020</year><volume>2</volume><issue>1</issue><elocation-id>013111</elocation-id></element-citation></ref><ref id="R70"><label>[70]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dubreuil</surname><given-names>Alexis</given-names></name><name><surname>Valente</surname><given-names>Adrian</given-names></name><name><surname>Beiran</surname><given-names>Manuel</given-names></name><name><surname>Mastrogiuseppe</surname><given-names>Francesca</given-names></name><name><surname>Ostojic</surname><given-names>Srdjan</given-names></name></person-group><article-title>The role of population structure in computations through neural dynamics</article-title><source>Nature Neuroscience</source><year>2022</year><volume>25</volume><issue>6</issue><fpage>783</fpage><lpage>794</lpage><pub-id pub-id-type="pmcid">PMC9284159</pub-id><pub-id pub-id-type="pmid">35668174</pub-id><pub-id pub-id-type="doi">10.1038/s41593-022-01088-4</pub-id></element-citation></ref><ref id="R71"><label>[71]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wan</surname><given-names>Yue</given-names></name><name><surname>Rosenbaum</surname><given-names>Robert</given-names></name></person-group><article-title>High-dimensional dynamics in low-dimensional networks</article-title><source>arXiv preprint</source><year>2025</year><elocation-id>arXiv:2504.13727</elocation-id></element-citation></ref><ref id="R72"><label>[72]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mastrogiuseppe</surname><given-names>Francesca</given-names></name><name><surname>Carmona</surname><given-names>Joana</given-names></name><name><surname>Machens</surname><given-names>Christian</given-names></name></person-group><article-title>Stochastic activity in low-rank recurrent neural networks</article-title><source>bioRxiv</source><year>2025</year><month>04</month><comment>2025</comment></element-citation></ref><ref id="R73"><label>[73]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Veltz</surname><given-names>Romain</given-names></name><name><surname>Faugeras</surname><given-names>Olivier</given-names></name></person-group><article-title>Local/global analysis of the stationary solutions of some neural field equations</article-title><source>SIAM Journal on Applied Dynamical Systems</source><year>2010</year><volume>9</volume><issue>3</issue><fpage>954</fpage><lpage>998</lpage></element-citation></ref><ref id="R74"><label>[74]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>Jaehoon</given-names></name><name><surname>Sohl-dickstein</surname><given-names>Jascha</given-names></name><name><surname>Pennington</surname><given-names>Jeffrey</given-names></name><name><surname>Novak</surname><given-names>Roman</given-names></name><name><surname>Schoenholz</surname><given-names>Sam</given-names></name><name><surname>Bahri</surname><given-names>Yasaman</given-names></name></person-group><source>Deep neural networks as gaussian processes</source><conf-name>International Conference on Learning Representations</conf-name><year>2018</year></element-citation></ref><ref id="R75"><label>[75]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jacot</surname><given-names>Arthur</given-names></name><name><surname>Gabriel</surname><given-names>Franck</given-names></name><name><surname>Hongler</surname><given-names>Clement</given-names></name></person-group><chapter-title>Neural tangent kernel: Convergence and generalization in neural networks</chapter-title><source>Advances in Neural Information Processing Systems</source><year>2018</year><volume>31</volume></element-citation></ref><ref id="R76"><label>[76]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Geifman</surname><given-names>Amnon</given-names></name><name><surname>Yadav</surname><given-names>Abhay</given-names></name><name><surname>Kasten</surname><given-names>Yoni</given-names></name><name><surname>Galun</surname><given-names>Meirav</given-names></name><name><surname>Jacobs</surname><given-names>David</given-names></name><name><surname>Ronen</surname><given-names>Basri</given-names></name></person-group><chapter-title>On the similarity between the laplace and neural tangent kernels</chapter-title><source>Advances in Neural Information Processing Systems</source><year>2020</year><volume>33</volume><fpage>1451</fpage><lpage>1461</lpage></element-citation></ref><ref id="R77"><label>[77]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bietti</surname><given-names>Alberto</given-names></name><name><surname>Mairal</surname><given-names>Julien</given-names></name></person-group><chapter-title>On the inductive bias of neural tangent kernels</chapter-title><source>Advances in Neural Information Processing Systems</source><year>2019</year><volume>32</volume></element-citation></ref><ref id="R78"><label>[78]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Smola</surname><given-names>Alex</given-names></name><name><surname>Ovári</surname><given-names>Zoltán</given-names></name><name><surname>Williamson</surname><given-names>Robert C</given-names></name></person-group><chapter-title>Regularization with dot-product kernels</chapter-title><source>Advances in Neural Information Processing Systems</source><year>2000</year><volume>13</volume></element-citation></ref><ref id="R79"><label>[79]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Minh</surname><given-names>Ha Quang</given-names></name><name><surname>Niyogi</surname><given-names>Partha</given-names></name><name><surname>Yao</surname><given-names>Yuan</given-names></name></person-group><source>Mercer’s theorem, feature maps, and smoothing</source><conf-name>International Conference on Computational Learning Theory</conf-name><conf-sponsor>Springer</conf-sponsor><year>2006</year><fpage>154</fpage><lpage>168</lpage></element-citation></ref><ref id="R80"><label>[80]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prince</surname><given-names>Luke Y</given-names></name><name><surname>Bakhtiari</surname><given-names>Shahab</given-names></name><name><surname>Gillon</surname><given-names>Colleen J</given-names></name><name><surname>Richards</surname><given-names>Blake A</given-names></name></person-group><article-title>Parallel inference of hierarchical latent dynamics in two-photon calcium imaging of neuronal populations</article-title><source>bioRxiv</source><year>2021</year><month>03</month><comment>2021</comment></element-citation></ref><ref id="R81"><label>[81]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>Feng</given-names></name><name><surname>Grier</surname><given-names>Harrison A</given-names></name><name><surname>Tandon</surname><given-names>Raghav</given-names></name><name><surname>Cai</surname><given-names>Changjia</given-names></name><name><surname>Agarwal</surname><given-names>Anjali</given-names></name><name><surname>Giovannucci</surname><given-names>Andrea</given-names></name><name><surname>Kaufman</surname><given-names>Matthew T</given-names></name><name><surname>Pandarinath</surname><given-names>Chethan</given-names></name></person-group><article-title>A deep learning framework for inference of single-trial neural population dynamics from calcium imaging with subframe temporal resolution</article-title><source>Nature Neuroscience</source><year>2022</year><volume>25</volume><issue>12</issue><fpage>1724</fpage><lpage>1734</lpage><pub-id pub-id-type="pmcid">PMC9825112</pub-id><pub-id pub-id-type="pmid">36424431</pub-id><pub-id pub-id-type="doi">10.1038/s41593-022-01189-0</pub-id></element-citation></ref><ref id="R82"><label>[82]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koh</surname><given-names>Tze Hui</given-names></name><name><surname>Bishop</surname><given-names>William E</given-names></name><name><surname>Kawashima</surname><given-names>Takashi</given-names></name><name><surname>Jeon</surname><given-names>Brian B</given-names></name><name><surname>Srinivasan</surname><given-names>Ranjani</given-names></name><name><surname>Mu</surname><given-names>Yu</given-names></name><name><surname>Wei</surname><given-names>Ziqiang</given-names></name><name><surname>Kuhlman</surname><given-names>Sandra J</given-names></name><name><surname>Ahrens</surname><given-names>Misha B</given-names></name><name><surname>Chase</surname><given-names>Steven M</given-names></name><etal/></person-group><article-title>Dimensionality reduction of calcium-imaged neuronal population activity</article-title><source>Nature Computational Science</source><year>2023</year><volume>3</volume><issue>1</issue><fpage>71</fpage><lpage>85</lpage><pub-id pub-id-type="pmcid">PMC10358781</pub-id><pub-id pub-id-type="pmid">37476302</pub-id><pub-id pub-id-type="doi">10.1038/s43588-022-00390-2</pub-id></element-citation></ref><ref id="R83"><label>[83]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneider</surname><given-names>Steffen</given-names></name><name><surname>Lee</surname><given-names>Jin Hwa</given-names></name><name><surname>Mathis</surname><given-names>Mackenzie Weygandt</given-names></name></person-group><article-title>Learnable latent embeddings for joint behavioural and neural analysis</article-title><source>Nature</source><year>2023</year><month>May</month><volume>617</volume><issue>7960</issue><fpage>360</fpage><lpage>368</lpage><pub-id pub-id-type="pmcid">PMC10172131</pub-id><pub-id pub-id-type="pmid">37138088</pub-id><pub-id pub-id-type="doi">10.1038/s41586-023-06031-6</pub-id></element-citation></ref><ref id="R84"><label>[84]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>Carsen</given-names></name><name><surname>Zhong</surname><given-names>Lin</given-names></name><name><surname>Syeda</surname><given-names>Atika</given-names></name><name><surname>Du</surname><given-names>Fengtong</given-names></name><name><surname>Kesa</surname><given-names>Maria</given-names></name><name><surname>Pachitariu</surname><given-names>Marius</given-names></name></person-group><article-title>Rastermap: a discovery method for neural population recordings</article-title><source>Nature Neuroscience</source><year>2025</year><volume>28</volume><issue>1</issue><fpage>201</fpage><lpage>212</lpage><pub-id pub-id-type="pmcid">PMC11706777</pub-id><pub-id pub-id-type="pmid">39414974</pub-id><pub-id pub-id-type="doi">10.1038/s41593-024-01783-4</pub-id></element-citation></ref><ref id="R85"><label>[85]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cogno</surname><given-names>Soledad Gonzalo</given-names></name><name><surname>Obenhaus</surname><given-names>Horst A</given-names></name><name><surname>Lautrup</surname><given-names>Ane</given-names></name><name><surname>Jacobsen</surname><given-names>R Irene</given-names></name><name><surname>Clopath</surname><given-names>Claudia</given-names></name><name><surname>Andersson</surname><given-names>Sebastian O</given-names></name><name><surname>Donato</surname><given-names>Flavio</given-names></name><name><surname>Moser</surname><given-names>May-Britt</given-names></name><name><surname>Moser</surname><given-names>Edvard I</given-names></name></person-group><article-title>Minute-scale oscillatory sequences in medial entorhinal cortex</article-title><source>Nature</source><year>2024</year><volume>625</volume><issue>7994</issue><fpage>338</fpage><lpage>344</lpage><pub-id pub-id-type="pmcid">PMC10781645</pub-id><pub-id pub-id-type="pmid">38123682</pub-id><pub-id pub-id-type="doi">10.1038/s41586-023-06864-1</pub-id></element-citation></ref><ref id="R86"><label>[86]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Neal</surname><given-names>Radford M</given-names></name></person-group><chapter-title>Priors for infinite networks</chapter-title><source>Bayesian learning for neural networks</source><year>1996</year><fpage>29</fpage><lpage>53</lpage></element-citation></ref><ref id="R87"><label>[87]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>Christopher</given-names></name></person-group><chapter-title>Computing with infinite networks</chapter-title><source>Advances in Neural Information Processing Systems</source><year>1996</year><volume>9</volume></element-citation></ref><ref id="R88"><label>[88]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>David G</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Litwin-Kumar</surname><given-names>Ashok</given-names></name></person-group><article-title>Dimension of activity in random neural networks</article-title><source>Physical Review Letters</source><year>2023</year><volume>131</volume><issue>11</issue><elocation-id>118401</elocation-id><pub-id pub-id-type="pmid">37774280</pub-id></element-citation></ref><ref id="R89"><label>[89]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tian</surname><given-names>Gengshuo John</given-names></name><name><surname>Zhu</surname><given-names>Ou</given-names></name><name><surname>Shirhatti</surname><given-names>Vinay</given-names></name><name><surname>Greenspon</surname><given-names>Charles M</given-names></name><name><surname>Downey</surname><given-names>John E</given-names></name><name><surname>Freedman</surname><given-names>David J</given-names></name><name><surname>Doiron</surname><given-names>Brent</given-names></name></person-group><article-title>Neuronal firing rate diversity lowers the dimension of population covariability</article-title><source>bioRxiv</source><year>2024</year><month>08</month><comment>2024</comment></element-citation></ref><ref id="R90"><label>[90]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhong</surname><given-names>Jian</given-names></name><name><surname>Natan</surname><given-names>Ryan G</given-names></name><name><surname>Zhang</surname><given-names>Qinrong</given-names></name><name><surname>Wong</surname><given-names>Justin SJ</given-names></name><name><surname>Miehl</surname><given-names>Christoph</given-names></name><name><surname>Bose</surname><given-names>Krishnashish</given-names></name><name><surname>Lu</surname><given-names>Xiaoyu</given-names></name><name><surname>St-Pierre</surname><given-names>François</given-names></name><name><surname>Guo</surname><given-names>Su</given-names></name><name><surname>Doiron</surname><given-names>Brent</given-names></name><name><surname>Tsia</surname><given-names>Kevin K</given-names></name><name><surname>Ji</surname><given-names>Na</given-names></name></person-group><article-title>Faced 2.0 enables large-scale voltage and calcium imaging in vivo</article-title><year>2025</year><elocation-id>2025.03.06.641784</elocation-id></element-citation></ref><ref id="R91"><label>[91]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pospisil</surname><given-names>Dean A</given-names></name><name><surname>Pillow</surname><given-names>Jonathan W</given-names></name></person-group><article-title>Revisiting the high-dimensional geometry of population responses in visual cortex</article-title><source>bioRxiv</source><year>2024</year><pub-id pub-id-type="doi">10.1101/2024.02.16.580726</pub-id></element-citation></ref><ref id="R92"><label>[92]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pachitariu</surname><given-names>Marius</given-names></name><name><surname>Zhong</surname><given-names>Lin</given-names></name><name><surname>Gracias</surname><given-names>Alexa</given-names></name><name><surname>Minisi</surname><given-names>Amanda</given-names></name><name><surname>Lopez</surname><given-names>Crystall</given-names></name><name><surname>Stringer</surname><given-names>Carsen</given-names></name></person-group><article-title>A critical initialization for biological neural networks</article-title><source>bioRxiv</source><year>2025</year><month>01</month><comment>2025</comment></element-citation></ref><ref id="R93"><label>[93]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Tsai-Wen</given-names></name><name><surname>Wardill</surname><given-names>Trevor J</given-names></name><name><surname>Sun</surname><given-names>Yi</given-names></name><name><surname>Pulver</surname><given-names>Stefan R</given-names></name><name><surname>Renninger</surname><given-names>Sabine L</given-names></name><name><surname>Baohan</surname><given-names>Amy</given-names></name><name><surname>Schreiter</surname><given-names>Eric R</given-names></name><name><surname>Kerr</surname><given-names>Rex A</given-names></name><name><surname>Orger</surname><given-names>Michael B</given-names></name><name><surname>Jayaraman</surname><given-names>Vivek</given-names></name><name><surname>Looger</surname><given-names>Loren L</given-names></name><name><surname>Svoboda</surname><given-names>Karel</given-names></name><name><surname>Kim</surname><given-names>Douglas S</given-names></name></person-group><article-title>Ultrasensitive fluorescent proteins for imaging neuronal activity</article-title><source>Nature</source><year>2013</year><volume>499</volume><issue>7458</issue><fpage>295</fpage><lpage>300</lpage><pub-id pub-id-type="pmcid">PMC3777791</pub-id><pub-id pub-id-type="pmid">23868258</pub-id><pub-id pub-id-type="doi">10.1038/nature12354</pub-id></element-citation></ref><ref id="R94"><label>[94]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hinton</surname><given-names>Geoffrey</given-names></name><name><surname>Vinyals</surname><given-names>Oriol</given-names></name><name><surname>Dean</surname><given-names>Jeff</given-names></name></person-group><article-title>Distilling the knowledge in a neural network</article-title><source>arXiv</source><year>2015</year><elocation-id>arXiv:1503.02531</elocation-id><comment>(arXiv:1503.02531)</comment></element-citation></ref><ref id="R95"><label>[95]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>Diederik P</given-names></name><name><surname>Ba</surname><given-names>Jimmy</given-names></name></person-group><article-title>Adam: A method for stochastic optimization</article-title><source>arXiv</source><year>2017</year><elocation-id>arXiv:1412.6980</elocation-id><comment>(arXiv:1412.6980)</comment></element-citation></ref><ref id="R96"><label>[96]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>Kaiming</given-names></name><name><surname>Zhang</surname><given-names>Xiangyu</given-names></name><name><surname>Ren</surname><given-names>Shaoqing</given-names></name><name><surname>Sun</surname><given-names>Jian</given-names></name></person-group><source>Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</source><conf-name>2015 IEEE International Conference on Computer Vision (ICCV)</conf-name><conf-sponsor>IEEE</conf-sponsor><year>2015</year><fpage>1026</fpage><lpage>1034</lpage></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Solvable RNN model.</title><p><bold>(A)</bold> Schematic of the RNN model. <bold>(B)</bold> Shifted cosine function defining the position-dependent synaptic weights <italic>W<sub>ij</sub></italic>. <bold>(C)</bold> Factorization of weight matrix <bold>W</bold> as the product of a 2-column matrix <bold>U</bold> and two-row matrix <bold>V</bold><sup>T</sup>. <bold>(D)</bold> Vector field of the dynamics of the latent variables, <italic>κ</italic><sub>1</sub> and <italic>κ</italic><sub>2</sub>, in the large-network limit. The latent dynamics produces a stable limit cycle on the unit circle. <bold>(E)</bold> Graphical representation of the RNN’s effective dynamics. <bold>(F)</bold> Post-activation covariance eigenspectrum. The eigenvalues follow a power-law with decay exponent <italic>α</italic> = 2. They are normalized such that the largest eigenvalue is 1. The activation function <italic>ϕ</italic> used is the step function defined in <xref ref-type="disp-formula" rid="FD3">Eq. (3)</xref>. Shown in D and F are theoretical values given by Eqs. (5) and (8), respectively, which closely match simulations of large networks (not shown).</p></caption><graphic xlink:href="EMS206299-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Eigenspectrum of the random feature kernel (11).</title><p><bold>(A)</bold> Schematic of the feedforward setup (in the case of three input variables). <bold>(B)</bold> Comparison between simulations of the eigenspectrum decay rate <italic>α</italic> and the theoretical value predicated by <xref ref-type="other" rid="P30">Conjecture 1</xref>, for <italic>p</italic> ∈ [0, 2] and <italic>d</italic> = 2, 3, 4. Up to finite-size effects (number of neurons <italic>N</italic> = 2 · 10<sup>4</sup> and number of inputs <italic>T</italic> = 10<sup>4</sup>), simulations match the theoretical prediction <inline-formula><mml:math id="M27"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>. (In this plot, <italic>c</italic> = 0.) <bold>(C)</bold> Simulations of the eigenspectrum for various bias parameter <italic>c</italic>, while keeping <italic>p</italic> and <italic>d</italic> fixed. (In this plot, <italic>p</italic> = 0 and <italic>d</italic> = 2, which gives <italic>α</italic> = 2. Eigenvalues are normalized as in <xref ref-type="fig" rid="F1">Fig. 1F</xref>.)</p></caption><graphic xlink:href="EMS206299-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Validation of the Neural Cross Encoder</title><p><bold>(A)</bold> Schematic of the NCE. <bold>B</bold> Simulated pre-activation latents of a two-dimensional toy model (top), and the inferred pre-activation latents using NCE (bottom). C Reconstruction score as a function of pre-activation dimensionality for Linear (RRR) and NCE models on simulated data.</p></caption><graphic xlink:href="EMS206299-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Pre-activation dimension in visual cortex depends on stimulus condition.</title><p><bold>(A)</bold> Rasters of true (bottom) and predicted (top) neuronal activity, and estimated latents (middle) for 1000 example neurons using an NCE with pre-activation dimension <italic>d</italic> = 3, for drifting grating responses. <bold>(B)</bold> Fraction of variance explained as a function of pre-activation dimension for an example session of grating responses. Plotted values are the mean across source/target selections for NCE (purple) and Reduced Rank Regression (green). Shaded regions are the standard deviation across source/target selections. All experiments contain 500 source and 1000 target neurons, and all scores are computed on a held-out test set. <bold>(C,D)</bold> Same as A,B for natural image responses. <bold>(E,F)</bold> Same as <bold>A,B</bold> for spontaneous activity. <bold>(G)</bold> Estimated pre-activation dimension <italic>d</italic> sufficient to capture 95% of explainable variance in the NCE across the three conditions. Each point is the mean for a single imaging session, error bars indicate the min/max across source/target selections. <bold>(H)</bold> Fraction of variance explained by NCE models with varying pre-activation dimension (log scale) across the three conditions. Each line represents a single imaging session.</p></caption><graphic xlink:href="EMS206299-f004"/></fig></floats-group></article>