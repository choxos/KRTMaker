<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS206940</article-id><article-id pub-id-type="doi">10.1101/2025.06.11.658893</article-id><article-id pub-id-type="archive">PPR1036407</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Sleep strengthens successor representations of learned sequences</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>He</surname><given-names>Xianhui</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Büchel</surname><given-names>Philipp K.</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Faghel-Soubeyrand</surname><given-names>Simon</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Klingspohr</surname><given-names>Janina</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Kehl</surname><given-names>Marcel S.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Staresina</surname><given-names>Bernhard P.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A4">4</xref><xref ref-type="corresp" rid="CR1">5</xref></contrib></contrib-group><aff id="A1"><label>1</label>Department of Experimental Psychology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap>, <city>Oxford</city>, <country country="GB">UK</country></aff><aff id="A2"><label>2</label>Department of Epileptology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01xnwqx93</institution-id><institution>University Hospital Bonn</institution></institution-wrap>, <addr-line>Venusberg Campus</addr-line>, <city>Bonn</city><postal-code>53127</postal-code>, <country country="DE">Germany</country></aff><aff id="A3"><label>3</label>Department of Systems Neuroscience, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01zgy1s35</institution-id><institution>Universitaetsklinikum Hamburg Eppendorf</institution></institution-wrap>, <addr-line>Martinistr. 52</addr-line>, <postal-code>20246</postal-code><city>Hamburg</city>, <country country="DE">Germany</country></aff><aff id="A4"><label>4</label>Oxford Centre for Human Brain Activity, Centre for Integrative Neuroimaging, Department of Psychiatry, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap>, <city>Oxford</city>, <country country="GB">UK</country></aff><author-notes><corresp id="CR1">
<label>5</label>Corresponding author: Bernhard P. Staresina <email>bernhard.staresina@psy.ox.ac.uk</email>
</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>11</day><month>07</month><year>2025</year></pub-date><pub-date pub-type="preprint"><day>15</day><month>06</month><year>2025</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Experiences reshape our internal representations of the world. However, the neural and cognitive dynamics of this process are largely unknown. Here, we investigated how sequence learning reorganizes neural representations and how sleep-dependent consolidation contributes to this transformation. Using high-density electroencephalography and multivariate decoding, we found that learning temporal sequences of visual information led to the incorporation of successor representations during a subsequent perceptual task, despite temporal information being task-irrelevant. Importantly, individuals with better sequence memory performance exhibited stronger successor incorporation during the perceptual task. Representational similarity analyses comparing neural patterns with different layers of a deep neural network revealed a learning-induced shift in representational format, from low-level visual features to higher-level abstract properties. Critically, both the strength and transformation of successor representations correlated with the proportion of slow-wave sleep during a post-learning nap. These findings support the idea that sequence learning induces lasting changes in visual representational geometry and that sleep strengthens these changes, providing mechanistic insights into how the brain updates internal models after exposure to environmental regularities.</p></abstract><kwd-group><kwd>learning</kwd><kwd>successor representation</kwd><kwd>sleep</kwd><kwd>representational geometry</kwd><kwd>EEG</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">External experiences continuously shape and update the brain’s internal model of the world. These experiences are dynamic though, and individuals do not merely encode representations of isolated events but also establish structured relationships between them. For example, a child who sees a Welsh Corgi for the first time may encode only a basic representation of a “small, fluffy creature”. However, upon repeatedly observing the Corgi following a girl into a house, the child might begin to associate these elements, gradually constructing a predictive model of their temporal co-occurrence (<xref ref-type="fig" rid="F1">Fig. 1a</xref>). While this example illustrates how we update our world model in daily life, the mechanisms underlying the integration of temporal regularities into existing representations remain poorly understood.</p><p id="P3">According to the ‘successor representation’ theory, the brain does not merely represent the current state of the world but also maintains a predictive map, encoding expected future states based on learned temporal transitions (<xref ref-type="bibr" rid="R6">Dayan, 1993</xref>; <xref ref-type="bibr" rid="R25">Momennejad et al., 2017</xref>; <xref ref-type="bibr" rid="R37">Stachenfeld et al., 2017</xref>). This predictive coding mechanism is thought to support flexible behaviour and inference by integrating both immediate states and anticipated future contingencies. While initially developed within the domain of reinforcement learning, recent research suggests that successor representations may also underlie broader cognitive processes, including the organization of human episodic memory (<xref ref-type="bibr" rid="R39">Tacikowski et al., 2024</xref>; <xref ref-type="bibr" rid="R17">John et al., 2025</xref>; <xref ref-type="bibr" rid="R46">Zhou et al., 2025</xref>).</p><p id="P4">Empirical studies provide supporting evidence that learning temporal regularities across experiences shapes subsequent neural representations. Specifically, representations of temporally proximal stimuli tend to be more similar to one another than to temporally distant stimuli (<xref ref-type="bibr" rid="R14">Greco et al., 2024</xref>; <xref ref-type="bibr" rid="R15">Hindy et al., 2016</xref>; <xref ref-type="bibr" rid="R17">John et al., 2025</xref>; <xref ref-type="bibr" rid="R32">Reddy et al., 2015</xref>; <xref ref-type="bibr" rid="R34">Schapiro et al., 2012</xref>; <xref ref-type="bibr" rid="R39">Tacikowski et al., 2024</xref>). Notably, this effect can emerge after relatively few exposures (<xref ref-type="bibr" rid="R32">Reddy et al., 2015</xref>), persists even when such regularities are irrelevant for current task demands (<xref ref-type="bibr" rid="R39">Tacikowski et al., 2024</xref>), and correlates with the replay of neuronal activity in the hippocampus during task-free awake breaks (<xref ref-type="bibr" rid="R39">Tacikowski et al., 2024</xref>). However, previous studies have not distinguished between representations of past and future information, instead demonstrating that hippocampal activity patterns exhibit greater similarity for temporally adjacent stimuli, regardless of sequence direction (<xref ref-type="bibr" rid="R17">John et al., 2025</xref>; <xref ref-type="bibr" rid="R39">Tacikowski et al., 2024</xref>). Recent work has further highlighted the bidirectional nature of these representations, showing that both past and future elements of a sequence are encoded (<xref ref-type="bibr" rid="R40">Tarder-Stoll et al., 2024</xref>). This raises the question whether sequence learning specifically promotes prospective representations (consistent with the successor representation theory), and whether such changes in neural representations generalise to other contexts where the initial sequence is no longer behaviourally relevant.</p><p id="P5">The finding that wake replay strengthens successor representations (<xref ref-type="bibr" rid="R39">Tacikowski et al., 2024</xref>) moreover begs the question whether sleep, known to constitute a privileged time window for memory reactivation and replay (<xref ref-type="bibr" rid="R2">Buzsáki, 2015</xref>; <xref ref-type="bibr" rid="R8">Diekelmann &amp; Born, 2010</xref>; <xref ref-type="bibr" rid="R24">Mölle et al., 2004</xref>; <xref ref-type="bibr" rid="R27">Niknazar et al., 2022</xref>; <xref ref-type="bibr" rid="R44">Wilson &amp; McNaughton, 1994</xref>), contributes to the incorporation of successor representations. Interestingly, a recent study has shown that a night of sleep after a real-life sequential learning experience (guided art tour) selectively bolsters temporal order memory while memory for visual-perceptual features steadily declined over the course of up to one year later (<xref ref-type="bibr" rid="R7">Diamond et al., 2025</xref>). Importantly, this selective strengthening of temporal memory was predicted by the duration of slow-wave sleep following learning (SWS; <xref ref-type="bibr" rid="R7">Diamond et al., 2025</xref>). However, it remains unclear exactly how sleep transforms memory content with regard to perceptual detail vs. higher-level relational structure.</p><p id="P6">In this study, we investigate whether sequence learning reshapes neural representations such that successor information is incorporated in a subsequent task context where the sequence is no longer behaviourally relevant (<xref ref-type="fig" rid="F1">Fig. 1b</xref>). Specifically, we address three key questions: (1) Does sequence learning reshape stimulus representations to incorporate successor information? (2) What is the representational format of successor representations (e.g., low-level perceptual vs. high-level conceptual)? (3) (How) does post-learning sleep contribute to the incorporation of successor representations? To tackle these questions, we combined behavioural tasks, high-density electroencephalography (EEG) including Polysomnography (PSG) and deep neural network (DNN)-based representational similarity analysis. Our findings revealed that successors of learned images could be reliably decoded in a subsequent non-memory task, with immediate post-learning memory performance predicting the strength of this effect. Additionally, DNN-based representational similarity analysis suggested that successor representations shifted toward high-level visual features after learning. Importantly, greater proportions of SWS predicted the strength of successor representations as well as their shifts towards high-level formats. These results suggest that learning reshapes the representational geometry of visual experiences and that SWS contributes to this transformation.</p></sec><sec id="S2" sec-type="results"><title>Results</title><p id="P7">Participants first performed a perceptual task, in which 50 unique images from five categories (objects, faces, scenes, letter strings, and body parts; <xref ref-type="fig" rid="F2">Fig. 2a</xref>) were shown in pseudorandom order, with the instruction to press a button when a given image repeated from one presentation to the next (10% of ‘target’ trials, ‘1-back’ task). The perceptual task was followed by the ‘Memory Arena’ task presenting the same images in fixed temporal sequences and spatial locations (<xref ref-type="fig" rid="F5">Extended Data Fig. 1</xref>; <xref ref-type="bibr" rid="R1">Büchel et al., 2024</xref>). After reaching a learning criterion, participants took a ~2-hour nap. Memory accuracy was tested before and after sleep, followed by a repetition of the perceptual task. By analysing neural activity during the two perceptual tasks, we examined how successor representations changed with learning. Sleep EEG recordings allowed assessment of how sleep architecture influences these changes.</p><sec id="S3"><title>Learning incorporates successor representations</title><p id="P8">Participants performed well in both the Memory Arena task (mean accuracy ± SD: pre-sleep test: 81.39 ± 13.44%; post-sleep test: 73.15 ± 18.07%; <italic>t</italic>(25) = 3.33, <italic>p</italic> = 0.002) and the perceptual task (mean RT ± SD to target detections: pre-learning: 0.581 ± 0.072s; post-learning: 0.584 ± 0.078s; <italic>t</italic>(25) = -0.52, <italic>p</italic> = 0.604).</p><p id="P9">To examine whether sequence learning reshapes neural representations of visual experiences, we first assessed how well predecessor and successor information could be decoded from neural activity (<xref ref-type="fig" rid="F2">Fig. 2a</xref>). Specifically, for each image category (e.g., objects), we trained decoders on EEG data from the pre-learning perceptual task using the remaining four categories (body parts, letter strings, faces, and scenes). We then applied these decoders to EEG signals during the post-learning perceptual task to generate a time-by-time matrix of classifier category prediction. This allowed us to assess the evidence for the immediate predecessor or successor image against the chance level (25%). Results revealed that decoding accuracy of successor images was significantly above chance (<italic>p</italic><sub>cluster</sub> = 0.028, one-tailed cluster-based permutation test; <xref ref-type="fig" rid="F2">Fig. 2b</xref>). In contrast, the decoding accuracies for predecessor, second predecessor, and second successor images did not exceed chance levels (all <italic>p</italic><sub>cluster</sub> &gt; 0.173; <xref ref-type="fig" rid="F2">Fig. 2b</xref> and <xref ref-type="fig" rid="F6">Extended Data Fig. 2a</xref>). Importantly, applying the same decoders to the pre-learning perceptual task revealed no significant effects (<italic>p</italic><sub>cluster</sub> &gt; 0.127). In fact, directly comparing successor decoding accuracies between pre- and post-learning perceptual tasks revealed a significant cluster with higher accuracies in the post-learning session (<italic>p</italic><sub>cluster</sub> = 0.034, one-tailed cluster-based permutation test; <xref ref-type="fig" rid="F6">Extended Data Fig. 2b</xref>). These results rule out the possibility that the successor representation resulted from perceptual similarities among particular image categories.</p><p id="P10">Next, we examined whether the strength of successor representation was linked to learning behaviour. We first extracted the successor decoding accuracy at the midpoint of image presentation intervals (0.5s after image onset for both training and test trials, see <xref ref-type="sec" rid="S7">Methods</xref>) for each participant and correlated it with their sequence learning accuracy. We found that greater immediate post-learning memory accuracy was associated with greater successor representational strength in the final perceptual task (Spearman’s rho = 0.43, <italic>p</italic> = 0.015; <xref ref-type="fig" rid="F2">Fig. 2c</xref>). Extending this analysis across all combinations of training and test time points, we identified a significant cluster where successor decoding accuracy positively correlated with immediate post-learning memory accuracy (<italic>p</italic><sub>cluster</sub> = 0.025; <xref ref-type="fig" rid="F7">Extended Data Fig. 3a</xref>). In contrast, delayed post-sleep memory accuracy showed no significant correlation with successor decoding accuracy (image presentation midpoint: Spearman’s rho = 0.09, <italic>p</italic> = 0.652; cluster-permutation: <italic>p</italic><sub>cluster</sub> &gt; 0.281; <xref ref-type="fig" rid="F7">Extended Data Fig. 3b</xref>).</p><p id="P11">Together, these findings suggest that learning image sequences incorporates successor representations, even when such information is not relevant to the current (perceptual) task. Moreover, better immediate post-learning memory performance predicted stronger successor representations. However, delayed post-sleep memory performance was not significantly associated with successor representations, arguing against active sequence recall during the final perceptual task driving the effect.</p></sec><sec id="S4"><title>Successor representations shift towards high-level visual information</title><p id="P12">What is the representational make-up of successor information? To address this question, we applied representational similarity analysis (<xref ref-type="bibr" rid="R18">Kriegeskorte et al., 2008</xref>) to compare neural representations with those of a deep neural network (DNN, specifically Alexnet; <xref ref-type="bibr" rid="R19">Krizhevsky et al., 2012</xref>). This DNN consisted of seven hidden layers (<xref ref-type="fig" rid="F3">Fig. 3a</xref>), with earlier layers primarily encoding low-level visual features (e.g., colour, contrast), and deeper layers encoding more abstract, higher-level properties (e.g., shape, object identity; <xref ref-type="bibr" rid="R4">Cichy et al., 2016</xref>; <xref ref-type="bibr" rid="R13">Goldstein et al., 2022</xref>; <xref ref-type="bibr" rid="R12">Güçlü &amp; van Gerven, 2015</xref>; <xref ref-type="bibr" rid="R45">Yamins &amp; DiCarlo, 2016</xref>). We first constructed an image-by-image EEG similarity matrix at each time point, based on pairwise correlations between neural activity patterns elicited during stimulus processing. This was done for both the pre-learning and post-learning perceptual tasks (see <xref ref-type="sec" rid="S7">Methods</xref>; <xref ref-type="fig" rid="F3">Fig. 3b</xref>), and only images remembered in both the pre- and post-sleep memory tests were included in this analysis. These matrices capture the time-resolved representational geometry of visual stimuli in the brain. In parallel, we derived model-based pairwise similarity matrices for successor images based on the learned Memory Arena sequence. This was done separately for each layer of the pretrained DNN, spanning a hierarchy from low- to high-level visual representations.</p><p id="P13">Confirming the sensitivity of these analyses, we found that both EEG and DNN carried category-specific information, with higher within-category similarity than between-category similarity (<xref ref-type="fig" rid="F8">Extended Data Fig. 4</xref>). To assess the level at which the brain encoded successor information, we correlated neural similarity matrices with DNN-derived similarity matrices at each layer. We hypothesised that neural representations of successor images in the post-learning perceptual task would consist more strongly of higher-level than lower-level DNN features, reflecting a shift toward more abstract representational formats. Consistent with this prediction, comparing EEG and DNN successor similarity matrices in the post-learning perceptual task against the pre-learning perceptual task (as the baseline) revealed a trend toward increased similarity in the deep layers (layer 6: <italic>p</italic><sub>cluster</sub> = 0.077; layer 7: <italic>p</italic><sub>cluster</sub> = 0.083, two-tailed cluster-based permutation tests; <xref ref-type="fig" rid="F3">Fig. 3c</xref>) and decreased similarity in one of the earlier layers (layer 3: <italic>p</italic><sub>cluster</sub> = 0.082).</p><p id="P14">To quantify this shift towards deep layers, we averaged the change in EEG-successor similarities from pre-to post-learning perceptual tasks within the 1s-image presentation window and correlated them with layer hierarchy (ranging from 1 to 7) for each participant (‘representational shift’ analysis; <xref ref-type="fig" rid="F3">Fig. 3d</xref> and <xref ref-type="fig" rid="F9">Extended Data Fig. 5a</xref>). We found that these correlation values were significantly greater than zero (<italic>t</italic>(25) = 2.88, <italic>p</italic> = 0.007; <xref ref-type="fig" rid="F3">Fig. 3e</xref>). In other words, the change in successor representation format after learning followed the progression from superficial to deep layers in a DNN. A time-resolved analysis also revealed a significant cluster with higher correlation values compared to zero (<italic>p</italic><sub>cluster</sub> = 0.018, two-tailed cluster-based permutation tests; <xref ref-type="fig" rid="F9">Extended Data Fig. 5b</xref>).</p><p id="P15">Lastly, we conducted the same analysis using the current (instead of the successor) image. All DNN layers showed significant correlations with EEG activity (all <italic>p</italic><sub>cluster</sub> &lt; 0.001; <xref ref-type="fig" rid="F10">Extended Data Fig. 6a–b</xref>). Interestingly, in both the pre- and post-learning perceptual tasks, we observed that correlation strengths gradually declined from layers conv2 to fc7. To test this observation statistically, we again averaged similarity values across the 1-second image presentation window for each layer and for each perceptual task session. Then, for each participant, we correlated the similarity values of the six deeper layers (conv2–fc7) with the layer hierarchy (ranging from 2 to 7). This revealed a significant negative correlation (<italic>t</italic>(25) = -2.24, <italic>p</italic> = 0.033; <xref ref-type="fig" rid="F10">Extended Data Fig. 6c</xref>), indicating that lower-level visual representations of a DNN were more strongly represented in the EEG data than higher-level representations. This pattern likely reflects the perceptual demands of the task, which required visual discrimination (1-back task, <xref ref-type="fig" rid="F2">Fig. 2a</xref>). However, in contrast to successor images, no significant differences were observed for current images between the pre- and post-learning perceptual tasks (<italic>p</italic>s &gt; 0.357; <xref ref-type="fig" rid="F10">Extended Data Fig. 6d-e</xref>).</p><p id="P16">In sum, these findings suggest that learning reshapes successor representations and that these representations take on higher-level visual formats, while the sensory encoding of current images emphasises lower-level features and remains largely unaffected by learning.</p></sec><sec id="S5"><title>SWS predicts successor strength and representational shift</title><p id="P17">Lastly, we examined how sleep contributes to incorporating successor representations after learning. Nap EEG data were scored with automatic algorithms and manually validated (<xref ref-type="fig" rid="F4">Fig. 4a</xref>; see <xref ref-type="sec" rid="S7">Methods</xref>). We then derived the proportion of SWS for each participant (mean ± SD: 24.09 ± 18.07%; <xref ref-type="fig" rid="F11">Extended Data Fig. 7a</xref>) and correlated it with successor representational strength (i.e., decoding accuracy) and shift using both the image presentation midpoint (0.5s after image onset) and in a time-resolved manner. Results showed that higher proportions of SWS were associated with greater successor representational strength across participants (image presentation midpoint: Spearman’s rho = 0.44, <italic>p</italic> = 0.015, <xref ref-type="fig" rid="F4">Fig. 4b</xref>; cluster-permutation: <italic>p</italic><sub>cluster</sub> = 0.047, <xref ref-type="fig" rid="F11">Extended Data Fig. 7d</xref>) as well as with the representational shift towards high-level formats (image presentation midpoint: Spearman’s rho = 0.37, <italic>p</italic> = 0.026, <xref ref-type="fig" rid="F4">Fig. 4c</xref>; cluster-permutation: <italic>p</italic><sub>cluster</sub> = 0.013, <xref ref-type="fig" rid="F11">Extended Data Fig. 7e</xref>). Importantly, no significant effects were found for any other sleep stage (image presentation midpoint: all <italic>p</italic> &gt; 0.152; cluster-permutation: all <italic>p</italic><sub>cluster</sub> &gt; 0.145; <xref ref-type="fig" rid="F11">Extended Data Fig. 7d-e</xref>). Additional analyses ruled out the possibility that these findings were driven by correlations between memory performance and SWS or by interdependence between successor representational strength and shift (<xref ref-type="fig" rid="F11">Extended Data Fig. 7b-c</xref>).</p></sec></sec><sec id="S6" sec-type="discussion"><title>Discussion</title><p id="P18">In this study, we tracked changes in representational geometry before and after learning an image sequence (<xref ref-type="fig" rid="F1">Fig. 1</xref>). By combining high-density EEG recordings and multivariate decoding methods, we found that learning a temporal sequence leads to the incorporation of successor representations in a subsequent non-memory task. Specifically, after learning, the neural activation profiles while viewing an image contained information about its immediate sequence successor, even though sequence information was irrelevant for the current (perceptual) task. Individuals with better pre-sleep memory performance exhibited greater levels of successor integration (<xref ref-type="fig" rid="F2">Fig. 2</xref>). Comparing the representational format of successor information with different layers of a DNN suggested that low-level visual features before learning gave way to higher-level, abstract visual properties after learning (<xref ref-type="fig" rid="F3">Fig. 3</xref>). Crucially, both the strength and shift in format of successor representations were associated with the amount of SWS in a post-learning nap (<xref ref-type="fig" rid="F4">Fig. 4</xref>). Together, these findings suggest that sequence learning incorporates high-level successor representations, and that sleep facilitates this process.</p><p id="P19">Our findings extend the theoretical framework of successor representations, which were originally proposed in the context of reinforcement learning to integrate current and anticipated future states (<xref ref-type="bibr" rid="R6">Dayan, 1993</xref>). More recent work has suggested that this framework may also apply to episodic memory (<xref ref-type="bibr" rid="R10">Gershman, 2012</xref>, <xref ref-type="bibr" rid="R9">2018</xref>; <xref ref-type="bibr" rid="R25">Momennejad et al., 2017</xref>; <xref ref-type="bibr" rid="R37">Stachenfeld et al., 2017</xref>). In line with this view, our results showed robust encoding of successor—but not predecessor—images after sequence learning, even in a task that did not require memory for temporal structure (<xref ref-type="fig" rid="F2">Fig. 2b</xref>). Furthermore, the strength of successor representations correlated with immediate recall performance (after learning and before sleep; <xref ref-type="fig" rid="F2">Fig. 2c</xref>), linking behavioural expressions of sequence learning efficacy to the phenomenon of successor incorporation. The tendency to integrate successive experiences is also consistent with longstanding models of temporal organisation in episodic memory. For instance, the temporal context model (<xref ref-type="bibr" rid="R16">Howard &amp; Kahana, 2002</xref>; Polyn et al., 2009) proposes that memory retrieval is guided by a context signal that evolves over time, favouring the recall of temporally subsequent items. Recent computational work has integrated this model with successor representation theory to better capture the predictive structure of memory-guided behaviour (<xref ref-type="bibr" rid="R46">Zhou et al., 2025</xref>). Complementing these behavioural and computational accounts, recent human intracranial EEG studies have shown that neurons in the medial temporal lobe encode the temporal structure of visual experiences (<xref ref-type="bibr" rid="R39">Tacikowski et al., 2024</xref>; <xref ref-type="bibr" rid="R17">John et al., 2025</xref>; <xref ref-type="bibr" rid="R42">Umbach et al., 2020</xref>). Although these studies have not distinguished between forward- and backward adjacency, our findings raise the possibility that these temporal codes may primarily reflect forward (successor) information. Future work using intracranial or single-unit recordings could test this hypothesis more directly.</p><p id="P20">To better understand the format of successor representations, we resorted to tools commonly used in vision neuroscience. Specifically, over the past decade, advances in convolutional DNNs in the domain of computer vision (e.g., Alexnet; <xref ref-type="bibr" rid="R19">Krizhevsky et al., 2012</xref>) and natural language processing (e.g., GPT-2; <xref ref-type="bibr" rid="R30">Radford et al., 2019</xref>) have provided powerful tools for understanding how the brain organizes complex information. After training, these DNNs form hierarchical representations that progressively transition from low-level sensory features to increasingly abstract, high-level properties—paralleling the structure of human cortical processing (<xref ref-type="bibr" rid="R4">Cichy et al., 2016</xref>; <xref ref-type="bibr" rid="R13">Goldstein et al., 2022</xref>; <xref ref-type="bibr" rid="R12">Güçlü &amp; van Gerven, 2015</xref>; <xref ref-type="bibr" rid="R45">Yamins &amp; DiCarlo, 2016</xref>). Using representational similarity analysis (<xref ref-type="bibr" rid="R18">Kriegeskorte et al., 2008</xref>), we assessed the alignment of neural activity with different latent layers of Alexnet. Our results revealed a shift in successor representational format: after learning, neural activity patterns became less aligned with low-level visual features and more aligned with high-level, abstract properties (<xref ref-type="fig" rid="F3">Fig. 3b–d</xref>). These findings suggest that learning not only incorporates successor representation but also reorganizes it into a more conceptual format. The observed shift of successor representation complements and extends prior intracranial EEG studies, which revealed a transformation of currently displayed image representations from perceptual to conceptual formats with repeated exposure in short- as well as long-term memory tasks (<xref ref-type="bibr" rid="R22">Liu et al., 2020</xref>, <xref ref-type="bibr" rid="R23">2021</xref>; <xref ref-type="bibr" rid="R31">Rau et al., 2025</xref>). It further aligns with theories proposing abstraction as a key feature of memory consolidation (<xref ref-type="bibr" rid="R3">Buzsáki &amp; Tingley, 2018</xref>; <xref ref-type="bibr" rid="R5">Cowan et al., 2021</xref>; <xref ref-type="bibr" rid="R11">Gilboa &amp; Moscovitch, 2021</xref>; <xref ref-type="bibr" rid="R20">Kumaran et al., 2016</xref>; <xref ref-type="bibr" rid="R26">Nieh et al., 2021</xref>; <xref ref-type="bibr" rid="R33">Roüast &amp; Schönauer, 2023</xref>).</p><p id="P21">Intriguingly, we found that the learning-induced changes in neural representations were modulated by sleep. Specifically, the proportion of SWS during a post-learning nap predicted both the strength of successor representations and the extent of their abstraction. These results are consistent with previous work highlighting SWS as a critical window for neural replay, memory reactivation, and representational reorganisation (<xref ref-type="bibr" rid="R2">Buzsáki, 2015</xref>; <xref ref-type="bibr" rid="R8">Diekelmann &amp; Born, 2010</xref>; <xref ref-type="bibr" rid="R24">Mölle et al., 2004</xref>; <xref ref-type="bibr" rid="R27">Niknazar et al., 2022</xref>; <xref ref-type="bibr" rid="R44">Wilson &amp; McNaughton, 1994</xref>). Although our study did not include a wake control group, recent work offers compelling support for a sleep-specific effect. A study by <xref ref-type="bibr" rid="R7">Diamond et al. (2025)</xref> found that participants who slept after real-life sequential learning (a guided art tour) exhibited enhanced temporal order memory but a loss of low-level perceptual detail in behaviour—an effect that was specifically linked to SWS.</p><p id="P22">This pattern closely parallels our representational-level findings, suggesting that SWS may bias memory consolidation towards abstract, temporally structured information. At the mechanistic level, slow oscillations during SWS are thought to coordinate the timing of sleep spindles and hippocampal ripples (<xref ref-type="bibr" rid="R21">Latchoumane et al., 2017</xref>; <xref ref-type="bibr" rid="R38">Staresina, 2024</xref>), which in turn supports memory reactivation and consolidation (<xref ref-type="bibr" rid="R36">Schreiner et al., 2021</xref>, <xref ref-type="bibr" rid="R35">2024</xref>). This coordinated neural activity may underlie the observed transformation of successor representations. Future studies employing intracranial EEG could directly test whether such cross-frequency coupling facilitates the integration of predictive structures into long-term memory networks.</p><p id="P23">More broadly, our findings shed light on how the brain updates its internal model of the world. Successor representations provide a mechanism by which experience can reshape predictive structures, allowing individuals to anticipate future events based on learned regularities. This predictive capacity is crucial in everyday contexts where behaviour unfolds over time. An open question in our study, however, concerns the longevity of these representations. Do successor representations persist over time, or do they eventually “wash out” and revert to more veridical representations in the absence of repeated/continued learning? Investigating the durability and stability of these effects over longer timescales remains an important direction for future work.</p><p id="P24">In conclusion, our findings show that sequential learning induces successor representations in the human brain, even in a perceptual task unrelated to sequential information. These representations shift from perceptual to abstract formats and are supported by post-learning sleep, particularly slow-wave sleep. Together, these results advance our understanding of how the human brain integrates temporal structure into our internal world model.</p></sec><sec id="S7" sec-type="methods"><title>Methods</title><sec id="S8" sec-type="subjects"><title>Participants</title><p id="P25">Thirty healthy adults (9 males; mean age = 25 years; range = 19–39 years) participated in the study. They received either course credit or monetary compensation for their participation. To ensure sleep-impairing factors did not confound the results, participants were screened for the following exclusion criteria: engagement in night shift work within the past year, recent travel across time zones (within the past two weeks), current use of medications affecting sleep, any history of neurological, psychiatric, or sleep disorders and regular consumption of more than one cigarette per day. Sample outliers were detected using MATLAB’s <italic>isoutlier()</italic> function applied to performance during the final learning block and pre-sleep accuracy scores. This resulted exclusion of 4 participants and a final sample of 26 participants included in the reported analyses. All participants provided written informed consent, and the study was approved by the University of Oxford’s ethics committee (approval code: R85832/RE001).</p></sec><sec id="S9"><title>Experimental design</title><p id="P26">Participants first performed a perceptual task including 50 unique images, followed by the ‘Memory Arena’ task presenting the same images in fixed temporal and spatial sequences (<xref ref-type="bibr" rid="R1">Büchel et al., 2024</xref>; for an earlier version of the task, see also <xref ref-type="bibr" rid="R29">Petzka et al., 2021</xref>). After reaching a learning criterion, participants took a ~2-hour nap. Memory accuracy was tested before and after sleep, followed by a repetition of the perceptual task. EEG recordings were applied throughout the experiment.</p><p id="P27">The perceptual task included 50 images from five categories (objects, faces, scenes, letter strings, and body parts). Specifically, participants performed a one-back repetition detection task, pressing the ‘down arrow’ key whenever an image was repeated. Each session comprised 660 trials (including 10% repeated target trials), and participants performed the task with high accuracy (mean ± SD: 98.32 ± 0.74%). For subsequent analyses, only correct trials involving non-repeated images were included. Importantly, the image presentation order in the two perceptual task sessions was pseudo-randomised and unrelated to the sequences learned in the Memory Arena task, enabling us to isolate the effects of sequence learning on visual brain representations.</p><p id="P28">In the Memory Arena task, participants learned the sequential and spatial structure of 50 images across repeated learning cycles (<xref ref-type="fig" rid="F5">Extended Data Fig. 1</xref>, detailed in <xref ref-type="bibr" rid="R1">Büchel et al., 2024</xref>). These 50 images were organized into 10 subsequences of five images each, following one of two fixed category orders: (i) letter string, scene, object, face, or (ii) object, scene, letter string, face, with body part images randomly inserted to obscure the primary category sequences. The two subsequence types were counterbalanced across participants. Each learning cycle consisted of two exposure blocks, where images were presented sequentially at their respective locations, followed by a test block in which participants reconstructed the sequence and spatial layout. Learning continued until participants reached at least 66% accuracy in selecting correctly ordered image pairs during a test block or a maximum of 60 minutes had elapsed.</p><p id="P29">Following the learning phase, participants performed a 5-minute attention task. They were instructed to fixate on a central cross and count each instance it turned dark grey, while ignoring instances where it turned light grey. After that, participants completed the same memory test as in the Memory Arena task, which served as an assessment of pre-sleep memory performance. They were then given a 2-hour nap opportunity (mean sleep duration ± SD: 71 ± 28 minutes), during which polysomnographic data were recorded to monitor sleep stages. Upon waking, participants engaged in an interference task. In this phase, they re-encoded the same set of 50 images, but with both the sequence and spatial locations altered. Each image’s new location differed from its original location by at least 5 pixels (Euclidean distance). The encoding procedure for this new configuration mirrored that of the initial learning phase, but consisted of only one round. Following the interference encoding, participants were asked to recall the new sequence and spatial arrangement. Next, they completed a memory test to retrieve the originally learned sequence and layout, which served as an assessment of post-sleep memory performance. This post-sleep assessment was followed by a final repetition of the perceptual task.</p></sec><sec id="S10"><title>EEG recording and preprocessing</title><p id="P30">EEG data were recorded using a 64-channel Brain Products system at a sampling rate of 500 Hz. Electrodes were positioned according to the international 10–20 system, with FCz as the recording reference and AFz as the ground. Six electrodes were reassigned for auxiliary recordings: two for mastoids, two for electromyography (EMG), and two for electrooculography (EOG). This configuration left 58 channels available for scalp EEG recording. Our analyses focused on the EEG data collected during the perceptual task sessions.</p><p id="P31">Preprocessing was conducted using the following procedures: First, data were downsampled to 250 Hz and high-pass filtered at 0.1 Hz. Line noise in the 49–51 Hz range was removed, and data were re-referenced to the common average. Channels identified as noisy during visual inspection were interpolated; in total, 11 bad channels were replaced across 26 participants.</p><p id="P32">The filtered EEG signal was then segmented into epochs ranging from 500 ms before to 1500 ms after image onset. Independent component analysis (ICA) was performed on these epochs to identify and remove components reflecting eye blinks, based on visual inspection. The cleaned data were smoothed using a 200 ms moving mean window (MATLAB’s <italic>smoothdata</italic> function), baseline corrected using the 200 ms pre-stimulus interval, and finally z-scored across trials using MATLAB’s <italic>normalize</italic> function.</p></sec><sec id="S11"><title>Successor representation decoding</title><p id="P33">To investigate whether sequence learning changes visual brain representations, we conducted multivariate pattern classification analyses using EEG voltage signals from all channels. Within each participant, we first averaged the EEG signal in sliding time windows of 20 ms (five data points) with a step size of 12 ms (three data points). The resulting voltage patterns across all channels served as input features for a multiclass linear discriminant analysis (LDA), implemented using the MVPA-Light toolbox (<xref ref-type="bibr" rid="R41">Treder, 2020</xref>).</p><p id="P34">To assess whether learning induced representations that incorporated sequential structure—such as successors—we examined how well neural activity during post-learning perceptual task trials reflected category information from nearby positions in the learned Memory Arena sequence. For each category (e.g., objects), we trained the classifier using EEG data from the other four categories (e.g., body parts, letter strings, faces, and scenes) in the pre-learning perceptual task session and tested it on untrained object images in the post-learning perceptual task session. This cross-session generalisation design allowed us to identify changes in neural representations that emerged specifically as a result of learning, while minimizing concerns of overfitting or confounding temporal proximity effects. We then compared the predicted category label with the category of neighbouring items in the sequence participants had learned during the Memory Arena task. For instance, if participants had learned a subsequence such as “letter string → object → face,” we examined whether EEG patterns during object trials were more likely to be classified as the successor category (face) or the predecessor category (letter string). This procedure yielded an accuracy estimate for each time point that the decoded category matched the immediate predecessor or successor in the learned sequence. These accuracies were averaged across trials to generate a time-by-time matrix of classifier predictions, which were then compared against the chance level (25%) using cluster-based permutation statistical tests (see <xref ref-type="sec" rid="S14">Statistical Analysis</xref>). We also extended this analysis to more distal sequence positions—examining decoding accuracies for the second predecessor and successor—to further probe the spread of sequence-related representations (<xref ref-type="fig" rid="F6">Extended Data Fig. 2a</xref>).</p><p id="P35">To relate successor representation to memory performance, we correlated decoding accuracies with pre- or post-sleep sequential memory accuracies across participants at each training and test time point.</p><p id="P36">Lastly, to confirm that particular effects emerged as a consequence of learning, we applied the same decoding procedures to pre-learning perceptual task data. Successor decoding accuracies from the pre-learning perceptual task session were then compared to those from the post-learning perceptual task session using the same statistical approach. The absence of sequence-based classification patterns prior to learning, along with a significant increase in successor decoding after learning, served as evidence that the observed representational changes were specifically induced by sequence learning occurring in between (<xref ref-type="fig" rid="F6">Extended Data Fig. 2c</xref>).</p></sec><sec id="S12"><title>Representational similarity analysis</title><p id="P37">To investigate the format of the successor information, we compared neural representations with those derived from a DNN (Alexnet; <xref ref-type="bibr" rid="R19">Krizhevsky et al., 2012</xref>). Alexnet is pretrained on the ImageNet dataset to perform object classification, and has been shown to exhibit a hierarchical organisation of visual representations similar to the ventral visual stream in humans and other primates (<xref ref-type="bibr" rid="R4">Cichy et al., 2016</xref>; <xref ref-type="bibr" rid="R13">Goldstein et al., 2022</xref>; <xref ref-type="bibr" rid="R12">Güçlü &amp; van Gerven, 2015</xref>; <xref ref-type="bibr" rid="R45">Yamins &amp; DiCarlo, 2016</xref>). Specifically, earlier layers (e.g., conv1–conv3) predominantly encode low-level features such as edges and textures, whereas deeper layers (e.g., fc6–fc7) capture increasingly abstract and categorical information (<xref ref-type="bibr" rid="R45">Yamins &amp; DiCarlo, 2016</xref>).</p><p id="P38">For each image used in the experiment, we extracted activation patterns (expressed as feature vectors) from each layer of Alexnet. These patterns were used to construct representational similarity matrices by computing pairwise Spearman’s correlations between feature vectors across images. Two types of DNN similarity matrices were created: <list list-type="order" id="L1"><list-item><p id="P39">DNN current similarity matrices: computed using the features of the currently displayed image.</p></list-item><list-item><p id="P40">DNN successor similarity matrices: computed using the features of each image’s successor (i.e., the image that followed in the learned sequence), rather than the image itself.</p></list-item></list>
</p><p id="P41">After deriving DNN similarity matrices, we next computed EEG-based similarity matrices. For each participant, we selected images whose successors were correctly identified in both memory tests, ensuring reliable recall. Using these remembered images, we computed pairwise Spearman’s correlations of EEG voltage patterns across channels, using the same sliding time window approach as in our decoding analyses.</p><p id="P42">To validate these similarity matrices, we assessed category-level fidelity by comparing within-and between-category similarities. Both DNN and EEG matrices showed significantly higher within-category similarity, confirming that category-level structure was present (<xref ref-type="fig" rid="F8">Extended Data Fig. 4</xref>).</p><p id="P43">We next assessed the correspondence between EEG and DNN representations using the same subset of images for each participant’s analyses. First, we examined the correlation between EEG and the DNN current similarity matrix for each layer (EEG–current similarity). This analysis replicated prior findings showing a strong alignment between EEG activity during perception and DNN-derived representations (<xref ref-type="fig" rid="F10">Extended Data Fig. 6a–b</xref>; <xref ref-type="bibr" rid="R22">Liu et al., 2020</xref>, <xref ref-type="bibr" rid="R23">2021</xref>; <xref ref-type="bibr" rid="R31">Rau et al., 2025</xref>). To evaluate whether EEG activity differentially reflected low- or high-level DNN features, we first averaged, for each participant, EEG–current similarity across the two perceptual task sessions and across the 1-second image presentation window. Then the averaged layer-specific EEG–current similarity was correlated with DNN layer hierarchy (ranks of conv2 to fc7). Positive values indicated stronger alignment with higher-level layers, and negative values indicated alignment with lower-level layers. Correlations were Fisher z-transformed and correlation values equal to ±1 were winsorized to ±0.99 to avoid infinite values during Fisher z-transformation.</p><p id="P44">To examine how learning influenced neural representations, we assessed the similarity between EEG and DNN successor representations (EEG-successor similarity). To minimize the influence of current image representations in the EEG data, we regressed out the DNN current matrices from the EEG similarity matrices and correlated the residuals with the DNN successor similarity matrices. This correspondence was compared before and after learning for each DNN layer, using cluster-based permutation tests to correct for multiple comparisons across time.</p><p id="P45">Additionally, to quantify learning-related changes during stimulus presentation, we averaged the similarity differences across the 0–1 s post-onset window.</p><p id="P46">Finally, to quantify representational shifts across the DNN hierarchy (conv1 to fc7), we correlated the magnitude of EEG–successor similarity changes with layer hierarchy (ranks 1 to 7). This was done both across the entire time window and for the average within that window. As before, correlations were Fisher z-transformed and winsorized to ±0.99.</p></sec><sec id="S13"><title>Sleep architecture and its relation to successor representations</title><p id="P47">Sleep staging for each participant was performed based on 30-second epochs, using EEG polysomnographic recordings in accordance with the American Academy of Sleep Medicine (AASM) guidelines. Stages included wakefulness, NREM1, NREM2, NREM3 (SWS), and REM sleep. Initial automated staging was conducted using two AI-based tools, YASA (<xref ref-type="bibr" rid="R43">Vallat &amp; Walker, 2021</xref>) and Somnobot (<ext-link ext-link-type="uri" xlink:href="https://somnobot.fh-aachen.de">https://somnobot.fh-aachen.de</ext-link>). The transitions between stages and any discrepancies between the two automated outputs were then carefully reviewed by two experienced sleep researchers, who finalized the classification of each epoch.</p><p id="P48">Following staging, we calculated the proportion of time each participant spent in each sleep stage. To examine the relationship between sleep architecture and successor representations, we correlated the proportion of each stage with (i) the successor representational strength (decoding accuracies) obtained from the decoding analysis and (ii) the successor representational shift obtained from the representational similarity analysis.</p></sec><sec id="S14"><title>Statistical analysis</title><p id="P49">To assess the temporal profile of successor representations, we used cluster-based permutation testing (implemented in FieldTrip; <xref ref-type="bibr" rid="R28">Oostenveld et al., 2011</xref>) to correct for multiple comparisons across time in both decoding and representational similarity analyses. This nonparametric approach identifies clusters of consecutive time points that exceed a predefined threshold (here: <italic>p</italic> &lt; .05), with cluster-level significance evaluated through 2,000 random permutations of the data. For decoding analyses, we used one-tailed tests, as only above-chance effects were theoretically meaningful. For representational similarity analyses, we used two-tailed tests to allow for both positive and negative effects.</p><p id="P50">To examine the relationship between representational changes and behavioural performance or sleep architecture, we computed correlations between the magnitude of representational change and either sequential memory accuracy (pre- and post-sleep) or the proportion of time spent in each sleep stage, across participants. Correlations were computed at each time point, and the resulting statistical maps were corrected for multiple comparisons using the same cluster-based permutation procedure. We used one-tailed testing for these analyses, based on the hypothesis that better memory performance or a greater proportion of time in slow-wave sleep would be associated with stronger neural representations.</p><p id="P51">For clearer visualisation and summary of the correlation analyses, we additionally focused on an a priori defined midpoint of the image presentation interval—500 ms after image onset. This time point has previously been associated with both perceptual and conceptual processing (<xref ref-type="bibr" rid="R23">Liu et al., 2021</xref>) and thus represented an unbiased choice for evaluating perceptual/conceptual features. At this time point, we evaluated correlation significance using a permutation procedure designed to account for variability in small samples. Specifically, we generated a null distribution by randomly shuffling the correspondence between the two variables across participants 2,000 times and determined p-values based on the proportion of permutations in which the permuted correlation exceeded the observed value.</p></sec></sec><sec id="S15" sec-type="extended-data"><title>Extended Data</title><fig id="F5" position="anchor"><label>Extended Data Figure 1</label><caption><title>Task and behavioural results.</title><p id="P52"><bold>(a)</bold> Memory Arena sequence design. Participants (N = 26) were tasked with learning the spatiotemporal structure of 50 images. These images belonged to five distinct categories (letter strings, scenes, objects, faces, and body parts) and were organized into 10 subsequences of five images each, following one of two fixed category orders: (i) letter string, scene, object, face, or (ii) object, scene, letter string, face, with body part images randomly inserted to obscure the primary category sequences. The two subsequence types were counterbalanced across participants; <bold>(b)</bold> Memory Arena location design. The Arena was spatially organized into five principal ‘slices’, with each slice corresponding to one of the five main image categories. Each main slice was subdivided into two sub-slices, corresponding to the two subcategories of the main category; <bold>(c)</bold> Exemplary learning trial. Images were presented sequentially at their unique positions. Participants were asked to learn the image and its location; <bold>(d)</bold> Exemplary test trial. All 50 images were randomly distributed around the Arena. Participants selected each image by clicking on it and then click exact position within the arena where they remembered the image had been located. Examples of face stimuli shown in panels a-d were sourced from an AI-generated dataset (<ext-link ext-link-type="uri" xlink:href="https://github.com/RichardErkhov/ai_generated_faces">https://github.com/RichardErkhov/ai_generated_faces</ext-link>), used under the MIT License; <bold>(e)</bold> Memory accuracy in pre- and post-sleep test. Each dot represents one participant. ** indicates significant difference in memory accuracy between pre- and post-sleep test (<italic>p</italic> = 0.002, N = 26, two-tailed paired <italic>t</italic>-test).</p></caption><graphic xlink:href="EMS206940-f005"/></fig><fig id="F6" position="anchor"><label>Extended Data Figure 2</label><caption><title>Decoding results of second predecessor and successor.</title><p id="P53"><bold>(a)</bold> Group-level time-by-time decoding accuracies of second predecessor (left) and second successor (right) images across participants (N = 26). Decoders were trained in each time point in the pre-learning perceptual task and were then tested in the post-learning perceptual task; <bold>(b)</bold> Comparison of successor decoding accuracies between the pre- and post-learning sessions. Black contour indicates significantly higher correlations than zero (<italic>p</italic><sub>cluster</sub> = 0.034, N = 26, one-tailed cluster permutation tests).</p></caption><graphic xlink:href="EMS206940-f006"/></fig><fig id="F7" position="anchor"><label>Extended Data Figure 3</label><caption><title>Correlation between successor representational strength and behaviour.</title><p id="P54"><bold>(a)</bold> Correlation between successor decoding accuracy and pre-sleep memory accuracy across participants. Black contour indicates significantly higher correlations than zero (<italic>p</italic><sub>cluster</sub> = 0.025, N = 26, one-tailed cluster permutation tests); <bold>(b)</bold> Correlation between successor decoding accuracy and post-sleep memory accuracy (<italic>p</italic><sub>cluster</sub> &gt; 0.281, N = 26).</p></caption><graphic xlink:href="EMS206940-f007"/></fig><fig id="F8" position="anchor"><label>Extended Data Figure 4</label><caption><title>Category representation in DNN and EEG.</title><p id="P55"><bold>(a)</bold> DNN category-specific information (within-category similarity minus between-category similarity) across hidden layers. Error bars indicate standard error across categories (n = 5); <bold>(b)</bold> Neural category-specific information in the perceptual task. Shaded areas indicate standard error across participants. Horizontal lines indicate significant clusters in time (<italic>p</italic><sub>cluster</sub> &lt; 0.001, N = 26, two-tailed cluster permutation tests).</p></caption><graphic xlink:href="EMS206940-f008"/></fig><fig id="F9" position="anchor"><label>Extended Data Figure 5</label><caption><title>Representational changes between EEG and DNN for successor images.</title><p id="P56"><bold>(a)</bold> Individual participant changes in EEG-successor similarity across DNN layers, averaged across the 0-1s time window. Each line represents one participant (from yellow to blue: conv1, conv2, conv3, conv4, conv5, fc6, fc7); <bold>(b)</bold> Time-resolved correlation between layer hierarchy and representational changes. Shading indicates standard error across participants. Correlations were Fisher z-transformed for statistical testing, with extreme values (&gt;0.99 or &lt;-0.99) winsorized. Horizontal line indicates significant clusters in time (<italic>p</italic><sub>cluster</sub> = 0.018, N =26, two-tailed cluster permutation tests).</p></caption><graphic xlink:href="EMS206940-f009"/></fig><fig id="F10" position="anchor"><label>Extended Data Figure 6</label><caption><title>Representational similarity analyses between EEG and DNN for currently displayed images.</title><p id="P57"><bold>(a)</bold> Spearman’s correlations (EEG-current similarity) between EEG similarity matrix in the pre-learning session and DNN current similarity matrices of each layer (from yellow to blue: conv1, conv2, conv3, conv4, conv5, fc6, fc7). Horizontal lines indicate significant correlations (<italic>p</italic><sub>cluster</sub> &lt; 0.001, N = 26, two-tailed cluster permutation tests); <bold>(b)</bold> EEG-current similarity in the post-learning session (<italic>p</italic><sub>cluster</sub> &lt; 0.001, N = 26, two-tailed cluster permutation tests); <bold>(c)</bold> Average EEG-current similarity across the two sessions. Bar plot shows mean similarity for each DNN layer in the 0-1s time window. Error bars indicate standard error across participants. Inset plot shows the correlations between average similarity and layer hierarchy (from conv2 to fc7). Correlations were Fisher z-transformed for statistical testing, with extreme values (&gt;0.99 or &lt;-0.99) winsorized. * indicates significant difference from zero (<italic>p</italic> = 0.033, two-tailed t-test); <bold>(d)</bold> EEG-current similarity change from pre- to post-learning perceptual task across time. No significant difference was found between the two sessions (<italic>p</italic><sub>cluster</sub> &gt; 0.800, N = 26, two-tailed cluster permutation tests); <bold>(e)</bold> Average EEG-current similarity change within the 1-second image presentation time window. Inset plot shows the correlations between average similarity change and layer hierarchy (from conv1 to fc7). <italic>n</italic>.<italic>s</italic>. indicates non-significant (<italic>p</italic> = 0.357, two-tailed t-test); <bold>(f)</bold> Time-resolved correlation between layer hierarchy and similarity changes (representational shift). Shaded areas indicate standard error across participants. No significant difference was found between correlation values and zero (<italic>p</italic><sub>cluster</sub> &gt; 0.800, N = 26, two-tailed cluster permutation tests).</p></caption><graphic xlink:href="EMS206940-f010"/></fig><fig id="F11" position="anchor"><label>Extended Data Figure 7</label><caption><title>Correlations between successor representational changes and the proportions of sleep stages.</title><p id="P58"><bold>(a)</bold> The proportions of all sleep stages during an approximately 2-hour nap across participants. Each dot represents a participant; <bold>(b)</bold> Correlation between pre-sleep memory accuracy and slow-wave sleep proportion (NREM3; <bold>(c)</bold> Correlation between successor representational strength and shift. The solid line in panels b and c shows the best-fit linear regression line. The dashed lines indicate the 95% confidence bounds for the fitted line; <bold>(d)</bold> Correlation between the proportions of sleep stages and successor representational strength (from left to right: NREM1, NREM2, NREM3, and REM). Black clusters indicate significant correlation (NREM3: <italic>p</italic><sub>cluster</sub> = 0.047; Other stages: <italic>p</italic><sub>cluster</sub> &gt; 0.145; N = 26, one-tailed cluster permutation tests); <bold>(e)</bold> Correlation between the proportions of sleep stages and successor representational shift (from left to right: NREM1, NREM2, NREM3, and REM). Dashed line indicates 95% CI of 1,000 bootstrapping samples. Horizontal lines indicate significant correlation (NREM3: <italic>p</italic><sub>cluster</sub> = 0.013; Other stages: <italic>p</italic><sub>cluster</sub> &gt; 0.341; N = 26, one-tailed cluster permutation tests).</p></caption><graphic xlink:href="EMS206940-f011"/></fig></sec></body><back><ack id="S16"><title>Acknowledgements</title><p>This project was funded by the European Research Council (ERC) under the European Union’s Horizon 2020 (grant agreement no. 101001121) awarded to B.P.S and was supported by Medical Sciences Graduate School Studentship at the University of Oxford awarded to X.H. as well as funding support from The Royal Society (project NIF\R1\221006).</p></ack><sec id="S17" sec-type="data-availability"><title>Data and code availability</title><p id="P59">Data and code for this project will be available upon publication on the Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/utdgv/">https://osf.io/utdgv/</ext-link>).</p></sec><fn-group><fn fn-type="con" id="FN1"><p id="P60"><bold>Author contributions</bold></p><p id="P61">Conceptualisation, X.H. and B.P.S.; methodology, X.H., S.F.S., M.K., P.K.B., J.K., and B.P.S.; validation, X.H.; formal analysis, X.H.; investigation, X.H.; writing – original draft, X.H.; writing – review &amp; editing, X.H., S.F.S., M.K., P.K.B., and B.P.S.; supervision, funding acquisition, B.P.S.</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Büchel</surname><given-names>PK</given-names></name><name><surname>Klingspohr</surname><given-names>J</given-names></name><name><surname>Kehl</surname><given-names>MS</given-names></name><name><surname>Staresina</surname><given-names>BP</given-names></name></person-group><article-title>Brain and eye movement dynamics track the transition from learning to memory-guided action</article-title><source>Current Biology</source><year>2024</year><volume>34</volume><issue>21</issue><fpage>5054</fpage><lpage>5061</lpage><elocation-id>e4</elocation-id><pub-id pub-id-type="pmid">39437781</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname><given-names>G</given-names></name></person-group><article-title>Hippocampal sharp wave-ripple: A cognitive biomarker for episodic memory and planning</article-title><source>Hippocampus</source><year>2015</year><volume>25</volume><issue>10</issue><fpage>1073</fpage><lpage>1188</lpage><pub-id pub-id-type="doi">10.1002/hipo.22488</pub-id><pub-id pub-id-type="pmcid">PMC4648295</pub-id><pub-id pub-id-type="pmid">26135716</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname><given-names>G</given-names></name><name><surname>Tingley</surname><given-names>D</given-names></name></person-group><article-title>Space and Time: The Hippocampus as a Sequence Generator</article-title><source>Trends in Cognitive Sciences</source><year>2018</year><volume>22</volume><issue>10</issue><fpage>853</fpage><lpage>869</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2018.07.006</pub-id><pub-id pub-id-type="pmcid">PMC6166479</pub-id><pub-id pub-id-type="pmid">30266146</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Khosla</surname><given-names>A</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Torralba</surname><given-names>A</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><article-title>Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence</article-title><source>Scientific Reports</source><year>2016</year><volume>6</volume><issue>1</issue><elocation-id>27755</elocation-id><pub-id pub-id-type="doi">10.1038/srep27755</pub-id><pub-id pub-id-type="pmcid">PMC4901271</pub-id><pub-id pub-id-type="pmid">27282108</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cowan</surname><given-names>ET</given-names></name><name><surname>Schapiro</surname><given-names>AC</given-names></name><name><surname>Dunsmoor</surname><given-names>JE</given-names></name><name><surname>Murty</surname><given-names>VP</given-names></name></person-group><article-title>Memory consolidation as an adaptive process</article-title><source>Psychonomic Bulletin &amp; Review</source><year>2021</year><volume>28</volume><issue>6</issue><fpage>1796</fpage><lpage>1810</lpage><pub-id pub-id-type="pmid">34327677</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><article-title>Improving generalization for temporal difference learning: The successor representation</article-title><source>Neural Computation</source><year>1993</year><volume>5</volume><issue>4</issue><fpage>613</fpage><lpage>624</lpage><pub-id pub-id-type="doi">10.1162/neco.1993.5.4.613</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diamond</surname><given-names>NB</given-names></name><name><surname>Simpson</surname><given-names>S</given-names></name><name><surname>Baena</surname><given-names>D</given-names></name><name><surname>Murray</surname><given-names>B</given-names></name><name><surname>Fogel</surname><given-names>S</given-names></name><name><surname>Levine</surname><given-names>B</given-names></name></person-group><article-title>Sleep selectively and durably enhances memory for the sequence of real-world experiences</article-title><source>Nature Human Behaviour</source><year>2025</year><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="pmid">40069368</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diekelmann</surname><given-names>S</given-names></name><name><surname>Born</surname><given-names>J</given-names></name></person-group><article-title>The memory function of sleep</article-title><source>Nature Reviews Neuroscience</source><year>2010</year><volume>11</volume><issue>2</issue><fpage>114</fpage><lpage>126</lpage><pub-id pub-id-type="pmid">20046194</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><article-title>The Successor Representation: Its Computational Logic and Neural Substrates</article-title><source>Journal of Neuroscience</source><year>2018</year><volume>38</volume><issue>33</issue><fpage>7193</fpage><lpage>7200</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0151-18.2018</pub-id><pub-id pub-id-type="pmcid">PMC6096039</pub-id><pub-id pub-id-type="pmid">30006364</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Moore</surname><given-names>CD</given-names></name><name><surname>Todd</surname><given-names>MT</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name><name><surname>Sederberg</surname><given-names>PB</given-names></name></person-group><article-title>The successor representation and temporal context</article-title><source>Neural Computation</source><year>2012</year><volume>24</volume><issue>6</issue><fpage>1553</fpage><lpage>1568</lpage><pub-id pub-id-type="pmid">22364500</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilboa</surname><given-names>A</given-names></name><name><surname>Moscovitch</surname><given-names>M</given-names></name></person-group><article-title>No consolidation without representation: Correspondence between neural and psychological representations in recent and remote memory</article-title><source>Neuron</source><year>2021</year><volume>109</volume><issue>14</issue><fpage>2239</fpage><lpage>2255</lpage><pub-id pub-id-type="pmid">34015252</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Güçlü</surname><given-names>U</given-names></name><name><surname>van Gerven</surname><given-names>MAJ</given-names></name></person-group><article-title>Deep Neural Networks Reveal a Gradient in the Complexity of Neural Representations across the Ventral Stream</article-title><source>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</source><year>2015</year><volume>35</volume><issue>27</issue><fpage>10005</fpage><lpage>10014</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5023-14.2015</pub-id><pub-id pub-id-type="pmcid">PMC6605414</pub-id><pub-id pub-id-type="pmid">26157000</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldstein</surname><given-names>A</given-names></name><name><surname>Zada</surname><given-names>Z</given-names></name><name><surname>Buchnik</surname><given-names>E</given-names></name><name><surname>Schain</surname><given-names>M</given-names></name><name><surname>Price</surname><given-names>A</given-names></name><name><surname>Aubrey</surname><given-names>B</given-names></name><name><surname>Nastase</surname><given-names>SA</given-names></name><name><surname>Feder</surname><given-names>A</given-names></name><name><surname>Emanuel</surname><given-names>D</given-names></name><name><surname>Cohen</surname><given-names>A</given-names></name><name><surname>Jansen</surname><given-names>A</given-names></name><etal/></person-group><article-title>Shared computational principles for language processing in humans and deep language models</article-title><source>Nature Neuroscience</source><year>2022</year><volume>25</volume><issue>3</issue><fpage>369</fpage><lpage>380</lpage><pub-id pub-id-type="doi">10.1038/s41593-022-01026-4</pub-id><pub-id pub-id-type="pmcid">PMC8904253</pub-id><pub-id pub-id-type="pmid">35260860</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greco</surname><given-names>A</given-names></name><name><surname>Moser</surname><given-names>J</given-names></name><name><surname>Preissl</surname><given-names>H</given-names></name><name><surname>Siegel</surname><given-names>M</given-names></name></person-group><article-title>Predictive learning shapes the representational geometry of the human brain</article-title><source>Nature Communications</source><year>2024</year><volume>15</volume><issue>1</issue><elocation-id>9670</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-024-54032-4</pub-id><pub-id pub-id-type="pmcid">PMC11549346</pub-id><pub-id pub-id-type="pmid">39516221</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hindy</surname><given-names>NC</given-names></name><name><surname>Ng</surname><given-names>FY</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name></person-group><article-title>Linking pattern completion in the hippocampus to predictive coding in visual cortex</article-title><source>Nature Neuroscience</source><year>2016</year><volume>19</volume><issue>5</issue><fpage>665</fpage><lpage>667</lpage><pub-id pub-id-type="doi">10.1038/nn.4284</pub-id><pub-id pub-id-type="pmcid">PMC4948994</pub-id><pub-id pub-id-type="pmid">27065363</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howard</surname><given-names>MW</given-names></name><name><surname>Kahana</surname><given-names>MJ</given-names></name></person-group><article-title>A Distributed Representation of Temporal Context</article-title><source>Journal of Mathematical Psychology</source><year>2002</year><volume>46</volume><issue>3</issue><fpage>269</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1006/jmps.2001.1388</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>John</surname><given-names>T</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name><name><surname>Aljishi</surname><given-names>A</given-names></name><name><surname>Rieck</surname><given-names>B</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name><name><surname>Damisah</surname><given-names>EC</given-names></name></person-group><article-title>Representation of visual sequences in the tuning and topology of neuronal activity in the human hippocampus</article-title><source>bioRxiv</source><year>2025</year><elocation-id>2025.03.04.641300</elocation-id><pub-id pub-id-type="doi">10.1101/2025.03.04.641300</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name></person-group><article-title>Representational similarity analysis—Connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><year>2008</year><volume>2</volume><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id><pub-id pub-id-type="pmcid">PMC2605405</pub-id><pub-id pub-id-type="pmid">19104670</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><article-title>ImageNet Classification with Deep Convolutional Neural Networks</article-title><person-group person-group-type="editor"><name><surname>Pereira</surname><given-names>F</given-names></name><name><surname>Burges</surname><given-names>CJ</given-names></name><name><surname>Bottou</surname><given-names>L</given-names></name><name><surname>Weinberger</surname><given-names>KQ</given-names></name></person-group><source>Advances in Neural Information Processing Systems</source><year>2012</year><volume>25</volume></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumaran</surname><given-names>D</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>McClelland</surname><given-names>JL</given-names></name></person-group><article-title>What Learning Systems do Intelligent Agents Need? Complementary Learning Systems Theory Updated</article-title><source>Trends in Cognitive Sciences</source><year>2016</year><volume>20</volume><issue>7</issue><fpage>512</fpage><lpage>534</lpage><pub-id pub-id-type="pmid">27315762</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Latchoumane</surname><given-names>C-FV</given-names></name><name><surname>Ngo</surname><given-names>H-VV</given-names></name><name><surname>Born</surname><given-names>J</given-names></name><name><surname>Shin</surname><given-names>H-S</given-names></name></person-group><article-title>Thalamic Spindles Promote Memory Formation during Sleep through Triple Phase-Locking of Cortical, Thalamic, and Hippocampal Rhythms</article-title><source>Neuron</source><year>2017</year><volume>95</volume><issue>2</issue><fpage>424</fpage><lpage>435</lpage><elocation-id>e6</elocation-id><pub-id pub-id-type="pmid">28689981</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Yu</surname><given-names>T</given-names></name><name><surname>Ni</surname><given-names>D</given-names></name><name><surname>Ren</surname><given-names>L</given-names></name><name><surname>Yang</surname><given-names>Q</given-names></name><name><surname>Lu</surname><given-names>B</given-names></name><name><surname>Wang</surname><given-names>D</given-names></name><name><surname>Heinen</surname><given-names>R</given-names></name><name><surname>Axmacher</surname><given-names>N</given-names></name><name><surname>Xue</surname><given-names>G</given-names></name></person-group><article-title>Stable maintenance of multiple representational formats in human visual short-term memory</article-title><source>Proceedings of the National Academy of Sciences</source><year>2020</year><volume>117</volume><issue>51</issue><fpage>32329</fpage><lpage>32339</lpage><pub-id pub-id-type="doi">10.1073/pnas.2006752117</pub-id><pub-id pub-id-type="pmcid">PMC7768765</pub-id><pub-id pub-id-type="pmid">33288707</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Yu</surname><given-names>T</given-names></name><name><surname>Ren</surname><given-names>L</given-names></name><name><surname>Ni</surname><given-names>D</given-names></name><name><surname>Yang</surname><given-names>Q</given-names></name><name><surname>Lu</surname><given-names>B</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Axmacher</surname><given-names>N</given-names></name><name><surname>Xue</surname><given-names>G</given-names></name></person-group><article-title>Transformative neural representations support long-term episodic memory</article-title><source>Science Advances</source><year>2021</year><volume>7</volume><issue>41</issue><elocation-id>eabg9715</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.abg9715</pub-id><pub-id pub-id-type="pmcid">PMC8500506</pub-id><pub-id pub-id-type="pmid">34623910</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mölle</surname><given-names>M</given-names></name><name><surname>Marshall</surname><given-names>L</given-names></name><name><surname>Gais</surname><given-names>S</given-names></name><name><surname>Born</surname><given-names>J</given-names></name></person-group><article-title>Learning increases human electroencephalographic coherence during subsequent slow sleep oscillations</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2004</year><volume>101</volume><issue>38</issue><fpage>13963</fpage><lpage>13968</lpage><pub-id pub-id-type="doi">10.1073/pnas.0402820101</pub-id><pub-id pub-id-type="pmcid">PMC518860</pub-id><pub-id pub-id-type="pmid">15356341</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Momennejad</surname><given-names>I</given-names></name><name><surname>Russek</surname><given-names>EM</given-names></name><name><surname>Cheong</surname><given-names>JH</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><article-title>The successor representation in human reinforcement learning</article-title><source>Nature Human Behaviour</source><year>2017</year><volume>1</volume><issue>9</issue><fpage>680</fpage><lpage>692</lpage><pub-id pub-id-type="doi">10.1038/s41562-017-0180-8</pub-id><pub-id pub-id-type="pmcid">PMC6941356</pub-id><pub-id pub-id-type="pmid">31024137</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nieh</surname><given-names>EH</given-names></name><name><surname>Schottdorf</surname><given-names>M</given-names></name><name><surname>Freeman</surname><given-names>NW</given-names></name><name><surname>Low</surname><given-names>RJ</given-names></name><name><surname>Lewallen</surname><given-names>S</given-names></name><name><surname>Koay</surname><given-names>SA</given-names></name><name><surname>Pinto</surname><given-names>L</given-names></name><name><surname>Gauthier</surname><given-names>JL</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name></person-group><article-title>Geometry of abstract learned knowledge in the hippocampus</article-title><source>Nature</source><year>2021</year><volume>595</volume><issue>7865</issue><fpage>80</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03652-7</pub-id><pub-id pub-id-type="pmcid">PMC9549979</pub-id><pub-id pub-id-type="pmid">34135512</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niknazar</surname><given-names>H</given-names></name><name><surname>Malerba</surname><given-names>P</given-names></name><name><surname>Mednick</surname><given-names>SC</given-names></name></person-group><article-title>Slow oscillations promote long-range effective communication: The key for memory consolidation in a broken-down network</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2022</year><volume>119</volume><issue>26</issue><elocation-id>e2122515119</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2122515119</pub-id><pub-id pub-id-type="pmcid">PMC9245646</pub-id><pub-id pub-id-type="pmid">35733258</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Schoffelen</surname><given-names>J-M</given-names></name></person-group><article-title>FieldTrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Computational Intelligence and Neuroscience</source><year>2011</year><volume>2011</volume><elocation-id>156869</elocation-id><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id><pub-id pub-id-type="pmcid">PMC3021840</pub-id><pub-id pub-id-type="pmid">21253357</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petzka</surname><given-names>M</given-names></name><name><surname>Charest</surname><given-names>I</given-names></name><name><surname>Balanos</surname><given-names>GM</given-names></name><name><surname>Staresina</surname><given-names>BP</given-names></name></person-group><article-title>Does sleep-dependent consolidation favour weak memories?</article-title><source>Cortex</source><year>2021</year><volume>134</volume><fpage>65</fpage><lpage>75</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2020.10.005</pub-id><pub-id pub-id-type="pmcid">PMC7805594</pub-id><pub-id pub-id-type="pmid">33259969</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Radford</surname><given-names>A</given-names></name><name><surname>Wu</surname><given-names>J</given-names></name><name><surname>Child</surname><given-names>R</given-names></name><name><surname>Luan</surname><given-names>D</given-names></name><name><surname>Amodei</surname><given-names>D</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name></person-group><article-title>Language Models are Unsupervised Multitask Learners</article-title><source>OpenAI</source><year>2019</year></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rau</surname><given-names>EMB</given-names></name><name><surname>Fellner</surname><given-names>M-C</given-names></name><name><surname>Heinen</surname><given-names>R</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Yin</surname><given-names>Q</given-names></name><name><surname>Vahidi</surname><given-names>P</given-names></name><name><surname>Kobelt</surname><given-names>M</given-names></name><name><surname>Asano</surname><given-names>E</given-names></name><name><surname>Kim-McManus</surname><given-names>O</given-names></name><name><surname>Sattar</surname><given-names>S</given-names></name><name><surname>Lin</surname><given-names>JJ</given-names></name><etal/></person-group><article-title>Reinstatement and transformation of memory traces for recognition</article-title><source>Science Advances</source><year>2025</year><volume>11</volume><issue>8</issue><elocation-id>eadp9336</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.adp9336</pub-id><pub-id pub-id-type="pmcid">PMC11838014</pub-id><pub-id pub-id-type="pmid">39970226</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reddy</surname><given-names>L</given-names></name><name><surname>Poncet</surname><given-names>M</given-names></name><name><surname>Self</surname><given-names>MW</given-names></name><name><surname>Peters</surname><given-names>JC</given-names></name><name><surname>Douw</surname><given-names>L</given-names></name><name><surname>van Dellen</surname><given-names>E</given-names></name><name><surname>Claus</surname><given-names>S</given-names></name><name><surname>Reijneveld</surname><given-names>JC</given-names></name><name><surname>Baayen</surname><given-names>JC</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><article-title>Learning of anticipatory responses in single neurons of the human medial temporal lobe</article-title><source>Nature Communications</source><year>2015</year><volume>6</volume><issue>1</issue><elocation-id>8556</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms9556</pub-id><pub-id pub-id-type="pmcid">PMC4617602</pub-id><pub-id pub-id-type="pmid">26449885</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roüast</surname><given-names>NM</given-names></name><name><surname>Schönauer</surname><given-names>M</given-names></name></person-group><article-title>Continuously changing memories: A framework for proactive and non-linear consolidation</article-title><source>Trends in Neurosciences</source><year>2023</year><volume>46</volume><issue>1</issue><fpage>8</fpage><lpage>19</lpage><pub-id pub-id-type="pmid">36428193</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schapiro</surname><given-names>AC</given-names></name><name><surname>Kustner</surname><given-names>LV</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name></person-group><article-title>Shaping of Object Representations in the Human Medial Temporal Lobe Based on Temporal Regularities</article-title><source>Current Biology</source><year>2012</year><volume>22</volume><issue>17</issue><fpage>1622</fpage><lpage>1627</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2012.06.056</pub-id><pub-id pub-id-type="pmcid">PMC3443305</pub-id><pub-id pub-id-type="pmid">22885059</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schreiner</surname><given-names>T</given-names></name><name><surname>Griffiths</surname><given-names>BJ</given-names></name><name><surname>Kutlu</surname><given-names>M</given-names></name><name><surname>Vollmar</surname><given-names>C</given-names></name><name><surname>Kaufmann</surname><given-names>E</given-names></name><name><surname>Quach</surname><given-names>S</given-names></name><name><surname>Remi</surname><given-names>J</given-names></name><name><surname>Noachtar</surname><given-names>S</given-names></name><name><surname>Staudigl</surname><given-names>T</given-names></name></person-group><article-title>Spindle-locked ripples mediate memory reactivation during human NREM sleep</article-title><source>Nature Communications</source><year>2024</year><volume>15</volume><issue>1</issue><elocation-id>5249</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-024-49572-8</pub-id><pub-id pub-id-type="pmcid">PMC11187142</pub-id><pub-id pub-id-type="pmid">38898100</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schreiner</surname><given-names>T</given-names></name><name><surname>Petzka</surname><given-names>M</given-names></name><name><surname>Staudigl</surname><given-names>T</given-names></name><name><surname>Staresina</surname><given-names>BP</given-names></name></person-group><article-title>Endogenous memory reactivation during sleep in humans is clocked by slow oscillation-spindle complexes</article-title><source>Nature Communications</source><year>2021</year><volume>12</volume><issue>1</issue><elocation-id>3112</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-23520-2</pub-id><pub-id pub-id-type="pmcid">PMC8149676</pub-id><pub-id pub-id-type="pmid">34035303</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stachenfeld</surname><given-names>KL</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><article-title>The hippocampus as a predictive map</article-title><source>Nature Neuroscience</source><year>2017</year><volume>20</volume><issue>11</issue><fpage>1643</fpage><lpage>1653</lpage><pub-id pub-id-type="pmid">28967910</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Staresina</surname><given-names>BP</given-names></name></person-group><article-title>Coupled sleep rhythms for memory consolidation</article-title><source>Trends in Cognitive Sciences</source><year>2024</year><volume>28</volume><issue>4</issue><fpage>339</fpage><lpage>351</lpage><pub-id pub-id-type="pmid">38443198</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tacikowski</surname><given-names>P</given-names></name><name><surname>Kalender</surname><given-names>G</given-names></name><name><surname>Ciliberti</surname><given-names>D</given-names></name><name><surname>Fried</surname><given-names>I</given-names></name></person-group><article-title>Human hippocampal and entorhinal neurons encode the temporal structure of experience</article-title><source>Nature</source><year>2024</year><volume>635</volume><issue>8037</issue><fpage>160</fpage><lpage>167</lpage><pub-id pub-id-type="doi">10.1038/s41586-024-07973-1</pub-id><pub-id pub-id-type="pmcid">PMC11540853</pub-id><pub-id pub-id-type="pmid">39322671</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tarder-Stoll</surname><given-names>H</given-names></name><name><surname>Baldassano</surname><given-names>C</given-names></name><name><surname>Aly</surname><given-names>M</given-names></name></person-group><article-title>The brain hierarchically represents the past and future during multistep anticipation</article-title><source>Nature Communications</source><year>2024</year><volume>15</volume><issue>1</issue><elocation-id>9094</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-024-53293-3</pub-id><pub-id pub-id-type="pmcid">PMC11496687</pub-id><pub-id pub-id-type="pmid">39438448</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treder</surname><given-names>MS</given-names></name></person-group><article-title>MVPA-Light: A Classification and Regression Toolbox for Multi-Dimensional Data</article-title><source>Frontiers in Neuroscience</source><year>2020</year><volume>14</volume><pub-id pub-id-type="doi">10.3389/fnins.2020.00289</pub-id><pub-id pub-id-type="pmcid">PMC7287158</pub-id><pub-id pub-id-type="pmid">32581662</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Umbach</surname><given-names>G</given-names></name><name><surname>Kantak</surname><given-names>P</given-names></name><name><surname>Jacobs</surname><given-names>J</given-names></name><name><surname>Kahana</surname><given-names>M</given-names></name><name><surname>Pfeiffer</surname><given-names>BE</given-names></name><name><surname>Sperling</surname><given-names>M</given-names></name><name><surname>Lega</surname><given-names>B</given-names></name></person-group><article-title>Time cells in the human hippocampus and entorhinal cortex support episodic memory</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2020</year><volume>117</volume><issue>45</issue><fpage>28463</fpage><lpage>28474</lpage><pub-id pub-id-type="doi">10.1073/pnas.2013250117</pub-id><pub-id pub-id-type="pmcid">PMC7668099</pub-id><pub-id pub-id-type="pmid">33109718</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vallat</surname><given-names>R</given-names></name><name><surname>Walker</surname><given-names>MP</given-names></name></person-group><article-title>An open-source, high-performance tool for automated sleep staging</article-title><source>eLife</source><year>2021</year><volume>10</volume><elocation-id>e70092</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.70092</pub-id><pub-id pub-id-type="pmcid">PMC8516415</pub-id><pub-id pub-id-type="pmid">34648426</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>MA</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name></person-group><article-title>Reactivation of Hippocampal Ensemble Memories During Sleep</article-title><source>Science</source><year>1994</year><volume>265</volume><issue>5172</issue><fpage>676</fpage><lpage>679</lpage><pub-id pub-id-type="pmid">8036517</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><article-title>Using goal-driven deep learning models to understand sensory cortex</article-title><source>Nature Neuroscience</source><year>2016</year><volume>19</volume><issue>3</issue><fpage>356</fpage><lpage>365</lpage><pub-id pub-id-type="pmid">26906502</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>CY</given-names></name><name><surname>Talmi</surname><given-names>D</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Mattar</surname><given-names>MG</given-names></name></person-group><article-title>Episodic retrieval for model-based evaluation in sequential decision tasks</article-title><source>Psychological Review</source><year>2025</year><volume>132</volume><issue>1</issue><fpage>18</fpage><lpage>49</lpage><pub-id pub-id-type="pmid">39869686</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Emergence of Successor Representations and Experimental Design.</title><p><bold>(a)</bold> Example of how sequence learning and sleep might change neural representations. Upon encountering a Welsh Corgi, the brain primarily represents the current stimulus entity. If the Corgi is part of a recurring temporal sequence (Corgi → Girl → House), subsequent stimuli (Girl and House) might be integrated into the Corgi representation. Post-learning sleep might provide an opportunity for the brain to replay learned experiences and thereby further strengthen successor representations. Upon post sleep exposure to a Corgi image, brain activation patterns might reflect both the current stimulus (Corgi) as well as learned successors (Girl, House). Faded images indicate weaker representations. Images sourced from AI-generated content and <ext-link ext-link-type="uri" xlink:href="https://www.flaticon.com">https://www.flaticon.com</ext-link>; <bold>(b)</bold> Timeline of the experiment. Participants first completed a perceptual task, followed by a sequence learning task (Memory Arena). Memory for the learned sequence was then assessed both before and after a period of sleep. Finally, participants completed the perceptual task again.</p></caption><graphic xlink:href="EMS206940-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Learning incorporates successor representations.</title><p><bold>(a)</bold> Left: example trials from the perceptual task, including image presentation (1000 ms) and fixation period (750-1250 ms). Right: decoding approach for the currently displayed image (bunny). Example image sequence from the Memory Arena task (from previous previous, previous, current, next, to next next around the ‘bunny’ image). Decoders were trained on EEG data from the pre-learning perceptual task using the four nearby categories (body parts, letter strings, faces, and scenes) and were tested on EEG data of the ‘bunny’ (object) image in the post-learning perceptual task. Examples of face stimuli shown were sourced from an AI-generated dataset (<ext-link ext-link-type="uri" xlink:href="https://github.com/RichardErkhov/ai_generated_faces">https://github.com/RichardErkhov/ai_generated_faces</ext-link>), used under the MIT License; <bold>(b)</bold> Group-level time-by-time decoding accuracies of predecessor and successor images across participants. Black contour indicates significantly higher decoding accuracy than chance level (<italic>p</italic><sub>cluster</sub> = 0.028, N = 26, one-tailed cluster-based permutation test); <bold>(c)</bold> Correlation between successor decoding accuracy and immediate post-learning memory accuracy at the <italic>a priori</italic> chosen midpoint of image presentation intervals (0.5s after image onset for both training and test trials). The solid line shows the best-fit linear regression line. The dashed lines indicate the 95% confidence bounds for the fitted line.</p></caption><graphic xlink:href="EMS206940-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Successor representations shift towards higher-level visual representations.</title><p><bold>(a)</bold> Architecture of Alexnet (<xref ref-type="bibr" rid="R19">Krizhevsky et al., 2012</xref>). Hidden layers contain conv1-conv5 and fc6-fc7; <bold>(b)</bold> Illustration of EEG-successor similarity analysis. Left: example images used in the Memory Arena task. Middle: Example EEG similarity matrix. Right: Example DNN similarity matrix of successor images in DNN layer 7 (‘fc7’). Examples of face stimuli shown were sourced from an AI-generated dataset (<ext-link ext-link-type="uri" xlink:href="https://github.com/RichardErkhov/ai_generated_faces">https://github.com/RichardErkhov/ai_generated_faces</ext-link>), used under the MIT License; <bold>(c)</bold> EEG-successor similarity change from pre- to post-learning perceptual tasks across time; <bold>(d)</bold> Group-level EEG-successor similarity change from pre-learning perceptual tasks within the image presentation time window across DNN layers; <bold>(e)</bold> Representational shift: Spearman’s correlations between averaged EEG-successor similarity change and layer hierarchies. Correlations were Fisher z-transformed for statistical testing. ** indicates <italic>p</italic> &lt; 0.01, N =26, two-tailed paired <italic>t</italic>-test.</p></caption><graphic xlink:href="EMS206940-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>SWS predicts successor representational strength and shift.</title><p><bold>(a)</bold> Example sleep hypnogram of one participant. Red lines highlight slow wave sleep (SWS); <bold>(b)</bold> Correlation between SWS proportions and successor representational strength across participants at the image presentation midpoint (0.5s after image onset for both training and test); <bold>(c)</bold> Correlation between SWS proportions and successor representational shift across participants at the image presentation midpoint (0.5s after image onset). The solid line in panels b and c shows the best-fit linear regression line. The dashed lines indicate the 95% confidence bounds for the fitted line.</p></caption><graphic xlink:href="EMS206940-f004"/></fig></floats-group></article>