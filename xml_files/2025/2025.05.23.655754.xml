<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS205917</article-id><article-id pub-id-type="doi">10.1101/2025.05.23.655754</article-id><article-id pub-id-type="archive">PPR1026416</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Task-relevant cognitive maps in episodic memory</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Rau</surname><given-names>Elias M.B.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Heinen</surname><given-names>Rebekka</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Herweg</surname><given-names>Nora A.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Patai</surname><given-names>Eva Z.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Patai</surname><given-names>Eva Z.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Kobelt</surname><given-names>Malte</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Ahmadi</surname><given-names>Khazar</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Axmacher</surname><given-names>Nikolai</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib></contrib-group><aff id="A1"><label>1</label>Department of Neuropsychology, Institute of Cognitive Neuroscience, Faculty of Psychology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/04tsk2644</institution-id><institution>Ruhr University Bochum</institution></institution-wrap>, <postal-code>44801</postal-code><city>Bochum</city>, <country country="DE">Germany</country></aff><author-notes><corresp id="CR1"><label>*</label>Corresponding author: Elias MB Rau <email>elias.rau.er@gmail.com</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>28</day><month>05</month><year>2025</year></pub-date><pub-date pub-type="preprint"><day>25</day><month>05</month><year>2025</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Cognitive maps support navigation by representing locations in internal spaces with Euclidean distances. Similar organizational principles may govern the representation of non-spatial sensory and conceptual features, enabling higher cognitive functions including decision making and model-based reinforcement learning. Previous studies have shown that medial temporal lobe (MTL) structures support the embedding of stimuli into cognitive maps. However, these studies investigated the generation of novel cognitive maps, while the recruitment of existing semantic knowledge into task-relevant representational spaces and their “remapping” according to changing goals has not been investigated. In addition, it is not clear how the MTL interacts with neocortical representations of task-relevant stimulus features and whether this influences the formation of novel episodic memory traces. Here we show that the neural representations of natural stimuli in both MTL and neocortex are organized in behaviorally relevant cognitive maps of conceptual features and influence the accessibility of novel memory traces. Cognitive maps organize information into internal spaces with Euclidean distances, adapt to ongoing task demands, and influence performance. Their representational structure matches neural similarities in the MTL, with a specific role of the hippocampus for taskdependent remapping. Further, we isolated task-relevant and stimulus-driven representations of natural stimuli and show how they contribute to the formation of memory traces. Together, our results suggest that conceptual representations are flexibly recruited in spatially organized cognitive maps of task-relevant features and that Euclidean distances in these cognitive maps affect subsequent memory.</p></abstract><kwd-group><kwd>Cognitive map</kwd><kwd>hippocampus</kwd><kwd>remapping</kwd><kwd>episodic memory traces</kwd><kwd>representational formats</kwd><kwd>deep neural networks</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Humans and other animals are able to perform shortcuts during navigation. This ability requires cognitive maps, i.e., the embedding of spatial locations into a common representational space with metric (i.e., Euclidean) distances (<xref ref-type="bibr" rid="R103">Tolman 1948</xref>). In addition to spatial navigation, cognitive maps of conceptual information have been proposed to underlie cognitive functions such as the organization of knowledge in memory, decision making, and planning (<xref ref-type="bibr" rid="R6">Behrens et al. 2018</xref>; <xref ref-type="bibr" rid="R8">Bellmund et al. 2018</xref>; <xref ref-type="bibr" rid="R9">Boffini and Doeller 2020</xref>). These frameworks propose similar coding schemes of spatial and conceptual information – i.e., they suggest that both depend on relational processing in a domain-general format (<xref ref-type="bibr" rid="R24">Eichenbaum 2017</xref>; <xref ref-type="bibr" rid="R25">Eichenbaum and Cohen 2014</xref>; <xref ref-type="bibr" rid="R51">Jeffery 2018</xref>). Accordingly, cognitive maps are also discussed in the fields of reinforcement learning (<xref ref-type="bibr" rid="R35">Gershman 2018</xref>) and value-based decision making (<xref ref-type="bibr" rid="R75">Nitsch et al. 2024</xref>), where they are defined by task-relevant features (<xref ref-type="bibr" rid="R76">Niv 2019</xref>; <xref ref-type="bibr" rid="R77">Niv et al. 2015</xref>). Cognitive maps therefore unite research on spatial navigation, reinforcement learning, and semantic memory, by characterizing the organizational principles of neural representations and their relevance to human cognition and behavior.</p><p id="P3">Cognitive maps can be defined by three criteria. First, to enable inferences between pairs of stimuli that were not jointly experienced before – i.e., “shortcuts” – cognitive maps should organize stimuli in a metric space with Euclidean distances (e.g., an ordinal topology with non-Euclidean distances does not allow for the computation of shortcuts). Second, to flexibly support goal-directed behavior, cognitive maps should be defined by task-relevant dimensions; for example, a spatial coding scheme typically does not allow one to infer social relationships. Third, using these maps should influence behavioral performance, e.g. by priming stimuli at neighboring map locations, or reversely, by facilitating comparisons of stimuli at remote locations.</p><p id="P4">There is considerable evidence that cells in the medial temporal lobe (MTL) form the neural basis of cognitive maps (<xref ref-type="bibr" rid="R68">Moser, Kropff, and Moser 2008</xref>). For instance, grid cells in entorhinal cortex (EC) exhibit a hexagonal firing pattern, providing metric information about distances within a cognitive map (<xref ref-type="bibr" rid="R32">Fyhn et al. 2008</xref>; <xref ref-type="bibr" rid="R50">Jacobs et al. 2013</xref>); hippocampal place cells encode individual locations in these maps (<xref ref-type="bibr" rid="R79">O’Keefe 1976</xref>). In humans, HC neurons were found to code for distinct concepts (<xref ref-type="bibr" rid="R86">Quiroga 2012</xref>; <xref ref-type="bibr" rid="R85">Quiroga et al. 2005</xref>; <xref ref-type="bibr" rid="R89">Rey et al. 2015</xref>) as well, possibly corresponding to locations in abstract cognitive maps.</p><p id="P5">More recently, it was shown that some human HC neurons represent conjunctions of episodic elements (<xref ref-type="bibr" rid="R58">Kolibius et al. 2023</xref>), indicating that their response to stimuli is contextually modulated and may therefore be more flexible than the context-invariant responses of concept cells. Context-dependent representations in the MTL are further supported by studies on mixed selectivity, i.e., a flexible change in neuronal responsiveness depending on task demands (<xref ref-type="bibr" rid="R23">Donoghue et al. 2023</xref>; <xref ref-type="bibr" rid="R30">Fusi, Miller, and Rigoffi 2016</xref>; <xref ref-type="bibr" rid="R41">Han et al. 2023</xref>; <xref ref-type="bibr" rid="R106">Tye et al. 2024</xref>). In rodents, HC place cells have been shown to exhibit remapping, i.e., to change their responses between contexts (<xref ref-type="bibr" rid="R12">Colgin, Moser, and Moser 2008</xref>; <xref ref-type="bibr" rid="R31">Fyhn et al. 2007</xref>) – a possible mechanism to enable orthogonalized, context-dependent memory representations (<xref ref-type="bibr" rid="R62">Kubie, Levy, and Fenton 2020</xref>). In these studies, remapping, mixed selectivity, and contextual modulation of responsiveness may all be conceived of as the employment of different cognitive maps which localize individual concepts according to different task-relevant features – suggesting that the representation of a given concept differs depending on which features are currently behaviorally relevant.</p><p id="P6">At a macroscopic level, representational similarity analysis (RSA; (<xref ref-type="bibr" rid="R59">Kriegeskorte 2008</xref>) may reveal how conceptual cognitive maps shape HC representations in different behavioral contexts. Previous RSA studies indeed provided evidence for cognitive-map representations in the HC according to various different task-relevant dimensions. They showed that differences in HC activity (i.e., neural representational distances) do not only correspond to distances between spatial positions (<xref ref-type="bibr" rid="R19">Deuker et al. 2016</xref>; <xref ref-type="bibr" rid="R43">Hassabis et al. 2009</xref>), but also to distances along perceptual (<xref ref-type="bibr" rid="R108">Viganò et al. 2021</xref>; <xref ref-type="bibr" rid="R107">Viganò and Piazza 2020</xref>), temporal (<xref ref-type="bibr" rid="R19">Deuker et al. 2016</xref>; <xref ref-type="bibr" rid="R33">Garvert, Dolan, and Behrens 2017</xref>), associative (<xref ref-type="bibr" rid="R13">Constantinescu, O’Reilly, and Behrens 2016</xref>; <xref ref-type="bibr" rid="R100">Theves, Fernandez, and Doeller 2019</xref>; <xref ref-type="bibr" rid="R101">Theves, Fernández, and Doeller 2020</xref>), or abstract dimensions (<xref ref-type="bibr" rid="R34">Garvert et al. 2023</xref>; <xref ref-type="bibr" rid="R97">Tavares et al. 2015</xref>). While all these studies assessed the formation of new cognitive maps, many real-world situations arguably depend on the extraction of pre-existing semantic knowledge according to task-relevant criteria. In the current study, we thus aimed to investigate the embedding of familiar concepts into cognitive maps according to different task-relevant features.</p><p id="P7">In addition, we wondered whether and how task-relevant cognitive maps are embedded into episodic memory traces. According to the influential indexing theory of HC functioning, the HC serves as a versatile coordinate system for sensory representations stored across the neocortex (<xref ref-type="bibr" rid="R98">Teyler and DiScenna 1986</xref>; <xref ref-type="bibr" rid="R99">Teyler and Rudy 2007</xref>). Specifically, we hypothesized that HC distance representations that reflect the embedding of stimuli in cognitive maps may also influence the formation of episodic memories: The coding of task-relevant distances in cognitive maps may indicate the context-dependent deviance of an item from other items, possibly enhancing memory formation (<xref ref-type="bibr" rid="R5">Axmacher et al. 2010</xref>; <xref ref-type="bibr" rid="R66">Lisman and Grace 2005</xref>). Moreover, the task-relevant dimensions of cognitive maps may affect the representational formats of memory traces: Neural representations during both perception and memory contain multiple representational formats that reflect the perceptual and conceptual properties of stimuli (<xref ref-type="bibr" rid="R45">Heinen et al. 2023</xref>). Perceptual formats can be studied using convolutional deep neural networks (DNNs), which capture increasingly complex visual features of neural representations along the processing hierarchy of the ventral visual stream (VVS) (<xref ref-type="bibr" rid="R11">Cichy et al. 2016</xref>; <xref ref-type="bibr" rid="R39">Güclü, and van Gerven 2015</xref>; <xref ref-type="bibr" rid="R63">Kuzovkin et al. 2018</xref>) and have more recently been applied to mnemonic representations (<xref ref-type="bibr" rid="R16">Davis et al. 2021</xref>; <xref ref-type="bibr" rid="R46">Heinen et al. 2025</xref>; <xref ref-type="bibr" rid="R88">Rau et al. 2025</xref>). In addition, conceptual or semantic formats have been found in neural representations in both VVS and anterior temporal cortices (<xref ref-type="bibr" rid="R20">Devereux, Clarke, and Tyler 2018</xref>; <xref ref-type="bibr" rid="R49">Huth et al. 2012</xref>).</p><p id="P8">Importantly, both perceptual and conceptual formats are determined by the stimulus itself and are thus independent of task-relevance. However, the relative prominence of these formats has been shown to depend on task-demands during both encoding and retrieval (<xref ref-type="bibr" rid="R16">Davis et al. 2021</xref>; <xref ref-type="bibr" rid="R45">Heinen et al. 2023</xref>, <xref ref-type="bibr" rid="R46">2025</xref>), suggesting flexible remapping of not only HC, but also neocortical representations according to task-demands (<xref ref-type="bibr" rid="R36">Gilbert and Li 2013</xref>; <xref ref-type="bibr" rid="R42">Harel, Kravitz, and Baker 2014</xref>; <xref ref-type="bibr" rid="R54">Kaiser, Turini, and Cichy 2019</xref>). How stimulus-driven representational formats and formats of task-relevant cognitive maps interact in memory traces remains to be understood.</p><p id="P9">Here, we investigated the organization of natural stimuli into cognitive maps according to task demands and their embedding into memory traces. Seminal research proposed that distances in abstract spaces may be measured on the behavioral level via similarity judgements (<xref ref-type="bibr" rid="R48">Hutchinson and Lockhead 1977</xref>) highlighting similarities in pairwise comparisons based on perceptual and abstract conceptual features (<xref ref-type="bibr" rid="R69">Moyer 1973</xref>; <xref ref-type="bibr" rid="R70">Moyer and Landauer 1967</xref>). We thus sampled trajectories in spaces of natural animal concepts via distance ratings along predefined conceptual dimensions and subsequently tested the accessibility of stimuli for episodic memory. We determined how task-relevance shapes neural representations of the same concept in distinct behavioral contexts. We further investigated how distances in these cognitive maps influenced the formation of memories and how task-specific vs. stimulus-dependent representational formats contribute to the neural representations of memory traces.</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>Task-relevant cognitive maps of natural concepts</title><p id="P10">While undergoing fMRI scanning, participants engaged in a “conceptual navigation” task in which they rated the abstract distance of two animals on a continuous scale according to two conceptual features (e.g., size &amp; climate, or approachability &amp; activity; see <xref ref-type="fig" rid="F1">Fig. 1A</xref>; <xref ref-type="supplementary-material" rid="SD1">fig. S1</xref>). Task-relevant features changed between blocks, but each block contained the same animal concept with a unique exemplar image. We first established that our paradigm could capture the organization of natural stimuli in cognitive maps – i.e., that the animal concepts were embedded into spaces with Euclidean distances, that these spaces were defined by the task-relevant dimensions in each block, and that the distances in these cognitive maps were relevant for behavior.</p><p id="P11">We reconstructed cognitive maps of all animal concepts via least-squares solutions, i.e. concept positions on each 1D feature dimension (<xref ref-type="fig" rid="F1">Fig. 1B, C</xref>; <xref ref-type="supplementary-material" rid="SD1">fig. S1</xref>), based on all pairwise ratings obtained by a participant. Importantly, estimated concept positions predicted behavioral ratings on each dimension significantly better than chance (permutation test for each dimension, all <italic>P</italic> &lt; 0.001; <xref ref-type="supplementary-material" rid="SD1">fig. S1</xref>; for details see <xref ref-type="sec" rid="S9">Methods</xref>), and since the least-square solutions were based on linear regression, this indicates that stimuli were indeed embedded into Euclidean spaces – i.e., spaces with interval-scaled linear distances.</p><p id="P12">We next tested whether cognitive maps differed according to the task-relevant dimensions in each block. During conceptual navigation, different animal concepts were compared in the two blocks (e.g., a dog was compared to a zebra in one block, but to a penguin in another block). However, the reconstruction of cognitive maps allowed us to infer ratings of pairs that were not explicitly compared and thus to compare different cognitive maps each including all combinations of animal concepts. Importantly, we found that cognitive maps indeed differed depending on the respective task-relevant dimensions: Reconstructed maps of a given space (i.e. activity &amp; size) were more similar between participants (between participant similarity: 0.537±0.082, <italic>M</italic>±<italic>SD;</italic> measured via Spearman’s rank correlation between participant-specific and global cognitive maps; for <xref ref-type="supplementary-material" rid="SD1">fig. S2</xref>; for details see <xref ref-type="sec" rid="S9">methods</xref>) as compared to the similarity of the both cognitive maps rated by a participant (within participant similarity: 0.097±0.098; <italic>T</italic><sub>45</sub> = 29.943, <italic>P</italic> &lt; 0.001), suggesting that different cognitive maps are recruited to support behavior according to task-relevant features (“space remapping”; see below).</p><p id="P13">Finally, we found that task-dependent cognitive maps were relevant for behavioral performance, i.e. rating response times (RT). Using linear mixed models (LMMs), we found that RTs were shorter when animals differed more in the relevant dimensions (all <italic>Z</italic> &gt; -the cognitive maps all <italic>χ</italic><sup>2</sup> &gt; 171.60, all <italic>P</italic> &lt; 0.001; <xref ref-type="fig" rid="F1">Fig. 1D</xref>). By contrast, RTs did not depend on the similarity of perceptual features of the animals extracted via a convolutional DNN, indicating that the ratings indeed depended on task-relevant features rather than stimulus-driven features (see Supplement; <xref ref-type="supplementary-material" rid="SD1">fig. S2</xref>). Task-relevant Euclidean distances from reconstructed cognitive maps predicted rating RTs as well (<italic>Z</italic> = -18.528, <italic>χ</italic><sup>2</sup> = 339.33, <italic>P</italic> &lt; 0.001).</p></sec><sec id="S4"><title>Neural representations of task-relevant cognitive maps in the medial temporal lobe</title><p id="P14">Next, we tested whether distances in the cognitive maps - specifically of concept pairs that were not explicitly probed during conceptual navigation - were correlated with neural representational distances in the MTL (<xref ref-type="fig" rid="F1">Fig. 1E</xref>). For this analysis, we ensured that representations of cognitive maps are specific to task-relevant features, generalize to concepts not explicitly probed during the experiment, and go beyond stimulus-related perceptual features (<xref ref-type="fig" rid="F1">Fig. 1F</xref>; for details see <xref ref-type="sec" rid="S9">Methods</xref>). We found significant correlations between representational similarity matrices (RSMs) of task-relevant cognitive maps and neural RSMs in both HC (<xref ref-type="fig" rid="F1">Fig. 1G</xref>, “same space”; <italic>T</italic><sub>46</sub> = 2.455, <italic>P</italic> = 0.018) and PRC (<italic>T</italic><sub>46</sub> = 2.654, <italic>P</italic> = 0.011), but not in PHC (<italic>T</italic><sub>46</sub> = 1.823, <italic>P</italic> = 0.075) or EC (<italic>T</italic><sub>46</sub> = 0.261, <italic>P</italic> = 0.795). Further, we found significant negative correlations between task-irrelevant cognitive maps and neural RSMs in PRC (<xref ref-type="fig" rid="F1">Fig. 1G</xref>, “different space”; <italic>T</italic><sub>46</sub> = -3.582, <italic>P</italic> = 0.001) and PHC (<italic>T</italic><sub>46</sub> = -3.379, <italic>P</italic> = 0.002) but not HC (<italic>T</italic><sub>46</sub> = -0.252, <italic>P</italic> = 0.802) or EC (<italic>T</italic><sub>46</sub> = 0.102, <italic>P</italic> = 0.919). Importantly, neural RSMs were significantly more similar to task-relevant than to task-irrelevant cognitive maps in HC (<italic>T</italic><sub>46</sub> = 2.320, <italic>P</italic> = 0.025), PRC (<italic>T</italic><sub>46</sub> 4.512, <italic>P</italic> &lt; 0.001), and PHC (<italic>T</italic><sub>46</sub> = 3.646, <italic>P</italic> = 0.001), but not in EC (<italic>T</italic><sub>46</sub> = 0.139, <italic>P</italic> = 0.890). Together, these results show that representations in several MTL regions capture the organization of natural stimuli into cognitive maps according to task-relevant features.</p></sec><sec id="S5"><title>Task-specific remapping of conceptual representations in HC</title><p id="P15">Next, we analyzed remapping between the two different cognitive maps. Specifically, we tested whether the differences between the two cognitive maps – “space remapping” – and the differences between the positions of individual concepts in the two maps – “concept remapping” – corresponded to differences in neural similarity in the MTL.</p><p id="P16">To analyze the neural basis of space remapping, we first calculated in each participant the similarity between representations of the two cognitive maps based on behaviorally reconstructed and neural similarities (space<sub>1</sub>-space<sub>2</sub> similarity). To quantify behaviorally relevant space remapping, we correlated both measures across participants in each MTL region. We found a significant correlation between behavioral and neural remapping in the HC (<italic>ρ</italic> = 0.384, <italic>P</italic> = 0.008; <xref ref-type="fig" rid="F2">Fig. 2A</xref>), but not in any other MTL region (all <italic>ρ</italic> &lt; 0.230, all <italic>P</italic> &gt; 0.14). These results suggest a specific role of the HC for the flexible remapping of cognitive maps according to task demands.</p><p id="P17">To test for concept remapping, we investigated whether differences in the position of individual concepts in the two cognitive maps matched their representational distances in the HC (or in other MTL regions). Specifically, we hypothesized that animal concepts whose positions in the two cognitive maps differed more would also be represented more differently in the HC, i.e. that behavioral concept remapping values were associated with the neural dissimilarity of exemplars of the same animal concept, encountered in different conceptual spaces. For each concept, we extracted its Euclidean distance from the respective map origins (i.e., position [0;0]; <xref ref-type="fig" rid="F2">Fig. 2B</xref>) and the difference between its neural representations and correlated the two values across all concepts in each participant. Positive correlations indicate that concepts whose cognitive map positions differ more between spaces also show more dissimilar neural patterns of animal exemplars encountered in different maps. We found that correlations were consistently positive in the HC (<italic>T</italic><sub>46</sub> = 2.725, <italic>P</italic> = 0.009; <xref ref-type="fig" rid="F2">Fig. 2C</xref>). No other MTL subregion showed this effect (PRC: <italic>T</italic><sub>46</sub> = -0.348, <italic>P</italic> = 0.729 | PHC: <italic>T</italic><sub>46</sub> = -1.607, <italic>P</italic> = 0.115 | EC: <italic>T</italic><sub>46</sub> = - 0.138, <italic>P</italic> = 0.891), suggesting a selective role of the HC for remapping. We did not find significant correlations with neural similarity within spaces (all <italic>P</italic>s &gt; 0.084). Similar results were found in a LMM that predicted behavioral concept remapping by fixed effects of within- and between-space HC pattern similarity and participants with random intercepts (<xref ref-type="fig" rid="F2">Fig. 2D</xref>). Again, we found that between-space similarity (<italic>Z</italic> = 2.006, <italic>χ</italic><sup>2</sup> = 4.02, <italic>P</italic> = 0.045) but not within-space similarity (<italic>Z</italic> = -0.659, <italic>χ</italic><sup>2</sup> = 0.19, <italic>P</italic> = 0.659) predicted concept remapping, suggesting that concept representations in HC depended on task-specific cognitive maps.</p></sec><sec id="S6"><title>Distances in cognitive maps shape the formation of memory traces</title><p id="P18">How does the embedding of stimuli in task-relevant cognitive maps affect the formation of memory traces for individual exemplars – i.e., how is cognitive map structure incorporated into episodic memory traces? Different from earlier studies on conceptual cognitive maps (<xref ref-type="bibr" rid="R13">Constantinescu et al. 2016</xref>; <xref ref-type="bibr" rid="R34">Garvert et al. 2023</xref>, <xref ref-type="bibr" rid="R33">2017</xref>; <xref ref-type="bibr" rid="R97">Tavares et al. 2015</xref>; <xref ref-type="bibr" rid="R102">Theves et al. 2021</xref>, <xref ref-type="bibr" rid="R100">2019</xref>), our paradigm involves natural spaces of pre-existing knowledge. We hypothesized that the employment of MTL cognitive map representations during our task influences the formation of episodic memory traces.</p><p id="P19">Following conceptual navigation, participants completed a surprise recognition memory task outside of the scanner, where they viewed previously presented animal exemplars, new exemplars of previously presented animal concepts, and new animal concepts (<xref ref-type="fig" rid="F3">Fig. 3A</xref>). For each animal image, they indicated on a continuous scale their confidence of having seen it before; for responses in the “old” range of this scale, they also indicated with a binary response the corresponding cognitive map (source memory). Participants successfully discriminated between old and new images (<italic>d</italic>-prime: <italic>M</italic> = 1.080, <italic>SD</italic> = 0.256; area under the curve (AUC): <italic>T</italic><sub>45</sub> = 29.751, <italic>P</italic> &lt; 0.001; <xref ref-type="fig" rid="F3">Fig. 3B</xref>), and detection of new concepts was better than detection of new exemplars (AUC exemplar vs. concept discrimination: <italic>T</italic><sub>45</sub> = -21.414, <italic>P</italic> &lt; 0.001; <xref ref-type="supplementary-material" rid="SD1">fig. S3</xref>; for details see Supplement). Source memory accuracy was reliably above chance (i.e., 0.5; <italic>T</italic><sub>45</sub> = 4.556, <italic>P</italic> &lt; 0.001), even though it was relatively low overall (0.532±0.047).</p><p id="P20">We then analyzed subsequent memory effects (SMEs). ROI-based analyses revealed SMEs in all MTL regions (<xref ref-type="fig" rid="F3">Fig. 3C</xref> left; HC: <italic>T</italic><sub>45</sub> = 2.639, <italic>P</italic> = 0.012; PRC: <italic>T</italic><sub>45</sub> = 6.780, <italic>P</italic> &lt; 0.001; PHC: <italic>T</italic><sub>45</sub> = 4.936, <italic>P</italic> &lt; 0.001; EC: <italic>T</italic>4<sub>5</sub> = 2.567, <italic>P</italic> = 0.029). A whole-brain analysis showed SME clusters in VVS (including occipitotemporal, fusiform and parahippocampal cortices; <xref ref-type="fig" rid="F3">Fig. 3C</xref>, right; <xref ref-type="supplementary-material" rid="SD1">Table S1</xref>), while negative SMEs were found in regions of the default mode network, in middle and anterior cingulate cortex, and in the inferior parietal cortex (<xref ref-type="supplementary-material" rid="SD1">Table S1</xref>). A parametric modulation analysis showed activity associated with memory confidence of remembered images in similar regions (<xref ref-type="supplementary-material" rid="SD1">fig. S3</xref>; <xref ref-type="supplementary-material" rid="SD1">Table S1</xref>).</p><p id="P21">We next tested whether task-relevant distances, the major structural property of cognitive maps, affected the formation of memory traces – i.e., whether distance ratings, as well as reconstructed Euclidean distances, of each image influenced its accessibility during the subsequent recognition memory test. We set up LMMs predicting memory confidence of subsequently remembered images based on distance ratings during encoding (for details see <xref ref-type="sec" rid="S9">Methods</xref>), and modeled participants with random intercepts. We found a significant main effect of 2D distance (<italic>Z</italic> = 3.552, <italic>χ</italic><sup>2</sup> = 12.61, <italic>P</italic> &lt; 0.001), indicating higher memory confidence of images that were associated with higher distances during encoding. These effects were observed both for the first (reference) and for the second (comparison) image in each trial (reference image: <italic>Z</italic> = 2.342, <italic>χ</italic><sup>2</sup> = 5.48, <italic>P</italic> = 0.019; comparison image: <italic>Z</italic> = 2.765, <italic>χ</italic><sup>2</sup> = 7.64, <italic>P</italic> = 0.006; <xref ref-type="fig" rid="F3">Fig. 3D left</xref>). Importantly, these effects were only observed for the task-relevant distances, but not for the task-irrelevant distances, as shown in a LMM with reconstructed distances from both task-relevant and task-irrelevant cognitive maps as regressors to predict memory confidence (<xref ref-type="fig" rid="F3">Fig. 3</xref>, middle; task-relevant distances: <italic>Z</italic> = 2.373, <italic>χ</italic><sup>2</sup> = 5.63, <italic>P</italic> = 0.0178; task-irrelevant distances: <italic>Z</italic> = 1.094, <italic>χ</italic><sup>2</sup> = 1.20, <italic>P =</italic> 0.273). Similar effects were found for source memory responses, for which a logistic model showed higher performance of items encoded with larger task-relevant distances (<italic>Z</italic> = 3.995, <italic>χ</italic><sup>2</sup> = 16.01, <italic>P</italic> &lt; 0.001) and even an opposing effect of task-irrelevant distances (<italic>Z</italic> = -2.667, <italic>χ</italic><sup>2</sup> = 7.12, <italic>P</italic> = 0.007) (<xref ref-type="fig" rid="F3">Fig. 3</xref>, middle), i.e. worse source memory if distances in the non-relevant spaces were large. Finally, we found that concept remapping predicted better subsequent memory (<italic>Z</italic> = 2.758, <italic>χ</italic><sup>2</sup> = 7.60, <italic>P</italic> = 0.006), and a model with both task-relevant distance and concept remapping showed significant effects of both predictors (task-relevant distance: <italic>Z</italic> = 2.352, <italic>χ</italic><sup>2</sup> = 5.53, <italic>P</italic> = 0.019; concept remapping: <italic>Z</italic> = 2.647, <italic>χ</italic><sup>2</sup> = 7.01, <italic>P</italic> = 0.008; <xref ref-type="fig" rid="F3">Fig. 3</xref> right), suggesting that distance metrics within and across cognitive maps contribute to memory.</p><p id="P22">Together, these findings indicate that task-relevant distances in cognitive maps are related to memory formation, and that cognitive map representations and SMEs coincide in the MTL. Further, we find that associative, but not primary sensory cortices is accompanied with successful memory formation. This suggests that higher-level representational formats may be preferentially encoded into memory traces.</p></sec><sec id="S7"><title>Disentangling task-relevant and stimulus-driven representational formats in memory traces</title><p id="P23">Next, we aimed to test this prediction of higher-level representational formats in episodic memory traces directly. It is widely assumed that memory traces in the MTL, and in particular the HC, serve as “pointers” or “indices” to neocortical representations (<xref ref-type="bibr" rid="R98">Teyler and DiScenna 1986</xref>; <xref ref-type="bibr" rid="R99">Teyler and Rudy 2007</xref>). While our findings presented thus far suggest that the accessibility and the representational structure of episodic memory traces involve task-relevant cognitive maps, the representational format of neocortical information in these traces still remains unclear.</p><p id="P24"><italic>We</italic> identified the neural representations of stimulus-driven perceptual and conceptual formats and compared them to the formats representing task-relevant cognitive maps and to subsequent memory effects. Perceptual formats were captured via the different layers of a convolutional DNN model (“AlexNet”) trained to classify images into semantic categories (<xref ref-type="bibr" rid="R61">Krizhevsky, Sutskever, and Hinton 2017</xref>). This model hierarchically processes image features of increasing visual complexity. Specifically, we obtained model RSMs for each layer of the DNN (convolutional layers 1-5, fully-connected layers 6-8), representing the similarity structure between all pairs of images, and then used these RSMs to identify brain regions with a similar representational structure (<xref ref-type="fig" rid="F4">Fig. 4A</xref>, left; <xref ref-type="supplementary-material" rid="SD1">fig. S4</xref>). Replicating findings from earlier work (<xref ref-type="bibr" rid="R11">Cichy et al. 2016</xref>; <xref ref-type="bibr" rid="R39">Güclü, and van Gerven 2015</xref>; <xref ref-type="bibr" rid="R63">Kuzovkin et al. 2018</xref>), we found that regions across occipital cortex and VVS contain representations that correspond to DNN model features, showing matching of basic visual features from convolutional layers (“Conv format”) with representations in occipital cortices, and matching of more complex visual features from fully-connected layers (“Fc format”) in lateral occipital and inferior temporal cortices (<xref ref-type="fig" rid="F4">Fig. 4A</xref>, right; <xref ref-type="supplementary-material" rid="SD1">Table S2</xref>).</p><p id="P25">Beyond these lower-level and higher-level perceptual formats, we identified stimulus-driven conceptual representations (“Concept format”) by conducting a searchlight-based RSA that compared the neural similarity between animal exemplars of the same concept with exemplars of different concepts (<xref ref-type="fig" rid="F4">Fig. 4B</xref>, left), while controlling for their perceptual similarity (details see <xref ref-type="sec" rid="S9">Methods</xref>). This analysis showed that medial regions of the VVS extending into bilateral PHC (<xref ref-type="fig" rid="F4">Fig. 4B</xref>, right, <xref ref-type="supplementary-material" rid="SD1">Table S3</xref>) are associated with the representation of concepts in a stimulus-driven (i.e., task-independent) format, above and beyond perceptual similarities between concept exemplars.</p><p id="P26">In comparison to these stimulus-driven (i.e., task-independent) representational formats, we also assessed task-relevant representational formats from cognitive maps (“Map format”) across the entire brain. Using the same approach as shown in <xref ref-type="fig" rid="F1">Fig. 1G</xref> for MTL regions but based on searchlight analyses rather than ROI-based analyses, we found selective representations of task-relevant vs. task-irrelevant cognitive maps across extended areas of ventral and dorsal visual stream (occipitotemporal cortex, fusiform gyrus, middle and superior temporal gyrus and inferior parietal cortex; <xref ref-type="fig" rid="F3">Fig. 3C</xref>; <xref ref-type="supplementary-material" rid="SD1">Table S4</xref>).</p><p id="P27">We then compared the spatial extent of the diverse representational formats across the brain. Among all voxels that were involved in the representation of at least one format, more voxels represented the stimuli in Map format (62%) than in any of the stimulus-driven formats (Conv format: 16%; Fc format: 14%; concept format: 8%; deviation from uniform distribution, binomial test: <italic>P</italic> &lt; 0.001; <xref ref-type="fig" rid="F4">Fig. 4D</xref>, inner circle). Further, we find that most voxels showing stimulus-driven representational formats (Conv, Fc, Concept formats) are non-unique, i.e. also show representations in other formats (Conv: 78%, Fc: 97%; Concept: 94%). This overlap extended across large portions of the VVS (<xref ref-type="fig" rid="F4">Fig. 4E</xref>). Interestingly, we found that the Map format extends into occipital and occipitotemporal regions, suggesting that task-relevant cognitive maps influence sensory processing over and above stimulus-driven representational formats, even at early stages of the VVS.</p><p id="P28">Finally, we tested for the overlap of representational formats with the voxels that were relevant for memory formation (SME voxels, see <xref ref-type="fig" rid="F3">Fig. 3C</xref>). We observed that 81% of SME voxels overlapped with stimulus-driven and/or task-dependent representational formats, suggesting memory traces with multiple representational formats. Specifically, 78% of SME voxels were involved in representations in the Map format, 5% in Conv format, 50% in Fc format, and 14% in Concept format (<xref ref-type="fig" rid="F4">Fig. 4F</xref>).</p><p id="P29">In summary, our results demonstrate that stimulus-driven DNN and conceptual formats are represented in distinct sensory and associative VVS areas, while task-relevant features from cognitive maps involve broader cortical networks. We show that task-relevant formats overlap with stimulus-driven formats, even at early stages of sensory processing, suggesting dynamic integration of stimulus-dependent and task-relevant formats in the brain. Finally, our findings highlight the relevance of both task-relevant and high-level sensory representational formats in the formation of multi-layered memory traces.</p></sec></sec><sec id="S8" sec-type="discussion"><title>Discussion</title><p id="P30">Spatial and conceptual cognitive maps in the MTL have been demonstrated in a variety of tasks (<xref ref-type="bibr" rid="R13">Constantinescu et al. 2016</xref>; <xref ref-type="bibr" rid="R19">Deuker et al. 2016</xref>; <xref ref-type="bibr" rid="R33">Garvert et al. 2017</xref>; <xref ref-type="bibr" rid="R97">Tavares et al. 2015</xref>; <xref ref-type="bibr" rid="R100">Theves et al. 2019</xref>). However, previous studies focused on the formation of the cognitive map itself, but not on how preexisting conceptual knowledge may be flexibly retrieved and organized in cognitive maps in the service of goal-directed behavior. To the best of our knowledge, this study is the first to demonstrate that cognitive maps in the MTL dynamically organize conceptual knowledge in low-dimensional (2D) feature spaces and that these maps can be flexibly updated according to task-demands. We speculate that our findings point towards a general mechanism that extends to arbitrary feature dimensions, enabling adaptive behavior and shaping the subsequent accessibility of information in memory traces.</p><p id="P31">On a behavioral level, we found that response times during conceptual comparisons depended on the magnitude of rated distances. Similar effects have been observed in perceptual discrimination tasks and are in line with seminal work suggesting that distances between abstract features may be measured via similarity judgements (<xref ref-type="bibr" rid="R18">Dehaene, Bossini, and Giraux 1993</xref>; <xref ref-type="bibr" rid="R48">Hutchinson and Lockhead 1977</xref>; <xref ref-type="bibr" rid="R69">Moyer 1973</xref>; <xref ref-type="bibr" rid="R70">Moyer and Landauer 1967</xref>), putatively reflecting a spatial component in the organization of abstract knowledge (<xref ref-type="bibr" rid="R9">Boffini and Doeller 2020</xref>). However, these behavioral observations alone are not sufficient to imply the relevance of cognitive maps during conceptual similarity judgements. By only considering distances between untested concept pairs (i.e. inferred pairs) in the neural basis of cognitive maps, we demonstrate that concept representations from individual trials are embedded in task-specific cognitive maps that carry information beyond the explicitly rated concept pairs. Our results thus show that neural representations of cognitive maps represent distances between concepts in a low-dimensional feature space, that these maps of relational knowledge generalize to arbitrary concept comparisons, and that they flexibly adapt to current task demands. While we found that neural similarities in MTL reflect task-relevant distances, we also observed negative relationships between neural similarities and task-irrelevant distances in PHC and PRC. Given that natural feature dimensions may not be entirely orthogonal and thus may overlap (i.e. larger animals are typically less approachable than smaller animals), negative alignment with task-irrelevant features may reflect a “repulsion” mechanism to enhance the distinctiveness of the representation of task-relevant features (<xref ref-type="bibr" rid="R10">Chanales et al. 2017</xref>; <xref ref-type="bibr" rid="R27">Favila, Chanales, and Kuhl 2016</xref>).</p><p id="P32">There has been ongoing debate on whether the MTL is involved in the representation of abstract concepts in a context-dependent or context-independent manner (<xref ref-type="bibr" rid="R87">Quiroga 2020</xref>; <xref ref-type="bibr" rid="R96">Suthana et al. 2021</xref>). While research on concept cells show invariant responses to the same concept disregarding contextual aspects (<xref ref-type="bibr" rid="R86">Quiroga 2012</xref>; <xref ref-type="bibr" rid="R89">Rey et al. 2015</xref>, <xref ref-type="bibr" rid="R90">2025</xref>), other studies posit that HC cells rather represent conjunctions of concept- and episodic features (<xref ref-type="bibr" rid="R58">Kolibius et al. 2023</xref>) and show mixed-selectivity to a variety of features depending on task demands (<xref ref-type="bibr" rid="R23">Donoghue et al. 2023</xref>). We find evidence for contextdependent conceptual representations in HC, MTL and beyond, indicating wide-spread representations of task-relevant cognitive maps in the brain at the macroscopic level of fMRI responses. Further, remapping of HC representations of the same animal concept across two behavioral contexts argues against context-independent representations in HC and rather suggests concept representations in the MTL that depend on task-demands.</p><p id="P33">Indeed, whole-brain analyses revealed that map-like representational formats were also found in ventral temporal and lateral parietal cortices, confirming that representations of task-relevant representational formats are not restricted to the MTL but extend into neocortical regions. Although previous studies identified prefrontal cortex (PFC) in representing spaces of abstract features (<xref ref-type="bibr" rid="R13">Constantinescu et al. 2016</xref>) or influencing inference in hippocampal representations (<xref ref-type="bibr" rid="R34">Garvert et al. 2023</xref>), we did not find involvement of PFC in task-relevant cognitive maps, likely due to the sensory nature of our stimuli and the analysis of task-relevant cognitive maps during their viewing. However, while the distinct contributions of MTL and neocortex remain speculative, we found that hippocampal neural pattern similarity was related to both space and concept remapping metrics, suggesting that the behavioral relevance of neocortical representations may be mediated at least in part by the HC. Indeed, HC has been described as a hub that integrates perceptual and mnemonic formats (<xref ref-type="bibr" rid="R104">Treder et al. 2021</xref>), coordinating representational reinstatement in neocortical areas (<xref ref-type="bibr" rid="R80">Pacheco Estefan et al. 2019</xref>). Evidence from rodents further suggests that HC and midbrain regions project context-specific mnemonic information to neocortical layer 1, highlighting task-relevant features, even during early stages of sensory processing (<xref ref-type="bibr" rid="R93">Shin, Doron, and Larkum 2021</xref>). Our data support this idea by showing that task-relevant information affected neural representations during the viewing of the images, before behavioral ratings were obtained, suggesting that context-specific information interacted with sensory processing of stimuli at multiple stages of the ventral visual stream hierarchy. In the future, studies using invasive electrophysiological (iEEG) recordings in human epilepsy patients, combining high spatial and temporal resolution, could shed further light on the distinct roles of HC and neocortical reinstatement of task-specific formats in humans.</p><p id="P34">Probing conceptual cognitive maps of natural stimuli allowed us to test the relevance of task-relevant distances for later memory. We speculated that conceptual distances may index the degree of behavioral discriminability between concepts, providing resistance to interference in neural representations and relating to the strength and accessibility of memories. Interestingly, we found that Euclidean distances affected memory not only for comparison images, i.e. images whose conceptual features were explicitly related to their respective reference images, but also for the reference images themselves. This result suggests an impact of cognitive map features on subsequent memory. Specifically, while rated distances for comparison images presumably reflect behavioral measures of contextual deviance, i.e. local or trial-level distances, representations of reference images may reflect the embedding of concepts into a global reference frame. For example, when the size of an animal is task-relevant, an elephant presented as a reference will likely induce expectations of a relatively smaller comparison animal, leading to effects of conceptual distance on reference images as well. Map-based effects on memory are further supported by our finding that concept remapping, a metric reflecting concept-level distinctiveness between cognitive maps irrespective of trial-level distance, was associated with subsequent memory strength as well.</p><p id="P35">We reasoned that memory traces of individual exemplars may consist of both stimulus-driven (i.e. perceptual and/or conceptual) and context-dependent (i.e. task-relevant) representational formats. Replicating previous studies, we observed hierarchically organized perceptual representational formats in occipital cortex and VVS that matched the formats in convolutional and fully-connected DNN layers (Güclü, and van Gerven 2015; <xref ref-type="bibr" rid="R63">Kuzovkin et al. 2018</xref>). We (<xref ref-type="bibr" rid="R45">Heinen et al. 2023</xref>, <xref ref-type="bibr" rid="R46">2025</xref>) and others (<xref ref-type="bibr" rid="R16">Davis et al. 2021</xref>) have previously shown that either perceptual or conceptual formats can be preferentially encoded and reactivated depending on task demands. Conceptual formats of various object categories have been identified across the neocortex, and animal concepts occurred predominantly in lateral occipital and inferior temporal cortices (<xref ref-type="bibr" rid="R49">Huth et al. 2012</xref>). Our results align with these findings as they provide evidence for exemplar-invariant concept representations in VVS. Importantly, compared to these stimulus-driven perceptual and conceptual representations, we observed context-dependent representations in anatomically overlapping brain regions, suggesting multi-layered representational formats and potentially a tuning of stimulus-dependent representations towards task-relevant features in support of behavior. Indeed, our findings suggest that stimulus-dependent conceptual representations as well as task-relevant representations are also found in sensory cortices, suggesting that task-relevant features affect sensory processing even at early stages of the processing hierarchy (<xref ref-type="bibr" rid="R36">Gilbert and Li 2013</xref>; <xref ref-type="bibr" rid="R42">Harel et al. 2014</xref>; <xref ref-type="bibr" rid="R54">Kaiser et al. 2019</xref>). While our findings do suggest that representations in feedforward DNNs can account for representations along the visual hierarchy, these models are unable to account for task-dependent representations in sensory cortices. However, recurrent deep neural networks (<xref ref-type="bibr" rid="R56">Kietzmann et al. 2019</xref>; <xref ref-type="bibr" rid="R81">Pacheco-Estefan et al. 2024</xref>), variational auto encoders (<xref ref-type="bibr" rid="R28">Fayyaz et al. 2022</xref>) or deep-Q networks (<xref ref-type="bibr" rid="R109">Walther et al. 2021</xref>) could potentially extend the predictive nature of DNN models to account for a broader variety of cognitive functions, behaviorally relevant formats, and their neural representations (<xref ref-type="bibr" rid="R22">Doerig et al. 2023</xref>). The relevance of both stimulus- and task-dependent representational formats is further supported by anatomically overlapping effects in brain regions associated with successful memory formation. In these regions specifically, we found that both task-relevant conceptual as well as high-level sensory features were represented, suggesting that both are encoded into memory traces and are putatively relevant to allow for later recognition.</p><p id="P36">Taken together, our findings demonstrate how natural concepts are embedded in task-relevant cognitive maps to flexibly guide goal-directed decision making. Our results provide insights on how preexisting conceptual knowledge is flexibly accessed and represented and how this relates to the formation of memory traces. We extend previous findings by showing context-dependent remapping of hippocampal representations in abstract conceptual spaces of natural stimuli. Further, we show how rated distances from cognitive maps, as well as stimulus- and task-dependent representational formats relate to successful memory formation, suggesting multi-layered representational formats in memory traces.</p></sec><sec id="S9" sec-type="methods"><title>Methods</title><sec id="S10"><title>Sample</title><p id="P37"><italic>N</italic> = 53 participants (15 male, 38 female) engaged in the experiment. All participants were right-handed, aged between 18-36 years (24 ± 4 years), without past or current psychiatric disorders and eligible for MRI protocols according to the requirements of Leibniz Institut für Arbeitsforschung (IfADo), Dortmund, Germany. All participants gave written and informed consent and received either monetary compensation (10€ per hour, total 40€) or student credits for their participation. The study was approved by the ethics committee of the Faculty of Psychology at Ruhr University Bochum (proposal #730) in accordance with the declaration of Helsinki.</p></sec><sec id="S11"><title>Conceptual Navigation Task</title><p id="P38">In the conceptual navigation task performed while undergoing fMRI, participants were presented with pairs of animal images and asked to evaluate the relationship between the animals along predefined conceptual feature dimensions. The four feature dimensions we chose were: <italic>Size</italic> – representing the real-world size of the animals (small - large), <italic>Climate</italic> – indicating the typical climate of the animals’ habitats (cold - warm), <italic>Proximity</italic> – reflecting the distance one would want to have to the animal (avoid - approach), and <italic>Activity</italic> – assessing the perceived activity level of the animal (inactive - active). The pole orientation for each feature dimension (e.g., Climate: cold-left, warm-right) was identical for all participants. Participants completed four functional runs, each comprised of two blocks, where each block required ratings of dimensions from one of two different cognitive maps. Among all participants eligible for analysis (<italic>N</italic>=46, see exclusion criteria below), <italic>N</italic>=23 participants rated <italic>Size</italic> &amp; <italic>Climate</italic> (space<sub>1</sub>) and <italic>Activity</italic> &amp; <italic>Proximity</italic> (space<sub>2</sub>), <italic>N</italic>=16 participants rated <italic>Climate</italic> &amp; <italic>Proximity</italic> (space<sub>1</sub>) and <italic>Activity</italic> &amp; <italic>Size</italic> (space<sub>2</sub>), and <italic>N</italic>=7 participants rated <italic>Size</italic> &amp; <italic>Proximity</italic> (space<sub>1</sub>) and <italic>Activity</italic> &amp; <italic>Climate</italic> (space<sub>2</sub>). The order of spaces within a functional run as well as the order of feature dimensions within a trial was counterbalanced such that each space and dimension was rated equally often as first or second space.</p><p id="P39">Within a block, the order of rating dimensions was randomized. In total, participants encountered 40 animal concepts. Each animal concept occurred in every block, leading to 20 trials in each block. Different exemplars were presented in each block, leading to 8x40 = 320 images in 160 trials across the entire experiment. In each of both spaces, participants encountered four exemplars of a concept. However, the pairs in which a concept was presented in each trial was randomized across blocks and spaces, yielding largely different pairs across spaces.</p><p id="P40">Each block began with an instruction screen, which indicated the two relevant feature dimensions for this block. Each trial was composed of the TR-locked presentation of two animal exemplars of different concepts for 1500ms each, separated by a variable inter-stimulus-interval of either two or three TRs and followed by the consecutive presentation of two rating scales with a maximum duration of 8000ms for response inputs (<xref ref-type="fig" rid="F1">Fig. 1A</xref>). If no response was given in time, participants were shown a message to respond faster in the upcoming trials and the experiment continued with the next trial. On average, participants missed 0.5 ± 0.65 ratings. These trials (image and rating events) were excluded from all analyses.</p></sec><sec id="S12"><title>Recognition memory task</title><p id="P41">The recognition memory task was performed on a laptop outside of the MRI scanner. In total, participants saw 480 animal exemplars split into 6 blocks of 80 trials. Of those 480 images, 320 were exemplars from the first task (i.e. old); 80 images were new exemplars (i.e. there were 2 new exemplars of each of the 40 animal concepts encountered in the first task); and 80 images were new concepts (2 exemplars of 40 novel animal concepts; <xref ref-type="fig" rid="F1">Fig. 1D</xref>).</p><p id="P42">Each trial began with the presentation of a fixation cross for 500ms. This was followed by the presentation of the animal image which was shown until a response was given. Below the image, participants saw a continuous rating scale that ranged from “old” to “new”. Participants were asked to indicate their confidence of having seen the presented image (the exact exemplar, not the animal concept) in the conceptual navigation task using the continuous rating scale. They were explicitly instructed to not only give binary judgements (old, new) but to indicate their level of confidence using the entire rating scale. In case participants indicated having seen the presented image (i.e. responded “old” by selecting a point on the left side of the scale), they were probed on source memory. In a forced choice task, they indicated the space in which they had encountered the presented image (e.g. <italic>Proximity</italic> &amp; <italic>Climate</italic> or <italic>Size</italic> &amp; <italic>Activity</italic>). In case participants indicated that the presented image was new, no source memory test was performed and the experiment continued with the next trial. As a measure of recognition performance, we computed <italic>d</italic>-prime (<xref ref-type="bibr" rid="R111">Yonelinas 2002</xref>; <xref ref-type="bibr" rid="R112">Yonelinas and Parks 2007</xref>) defined as the standardized difference of hits vs. false alarms to control for a participant’s response bias. Further, we tested whether the area under the curve (AUC) across confidence bins (bin size = 5) was reliably above change (0.5) using one-sample <italic>T</italic> tests.</p></sec><sec id="S13"><title>Stimuli &amp; Software</title><p id="P43">All animal images were taken from the <italic>THINGS</italic> database (<xref ref-type="bibr" rid="R44">Hebart et al. 2019</xref>) which provides multiple high-resolution natural images of various different animal concepts. We excluded concepts if they were highly-arousing (e.g. spiders, snakes, ticks) or if not enough images with single animal exemplars were available. We included 93 animal concepts, with ten suitable exemplar images per concept (eight for the conceptual navigation task and two novel exemplars for the recognition memory test). For each participant, we randomly selected 40 animal concepts for the conceptual navigation task, and another 40 animal concepts during the recognition memory task. The experimental tasks were programmed in jspsych (Leeuw, Gilbert, and Luchterhandt 2023) and run in a browser on a Windows 10 Desktop Computer (conceptual navigation task) and a laptop (recognition memory task), respectively. Image presentations were locked to the onset of a TR, realized via simulated button presses emulated by the MR scanner and recognized by the experiment. Exemplary animal images in the figures of the manuscript are either license-free versions (Creative Commons Zero; CC0) provided by the THINGS+ database (<xref ref-type="bibr" rid="R95">Stoinski, Perkuhn, and Hebart 2022</xref>) or custom photographs.</p></sec><sec id="S14"><title>Rated and reconstructed Euclidean distances in conceptual spaces</title><p id="P44">During the conceptual navigation task, we obtained 80 pairwise similarity judgements for each conceptual feature dimension. We used these ratings to compute a trial-level Euclidean distance metric indicating the relationship of each pair of animals given the task-relevant features. To do so, we conceptualized the relative position of the animals in two-dimensional feature spaces where the x- and y-coordinates correspond to the first and second rating response, respectively (<xref ref-type="fig" rid="F2">Fig. 2A</xref>). As participants rated the second animal relative to the first, we defined the position of the reference image (first image) as the origin at coordinates (0,0) and the position of the comparison image (second image) at coordinates (ratingl, rating2) to compute their Euclidean distance. This yielded a trial-level Euclidean distance metric derived from behavioral distance judgements given two conceptual features.</p><p id="P45">However, this trial-level Euclidean distance metric is restricted to pairs of animal concepts that were explicitly rated during the experiment, comprising only a fraction (~10 %) of all possible pairwise combinations of the 40 animal concepts (40x39/2 = 780 unique concept pairs for 40 animal concepts). To estimate conceptual feature distances of all concept pairs, we approximated the optimal position for each concept and feature dimension to reconstruct Euclidean distance metrics between all pairs of animal concepts. To do so, we set up a compressed sparse column matrix (design matrix M) of shape (#ratings x #animals; 80 x 40) for each dimension, where each row in M is composed of {-1,0,1} at cells indexing those animal concepts that were explicitly rated in a given trial, with -1 indicating the cell of the reference image and +1 indicating the cell of comparison image (<xref ref-type="fig" rid="F2">Fig. 2B</xref>). Then, separately for each feature dimension (i.e. Size, Climate, Proximity, Activity), we use a least-squares approach to solve M to best approximate the behavioral ratings obtained on each dimension, yielding a solution for each animal reflecting its estimated position on the feature dimension given the entirety of ratings obtained by a participant. The estimated positions for each animal concept were then used as x- and y-coordinates in the two participant-specific spaces (<xref ref-type="fig" rid="F2">Fig. 2C</xref>). Importantly, we estimated both participant-specific reconstructions of spaces (i.e. based on ratings obtained by a participant) as well as global reconstructions of spaces (i.e. based on all ratings obtained across all participants). For global space reconstructions, we combined individual dimensions into all possible 2D spaces (i.e. Activity &amp; Size, Activity &amp; Climate, Activity &amp; Proximity, Size &amp; Climate, Size &amp; Proximity and Activity &amp; Proximity).</p><p id="P46">To ensure that the obtained solutions for each dimension fit the behavioral responses better than chance, we compared the error in the empirical least-squares solution to a null distribution of random data. Specifically, we estimated concept locations in each participant, extracted the least squares error from the empirical estimation and repeated the analysis 5,000 times, each time shuffling the behavioral responses within participant while keeping the design matrix M constant. Then, we averaged the empirical error across participants and ranked the group-mean in the averaged permutation distribution to obtain a <italic>P</italic>-value of the empirical value.</p><p id="P47">We performed linear mixed models of the statsmodels package (<xref ref-type="bibr" rid="R82">Perktold, J. et al. 2023</xref>) in Python 3.8. To determine the influence of different fixed effects on behavioral metrics, we set up a full model containing all fixed effects and participants as a random effect. We then use a likelihood test to determine whether the exclusion of the factor of interest significantly reduces the explained variance compared with the full model. For main effects with one predictor the reduced model included a constant as fixed effect (1). For main effects with two or more predictors the reduced model contained all other main effects except the effect of interest. For interaction effects, the reduced model included both main effects without the interaction term. For each fixed effect, we report <italic>Z</italic>-statistics from the initial estimation of the model and the respective likelihood derived from model comparison (χ2) and its significance level (<italic>P</italic>).</p></sec><sec id="S15"><title>fMRI acquisition</title><p id="P48">Imaging data were collected on a 3T Siemens Prisma scanner (Magnetom) at the IfaDo, Dortmund, Germany with a 64-channel head coil. We acquired four task-based functional T2* weighted runs using echo-planar imaging (EPI) sequence at multiband acceleration factor 6 (isotropic voxel resolution = 1.6 mm, FOV = 132x132x78, FA = 52°, TR = 1200ms, TE = 31.4ms, phase-encoding direction PE = A-P). The average number of volumes per functional run was 514 ± 47 volumes, with a total average of 2,059 ± 170 volumes per participant. Before the first and after the last functional runs, we acquired 5 volumes with a reversed PE EPI fieldmap sequence (PE = P-A) to correct for susceptibility-induced distortions in the functional images. The acquisition of each EPI scan was preceded by the acquisition of a singleband reference volume. Next, we acquired two T1-weighted anatomical scans, using MPRAGE (isotropic voxel resolution = 1mm, FOV = 256x256x176, FA= 7°, TR = 2530ms, TE = 2.36ms) and MP2RAGE sequences (isotropic voxel resolution = 1mm<sup>3</sup>, FOV = 256x240x176, FA<sub>1/2</sub> = 4°/5°, TI<sub>1/2</sub> = 700ms/2500ms, TR = 5000ms, TE = 2.03ms). Finally, we acquired a T2-weighted structural scan using turbo spin-echo sequence (isotropic voxel resolution = 1 mm<sup>3</sup>, FOV = 256x256x224, FA = 120°, TR = 2800ms, TE = 405ms). As T1-weighted reference image we used the MP2RAGE scan, that was preprocessed using <italic>presurfer (</italic>freely available at <ext-link ext-link-type="uri" xlink:href="https://github.com/srikash/presurfer">https://github.com/srikash/presurfer</ext-link>)(<xref ref-type="bibr" rid="R55">Kashyap 2021</xref>) for compatibility with freesurfer-based reconstruction. For one subject, we used the MPRAGE anatomical image due to movement distortions during the acquisition of the MP2RAGE sequence. Raw MRI data were organized according to “Brain Imaging Data Structure” (BIDS) (<xref ref-type="bibr" rid="R37">Gorgolewski et al. 2016</xref>) using <italic>heudiconv</italic> (v. 0.13.1 freely available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/ReproNim/reproin">https://github.com/ReproNim/reproin</ext-link>) and preprocessed using the <italic>fMRIPrep</italic> (<xref ref-type="bibr" rid="R26">Esteban et al. 2019</xref>) toolbox (v. 21.0.2) run inside a Docker (<xref ref-type="bibr" rid="R67">Merkel 2014</xref>) container. The following sections (Preprocessing of B0 inhomogeneity mappings, Anatomical data preprocessing, Functional data preprocessing) are boilerplate text generated by <italic>fMRIPrep.</italic></p></sec><sec id="S16"><title>Preprocessing of B0 inhomogeneity mappings</title><p id="P49">A total of 4 fieldmaps were found available within the input BIDS structure. A B0-nonuniformity map (or fieldmap) was estimated based on two (or more) echo-planar imaging (EPI) references with topup (<xref ref-type="bibr" rid="R2">Andersson, Skare, and Ashburner 2003</xref>); FSL 6.0.5.1:57b01774).</p></sec><sec id="S17"><title>Anatomical data preprocessing</title><p id="P50">A total of 1 T1-weighted (T1w) images were found within the input BIDS dataset. The T1-weighted (T1w) image was corrected for intensity non-uniformity (INU) with N4BiasFieldCorrection (<xref ref-type="bibr" rid="R105">Tustison et al. 2010</xref>), distributed with ANTs 2.3.3 (<xref ref-type="bibr" rid="R3">Avants et al. 2008</xref>) (RRID:SCR_004757), and used as T1w-reference throughout the workflow. The T1w-reference was then skull-stripped with a Nipype implementation of the antsBrainExtraction.sh workflow (from ANTs), using OASIS30ANTs as target template. Brain tissue segmentation of cerebrospinal fluid (CSF), white-matter (WM) and gray-matter (GM) was performed on the brain-extracted T1w using fast (<xref ref-type="bibr" rid="R114">Zhang, Brady, and Smith 2001</xref>) (FSL 6.0.5.1:57b01774, RRID:SCR_002823). Brain surfaces were reconstructed using recon-all (<xref ref-type="bibr" rid="R15">Dale, Fischl, and Sereno 1999</xref>) (FreeSurfer 6.0.1, RRID:SCR_001847), and the brain mask estimated previously was refined with a custom variation of the method to reconcile ANTs-derived and FreeSurfer-derived segmentations of the cortical gray-matter of Mindboggle (<xref ref-type="bibr" rid="R57">Klein et al. 2017</xref>) (RRID:SCR_002438). Volume-based spatial normalization to one standard space (MNI152NLin2009cAsym) was performed through nonlinear registration with <italic>antsRegistration</italic> (ANTs 2.3.3), using brain-extracted versions of both T1w reference and the T1w template. The following template was selected for spatial normalization: ICBM 152 Nonlinear Asymmetrical template version 2009c (<xref ref-type="bibr" rid="R29">Fonov et al. 2009</xref>) (RRID:SCR_008796; TemplateFlow ID: MNI152NLin2009cAsym).</p></sec><sec id="S18"><title>Functional data preprocessing</title><p id="P51">For each of the 4 BOLD runs found per subject (across all tasks and sessions), the following preprocessing was performed. First, a reference volume and its skull-stripped version were generated by aligning and averaging 1 single-band reference (SBRef). Head-motion parameters with respect to the BOLD reference (transformation matrices, and six corresponding rotation and translation parameters) are were estimated before any spatiotemporal filtering using mcflirt (<xref ref-type="bibr" rid="R52">Jenkinson et al. 2002</xref>) (FSL 6.0.5.1:57b01774). The estimated fieldmap was then aligned with rigid-registration to the target EPI (echo-planar imaging) reference run. The field coefficients were mapped on to the reference EPI using the transform. BOLD runs were slice-time corrected to 0.545s (0.5 of slice acquisition range 0s- 1.09s) using 3dTshift from AFNI (<xref ref-type="bibr" rid="R14">Cox and Hyde 1997</xref>) (RRID:SCR_005927). The BOLD reference was then co-registered to the T1w reference using bbregister (FreeSurfer) which implements boundary-based registration (<xref ref-type="bibr" rid="R38">Greve and Fischl 2009</xref>). Co-registration was configured with six degrees of freedom. First, a reference volume and its skull-stripped version were generated using a custom methodology of fMRIPrep. Several confounding time-series were calculated based on the preprocessed BOLD: framewise displacement (FD), DVARS and three region-wise global signals. FD was computed using two formulations following (<xref ref-type="bibr" rid="R84">Power et al. 2014</xref>) (absolute sum of relative motions) and (<xref ref-type="bibr" rid="R52">Jenkinson et al. 2002</xref>) (relative root mean square displacement between affines). FD and DVARS were calculated for each functional run, both using their implementations in Nipype (following the definitions by (<xref ref-type="bibr" rid="R84">Power et al. 2014</xref>)). The three global signals were extracted within the CSF, the WM, and the whole-brain masks. Additionally, a set of physiological regressors were extracted to allow for component-based noise correction (<xref ref-type="bibr" rid="R7">Behzadi et al. 2007</xref>) (CompCor). Principal components were estimated after high-pass filtering the preprocessed BOLD time-series (using a discrete cosine filter with 128s cut-off) for the two CompCor variants: temporal (tCompCor) and anatomical (aCompCor). tCompCor components were then calculated from the top 2% variable voxels within the brain mask. For aCompCor, three probabilistic masks (CSF, WM and combined CSF+WM) were generated in anatomical space. The implementation differs from that of Behzadi et al. in that instead of eroding the masks by 2 pixels on BOLD space, the aCompCor masks were subtracted a mask of pixels that likely contain a volume fraction of GM. This mask was obtained by dilating a GM mask extracted from the FreeSurfer’s aseg segmentation, and it ensures components are not extracted from voxels containing a minimal fraction of GM. Finally, these masks were resampled into BOLD space and binarized by thresholding at 0.99 (as in the original implementation). Components were also calculated separately within the WM and CSF masks. For each CompCor decomposition, the k components with the largest singular values were retained, such that the retained components’ time series are sufficient to explain 50 percent of variance across the nuisance mask (CSF, WM, combined, or temporal). The remaining components were dropped from consideration. The head-motion estimates calculated in the correction step were also placed within the corresponding confounds file. The confound time series derived from head motion estimates and global signals were expanded with the inclusion of temporal derivatives and quadratic terms for each (<xref ref-type="bibr" rid="R92">Satterthwaite et al. 2013</xref>). Frames that exceeded a threshold of 0.5 mm FD or 1.5 standardised DVARS were annotated as motion outliers. The BOLD time-series were resampled into standard space, generating a preprocessed BOLD run in MNIl52NLin2009cAsym space. First, a reference volume and its skull-stripped version were generated using a custom methodology of <italic>fMRIPrep</italic>. All resamplings can be performed with a single interpolation step by composing all the pertinent transformations (i.e. head-motion transform matrices, susceptibility distortion correction when available, and co-registrations to anatomical and output spaces). Gridded (volumetric) resamplings were performed using antsApplyTransforms (ANTs), configured with Lanczos interpolation to minimize the smoothing effects of other kernels (<xref ref-type="bibr" rid="R64">Lanczos 1964</xref>). Non-gridded (surface) resamplings were performed using mri_vol2surf (FreeSurfer).</p></sec><sec id="S19"><title>fMRI data quality control and exclusion criteria</title><p id="P52">As a decision criterion we required average framewise displacement (<xref ref-type="bibr" rid="R83">Power et al. 2012</xref>) (FD), quantifying estimated bulk-head motion, to be lower than 2mm across runs and no single displacement to be larger than 3.9mm. <italic>N</italic>=3 participants did not meet these criteria and were removed from all functional and behavioral analyses. The remaining participants had an average FD of 0.156 ± 0.015 mm across runs. We excluded another <italic>n</italic> = 3 participants because of a zipper artifact contaminating more than two functional runs of functional T2* weighted data runs, likely caused due to a malfunctioning head-coil. The zipper artefact was also present in <italic>n</italic>=8 participants in one half of one functional run. We did not exclude these participants entirely but merely excluded the data from the contaminated runs from functional analyses and confirmed that the remaining data was non-contaminated and of sufficient quality. Importantly, as described in earlier sections, all experimental conditions, the number of concept presentations and ratings per dimension were equally distributed across functional runs. Therefore, for affected participants, the number of trials per condition were still constant, merely reducing to 60 instead of 80 trials per condition, ensuring balanced contrasts estimated at the subjectlevel. Finally, one participant aborted data acquisition and had to be excluded from further analyses, resulting in a final sample of <italic>N</italic>=46 participants.</p><p id="P53">To ensure high data quality, raw and preprocessed anatomical and functional scans were visually inspected by trained experts and evaluated by computing metrics for quality control using the LN_SKEW function (v. 2.5.2; <ext-link ext-link-type="uri" xlink:href="https://github.com/layerfMRI/LAYNII">https://github.com/layerfMRI/LAYNII</ext-link>) provided by LayNii (<xref ref-type="bibr" rid="R47">Huber et al. 2021</xref>). Finally, personal facial features were removed from anatomical T1w scans using pydeface (v. 2.0.0; <ext-link ext-link-type="uri" xlink:href="https://github.com/poldracklab/pydeface/blob/master/Dockerfile">https://github.com/poldracklab/pydeface/blob/master/Dockerfile</ext-link>) (<xref ref-type="bibr" rid="R40">Gulban et al. 2022</xref>).</p></sec><sec id="S20"><title>Regions of interest</title><p id="P54">For wholebrain searchlight analyses, we restricted the search space to subject-specific masks of gray matter obtained from <italic>freesurfer</italic> segmentation. Since we had strong hypotheses for effects in the MTL, and more specifically HC, we additionally conducted analyses in predefined regions of interest (ROI) in PHC, PRC, EC which were obtained from detailed participant-specific segmentations in native space using the cloud-based Automated Segmentation of Hippocampal Subfields (ASHS) ASHS-PMC-T1 atlas (<xref ref-type="bibr" rid="R113">Yushkevich et al. 2015</xref>; <xref ref-type="bibr" rid="R110">Xie et al. 2016</xref>). For descriptive statistics on average number of voxels and temporal signal-to-noise ratio please see Supplement (<xref ref-type="supplementary-material" rid="SD1">fig. S5</xref>). For second level analyses, all participant-specific masks were transformed to MNI space (template MNI152NLin2009cAsym) using functionality provided by ANTs and merged into one group-level mask.</p></sec><sec id="S21"><title>Univariate fMRI analyses</title><p id="P55">We identified regions across the brain where BOLD activity during viewing of images in the conceptual navigation task was associated with later memory. The data for each MNI-transformed functional run was modeled using a general linear model (GLM) composed of stimulus onset regressors for 1) subsequently remembered images, 2) subsequently forgotten images, 3) rating events, 4) button presses, and 5) a parametric modulator weighting the magnitude of subsequently remembered images by the mean-centered memory confidence ratings obtained during the recognition task. The modulated regressor followed the unmodulated image onset regressor for remembered scenes in the model (<xref ref-type="bibr" rid="R72">Mumford, Poline, and Poldrack 2015</xref>). The duration of image events as well as their parametric modulation was equal to the presentation duration of images, i.e. 1500ms.</p><p id="P56">In a second GLM, we tested whether the magnitude of trial-level 2D distances (rated) was reflected in BOLD activity during image presentations. Therefore, the data for each MNI-transformed functional run was modeled by stimulus onset regressors for 1) images, 2) ratings, 3) button presses, and 4) a parametric modulator weighting the magnitude of BOLD activity during image presentations by the mean-centered trial-level 2D distance obtained from the ratings in each trial. Again, the modulated regressor followed the unmodulated image onset regressor in the model. Importantly, the parametric modulator was the same for both image onsets in a trial (reference and comparison images).</p><p id="P57">In both GLMs, we included six motion regressors (three rotational, three translational), six drift model regressors, and an intercept as regressors of no interest. In case <italic>fMRIPrep</italic> confounds estimation indicated the presence of non-steady-state volumes in the beginning of a functional run, we included those in the model, with separate onset regressors for each marked non-steady state volume. Each regressor of interest was convolved with a double-gamma hemodynamic response function (HRF). On the subject-level, we applied 4.8mm smoothing and used <italic>Z</italic>-scored beta maps for the individual regressors of interest for subsequent group-level analysis.</p></sec><sec id="S22"><title>Extraction of beta-series</title><p id="P58">To estimate event-specific BOLD activation maps, we followed the least-squares separate (LS-S) approach for subsequent multivariate pattern analyses, which has been described to be well suited for fast event-related designs (<xref ref-type="bibr" rid="R1">Abdulrahman and Henson 2016</xref>; <xref ref-type="bibr" rid="R73">Mumford et al. 2012</xref>). Separately for each functional run and event (reference image, comparison image, rating1, rating2) we set up a GLM with an event onset regressor containing only the onset of the to-be-estimated event of interest and two other regressors containing all event onsets of the remaining image and rating events. Durations for image events were equal to the stimulus presentation (1.5s) while the duration of rating events corresponded to the response times associated with each rating event. Additional confound regressors were six regressors modelling motion (three translational, three rotational), six drift-model regressors, and one constant intercept regressor. In case <italic>fMRIPrep</italic> confounds estimation indicated the presence of non-steady-state volumes in the beginning of a functional run, we included those in the model, with separate onset regressors for each marked non-steady state volume. After model estimation, we extracted the beta-values for the event of interest and used the resulting brain activation map for subsequent multivariate searchlight representational similarity analysis.</p></sec><sec id="S23"><title>Representational similarity analysis</title><p id="P59">We conducted representational similarity analysis based on the event-specific activity maps obtained from LS-S trial-level beta estimation (<xref ref-type="bibr" rid="R73">Mumford et al. 2012</xref>). The following procedure was equally applied to predefined regions of interest in MTL or individual searchlight spheres in native space. For all individual presentations of an animal image (only image events), we computed pairwise correlations across the multi-voxel activity patterns using Spearman’s rank correlation, resulting in a symmetric image x image similarity matrix (size = 320x320) for each region of interest or searchlight that was Fisher-<italic>Z</italic>-transformed before further computations. Importantly, to minimize the influence of false positives in pattern similarity estimates, we restricted all analyses to between-run similarities only (<xref ref-type="bibr" rid="R71">Mumford, Davis, and Poldrack 2014</xref>), thereby avoiding confounds due to temporal autocorrelations among volumes acquired in the same functional run.</p></sec><sec id="S24"><title>Searchlight</title><p id="P60">We conducted wholebrain RSA (<xref ref-type="bibr" rid="R60">Kriegeskorte, Goebel, and Bandeffini 2006</xref>) with the searchlight approach, building on python-based functionality provided by the <italic>rsatoolbox</italic> (<xref ref-type="bibr" rid="R74">Nili et al. 2014</xref>) (v 0.1.5). We scanned the gray-matter masks obtained from freesurfer-based registration in native space of each participant with spherical searchlights of radius n = 5 voxels and required a minimum of 50 voxels per searchlight to be considered for further analyses. For each individual searchlight sphere, we controlled for mean BOLD activity and visual similarity (for details see below). The resulting first-level maps after RSA contrasts were then transformed into MNI space using ANTs (<xref ref-type="bibr" rid="R4">Avants et al. 2014</xref>) <italic>applyTransforms</italic> (v 2.5.0) for second-level analysis.</p></sec><sec id="S25"><title>Second-level analysis</title><p id="P61">First-level statistical maps from univariate and searchlight RSA analyses were subjected to non-parametric second-level analysis with 5,000 permutations and threshold-free cluster enhancement (<xref ref-type="bibr" rid="R94">Smith and Nichols 2009</xref>) (<italic>TFCE</italic>) as implemented in <italic>nilearn</italic>. We report <italic>T</italic> and <italic>TFCE</italic> statistics for clusters surviving <italic>TFCE</italic>-correction for multiple comparisons at a family-wise error rate (<italic>FWER</italic>) of <italic>P</italic><sub>FWER</sub>&lt;0.05. MNI coordinates, and peak statistical values associated with each cluster as well as associated anatomical labels for peak voxels according to the AAL2 atlas (Rolls, Joliot, and Tzourio-Mazoyer 2015) were extracted using AtlasReader (<xref ref-type="bibr" rid="R78">Notter et al. 2019</xref>) and are reported in the Supplement.</p></sec><sec id="S26"><title>Deep neural network model</title><p id="P62">We used a pre-trained convolutional deep neural network (cDNN), “AlexNet”(<xref ref-type="bibr" rid="R61">Krizhevsky et al. 2017</xref>), as implemented in the Caffee framework (<xref ref-type="bibr" rid="R53">Jia et al. 2014</xref>) in python 2.7. To analyze cDNN similarities, we first generated features for each layer of the AlexNet for all images. In each layer, we averaged across the spatial dimension keeping only one value per feature. We then correlated the features of each image with all other images using Spearman’s rank correlation, resulting in 8 image x image correlation matrices, one for each layer. These correlation maps represent the similarity of each image to all other images across the layers of the cDNN based on perceptual properties of increasing complexity, starting with similarities based on early visual information in convolutional layers and concluding with higher-order visual information in fully-connected layers.</p><p id="P63">To relate the representational formats of the cDNN layers to brain representations, we conducted searchlight-based RSA separately for each layer of the network. For each searchlight, we took the fisher- <italic>Z</italic>-transformed similarities between all images (image x image matrix) and used Spearman’s rank correlation to compare matrices between brain and model layers, yielding a correlation map across the brain for each participant. To determine the best layer fit for each voxel in the brain (<xref ref-type="fig" rid="F3">Fig. 3B</xref>) we first identified all voxels that were part of at least one layer-specific cluster after <italic>TFCE</italic> correction at <italic>P</italic><sub>FWER</sub> &lt; 0.05. If a voxel showed significant alignment with multiple layers of the cDNN, we compared the voxel-level <italic>T</italic>-value from second-level comparisons and color coded the voxel by the layer with the highest <italic>T</italic>-value contrast.</p></sec><sec id="S27"><title>Controlling neural similarity for mean BOLD activity and perceptual formats</title><p id="P64">We regressed variance from neural similarity estimates attributable to 1) the mean levels of BOLD activation during image presentation (<xref ref-type="bibr" rid="R17">Davis et al. 2014</xref>; <xref ref-type="bibr" rid="R73">Mumford et al. 2012</xref>) and 2) the similarity due to low- and high-level perceptual features as predicted by the cDNN. For each ROI or searchlight sphere, we set up a GLM predicting the neural pattern similarity for all between-run similarities by two regressors corresponding to the mean BOLD activation across voxels for each image that contribute to the pairwise similarity estimate and 2) eight regressors corresponding to the perceptual similarity of individual cDNN layers (conv1-5, fc6-8) reflecting the magnitude of perceptual similarity of increasingly complex visual formats (<xref ref-type="supplementary-material" rid="SD1">fig. S6</xref>; for similar procedure see (<xref ref-type="bibr" rid="R54">Kaiser et al. 2019</xref>)). The obtained residuals from this GLM were used for further analyses. Note that to identify cDNN-predicted perceptual formats of the brain, we only regressed mean BOLD activity from neural similarity estimates.</p></sec><sec id="S28"><title>Stimulus-dependent conceptual representations</title><p id="P65">We conducted a wholebrain searchlight analysis testing for stimulus-driven conceptual representations in the brain. To do so, we used neural similarity estimates adjusted for mean BOLD activity and visual formats (see above) for all images. Then, we average across all pairwise similarities of animal exemplars belonging to the same animal concept (e.g. bird1 – bird2, bird2 - bird3) and contrasted it with the average pairwise similarity between animal exemplars belonging to a different animal concept (e.g. bird 1 – monkey 1, bird 1 – sheep 2).</p></sec><sec id="S29"><title>Neural representation of cognitive map-like formats</title><p id="P66">To compare neural representational similarities with reconstructed distances from behavioral cognitive maps we used the residuals after regression of BOLD magnitude and DNN-predicted perceptual similarities of all between-run similarities of the image x image representational similarity matrix (RSM), separately for space<sub>1</sub> and space<sub>2</sub>. Next, for each space, we averaged similarities across exemplars of the same animal, yielding a space-specific RSM of shape concept x concept. Similarly, we obtained a dissimilarity matrix (RDM) of behavioral distance for pairs of concepts by extracting the task-relevant (same space) and task-irrelevant (difference space) distances from reconstructed cognitive maps. To restrict our analysis to inferred concept relationships, in each subject we determined pairs of concepts that were explicitly rated during similarity judgements and excluded these from both neuronal and rated matrices. Next, we concatenated space1 and space2 vectors for both neuronal similarities and reconstructed distances with matching order (i.e. task-relevant space; neural space<sub>12</sub> – rated space<sub>12</sub>) and once with flipped order for reconstructed distances (i.e. task-irrelevant space; neural space<sub>12</sub> – rated space<sub>21</sub>) for task-specific comparisons (i.e. task-relevant spaces vs. task-irrelevant spaces). Then, we correlated between neural and rated vectors, separately for task-relevant (same space) and task-irrelevant space (different space). Since we correlated neural similarities with rated distances, we sign-flipped the resulting correlation values for further comparisons. Finally, we subtracted task-irrelevant similarity from task-relevant similarity to assess task-specific similarity. This analysis yielded three statistical maps (task-relevant; task-irrelevant; task-specific) that were used for second level analysis.</p></sec><sec id="S30"><title>Space- and concept-level remapping</title><p id="P67">To test the relatedness of behavioral and neural space representations, we conducted correlation analysis using Spearman’s rank correlation between rated and neural RSMs (i.e. neural space<sub>1</sub> – neural space<sub>2</sub>; rated space<sub>1</sub> – rated space<sub>2</sub>). For neural RSMs, we used the concept x concept RSMs computed across voxels in predefined ROIs of the MTL. This yielded two correlation values per participant, i.e. space<sub>1</sub>-space<sub>2</sub> similarity based on reconstructed and neural cognitive maps. In a second step, we correlated between neural and rated space<sub>1</sub>-space<sub>2</sub> similarity values across participants.</p><p id="P68">Further, we quantified the behavioral distinctiveness of individual concepts across cognitive maps and use the term “concept remapping” for this metric. Specifically, we capitalized on participant-specific reconstructed cognitive maps and for each concept extracted the Euclidean distance to the origin within each map (coordinates [0,0]), yielding two distance metrics per concept. Note that we used the distance to the origin since this metric is agnostic to the direction of the vector to the origin which is crucially different between both spaces that are constituted of different conceptual features that are not directly comparable. Finally, for each concept, we computed the difference in the distance to the origin between spaces (i.e. concept remapping = dist2origin space<sub>1</sub> – dist2origin space<sub>2</sub>) yielding one concept remapping value per animal concept. Note that this metric quantified the degree to which a concept is differently positioned across cognitive maps. Specifically, this metric would be low for animals equally exposed position in both maps (i.e. a lion may be considered both big, avoidable, active and living in a warm climate), but high for animals that are attributed special characteristics in one but not the other space (i.e. a grasshopper may be considered relatively small and active but indifferently rated regarding proximity or climate).</p><p id="P69">We hypothesized that concept remapping would be a suitable candidate to quantify concept-level distinctiveness across cognitive maps. Separately for each MTL region, we again extracted pairwise similarities across all images and averaged across all exemplars of the same concept that were encountered in the same space or in a different space, yielding two similarity estimates per concept. Next, within participants, we correlated concept remapping with concept averaged same and different neural similarities using Spearman’s rank correlation and performed a one-sample <italic>T</italic> test across subjects. Accordingly, we conducted an LMM, predicting behavioral concept remapping with same and different neural similarity as fixed effects and participants as random effects.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Materials</label><media xlink:href="EMS205917-supplement-Supplementary_Materials.pdf" mimetype="application" mime-subtype="pdf" id="d17aAcEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S31"><title>Acknowledgements</title><sec id="S32"><title>Funding</title><p>This work was supported by grants from the European Research Council (grant: CoG 864164 to N.A.), the German Israeli Foundation (grant: I-1478-418.13/2018 to N.A.) and the German Research Foundation (grant: 419049386 to N.A.).</p></sec></ack><sec id="S33" sec-type="data-availability"><title>Data and materials availability statement</title><p id="P70">All original code, behavioral data, de-identified anatomical and processed functional MRI data have been deposited at Zenodo and will be publicly available as of the date of publication in a peer-reviewed journal.</p></sec><fn-group><fn id="FN1" fn-type="con"><p id="P71"><bold>Author contributions</bold></p><p id="P72">Conceptualization: E.M.B.R, R.H., N.A.H., E.Z.P., M.K., N.A.</p><p id="P73">Methodology: E.M.B.R., R.H., N.A.H., E.Z.P., M.K., K.A., N.A.</p><p id="P74">Software: E.M.B.R, R.H., N.A.H., M.K., K.A.</p><p id="P75">Validation: E.M.B.R, R.H., M.K., K.A.</p><p id="P76">Formal Analysis: E.M.B.R., R.H., M.K.</p><p id="P77">Investigation: E.M.B.R., R.H., N.A.H.</p><p id="P78">Resources: E.M.B.R., R.H., N.A.H., K.A.</p><p id="P79">Data curation: E.M.B.R., R.H., N.A.H.</p><p id="P80">Writing – original draft: E.M.B.R.</p><p id="P81">Writing – review &amp; editing: E.M.B.R., R.H., E.Z.P., M.K., K.A., N.A.</p><p id="P82">Visualization: E.M.B.R., R.H., N.A.H.</p><p id="P83">Supervision: N.A.</p><p id="P84">Project administration: E.M.B.R., N.A.</p><p id="P85">Funding acquisition: N.A.</p><p id="P86">We thank E. Genç and M. Burke for their contributions to MRI sequence development and technical support during data acquisition. Further, we thank L. Ehrlich, A. Venjakob and R. Merz for assistance during data collection and preparation.</p></fn><fn id="FN2" fn-type="conflict"><p id="P87"><bold>Competing interests</bold></p><p id="P88">The authors declare no competing interests.</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abdulrahman</surname><given-names>Hunar</given-names></name><name><surname>Henson</surname><given-names>Richard N</given-names></name></person-group><article-title>Effect of Trial-to-Trial Variability on Optimal Event-Related fMRI Design: Implications for Beta-Series Correlation and Multi-Voxel Pattern Analysis</article-title><source>NeuroImage</source><year>2016</year><volume>125</volume><fpage>756</fpage><lpage>66</lpage><pub-id pub-id-type="pmcid">PMC4692520</pub-id><pub-id pub-id-type="pmid">26549299</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.11.009</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andersson</surname><given-names>Jesper LR</given-names></name><name><surname>Skare</surname><given-names>Stefan</given-names></name><name><surname>Ashburner</surname><given-names>John</given-names></name></person-group><article-title>How to Correct Susceptibility Distortions in Spin-Echo Echo-Planar Images: Application to Diffusion Tensor Imaging</article-title><source>NeuroImage</source><year>2003</year><volume>20</volume><issue>2</issue><fpage>870</fpage><lpage>88</lpage><pub-id pub-id-type="pmid">14568458</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avants</surname><given-names>BB</given-names></name><name><surname>Epstein</surname><given-names>CL</given-names></name><name><surname>Grossman</surname><given-names>M</given-names></name><name><surname>Gee</surname><given-names>JC</given-names></name></person-group><article-title>Symmetric Diffeomorphic Image Registration with Cross-Correlation: Evaluating Automated Labeling of Elderly and Neurodegenerative Brain</article-title><source>Medical Image Analysis</source><year>2008</year><volume>12</volume><issue>1</issue><fpage>26</fpage><lpage>41</lpage><pub-id pub-id-type="pmcid">PMC2276735</pub-id><pub-id pub-id-type="pmid">17659998</pub-id><pub-id pub-id-type="doi">10.1016/j.media.2007.06.004</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avants</surname><given-names>Brian B</given-names></name><name><surname>Tustison</surname><given-names>Nicholas J</given-names></name><name><surname>Stauffer</surname><given-names>Michael</given-names></name><name><surname>Song</surname><given-names>Gang</given-names></name><name><surname>Wu</surname><given-names>Baohua</given-names></name><name><surname>Gee</surname><given-names>James C</given-names></name></person-group><article-title>The Insight ToolKit Image Registration Framework</article-title><source>Frontiers in Neuroinformatics</source><year>2014</year><volume>8</volume><fpage>44</fpage><pub-id pub-id-type="pmcid">PMC4009425</pub-id><pub-id pub-id-type="pmid">24817849</pub-id><pub-id pub-id-type="doi">10.3389/fninf.2014.00044</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Axmacher</surname><given-names>Nikolai</given-names></name><name><surname>Cohen</surname><given-names>Michael X</given-names></name><name><surname>Fell</surname><given-names>Juergen</given-names></name><name><surname>Haupt</surname><given-names>Sven</given-names></name><name><surname>Dümpelmann</surname><given-names>Matthias</given-names></name><name><surname>Elger</surname><given-names>Christian E</given-names></name><name><surname>Schlaepfer</surname><given-names>Thomas E</given-names></name><name><surname>Lenartz</surname><given-names>Doris</given-names></name><name><surname>Sturm</surname><given-names>Volker</given-names></name><name><surname>Ranganath</surname><given-names>Charan</given-names></name></person-group><article-title>Intracranial EEG Correlates of Expectancy and Memory Formation in the Human Hippocampus and Nucleus Accumbens</article-title><source>Neuron</source><year>2010</year><volume>65</volume><issue>4</issue><fpage>541</fpage><lpage>49</lpage><pub-id pub-id-type="pmid">20188658</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrens</surname><given-names>Timothy EJ</given-names></name><name><surname>Muller</surname><given-names>Timothy H</given-names></name><name><surname>Whiffington</surname><given-names>James CR</given-names></name><name><surname>Mark</surname><given-names>Shirley</given-names></name><name><surname>Baram</surname><given-names>Alon B</given-names></name><name><surname>Stachenfeld</surname><given-names>Kimberly L</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Zeb</given-names></name></person-group><article-title>What Is a Cognitive Map? Organizing Knowledge for Flexible Behavior</article-title><source>Neuron</source><year>2018</year><volume>100</volume><issue>2</issue><fpage>490</fpage><lpage>509</lpage><pub-id pub-id-type="pmid">30359611</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behzadi</surname><given-names>Yashar</given-names></name><name><surname>Restom</surname><given-names>Khaled</given-names></name><name><surname>Liau</surname><given-names>Joy</given-names></name><name><surname>Liu</surname><given-names>Thomas T</given-names></name></person-group><article-title>A Component Based Noise Correction Method (CompCor) for BOLD and Perfusion Based fMRI</article-title><source>Neuroimage</source><year>2007</year><volume>37</volume><issue>1</issue><fpage>90</fpage><lpage>101</lpage><pub-id pub-id-type="pmcid">PMC2214855</pub-id><pub-id pub-id-type="pmid">17560126</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.04.042</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bellmund</surname><given-names>Jacob LS</given-names></name><name><surname>Gärdenfors</surname><given-names>Peter</given-names></name><name><surname>Moser</surname><given-names>Edvard I</given-names></name><name><surname>Doeller</surname><given-names>Christian F</given-names></name></person-group><article-title>Navigating Cognition: Spatial Codes for Human Thinking</article-title><source>Science</source><year>2018</year><volume>362</volume><issue>6415</issue><elocation-id>eaat6766</elocation-id><pub-id pub-id-type="pmid">30409861</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boffini</surname><given-names>Roberto</given-names></name><name><surname>Doeller</surname><given-names>Christian F</given-names></name></person-group><article-title>Knowledge Across Reference Frames: Cognitive Maps and Image Spaces</article-title><source>Trends in Cognitive Sciences</source><year>2020</year><volume>24</volume><issue>8</issue><fpage>606</fpage><lpage>19</lpage><pub-id pub-id-type="pmid">32586649</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chanales</surname><given-names>Avi JH</given-names></name><name><surname>Oza</surname><given-names>Ashima</given-names></name><name><surname>Favila</surname><given-names>Serra E</given-names></name><name><surname>Kuhl</surname><given-names>Brice A</given-names></name></person-group><article-title>Overlap among Spatial Memories Triggers Repulsion of Hippocampal Representations</article-title><source>Current Biology</source><year>2017</year><volume>27</volume><issue>15</issue><fpage>2307</fpage><lpage>2317</lpage><elocation-id>e5</elocation-id><pub-id pub-id-type="pmcid">PMC5576038</pub-id><pub-id pub-id-type="pmid">28736170</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2017.06.057</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>Radoslaw Martin</given-names></name><name><surname>Khosla</surname><given-names>Aditya</given-names></name><name><surname>Pantazis</surname><given-names>Dimitrios</given-names></name><name><surname>Torralba</surname><given-names>Antonio</given-names></name><name><surname>Oliva</surname><given-names>Aude</given-names></name></person-group><article-title>Comparison of Deep Neural Networks to Spatio-Temporal Cortical Dynamics of Human Visual Object Recognition Reveals Hierarchical Correspondence</article-title><source>Scientific Reports</source><year>2016</year><volume>6</volume><issue>1</issue><elocation-id>27755</elocation-id><pub-id pub-id-type="pmcid">PMC4901271</pub-id><pub-id pub-id-type="pmid">27282108</pub-id><pub-id pub-id-type="doi">10.1038/srep27755</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colgin</surname><given-names>Laura Lee</given-names></name><name><surname>Moser</surname><given-names>Edvard I</given-names></name><name><surname>Moser</surname><given-names>May-Britt</given-names></name></person-group><article-title>Understanding Memory through Hippocampal Remapping</article-title><source>Trends in Neurosciences</source><year>2008</year><volume>31</volume><issue>9</issue><fpage>469</fpage><lpage>77</lpage><pub-id pub-id-type="pmid">18687478</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Constantinescu</surname><given-names>Alexandra O</given-names></name><name><surname>O’Reilly</surname><given-names>Jill X</given-names></name><name><surname>Behrens</surname><given-names>Timothy EJ</given-names></name></person-group><article-title>Organizing Conceptual Knowledge in Humans with a Gridlike Code</article-title><source>Science</source><year>2016</year><volume>352</volume><issue>6292</issue><fpage>1464</fpage><lpage>68</lpage><pub-id pub-id-type="pmcid">PMC5248972</pub-id><pub-id pub-id-type="pmid">27313047</pub-id><pub-id pub-id-type="doi">10.1126/science.aaf0941</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>Robert W</given-names></name><name><surname>Hyde</surname><given-names>James S</given-names></name></person-group><article-title>Software Tools for Analysis and Visualization of fMRI Data</article-title><source>NMR in Biomedicine</source><year>1997</year><volume>10</volume><issue>4-5</issue><fpage>171</fpage><lpage>78</lpage><pub-id pub-id-type="pmid">9430344</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname><given-names>Anders M</given-names></name><name><surname>Fischl</surname><given-names>Bruce</given-names></name><name><surname>Sereno</surname><given-names>Martin I</given-names></name></person-group><article-title>Cortical Surface-Based Analysis: I. Segmentation and Surface Reconstruction</article-title><source>NeuroImage</source><year>1999</year><volume>9</volume><issue>2</issue><fpage>179</fpage><lpage>94</lpage><pub-id pub-id-type="pmid">9931268</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davis</surname><given-names>Simon W</given-names></name><name><surname>Geib</surname><given-names>Benjamin R</given-names></name><name><surname>Wing</surname><given-names>Erik A</given-names></name><name><surname>Wang</surname><given-names>Wei-Chun</given-names></name><name><surname>Hovhannisyan</surname><given-names>Mariam</given-names></name><name><surname>Monge</surname><given-names>Zachary A</given-names></name><name><surname>Cabeza</surname><given-names>Roberto</given-names></name></person-group><article-title>Visual and Semantic Representations Predict Subsequent Memory in Perceptual and Conceptual Memory Tests</article-title><source>Cerebral Cortex (New York, N.Y: 1991)</source><year>2021</year><volume>31</volume><issue>2</issue><fpage>974</fpage><lpage>92</lpage><pub-id pub-id-type="pmcid">PMC8485078</pub-id><pub-id pub-id-type="pmid">32935833</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhaa269</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davis</surname><given-names>Tyler</given-names></name><name><surname>LaRocque</surname><given-names>Karen F</given-names></name><name><surname>Mumford</surname><given-names>Jeanette A</given-names></name><name><surname>Norman</surname><given-names>Kenneth A</given-names></name><name><surname>Wagner</surname><given-names>Anthony D</given-names></name><name><surname>Poldrack</surname><given-names>Russell A</given-names></name></person-group><article-title>What Do Differences between Multi-Voxel and Univariate Analysis Mean? How Subject-, Voxel-, and Trial-Level Variance Impact fMRI Analysis</article-title><source>NeuroImage</source><year>2014</year><volume>97</volume><fpage>271</fpage><lpage>83</lpage><pub-id pub-id-type="pmcid">PMC4115449</pub-id><pub-id pub-id-type="pmid">24768930</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.04.037</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname><given-names>Stanislas</given-names></name><name><surname>Bossini</surname><given-names>Serge</given-names></name><name><surname>Giraux</surname><given-names>Pascal</given-names></name></person-group><article-title>The Mental Representation of Parity and Number Magnitude</article-title><source>Journal of Experimental Psychology</source><year>1993</year><volume>122</volume><fpage>371</fpage><lpage>96</lpage><pub-id pub-id-type="doi">10.1037/00963445.122.3.371</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deuker</surname><given-names>Lorena</given-names></name><name><surname>Bellmund</surname><given-names>Jacob LS</given-names></name><name><surname>Schröder</surname><given-names>Tobias Navarro</given-names></name><name><surname>Doeller</surname><given-names>Christian F</given-names></name></person-group><article-title>An Event Map of Memory Space in the Hippocampus</article-title><person-group person-group-type="editor"><name><surname>Davachi</surname><given-names>L</given-names></name></person-group><source>eLife</source><year>2016</year><volume>5</volume><elocation-id>e16534</elocation-id><pub-id pub-id-type="pmcid">PMC5053807</pub-id><pub-id pub-id-type="pmid">27710766</pub-id><pub-id pub-id-type="doi">10.7554/eLife.16534</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devereux</surname><given-names>Barry J</given-names></name><name><surname>Clarke</surname><given-names>Alex</given-names></name><name><surname>Tyler</surname><given-names>Lorraine K</given-names></name></person-group><article-title>Integrated Deep Visual and Semantic Attractor Neural Networks Predict fMRI Pattern-Information along the Ventral Object Processing Pathway</article-title><source>Scientific Reports</source><year>2018</year><volume>8</volume><issue>1</issue><elocation-id>10636</elocation-id><pub-id pub-id-type="pmcid">PMC6045572</pub-id><pub-id pub-id-type="pmid">30006530</pub-id><pub-id pub-id-type="doi">10.1038/s41598-018-28865-1</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doeller</surname><given-names>Christian F</given-names></name><name><surname>Barry</surname><given-names>Caswell</given-names></name><name><surname>Burgess</surname><given-names>Neil</given-names></name></person-group><article-title>Evidence for Grid Cells in a Human Memory Network</article-title><source>Nature</source><year>2010</year><volume>463</volume><issue>7281</issue><fpage>657</fpage><lpage>61</lpage><pub-id pub-id-type="pmcid">PMC3173857</pub-id><pub-id pub-id-type="pmid">20090680</pub-id><pub-id pub-id-type="doi">10.1038/nature08704</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doerig</surname><given-names>Adrien</given-names></name><name><surname>Sommers</surname><given-names>Rowan P</given-names></name><name><surname>Seeliger</surname><given-names>Katja</given-names></name><name><surname>Richards</surname><given-names>Blake</given-names></name><name><surname>Ismael</surname><given-names>Jenann</given-names></name><name><surname>Lindsay</surname><given-names>Grace W</given-names></name><name><surname>Kording</surname><given-names>Konrad P</given-names></name><name><surname>Konkle</surname><given-names>Talia</given-names></name><name><surname>van Gerven</surname><given-names>Marcel AJ</given-names></name><name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name><name><surname>Kietzmann</surname><given-names>Tim C</given-names></name></person-group><article-title>The Neuroconnectionist Research Programme</article-title><source>Nature Reviews Neuroscience</source><year>2023</year><fpage>1</fpage><lpage>20</lpage><pub-id pub-id-type="pmid">37253949</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donoghue</surname><given-names>Thomas</given-names></name><name><surname>Cao</surname><given-names>Runnan</given-names></name><name><surname>Han</surname><given-names>Claire Z</given-names></name><name><surname>Holman</surname><given-names>Cameron Monteith</given-names></name><name><surname>Brandmeir</surname><given-names>Nicholas J</given-names></name><name><surname>Wang</surname><given-names>Shuo</given-names></name><name><surname>Jacobs</surname><given-names>Joshua</given-names></name></person-group><article-title>Single Neurons in the Human Medial Temporal Lobe Flexibly Shift Representations across Spatial and Memory Tasks</article-title><source>Hippocampus</source><year>2023</year><volume>33</volume><issue>5</issue><fpage>600</fpage><lpage>615</lpage><pub-id pub-id-type="pmcid">PMC10231142</pub-id><pub-id pub-id-type="pmid">37060325</pub-id><pub-id pub-id-type="doi">10.1002/hipo.23539</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eichenbaum</surname><given-names>Howard</given-names></name></person-group><article-title>The Role of the Hippocampus in Navigation Is Memory</article-title><source>Journal of Neurophysiology</source><year>2017</year><volume>117</volume><issue>4</issue><fpage>1785</fpage><lpage>96</lpage><pub-id pub-id-type="pmcid">PMC5384971</pub-id><pub-id pub-id-type="pmid">28148640</pub-id><pub-id pub-id-type="doi">10.1152/jn.00005.2017</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eichenbaum</surname><given-names>Howard</given-names></name><name><surname>Cohen</surname><given-names>Neal J</given-names></name></person-group><article-title>Can We Reconcile the Declarative Memory and Spatial Navigation Views on Hippocampal Function?</article-title><source>Neuron</source><year>2014</year><volume>83</volume><issue>4</issue><fpage>764</fpage><lpage>70</lpage><pub-id pub-id-type="pmcid">PMC4148642</pub-id><pub-id pub-id-type="pmid">25144874</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2014.07.032</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esteban</surname><given-names>Oscar</given-names></name><name><surname>Markiewicz</surname><given-names>Christopher J</given-names></name><name><surname>Blair</surname><given-names>Ross W</given-names></name><name><surname>Moodie</surname><given-names>Craig A</given-names></name><name><surname>Ilkay Isik</surname><given-names>A</given-names></name><name><surname>Erramuzpe</surname><given-names>Asier</given-names></name><name><surname>Kent</surname><given-names>James D</given-names></name><name><surname>Goncalves</surname><given-names>Mathias</given-names></name><name><surname>DuPre</surname><given-names>Elizabeth</given-names></name><name><surname>Snyder</surname><given-names>Madeleine</given-names></name><name><surname>Oya</surname><given-names>Hiroyuki</given-names></name><etal/></person-group><article-title>fMRIPrep: A Robust Preprocessing Pipeline for Functional MRI</article-title><source>Nature Methods</source><year>2019</year><volume>16</volume><issue>1</issue><fpage>111</fpage><lpage>16</lpage><pub-id pub-id-type="pmcid">PMC6319393</pub-id><pub-id pub-id-type="pmid">30532080</pub-id><pub-id pub-id-type="doi">10.1038/s41592-018-0235-4</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Favila</surname><given-names>Serra E</given-names></name><name><surname>Chanales</surname><given-names>Avi JH</given-names></name><name><surname>Kuhl</surname><given-names>Brice A</given-names></name></person-group><article-title>Experience-Dependent Hippocampal Pattern Differentiation Prevents Interference during Subsequent Learning</article-title><source>Nature Communications</source><year>2016</year><volume>7</volume><issue>1</issue><elocation-id>11066</elocation-id><pub-id pub-id-type="pmcid">PMC4820837</pub-id><pub-id pub-id-type="pmid">27925613</pub-id><pub-id pub-id-type="doi">10.1038/ncomms11066</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fayyaz</surname><given-names>Zahra</given-names></name><name><surname>Altamimi</surname><given-names>Aya</given-names></name><name><surname>Zoellner</surname><given-names>Carina</given-names></name><name><surname>Klein</surname><given-names>Nicole</given-names></name><name><surname>Wolf</surname><given-names>Oliver T</given-names></name><name><surname>Cheng</surname><given-names>Sen</given-names></name><name><surname>Wiskott</surname><given-names>Laurenz</given-names></name></person-group><article-title>A Model of Semantic Completion in Generative Episodic Memory</article-title><source>Neural Computation</source><year>2022</year><volume>34</volume><issue>9</issue><fpage>1841</fpage><lpage>70</lpage><pub-id pub-id-type="pmid">35896150</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fonov</surname><given-names>VS</given-names></name><name><surname>Evans</surname><given-names>AC</given-names></name><name><surname>McKinstry</surname><given-names>RC</given-names></name><name><surname>Almli</surname><given-names>CR</given-names></name><name><surname>Collins</surname><given-names>DL</given-names></name></person-group><article-title>Unbiased Nonlinear Average Age-Appropriate Brain Templates from Birth to Adulthood</article-title><source>Neuroimage</source><year>2009</year><volume>47</volume><elocation-id>S102</elocation-id><pub-id pub-id-type="doi">10.1016/S1053-8119(09)70884-5</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fusi</surname><given-names>Stefano</given-names></name><name><surname>Miller</surname><given-names>Earl K</given-names></name><name><surname>Rigoffi’</surname><given-names>Maffia</given-names></name></person-group><article-title>Why Neurons Mix: High Dimensionality for Higher Cognition</article-title><source>Current Opinion in Neurobiology</source><year>2016</year><volume>37</volume><fpage>66</fpage><lpage>74</lpage><pub-id pub-id-type="pmid">26851755</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fyhn</surname><given-names>Marianne</given-names></name><name><surname>Hafting</surname><given-names>Torkel</given-names></name><name><surname>Treves</surname><given-names>Alessandro</given-names></name><name><surname>Moser</surname><given-names>May-Britt</given-names></name><name><surname>Moser</surname><given-names>Edvard I</given-names></name></person-group><article-title>Hippocampal Remapping and Grid Realignment in Entorhinal Cortex</article-title><source>Nature</source><year>2007</year><volume>446</volume><issue>7132</issue><fpage>190</fpage><lpage>94</lpage><pub-id pub-id-type="pmid">17322902</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fyhn</surname><given-names>Marianne</given-names></name><name><surname>Hafting</surname><given-names>Torkel</given-names></name><name><surname>Witter</surname><given-names>Menno P</given-names></name><name><surname>Moser</surname><given-names>Edvard I</given-names></name><name><surname>Moser</surname><given-names>May-Britt</given-names></name></person-group><article-title>Grid Cells in Mice</article-title><source>Hippocampus</source><year>2008</year><volume>18</volume><issue>12</issue><fpage>1230</fpage><lpage>38</lpage><pub-id pub-id-type="pmid">18683845</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garvert</surname><given-names>Mona M</given-names></name><name><surname>Dolan</surname><given-names>Raymond J</given-names></name><name><surname>Behrens</surname><given-names>Timothy EJ</given-names></name></person-group><article-title>A Map of Abstract Relational Knowledge in the Human Hippocampal–Entorhinal Cortex</article-title><person-group person-group-type="editor"><name><surname>Davachi</surname><given-names>L</given-names></name></person-group><source>eLife</source><year>2017</year><volume>6</volume><elocation-id>e17086</elocation-id><pub-id pub-id-type="pmcid">PMC5407855</pub-id><pub-id pub-id-type="pmid">28448253</pub-id><pub-id pub-id-type="doi">10.7554/eLife.17086</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garvert</surname><given-names>Mona M</given-names></name><name><surname>Saanum</surname><given-names>Tankred</given-names></name><name><surname>Schulz</surname><given-names>Eric</given-names></name><name><surname>Schuck</surname><given-names>Nicolas W</given-names></name><name><surname>Doeller</surname><given-names>Christian F</given-names></name></person-group><article-title>Hippocampal Spatio-Predictive Cognitive Maps Adaptively Guide Reward Generalization</article-title><source>Nature Neuroscience</source><year>2023</year><volume>26</volume><issue>4</issue><fpage>615</fpage><lpage>26</lpage><pub-id pub-id-type="pmcid">PMC10076220</pub-id><pub-id pub-id-type="pmid">37012381</pub-id><pub-id pub-id-type="doi">10.1038/s41593-023-01283-x</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>Samuel J</given-names></name></person-group><article-title>The Successor Representation: Its Computational Logic and Neural Substrates</article-title><source>Journal of Neuroscience</source><year>2018</year><volume>38</volume><issue>33</issue><fpage>7193</fpage><lpage>7200</lpage><pub-id pub-id-type="pmcid">PMC6096039</pub-id><pub-id pub-id-type="pmid">30006364</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0151-18.2018</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilbert</surname><given-names>Charles D</given-names></name><name><surname>Li</surname><given-names>Wu</given-names></name></person-group><article-title>Top-down Influences on Visual Processing</article-title><source>Nature Reviews Neuroscience</source><year>2013</year><volume>14</volume><issue>5</issue><fpage>350</fpage><lpage>63</lpage><pub-id pub-id-type="pmcid">PMC3864796</pub-id><pub-id pub-id-type="pmid">23595013</pub-id><pub-id pub-id-type="doi">10.1038/nrn3476</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorgolewski</surname><given-names>Krzysztof J</given-names></name><name><surname>Auer</surname><given-names>Tibor</given-names></name><name><surname>Calhoun</surname><given-names>Vince D</given-names></name><name><surname>Cameron Craddock</surname><given-names>R</given-names></name><name><surname>Das</surname><given-names>Samir</given-names></name><name><surname>Duff</surname><given-names>Eugene P</given-names></name><name><surname>Flandin</surname><given-names>Guillaume</given-names></name><name><surname>Ghosh</surname><given-names>Satrajit S</given-names></name><name><surname>Glatard</surname><given-names>Tristan</given-names></name><name><surname>Halchenko</surname><given-names>Yaroslav O</given-names></name><name><surname>Handwerker</surname><given-names>Daniel A</given-names></name><etal/></person-group><article-title>The Brain Imaging Data Structure, a Format for Organizing and Describing Outputs of Neuroimaging Experiments</article-title><source>Scientific Data</source><year>2016</year><volume>3</volume><issue>1</issue><elocation-id>160044</elocation-id><pub-id pub-id-type="pmcid">PMC4978148</pub-id><pub-id pub-id-type="pmid">27326542</pub-id><pub-id pub-id-type="doi">10.1038/sdata.2016.44</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greve</surname><given-names>Douglas N</given-names></name><name><surname>Fischl</surname><given-names>Bruce</given-names></name></person-group><article-title>Accurate and Robust Brain Image Alignment Using Boundary-Based Registration</article-title><source>NeuroImage</source><year>2009</year><volume>48</volume><issue>1</issue><fpage>63</fpage><lpage>72</lpage><pub-id pub-id-type="pmcid">PMC2733527</pub-id><pub-id pub-id-type="pmid">19573611</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.06.060</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Güclü</surname><given-names>U</given-names></name><name><surname>van Gerven</surname><given-names>MAJ</given-names></name></person-group><article-title>Deep Neural Networks Reveal a Gradient in the Complexity of Neural Representations across the Ventral Stream</article-title><source>Journal of Neuroscience</source><year>2015</year><volume>35</volume><issue>27</issue><fpage>10005</fpage><lpage>14</lpage><pub-id pub-id-type="pmcid">PMC6605414</pub-id><pub-id pub-id-type="pmid">26157000</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5023-14.2015</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Gulban</surname><given-names>Omer Faruk</given-names></name><name><surname>Nielson</surname><given-names>Dylan</given-names></name><name><surname>Lee</surname><given-names>John</given-names></name><name><surname>Poldrack</surname><given-names>Russ</given-names></name><name><surname>Gorgolewski</surname><given-names>Chris</given-names></name><name><surname>Vanessasaurus</surname></name><name><surname>Markiewicz</surname><given-names>Chris</given-names></name></person-group><source>Poldracklab/Pydeface: PyDeface v2.0.2.”</source><year>2022</year></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname><given-names>Claire Z</given-names></name><name><surname>Donoghue</surname><given-names>Thomas</given-names></name><name><surname>Cao</surname><given-names>Runnan</given-names></name><name><surname>Kunz</surname><given-names>Lukas</given-names></name><name><surname>Wang</surname><given-names>Shuo</given-names></name><name><surname>Jacobs</surname><given-names>Joshua</given-names></name></person-group><article-title>Using Multi-Task Experiments to Test Principles of Hippocampal Function</article-title><source>Hippocampus</source><year>2023</year><volume>33</volume><issue>5</issue><fpage>646</fpage><lpage>57</lpage><pub-id pub-id-type="pmcid">PMC10249632</pub-id><pub-id pub-id-type="pmid">37042212</pub-id><pub-id pub-id-type="doi">10.1002/hipo.23540</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harel</surname><given-names>Assaf</given-names></name><name><surname>Kravitz</surname><given-names>Dwight J</given-names></name><name><surname>Baker</surname><given-names>Chris I</given-names></name></person-group><article-title>Task Context Impacts Visual Object Processing Differentially across the Cortex</article-title><source>Proceedings of the National Academy of Sciences</source><year>2014</year><volume>111</volume><issue>10</issue><fpage>E962</fpage><lpage>71</lpage><pub-id pub-id-type="pmcid">PMC3956196</pub-id><pub-id pub-id-type="pmid">24567402</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1312567111</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hassabis</surname><given-names>Demis</given-names></name><name><surname>Chu</surname><given-names>Carlton</given-names></name><name><surname>Rees</surname><given-names>Geraint</given-names></name><name><surname>Weiskopf</surname><given-names>Nikolaus</given-names></name><name><surname>Molyneux</surname><given-names>Peter D</given-names></name><name><surname>Maguire</surname><given-names>Eleanor A</given-names></name></person-group><article-title>Decoding Neuronal Ensembles in the Human Hippocampus</article-title><source>Current Biology</source><year>2009</year><volume>19</volume><issue>7</issue><fpage>546</fpage><lpage>54</lpage><pub-id pub-id-type="pmcid">PMC2670980</pub-id><pub-id pub-id-type="pmid">19285400</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2009.02.033</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>Martin N</given-names></name><name><surname>Dickter</surname><given-names>Adam H</given-names></name><name><surname>Kidder</surname><given-names>Alexis</given-names></name><name><surname>Kwok</surname><given-names>Wan Y</given-names></name><name><surname>Corriveau</surname><given-names>Anna</given-names></name><name><surname>Van Wicklin</surname><given-names>Caitlin</given-names></name><name><surname>Baker</surname><given-names>Chris I</given-names></name></person-group><article-title>THINGS: A Database of 1,854 Object Concepts and More than 26,000 Naturalistic Object Images</article-title><source>PLOS ONE</source><year>2019</year><volume>14</volume><issue>10</issue><elocation-id>e0223792</elocation-id><pub-id pub-id-type="pmcid">PMC6793944</pub-id><pub-id pub-id-type="pmid">31613926</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0223792</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heinen</surname><given-names>Rebekka</given-names></name><name><surname>Bierbrauer</surname><given-names>Anne</given-names></name><name><surname>Wolf</surname><given-names>Oliver T</given-names></name><name><surname>Axmacher</surname><given-names>Nikolai</given-names></name></person-group><article-title>Representational Formats of Human Memory Traces</article-title><source>Brain Structure &amp; Function</source><year>2023</year><pub-id pub-id-type="pmcid">PMC10978732</pub-id><pub-id pub-id-type="pmid">37022435</pub-id><pub-id pub-id-type="doi">10.1007/s00429-023-02636-9</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heinen</surname><given-names>Rebekka</given-names></name><name><surname>Rau</surname><given-names>Elias MB</given-names></name><name><surname>Herweg</surname><given-names>Nora A</given-names></name><name><surname>Axmacher</surname><given-names>Nikolai</given-names></name></person-group><article-title>Task-Relevant Representational Spaces in Human Memory Traces</article-title><year>2025</year><elocation-id>2024.09.10.612205</elocation-id></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huber</surname><given-names>Laurentius (Renzo)</given-names></name><name><surname>Poser</surname><given-names>Benedikt A</given-names></name><name><surname>Bandeffini</surname><given-names>Peter A</given-names></name><name><surname>Arora</surname><given-names>Kabir</given-names></name><name><surname>Wagstyl</surname><given-names>Konrad</given-names></name><name><surname>Cho</surname><given-names>Shinho</given-names></name><name><surname>Goense</surname><given-names>Jozien</given-names></name><name><surname>Nothnagel</surname><given-names>Nils</given-names></name><name><surname>Morgan</surname><given-names>Andrew Tyler</given-names></name><name><surname>van den Hurk</surname><given-names>Job</given-names></name><name><surname>Müller</surname><given-names>Anna K</given-names></name><etal/></person-group><article-title>LayNii: A Software Suite for Layer-fMRI</article-title><source>NeuroImage</source><year>2021</year><volume>237</volume><elocation-id>118091</elocation-id><pub-id pub-id-type="pmcid">PMC7615890</pub-id><pub-id pub-id-type="pmid">33991698</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2021.118091</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hutchinson</surname><given-names>J Wesley</given-names></name><name><surname>Lockhead</surname><given-names>GR</given-names></name></person-group><article-title>Similarity as Distance: A Structural Principle for Semantic Memory</article-title><source>Journal of Experimental Psychology: Human Learning and Memory</source><year>1977</year><volume>3</volume><issue>6</issue><fpage>660</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1037/0278-7393.3.6.660</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huth</surname><given-names>Alexander G</given-names></name><name><surname>Nishimoto</surname><given-names>Shinji</given-names></name><name><surname>Vu</surname><given-names>An T</given-names></name><name><surname>Gallant</surname><given-names>Jack L</given-names></name></person-group><article-title>A Continuous Semantic Space Describes the Representation of Thousands of Object and Action Categories across the Human Brain</article-title><source>Neuron</source><year>2012</year><volume>76</volume><issue>6</issue><fpage>1210</fpage><lpage>24</lpage><pub-id pub-id-type="pmcid">PMC3556488</pub-id><pub-id pub-id-type="pmid">23259955</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2012.10.014</pub-id></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacobs</surname><given-names>Joshua</given-names></name><name><surname>Weidemann</surname><given-names>Christoph T</given-names></name><name><surname>Miller</surname><given-names>Jonathan F</given-names></name><name><surname>Solway</surname><given-names>Alec</given-names></name><name><surname>Burke</surname><given-names>John F</given-names></name><name><surname>Wei</surname><given-names>Xue-Xin</given-names></name><name><surname>Suthana</surname><given-names>Nanthia</given-names></name><name><surname>Sperling</surname><given-names>Michael R</given-names></name><name><surname>Sharan</surname><given-names>Ashwini D</given-names></name><name><surname>Fried</surname><given-names>Itzhak</given-names></name><name><surname>Kahana</surname><given-names>Michael J</given-names></name></person-group><article-title>Direct Recordings of Grid-like Neuronal Activity in Human Spatial Navigation</article-title><source>Nature Neuroscience</source><year>2013</year><volume>16</volume><issue>9</issue><fpage>1188</fpage><lpage>90</lpage><pub-id pub-id-type="pmcid">PMC3767317</pub-id><pub-id pub-id-type="pmid">23912946</pub-id><pub-id pub-id-type="doi">10.1038/nn.3466</pub-id></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jeffery</surname><given-names>Kate J</given-names></name></person-group><article-title>The Hippocampus: From Memory, to Map, to Memory Map</article-title><source>Trends in Neurosciences</source><year>2018</year><volume>41</volume><issue>2</issue><fpage>64</fpage><lpage>66</lpage><pub-id pub-id-type="pmid">29405927</pub-id></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname><given-names>Mark</given-names></name><name><surname>Bannister</surname><given-names>Peter</given-names></name><name><surname>Brady</surname><given-names>Michael</given-names></name><name><surname>Smith</surname><given-names>Stephen</given-names></name></person-group><article-title>Improved Optimization for the Robust and Accurate Linear Registration and Motion Correction of Brain Images</article-title><source>NeuroImage</source><year>2002</year><volume>17</volume><issue>2</issue><fpage>825</fpage><lpage>41</lpage><pub-id pub-id-type="pmid">12377157</pub-id></element-citation></ref><ref id="R53"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Jia</surname><given-names>Yangqing</given-names></name><name><surname>Shelhamer</surname><given-names>Evan</given-names></name><name><surname>Donahue</surname><given-names>Jeff</given-names></name><name><surname>Karayev</surname><given-names>Sergey</given-names></name><name><surname>Long</surname><given-names>Jonathan</given-names></name><name><surname>Girshick</surname><given-names>Ross</given-names></name><name><surname>Guadarrama</surname><given-names>Sergio</given-names></name><name><surname>Darrell</surname><given-names>Trevor</given-names></name></person-group><source>Caffe: Convolutional Architecture for Fast Feature Embedding</source><conf-name>Proceedings of the 22nd ACM international conference on Multimedia</conf-name><conf-loc>Orlando Florida USA</conf-loc><conf-sponsor>ACM</conf-sponsor><year>2014</year><fpage>675</fpage><lpage>78</lpage></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaiser</surname><given-names>Daniel</given-names></name><name><surname>Turini</surname><given-names>Jacopo</given-names></name><name><surname>Cichy</surname><given-names>Radoslaw M</given-names></name></person-group><article-title>A Neural Mechanism for Contextualizing Fragmented Inputs during Naturalistic Vision</article-title><person-group person-group-type="editor"><name><surname>Luo</surname><given-names>H</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name></person-group><source>eLife</source><year>2019</year><volume>8</volume><elocation-id>e48182</elocation-id><pub-id pub-id-type="pmcid">PMC6802952</pub-id><pub-id pub-id-type="pmid">31596234</pub-id><pub-id pub-id-type="doi">10.7554/eLife.48182</pub-id></element-citation></ref><ref id="R55"><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Kashyap</surname><given-names>Sriranga</given-names></name></person-group><source>Srikash/Presurfer: Ondu.”</source><year>2021</year></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kietzmann</surname><given-names>Tim C</given-names></name><name><surname>Spoerer</surname><given-names>Courtney J</given-names></name><name><surname>Sörensen</surname><given-names>Lynn KA</given-names></name><name><surname>Cichy</surname><given-names>Radoslaw M</given-names></name><name><surname>Hauk</surname><given-names>Olaf</given-names></name><name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name></person-group><article-title>Recurrence Is Required to Capture the Representational Dynamics of the Human Visual System</article-title><source>Proceedings of the National Academy of Sciences</source><year>2019</year><volume>116</volume><issue>43</issue><fpage>21854</fpage><lpage>63</lpage><pub-id pub-id-type="pmcid">PMC6815174</pub-id><pub-id pub-id-type="pmid">31591217</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1905544116</pub-id></element-citation></ref><ref id="R57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klein</surname><given-names>Arno</given-names></name><name><surname>Ghosh</surname><given-names>Satrajit S</given-names></name><name><surname>Bao</surname><given-names>Forrest S</given-names></name><name><surname>Giard</surname><given-names>Joachim</given-names></name><name><surname>Häme</surname><given-names>Yrjö</given-names></name><name><surname>Stavsky</surname><given-names>Eliezer</given-names></name><name><surname>Lee</surname><given-names>Noah</given-names></name><name><surname>Rossa</surname><given-names>Brian</given-names></name><name><surname>Reuter</surname><given-names>Martin</given-names></name><name><surname>Neto</surname><given-names>Elias Chaibub</given-names></name><name><surname>Keshavan</surname><given-names>Anisha</given-names></name></person-group><article-title>Mindboggling Morphometry of Human Brains</article-title><source>PLOS Computational Biology</source><year>2017</year><volume>13</volume><issue>2</issue><elocation-id>e1005350</elocation-id><pub-id pub-id-type="pmcid">PMC5322885</pub-id><pub-id pub-id-type="pmid">28231282</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005350</pub-id></element-citation></ref><ref id="R58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kolibius</surname><given-names>Luca D</given-names></name><name><surname>Roux</surname><given-names>Frederic</given-names></name><name><surname>Parish</surname><given-names>George</given-names></name><name><surname>Ter Wal</surname><given-names>Marije</given-names></name><name><surname>Van Der Plas</surname><given-names>Mircea</given-names></name><name><surname>Chelvarajah</surname><given-names>Ramesh</given-names></name><name><surname>Sawlani</surname><given-names>Vijay</given-names></name><name><surname>Rollings</surname><given-names>David T</given-names></name><name><surname>Lang</surname><given-names>Johannes D</given-names></name><name><surname>Gollwitzer</surname><given-names>Stephanie</given-names></name><name><surname>Walther</surname><given-names>Katrin</given-names></name></person-group><article-title>Hippocampal Neurons Code Individual Episodic Memories in Humans</article-title><source>Nature Human Behaviour</source><year>2023</year><volume>7</volume><issue>11</issue><fpage>1968</fpage><lpage>79</lpage><pub-id pub-id-type="pmcid">PMC10663153</pub-id><pub-id pub-id-type="pmid">37798368</pub-id><pub-id pub-id-type="doi">10.1038/s41562-023-01706-6</pub-id></element-citation></ref><ref id="R59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name></person-group><article-title>Representational Similarity Analysis - Connecting the Branches of Systems Neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><year>2008</year><pub-id pub-id-type="pmcid">PMC2605405</pub-id><pub-id pub-id-type="pmid">19104670</pub-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id></element-citation></ref><ref id="R60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name><name><surname>Goebel</surname><given-names>Rainer</given-names></name><name><surname>Bandeffini</surname><given-names>Peter</given-names></name></person-group><article-title>Information-Based Functional Brain Mapping</article-title><source>Proceedings of the National Academy of Sciences</source><year>2006</year><volume>103</volume><issue>10</issue><fpage>3863</fpage><lpage>68</lpage><pub-id pub-id-type="pmcid">PMC1383651</pub-id><pub-id pub-id-type="pmid">16537458</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0600244103</pub-id></element-citation></ref><ref id="R61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>Alex</given-names></name><name><surname>Sutskever</surname><given-names>Ilya</given-names></name><name><surname>Hinton</surname><given-names>Geoffrey E</given-names></name></person-group><article-title>ImageNet Classification with Deep Convolutional Neural Networks</article-title><source>Communications of the ACM</source><year>2017</year><volume>60</volume><issue>6</issue><fpage>84</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1145/3065386</pub-id></element-citation></ref><ref id="R62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kubie</surname><given-names>John L</given-names></name><name><surname>Levy</surname><given-names>Eliott RJ</given-names></name><name><surname>Fenton</surname><given-names>André A</given-names></name></person-group><article-title>Is Hippocampal Remapping the Physiological Basis for Context?</article-title><source>Hippocampus</source><year>2020</year><volume>30</volume><issue>8</issue><fpage>851</fpage><lpage>64</lpage><pub-id pub-id-type="pmcid">PMC7954664</pub-id><pub-id pub-id-type="pmid">31571314</pub-id><pub-id pub-id-type="doi">10.1002/hipo.23160</pub-id></element-citation></ref><ref id="R63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuzovkin</surname><given-names>Ilya</given-names></name><name><surname>Vicente</surname><given-names>Raul</given-names></name><name><surname>Petton</surname><given-names>Mathilde</given-names></name><name><surname>Lachaux</surname><given-names>Jean-Philippe</given-names></name><name><surname>Baciu</surname><given-names>Monica</given-names></name><name><surname>Kahane</surname><given-names>Philippe</given-names></name><name><surname>Rheims</surname><given-names>Sylvain</given-names></name><name><surname>Vidal</surname><given-names>Juan R</given-names></name><name><surname>Aru</surname><given-names>Jaan</given-names></name></person-group><article-title>Activations of Deep Convolutional Neural Networks Are Aligned with Gamma Band Activity of Human Visual Cortex</article-title><source>Communications Biology</source><year>2018</year><volume>1</volume><issue>1</issue><fpage>107</fpage><pub-id pub-id-type="pmcid">PMC6123818</pub-id><pub-id pub-id-type="pmid">30271987</pub-id><pub-id pub-id-type="doi">10.1038/s42003-018-0110-y</pub-id></element-citation></ref><ref id="R64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lanczos</surname><given-names>C</given-names></name></person-group><article-title>Evaluation of Noisy Data</article-title><source>Journal of the Society for Industrial and Applied Mathematics Series B Numerical Analysis</source><year>1964</year><volume>1</volume><issue>1</issue><fpage>76</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1137/0701007</pub-id></element-citation></ref><ref id="R65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Leeuw</surname><given-names>Joshua R</given-names></name><name><surname>Gilbert</surname><given-names>Rebecca A</given-names></name><name><surname>Luchterhandt</surname><given-names>Björn</given-names></name></person-group><article-title>jsPsych: Enabling an OpenSource Collaborative Ecosystem of Behavioral Experiments</article-title><source>Journal of Open Source Software</source><year>2023</year><volume>8</volume><issue>85</issue><elocation-id>5351</elocation-id><pub-id pub-id-type="doi">10.21105/joss.05351</pub-id></element-citation></ref><ref id="R66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lisman</surname><given-names>John E</given-names></name><name><surname>Grace</surname><given-names>Anthony A</given-names></name></person-group><article-title>The Hippocampal-VTA Loop: Controlling the Entry of Information into Long-Term Memory</article-title><source>Neuron</source><year>2005</year><volume>46</volume><issue>5</issue><fpage>703</fpage><lpage>13</lpage><pub-id pub-id-type="pmid">15924857</pub-id></element-citation></ref><ref id="R67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Merkel</surname><given-names>Dirk</given-names></name></person-group><article-title>Docker: Lightweight Linux Containers for Consistent Development and Deployment</article-title><source>Linux J</source><year>2014</year><volume>2014</volume><issue>239</issue><fpage>2</fpage><comment>2</comment></element-citation></ref><ref id="R68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moser</surname><given-names>Edvard I</given-names></name><name><surname>Kropff</surname><given-names>Emilio</given-names></name><name><surname>Moser</surname><given-names>May-Britt</given-names></name></person-group><article-title>Place Cells, Grid Cells, and the Brain’s Spatial Representation System</article-title><source>Annual Review of Neuroscience</source><year>2008</year><volume>31</volume><fpage>69</fpage><lpage>89</lpage><pub-id pub-id-type="pmid">18284371</pub-id></element-citation></ref><ref id="R69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moyer</surname><given-names>Robert S</given-names></name></person-group><article-title>Comparing Objects in Memory: Evidence Suggesting an Internal Psychophysics</article-title><source>Perception &amp; Psychophysics</source><year>1973</year><volume>13</volume><issue>2</issue><fpage>180</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.3758/BF03214124</pub-id></element-citation></ref><ref id="R70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moyer</surname><given-names>Robert S</given-names></name><name><surname>Landauer</surname><given-names>Thomas K</given-names></name></person-group><article-title>Time Required for Judgements of Numerical Inequality</article-title><source>Nature</source><year>1967</year><volume>215</volume><issue>5109</issue><fpage>1519</fpage><lpage>20</lpage><pub-id pub-id-type="pmid">6052760</pub-id></element-citation></ref><ref id="R71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mumford</surname><given-names>Jeanette A</given-names></name><name><surname>Davis</surname><given-names>Tyler</given-names></name><name><surname>Poldrack</surname><given-names>Russell A</given-names></name></person-group><article-title>The Impact of Study Design on Pattern Estimation for Single-Trial Multivariate Pattern Analysis</article-title><source>NeuroImage</source><year>2014</year><volume>103</volume><fpage>130</fpage><lpage>38</lpage><pub-id pub-id-type="pmid">25241907</pub-id></element-citation></ref><ref id="R72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mumford</surname><given-names>Jeanette A</given-names></name><name><surname>Poline</surname><given-names>Jean-Baptiste</given-names></name><name><surname>Poldrack</surname><given-names>Russell A</given-names></name></person-group><article-title>Orthogonalization of Regressors in fMRI Models</article-title><source>PLOS ONE</source><year>2015</year><volume>10</volume><issue>4</issue><elocation-id>e0126255</elocation-id><pub-id pub-id-type="pmcid">PMC4412813</pub-id><pub-id pub-id-type="pmid">25919488</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0126255</pub-id></element-citation></ref><ref id="R73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mumford</surname><given-names>Jeanette A</given-names></name><name><surname>Turner</surname><given-names>Benjamin O</given-names></name><name><surname>Gregory Ashby</surname><given-names>F</given-names></name><name><surname>Poldrack</surname><given-names>Russell A</given-names></name></person-group><article-title>Deconvolving BOLD Activation in Event-Related Designs for Multivoxel Pattern Classification Analyses</article-title><source>NeuroImage</source><year>2012</year><volume>59</volume><issue>3</issue><fpage>2636</fpage><lpage>43</lpage><pub-id pub-id-type="pmcid">PMC3251697</pub-id><pub-id pub-id-type="pmid">21924359</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.08.076</pub-id></element-citation></ref><ref id="R74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nili</surname><given-names>Hamed</given-names></name><name><surname>Wingfield</surname><given-names>Cai</given-names></name><name><surname>Walther</surname><given-names>Alexander</given-names></name><name><surname>Su</surname><given-names>Li</given-names></name><name><surname>Marslen-Wilson</surname><given-names>William</given-names></name><name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name></person-group><article-title>A Toolbox for Representational Similarity Analysis</article-title><person-group person-group-type="editor"><name><surname>Prlic</surname><given-names>A</given-names></name></person-group><source>PLoS Computational Biology</source><year>2014</year><volume>10</volume><issue>4</issue><elocation-id>e1003553</elocation-id><pub-id pub-id-type="pmcid">PMC3990488</pub-id><pub-id pub-id-type="pmid">24743308</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003553</pub-id></element-citation></ref><ref id="R75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nitsch</surname><given-names>Alexander</given-names></name><name><surname>Garvert</surname><given-names>Mona M</given-names></name><name><surname>Bellmund</surname><given-names>Jacob LS</given-names></name><name><surname>Schuck</surname><given-names>Nicolas W</given-names></name><name><surname>Doeller</surname><given-names>Christian F</given-names></name></person-group><article-title>Grid-like Entorhinal Representation of an Abstract Value Space during Prospective Decision Making</article-title><source>Nature Communications</source><year>2024</year><volume>15</volume><issue>1</issue><elocation-id>1198</elocation-id><pub-id pub-id-type="pmcid">PMC10858181</pub-id><pub-id pub-id-type="pmid">38336756</pub-id><pub-id pub-id-type="doi">10.1038/s41467-024-45127-z</pub-id></element-citation></ref><ref id="R76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niv</surname><given-names>Yael</given-names></name></person-group><article-title>Learning Task-State Representations</article-title><source>Nature Neuroscience</source><year>2019</year><volume>22</volume><issue>10</issue><fpage>1544</fpage><lpage>53</lpage><pub-id pub-id-type="pmcid">PMC7241310</pub-id><pub-id pub-id-type="pmid">31551597</pub-id><pub-id pub-id-type="doi">10.1038/s41593-019-0470-8</pub-id></element-citation></ref><ref id="R77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niv</surname><given-names>Yael</given-names></name><name><surname>Daniel</surname><given-names>Reka</given-names></name><name><surname>Geana</surname><given-names>Andra</given-names></name><name><surname>Gershman</surname><given-names>Samuel J</given-names></name><name><surname>Leong</surname><given-names>Yuan Chang</given-names></name><name><surname>Radulescu</surname><given-names>Angela</given-names></name><name><surname>Wilson</surname><given-names>Robert C</given-names></name></person-group><article-title>Reinforcement Learning in Multidimensional Environments Relies on Attention Mechanisms</article-title><source>Journal of Neuroscience</source><year>2015</year><volume>35</volume><issue>21</issue><fpage>8145</fpage><lpage>57</lpage><pub-id pub-id-type="pmcid">PMC4444538</pub-id><pub-id pub-id-type="pmid">26019331</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2978-14.2015</pub-id></element-citation></ref><ref id="R78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Notter</surname><given-names>Michael Philipp</given-names></name><name><surname>Gale</surname><given-names>Dan</given-names></name><name><surname>Herholz</surname><given-names>Peer</given-names></name><name><surname>Markello</surname><given-names>Ross</given-names></name><name><surname>Notter-Bielser</surname><given-names>Marie-Laure</given-names></name><name><surname>Whitaker</surname><given-names>Kirstie</given-names></name></person-group><article-title>AtlasReader: A Python Package to Generate Coordinate Tables, Region Labels, and Informative Figures from Statistical MRI Images</article-title><source>Journal of Open Source Software</source><year>2019</year><volume>4</volume><issue>34</issue><elocation-id>1257</elocation-id><pub-id pub-id-type="doi">10.21105/joss.01257</pub-id></element-citation></ref><ref id="R79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Keefe</surname><given-names>John</given-names></name></person-group><article-title>Place Units in the Hippocampus of the Freely Moving Rat</article-title><source>Experimental Neurology</source><year>1976</year><volume>51</volume><issue>1</issue><fpage>78</fpage><lpage>109</lpage><pub-id pub-id-type="pmid">1261644</pub-id></element-citation></ref><ref id="R80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pacheco Estefan</surname><given-names>D</given-names></name><name><surname>Sánchez-Fibla</surname><given-names>M</given-names></name><name><surname>Duff</surname><given-names>A</given-names></name><name><surname>Principe</surname><given-names>A</given-names></name><name><surname>Rocamora</surname><given-names>R</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Axmacher</surname><given-names>N</given-names></name><name><surname>Verschure</surname><given-names>PFMJ</given-names></name></person-group><article-title>Coordinated Representational Reinstatement in the Human Hippocampus and Lateral Temporal Cortex during Episodic Memory Retrieval</article-title><source>Nature Communications</source><year>2019</year><volume>10</volume><issue>1</issue><elocation-id>2255</elocation-id><pub-id pub-id-type="pmcid">PMC6529470</pub-id><pub-id pub-id-type="pmid">31113952</pub-id><pub-id pub-id-type="doi">10.1038/s41467-019-09569-0</pub-id></element-citation></ref><ref id="R81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pacheco-Estefan</surname><given-names>Daniel</given-names></name><name><surname>Fellner</surname><given-names>Marie-Christin</given-names></name><name><surname>Kunz</surname><given-names>Lukas</given-names></name><name><surname>Zhang</surname><given-names>Hui</given-names></name><name><surname>Reinacher</surname><given-names>Peter</given-names></name><name><surname>Roy</surname><given-names>Charlotte</given-names></name><name><surname>Brandt</surname><given-names>Armin</given-names></name><name><surname>Schulze-Bonhage</surname><given-names>Andreas</given-names></name><name><surname>Yang</surname><given-names>Linglin</given-names></name><name><surname>Wang</surname><given-names>Shuang</given-names></name><name><surname>Liu</surname><given-names>Jing</given-names></name><etal/></person-group><article-title>Maintenance and Transformation of Representational Formats during Working Memory Prioritization</article-title><source>Nature Communications</source><year>2024</year><volume>15</volume><issue>1</issue><elocation-id>8234</elocation-id><pub-id pub-id-type="pmcid">PMC11412997</pub-id><pub-id pub-id-type="pmid">39300141</pub-id><pub-id pub-id-type="doi">10.1038/s41467-024-52541-w</pub-id></element-citation></ref><ref id="R82"><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Perktold</surname><given-names>J</given-names></name><name><surname>Seabold</surname><given-names>Skipper</given-names></name><name><surname>Sheppard</surname><given-names>Kevin</given-names></name><collab>ChadFulton</collab><name><surname>Shedden</surname><given-names>Kerby</given-names></name><collab>jbrockmendel, j-grana6</collab><name><surname>Quackenbush</surname><given-names>Peter</given-names></name><name><surname>Arel-Bundock</surname><given-names>Vincent</given-names></name><name><surname>McKinney</surname><given-names>Wes</given-names></name><name><surname>Langmore</surname><given-names>Ian</given-names></name><name><surname>Baker</surname><given-names>Bart</given-names></name><etal/></person-group><source>Statsmodels/Statsmodels: Release 0.14.1.”</source><year>2023</year></element-citation></ref><ref id="R83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Power</surname><given-names>Jonathan D</given-names></name><name><surname>Barnes</surname><given-names>Kelly A</given-names></name><name><surname>Snyder</surname><given-names>Abraham Z</given-names></name><name><surname>Schlaggar</surname><given-names>Bradley L</given-names></name><name><surname>Petersen</surname><given-names>Steven E</given-names></name></person-group><article-title>Spurious but Systematic Correlations in Functional Connectivity MRI Networks Arise from Subject Motion</article-title><source>NeuroImage</source><year>2012</year><volume>59</volume><issue>3</issue><fpage>2142</fpage><lpage>54</lpage><pub-id pub-id-type="pmcid">PMC3254728</pub-id><pub-id pub-id-type="pmid">22019881</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.10.018</pub-id></element-citation></ref><ref id="R84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Power</surname><given-names>Jonathan D</given-names></name><name><surname>Mitra</surname><given-names>Anish</given-names></name><name><surname>Laumann</surname><given-names>Timothy O</given-names></name><name><surname>Snyder</surname><given-names>Abraham Z</given-names></name><name><surname>Schlaggar</surname><given-names>Bradley L</given-names></name><name><surname>Petersen</surname><given-names>Steven E</given-names></name></person-group><article-title>Methods to Detect, Characterize, and Remove Motion Artifact in Resting State fMRI</article-title><source>NeuroImage</source><year>2014</year><volume>84</volume><fpage>320</fpage><lpage>41</lpage><pub-id pub-id-type="pmcid">PMC3849338</pub-id><pub-id pub-id-type="pmid">23994314</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.08.048</pub-id></element-citation></ref><ref id="R85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quiroga</surname><given-names>R Quian</given-names></name><name><surname>Reddy</surname><given-names>L</given-names></name><name><surname>Kreiman</surname><given-names>G</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Fried</surname><given-names>I</given-names></name></person-group><article-title>Invariant Visual Representation by Single Neurons in the Human Brain</article-title><source>Nature</source><year>2005</year><volume>435</volume><issue>7045</issue><fpage>1102</fpage><lpage>7</lpage><pub-id pub-id-type="pmid">15973409</pub-id></element-citation></ref><ref id="R86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quiroga</surname><given-names>Rodrigo Quian</given-names></name></person-group><article-title>Concept Cells: The Building Blocks of Declarative Memory Functions</article-title><source>Nature Reviews Neuroscience</source><year>2012</year><volume>13</volume><issue>8</issue><fpage>587</fpage><lpage>97</lpage><pub-id pub-id-type="pmid">22760181</pub-id></element-citation></ref><ref id="R87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quiroga</surname><given-names>Rodrigo Quian</given-names></name></person-group><article-title>No Pattern Separation in the Human Hippocampus</article-title><source>Trends in Cognitive Sciences</source><year>2020</year><volume>24</volume><issue>12</issue><fpage>994</fpage><lpage>1007</lpage><pub-id pub-id-type="pmid">33162337</pub-id></element-citation></ref><ref id="R88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rau</surname><given-names>Elias MB</given-names></name><name><surname>Fellner</surname><given-names>Marie-Christin</given-names></name><name><surname>Heinen</surname><given-names>Rebekka</given-names></name><name><surname>Zhang</surname><given-names>Hui</given-names></name><name><surname>Yin</surname><given-names>Qin</given-names></name><name><surname>Vahidi</surname><given-names>Parisa</given-names></name><name><surname>Kobelt</surname><given-names>Malte</given-names></name><name><surname>Asano</surname><given-names>Eishi</given-names></name><name><surname>Kim-McManus</surname><given-names>Olivia</given-names></name><name><surname>Sattar</surname><given-names>Shifteh</given-names></name><name><surname>Lin</surname><given-names>Jack J</given-names></name><etal/></person-group><article-title>Reinstatement and Transformation of Memory Traces for Recognition</article-title><source>Science Advances</source><year>2025</year><volume>11</volume><issue>8</issue><elocation-id>eadp9336</elocation-id><pub-id pub-id-type="pmcid">PMC11838014</pub-id><pub-id pub-id-type="pmid">39970226</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.adp9336</pub-id></element-citation></ref><ref id="R89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rey</surname><given-names>Hernan G</given-names></name><name><surname>Ison</surname><given-names>Matias J</given-names></name><name><surname>Pedreira</surname><given-names>Carlos</given-names></name><name><surname>Valentin</surname><given-names>Antonio</given-names></name><name><surname>Alarcon</surname><given-names>Gonzalo</given-names></name><name><surname>Selway</surname><given-names>Richard</given-names></name><name><surname>Richardson</surname><given-names>Mark P</given-names></name><name><surname>Quiroga</surname><given-names>Rodrigo Quian</given-names></name></person-group><article-title>Single-Cell Recordings in the Human Medial Temporal Lobe</article-title><source>Journal of Anatomy</source><year>2015</year><volume>227</volume><issue>4</issue><fpage>394</fpage><lpage>408</lpage><pub-id pub-id-type="pmcid">PMC4580099</pub-id><pub-id pub-id-type="pmid">25163775</pub-id><pub-id pub-id-type="doi">10.1111/joa.12228</pub-id></element-citation></ref><ref id="R90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rey</surname><given-names>Hernan G</given-names></name><name><surname>Panagiotaropoulos</surname><given-names>Theofanis I</given-names></name><name><surname>Gutierrez</surname><given-names>Lorenzo</given-names></name><name><surname>Chaure</surname><given-names>Fernando J</given-names></name><name><surname>Nasimbera</surname><given-names>Alejandro</given-names></name><name><surname>Cordisco</surname><given-names>Santiago</given-names></name><name><surname>Nishida</surname><given-names>Fabian</given-names></name><name><surname>Valentin</surname><given-names>Antonio</given-names></name><name><surname>Alarcon</surname><given-names>Gonzalo</given-names></name><name><surname>Richardson</surname><given-names>Mark P</given-names></name><name><surname>Kochen</surname><given-names>Silvia</given-names></name><name><surname>Quiroga</surname><given-names>Rodrigo Quian</given-names></name></person-group><article-title>Lack of Context Modulation in Human Single Neuron Responses in the Medial Temporal Lobe</article-title><source>Cell Reports</source><year>2025</year><volume>44</volume><issue>1</issue><elocation-id>115218</elocation-id><pub-id pub-id-type="pmcid">PMC11781864</pub-id><pub-id pub-id-type="pmid">39823228</pub-id><pub-id pub-id-type="doi">10.1016/j.celrep.2024.115218</pub-id></element-citation></ref><ref id="R91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolls</surname><given-names>Edmund T</given-names></name><name><surname>Joliot</surname><given-names>Marc</given-names></name><name><surname>Tzourio-Mazoyer</surname><given-names>Nathalie</given-names></name><etal/></person-group><article-title>Implementation of a New Parcellation of the Orbitofrontal Cortex in the Automated Anatomical Labeling Atlas</article-title><source>NeuroImage</source><year>2015</year><volume>122</volume><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="pmid">26241684</pub-id></element-citation></ref><ref id="R92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Satterthwaite</surname><given-names>Theodore D</given-names></name><name><surname>Elliott</surname><given-names>Mark A</given-names></name><name><surname>Gerraty</surname><given-names>Raphael T</given-names></name><name><surname>Ruparel</surname><given-names>Kosha</given-names></name><name><surname>Loughead</surname><given-names>James</given-names></name><name><surname>Calkins</surname><given-names>Monica E</given-names></name><name><surname>Eickhoff</surname><given-names>Simon B</given-names></name><name><surname>Hakonarson</surname><given-names>Hakon</given-names></name><name><surname>Gur</surname><given-names>Ruben C</given-names></name><name><surname>Gur</surname><given-names>Raquel E</given-names></name><name><surname>Wolf</surname><given-names>Daniel H</given-names></name></person-group><article-title>An Improved Framework for Confound Regression and Filtering for Control of Motion Artifact in the Preprocessing of Resting-State Functional Connectivity Data</article-title><source>NeuroImage</source><year>2013</year><volume>64</volume><fpage>240</fpage><lpage>56</lpage><pub-id pub-id-type="pmcid">PMC3811142</pub-id><pub-id pub-id-type="pmid">22926292</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.08.052</pub-id></element-citation></ref><ref id="R93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shin</surname><given-names>Jiyun N</given-names></name><name><surname>Doron</surname><given-names>Guy</given-names></name><name><surname>Larkum</surname><given-names>Matthew E</given-names></name></person-group><article-title>Memories off the Top of Your Head</article-title><source>Science (New York, N.Y.)</source><year>2021</year><volume>374</volume><issue>6567</issue><fpage>538</fpage><lpage>39</lpage><pub-id pub-id-type="pmcid">PMC7612398</pub-id><pub-id pub-id-type="pmid">34709915</pub-id><pub-id pub-id-type="doi">10.1126/science.abk1859</pub-id></element-citation></ref><ref id="R94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>Stephen M</given-names></name><name><surname>Nichols</surname><given-names>Thomas E</given-names></name></person-group><article-title>Threshold-Free Cluster Enhancement: Addressing Problems of Smoothing, Threshold Dependence and Localisation in Cluster Inference</article-title><source>NeuroImage</source><year>2009</year><volume>44</volume><issue>1</issue><fpage>83</fpage><lpage>98</lpage><pub-id pub-id-type="pmid">18501637</pub-id></element-citation></ref><ref id="R95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stoinski</surname><given-names>Laura</given-names></name><name><surname>Perkuhn</surname><given-names>Jonas</given-names></name><name><surname>Hebart</surname><given-names>Martin</given-names></name></person-group><article-title>THINGSplus: New Norms and Metadata for the THINGS Database of 1,854 Object Concepts and 26,107 Natural Object Images.”</article-title><year>2022</year><pub-id pub-id-type="pmcid">PMC10991023</pub-id><pub-id pub-id-type="pmid">37095326</pub-id><pub-id pub-id-type="doi">10.3758/s13428-023-02110-8</pub-id></element-citation></ref><ref id="R96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suthana</surname><given-names>Nanthia</given-names></name><name><surname>Ekstrom</surname><given-names>Arne D</given-names></name><name><surname>Yassa</surname><given-names>Michael A</given-names></name><name><surname>Stark</surname><given-names>Craig</given-names></name></person-group><article-title>Pattern Separation in the Human Hippocampus: Response to Quiroga</article-title><source>Trends in Cognitive Sciences</source><year>2021</year><volume>25</volume><issue>6</issue><fpage>423</fpage><lpage>24</lpage><pub-id pub-id-type="pmcid">PMC8725200</pub-id><pub-id pub-id-type="pmid">33820660</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2021.02.005</pub-id></element-citation></ref><ref id="R97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tavares</surname><given-names>Rita Morais</given-names></name><name><surname>Mendelsohn</surname><given-names>Avi</given-names></name><name><surname>Grossman</surname><given-names>Yael</given-names></name><name><surname>Williams</surname><given-names>Christian Hamilton</given-names></name><name><surname>Shapiro</surname><given-names>Matthew</given-names></name><name><surname>Trope</surname><given-names>Yaacov</given-names></name><name><surname>Schiller</surname><given-names>Daniela</given-names></name></person-group><article-title>A Map for Social Navigation in the Human Brain</article-title><source>Neuron</source><year>2015</year><volume>87</volume><issue>1</issue><fpage>231</fpage><lpage>43</lpage><pub-id pub-id-type="pmcid">PMC4662863</pub-id><pub-id pub-id-type="pmid">26139376</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2015.06.011</pub-id></element-citation></ref><ref id="R98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teyler</surname><given-names>TJ</given-names></name><name><surname>DiScenna</surname><given-names>P</given-names></name></person-group><article-title>The Hippocampal Memory Indexing Theory</article-title><source>Behavioral Neuroscience</source><year>1986</year><volume>100</volume><issue>2</issue><fpage>147</fpage><lpage>54</lpage><pub-id pub-id-type="pmid">3008780</pub-id></element-citation></ref><ref id="R99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teyler</surname><given-names>Timothy J</given-names></name><name><surname>Rudy</surname><given-names>Jerry W</given-names></name></person-group><article-title>The Hippocampal Indexing Theory and Episodic Memory: Updating the Index</article-title><source>Hippocampus</source><year>2007</year><volume>17</volume><issue>12</issue><fpage>1158</fpage><lpage>69</lpage><pub-id pub-id-type="pmid">17696170</pub-id></element-citation></ref><ref id="R100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Theves</surname><given-names>Stephanie</given-names></name><name><surname>Fernandez</surname><given-names>Guillén</given-names></name><name><surname>Doeller</surname><given-names>Christian F</given-names></name></person-group><article-title>The Hippocampus Encodes Distances in Multidimensional Feature Space</article-title><source>Current Biology</source><year>2019</year><volume>29</volume><issue>7</issue><fpage>1226</fpage><lpage>1231</lpage><elocation-id>e3</elocation-id><pub-id pub-id-type="pmid">30905602</pub-id></element-citation></ref><ref id="R101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Theves</surname><given-names>Stephanie</given-names></name><name><surname>Fernández</surname><given-names>Guillén</given-names></name><name><surname>Doeller</surname><given-names>Christian F</given-names></name></person-group><article-title>The Hippocampus Maps Concept Space, Not Feature Space</article-title><source>Journal of Neuroscience</source><year>2020</year><volume>40</volume><issue>38</issue><fpage>7318</fpage><lpage>25</lpage><pub-id pub-id-type="pmcid">PMC7534914</pub-id><pub-id pub-id-type="pmid">32826311</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0494-20.2020</pub-id></element-citation></ref><ref id="R102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Theves</surname><given-names>Stephanie</given-names></name><name><surname>Neville</surname><given-names>David A</given-names></name><name><surname>Fernández</surname><given-names>Guillén</given-names></name><name><surname>Doeller</surname><given-names>Christian F</given-names></name></person-group><article-title>Learning and Representation of Hierarchical Concepts in Hippocampus and Prefrontal Cortex</article-title><source>Journal of Neuroscience</source><year>2021</year><volume>41</volume><issue>36</issue><fpage>7675</fpage><lpage>86</lpage><pub-id pub-id-type="pmcid">PMC8425977</pub-id><pub-id pub-id-type="pmid">34330775</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0657-21.2021</pub-id></element-citation></ref><ref id="R103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tolman</surname><given-names>Edward C</given-names></name></person-group><article-title>Cognitive Maps in Rats and Men</article-title><source>Psychological Review</source><year>1948</year><volume>55</volume><issue>4</issue><fpage>189</fpage><lpage>208</lpage><pub-id pub-id-type="pmid">18870876</pub-id></element-citation></ref><ref id="R104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treder</surname><given-names>Matthias S</given-names></name><name><surname>Charest</surname><given-names>Ian</given-names></name><name><surname>Michelmann</surname><given-names>Sebastian</given-names></name><name><surname>Martin-Buro</surname><given-names>Maria Carmen</given-names></name><name><surname>Roux</surname><given-names>Frédéric</given-names></name><name><surname>Carceller-Benito</surname><given-names>Fernando</given-names></name><name><surname>Ugalde-Canitrot</surname><given-names>Arturo</given-names></name><name><surname>Rollings</surname><given-names>David T</given-names></name><name><surname>Sawlani</surname><given-names>Vijay</given-names></name><name><surname>Chelvarajah</surname><given-names>Ramesh</given-names></name><name><surname>Wimber</surname><given-names>Maria</given-names></name><etal/></person-group><article-title>The Hippocampus as the Switchboard between Perception and Memory</article-title><source>Proceedings of the National Academy of Sciences</source><year>2021</year><volume>118</volume><issue>50</issue><elocation-id>e2114171118</elocation-id><pub-id pub-id-type="pmcid">PMC8685930</pub-id><pub-id pub-id-type="pmid">34880133</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2114171118</pub-id></element-citation></ref><ref id="R105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tustison</surname><given-names>Nicholas J</given-names></name><name><surname>Avants</surname><given-names>Brian B</given-names></name><name><surname>Cook</surname><given-names>Philip A</given-names></name><name><surname>Zheng</surname><given-names>Yuanjie</given-names></name><name><surname>Egan</surname><given-names>Alexander</given-names></name><name><surname>Yushkevich</surname><given-names>Paul A</given-names></name><name><surname>Gee</surname><given-names>James C</given-names></name></person-group><article-title>N4ITK: Improved N3 Bias Correction</article-title><source>IEEE Transactions on Medical Imaging</source><year>2010</year><volume>29</volume><issue>6</issue><fpage>1310</fpage><lpage>20</lpage><pub-id pub-id-type="pmcid">PMC3071855</pub-id><pub-id pub-id-type="pmid">20378467</pub-id><pub-id pub-id-type="doi">10.1109/TMI.2010.2046908</pub-id></element-citation></ref><ref id="R106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tye</surname><given-names>Kay M</given-names></name><name><surname>Miller</surname><given-names>Earl K</given-names></name><name><surname>Taschbach</surname><given-names>Felix H</given-names></name><name><surname>Benna</surname><given-names>Marcus K</given-names></name><name><surname>Rigoffi’</surname><given-names>Maffia</given-names></name><name><surname>Fusi</surname><given-names>Stefano</given-names></name></person-group><article-title>Mixed Selectivity: Cellular Computations for Complexity</article-title><source>Neuron</source><year>2024</year><volume>112</volume><issue>14</issue><fpage>2289</fpage><lpage>2303</lpage><pub-id pub-id-type="pmcid">PMC11257803</pub-id><pub-id pub-id-type="pmid">38729151</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2024.04.017</pub-id></element-citation></ref><ref id="R107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Viganò</surname><given-names>Simone</given-names></name><name><surname>Piazza</surname><given-names>Manuela</given-names></name></person-group><article-title>Distance and Direction Codes Underlie Navigation of a Novel Semantic Space in the Human Brain</article-title><source>Journal of Neuroscience</source><year>2020</year><volume>40</volume><issue>13</issue><fpage>2727</fpage><lpage>36</lpage><pub-id pub-id-type="pmcid">PMC7096136</pub-id><pub-id pub-id-type="pmid">32060171</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1849-19.2020</pub-id></element-citation></ref><ref id="R108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Viganò</surname><given-names>Simone</given-names></name><name><surname>Rubino</surname><given-names>Valerio</given-names></name><name><surname>Buiaffi’</surname><given-names>Marco</given-names></name><name><surname>Piazza</surname><given-names>Manuela</given-names></name></person-group><article-title>The Neural Representation of Absolute Direction during Mental Navigation in Conceptual Spaces</article-title><source>Communications Biology</source><year>2021</year><volume>4</volume><issue>1</issue><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="pmcid">PMC8595308</pub-id><pub-id pub-id-type="pmid">34785757</pub-id><pub-id pub-id-type="doi">10.1038/s42003-021-02806-7</pub-id></element-citation></ref><ref id="R109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walther</surname><given-names>Thomas</given-names></name><name><surname>Diekmann</surname><given-names>Nicolas</given-names></name><name><surname>Vijayabaskaran</surname><given-names>Sandhiya</given-names></name><name><surname>Donoso</surname><given-names>José R</given-names></name><name><surname>Manahan-Vaughan</surname><given-names>Denise</given-names></name><name><surname>Wiskott</surname><given-names>Laurenz</given-names></name><name><surname>Cheng</surname><given-names>Sen</given-names></name></person-group><article-title>Context-Dependent Extinction Learning Emerging from Raw Sensory Inputs: A Reinforcement Learning Approach</article-title><source>Scientific Reports</source><year>2021</year><volume>11</volume><issue>1</issue><elocation-id>2713</elocation-id><pub-id pub-id-type="pmcid">PMC7851139</pub-id><pub-id pub-id-type="pmid">33526840</pub-id><pub-id pub-id-type="doi">10.1038/s41598-021-81157-z</pub-id></element-citation></ref><ref id="R110"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>Long</given-names></name><name><surname>Wisse</surname><given-names>Laura EM</given-names></name><name><surname>Das</surname><given-names>Sandhitsu R</given-names></name><name><surname>Wang</surname><given-names>Hongzhi</given-names></name><name><surname>Wolk</surname><given-names>David A</given-names></name><name><surname>Manjón</surname><given-names>Jose V</given-names></name><name><surname>Yushkevich</surname><given-names>Paul A</given-names></name></person-group><chapter-title>Accounting for the Confound of Meninges in Segmenting Entorhinal and Perirhinal Cortices in T1-Weighted MRI</chapter-title><person-group person-group-type="editor"><name><surname>Ourselin</surname><given-names>S</given-names></name><name><surname>Joskowicz</surname><given-names>L</given-names></name><name><surname>Sabuncu</surname><given-names>MR</given-names></name><name><surname>Unal</surname><given-names>G</given-names></name><name><surname>Wells</surname><given-names>W</given-names></name></person-group><source>Medical Image Computing and Computer-Assisted Intervention – MICCAI2016</source><publisher-loc>Cham</publisher-loc><publisher-name>Springer International Publishing</publisher-name><year>2016</year><fpage>564</fpage><lpage>71</lpage><pub-id pub-id-type="pmcid">PMC5526195</pub-id><pub-id pub-id-type="pmid">28752156</pub-id><pub-id pub-id-type="doi">10.1007/978-3-319-46723-8_65</pub-id></element-citation></ref><ref id="R111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yonelinas</surname><given-names>Andrew P</given-names></name></person-group><article-title>The Nature of Recollection and Familiarity: A Review of 30 Years of Research</article-title><source>Journal of Memory and Language</source><year>2002</year><volume>46</volume><issue>3</issue><fpage>441</fpage><lpage>517</lpage><pub-id pub-id-type="doi">10.1006/jmla.2002.2864</pub-id></element-citation></ref><ref id="R112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yonelinas</surname><given-names>Andrew P</given-names></name><name><surname>Parks</surname><given-names>Colleen M</given-names></name></person-group><article-title>Receiver Operating Characteristics (ROCs) in Recognition Memory: A Review</article-title><source>Psychological Bulletin</source><year>2007</year><volume>133</volume><issue>5</issue><fpage>800</fpage><lpage>832</lpage><pub-id pub-id-type="pmid">17723031</pub-id></element-citation></ref><ref id="R113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yushkevich</surname><given-names>Paul A</given-names></name><name><surname>Pluta</surname><given-names>John B</given-names></name><name><surname>Wang</surname><given-names>Hongzhi</given-names></name><name><surname>Xie</surname><given-names>Long</given-names></name><name><surname>Ding</surname><given-names>Song-Lin</given-names></name><name><surname>Gertje</surname><given-names>Eske C</given-names></name><name><surname>Mancuso</surname><given-names>Lauren</given-names></name><name><surname>Kliot</surname><given-names>Daria</given-names></name><name><surname>Das</surname><given-names>Sandhitsu R</given-names></name><name><surname>Wolk</surname><given-names>David A</given-names></name></person-group><article-title>Automated Volumetry and Regional Thickness Analysis of Hippocampal Subfields and Medial Temporal Cortical Structures in Mild Cognitive Impairment</article-title><source>Human Brain Mapping</source><year>2015</year><volume>36</volume><issue>1</issue><fpage>258</fpage><lpage>87</lpage><pub-id pub-id-type="pmcid">PMC4313574</pub-id><pub-id pub-id-type="pmid">25181316</pub-id><pub-id pub-id-type="doi">10.1002/hbm.22627</pub-id></element-citation></ref><ref id="R114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Brady</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name></person-group><article-title>Segmentation of Brain MR Images through a Hidden Markov Random Field Model and the Expectation-Maximization Algorithm</article-title><source>IEEE Transactions on Medical Imaging</source><year>2001</year><volume>20</volume><issue>1</issue><fpage>45</fpage><lpage>57</lpage><pub-id pub-id-type="pmid">11293691</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Conceptual cognitive maps of natural stimuli in the MTL</title><p>A) Exemplary trial of the conceptual navigation task performed during fMRI. B) Analysis approach to reconstruct distances between all pairs of concepts (rated and inferred) for each conceptual dimension separately using a least-squares approach to approximate behavioral ratings (for details see <xref ref-type="sec" rid="S9">Methods</xref>). C) Reconstructed cognitive maps in an exemplary participant. Positions for each concept were estimated based on the procedure depicted in B) and combined into the two 2D spaces rated by the participant. Grey lines between concept points show explicitly rated pairs of concepts in the conceptual navigation task. Bold lines indicate task-relevant (green) and task-irrelevant (grey) distances for the exemplary concept pair shown in A), i.e. puppy and polar bear. D) Mixed-models showing response times for individual 1D ratings as a function of rated conceptual distances according to task-relevant feature dimensions. E) Regions of interest in the MTL: Hippocampus (HC), Perirhinal Cortex (PRC), Parahippocampal Cortex (PHC), Entorhinal Cortex (EC). Merged masks across participants, plotted onto template brain in MNI space. F) Analysis strategy to test whether neural representational geometries between animal concepts reflect representations in cognitive maps reconstructed from behavior. We correlated neural and behavioral representational geometries of inferred concept pairs for same spaces (task-relevant; i.e. neural<sub>space1</sub>-behavioral<sub>space1</sub> and neural<sub>space2</sub>-behavioral<sub>space2</sub>) and for different spaces (task-irrelevant; i.e. neural<sub>space1</sub>-behavioral<sub>space2</sub> and neural<sub>space2</sub>-behavioral<sub>space1</sub>). G) Neural-Behavioral similarity of representational geometries for task-relevant (teal) and task-irrelevant (grey) spaces across regions of the MTL. * indicates beta-coefficient with <italic>P</italic> &lt; 0.05 after model comparison. Circled * indicates <italic>P</italic> &lt; 0.05 for one-sample <italic>T</italic> test and line * indicates P &lt; 0.05 for paired <italic>T</italic> test.</p></caption><graphic xlink:href="EMS205917-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Flexible remapping of conceptual representations in hippocampus</title><p>A) Space remapping: Correlation of cognitive space remapping at the neural level (x-axis: neural space<sub>1</sub>- space<sub>2</sub> similarity) and behavioral level (y-axis: behavioral space<sub>1</sub>-space<sub>2</sub> similarity). Each dot represents one participant. B) Concept remapping: Schematic depiction of analysis procedure. Participant-specific reconstructed spaces, constituted by the same animal concepts across behavioral contexts (left), and vectors showing the Euclidean distance between exemplary concepts to the origin of the 2D space. Concept remapping is computed as the difference in the distance of a concept to the origin between the two spaces, i.e. dist2origin<sub>space1</sub> – dist2origin<sub>space2</sub> (right). Distribution displays exemplary concepts of a given participant with high and low remapping values. Schematic neural similarity matrix across exemplars of the same animal concept between spaces (blue). C) Matching of neural and behavioral concept remapping, separately for regions of the MTL (left). LMM estimation of neural between-space dissimilarity predicting concept-level remapping. Circled * indicates <italic>P</italic> &lt; 0.05 of one-sample <italic>T</italic>-test. * indicates significant beta-coefficient at <italic>P</italic> &lt; 0.05 after model comparison.</p></caption><graphic xlink:href="EMS205917-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Task-specific distances in cognitive maps relate to memory strength</title><p>A) Exemplary trial from the recognition memory task. Old: Images that have been shown during the conceptual navigation task. New exemplar: Novel image of an animal concept that has been shown before. New concept: Animal concept that has not been shown before. B) Receiver-Operator Characteristics (ROC) curve showing the ratio of hits vs. false alarms across the confidence scale (bins of 5 at scale from -50 to +50). Light grey lines represent individual participants, dark grey line indicates sample average. C) Subsequent memory effects in regions of the MTL (left) and across the whole brain (right). D) Trial-level (reference images – blue; comparison images – red), reconstructed (task-relevant – green; task-irrelevant – grey) and concept remapping (blue) distances from cognitive maps predict memory strength. * indicates significant main effects after model comparison at <italic>P</italic> &lt; 0.05. Circled * indicates <italic>P</italic> &lt; 0.05 for one-sample <italic>T</italic> test.</p></caption><graphic xlink:href="EMS205917-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Stimulus-driven and task-dependent representational formats in memory traces</title><p>A) Analysis strategy to analyze perceptual formats by comparing pairwise neural similarity matrices with DNN-derived model predictions (top). Visualization of the best layer-model fit plotted onto the surface of a canonical MNI template brain (bottom). All colored voxels contribute to a significant cluster with at least one model layer that survived <italic>P</italic><sub>FWER</sub> &lt; 0.05 at the cluster-level after <italic>TFCE</italic>, and the color code depicts the DNN model layer with the best fit for each voxel. B) Analysis strategy to test for conceptual representational formats across the brain using a searchlight-based RSA approach (top). Concept-specific representations across the VVS (bottom) as indicated by <italic>T</italic>-values of significant clusters of the Same vs. Different concept contrast thresholded at the cluster-level with at <italic>P</italic><sub>FWER</sub> &lt; 0.05 after <italic>TFCE</italic>. C) Whole brain contrast of task-relevant cognitive map representations (contrast of similarity with task-relevant vs. task-irrelevant maps), <italic>T</italic>-Values thresholded at the cluster-level with <italic>P</italic><sub>FWER</sub> &lt; 0.05 after <italic>TFCE</italic>. D) Distribution of active voxels showing stimulus-driven (perceptual, conceptual) or task-dependent representational formats. Inner circle: Percentage of voxels with distinct representational format. Outer circle: Percentage of voxels with unique (grey) and overlapping (green) format. E) Surface plot showing active voxels with unique or shared (two or more) formats. F) Distribution of formats across voxels showing positive SMEs as depicted in <xref ref-type="fig" rid="F3">Fig. 3C</xref>.</p></caption><graphic xlink:href="EMS205917-f004"/></fig></floats-group></article>