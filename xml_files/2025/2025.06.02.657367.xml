<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS206239</article-id><article-id pub-id-type="doi">10.1101/2025.06.02.657367</article-id><article-id pub-id-type="archive">PPR1031795</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Animate-inanimate object categorization from minimal visual information in the human brain, human behavior, and deep neural networks</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Spriet</surname><given-names>Céline</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Rostami</surname><given-names>Farzad</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Hochmann</surname><given-names>Jean-Rémy</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Papeo</surname><given-names>Liuba</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><aff id="A1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/058hz8544</institution-id><institution>Institut des Sciences Cognitives</institution></institution-wrap>—Marc Jeannerod, UMR5229, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02feahw73</institution-id><institution>Centre National de la Recherche Scientifique (CNRS)</institution></institution-wrap> and <institution-wrap><institution-id institution-id-type="ror">https://ror.org/029brtt94</institution-id><institution>Université Claude Bernard Lyon 1</institution></institution-wrap>, <addr-line>67 Bd. Pinel</addr-line> – <postal-code>69675</postal-code><city>Bron</city> (<country country="FR">France</country>)</aff></contrib-group><author-notes><corresp id="CR1">Correspondence to: C.S. <email>cel.spr@gmail.com</email>, L.P. <email>liuba.papeo@isc.cnrs.fr</email>
</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>07</day><month>06</month><year>2025</year></pub-date><pub-date pub-type="preprint"><day>04</day><month>06</month><year>2025</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">The distinction between animate and inanimate <italic>things</italic> is a main organizing principle of information in perception and cognition. Yet, animacy, as a visual property, has so far eluded operationalization. Which visual features are necessary and sufficient to see animacy? At which level of the visual hierarchy does the animate-inanimate distinction emerge? Here, we show that the animate-inanimate distinction is preserved even among images of objects that are made unrecognizable and only retain low- and mid-level visual features of their natural version. In particular, in three experiments, healthy human adults saw rapid sequences of images (6 Hz) where every five exemplars of a category (i.e., animate), an exemplar of another category (i.e., inanimate) was shown (1.2 Hz). Using frequency-tagging electroencephalography (ftEEG), we found significant neural responses at 1.2 Hz, indicating rapid and automatic detection of the periodic categorical change. Moreover, such effect was found –although increasingly weaker– for ‘impoverished’ stimulus-sets that retained only certain (high-, mid- or low-level) features of the original colorful images (i.e., grayscale, <italic>texform</italic> and phase-scrambled images), and even if the images were unrecognizable. Similar effects were found with two Deep Neural networks (DNNs) presented with the same stimulus-sets. In sum, reliable categorization effects for dramatically impoverished and unrecognizable images, in humans’ EEG and DNN data, demonstrate that animacy representation emerges early in the visual hierarchy and is remarkably resilient to the loss of visual information.</p></abstract><kwd-group><kwd>Categorization</kwd><kwd>Visual Perception</kwd><kwd>frequency-tagging EEG</kwd><kwd>DNN</kwd><kwd>Animacy perception</kwd><kwd>Semantic categories</kwd><kwd>Visual system.</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><label>1</label><title>Introduction</title><p id="P2">An astonishing capacity for <italic>life detection</italic> underlies animals’ survival. Humans and other species are endowed with mechanisms for rapid and accurate discrimination between animate and inanimate <italic>things</italic>. Accordingly, categorization by animacy is a main organizing principle of object-related information in mind/brain (<xref ref-type="bibr" rid="R53">Warrington and Shallice, 1984</xref>; <xref ref-type="bibr" rid="R5">Caramazza and Shelton, 1998</xref>; <xref ref-type="bibr" rid="R26">Martin, 2007</xref>; <xref ref-type="bibr" rid="R13">Grill-Spector and Weiner, 2014</xref>), the first to emerge in infancy (<xref ref-type="bibr" rid="R1">Ayzenberg &amp; Behrmann, 2024</xref>; <xref ref-type="bibr" rid="R31">Pauen, 2002</xref>; <xref ref-type="bibr" rid="R45">Spriet et al., 2022</xref>), and one of the most efficient mechanisms of visual perception (<xref ref-type="bibr" rid="R28">New et al., 2007</xref>; <xref ref-type="bibr" rid="R49">Thorpe et al., 1996</xref>; <xref ref-type="bibr" rid="R51">VanRullen &amp; Thorpe, 2001</xref>).</p><p id="P3">Categorization by animacy is solved, in large part, by visual information. While motion is a cue to distinguish animate from inanimate objects, this distinction is computed within milliseconds even for static stimuli (<xref ref-type="bibr" rid="R6">Carlson et al., 2013</xref>; <xref ref-type="bibr" rid="R7">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="R9">Contini et al., 2017</xref>; <xref ref-type="bibr" rid="R32">Proklova et al., 2019</xref>). Yet, the features that are necessary and sufficient to <italic>see</italic> animacy remain debated (<xref ref-type="bibr" rid="R3">Bracci &amp; Op De Beeck, 2023</xref>; <xref ref-type="bibr" rid="R15">Jozwik et al., 2022</xref>; <xref ref-type="bibr" rid="R30">Papeo et al., 2017</xref>; <xref ref-type="bibr" rid="R48">Thorat et al., 2019</xref>).</p><p id="P4">Object categorization is the result of tuning to complex visual features in higher-level visual areas; however, new results show that distinctive information about animacy is also carried by mid-level features such as shape and texture, encoded in middle areas of the visual ventral stream (<xref ref-type="bibr" rid="R43">Schmidt et al., 2017</xref>; <xref ref-type="bibr" rid="R50">Tiedemann et al., 2022</xref>; <xref ref-type="bibr" rid="R24">Long et al., 2017</xref>; <xref ref-type="bibr" rid="R14">Grootswagers et al., 2019</xref>; Li and Bonner, 2020; <xref ref-type="bibr" rid="R52">Wang et al., 2022</xref>; <xref ref-type="bibr" rid="R18">Kramer et al., 2023</xref>; Jagadeesh and Livingstone, 2024; Lieber et al., 2024), and low-level feature such as color, encoded in posterior areas of the ventral stream (<xref ref-type="bibr" rid="R39">Rosenthal et al., 2018</xref>). Thus, many features may contribute to the animate-inanimate distinction; but, what accounts for the fast, automatic categorization that supports life detection?</p><p id="P5">We used frequency-tagging electroencephalography (ftEEG), to capture the fast and automatic neural response locked to the stimulus appearance. We tested whether such response evoked by visual object perception distinguishes between animate and inanimate objects, and, if so, which features –low-, mid- or high-level– are sufficient to observe such categorization-response.</p><p id="P6">In ftEEG, stimuli presented in rapid sequence at a regular frequency, elicit steady-state visual evoked potentials (SSVP) at the same frequency, which allegedly capture the immediate, automatic response to stimulus perception (<xref ref-type="bibr" rid="R22">Liu-Shuang et al., 2014</xref>; <xref ref-type="bibr" rid="R29">Norcia et al., 2015</xref>; <xref ref-type="bibr" rid="R40">Rossion, 2014</xref>; <xref ref-type="bibr" rid="R41">Rossion &amp; Boremanse, 2011</xref>). We presented stimuli at 6 Hz with a regular categorical change at 1.2 Hz, such that every five exemplars of a category (e.g., inanimate), an exemplar of another category (i.e., animate) was presented. We expected periodic ftEEG-response at 6 Hz; furthermore, if animates are readily represented as distinct from inanimate objects, we should observe a distinct response at 1.2 Hz, signaling detection of the categorical change. This approach has been used to show automatic distinctions between faces <italic>vs</italic>. non-face stimuli (<xref ref-type="bibr" rid="R42">Rossion et al., 2015</xref>; <xref ref-type="bibr" rid="R34">Rekow et al., 2022</xref>), or natural <italic>vs</italic>. artificial objects (<xref ref-type="bibr" rid="R46">Stothart et al., 2017</xref>). Here, we considered the broader distinction between animate and inanimate (and the largest stimulus set so far), testing whether, say, a zebra, a fish, a hamster and a turtle, however visually different, are readily <italic>seen</italic> as more similar to each other than to a hammer, a rock, a flower and a plane, and <italic>vice versa</italic>. Moreover, in different conditions and experiments, we manipulated the original images to test whether mid-level (global form, texture) or low-level features (spectral power, color, contrast, luminance, number of pixels) alone could support fast and automatic categorization by animacy. The same question was studied using behavioral judgments obtained from human participants, and the categorization performance of VVG-19 (<xref ref-type="bibr" rid="R44">Simonyan &amp; Zisserman, 2015</xref>). In this deep convolutional neural network (DNN), which provide successful artificial model for human visual object recognition, we tested how different sets of images were classified in different layers.</p></sec><sec id="S2" sec-type="materials | methods"><label>2</label><title>Materials and methods</title><sec id="S3"><label>2.1</label><title>Experiment 1</title><sec id="S4" sec-type="subjects"><label>2.1.1</label><title>Participants</title><p id="P7">Experiment 1 involved 12 healthy adults (7 identified their gender as female, 5 as male, mean age 25.7 ± 4.7 years). One additional participant was tested and excluded from the analysis for falling asleep during the experiment. Without prior data, Experiment 1 was exploratory with respect to the sample size. Results were used for power analysis to select the sample size for the following experiments. These and next experiments were approved by the local ethics committee (CPP Ile de France VIII).</p></sec><sec id="S5"><label>2.1.2</label><title>Stimuli</title><p id="P8">Experiments 1 involved two sets of stimuli: 640 colorful images (hereafter, <italic>original</italic> stimuli), and the corresponding ‘impoverished’ set consisting of phase-scramble versions of the original set.</p><sec id="S6"><title>Original stimuli</title><p id="P9">Stimuli were created from 640 colorful photographs of real-world animate (<italic>n</italic>=320) and inanimate (<italic>n</italic>=320) objects taken from the internet, to broadly sample the variety of objects in the real world. The animate set included 51 fish, 70 birds, 179 nonhuman mammals, 14 amphibians and 6 turtles, all different from one another. Humans were excluded to prevent a bias in the animate-inanimate categorization. Insects, spiders and reptiles (except for turtles) were excluded to prevent emotional reactions (e.g., disgust or fear). The inanimate set included 223 artificial objects (16 exemplars of buildings and constructions, 108 exemplars of clothes, pieces of jewelry, buttons, coins, and tools, 66 pieces of furniture and 33 vehicles) and 97 natural objects (54 different fruits and vegetables, and 43 different flowers, bushes and trees). Objects could appear in all sorts of orientations and visual angles and resized to fit a box of 629x629 pixels with a gray background (<xref ref-type="fig" rid="F1">Figure 1A</xref>). Two types of stimulation sequences were created, ‘animate’ and ‘inanimate’, named after the category that served as <italic>oddball</italic>, i.e., the less frequent stimulus-category used to elicit the periodic categorization-response (<xref ref-type="fig" rid="F1">Figure 1B</xref>). Thus, in the animate-sequence, 320 inanimate objects were shown as <italic>standard</italic> (i.e., the frequent stimulus-category in the sequence), and 68 animate objects were shown as <italic>oddball</italic>; in the inanimate-sequence, 320 animate objects were shown as standard, and 68 inanimate objects were shown as oddball.</p></sec><sec id="S7"><title>Phase-scramble stimuli</title><p id="P10">Phase-scramble stimuli were created by manipulating the spatial spectrum of the original stimuli through phase-scrambling, using the <italic>imscramble</italic> function (<ext-link ext-link-type="uri" xlink:href="http://martin-hebart.de/webpages/code/stimuli.html">http://martin-hebart.de/webpages/code/stimuli.html</ext-link>) in Matlab (The Mathworks, Natick, MA). This manipulation preserved the number of pixels, color, contrast, luminance and power-spectral distribution of the original images, but removed mid-level (shape and texture) information, effectively rendering the object identity unrecognizable (see Experiment 4, <xref ref-type="sec" rid="S25">section 2.4</xref>.).</p></sec></sec><sec id="S8" sec-type="methods"><label>2.1.3</label><title>Procedures</title><sec id="S9"><title>Task</title><p id="P11">In Experiment 1, participants sat on a chair ~60 cm away from a 60 Hz computer screen (resolution 1920x1200 pixels, size 51.5x32.2cm), where stimuli were presented centrally (16° of visual angle). Stimulus presentation was controlled using Psychtoolbox (<xref ref-type="bibr" rid="R4">Brainard, 1997</xref>) through Matlab. Each participant was tested in two conditions involving the ‘intact’ (i.e., original stimuli) and the ‘impoverished’ (i.e., phase-scramble stimuli) set, respectively. Each condition included 16 trials of animate-sequences (i.e., animate-<italic>oddballs</italic> among inanimate-<italic>standards</italic>), and 16 trials of inanimate-sequences (i.e., inanimate-<italic>oddballs</italic> among animate-<italic>standards</italic>). Therefore, each participant completed a total of 64 trials (16 trials x 2 conditions x 2 types of sequences). Each trial started by a fade-in phase (increase in contrast) of 2 s and ended with a fade-out phase (decrease in contrast) of 2 s, to ease the stimulus presentation for the participant’s eyes and avoid eye movements caused by abrupt appearance or disappearance. Each trial lasted 32 s, and consisted of a sequence of stimuli, presented at a base frequency of 6 Hz (6 images per s; 166.67 ms per image), in a squarewave design, where every 5 images of one category, one image of the other category was presented. With this trial structure, a categorization-response was expected at 6/5, i.e., 1.2 Hz (<xref ref-type="fig" rid="F1">Figure 1 B</xref>). The five standard stimuli that preceded each oddball were pseudo-randomly selected to prevent the repetition of the same image in a trial and the presentation of two images of the same subordinate-level category at the same frequency as the oddball presentation (e.g., a dolphin and a cat, both from the mammal category, presented with only five images in between). For each participant, the same list of images was used in the two conditions.</p><p id="P12">The 16 trials of a condition were presented in a single block, yielding a total of four blocks (animate and inanimate sequences in intact and impoverished conditions). The two blocks of the same condition were presented one after the other, with the order of type of sequence (animate or inanimate) and condition (intact or impoverished) counterbalanced across participants. At the end of the trial, a test-image from the standard category was shown. Participants were instructed to pay attention to each image in a trial and to report whether the final test-image was shown during the trial, by pressing “A” or “P” (“yes” or “no”). This task was included to invite participants to pay attention to the stimuli. The whole experiment lasted ~45 minutes.</p></sec><sec id="S10"><title>EEG recordin</title><p id="P13">EEG data were acquired using 128-channel EGI nets (Electrical Geodesics, Inc.). Data were acquired with vertex reference, using the EGI Net Station acquisition software, continuously digitized at a sampling rate of 1kHz (net amp 400 system EGI). Impedance was lowered for each participant as much as possible, and did not exceed 40kΩ. During the experiment, triggers were sent from the experimental computer to the acquisition computer via a light sensor: a white square appeared in correspondence to the sensor (i.e., at the bottom right of the screen) at the beginning and at the end of each trial. This manipulation allowed a high precision in timing the beginning and the end of a trial, allowing to extract the EEG data corresponding to the presentation of the trial and time-locking EEG data to the stimuli presentation.</p></sec><sec id="S11"><title>EEG preprocessing</title><p id="P14">Standard data preprocessing was performed using the EEGLAB toolbox (<xref ref-type="bibr" rid="R10">Delorme &amp; Makeig, 2004</xref>) and MATLAB R2015b. Raw data for each participant were filtered using a 4th-order high pass butterworth filter at 0.1 Hz and a 4th-order low pass butterworth filter at 100 Hz. All electrodes were re-referenced using the average of all electrodes as a reference. Data were segmented by trials, taking 25 s for each trial (30 complete cycles) starting 2.5 s after the trial onset. Trials were averaged separately for each participant, each condition (intact and impoverished) and each type of sequence (animate and inanimate). A Fast Fourier Transform (FFT) was applied for data examination in the EEG frequency-domain at the high frequency resolution of 0.04 Hz (1/25 seconds). Baseline-subtracted amplitudes were computed for each participant, for each condition, at each electrode, and for each harmonic of the response (base and oddball) by subtracting from the amplitude of interest, the average amplitude of the local baseline, that is, the mean of amplitudes of the 24 surrounding bins (12 on each side excluding the immediate adjacent bins for a frequency range of ±0.48 Hz) excluding the maximum and minimum (<xref ref-type="bibr" rid="R33">Quek &amp; Rossion, 2017</xref>; <xref ref-type="bibr" rid="R35">Retter &amp; Rossion, 2016</xref>). To define the peaks at the frequencies of interest (base and oddball frequencies and corresponding harmonics), the EEG signal of all participants was averaged in the time domain to obtain grand-averaged spectra at each electrode, for each condition. After applying the FFT, data were averaged across the type of sequences, for each condition. <italic>Z</italic>-scores were computed for the ‘intact’ and ‘impoverished’ condition, on the average over all electrodes, by subtracting from the amplitude of interest, the average amplitude of the local baseline and dividing this difference by the standard deviation of the local baseline. Finally, electrodes showing an effect were identified computing the <italic>z</italic>-score at each electrode and for each condition.</p></sec></sec><sec id="S12"><label>2.1.4</label><title>EEG data analyses</title><p id="P15">Data were analyzed considering the two conditions (‘intact’ and ‘impoverished’, thereafter referred to as ‘Image Type’), averaging across the type of sequences (animate, inanimate, thereafter referred to as ‘Sequence Type’), after applying the FFT (see preprocessing). First, the harmonics showing a response were identified as follows. We computed the <italic>z</italic>-score (see above) for all harmonics of the oddball frequency below 12 Hz (excluding the base frequency 6 and its 12 Hz harmonic), and for all harmonics of the base frequency below 50 Hz. We selected harmonics with a <italic>z</italic>-score higher than a threshold of 1.64 (<italic>α</italic> = .05, one-tailed). Second, we identified the electrodes showing a significant response at the selected harmonics. This was done by computing a <italic>z</italic>-score on the grand-averaged spectrum for each electrode independently. <italic>Z</italic>-scores were averaged and tested against a threshold of 3.33 (corresponding to <italic>α</italic> = .0004, one-tailed, Bonferroni correction for 128 electrodes). We used parametric statistical tests to measure the following effects. First, we assessed whether the amplitude of the response at the base or oddball frequency was significantly higher than the noise level. To this end, each participant’s baseline-subtracted amplitude, summed over the identified harmonics, averaged over the identified electrodes was tested against 0 (one-tailed <italic>t</italic>-test). Second, the baseline-subtracted amplitudes, summed across the identified harmonics, averaged across the identified electrodes, were compared between the two critical conditions (intact <italic>vs</italic>. impoverished Image Type; <italic>t</italic>-test). Additionally, a 2 Image Type x 2 Sequence Type repeated-measures ANOVA was used to test whether the oddball response of the Image Type (intact and impoverished sets) was affected by the Sequence Type (animate or inanimate).</p></sec></sec><sec id="S13"><label>2.2</label><title>Experiment 2</title><sec id="S14" sec-type="subjects"><label>2.2.1</label><title>Participants</title><p id="P16">Experiment 2 involved 12 healthy adults (9 identified their gender as female, 3 as male, mean age 26.3 ±6.3 years). One additional participant was tested and excluded from the analysis for falling asleep during the experiment. A power analysis based on data from Experiment 1 estimated that a sample size of 7 was required to obtain the smallest categorization effect observed in Experiment 1 (phase-scramble condition; Cohen’s <italic>d</italic> = 1.421, ß= 0.95, <italic>α</italic> = 0.05; GPower 3.1). Therefore, in this and in the following experiment, we decided to keep the same sample size of 12 as in Experiment 1. Participants had normal or corrected-to-normal vision and reported no history of psychiatric or neurological conditions. An independent sample of 19 native-French speakers (17 identified their gender as female, 2 as male, mean age 24.0 ± 3.8 years) took part in an online behavioral-judgement task for stimulus recognizability assessment (see <italic>Grayscale phase-scramble stimuli</italic> below). They all reported to have normal or corrected-to-normal vision and no history of psychiatric or neurological conditions.</p></sec><sec id="S15"><label>2.2.2</label><title>Stimuli</title><p id="P17">Experiments 2 involved two sets of stimuli: 640 grayscale images of animate and inanimate objects (‘intact’ set) and the corresponding (‘impoverished’) phase-scramble versions.</p><sec id="S16"><title>Grayscale stimuli</title><p id="P18">Grayscale images were obtained from the ‘intact’ set (original stimuli) of Experiment 1, using the step1_lumContrastOriginals_fromGreen function (<ext-link ext-link-type="uri" xlink:href="https://github.com/brialorelle/texformgen">https://github.com/brialorelle/texformgen</ext-link>).</p></sec><sec id="S17"><title>Grayscale phase-scramble stimul</title><p id="P19">Phase-scramble stimuli were created as in Experiment 1, to remove mid-level (shape and texture) information, while preserving the number of pixels, color, contrast, luminance and power-spectral distribution of the grayscale images. We verified that the objects in these images were unrecognizable with a naming task administered to an independent sample of 19 native-French speakers participants, recruited and tested on the online platform <ext-link ext-link-type="uri" xlink:href="http://Testable.org">Testable.org</ext-link> (<xref ref-type="bibr" rid="R36">Rezlescu et al., 2020</xref>). An image was considered ‘unrecognizable’ when no more than one participant named it correctly (i.e., naming a fish ‘fish’, a flower ‘flower’, etc.). All images resulted to be ‘unrecognizable’, as no more than one participant could name them correctly (only 2 images were named correctly by one participant; all others were never correctly named).</p></sec></sec><sec id="S18" sec-type="methods"><label>2.2.3</label><title>Procedures</title><p id="P20">The design for the EEG experiment, the EEG recording, preprocessing and analyses were identical to Experiment 1.</p></sec></sec><sec id="S19"><label>2.3</label><title>Experiment 3</title><sec id="S20" sec-type="subjects"><label>2.3.1</label><title>Participants</title><p id="P21">Experiment 3 involved 12 healthy adults (9 identified their gender as female, 3 as male, mean age 23.7 ± 2.7 years). One additional participant was tested and excluded from the analysis for falling asleep during the experiment. An independent sample of 19 native-French speakers (17 identified their gender as female, mean age 23.3 ± 3.8 years) took part in an online behavioral-judgement task for stimulus recognizability assessment. They all reported to have normal or corrected-to-normal vision and no history of psychiatric or neurological conditions.</p></sec><sec id="S21"><label>2.3.2</label><title>Stimuli</title><p id="P22">Experiments 3 involved two sets of stimuli: 350 ‘intact’ grayscale image and the corresponding ‘impoverished’ <italic>texforms</italic>.</p><sec id="S22"><title>Texforms stimuli</title><p id="P23">Texforms are unrecognizable versions of the animate and inanimate stimuli that preserved some textural and form information (<xref ref-type="bibr" rid="R12">Freeman &amp; Simoncelli, 2011</xref>; <xref ref-type="bibr" rid="R24">Long et al., 2017</xref>). Texforms were created from the grayscale ‘intact’ images used in Experiment 2, following the method in <xref ref-type="bibr" rid="R11">Deza et al. (2019)</xref>. From that set, we selected the unrecognizable texform stimuli, as assessed with the online behavioral-judgement task, yielding a final set of 195 animate and 175 inanimate texform-images. This way, we could test categorization based on the visual features preserved in the texforms, without the images being recognized at a higher level. To match the number of items between conditions, we randomly selected and removed 20 animate images, resulting in a total of 350 stimuli (175 animates and 175 inanimates).</p></sec><sec id="S23"><title>Grayscale stimuli</title><p id="P24">From the ‘intact’ set of Experiment 2 (grayscale stimuli), we picked the 175 inanimate and 175 animate stimuli corresponding to the texforms selected above.</p></sec></sec><sec id="S24" sec-type="methods"><label>2.3.3</label><title>Procedures</title><p id="P25">The design for the EEG experiment, the EEG recording, preprocessing and analyses were identical to Experiments 1-2, except for the total number of images (175 in each category, instead of 320) and the number of items used as oddball in each sequence (47 instead of 68).</p></sec></sec><sec id="S25"><label>2.4</label><title>Experiment 4</title><sec id="S26" sec-type="subjects"><label>2.4.1</label><title>Participants</title><p id="P26">Experiment 4 involved a total of 60 healthy, adult, native-English speakers (28 identified their gender as female, 31 as male, 1 as “other”, mean age 26.6 ± 3.8 years), external to the above EEG and naming studies. They were recruited online through the platform <ext-link ext-link-type="uri" xlink:href="http://Testable.org">Testable.org</ext-link> (<xref ref-type="bibr" rid="R36">Rezlescu et al., 2020</xref>), and randomly assigned to one of three animate-inanimate categorization tasks involving respectively, the phase-scramble stimuli of Experiment 1 (<italic>n</italic>=20, 12 identified their gender as female, 8 as male, mean age 26.6 ± 4 years), the grayscale phase-scramble stimuli in Experiment 2 (<italic>n</italic>=20, 7 identified their gender as female, 12 as male, 1 as other, mean age 26.5 ± 4.1) and the texforms used to select stimuli for Experiment 3 (<italic>n</italic>=20, 9 identified their gender as female, 11 as male, mean age 26.7 ± 3.3).</p></sec><sec id="S27" sec-type="methods"><label>2.4.2</label><title>Procedures</title><p id="P27">Participants were instructed to sit 60 cm away from the screen (about the length of an arm), align their eyes with the center of the screen, and make an effort to not move during the experiment. They were also asked to turn off their phone and any other device. Prior to the experiment, they gave informed consent and followed instructions for screen calibration. We used a yes-or-no forced-choice task in which participants were instructed to decide whether each image depicted an animal or not. They were informed that they would see 640 transformed images of existing objects, presented one by one, and had to decide whether the image could be an animal by clicking on the corresponding “yes” or “no” box on the screen, using the mouse. Stimuli were shown in the center of the screen (16° of visual angle, assuming a distance of ~60 cm from screen) until the participant responded. Underneath each image, two buttons appeared, for yes or no response. The side of the “yes” and “no” buttons were counterbalanced between participants but were always the same for one participant. Images were presented in a random order. Three different groups performed the task on the phase-scramble stimuli of Experiment 1, the grayscale phase-scramble stimuli in Experiment 2 and the texform stimuli of Experiment 3, respectively.</p></sec><sec id="S28"><label>2.4.3</label><title>Analyses</title><p id="P28">Using data from the yes-or-no forced-choice task, we tested whether even if a specific object in the impoverished set (i.e., phase-scramble, grayscale phase-scramble, texform) was not recognizable, participants could still <italic>guess</italic> its superordinate-level category (i.e., animate or inanimate) in a forced-choice. To this end, we performed a signal detection analysis by computing a <italic>d’</italic>, as a measure of participants’ performance to detect animate stimuli, and tested <italic>d’</italic> against chance (one-sample <italic>t</italic>-test, one-sided).</p></sec></sec><sec id="S29"><label>2.5</label><title>Experiment 5</title><sec id="S30"><label>2.5.1</label><title>Stimuli and Procedures</title><p id="P29">We selected a convolutional deep neural network, VGG-19 (<xref ref-type="bibr" rid="R44">Simonyan &amp; Zisserman, 2015</xref>), which provide a reliable model of the primate visual system, particularly the ventral temporal cortex (VTC) and has proven successful in reaching human-level performance in object recognition (<xref ref-type="bibr" rid="R16">Khaligh-Razavi and Kriegeskorte, 2014</xref>; <xref ref-type="bibr" rid="R54">Yamins et al., 2014</xref>; <xref ref-type="bibr" rid="R17">Kheradpisheh et al., 2016</xref>; <xref ref-type="bibr" rid="R20">Kubilius et al., 2016</xref>). With this DNN, we explored whether the different features preserved in each of the five stimulus sets of Experiments 1-3 (original and phase-scramble stimuli of Experiment 1, grayscale stimuli of Experiments 2-3, grayscale phase-scramble stimuli of Experiment 2 and texform stimuli of Experiment 3) allowed for the animate-inanimate categorization in processing stages, or different layers, of the network. In particular, we tested whether, for each stimulus set, categorization by animacy emerged in the first, middle or deeper layers of the DNN, modelling low-, mid- and higher-level visual areas (VTC), respectively.</p><p id="P30">Multiple processing stages in this DNN transform input images through a series of nonlinear operations. Initially, convolutional layers apply kernels to small regions of the input image, capturing spatial hierarchies of visual features. Then, a rectified linear unit (ReLU) function introduces nonlinearity by thresholding activations at zero, allowing only positive activations to pass forward. Then, max pooling layers perform downsampling, which reduces the spatial dimensions of the input while preserving important features. Finally, fully connected layers flatten the processed input into a one-dimensional vector, which represents the class scores. This network was trained on approximately 1.2 million images from the ImageNet database (ILSVRC2012), encompassing 1,000 classes including animals (40%) and objects (60%). We used a pretrained version of this model available in MATLAB (MatConvNet; Vedaldi and Lenc, 2016), with standardized preprocessing steps such as mean subtraction of the training images and scaling of all stimuli to 224x224 pixels.</p></sec><sec id="S31"><label>2.5.2</label><title>Analyses</title><p id="P31">We extracted features from convolutional layers (‘conv1_1’ through ‘conv5_4’) and fully connected layers (‘fc6’, ‘fc7’, and ‘fc8’). For each of these layers, for each set of stimuli used in Experiments 1-3, we computed a dissimilarity matrix (<xref ref-type="bibr" rid="R19">Kriegeskorte et al., 2008</xref>; see <xref ref-type="fig" rid="F4">Figure 4</xref> for examples) representing the pairwise dissimilarities between the features extracted from the stimuli computed as 1-<italic>rho</italic>, where <italic>rho</italic> is the <italic>Pearson</italic> coefficient of the correlation between vectors of features extracted for the two stimuli. Separately for each stimulus set, we performed <italic>Spearman</italic> correlations between the matrix extracted from each layer and the <italic>reference matrix</italic>, corresponding to the dissimilarity matrix extracted for the original set of stimuli from the output layer of the DNN (‘fc8’ for VGG-19 and ‘loss3_classifier’ for GoogLeNet). This reference matrix was chosen as it reflected the performance on the stimulus set with the highest complexity (i.e., highest number of features), and the closest to the stimuli used for training the DNN, thus providing the best representation of the stimuli by the DNN, typically associated with the best performance of the model. Finally, the performance throughout all the layers of a DNN was compared between the different stimulus sets, using pairwise <italic>t</italic>-tests on the Fisher-transformed correlation coefficients.</p></sec></sec><sec id="S32" sec-type="data-availability"><label>2.6</label><title>Data availability</title><p id="P32">Stimuli, EEG data and code for the main analyses will be deposited in the Open Science Framework repository created for this project.</p></sec></sec><sec id="S33" sec-type="results"><label>3</label><title>Results</title><sec id="S34"><label>3.1</label><title>Experiments 1-3: ftEEG test of the animate-inanimate distinction in different stimulus-sets</title><p id="P33">Experiment 1 tested whether the fast presentation of a sequence of images from a large and heterogeneous set could elicit a response in correspondence to the regular appearance of an animate object among inanimate objects, or <italic>vice versa</italic>, thus signaling automatic detection of the categorical change. By using phase-scramble versions of the same images, we tested whether such categorization-response could be elicited by stimuli that only preserved low-level visual features of animate and inanimate objects (i.e., power-spectrum, color, contrast, and luminance).</p><p id="P34">First, we verified that a response was observed at the base stimulation frequency (6 Hz), indicating reliable synchronization of neural activity with visual stimulation. All harmonics below 50 Hz showed a significant response (<italic>z</italic> &gt; 1.64), widely distributed over the scalp (<xref ref-type="fig" rid="F2">Figure 2A</xref>), in both the ‘intact’ (<italic>M</italic><sub><italic>Amplitude</italic></sub>±<italic>sd</italic> = 0.802±0.360; 95% CI = 0.615 – Inf; <italic>t</italic>(11) = 7.714; <italic>P</italic> &lt; .0001; <italic>d</italic> = 2.227) and ‘impoverished’ (phase-scramble) condition (<italic>M</italic><sub><italic>Amplitude</italic></sub>±<italic>sd</italic> = 0.771±0.348; 95% CI = 0.591 – Inf; <italic>t</italic>(11) = 7.671; <italic>P</italic> &lt; .0001; <italic>d</italic> = 2.215). Second, we analyzed the response at the oddball frequency, indicating detection of a categorical change. A significant (above noise-level) widespread categorization response, peaking over posterior electrodes, was found in both the ‘intact’ condition (first eight harmonics; <italic>M</italic><sub><italic>Amplitude</italic></sub>±<italic>sd</italic> = 0.319±0.083; 95% CI = 0.276 – Inf; <italic>t</italic>(11) = 13.254; <italic>P</italic> &lt; .0001; <italic>d</italic> = 3.826; <xref ref-type="fig" rid="F2">Figure 2 A</xref>, framed in black) and the ‘impoverished’ condition (first four harmonics; <italic>M</italic><sub><italic>Amplitude</italic></sub>±<italic>sd</italic> = 0.126±0.059; 95% CI = 0.096 – Inf; <italic>t</italic>(11) = 7.480; <italic>P</italic> &lt; .0001; <italic>d</italic> = 2.159; <xref ref-type="fig" rid="F2">Figure 2 A</xref>, framed in black; <xref ref-type="table" rid="T1">Table 1</xref>). This response was larger in the former condition (<italic>M</italic><sub><italic>Difference</italic></sub>± <italic>sd</italic> = 0.205 ±0.100; 95% CI = 0.142 – 0.269; <italic>t</italic>(11) = 7.108; <italic>P</italic> &lt; .0001; <italic>d</italic> = 2.052) with no difference between the two Sequence-Type conditions (a 2 Image Type x 2 Sequence-Type ANOVA showed an effect of Image Type, <italic>F</italic>(1,11) = 51.006, <italic>P</italic> &lt; .0001, <italic>η</italic><sup><italic>2</italic></sup> = .823, but no effect of Type of sequence, <italic>F</italic>(1,11) = 1.146, <italic>P</italic> = .307, <italic>η</italic><sup><italic>2</italic></sup> = .094, or interaction, <italic>F</italic>(1,11) = 0.959, <italic>P</italic> = .349, <italic>η</italic><sup><italic>2</italic></sup> = .080).</p><p id="P35">Results of Experiment 2 replicated those of Experiment 1, with grayscale (‘intact’) and grayscale phase-scramble (‘impoverished’) images, matched for contrast and luminance between the animate and inanimate set. In particular, we found a significant response at the base fequency across all harmonics (below 50 Hz) and widely distributed over the scalp, in both conditions (grayscale: <italic>M</italic><sub><italic>Amplitude</italic></sub> ±<italic>sd</italic> = 0.439 ±0.171; 95% CI = 0.351 – Inf; <italic>t</italic>(11) = 8.927; <italic>P</italic> &lt; .0001; <italic>d</italic> = 2.577; grayscale phase-scramble: <italic>M</italic><sub><italic>Amplitude</italic></sub>±<italic>sd</italic> = 0.359 ±0.156; 95% CI = 0.278 – Inf; <italic>t</italic>(11) = 7.983; <italic>P</italic> &lt; .0001; <italic>d</italic> = 2.305) (<xref ref-type="fig" rid="F2">Figure 2 B</xref>). Moreover, a significant categorization response peaking over posterior electrodes was found in both the grayscale ‘intact’ condition (first eight harmonics; <italic>M</italic><sub><italic>Amplitude</italic></sub> ±<italic>sd</italic> = 0.240±0.153; 95% CI = 0.161 – Inf; <italic>t</italic>(11) = 5.424; <italic>P</italic> &lt; .001; <italic>d</italic> = 1.566; <xref ref-type="table" rid="T1">Table 1</xref>; <xref ref-type="fig" rid="F2">Figure 2 B</xref>, framed in black) and the ‘impoverished’ phase-scramble condition (first three harmonics; <italic>M</italic><sub><italic>Amplitude</italic></sub>±<italic>sd</italic> = 0.012 ±0.017; 95% CI = 0.003 – Inf; <italic>t</italic>(11) = 2.459; <italic>P</italic> = .016; <italic>d</italic> = 0.710 (<xref ref-type="table" rid="T1">Table 1</xref>; <xref ref-type="fig" rid="F2">Figure 2 B</xref>, framed in black). The categorization response was larger in the ‘intact’ (<italic>vs</italic>. ‘impoverished’) condition (<italic>M</italic><sub><italic>Difference</italic></sub> ±<italic>sd</italic> = 0.180±0.171; 95% CI = 0.071 – 0.289; <italic>t</italic>(11) = 3.640; <italic>P</italic> = .004; <italic>d</italic> = 1.051), with no differences between Sequence Types (a 2 Image Type x 2 Sequence Type ANOVA revealed an effect of Image Type, <italic>F</italic>(1,11) = 13.066, <italic>P</italic> = .004, <italic>η</italic><sup><italic>2</italic></sup> = .543, but no effect of Sequence Type, <italic>F</italic>(1,11) = 0.013, <italic>P</italic> = .913, <italic>η</italic><sup><italic>2</italic></sup> = .001, or interaction (<italic>F</italic>(1,11) = 0.232, <italic>P</italic> = .640, <italic>η</italic><sup><italic>2</italic></sup> = .021).</p><p id="P36">Finally, Experiment 3 showed animate-inanimate categorization for grayscale (‘intact’) images (same as in Experiment 2) as well as for texforms, which only retained mid-level features of the objects (texture and global form). In particular, we found a response at the base fequency across all harmonics (below 50 Hz), and widely distributed over the scalp, in both conditions (grayscale: <italic>M</italic><sub><italic>Amplitude</italic></sub> ±<italic>sd</italic> = 0.605±0.311; 95% CI = 0.444 – Inf; <italic>t</italic>(11) = 6.745; <italic>P</italic> &lt; .0001; <italic>d</italic> = 1.947; texforms: <italic>M</italic><sub><italic>Amplitude</italic></sub> ±<italic>sd</italic> = 0.456±0.191; 95% CI = 0.357 – Inf; <italic>t</italic>(11) = 8.262; <italic>P</italic> &lt; .0001; <italic>d</italic> = 2.385) (<xref ref-type="fig" rid="F2">Figure 2 C</xref>). Moreover, a categorization response (i.e., at the oddball frequency), peaking over posterior electrodes, was found for both grayscale ‘intact’ stimuli (first eight harmonics; <italic>M</italic><sub><italic>Amplitude</italic></sub> ±<italic>sd</italic> = 0.270±0.087; 95% CI = 0.224 – Inf; <italic>t</italic>(11) = 10.697; <italic>P</italic> &lt; .0001; <italic>d</italic> = 3.088) and texforms (first two harmonics; <italic>M</italic><sub><italic>Amplitude</italic></sub> ±<italic>sd</italic> = 0.018±0.023; 95% CI = 0.006 – Inf; <italic>t</italic>(11) = 2.648; <italic>P</italic> = .011; <italic>d</italic> = 0.764) (<xref ref-type="fig" rid="F2">Figure 2 C</xref>, framed in black; <xref ref-type="table" rid="T1">Table 1</xref>). The categorization response was larger in the ‘intact’ (<italic>vs</italic>. ‘impoverished’) condition (<italic>M</italic><sub><italic>Difference</italic></sub> ±<italic>sd</italic> = 0.241±0.101; 95% CI = 0.177 – 0.305; <italic>t</italic>(11) = 8.292; <italic>P</italic> &lt; .0001; <italic>d</italic> = 2.394), with no differences between Sequence Types (a 2 Image Type x 2 Sequence Type ANOVA revealed an effect of Image Type, <italic>F</italic>(1,11) = 70.443, <italic>P</italic> &lt; .0001, <italic>η</italic><sup><italic>2</italic></sup> = .865, but no effect of Sequence Type, <italic>F</italic>(1,11) = 0.027, <italic>P</italic> = .872, <italic>η</italic><sup><italic>2</italic></sup> = .002, or interaction (<italic>F</italic>(1,11) = 3.808, <italic>P</italic> = .077, <italic>η</italic><sup><italic>2</italic></sup> = .257).</p></sec><sec id="S35"><label>3.2</label><title>Experiments 1-3: Comparison of categorization effects across different stimulus-sets</title><p id="P37">We compared the categorization-response across experiments, and therefore across stimulus-sets, to study whether and how such response was affected by the complexity of the images. For each participant, for each experiment and condition, we computed a single response corresponding to the sum of baseline-subtracted amplitudes of all harmonics showing a significant categorization-response in at least one condition, averaged across all 128 electrodes and across the sequence types (animate, inanimate). These values were entered in an ANOVA with Image Type (‘intact’ <italic>vs</italic>. ‘impoverished’) as within-subjects factor and Experiment as between-subjects factor. Results showed effects of Image Type, <italic>F</italic>(1,33) = 96.203, <italic>P</italic> &lt; .0001, <italic>η</italic><sup><italic>2</italic></sup> = .745, and Experiment, <italic>F</italic>(2,33) = 7.566, <italic>P</italic> = .002, <italic>η</italic><sup><italic>2</italic></sup> = .314, but no interaction between the two, <italic>F</italic>(2,33) = 0.775, <italic>P</italic> = .469, <italic>η</italic><sup><italic>2</italic></sup> = .045.</p><p id="P38">The effect of Image Type revealed that, in all experiments, the categorization-response was larger in the ‘intact’ than in the ‘impoverished’ condition. The effect of Experiment revealed that the categorization-response was larger in Experiment 1 than in Experiment 2 (<italic>M</italic><sub><italic>Difference</italic></sub> ±<italic>sd</italic> = 0.105±0.075; 95% CI = 0.041 – 0.169; <italic>t</italic>(22) = 3.399; <italic>P</italic> = .003; <italic>d</italic> = 1.388) and in Experiment 1 than in Experiment 3 (<italic>M</italic><sub><italic>Difference</italic></sub> ±<italic>sd</italic> = 0.065±0.047; 95% CI = 0.025 – 0.105; <italic>t</italic>(22) = 3.397; <italic>P</italic> = .003; <italic>d</italic> = 1.387), but did not differ between Experiments 2 and 3 (<italic>M</italic><sub><italic>Difference</italic></sub>±<italic>sd</italic> = -0.040±0.073; 95% CI = -0.023 – 0.102; <italic>t</italic>(22) = -1.317; <italic>P</italic> = .202; <italic>d</italic> = 0.538).</p><p id="P39">Then, we compared the categorization response across the ‘intact’ sets and across the ‘impoverished’ sets, separately, using <italic>t</italic> tests. As anticipated by the main effect of Experiment, in the comparisons across experiments, both the ‘intact’ and the ‘impoverished’ conditions of Experiment 1yielded qualitatively the strongest effect. In particular, for the ‘intact’ sets, there was a significant difference between Experiments 1 and 2 (<italic>M</italic><sub><italic>Difference</italic></sub>±<italic>sd</italic> = 0.117±0.134; 95% CI = 0.004 – 0.231; <italic>t</italic>(22) = 2.141; <italic>P</italic> = .044; <italic>d</italic> = 0.874), but not between Experiments 1 and 3 (<italic>M</italic><sub><italic>Difference</italic></sub>±<italic>sd</italic> = 0.045±0.082; 95% CI = -0.025 – 0.115; <italic>t</italic>(22) = 1.346; <italic>P</italic> = .192; <italic>d</italic> = 0.550), or between Experiments 2 and 3 (<italic>M</italic><sub><italic>Difference</italic></sub>±<italic>sd</italic> = 0.072±0.136; 95% CI = -0.043 – 0.187; <italic>t</italic>(22) = 1.298; <italic>P</italic> = .208; <italic>d</italic> = 0.530). For the ‘impoverished’ sets, there was a significant difference between Experiments 1 and 2 (<italic>M</italic><sub><italic>Difference</italic></sub>±<italic>sd</italic> = 0.092±0.057; 95% CI = 0.044 – 0.140; <italic>t</italic>(22) = 3.958; <italic>P</italic> &lt; .001; <italic>d</italic> = 1.616) and Experiments 1 and 3 (<italic>M</italic><sub><italic>Difference</italic></sub>±<italic>sd</italic> = 0.085±0.052; 95% CI = 0.041 – 0.129; <italic>t</italic>(22) = 4.035; <italic>P</italic> &lt; .001; <italic>d</italic> = 1.647), but not between Experiments 2 and 3 (<italic>M</italic><sub><italic>Difference</italic></sub>±<italic>sd</italic> = 0.007±0.048; 95% CI = -0.033 – 0.048; <italic>t</italic>(22) = 0.364; <italic>P</italic> = .720; <italic>d</italic> = 0.149).</p></sec><sec id="S36"><label>3.3</label><title>Experiment 4: Explicit category judgments</title><p id="P40">Using an explicit forced-choice categorization task, we tested whether accurate (above-chance) animate-inanimate categorization could be informed by the low- and mid-level features preserved in unrecognizable images, which yielded the categorization-response in EEG.</p><p id="P41">Results showed that categorization was at chance for grayscale phase-scramble stimuli (<italic>M</italic><sub><italic>d’</italic></sub> = - 0.014±0.0211; 95% CI = -0.096 – Inf; <italic>t</italic>(19) = -0.299; <italic>P</italic> = 0.616; <italic>d</italic> = 0.067), but above chance for colorful phase-scramble stimuli (<italic>M</italic><sub><italic>d’</italic></sub> = 0.123 ±0.200; 95% CI = 0.045 – Inf; <italic>t</italic>(19) = 2.742; <italic>P</italic> = 0.007; <italic>d</italic> = 0.613), and texforms (<italic>M</italic><sub><italic>d’</italic></sub> = 0.743±0.262; 95% CI = 0.642 – Inf; <italic>t</italic>(19) = 12.690; <italic>P</italic> &lt; 0.0001; <italic>d</italic> = 2.838). Moreover, categorization was higher for texforms compare to phase-scramble stimuli (<italic>M</italic><sub><italic>difference</italic></sub> = 0.621±0.233; 95% CI = 0.472 – 0.770; <italic>t</italic>(38) = 8.423; <italic>P</italic> &lt; 0.0001; <italic>d</italic> = 2.664) and grayscale phase-scramble stimuli (<italic>M</italic><sub><italic>difference</italic></sub> = 0.758±0.238; 95% CI = 0.605 – 0.910; <italic>t</italic>(38) = 10.067; <italic>P</italic> &lt; 0.0001; <italic>d</italic> = 3.184), and higher for phase-scramble stimuli than for grayscale phase-scramble stimuli (<italic>M</italic><sub><italic>difference</italic></sub> = 0.137±0.206; 95% CI = 0.005 – 0.268; <italic>t</italic>(38) = 2.102; <italic>P</italic> = 0.042; <italic>d</italic> = 0.665). Results did not change when we considered only the unrecognizable texforms selected for Experiment 3: categorization for this set was above chance (<italic>M</italic><sub><italic>d’</italic></sub> = 0.529±0.216; 95% CI = 0.445 – Inf; <italic>t</italic>(19) = 10.969; <italic>P</italic> &lt; 0.0001; <italic>d</italic> = 2.453), and higher than for phase-scramble (<italic>M</italic><sub><italic>difference</italic></sub> = 0.441±0.211; 95% CI = 0.306 – 0.576; <italic>t</italic>(38) = 6.614; <italic>P</italic> &lt; 0.0001; <italic>d</italic> = 2.092) and grayscale phase-scramble stimuli (<italic>M</italic><sub><italic>difference</italic></sub> = 0.552±0.217; 95% CI = 0.414 – 0.691; <italic>t</italic>(38) = 8.060; <italic>P</italic> &lt; 0.0001; <italic>d</italic> = 2.549).</p><p id="P42">In sum, participants’ performance in forced-choice categorization showed that color, contrast, luminance, texture and global form (i.e., the features preserved in colorful-phase-scramble images and texforms) carry information that aids the recognition of objects as animate or inanimate; among those features, texture and global form appear to be the most reliable towards such decision.</p></sec><sec id="S37"><label>3.4</label><title>Experiment 5: animate-inanimate categorization from different sets in Deep Neural Network</title><p id="P43">The EEG results suggest that categorization by animacy may emerge at various levels of the visual hierarchy (i.e., in higher-level anterior areas and early or middle areas of the ventral stream) as highlighted by the categorization effects obtained for both ‘intact’ and ‘impoverished’ stimuli. To probe this, we investigated which visual features, carried by different stimulus sets, distinguished between animate and inanimate objects, across different layers of an artificial DNN –VGG19. In particular, we asked whether animate-inanimate categorical information could be extracted in deeper as well as middle and first layers of the model.</p><p id="P44">First, we tested the validity of our <italic>reference</italic>-matrix (i.e., the dissimilarity matrix extracted from the output ‘fc8’ layer of VGG-19, for the original stimulus set), showing that it correlated significantly with a synthetic <italic>predictor</italic>-matrix representing the distinction between animate and inanimate objects (<italic>ρ</italic> = 0.826, <italic>P</italic> &lt; .001). Second, we found that the dissimilarity matrices for the other stimulus sets increasingly matched the reference matrix as stimulus complexity increased. In particular, correlations were the highest for the grayscale set (maximal correlation in layer fc8, <italic>ρ</italic> = .817, <italic>P</italic> &lt; .001), although they were also significant for all other –more impoverished– sets (maximal correlation: phase-scramble, layer conv2_1, <italic>ρ</italic> = .345, <italic>P</italic> &lt; .001; grayscale phase-scramble, layer conv2_2, <italic>ρ</italic> = .270, <italic>P</italic> &lt; .001; texform, layer conv2_2, <italic>ρ</italic> = .265, <italic>P</italic> &lt; .001) (<xref ref-type="table" rid="T2">Table 2</xref>).</p><p id="P45">Third, we asked <italic>where</italic> the animate-inanimate categorical distinction emerged and peaked for the different stimulus sets. For the intact sets (colorful and grayscale), the coefficients of the correlation with the reference matrix increased as layers got deeper in the DNN (<xref ref-type="table" rid="T2">Table 2</xref>), peaking in the last fully connected layers, with a significant difference between the two sets (<italic>M</italic><sub><italic>difference</italic></sub> = 0.066 ±0.062; 95% CI = 0.036 – 0.096; <italic>t</italic>(18) = 4.644; <italic>P</italic> &lt; 0.001; <italic>d</italic> = 1.065). For the impoverished (unrecognizable) sets, correlations across layers with the reference matrix were weaker, compared with the intact colorful set (phase-scramble: <italic>M</italic><sub><italic>difference</italic></sub> = 0.157 ±0.247; 95% CI = 0.037 – 0.276; <italic>t</italic>(18) = 2.758; <italic>P</italic> = 0.013; <italic>d</italic> = .633; grayscale phase-scramble: <italic>M</italic><sub><italic>difference</italic></sub> = 0.207 ±0.237; 95% CI = 0.093 – 0.321; <italic>t</italic>(18) = 3.814; <italic>P</italic> = 0.001; <italic>d</italic> = .875; texform: <italic>M</italic><sub><italic>difference</italic></sub> = 0.130 ±0.217; 95% CI = 0.025 – 0.235; <italic>t</italic>(18) = 2.605; <italic>P</italic> = 0.018; <italic>d</italic> = .598). In effect, for the phase-scramble and texform sets, correlation coefficients did not change much from the first and middle layers to the deeper layers (<xref ref-type="fig" rid="F4">Figure 4A</xref>), implying that the information extracted in deeper layers did not add to the representation of those images. In some cases (i.e., for both phase-scramble sets), correlation coefficients were even higher in middle than deepest layers, meaning that middle layers were better tuned to the features carried by those stimuli. These results show that the animate-inanimate object distinction can emerge across different processing stages of the visual hierarchy, and rely on high-level, as well as mid- and low-level visual features. These results were fully replicated with another DNN –GoogLeNet (<xref ref-type="bibr" rid="R47">Szegedy et al., 2015</xref>).</p></sec></sec><sec id="S38" sec-type="discussion"><label>4</label><title>Discussion</title><p id="P46">The animate-inanimate distinction underlies the <italic>life detection</italic> capacity that is crucial for the survival of many animal species. Thus, in an evolutionary perspective, it is not surprising that such distinction emerges shortly after visual information reaches the visual cortex, suggesting an underlying feedforward mechanism (<xref ref-type="bibr" rid="R6">Carlson et al., 2013</xref>; <xref ref-type="bibr" rid="R7">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="R9">Contini et al., 2017</xref>; <xref ref-type="bibr" rid="R32">Proklova et al., 2019</xref>). On the hypothesis that categorization by animacy is primarily a visual process, we used fast periodic visual stimulation combined with ftEEG, to investigate: (i) whether the automatic neural response locked to the stimulus appearance already carries information about the categorical animate-inanimate distinction, and (ii) whether such information can be extracted from impoverished images, only retaining low- to mid-level features of animate or inanimate objects. To this end, we sampled the variety of real-world objects in a largely heterogeneous set of images, in which animate objects were as varied as mammals, birds, fish and amphibians, and inanimate objects were as varied as furniture, vehicles, tools, plants and vegetables.</p><p id="P47">ftEEG results demonstrated that information relevant for the animate-inanimate distinction was extracted rapidly from higher-level as well as mid- and low-level visual features. In particular, a strong, widespread response to a regular categorical change in the stream of visual images was found for colorful, <italic>intact</italic>, depictions of real-world objects. This response was only moderately diminished when color, luminance and contrast differences were removed (i.e., in gray-scale ‘intact’ images), and persisted, although significantly weaker, for unrecognizable images that only kept low-level spectral information (color, contrast, luminance, number of pixels, power spectrum, in the phase-scramble sets) or mid-level information (texture and global form in texforms). These findings add to a growing body of studies demonstrating that object categorization is the result of tuning to complex visual features in higher-level visual areas, as well as to low- and mid-level visual information, allegedly encoded in early and middle aspects of the visual ventral stream (<xref ref-type="bibr" rid="R8">Coggan et al., 2016</xref>; <xref ref-type="bibr" rid="R24">Long et al., 2017</xref>, <xref ref-type="bibr" rid="R25">2018</xref>; <xref ref-type="bibr" rid="R39">Rosenthal et al., 2018</xref>; <xref ref-type="bibr" rid="R55">Zachariou et al., 2018</xref>; <xref ref-type="bibr" rid="R52">Wang et al., 2022</xref>; <xref ref-type="bibr" rid="R18">Kramer et al., 2023</xref>). Furthermore, these findings characterize object recognition as a process in which the accumulation of visual information strengthens categorical distinctions, resulting in the increasing categorization response captured with EEG.</p><p id="P48">Our analysis of the VGG-19 DNN, replicated with another DNN –GoogLeNet (<xref ref-type="bibr" rid="R47">Szegedy et al., 2015</xref>)– confirmed this model. We found that intact (colorful or grayscale) images were represented across all layers of the DNNs, with increasingly high accuracy, and the highest accuracy in deeper layers, modeling higher-level visual areas (VTC). Instead, for impoverished (phase-scramble) images, representation was the most accurate in middle layers, suggesting that those layers are better tuned to the low- and middle-level features carried by those stimuli. However, above and beyond these differences, the animate-inanimate distinction emerged across all layers and for all stimulus sets (i.e., correlations with the reference-matrix were significant). This supports the view that the earlier layers of the networks, modelling early and middle visual areas, are tuned to visual features that are informative with respect to the animate-inanimate distinction.</p><p id="P49">Finally, with a behavioral study, we addressed the extent to which features at different levels of complexity could support explicit categorization by animacy in a forced-choice task. The analysis of the participants’ responses showed that, while animate-inanimate classification was at chance for the grayscale phase-scramble set, texforms and colorful phase-scramble images yielded above chance performance. These findings suggest that the categorization response in the EEG signal, weak as it might be (in the case of phase-scramble and texform sets), is behaviorally relevant, driving the explicit categorization of stimuli in the forced-choice task (see also <xref ref-type="bibr" rid="R24">Long et al., 2017</xref>).</p><p id="P50">Interestingly, these findings also imply that information about low- and mid-level features gives access to supra-ordinate (animate-inanimate) categories. In the preliminary study of our stimuli, we asked participants to name texforms and phase-scramble images, and expected them to privilege basic-level category labels (e.g., naming a zucchini “zucchini”, rather than “vegetable” or “inanimate object”; <xref ref-type="bibr" rid="R38">Rosch, 1978</xref>; <xref ref-type="bibr" rid="R27">Murphy and Brownell, 1985</xref>; <xref ref-type="bibr" rid="R37">Rogers and Patterson, 2007</xref>; <xref ref-type="bibr" rid="R23">Long and Konkle, 2017</xref>; <xref ref-type="bibr" rid="R24">Long et al., 2017</xref>). None of the phase-scramble images was named correctly, and only a subset of texforms was named by ~15% of participants (see <italic>Materials and methods</italic>). However, when subjects were ‘forced’ to choose between the two supra-ordinate (animate-inanimate) categories, performance improved reaching above-chance level. It remains possible that participants would succeed in a forced-choice task in which they had to choose between two basic-level categories (e.g., cat or dog). However, the priority access to supra-ordinate (animate-inanimate) categories with impoverished sets is conceptually consistent with recent findings showing that, while young (4-month-old) infants can represent the broad animate-inanimate distinction among real-world objects, the representation of finer-grained (e.g., basic-level) categories develops later and requires the progressive recruitment and integration of more and more feature spaces distributed across the visual cortex (<xref ref-type="bibr" rid="R45">Spriet et al., 2022</xref>). Thus, based on previous and the current results, we suggest that low- and mid-level features give access to supraordinate (animate-inanimate) categories, while access to finer-grained categorical distinctions (basic-level categories) may require availability and integration of more complex visual information.</p><p id="P51">In sum, in this study we asked what accounts for the efficient animate-inanimate categorization that supports life detection. Our results demonstrate that information relevant for the animate-inanimate distinction is already extracted from low- and mid-level features, suggesting tuning to animacy in early and middle level areas of the visual stream. Put in another way, the absence of higher-level information in our impoverished sets implies that the animate-inanimate distinction observed for those stimuli did not result from top-down connections, but perception is optimized to classify animate and inanimate stimuli in the early stages of visual processing.</p><p id="P52">In the light of the present results, <italic>what makes things look animate to humans?</italic> It is clear that a variety of features, from low- to higher-level, participate in this categorization. Our approach, in which the amplitude of the EEG categorization-response is taken as an index of classification accuracy, suggests a hierarchy of distinctiveness, according to which higher-level features (in the ‘intact’ sets) are more distinctive than mid- and low-level features (isolated in the ‘impoverished’ sets), and mid-level features (in texforms) are more distinctive than low-level features (in phase-scramble sets) (<xref ref-type="bibr" rid="R14">Grootswagers et al., 2019</xref>; see also <xref ref-type="bibr" rid="R24">Long et al., 2017</xref>, <xref ref-type="bibr" rid="R25">2018</xref>; <xref ref-type="bibr" rid="R52">Wang et al., 2022</xref>). Among the low- and mid-level features, color seems to add significant gain in classification accuracy: presence of color increased the categorization-response from the colorful to the grayscale ‘intact’ set, and from the colorful to the grayscale phase-scramble set, and yielded above-chance classification in the force-choice task with phase-scramble stimuli (see also <xref ref-type="bibr" rid="R39">Rosenthal et al., 2018</xref>).</p><p id="P53">Thus, there is not <italic>one single feature</italic> responsible for animacy perception. Instead, we propose that animacy perception could be conceived as a case of <italic>lack of invariance problem</italic> in vision. The lack of invariance problem captures a characteristic of speech perception, whereby the categorization of a sound (e.g., /t/) is not defined by one particular acoustic feature but depends on the context (the surrounding sounds) in which the sound is produced (<xref ref-type="bibr" rid="R21">Liberman et al., 1967</xref>). By analogy, no one visual feature would define animacy on its own, but animacy perception would result from the combination of a range of visual features, at different level of complexity, which can be further disambiguated by co-occurring dynamic and non-visual information.</p><p id="P54">In conclusion, we showed that the animate-inanimate distinction is pervasive in the processing of visual stimuli, and resilient to the loss of information in the visual input: low- and mid-level features, encoded in early and middle-level aspects of the visual ventral stream for object recognition, are sufficient to elicit the fast, automatic categorization response in the brain, and can inform deliberation on the animate-inanimate distinction. However, impoverished stimuli induced weaker categorization response relative to intact objects, and could not be recognized beyond a coarse (forced) animate-inanimate distinction. This contributes to defining visual categorization as an incremental process involving the integration of various features distributed across the visual cortex –or encoded across different layers of a DNN. Based on the present behavioral results and previous research (<xref ref-type="bibr" rid="R45">Spriet et al., 2022</xref>), this incremental process may explain how finer-grained (e.g., basic-level) categories emerge from broader visual categories such as animate and inanimate.</p></sec></body><back><ack id="S39"><title>Acknowledgements</title><p>We thank Sofie Vettori for her help with testing participants. Funding for this project was provided by a European Research Council Starting Grant awarded to L. P. (Project: THEMPO, Grant Agreement 758473). C.S. was supported by a fellowship of the Fondation pour la Recherche Médicale (FDT202304016547).</p><sec id="S40"><label>Funding</label><p>this work was supported by the European Research Council Starting Grant awarded to L. P. (Project: THEMPO, Grant Agreement 758473) and the Fondation pour la Recherche Médicale awarded to C.S. (FDT202304016547).</p></sec></ack><fn-group><fn fn-type="conflict" id="FN2"><p id="P55"><bold>Competing interests</bold></p><p id="P56">Authors declare no competing of interest.</p></fn><fn fn-type="con" id="FN1"><p id="P57"><bold>Authors’ contributions - CRediT:</bold> <italic>Céline Spriet</italic>: Conceptualization, Methodology, Formal Analysis, Investigation, Data Curation, Writing – Original Draft, Writing – Review &amp; Editing, Visualization; <italic>Farzad Rostami</italic>: Formal Analysis, Data Curation, Writing – Original Draft, Writing – Review &amp; Editing, Visualization; <italic>Jean-Rémy Hochmann</italic>: Conceptualization, Methodology, Writing – Review &amp; Editing, Supervision; <italic>Liuba Papeo</italic>: Conceptualization, Methodology, Writing – Review &amp; Editing, Supervision, Funding Acquisition.</p></fn></fn-group><ref-list><title>Bibliography</title><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ayzenberg</surname><given-names>V</given-names></name><name><surname>Behrmann</surname><given-names>M</given-names></name></person-group><article-title>Development of visual object recognition</article-title><source>Nature Reviews Psychology</source><year>2024</year><volume>3</volume><issue>2</issue><fpage>73</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1038/s44159-023-00266-w</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bell</surname><given-names>AH</given-names></name><name><surname>Hadj-Bouziane</surname><given-names>F</given-names></name><name><surname>Frihauf</surname><given-names>JB</given-names></name><name><surname>Tootell</surname><given-names>RBH</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name></person-group><article-title>Object Representations in the Temporal Cortex of Monkeys and Humans as Revealed by Functional Magnetic Resonance Imaging</article-title><source>Journal of Neurophysiology</source><year>2009</year><volume>101</volume><issue>2</issue><fpage>688</fpage><lpage>700</lpage><pub-id pub-id-type="pmcid">PMC2657058</pub-id><pub-id pub-id-type="pmid">19052111</pub-id><pub-id pub-id-type="doi">10.1152/jn.90657.2008</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bracci</surname><given-names>S</given-names></name><name><surname>Op De Beeck</surname><given-names>HP</given-names></name></person-group><article-title>Understanding Human Object Vision : A Picture Is Worth a Thousand Representations</article-title><source>Annual Review of Psychology</source><year>2023</year><volume>74</volume><issue>1</issue><fpage>113</fpage><lpage>135</lpage><pub-id pub-id-type="pmid">36378917</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><article-title>The psychophysics toolbox</article-title><source>Spatial Vision</source><year>1997</year><volume>10</volume><issue>4</issue><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caramazza</surname><given-names>A</given-names></name><name><surname>Shelton</surname><given-names>JR</given-names></name></person-group><article-title>Domain-specific knowledge systems in the brain : The animate-inanimate distinction</article-title><source>Journal of Cognitive Neuroscience</source><year>1998</year><volume>10</volume><issue>1</issue><fpage>1</fpage><lpage>34</lpage><pub-id pub-id-type="pmid">9526080</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlson</surname><given-names>TA</given-names></name><name><surname>Tovar</surname><given-names>DA</given-names></name><name><surname>Alink</surname><given-names>A</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Representational dynamics of object vision : The first 1000 ms</article-title><source>Journal of Vision</source><year>2013</year><volume>13</volume><issue>10</issue><fpage>1</fpage><pub-id pub-id-type="pmid">23908380</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><article-title>Resolving human object recognition in space and time</article-title><source>Nature neuroscience</source><year>2014</year><volume>17</volume><issue>3</issue><fpage>455</fpage><lpage>462</lpage><pub-id pub-id-type="pmcid">PMC4261693</pub-id><pub-id pub-id-type="pmid">24464044</pub-id><pub-id pub-id-type="doi">10.1038/nn.3635</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coggan</surname><given-names>DD</given-names></name><name><surname>Liu</surname><given-names>W</given-names></name><name><surname>Baker</surname><given-names>DH</given-names></name><name><surname>Andrews</surname><given-names>TJ</given-names></name></person-group><article-title>Category-selective patterns of neural response in the ventral visual pathway in the absence of categorical information</article-title><source>NeuroImage</source><year>2016</year><volume>135</volume><fpage>107</fpage><lpage>114</lpage><pub-id pub-id-type="pmid">27132543</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Contini</surname><given-names>EW</given-names></name><name><surname>Wardle</surname><given-names>SG</given-names></name><name><surname>Carlson</surname><given-names>TA</given-names></name></person-group><article-title>Decoding the time-course of object recognition in the human brain : From visual features to categorical decisions</article-title><source>Neuropsychologia</source><year>2017</year><volume>105</volume><fpage>165</fpage><lpage>176</lpage><pub-id pub-id-type="pmid">28215698</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Delorme</surname><given-names>A</given-names></name><name><surname>Makeig</surname><given-names>S</given-names></name></person-group><article-title>EEGLAB : An open source toolbox for analysis of single-trial EEG dynamics including independent component analysis</article-title><source>Journal of Neuroscience Methods</source><year>2004</year><volume>134</volume><issue>1</issue><fpage>9</fpage><lpage>21</lpage><pub-id pub-id-type="pmid">15102499</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Deza</surname><given-names>A</given-names></name><name><surname>Chen</surname><given-names>Y-C</given-names></name><name><surname>Long</surname><given-names>B</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name></person-group><source>Accelerated Texforms : Alternative Methods for Generating Unrecognizable Object Images with Preserved Mid-Level Features</source><conf-name>2019 Conference on Cognitive Computational Neuroscience</conf-name><conf-loc>Berlin, Germany</conf-loc><year>2019</year><comment>2019 Conference on Cognitive Computational Neuroscience</comment><pub-id pub-id-type="doi">10.32470/CCN.2019.1412-0</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freeman</surname><given-names>J</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><article-title>Metamers of the ventral stream</article-title><source>Nature Neuroscience</source><year>2011</year><volume>14</volume><issue>9</issue><fpage>1195</fpage><lpage>1201</lpage><pub-id pub-id-type="pmcid">PMC3164938</pub-id><pub-id pub-id-type="pmid">21841776</pub-id><pub-id pub-id-type="doi">10.1038/nn.2889</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Weiner</surname><given-names>KS</given-names></name></person-group><article-title>The functional architecture of the ventral temporal cortex and its role in categorization</article-title><source>Nature Reviews Neuroscience</source><year>2014</year><volume>15</volume><issue>8</issue><fpage>536</fpage><lpage>548</lpage><pub-id pub-id-type="pmcid">PMC4143420</pub-id><pub-id pub-id-type="pmid">24962370</pub-id><pub-id pub-id-type="doi">10.1038/nrn3747</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grootswagers</surname><given-names>T</given-names></name><name><surname>Robinson</surname><given-names>AK</given-names></name><name><surname>Shatek</surname><given-names>SM</given-names></name><name><surname>Carlson</surname><given-names>TA</given-names></name></person-group><article-title>Untangling featural and conceptual object representations</article-title><source>NeuroImage</source><year>2019</year><volume>202</volume><elocation-id>116083</elocation-id><pub-id pub-id-type="pmid">31400529</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jozwik</surname><given-names>KM</given-names></name><name><surname>Najarro</surname><given-names>E</given-names></name><name><surname>van den Bosch</surname><given-names>JJF</given-names></name><name><surname>Charest</surname><given-names>I</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Disentangling five dimensions of animacy in human brain and behaviour</article-title><source>Communications Biology</source><year>2022</year><volume>5</volume><issue>1</issue><elocation-id>1247</elocation-id><pub-id pub-id-type="pmcid">PMC9663603</pub-id><pub-id pub-id-type="pmid">36376446</pub-id><pub-id pub-id-type="doi">10.1038/s42003-022-04194-y</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khaligh-Razavi</surname><given-names>S-M</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Deep Supervised, but Not Unsupervised, Models May Explain IT Cortical Representation</article-title><source>PLoS Computational Biology</source><year>2014</year><volume>10</volume><issue>11</issue><elocation-id>e1003915</elocation-id><pub-id pub-id-type="pmcid">PMC4222664</pub-id><pub-id pub-id-type="pmid">25375136</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003915</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kheradpisheh</surname><given-names>SR</given-names></name><name><surname>Ghodrati</surname><given-names>M</given-names></name><name><surname>Ganjtabesh</surname><given-names>M</given-names></name><name><surname>Masquelier</surname><given-names>T</given-names></name></person-group><article-title>Deep Networks Can Resemble Human Feed-forward Vision in Invariant Object Recognition</article-title><source>Scientific Reports</source><year>2016</year><volume>6</volume><issue>1</issue><elocation-id>32672</elocation-id><pub-id pub-id-type="pmcid">PMC5013454</pub-id><pub-id pub-id-type="pmid">27601096</pub-id><pub-id pub-id-type="doi">10.1038/srep32672</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kramer</surname><given-names>LE</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name><name><surname>Chen</surname><given-names>Y-C</given-names></name><name><surname>Long</surname><given-names>B</given-names></name><name><surname>Cohen</surname><given-names>MR</given-names></name></person-group><article-title>Contributions of early and mid-level visual cortex to high-level object categorization</article-title><source>bioRxiv</source><year>2023</year><elocation-id>2023.05.31.541514</elocation-id><pub-id pub-id-type="doi">10.1101/2023.05.31.541514</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name></person-group><article-title>Representational similarity analysis – connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><year>2008</year><volume>2</volume><fpage>1</fpage><lpage>28</lpage><pub-id pub-id-type="pmcid">PMC2605405</pub-id><pub-id pub-id-type="pmid">19104670</pub-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Bracci</surname><given-names>S</given-names></name><name><surname>Op De Beeck</surname><given-names>HP</given-names></name></person-group><article-title>Deep Neural Networks as a Computational Model for Human Shape Sensitivity</article-title><source>PLOS Computational Biology</source><year>2016</year><volume>12</volume><issue>4</issue><elocation-id>e1004896</elocation-id><pub-id pub-id-type="pmcid">PMC4849740</pub-id><pub-id pub-id-type="pmid">27124699</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004896</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liberman</surname><given-names>AM</given-names></name><name><surname>Cooper</surname><given-names>FS</given-names></name><name><surname>Shankweiler</surname><given-names>DP</given-names></name><name><surname>Studdert-Kennedy</surname><given-names>M</given-names></name></person-group><article-title>Perception of the speech code</article-title><source>Psychological Review</source><year>1967</year><volume>74</volume><issue>6</issue><fpage>431</fpage><pub-id pub-id-type="pmid">4170865</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu-Shuang</surname><given-names>J</given-names></name><name><surname>Norcia</surname><given-names>AM</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><article-title>An objective index of individual face discrimination in the right occipito-temporal cortex by means of fast periodic oddball stimulation</article-title><source>Neuropsychologia</source><year>2014</year><volume>52</volume><fpage>57</fpage><lpage>72</lpage><pub-id pub-id-type="pmid">24200921</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Long</surname><given-names>B</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name></person-group><article-title>A familiar-size Stroop effect in the absence of basic-level recognition</article-title><source>Cognition</source><year>2017</year><volume>168</volume><fpage>234</fpage><lpage>242</lpage><pub-id pub-id-type="pmid">28732302</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Long</surname><given-names>B</given-names></name><name><surname>Störmer</surname><given-names>VS</given-names></name><name><surname>Alvarez</surname><given-names>GA</given-names></name></person-group><article-title>Mid-level perceptual features contain early cues to animacy</article-title><source>Journal of Vision</source><year>2017</year><volume>17</volume><issue>6</issue><fpage>20</fpage><pub-id pub-id-type="pmid">28654965</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Long</surname><given-names>B</given-names></name><name><surname>Yu</surname><given-names>C-P</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name></person-group><article-title>Mid-level visual features underlie the high-level categorical organization of the ventral stream</article-title><source>Proceedings of the National Academy of Sciences</source><year>2018</year><volume>115</volume><issue>38</issue><fpage>E9015</fpage><lpage>E9024</lpage><pub-id pub-id-type="pmcid">PMC6156638</pub-id><pub-id pub-id-type="pmid">30171168</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1719616115</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname><given-names>A</given-names></name></person-group><article-title>The Representation of Object Concepts in the Brain</article-title><source>Annual Review of Psychology</source><year>2007</year><volume>58</volume><issue>1</issue><fpage>25</fpage><lpage>45</lpage><pub-id pub-id-type="pmid">16968210</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murphy</surname><given-names>GL</given-names></name><name><surname>Brownell</surname><given-names>HH</given-names></name></person-group><article-title>Category Differentiation in Object Recognition : Typicality Constraints on the Basic Category Advantage</article-title><source>Journal of Experimental Psychology: Learning, Memory and Cognition</source><year>1985</year><volume>11</volume><issue>1</issue><fpage>70</fpage><lpage>84</lpage><pub-id pub-id-type="pmid">3156953</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>New</surname><given-names>J</given-names></name><name><surname>Cosmides</surname><given-names>L</given-names></name><name><surname>Tooby</surname><given-names>J</given-names></name></person-group><article-title>Category-specific attention for animals reflects ancestral priorities, not expertise</article-title><source>Proceedings of the National Academy of Sciences</source><year>2007</year><volume>104</volume><issue>42</issue><fpage>16598</fpage><lpage>16603</lpage><pub-id pub-id-type="pmcid">PMC2034212</pub-id><pub-id pub-id-type="pmid">17909181</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0703913104</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norcia</surname><given-names>AM</given-names></name><name><surname>Appelbaum</surname><given-names>LG</given-names></name><name><surname>Ales</surname><given-names>JM</given-names></name><name><surname>Cottereau</surname><given-names>BR</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><article-title>The steady-state visual evoked potential in vision research : A review</article-title><source>Journal of Vision</source><year>2015</year><volume>15</volume><issue>6</issue><fpage>4</fpage><pub-id pub-id-type="pmcid">PMC4581566</pub-id><pub-id pub-id-type="pmid">26024451</pub-id><pub-id pub-id-type="doi">10.1167/15.6.4</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Papeo</surname><given-names>L</given-names></name><name><surname>Wurm</surname><given-names>MF</given-names></name><name><surname>Oosterhof</surname><given-names>NN</given-names></name><name><surname>Caramazza</surname><given-names>A</given-names></name></person-group><article-title>The neural representation of human versus nonhuman bipeds and quadrupeds</article-title><source>Scientific Reports</source><year>2017</year><volume>7</volume><issue>1</issue><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="pmcid">PMC5656636</pub-id><pub-id pub-id-type="pmid">29070901</pub-id><pub-id pub-id-type="doi">10.1038/s41598-017-14424-7</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pauen</surname><given-names>S</given-names></name></person-group><article-title>The global-to-basic level shift in infants’ categorical thinking : First evidence from a longitudinal study</article-title><source>International Journal of Behavioral Development</source><year>2002</year><volume>26</volume><issue>6</issue><fpage>492</fpage><lpage>499</lpage><pub-id pub-id-type="doi">10.1080/01650250143000445</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Proklova</surname><given-names>D</given-names></name><name><surname>Kaiser</surname><given-names>D</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><article-title>MEG sensor patterns reflect perceptual but not categorical similarity of animate and inanimate objects</article-title><source>NeuroImage</source><year>2019</year><volume>193</volume><fpage>167</fpage><lpage>177</lpage><pub-id pub-id-type="pmid">30885785</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quek</surname><given-names>GL</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><article-title>Category-selective human brain processes elicited in fast periodic visual stimulation streams are immune to temporal predictability</article-title><source>Neuropsychologia</source><year>2017</year><volume>104</volume><fpage>182</fpage><lpage>200</lpage><pub-id pub-id-type="pmid">28811258</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rekow</surname><given-names>D</given-names></name><name><surname>Baudouin</surname><given-names>J-Y</given-names></name><name><surname>Durand</surname><given-names>K</given-names></name><name><surname>Leleu</surname><given-names>A</given-names></name></person-group><article-title>Smell what you hardly see : Odors assist visual categorization in the human brain</article-title><source>NeuroImage</source><year>2022</year><elocation-id>119181</elocation-id><pub-id pub-id-type="pmid">35413443</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Retter</surname><given-names>TL</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><article-title>Uncovering the neural magnitude and spatio-temporal dynamics of natural image categorization in a fast visual stream</article-title><source>Neuropsychologia</source><year>2016</year><volume>91</volume><fpage>9</fpage><lpage>28</lpage><pub-id pub-id-type="pmid">27461075</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rezlescu</surname><given-names>C</given-names></name><name><surname>Danaila</surname><given-names>I</given-names></name><name><surname>Miron</surname><given-names>A</given-names></name><name><surname>Amariei</surname><given-names>C</given-names></name></person-group><article-title>More time for science : Using Testable to create and share behavioral experiments faster, recruit better participants, and engage students in hands-on research</article-title><source>Progress in Brain Research</source><publisher-name>Elsevier</publisher-name><year>2020</year><volume>253</volume><fpage>243</fpage><lpage>262</lpage><pub-id pub-id-type="pmid">32771126</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rogers</surname><given-names>TT</given-names></name><name><surname>Patterson</surname><given-names>K</given-names></name></person-group><article-title>Object categorization : Reversals and explanations of the basic-level advantage</article-title><source>Journal of Experimental Psychology: General</source><year>2007</year><volume>136</volume><issue>3</issue><fpage>451</fpage><lpage>469</lpage><pub-id pub-id-type="pmid">17696693</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rosch</surname><given-names>E</given-names></name></person-group><chapter-title>Principles of Categorization</chapter-title><source>Cognition and Categorization</source><publisher-name>Routledge</publisher-name><year>1978</year><fpage>27</fpage><lpage>48</lpage></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenthal</surname><given-names>I</given-names></name><name><surname>Ratnasingam</surname><given-names>S</given-names></name><name><surname>Haile</surname><given-names>T</given-names></name><name><surname>Eastman</surname><given-names>S</given-names></name><name><surname>Fuller-Deets</surname><given-names>J</given-names></name><name><surname>Conway</surname><given-names>BR</given-names></name></person-group><article-title>Color statistics of objects, and color tuning of object cortex in macaque monkey</article-title><source>Journal of Vision</source><year>2018</year><volume>18</volume><issue>11</issue><fpage>1</fpage><pub-id pub-id-type="pmcid">PMC6168048</pub-id><pub-id pub-id-type="pmid">30285103</pub-id><pub-id pub-id-type="doi">10.1167/18.11.1</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><article-title>Understanding individual face discrimination by means of fast periodic visual stimulation</article-title><source>Experimental Brain Research</source><year>2014</year><volume>232</volume><issue>6</issue><fpage>1599</fpage><lpage>1621</lpage><pub-id pub-id-type="pmid">24728131</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rossion</surname><given-names>B</given-names></name><name><surname>Boremanse</surname><given-names>A</given-names></name></person-group><article-title>Robust sensitivity to facial identity in the right human occipito-temporal cortex as revealed by steady-state visual-evoked potentials</article-title><source>Journal of Vision</source><year>2011</year><volume>11</volume><issue>2</issue><fpage>1</fpage><lpage>21</lpage><pub-id pub-id-type="pmid">21346000</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rossion</surname><given-names>B</given-names></name><name><surname>Torfs</surname><given-names>K</given-names></name><name><surname>Jacques</surname><given-names>C</given-names></name><name><surname>Liu-Shuang</surname><given-names>J</given-names></name></person-group><article-title>Fast periodic presentation of natural images reveals a robust face-selective electrophysiological response in the human brain</article-title><source>Journal of Vision</source><year>2015</year><volume>15</volume><issue>1</issue><fpage>18</fpage><pub-id pub-id-type="pmid">25597037</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidt</surname><given-names>F</given-names></name><name><surname>Hegele</surname><given-names>M</given-names></name><name><surname>Fleming</surname><given-names>RW</given-names></name></person-group><article-title>Perceiving animacy from shape</article-title><source>Journal of Vision</source><year>2017</year><volume>17</volume><issue>11</issue><fpage>10</fpage><pub-id pub-id-type="pmid">28973562</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group><source>Very Deep Convolutional Networks for Large-Scale Image Recognition</source><conf-name>Proceedings of the International Conference on Learning Representations (ICLR)</conf-name><year>2015</year></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spriet</surname><given-names>C</given-names></name><name><surname>Abassi</surname><given-names>E</given-names></name><name><surname>Hochmann</surname><given-names>J-R</given-names></name><name><surname>Papeo</surname><given-names>L</given-names></name></person-group><article-title>Visual object categorization in infancy</article-title><source>Proceedings of the National Academy of Sciences</source><year>2022</year><volume>119</volume><issue>8</issue><elocation-id>e2105866119</elocation-id><pub-id pub-id-type="pmcid">PMC8872728</pub-id><pub-id pub-id-type="pmid">35169072</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2105866119</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stothart</surname><given-names>G</given-names></name><name><surname>Quadflieg</surname><given-names>S</given-names></name><name><surname>Milton</surname><given-names>A</given-names></name></person-group><article-title>A fast and implicit measure of semantic categorisation using steady state visual evoked potentials</article-title><source>Neuropsychologia</source><year>2017</year><volume>102</volume><fpage>11</fpage><lpage>18</lpage><pub-id pub-id-type="pmid">28552782</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>C</given-names></name><name><surname>Liu</surname><given-names>Wei</given-names></name><name><surname>Jia</surname><given-names>Yangqing</given-names></name><name><surname>Sermanet</surname><given-names>P</given-names></name><name><surname>Reed</surname><given-names>S</given-names></name><name><surname>Anguelov</surname><given-names>D</given-names></name><name><surname>Erhan</surname><given-names>D</given-names></name><name><surname>Vanhoucke</surname><given-names>V</given-names></name><name><surname>Rabinovich</surname><given-names>A</given-names></name></person-group><source>Going deeper with convolutions</source><conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><year>2015</year><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2015.7298594</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thorat</surname><given-names>S</given-names></name><name><surname>Proklova</surname><given-names>D</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><article-title>The nature of the animacy organization in human ventral temporal cortex</article-title><source>eLife</source><year>2019</year><volume>8</volume><elocation-id>e47142</elocation-id><pub-id pub-id-type="pmcid">PMC6733573</pub-id><pub-id pub-id-type="pmid">31496518</pub-id><pub-id pub-id-type="doi">10.7554/eLife.47142</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thorpe</surname><given-names>S</given-names></name><name><surname>Fize</surname><given-names>D</given-names></name><name><surname>Marlot</surname><given-names>C</given-names></name></person-group><article-title>Speed of processing in the human visual system</article-title><source>Nature</source><year>1996</year><volume>381</volume><issue>6582</issue><fpage>520</fpage><lpage>522</lpage><pub-id pub-id-type="pmid">8632824</pub-id></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tiedemann</surname><given-names>H</given-names></name><name><surname>Schmidt</surname><given-names>F</given-names></name><name><surname>Fleming</surname><given-names>RW</given-names></name></person-group><article-title>Superordinate Categorization Based on the Perceptual Organization of Parts</article-title><source>Brain Sciences</source><year>2022</year><volume>12</volume><issue>5</issue><fpage>667</fpage><pub-id pub-id-type="pmcid">PMC9139997</pub-id><pub-id pub-id-type="pmid">35625053</pub-id><pub-id pub-id-type="doi">10.3390/brainsci12050667</pub-id></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>VanRullen</surname><given-names>R</given-names></name><name><surname>Thorpe</surname><given-names>SJ</given-names></name></person-group><article-title>Is it a Bird? Is it a Plane? Ultra-Rapid Visual Categorisation of Natural and Artifactual Objects</article-title><source>Perception</source><year>2001</year><volume>30</volume><issue>6</issue><fpage>655</fpage><lpage>668</lpage><pub-id pub-id-type="pmid">11464555</pub-id></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>R</given-names></name><name><surname>Janini</surname><given-names>D</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name></person-group><article-title>Mid-level Feature Differences Support Early Animacy and Object Size Distinctions : Evidence from Electroencephalography Decoding</article-title><source>Journal of Cognitive Neuroscience</source><year>2022</year><volume>34</volume><issue>9</issue><fpage>1670</fpage><lpage>1680</lpage><pub-id pub-id-type="pmcid">PMC9438936</pub-id><pub-id pub-id-type="pmid">35704550</pub-id><pub-id pub-id-type="doi">10.1162/jocn_a_01883</pub-id></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warrington</surname><given-names>EK</given-names></name><name><surname>Shallice</surname><given-names>T</given-names></name></person-group><article-title>Category specific semantic impairments</article-title><source>Brain</source><year>1984</year><volume>107</volume><fpage>829</fpage><lpage>854</lpage><pub-id pub-id-type="pmid">6206910</pub-id></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Cadieu</surname><given-names>CF</given-names></name><name><surname>Solomon</surname><given-names>EA</given-names></name><name><surname>Seibert</surname><given-names>D</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title><source>Proceedings of the National Academy of Sciences</source><year>2014</year><volume>111</volume><issue>23</issue><fpage>8619</fpage><lpage>8624</lpage><pub-id pub-id-type="pmcid">PMC4060707</pub-id><pub-id pub-id-type="pmid">24812127</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zachariou</surname><given-names>V</given-names></name><name><surname>Del Giacco</surname><given-names>AC</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name><name><surname>Yue</surname><given-names>X</given-names></name></person-group><article-title>Bottom-up processing of curvilinear visual features is sufficient for animate/inanimate object categorization</article-title><source>Journal of Vision</source><year>2018</year><volume>18</volume><issue>12</issue><fpage>3</fpage><pub-id pub-id-type="pmcid">PMC6222807</pub-id><pub-id pub-id-type="pmid">30458511</pub-id><pub-id pub-id-type="doi">10.1167/18.12.3</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Illustration of stimuli and experimental design in Experiments 1-3.</title><p>(A) Examples of stimuli from the <italic>original stimuli</italic> (Experiment 1) in the inanimate condition (top) and animate condition (bottom) and the different versions of those images: phase-scramble (Experiment 1), grayscale (Experiments 2-3), grayscale phase-scamble (Experiment 2) and texforms (Experiment 3). (B) One-second extract from the “animate” stimulation sequence based on the <italic>original stimuli</italic>, testing for the categorization of animates (<italic>oddball</italic> category) among inanimate objects (<italic>standard</italic> category).</p></caption><graphic xlink:href="EMS206239-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Response at the base and oddball frequencies.</title><p><italic>Left</italic>. Baseline subtracted amplitudes as a function of frequencies in the conditions with original (orange) and phase-scramble stimuli (blue) of Experiment 1 <bold>(A)</bold>, with grayscale (orange) and grayscale phase-scramble stimuli (blue) of Experiment 2 (<bold>B</bold>), and with grayscale stimuli (orange) and texforms (blue) of Experiment 3 (<bold>C</bold>). Triangles signal harmonics showing a significant base response at the group level. Framed is the zoom-in for the oddball response, where circles signal harmonics showing a significant oddball response. The scalp distribution of baseline subtracted amplitudes summed over harmonics show that responses consistently peaked over posterior electrodes. <italic>Right</italic>: Baseline subtracted amplitudes summed over harmonics and averaged over electrodes for each of the two conditions (‘intact’ in orange and ‘impoverished’ in blue), in each experiment. Small dots/triangles denote individual responses, thicker dots/triangles denote group-average responses, lines denote standard errors of the mean.</p></caption><graphic xlink:href="EMS206239-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Comparison between Experiments.</title><p>Mean categorization response (sum of baseline subtracted amplitude averaged over electrodes, and standard error of the mean), for the ‘intact’ and ‘impoverished’ condition in Experiment 1-3. Smaller dots denote individual participants’ means. Horizontal bars and * indicate the difference between experiments for intact and for impoverished conditions.</p></caption><graphic xlink:href="EMS206239-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Correlation coefficients and dissimilarity matrices across image types and layers of VGG-19.</title><p><bold>(A)</bold> Coefficients of the correlation (y-axis) between the representation of a given stimulus set in each layer, and the representation of the original images in the layer fc8 (x-axis). Each plot corresponds to an image type: intact original, intact grayscale, phase-scramble, grayscale phase-scramble, and texform. <bold>(B)</bold> Examples of dissimilarity matrices extracted from different layers for different image type. Each row represents a type of image, each column corresponds to a different layer. Dissimilarity values are from 0 (lowest dissimilarity, dark blue) to 1 (highest dissimilarity, dark red). This figure illustrates the extent to which dissimilarity changed across layers and image type.</p></caption><graphic xlink:href="EMS206239-f004"/></fig><table-wrap id="T1" orientation="portrait" position="float"><label>Table 1</label><caption><title>Categorization-response in Experiments 1-3.</title><p id="P58">Z-scores of the first eight harmonics, excluding the 5<sup>th</sup>, for the category-selective frequency and for each condition and sub-condition of Experiments 1-3.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" align="left" style="border-top: 1px solid #000000">Exp.</th><th valign="top" align="left" style="border-top: 1px solid #000000">Condition</th><th valign="top" align="left" colspan="8" style="border-top: 1px solid"/></tr><tr><th valign="top" align="left" style="border-top: 1px solid"/><th valign="top" align="left" style="border-top: 1px solid"/><th valign="top" align="right" style="border-top: 1px solid #000000">1.2 Hz</th><th valign="top" align="right" style="border-top: 1px solid #000000">2.4 Hz</th><th valign="top" align="right" style="border-top: 1px solid #000000">3.6 Hz</th><th valign="top" align="right" style="border-top: 1px solid #000000">4.8 Hz</th><th valign="top" align="right" style="border-top: 1px solid #000000">7.2 Hz</th><th valign="top" align="right" style="border-top: 1px solid #000000">8.4 Hz</th><th valign="top" align="right" style="border-top: 1px solid #000000">9.6 Hz</th><th valign="top" align="right" style="border-top: 1px solid #000000">10.8 Hz</th></tr></thead><tbody><tr><td valign="top" align="left" style="border-top: 1px solid">1</td><td valign="top" align="left" style="border-top: 1px solid">Original<break/>Phase-scramble</td><td valign="top" align="right" style="border-top: 1px solid"><bold>6.170</bold><break/><bold>1.877</bold></td><td valign="top" align="right" style="border-top: 1px solid"><bold>13.400</bold><break/><bold>6.943</bold></td><td valign="top" align="right" style="border-top: 1px solid"><bold>23.622</bold><break/><bold>6.824</bold></td><td valign="top" align="right" style="border-top: 1px solid"><bold>13.717</bold><break/><bold>4.062</bold></td><td valign="top" align="right" style="border-top: 1px solid"><bold>9.372</bold><break/>0.886</td><td valign="top" align="right" style="border-top: 1px solid"><bold>11.232</bold><break/>0.854</td><td valign="top" align="right" style="border-top: 1px solid"><bold>2.853</bold><break/>-0.146</td><td valign="top" align="right" style="border-top: 1px solid"><bold>3.432</bold><break/>-2.009</td></tr><tr><td valign="top" align="left">2</td><td valign="top" align="left">Grayscale<break/>Grayscale Phase-<break/>scramble</td><td valign="top" align="right"><bold>3.148</bold><break/>-0.231</td><td valign="top" align="right"><bold>6.205</bold><break/>1.538</td><td valign="top" align="right"><bold>7.002</bold><break/><bold>2.884</bold></td><td valign="top" align="right"><bold>7.598</bold><break/>0.881</td><td valign="top" align="right"><bold>12.047</bold><break/>-3.189</td><td valign="top" align="right"><bold>7.254</bold><break/>0.818</td><td valign="top" align="right"><bold>4.189</bold><break/>-0.060</td><td valign="top" align="right"><bold>2.903</bold><break/>-0.284</td></tr><tr><td valign="top" align="left" style="border-bottom: solid thin">3</td><td valign="top" align="left" style="border-bottom:solid 1px #000000">Grayscale<break/>Texform</td><td valign="top" align="right" style="border-bottom:solid 1px #000000"><bold>4.635</bold><break/>0.721</td><td valign="top" align="right" style="border-bottom:solid 1px #000000"><bold>24.176</bold><break/><bold>2.454</bold></td><td valign="top" align="right" style="border-bottom:solid 1px #000000"><bold>17.457</bold><break/>0.939</td><td valign="top" align="right" style="border-bottom:solid 1px #000000"><bold>16.135</bold><break/>0.960</td><td valign="top" align="right" style="border-bottom:solid 1px #000000"><bold>11.941</bold><break/><bold>3.536</bold></td><td valign="top" align="right" style="border-bottom:solid 1px #000000"><bold>6.316</bold><break/>1.233</td><td valign="top" align="right" style="border-bottom:solid 1px #000000"><bold>3.702</bold><break/>-1.050</td><td valign="top" align="right" style="border-bottom:solid 1px #000000"><bold>2.915</bold><break/>0.654</td></tr></tbody></table><table-wrap-foot><fn id="TFN1"><p id="P59"><italic>Note: Exp</italic>., <italic>experiment; Highlighted in bold are significant effects (z-scores &gt; 1.64)</italic>.</p></fn></table-wrap-foot></table-wrap><table-wrap id="T2" orientation="portrait" position="float"><label>Table 2</label><caption><title>Correlations between the <italic>reference-matrix</italic> extracted for the intact colorful set, from the last layer Fc8 (highlighted with the frame), and the matrix extracted for each stimulus set, from each layer of the VGG19.</title></caption><table frame="void" rules="none"><thead><tr><th valign="top" align="left" style="border-top: solid thin; border-bottom: solid thin"/><th valign="top" align="right" colspan="2" style="border-top: solid thin; border-bottom: solid thin">Original intact</th><th valign="top" align="right" colspan="2" style="border-top: solid thin; border-bottom: solid thin">Grayscale intact</th><th valign="top" align="right" colspan="2" style="border-top: solid thin; border-bottom: solid thin">Phase-scramble</th><th valign="top" align="right" colspan="2" style="border-top: solid thin; border-bottom: solid thin">Grayscale<break/>phase- scramble</th><th valign="top" align="right" colspan="2" style="border-top: solid thin; border-bottom: solid thin">Texform</th></tr></thead><tbody><tr><td valign="top" align="left">Layers</td><td valign="top" align="right"><italic>ρ</italic></td><td valign="top" align="center"><italic>P</italic></td><td valign="top" align="right"><italic>ρ</italic></td><td valign="top" align="right"><italic>P</italic></td><td valign="top" align="right"><italic>ρ</italic></td><td valign="top" align="right"><italic>P</italic></td><td valign="top" align="right"><italic>ρ</italic></td><td valign="top" align="right"><italic>P</italic></td><td valign="top" align="right"><italic>ρ</italic></td><td valign="top" align="right"><italic>P</italic></td></tr><tr><td valign="top" align="left">conv1_1</td><td valign="top" align="right" style="background: #54e265">.225</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #52ace4">.136</td><td valign="top" align="right"><bold>&lt;.0001</bold></td><td valign="top" align="right" style="background: #54e265">.277</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #52ace4">.190</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #52ace4">.185</td><td valign="top" align="right"><bold>&lt;.001</bold></td></tr><tr><td valign="top" align="left">conv1_2</td><td valign="top" align="right" style="background: #54e265">.216</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #52ace4">.131</td><td valign="top" align="right"><bold>&lt;.0001</bold></td><td valign="top" align="right" style="background: #54e265">.254</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #52ace4">.183</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #52ace4">.181</td><td valign="top" align="right"><bold>&lt;.001</bold></td></tr><tr><td valign="top" align="left">conv2_1</td><td valign="top" align="right" style="background: #54e265">.331</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #52ace4">.183</td><td valign="top" align="right"><bold>&lt;.0001</bold></td><td valign="top" align="right" style="background: #54e265">.345</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #54e265">.238</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #54e265">.218</td><td valign="top" align="right"><bold>&lt;.001</bold></td></tr><tr><td valign="top" align="left">conv2_2</td><td valign="top" align="right" style="background: #54e265">.221</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #54e265">.242</td><td valign="top" align="right"><bold>&lt;.0001</bold></td><td valign="top" align="right" style="background: #54e265">.237</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #54e265">.270</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #54e265">.265</td><td valign="top" align="right"><bold>&lt;.001</bold></td></tr><tr><td valign="top" align="left">conv3_1</td><td valign="top" align="right" style="background: #54e265">.218</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #54e265">.220</td><td valign="top" align="right"><bold>&lt;.0001</bold></td><td valign="top" align="right" style="background: #52ace4">.171</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #52ace4">.136</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #54e265">.241</td><td valign="top" align="right"><bold>&lt;.001</bold></td></tr><tr><td valign="top" align="left">conv3_2</td><td valign="top" align="right" style="background: #54e265">.273</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #54e265">.243</td><td valign="top" align="right"><bold>&lt;.0001</bold></td><td valign="top" align="right" style="background: #52ace4">.199</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #52ace4">.126</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #54e265">.262</td><td valign="top" align="right"><bold>&lt;.001</bold></td></tr><tr><td valign="top" align="left">conv3_3</td><td valign="top" align="right" style="background: #54e265">.273</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #54e265">.244</td><td valign="top" align="right"><bold>&lt;.0001</bold></td><td valign="top" align="right" style="background: #52ace4">.166</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #52ace4">.107</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #54e265">.255</td><td valign="top" align="right"><bold>&lt;.001</bold></td></tr><tr><td valign="top" align="left">conv3_4</td><td valign="top" align="right" style="background: #54e265">.339</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #54e265">.257</td><td valign="top" align="right"><bold>&lt;.0001</bold></td><td valign="top" align="right" style="background: #54e265">.201</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #52ace4">.109</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #54e265">.252</td><td valign="top" align="right"><bold>&lt;.001</bold></td></tr><tr><td valign="top" align="left">conv4_1</td><td valign="top" align="right" style="background: #54e265">.260</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #54e265">.224</td><td valign="top" align="right"><bold>&lt;.0001</bold></td><td valign="top" align="right" style="background: #52ace4">.179</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #52ace4">.088</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #54e265">.229</td><td valign="top" align="right"><bold>&lt;.001</bold></td></tr><tr><td valign="top" align="left">conv4_2</td><td valign="top" align="right" style="background: #54e265">.263</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #54e265">.237</td><td valign="top" align="right"><bold>&lt;.0001</bold></td><td valign="top" align="right" style="background: #52ace4">.162</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #52ace4">.107</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #54e265">.239</td><td valign="top" align="right"><bold>&lt;.001</bold></td></tr><tr><td valign="top" align="left">conv4_3</td><td valign="top" align="right" style="background: #54e265">.248</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #54e265">.232</td><td valign="top" align="right"><bold>&lt;.0001</bold></td><td valign="top" align="right" style="background: #54e265">.200</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #52ace4">.143</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #54e265">.248</td><td valign="top" align="right"><bold>&lt;.001</bold></td></tr><tr><td valign="top" align="left">conv4_4</td><td valign="top" align="right" style="background: #54e265">.223</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #54e265">.201</td><td valign="top" align="right"><bold>&lt;.0001</bold></td><td valign="top" align="right" style="background: #54e265">.243</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #54e265">.216</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #54e265">.236</td><td valign="top" align="right"><bold>&lt;.001</bold></td></tr><tr><td valign="top" align="left">conv5_1</td><td valign="top" align="right" style="background: #54e265">.274</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #54e265">.227</td><td valign="top" align="right"><bold>&lt;.0001</bold></td><td valign="top" align="right" style="background: #54e265">.243</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #52ace4">.189</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #54e265">.224</td><td valign="top" align="right"><bold>&lt;.001</bold></td></tr><tr><td valign="top" align="left">conv5_2</td><td valign="top" align="right" style="background: #54e265">.357</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #54e265">.283</td><td valign="top" align="right"><bold>&lt;.0001</bold></td><td valign="top" align="right" style="background: #54e265">.229</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #52ace4">.176</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #54e265">.227</td><td valign="top" align="right"><bold>&lt;.001</bold></td></tr><tr><td valign="top" align="left">conv5_3</td><td valign="top" align="right" style="background: #54e265">.377</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #54e265">.327</td><td valign="top" align="right"><bold>&lt;.0001</bold></td><td valign="top" align="right" style="background: #54e265">.218</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #52ace4">.168</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #54e265">.253</td><td valign="top" align="right"><bold>&lt;.001</bold></td></tr><tr><td valign="top" align="left">conv5_4</td><td valign="top" align="right" style="background: #54e265">.348</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #54e265">.331</td><td valign="top" align="right"><bold>&lt;.0001</bold></td><td valign="top" align="right" style="background: #52ace4">.189</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #52ace4">.171</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #54e265">.254</td><td valign="top" align="right"><bold>&lt;.001</bold></td></tr><tr><td valign="top" align="left">fc6</td><td valign="top" align="right" style="background:#ED9349">.745</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background:#ECEC4A">.582</td><td valign="top" align="right"><bold>&lt;.0001</bold></td><td valign="top" align="right" style="background:#52ACE4">.187</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background:#52ACE4">.164</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background:#54E265">.248</td><td valign="top" align="right"><bold>&lt;.001</bold></td></tr><tr><td valign="top" align="left" style="border-bottom: solid thin">fc7</td><td valign="top" align="right" style="border-bottom: solid thin;background: #f04646">.818</td><td valign="top" align="right" style="border-bottom: solid thin"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #ed9349">.636</td><td valign="top" align="right"><bold>&lt;.0001</bold></td><td valign="top" align="right" style="background: #52ace4">.190</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #52ace4">.166</td><td valign="top" align="right"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #54e265">.261</td><td valign="top" align="right"><bold>&lt;.001</bold></td></tr><tr><td valign="top" align="left" style="border-bottom: solid thin; border-left: solid thin">fc8</td><td valign="top" align="right" style="border-bottom: solid thin; background: #f04646">1.000</td><td valign="top" align="right" style="border-bottom: solid thin; border-right: solid thin"><bold>&lt;.001</bold></td><td valign="top" align="right" style="border-bottom: hidden; background: #f04646">.817</td><td valign="top" align="right" style="border-bottom: hidden"><bold>&lt;.0001</bold></td><td valign="top" align="right" style="background: #52ace4; border-bottom: hidden">.145</td><td valign="top" align="right" style="border-bottom: hidden"><bold>&lt;.001</bold></td><td valign="top" align="right" style="border-bottom: hidden; background: #52ace4">.131</td><td valign="top" align="right" style="border-bottom: hidden"><bold>&lt;.001</bold></td><td valign="top" align="right" style="background: #54e265; border-bottom: hidden">.265</td><td valign="top" align="right" style="border-bottom: hidden"><bold>&lt;.001</bold></td></tr></tbody></table><table-wrap-foot><fn id="TFN2"><p id="P60"><italic>Note: Colors denote the strength of the correlation from very strong (red: ρ = 0.8 to 1), to increasingly weaker (orange: ρ = 0.6 to 0.79; yellow, ρ = 0.4 to 0.59; green, ρ = 0.2 to 0.39; blue, ρ = 0 to 0.19)</italic>.</p></fn></table-wrap-foot></table-wrap></floats-group></article>