<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS205596</article-id><article-id pub-id-type="doi">10.1101/2025.05.13.653501</article-id><article-id pub-id-type="archive">PPR1021527</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Layer-specific spatiotemporal dynamics of feedforward and feedback in human visual object perception</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Carricarte</surname><given-names>Tony</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Xie</surname><given-names>Siying</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Singer</surname><given-names>Johannes</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Trampel</surname><given-names>Robert</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Huber</surname><given-names>Laurentius</given-names></name></contrib><contrib contrib-type="author"><name><surname>Weiskopf</surname><given-names>Nikolaus</given-names></name><xref ref-type="aff" rid="A4">4</xref><xref ref-type="aff" rid="A5">6</xref><xref ref-type="aff" rid="A6">7</xref></contrib><contrib contrib-type="author"><name><surname>Cichy</surname><given-names>Radoslaw M.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A7">8</xref></contrib></contrib-group><aff id="A1"><label>1</label>Department of Education and Psychology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/046ak2485</institution-id><institution>Freie Universität Berlin</institution></institution-wrap>, <postal-code>14195</postal-code><city>Berlin</city>, <country country="DE">Germany</country></aff><aff id="A2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05s5xvk70</institution-id><institution>Einstein Center for Neurosciences Berlin</institution></institution-wrap>, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/001w7jn25</institution-id><institution>Charité - Universitätsmedizin Berlin</institution></institution-wrap>, <postal-code>10117</postal-code><city>Berlin</city>, <country country="DE">Germany</country></aff><aff id="A3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05ewdps05</institution-id><institution>Bernstein Center for Computational Neuroscience Berlin</institution></institution-wrap>, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01hcx6992</institution-id><institution>Humboldt-Universität zu Berlin</institution></institution-wrap>, <postal-code>10117</postal-code><city>Berlin</city>, <country country="DE">Germany</country></aff><aff id="A4"><label>4</label>Department of Neurophysics, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/0387jng26</institution-id><institution>Max Planck Institute for Human Cognitive and Brain Sciences</institution></institution-wrap>, <postal-code>04103</postal-code><city>Leipzig</city>, <country country="DE">Germany</country></aff><aff id="A5"><label>6</label>Felix Bloch Institute for Solid State Physics, Faculty of Physics and Earth Sciences, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/03s7gtk40</institution-id><institution>Universität Leipzig</institution></institution-wrap>, <postal-code>04103</postal-code><city>Leipzig</city>, <country country="DE">Germany</country></aff><aff id="A6"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02704qw51</institution-id><institution>Wellcome Centre for Human Neuroimaging</institution></institution-wrap>, UCL Queen Square Institute of Neurology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap>, <postal-code>WC1N 3AR</postal-code><city>London</city>, <country country="GB">United Kingdom</country></aff><aff id="A7"><label>8</label>Berlin School of Mind and Brain, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01hcx6992</institution-id><institution>Humboldt-Universität zu Berlin</institution></institution-wrap>, <postal-code>10117</postal-code><city>Berlin</city>, <country country="DE">Germany</country></aff><author-notes><corresp id="CR1">
<label>*</label>Correspondence: <email>tcarricarte@gmail.com</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>18</day><month>05</month><year>2025</year></pub-date><pub-date pub-type="preprint"><day>16</day><month>05</month><year>2025</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Visual object perception is mediated by information flow between regions of the ventral visual stream along feedforward and feedback anatomical connections. However, feedforward and feedback signals during naturalistic vision are rapid and overlapping, complicating their identification and precise functional specification. Here we recorded human layer-specific fMRI responses to naturalistic object images in early visual cortex (EVC) and lateral occipital complex (LOC) to isolate feedforward and feedback information signals spatially by their cortical layer specific termination pattern. We combined these layer-specific fMRI responses with electroencephalography (EEG) responses for the same images to segregate feedforward and feedback signals in both time and space. Feedforward signals emerge early in the middle layers of EVC and LOC, followed by feedback signals in the superficial layer of both regions, and the deep layer of EVC. Comparing the identified dynamics in LOC to a visual deep neural network (DNN), revealed that early feedforward signals in LOC encode medium complexity features, whereas later feedback signals increase the representational format to high complexity features. Together this specifies the spatiotemporal dynamics and functional role of feedforward and feedback information flow mediating visual object perception.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Human object vision relies on anatomical bidirectional connections along the ventral visual stream<sup><xref ref-type="bibr" rid="R1">1</xref>,<xref ref-type="bibr" rid="R2">2</xref></sup>, spanning the visual hierarchy from early visual cortex (EVC) to the lateral occipital complex (LOC)<sup><xref ref-type="bibr" rid="R3">3</xref></sup>. These connections mediate visual computations via feedforward and feedback information flows, with complex overlapping spatiotemporal dynamics<sup><xref ref-type="bibr" rid="R4">4</xref>–<xref ref-type="bibr" rid="R6">6</xref></sup>. While the feedforward flow carries sensory information up the visual processing hierarchy<sup><xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R8">8</xref></sup>, downstream feedback concurrently carries information down the hierarchy, refining and shaping feedforward neural dynamics<sup><xref ref-type="bibr" rid="R9">9</xref></sup>. Identifying the distinct contributions of these information flows to visual computation in space and time is therefore crucial for understanding the mechanism of human object vision.</p><p id="P3">Previous efforts to disentangle the specific signatures of feedforward from feedback have typically used one of two main approaches. The first involves invasive manipulations on the anatomically-<sup><xref ref-type="bibr" rid="R8">8</xref></sup> and functionally-defined<sup><xref ref-type="bibr" rid="R6">6</xref>,<xref ref-type="bibr" rid="R10">10</xref>,<xref ref-type="bibr" rid="R11">11</xref></sup> neural circuits in non-human primates, selectively targeting bottom-up vs top-down processes. While these interventions offer precise circuit-level control, their invasive nature makes them largely impractical for human research. The second approach comprises contrasting experimental conditions where the feedforward and feedback contributions vary, such as perception vs mental imagery<sup><xref ref-type="bibr" rid="R12">12</xref></sup>, attention vs non-attention<sup><xref ref-type="bibr" rid="R13">13</xref></sup>, and early vs late backward masking<sup><xref ref-type="bibr" rid="R5">5</xref></sup>. However, this method does not directly assess information flow during naturalistic vision and has conceptual limitations, as it relies on indirect comparisons of experimental conditions that may be confounded by incompletely controlled differences between them<sup><xref ref-type="bibr" rid="R14">14</xref>–<xref ref-type="bibr" rid="R16">16</xref></sup>.</p><p id="P4">Instead, here we capitalized on the layer-specific anatomical connectivity found in the primate visual cortex<sup><xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R8">8</xref>,<xref ref-type="bibr" rid="R17">17</xref></sup>, to dissect feedforward from feedback signals: while feedforward connections terminate primarily in the middle layer<sup><xref ref-type="bibr" rid="R18">18</xref></sup>, feedback connections target superficial and deep layers<sup><xref ref-type="bibr" rid="R19">19</xref>,<xref ref-type="bibr" rid="R20">20</xref></sup>.</p><p id="P5">Based on this three-compartment model of cortical depth, we characterized the layer-specific spatiotemporal dynamics of feedforward and feedback signals in object perception. Using sub-millimeter resolution fMRI at 7T, we recorded layer-specific brain activity in human EVC and LOC while participants viewed naturalistic object images. We then combined these layer-specific fMRI responses with time-resolved EEG responses for the same images within the framework of representational similarity analysis (RSA)<sup><xref ref-type="bibr" rid="R21">21</xref>–<xref ref-type="bibr" rid="R23">23</xref></sup> to resolve feedforward and feedback processing in millisecond resolution. Based on this, we then characterized the representational format in terms of visual feature complexity of feedforward and feedback processing using artificial deep neural networks.</p></sec><sec id="S2" sec-type="results"><title>Results</title><p id="P6">Using sub-millimeter resolution 7T MRI, we recorded gradient-echo blood oxygenation level-dependent (GE-BOLD) signals in human EVC and LOC in response to 24 different naturalistic object images (<xref ref-type="fig" rid="F1">Fig. 1A</xref>). During the acquisition participants viewed naturalistic object images on real-world backgrounds in random order (<xref ref-type="fig" rid="F1">Fig 1B</xref>). We estimated the neural response to each condition, i.e., object image, by fitting a general linear model.</p><p id="P7">Robust object information in both EVC and LOC is a precondition for further dissecting feedback and feedforward-related aspects. To ascertain this, we extracted voxel values from EVC and LOC separately to form condition-specific pattern vectors (<xref ref-type="fig" rid="F1">Fig. 1C</xref>). Based on these pattern vectors we trained and tested a support vector machine (SVM) to perform pairwise-object classification of objects. We found robust decoding accuracy in both EVC (71,16 %, <italic>P</italic> = 0.0039, one-sample permutation test) and LOC (60,69 %, <italic>P</italic> = 0.0092), ascertaining reliable object information in those regions (<xref ref-type="fig" rid="F1">Fig. 1D</xref>).</p><p id="P8">We then proceeded to identify and examine the spatiotemporal neural dynamics of visual representations in two steps that build upon each other. First, we assessed the macroscale of cortical regions (i.e., EVC and LOC) to establish and thus validate representational EEG-fMRI fusion at 7T, and to characterize the representational format of the identified dynamics. Based on this validation, we dissect feedforward from feedback neural processing at the finer mesoscale level of cortical layers.</p><sec id="S3"><title>Spatiotemporal neural dynamics of object representations in EVC and LOC at the macroscale of brain regions</title><p id="P9">To establish the time course of object processing in EVC and LOC at the macroscale, we employed representational EEG-fMRI fusion<sup><xref ref-type="bibr" rid="R22">22</xref>–<xref ref-type="bibr" rid="R24">24</xref></sup>, which integrates time-resolved EEG with spatially resolved fMRI measurements for a combined spatiotemporally resolved view. For MRI, we used the dataset collected in this study, complemented with EEG responses from an existing dataset to the same set of 24 images<sup><xref ref-type="bibr" rid="R5">5</xref></sup> as in the MRI experiment. We assessed the representational geometry of EVC and LOC with region-specific representational dissimilarity matrices (RDMs; <xref ref-type="fig" rid="F2">Fig. 2A</xref>), and the representational geometry of EEG signals with RDMs in a time-resolved way from -200 to 600 ms with respect to image onset (<xref ref-type="fig" rid="F2">Fig. 2B</xref>). We related EVC and LOC RDMs to the time-resolved EEG-RDMs, yielding a time course of representational similarity for EVC and LOC each, for which we report peak latency and peak latency differences with 95% confidence intervals in parenthesis as assessed by bootstrapping (1,000 iterations).</p><p id="P10">As expected from the well-established hierarchical organization of the visual system<sup><xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R8">8</xref>,<xref ref-type="bibr" rid="R25">25</xref></sup>, object processing in EVC preceded that in LOC (<xref ref-type="fig" rid="F2">Fig. 2C</xref>): The EVC time course peaked at 120 ms (100 – 120 ms), whereas the LOC time course peaked later at 180 ms (170 – 370 ms), with a significant difference between peak times of 60 ms (50 – 250 ms; <italic>P</italic> &lt; 0.001). Furthermore, in direct comparison by subtraction of the correlation curves of LOC from EVC, early representations correlated stronger with EVC than with LOC at 90 ms (90 – 120 ms; <xref ref-type="fig" rid="F2">Fig. 2D</xref>), whereas late representations correlated stronger with LOC than with EVC at 300 ms (180 – 510 ms). A supplementary EEG-fMRI fusion analysis accounting for similarities in representational geometry between LOC and EVC by partial correlations confirmed these observations (<xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. 1A and B</xref>). Our results extend EEG-fMRI fusion<sup><xref ref-type="bibr" rid="R5">5</xref>,<xref ref-type="bibr" rid="R22">22</xref>,<xref ref-type="bibr" rid="R24">24</xref>,<xref ref-type="bibr" rid="R26">26</xref></sup> from 3T to 7T fMRI and provide additional support to the procedure, thus warranting a spatially finer investigation of the spatiotemporal neural dynamics at the mesoscale level of cortical layers.</p></sec><sec id="S4"><title>The format of object representations in EVC and LOC at the macroscale of brain regions</title><p id="P11">The visual cortex represents objects in formats of increasing feature complexity from low to high along the ventral visual stream<sup><xref ref-type="bibr" rid="R27">27</xref>–<xref ref-type="bibr" rid="R30">30</xref></sup>. To confirm this progression here, we related the neural dynamics of EVC and LOC to the AlexNet deep neural network (DNN) trained on object categorization<sup><xref ref-type="bibr" rid="R31">31</xref></sup>. The underlying rationale is that the feature complexity of visual representations progressively increases from lower to higher layers across the network’s layers, so that assessing the fit of the model layers to brain data reveals feature complexity of the neural representations<sup><xref ref-type="bibr" rid="R30">30</xref>,<xref ref-type="bibr" rid="R32">32</xref></sup>. We conducted commonality analysis, linking each of the 7 layer-specific DNN-RDMs to the ROI-specific fMRI-RDMs from EVC and LOC, and to the mean EEG-RDM at corresponding time intervals, defined by the significant time points of the raw curves in <xref ref-type="fig" rid="F2">Fig. 2C</xref>. We report peak layers with 95% confidence intervals in parenthesis (1,000 bootstraps).</p><p id="P12">We found commonality with predominantly low model layers, with a peak at model layer 2 (2 – 2) in EVC. and with predominantly middle to high model layers with a peak at model layer 5 (4 – 5) in LOC (<xref ref-type="fig" rid="F2">Fig. 2F</xref>). This indicates representations of primarily low complexity in EVC and mid- to high-complexity in LOC. This pattern of results remained consistent when using an alternative experimental choice based on significant time points from the difference between EVC and LOC curves in <xref ref-type="fig" rid="F2">Fig. 2D</xref> (<xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. 2</xref>). Our results thus confirm a transition from representations of low complexity processed early in EVC into representations of higher complexity processed later in LOC<sup><xref ref-type="bibr" rid="R28">28</xref></sup>, further providing additional support to the analytic approach at 7T and warranting a finer investigation of representational format at the mesoscale level.</p></sec><sec id="S5"><title>Spatiotemporal neural dynamics of object representations in EVC and LOC at the mesoscale of cortical layers</title><p id="P13">To investigate the spatiotemporal neural dynamics at the mesoscale level, we applied the research procedure validated above to the finer level of cortical layers (<xref ref-type="fig" rid="F3">Fig. 3A</xref>). To this end, we segmented the cortical ribbon into three equidistant layers<sup><xref ref-type="bibr" rid="R33">33</xref></sup>: deep, middle and superficial. We then applied representational EEG-fMRI fusion as established above, but now at the level of layers, to yield time-resolved and layer-specific visual object processing time courses in EVC and LOC. To control for non-specific macrovascular responses<sup><xref ref-type="bibr" rid="R34">34</xref></sup> that affect layer specificity<sup><xref ref-type="bibr" rid="R35">35</xref></sup>, we conducted EEG-fMRI fusion analysis partialing out for each layer the effect of the layers beneath.</p><p id="P14">In EVC we observed a correlation pattern suggesting two processing stages (<xref ref-type="fig" rid="F3">Fig. 3B, C</xref>) indexed by different profiles across layers and in timing. The first stage is marked by a peak in the middle layer at 100 ms (90 – 120 ms; <xref ref-type="fig" rid="F3">Fig. 3B</xref>). The second stage is characterized by peaks at 140 ms in both the deep (120 –180 ms) and superficial (120 – 190 ms) layers, with significant latency differences of 40 ms (10 – 50 ms; <italic>P</italic> = 0.004) between the middle layer and the deep layer and 40 ms (20 – 80 ms; <italic>P</italic> = 0.002) between the middle layer and superficial layer, but not between the deep layer and the superficial layer 0 ms (-20 – 40 ms; <italic>P</italic> = 0.814). This pattern was further substantiated by subtracting the layer-specific time courses which showed significant effects (<xref ref-type="fig" rid="F3">Fig. 3C</xref>). We observed stronger correlations of late representations at 140 ms with the deep (140–150 ms) and the superficial (140–140 ms) layers than with the middle layer. This suggests initial feedforward processing in the middle layer and a later emerging feedback processing with a distinctive layer profile in the deep and superficial layers in EVC.</p><p id="P15">In LOC, the results pattern also indicated two stages with a distinctive layer and temporal profile (<xref ref-type="fig" rid="F3">Fig. 3D, E</xref>). We observe earlier processing in the middle layer, followed by processing in the superficial layer later (<xref ref-type="fig" rid="F3">Fig. 3D</xref>). In detail, time courses of object processing peaked first in the middle layer at 160 ms (160 – 170 ms), and later in the superficial layer at 400 ms (270 – 450 ms), with a significant difference in peak latency of 240 ms (110 – 290 ms; <italic>P</italic> &lt; 0.012). A direct comparison of time courses by subtraction (<xref ref-type="fig" rid="F3">Fig. 3E</xref>) substantiated the observation in that representations correlated stronger with the middle layer early than the superficial layer at 160 ms (160 – 180 ms), and late representations correlated stronger with the superficial layer than the middle layer later, at 400 ms (90 – 410 ms). This suggests an initial feedforward processing in the middle layer and a later emerging feedback processing with a distinctive layer profile in the superficial layer in LOC.</p><p id="P16">An analogous results pattern in EVC (<xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. 3A, B</xref>) and LOC (<xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. 3C, D</xref>) was observed when assessing layers directly without partialing out the effect of deeper layers, confirming the robustness of the results to particular analysis choices.</p><p id="P17">Together, this resolves the spatiotemporal dynamics of feedforward and feedback information flow during visual object processing across EVC and LOC through cortical layer-specific and temporally distinct response profiles.</p></sec><sec id="S6"><title>The format of object representations in EVC and LOC at the mesoscale of cortical layers</title><p id="P18">One interpretation of the observed layer-specific spatiotemporal dynamics, being guided by the functional pattern of cortical layer connectivity<sup><xref ref-type="bibr" rid="R36">36</xref>,<xref ref-type="bibr" rid="R37">37</xref></sup>, is that they indicate interareal communication via feedforward and feedback connections<sup><xref ref-type="bibr" rid="R38">38</xref>,<xref ref-type="bibr" rid="R39">39</xref></sup>. However, an alternative explanation is that they arise intra-areal communication via lateral connections<sup><xref ref-type="bibr" rid="R40">40</xref></sup> or from superficial bias in the fMRI measurements<sup><xref ref-type="bibr" rid="R41">41</xref></sup>.</p><p id="P19">To disambiguate between these options, we characterized the format of representations by assessing feature complexity as present in DNN layers, from low to high, applying the research approach as used at the macroscale but now at the level of cortical layers. If the observed layer-specific spatiotemporal dynamics reflect an interplay of feedforward and feedback information flow, we would expect the flow to carry information of varying complexity<sup><xref ref-type="bibr" rid="R42">42</xref>,<xref ref-type="bibr" rid="R43">43</xref></sup> across different levels of the visual hierarchy<sup><xref ref-type="bibr" rid="R44">44</xref>,<xref ref-type="bibr" rid="R45">45</xref></sup>, resulting in distinct representational formats across cortical layers. In contrast, if the observed layer-specific spatiotemporal dynamics reflect lateral connections modulating the neural gain<sup><xref ref-type="bibr" rid="R46">46</xref>–<xref ref-type="bibr" rid="R50">50</xref></sup> or a superficial bias, we would expect uniform feature complexity across layers<sup><xref ref-type="bibr" rid="R51">51</xref></sup>.</p><p id="P20">We conducted commonality analysis, linking each model layer-specific DNN-RDM to the deep, middle and superficial layer-specific fMRI-RDM and mean EEG-RDM at corresponding time intervals, defined by the significant time points of the raw curves in <xref ref-type="fig" rid="F3">Fig. 3B and D</xref> indicating periods of layer-specific neural dynamics (<xref ref-type="fig" rid="F4">Fig. 4A</xref>).</p><p id="P21">In EVC (<xref ref-type="fig" rid="F4">Fig. 4B</xref>) we observed a uniform results pattern across cortical layers: object representations shared variance with all DNN layers, but most strongly with model layer 2 (2 – 3), indicating mainly representations of low-to-mid level complexity. This suggests that the observed layer-specific spatiotemporal dynamics in EVC reflect lateral connections modulating the neural gain<sup><xref ref-type="bibr" rid="R46">46</xref>–<xref ref-type="bibr" rid="R50">50</xref></sup> or superficial bias<sup><xref ref-type="bibr" rid="R41">41</xref></sup> rather than feedback.</p><p id="P22">In contrast, in LOC (<xref ref-type="fig" rid="F4">Fig. 4C</xref>) we observed a shift from mid-to high feature complexity across cortical layers. The early emerging representations in the middle cortical layer (<xref ref-type="fig" rid="F3">Fig. 3D</xref>) were of mid-complexity, as indicated by a peak at model layer 4 (4 – 6). In contrast, the late emerging representations in the superficial layers were of high complexity, with a peak at model layer 7 (4 – 7). This results pattern was robust: making alternative experimental choices based on the average of the significant time intervals from the difference between layer curves in <xref ref-type="fig" rid="F3">Fig. 3C and E</xref> (<xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. 4 and 5</xref>) and individual time points spanning the full-time course (<xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. 6</xref>) yielded equivalent results. Together, these findings suggest a dynamic shift in representational format in LOC, transitioning from early representations of mid-level feature complexity in the feedforward flow to later representations of high feature complexity through interareal feedback.</p></sec></sec><sec id="S7" sec-type="discussion"><title>Discussion</title><p id="P23">We leveraged layer-resolved fMRI, time-resolved EEG data and DNNs to identify and characterize the spatiotemporal neural dynamics of feedforward and feedback information flow underlying object perception. We validated our methods using 7T fMRI at the macroscale of cortical regions by replicating the temporal dynamics and representational format in EVC and LOC observed in 3T fMRI studies<sup><xref ref-type="bibr" rid="R5">5</xref>,<xref ref-type="bibr" rid="R22">22</xref>,<xref ref-type="bibr" rid="R26">26</xref>,<xref ref-type="bibr" rid="R30">30</xref></sup>, allowing us to proceed to the mesoscale of cortical layers. There we made two key observations. First, we observed distinct layer-specific temporal profiles for EVC and LOC. Visual representations in the middle layers emerged earlier than in the deep and superficial layers of EVC and the superficial layers of LOC. Second, the identified layer-specific dynamics in LOC had distinctive visual feature complexity profiles: the early emerging middle layer representations were of mid-complexity and the later emerging superficial layer representations were of high complexity.</p><sec id="S8"><title>Layer-specific EEG-fMRI fusion reveals sequential feedforward and feedback processing in visual object perception</title><p id="P24">Visual object perception unfolds through intricate spatiotemporal neural dynamics<sup><xref ref-type="bibr" rid="R4">4</xref>–<xref ref-type="bibr" rid="R6">6</xref>,<xref ref-type="bibr" rid="R52">52</xref>,<xref ref-type="bibr" rid="R53">53</xref></sup>, mediating feedforward and feedback information flow<sup><xref ref-type="bibr" rid="R22">22</xref>,<xref ref-type="bibr" rid="R54">54</xref></sup> rapidly and through temporally overlapping responses. Here, we disentangle the temporal dynamics of feedforward from feedback signals by leveraging the anatomical canonical microcircuit of cortex<sup><xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R8">8</xref></sup> at the input and output stage of cortical object processing<sup><xref ref-type="bibr" rid="R3">3</xref>,<xref ref-type="bibr" rid="R55">55</xref>,<xref ref-type="bibr" rid="R56">56</xref></sup> – EVC and LOC. We find that feedforward signals emerge early in the middle layer of both EVC at 100 ms and in LOC at 160 ms, while feedback appears relatively later in deep and superficial layers of EVC at 140 ms and superficial layer of LOC at 400 ms, supporting the idea of sequential processing of visual information first through feedforward, then through feedback processing<sup><xref ref-type="bibr" rid="R37">37</xref>,<xref ref-type="bibr" rid="R57">57</xref></sup>. Our results thus provide a functional temporal characterization for the anatomical canonical cortical microcircuit model of feedforward and feedback connectivity in the human ventral visual stream.</p><p id="P25">Our findings have theoretical implications. For example, they specify in spatiotemporal terms the dynamical communication model in predictive coding theory<sup><xref ref-type="bibr" rid="R58">58</xref></sup> which posits distinct neural channels for the transmission of sensory and predictive information<sup><xref ref-type="bibr" rid="R59">59</xref></sup>. Our results indicate that whereas sensory signals are convened early in middle layers, subsequent predictive signals are transmitted later as feedback in deep and superficial layers. This specification opens a new path for further testing predictions of predictive coding by investigating the content and integration of predictive feedback and sensory feedforward signals<sup><xref ref-type="bibr" rid="R60">60</xref>–<xref ref-type="bibr" rid="R62">62</xref></sup>. For example, our results invite investigating layer-specific neural dynamics at distinct frequency bands for feedforward and feedback processing as predicted from human<sup><xref ref-type="bibr" rid="R63">63</xref>,<xref ref-type="bibr" rid="R64">64</xref></sup> and non-human invasive studies<sup><xref ref-type="bibr" rid="R59">59</xref>,<xref ref-type="bibr" rid="R65">65</xref>–<xref ref-type="bibr" rid="R67">67</xref></sup>.</p><p id="P26">Our approach facilitates resolving the interplay of feedforward and feedback processing in a variety of visual research contexts. It may allow dissecting the distinctive roles of feedforward and feedback information flow crucial to cognitive functions such as attention, expectation and memory by identifying and characterizing their distinct spatiotemporal dynamics. Similarly, using multi- rather than univariate methods<sup><xref ref-type="bibr" rid="R12">12</xref>,<xref ref-type="bibr" rid="R59">59</xref>,<xref ref-type="bibr" rid="R68">68</xref></sup> to assess layer-specific fMRI extends content-sensitive analysis<sup><xref ref-type="bibr" rid="R69">69</xref>–<xref ref-type="bibr" rid="R71">71</xref></sup> from the macro- to the mesoscale of cortical organization, allowing for the assessment of the representational contents of feedforward and feedback information flow underlying human cognition.</p></sec><sec id="S9"><title>Layer-specific EEG-fMRI fusion clarifies neural dynamics of high-level visual cortex regions</title><p id="P27">Neural dynamics in response to faces<sup><xref ref-type="bibr" rid="R22">22</xref></sup>, scenes<sup><xref ref-type="bibr" rid="R72">72</xref></sup> and objects<sup><xref ref-type="bibr" rid="R73">73</xref>,<xref ref-type="bibr" rid="R74">74</xref></sup> assessed at the macroscale of cortical regions commonly display a double peak pattern (see also <xref ref-type="fig" rid="F2">Fig. 2B</xref>), with a sharp, early peak around 100–130 ms followed by a wide second peak around 200-450 ms, suggesting distinct contributing neural circuits and kinds of processing. Our layer-specific results clarify the neural circuits<sup><xref ref-type="bibr" rid="R22">22</xref></sup> and functional nature of the components underlying this pattern: it results from mixing the early, narrow-peaked and neural dynamics at the middle layer ~160 ms of feedforward information flow with the later, wide-peaked and thus more persistent dynamics at the superficial layer ~400 ms of feedback information flow. Whereas the early peak latency matches that of sensory feedforward signals<sup><xref ref-type="bibr" rid="R75">75</xref></sup>, the late peak dynamics match that of and attention-<sup><xref ref-type="bibr" rid="R76">76</xref></sup>, consciousness-<sup><xref ref-type="bibr" rid="R77">77</xref></sup> or task-related<sup><xref ref-type="bibr" rid="R78">78</xref></sup> feedback, originating from higher-order brain regions, such as the frontal eye fields<sup><xref ref-type="bibr" rid="R10">10</xref></sup> or the prefrontal cortex<sup><xref ref-type="bibr" rid="R79">79</xref></sup>, outside the visual cortex. Thus, our results provide circuit-level mechanistic as well as functional interpretative guidance for standard 3T human neuroimaging studies that typically cannot resolve visual information flow at this fine spatiotemporal level.</p></sec><sec id="S10"><title>Differential representational format across layer-specific dynamics in LOC implies interareal feedback mediating high-complexity visual features</title><p id="P28">The analysis of the representational format of the neural dynamics in LOC revealed that early feedforward signals primarily convey mid-complexity features, while feedback signals convey high-complexity features. This indicates that feedback does not merely modulate the gain of pre-existing features but is actively involved in the emergence of additional, more complex features that are absent in feedforward processing<sup><xref ref-type="bibr" rid="R5">5</xref></sup>. This finding supports the notion that interareal feedback is integral to core object recognition<sup><xref ref-type="bibr" rid="R80">80</xref>–<xref ref-type="bibr" rid="R82">82</xref></sup>. A potential source of this feedback might be the dorsolateral prefrontal cortex (DLPFC)<sup><xref ref-type="bibr" rid="R83">83</xref></sup>, whose silencing decreases feature complexity in monkey inferior temporal cortex<sup><xref ref-type="bibr" rid="R84">84</xref></sup>, the homologue of human LOC. Based on our results we predict that disrupting processing in human DLPFC, e.g. through transcranial magnetic stimulation<sup><xref ref-type="bibr" rid="R85">85</xref>,<xref ref-type="bibr" rid="R86">86</xref></sup>, will yield analogous effects specifically on superficial, late cortical layer responses in LOC.</p></sec><sec id="S11"><title>Limitations of the study</title><p id="P29">We based our analyses on the GE-BOLD signal that is affected by locally nonspecific responses from macrovasculature<sup><xref ref-type="bibr" rid="R41">41</xref></sup>, compromising the estimation of the laminar response. To mitigate this effect, we used Pearson’s correlation as a scale-invariant measure and partialed out effects of lower cortical layers when assessing the layers above. However, some residual bias may persist, particularly if the GE-BOLD response across layers follows a point spread function<sup><xref ref-type="bibr" rid="R87">87</xref></sup> that a linear model cannot fully account for. Human fMRI studies using contrasts with higher spatial specificity<sup><xref ref-type="bibr" rid="R88">88</xref>–<xref ref-type="bibr" rid="R90">90</xref></sup>, optimized for condition-rich experimental designs<sup><xref ref-type="bibr" rid="R21">21</xref></sup>, are needed to confirm our findings.</p></sec></sec><sec id="S12" sec-type="conclusions"><title>Conclusion</title><p id="P30">Understanding how the abundant feedforward and feedback connections in visual cortex mediate object vision requires specifying their functional role. Here we provided two key advances in this regard. First, leveraging layer-specific fMRI with EEG-fMRI fusion we dissociated temporally overlapping feedforward and feedback processing in LOC and EVC, specifying their unique temporal profile. Second, by assessing feature complexity, we showed that feedback in LOC actively increases the complexity of LOC’s feature format.</p></sec><sec id="S13" sec-type="materials | methods"><title>Materials and Methods</title><sec id="S14"><title>fMRI participants</title><p id="P31">10 adult volunteers (mean age 29.4 years; age range 20-37 years; 5 female) participated in the study and provided written informed consent. The sample size was based on previous conventional layer fMRI studies conducted at 7T<sup><xref ref-type="bibr" rid="R68">68</xref>,<xref ref-type="bibr" rid="R88">88</xref>,<xref ref-type="bibr" rid="R91">91</xref></sup>. All participants had normal or corrected-to-normal vision and no history of neurological disorders. All participants received a monetary reward at the end of the study. The study was approved by the Ethics Committee of the Faculty of Medicine at Leipzig University, Germany, and conducted in accordance with the ethical principles of the Declaration of Helsinki, except for study preregistration, which was not performed.</p></sec><sec id="S15"><title>Stimulus set</title><p id="P32">The stimulus set consisted of 24 naturalistic color images of everyday objects on real-world backgrounds, each of a different category from the ImageNet image database (i.e. animals, tools, vehicles, foods and others)<sup><xref ref-type="bibr" rid="R30">30</xref></sup>.</p></sec><sec id="S16"><title>fMRI experimental design</title><p id="P33">The study was composed of a main experimental part and a localizer run.</p><p id="P34">The main experimental part consisted of 6 to 14 runs, each lasting 621 s. Each run started and ended with a 10.5-s baseline period and included all 24 images, each repeated five times, for a total of 120 trials presented in random order. Each trial began with a 1-s stimulus-on interval, during which a centrally displayed object image (5° visual angle) appeared on a grey background with a pink fixation cross. This was followed by a 4-s stimulus-off interval, where only the fixation cross was shown. Participants were instructed to maintain their gaze on the fixation cross throughout the experiment and to perform a color-change detection task, pressing a button as soon as the fixation cross turned blue for 300 ms.</p><p id="P35">The functional localizer run was intended to define regions of interest (ROIs) EVC and LOC. Participants completed it at the beginning of the recording session. The localizer consisted of 15-s blocks displaying objects (not included in the main experiment) and scrambled objects overlayed with a fixation cross, interleaved with 7.5-s baseline blocks showing only a fixation cross on a grey background. In each block images were centrally presented (12° visual angle) for 400 ms, followed by a 350-ms display of the fixation cross. Participants were instructed to maintain their gaze on the fixation cross and to press a button if the same image appeared in consecutive trials. The localizer run included 12 blocks of each image type, resulting in a total duration of 465 s. Block order was pseudo-randomized to avoid immediate repetition of the same block type.</p></sec><sec id="S17"><title>MRI Procedure</title><p id="P36">MRI data were acquired at the Max Planck Institute for Human Cognitive and Brain Sciences in Leipzig, Germany. Four participants completed the experiment in one scanning session. Six participants completed the experiment in two scanning sessions on two separate days. During the first scanning session, we acquired a T1-weighted anatomical image, the functional localizer and 5 to 8 runs of the main experiment. During the second scanning session, participants completed 6 to 8 runs of the main experiment. Additionally, to enable distortion correction, five volumes with reversed phase-encoding polarity were acquired following the first run of each main experimental session. To ensure that participants were familiar with the experimental tasks, we provided them with verbal and written instructions prior to the scanning, and the participants completed a 2-min training for both the localizer and the main tasks.</p></sec><sec id="S18"><title>MRI acquisition parameters</title><p id="P37">We acquired MR images on a Siemens Magnetom Terra 7T whole-body system (Siemens Healthineers, Erlangen, Germany) with a single-channel-transmit and a 32-channel radio-frequency (RF) receive head coil (Nova Medical Inc, Wilmington, USA). We acquired the functional data using a 2D Gradient-echo (GE) echo planar imaging (EPI) sequence<sup><xref ref-type="bibr" rid="R92">92</xref></sup> (voxel size = 0.9 mm isotropic resolution, TE/TR = 26.2/3500 ms, in-plane field of view (FoV) 192 × 192 mm<sup>2</sup>, 48 axial slices, flip angle = 75°, echo spacing = 1.0 ms, GRAPPA factor = 3, partial Fourier = 6/8, phase encoding direction anterior-posterior). We recorded anatomical data using an MP2RAGE sequence<sup><xref ref-type="bibr" rid="R93">93</xref></sup> (voxel size = 0.7 mm isotropic resolution, TE/TR = 2.01/5590 ms, in-plane FoV 224 × 224 mm, GRAPPA factor = 2) yielding two inversion contrasts (TI1 = 900 ms, flip angle 1 = 5°; TI2 = 2750 ms, flip angle 2 = 3°). The two inversion contrasts were combined to produce T1-weighted MP2RAGE uniform (UNI) images with high contrast to noise ratio.</p></sec><sec id="S19"><title>MRI preprocessing</title><p id="P38">For each recording session, we spatially realigned the functional volumes to their mean volume using SPM12 (<ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm">http://www.fil.ion.ucl.ac.uk/spm</ext-link>). To correct for geometric distortions in the phase encoding direction, we calculated a deformation field based on reverse gradient estimation, using the Advanced Normalization Tools (ANTs) software package (<ext-link ext-link-type="uri" xlink:href="http://stnava.github.io/ANTs/">http://stnava.github.io/ANTs/</ext-link>). In detail, we combined the mean functional volume (forward image) with the mean volume acquired with opposing phase encoding direction to generate a distortion-corrected template. Next, we estimated the deformation map by registering the forward image to the corrected template reference using non-linear (SyN) transformations. Finally, the deformation map was used to produce a distortion-corrected mean functional volume.</p><p id="P39">To co-register the anatomical and the distortion-corrected mean functional volumes, we initially referred to the Glasser’s atlas<sup><xref ref-type="bibr" rid="R94">94</xref></sup> and the Kanwisher’s atlas<sup><xref ref-type="bibr" rid="R95">95</xref></sup> to identify the approximate location of EVC and LOC, respectively. Then we outlined a volume containing these regions in the occipito-temporal cortex of both hemispheres on the individual participant’s native space (from here on referred to as manual mask) using ITK-SNAP with the 3D paintbrush tool (v.3.8)<sup><xref ref-type="bibr" rid="R96">96</xref></sup>. We then estimated a second deformation map in ANTs by registering the distortion-corrected mean volume to the T1-weighted volume, applying nonlinear (SyN) transformations within the manual mask. We visually inspected the fixed and registered volumes in ITK-SNAP for each participant. If the volume was not correctly registered within the region of the manual mask, we repeated the registration, adding linear transformations (rigid and affine) with stricter convergence criteria and increased iterations, until an accurate alignment was achieved. To minimize spatial resolution loss during resampling, we combined both deformation maps and resampled the functional images to the anatomical reference in a single interpolation step using a fifth-order spline function.</p><p id="P40">Finally, we spatially smoothed the functional localizer images using a 6-mm full width at half maximum (FWHM) Gaussian kernel. Functional images of the main runs were not smoothed to preserve spatial specificity.</p><p id="P41">Before segmenting the T1-weighted UNI volume into grey matter, white matter and cerebrospinal fluid following the procedure in ITK-SNAP outlined here (<ext-link ext-link-type="uri" xlink:href="https://www.youtube.com/watch?v=tSA77mFTwcg&amp;t=1042s">https://www.youtube.com/watch?v=tSA77mFTwcg&amp;t=1042s</ext-link>), we corrected for bias field effects with a customized script<sup><xref ref-type="bibr" rid="R97">97</xref></sup>. Next, we divided the cortical ribbon into laminar and columnar compartments using LAYNII (v2.2.1)<sup><xref ref-type="bibr" rid="R98">98</xref></sup>. In detail, applying the equi-distant model<sup><xref ref-type="bibr" rid="R33">33</xref></sup>, we segmented the gray matter into three cortical depths: deep, middle and superficial (<xref ref-type="fig" rid="F3">Fig. 3A</xref>). Here, we used the term cortical layers to refer to the depth-dependent compartments along the cortical ribbon, distinct from the actual anatomical layers found in cortex. Additionally, we segmented the gray matter into columnar compartments within the manual masks.</p></sec><sec id="S20"><title>fMRI univariate analysis</title><p id="P42">To estimate neural responses, we ran separate General Linear Model (GLM) analyses in SPM12 for each pre-processed functional run, i.e., all main experimental runs and the localizer run. All analyses were conducted in each participant’s native anatomical space. Specifically, we modelled 25 regressors (i.e., 24 object images + baseline) for each main experimental run, and 3 regressors (i.e., objects, scrambled objects, and baseline) for the localizer run. We created the regressors by convolving a boxcar function representing the onsets and durations of the corresponding condition with the canonical (2 Gamma) hemodynamic response function (HRF). We incorporated the motion estimates into the model as nuisance regressors. By fitting a GLM, we obtained beta weight estimates for each condition (i.e., regressor) for each run, which were subsequently used in further analyses.</p></sec><sec id="S21"><title>Definition of regions of interest</title><p id="P43">At the macroscale, we defined two regions of interest (ROIs) for each participant: EVC, comprising areas V1, V2, and V3, and LOC in a two-step procedure. First, we used anatomical masks from the above-mentioned brain atlases for EVC<sup><xref ref-type="bibr" rid="R94">94</xref></sup> and for LOC<sup><xref ref-type="bibr" rid="R95">95</xref></sup>. These masks were resampled from the MNI152 space into each participant’s individual space. Second, we identified the overlap between the participant-specific anatomical masks and the corresponding functional contrast T-statistic map from the localizer experiment, retaining the top 2,000 voxels. Specifically, we ranked the voxels according to the objects + scrambled &gt; baseline contrast T-statistic for EVC, and the objects &gt; scrambled contrast T-statistic for LOC. Due to lack of activation in the lateral occipitotemporal cortex for one participant (3), we used the objects + scrambled &gt; baseline contrast to define LOC. Any voxels overlapping across ROIs were excluded. This process resulted in one final EVC and LOC mask for each participant.</p><p id="P44">At the mesoscale, we defined six ROIs based on the two brain regions – EVC and LOC – and the three cortical layers – deep, middle and superficial (see above for definition of cortical layers). Here, the ROI definition was analogous, with the following deviations to address issues arising specifically at the mesoscale level. Voxels with higher spatial resolution exhibit a reduced SNR and increased susceptibility to noise sources<sup><xref ref-type="bibr" rid="R41">41</xref></sup>, compromising the quality of the recorded signal. Additionally, signal reliability gradually decreases along the ventral visual cortex from lower to higher visual areas<sup><xref ref-type="bibr" rid="R99">99</xref></sup>, likely due to signal loss and susceptibility-induced distortions<sup><xref ref-type="bibr" rid="R100">100</xref></sup>. To address this, we chose a higher number of voxels for EVC compared to LOC. This resulted in 3000 and 1500 voxels with the highest T-statistic within the cortical ribbon of EVC and LOC, respectively. We then assigned each voxel to one of the three layers and selected only those sharing full overlap of columnar compartments.</p></sec><sec id="S22"><title>EEG data – paradigm, acquisition and analysis</title><p id="P45">We used a subset of the EEG data (<italic>N</italic> = 32) collected by Xie et al., 2024<sup><xref ref-type="bibr" rid="R5">5</xref></sup> for the same images as in the fMRI experiment. Below is a summary of the relevant experimental procedures and acquisition steps.</p><p id="P46">The EEG experiment employed and backward masking paradigm consisting of 2,544 trials. In each trial, an object image was briefly presented for 17 ms and followed by a dynamic mask under two conditions: an early mask condition with inter-stimulus interval (ISI) of 17 ms or a late mask condition with an ISI of 600 ms. Object images and masks were randomly paired on each trial. All stimuli were centrally displayed on a gray background, with a size of 5° visual angle, and overlaid with a bull’s-eye fixation symbol. Participants were instructed to maintain fixation and refrain from blinking during trials, except during designated task trials (i.e., two-alternative forced-choice task, 25% of total trials) where blinking was allowed after making a response. The inter-trial interval (ITI) ranged from 900 – 1,100 ms; following the task trials, the ITI was extended to 2,000 ms to reduce motor artifacts.</p><p id="P47">For the present analyses, we focused on data from the late mask condition (ISI = 600 ms), yielding approximately 53 trials per object image. We analyzed the time window from 200 ms pre-stimulus to 600 ms post-stimulus, before mask onset.</p><p id="P48">EEG data were recorded with a 64-electrode ActiCap system and a Brainvision actiChamp amplifier. Electrodes were positioned according to the international 10-10 system, with a ground electrode and a reference electrode placed on the scalp. The signals were sampled at 1,000 Hz and online filtered between 0.03 and 100 Hz. EEG data were preprocessed using Brainstorm-3<sup><xref ref-type="bibr" rid="R101">101</xref></sup>. Noisy channels (mean = 2.2, SD = 1.8) were removed, and the data were low-pass filtered at 40 Hz. Independent component analysis was applied to remove eye movement and other artifact components (mean = 2.7, SD = 0.9). Data were then segmented into epochs from -200 ms to 600 ms relative to stimulus onset, baseline-corrected, and multivariate noise normalization<sup><xref ref-type="bibr" rid="R102">102</xref></sup> was applied for decoding analyses.</p><p id="P49">Multivariate analysis was performed on a participant-specific basis using SVMs as implemented in the LIBSVM toolbox in MATLAB (2021a). To determine when the brain processes object information, time-resolved decoding analysis was conducted from -200 ms to 600 ms relative to target image onset in 10 ms intervals. At each time point, trial-specific EEG channel activations were extracted and arranged into 64-dimensional pattern vectors for each of the 24 object image conditions. For each condition, trials were randomly grouped into four equally sized bins and averaged to create four pseudo-trials, repeated over 100 permutations. Pseudo-trials were then divided into a training set (three pseudo-trials) and a testing set (one pseudo-trial) for pairwise object identity decoding. This process was repeated for all pairwise combinations of object conditions.</p></sec><sec id="S23"><title>Multivariate pattern analysis of fMRI data</title><p id="P50">To decode object information from voxel activation patterns in EVC and LOC, we used SVMs as implemented in the scikit-learn library in Python. Analyses were conducted separately for each participant and for EVC and LOC at the macroscale. We aggregated run-specific voxel-wise beta estimates derived from the GLM into pattern vectors for each of the 24 conditions. To enhance the signal to noise ratio, we averaged beta weights across two randomly selected subsets of trials into two pseudo-trials for each condition. We repeated this process 300 times, each time performing pairwise object decoding, for all object condition combinations, using a two-fold cross-validation approach. We averaged the results across all iterations and all object condition combinations to yield one grand-average decoding accuracy for each ROI and participant.</p></sec><sec id="S24"><title>Representational similarity analysis</title><p id="P51">To relate object representations across different signal spaces (i.e., voxel activation patterns in fMRI, sensor activation patterns in EEG, embeddings in DNNs), we used representational similarity analysis<sup><xref ref-type="bibr" rid="R21">21</xref></sup>. This approach is based on the rationale that if two images elicit similar neural representations, their corresponding fMRI, EEG or DNN signal patterns should also be similar. We used two variants of RSA: (i) representational similarity analysis-based fusion<sup><xref ref-type="bibr" rid="R21">21</xref>,<xref ref-type="bibr" rid="R23">23</xref>,<xref ref-type="bibr" rid="R24">24</xref></sup>, relating spatially localized object representations (from fMRI) to specific temporal dynamics (from EEG) to resolve the spatiotemporal dynamics with which visual representations emerge, and ii) representational similarity analysis-based commonality analysis<sup><xref ref-type="bibr" rid="R103">103</xref>,<xref ref-type="bibr" rid="R104">104</xref></sup> to determine the visual feature complexity (from layer-specific DNN embeddings) of spatiotemporally identified dynamics. For this we calculated the common variance between spatially localized object representations (from fMRI), specific temporal dynamics (from EEG), and layer embeddings (from DNNs). We detail both approaches below after describing the specifics of summarizing representational similarity for each signal space in representational dissimilarity matrices (RDMs).</p></sec><sec id="S25"><title>Construction of fMRI representational dissimilarity matrices</title><p id="P52">To construct the fMRI-RDMs, we first extracted and vectorized beta weight estimates for each of the 24 conditions to form fMRI neural patterns for a given ROI (area or cortical layer) and applied multivariate noise normalization in order to enhance the SNR. We then quantified the extent of pattern similarity by calculating the Pearson’s correlation for all pairwise combinations of experimental conditions. Output correlations were transformed into dissimilarity values using 1 – Pearson’s correlation, and organized into a 24×24 fMRI representational dissimilarity matrix (RDM), indexed in rows and columns by the compared conditions. For each participant, this yielded two fMRI-RDMs at the macroscale level of brain areas: the EVC RDM and LOC RDM; and six layer-specific fMRI-RDMs at the mesoscale level of cortical layers: Deep EVC RDM, Middle EVC RDM, Superficial EVC RDM, and Deep LOC RDM, Middle LOC RDM and Superficial LOC RDM. For all further analyses we averaged the MRI-RDMs across participants.</p></sec><sec id="S26"><title>Construction of EEG representational dissimilarity matrices</title><p id="P53">A detailed description of the creation of the EEG-RDMs is provided by Xie et. al., (2024)5. Briefly, for each participant and each time point in the epoch from -200 to 600 ms, we pairwise decoded object information using EEG channel activation patterns, via SVMs as implemented in the LIBSVM toolbox<sup><xref ref-type="bibr" rid="R105">105</xref></sup> in MATLAB (2021a). The decoding accuracies for each pair of object images were assembled into 24×24 EEG-RDM for each time point (801 in total), with rows and columns indexed by the conditions. Each matrix was symmetric across the diagonal, with diagonal entries left undefined.</p></sec><sec id="S27"><title>Construction of DNN representational dissimilarity matrices</title><p id="P54">To compute DNN-RDMs, we used the AlexNet architecture<sup><xref ref-type="bibr" rid="R31">31</xref></sup> trained on visual object categorization on the ImageNet dataset<sup><xref ref-type="bibr" rid="R106">106</xref></sup>. In detail, we fed the pre-trained network with the same set of 24 object images as input and extracted the activation pattern for each object condition from each of the five convolutional layers and the two subsequent fully-connected layers. Next, we quantified the pairwise pattern dissimilarity using 1 – Pearson’s correlation for all pairs of image combinations and organized the outputs into 24×24 DNN-RDMs, with rows and columns representing Pearson’s based dissimilarity measure between object conditions. This resulted in a total of 7 layer-specific DNN RDMs.</p></sec><sec id="S28"><title>Representational similarity analysis-based EEG-fMRI fusion</title><p id="P55">We used representational similarity analysis-based fusion<sup><xref ref-type="bibr" rid="R21">21</xref>,<xref ref-type="bibr" rid="R23">23</xref>,<xref ref-type="bibr" rid="R24">24</xref></sup>, relating fMRI-RDMs to EEG-RDMs using Spearman rank order correlation. We applied this analysis at the macroscale and at the mesoscale, for each participant separately. At the macroscale, we related region-specific RDMs (i.e., EEG, LOC) to EEG-RDMs, yielding one time course for each ROI for each participant. At the mesoscale we related region- and layers-specific ROIs to EEG-RDMs, yielding one time course for each of the six ROI-layer combinations for each participant.</p></sec><sec id="S29"><title>Representational similarity analysis-based commonality analysis</title><p id="P56">To characterize the level of feature complexity of the neural representations identified above in space and time, we related them to DNN embeddings across model layers. Feature complexity increases progressively across DNN layers: early model layers are more sensitive to low complexity features, whereas later layers are tuned to high-complexity features<sup><xref ref-type="bibr" rid="R107">107</xref>,<xref ref-type="bibr" rid="R108">108</xref></sup>, analogous to the ventral visual hierarchy in humans and non-human primates<sup><xref ref-type="bibr" rid="R30">30</xref>,<xref ref-type="bibr" rid="R32">32</xref></sup>. Thus, by linking the neural representations to those from the DNN layers, we can determine the feature complexity of spatially localized object representations to particular time points. We did this using representational similarity analysis-based commonality analysis<sup><xref ref-type="bibr" rid="R109">109</xref></sup>. In detail, we calculated the commonality coefficient corresponding to the shared variance among fMRI-RDMs for each brain region and cortical layer, EEG-RDMs at time points with significant effects in EEG-fMRI fusion (averaged across time), and DNN RDMs for each model layer.</p></sec><sec id="S30"><title>Quantification and statistical analysis</title><p id="P57">To assess statistical significance, we performed non-parametric statistical analyses that do not rely on assumptions about the data distribution. The empirical estimations, including decoding accuracy as well as correlation values from the representational similarity analysis-based fusion and from the commonality analysis, were tested against a null distribution created by sign permutation (1,000 permutations). We randomly multiplied the participant-specific data by ±1 to generate permutation samples, recomputed the statistic for each sample, and derived <italic>P</italic> values.</p><p id="P58">We controlled the familywise error rate across time points using cluster-based statistics. First, <italic>P</italic>-value maps were thresholded at <italic>P</italic> &lt; 0.05 to define temporally contiguous suprathreshold clusters. These clusters were then used to construct an empirical null distribution of maximum cluster weights, and a corrected threshold was determined at the 95th percentile of the right tail of this distribution. For the commonality analyses we corrected <italic>P</italic> values for multiple comparisons by FDR-correction.</p><p id="P59">We estimated the 95% confidence intervals for the peak latencies in the RSA-derived time courses using bootstrapping (1,000 paired bootstrap resamples with replacement). For each bootstrap we calculated the statistic, resulting in a bootstrap estimate of peak latencies from which we derived the confidence intervals.</p><p id="P60">To estimate confidence intervals for peak latency differences, we used an analogous bootstrapping approach, resampling the mean peak-to-peak latency difference for each resample. This generated a distribution of mean differences, from which we derived the 95% confidence interval. We set <italic>P</italic> &lt; 0.05, i.e., if the 95% confidence interval did not include 0, we rejected the null hypothesis of no peak-to-peak latency differences.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Material</label><media xlink:href="EMS205596-supplement-Supplementary_Material.pdf" mimetype="application" mime-subtype="pdf" id="d98aAcFbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S31"><title>Acknowledgements</title><p>This work was funded by the German Research Foundation (DFG, CI241/1-1, CI241/3-1, INST 272/297-1 to R.M.C.), by the European Research Council (ERC) starting grant (ERC-2018-STG 803370 to R.M.C.), supported by the NIH Intramural Program of NIMH/NINDS (#ZIC MH002884 to R. H.) and by the Einstein Center for Neuroscience (to T.C.). Computing resources were provided by the high-performance computing facilities at ZEDAT, Freie Universität Berlin, Germany.</p></ack><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mishkin</surname><given-names>M</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name><name><surname>Macko</surname><given-names>KA</given-names></name></person-group><article-title>Object vision and spatial vision: two cortical pathways</article-title><source>Trends in Neurosciences</source><year>1983</year><volume>6</volume><fpage>414</fpage><lpage>417</lpage></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kravitz</surname><given-names>DJ</given-names></name><name><surname>Saleem</surname><given-names>KS</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name><name><surname>Mishkin</surname><given-names>M</given-names></name></person-group><article-title>The ventral visual pathway: an expanded neural framework for the processing of object quality</article-title><source>Trends in Cognitive Sciences</source><year>2013</year><volume>17</volume><fpage>26</fpage><lpage>49</lpage><pub-id pub-id-type="pmcid">PMC3532569</pub-id><pub-id pub-id-type="pmid">23265839</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2012.10.011</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Kourtzi</surname><given-names>Z</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><article-title>The lateral occipital complex and its role in object recognition</article-title><source>Vision Research</source><year>2001</year><volume>41</volume><fpage>1409</fpage><lpage>1422</lpage><pub-id pub-id-type="pmid">11322983</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lamme</surname><given-names>VA</given-names></name><name><surname>Supèr</surname><given-names>H</given-names></name><name><surname>Spekreijse</surname><given-names>H</given-names></name></person-group><article-title>Feedforward, horizontal, and feedback processing in the visual cortex</article-title><source>Current Opinion in Neurobiology</source><year>1998</year><volume>8</volume><fpage>529</fpage><lpage>535</lpage><pub-id pub-id-type="pmid">9751656</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>S</given-names></name><name><surname>Singer</surname><given-names>J</given-names></name><name><surname>Yilmaz</surname><given-names>B</given-names></name><name><surname>Kaiser</surname><given-names>D</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><article-title>The representational nature of spatio-temporal recurrent processing in visual object recognition</article-title><year>2024</year><elocation-id>2024.07.30.605751</elocation-id><pub-id pub-id-type="doi">10.1101/2024.07.30.605751</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lamme</surname><given-names>VAF</given-names></name><name><surname>Zipser</surname><given-names>K</given-names></name><name><surname>Spekreijse</surname><given-names>H</given-names></name></person-group><article-title>Figure-ground activity in primary visual cortex is suppressed by anesthesia</article-title><source>Proceedings of the National Academy of Sciences</source><year>1998</year><volume>95</volume><fpage>3263</fpage><lpage>3268</lpage><pub-id pub-id-type="pmcid">PMC19730</pub-id><pub-id pub-id-type="pmid">9501251</pub-id><pub-id pub-id-type="doi">10.1073/pnas.95.6.3263</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markov</surname><given-names>NT</given-names></name><etal/></person-group><article-title>Anatomy of hierarchy: Feedforward and feedback pathways in macaque visual cortex</article-title><source>Journal of Comparative Neurology</source><year>2014</year><volume>522</volume><fpage>225</fpage><lpage>259</lpage><pub-id pub-id-type="pmcid">PMC4255240</pub-id><pub-id pub-id-type="pmid">23983048</pub-id><pub-id pub-id-type="doi">10.1002/cne.23458</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Felleman</surname><given-names>D</given-names></name><name><surname>Van Essen</surname><given-names>D</given-names></name></person-group><article-title>Distributed hierarchical processing in the primate cerebral cortex</article-title><source>Cereb Cortex</source><year>1991</year><volume>1</volume><fpage>1</fpage><lpage>47</lpage><pub-id pub-id-type="pmid">1822724</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kreiman</surname><given-names>G</given-names></name><name><surname>Serre</surname><given-names>T</given-names></name></person-group><article-title>Beyond the feedforward sweep: feedback computations in the visual cortex</article-title><source>Annals of the New York Academy of Sciences</source><year>2020</year><volume>1464</volume><fpage>222</fpage><lpage>241</lpage><pub-id pub-id-type="pmcid">PMC7456511</pub-id><pub-id pub-id-type="pmid">32112444</pub-id><pub-id pub-id-type="doi">10.1111/nyas.14320</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hüer</surname><given-names>J</given-names></name><name><surname>Saxena</surname><given-names>P</given-names></name><name><surname>Treue</surname><given-names>S</given-names></name></person-group><article-title>Pathway-selective optogenetics reveals the functional anatomy of top–down attentional modulation in the macaque visual cortex</article-title><source>Proc Natl Acad Sci USA</source><year>2024</year><volume>121</volume><elocation-id>e2304511121</elocation-id><pub-id pub-id-type="pmcid">PMC10801865</pub-id><pub-id pub-id-type="pmid">38194453</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2304511121</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nurminen</surname><given-names>L</given-names></name><name><surname>Merlin</surname><given-names>S</given-names></name><name><surname>Bijanzadeh</surname><given-names>M</given-names></name><name><surname>Federer</surname><given-names>F</given-names></name><name><surname>Angelucci</surname><given-names>A</given-names></name></person-group><article-title>Top-down feedback controls spatial summation and response amplitude in primate visual cortex</article-title><source>Nature Communications</source><year>2018</year><volume>9</volume><elocation-id>2281</elocation-id><pub-id pub-id-type="pmcid">PMC5995810</pub-id><pub-id pub-id-type="pmid">29892057</pub-id><pub-id pub-id-type="doi">10.1038/s41467-018-04500-5</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bergmann</surname><given-names>J</given-names></name><etal/></person-group><article-title>Cortical depth profiles in primary visual cortex for illusory and imaginary experiences</article-title><source>Nature Communications</source><year>2024</year><volume>15</volume><elocation-id>1002</elocation-id><pub-id pub-id-type="pmcid">PMC10837448</pub-id><pub-id pub-id-type="pmid">38307834</pub-id><pub-id pub-id-type="doi">10.1038/s41467-024-45065-w</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lawrence</surname><given-names>SJ</given-names></name><name><surname>Norris</surname><given-names>DG</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><article-title>Dissociable laminar profiles of concurrent bottom-up and top-down modulation in the human visual cortex</article-title><source>eLife</source><year>2019</year><volume>8</volume><elocation-id>e44422</elocation-id><pub-id pub-id-type="pmcid">PMC6538372</pub-id><pub-id pub-id-type="pmid">31063127</pub-id><pub-id pub-id-type="doi">10.7554/eLife.44422</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carricarte</surname><given-names>T</given-names></name><etal/></person-group><article-title>Laminar dissociation of feedforward and feedback in high-level ventral visual cortex during imagery and perception</article-title><source>iScience</source><year>2024</year><volume>27</volume><elocation-id>110229</elocation-id><pub-id pub-id-type="pmcid">PMC11246059</pub-id><pub-id pub-id-type="pmid">39006482</pub-id><pub-id pub-id-type="doi">10.1016/j.isci.2024.110229</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dowdle</surname><given-names>L</given-names></name><etal/></person-group><article-title>Characterizing top-down microcircuitry of complex human behavior across different levels of the visual hierarchy</article-title><year>2023</year><pub-id pub-id-type="doi">10.1101/2022.12.03.518973</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>C</given-names></name><etal/></person-group><article-title>Layer-dependent multiplicative effects of spatial attention on contrast responses in human early visual cortex</article-title><source>Progress in Neurobiology</source><year>2021</year><volume>207</volume><elocation-id>101897</elocation-id><pub-id pub-id-type="pmid">32818495</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rockland</surname><given-names>KS</given-names></name><name><surname>Pandya</surname><given-names>DN</given-names></name></person-group><article-title>Laminar origins and terminations of cortical connections of the occipital lobe in the rhesus monkey</article-title><source>Brain Research</source><year>1979</year><volume>179</volume><fpage>3</fpage><lpage>20</lpage><pub-id pub-id-type="pmid">116716</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rockland</surname><given-names>KS</given-names></name></person-group><article-title>What do we know about laminar connectivity?</article-title><source>NeuroImage</source><year>2019</year><volume>197</volume><fpage>772</fpage><lpage>784</lpage><pub-id pub-id-type="pmid">28729159</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siu</surname><given-names>C</given-names></name><name><surname>Balsor</surname><given-names>J</given-names></name><name><surname>Merlin</surname><given-names>S</given-names></name><name><surname>Federer</surname><given-names>F</given-names></name><name><surname>Angelucci</surname><given-names>A</given-names></name></person-group><article-title>A direct interareal feedback-to-feedforward circuit in primate visual cortex</article-title><source>Nat Commun</source><year>2021</year><volume>12</volume><elocation-id>4911</elocation-id><pub-id pub-id-type="pmcid">PMC8363744</pub-id><pub-id pub-id-type="pmid">34389710</pub-id><pub-id pub-id-type="doi">10.1038/s41467-021-24928-6</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Federer</surname><given-names>F</given-names></name><name><surname>Ta’afua</surname><given-names>S</given-names></name><name><surname>Merlin</surname><given-names>S</given-names></name><name><surname>Hassanpour</surname><given-names>MS</given-names></name><name><surname>Angelucci</surname><given-names>A</given-names></name></person-group><article-title>Stream-specific feedback inputs to the primate primary visual cortex</article-title><source>Nature Communications</source><year>2021</year><volume>12</volume><fpage>228</fpage><pub-id pub-id-type="pmcid">PMC7801467</pub-id><pub-id pub-id-type="pmid">33431862</pub-id><pub-id pub-id-type="doi">10.1038/s41467-020-20505-5</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name></person-group><article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><year>2008</year><volume>2</volume><pub-id pub-id-type="pmcid">PMC2605405</pub-id><pub-id pub-id-type="pmid">19104670</pub-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><article-title>Resolving human object recognition in space and time</article-title><source>Nature Neuroscience</source><year>2014</year><volume>17</volume><fpage>455</fpage><lpage>462</lpage><pub-id pub-id-type="pmcid">PMC4261693</pub-id><pub-id pub-id-type="pmid">24464044</pub-id><pub-id pub-id-type="doi">10.1038/nn.3635</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><article-title>A M/EEG-fMRI Fusion Primer: Resolving Human Brain Responses in Space and Time</article-title><source>Neuron</source><year>2020</year><volume>107</volume><fpage>772</fpage><lpage>781</lpage><pub-id pub-id-type="pmcid">PMC7612024</pub-id><pub-id pub-id-type="pmid">32721379</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2020.07.001</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><article-title>Similarity-Based Fusion of MEG and fMRI Reveals SpatioTemporal Dynamics in Human Cortex During Visual Object Recognition</article-title><source>Cerebral Cortex</source><year>2016</year><volume>26</volume><fpage>3563</fpage><lpage>3579</lpage><pub-id pub-id-type="pmcid">PMC4961022</pub-id><pub-id pub-id-type="pmid">27235099</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhw135</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Essen</surname><given-names>DCV</given-names></name><name><surname>Maunsell</surname><given-names>JHR</given-names></name></person-group><article-title>Hierarchical organization and functional streams in the visual cortex</article-title><source>Trends in Neurosciences</source><year>1983</year><volume>6</volume><fpage>370</fpage><lpage>375</lpage></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singer</surname><given-names>JJD</given-names></name><name><surname>Karapetian</surname><given-names>A</given-names></name><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><article-title>Identifying and characterizing scene representations relevant for categorization behavior</article-title><source>Imaging Neuroscience</source><year>2025</year><volume>3</volume><elocation-id>imag_a_00449</elocation-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name><name><surname>Rust</surname><given-names>NC</given-names></name></person-group><article-title>How Does the Brain Solve Visual Object Recognition?</article-title><source>Neuron</source><year>2012</year><volume>73</volume><fpage>415</fpage><lpage>434</lpage><pub-id pub-id-type="pmcid">PMC3306444</pub-id><pub-id pub-id-type="pmid">22325196</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2012.01.010</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kourtzi</surname><given-names>Z</given-names></name><name><surname>Connor</surname><given-names>CE</given-names></name></person-group><article-title>Neural Representations for Object Perception: Structure, Category, and Adaptive Coding</article-title><source>Annual Review of Neuroscience</source><year>2011</year><volume>34</volume><fpage>45</fpage><lpage>67</lpage><pub-id pub-id-type="pmid">21438683</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muukkonen</surname><given-names>I</given-names></name><name><surname>Ölander</surname><given-names>K</given-names></name><name><surname>Numminen</surname><given-names>J</given-names></name><name><surname>Salmela</surname><given-names>VR</given-names></name></person-group><article-title>Spatio-temporal dynamics of face perception</article-title><source>NeuroImage</source><year>2020</year><volume>209</volume><elocation-id>116531</elocation-id><pub-id pub-id-type="pmid">31931156</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Khosla</surname><given-names>A</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Torralba</surname><given-names>A</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><article-title>Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence</article-title><source>Scientific Reports</source><year>2016</year><volume>6</volume><elocation-id>27755</elocation-id><pub-id pub-id-type="pmcid">PMC4901271</pub-id><pub-id pub-id-type="pmid">27282108</pub-id><pub-id pub-id-type="doi">10.1038/srep27755</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><chapter-title>ImageNet Classification with Deep Convolutional Neural Networks</chapter-title><person-group person-group-type="editor"><name><surname>Pereira</surname><given-names>F</given-names></name><name><surname>Burges</surname><given-names>CJ</given-names></name><name><surname>Bottou</surname><given-names>L</given-names></name><name><surname>Weinberger</surname><given-names>KQ</given-names></name></person-group><source>Advances in Neural Information Processing Systems</source><publisher-name>Curran Associates, Inc</publisher-name><year>2012</year><volume>25</volume></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Güçlü</surname><given-names>U</given-names></name><name><surname>van Gerven</surname><given-names>MAJ</given-names></name></person-group><article-title>Deep Neural Networks Reveal a Gradient in the Complexity of Neural Representations across the Ventral Stream</article-title><source>J Neurosci</source><year>2015</year><volume>35</volume><elocation-id>10005</elocation-id><pub-id pub-id-type="pmcid">PMC6605414</pub-id><pub-id pub-id-type="pmid">26157000</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5023-14.2015</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waehnert</surname><given-names>MD</given-names></name><etal/></person-group><article-title>Anatomically motivated modeling of cortical laminae</article-title><source>NeuroImage</source><year>2014</year><volume>93</volume><fpage>210</fpage><lpage>220</lpage><pub-id pub-id-type="pmid">23603284</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barth</surname><given-names>M</given-names></name><name><surname>Norris</surname><given-names>DG</given-names></name></person-group><article-title>Very high-resolution three-dimensional functional MRI of the human visual cortex with elimination of large venous vessels</article-title><source>NMR in Biomedicine</source><year>2007</year><volume>20</volume><fpage>477</fpage><lpage>484</lpage><pub-id pub-id-type="pmid">17405190</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markuerkiaga</surname><given-names>I</given-names></name><name><surname>Barth</surname><given-names>M</given-names></name><name><surname>Norris</surname><given-names>DG</given-names></name></person-group><article-title>A cortical vascular model for examining the specificity of the laminar BOLD signal</article-title><source>NeuroImage</source><year>2016</year><volume>132</volume><fpage>491</fpage><lpage>498</lpage><pub-id pub-id-type="pmid">26952195</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Kerkoerle</surname><given-names>T</given-names></name><name><surname>Self</surname><given-names>MW</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><article-title>Layer-specificity in the effects of attention and working memory on activity in primary visual cortex</article-title><source>Nature Communications</source><year>2017</year><volume>8</volume><elocation-id>13804</elocation-id><pub-id pub-id-type="pmcid">PMC5227065</pub-id><pub-id pub-id-type="pmid">28054544</pub-id><pub-id pub-id-type="doi">10.1038/ncomms13804</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Self</surname><given-names>MW</given-names></name><name><surname>van Kerkoerle</surname><given-names>T</given-names></name><name><surname>Supèr</surname><given-names>H</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><article-title>Distinct Roles of the Cortical Layers of Area V1 in Figure-Ground Segregation</article-title><source>Current Biology</source><year>2013</year><volume>23</volume><fpage>2121</fpage><lpage>2129</lpage><pub-id pub-id-type="pmid">24139742</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barzegaran</surname><given-names>E</given-names></name><name><surname>Plomp</surname><given-names>G</given-names></name></person-group><article-title>Four concurrent feedforward and feedback networks with different roles in the visual cortical hierarchy</article-title><source>PLOS Biology</source><year>2022</year><volume>20</volume><elocation-id>e3001534</elocation-id><pub-id pub-id-type="pmcid">PMC8865670</pub-id><pub-id pub-id-type="pmid">35143472</pub-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3001534</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grossberg</surname><given-names>S</given-names></name></person-group><article-title>Linking the laminar circuits of visual cortex to visual perception: Development, grouping, and attention</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><year>2001</year><volume>25</volume><fpage>513</fpage><lpage>526</lpage><pub-id pub-id-type="pmid">11595271</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>MA</given-names></name><name><surname>Jia</surname><given-names>X</given-names></name><name><surname>Zandvakili</surname><given-names>A</given-names></name><name><surname>Kohn</surname><given-names>A</given-names></name></person-group><article-title>Laminar dependence of neuronal correlations in visual cortex</article-title><source>Journal of Neurophysiology</source><year>2013</year><volume>109</volume><fpage>940</fpage><lpage>947</lpage><pub-id pub-id-type="pmcid">PMC3569140</pub-id><pub-id pub-id-type="pmid">23197461</pub-id><pub-id pub-id-type="doi">10.1152/jn.00846.2012</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polimeni</surname><given-names>JR</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Greve</surname><given-names>DN</given-names></name><name><surname>Wald</surname><given-names>LL</given-names></name></person-group><article-title>Laminar analysis of 7T BOLD using an imposed spatial activation pattern in human V1</article-title><source>NeuroImage</source><year>2010</year><volume>52</volume><fpage>1334</fpage><lpage>1346</lpage><pub-id pub-id-type="pmcid">PMC3130346</pub-id><pub-id pub-id-type="pmid">20460157</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.05.005</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwiedrzik</surname><given-names>CM</given-names></name><name><surname>Freiwald</surname><given-names>WA</given-names></name></person-group><article-title>High-Level Prediction Signals in a Low-Level Area of the Macaque Face-Processing Hierarchy</article-title><source>Neuron</source><year>2017</year><volume>96</volume><fpage>89</fpage><lpage>97</lpage><elocation-id>e4</elocation-id><pub-id pub-id-type="pmcid">PMC5757317</pub-id><pub-id pub-id-type="pmid">28957679</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2017.09.007</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stecher</surname><given-names>R</given-names></name><name><surname>Kaiser</surname><given-names>D</given-names></name></person-group><article-title>Representations of imaginary scenes and their properties in cortical alpha activity</article-title><source>Scientific Reports</source><year>2024</year><volume>14</volume><elocation-id>12796</elocation-id><pub-id pub-id-type="pmcid">PMC11150249</pub-id><pub-id pub-id-type="pmid">38834699</pub-id><pub-id pub-id-type="doi">10.1038/s41598-024-63320-4</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morgan</surname><given-names>AT</given-names></name><name><surname>Petro</surname><given-names>LS</given-names></name><name><surname>Muckli</surname><given-names>L</given-names></name></person-group><article-title>Scene Representations Conveyed by Cortical Feedback to Early Visual Cortex Can Be Described by Line Drawings</article-title><source>J Neurosci</source><year>2019</year><volume>39</volume><elocation-id>9410</elocation-id><pub-id pub-id-type="pmcid">PMC6867807</pub-id><pub-id pub-id-type="pmid">31611306</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0852-19.2019</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Papale</surname><given-names>P</given-names></name><etal/></person-group><article-title>The representation of occluded image regions in area V1 of monkeys and humans</article-title><source>Current Biology</source><year>2023</year><volume>33</volume><fpage>3865</fpage><lpage>3871</lpage><elocation-id>e3</elocation-id><pub-id pub-id-type="pmid">37643620</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Del Rosario</surname><given-names>J</given-names></name><etal/></person-group><article-title>Lateral inhibition in V1 controls neural and perceptual contrast sensitivity</article-title><source>Nature Neuroscience</source><year>2025</year><pub-id pub-id-type="pmid">40033123</pub-id></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilbert</surname><given-names>CD</given-names></name><name><surname>Wiesel</surname><given-names>TN</given-names></name></person-group><article-title>Morphology and intracortical projections of functionally characterised neurones in the cat visual cortex</article-title><source>Nature</source><year>1979</year><volume>280</volume><fpage>120</fpage><lpage>125</lpage><pub-id pub-id-type="pmid">552600</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rockland</surname><given-names>KS</given-names></name><name><surname>Lund</surname><given-names>JS</given-names></name></person-group><article-title>Widespread Periodic Intrinsic Connections in the Tree Shrew Visual Cortex</article-title><source>Science</source><year>1982</year><volume>215</volume><fpage>1532</fpage><lpage>1534</lpage><pub-id pub-id-type="pmid">7063863</pub-id></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilbert</surname><given-names>C</given-names></name><name><surname>Wiesel</surname><given-names>T</given-names></name></person-group><article-title>Clustered intrinsic connections in cat visual cortex</article-title><source>J Neurosci</source><year>1983</year><volume>3</volume><elocation-id>1116</elocation-id><pub-id pub-id-type="pmcid">PMC6564507</pub-id><pub-id pub-id-type="pmid">6188819</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.03-05-01116.1983</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stettler</surname><given-names>DD</given-names></name><name><surname>Das</surname><given-names>A</given-names></name><name><surname>Bennett</surname><given-names>J</given-names></name><name><surname>Gilbert</surname><given-names>CD</given-names></name></person-group><article-title>Lateral Connectivity and Contextual Interactions in Macaque Primary Visual Cortex</article-title><source>Neuron</source><year>2002</year><volume>36</volume><fpage>739</fpage><lpage>750</lpage><pub-id pub-id-type="pmid">12441061</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fujita</surname><given-names>I</given-names></name><name><surname>Fujita</surname><given-names>T</given-names></name></person-group><article-title>Intrinsic connections in the macaque inferior temporal cortex</article-title><source>Journal of Comparative Neurology</source><year>1996</year><volume>368</volume><fpage>467</fpage><lpage>486</lpage><pub-id pub-id-type="pmid">8744437</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grootswagers</surname><given-names>T</given-names></name><name><surname>Robinson</surname><given-names>AK</given-names></name><name><surname>Shatek</surname><given-names>SM</given-names></name><name><surname>Carlson</surname><given-names>TA</given-names></name></person-group><article-title>Mapping the dynamics of visual feature coding: Insights into perception and integration</article-title><source>PLOS Computational Biology</source><year>2024</year><volume>20</volume><elocation-id>e1011760</elocation-id><pub-id pub-id-type="pmcid">PMC10798643</pub-id><pub-id pub-id-type="pmid">38190390</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1011760</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallimore</surname><given-names>CG</given-names></name><name><surname>Ricci</surname><given-names>DA</given-names></name><name><surname>Hamm</surname><given-names>JP</given-names></name></person-group><article-title>Spatiotemporal dynamics across visual cortical laminae support a predictive coding framework for interpreting mismatch responses</article-title><source>Cerebral Cortex</source><year>2023</year><volume>33</volume><fpage>9417</fpage><lpage>9428</lpage><pub-id pub-id-type="pmcid">PMC10393498</pub-id><pub-id pub-id-type="pmid">37310190</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhad215</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dobs</surname><given-names>K</given-names></name><name><surname>Isik</surname><given-names>L</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><article-title>How face perception unfolds over time</article-title><source>Nature Communications</source><year>2019</year><volume>10</volume><elocation-id>1258</elocation-id><pub-id pub-id-type="pmcid">PMC6425020</pub-id><pub-id pub-id-type="pmid">30890707</pub-id><pub-id pub-id-type="doi">10.1038/s41467-019-09239-1</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name></person-group><article-title>The human visual cortex</article-title><source>Annual Review of Neuroscience</source><year>2004</year><volume>27</volume><fpage>649</fpage><lpage>677</lpage><pub-id pub-id-type="pmid">15217346</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name><etal/></person-group><article-title>A sequence of object-processing stages revealed by fMRI in the human occipital lobe</article-title><source>Human Brain Mapping</source><year>1998</year><volume>6</volume><fpage>316</fpage><lpage>328</lpage><pub-id pub-id-type="pmcid">PMC6873387</pub-id><pub-id pub-id-type="pmid">9704268</pub-id><pub-id pub-id-type="doi">10.1002/(SICI)1097-0193(1998)6:4&amp;#x0003c;316::AID-HBM9&amp;#x0003e;3.0.CO;2-6</pub-id></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Callaway</surname><given-names>EM</given-names></name></person-group><article-title>Feedforward, feedback and inhibitory connections in primate visual cortex</article-title><source>Neural Networks</source><year>2004</year><volume>17</volume><fpage>625</fpage><lpage>632</lpage><pub-id pub-id-type="pmid">15288888</pub-id></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>RPN</given-names></name><name><surname>Ballard</surname><given-names>DH</given-names></name></person-group><article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title><source>Nat Neurosci</source><year>1999</year><volume>2</volume><fpage>79</fpage><lpage>87</lpage><pub-id pub-id-type="pmid">10195184</pub-id></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scheeringa</surname><given-names>R</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name></person-group><article-title>Cortical layers, rhythms and BOLD signals</article-title><source>NeuroImage</source><year>2019</year><volume>197</volume><fpage>689</fpage><lpage>698</lpage><pub-id pub-id-type="pmcid">PMC6666418</pub-id><pub-id pub-id-type="pmid">29108940</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.11.002</pub-id></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Larkum</surname><given-names>ME</given-names></name><name><surname>Zhu</surname><given-names>JJ</given-names></name><name><surname>Sakmann</surname><given-names>B</given-names></name></person-group><article-title>A new cellular mechanism for coupling inputs arriving at different cortical layers</article-title><source>Nature</source><year>1999</year><volume>398</volume><fpage>338</fpage><lpage>341</lpage><pub-id pub-id-type="pmid">10192334</pub-id></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aru</surname><given-names>J</given-names></name><name><surname>Suzuki</surname><given-names>M</given-names></name><name><surname>Larkum</surname><given-names>ME</given-names></name></person-group><article-title>Cellular Mechanisms of Conscious Processing</article-title><source>Trends in Cognitive Sciences</source><year>2020</year><volume>24</volume><fpage>814</fpage><lpage>825</lpage><pub-id pub-id-type="pmid">32855048</pub-id></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schuman</surname><given-names>B</given-names></name><name><surname>Dellal</surname><given-names>S</given-names></name><name><surname>Prönneke</surname><given-names>A</given-names></name><name><surname>Machold</surname><given-names>R</given-names></name><name><surname>Rudy</surname><given-names>B</given-names></name></person-group><article-title>Neocortical Layer 1: An Elegant Solution to Top-Down and Bottom-Up Integration</article-title><source>Annual Review of Neuroscience</source><year>2021</year><volume>44</volume><fpage>221</fpage><lpage>252</lpage><pub-id pub-id-type="pmcid">PMC9012327</pub-id><pub-id pub-id-type="pmid">33730511</pub-id><pub-id pub-id-type="doi">10.1146/annurev-neuro-100520-012117</pub-id></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Kaiser</surname><given-names>D</given-names></name></person-group><article-title>Alpha-frequency feedback to early visual cortex orchestrates coherent naturalistic vision</article-title><source>Science Advances</source><year>2023</year><volume>9</volume><elocation-id>eadi2321</elocation-id><pub-id pub-id-type="pmcid">PMC10637741</pub-id><pub-id pub-id-type="pmid">37948520</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.adi2321</pub-id></element-citation></ref><ref id="R64"><label>64</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>S</given-names></name><name><surname>Kaiser</surname><given-names>D</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><article-title>Visual Imagery and Perception Share Neural Representations in the Alpha Frequency Band</article-title><source>Current Biology</source><year>2020</year><volume>30</volume><fpage>2621</fpage><lpage>2627</lpage><elocation-id>e5</elocation-id><pub-id pub-id-type="pmcid">PMC7342016</pub-id><pub-id pub-id-type="pmid">32531274</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2020.04.074</pub-id></element-citation></ref><ref id="R65"><label>65</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bastos</surname><given-names>AM</given-names></name><etal/></person-group><article-title>Visual Areas Exert Feedforward and Feedback Influences through Distinct Frequency Channels</article-title><source>Neuron</source><year>2015</year><volume>85</volume><fpage>390</fpage><lpage>401</lpage><pub-id pub-id-type="pmid">25556836</pub-id></element-citation></ref><ref id="R66"><label>66</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Kerkoerle</surname><given-names>T</given-names></name><etal/></person-group><article-title>Alpha and gamma oscillations characterize feedback and feedforward processing in monkey visual cortex</article-title><source>Proceedings of the National Academy of Sciences</source><year>2014</year><volume>111</volume><fpage>14332</fpage><lpage>14341</lpage><pub-id pub-id-type="pmcid">PMC4210002</pub-id><pub-id pub-id-type="pmid">25205811</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1402773111</pub-id></element-citation></ref><ref id="R67"><label>67</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Michalareas</surname><given-names>G</given-names></name><etal/></person-group><article-title>Alpha-Beta and Gamma Rhythms Subserve Feedback and Feedforward Influences among Human Visual Cortical Areas</article-title><source>Neuron</source><year>2016</year><volume>89</volume><fpage>384</fpage><lpage>397</lpage><pub-id pub-id-type="pmcid">PMC4871751</pub-id><pub-id pub-id-type="pmid">26777277</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2015.12.018</pub-id></element-citation></ref><ref id="R68"><label>68</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muckli</surname><given-names>L</given-names></name><etal/></person-group><article-title>Contextual Feedback to Superficial Layers of V1</article-title><source>Current Biology</source><year>2015</year><volume>25</volume><fpage>2690</fpage><lpage>2695</lpage><pub-id pub-id-type="pmcid">PMC4612466</pub-id><pub-id pub-id-type="pmid">26441356</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2015.08.057</pub-id></element-citation></ref><ref id="R69"><label>69</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haynes</surname><given-names>J-D</given-names></name><name><surname>Rees</surname><given-names>G</given-names></name></person-group><article-title>Decoding mental states from brain activity in humans</article-title><source>Nature Reviews Neuroscience</source><year>2006</year><volume>7</volume><fpage>523</fpage><lpage>534</lpage><pub-id pub-id-type="pmid">16791142</pub-id></element-citation></ref><ref id="R70"><label>70</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Kievit</surname><given-names>RA</given-names></name></person-group><article-title>Representational geometry: integrating cognition, computation, and the brain</article-title><source>Trends in Cognitive Sciences</source><year>2013</year><volume>17</volume><fpage>401</fpage><lpage>412</lpage><pub-id pub-id-type="pmcid">PMC3730178</pub-id><pub-id pub-id-type="pmid">23876494</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2013.06.007</pub-id></element-citation></ref><ref id="R71"><label>71</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Revealing representational content with pattern-information fMRI—an introductory guide</article-title><source>Social Cognitive and Affective Neuroscience</source><year>2009</year><volume>4</volume><fpage>101</fpage><lpage>109</lpage><pub-id pub-id-type="pmcid">PMC2656880</pub-id><pub-id pub-id-type="pmid">19151374</pub-id><pub-id pub-id-type="doi">10.1093/scan/nsn044</pub-id></element-citation></ref><ref id="R72"><label>72</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henriksson</surname><given-names>L</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Rapid Invariant Encoding of Scene Layout in Human OPA</article-title><source>Neuron</source><year>2019</year><volume>103</volume><fpage>161</fpage><lpage>171</lpage><elocation-id>e3</elocation-id><pub-id pub-id-type="pmid">31097360</pub-id></element-citation></ref><ref id="R73"><label>73</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mohsenzadeh</surname><given-names>Y</given-names></name><name><surname>Qin</surname><given-names>S</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name></person-group><article-title>Ultra-Rapid serial visual presentation reveals dynamics of feedforward and feedback processes in the ventral visual pathway</article-title><source>eLife</source><year>2018</year><volume>7</volume><elocation-id>e36329</elocation-id><pub-id pub-id-type="pmcid">PMC6029845</pub-id><pub-id pub-id-type="pmid">29927384</pub-id><pub-id pub-id-type="doi">10.7554/eLife.36329</pub-id></element-citation></ref><ref id="R74"><label>74</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Motlagh</surname><given-names>SC</given-names></name><name><surname>Joanisse</surname><given-names>M</given-names></name><name><surname>Wang</surname><given-names>B</given-names></name><name><surname>Mohsenzadeh</surname><given-names>Y</given-names></name></person-group><article-title>Unveiling the neural dynamics of conscious perception in rapid object recognition</article-title><source>NeuroImage</source><year>2024</year><volume>296</volume><elocation-id>120668</elocation-id><pub-id pub-id-type="pmid">38848982</pub-id></element-citation></ref><ref id="R75"><label>75</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hung</surname><given-names>CP</given-names></name><name><surname>Kreiman</surname><given-names>G</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><article-title>Fast readout of object identity from macaque inferior temporal cortex</article-title><source>Science</source><year>2005</year><volume>310</volume><fpage>863</fpage><lpage>866</lpage><pub-id pub-id-type="pmid">16272124</pub-id></element-citation></ref><ref id="R76"><label>76</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopf</surname><given-names>J-M</given-names></name><etal/></person-group><article-title>Neural Sources of Focused Attention in Visual Search</article-title><source>Cerebral Cortex</source><year>2000</year><volume>10</volume><fpage>1233</fpage><lpage>1241</lpage><pub-id pub-id-type="pmid">11073872</pub-id></element-citation></ref><ref id="R77"><label>77</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cul</surname><given-names>AD</given-names></name><name><surname>Baillet</surname><given-names>S</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><article-title>Brain Dynamics Underlying the Nonlinear Threshold for Access to Consciousness</article-title><source>PLOSBiology</source><year>2007</year><volume>5</volume><fpage>e260</fpage><pub-id pub-id-type="pmcid">PMC1988856</pub-id><pub-id pub-id-type="pmid">17896866</pub-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0050260</pub-id></element-citation></ref><ref id="R78"><label>78</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duan</surname><given-names>Y</given-names></name><name><surname>Zhan</surname><given-names>J</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Ince</surname><given-names>RAA</given-names></name><name><surname>Schyns</surname><given-names>PG</given-names></name></person-group><article-title>Pre-frontal cortex guides dimension-reducing transformations in the occipito-ventral pathway for categorization behaviors</article-title><source>Current Biology</source><year>2024</year><volume>34</volume><fpage>3392</fpage><lpage>3404</lpage><elocation-id>e5</elocation-id><pub-id pub-id-type="pmid">39029470</pub-id></element-citation></ref><ref id="R79"><label>79</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar</surname><given-names>M</given-names></name></person-group><article-title>A Cortical Mechanism for Triggering Top-Down Facilitation in Visual Object Recognition</article-title><source>Journal of Cognitive Neuroscience</source><year>2003</year><volume>15</volume><fpage>600</fpage><lpage>609</lpage><pub-id pub-id-type="pmid">12803970</pub-id></element-citation></ref><ref id="R80"><label>80</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kietzmann</surname><given-names>TC</given-names></name><etal/></person-group><article-title>Recurrence is required to capture the representational dynamics of the human visual system</article-title><source>Proceedings of the National Academy of Sciences</source><year>2019</year><volume>116</volume><fpage>21854</fpage><lpage>21863</lpage><pub-id pub-id-type="pmcid">PMC6815174</pub-id><pub-id pub-id-type="pmid">31591217</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1905544116</pub-id></element-citation></ref><ref id="R81"><label>81</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Schmidt</surname><given-names>K</given-names></name><name><surname>Issa</surname><given-names>EB</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><article-title>Evidence that recurrent circuits are critical to the ventral stream’s execution of core object recognition behavior</article-title><source>Nature Neuroscience</source><year>2019</year><volume>22</volume><fpage>974</fpage><lpage>983</lpage><pub-id pub-id-type="pmcid">PMC8785116</pub-id><pub-id pub-id-type="pmid">31036945</pub-id><pub-id pub-id-type="doi">10.1038/s41593-019-0392-5</pub-id></element-citation></ref><ref id="R82"><label>82</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>von Seth</surname><given-names>J</given-names></name><name><surname>Nicholls</surname><given-names>VI</given-names></name><name><surname>Tyler</surname><given-names>LK</given-names></name><name><surname>Clarke</surname><given-names>A</given-names></name></person-group><article-title>Recurrent connectivity supports higher-level visual and semantic object representations in the brain</article-title><source>Commun Biol</source><year>2023</year><volume>6</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="pmcid">PMC10682037</pub-id><pub-id pub-id-type="pmid">38012301</pub-id><pub-id pub-id-type="doi">10.1038/s42003-023-05565-9</pub-id></element-citation></ref><ref id="R83"><label>83</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamm</surname><given-names>JP</given-names></name><name><surname>Shymkiv</surname><given-names>Y</given-names></name><name><surname>Han</surname><given-names>S</given-names></name><name><surname>Yang</surname><given-names>W</given-names></name><name><surname>Yuste</surname><given-names>R</given-names></name></person-group><article-title>Cortical ensembles selective for context</article-title><source>Proceedings of the National Academy of Sciences</source><year>2021</year><volume>118</volume><elocation-id>e2026179118</elocation-id><pub-id pub-id-type="pmcid">PMC8040629</pub-id><pub-id pub-id-type="pmid">33811144</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2026179118</pub-id></element-citation></ref><ref id="R84"><label>84</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><article-title>Fast Recurrent Processing via Ventrolateral Prefrontal Cortex Is Needed by the Primate Ventral Stream for Robust Core Visual Object Recognition</article-title><source>Neuron</source><year>2021</year><volume>109</volume><fpage>164</fpage><lpage>176</lpage><elocation-id>e5</elocation-id><pub-id pub-id-type="pmid">33080226</pub-id></element-citation></ref><ref id="R85"><label>85</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hallett</surname><given-names>M</given-names></name></person-group><article-title>Transcranial Magnetic Stimulation: A Primer</article-title><source>Neuron</source><year>2007</year><volume>55</volume><fpage>187</fpage><lpage>199</lpage><pub-id pub-id-type="pmid">17640522</pub-id></element-citation></ref><ref id="R86"><label>86</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bergmann</surname><given-names>TO</given-names></name><etal/></person-group><article-title>Concurrent TMS-fMRI for causal network perturbation and proof of target engagement</article-title><source>NeuroImage</source><year>2021</year><volume>237</volume><elocation-id>118093</elocation-id><pub-id pub-id-type="pmid">33940146</pub-id></element-citation></ref><ref id="R87"><label>87</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markuerkiaga</surname><given-names>I</given-names></name><name><surname>Marques</surname><given-names>JP</given-names></name><name><surname>Gallagher</surname><given-names>TE</given-names></name><name><surname>Norris</surname><given-names>DG</given-names></name></person-group><article-title>Estimation of laminar BOLD activation profiles using deconvolution with a physiological point spread function</article-title><source>J Neurosci Methods</source><year>2021</year><volume>353</volume><elocation-id>109095</elocation-id><pub-id pub-id-type="pmid">33549635</pub-id></element-citation></ref><ref id="R88"><label>88</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huber</surname><given-names>L</given-names></name><etal/></person-group><article-title>High-Resolution CBV-fMRI Allows Mapping of Laminar Activity and Connectivity of Cortical Input and Output in Human M1</article-title><source>Neuron</source><year>2017</year><volume>96</volume><fpage>1253</fpage><lpage>1263</lpage><elocation-id>e7</elocation-id><pub-id pub-id-type="pmcid">PMC5739950</pub-id><pub-id pub-id-type="pmid">29224727</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2017.11.005</pub-id></element-citation></ref><ref id="R89"><label>89</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huber</surname><given-names>L</given-names></name><etal/></person-group><article-title>Techniques for blood volume fMRI with VASO: From low-resolution mapping towards sub-millimeter layer-dependent applications</article-title><source>NeuroImage</source><year>2018</year><volume>164</volume><fpage>131</fpage><lpage>143</lpage><pub-id pub-id-type="pmcid">PMC5436958</pub-id><pub-id pub-id-type="pmid">27867088</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.11.039</pub-id></element-citation></ref><ref id="R90"><label>90</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pizzuti</surname><given-names>A</given-names></name><etal/></person-group><article-title>Imaging the columnar functional organization of human area MT+ to axis-of-motion stimuli using VASO at 7 Tesla</article-title><source>Cerebral Cortex</source><year>2023</year><volume>33</volume><fpage>8693</fpage><lpage>8711</lpage><pub-id pub-id-type="pmcid">PMC10321107</pub-id><pub-id pub-id-type="pmid">37254796</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhad151</pub-id></element-citation></ref><ref id="R91"><label>91</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kok</surname><given-names>P</given-names></name><name><surname>Bains</surname><given-names>LJ</given-names></name><name><surname>van Mourik</surname><given-names>T</given-names></name><name><surname>Norris</surname><given-names>DG</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><article-title>Selective Activation of the Deep Layers of the Human Primary Visual Cortex by Top-Down Feedback</article-title><source>Current Biology</source><year>2016</year><volume>26</volume><fpage>371</fpage><lpage>376</lpage><pub-id pub-id-type="pmid">26832438</pub-id></element-citation></ref><ref id="R92"><label>92</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moeller</surname><given-names>S</given-names></name><etal/></person-group><article-title>Multiband multislice GE-EPI at 7 tesla, with 16-fold acceleration using partial parallel imaging with application to high spatial and temporal whole-brain fMRI</article-title><source>Magnetic Resonance in Medicine</source><year>2010</year><volume>63</volume><fpage>1144</fpage><lpage>1153</lpage><pub-id pub-id-type="pmcid">PMC2906244</pub-id><pub-id pub-id-type="pmid">20432285</pub-id><pub-id pub-id-type="doi">10.1002/mrm.22361</pub-id></element-citation></ref><ref id="R93"><label>93</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marques</surname><given-names>JP</given-names></name><etal/></person-group><article-title>MP2RAGE, a self bias-field corrected sequence for improved segmentation and T1-mapping at high field</article-title><source>NeuroImage</source><year>2010</year><volume>49</volume><fpage>1271</fpage><lpage>1281</lpage><pub-id pub-id-type="pmid">19819338</pub-id></element-citation></ref><ref id="R94"><label>94</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glasser</surname><given-names>MF</given-names></name><etal/></person-group><article-title>A multi-modal parcellation of human cerebral cortex</article-title><source>Nature</source><year>2016</year><volume>536</volume><fpage>171</fpage><lpage>178</lpage><pub-id pub-id-type="pmcid">PMC4990127</pub-id><pub-id pub-id-type="pmid">27437579</pub-id><pub-id pub-id-type="doi">10.1038/nature18933</pub-id></element-citation></ref><ref id="R95"><label>95</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Julian</surname><given-names>JB</given-names></name><name><surname>Fedorenko</surname><given-names>E</given-names></name><name><surname>Webster</surname><given-names>J</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><article-title>An algorithmic method for functionally defining regions of interest in the ventral visual pathway</article-title><source>NeuroImage</source><year>2012</year><volume>60</volume><fpage>2357</fpage><lpage>2364</lpage><pub-id pub-id-type="pmid">22398396</pub-id></element-citation></ref><ref id="R96"><label>96</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yushkevich</surname><given-names>PA</given-names></name><etal/></person-group><article-title>User-Guided Segmentation of Multi-modality Medical Imaging Datasets with ITK-SNAP</article-title><source>Neuroinformatics</source><year>2019</year><volume>17</volume><fpage>83</fpage><lpage>102</lpage><pub-id pub-id-type="pmcid">PMC6310114</pub-id><pub-id pub-id-type="pmid">29946897</pub-id><pub-id pub-id-type="doi">10.1007/s12021-018-9385-x</pub-id></element-citation></ref><ref id="R97"><label>97</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lüsebrink</surname><given-names>F</given-names></name><name><surname>Sciarra</surname><given-names>A</given-names></name><name><surname>Mattern</surname><given-names>H</given-names></name><name><surname>Yakupov</surname><given-names>R</given-names></name><name><surname>Speck</surname><given-names>O</given-names></name></person-group><article-title>T1-weighted in vivo human whole brain MRI dataset with an ultrahigh isotropic resolution of 250 μm</article-title><source>Scientific Data</source><year>2017</year><volume>4</volume><elocation-id>170032</elocation-id><pub-id pub-id-type="pmcid">PMC5349250</pub-id><pub-id pub-id-type="pmid">28291265</pub-id><pub-id pub-id-type="doi">10.1038/sdata.2017.32</pub-id></element-citation></ref><ref id="R98"><label>98</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huber</surname><given-names>L (Renzo)</given-names></name><etal/></person-group><article-title>LayNii: A software suite for layer-fMRI</article-title><source>NeuroImage</source><year>2021</year><volume>237</volume><elocation-id>118091</elocation-id><pub-id pub-id-type="pmcid">PMC7615890</pub-id><pub-id pub-id-type="pmid">33991698</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2021.118091</pub-id></element-citation></ref><ref id="R99"><label>99</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badwal</surname><given-names>MW</given-names></name><name><surname>Bergmann</surname><given-names>J</given-names></name><name><surname>Roth</surname><given-names>J</given-names></name><name><surname>Doeller</surname><given-names>CF</given-names></name><name><surname>Hebart</surname><given-names>MN</given-names></name></person-group><article-title>The scope and limits of fine-grained image and category information in the ventral visual pathway</article-title><source>J Neurosci</source><year>2024</year><volume>45</volume><elocation-id>e0936242024</elocation-id><pub-id pub-id-type="pmcid">PMC11735656</pub-id><pub-id pub-id-type="pmid">39505406</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0936-24.2024</pub-id></element-citation></ref><ref id="R100"><label>100</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malekian</surname><given-names>V</given-names></name><etal/></person-group><article-title>Mitigating susceptibility-induced distortions in high-resolution 3DEPI fMRI at 7T</article-title><source>NeuroImage</source><year>2023</year><volume>279</volume><elocation-id>120294</elocation-id><pub-id pub-id-type="pmcid">PMC10951962</pub-id><pub-id pub-id-type="pmid">37517572</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2023.120294</pub-id></element-citation></ref><ref id="R101"><label>101</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tadel</surname><given-names>F</given-names></name><name><surname>Baillet</surname><given-names>S</given-names></name><name><surname>Mosher</surname><given-names>JC</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Leahy</surname><given-names>RM</given-names></name></person-group><article-title>Brainstorm: A User-Friendly Application for MEG/EEG Analysis</article-title><source>Computational Intelligence and Neuroscience</source><year>2011</year><volume>2011</volume><elocation-id>879716</elocation-id><pub-id pub-id-type="pmcid">PMC3090754</pub-id><pub-id pub-id-type="pmid">21584256</pub-id><pub-id pub-id-type="doi">10.1155/2011/879716</pub-id></element-citation></ref><ref id="R102"><label>102</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guggenmos</surname><given-names>M</given-names></name><name><surname>Sterzer</surname><given-names>P</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><article-title>Multivariate pattern analysis for MEG: A comparison of dissimilarity measures</article-title><source>Neuroimage</source><year>2018</year><volume>173</volume><fpage>434</fpage><lpage>447</lpage><pub-id pub-id-type="pmid">29499313</pub-id></element-citation></ref><ref id="R103"><label>103</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mood</surname><given-names>AM</given-names></name></person-group><article-title>Partitioning Variance in Multiple Regression Analyses as a Tool For Developing Learning Models</article-title><source>American Educational Research Journal</source><year>1971</year><volume>8</volume><fpage>191</fpage><lpage>202</lpage></element-citation></ref><ref id="R104"><label>104</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reichwein Zientek</surname><given-names>L</given-names></name><name><surname>Thompson</surname><given-names>B</given-names></name></person-group><article-title>Commonality Analysis: Partitioning Variance to Facilitate Better Understanding of Data</article-title><source>Journal of Early Intervention</source><year>2006</year><volume>28</volume><fpage>299</fpage><lpage>307</lpage></element-citation></ref><ref id="R105"><label>105</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>C-C</given-names></name><name><surname>Lin</surname><given-names>C-J</given-names></name></person-group><article-title>LIBSVM: A library for support vector machines</article-title><source>ACM Trans Intell Syst Technol</source><year>2011</year><volume>2</volume><fpage>27:1</fpage><lpage>27:27</lpage></element-citation></ref><ref id="R106"><label>106</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>J</given-names></name><etal/></person-group><source>ImageNet: A large-scale hierarchical image database</source><conf-name>2009 IEEE Conference on Computer Vision and Pattern Recognition</conf-name><year>2009</year><fpage>248</fpage><lpage>255</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2009.5206848</pub-id></element-citation></ref><ref id="R107"><label>107</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeiler</surname><given-names>MD</given-names></name><name><surname>Fergus</surname><given-names>R</given-names></name></person-group><article-title>Visualizing and Understanding Convolutional Networks</article-title><year>2013</year><pub-id pub-id-type="doi">10.48550/arXiv.1311.2901</pub-id></element-citation></ref><ref id="R108"><label>108</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>B</given-names></name><name><surname>Khosla</surname><given-names>A</given-names></name><name><surname>Lapedriza</surname><given-names>A</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name><name><surname>Torralba</surname><given-names>A</given-names></name></person-group><article-title>Object Detectors Emerge in Deep Scene CNNs</article-title><year>2015</year><pub-id pub-id-type="doi">10.48550/arXiv.1412.6856</pub-id></element-citation></ref><ref id="R109"><label>109</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Bankson</surname><given-names>BB</given-names></name><name><surname>Harel</surname><given-names>A</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><article-title>The representational dynamics of task and object processing in humans</article-title><source>Elife</source><year>2018</year><volume>7</volume><elocation-id>e32816</elocation-id><pub-id pub-id-type="pmcid">PMC5811210</pub-id><pub-id pub-id-type="pmid">29384473</pub-id><pub-id pub-id-type="doi">10.7554/eLife.32816</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Stimuli, experimental design and multivariate pattern classification.</title><p><bold>(A)</bold> Stimulus set. The stimuli consisted of 24 different naturalistic object images. <bold>(B)</bold> fMRI experimental design. On each trial, participants viewed images for 1 s followed by a 4-s baseline interval. Participants were required to perform a color-change detection task on the fixation cross that occurred randomly throughout the experiment. (<bold>C</bold>) Extraction of voxel values to form condition-specific pattern vectors from region of interest (example here: EVC) and pairwise-object classification using a support vector machine. ROIs are depicted for visualization purposes only (<bold>D</bold>) Object-pairwise multivariate decoding output. Robust object-specific information was reliably decoded from EVC (71,16 %, <italic>P</italic> = 0.0039) and LOC (60,69 %, <italic>P</italic> = 0.0092) using one-sample permutation tests. Error bars indicate the standard error of the mean across participants.</p></caption><graphic xlink:href="EMS205596-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Representational EEG-fMRI fusion at the macroscale.</title><p>(<bold>A</bold>) Representational similarity analysis. For each condition, we extracted the neural pattern from the region of interest (example here: EVC). We assessed the extent of pattern dissimilarity by calculating 1 – Pearson’s correlation for all combinations of experimental conditions (<italic>i</italic>, <italic>j</italic>) and assigned the dissimilarity values to an fMRI representational dissimilarity matrix (RDM) indexed by the conditions in rows and columns, at entry (<italic>i</italic>, <italic>j</italic>). ROIs are depicted for visualization purposes only (<bold>B</bold>) Representational EEG-fMRI fusion. For each time point <italic>t</italic>, we correlated the EEG-RDM to the fMRI-RDMs of EVC and LOC using Spearman’s rank order correlation. (<bold>C</bold>) Spatiotemporal neural dynamics at the macroscale level. Time course in EVC peaked earlier than in LOC. (<bold>D</bold>) Difference between EVC and LOC curves in <bold>C</bold>. EEG signals correlated first more with EVC than with LOC and later more with LOC than with EVC. Shaded area indicates the standard error of the mean across participants; colored circles indicate significant time points (<italic>N</italic> = 32, cluster-defining threshold <italic>P</italic> &lt; 0.05, cluster threshold <italic>P</italic> &lt; 0.05); uncolored circles and horizontal lines indicate peak latency means and 95% confidence intervals, respectively. (<bold>E</bold>) Commonality analysis. For each AlexNet layer, we correlated its RDM to each ROI-specific fMRI-RDM and the mean EEG-RDM at the time interval with significant ROI-specific temporal dynamics. (<bold>F</bold>) Format of representation (≈ feature complexity) in EVC and LOC. Visual representations of low-complexity emerge early in EVC, while mid-to-high-level object representations emerge later in LOC. Error bars indicate the standard error of the mean across participants; colored asterisks indicate significant correlations (<italic>N</italic> = 32, right-tailed permutation tests, FDR-corrected; *<italic>P</italic> &lt; 0.05; **<italic>P</italic> &lt; 0.01; ***<italic>P</italic> &lt; 0.001); colored triangles represent model layers with the highest occurrence proportion, determined through 1,000-iteration bootstraps.</p></caption><graphic xlink:href="EMS205596-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Representational EEG-fMRI fusion at the mesoscale.</title><p>(<bold>A</bold>). For each cortical layer — deep, middle, and superficial — (example here: for EVC) we computed the partial Spearman’s rank-order correlation between its layer-specific fMRI-RDM and the EEG-RDM at each time point <italic>t</italic>. (<bold>B</bold>) Layer-specific spatiotemporal neural dynamics in EVC. EEG signals correlated early across layers, with the time course in the middle layer peaking earlier than in the deep and superficial layers. (<bold>C</bold>) Difference between EVC layer curves in <bold>B</bold>. EEG signals correlated lately more with deep and superficial layers than with the middle layer (<bold>D</bold>) Layer-specific spatiotemporal neural dynamics in LOC. EEG signals correlated early in the middle layer and later in the superficial layer. (<bold>E</bold>) Difference between LOC layer curves in <bold>D</bold>. EEG signals correlated early more with the middle layer than with the superficial layer, and later more with the superficial layer than with the middle layer. Shaded area indicates the standard error of the mean across participants; colored circles indicate significant time points (<italic>N</italic> = 32, cluster-defining threshold <italic>P</italic> &lt; 0.05, cluster threshold <italic>P</italic> &lt; 0.05); uncolored circles and horizontal lines indicate peak latency means and 95% confidence intervals, respectively.</p></caption><graphic xlink:href="EMS205596-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Commonality analysis between fMRI, EEG and AlexNet at the mesoscale.</title><p>(<bold>A</bold>) Procedure. For each AlexNet layer, we correlated its RDM to each layer-specific fMRI-RDM in EVC and LOC and the mean EEG-RDM at the time interval with significant layer-specific temporal dynamics. (<bold>B</bold>) Format of representation (≈ feature complexity) across cortical layers in EVC. Low model layers correlated strongly across layers in EVC. (<bold>C</bold>) Format of representation (≈ feature complexity) across cortical layers in LOC. Middle model layers correlated strongly with the middle layer in LOC, while high model layers correlated primarily with the superficial layer. Colored asterisks indicate significant correlations (<italic>N =</italic> 32, right-tailed permutation tests, FDR-corrected; *<italic>P</italic> &lt; 0.05; **<italic>P</italic> &lt; 0.01; ***<italic>P</italic> &lt; 0.001); colored triangles represent model layers with the highest occurrence proportion, determined through 1,000-iteration bootstraps.</p></caption><graphic xlink:href="EMS205596-f004"/></fig></floats-group></article>