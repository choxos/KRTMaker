<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS205784</article-id><article-id pub-id-type="doi">10.1101/2025.05.20.654789</article-id><article-id pub-id-type="archive">PPR1023774</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">2</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Semi-automatic Geometrical Reconstruction and Analysis of Filopodia Dynamics in 4D Two-Photon Microscopy Images</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Brence</surname><given-names>Blaž</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Brummer</surname><given-names>Josephine</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Dercksen</surname><given-names>Vincent J.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Özel</surname><given-names>Mehmet Neset</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Kulkarni</surname><given-names>Abhishkek</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Wolterhoff</surname><given-names>Neele</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Prohaska</surname><given-names>Steffen</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Hiesinger</surname><given-names>Peter Robin</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Baum</surname><given-names>Daniel</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><aff id="A1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02eva5865</institution-id><institution>Zuse Institute Berlin</institution></institution-wrap>, <city>Berlin</city>, <country country="DE">Germany</country></aff><aff id="A2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/046ak2485</institution-id><institution>Freie Universität Berlin</institution></institution-wrap>, <city>Berlin</city>, <country country="DE">Germany</country></aff><aff id="A3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04bgfm609</institution-id><institution>Stowers Institute for Medical Research</institution></institution-wrap>, <city>Kansas City</city>, <country country="US">United States</country></aff></contrib-group><author-notes><corresp id="CR1">
<label>*</label> Corresponding authors. Blažz Brence - <email>brence@zib.de</email>; Daniel Baum - <email>baum@zib.de</email>;</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>23</day><month>05</month><year>2025</year></pub-date><pub-date pub-type="preprint"><day>22</day><month>05</month><year>2025</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 International license</ext-link>.</license-p></license></permissions><abstract><sec id="S1"><title>Background</title><p id="P1">Filopodia are thin and dynamic membrane protrusions that play a crucial role in cell migration, axon guidance, and other processes where cells explore and interact with their surroundings. Historically, filopodial dynamics have been studied in great detail in 2D in cultured cells, and more recently in 3D culture as well as living brains. However, there is a lack of efficient tools to trace and track filopodia in 4D images of complex brain cells.</p></sec><sec id="S2"><title>Results</title><p id="P2">To address this issue, we have developed a semi-automatic workflow for tracing filopodia in 3D images and tracking the traced filopodia over time. The workflow was developed based on high-resolution data of photoreceptor axon terminals in the in vivo context of normal <italic>Drosophila</italic> brain development, but devised to be applicable to filopodia in any system, including at different temporal and spatial scales. In contrast to the pre-existing methods, our workflow relies solely on the original intensity images without the requirement for segmentation or complex preprocessing. The workflow was realized in C++ within the <italic>Amira</italic> software system and consists of two main parts, dataset pre-processing, and geometrical filopodia reconstruction, where each of the two parts comprises multiple steps. In this paper, we provide an extensive workflow description and demonstrate its versatility for two different axo-dendritic morphologies, R7 and Dm8 cells. Finally, we provide an analysis of the time requirements for user input and data processing.</p></sec><sec id="S3"><title>Conclusion</title><p id="P3">To facilitate simple application within <italic>Amira</italic> or other frameworks, we share the source code, which is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/zibamira/filopodia-tool">https://github.com/zibamira/filopodia-tool</ext-link>.</p></sec></abstract><kwd-group><kwd>4D image analysis</kwd><kwd>filopodia</kwd><kwd>two-photon microscopy</kwd><kwd>tracing</kwd><kwd>tracking</kwd><kwd>semi-automatic workflow.</kwd></kwd-group></article-meta></front><body><sec id="S4" sec-type="intro"><label>1</label><title>Background</title><p id="P4">Filopodia are thin, spike-like protrusions that dynamically extend and retract from the surface of cells and neuronal axon terminals. Filopodial dynamics are often interpreted as exploratory and may aid in directed growth, amongst other functions [<xref ref-type="bibr" rid="R1">1</xref>–<xref ref-type="bibr" rid="R3">3</xref>]. Neurons develop complicated morphologies through the slow path-finding of filopodiarich axon terminal structures and branched dendritic structures that are preceded by continuously extending and retracting filopodia. While filopodial dynamics are thus intuitively associated with cellular morphogenesis and growth, the precise types of filopodial dynamics and their effects on biological functions has rarely been studied quantitatively based on live data that capture dynamics at sufficient resolution in time and space.</p><p id="P5">To address this issue, we have previously developed a <italic>Drosophila melanogaster</italic> brain culture live imaging system that takes advantage of the fly’s brain size, genetic tools, and limited time period of brain development [<xref ref-type="bibr" rid="R4">4</xref>]. Recently, fast filopodial dynamics have also been measured inside the intact developing <italic>Drosophila</italic> pupal brain [<xref ref-type="bibr" rid="R5">5</xref>]. In short, two-photon microscopy allows to visualize the in-vivo dynamics of filopodia anywhere in the <italic>Drosophila</italic> brain at high spatial and temporal resolution throughout the entire time window of axon pathfinding and synapse formation (for up to 2 days). Similar live observations of filopodial dynamics have been achieved in model systems ranging from 3D culture [<xref ref-type="bibr" rid="R6">6</xref>] to mice [<xref ref-type="bibr" rid="R7">7</xref>] and birds [<xref ref-type="bibr" rid="R8">8</xref>]. Advances in two-photon microscopy and newer imaging technologies like lattice-lightsheet microscopy increasingly allow to obtain such high-resolution data, leading to an analytical bottleneck.</p><p id="P6">To facilitate the quantitative analysis of <italic>Drosophila</italic> filopodial dynamics and their comparison across different experimental conditions, we developed computational approaches based on 3D time series of photoreceptor axon terminals (see <xref ref-type="fig" rid="F1">Fig. 1</xref>). The workflow has been successfully applied for the analysis of filopodial dynamics in several studies [<xref ref-type="bibr" rid="R9">9</xref>–<xref ref-type="bibr" rid="R11">11</xref>].</p><p id="P7">To analyze and compare filopodia dynamics in different datasets, an efficient method for information extraction and quantification is required. Of particular interest is information about filopodia length and lifetime as well as the number of extension or retraction events. To obtain such information, a 4D geometrical reconstruction of the filopodia geometry tracked over time is required. While the 4D images (3D + time) acquired using two-photon microscopy provide a high resolution in the first two dimensions (see <xref ref-type="fig" rid="F2">Fig. 2a</xref>), the limited resolution in the third dimension constitutes a challenge (see <xref ref-type="fig" rid="F2">Fig. 2b</xref>).</p><p id="P8">There are several existing software solutions to analyze filopodia. These can be divided into general purpose protrusion tracing and tracking tools, and tools designed specifically for the study of filopodia. General tools are mostly dedicated to determining the cell shape [<xref ref-type="bibr" rid="R12">12</xref>] or general cellular protrusion analysis [<xref ref-type="bibr" rid="R13">13</xref>, <xref ref-type="bibr" rid="R14">14</xref>]. As filopodia are small, high-resolution images are needed to ensure tracking efficiency with these tools [<xref ref-type="bibr" rid="R13">13</xref>]. Filopodia-specific tools typically offer similar analysis in terms of filopodia quantification. Previous studies demonstrated analyses of parameters including length, shape, density, elongation and retraction speed [<xref ref-type="bibr" rid="R15">15</xref>–<xref ref-type="bibr" rid="R21">21</xref>]. A few solutions additionally provide tools for spatio-temporal analyses of protein concentrations in filopodia [<xref ref-type="bibr" rid="R15">15</xref>, <xref ref-type="bibr" rid="R18">18</xref>]. Importantly, all the tools described so far are based on 2D data and time, or 3D data with reduced dimensionality (working with 2D projections and time). To accurately analyze filopodial dynamics, 3D analysis must be performed. 3D tracing of filopodia can be achieved using one of the commercial microscopy image analysis tools, such as <italic>Imaris</italic>. Its <italic>Filament tracer</italic> module can be used to trace filopodia as well [<xref ref-type="bibr" rid="R4">4</xref>]. <italic>TeraVR</italic> is another tool that is capable of 3D geometrical neuron reconstruction [<xref ref-type="bibr" rid="R22">22</xref>]. It allows the user to work within immersive virtual reality, where one can visualize and interact with images. However, both <italic>Imaris</italic> and <italic>TeraVR</italic> do not allow for automatic filopodia propagation or automatic labeling of the same filopodium in successive time steps. At the time of this study, 3D analysis of filopodia or cellular protrusions over time was conducted only in a few publications [<xref ref-type="bibr" rid="R23">23</xref>–<xref ref-type="bibr" rid="R25">25</xref>]. These tools use convolutional neural networks for image segmentation, hence they need a potentially large amount of training data of diverse datasets to generalize for different types of data.</p><p id="P9">Here, we present a semi-automatic workflow for filopodia tracing and tracking that does not require training data or image segmentation. Instead, our workflow works directly with the intensity images as input. The workflow was developed within the 3D visualization and data analysis software <italic>Amira</italic> [<xref ref-type="bibr" rid="R26">26</xref>], extending its <italic>Filament Editor</italic> [<xref ref-type="bibr" rid="R27">27</xref>], but it can be implemented in other environments. The semi-automatic tool supports interaction in both 2D and 3D viewers, thus allowing for more intuitive and faster data processing. The results of the 4D geometrical filopodia reconstruction (referred to as ‘reconstruction’ throughout the paper for simplicity) are represented as skeleton graphs. Such graphs simplify the extraction of statistical information, such as filopodial length, number, orientation, retraction, and extensions. Using this semi-automatic approach, manual input is significantly reduced. We provide an open-source extension package, which allows implementation of this workflow in <italic>Amira</italic> and can serve as a guide for implementation in an alternative environment.</p></sec><sec id="S5"><label>2</label><title>Implementation</title><p id="P10">The semi-automatic workflow and a dedicated set of tools have been developed within the <italic>Filament Editor</italic> framework [<xref ref-type="bibr" rid="R27">27</xref>] of <italic>Amira</italic>, as it already contains a lot of functionality necessary for its implementation. For example, it provides general 2D/3D visualizations (<xref ref-type="fig" rid="F3">Fig. 3</xref>) and interactive editing of filamentous, graph-like objects embedded in 3D space. The 2D view displays an image cross-section with the graph superimposed and offers tools for interactive tracing. In addition, the 3D view can display the graph and a 3D direct volume rendering visualization of the image data. To support the tracing and tracking of filopodia over time, we extended the capabilities of <italic>Amira</italic>’s <italic>Filament Editor</italic> to process time-dependent geometry and support the specific functionality for the tracing of filopodia described below.</p><sec id="S6"><label>2.1</label><title>Data acquisition</title><p id="P11">We developed the workflow based on live imaging data of R7 photoreceptor neurons and subsequently tested the methods for other neurons inside the intact <italic>Drosophila</italic> brain. The datasets were imaged using two-photon microscopy, yielding 4D data (3D + time) with spatiotemporal resolution detailed enough to discern individual filopodial dynamics [<xref ref-type="bibr" rid="R4">4</xref>, <xref ref-type="bibr" rid="R5">5</xref>]. Specifically, all datasets used here cover a period of 60 minutes with a temporal resolution of 1 minute. However, the images have a limited axial resolution in z direction that leads to an anisotropic voxel size of 0.1 × 0.1 × 0.5 <italic>µm</italic><sup>3</sup>. Images were acquired with a Leica SP8 OPO-MPX multiphoton microscope using a 40x oil immersion lens.</p></sec><sec id="S7"><label>2.2</label><title>Data representation</title><p id="P12">In order to facilitate tracking of filopodia and subsequent analysis, R7 photoreceptor axon terminals (including filopodia) are represented by their skeleton graphs (trees). Nodes of the graph have one of the following types, as shown in <xref ref-type="fig" rid="F4">Figure 4</xref>: <list list-type="order" id="L1"><list-item><p id="P13">Root node (axon terminal center)</p></list-item><list-item><p id="P14">Base node</p></list-item><list-item><p id="P15">Branching node</p></list-item><list-item><p id="P16">Tip node</p></list-item></list></p><p id="P17">Root, base and tip nodes are mandatory when tracing filopodia whereas branching nodes are not because filopodia branching happens rarely.</p><p id="P18">Edges of the graph connect the nodes. Edges that are part of the paths between root and base are labeled as <italic>axon terminal</italic>, the ones between base and tip as <italic>filopodium</italic>. The skeleton graphs of all time steps are combined in one dataset. All nodes and edges obtain a time label to mark the time step of their occurrence.</p></sec><sec id="S8"><label>2.3</label><title>Workflow</title><p id="P19">The overall workflow consists of the following steps: <list list-type="order" id="L2"><list-item><p id="P20">Preprocessing <list list-type="alpha-lower" id="L3"><list-item><p id="P21">Determination of root nodes</p></list-item><list-item><p id="P22">Extraction of axon images</p></list-item></list></p></list-item><list-item><p id="P23">Filopodia reconstruction (per axon) <list list-type="alpha-lower" id="L4"><list-item><p id="P24">Filopodia tracing</p></list-item><list-item><p id="P25">Filopodia tracking</p></list-item><list-item><p id="P26">Proofreading</p></list-item></list></p></list-item><list-item><p id="P27">Statistical analysis</p></list-item></list></p><p id="P28">The individual workflow steps are introduced and explained in the following sub-sections. All user-adjustable workflow parameters including parameter domain and parameter values used in this paper are listed in <xref ref-type="table" rid="T1">Table 1</xref>.</p><sec id="S9"><label>2.3.1</label><title>Preprocessing</title><p id="P29">Some preprocessing is required to facilitate the filopodia reconstruction in step 2. This preprocessing separates the axon terminals from another. It is done in two steps, as described below.</p><sec id="S10"><title>Determination of axon terminal centers</title><p id="P30">First, the user imports the image data into <italic>Amira</italic> (<xref ref-type="fig" rid="F5">Fig. 5a</xref>). Within the 2D viewer, the user selects the center of an axon terminal to be processed in the first time step. Centers for the remaining time steps are automatically detected with a template matching algorithm [<xref ref-type="bibr" rid="R28">28</xref>]. For this, the image region around the root node in the previous time step is used as template. The root node is iteratively repositioned to all the voxel centers inside a user-defined search window. At each position, the similarity of the image with the template is computed using the normalized cross correlation (NCC), utilizing the Insight Toolkit (ITK) [<xref ref-type="bibr" rid="R29">29</xref>] implementation. NCC is used to account for possible intensity shifts. The algorithm automatically selects the voxel with the maximal NCC value as the new root node. If the NCC value is less than the chosen threshold <italic>γ</italic> ∈ [0, <xref ref-type="bibr" rid="R1">1</xref>], the new location is considered unreliable and the location of the current time step is used. In such a case, the user is notified and it is advisable to carefully proofread the positions of base nodes. The template size <italic>T</italic><sub><italic>R</italic></sub> ∈ IR<sup>3</sup> approximates the size of the axon terminal; the search window size <italic>S</italic><sub><italic>R</italic></sub> ∈ IR<sup>3</sup> is set to the maximal expected drift of the root node (during the workflow, no other drift-correction steps are necessary). Both template size and window size are user-adjustable with respect to the anisotropic voxel size. Center nodes are assigned an axon terminal ID and a time step label. In the next step, the user verifies the center locations for all the time steps by displaying the nodes together inside a 3D semi-transparent volume rendering of the image (<xref ref-type="fig" rid="F5">Fig. 5b</xref>).</p></sec><sec id="S11"><title>Extraction of axon terminal images</title><p id="P31">Filopodia are traced and tracked for one axon at a time. To facilitate this, cropped datasets are created containing single axons. Using cropped images significantly reduces computation time and reduces occlusion by other axons. The user can determine the size and location of the cropping box either numerically or by dragging the handles of a box in the 3D viewer. The same box size is used for all time steps. The origin of the box is shifted such that it always has the same position relative to the root node position. Initial estimation of the box location and its size are computed automatically. The estimation is based on the Contour Tree Segmentation (CTS) [<xref ref-type="bibr" rid="R30">30</xref>], using two user-adjustable parameters, intensity threshold <italic>θ</italic> and a persistence value <italic>p</italic>. Connected-component labeling of the binary image is not applicable since multiple axons can be very close to each other or even touch and, therefore, might not be separable using connected-component labeling.</p><p id="P32">The cropping box for a particular time step and axon is displayed in 3D, together with the root nodes and a volume rendering of the 3D image, allowing users to quickly verify whether the box completely surrounds the axon terminal (<xref ref-type="fig" rid="F5">Fig. 5c</xref>). After visual validation, the images are cropped and saved (<xref ref-type="fig" rid="F5">Fig. 5d</xref>). In addition, Dijkstra graphs [<xref ref-type="bibr" rid="R31">31</xref>] are pre-computed for each cropped image during the last step. They are used in the following steps.</p></sec></sec><sec id="S12"><label>2.3.2</label><title>Filopodia reconstruction</title><p id="P33">The user traces the filopodia for one axon terminal at a time. The cropped images and pre-computed Dijkstra graphs of all time steps of a previously pre-processed dataset are loaded into the software. The process of filopodia reconstruction involves the following steps: <list list-type="order" id="L5"><list-item><p id="P34">Filopodia tracing in one time step: A new filopodium is added by interactively specifying the position of its tip. The software determines the filopodium base automatically and traces the path from the tip node to the root node automatically.</p></list-item><list-item><p id="P35">Filopodia tracking across time steps: All filopodia, both new and previously propagated ones, are automatically propagated to the next time step.</p></list-item><list-item><p id="P36">Proofreading: The traced paths and tip locations, including the filopodia bases, are visually verified and interactively corrected if necessary.</p></list-item></list></p><p id="P37">All steps must be repeated as many times as necessary to reconstruct all filopodia in the desired amount of time steps. The full reconstruction workflow can be seen in <xref ref-type="fig" rid="F6">Figure 6</xref>. The part of the workflow enframed with the red dotted line is described in more detail in <xref ref-type="fig" rid="F7">Figure 7</xref>. The following sections contain detailed description of each reconstruction step.</p><sec id="S13"><title>Filopodia tracing - path generation</title><p id="P38">The path between two nodes <italic>N</italic><sub><italic>B</italic></sub> and <italic>N</italic><sub><italic>T</italic></sub> is traced using an intensity-weighted Dijkstra shortest path algorithm [<xref ref-type="bibr" rid="R31">31</xref>]. A Dijkstra graph consists of nodes and weighted edges. Here, the set of nodes is given by the voxels contained in an axis-aligned box. This box is bounded by <italic>N</italic><sub><italic>B</italic></sub> and <italic>N</italic><sub><italic>T</italic></sub>, extended in each dimension by <italic>n</italic> voxels to account for potential shortest paths that are not contained within original box bounded by <italic>N</italic><sub><italic>B</italic></sub> and <italic>N</italic><sub><italic>T</italic></sub>. Empirically, we determined <italic>n</italic> = 10 to be a suitable value for our implementation. The set of edges consists of all connections between neighboring voxels (using the 26-neighborhood). The edge weight <italic>w</italic><sub><italic>ij</italic></sub> between two nodes <italic>v</italic><sub><italic>i</italic></sub> and <italic>v</italic><sub><italic>j</italic></sub> having intensity values <italic>I</italic><sub><italic>i</italic></sub> and <italic>I</italic><sub><italic>j</italic></sub> ∈ [0, 255] is defined as follows: <disp-formula id="FD1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>v</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>i</mml:mi></mml:mstyle></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>v</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>j</mml:mi></mml:mstyle></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mo>|</mml:mo><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mn>2</mml:mn><mml:mi>c</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>I</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>i</mml:mi></mml:mstyle><mml:mo>′</mml:mo></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>I</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>j</mml:mi></mml:mstyle><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>I</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>j</mml:mi></mml:mstyle></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mi>min</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <list list-type="bullet" id="L6"><list-item><p id="P39"><bold>v</bold><sub><bold>i</bold></sub>, <bold>v</bold><sub><bold>j</bold></sub> are node coordinates,</p></list-item><list-item><p id="P40"><inline-formula><mml:math id="M2"><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>I</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>i</mml:mi></mml:mstyle><mml:mo>′</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>I</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>j</mml:mi></mml:mstyle><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> are the intensity values of the voxels <bold>v</bold><sub><bold>i</bold></sub> and <bold>v</bold><sub><bold>j</bold></sub>, respectively, and</p></list-item><list-item><p id="P41"><bold>c</bold> is a user-adjustable intensity weight.</p></list-item></list></p><p id="P42">As can be seen in <xref ref-type="disp-formula" rid="FD1">Equation (1)</xref>, the weights <italic>w</italic><sub><italic>ij</italic></sub> are computed using a distance term between the nodes and an intensity term penalizing dark voxels. The user-adjustable intensity weight <bold>c</bold> balances these terms. Empirically, <bold>c</bold> = 50 has been found to be a suitable value for the sample data used during development. To avoid that long paths through bright voxels are preferred over slightly shorter paths with darker foreground voxels, the foreground intensity is capped at <italic>I</italic><sub><italic>max</italic></sub>, which is a user-adjustable parameter. Based on the tracing with many membrane-labeled filopodia in multiple datasets of various quality, we can assess that tracing from filopodial tips to root nodes is aided by brighter voxels along the path but also works robustly along paths that are only weakly labeled and have discontinuous voxel brightness.</p><p id="P43">To facilitate an efficient tracing from a point to the root node, Dijkstra graphs storing the shortest path tree (rooted at the axon terminal center) are pre-computed. The tree is encoded by storing the neighbour of each voxel that is next on the shortest path to the root. The shortest path from any voxel to the root can then be efficiently found by iteratively moving to the neighbor closest to the root until the root has been reached. To compute the path between a pair of non-root nodes, the Dijkstra graph is generated on the fly. This might be the case during graph corrections, for example.</p><p id="P44">To avoid coinciding paths, the algorithm performs an intersection test with the existing edges of the graph when adding a new path. Should an intersection be detected, a branching node is created. In case the branching node lies between the tip and the base, the branch integrates into the existing filopodium. If the filopodium branches between base and root node, it is treated as an independent filopodium.</p></sec><sec id="S14"><title>Filopodia tracing - base estimation</title><p id="P45">After tracing a filopodium from a tip to the root node, the location of the filopodium base is automatically determined. Here, the assumption is made that the 2D intensity profile in a plane orthogonal to the traced path has Gaussian shape for the filopodium part and is non-Gaussian (ideally uniform) inside the axon terminal body. The base location is specified as the point where the intensity profile changes from Gaussian to non-Gaussian (<xref ref-type="fig" rid="F8">Fig. 8c</xref>). The “Gaussian-ness” is estimated by computing the Root Mean Squared Deviation (RMSD) of radially sampled intensity values in a 2D plane orthogonal to the edge. Ideal Gaussian parameters are estimated from the sampled intensities. The RMSD error remains relatively constant inside a filopodium but increases drastically when entering the axon body (<xref ref-type="fig" rid="F8">Fig. 8c</xref>).</p></sec><sec id="S15"><title>Filopodia tracking</title><p id="P46">Filopodia are tracked over all time steps by automatic propagation. The propagation algorithm aims to detect the same filopodium in the successive time step. Filopodia tip and base are propagated using the same template matching algorithm used for propagating root nodes. Since root and base nodes rarely move, the search window for both node types can be small. In contrast, the filopodia tips show high motility. Therefore, the search window has to be adequately large to capture this movement. Template sizes differ as well since the high-intensity region around a tip will in general be much smaller than the high-intensity region around the base. Both search window and template size have to be determined experimentally and can be adjusted in the software. The paths between the nodes are traced as explained above in the <italic>Filopodia tracing</italic> section. The newly traced filopodium is assigned the same track ID as its “predecessor” in the previous time step.</p><p id="P47">In case a tip cannot be propagated (i.e. there is no location with NCC value above the threshold <italic>γ</italic>), the filopodium is considered to have retracted. If no base can be found, the location of the base in the previous time step is used. If the length of the path between base and tip is shorter than a user-specified threshold <italic>λ</italic>, the filopodium is also considered to have retracted, and is removed.</p><p id="P48">After propagation, the user verifies whether the automatically created traces are correct (see next paragraph). In addition, newly emerging filopodia are interactively added as before. These steps must be repeated until all time steps have been processed.</p></sec><sec id="S16"><title>Proofreading</title><p id="P49">The automated tracing and tracking are not always accurate and especially minor corrections are common. Hence, interactive verification and correction is an important part of the workflow and is especially facilitated by the software tool. The following parameters need to be validated when moving through successive time steps: <list list-type="bullet" id="L7"><list-item><p id="P50">Location of tip and base</p></list-item><list-item><p id="P51">Location of the traced path</p></list-item><list-item><p id="P52">Location of any intersection points found</p></list-item><list-item><p id="P53">Assignment of track ID</p></list-item></list></p><p id="P54">Dislocated base or tip nodes can be manually moved to new positions by selecting the node and clicking at the new location; incident edges are retraced automatically. Erroneous paths can be redirected by defining an additional supporting point or by selecting a point on an existing path; a branching node is then created at the inter-section location. Incorrect track IDs can be corrected by explicitly selecting filopodia in subsequent time steps and marking them as matched. This is only the case when multiple filopodia are close to each other.</p></sec><sec id="S17"><title>Workflow realization using low-level basic operations</title><p id="P55">The high-level tasks described above are realized by several basic operations that are implemented in the software: <list list-type="bullet" id="L8"><list-item><p id="P56"><italic>add filopodium</italic>: This operation is performed by defining a filopodia tip either in the 2D viewer (in either an orthogonal or an oblique slice) or in the 3D viewer using the volume rendering. The path is then automatically traced from the filopodium tip to the axon terminal center and the base is also automatically determined.</p></list-item><list-item><p id="P57"><italic>delete filopodium</italic>: This operation is performed when a filopodium has been inaccurately propagated.</p></list-item><list-item><p id="P58"><italic>move tip</italic>: Similar to the <italic>add filopodium</italic> operation, this operation can be done in the 2D or the 3D viewer.</p></list-item><list-item><p id="P59"><italic>move base</italic>: This operation can be done in the 2D viewer or in the 3D viewer. The base can be moved to a completely new location or dragged along the existing edge.</p></list-item><list-item><p id="P60"><italic>move edge</italic>: This operation is performed when the position of the traced path is found to be incorrect. In this case, further points can be added in 2D or 3D through which the path has to pass. The path is then retraced between the added points and the tip and base points.</p></list-item><list-item><p id="P61"><italic>propagate filopodia</italic>: This operation is performed to propagate all filopodia from the current time step to the next. The operation is required after finishing time step reconstruction and is rarely invoked otherwise.</p></list-item><list-item><p id="P62"><italic>match filopodium</italic>: This operation should be performed after adding a filopodium that was mistakenly not automatically propagated. The operation ensures that the same filopodium at consecutive time points has the same ID. It can also be used if filopodia in consecutive time steps are wrongly matched.</p></list-item></list></p></sec></sec><sec id="S18"><label>2.3.3</label><title>Statistical analysis</title><p id="P63">The final result of the geometric reconstruction is a 4D skeleton graph representing the axon terminal and all filopodia for each time step. The following quantities are computed from the graph: <list list-type="bullet" id="L9"><list-item><p id="P64">Length of each filopodium in each time step</p></list-item><list-item><p id="P65">Number of extension and retraction events</p></list-item><list-item><p id="P66">Mean extension and retraction velocities</p></list-item><list-item><p id="P67">Mean length and lifetime for each filopodium (through different time steps)</p></list-item><list-item><p id="P68">Filopodia growth angles</p></list-item></list></p><p id="P69">The length is calculated as the sum of the Euclidean distances between consecutive edge points on the traced path. In case a filopodium branches between tip and base, the length of the branch is added to the filopodium length. From changes in length over time, we compute the number of extensions (filopodium is longer than in the previous time step) and retractions (filopodium is shorter than in the previous time step). The filopodium angle is defined as the angle between the unit vector and a vector spanning from the root to the base node at the time of their initial formation. The angle is computed as: <disp-formula id="FD2"><label>(2)</label><mml:math id="M3"><mml:mrow><mml:mi>cos</mml:mi><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>&lt;</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&gt;</mml:mo></mml:mrow><mml:mo stretchy="true">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mo>|</mml:mo><mml:mo>∗</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="true">→</mml:mo></mml:mover></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <inline-formula><mml:math id="M4"><mml:mrow><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>R</mml:mi><mml:mi>B</mml:mi></mml:mrow><mml:mo stretchy="true">→</mml:mo></mml:mover><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> is the vector spanned from the root node <italic>N</italic><sub><italic>R</italic></sub> (axon terminal center) to the base node <italic>N</italic><sub><italic>B</italic></sub> of the filopodium, both projected onto the x-y plane and <inline-formula><mml:math id="M5"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="true">→</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> is the unit vector.</p><p id="P70">Since the coordinates of all nodes (root, base, tip) are known, the calculation of other angles can easily be performed. The mentioned filopodia growth angle is of interest as it provides information about the distribution of filopodia (bases) around the axon terminal. All computed quantities are stored in spreadsheets, which allow further validations and analyses with other tools, such as <italic>Python, MATLAB</italic> or <italic>R</italic>.</p></sec></sec></sec><sec id="S19" sec-type="results"><label>3</label><title>Results</title><p id="P71">The semi-automatic workflow was devised to fulfill two main requirements: first, to facilitate efficient 3D reconstruction of neuronal axon terminals and their filopodia; second, applicability to a wide range of datasets. To benchmark the efficiency of the method, we analyzed the time necessary for dataset preprocessing. To test applicability, we tested the method on a neuronal cell type with very different and much more elaborate axo-dendritic morphology.</p><sec id="S20"><title>Analysis of preprocessing</title><p id="P72">As described above, the necessary preprocessing includes the determination of root nodes, image cropping, and the computation of the Dijkstra graphs. <xref ref-type="table" rid="T2">Table 2</xref> shows the time taken for different steps of the preprocessing for a single user. In the data used for tool and workflow development, the entire preprocessing only took 3-4 minutes per dataset, while the actual filopodia tracing and tracking can be on the scale of hours.</p></sec><sec id="S21"><title>Workflow application to alternative cell types</title><p id="P73">The workflow was originally developed for <italic>Drosophila</italic> R7 axon terminals, which have a compact morphology. They have a single bulbous axon terminal with radially extending, relatively clearly discernible filopodial protrusion. To test the applicability of the workflow for different morpohologies, we also analyzed <italic>Drosophila</italic> Dm8 cells and a simulated A549-SIM cell.</p><sec id="S22"><title>Dm8 cell</title><p id="P74">Amacrine-like interneurons Dm8 exhibit an extended and mixed axo-dendritic branched morphology with extensive filopodial dynamics across the entire branched structure. We set the base node in the center of this branching structure and propagated it through the time steps just as in the case of R7 cells. Both cells can be seen and visually compared in <xref ref-type="fig" rid="F9">Figure 9</xref>.</p><p id="P75">The processes of tracing and tracking filopodial of Dm8s are very similar to R7 neurons. However, since Dm8 filopodia tend to be more branched than R7, they often cross and overlap. This can cause the software to trace a path from the tip to the root node that does not correspond to the traced filopodium. In such cases, it is necessary to carefully proofread the tracing results and correct potentially wrong traced paths using the implemented correction tools.</p><p id="P76">Reconstruction of the datasets was done by four neurobiologists without formal training in computational image analysis. We logged the overall time required for tracing, tracking and proofreading and used this data to analyze the required time as a function of filopodia number, lifetime and length (<xref ref-type="fig" rid="F10">Fig. 10</xref>).</p><p id="P77">These analyses indicate that the average working time for reconstructing a cell with 50 to 200 filopodia is approximately 3 to 5 hours, independent of the cell type. The average lifetime and length of filopodia do not correlate with reconstruction time. In a typical time step, the user works with 5 to 15 different filopodia. A video of an R7 cell, reconstructed in all time steps, is provided in the <xref ref-type="supplementary-material" rid="SD1">supplementary material (Video1)</xref>.</p></sec><sec id="S23"><title>Simulated cell</title><p id="P78">To further show that the workflow can be used for diverse cells, we have applied the workflow to the simulated cell A549-SIM from the <italic>celltrackingchallenge.net</italic>. It is important to note that the cell has a significantly different morphology and labeling compared to the R7 or Dm8 cells. The cell body is round, the inside of the cell body contains much darker values than its border, and the filopodia are extending from it linearly. Nevertheless, tracing worked robustly with the difference that traces between base nodes and root node can take unconventional paths due to the cell body being much brighter at its borders. Importantly, the tracing of filopodia worked comparably to the R7 and Dm8 neurons. <xref ref-type="fig" rid="F11">Figure 11</xref> shows a volume rendering of the cell, a cross section through its center, and the ‘filopodia’ reconstruction in a single time step.</p><p id="P79">We evaluated the reconstruction results by calculating the percentage of filopodia traces lying within the segmentation mask, which is 85% on average over all time steps. The missing 15% are all close to the base where the path is distracted due to the characteristic of the cell having bright values at the cell border but much darker values inside the cell.</p></sec></sec></sec><sec id="S24" sec-type="discussion"><label>4</label><title>Discussion</title><sec id="S25"><title>Time requirements and applicability</title><p id="P80">The first step in the workflow is the preprocessing. The specific time needed for preprocessing depends on the dataset; in particular, the number of separate structures in a single dataset, e.g. axon terminals, results in longer manual validation times, more elaborate image cropping and Dijkstra graph calculations. The size of the axonal or branched structure also affects the preprocessing time, as larger axon structures (and hence larger cropped images) require longer calculations of the Dijkstra graphs. Both model structures analyzed here had similar preprocessing time requirements. Compared to the rest of the workflow, the preprocessing usually takes less than 5% of the total required time.</p><p id="P81">The software tool was tested by four neurobiologists who reconstructed 17 neurons of two highly divergent morphologies, namely R7 and Dm8 cells in the <italic>Drosophila</italic> visual system. We compared the times necessary for reconstruction (<xref ref-type="fig" rid="F10">Fig. 10</xref>). On average 3 to 5 hours were necessary to reconstruct a single cell (R7 or Dm8) in 60 time steps. Neither filopodia number, lifetimes nor lengths had a significant influence on reconstruction time. The time needed for processing the automated steps is negligible compared to the time required for the manual steps.</p><p id="P82">We note that different users varied significantly in their reconstruction approaches, affecting necessary reconstruction times. To minimize bias, we applied a standardized user training protocol. However, even after training, users still varied significantly, especially with respect to the amount of time taken to validate node positions and in spotting new filopodia. Based on the difficulty of avoiding user bias, we suggest that for a given study, a single person should perform all the necessary reconstructions and thus eliminate inter-user variability. This approach is useful when relative changes between a control and experiments are more important than absolute values. When absolute values of measurements are relevant, biological criteria have to be quantitatively restricted to avoid inter-user variability.</p><p id="P83">When reconstructing a A549-SIM cell, the algorithm that generates the Dijkstra graphs needed tracing parameter adjustments to ensure the generation of satisfactory traces. We also noted that the estimation of base node locations is poorer and that they needed to be corrected more often than for the R7 or Dm8 cells. Our assumption is that the hollow body of the cell, despite the parameter adjustments, still contributes to the atypical graph, causing unconventional path generation within the cell body (as can be seen in <xref ref-type="fig" rid="F11">Fig. 11</xref>). The cell body intensity distribution and the unconventional paths cause the base node to be estimated poorly. This also explains the average of 85% overlap between the traces and the segmentation mask, where the majority of the non-overlapping traces lie around the bases of the filopodia.</p></sec><sec id="S26"><title>Key features of the software</title><p id="P84">Our software tool was developed for tracing and tracking filopodial structures in 4D (3D+time). However, it can be applied to many different types of neuronal cells and subcellular structures without modifications. The underlying semi-automatic approach does not require training data. Apart from functioning as a stand-alone tool, the software can also serve as a means to create training data for fully automatic approaches based, for example, using deep learning [<xref ref-type="bibr" rid="R23">23</xref>].</p><p id="P85">Previously proposed solutions focused on 2D analysis to achieve ease of use and produce results comparably fast [<xref ref-type="bibr" rid="R17">17</xref>]. However, filopodia are inherently three-dimensional structures and often change growth directions in space; 2D analysis is thus likely to underestimate filopodial lengths and incorrectly represent growth directions in the observed plane. Our focus was to capture the three-dimensional movement of dynamic subcellular structures in time and provide all quantitative data of the underlying dynamics.</p><p id="P86">To facilitate the tracing and tracking in three dimensions over time, a key strength of our workflow is the ability to propagate filopodia through consecutive time steps. The user only has to validate or correct the propagated filopodia and add newly emerging filopodia when they originate. In addition, tracing of filopodial structures in space is done automatically using Dijkstra graphs; here, users only have to select the tip of a filopodium and subsequently validate the automized tracing and positioning of the base node. Results of the analysis are graphs with filopodia represented as branches originating from a root node. The representation using graphs allows practical 3D visualization and analysis of filopodia including growth direction, angle, length and branching.</p><p id="P87">All steps of the proposed workflow are supported by proofreading tools that allow the user to add, remove or modify the traced and tracked filopodia to ensure high quality output data.</p></sec><sec id="S27"><title>Possible improvements of the software</title><p id="P88">Validation and proofreading represent the most significant manual work and time investment using our workflow. Hence, improvements of tracing and tracking accuracy as well as facilitation of proofreading would most significantly reduce processing time. In addition, such improvement would simultaneously reduce inter-user variability.</p><p id="P89">The template matching algorithm used in the root node propagation works best for the time steps immediately following the first one. If a root node is poorly estimated for a specific time step, this error will be propagated through all the following time steps. However, for thin filopodia, the root node location does not significantly affect tracing. Future tool development may include the analysis of the dynamics of the filopodia bearing structures (in our case axon terminals or axo-dendritic branched structures). Possible implementations could include elastic image registration or the use of machine learning.</p><p id="P90">Finally, significant workflow improvements are possible by improving the interface based on feedback from experienced users. For example, a more interactive way of entering corrections of filopodial nodes or tracks using a simple “drag-and-drop” approach could replace the manual selection of an object and moving it to a new location through the graphical interface.</p></sec><sec id="S28"><title>Tool’s significance for cellular and developmental neuroscience</title><p id="P91">The development and application of advanced 4D live image acquisition in neuroscience, as elsewhere, has increased substantially in recent years. New microscopy and non-invasive imaging technologies allow to obtain live imaging data of three-dimensional structures with increasing temporal resolution. However, 4D data analysis and interpretation have remained far behind these developments. For example, tracking of subcellular structures in three-dimensional data over time has remained a largely unmet challenge and currently represents a substantial barrier to progress [<xref ref-type="bibr" rid="R32">32</xref>]. Similarly, the extraction of principles from 4D data typically requires quantitative data analysis for subsequent modeling.</p><p id="P92">All across cell biology, dynamic processes in three dimensions underlie mechanisms that may not be fully understood based on fixed data. Neurons in particular are highly polarized cells with long-range trafficking and morphogenetic dynamics that play critical roles during development and function. In this work, we focused on the dynamics of axonal growth cones and axo-dendritic arborizations, structures whose dynamic properties may directly influence the development of brain wiring [<xref ref-type="bibr" rid="R1">1</xref>]. For example, the development of brain wiring is still mostly studied based on adult outcomes, rather than during the dynamic period, before, during and after synaptic partner choice, which is certainly, at least in part, due to the technical challenges involved [<xref ref-type="bibr" rid="R33">33</xref>].</p></sec></sec><sec id="S29" sec-type="conclusions"><label>5</label><title>Conclusion</title><p id="P93">We developed a semi-automatic workflow for the time-efficient tracing and tracking of filopodia in 4D microscopy data (3D+time). In this workflow, most of the tasks are automized and the majority of the user input is restricted to validating parameters and results. The implemented software tool supports this proofreading by providing several options for interactive corrections. Finally, filopodial structures, like the ones at the center of this study, are not restricted to neurons. Many motile cell types utilize filopodial dynamics to explore and move in their environments. While many in vitro studies focused on cellular movement in artificial two-dimensional environments, the migration and underlying dynamics of motile cells are now increasingly accessible in the three-dimensional organismal context. The tool developed here was devised to facilitate such a diverse range of applications. Its application to the R7, Dm8 and A549-SIM datasets showed robust function and wide ranging applicability for filopodial tracing across different datasets. The tool is publicly available at <ext-link ext-link-type="uri" xlink:href="https://github.com/zibamira/filopodia-tool">https://github.com/zibamira/filopodia-tool</ext-link> along with the user manual and example datasets.</p></sec><sec id="S30"><label>6</label><title>Availability and requirements</title><p id="P94"><bold>Project name:</bold> 3D filopodia tracing and tracking tool</p><p id="P95"><bold>Project home page:</bold> <ext-link ext-link-type="uri" xlink:href="https://github.com/zibamira/filopodia-tool">https://github.com/zibamira/filopodia-tool</ext-link></p><p id="P96"><bold>Operating system(s):</bold> Platform independent</p><p id="P97"><bold>Programming language:</bold> C++</p><p id="P98"><bold>Other requirements:</bold> Development version of Amira software</p><p id="P99"><bold>License:</bold> MIT-style</p><p id="P100"><bold>Any restrictions to use by non-academics:</bold> None</p></sec><sec id="S31"><label>7</label><title>List of abbreviations</title><glossary><def-list><def-item><term>2D</term><def><p id="P101">Two dimensions or two-dimensional</p></def></def-item><def-item><term>3D</term><def><p id="P102">Three dimensions or three-dimensional</p></def></def-item><def-item><term>4D</term><def><p id="P103">Four dimensions or four-dimensional</p></def></def-item><def-item><term>CTS</term><def><p id="P104">Contour Tree Segmentation</p></def></def-item><def-item><term>NCC</term><def><p id="P105">Normalized Cross Correlation</p></def></def-item><def-item><term>RMSD</term><def><p id="P106">Root Mean Squared Deviation</p></def></def-item></def-list></glossary></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>A video of an R7 cell, reconstructed in all time steps, is provided in the supplementary material</label><media xlink:href="EMS205784-supplement-A_video_of_an_R7_cell__reconstructed_in_all_time_steps__is_provided_in_the_supplementary_material.mpg" mimetype="video" mime-subtype="mpeg" id="d7aAcHbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S32"><title>Acknowledgements</title><p>We would like to thank all members of the Baum and Hiesinger labs as well as members of the Research Consortium <italic>RobustCircuit</italic> for discussion. We are particularly grateful to Joachim Fuchs for providing feedback to the manuscript and Justus Vogel for supporting the development of the Amira software.</p><sec id="S33"><title>Funding</title><p>This work was supported by grant from the German Research Foundation (DFG) to B.B., D.B. and P.R.H (Research Unit 5289 RobustCircuit, project Z1) and J.B. (CRC 958). P.R.H. was further funded by the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement no. 101019191).</p></sec></ack><sec id="S34" sec-type="data-availability"><title>Availability of data and materials</title><p id="P107">Two example datasets are provided in the online repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/zibamira/filopodia-tool">https://github.com/zibamira/filopodia-tool</ext-link>). Additional datasets and results can be provided upon request.</p></sec><fn-group><label>8</label><title>Declarations</title><fn id="FN1"><p id="P108"><bold>Ethics approval and consent to participate</bold></p><p id="P109">Not applicable.</p></fn><fn id="FN2"><p id="P110"><bold>Consent for publication</bold></p><p id="P111">Not applicable.</p></fn><fn fn-type="conflict" id="FN3"><p id="P112"><bold>Competing interests</bold></p><p id="P113">The authors declare that they have no competing interests</p></fn><fn fn-type="con" id="FN4"><p id="P114"><bold>Authors’ contributions</bold></p><p id="P115">BB: Methodology, software, formal analysis, writing (original draft, review &amp; editing)</p><p id="P116">JB: Methodology, software, formal analysis, writing (original draft, review &amp; editing)</p><p id="P117">VJD: Software, conceptualization, supervision, writing (review &amp; editing)</p><p id="P118">MNÖ : Data curation, methodology, investigation, writing (review &amp; editing)</p><p id="P119">AK: Data curation, methodology, investigation, writing (review &amp; editing)</p><p id="P120">NW: Data curation, methodology, investigation, writing (review &amp; editing)</p><p id="P121">SP: Conceptualization, funding acquisition, project administration, supervision, writing (review &amp; editing)</p><p id="P122">PRH: Conceptualization, funding acquisition, project administration, writing (review &amp; editing)</p><p id="P123">DB: Methodology, software, formal analysis, conceptualization, funding acquisition, supervision, writing (review &amp; editing)</p></fn></fn-group><ref-list><ref id="R1"><label>[1]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wit</surname><given-names>CB</given-names></name><name><surname>Hiesinger</surname><given-names>PR</given-names></name></person-group><article-title>Neuronal filopodia: From stochastic dynamics to robustness of brain morphogenesis</article-title><source>Seminars in Cell &amp; Developmental Biology</source><year>2023</year><volume>133</volume><fpage>10</fpage><lpage>19</lpage><pub-id pub-id-type="pmid">35397971</pub-id></element-citation></ref><ref id="R2"><label>[2]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischer</surname><given-names>RS</given-names></name><name><surname>Lam</surname><given-names>P-Y</given-names></name><name><surname>Huttenlocher</surname><given-names>A</given-names></name><name><surname>Waterman</surname><given-names>CM</given-names></name></person-group><article-title>Filopodia and focal adhesions: An integrated system driving branching morphogenesis in neuronal pathfinding and angiogenesis</article-title><source>Developmental biology</source><year>2019</year><volume>451</volume><issue>1</issue><fpage>86</fpage><lpage>95</lpage><pub-id pub-id-type="pmcid">PMC7082808</pub-id><pub-id pub-id-type="pmid">30193787</pub-id><pub-id pub-id-type="doi">10.1016/j.ydbio.2018.08.015</pub-id></element-citation></ref><ref id="R3"><label>[3]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blake</surname><given-names>T</given-names></name><name><surname>Gallop</surname><given-names>J</given-names></name></person-group><article-title>Filopodia in vitro and in vivo</article-title><source>Annual review of cell and developmental biology</source><year>2023</year><volume>39</volume><pub-id pub-id-type="pmid">37406300</pub-id></element-citation></ref><ref id="R4"><label>[4]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Özel</surname><given-names>MN</given-names></name><name><surname>Langen</surname><given-names>M</given-names></name><name><surname>Hassan</surname><given-names>BA</given-names></name><name><surname>Hiesinger</surname><given-names>PR</given-names></name></person-group><article-title>Filopodial dynamics and growth cone stabilization in <italic>Drosophila</italic> visual circuit development</article-title><source>eLife</source><year>2015</year><volume>4</volume><elocation-id>10721</elocation-id><pub-id pub-id-type="pmcid">PMC4728134</pub-id><pub-id pub-id-type="pmid">26512889</pub-id><pub-id pub-id-type="doi">10.7554/eLife.10721</pub-id></element-citation></ref><ref id="R5"><label>[5]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Agi</surname><given-names>E</given-names></name><name><surname>Reifenstein</surname><given-names>ET</given-names></name><name><surname>Wit</surname><given-names>C</given-names></name><name><surname>Schneider</surname><given-names>T</given-names></name><name><surname>Kauer</surname><given-names>M</given-names></name><name><surname>Kehribar</surname><given-names>M</given-names></name><name><surname>Kulkarni</surname><given-names>A</given-names></name><name><surname>Kleist</surname><given-names>M</given-names></name><name><surname>Hiesinger</surname><given-names>PR</given-names></name></person-group><article-title>Axonal self-sorting without target guidance in drosophila visual map formation</article-title><source>Science</source><year>2024</year><volume>383</volume><issue>6687</issue><fpage>1084</fpage><lpage>1092</lpage><pub-id pub-id-type="pmid">38452066</pub-id></element-citation></ref><ref id="R6"><label>[6]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Santos</surname><given-names>T</given-names></name><name><surname>Schaffran</surname><given-names>B</given-names></name><name><surname>Broguiere</surname><given-names>N</given-names></name><name><surname>Meyn</surname><given-names>L</given-names></name><name><surname>Zenobi</surname><given-names>M</given-names></name><name><surname>Bradke</surname><given-names>F</given-names></name></person-group><article-title>Axon growth of cns neurons in three dimensions is amoeboid and independent of adhesions</article-title><source>Cell Reports</source><year>2020</year><volume>32</volume><elocation-id>107907</elocation-id><pub-id pub-id-type="pmid">32698008</pub-id></element-citation></ref><ref id="R7"><label>[7]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adler</surname><given-names>A</given-names></name><name><surname>Lai</surname><given-names>CSW</given-names></name><name><surname>Yang</surname><given-names>G</given-names></name><name><surname>Geron</surname><given-names>E</given-names></name><name><surname>Bai</surname><given-names>Y</given-names></name><name><surname>Gan</surname><given-names>W-B</given-names></name></person-group><article-title>Sleep promotes the formation of dendritic filopodia and spines near learning-inactive existing spines</article-title><source>Proceedings of the National Academy of Sciences</source><year>2021</year><volume>118</volume><issue>50</issue><elocation-id>2114856118</elocation-id><pub-id pub-id-type="pmcid">PMC8685900</pub-id><pub-id pub-id-type="pmid">34873044</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2114856118</pub-id></element-citation></ref><ref id="R8"><label>[8]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hedrick</surname><given-names>NG</given-names></name><name><surname>Lu</surname><given-names>Z</given-names></name><name><surname>Bushong</surname><given-names>EA</given-names></name><name><surname>Singhi</surname><given-names>S</given-names></name><name><surname>Nguyen</surname><given-names>P</given-names></name><name><surname>Maganã</surname><given-names>Y</given-names></name><name><surname>Jilani</surname><given-names>S</given-names></name><name><surname>Lim</surname><given-names>BK</given-names></name><name><surname>Ellisman</surname><given-names>M</given-names></name><name><surname>Komiyama</surname><given-names>T</given-names></name></person-group><article-title>Learning binds new inputs into functional synaptic clusters via spinogenesis</article-title><source>Nature Neuroscience</source><year>2022</year><volume>25</volume><fpage>726</fpage><lpage>737</lpage><pub-id pub-id-type="pmid">35654957</pub-id></element-citation></ref><ref id="R9"><label>[9]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Özel</surname><given-names>MN</given-names></name><name><surname>Kulkarni</surname><given-names>A</given-names></name><name><surname>Hasan</surname><given-names>A</given-names></name><name><surname>Brummer</surname><given-names>J</given-names></name><name><surname>Moldenhauer</surname><given-names>M</given-names></name><name><surname>Daumann</surname><given-names>I-M</given-names></name><name><surname>Wolfenberg</surname><given-names>H</given-names></name><name><surname>Dercksen</surname><given-names>VJ</given-names></name><name><surname>Kiral</surname><given-names>FR</given-names></name><name><surname>Weiser</surname><given-names>M</given-names></name><name><surname>Prohaska</surname><given-names>S</given-names></name><etal/></person-group><article-title>Serial synapse formation through filopodial competition for synaptic seeding factors</article-title><source>Developmental Cell</source><year>2019</year><volume>50</volume><issue>4</issue><fpage>447</fpage><lpage>4618</lpage><pub-id pub-id-type="pmcid">PMC6702111</pub-id><pub-id pub-id-type="pmid">31353313</pub-id><pub-id pub-id-type="doi">10.1016/j.devcel.2019.06.014</pub-id></element-citation></ref><ref id="R10"><label>[10]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiral</surname><given-names>FR</given-names></name><name><surname>Linneweber</surname><given-names>GA</given-names></name><name><surname>Georgiev</surname><given-names>SV</given-names></name><name><surname>Hassan</surname><given-names>BA</given-names></name><name><surname>Kleist</surname><given-names>M</given-names></name><name><surname>Hiesinger</surname><given-names>PR</given-names></name></person-group><article-title>Autophagy-dependent filopodial kinetics restrict synaptic partner choice during drosophila brain wiring</article-title><source>bioRxiv</source><year>2020</year><pub-id pub-id-type="pmcid">PMC7067798</pub-id><pub-id pub-id-type="pmid">32165611</pub-id><pub-id pub-id-type="doi">10.1038/s41467-020-14781-4</pub-id></element-citation></ref><ref id="R11"><label>[11]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiral</surname><given-names>FR</given-names></name><name><surname>Dutta</surname><given-names>SB</given-names></name><name><surname>Linneweber</surname><given-names>GA</given-names></name><name><surname>Hilgert</surname><given-names>S</given-names></name><name><surname>Poppa</surname><given-names>C</given-names></name><name><surname>Duch</surname><given-names>C</given-names></name><name><surname>von Kleist</surname><given-names>M</given-names></name><name><surname>Hassan</surname><given-names>BA</given-names></name><name><surname>Hiesinger</surname><given-names>PR</given-names></name></person-group><article-title>Brain connectivity inversely scales with developmental temperature in drosophila</article-title><source>Cell Reports</source><year>2021</year><volume>37</volume><issue>12</issue><elocation-id>110145</elocation-id><pub-id pub-id-type="pmid">34936868</pub-id></element-citation></ref><ref id="R12"><label>[12]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Styner</surname><given-names>M</given-names></name><name><surname>Gerig</surname><given-names>G</given-names></name><name><surname>Lieberman</surname><given-names>JA</given-names></name><name><surname>Jones</surname><given-names>D</given-names></name><name><surname>Weinberger</surname><given-names>DR</given-names></name></person-group><article-title>Statistical shape analysis of neuroanatomical structures based on medial models</article-title><source>Medical image analysis</source><year>2003</year><volume>73</volume><fpage>207</fpage><lpage>20</lpage><pub-id pub-id-type="pmid">12946464</pub-id></element-citation></ref><ref id="R13"><label>[13]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsygankov</surname><given-names>D</given-names></name><name><surname>Bilancia</surname><given-names>CG</given-names></name><name><surname>Vitriol</surname><given-names>EA</given-names></name><name><surname>Hahn</surname><given-names>KM</given-names></name><name><surname>Peifer</surname><given-names>M</given-names></name><name><surname>Elston</surname><given-names>TC</given-names></name></person-group><article-title>Cellgeo: A computational platform for the analysis of shape changes in cells with complex geometries</article-title><source>The Journal of Cell Biology</source><year>2014</year><volume>204</volume><fpage>443</fpage><lpage>460</lpage><pub-id pub-id-type="pmcid">PMC3912527</pub-id><pub-id pub-id-type="pmid">24493591</pub-id><pub-id pub-id-type="doi">10.1083/jcb.201306067</pub-id></element-citation></ref><ref id="R14"><label>[14]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barry</surname><given-names>DJ</given-names></name><name><surname>Durkin</surname><given-names>CH</given-names></name><name><surname>Abella</surname><given-names>JVG</given-names></name><name><surname>Way</surname><given-names>M</given-names></name></person-group><article-title>Open source software for quantification of cell migration, protrusions, and fluorescence intensities</article-title><source>The Journal of Cell Biology</source><year>2015</year><volume>209</volume><fpage>163</fpage><lpage>180</lpage><pub-id pub-id-type="pmcid">PMC4395480</pub-id><pub-id pub-id-type="pmid">25847537</pub-id><pub-id pub-id-type="doi">10.1083/jcb.201501081</pub-id></element-citation></ref><ref id="R15"><label>[15]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Urbançiç</surname><given-names>V</given-names></name><name><surname>Butler</surname><given-names>R</given-names></name><name><surname>Richier</surname><given-names>B</given-names></name><name><surname>Peter</surname><given-names>M</given-names></name><name><surname>Mason</surname><given-names>J</given-names></name><name><surname>Livesey</surname><given-names>FJ</given-names></name><name><surname>Holt</surname><given-names>CE</given-names></name><name><surname>Gallop</surname><given-names>JL</given-names></name></person-group><article-title>Filopodyan: An open-source pipeline for the analysis of filopodia</article-title><source>The Journal of Cell Biology</source><year>2017</year><volume>216</volume><fpage>3405</fpage><lpage>3422</lpage><pub-id pub-id-type="pmcid">PMC5626553</pub-id><pub-id pub-id-type="pmid">28760769</pub-id><pub-id pub-id-type="doi">10.1083/jcb.201705113</pub-id></element-citation></ref><ref id="R16"><label>[16]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nilufar</surname><given-names>S</given-names></name><name><surname>Morrow</surname><given-names>A</given-names></name><name><surname>Lee</surname><given-names>J</given-names></name><name><surname>Perkins</surname><given-names>TJ</given-names></name></person-group><article-title>Filodetect: automatic detection of filopodia from fluorescence microscopy images</article-title><source>BMC Syst Biol</source><year>2013</year><volume>7</volume><fpage>66</fpage><pub-id pub-id-type="pmcid">PMC3726292</pub-id><pub-id pub-id-type="pmid">23880086</pub-id><pub-id pub-id-type="doi">10.1186/1752-0509-7-66</pub-id></element-citation></ref><ref id="R17"><label>[17]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacquemet</surname><given-names>G</given-names></name><name><surname>Paatero</surname><given-names>I</given-names></name><name><surname>Carisey</surname><given-names>AF</given-names></name><name><surname>Padzik</surname><given-names>A</given-names></name><name><surname>Orange</surname><given-names>JS</given-names></name><name><surname>Hamidi</surname><given-names>H</given-names></name><name><surname>Ivaska</surname><given-names>J</given-names></name></person-group><article-title>Filoquant reveals increased filopodia density during breast cancer progression</article-title><source>The Journal of Cell Biology</source><year>2017</year><volume>216</volume><fpage>3387</fpage><lpage>3403</lpage><pub-id pub-id-type="pmcid">PMC5626550</pub-id><pub-id pub-id-type="pmid">28765364</pub-id><pub-id pub-id-type="doi">10.1083/jcb.201704045</pub-id></element-citation></ref><ref id="R18"><label>[18]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saha</surname><given-names>T</given-names></name><name><surname>Rathmann</surname><given-names>I</given-names></name><name><surname>Viplav</surname><given-names>A</given-names></name><name><surname>Panzade</surname><given-names>S</given-names></name><name><surname>Begemann</surname><given-names>I</given-names></name><name><surname>Rasch</surname><given-names>C</given-names></name><name><surname>Klingauf</surname><given-names>J</given-names></name><name><surname>Matis</surname><given-names>M</given-names></name><name><surname>Galic</surname><given-names>M</given-names></name></person-group><article-title>Automated analysis of filopodial length and spatially resolved protein concentration via adaptive shape tracking</article-title><source>Molecular Biology of the Cell</source><year>2016</year><volume>27</volume><fpage>3616</fpage><lpage>3626</lpage><pub-id pub-id-type="pmcid">PMC5221593</pub-id><pub-id pub-id-type="pmid">27535428</pub-id><pub-id pub-id-type="doi">10.1091/mbc.E16-06-0406</pub-id></element-citation></ref><ref id="R19"><label>[19]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hendricusdottir</surname><given-names>R</given-names></name><name><surname>Bergmann</surname><given-names>JHM</given-names></name></person-group><article-title>F-dynamics: Automated quantification of dendrite filopodia dynamics in living neurons</article-title><source>Journal of Neuroscience Methods</source><year>2014</year><volume>236</volume><fpage>148</fpage><lpage>156</lpage><pub-id pub-id-type="pmid">25158318</pub-id></element-citation></ref><ref id="R20"><label>[20]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fanti</surname><given-names>Z</given-names></name><name><surname>Martinez-Perez</surname><given-names>ME</given-names></name><name><surname>De-Miguel</surname><given-names>FF</given-names></name></person-group><article-title>Neurongrowth, a software for automatic quantification of neurite and filopodial dynamics from time-lapse sequences of digital images</article-title><source>Developmental Neurobiology</source><year>2011</year><volume>71</volume><pub-id pub-id-type="pmid">21913334</pub-id></element-citation></ref><ref id="R21"><label>[21]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Costantino</surname><given-names>S</given-names></name><name><surname>Kent</surname><given-names>CB</given-names></name><name><surname>Godin</surname><given-names>AG</given-names></name><name><surname>Kennedy</surname><given-names>TE</given-names></name><name><surname>Wiseman</surname><given-names>PW</given-names></name><name><surname>Fournier</surname><given-names>AE</given-names></name></person-group><article-title>Semi-automated quantification of filopodial dynamics</article-title><source>Journal of Neuroscience Methods</source><year>2008</year><volume>171</volume><fpage>165</fpage><lpage>173</lpage><pub-id pub-id-type="pmid">18394712</pub-id></element-citation></ref><ref id="R22"><label>[22]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Li</surname><given-names>Q</given-names></name><name><surname>Liu</surname><given-names>L</given-names></name><name><surname>Zhou</surname><given-names>Z</given-names></name><name><surname>Ruan</surname><given-names>Z</given-names></name><name><surname>Kong</surname><given-names>L</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Zhong</surname><given-names>N</given-names></name><name><surname>Chai</surname><given-names>R</given-names></name><name><surname>Luo</surname><given-names>X</given-names></name><etal/></person-group><article-title>Teravr empowers precise reconstruction of complete 3-d neuronal morphology in the whole brain</article-title><source>Nature Communications</source><year>2019</year><volume>10</volume><pub-id pub-id-type="pmcid">PMC6677772</pub-id><pub-id pub-id-type="pmid">31375678</pub-id><pub-id pub-id-type="doi">10.1038/s41467-019-11443-y</pub-id></element-citation></ref><ref id="R23"><label>[23]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castilla</surname><given-names>C</given-names></name><name><surname>Maška</surname><given-names>M</given-names></name><name><surname>Sorokin</surname><given-names>DV</given-names></name><name><surname>Meijering</surname><given-names>EHW</given-names></name><name><surname>Ortíz-de-Solórzano</surname><given-names>C</given-names></name></person-group><article-title>3-d quantification of filopodia in motile cancer cells</article-title><source>IEEE Transactions on Medical Imaging</source><year>2019</year><volume>38</volume><fpage>862</fpage><lpage>872</lpage><pub-id pub-id-type="pmid">30296215</pub-id></element-citation></ref><ref id="R24"><label>[24]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lefevre</surname><given-names>JG</given-names></name><name><surname>Koh</surname><given-names>YWH</given-names></name><name><surname>Wall</surname><given-names>AA</given-names></name><name><surname>Condon</surname><given-names>ND</given-names></name><name><surname>Stow</surname><given-names>JL</given-names></name><name><surname>Hamilton</surname><given-names>NA</given-names></name></person-group><article-title>Llama: a robust and scalable machine learning pipeline for analysis of cell surface projections in large scale 4d microscopy data</article-title><source>bioRxiv</source><year>2020</year><comment><ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/early/2020/12/11/2020.12.10.420414.full.pdf">https://www.biorxiv.org/content/early/2020/12/11/2020.12.10.420414.full.pdf</ext-link></comment><pub-id pub-id-type="pmcid">PMC8375126</pub-id><pub-id pub-id-type="pmid">34412593</pub-id><pub-id pub-id-type="doi">10.1186/s12859-021-04324-z</pub-id></element-citation></ref><ref id="R25"><label>[25]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Offord</surname><given-names>E</given-names></name><name><surname>Lutton</surname><given-names>J</given-names></name><name><surname>Bretschneider</surname><given-names>T</given-names></name></person-group><source>Heterogeneous graph neural networks for analysing spatio-temporal cell surface dynamics</source><conf-name>2024 IEEE International Symposium on Biomedical Imaging (ISBI)</conf-name><year>2024</year><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1109/ISBI56570.2024.10635109</pub-id></element-citation></ref><ref id="R26"><label>[26]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Stalling</surname><given-names>D</given-names></name><name><surname>Westerhoff</surname><given-names>M</given-names></name><name><surname>Hege</surname><given-names>H-C</given-names></name></person-group><chapter-title>Amira: a highly interactive system for visual data analysis</chapter-title><source>The Visualization Handbook</source><year>2005</year><fpage>749</fpage><lpage>767</lpage><publisher-name>Elsevier</publisher-name><pub-id pub-id-type="doi">10.1016/B978-012387582-2/50040-X</pub-id></element-citation></ref><ref id="R27"><label>[27]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dercksen</surname><given-names>VJ</given-names></name><name><surname>Hege</surname><given-names>H-C</given-names></name><name><surname>Oberlaender</surname><given-names>M</given-names></name></person-group><article-title>The filament editor: An interactive software environment for visualization, proof-editing and analysis of 3d neuron morphology</article-title><source>NeuroInformatics</source><year>2014</year><volume>12</volume><issue>2</issue><fpage>325</fpage><lpage>339</lpage><pub-id pub-id-type="pmid">24323305</pub-id></element-citation></ref><ref id="R28"><label>[28]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Brunelli</surname><given-names>R</given-names></name></person-group><source>Template Matching Techniques in Computer Vision: Theory and Practice</source><year>2009</year><publisher-name>Wiley Publishing</publisher-name><pub-id pub-id-type="doi">10.1002/9780470744055</pub-id></element-citation></ref><ref id="R29"><label>[29]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>HJ</given-names></name><name><surname>McCormick</surname><given-names>MM</given-names></name><name><surname>Ibáñez</surname><given-names>L</given-names></name></person-group><collab>Consortium, I.S</collab><chapter-title>The ITK Software Guide: Introduction and Development Guidelines</chapter-title><source>The ITK Software Guide: ITK 47</source><year>2015</year><publisher-name>Kitware</publisher-name></element-citation></ref><ref id="R30"><label>[30]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carr</surname><given-names>H</given-names></name><name><surname>Snoeyink</surname><given-names>J</given-names></name><name><surname>Axen</surname><given-names>U</given-names></name></person-group><article-title>Computing contour trees in all dimensions</article-title><source>Computational Geometry</source><year>2002</year><volume>24</volume><pub-id pub-id-type="doi">10.1016/S0925-7721(02)00093-7</pub-id></element-citation></ref><ref id="R31"><label>[31]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dijkstra</surname><given-names>EW</given-names></name></person-group><article-title>A note on two problems in connexion with graphs</article-title><source>Numerische Mathematik</source><year>1959</year><volume>1</volume><fpage>269</fpage><lpage>271</lpage><pub-id pub-id-type="doi">10.1007/BF01386390</pub-id></element-citation></ref><ref id="R32"><label>[32]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skylaki</surname><given-names>S</given-names></name><name><surname>Hilsenbeck</surname><given-names>O</given-names></name><name><surname>Schroeder</surname><given-names>T</given-names></name></person-group><article-title>Challenges in long-term imaging and quantification of single-cell dynamics</article-title><source>Nature Biotechnology</source><year>2016</year><volume>34</volume><fpage>1137</fpage><lpage>1144</lpage><pub-id pub-id-type="pmid">27824848</pub-id></element-citation></ref><ref id="R33"><label>[33]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolterhoff</surname><given-names>N</given-names></name><name><surname>Hiesinger</surname><given-names>PR</given-names></name></person-group><article-title>Synaptic promiscuity in brain development</article-title><source>Current Biology</source><year>2024</year><volume>34</volume><issue>3</issue><fpage>102</fpage><lpage>116</lpage><pub-id pub-id-type="pmcid">PMC10849093</pub-id><pub-id pub-id-type="pmid">38320473</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2023.12.037</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Fig. 1</label><caption><title>2D cross-sections of 3D two-photon microscopy images of different axon terminals with filopodia.</title></caption><graphic xlink:href="EMS205784-f001"/></fig><fig id="F2" position="float"><label>Fig. 2</label><caption><title>3D images of example axon terminals showing filopodia, visualized using volume rendering.</title><p>a) Two-photon microscopy images of two axon terminals shown in x-y orientation. b) The axial resolution in z direction (here the horizontal direction) is poorer (0.1 × 0.1 × 0.5 <italic>µm</italic><sup>3</sup>).</p></caption><graphic xlink:href="EMS205784-f002"/></fig><fig id="F3" position="float"><label>Fig. 3</label><caption><title><italic>Filament Editor</italic> with 2D and 3D viewer.</title><p>The <italic>Filament Editor</italic> provides a 2D (center) and 3D (right) viewer to display the neuronal axon terminals. Both 2D and 3D viewer show the skeleton graph superimposed in the image. Various tools allow the display, processing, and analysis of the filopodia.</p></caption><graphic xlink:href="EMS205784-f003"/></fig><fig id="F4" position="float"><label>Fig. 4</label><caption><title>Schematic of the 3D tree-like structure of neuronal axon terminals.</title><p>A neuronal axon terminal with its filopodial protuberances can be represented as a skeleton graph (tree). Here, a single branch of the tree is shown. Branches extend from the <italic>root</italic> node (blue) to the filopodia <italic>tips</italic> (orange), passing through the <italic>base</italic> locations (green) and, potentially, <italic>branching</italic> nodes (yellow), that is, when these exist. The part of the path between tip and base is the <italic>filopodium</italic>; the part of the path between base and center is labeled as <italic>axon terminal</italic>.</p></caption><graphic xlink:href="EMS205784-f004"/></fig><fig id="F5" position="float"><label>Fig. 5</label><caption><title>Preprocessing workflow.</title><p>a, b) Images are loaded into <italic>Amira</italic>. c, d) User selects centers of the axon terminals to be analyzed. Centers are automatically propagated to the succeeding time steps. e, f) Crop box must be defined around each axon terminal. An initial cropping size is approximated with the Contour Tree Segmentation module and is user-adjustable afterwards. It is set to the minimal box including all voxels of the Axon terminal. g, h) Result of preprocessing are multiple datasets, each containing one axon terminal with marked centers and precomputed Dijkstra graphs. Upper and lower row show the same data but from different perspectives.</p></caption><graphic xlink:href="EMS205784-f005"/></fig><fig id="F6" position="float"><label>Fig. 6</label><caption><title>Reconstruction workflow.</title><p>a) Reconstruction begins with loading dataset of a single cropped axon terminal and its graph, which at this point only contains the root nodes. b) In the next step, filopodia tips are manually selected and each filopodium is automatically traced to the root node. Traces and estimated base nodes must be proofread. c) Filopodia from time step 1 are then propagated to time step 2. All propagated filopodia must be proofread before selecting newly emerged tips and an automatic tracing to the root node. d) Process must be repeated for all time steps that are wished to be included in the analysis. e, f) From fully reconstructed axon terminal in all the time steps, statistical results are derived. Images show the same result from two different perspectives. The part of workflow enframed by the red dotted line is described in detail in Fig. 7.</p></caption><graphic xlink:href="EMS205784-f006"/></fig><fig id="F7" position="float"><label>Fig. 7</label><caption><title>Reconstruction workflow.</title><p>Generalized detailed workflow depicting reconstruction of an axon terminal between two consecutive time steps.</p></caption><graphic xlink:href="EMS205784-f007"/></fig><fig id="F8" position="float"><label>Fig. 8</label><caption><title>Adding a filopodium.</title><p>a) The user adds a new filopodium by interactively specifying the tip (orange). The path to the root node (blue) is automatically traced. b) The base of the filopodium is detected by computing the Gaussian-ness of the intensity profile at each path point. From left to right: intensity profile from example point within filopodium, at base node (green), and from example point within axon terminal. c) The change from Gaussian-ness to non-Gaussian-ness indicates the base. The local peaks are caused by branches and the limited axial resolution in z.</p></caption><graphic xlink:href="EMS205784-f008"/></fig><fig id="F9" position="float"><label>Fig. 9</label><caption><title>Different cell morphologies.</title><p>A typical R7 cell (left) has a vastly different morphology than a typical Dm8 cell (right). Dm8 does not have an axon terminal, but an axo-dendritic branched structure which can be used for setting the root node. Reconstructions for both cell types are also shown.</p></caption><graphic xlink:href="EMS205784-f009"/></fig><fig id="F10" position="float"><label>Fig. 10</label><caption><title>Working time comparison.</title><p>To investigate the influence of filopodia properties on the time requirements, several Dm8 and R7 cells were reconstructed. The plots show reconstruction time vs. total number of filopodia, average lifetime and average length over 60 time steps. Dashed lines represent the mean values of the quantities represented by the axes.</p></caption><graphic xlink:href="EMS205784-f010"/></fig><fig id="F11" position="float"><label>Fig. 11</label><caption><title>A549-SIM cell.</title><p>Volume rendering of the cell (left), cross section of the cell (center), reconstruction overlaid with the cell (right).</p></caption><graphic xlink:href="EMS205784-f011"/></fig><table-wrap id="T1" orientation="portrait" position="float"><label>Table 1</label><caption><title>User-adjustable parameters.</title><p>Overview of the user-adjustable parameters used during the workflow execution. The images of the used datasets have integer intensity values in the range [0, 255]. The template and search window sizes <italic>T</italic> and <italic>S</italic> for the node propagation depend on the cropped image size, which can be different for each axon terminal. <italic>θ, p</italic> and <italic>I</italic><sub><italic>max</italic></sub> depend on the range of intensity values in the images. Threshold distance <italic>d</italic> is the maximum distance that two nodes can be apart from each other to be considered matching and can be connected in the tracking process.</p></caption><table frame="box" rules="groups"><thead><tr><th valign="top" align="left" style="border-top: 1px solid #000000;border-left: 1px solid">Task</th><th valign="top" align="left" style="border-top: 1px solid #000000;border-left: 1px solid">Acr.</th><th valign="top" align="left" style="border-top: 1px solid #000000;border-left: 1px solid">Parameter</th><th valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid">Domain</th><th valign="top" align="center" style="border-top: 1px solid #000000;border-right: 1px solid #000000;border-left: 1px solid">Used value</th></tr></thead><tbody><tr><td valign="top" align="left" colspan="5" style="border-top: 1px solid #000000;border-right: 1px solid #ffffff;border-left: 1px solid #ffffff"/></tr><tr><td valign="top" align="left" rowspan="5" style="border-top: 1px solid #000000;border-left: 1px solid #000000">Preprocessing</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000"><italic>θ</italic></td><td valign="top" align="left" style="border-top: 1px solid #000000;border-left: 1px solid #000000">Threshold of CTS</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000">[0, 255] ⊂ ℕ</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-right: 1px solid #000000;border-left: 1px solid #000000">50</td></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000"><italic>p</italic></td><td valign="top" align="left" style="border-left: 1px solid #000000">Persistence of CTS</td><td valign="top" align="center" style="border-left: 1px solid">[0, 255] ⊂ ℕ</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid #000000">150</td></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000"><italic>T</italic><sub><italic>R</italic></sub></td><td valign="top" align="left" style="border-left: 1px solid">Root template size [vx]</td><td valign="top" align="center" style="border-left: 1px solid">ℕ<sup>3</sup></td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid">15 × 15 × 3</td></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000"><italic>S</italic><sub><italic>R</italic></sub></td><td valign="top" align="left" style="border-left: 1px solid">Root search window [vx]</td><td valign="top" align="center" style="border-left: 1px solid">ℕ<sup>3</sup></td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid">70 × 70 × 7</td></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000"><italic>γ</italic><sub><italic>R</italic></sub></td><td valign="top" align="left" style="border-left: 1px solid #000000">Root NCC value</td><td valign="top" align="center" style="border-left: 1px solid #000000">[0,<xref ref-type="bibr" rid="R1">1</xref>] ⊂ ℝ</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid #000000">0.8</td></tr><tr><td valign="top" align="left" rowspan="2" style="border-top: 1px solid #000000;border-left: 1px solid">Tracing</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000"><italic>c</italic></td><td valign="top" align="left" style="border-top: 1px solid #000000;border-left: 1px solid">Intensity weight</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid">ℝ</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-right: 1px solid #000000;border-left: 1px solid">50.0</td></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000"><italic>I</italic><sub><italic>max</italic></sub></td><td valign="top" align="left" style="border-left: 1px solid">Max. brightness threshold</td><td valign="top" align="center" style="border-left: 1px solid">[0, 255] ⊂ ℕ</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid">100</td></tr><tr><td valign="top" align="left" rowspan="8" style="border-top: 1px solid #000000;border-bottom: 1px solid #000000;border-left: 1px solid">Tracking</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid #000000"><italic>d</italic></td><td valign="top" align="left" style="border-top: 1px solid #000000;border-left: 1px solid">Threshold distance [<italic>μ</italic>m]</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid">ℝ</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-right: 1px solid #000000;border-left: 1px solid">1.0</td></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000"><italic>l</italic></td><td valign="top" align="left" style="border-left: 1px solid">Min. filopodia length [<italic>μ</italic>m]</td><td valign="top" align="center" style="border-left: 1px solid">ℝ</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid">0.5</td></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000"><italic>T</italic><sub><italic>B</italic></sub></td><td valign="top" align="left" style="border-left: 1px solid">Base template size [vx]</td><td valign="top" align="center" style="border-left: 1px solid">ℕ<sup>3</sup></td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid">50 × 50 × 10</td></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000"><italic>S</italic><sub><italic>B</italic></sub></td><td valign="top" align="left" style="border-left: 1px solid">Base search window [vx]</td><td valign="top" align="center" style="border-left: 1px solid">ℕ<sup>3</sup></td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid">10 × 10 × 4</td></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000">γ<sub><italic>R</italic></sub></td><td valign="top" align="left" style="border-left: 1px solid #000000">Base NCC value</td><td valign="top" align="center" style="border-left: 1px solid #000000">[0,<xref ref-type="bibr" rid="R1">1</xref>] ⊂ ℝ</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid #000000">0.8</td></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000"><italic>T</italic><sub><italic>T</italic></sub></td><td valign="top" align="left" style="border-left: 1px solid">Tip template size [vx]</td><td valign="top" align="center" style="border-left: 1px solid">ℕ<sup>3</sup></td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid">10 × 10 × 4</td></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000"><italic>S</italic><sub><italic>T</italic></sub></td><td valign="top" align="left" style="border-left: 1px solid">Tip search window [vx]</td><td valign="top" align="center" style="border-left: 1px solid">ℕ<sup>3</sup></td><td valign="top" align="center" style="border-left: 1px solid #000000;border-right: 1px solid">15 × 15 × 4</td></tr><tr><td valign="top" align="center" style="border-left: 1px solid #000000;border-bottom: 1px solid #000000"><italic>γ</italic><sub><italic>T</italic></sub></td><td valign="top" align="left" style="border-left: 1px solid #000000;border-bottom: 1px solid #000000">Tip NCC value</td><td valign="top" align="center" style="border-left: 1px solid #000000;border-bottom: 1px solid #000000">[0,<xref ref-type="bibr" rid="R1">1</xref>] ⊂ ℝ</td><td valign="top" align="center" style="border-right: 1px solid #000000;border-bottom: 1px solid #000000;border-left: 1px solid #000000">0.8</td></tr></tbody></table></table-wrap><table-wrap id="T2" orientation="portrait" position="float"><label>Table 2</label><caption><title>Time requirements for preprocessing steps for R7 cells.</title><p>Preprocessing of a dataset normally depends on its size and number of axon terminals, and in the case of data analyzed here, it took less than 15 minutes per sample. Different steps are distinguished between automatic ones and those that are performed manually.</p></caption><table frame="box" rules="groups"><thead><tr><th valign="top" align="left" style="border-top: 1px solid #000000;border-left: 1px solid"/><th valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid">Dataset 1</th><th valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid">Dataset 2</th><th valign="top" align="center" style="border-top: 1px solid #000000;border-right: 1px solid #000000;border-left: 1px solid">Dataset 3</th></tr></thead><tbody><tr><td valign="top" align="left" style="border-top: 2.5px solid #000000;border-left: 1px solid">Image size</td><td valign="top" align="center" style="border-top: 2.5px solid #000000;border-left: 1px solid">512×512×35</td><td valign="top" align="center" style="border-top: 2.5px solid #000000;border-left: 1px solid">512×512×36</td><td valign="top" align="center" style="border-top: 2.5px solid #000000;border-right: 1px solid #000000;border-left: 1px solid">512×512×38</td></tr><tr><td valign="top" align="left" style="border-top: 1px solid #000000;border-left: 1px solid"># Axon terminals</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid">3</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid">4</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-right: 1px solid #000000;border-left: 1px solid">5</td></tr><tr><td valign="top" align="left" style="border-top: 2.5px solid #000000;border-left: 1px solid">Set root nodes (manual)</td><td valign="top" align="center" style="border-top: 2.5px solid #000000;border-left: 1px solid">&lt; 1 min</td><td valign="top" align="center" style="border-top: 2.5px solid #000000;border-left: 1px solid">&lt; 1 min</td><td valign="top" align="center" style="border-top: 2.5px solid #000000;border-right: 1px solid #000000;border-left: 1px solid">∼ 1 min</td></tr><tr><td valign="top" align="left" style="border-top: 1px solid #000000;border-left: 1px solid">Propagate root nodes (automatic)</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid">&lt; 1 min</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid">∼ 1 min</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-right: 1px solid #000000;border-left: 1px solid">∼ 1 min</td></tr><tr><td valign="top" align="left" style="border-top: 1px solid #000000;border-left: 1px solid">Verify root nodes (manual)</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid">∼ 2 min</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid">∼ 3 min</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-right: 1px solid #000000;border-left: 1px solid">∼ 4 min</td></tr><tr><td valign="top" align="left" style="border-top: 1px solid #000000;border-left: 1px solid">Find cropping parameters (manual)</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid">∼ 1 min</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid">∼ 2 min</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-right: 1px solid #000000;border-left: 1px solid">∼ 2 min</td></tr><tr><td valign="top" align="left" style="border-top: 1px solid #000000;border-left: 1px solid">Crop images and compute<break/>Dijkstra graphs (automatic)</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid">∼ 7 min</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-left: 1px solid">∼ 6 min</td><td valign="top" align="center" style="border-top: 1px solid #000000;border-right: 1px solid #000000;border-left: 1px solid">∼ 7 min</td></tr><tr><td valign="top" align="left" style="border-top: 2.5px solid #000000;border-bottom: 1px solid #000000;border-left: 1px solid">Total</td><td valign="top" align="center" style="border-top: 2.5px solid #000000;border-bottom: 1px solid #000000;border-left: 1px solid">∼ 11 min</td><td valign="top" align="center" style="border-top: 2.5px solid #000000;border-bottom: 1px solid #000000;border-left: 1px solid">∼ 13 min</td><td valign="top" align="center" style="border-top: 2.5px solid #000000;border-bottom: 1px solid #000000;border-right: 1px solid #000000;border-left: 1px solid #000000">∼ 15 min</td></tr></tbody></table></table-wrap></floats-group></article>