<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS206394</article-id><article-id pub-id-type="doi">10.1101/2025.06.08.658221</article-id><article-id pub-id-type="archive">PPR1033485</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Modality-Agnostic Decoding of Vision and Language from fMRI</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Nikolaus</surname><given-names>Mitja</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Mozafari</surname><given-names>Milad</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Berry</surname><given-names>Isabelle</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Asher</surname><given-names>Nicholas</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Reddy</surname><given-names>Leila</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>VanRullen</surname><given-names>Rufin</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib></contrib-group><aff id="A1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/004raaa70</institution-id><institution>Université de Toulouse</institution></institution-wrap>, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02feahw73</institution-id><institution>CNRS</institution></institution-wrap>, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/04fhrs205</institution-id><institution>CerCo</institution></institution-wrap>, <city>Toulouse</city>, <country country="FR">France</country></aff><aff id="A2"><label>2</label>Torus AI, Toulouse, France</aff><aff id="A3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/004raaa70</institution-id><institution>Université de Toulouse</institution></institution-wrap>, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01rx4qw44</institution-id><institution>IRIT</institution></institution-wrap>, <city>Toulouse</city>, <country country="FR">France</country></aff><author-notes><corresp id="CR1">
<label>*</label>For correspondence: <email>mitja.nikolaus@cnrs.fr</email> (MN)</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>14</day><month>06</month><year>2025</year></pub-date><pub-date pub-type="preprint"><day>08</day><month>06</month><year>2025</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Humans perform tasks involving the manipulation of inputs regardless of how these signals are perceived by the brain, thanks to representations that are agnostic to the stimulus modality. Investigating such modality-agnostic representations requires experimental datasets with multiple modalities of presentation. In this paper, we introduce and analyze SemReps-8K, a new large-scale fMRI dataset of 6 subjects watching both images and short text descriptions of such images, as well as conditions during which the subjects were imagining visual scenes. The multimodal nature of this dataset enables the development of modality-agnostic decoders, trained to predict which stimulus a subject is seeing, irrespective of the modality in which the stimulus is presented. Further, we performed a searchlight analysis revealing that large areas of the brain contain modality-agnostic representations. Such areas are also particularly suitable for decoding visual scenes from the mental imagery condition. The dataset will be made publicly available.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Several regions in the human brain have developed a high degree of specialization for particular lower-level perceptive as well as higher-level cognitive functions (<xref ref-type="bibr" rid="R56">Kanwisher, 2010</xref>). For many higher-level functions, it is crucial to be able to manipulate inputs regardless of the modality in which a stimulus was perceived by the brain. Such manipulations can be performed thanks to representations that are abstracted away from particularities of specific modalities, and are therefore <italic>modality-agnostic</italic>. A range of theories have been developed to explain how and where in the human brain such abstract representations are created (<xref ref-type="bibr" rid="R22">Damasio, 1989</xref>; <xref ref-type="bibr" rid="R13">Binder <italic>et al.,</italic> 2009</xref>; <xref ref-type="bibr" rid="R75">Martin, 2016</xref>; <xref ref-type="bibr" rid="R8">Barsalou, 2016</xref>; <xref ref-type="bibr" rid="R97">Ralph <italic>et al.,</italic> 2017</xref>).</p><p id="P3">In order to study how modality-agnostic information is represented in the brain and to exploit it for modality-agnostic decoding, large multimodal neuroimaging datasets with well-controlled stimuli across modalities are required. While large-scale datasets exist for vision (<xref ref-type="bibr" rid="R50">Huth <italic>et al.,</italic> 2012</xref>; <xref ref-type="bibr" rid="R20">Chang <italic>et al.,</italic> 2019</xref>; <xref ref-type="bibr" rid="R3">Allen <italic>et al.,</italic> 2022</xref>), language (<xref ref-type="bibr" rid="R17">Brennan and Hale, 2019</xref>; <xref ref-type="bibr" rid="R83">Nastase <italic>et al.,</italic> 2021</xref>; <xref ref-type="bibr" rid="R102">Schoffelen <italic>et al.,</italic> 2019</xref>; <xref ref-type="bibr" rid="R116">Tang <italic>et al.,</italic> 2023a</xref>), and video (naturalistic movies) (<xref ref-type="bibr" rid="R2">Aliko <italic>et al.,</italic> 2020</xref>; <xref ref-type="bibr" rid="R86">Visconti di Oleggio Castello <italic>et al.,</italic> 2020</xref>; <xref ref-type="bibr" rid="R15">Boyle <italic>et al.,</italic> 2020</xref>), none of these contain a controlled set of equivalent stimuli that are presented separately in both modalities. For instance, the different modalities in movies (vision and language) are complementary but do not always carry the same semantics. Further, they are not presented separately but simultaneously, impeding a study of the respective activity pattern caused by each modality in isolation.</p><p id="P4">Here, we present <bold>SemReps-8K</bold>, a new large-scale multimodal fMRI dataset of 6 subjects each viewing more than 8,000 stimuli which are presented separately in one of two modalities, as images of visual scenes or as descriptive captions of such images. In addition, the dataset also contains 3 imagery conditions for each subject, where they had to imagine an visual scene based on a caption description they had received before the start of the fMRI experiment. We exploit this new data to develop decoders that are specifically trained to leverage modality-agnostic patterns in the brain. Such modality-agnostic decoders are trained on brain imaging data from multiple modalities, which we demonstrate here for the case of vision and language. In contrast to <italic>modality-specific</italic> decoders that can be applied only in the modality that they were trained on, <italic>modality-agnostic</italic> decoders can be applied to decode stimuli from multiple modalities, even without knowing a priori the modality the stimulus was presented in.</p><p id="P5">We find that modality-agnostic decoders trained on this dataset perform on par with their respective modality-specific counterparts for decoding images, despite the additional challenge of uncertainty about the stimulus modality. For decoding captions, the modality-agnostic decoders even outperform the respective modality-specific decoders (because the former, but not the latter, can leverage the additional training data from the other image modality).</p><p id="P6">Additionally, we use this novel kind of decoders for a searchlight analysis to localize regions with modality-agnostic representations in the brain. Previous studies that aimed to localize modality-agnostic patterns were based on limited and rather simple stimulus sets and did not always agree on the exact location and extent of such regions (e.g. <xref ref-type="bibr" rid="R121">Vandenberghe <italic>et al.,</italic> 1996</xref>; <xref ref-type="bibr" rid="R105">Shinkareva <italic>et al.,</italic> 2011</xref>; <xref ref-type="bibr" rid="R25">Devereux <italic>et al.,</italic> 2013</xref>; <xref ref-type="bibr" rid="R32">Fairhall and Caramazza, 2013</xref>; <xref ref-type="bibr" rid="R55">Jung <italic>et al.,</italic> 2018</xref>). We design a search-light analysis based on a combination of modality-agnostic decoders and cross-decoding. The results reveal that modality-agnostic patterns can be found in a widespread left-lateralized network across the brain, encompassing virtually all regions that have been proposed previously.</p><p id="P7">Finally, we find that modality-agnostic decoders trained only on data with perceptual input also generalize to conditions during which the subjects were performing mental imagery. There is a large overlap in the areas that we identified as modality-agnostic and those that are suitable for decoding mental imagery.</p></sec><sec id="S2"><title>Related Work</title><sec id="S3"><title>Modality-agnostic representations</title><p id="P8">Decades of neuro-anatomical research (e.g. based on clinical lesions) and electrophysiology in non-human and human primates, as well as modern experiments leveraging recent brain imaging techniques have provided evidence that the activation patterns in certain brain areas are modality-specific; for example, the occipital cortex responds predominantly to visual stimulation (<xref ref-type="bibr" rid="R36">Felleman and Van Essen, 1991</xref>; <xref ref-type="bibr" rid="R103">Sereno <italic>et al.,</italic> 1995</xref>; <xref ref-type="bibr" rid="R45">Grill-Spector and Malach, 2004</xref>), and a commonly left-lateralized network responds to language processing tasks (<xref ref-type="bibr" rid="R129">Zola-Morgan, 1995</xref>; <xref ref-type="bibr" rid="R35">Fedorenko <italic>et al.,</italic> 2010</xref>, <xref ref-type="bibr" rid="R34">2011</xref>; <xref ref-type="bibr" rid="R40">Friederici, 2017</xref>; <xref ref-type="bibr" rid="R16">Brennan, 2022</xref>).</p><p id="P9">More recent research has started to focus on higher-level regions that respond with <italic>modality-agnostic</italic> patterns, i.e., patterns that are abstracted away from any modality-specific information. A modality-agnostic region responds with similar patterns to input stimuli of the same meaning, even if they are presented in different modalities (e.g. the word “cat” and picture of a cat). Such regions have also been described as abstract/conceptual (<xref ref-type="bibr" rid="R12">Binder, 2016</xref>), modality-invariant (<xref ref-type="bibr" rid="R73">Man <italic>et al.,</italic> 2012</xref>), modality-independent (<xref ref-type="bibr" rid="R29">Dirani and Pylkkänen, 2024</xref>), supramodal (<xref ref-type="bibr" rid="R101">Sanchez <italic>et al.,</italic> 2020</xref>), or amodal (<xref ref-type="bibr" rid="R32">Fairhall and Caramazza, 2013</xref>) (although see distinction made in <xref ref-type="bibr" rid="R8">Barsalou, 2016</xref>).</p><p id="P10">Several theories and frameworks on how the brain forms modality-agnostic representations from modality-specific inputs have been proposed. The <bold>convergence zones view</bold> proposes that information coming from modality-specific sensory cortices is integrated in multiple convergence zones that are distributed across the cortex, predominantly in temporal and parietal lobes (<xref ref-type="bibr" rid="R22">Damasio, 1989</xref>; <xref ref-type="bibr" rid="R120">Tranel <italic>et al.,</italic> 1997</xref>; <xref ref-type="bibr" rid="R78">Meyer and Damasio, 2009</xref>). These convergence zones are organized hierarchically, learned associations are used to create abstractions from lower-level to higher-level feature representations (<xref ref-type="bibr" rid="R107">Simmons and Barsalou, 2003</xref>; <xref ref-type="bibr" rid="R78">Meyer and Damasio, 2009</xref>). A perceived stimulus first causes activity in the related low-level modality-specific region (e.g. visual cortex), subsequently higher-level convergence zones serve as relays that cause associated activity in other regions of the brain (e.g. the language network) (<xref ref-type="bibr" rid="R78">Meyer and Damasio, 2009</xref>; <xref ref-type="bibr" rid="R58">Kiefer and Pulver-müller, 2012</xref>). According to <xref ref-type="bibr" rid="R12">Binder (2016</xref>), the most high-level convergence zones can become so abstract that they are representing amodal symbols.</p><p id="P11">The <bold>GRAPES framework</bold> (Grounding Representations in Action, Perception, and Emotion Systems) also suggests that representations are distributed across temporal and parietal areas of the cortex. More specifically, they are hypothesized to be situated in areas connected to the perception and manipulation of the environment, as well as in the language system (<xref ref-type="bibr" rid="R74">Martin, 2009</xref>, <xref ref-type="bibr" rid="R75">2016</xref>). According to this theory, conceptual knowledge is organized in domains: For example, semantic information related to object form and object motion is represented within specific visual processing systems, regardless of the stimulus modality, and both for perception as well as imagination.</p><p id="P12">The <bold>hub-and-spoke theory</bold> states that cross-modal interactions are mediated by a single modalityagnostic hub, located in the anterior temporal lobes (<xref ref-type="bibr" rid="R100">Rogers <italic>et al.,</italic> 2004</xref>; <xref ref-type="bibr" rid="R64">Lambon Ralph <italic>et al.,</italic> 2006</xref>; <xref ref-type="bibr" rid="R89">Patterson and Lambon Ralph, 2016</xref>; <xref ref-type="bibr" rid="R97">Ralph <italic>et al.,</italic> 2017</xref>). The hub contains a “continuous distributed representation space that expresses conceptual similarities among items even though its dimensions are not independently interpretable” (<xref ref-type="bibr" rid="R41">Frisby <italic>et al.,</italic> 2023</xref>, p. 262). The spokes form the links between the hubs and the modality-specific association cortices. Most importantly, semantic representations are not solely based in the hub, for a given concept all spokes that are linked to modalities in which the concept can be experienced do contribute to the semantic representation. This explains why selective damage to spokes can cause category-specific deficits (<xref ref-type="bibr" rid="R93">Pobric <italic>et al.,</italic> 2010</xref>).</p><p id="P13">This is conceptually similar to some aspects of the <bold>Global Workspace Theory</bold>, which assumes both a multimodal convergence of inputs towards a specific (network of) region(s), and the possibility of flexibly recruiting unimodal regions into this Global Workspace (<xref ref-type="bibr" rid="R6">Baars, 1993</xref>, <xref ref-type="bibr" rid="R7">2005</xref>).</p><p id="P14">While these and other theories partly disagree on <italic>how</italic> modality-agnostic information is represented in the brain, they agree that such information is distributed across the cortex, and possibly overlapping with the semantic network (<xref ref-type="bibr" rid="R13">Binder <italic>et al.,</italic> 2009</xref>; <xref ref-type="bibr" rid="R50">Huth <italic>et al.,</italic> 2012</xref>; <xref ref-type="bibr" rid="R4">Andrews <italic>et al.,</italic> 2014</xref>; <xref ref-type="bibr" rid="R130">Zwaan, 2016</xref>).</p></sec><sec id="S4"><title>Decoding of vision and language from fMRI</title><p id="P15">Early approaches of brain decoding focused on identifying and reconstructing limited sets of simple visual stimuli (<xref ref-type="bibr" rid="R48">Haxby <italic>et al.,</italic> 2001</xref>; <xref ref-type="bibr" rid="R21">Cox and Savoy, 2003</xref>; <xref ref-type="bibr" rid="R57">Kay <italic>et al.,</italic> 2008</xref>; <xref ref-type="bibr" rid="R82">Naselaris <italic>et al.,</italic> 2009</xref>; <xref ref-type="bibr" rid="R84">Nishimoto <italic>et al.,</italic> 2011</xref>). Soon after, attempts to decode linguistic stimuli could identify single words and short paragraphs with the help of models trained to predict features extracted from word embeddings (<xref ref-type="bibr" rid="R92">Pereira <italic>et al.,</italic> 2018</xref>).</p><p id="P16">More recently, large-scale open source fMRI datasets for both vision and language have become available (<xref ref-type="bibr" rid="R20">Chang <italic>et al.,</italic> 2019</xref>; <xref ref-type="bibr" rid="R3">Allen <italic>et al.,</italic> 2022</xref>; <xref ref-type="bibr" rid="R102">Schoffelen <italic>et al.,</italic> 2019</xref>; <xref ref-type="bibr" rid="R117">Tang <italic>et al.,</italic> 2023b</xref>) and allowed for the training of decoding models for a larger range and more complex naturalistic stimuli with the help of features extracted from deep learning models. For example, modality-specific decoders for vision can be trained by mapping the brain activity of subjects viewing naturalistic images to feature representation spaces of computational models of the same modality (i.e. vision models) (<xref ref-type="bibr" rid="R104">Shen <italic>et al.,</italic> 2019</xref>; <xref ref-type="bibr" rid="R10">Beliy <italic>et al.,</italic> 2019</xref>; <xref ref-type="bibr" rid="R68">Lin <italic>et al.,</italic> 2022</xref>; <xref ref-type="bibr" rid="R115">Takagi and Nishimoto, 2023</xref>; <xref ref-type="bibr" rid="R88">Ozcelik and VanRullen, 2023</xref>). Moreover, a range of studies provided evidence that certain representations can transfer between vision and language by evaluating decoders in a modality that they were not trained on (<xref ref-type="bibr" rid="R105">Shinkareva <italic>et al.,</italic> 2011</xref>; <xref ref-type="bibr" rid="R73">Man <italic>et al.,</italic> 2012</xref>; <xref ref-type="bibr" rid="R32">Fairhall and Caramazza, 2013</xref>; <xref ref-type="bibr" rid="R106">Simanova <italic>et al.,</italic> 2014</xref>; <xref ref-type="bibr" rid="R55">Jung <italic>et al.,</italic> 2018</xref>). The performance in such <italic>cross-modal decoding</italic> evaluations always lags behind when compared to within-modality decoding. One explanation is that modality-specific decoders are not explicitly encouraged to pick up on modality-agnostic features during training, and modality-specific features do not transfer to other modalities.</p><p id="P17">To address this limitation, we here propose to directly train <italic>modality-agnostic decoders</italic>, i.e. models that are exposed to multiple stimulus modalities during training in order to make it more likely that they are leveraging representations that are modality-agnostic. Training this kind of decoder is enabled by the multimodal nature of our fMRI dataset: The stimuli are taken from COCO, a multimodal dataset of images with associated descriptive captions (<xref ref-type="bibr" rid="R69">Lin <italic>et al.,</italic> 2014</xref>). During the experiment the subjects are exposed to stimuli in both modalities (images and captions) in separate trials. Crucially, we can map the brain activity of each trial (e.g. the subject viewing an image) to modality-agnostic features extracted from both modalities (the image and the corresponding caption) when training the decoder models. After training, a single modality-agnostic decoder can be used to decode stimuli from multiple modalities, leveraging representations that are common to all modalities.</p></sec><sec id="S5"><title>Decoding of mental imagery</title><p id="P18">Apart from decoding perceived stimuli, it is also possible to decode representations when subjects were performing mental imagery, without being exposed to any perceptual input.</p><p id="P19">Different theories on mental imagery processes emphasize either the role of the early visual areas (<xref ref-type="bibr" rid="R60">Kosslyn <italic>et al.,</italic> 1999</xref>; <xref ref-type="bibr" rid="R90">Pearson, 2019</xref>) or the role of the high-level visual areas in the ventral temporal cortex and frontoparietal networks (<xref ref-type="bibr" rid="R111">Spagna <italic>et al.,</italic> 2021</xref>; <xref ref-type="bibr" rid="R46">Hajhajate <italic>et al.,</italic> 2022</xref>; <xref ref-type="bibr" rid="R71">Liu <italic>et al.,</italic> 2025</xref>). There is evidence for both kinds of theories in the form of neuroimaging studies that used decoding to identify stimuli during mental imagery. Some of these found relevant patterns in the early visual cortex (<xref ref-type="bibr" rid="R1">Albers <italic>et al.,</italic> 2013</xref>; <xref ref-type="bibr" rid="R81">Naselaris <italic>et al.,</italic> 2015</xref>), others highlighted the role of higher-level areas in the ventral visual processing stream (<xref ref-type="bibr" rid="R114">Stokes <italic>et al.,</italic> 2009</xref>; <xref ref-type="bibr" rid="R98">Reddy <italic>et al.,</italic> 2010</xref>; <xref ref-type="bibr" rid="R65">Lee <italic>et al.,</italic> 2012</xref>; <xref ref-type="bibr" rid="R122">VanRullen and Reddy, 2019</xref>; <xref ref-type="bibr" rid="R14">Boccia <italic>et al.,</italic> 2019</xref>) as well as the precuneus and the intraparietal sulcus (<xref ref-type="bibr" rid="R54">Johnson and Johnson, 2014</xref>). These discrepancies can possibly be explained by differences in experimental design: For example, the early visual cortex might only become involved if the task requires the imagination of high-resolution details, which are represented in lower levels of the visual processing hierarchy (<xref ref-type="bibr" rid="R61">Kosslyn and Thompson, 2003</xref>).</p><p id="P20">Crucially, it has been shown that decoders trained exclusively on trials <italic>with</italic> perceptual input can generalize to imagery trials (<xref ref-type="bibr" rid="R114">Stokes <italic>et al.,</italic> 2009</xref>; <xref ref-type="bibr" rid="R98">Reddy <italic>et al.,</italic> 2010</xref>; <xref ref-type="bibr" rid="R65">Lee <italic>et al.,</italic> 2012</xref>; <xref ref-type="bibr" rid="R54">Johnson and Johnson, 2014</xref>; <xref ref-type="bibr" rid="R81">Naselaris <italic>et al.,</italic> 2015</xref>), providing evidence that representations formed during perception overlap to some degree with representations formed during mental imagery (<xref ref-type="bibr" rid="R28">Dijkstra <italic>et al.,</italic> 2019</xref>).</p><p id="P21">In our study, we explored to what extent these findings hold true for more varied and complex stimuli. Following previous approaches, we use decoders trained exclusively on trials where subjects were viewing images and captions, and evaluated them on their ability to decode the imagery trials. We additionally hypothesized that mental imagery should be at least as conceptual as it is sensory and therefore primarily recruit modality-agnostic representations. Consequently, modality-agnostic decoders should be ideally suited to decode mental imagery and outperform modality-specific decoders on that task.</p></sec><sec id="S6"><title>Methods for localizing modality-agnostic regions</title><p id="P22">The first evidence for the existence of modality-agnostic regions came from observations of patients with lesions in particular cortical regions which lead to deficits in the retrieval and use of knowledge across modalities (<xref ref-type="bibr" rid="R125">Warrington and Shallice, 1984</xref>; <xref ref-type="bibr" rid="R124">Warrington and Mccarthy, 1987</xref>; <xref ref-type="bibr" rid="R42">Gainotti, 2000</xref>; <xref ref-type="bibr" rid="R23">Damasio <italic>et al.,</italic> 2004</xref>). Semantic impairments across modalities have also been observed in patients with the neurodegenerative disorder semantic dementia (<xref ref-type="bibr" rid="R123">Warrington, 1975</xref>; <xref ref-type="bibr" rid="R110">Snowden <italic>et al.,</italic> 1989</xref>; <xref ref-type="bibr" rid="R51">Jefferies <italic>et al.,</italic> 2009</xref>).</p><p id="P23">In early work exploring the possible locations of modality-agnostic regions in healthy subjects, brain activity was recorded using imaging techniques while they were presented with a range of concepts in two modalities (e.g. words and pictures). Regions that were active during semantic processing of stimuli in the first modality were compared to regions that were active during semantic processing of stimuli in the second modality. The conjunction of these regions was proposed to be modality-agnostic (<xref ref-type="bibr" rid="R121">Vandenberghe <italic>et al.,</italic> 1996</xref>; <xref ref-type="bibr" rid="R79">Moore and Price, 1999</xref>; <xref ref-type="bibr" rid="R18">Bright <italic>et al.,</italic> 2004</xref>).</p><p id="P24">While this methodology allows for the identification of candidate regions in which semantic processing of multiple modalities occurs, it can not be used to probe the information represented in these regions. In order to compare the information content (i.e. multivariate patterns) of brain regions, researchers have developed Representational Similarity Analysis (RSA, <xref ref-type="bibr" rid="R63">Kriegeskorte <italic>et al.,</italic> 2008</xref>) as well as encoding and decoding analyses (<xref ref-type="bibr" rid="R80">Naselaris <italic>et al.,</italic> 2011</xref>). More specifically, RSA has been used to find modality-agnostic regions by comparing activation patterns of a candidate region when subjects are viewing stimuli from different modalities (<xref ref-type="bibr" rid="R25">Devereux <italic>et al.,</italic> 2013</xref>; <xref ref-type="bibr" rid="R47">Handjaras <italic>et al.,</italic> 2016</xref>; <xref ref-type="bibr" rid="R72">Liuzzi <italic>et al.,</italic> 2017</xref>). This comparison is performed in an indirect way, by measuring the correlation of dissimilarity matrices of activation patterns. In turn, cross-decoding analysis can be leveraged to identify modality-agnostic regions by training a classifier to predict the category of a stimulus in a given modality, and then evaluating its performance to predict the category of stimuli that were presented in another modality (<xref ref-type="bibr" rid="R105">Shinkareva <italic>et al.,</italic> 2011</xref>; <xref ref-type="bibr" rid="R73">Man <italic>et al.,</italic> 2012</xref>; <xref ref-type="bibr" rid="R32">Fairhall and Caramazza, 2013</xref>; <xref ref-type="bibr" rid="R106">Simanova <italic>et al.,</italic> 2014</xref>; <xref ref-type="bibr" rid="R55">Jung <italic>et al.,</italic> 2018</xref>). However, all these studies relied on a predefined set of stimulus categories, and can therefore not be easily extended to more realistic and complex stimuli, as we perceive them in our everyday life.</p><p id="P25">We summarize candidates for modality-agnostic regions that have been identified by previous studies in <xref ref-type="supplementary-material" rid="SD1">Appendix 3</xref>. This overview reveals substantial disagreement regarding the possible locations of modality-agnostic patterns in the brain. For example, <xref ref-type="bibr" rid="R32">Fairhall and Caramazza (2013</xref>) found modality-agnostic representations in the left ventral temporal cortex (fusiform, parahippocampal, and perirhinal cortex), middle and inferior temporal gyrus, angular gyrus, parts of the prefrontal cortex as well as the precuneus. <xref ref-type="bibr" rid="R105">Shinkareva <italic>et al.</italic> (2011</xref>) found a larger network of left-lateralized regions, including additionally the left superior temporal, inferior parietal, supramarginal, inferior and inferior occipital, precentral and postcentral gyrus, supplementary motor area, intraparietal sulcus, cuneus, posterior cingulum as well as the right fusiform gyrus and the superior parietal gyrus, paracentral lobule on both hemispheres. In contrast, <xref ref-type="bibr" rid="R55">Jung <italic>et al.</italic> (2018</xref>) found modality-agnostic representations only in the right prefrontal cortex. These diverging results can probably be explained by the limited number of stimuli as well as the use of artificially constructed stimuli in certain studies.</p><p id="P26">Recent advances in machine learning have enabled another generation of fMRI analyses based on large-scale naturalistic datasets. Here, we present a new multimodal dataset of subjects viewing both images and text. Most importantly, the dataset contains a large number of naturalistic stimuli in the form of complex visual scenes and full sentence descriptions of the same type of complex scenes, instead of pictures of single objects and words as commonly used in previous studies. This data enables the development of modality-agnostic decoders that are explicitly trained to leverage features that are shared across modalities. Further, we use this data to localize modality-agnostic regions in the brain by applying decoders in a multimodal searchlight analysis.</p></sec></sec><sec id="S7" sec-type="methods"><title>Methods</title><sec id="S8"><title>fMRI Experiment</title><p id="P27">Six subjects (2 female, age between 20 and 50 years, all right-handed and fluent English speakers) participated in the experiment after providing informed consent. The study was performed in accordance with French national ethical regulations (Comité de Protection des Personnes, ID 2019-A01920-57). We collected functional MRI data using a 3T Philips ACHIEVA scanner (gradient echo pulse sequence, TR=2s, TE=30ms, 46 slices with a 32-channel head coil, slice thickness=3mm with 0.2mm gap, in-plane voxel dimensions 3×3mm). At the start of each session, we further acquired high-resolution anatomical images for each subject (voxel size=1mm<sup>3</sup>, TR=8.13ms, TE=3.74ms, 170 sagittal slices).</p><p id="P28">Scanning was spanned over 10 sessions (except for sub-01: 11 sessions), each consisting of 13 to 16 runs during which the subjects were presented with 86 stimuli. Each run started and ended with an 8s fixation period. The stimulus type varied randomly inside each run between images and captions. Each stimulus was presented for 2.5 seconds at the center of the screen (visual angle: 14.6 degrees; captions were displayed in white on a dark gray background (font: “Consolas”), the inter-stimulus interval was 1s. Every 10 stimuli there was a fixation trial that lasted for 2.5s. Every 5min there was a longer fixation trial for 16s.</p><p id="P29">Subjects performed a one-back matching task: They were instructed to press a button whenever the stimulus matched the immediately preceding one (cf. <xref ref-type="fig" rid="F1">Figure 1</xref> Panel A). In case the previous stimulus was of the same modality (e.g. two captions in a row), the subjects were instructed to press a button if the stimuli matched exactly. In the cross-modal case (e.g. an image followed by a caption), the button had to be pressed if the caption was a valid description of the image, and vice versa. Positive one-back trials occurred on average every 10 stimuli.</p><p id="P30">Images and captions were taken from the training and validation sets of the COCO dataset (<xref ref-type="bibr" rid="R69">Lin <italic>et al.,</italic> 2014</xref>). This dataset contains 5 matching captions for each image, of which we only considered the shortest one in order to fit on the screen and to ensure a comparable length for all captions. Spelling errors were corrected manually. As our training set, a random subset of images and another random subset of captions were selected for each subject. All these stimuli were presented only a single time. Information on the number of training stimuli for each subject is shown in <xref ref-type="fig" rid="F1">Figure 1</xref> Panel B. Additionally, a shared subset of 140 stimuli (70 images and 70 captions) was presented repeatedly to each subject in order to reduce noise, serving as our test set (on average: 26 times, min: 22, max: 31).<sup><xref ref-type="fn" rid="FN1">1</xref></sup> These stimuli were inserted randomly between the training stimuli.</p><p id="P31">Note that for each stimulus presented to the subject (e.g. an image), we also have access to the corresponding stimulus in the other modality (the corresponding caption from the COCO dataset), allowing us to estimate model features based on both modalities (vision model features extracted from the image and language model features extracted from the corresponding caption) as well as multimodal features extracted from both the image and the caption.</p><p id="P32">In addition to these perceptual trials, there were 3 imagery trials for each subject (see also <xref ref-type="fig" rid="F1">Figure 1</xref> Panel C). Prior to the first fMRI scanning session, each subject was presented with a set of 20 captions (manually selected to be diverse and easy to visualize) that were not part of the perceptual trials, and they selected 3 captions for which they felt comfortable imagining a corresponding image. Then, they learned a mapping of each caption to a number (1, 2, and 3) so that they could be instructed to perform mental imagery of a specific stimulus, without having to present them with the caption again. The imagery trials occurred every second run, either at the beginning or the end of the run, so that each of the 3 imagery conditions were repeated on average 26 times (min: 23, max: 29). At the start of the imagery trial, the imagery instruction number was presented for 2s, then there was a 1s fixation period followed by the actual imagery period during which a light gray box was depicted for 10s on a dark gray background (the same background that was also used for trials with perceptual input). The light gray box was meant to represent the area in which the mental image should be "projected". At the end of the experiment, the subjects drew sketches of the images they had been imagining during the imagery trials.</p></sec><sec id="S9"><title>fMRI Preprocessing</title><p id="P33">Preprocessing of the fMRI data was performed using SPM12 (<xref ref-type="bibr" rid="R5">Ashburner <italic>et al.,</italic> 2014</xref>) via nipype (<xref ref-type="bibr" rid="R44">Gorgolewski <italic>et al.,</italic> 2011</xref>). We applied slice time correction and realignment for each subject. Each session was coregistered with an anatomical scan of the respective subject’s first session (downsampled to 2mm<sup>3</sup>). We created and applied explicit gray matter masks for each subject based on their anatomical scans using a maximally lenient threshold (probability&gt;0).</p><p id="P34">In order to obtain beta-values for each stimulus, for each subject we fit a GLM (using SPM12) on data from all sessions. We included regressors for train images, train captions, test images, test captions, imagery trials, fixations, blank screens, button presses, and one-back target trials. One-back target trials as well as trials in which the participant pressed the button were excluded in the calculation of all training and test stimulus betas. As output of these GLMs we obtained beta-values for each training and test caption and image as well as the imagery trials.</p><p id="P35">Finally, we transformed the volume-space data to surface space Freesurfer (<xref ref-type="bibr" rid="R39">Fischl, 2012</xref>). We used trilinear interpolation and the fsaverage template in the highest possible resolution (163,842 vertices on each hemisphere) as target.</p></sec><sec id="S10"><title>Modality-Agnostic Decoders</title><p id="P36">The multimodal nature of our dataset allowed for the training of modality-agnostic decoders. We trained decoders by fitting ridge regression models that take fMRI beta-values as input and predict latent representations extracted from a pretrained deep learning model. Further details on decoder training can be found in <xref ref-type="fig" rid="F2">Figure 2</xref> as well as <xref ref-type="supplementary-material" rid="SD1">Appendix 1</xref>.</p><p id="P37">While modality-specific decoders are trained only on brain imaging data of a single modality, modality-agnostic decoders are trained on brain imaging data from multiple modalities and therefore allow for decoding of stimuli irrespective of their modality.</p><p id="P38">More specifically, in our case the modality-specific decoders are trained on fMRI beta-values from one stimulus modality, e.g. when subjects were watching images (cf. <xref ref-type="fig" rid="F2">Figure 2</xref> panel A). Conversely, modality-agnostic decoders are trained jointly using fMRI data from both stimulus modalities (images and captions; cf. <xref ref-type="fig" rid="F2">Figure 2</xref> panel B). For all decoders, the features that serve as regression targets can either be unimodal (e.g. extracted from images using a vision model) or multimodal (e.g. extracted from both stimulus modalities using a multimodal model, cf. <xref ref-type="fig" rid="F2">Figure 2</xref> panel C).</p><p id="P39">We considered features extracted from a range of vision, language, and multimodal models:</p><p id="P40">For vision features, we considered ResNet (<xref ref-type="bibr" rid="R49">He <italic>et al.,</italic> 2016</xref>), ViT (<xref ref-type="bibr" rid="R30">Dosovitskiy <italic>et al.,</italic> 2020</xref>), and DI-NOv2 (<xref ref-type="bibr" rid="R87">Oquab <italic>et al.,</italic> 2023</xref>); for language features BERT (<xref ref-type="bibr" rid="R26">Devlin <italic>et al.,</italic> 2019</xref>), GPT2 (<xref ref-type="bibr" rid="R96">Radford <italic>et al.,</italic> 2019</xref>), Llama2 (<xref ref-type="bibr" rid="R119">Touvron <italic>et al.,</italic> 2023</xref>), mistral and mixtral (<xref ref-type="bibr" rid="R53">Jiang <italic>et al.,</italic> 2023</xref>). Regarding multimodal features, we extracted features from VisualBERT (<xref ref-type="bibr" rid="R67">Li <italic>et al.,</italic> 2019</xref>), BridgeTower (<xref ref-type="bibr" rid="R127">Xu <italic>et al.,</italic> 2023</xref>), ViLT (<xref ref-type="bibr" rid="R59">Kim <italic>et al.,</italic> 2021</xref>), CLIP (<xref ref-type="bibr" rid="R95">Radford <italic>et al.,</italic> 2021</xref>), ImageBind (<xref ref-type="bibr" rid="R43">Girdhar <italic>et al.,</italic> 2023</xref>), Flava (<xref ref-type="bibr" rid="R108">Singh <italic>et al.,</italic> 2022</xref>), Blip2 (<xref ref-type="bibr" rid="R66">Li <italic>et al.,</italic> 2023</xref>), SigLip (<xref ref-type="bibr" rid="R128">Zhai <italic>et al.,</italic> 2023</xref>), and Paligemma2 (<xref ref-type="bibr" rid="R113">Steiner <italic>et al.,</italic> 2024</xref>). In order to estimate the effect of model training, we further extracted features from a randomly initialized ImageBind model as a baseline. Further details on feature extraction can be found in <xref ref-type="supplementary-material" rid="SD1">Appendix 1</xref>.</p><p id="P41">All decoders were evaluated on the held-out test data (140 stimuli, 70 captions and 70 images) using pairwise accuracy calculated using cosine distance. Prior to calculating the pairwise accuracy, the model predictions for all stimuli were standardized to have mean of 0 and standard deviation of 1.<sup><xref ref-type="fn" rid="FN2">2</xref></sup> In the case of cross-modal decoding (e.g. mapping an image stimulus into the latent space of a language model), a trial was counted as correct if the caption corresponding to the image (according to the ground-truth in COCO) was closest.</p><p id="P42"><xref ref-type="fig" rid="F3">Figure 3</xref> provides an overview on the evaluation metrics. A modality-specific decoder for images can be evaluated on its ability to decode images (Panel A, top) and in a cross-decoding setup for captions (Panel B, bottom). In the same way, we can compute the respective evaluation metrics for modality-specific decoders trained on captions. For the case of modality-agnostic decoders, we evaluate performance for decoding both images and captions using the same single decoder that is trained on both modalities (Panel C).</p></sec></sec><sec id="S11" sec-type="results"><title>Results</title><sec id="S12"><title>Modality-Agnostic Decoders</title><p id="P43">We first compared the performance of modality-specific and modality-agnostic decoders that are trained on the whole brain fMRI data based on different unimodal and multimodal features. The average pairwise accuracy scores are presented in <xref ref-type="fig" rid="F4">Figure 4</xref>. <xref ref-type="fig" rid="F5">Figure 5</xref> presents pairwise accuracy scores separately for decoding images and for decoding captions. Results for individual subjects can be found in <xref ref-type="supplementary-material" rid="SD1">Appendix 5</xref>.</p><p id="P44">When analyzing the average decoding accuracy (<xref ref-type="fig" rid="F4">Figure 4</xref>), we find that modality-agnostic decoders perform better than modality-specific decoders, irrespective of the features that the decoders were trained on. This high performance (which can be attributed to the large training dataset used by modality-agnostic decoders) is achieved despite the additional challenge of not knowing the modality of the stimulus the subject was seeing.</p><p id="P45">Further, we observed that modality-agnostic decoders based on the best multimodal features (imagebind: 85.71% ± 2.58%) do not perform substantially better than decoders based on the best language features (GPT2-large: 85.31%±2.83%) and only slightly better than decoders trained on the best vision features (Dino-giant: 82.02%±2.43%). This result suggests that high-performing modality-agnostic decoders do not necessarily need to rely on multimodal features; features extracted from language models can lead to equally high performance. When comparing the different architecture types of models for multimodal feature extraction (dual stream vs. single stream with early fusion vs. single stream with late fusion; cf. Panel C in <xref ref-type="fig" rid="F2">Figure 2</xref>), we only observed a slight performance disadvantage for single-stream models with early fusion (Mean accuracy values for dual stream models: 85.04%; for single stream models with early fusion: 81.01%; for single stream models with late fusion 83.63%).<sup><xref ref-type="fn" rid="FN3">3</xref></sup></p><p id="P46">When analyzing the performance specifically for decoding images (<xref ref-type="fig" rid="F5">Figure 5</xref>, top), we find that modality-agnostic decoders perform as well as the modality-specific decoders trained on images (crosses in top row are at the same level as the bars in <xref ref-type="fig" rid="F5">Figure 5</xref>; we find no statistically significant difference in their performances.<sup><xref ref-type="fn" rid="FN4">4</xref></sup>) Further, modality-agnostic decoders even outperform modality-specific decoders trained on captions for decoding captions (dots in the bottom row are lower than bars in <xref ref-type="fig" rid="F5">Figure 5</xref>).<sup><xref ref-type="fn" rid="FN5">5</xref></sup> In other words, even if we know that a brain pattern was recorded in response to the subject reading a caption, we are more likely to decode it accurately if we choose to apply a decoder trained using both modalities, than if we apply the appropriate decoder, trained only on captions.</p><p id="P47">Furthermore, we found that the cross-modal decoding performance for decoding visual stimuli (images) using decoders trained on linguistic stimuli (captions) is higher than the cross-modal decoding performance in the other direction, corroborating similar results from <xref ref-type="bibr" rid="R116">Tang <italic>et al.</italic> (2023a</xref>) on movies and audio books (dots in top row are higher than crosses in bottom row in <xref ref-type="fig" rid="F5">Figure 5</xref>).</p><sec id="S13"><title>Qualitative Decoding Results</title><p id="P48">To obtain a better understanding of the decoding performance of the modality-agnostic decoders, we inspected the decoding results for 5 randomly selected test stimuli. We created a large candidate set of 41,118 stimuli by combining the test stimuli and the training stimuli from all subjects. For each stimulus, we ranked these candidate set stimuli based on their similarity to the predicted feature vector. As the test stimuli were shared among all subjects, we could average the prediction feature vectors across subjects to obtain the best decoding results.</p><p id="P49"><xref ref-type="fig" rid="F6">Figure 6</xref> presents the results for decoding images using a modality-agnostic decoder trained on ImageBind features. We display the target stimulus along with the top-5 ranked test stimuli. We can observe some clear success cases (the train in the first row) but also failure cases (the teddy bear decoded as pizza). For the other stimuli, some aspects such as the high-level semantic class (e.g. vehicle, animal) are correctly decoded: For the cars on the highway (last row), the top-ranked images depict trains, which are also vehicles. For the dog (2nd row), the top images contain cats of similar colors. Regarding the giraffe (3rd row), the model appears to have picked up on the fact that there was a body of water depicted in the image.</p><p id="P50">Note that these qualitative results are not directly comparable with previous work on retrieval or reconstruction using the NSD dataset (<xref ref-type="bibr" rid="R3">Allen <italic>et al.,</italic> 2022</xref>; <xref ref-type="bibr" rid="R68">Lin <italic>et al.,</italic> 2022</xref>; <xref ref-type="bibr" rid="R115">Takagi and Nishimoto, 2023</xref>; <xref ref-type="bibr" rid="R88">Ozcelik and VanRullen, 2023</xref>), as our data was collected on a 3T MRI scanner with lower signal-to-noise-ratio than NSD’s 7T MRI scanner.</p><p id="P51">The ranking results for decoding captions are depicted in <xref ref-type="fig" rid="F7">Figure 7</xref>. The results are somewhat similar to the image decoding results, stimuli that were decoded successfully when presented as image such as the train are also decoded successfully when presented as caption; cases that were failures in the case of image decoding (e.g. the teddy bear) also fail here. However, the top-ranked stimuli for the dog (2nd row) do not always contain animals (the decoder seems to have picked up on the presence of a vehicle in the caption), but the cars on the highway (last row) get decoded rather successfully.</p><p id="P52">We additionally provide qualitative results for modality-specific decoders in <xref ref-type="supplementary-material" rid="SD1">Appendix 2</xref>. These results generally reflect the observations from the quantitative results: Modality-agnostic decoders perform similarly to modality-specific decoders evaluated in a within-modality decoding setup, but substantially better than modality-specific decoders when evaluated in cross-decoding setups.</p></sec></sec><sec id="S14"><title>Modality-Agnostic Regions</title><p id="P53">To provide insight into the spatial organization of modality-agnostic representations in the brain, we performed a surface-based searchlight analysis.</p><p id="P54">Modality-agnostic regions should contain patterns that generalize between stimulus modalities. Therefore, such regions should allow for decoding of stimuli in both modalities using a decoder that is trained to pick up on modality-agnostic features, i.e. the decoding performance for images and captions of a modality-agnostic decoder should both be above chance. However, as a modality-agnostic decoder is trained on stimuli from both modalities, it could have learned to leverage certain features to project stimuli from one modality and different features to project stimuli from the other modality. We added two conditions to control that the representations directly transfer between the modalities by additionally training two modality-specific decoders and evaluating them according to their cross-decoding performance, i.e. we require that their decoding performance in the modality they were not trained on is above chance. These four conditions are summarized at the top of <xref ref-type="fig" rid="F8">Figure 8</xref>.</p><p id="P55">We used ImageBind features for these searchlight analyses as they led to the highest decoding performance when using the whole brain data. The decoders were trained based on the surface projection of the fMRI beta-values. For each vertex, we defined a searchlight with a fixed size by selecting the 750 closest vertices, corresponding to an average radius of ~ 9.4mm.<sup><xref ref-type="fn" rid="FN6">6</xref></sup></p><p id="P56">We trained and evaluated a modality-agnostic decoder and modality-specific decoders for both modalities on the beta-values for each searchlight location and each subject, providing us with a decoding accuracy scores for each location on the cortex. Then we performed t-tests to identify locations in which the decoding performance is above chance (<italic>acc</italic> &gt; 0.5). We aggregated all 4 comparisons by taking the minimum of the 4 t-values at each spatial location. Finally, we performed a threshold-free cluster analysis (TFCE, <xref ref-type="bibr" rid="R109">Smith and Nichols, 2009</xref>) to identify modality-agnostic ROIs (<xref ref-type="fig" rid="F8">Figure 8</xref>, bottom).<sup><xref ref-type="fn" rid="FN7">7</xref></sup></p><p id="P57">To estimate the statistical significance of the resulting clusters we performed a permutation test. For each subject, we evaluated the decoders 100 times with shuffled labels to create a surrogate distribution. Then, we sampled 10,000 permutations of the 6 subjects’ surrogate distributions and calculated group-level statistics (TFCE values) for each of them. Based on this null distribution, we calculated p-values for each cluster. To control for multiple comparisons across space, we took the maximum TFCE score across vertices for each permutation (<xref ref-type="bibr" rid="R109">Smith and Nichols, 2009</xref>).</p><p id="P58">The results of the surface-based searchlight analysis are presented in <xref ref-type="fig" rid="F9">Figure 9</xref>. The analysis revealed that modality-agnostic patterns are actually widespread across the brain, especially on the left hemisphere. Peak cluster values were found in the left supramarginal gyrus, inferior parietal gyrus and posterior superior temporal sulcus. Regions belonging to the precuneus, isthmus of the cingulate gyrus, parahippocampus, middle temporal gyrus, inferior temporal gyrus and fusiform gyrus also showed high cluster values.</p></sec><sec id="S15"><title>Imagery Decoding</title><p id="P59">Finally, we evaluated the ability of decoders trained on the fMRI data with perceptual input to decode stimuli during the imagery conditions.</p><p id="P60">For a modality-agnostic decoder trained on the whole-brain data, the imagery pairwise decoding accuracy reaches 84.48% (averaged across subjects and model features) when using the 3 imagery stimuli as the candidate set.<sup><xref ref-type="fn" rid="FN8">8</xref></sup> When the whole test set is added to the candidate set (in total: 73 stimuli), the average pairwise accuracy drops to 72.47%. This substantial drop in performance is most likely explained by the fact that the predicted features for the imagery trials were standardized using only 3 stimuli, and this transformation emphasized differences that enabled distinguishing the 3 imagery trials but do not generalize to the larger test set.<sup><xref ref-type="fn" rid="FN9">9</xref></sup></p><p id="P61">As expected, we also found that modality-agnostic decoders are better suited for imagery decoding than modality-specific decoders. We compared the imagery decoding accuracy of both decoder types taking into account the results for all features and all subjects. To this end, we performed two repeated measures ANOVAs (grouping the data by subject), once comparing the accuracy values of modality-agnostic decoders with those of modality-specific decoders trained on images, and once comparing modality-agnostic decoders to modality-specific decoders trained on captions. The average decoding accuracies were 69.42% for a modality-specific decoder trained on images and 70.02% for a modality-specific decoder trained on captions (vs. 72.47% for a modality-agnostic decoder, as mentioned above). In both comparisons, the accuracy values for the two decoder types were significantly different (when comparing modality-agnostic decoders to modality-specific decoders trained on images: decoder_type: <italic>β</italic> = 0.03, <italic>SE</italic> = 0.011, <italic>p</italic> &lt; 0.01; and when comparing to modality-specific decoders trained on captions: decoder_type: <italic>β</italic> = 0.024, <italic>SE</italic> = 0.012, <italic>p</italic> &lt; 0.04).</p><p id="P62"><xref ref-type="supplementary-material" rid="SD1">Appendix 6</xref> presents qualitative decoding results for the imagery trials for each subject as well as the sketches of the mental images drawn at the end of the experiment. As expected, the results are worse than those for perceived stimuli, but for several subjects it was possible to decode some major semantic concepts.</p><p id="P63">We further computed the imagery decoding accuracy during the searchlight analysis. <xref ref-type="fig" rid="F10">Figure 10</xref> shows the result clusters for decoding imagery (using the whole test set + the 3 imagery trials as potential candidates).</p><p id="P64">We observe that many regions that were found to contain modality-agnostic patterns (cf. <xref ref-type="fig" rid="F9">Figure 9</xref>) are also regions in which decoding of mental imagery is possible.</p><p id="P65">One main difference is that the imagery decoding clusters appear to be less left-lateralized than the modality-agnostic region clusters (peak cluster values can be found both on the right inferior parietal cortex and bilaterally in the precuneus). To estimate overlap of the regions allowing for imagery decoding and modality-agnostic regions we calculated the correlation between the TFCE values that were used for identifying modality-agnostic regions (<xref ref-type="fig" rid="F9">Figure 9</xref>) and the TFCE values for imagery decoding (<xref ref-type="fig" rid="F10">Figure 10</xref>). The Pearson correlation score for the left hemisphere is 0.41 (<italic>p</italic> &lt; 1<italic>e</italic> − 8), and for the right hemisphere 0.62 (<italic>p</italic> &lt; 1<italic>e</italic> − 8). Importantly, these correlation scores are substantially higher when compared to the correlation with decoding accuracy of modality-specific decoders: The correlation between the TFCE values for imagery decoding and TFCE values for image decoding of a modality-specific decoder trained on images is 0.28 on the left hemisphere and 0.40 on the right hemisphere. When using TFCE values based on the caption decoding accuracy of a modality-specific decoder trained on captions we obtain 0.19 on the left hemisphere and 0.45 on the right hemisphere.</p></sec></sec><sec id="S16" sec-type="discussion"><title>Discussion</title><p id="P66">In this work, we introduced a new large-scale multimodal fMRI dataset that enables the development of models for modality-agnostic decoding of visual and linguistic stimuli using a single model. These modality-agnostic decoders were specifically trained to pick up on modality-agnostic patterns, enabling a performance increase over modality-specific decoders when decoding linguistic stimuli in the form of captions.</p><p id="P67">According to a range of theories, modality-agnostic representations are tightly linked to (lexical-) semantic representations (<xref ref-type="bibr" rid="R107">Simmons and Barsalou, 2003</xref>; <xref ref-type="bibr" rid="R13">Binder <italic>et al.,</italic> 2009</xref>; <xref ref-type="bibr" rid="R77">Meschke and Gallant, 2024</xref>). Most importantly, a range of studies that aimed to identify brain regions linked to semantic/conceptual representations by asking subjects to perform tasks that require semantic processing of words found evidence for such regions that overlap to a high degree with the regions identified in our study (<xref ref-type="bibr" rid="R37">Fernandino <italic>et al.,</italic> 2016</xref>; <xref ref-type="bibr" rid="R76">Martin <italic>et al.,</italic> 2018</xref>; <xref ref-type="bibr" rid="R19">Carota <italic>et al.,</italic> 2021</xref>; <xref ref-type="bibr" rid="R38">Fernandino <italic>et al.,</italic> 2022</xref>; <xref ref-type="bibr" rid="R118">Tong <italic>et al.,</italic> 2022</xref>). A strong link between these systems could also explain our result that modality-agnostic decoders based on unimodal representations from language models are performing as well as decoders based on multimodal representations (cf. <xref ref-type="fig" rid="F4">Figure 4</xref>), as well as the partial left-lateralization of the identified modality-agnostic regions.</p><p id="P68">In a second analysis, we additionally leveraged our dataset to localize modality-agnostic regions in the brain by searching for areas in which decoding of both stimulus modalities is possible using modality-agnostic decoders as well as in a cross-decoding setup. This approach lead to the identification of a large network involving temporal, parietal, and frontal regions, and peak cluster values on the left hemisphere (cf. <xref ref-type="fig" rid="F9">Figure 9</xref>). All areas with high cluster values confirm findings from previous studies: The left precuneus (<xref ref-type="bibr" rid="R105">Shinkareva <italic>et al.,</italic> 2011</xref>; <xref ref-type="bibr" rid="R32">Fairhall and Caramazza, 2013</xref>; <xref ref-type="bibr" rid="R94">Popham <italic>et al.,</italic> 2021</xref>; <xref ref-type="bibr" rid="R47">Handjaras <italic>et al.,</italic> 2016</xref>), posterior cingulate/ retrosplenial cortex (<xref ref-type="bibr" rid="R32">Fairhall and Caramazza, 2013</xref>; <xref ref-type="bibr" rid="R47">Handjaras <italic>et al.,</italic> 2016</xref>), supramarginal gyrus (<xref ref-type="bibr" rid="R105">Shinkareva <italic>et al.,</italic> 2011</xref>), inferior parietal cortex (<xref ref-type="bibr" rid="R73">Man <italic>et al.,</italic> 2012</xref>; <xref ref-type="bibr" rid="R121">Vandenberghe <italic>et al.,</italic> 1996</xref>; <xref ref-type="bibr" rid="R105">Shinkareva <italic>et al.,</italic> 2011</xref>; <xref ref-type="bibr" rid="R25">Devereux <italic>et al.,</italic> 2013</xref>; <xref ref-type="bibr" rid="R94">Popham <italic>et al.,</italic> 2021</xref>; <xref ref-type="bibr" rid="R106">Simanova <italic>et al.,</italic> 2014</xref>; <xref ref-type="bibr" rid="R47">Handjaras <italic>et al.,</italic> 2016</xref>), superior temporal sulcus (<xref ref-type="bibr" rid="R73">Man <italic>et al.,</italic> 2012</xref>), middle temporal gyrus (<xref ref-type="bibr" rid="R121">Vandenberghe <italic>et al.,</italic> 1996</xref>; <xref ref-type="bibr" rid="R105">Shinkareva <italic>et al.,</italic> 2011</xref>; <xref ref-type="bibr" rid="R32">Fairhall and Caramazza, 2013</xref>; <xref ref-type="bibr" rid="R25">Devereux <italic>et al.,</italic> 2013</xref>; <xref ref-type="bibr" rid="R47">Handjaras <italic>et al.,</italic> 2016</xref>), inferior temporal gyrus (<xref ref-type="bibr" rid="R121">Vandenberghe <italic>et al.,</italic> 1996</xref>; <xref ref-type="bibr" rid="R105">Shinkareva <italic>et al.,</italic> 2011</xref>; <xref ref-type="bibr" rid="R32">Fairhall and Caramazza, 2013</xref>; <xref ref-type="bibr" rid="R106">Simanova <italic>et al.,</italic> 2014</xref>; <xref ref-type="bibr" rid="R47">Handjaras <italic>et al.,</italic> 2016</xref>), fusiform gyrus (<xref ref-type="bibr" rid="R121">Vandenberghe <italic>et al.,</italic> 1996</xref>; <xref ref-type="bibr" rid="R79">Moore and Price, 1999</xref>; <xref ref-type="bibr" rid="R18">Bright <italic>et al.,</italic> 2004</xref>; <xref ref-type="bibr" rid="R105">Shinkareva <italic>et al.,</italic> 2011</xref>; <xref ref-type="bibr" rid="R32">Fairhall and Caramazza, 2013</xref>; <xref ref-type="bibr" rid="R106">Simanova <italic>et al.,</italic> 2014</xref>), and parahippocampus (<xref ref-type="bibr" rid="R121">Vandenberghe <italic>et al.,</italic> 1996</xref>). However, previous studies have led to contradicting results regarding the locality of modality-agnostic regions (they were identifying varying subsets of these regions; see also <xref ref-type="supplementary-material" rid="SD1">Appendix 3</xref>), probably due to the limited number and artificial nature of stimuli employed. Our method identified <italic>almost all</italic> of the previously proposed regions as regions with modality-agnostic patterns, highlighting the advantage of this large multi-modal dataset in which subjects are viewing photographs of complex natural scenes and reading full English sentences.<sup><xref ref-type="fn" rid="FN10">10</xref></sup></p><p id="P69">The fact that the presence of modality-agnostic patterns is positively correlated with the imagery decoding performance in different locations provides further evidence that the identified patterns are truly modality-agnostic. We further found that decoders trained exclusively on data for which participants were exposed to perceptual input do generalize to imagery trials, confirming previous findings that were based on more limited stimulus sets (<xref ref-type="bibr" rid="R114">Stokes <italic>et al.,</italic> 2009</xref>; <xref ref-type="bibr" rid="R98">Reddy <italic>et al.,</italic> 2010</xref>; <xref ref-type="bibr" rid="R65">Lee <italic>et al.,</italic> 2012</xref>; <xref ref-type="bibr" rid="R54">Johnson and Johnson, 2014</xref>; <xref ref-type="bibr" rid="R81">Naselaris <italic>et al.,</italic> 2015</xref>). Regarding the representations involved in mental imagery, we found that modality-agnostic decoders outperform modality-specific decoders in terms of imagery decoding. This finding can be seen as support for the involvement of modality-agnostic representations in mental imagery, as modality-agnostic decoders were trained explicitly to pick up on such patterns.</p><p id="P70">The findings of our searchlight analysis for imagery decoding suggest that mental imagery indeed involves a large network of regions across both hemispheres of the cerebral cortex. This includes high-level visual areas, parietal areas such as the precuneus and inferior parietal cortex and several frontal regions, but also parts of the early visual cortex. Results are highly similar on both hemispheres, highlighting the involvement of large-scale bilateral brain networks during mental imagery of complex scenes.</p><p id="P71">While there are lesion studies on hemispheric asymmetries that suggest that regions in the left hemisphere are crucial for mental imagery (<xref ref-type="bibr" rid="R33">Farah, 1984</xref>; <xref ref-type="bibr" rid="R9">Bartolomeo, 2002</xref>), a more recent review that additionally considers evidence from neuroimaging and direct cortical stimulation studies suggests that frontoparietal networks in both hemispheres are involved in mental imagery, and that lateralization patterns can be found in the temporal lobes (<xref ref-type="bibr" rid="R70">Liu <italic>et al.,</italic> 2022</xref>). Such lateralization was found to depend on the nature of the imagined items, the imagination of objects and words involving the left inferior temporal cortex while the imagination of faces and people was found to be more right-lateralized and the imagination of complex scenes (as in our study) leads to significant activity in both hemispheres (<xref ref-type="bibr" rid="R85">O’Craven and Kanwisher, 2000</xref>; <xref ref-type="bibr" rid="R112">Steel <italic>et al.,</italic> 2021</xref>; <xref ref-type="bibr" rid="R111">Spagna <italic>et al.,</italic> 2021</xref>). Crucially, in more recent decoding studies, results were either observed bilaterally, or the analyses did not target hemispheric asymmetries (<xref ref-type="bibr" rid="R98">Reddy <italic>et al.,</italic> 2010</xref>; <xref ref-type="bibr" rid="R65">Lee <italic>et al.,</italic> 2012</xref>).<sup><xref ref-type="fn" rid="FN11">11</xref></sup> Our work shows for the first time results of a searchlight analysis of imagery decoding of complex visual scenes. Above-chance decoding is possible on both hemispheres, with the highest decoding accuracies in the precuneus and the right inferior parietal cortex and the superior temporal sulcus. Future investigations with larger sets of imagined scenes could address the question whether lateralization patterns depend on the nature of the imagined objects.</p><p id="P72">It remains an open question whether the activation patterns in the modality-agnostic regions identified in our study relate to abstract concepts or to lower-level features that are shared between the two modalities. <xref ref-type="bibr" rid="R12">Binder (2016</xref>) puts this dichotomy into question, considering that “there is no absolute demarcation between embodied/perceptual and abstract/conceptual representation in the brain.” (p. 1098). The author argues for a hierarchical system in which representational patterns become increasingly abstract, creating a continuum between actual experiential information up to higher-level conceptual information (see also <xref ref-type="bibr" rid="R4">Andrews <italic>et al.,</italic> 2014</xref>).</p><p id="P73">According to the results of our searchlight analysis, the anterior temporal lobes are not among the regions with the highest probability of being modality-agnostic, contradicting the hypothesis of the hub-and-spoke theory that these areas are the major semantic hub in the brain. However, MRI signals from these regions have a lower signal-to-noise ratio with standard fMRI pulse sequences (<xref ref-type="bibr" rid="R27">Devlin <italic>et al.,</italic> 2000</xref>; <xref ref-type="bibr" rid="R31">Embleton <italic>et al.,</italic> 2010</xref>). A more targeted study with an adapted fMRI protocol would be required to shed light on the nature of patterns in these regions. More generally, the hub-and-spoke theory also puts emphasis on the role of spokes for the formation of conceptual representations (<xref ref-type="bibr" rid="R93">Pobric <italic>et al.,</italic> 2010</xref>; <xref ref-type="bibr" rid="R97">Ralph <italic>et al.,</italic> 2017</xref>). Future work could be aimed at testing the hub-and-spoke theory proposal that features in hierarchically lower level representation spaces of the spokes are not directly relatable to features in the representation space of the hubs: Object representations in the spokes are based on interpretable features (e.g. shape, color, affordances of an object) and get translated into another representational format that is representing conceptual similarities (but its dimensions do not directly map to interpretable features) in the semantic hub (<xref ref-type="bibr" rid="R41">Frisby <italic>et al.,</italic> 2023</xref>). To test this hypothesis, modality-agnostic representations in the anterior temporal lobes (measured with targeted fMRI pulse sequences) could be compared to representations in candidate regions for modality-specific spokes using RSA.</p><p id="P74">The modality-agnostic regions we found in the searchlight analysis can also be seen as candidates for convergence zones, in which increasingly abstract representations are formed (<xref ref-type="bibr" rid="R22">Damasio, 1989</xref>; <xref ref-type="bibr" rid="R120">Tranel <italic>et al.,</italic> 1997</xref>; <xref ref-type="bibr" rid="R78">Meyer and Damasio, 2009</xref>). To obtain further insight into the hierarchical organization of these zones, future work could take advantage of the improved temporal resolution of other brain imaging techniques such as MEG to explore in which areas modality-agnostic patterns are formed first, and how they are being transformed when spreading to higher-level areas of the brain (<xref ref-type="bibr" rid="R29">Dirani and Pylkkänen, 2024</xref>; <xref ref-type="bibr" rid="R11">Benchetrit <italic>et al.,</italic> 2024</xref>).</p><p id="P75">In line with the GRAPES framework, (<xref ref-type="bibr" rid="R74">Martin, 2009</xref>, <xref ref-type="bibr" rid="R75">2016</xref>), we found that modality-agnostic representations are distributed across temporal and parietal areas. To test the related hypothesis that conceptual information is organized in domains, we plan to use RSA to understand which kind of semantic information is represented in the different modality-agnostic regions identified.</p><p id="P76">Finally, our results can be interpreted with respect to the Global Workspace Theory. All modality-agnostic regions are good candidate regions for a global workspace. They could, however, also be part of modality-specific modules that get activated in a modality-agnostic fashion through a “broadcast” operation as a stimulus is perceived consciously (<xref ref-type="bibr" rid="R6">Baars, 1993</xref>, <xref ref-type="bibr" rid="R7">2005</xref>). To distinguish these two cases, an experimental manipulation of attention could be used: according to Global Workspace Theory, attention is required for information to enter the workspace, but not for the workspace signals to reach other brain regions via broadcast. In the future, we plan to investigate how modality-agnostic patterns are modulated by attention, by analyzing additional test sessions from the same subjects in which they were instructed in specific runs to pay attention to only one of the modalities.<sup><xref ref-type="fn" rid="FN12">12</xref></sup></p><p id="P77">To conclude, the results from our searchlight analysis so far are in line with all major theories on modality-agnostic representations that were considered. As this dataset will be shared publicly, more targeted investigations can be performed by the research community in order to adjudicate between different theories.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Appendix</label><media xlink:href="EMS206394-supplement-Appendix.pdf" mimetype="application" mime-subtype="pdf" id="d119aAcFbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S17"><title>Acknowledgments</title><p>This research was funded by grants from the French Agence Nationale de la Recherche (ANR: AI-REPS grant number ANR-18-CE37-0007-01 and ANITI grant number ANR-19-PI3A-0004) as well as the European Union (ERC Advanced grant GLoW, 101096017). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them.</p><p>We thank the Inserm/UPS UMR1214 Technical Platform for their help in setting up and for the acquisitions of the MRI sequences.</p></ack><fn-group><fn id="FN1"><label>1</label><p id="P78">Contrary to the training stimuli which were randomly selected from the COCO dataset, the 70 test stimuli were chosen by hand to avoid including multiple scenes that could match the same semantic description. The 70 chosen images as well as their 70 corresponding captions constituted the test set.</p></fn><fn id="FN2"><label>2</label><p id="P79">In the case of imagery decoding, the model predictions were standardized separately.</p></fn><fn id="FN3"><label>3</label><p id="P80">We performed a repeated measures ANOVA (grouping the data by subject), comparing the decoding accuracy values of modality-agnostic decoders based on different families of multimodal features. The only significant effect was: model_family_single_stream_early_fusion: <italic>β</italic> = −0.04, <italic>SE</italic> = 0.011, <italic>p</italic> &lt; 1<italic>e</italic> − 3.</p></fn><fn id="FN4"><label>4</label><p id="P81">We performed a repeated measures ANOVA (grouping the data by subject), comparing the image decoding accuracy values of modality-agnostic decoders with those of modality-specific decoders trained on images. The resulting p-value for the effect of the decoder_type was <italic>p</italic> = 0.73.</p></fn><fn id="FN5"><label>5</label><p id="P82">We performed a repeated measures ANOVA (grouping the data by subject), comparing the caption decoding accuracy values of modality-agnostic decoders with those of modality-specific decoders trained on captions. The result was: decoder_type : <italic>β</italic> = 0.036, <italic>SE</italic> = 0.006, <italic>p</italic> &lt; 1 ⋅ 10<sup>−8</sup>.</p></fn><fn id="FN6"><label>6</label><p id="P83">Details on how this size was selected are outlined in <xref ref-type="supplementary-material" rid="SD1">Appendix 4</xref>.</p></fn><fn id="FN7"><label>7</label><p id="P84">We used the default hyperparameters of <italic>h</italic> = 2 and <italic>e</italic> = 1 for surface-based TFCE (<xref ref-type="bibr" rid="R52">Jenkinson <italic>et al.,</italic> 2012</xref>).</p></fn><fn id="FN8"><label>8</label><p id="P85">Note that we used the ground-truth caption and corresponding image from COCO in this candidate set, and not the sketches drawn by the subjects.</p></fn><fn id="FN9"><label>9</label><p id="P86">We also attempted decoding without standardization of the predicted feature vectors, but this led to much lower performance.</p></fn><fn id="FN10"><label>10</label><p id="P87">The left superior occipital gyrus was not identified in our study, but in previous studies by <xref ref-type="bibr" rid="R121">Vandenberghe <italic>et al.</italic> (1996</xref>); <xref ref-type="bibr" rid="R105">Shinkareva <italic>et al.</italic> (2011</xref>). However, we found that a major part of the left superior occipital <italic>sulcus</italic> represents modality-agnostic information. Further, <xref ref-type="bibr" rid="R55">Jung <italic>et al.</italic> (2018</xref>) found modality-agnostic patterns in the right superior frontal gyrus. One major difference between their study and ours is that they used auditory input as a second modality instead of text. Further work is required to investigate to what extent the modality-agnostic regions identified in our work generalize to all modalities.</p></fn><fn id="FN11"><label>11</label><p id="P88">But see <xref ref-type="bibr" rid="R114">Stokes <italic>et al.</italic> (2009</xref>) in which perception-to-imagery generalization of single letters was left-lateralized.</p></fn><fn id="FN12"><label>12</label><p id="P89">These sessions will be released as part of another future dataset and publication.</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albers</surname><given-names>A</given-names></name><name><surname>Kok</surname><given-names>P</given-names></name><name><surname>Toni</surname><given-names>I</given-names></name><name><surname>Dijkerman</surname><given-names>HC</given-names></name><name><surname>de Lange</surname><given-names>F</given-names></name></person-group><article-title>Shared Representations for Working Memory and Mental Imagery in Early Visual Cortex</article-title><source>Current Biology</source><year>2013</year><month>Aug</month><volume>23</volume><issue>15</issue><fpage>1427</fpage><lpage>1431</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0960982213006908">https://linkinghub.elsevier.com/retrieve/pii/S0960982213006908</ext-link></comment><pub-id pub-id-type="pmid">23871239</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aliko</surname><given-names>S</given-names></name><name><surname>Huang</surname><given-names>J</given-names></name><name><surname>Gheorghiu</surname><given-names>F</given-names></name><name><surname>Meliss</surname><given-names>S</given-names></name><name><surname>Skipper</surname><given-names>JI</given-names></name></person-group><article-title>A naturalistic neuroimaging database for understanding the brain using ecological stimuli</article-title><source>Scientific Data</source><year>2020</year><month>Oct</month><volume>7</volume><issue>1</issue><fpage>347</fpage><pub-id pub-id-type="pmcid">PMC7555491</pub-id><pub-id pub-id-type="pmid">33051448</pub-id><pub-id pub-id-type="doi">10.1038/s41597-020-00680-2</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>EJ</given-names></name><name><surname>St-Yves</surname><given-names>G</given-names></name><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>Breedlove</surname><given-names>JL</given-names></name><name><surname>Prince</surname><given-names>JS</given-names></name><name><surname>Dowdle</surname><given-names>LT</given-names></name><name><surname>Nau</surname><given-names>M</given-names></name><name><surname>Caron</surname><given-names>B</given-names></name><name><surname>Pestilli</surname><given-names>F</given-names></name><name><surname>Charest</surname><given-names>I</given-names></name><name><surname>Hutchinson</surname><given-names>JB</given-names></name><etal/></person-group><article-title>A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence</article-title><source>Nature Neuroscience</source><year>2022</year><volume>25</volume><issue>1</issue><fpage>116</fpage><lpage>126</lpage><pub-id pub-id-type="pmid">34916659</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andrews</surname><given-names>M</given-names></name><name><surname>Frank</surname><given-names>S</given-names></name><name><surname>Vigliocco</surname><given-names>G</given-names></name></person-group><article-title>Reconciling Embodied and Distributional Accounts of Meaning in Language</article-title><source>Topics in Cognitive Science</source><year>2014</year><month>Jul</month><volume>6</volume><issue>3</issue><fpage>359</fpage><lpage>370</lpage><pub-id pub-id-type="pmid">24935903</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashburner</surname><given-names>J</given-names></name><name><surname>Barnes</surname><given-names>G</given-names></name><name><surname>Chen</surname><given-names>CC</given-names></name><name><surname>Daunizeau</surname><given-names>J</given-names></name><name><surname>Flandin</surname><given-names>G</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name><name><surname>Kiebel</surname><given-names>S</given-names></name><name><surname>Kilner</surname><given-names>J</given-names></name><name><surname>Litvak</surname><given-names>V</given-names></name><name><surname>Moran</surname><given-names>R</given-names></name></person-group><article-title>SPM12</article-title><source>Wellcome Trust Centre for Neuroimaging</source><year>2014</year></element-citation></ref><ref id="R6"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Baars</surname><given-names>BJ</given-names></name></person-group><source>A cognitive theory of consciousness</source><publisher-name>Cambridge University Press</publisher-name><year>1993</year></element-citation></ref><ref id="R7"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Baars</surname><given-names>BJ</given-names></name></person-group><article-title>Global workspace theory of consciousness: toward a cognitive neuroscience of human experience</article-title><source>Progress in Brain Research</source><publisher-name>Elsevier</publisher-name><year>2005</year><volume>150</volume><fpage>45</fpage><lpage>53</lpage><pub-id pub-id-type="pmid">16186014</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barsalou</surname><given-names>LW</given-names></name></person-group><article-title>On Staying Grounded and Avoiding Quixotic Dead Ends</article-title><source>Psychonomic Bulletin &amp; Review</source><year>2016</year><volume>23</volume><issue>4</issue><fpage>1122</fpage><lpage>1142</lpage><pub-id pub-id-type="pmcid">PMC4974262</pub-id><pub-id pub-id-type="pmid">27112560</pub-id><pub-id pub-id-type="doi">10.3758/s13423-016-1028-3</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartolomeo</surname><given-names>P</given-names></name></person-group><article-title>The Relationship Between Visual Perception and Visual Mental Imagery: A Reappraisal of the Neuropsychological Evidence</article-title><source>Cortex</source><year>2002</year><month>Jan</month><volume>38</volume><issue>3</issue><fpage>357</fpage><lpage>378</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0010945208706658">https://linkinghub.elsevier.com/retrieve/pii/S0010945208706658</ext-link></comment><pub-id pub-id-type="pmid">12146661</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beliy</surname><given-names>R</given-names></name><name><surname>Gaziv</surname><given-names>G</given-names></name><name><surname>Hoogi</surname><given-names>A</given-names></name><name><surname>Strappini</surname><given-names>F</given-names></name><name><surname>Golan</surname><given-names>T</given-names></name><name><surname>Irani</surname><given-names>M</given-names></name></person-group><article-title>From voxels to pixels and back: Self-supervision in natural-image reconstruction from fMRI</article-title><source>NeurIPS</source><year>2019</year></element-citation></ref><ref id="R11"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Benchetrit</surname><given-names>Y</given-names></name><name><surname>Banville</surname><given-names>H</given-names></name><name><surname>King</surname><given-names>JR</given-names></name></person-group><source>Brain decoding: toward real-time reconstruction of visual perception</source><conf-name>The Twelfth International Conference on Learning Representations</conf-name><year>2024</year></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Binder</surname><given-names>JR</given-names></name></person-group><article-title>In defense of abstract conceptual representations</article-title><source>Psychonomic Bulletin &amp; Review</source><year>2016</year><month>Aug</month><volume>23</volume><issue>4</issue><fpage>1096</fpage><lpage>1108</lpage><pub-id pub-id-type="pmid">27294428</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Binder</surname><given-names>JR</given-names></name><name><surname>Desai</surname><given-names>RH</given-names></name><name><surname>Graves</surname><given-names>WW</given-names></name><name><surname>Conant</surname><given-names>LL</given-names></name></person-group><article-title>Where Is the Semantic System? A Critical Review and Meta-Analysis of 120 Functional Neuroimaging Studies</article-title><source>Cerebral Cortex</source><year>2009</year><volume>19</volume><issue>12</issue><fpage>2767</fpage><lpage>2796</lpage><pub-id pub-id-type="pmcid">PMC2774390</pub-id><pub-id pub-id-type="pmid">19329570</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhp055</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boccia</surname><given-names>M</given-names></name><name><surname>Sulpizio</surname><given-names>V</given-names></name><name><surname>Teghil</surname><given-names>A</given-names></name><name><surname>Palermo</surname><given-names>L</given-names></name><name><surname>Piccardi</surname><given-names>L</given-names></name><name><surname>Galati</surname><given-names>G</given-names></name><name><surname>Guariglia</surname><given-names>C</given-names></name></person-group><article-title>The dynamic contribution of the high-level visual cortex to imagery and perception</article-title><source>Human Brain Mapping</source><year>2019</year><month>Jan</month><volume>40</volume><issue>8</issue><fpage>2449</fpage><lpage>2463</lpage><pub-id pub-id-type="pmcid">PMC6865452</pub-id><pub-id pub-id-type="pmid">30702203</pub-id><pub-id pub-id-type="doi">10.1002/hbm.24535</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Boyle</surname><given-names>JA</given-names></name><name><surname>Pinsard</surname><given-names>B</given-names></name><name><surname>Boukhdhir</surname><given-names>A</given-names></name><name><surname>Belleville</surname><given-names>S</given-names></name><name><surname>Brambatti</surname><given-names>S</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Cohen-Adad</surname><given-names>J</given-names></name><name><surname>Cyr</surname><given-names>A</given-names></name><name><surname>Fuente Rainville</surname><given-names>P</given-names></name><name><surname>Bellec</surname><given-names>P</given-names></name></person-group><source>The Courtois project on neuronal modelling-first data release</source><conf-name>26th annual meeting of the organization for human brain mapping</conf-name><year>2020</year></element-citation></ref><ref id="R16"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Brennan</surname><given-names>JR</given-names></name></person-group><source>Language and the brain: a slim guide to neurolinguistics</source><publisher-name>Oxford University Press</publisher-name><year>2022</year><comment><ext-link ext-link-type="uri" xlink:href="https://books.google.co.in/books?hl=en&amp;lr=&amp;id=cDRtEAAAQBAJ&amp;oi=fnd&amp;pg=PP1&amp;dq=brennan+language++brain&amp;ots=9SYgp7FBJy&amp;sig=PTuVqNSsF7NlwWWNboXe-ruH7uM&amp;redir_esc=y#v=onepage&amp;q=brennan%20language%20%20brain&amp;f=false">https://books.google.com/books?hl=en&amp;lr=&amp;id=cDRtEAAAQBAJ&amp;oi=fnd&amp;pg=PP1&amp;dq=brennan+language++brain&amp;ots=9SYgp7FBJy&amp;sig=PTuVqNSsF7NlwWWNboXe-ruH7uM</ext-link></comment></element-citation></ref><ref id="R17"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Brennan</surname><given-names>JR</given-names></name><name><surname>Hale</surname><given-names>JT</given-names></name></person-group><article-title>Hierarchical structure guides rapid linguistic predictions during naturalistic listening</article-title><source>PLOS ONE</source><publisher-name>Public Library of Science</publisher-name><year>2019</year><month>Jan</month><volume>14</volume><issue>1</issue><elocation-id>e0207741</elocation-id><pub-id pub-id-type="pmcid">PMC6334990</pub-id><pub-id pub-id-type="pmid">30650078</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0207741</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bright</surname><given-names>P</given-names></name><name><surname>Moss</surname><given-names>H</given-names></name><name><surname>Tyler</surname><given-names>LK</given-names></name></person-group><article-title>Unitary vs multiple semantics: PET studies of word and picture processing</article-title><source>Brain and Language</source><year>2004</year><volume>89</volume><issue>3</issue><fpage>417</fpage><lpage>432</lpage><pub-id pub-id-type="pmid">15120534</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carota</surname><given-names>F</given-names></name><name><surname>Nili</surname><given-names>H</given-names></name><name><surname>Pulvermüller</surname><given-names>F</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Distinct fronto-temporal substrates of distributional and tax-onomic similarity among words: evidence from RSA of BOLD signals</article-title><source>NeuroImage</source><year>2021</year><volume>224</volume><elocation-id>117408</elocation-id><pub-id pub-id-type="pmid">33049407</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>N</given-names></name><name><surname>Pyles</surname><given-names>JA</given-names></name><name><surname>Marcus</surname><given-names>A</given-names></name><name><surname>Gupta</surname><given-names>A</given-names></name><name><surname>Tarr</surname><given-names>MJ</given-names></name><name><surname>Aminoff</surname><given-names>EM</given-names></name></person-group><article-title>BOLD5000, a public fMRI dataset while viewing 5000 visual images</article-title><source>Scientific Data</source><year>2019</year><volume>6</volume><issue>1</issue><fpage>49</fpage><pub-id pub-id-type="pmcid">PMC6502931</pub-id><pub-id pub-id-type="pmid">31061383</pub-id><pub-id pub-id-type="doi">10.1038/s41597-019-0052-3</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>DD</given-names></name><name><surname>Savoy</surname><given-names>RL</given-names></name></person-group><article-title>Functional magnetic resonance imaging (fMRI) “brain reading”: detecting and classifying distributed patterns of fMRI activity in human visual cortex</article-title><source>NeuroImage</source><year>2003</year><month>Jun</month><volume>19</volume><issue>2</issue><fpage>261</fpage><lpage>270</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S1053811903000491">https://www.sciencedirect.com/science/article/pii/S1053811903000491</ext-link></comment><pub-id pub-id-type="pmid">12814577</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Damasio</surname><given-names>AR</given-names></name></person-group><article-title>The Brain Binds Entities and Events by Multiregional Activation from Convergence Zones</article-title><source>Neural Computation</source><year>1989</year><volume>1</volume><issue>1</issue><fpage>123</fpage><lpage>132</lpage></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Damasio</surname><given-names>H</given-names></name><name><surname>Tranel</surname><given-names>D</given-names></name><name><surname>Grabowski</surname><given-names>T</given-names></name><name><surname>Adolphs</surname><given-names>R</given-names></name><name><surname>Damasio</surname><given-names>A</given-names></name></person-group><article-title>Neural systems behind word and concept retrieval</article-title><source>Cognition</source><year>2004</year><month>May</month><volume>92</volume><issue>1-2</issue><fpage>179</fpage><lpage>229</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0010027703002312">https://linkinghub.elsevier.com/retrieve/pii/S0010027703002312</ext-link></comment><pub-id pub-id-type="pmid">15037130</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desikan</surname><given-names>RS</given-names></name><name><surname>Ségonne</surname><given-names>F</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Quinn</surname><given-names>BT</given-names></name><name><surname>Dickerson</surname><given-names>BC</given-names></name><name><surname>Blacker</surname><given-names>D</given-names></name><name><surname>Buckner</surname><given-names>RL</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Maguire</surname><given-names>RP</given-names></name><name><surname>Hyman</surname><given-names>BT</given-names></name><name><surname>Albert</surname><given-names>MS</given-names></name><etal/></person-group><article-title>An automated labeling system for subdividing the human cerebral cortex on MRI scans into gyral based regions of interest</article-title><source>NeuroImage</source><year>2006</year><volume>31</volume><issue>3</issue><fpage>968</fpage><lpage>980</lpage><pub-id pub-id-type="pmid">16530430</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devereux</surname><given-names>BJ</given-names></name><name><surname>Clarke</surname><given-names>A</given-names></name><name><surname>Marouchos</surname><given-names>A</given-names></name><name><surname>Tyler</surname><given-names>LK</given-names></name></person-group><article-title>Representational Similarity Analysis Reveals Commonalities and Differences in the Semantic Processing of Words and Objects</article-title><source>The Journal of Neuroscience</source><year>2013</year><volume>33</volume><issue>48</issue><pub-id pub-id-type="pmcid">PMC3852350</pub-id><pub-id pub-id-type="pmid">24285896</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3809-13.2013</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Devlin</surname><given-names>J</given-names></name><name><surname>Chang</surname><given-names>MW</given-names></name><name><surname>Lee</surname><given-names>K</given-names></name><name><surname>Toutanova</surname><given-names>K</given-names></name></person-group><source>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</source><conf-name>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</conf-name><conf-sponsor>Association for Computational Linguistics</conf-sponsor><conf-loc>Minneapolis, Minnesota</conf-loc><year>2019</year><fpage>4171</fpage><lpage>4186</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://aclanthology.org/N19-1423">https://aclanthology.org/N19-1423</ext-link></comment><pub-id pub-id-type="doi">10.18653/v1/N19-1423</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devlin</surname><given-names>JT</given-names></name><name><surname>Russell</surname><given-names>RP</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name><name><surname>Price</surname><given-names>CJ</given-names></name><name><surname>Wilson</surname><given-names>J</given-names></name><name><surname>Moss</surname><given-names>HE</given-names></name><name><surname>Matthews</surname><given-names>PM</given-names></name><name><surname>Tyler</surname><given-names>LK</given-names></name></person-group><article-title>Susceptibility-Induced Loss of Signal: Comparing PET and fMRI on a Semantic Task</article-title><source>NeuroImage</source><year>2000</year><month>Jun</month><volume>11</volume><issue>6</issue><fpage>589</fpage><lpage>600</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S1053811900905950">https://linkinghub.elsevier.com/retrieve/pii/S1053811900905950</ext-link></comment><pub-id pub-id-type="pmid">10860788</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dijkstra</surname><given-names>N</given-names></name><name><surname>Bosch</surname><given-names>SE</given-names></name><name><surname>Van Gerven</surname><given-names>MAJ</given-names></name></person-group><article-title>Shared Neural Mechanisms of Visual Perception and Imagery</article-title><source>Trends in Cognitive Sciences</source><year>2019</year><month>May</month><volume>23</volume><issue>5</issue><fpage>423</fpage><lpage>434</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S1364661319300592">https://linkinghub.elsevier.com/retrieve/pii/S1364661319300592</ext-link></comment><pub-id pub-id-type="pmid">30876729</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dirani</surname><given-names>J</given-names></name><name><surname>Pylkkänen</surname><given-names>L</given-names></name></person-group><article-title>MEG Evidence That Modality-Independent Conceptual Representations Contain Semantic and Visual Features</article-title><source>Journal of Neuroscience</source><publisher-name>Society for Neuroscience</publisher-name><year>2024</year><month>Jul</month><volume>44</volume><issue>27</issue><comment><ext-link ext-link-type="uri" xlink:href="https://www.jneurosci.org/content/44/27/e0326242024">https://www.jneurosci.org/content/44/27/e0326242024</ext-link> Section: Research Articles</comment><pub-id pub-id-type="pmcid">PMC11223456</pub-id><pub-id pub-id-type="pmid">38806251</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0326-24.2024</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Dosovitskiy</surname><given-names>A</given-names></name><name><surname>Beyer</surname><given-names>L</given-names></name><name><surname>Kolesnikov</surname><given-names>A</given-names></name><name><surname>Weissenborn</surname><given-names>D</given-names></name><name><surname>Zhai</surname><given-names>X</given-names></name><name><surname>Unterthiner</surname><given-names>T</given-names></name><name><surname>Dehghani</surname><given-names>M</given-names></name><name><surname>Minderer</surname><given-names>M</given-names></name><name><surname>Heigold</surname><given-names>G</given-names></name><name><surname>Gelly</surname><given-names>S</given-names></name><name><surname>Uszkoreit</surname><given-names>J</given-names></name><etal/></person-group><source>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</source><conf-name>International Conference on Learning Representations</conf-name><year>2020</year></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Embleton</surname><given-names>KV</given-names></name><name><surname>Haroon</surname><given-names>HA</given-names></name><name><surname>Morris</surname><given-names>DM</given-names></name><name><surname>Ralph</surname><given-names>MAL</given-names></name><name><surname>Parker</surname><given-names>GJM</given-names></name></person-group><article-title>Distortion correction for diffusion-weighted MRI tractography and fMRI in the temporal lobes</article-title><source>Human Brain Mapping</source><year>2010</year><volume>31</volume><issue>10</issue><fpage>1570</fpage><lpage>1587</lpage><pub-id pub-id-type="pmcid">PMC6870737</pub-id><pub-id pub-id-type="pmid">20143387</pub-id><pub-id pub-id-type="doi">10.1002/hbm.20959</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fairhall</surname><given-names>SL</given-names></name><name><surname>Caramazza</surname><given-names>A</given-names></name></person-group><article-title>Brain Regions That Represent Amodal Conceptual Knowledge</article-title><source>Journal of Neuroscience</source><year>2013</year><volume>33</volume><issue>25</issue><fpage>10552</fpage><lpage>10558</lpage><pub-id pub-id-type="pmcid">PMC6618586</pub-id><pub-id pub-id-type="pmid">23785167</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0051-13.2013</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Farah</surname><given-names>MJ</given-names></name></person-group><article-title>The neurological basis of mental imagery: A componential analysis</article-title><source>Cognition</source><year>1984</year><month>Dec</month><volume>18</volume><issue>1-3</issue><fpage>245</fpage><lpage>272</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/001002778490026X">https://linkinghub.elsevier.com/retrieve/pii/001002778490026X</ext-link></comment><pub-id pub-id-type="pmid">6396031</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fedorenko</surname><given-names>E</given-names></name><name><surname>Behr</surname><given-names>MK</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><article-title>Functional specificity for high-level linguistic processing in the human brain</article-title><source>Proceedings of the National Academy of Sciences</source><year>2011</year><month>Sep</month><volume>108</volume><issue>39</issue><fpage>16428</fpage><lpage>16433</lpage><comment>Proceedings of the National Academy of Sciences</comment><pub-id pub-id-type="pmcid">PMC3182706</pub-id><pub-id pub-id-type="pmid">21885736</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1112937108</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fedorenko</surname><given-names>E</given-names></name><name><surname>Hsieh</surname><given-names>PJ</given-names></name><name><surname>Nieto-Castañón</surname><given-names>A</given-names></name><name><surname>Whitfield-Gabrieli</surname><given-names>S</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><article-title>New Method for fMRI Investigations of Language: Defining ROIs Functionally in Individual Subjects</article-title><source>Journal of Neurophysiology</source><year>2010</year><month>Aug</month><volume>104</volume><issue>2</issue><fpage>1177</fpage><lpage>1194</lpage><pub-id pub-id-type="pmcid">PMC2934923</pub-id><pub-id pub-id-type="pmid">20410363</pub-id><pub-id pub-id-type="doi">10.1152/jn.00032.2010</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Felleman</surname><given-names>DJ</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><article-title>Distributed hierarchical processing in the primate cerebral cortex</article-title><source>Cerebral cortex (New York, NY)</source><year>1991</year><month>Jan</month><volume>1</volume><issue>1</issue><fpage>1</fpage><lpage>47</lpage><pub-id pub-id-type="pmid">1822724</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fernandino</surname><given-names>L</given-names></name><name><surname>Binder</surname><given-names>JR</given-names></name><name><surname>Desai</surname><given-names>RH</given-names></name><name><surname>Pendl</surname><given-names>SL</given-names></name><name><surname>Humphries</surname><given-names>CJ</given-names></name><name><surname>Gross</surname><given-names>WL</given-names></name><name><surname>Conant</surname><given-names>LL</given-names></name><name><surname>Seidenberg</surname><given-names>MS</given-names></name></person-group><article-title>Concept Representation Reflects Multimodal Abstraction: A Framework for Embodied Semantics</article-title><source>Cerebral Cortex</source><year>2016</year><volume>26</volume><issue>5</issue><fpage>2018</fpage><lpage>2034</lpage><pub-id pub-id-type="pmcid">PMC4830284</pub-id><pub-id pub-id-type="pmid">25750259</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhv020</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fernandino</surname><given-names>L</given-names></name><name><surname>Tong</surname><given-names>JQ</given-names></name><name><surname>Conant</surname><given-names>LL</given-names></name><name><surname>Humphries</surname><given-names>CJ</given-names></name><name><surname>Binder</surname><given-names>JR</given-names></name></person-group><article-title>Decoding the information structure underlying the neural representation of concepts</article-title><source>Proceedings of the National Academy of Sciences</source><year>2022</year><volume>119</volume><issue>6</issue><pub-id pub-id-type="pmcid">PMC8832989</pub-id><pub-id pub-id-type="pmid">35115397</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2108091119</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><article-title>FreeSurfer</article-title><source>NeuroImage</source><publisher-name>Elsevier Science</publisher-name><publisher-loc>Netherlands</publisher-loc><year>2012</year><volume>62</volume><issue>2</issue><fpage>774</fpage><lpage>781</lpage><pub-id pub-id-type="pmcid">PMC3685476</pub-id><pub-id pub-id-type="pmid">22248573</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.01.021</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Friederici</surname><given-names>AD</given-names></name></person-group><source>Language in Our Brain: The Origins of a Uniquely Human Capacity</source><publisher-name>The MIT Press</publisher-name><year>2017</year><comment><ext-link ext-link-type="uri" xlink:href="https://direct.mit.edu/books/oa-monograph/3653/Language-in-Our-BrainThe-Origins-of-a-Uniquely">https://direct.mit.edu/books/oa-monograph/3653/Language-in-Our-BrainThe-Origins-of-a-Uniquely</ext-link></comment><pub-id pub-id-type="doi">10.7551/mitpress/11173.001.0001</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frisby</surname><given-names>SL</given-names></name><name><surname>Halai</surname><given-names>AD</given-names></name><name><surname>Cox</surname><given-names>CR</given-names></name><name><surname>Lambon Ralph</surname><given-names>MA</given-names></name><name><surname>Rogers</surname><given-names>TT</given-names></name></person-group><article-title>Decoding semantic representations in mind and brain</article-title><source>Trends in Cognitive Sciences</source><year>2023</year><month>Mar</month><volume>27</volume><issue>3</issue><fpage>258</fpage><lpage>281</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S1364661322003230">https://linkinghub.elsevier.com/retrieve/pii/S1364661322003230</ext-link></comment><pub-id pub-id-type="pmid">36631371</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gainotti</surname><given-names>G</given-names></name></person-group><article-title>What the locus of brain lesion tells us about the nature of the cognitive defect underlying category-specific disorders: a review</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><year>2000</year><month>Sep</month><volume>36</volume><issue>4</issue><fpage>539</fpage><lpage>559</lpage><pub-id pub-id-type="pmid">11059454</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Girdhar</surname><given-names>R</given-names></name><name><surname>El-Nouby</surname><given-names>A</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Singh</surname><given-names>M</given-names></name><name><surname>Alwala</surname><given-names>KV</given-names></name><name><surname>Joulin</surname><given-names>A</given-names></name><name><surname>Misra</surname><given-names>I</given-names></name></person-group><source>ImageBind: One Embedding Space To Bind Them All</source><conf-name>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name><year>2023</year><fpage>15180</fpage><lpage>15190</lpage></element-citation></ref><ref id="R44"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gorgolewski</surname><given-names>K</given-names></name><name><surname>Burns</surname><given-names>CD</given-names></name><name><surname>Madison</surname><given-names>C</given-names></name><name><surname>Clark</surname><given-names>D</given-names></name><name><surname>Halchenko</surname><given-names>YO</given-names></name><name><surname>Waskom</surname><given-names>ML</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name></person-group><article-title>Nipype: a flexible, lightweight and extensible neuroimaging data processing framework in python</article-title><source>Frontiers in neuroinformatics</source><publisher-name>Frontiers</publisher-name><year>2011</year><volume>5</volume><elocation-id>12318</elocation-id><pub-id pub-id-type="pmcid">PMC3159964</pub-id><pub-id pub-id-type="pmid">21897815</pub-id><pub-id pub-id-type="doi">10.3389/fninf.2011.00013</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name></person-group><article-title>The Human Visual Cortex</article-title><source>Annual Review of Neuroscience</source><year>2004</year><month>Jul</month><volume>27</volume><issue>1</issue><fpage>649</fpage><lpage>677</lpage><pub-id pub-id-type="pmid">15217346</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hajhajate</surname><given-names>D</given-names></name><name><surname>Kaufmann</surname><given-names>BC</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Siuda-Krzywicka</surname><given-names>K</given-names></name><name><surname>Bartolomeo</surname><given-names>P</given-names></name></person-group><article-title>The connectional anatomy of visual mental imagery: evidence from a patient with left occipito-temporal damage</article-title><source>Brain Structure &amp; Function</source><year>2022</year><month>Dec</month><volume>227</volume><issue>9</issue><fpage>3075</fpage><lpage>3083</lpage><pub-id pub-id-type="pmid">35622159</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Handjaras</surname><given-names>G</given-names></name><name><surname>Ricciardi</surname><given-names>E</given-names></name><name><surname>Leo</surname><given-names>A</given-names></name><name><surname>Lenci</surname><given-names>A</given-names></name><name><surname>Cecchetti</surname><given-names>L</given-names></name><name><surname>Cosottini</surname><given-names>M</given-names></name><name><surname>Marotta</surname><given-names>G</given-names></name><name><surname>Pietrini</surname><given-names>P</given-names></name></person-group><article-title>How concepts are encoded in the human brain: A modality independent, category-based cortical organization of semantic knowledge</article-title><source>NeuroImage</source><year>2016</year><month>Jul</month><volume>135</volume><fpage>232</fpage><lpage>242</lpage><pub-id pub-id-type="pmid">27132545</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname><given-names>JV</given-names></name><name><surname>Gobbini</surname><given-names>MI</given-names></name><name><surname>Furey</surname><given-names>ML</given-names></name><name><surname>Ishai</surname><given-names>A</given-names></name><name><surname>Schouten</surname><given-names>JL</given-names></name><name><surname>Pietrini</surname><given-names>P</given-names></name></person-group><article-title>Distributed and Overlapping Representations of Faces and Objects in Ventral Temporal Cortex</article-title><source>Science</source><year>2001</year><month>Sep</month><volume>293</volume><issue>5539</issue><fpage>2425</fpage><lpage>2430</lpage><pub-id pub-id-type="pmid">11577229</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><source>Deep Residual Learning for Image Recognition</source><conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name><year>2016</year><fpage>770</fpage><lpage>778</lpage></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huth</surname><given-names>AG</given-names></name><name><surname>Nishimoto</surname><given-names>S</given-names></name><name><surname>Vu</surname><given-names>AT</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><article-title>A continuous semantic space describes the representation of thousands of object and action categories across the human brain</article-title><source>Neuron</source><year>2012</year><month>Dec</month><volume>76</volume><issue>6</issue><fpage>1210</fpage><lpage>1224</lpage><pub-id pub-id-type="pmcid">PMC3556488</pub-id><pub-id pub-id-type="pmid">23259955</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2012.10.014</pub-id></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jefferies</surname><given-names>E</given-names></name><name><surname>Patterson</surname><given-names>K</given-names></name><name><surname>Jones</surname><given-names>RW</given-names></name><name><surname>Ralph</surname><given-names>MAL</given-names></name></person-group><article-title>Comprehension of concrete and abstract words in semantic dementia</article-title><source>Neuropsychology</source><year>2009</year><month>Jul</month><volume>23</volume><issue>4</issue><fpage>492</fpage><pub-id pub-id-type="pmcid">PMC2801065</pub-id><pub-id pub-id-type="pmid">19586212</pub-id><pub-id pub-id-type="doi">10.1037/a0015452</pub-id></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name></person-group><article-title>FSL</article-title><source>NeuroImage</source><year>2012</year><month>Aug</month><volume>62</volume><issue>2</issue><fpage>782</fpage><lpage>790</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S1053811911010603">https://www.sciencedirect.com/science/article/pii/S1053811911010603</ext-link></comment><pub-id pub-id-type="pmid">21979382</pub-id></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>AQ</given-names></name><name><surname>Sablayrolles</surname><given-names>A</given-names></name><name><surname>Mensch</surname><given-names>A</given-names></name><name><surname>Bamford</surname><given-names>C</given-names></name><name><surname>Chaplot</surname><given-names>DS</given-names></name><name><surname>Ddl</surname><given-names>Casas</given-names></name><name><surname>Bressand</surname><given-names>F</given-names></name><name><surname>Lengyel</surname><given-names>G</given-names></name><name><surname>Lample</surname><given-names>G</given-names></name><name><surname>Saulnier</surname><given-names>L</given-names></name><name><surname>Lavaud</surname><given-names>LR</given-names></name><etal/></person-group><source>arXiv</source><year>2023</year><elocation-id>arXiv:2310.06825 [cs]</elocation-id><comment><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2310.06825">http://arxiv.org/abs/2310.06825</ext-link></comment><pub-id pub-id-type="doi">10.48550/arXiv.2310.06825</pub-id></element-citation></ref><ref id="R54"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>MR</given-names></name><name><surname>Johnson</surname><given-names>MK</given-names></name></person-group><article-title>Decoding individual natural scene representations during perception and imagery</article-title><source>Frontiers in Human Neuroscience</source><publisher-name>Frontiers</publisher-name><year>2014</year><month>Feb</month><volume>8</volume><comment><ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2014.00059/full">https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2014.00059/full</ext-link></comment><pub-id pub-id-type="pmcid">PMC3921604</pub-id><pub-id pub-id-type="pmid">24574998</pub-id><pub-id pub-id-type="doi">10.3389/fnhum.2014.00059</pub-id></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jung</surname><given-names>Y</given-names></name><name><surname>Larsen</surname><given-names>B</given-names></name><name><surname>Walther</surname><given-names>DB</given-names></name></person-group><article-title>Modality-Independent Coding of Scene Categories in Prefrontal Cortex</article-title><source>Journal of Neuroscience</source><year>2018</year><month>Jun</month><volume>38</volume><issue>26</issue><fpage>5969</fpage><lpage>5981</lpage><pub-id pub-id-type="pmcid">PMC6595974</pub-id><pub-id pub-id-type="pmid">29858483</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0272-18.2018</pub-id></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><article-title>Functional specificity in the human brain: A window into the functional architecture of the mind</article-title><source>Proceedings of the National Academy of Sciences</source><year>2010</year><month>Jun</month><volume>107</volume><issue>25</issue><fpage>11163</fpage><lpage>11170</lpage><comment>Proceedings of the National Academy of Sciences</comment><pub-id pub-id-type="pmcid">PMC2895137</pub-id><pub-id pub-id-type="pmid">20484679</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1005062107</pub-id></element-citation></ref><ref id="R57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname><given-names>KN</given-names></name><name><surname>Naselaris</surname><given-names>T</given-names></name><name><surname>Prenger</surname><given-names>RJ</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><article-title>Identifying natural images from human brain activity</article-title><source>Nature</source><year>2008</year><month>Mar</month><volume>452</volume><issue>7185</issue><fpage>352</fpage><lpage>355</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/nature06713">https://www.nature.com/articles/nature06713</ext-link></comment><pub-id pub-id-type="pmcid">PMC3556484</pub-id><pub-id pub-id-type="pmid">18322462</pub-id><pub-id pub-id-type="doi">10.1038/nature06713</pub-id></element-citation></ref><ref id="R58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiefer</surname><given-names>M</given-names></name><name><surname>Pulvermüller</surname><given-names>F</given-names></name></person-group><article-title>Conceptual representations in mind and brain: Theoretical developments, current evidence and future directions</article-title><source>Cortex</source><year>2012</year><month>Jul</month><volume>48</volume><issue>7</issue><fpage>805</fpage><lpage>825</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0010945211001018">https://linkinghub.elsevier.com/retrieve/pii/S0010945211001018</ext-link></comment><pub-id pub-id-type="pmid">21621764</pub-id></element-citation></ref><ref id="R59"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>W</given-names></name><name><surname>Son</surname><given-names>B</given-names></name><name><surname>Kim</surname><given-names>I</given-names></name></person-group><source>ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision</source><conf-name>Proceedings of the 38th International Conference on Machine Learning</conf-name><conf-sponsor>PMLR</conf-sponsor><year>2021</year><fpage>5583</fpage><lpage>5594</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://proceedings.mlr.press/v139/kim21k.html">https://proceedings.mlr.press/v139/kim21k.html</ext-link>, iSSN: 2640-3498</comment></element-citation></ref><ref id="R60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kosslyn</surname><given-names>SM</given-names></name><name><surname>Pascual-Leone</surname><given-names>A</given-names></name><name><surname>Felician</surname><given-names>O</given-names></name><name><surname>Camposano</surname><given-names>S</given-names></name><name><surname>Keenan</surname><given-names>JP</given-names></name><name><surname>Thompson</surname><given-names>W</given-names></name><name><surname>Ganis</surname><given-names>G</given-names></name><name><surname>Sukel</surname><given-names>KE</given-names></name><name><surname>Alpert</surname><given-names>NM</given-names></name></person-group><article-title>The Role of Area 17 in Visual Imagery: Convergent Evidence from PET and rTMS</article-title><source>Science</source><year>1999</year><month>Apr</month><volume>284</volume><issue>5411</issue><fpage>167</fpage><lpage>170</lpage><pub-id pub-id-type="pmid">10102821</pub-id></element-citation></ref><ref id="R61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kosslyn</surname><given-names>SM</given-names></name><name><surname>Thompson</surname><given-names>WL</given-names></name></person-group><article-title>When is early visual cortex activated during visual mental imagery?</article-title><source>Psychological Bulletin</source><year>2003</year><volume>129</volume><issue>5</issue><fpage>723</fpage><lpage>746</lpage><pub-id pub-id-type="pmid">12956541</pub-id></element-citation></ref><ref id="R62"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Krasnowska-Kieras</surname><given-names>K</given-names></name><name><surname>Wróblewska</surname><given-names>A</given-names></name></person-group><source>Empirical Linguistic Study of Sentence Embeddings</source><conf-name>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</conf-name><conf-sponsor>Association for Computational Linguistics</conf-sponsor><conf-loc>Florence, Italy</conf-loc><year>2019</year><fpage>5729</fpage><lpage>5739</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.aclweb.org/anthology/P19-1573">https://www.aclweb.org/anthology/P19-1573</ext-link></comment><pub-id pub-id-type="doi">10.18653/v1/P19-1573</pub-id></element-citation></ref><ref id="R63"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name></person-group><article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><publisher-name>Frontiers</publisher-name><year>2008</year><month>Nov</month><volume>2</volume><comment><ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/journals/systems-neuroscience/articles/10.3389/neuro.06.004.2008/full">https://www.frontiersin.org/journals/systems-neuroscience/articles/10.3389/neuro.06.004.2008/full</ext-link></comment><pub-id pub-id-type="pmcid">PMC2605405</pub-id><pub-id pub-id-type="pmid">19104670</pub-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id></element-citation></ref><ref id="R64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lambon Ralph</surname><given-names>MA</given-names></name><name><surname>Lowe</surname><given-names>C</given-names></name><name><surname>Rogers</surname><given-names>TT</given-names></name></person-group><article-title>Neural basis of category-specific semantic deficits for living things: evidence from semantic dementia, HSVE and a neural network model</article-title><source>Brain</source><year>2006</year><month>Nov</month><volume>130</volume><issue>4</issue><fpage>1127</fpage><lpage>1137</lpage><pub-id pub-id-type="pmid">17438021</pub-id></element-citation></ref><ref id="R65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>SH</given-names></name><name><surname>Kravitz</surname><given-names>DJ</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><article-title>Disentangling visual imagery and perception of real-world objects</article-title><source>NeuroImage</source><year>2012</year><month>Feb</month><volume>59</volume><issue>4</issue><fpage>4064</fpage><lpage>4073</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S1053811911012195">https://www.sciencedirect.com/science/article/pii/S1053811911012195</ext-link></comment><pub-id pub-id-type="pmcid">PMC3288657</pub-id><pub-id pub-id-type="pmid">22040738</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.10.055</pub-id></element-citation></ref><ref id="R66"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Li</surname><given-names>D</given-names></name><name><surname>Savarese</surname><given-names>S</given-names></name><name><surname>Hoi</surname><given-names>S</given-names></name></person-group><source>BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</source><conf-name>International Conference on Machine Learning</conf-name><year>2023</year></element-citation></ref><ref id="R67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>LH</given-names></name><name><surname>Yatskar</surname><given-names>M</given-names></name><name><surname>Yin</surname><given-names>D</given-names></name><name><surname>Hsieh</surname><given-names>CJ</given-names></name><name><surname>Chang</surname><given-names>KW</given-names></name></person-group><article-title>VisualBERT: A Simple and Performant Baseline for Vision and Language</article-title><source>arXiv:190803557 [cs]</source><year>2019</year><month>Aug</month><elocation-id>arXiv: 1908.03557</elocation-id><comment><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1908.03557">http://arxiv.org/abs/1908.03557</ext-link></comment></element-citation></ref><ref id="R68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>S</given-names></name><name><surname>Sprague</surname><given-names>T</given-names></name><name><surname>Singh</surname><given-names>AK</given-names></name></person-group><article-title>Mind Reader: Reconstructing complex images from brain activities</article-title><source>Advances in Neural Information Processing Systems</source><year>2022</year><month>Dec</month><volume>35</volume><fpage>29624</fpage><lpage>29636</lpage></element-citation></ref><ref id="R69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>TY</given-names></name><name><surname>Maire</surname><given-names>M</given-names></name><name><surname>Belongie</surname><given-names>S</given-names></name><name><surname>Hays</surname><given-names>J</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name><name><surname>Ramanan</surname><given-names>D</given-names></name><name><surname>Dollár</surname><given-names>P</given-names></name><name><surname>Zitnick</surname><given-names>CL</given-names></name></person-group><article-title>Microsoft COCO: Common Objects in Context</article-title><source>Computer Vision – ECCV 2014</source><year>2014</year><volume>8693</volume><fpage>740</fpage><lpage>755</lpage></element-citation></ref><ref id="R70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Spagna</surname><given-names>A</given-names></name><name><surname>Bartolomeo</surname><given-names>P</given-names></name></person-group><article-title>Hemispheric asymmetries in visual mental imagery</article-title><source>Brain Structure &amp; Function</source><year>2022</year><month>Mar</month><volume>227</volume><issue>2</issue><fpage>697</fpage><lpage>708</lpage><pub-id pub-id-type="pmid">33885966</pub-id></element-citation></ref><ref id="R71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Zhan</surname><given-names>M</given-names></name><name><surname>Hajhajate</surname><given-names>D</given-names></name><name><surname>Spagna</surname><given-names>A</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Cohen</surname><given-names>L</given-names></name><name><surname>Bartolomeo</surname><given-names>P</given-names></name></person-group><article-title>Visual mental imagery in typical imagers and in aphantasia: A millimeter-scale 7-T fMRI study</article-title><source>Cortex</source><year>2025</year><month>Apr</month><volume>185</volume><fpage>113</fpage><lpage>132</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0010945225000474">https://www.sciencedirect.com/science/article/pii/S0010945225000474</ext-link></comment><pub-id pub-id-type="pmid">40031090</pub-id></element-citation></ref><ref id="R72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liuzzi</surname><given-names>AG</given-names></name><name><surname>Bruffaerts</surname><given-names>R</given-names></name><name><surname>Peeters</surname><given-names>R</given-names></name><name><surname>Adamczuk</surname><given-names>K</given-names></name><name><surname>Keuleers</surname><given-names>E</given-names></name><name><surname>De Deyne</surname><given-names>S</given-names></name><name><surname>Storms</surname><given-names>G</given-names></name><name><surname>Dupont</surname><given-names>P</given-names></name><name><surname>Vandenberghe</surname><given-names>R</given-names></name></person-group><article-title>Cross-modal representation of spoken and written word meaning in left pars triangularis</article-title><source>NeuroImage</source><year>2017</year><month>Apr</month><volume>150</volume><fpage>292</fpage><lpage>307</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S105381191730143X">https://linkinghub.elsevier.com/retrieve/pii/S105381191730143X</ext-link></comment><pub-id pub-id-type="pmid">28213115</pub-id></element-citation></ref><ref id="R73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Man</surname><given-names>K</given-names></name><name><surname>Kaplan</surname><given-names>JT</given-names></name><name><surname>Damasio</surname><given-names>A</given-names></name><name><surname>Meyer</surname><given-names>K</given-names></name></person-group><article-title>Sight and Sound Converge to Form Modality-Invariant Representations in Temporoparietal Cortex</article-title><source>Journal of Neuroscience</source><year>2012</year><volume>32</volume><issue>47</issue><fpage>16629</fpage><lpage>16636</lpage><pub-id pub-id-type="pmcid">PMC3667662</pub-id><pub-id pub-id-type="pmid">23175818</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2342-12.2012</pub-id></element-citation></ref><ref id="R74"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Martin</surname><given-names>A</given-names></name></person-group><chapter-title>Circuits in Mind: The Neural Foundations for Object Concepts</chapter-title><person-group person-group-type="editor"><name><surname>Gazzaniga</surname><given-names>MS</given-names></name></person-group><source>The Cognitive Neurosciences</source><edition>4 ed</edition><publisher-name>The MIT Press</publisher-name><year>2009</year><comment><ext-link ext-link-type="uri" xlink:href="https://direct.mit.edu/books/book/5453/chapter/3965022/Circuits-in-Mind-The-Neural-Foundations-for-Object">https://direct.mit.edu/books/book/5453/chapter/3965022/Circuits-in-Mind-The-Neural-Foundations-for-Object</ext-link></comment><pub-id pub-id-type="doi">10.7551/mitpress/8029.003.0091</pub-id></element-citation></ref><ref id="R75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname><given-names>A</given-names></name></person-group><article-title>GRAPES—Grounding representations in action, perception, and emotion systems: How object properties and categories are represented in the human brain</article-title><source>Psychonomic Bulletin &amp; Review</source><year>2016</year><volume>23</volume><issue>4</issue><fpage>979</fpage><lpage>990</lpage><pub-id pub-id-type="pmcid">PMC5111803</pub-id><pub-id pub-id-type="pmid">25968087</pub-id><pub-id pub-id-type="doi">10.3758/s13423-015-0842-3</pub-id></element-citation></ref><ref id="R76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname><given-names>CB</given-names></name><name><surname>Douglas</surname><given-names>D</given-names></name><name><surname>Newsome</surname><given-names>RN</given-names></name><name><surname>Man</surname><given-names>LL</given-names></name><name><surname>Barense</surname><given-names>MD</given-names></name></person-group><article-title>Integrative and distinctive coding of visual and conceptual object features in the ventral visual stream</article-title><source>eLife</source><year>2018</year><volume>7</volume><pub-id pub-id-type="pmcid">PMC5832413</pub-id><pub-id pub-id-type="pmid">29393853</pub-id><pub-id pub-id-type="doi">10.7554/eLife.31873</pub-id></element-citation></ref><ref id="R77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meschke</surname><given-names>E</given-names></name><name><surname>Gallant</surname><given-names>J</given-names></name></person-group><article-title>Mapping Multimodal Conceptual Representations within the Lexical-Semantic Brain System</article-title><source>CCN</source><year>2024</year></element-citation></ref><ref id="R78"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Meyer</surname><given-names>K</given-names></name><name><surname>Damasio</surname><given-names>A</given-names></name></person-group><article-title>Convergence and divergence in a neural architecture for recognition and memory</article-title><source>Trends in Neurosciences</source><publisher-name>Elsevier</publisher-name><year>2009</year><month>Jul</month><volume>32</volume><issue>7</issue><fpage>376</fpage><lpage>382</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.cell.com/trends/neurosciences/abstract/S0166-2236(09)00090-3">https://www.cell.com/trends/neurosciences/abstract/S0166-2236(09)00090-3</ext-link></comment><pub-id pub-id-type="pmid">19520438</pub-id></element-citation></ref><ref id="R79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>CJ</given-names></name><name><surname>Price</surname><given-names>CJ</given-names></name></person-group><article-title>Three Distinct Ventral Occipitotemporal Regions for Reading and Object Naming</article-title><source>NeuroImage</source><year>1999</year><volume>10</volume><issue>2</issue><fpage>181</fpage><lpage>192</lpage><pub-id pub-id-type="pmid">10417250</pub-id></element-citation></ref><ref id="R80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naselaris</surname><given-names>T</given-names></name><name><surname>Kay</surname><given-names>KN</given-names></name><name><surname>Nishimoto</surname><given-names>S</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><article-title>Encoding and decoding in fMRI</article-title><source>NeuroImage</source><year>2011</year><month>May</month><volume>56</volume><issue>2</issue><fpage>400</fpage><lpage>410</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S1053811910010657">https://www.sciencedirect.com/science/article/pii/S1053811910010657</ext-link></comment><pub-id pub-id-type="pmcid">PMC3037423</pub-id><pub-id pub-id-type="pmid">20691790</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.07.073</pub-id></element-citation></ref><ref id="R81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naselaris</surname><given-names>T</given-names></name><name><surname>Olman</surname><given-names>CA</given-names></name><name><surname>Stansbury</surname><given-names>DE</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><article-title>A voxel-wise encoding model for early visual areas decodes mental images of remembered scenes</article-title><source>NeuroImage</source><year>2015</year><month>Jan</month><volume>105</volume><fpage>215</fpage><lpage>228</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S1053811914008428">https://www.sciencedirect.com/science/article/pii/S1053811914008428</ext-link></comment><pub-id pub-id-type="pmcid">PMC4364759</pub-id><pub-id pub-id-type="pmid">25451480</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.10.018</pub-id></element-citation></ref><ref id="R82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naselaris</surname><given-names>T</given-names></name><name><surname>Prenger</surname><given-names>RJ</given-names></name><name><surname>Kay</surname><given-names>KN</given-names></name><name><surname>Oliver</surname><given-names>M</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><article-title>Bayesian Reconstruction of Natural Images from Human Brain Activity</article-title><source>Neuron</source><year>2009</year><month>Sep</month><volume>63</volume><issue>6</issue><fpage>902</fpage><lpage>915</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0896627309006850">https://linkinghub.elsevier.com/retrieve/pii/S0896627309006850</ext-link></comment><pub-id pub-id-type="pmcid">PMC5553889</pub-id><pub-id pub-id-type="pmid">19778517</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2009.09.006</pub-id></element-citation></ref><ref id="R83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nastase</surname><given-names>SA</given-names></name><name><surname>Liu</surname><given-names>YF</given-names></name><name><surname>Hillman</surname><given-names>H</given-names></name><name><surname>Zadbood</surname><given-names>A</given-names></name><name><surname>Hasenfratz</surname><given-names>L</given-names></name><name><surname>Keshavarzian</surname><given-names>N</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Honey</surname><given-names>CJ</given-names></name><name><surname>Yeshurun</surname><given-names>Y</given-names></name><name><surname>Regev</surname><given-names>M</given-names></name><name><surname>Nguyen</surname><given-names>M</given-names></name><etal/></person-group><article-title>The “Narratives” fMRI dataset for evaluating models of naturalistic language comprehension</article-title><source>Scientific Data</source><year>2021</year><month>Sep</month><volume>8</volume><issue>1</issue><fpage>250</fpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41597-021-01033-3">https://www.nature.com/articles/s41597-021-01033-3</ext-link></comment><pub-id pub-id-type="pmcid">PMC8479122</pub-id><pub-id pub-id-type="pmid">34584100</pub-id><pub-id pub-id-type="doi">10.1038/s41597-021-01033-3</pub-id></element-citation></ref><ref id="R84"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Nishimoto</surname><given-names>S</given-names></name><name><surname>Vu</surname><given-names>AT</given-names></name><name><surname>Naselaris</surname><given-names>T</given-names></name><name><surname>Benjamini</surname><given-names>Y</given-names></name><name><surname>Yu</surname><given-names>B</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><article-title>Reconstructing Visual Experiences from Brain Activity Evoked by Natural Movies</article-title><source>Current Biology</source><publisher-name>Elsevier</publisher-name><year>2011</year><month>Oct</month><volume>21</volume><issue>19</issue><fpage>1641</fpage><lpage>1646</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.cell.com/current-biology/abstract/S0960-9822(11)00937-7">https://www.cell.com/current-biology/abstract/S0960-9822(11)00937-7</ext-link></comment><pub-id pub-id-type="pmcid">PMC3326357</pub-id><pub-id pub-id-type="pmid">21945275</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2011.08.031</pub-id></element-citation></ref><ref id="R85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Craven</surname><given-names>KM</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><article-title>Mental Imagery of Faces and Places Activates Corresponding Stimulus-Specific Brain Regions</article-title><source>Journal of Cognitive Neuroscience</source><year>2000</year><month>Nov</month><volume>12</volume><issue>6</issue><fpage>1013</fpage><lpage>1023</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://direct.mit.edu/jocn/article/12/6/1013/3493/Mental-Imagery-of-Faces-and-Places-Activates">https://direct.mit.edu/jocn/article/12/6/1013/3493/Mental-Imagery-of-Faces-and-Places-Activates</ext-link></comment><pub-id pub-id-type="pmid">11177421</pub-id></element-citation></ref><ref id="R86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Visconti di Oleggio Castello</surname><given-names>M</given-names></name><name><surname>Chauhan</surname><given-names>V</given-names></name><name><surname>Jiahui</surname><given-names>G</given-names></name><name><surname>Gobbini</surname><given-names>MI</given-names></name></person-group><article-title>An fMRI dataset in response to “The Grand Budapest Hotel”, a socially-rich, naturalistic movie</article-title><source>Scientific Data</source><year>2020</year><volume>7</volume><issue>1</issue><fpage>383</fpage><pub-id pub-id-type="pmcid">PMC7658985</pub-id><pub-id pub-id-type="pmid">33177526</pub-id><pub-id pub-id-type="doi">10.1038/s41597-020-00735-4</pub-id></element-citation></ref><ref id="R87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oquab</surname><given-names>M</given-names></name><name><surname>Darcet</surname><given-names>T</given-names></name><name><surname>Moutakanni</surname><given-names>T</given-names></name><name><surname>Vo</surname><given-names>H</given-names></name><name><surname>Szafraniec</surname><given-names>M</given-names></name><name><surname>Khalidov</surname><given-names>V</given-names></name><name><surname>Fernandez</surname><given-names>P</given-names></name><name><surname>Haziza</surname><given-names>D</given-names></name><name><surname>Massa</surname><given-names>F</given-names></name><name><surname>El-Nouby</surname><given-names>A</given-names></name><name><surname>Assran</surname><given-names>M</given-names></name><etal/></person-group><article-title>DINOv2: Learning Robust Visual Features without Supervision</article-title><source>arXiv</source><year>2023</year><elocation-id>arXiv:2304.07193 [cs]</elocation-id><comment><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2304.07193">http://arxiv.org/abs/2304.07193</ext-link></comment></element-citation></ref><ref id="R88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ozcelik</surname><given-names>F</given-names></name><name><surname>VanRullen</surname><given-names>R</given-names></name></person-group><article-title>Natural scene reconstruction from fMRI signals using generative latent diffusion</article-title><source>arXiv</source><year>2023</year><elocation-id>arxiv:2303.05334[cs,q-bio]</elocation-id><comment><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2303.05334">http://arxiv.org/abs/2303.05334</ext-link></comment><pub-id pub-id-type="pmcid">PMC10511448</pub-id><pub-id pub-id-type="pmid">37731047</pub-id><pub-id pub-id-type="doi">10.1038/s41598-023-42891-8</pub-id></element-citation></ref><ref id="R89"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Patterson</surname><given-names>K</given-names></name><name><surname>Lambon Ralph</surname><given-names>MA</given-names></name></person-group><article-title>The Hub-and-Spoke Hypothesis of Semantic Memory</article-title><source>Neurobiology of Language</source><publisher-name>Elsevier</publisher-name><year>2016</year><fpage>765</fpage><lpage>775</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/B9780124077942000614">https://linkinghub.elsevier.com/retrieve/pii/B9780124077942000614</ext-link></comment><pub-id pub-id-type="doi">10.1016/B978-0-12-407794-2.00061-4</pub-id></element-citation></ref><ref id="R90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearson</surname><given-names>J</given-names></name></person-group><article-title>The human imagination: the cognitive neuroscience of visual mental imagery</article-title><source>Nature Reviews Neuroscience</source><year>2019</year><month>Oct</month><volume>20</volume><issue>10</issue><fpage>624</fpage><lpage>634</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41583-019-0202-9">https://www.nature.com/articles/s41583-019-0202-9</ext-link></comment><pub-id pub-id-type="pmid">31384033</pub-id></element-citation></ref><ref id="R91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Michel</surname><given-names>V</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Grisel</surname><given-names>O</given-names></name><name><surname>Blondel</surname><given-names>M</given-names></name><name><surname>Prettenhofer</surname><given-names>P</given-names></name><name><surname>Weiss</surname><given-names>R</given-names></name><name><surname>Dubourg</surname><given-names>V</given-names></name></person-group><article-title>Scikit-learn: Machine learning in Python</article-title><source>the Journal of machine Learning research</source><year>2011</year><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></element-citation></ref><ref id="R92"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>F</given-names></name><name><surname>Lou</surname><given-names>B</given-names></name><name><surname>Pritchett</surname><given-names>B</given-names></name><name><surname>Ritter</surname><given-names>S</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>Botvinick</surname><given-names>M</given-names></name><name><surname>Fedorenko</surname><given-names>E</given-names></name></person-group><article-title>Toward a universal decoder of linguistic meaning from brain activation</article-title><source>Nature Communications</source><publisher-name>Nature Publishing Group</publisher-name><year>2018</year><month>Mar</month><volume>9</volume><issue>1</issue><fpage>963</fpage><comment>number: 1 <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41467-018-03068-4">https://www.nature.com/articles/s41467-018-03068-4</ext-link></comment><pub-id pub-id-type="pmcid">PMC5840373</pub-id><pub-id pub-id-type="pmid">29511192</pub-id><pub-id pub-id-type="doi">10.1038/s41467-018-03068-4</pub-id></element-citation></ref><ref id="R93"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pobric</surname><given-names>G</given-names></name><name><surname>Jefferies</surname><given-names>E</given-names></name><name><surname>Ralph</surname><given-names>MAL</given-names></name></person-group><article-title>Category-Specific versus Category-General Semantic Impairment Induced by Transcranial Magnetic Stimulation</article-title><source>Current Biology</source><publisher-name>Elsevier</publisher-name><year>2010</year><month>May</month><volume>20</volume><issue>10</issue><fpage>964</fpage><lpage>968</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.cell.com/current-biology/abstract/S0960-9822(10)00456-2">https://www.cell.com/current-biology/abstract/S0960-9822(10)00456-2</ext-link></comment><pub-id pub-id-type="pmcid">PMC2878637</pub-id><pub-id pub-id-type="pmid">20451381</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2010.03.070</pub-id></element-citation></ref><ref id="R94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Popham</surname><given-names>SF</given-names></name><name><surname>Huth</surname><given-names>AG</given-names></name><name><surname>Bilenko</surname><given-names>NY</given-names></name><name><surname>Deniz</surname><given-names>F</given-names></name><name><surname>Gao</surname><given-names>JS</given-names></name><name><surname>Nunez-Elizalde</surname><given-names>AO</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><article-title>Visual and linguistic semantic representations are aligned at the border of human visual cortex</article-title><source>Nature Neuroscience</source><year>2021</year><volume>24</volume><issue>11</issue><fpage>1628</fpage><lpage>1636</lpage><pub-id pub-id-type="pmid">34711960</pub-id></element-citation></ref><ref id="R95"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Radford</surname><given-names>A</given-names></name><name><surname>Kim</surname><given-names>JW</given-names></name><name><surname>Hallacy</surname><given-names>C</given-names></name><name><surname>Ramesh</surname><given-names>A</given-names></name><name><surname>Goh</surname><given-names>G</given-names></name><name><surname>Agarwal</surname><given-names>S</given-names></name><name><surname>Sastry</surname><given-names>G</given-names></name><name><surname>Askell</surname><given-names>A</given-names></name><name><surname>Mishkin</surname><given-names>P</given-names></name><name><surname>Clark</surname><given-names>J</given-names></name><name><surname>Krueger</surname><given-names>G</given-names></name><etal/></person-group><source>Learning Transferable Visual Models From Natural Language Supervision</source><conf-name>Proceedings of the 38th International Conference on Machine Learning</conf-name><year>2021</year><fpage>16</fpage></element-citation></ref><ref id="R96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Radford</surname><given-names>A</given-names></name><name><surname>Wu</surname><given-names>J</given-names></name><name><surname>Child</surname><given-names>R</given-names></name><name><surname>Luan</surname><given-names>D</given-names></name><name><surname>Amodei</surname><given-names>D</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name></person-group><article-title>Language models are unsupervised multitask learners</article-title><source>OpenAI blog</source><year>2019</year><volume>1</volume><issue>8</issue><fpage>9</fpage></element-citation></ref><ref id="R97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ralph</surname><given-names>MAL</given-names></name><name><surname>Jefferies</surname><given-names>E</given-names></name><name><surname>Patterson</surname><given-names>K</given-names></name><name><surname>Rogers</surname><given-names>TT</given-names></name></person-group><article-title>The neural and computational bases of semantic cognition</article-title><source>Nature Reviews Neuroscience</source><year>2017</year><volume>18</volume><issue>1</issue><fpage>42</fpage><lpage>55</lpage><pub-id pub-id-type="pmid">27881854</pub-id></element-citation></ref><ref id="R98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reddy</surname><given-names>L</given-names></name><name><surname>Tsuchiya</surname><given-names>N</given-names></name><name><surname>Serre</surname><given-names>T</given-names></name></person-group><article-title>Reading the mind’s eye: decoding category information during mental imagery</article-title><source>NeuroImage</source><year>2010</year><month>Apr</month><volume>50</volume><issue>2</issue><fpage>818</fpage><lpage>825</lpage><pub-id pub-id-type="pmcid">PMC2823980</pub-id><pub-id pub-id-type="pmid">20004247</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.11.084</pub-id></element-citation></ref><ref id="R99"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Reimers</surname><given-names>N</given-names></name><name><surname>Gurevych</surname><given-names>I</given-names></name></person-group><source>Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</source><conf-name>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</conf-name><conf-name>Association for Computational Linguistics</conf-name><conf-loc>Hong Kong, China</conf-loc><year>2019</year><fpage>3980</fpage><lpage>3990</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.aclweb.org/anthology/D19-1410">https://www.aclweb.org/anthology/D19-1410</ext-link></comment><pub-id pub-id-type="doi">10.18653/v1/D19-1410</pub-id></element-citation></ref><ref id="R100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rogers</surname><given-names>TT</given-names></name><name><surname>Lambon Ralph</surname><given-names>MA</given-names></name><name><surname>Garrard</surname><given-names>P</given-names></name><name><surname>Bozeat</surname><given-names>S</given-names></name><name><surname>McClelland</surname><given-names>JL</given-names></name><name><surname>Hodges</surname><given-names>JR</given-names></name><name><surname>Patterson</surname><given-names>K</given-names></name></person-group><article-title>Structure and Deterioration of Semantic Memory: A Neuropsychological and Computational Investigation</article-title><source>Psychological Review</source><year>2004</year><volume>111</volume><issue>1</issue><fpage>205</fpage><lpage>235</lpage><pub-id pub-id-type="pmid">14756594</pub-id></element-citation></ref><ref id="R101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanchez</surname><given-names>G</given-names></name><name><surname>Hartmann</surname><given-names>T</given-names></name><name><surname>Fuscà</surname><given-names>M</given-names></name><name><surname>Demarchi</surname><given-names>G</given-names></name><name><surname>Weisz</surname><given-names>N</given-names></name></person-group><article-title>Decoding across sensory modalities reveals common supramodal signatures of conscious perception</article-title><source>Proceedings of the National Academy of Sciences</source><year>2020</year><month>Mar</month><volume>117</volume><issue>13</issue><fpage>7437</fpage><lpage>7446</lpage><pub-id pub-id-type="pmcid">PMC7132110</pub-id><pub-id pub-id-type="pmid">32184331</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1912584117</pub-id></element-citation></ref><ref id="R102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schoffelen</surname><given-names>JM</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Lam</surname><given-names>NHL</given-names></name><name><surname>Uddén</surname><given-names>J</given-names></name><name><surname>Hultén</surname><given-names>A</given-names></name><name><surname>Hagoort</surname><given-names>P</given-names></name></person-group><article-title>A 204-subject multimodal neuroimaging dataset to study language processing</article-title><source>Scientific Data</source><year>2019</year><volume>6</volume><issue>1</issue><fpage>17</fpage><pub-id pub-id-type="pmcid">PMC6472396</pub-id><pub-id pub-id-type="pmid">30944338</pub-id><pub-id pub-id-type="doi">10.1038/s41597-019-0020-y</pub-id></element-citation></ref><ref id="R103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sereno</surname><given-names>MI</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Reppas</surname><given-names>JB</given-names></name><name><surname>Kwong</surname><given-names>KK</given-names></name><name><surname>Belliveau</surname><given-names>JW</given-names></name><name><surname>Brady</surname><given-names>TJ</given-names></name><name><surname>Rosen</surname><given-names>BR</given-names></name><name><surname>Tootell</surname><given-names>RBH</given-names></name></person-group><article-title>Borders of Multiple Visual Areas in Humans Revealed by Functional Magnetic Resonance Imaging</article-title><source>Science</source><year>1995</year><month>May</month><volume>268</volume><issue>5212</issue><fpage>889</fpage><lpage>893</lpage><pub-id pub-id-type="pmid">7754376</pub-id></element-citation></ref><ref id="R104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>G</given-names></name><name><surname>Horikawa</surname><given-names>T</given-names></name><name><surname>Majima</surname><given-names>K</given-names></name><name><surname>Kamitani</surname><given-names>Y</given-names></name></person-group><article-title>Deep image reconstruction from human brain activity</article-title><source>PLOS Computational Biology</source><year>2019</year><month>Jan</month><volume>15</volume><issue>1</issue><elocation-id>e1006633</elocation-id><pub-id pub-id-type="pmcid">PMC6347330</pub-id><pub-id pub-id-type="pmid">30640910</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006633</pub-id></element-citation></ref><ref id="R105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shinkareva</surname><given-names>SV</given-names></name><name><surname>Malave</surname><given-names>VL</given-names></name><name><surname>Mason</surname><given-names>RA</given-names></name><name><surname>Mitchell</surname><given-names>TM</given-names></name><name><surname>Just</surname><given-names>MA</given-names></name></person-group><article-title>Commonality of neural representations of words and pictures</article-title><source>NeuroImage</source><year>2011</year><volume>54</volume><issue>3</issue><fpage>2418</fpage><lpage>2425</lpage><pub-id pub-id-type="pmid">20974270</pub-id></element-citation></ref><ref id="R106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simanova</surname><given-names>I</given-names></name><name><surname>Hagoort</surname><given-names>P</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>van Gerven</surname><given-names>MAJ</given-names></name></person-group><article-title>Modality-Independent Decoding of Semantic Information from the Human Brain</article-title><source>Cerebral Cortex</source><year>2014</year><volume>24</volume><issue>2</issue><fpage>426</fpage><lpage>434</lpage><pub-id pub-id-type="pmid">23064107</pub-id></element-citation></ref><ref id="R107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simmons</surname><given-names>WK</given-names></name><name><surname>Barsalou</surname><given-names>LW</given-names></name></person-group><article-title>The similarity-in-topography principle: Reconciling theories of conceptual deficits</article-title><source>Cognitive Neuropsychology</source><year>2003</year><volume>20</volume><issue>3-6</issue><fpage>451</fpage><lpage>486</lpage><pub-id pub-id-type="pmid">20957580</pub-id></element-citation></ref><ref id="R108"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Singh</surname><given-names>A</given-names></name><name><surname>Hu</surname><given-names>R</given-names></name><name><surname>Goswami</surname><given-names>V</given-names></name><name><surname>Couairon</surname><given-names>G</given-names></name><name><surname>Galuba</surname><given-names>W</given-names></name><name><surname>Rohrbach</surname><given-names>M</given-names></name><name><surname>Kiela</surname><given-names>D</given-names></name></person-group><source>FLAVA: A Foundational Language and Vision Alignment Model</source><conf-name>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name><year>2022</year><fpage>15638</fpage><lpage>15650</lpage></element-citation></ref><ref id="R109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name></person-group><article-title>Threshold-free cluster enhancement: Addressing problems of smoothing, threshold dependence and localisation in cluster inference</article-title><source>NeuroImage</source><year>2009</year><volume>44</volume><issue>1</issue><fpage>83</fpage><lpage>98</lpage><pub-id pub-id-type="pmid">18501637</pub-id></element-citation></ref><ref id="R110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snowden</surname><given-names>J</given-names></name><name><surname>Goulding</surname><given-names>PJ</given-names></name><name><surname>Neary</surname><given-names>D</given-names></name></person-group><article-title>Semantic dementia: A form of circumscribed cerebral atrophy</article-title><source>Behavioural Neurology</source><year>1989</year><volume>2</volume><issue>3</issue><elocation-id>124043</elocation-id><pub-id pub-id-type="doi">10.1155/1989/124043</pub-id></element-citation></ref><ref id="R111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spagna</surname><given-names>A</given-names></name><name><surname>Hajhajate</surname><given-names>D</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Bartolomeo</surname><given-names>P</given-names></name></person-group><article-title>Visual mental imagery engages the left fusiform gyrus, but not the early visual cortex: A meta-analysis of neuroimaging evidence</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><year>2021</year><month>Mar</month><volume>122</volume><fpage>201</fpage><lpage>217</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0149763420307041">https://www.sciencedirect.com/science/article/pii/S0149763420307041</ext-link></comment><pub-id pub-id-type="pmid">33422567</pub-id></element-citation></ref><ref id="R112"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Steel</surname><given-names>A</given-names></name><name><surname>Billings</surname><given-names>MM</given-names></name><name><surname>Silson</surname><given-names>EH</given-names></name><name><surname>Robertson</surname><given-names>CE</given-names></name></person-group><article-title>A network linking scene perception and spatial memory systems in posterior cerebral cortex</article-title><source>Nature Communications</source><publisher-name>Nature Publishing Group</publisher-name><year>2021</year><month>May</month><volume>12</volume><issue>1</issue><fpage>2632</fpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41467-021-22848-z">https://www.nature.com/articles/s41467-021-22848-z</ext-link></comment><pub-id pub-id-type="pmcid">PMC8113503</pub-id><pub-id pub-id-type="pmid">33976141</pub-id><pub-id pub-id-type="doi">10.1038/s41467-021-22848-z</pub-id></element-citation></ref><ref id="R113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steiner</surname><given-names>A</given-names></name><name><surname>Pinto</surname><given-names>AS</given-names></name><name><surname>Tschannen</surname><given-names>M</given-names></name><name><surname>Keysers</surname><given-names>D</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Bitton</surname><given-names>Y</given-names></name><name><surname>Gritsenko</surname><given-names>A</given-names></name><name><surname>Minderer</surname><given-names>M</given-names></name><name><surname>Sherbondy</surname><given-names>A</given-names></name><name><surname>Long</surname><given-names>S</given-names></name><name><surname>Qin</surname><given-names>S</given-names></name><etal/></person-group><article-title>PaliGemma 2: A Family of Versatile VLMs for Transfer</article-title><source>arXiv</source><year>2024</year><elocation-id>arXiv:2412.03555 [cs]</elocation-id><comment><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2412.03555">http://arxiv.org/abs/2412.03555</ext-link></comment><pub-id pub-id-type="doi">10.48550/arXiv.2412.03555</pub-id></element-citation></ref><ref id="R114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stokes</surname><given-names>M</given-names></name><name><surname>Thompson</surname><given-names>R</given-names></name><name><surname>Cusack</surname><given-names>R</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><article-title>Top-Down Activation of Shape-Specific Population Codes in Visual Cortex during Mental Imagery</article-title><source>The Journal of Neuroscience</source><year>2009</year><month>Feb</month><volume>29</volume><issue>5</issue><fpage>1565</fpage><lpage>1572</lpage><pub-id pub-id-type="pmcid">PMC6666065</pub-id><pub-id pub-id-type="pmid">19193903</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4657-08.2009</pub-id></element-citation></ref><ref id="R115"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Takagi</surname><given-names>Y</given-names></name><name><surname>Nishimoto</surname><given-names>S</given-names></name></person-group><source>High-Resolution Image Reconstruction With Latent Diffusion Models From Human Brain Activity</source><conf-name>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name><year>2023</year><fpage>14453</fpage><lpage>14463</lpage></element-citation></ref><ref id="R116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>J</given-names></name><name><surname>Vo</surname><given-names>VA</given-names></name></person-group><article-title>Brain encoding models based on multimodal transformers can transfer across language and vision</article-title><source>NeurIPS</source><year>2023</year><pub-id pub-id-type="pmcid">PMC11250991</pub-id><pub-id pub-id-type="pmid">39015152</pub-id></element-citation></ref><ref id="R117"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>J</given-names></name><name><surname>LeBel</surname><given-names>A</given-names></name><name><surname>Jain</surname><given-names>S</given-names></name><name><surname>Huth</surname><given-names>AG</given-names></name></person-group><article-title>Semantic reconstruction of continuous language from non-invasive brain recordings</article-title><source>Nature Neuroscience</source><publisher-name>Nature Publishing Group</publisher-name><year>2023</year><month>May</month><volume>26</volume><issue>5</issue><fpage>858</fpage><lpage>866</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41593-023-01304-9">https://www.nature.com/articles/s41593-023-01304-9</ext-link>, number: 5</comment><pub-id pub-id-type="pmcid">PMC11304553</pub-id><pub-id pub-id-type="pmid">37127759</pub-id><pub-id pub-id-type="doi">10.1038/s41593-023-01304-9</pub-id></element-citation></ref><ref id="R118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tong</surname><given-names>J</given-names></name><name><surname>Binder</surname><given-names>JR</given-names></name><name><surname>Humphries</surname><given-names>C</given-names></name><name><surname>Mazurchuk</surname><given-names>S</given-names></name><name><surname>Conant</surname><given-names>LL</given-names></name><name><surname>Fernandino</surname><given-names>L</given-names></name></person-group><article-title>A Distributed Network for Multimodal Experiential Representation of Concepts</article-title><source>Journal of Neuroscience</source><year>2022</year><volume>42</volume><issue>37</issue><fpage>7121</fpage><lpage>7130</lpage><pub-id pub-id-type="pmcid">PMC9480893</pub-id><pub-id pub-id-type="pmid">35940877</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1243-21.2022</pub-id></element-citation></ref><ref id="R119"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Touvron</surname><given-names>H</given-names></name><name><surname>Martin</surname><given-names>L</given-names></name><name><surname>Stone</surname><given-names>K</given-names></name><name><surname>Albert</surname><given-names>P</given-names></name><name><surname>Almahairi</surname><given-names>A</given-names></name><name><surname>Babaei</surname><given-names>Y</given-names></name><name><surname>Bashlykov</surname><given-names>N</given-names></name><name><surname>Batra</surname><given-names>S</given-names></name><name><surname>Bhargava</surname><given-names>P</given-names></name><name><surname>Bhosale</surname><given-names>S</given-names></name><name><surname>Bikel</surname><given-names>D</given-names></name><etal/></person-group><article-title>Llama 2: Open Foundation and Fine-Tuned Chat Models</article-title><source>arXiv</source><year>2023</year><comment><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2307.09288">http://arxiv.org/abs/2307.09288</ext-link></comment><pub-id pub-id-type="doi">10.48550/arXiv.2307.09288</pub-id></element-citation></ref><ref id="R120"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tranel</surname><given-names>D</given-names></name><name><surname>Damasio</surname><given-names>H</given-names></name><name><surname>Damasio</surname><given-names>AR</given-names></name></person-group><article-title>A neural basis for the retrieval of conceptual knowledge</article-title><source>Neuropsychologia</source><year>1997</year><month>Oct</month><volume>35</volume><issue>10</issue><fpage>1319</fpage><lpage>1327</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0028393297000857">https://www.sciencedirect.com/science/article/pii/S0028393297000857</ext-link></comment><pub-id pub-id-type="pmid">9347478</pub-id></element-citation></ref><ref id="R121"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vandenberghe</surname><given-names>R</given-names></name><name><surname>Price</surname><given-names>C</given-names></name><name><surname>Wise</surname><given-names>R</given-names></name><name><surname>Josephs</surname><given-names>O</given-names></name><name><surname>Frackowiak</surname><given-names>RS</given-names></name></person-group><article-title>Functional anatomy of a common semantic system for words and pictures</article-title><source>Nature</source><year>1996</year><volume>383</volume><issue>6597</issue><pub-id pub-id-type="pmid">8805700</pub-id></element-citation></ref><ref id="R122"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>VanRullen</surname><given-names>R</given-names></name><name><surname>Reddy</surname><given-names>L</given-names></name></person-group><article-title>Reconstructing faces from fMRI patterns using deep generative neural networks</article-title><source>Communications Biology</source><publisher-name>Nature Publishing Group</publisher-name><year>2019</year><month>May</month><volume>2</volume><issue>1</issue><fpage>1</fpage><lpage>10</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s42003-019-0438-y">https://www.nature.com/articles/s42003-019-0438-y</ext-link>, number: 1</comment><pub-id pub-id-type="pmcid">PMC6529435</pub-id><pub-id pub-id-type="pmid">31123717</pub-id><pub-id pub-id-type="doi">10.1038/s42003-019-0438-y</pub-id></element-citation></ref><ref id="R123"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warrington</surname><given-names>EK</given-names></name></person-group><article-title>The Selective Impairment of Semantic Memory</article-title><source>Quarterly Journal of Experimental Psychology</source><year>1975</year><month>Nov</month><volume>27</volume><issue>4</issue><fpage>635</fpage><lpage>657</lpage><pub-id pub-id-type="pmid">1197619</pub-id></element-citation></ref><ref id="R124"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warrington</surname><given-names>EK</given-names></name><name><surname>Mccarthy</surname><given-names>RA</given-names></name></person-group><article-title>Categories of knowledge: Further fractionations and an attempted integration</article-title><source>Brain: a Journal of Neurology</source><year>1987</year><pub-id pub-id-type="pmid">3676701</pub-id></element-citation></ref><ref id="R125"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warrington</surname><given-names>EK</given-names></name><name><surname>Shallice</surname><given-names>T</given-names></name></person-group><article-title>Category Specific Semantic Impairments</article-title><source>Brain: a Journal of Neurology</source><year>1984</year><pub-id pub-id-type="pmid">6206910</pub-id></element-citation></ref><ref id="R126"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wolf</surname><given-names>T</given-names></name><name><surname>Debut</surname><given-names>L</given-names></name><name><surname>Sanh</surname><given-names>V</given-names></name><name><surname>Chaumond</surname><given-names>J</given-names></name><name><surname>Delangue</surname><given-names>C</given-names></name><name><surname>Moi</surname><given-names>A</given-names></name><name><surname>Cistac</surname><given-names>P</given-names></name><name><surname>Rault</surname><given-names>T</given-names></name><name><surname>Louf</surname><given-names>R</given-names></name><name><surname>Funtowicz</surname><given-names>M</given-names></name></person-group><source>Transformers: State-of-the-art natural language processing</source><conf-name>Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations</conf-name><year>2020</year><fpage>38</fpage><lpage>45</lpage></element-citation></ref><ref id="R127"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>X</given-names></name><name><surname>Wu</surname><given-names>C</given-names></name><name><surname>Rosenman</surname><given-names>S</given-names></name><name><surname>Lal</surname><given-names>V</given-names></name><name><surname>Che</surname><given-names>W</given-names></name><name><surname>Duan</surname><given-names>N</given-names></name></person-group><source>BridgeTower: Building Bridges between Encoders in Vision-Language Representation Learning</source><conf-name>Proceedings of the AAAI Conference on Artificial Intelligence</conf-name><year>2023</year><month>Jun</month><volume>37</volume><issue>9</issue><fpage>10637</fpage><lpage>10647</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://ojs.aaai.org/index.php/AAAI/article/view/26263">https://ojs.aaai.org/index.php/AAAI/article/view/26263</ext-link></comment><pub-id pub-id-type="doi">10.1609/aaai.v37i9.26263</pub-id></element-citation></ref><ref id="R128"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zhai</surname><given-names>X</given-names></name><name><surname>Mustafa</surname><given-names>B</given-names></name><name><surname>Kolesnikov</surname><given-names>A</given-names></name><name><surname>Beyer</surname><given-names>L</given-names></name></person-group><source>Sigmoid Loss for Language Image Pre-Training</source><conf-name>2023 IEEE/CVF International Conference on Computer Vision (ICCV)</conf-name><conf-sponsor>IEEE</conf-sponsor><conf-loc>Paris, France</conf-loc><year>2023</year><fpage>11941</fpage><lpage>11952</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://ieeexplore.ieee.org/document/10377550/">https://ieeexplore.ieee.org/document/10377550/</ext-link></comment><pub-id pub-id-type="doi">10.1109/ICCV51070.2023.01100</pub-id></element-citation></ref><ref id="R129"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zola-Morgan</surname><given-names>S</given-names></name></person-group><article-title>Localization of Brain Function: The Legacy of Franz Joseph Gall (1758-1828)</article-title><source>Annual Review of Neuroscience</source><publisher-name>Annual Reviews</publisher-name><year>1995</year><month>Mar</month><volume>18</volume><fpage>359</fpage><lpage>383</lpage><comment>(Volume 18, 1995)</comment><pub-id pub-id-type="pmid">7605066</pub-id></element-citation></ref><ref id="R130"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zwaan</surname><given-names>RA</given-names></name></person-group><article-title>Situation models, mental simulations, and abstract concepts in discourse comprehension</article-title><source>Psychonomic Bulletin &amp; Review</source><year>2016</year><month>Aug</month><volume>23</volume><issue>4</issue><fpage>1028</fpage><lpage>1034</lpage><pub-id pub-id-type="pmcid">PMC4974264</pub-id><pub-id pub-id-type="pmid">26088667</pub-id><pub-id pub-id-type="doi">10.3758/s13423-015-0864-x</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><p><bold>Panel A:</bold> Setup of the main fMRI experiment. Subjects were seeing images and captions in random alternation. Whenever the current stimulus matched the previous stimulus, the subjects were instructed to press a button (one-back matching task). Images and captions for illustration only; actual size and stimuli as described in the text. <bold>Panel B:</bold> Number of distinct training stimuli (excluding trials that were one-back targets or during which the subject pressed the response button). There was an additional set of 140 stimuli (70 images and 70 captions) used for testing. <bold>Panel C:</bold> Setup of the fMRI experiment for the imagery trials. Subjects were instructed to remember 3 image descriptions with corresponding indices (numbers 1 to 3). One of these indices was displayed during the instruction phase, followed by a fixation phase, and then the subjects were imagining the visual scene for 10s.</p></caption><graphic xlink:href="EMS206394-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Training of modality-specific and modality-agnostic decoders.</title><p><bold>Panel A:</bold> Modality-specific decoders are trained on fMRI data of one modality (e.g. subjects viewing images) by mapping it to features extracted from the same stimuli. <bold>Panel B:</bold> Modality-agnostic decoders are trained jointly on fMRI data of both modalities (subjects viewing images and captions). <bold>Panel C:</bold> To train decoders, features can be either extracted unimodally from the corresponding images or captions, or by creating multimodal features based on both modalities. For example, to train a modality-agnostic decoder based on features from a unimodal language model, we map the fMRI data of subjects viewing captions to features extracted from the respective captions using this language model, as well as the fMRI data of subjects viewing images to features extracted by the language model from the corresponding captions. We can also train modality-specific decoders on features from another modality, for example by mapping fMRI data of subjects viewing images to features extracted from the corresponding captions using a language model (cf. crosses on orange bars in <xref ref-type="fig" rid="F4">Figure 4</xref> or using multimodal features (cf. crosses on blue bars in <xref ref-type="fig" rid="F4">Figure 4</xref>).</p></caption><graphic xlink:href="EMS206394-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Evaluation of modality-specific and modality-agnostic decoders.</title><p>The matrices display cosine similarity scores between features extracted from the candidate stimuli and features predicted by the decoder. The evaluation metric is pairwise accuracy, which is calculated row-wise: For a given matrix row, we compare the similarity score of the target stimulus on the diagonal (in green) with the similarity scores of all other candidate stimuli (in red). <bold>Panel A:</bold> Within-modality decoding metrics of modality-specific decoders. To compute within-modality accuracy for image decoding, a modality-specific decoder trained on images is evaluated on all stimuli that were presented as images. To compute within-modality accuracy for caption decoding, a modality-specific decoder trained on captions is evaluated on all caption stimuli. <bold>Panel B:</bold> Cross-modality decoding metrics of modality-specific decoders. To compute cross-modality accuracy for image decoding, a modality-specific decoder trained on captions is evaluated on all stimuli that were presented as images. To compute cross-modality accuracy for caption decoding, a modality-specific decoder trained on images is evaluated on all caption stimuli. <bold>Panel C:</bold> Metrics for modality-agnostic decoders. To compute modality-agnostic accuracy for image decoding, a modality-agnostic decoder is evaluated on all stimuli that were presented as images. The same decoder is evaluated on caption stimuli to compute modality-agnostic accuracy for caption decoding. Here we show feature extraction based on unimodal features for modality-specific decoders and based on multimodal features for the modality-agnostic decoder, in practice the feature extraction can be unimodal or multimodal for any decoder type (see also <xref ref-type="fig" rid="F2">Figure 2</xref>).</p></caption><graphic xlink:href="EMS206394-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><p>Average decoding scores for modality-agnostic decoders (bars), compared to modality-specific decoders trained on data from subjects viewing captions (•) or on data from subjects viewing images (×). The metric is pairwise accuracy (see also <xref ref-type="fig" rid="F3">Figure 3</xref>). Error bars indicate 95% confidence intervals for modality-agnostic decoders. Chance performance is at 0.5.</p></caption><graphic xlink:href="EMS206394-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><p>Decoding accuracy for decoding captions (top) and for decoding images (bottom). The bars indicate modality-agnostic decoding accuracy. The crosses (×) in the top row and the dots (•) in the bottom row indicate within-modality decoding scores. The dots (•) in the top row indicate cross-decoding scores for images, the crosses (×) in the bottom row indicate cross-decoding scores for captions (see also <xref ref-type="fig" rid="F3">Figure 3</xref>).</p></caption><graphic xlink:href="EMS206394-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><p>Decoding examples for image decoding using a modality-agnostic decoder. The first column shows the image the subject was seeing and the 5 following columns show the candidate stimuli with highest similarity to the predicted features, in descending order. We display both the image and the caption of the candidate stimuli because the decoder is based on multimodal features that are extracted from both modalities. All images were taken from the CoCo dataset (<xref ref-type="bibr" rid="R69">Lin <italic>et al.,</italic> 2014</xref>).</p></caption><graphic xlink:href="EMS206394-f006"/></fig><fig id="F7" position="float"><label>Figure 7</label><caption><p>Decoding examples for caption decoding using a modality-agnostic decoder. For details see caption of <xref ref-type="fig" rid="F6">Figure 6</xref>. All images were taken from the CoCo dataset (<xref ref-type="bibr" rid="R69">Lin <italic>et al.,</italic> 2014</xref>).</p></caption><graphic xlink:href="EMS206394-f007"/></fig><fig id="F8" position="float"><label>Figure 8</label><caption><title>Searchlight method to identify modality-agnostic ROIs.</title><p>The top plots show performance (pairwise accuracy averaged over subjects) of modality-agnostic decoders for decoding images (top left) and decoding captions (top right). In the second row, we display cross-decoding performances: On the left, modality-specific decoders trained on captions are evaluated on images. On the right, modality-specific decoders trained on images are evaluated on captions. We identified modality-agnostic ROIs as clusters in which all 4 decoding accuracies are above chance by taking the minimum of the respective t-values at each location, then performed TFCE to calculate cluster values. The plot only shows left medial views of the brain to illustrate the method, different views of all resulting clusters are shown in <xref ref-type="fig" rid="F9">Figure 9</xref>.</p></caption><graphic xlink:href="EMS206394-f008"/></fig><fig id="F9" position="float"><label>Figure 9</label><caption><title>Searchlight results for modality-agnostic regions.</title><p>Maps thresholded at TFCE value of 1508, which is the significance threshold value for which <italic>p</italic> &lt; 10<sup>−4</sup> based on the permutation testing. Regions with highest cluster values are outlined and annotated based on the Desikan-Killiany atlas (<xref ref-type="bibr" rid="R24">Desikan <italic>et al.,</italic> 2006</xref>).</p></caption><graphic xlink:href="EMS206394-f009"/></fig><fig id="F10" position="float"><label>Figure 10</label><caption><title>Searchlight results for imagery decoding.</title><p>Maps thresholded at TFCE values that surpass the significance threshold of <italic>p</italic> &lt; 10<sup>−4</sup> based on a permutation test. Maps thresholded at TFCE value of 3897, which is the significance threshold value for which <italic>p</italic> &lt; 10<sup>−4</sup> based on the permutation testing We used the pairwise accuracy for imagery decoding using the large candidate set of 73 stimuli. We outlined the same regions as in <xref ref-type="fig" rid="F9">Figure 9</xref> to facilitate comparison.</p></caption><graphic xlink:href="EMS206394-f010"/></fig></floats-group></article>