<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS207198</article-id><article-id pub-id-type="doi">10.1101/2025.07.04.663147</article-id><article-id pub-id-type="archive">PPR1047980</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Statistical knockoffs improve biomarker discovery from transcriptomic data</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">https://orcid.org/0009-0006-7745-0634</contrib-id><name><surname>Cartier</surname><given-names>Julie</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Lagoas</surname><given-names>Johanna</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Fermanian</surname><given-names>Adeline</given-names></name><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1003-301X</contrib-id><name><surname>Azencott</surname><given-names>Chloé-Agathe</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5855-0935</contrib-id><name><surname>Massip</surname><given-names>Florian</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>Centre for Computational Biology, MinesParis, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/013cjyk83</institution-id><institution>PSL University</institution></institution-wrap>, <addr-line>60 bd Saint-Michel</addr-line>, <postal-code>75272</postal-code>, <city>Paris</city>, <country country="FR">France</country></aff><aff id="A2"><label>2</label>Institut Curie, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/013cjyk83</institution-id><institution>PSL University</institution></institution-wrap>, <addr-line>11 rue Pierre et Marie Curie</addr-line>, <postal-code>75005</postal-code>, <city>Paris</city>, <country country="FR">France</country></aff><aff id="A3"><label>3</label>U1331, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02vjkv261</institution-id><institution>INSERM</institution></institution-wrap>, <addr-line>11 rue Pierre et Marie Curie</addr-line>, <postal-code>75005</postal-code>, <city>Paris</city>, <country country="FR">France</country></aff><aff id="A4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02kbmgc12</institution-id><institution>AgroParisTech</institution></institution-wrap>, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/03xjwb503</institution-id><institution>Paris-Saclay University</institution></institution-wrap>, <addr-line>22 place de l’Agronomie</addr-line>, <postal-code>91123</postal-code>, <city>Palaiseau</city>, <country country="FR">France</country></aff><aff id="A5"><label>5</label>LOPF, LOPF Califrais’Machine Learning Lab, Paris, France</aff><author-notes><corresp id="CR1"><label>*</label>Corresponding author. <email>florian.massip@minesparis.psl.eu</email>, <email>julie.cartier@mineparis.psl.eu</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>14</day><month>07</month><year>2025</year></pub-date><pub-date pub-type="preprint"><day>07</day><month>07</month><year>2025</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Advances in sequencing technologies have enabled the generation of large amounts of data, offering new possibilities to identify relationships between biological units (<italic>e.g</italic>. genes) and phenotypic traits (<italic>e.g</italic>. disease outcomes). Yet, identifying these associations using variable selection methods remains challenging due to the high dimension (<italic>p</italic> ≫ <italic>n</italic>) and the correlation structure of the data. To address these challenges, we study the applicability of the knockoff (KO) procedure. Introduced by Barber and Candès in 2015, the KO variable selection procedure has shown promising results on real biological data, such as Genome Wide Association Studies. This method seeks to identify the truly important predictors by overcoming the correlation structure between variables while controlling the false discovery rate. Here, we study the applicability of the KO procedure on transcriptomic data in a classification setting. We conduct an extensive simulation study using real transcriptomic data to evaluate the performance of the KO framework in the context of high-dimensional classification. We find that the KO framework outperforms widely used variable selection models, and that using KO aggregation to mitigate the effect of KO stochasticity improves stability while maintaining the same power. Finally applied to three real transcriptomic datasets, the KO framework made very few discoveries, highlighting its conservative nature and suggesting that other methods may substantially overestimate the number of relevant features.</p></abstract><kwd-group><kwd>Knockoffs</kwd><kwd>Variable selection</kwd><kwd>Transcriptomic</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Thanks to the progress of sequencing technologies, it is now possible to collect and access sequencing and genomic information for large cohorts of patients, paving the way for the use of these technologies for many biotechnological and medical applications. In particular, patient stratification and disease grading based on the results of a sequencing experiment is seen as a promising avenue for several diseases, and in cancer in particular [<xref ref-type="bibr" rid="R1">1</xref>, <xref ref-type="bibr" rid="R2">2</xref>]. But despite a lot of research on the topic, discovering biomarkers using transcriptomic data remains challenging. In particular, it has often been noted that transcriptomic signatures for a disease are not always reliable [<xref ref-type="bibr" rid="R3">3</xref>], difficult to reproduce, and that the genes selected as biomarkers are usually highly unstable [<xref ref-type="bibr" rid="R4">4</xref>, <xref ref-type="bibr" rid="R5">5</xref>].</p><p id="P3">Several reasons make biomarker discovery particularly difficult in this context. First, despite the increase in cohort sizes, the number of explanatory variables remains much larger than the number of patients, which is well known to be a difficult statistical setup (often called =high dimension” or =large <italic>p</italic> small <italic>n</italic>”) [<xref ref-type="bibr" rid="R6">6</xref>, <xref ref-type="bibr" rid="R7">7</xref>]. An additional difficulty arises due to the high level of correlation between the different gene’s expression. Indeed, genes that interact to take part to common biological functions are often co-regulated by similar mechanisms, inducing correlation between genes’ expressions. As a result, it becomes difficult to differentiate genes that are truly associated to the outcome of interest from those that appear associated to the outcome solely because they are correlated to a causal gene.</p><p id="P4">Recently, a novel statistical method named Knockoff (KO) [<xref ref-type="bibr" rid="R8">8</xref>] has been proposed to tackle the problem of variable selection for correlated features in high dimension. KO have been applied to a range of biological questions, in particular in Genome-Wide Association Studies [<xref ref-type="bibr" rid="R9">9</xref>, <xref ref-type="bibr" rid="R10">10</xref>], where it was demonstrated that KO allowed to control the number of false discoveries. However, up to now, only few studies have suggested to apply KO to transcriptomic data [<xref ref-type="bibr" rid="R11">11</xref>, <xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R13">13</xref>] and to the best of our knowledge, applying KO to transcriptomic data, in high dimension, for classification, remains unexplored. Hence, whether KO can improve variable selection for transcriptomic data remains to be studied. In addition, because KO is a flexible method, identifying the optimal way to apply it to real data is complex, which can prevent its use in practice. For instance, many methods have been developed to generate KO matrices whose power might depend on the specificities of users’ data.</p><p id="P5">Moreover, one particular difficulty with biomarker discovery is that one often obtains unstable results, <italic>i.e</italic>. features selected by the model tend to change a lot with small perturbations of the input data, making it especially hard to transfer results from one cohort of patients to the next. While using KO has been shown to improve false discovery control, the impact of this statistical method on the stability of the selected features is rarely studied. Indeed, as KO generation is non-deterministic, results may vary between several runs of the same method. One way to alleviate this issue is to run the KO framework multiple times and aggregate the results [<xref ref-type="bibr" rid="R14">14</xref>, <xref ref-type="bibr" rid="R15">15</xref>, <xref ref-type="bibr" rid="R16">16</xref>, <xref ref-type="bibr" rid="R12">12</xref>].</p><p id="P6">The goal of this paper is to study whether KO improves variable selection when applied to transcriptomic data for classification tasks. First, we perform a systematic comparison of the performance of different KO generation methods and statistics. We then conduct simulations to compare the KO framework with baselines, namely Lasso regression and Wilcoxon rank-sum tests. We also include KO aggregation in this experiment to assess its performance. Next, we evaluate the stability of the KO framework for variable selection. We focus on different selection scenarios and demonstrate that feature correlation reduces the power of the method. Finally, we apply our resulting KO method on three real-world datasets: two include lung cancer patients and one includes breast cancer patients.</p><sec id="S2"><title>Overview of the KO framework</title><p id="P7">In this paper, KO refers to the versatile Model-X Knockoffs [<xref ref-type="bibr" rid="R17">17</xref>] which we briefly present here. A more detailed summary of the KO procedure can be found in the appendix (see <xref ref-type="supplementary-material" rid="SD1">Supp. Mat. A</xref>).</p><p id="P8">Let <bold>y</bold> ∈ {0, 1}<sup><italic>n</italic></sup>, <bold>y</bold> = (<italic>y</italic><sub>1</sub> … <italic>y<sub>n</sub></italic>)<sup><italic>T</italic></sup> be the random variables representing the outcomes and <italic>X</italic> = (<italic>X</italic>.,<sub>1</sub>, … <italic>X</italic>.,<sub><italic>p</italic></sub>) be the matrix of features (here, genes’ expressions) such that for any <italic>j</italic> ∈ {1, … <italic>p</italic>}, <italic>X</italic>.,<sub><italic>j</italic></sub> = (<italic>X</italic><sub>1,<italic>j</italic></sub>, …, <italic>X</italic><sub><italic>n</italic>,<italic>j</italic></sub>}<sup><italic>T</italic></sup> ∈ ℝ<sup><italic>n</italic></sup>. In the following <italic>X</italic>.,<sub><italic>j</italic></sub> will be noted <italic>X<sub>j</sub></italic>. Let ℋ<sub>0</sub> denote the set of indices of null features such that for all <italic>j</italic> ∈ ℋ<sub>0</sub>, <bold>y</bold> ╨ <italic>X<sub>j</sub></italic> |<italic>X</italic><sub>−<italic>j</italic></sub> where <italic>X</italic><sub>− <italic>j</italic></sub> is the matrix <italic>X</italic> without the <italic>j</italic><sup>th</sup> column. The goal of variable selection is to identify the set of non-null features given the observations <italic>X</italic> and <bold>y</bold>, <italic>i.e</italic>., the set <inline-formula><mml:math id="M1"><mml:mrow><mml:msubsup><mml:mi>ℋ</mml:mi><mml:mn>0</mml:mn><mml:mi>c</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>}</mml:mo><mml:mo>∖</mml:mo><mml:msub><mml:mi>ℋ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>. The KO framework does so while ensuring that the false discovery rate (FDR) is controlled. Therefore, running the KO framework results in the selection of a set of variables <inline-formula><mml:math id="M2"><mml:mrow><mml:mover accent="true"><mml:mi>𝒮</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>⊂</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> such that: <disp-formula id="FD1"><label>(1)</label><mml:math id="M3"><mml:mi>F</mml:mi><mml:mi>D</mml:mi><mml:mi>R</mml:mi><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mi>𝔼</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mover accent="true"><mml:mi>𝒮</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>∩</mml:mo><mml:msub><mml:mi>ℋ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mi>max</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>|</mml:mo><mml:mover accent="true"><mml:mi>𝒮</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>|</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo stretchy="false">]</mml:mo><mml:mo>⩽</mml:mo><mml:mi>q</mml:mi><mml:mo>,</mml:mo></mml:math></disp-formula> max where <italic>q</italic> is the target FDR level.</p><p id="P9">The KO framework relies on the construction of false copies of every feature in the data matrix <italic>X</italic>, which will be used as control variables. These copies are gathered to form the KO matrix <inline-formula><mml:math id="M4"><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. To guarantee FDR control (that is, <xref ref-type="disp-formula" rid="FD1">Eq. (1)</xref>), the KO matrix needs to fulfill two conditions, namely that the distribution of the fake variables is as similar as possible to the distribution of the real features (=swap property”), and that the KO matrix <inline-formula><mml:math id="M5"><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:math></inline-formula> is independent of the outcomes <bold>y</bold> conditionally on the real feature matrix <italic>X</italic> (=conditional independence property”).</p><p id="P10">A statistic <italic>W<sub>j</sub></italic> is then constructed for every feature <italic>j</italic> based on the knockoff <inline-formula><mml:math id="M6"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and the true feature <italic>X<sub>j</sub></italic>. A high value of <italic>W<sub>j</sub></italic> must provide evidence that feature <italic>j</italic> is non-null. Candès et al. [<xref ref-type="bibr" rid="R17">17</xref>] have demonstrated that, under some assumptions, the set of features <inline-formula><mml:math id="M7"><mml:mrow><mml:mover accent="true"><mml:mi>𝒮</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mi>j</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⩾</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>, with <disp-formula id="FD2"><mml:math id="M8"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mi>min</mml:mi><mml:mo>{</mml:mo><mml:mi>t</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo>|</mml:mo><mml:mo>{</mml:mo><mml:mi>j</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⩽</mml:mo><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mo>}</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mo>{</mml:mo><mml:mi>j</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⩾</mml:mo><mml:mi>t</mml:mi><mml:mo>}</mml:mo><mml:mo>|</mml:mo></mml:mrow></mml:mfrac><mml:mo>⩽</mml:mo><mml:mi>q</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:math></disp-formula> allows to control the FDR.</p><p id="P11">Additionally to control the FDR, a good variable selection method must ensure good power, that is, must be able to retrieve the non-null features. Mathematically, the power is defined by <disp-formula id="FD3"><mml:math id="M9"><mml:mrow><mml:mtext>Power</mml:mtext><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:mover accent="true"><mml:mi>𝒮</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>∩</mml:mo><mml:msubsup><mml:mi>ℋ</mml:mi><mml:mn>0</mml:mn><mml:mi>c</mml:mi></mml:msubsup><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:msubsup><mml:mi>ℋ</mml:mi><mml:mn>0</mml:mn><mml:mi>c</mml:mi></mml:msubsup><mml:mo>|</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P12">Unlike FDR control, the KO methodology does not guarantee good power by construction and power analysis is theoretically very challenging [<xref ref-type="bibr" rid="R18">18</xref>]. Intuitively, to have good power, <inline-formula><mml:math id="M10"><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:math></inline-formula> should be as different from the original matrix <italic>X</italic> as possible (the extreme choice <inline-formula><mml:math id="M11"><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula> being a valid choice in terms of FDR control but having a very bad power).</p><p id="P13">In summary, the task is to build KO matrices and statistics that satisfy the swap and conditional independence properties in order to control the number of false discoveries while achieving the best possible power. Various methods have been proposed to address this problem. Here, we present a few methods that can be used in the context of high-dimensional data and classification.</p><sec id="S3"><title>Generation of KO features</title><p id="P14">The most common methods assume that the feature matrix <italic>X</italic> follows a Gaussian distribution. Under this assumption, a parametrized family of distributions of <inline-formula><mml:math id="M12"><mml:mrow><mml:mover><mml:mrow><mml:mi>X</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>˜</mml:mo></mml:mover><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula> can be computed, from which a valid KO for the original features can be sampled [<xref ref-type="bibr" rid="R17">17</xref>]. Several families of approaches have been proposed under this assumption</p><p id="P15">In the family of Mean Absolute Correlation (MAC) Gaussian methods, the KO features are computed to minimize the correlation between a feature <italic>X<sub>j</sub></italic> and its knockoff <inline-formula><mml:math id="M13"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> [<xref ref-type="bibr" rid="R17">17</xref>]. Among the different MAC approaches, the semidefinite program (SDP) is expected to be the most powerful as <italic>p</italic> becomes large. The MAC-based KO frameworks suffer from a tendency to identify some KO of non-null original features as important features in the model instead of the non-null features themselves [<xref ref-type="bibr" rid="R19">19</xref>]. To address this issue, a new family of Gaussian KO generation methods called Minimum Reconstructability (MRC) has been developed. One typical approach is to maximize the conditional variance <inline-formula><mml:math id="M14"><mml:mi>𝕍</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mover><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>, for any <italic>j</italic> ∈ {1, …, <italic>p</italic>}. This approach is known as the minimum variance-based reconstructability (MVR) method. Additionnally, conditional independent (CI) KO [<xref ref-type="bibr" rid="R18">18</xref>, <xref ref-type="bibr" rid="R20">20</xref>] satisfies conditional independence between a feature <italic>X<sub>j</sub></italic> and its knockoff <inline-formula><mml:math id="M15"><mml:msub><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula>, given the other covariates <italic>X</italic><sub>−<italic>j</italic></sub>.</p><p id="P16">Finally, the =Sequential Conditional Independant Pairs” (SCIP) algorithms [<xref ref-type="bibr" rid="R17">17</xref>] allow for the construction of valid KO with no further assumption on the distribution of the features <italic>X</italic>. In practice, a linear empirical version of this algorithm is often used, called the Linear Sequential Conditional Independant Pairs algorithm (LSCIP) [<xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R21">21</xref>].</p><p id="P17">We stress that this list of methods for computing valid knockoffs is not exhaustive. We have focused on methods particularly suitable for transcriptomic-like data. For instance, methods using deep learning models have been proposed [<xref ref-type="bibr" rid="R22">22</xref>, <xref ref-type="bibr" rid="R23">23</xref>], but we will not consider these since our data sets have a small sample size.</p></sec><sec id="S4"><title>KO test statistics</title><p id="P18">Most KO statistics are built as differences between variable importance metrics. More precisely, they are obtained by applying a machine learning model to <inline-formula><mml:math id="M16"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> with target <bold>y</bold>, choosing a variable importance metric associated to the model, and computing the difference between the importance measure assigned to a feature and that assigned to its KO.</p><p id="P19">For linear settings, an intuitive option is to use the absolute values of the coefficients of a penalized logistic regression (LASSO Coefficient Difference (LCD) or Elastic-Net penalized logistic regression Coefficient Difference (EN-CD)) [<xref ref-type="bibr" rid="R8">8</xref>, <xref ref-type="bibr" rid="R17">17</xref>] as the variable importance. Likewise, variable importance in nonlinear models can be used to define KO test statistics. These include differences in Gini impurity in random forests [<xref ref-type="bibr" rid="R19">19</xref>], risk reduction in boosting (RRB) in boosted trees [<xref ref-type="bibr" rid="R13">13</xref>], and several other boosted tree-based statistics [<xref ref-type="bibr" rid="R11">11</xref>], as well as approaches based on deep learning [<xref ref-type="bibr" rid="R24">24</xref>].</p><p id="P20">Finally, some statistics proposed in the literature cannot be directly expressed as a variable difference. One example is the masked likelihood ratio (MLR) [<xref ref-type="bibr" rid="R25">25</xref>], which is based on the estimation of log-likelihood ratios. This method accounts for the high correlation between a feature and its KO by lowering the absolute value of their statistic when the correlation is large. Another example is the Lasso Signed Max (LSM) [<xref ref-type="bibr" rid="R8">8</xref>].</p></sec><sec id="S5"><title>KO aggregation</title><p id="P21">The stochastic nature of KO generation methods means that applying the same KO construction method twice to the same data matrix will generate different KO matrices. This results in different subsets of selected variables at each iteration. KO aggregation addresses this issue by applying the KO procedure multiple times and =aggregating knowledge” from these runs to select variables. Several aggregation methods have been proposed, the most recent of which is the Knockoff-<italic>π</italic> (KOPI) [<xref ref-type="bibr" rid="R12">12</xref>] framework.</p></sec></sec></sec><sec id="S6" sec-type="materials | methods"><title>Materials and Methods</title><p id="P22">To evaluate the performance of the KO framework on highdimensional transcriptomic data, we use simulated outcomes computed from real transcriptomic data. This allows us to keep the correlation structure of biological data (and its specificities such as non-Gaussianity) while knowing the true variables to recover.</p><sec id="S7"><title>Real data sets</title><p id="P23">The CRUKPAP data [<xref ref-type="bibr" rid="R26">26</xref>] is composed of healthy volunteers and patients with benign lung condition (<italic>n</italic> = 160) and subjects diagnosed with lung cancer (<italic>n</italic> = 199). For all 369 samples, we have transcriptomic data from nasal epithelium samples (RNA-sequencing, 18, 072 genes) from which 749 genes have been identified as involved in patients’ response to smoke. In the following, we keep only those 749 risk genes to form the data matrix (<italic>X</italic><sub>CRUKPAP</sub>)</p><p id="P24">The AEGIS (Airway Epithelial Gene Expression in the Diagnosis of Lung Cancer) data [<xref ref-type="bibr" rid="R27">27</xref>] comes from a clinical trial conducted in the USA, Canada and Europe. This data set is composed of 505 clinical patients including 308 patients diagnosed with lung cancer for whom we have gene expression levels (microarrays) of nasal epithelium (16, 127 genes). We use the set of 535 genes that were found differentially expressed in a subset of the entire cohort. For the experiments with real outcomes, we use cancer status as the outcome for both the CRUKPAP and AEGIS data sets.</p><p id="P25">Finally, the breast cancer (BC) cohort is composed of gene expression data (RNA sequencing, 18, 802 genes) from tumors of patients in the SCAN-B project (Sweden data canceronome analysis network breast initiative) [<xref ref-type="bibr" rid="R28">28</xref>]. These data were used to predict biomarkers in breast cancer, identifying 869 genes with non-zero weights in at least one of five biomarker classifiers. We focus on the estrogen receptor (ER), progesterone receptor (PgR) and human epidermal growth factor receptor 2 (HER2) status. Thus, our data matrix includes expression levels of these 869 genes for 2,827 samples with no missing data concerning the ER, PgR, or HER2 status. ER and PgR status described protein content, with, respectively 2, 600 and 2, 458 positives, while HER2 status reflects gene amplification, with 338 amplified cases.</p><p id="P26">We use the same pre-processing steps (quality check, batch effect correction, normalization) for these three transcriptomic data sets as in previous studies [<xref ref-type="bibr" rid="R26">26</xref>, <xref ref-type="bibr" rid="R27">27</xref>, <xref ref-type="bibr" rid="R28">28</xref>].</p></sec><sec id="S8"><title>Outcome simulation</title><p id="P27">Unless otherwise specified, the features are standardized to have zero-mean and unit-variance, that is, for <italic>j</italic> ∈ {1, …, <italic>p</italic>}, we apply the operation <inline-formula><mml:math id="M17"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>←</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M18"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the empirical mean of feature <italic>j</italic> and <italic>σ<sub>j</sub></italic> its standard deviation.</p><p id="P28">We denote by <italic>k</italic> the number of non-zero features, that is, <inline-formula><mml:math id="M19"><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mo>|</mml:mo><mml:msubsup><mml:mi>ℋ</mml:mi><mml:mn>0</mml:mn><mml:mi>c</mml:mi></mml:msubsup><mml:mo>|</mml:mo></mml:math></inline-formula> and set <italic>k</italic> = 10 in all experiments. We construct a vector <bold><italic>β</italic></bold> ∈ ℝ<sup><italic>p</italic></sup> by randomly sampling <italic>k</italic> indices from {1, …, <italic>p</italic>} to which we assign values drawn uniformly from {−1, 1}, and set all remaining components of <bold><italic>β</italic></bold> to zero. We then multiply <bold><italic>β</italic></bold> with a magnitude parameter <italic>a</italic>.</p><p id="P29">We consider two simulation settings. In the linear simulation setting, for each <italic>i</italic> ∈ {1, …, <italic>n</italic>}, we generate the <italic>i</italic><sup>th</sup> component of the outcome vector <bold>y</bold> ∈ {0, 1}<sup><italic>n</italic></sup> according to the following generalized linear model: <disp-formula id="FD4"><label>(2)</label><mml:math id="M20"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mtable columnalign="left" equalrows="true" equalcolumns="true"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mtext>if(</mml:mtext><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:msub><mml:mi>β</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn><mml:mtext>otherwise</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula> where <italic>X</italic><sub><italic>i</italic>,.</sub> is the <italic>i</italic><sup>th</sup> row of <italic>X</italic>. We take <italic>a</italic> = 10, calibrated such that the two classes are well separated for all data sets (see <xref ref-type="supplementary-material" rid="SD1">Supp. Mat. B.1: Fig. S1</xref>).</p><p id="P30">In the interaction simulation setting, we assume that <italic>p</italic> and <italic>k</italic> are even and define a new <inline-formula><mml:math id="M21"><mml:mrow><mml:mi>β</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mfrac><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> that has <inline-formula><mml:math id="M22"><mml:mrow><mml:mfrac><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula> non zero components. Without loss of generality, we assume that the columns of the data matrix <italic>X</italic>, and correspondingly the entries of <bold><italic>β</italic></bold>, can be reordered such that the indices of the non-zero features are the first ones. Then, the <italic>i</italic><sup>th</sup> component of the outcome vector <bold>y</bold> is defined as follows: <disp-formula id="FD5"><label>(3)</label><mml:math id="M23"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mtable columnalign="left" equalrows="true" equalcolumns="true"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mtext>if(</mml:mtext><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn><mml:mtext>otherwise</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula> where <inline-formula><mml:math id="M24"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">β</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mfrac><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:munderover><mml:msub><mml:mi mathvariant="bold">β</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> where <italic>β<sub>j</sub></italic> is the <italic>j<sub>th</sub></italic> coordinate of <bold><italic>β</italic></bold>. This allows to model pairwise/quadratic interactions between variables. Note that obtaining well-separated classes in the interaction simulation setting is difficult. We set <italic>a</italic> to 100 and selected outcome vectors that yielded classes of similar proportions by trial and error (see <xref ref-type="supplementary-material" rid="SD1">Supp. Mat. B.1: Fig. S2</xref>).</p></sec><sec id="S9"><title>Implementation details</title><p id="P31">We generate KO matrices and statistics using either the Python package <italic>knockpy</italic> [<xref ref-type="bibr" rid="R19">19</xref>] (v1.3.0) or the R package <italic>knockoff</italic> [<xref ref-type="bibr" rid="R17">17</xref>] (v0.3.6) except for the LSCIP method and the EN-CD statistic, for which we use the R package <italic>glmnet</italic> [<xref ref-type="bibr" rid="R29">29</xref>] (v4.1-7). The parameter <italic>λ</italic><sub>min</sub> refers to the penalization obtained by minimizing the mean cross-validated error which is by default computed with the deviance (difference of log-likelihoods) error over a 10-fold cross-validation. The parameter <italic>λ</italic><sub>oracle</sub> is chosen <italic>a posteriori</italic> and corresponds to the penalization term that maximizes the ratio between true discoveries and false discoveries in a grid of values ranging from 0 to 1 in increments of 0.001. Variable selection with KOPI is performed using the code provided by [<xref ref-type="bibr" rid="R12">12</xref>]. The Wilcoxon rank-sum test is applied on already preprocessed data as in [<xref ref-type="bibr" rid="R30">30</xref>] with the R <italic>stats</italic> package (v.4.2.0). We correct for multiple hypotheses testing with the Benjamini-Hochberg (BH) [<xref ref-type="bibr" rid="R31">31</xref>] procedure. Details are given in <xref ref-type="supplementary-material" rid="SD1">supplementary materials (see B.2)</xref>.</p></sec><sec id="S10"><title>Experimental setups</title><p id="P32">We denote by <italic>M</italic> the number of outcome vectors simulated for each experiment. The target FDR level (<italic>q</italic>) sequence, used by KO-based methods, varies from 0 to 1 in increments of 0.01. For all experiments, unless otherwise specified, we use <italic>X</italic><sub>CRUKPAP</sub> ∈ ℝ<sup>369×749</sup> as data matrix to compute <bold>y</bold>.</p></sec><sec id="S11"><title>KO generation methods and statistics benchmark</title><p id="P33"><italic>M</italic> is set to 100 in the linear simulation setting and to 10 in the interaction simulation setting. From one simulation to the next, only the identity of the genes that are selected to compute <bold>y</bold> changes. We compare methods and statistics that seem particularly appropriate for transcriptomic-like data. We include in the comparison the SDP method as the gold standard for KO generation methods, the MVR method, as representative of MRC methods, CI-based KO construction, and the LSCIP algorithm (see <xref ref-type="supplementary-material" rid="SD1">Supp. Mat. Algo. 1</xref>). We use the LCD statistic for the KO generation methods benchmark since it is the gold standard statistic in the linear simulation setting.</p><p id="P34">Concerning statistics, we benchmark the LCD, since it outperforms other LASSO-based statistics (see <xref ref-type="supplementary-material" rid="SD1">Supp. Mat. C.1.1</xref>: <xref ref-type="supplementary-material" rid="SD1">Fig. S4</xref>). We also consider for linear statistics: the Elasticnet coefficient difference (EN-CD), and the MLR statistic which has shown better performance than the LCD statistic in low-dimensional linear logistic regression [<xref ref-type="bibr" rid="R25">25</xref>]. For tree-based methods, the results (see <xref ref-type="supplementary-material" rid="SD1">Supp. Fig. S5</xref>) show that none of the statistics are particularly effective, but RRB and Gini variable importance seem to be slightly more powerful. However, as RRB is extremely time-consuming to compute, we chose to use Gini-based variable importance. Given the poor performance of deep learning based statistics (see <xref ref-type="supplementary-material" rid="SD1">Supp. Fig. S6</xref>), we did not include them in the comparison.</p></sec><sec id="S12"><title>Comparison between KO, LASSO, Wilcoxon rank-sum test and KOPI</title><p id="P35">For <italic>M</italic> = 100 outcome vectors, we run the KO as well as the KOPI procedures with LSCIP and LCD methods. We compare it with the Wilcoxon rank-sum test combined with the BH procedure for target FDR levels in {10<sup>−10</sup>, 5 × 10<sup>−10</sup>, 10<sup>−8</sup>, 5 × 10<sup>−8</sup>, …, 0.01, 0.05, 0.5, 1}—this is the equivalent to <italic>q</italic> in the KO framework. We also add in the comparison a LASSO-penalized logistic regression with <italic>λ</italic> ranging from 0 to 0.4 in steps of 0.004, <italic>λ</italic><sub>min</sub> and <italic>λ</italic><sub>oracle</sub> (see 3.3). Our metrics to assess the performances are the average power and its standard deviation, as well as the average FDP and its standard deviation.</p></sec><sec id="S13"><title>Stability study</title><p id="P36">To study the stability of the different variable selection methods with respect to perturbation in the input samples, we use ten times 10-fold cross-validation for a given simulated outcome <bold>y</bold>. At each step of the cross-validation we apply variable selection procedures to the training set, which contains 90% of the data. We extract the proportion of times each feature has been selected — referred to as the selection frequency — across the 10 ×10 iterations. We run the entire procedure with 10 different outcomes <bold>y</bold>.</p></sec><sec id="S14"><title>Real data application</title><p id="P37">We apply KOPI with LSCIP and LCD methods for different target FDR levels (<italic>q</italic> P t0.1, 0.2, 0.3, 0.5u) to the three real data sets 3.1.</p></sec></sec><sec id="S15" sec-type="results"><title>Results</title><p id="P38">In the following, we present the results of the simulation study conducted to investigate the applicability of the KO framework to transcriptomic data.</p><sec id="S16"><title>KO generation methods and statistics benchmark</title><p id="P39">We first compare the performance of different KO generation methods and statistics in the linear simulation setting.</p><p id="P40">We find that MVR, SDP, CI and LSCIP have similar power, as illustrated in <xref ref-type="fig" rid="F1">Fig. 1</xref>. Concerning the statistics, <xref ref-type="fig" rid="F1">Fig. 1</xref> clearly show better power for linear statistics (LCD, EN-CD, and MLR)—which was expected as we used a linear model to generate <bold>y</bold>. Over the 100 replicates, see <xref ref-type="supplementary-material" rid="SD1">supplementary figures S3 (c-d)</xref>), the EN-CD statistics have a lower power on average than the other two linear statistics for target FDR levels lower than 0.5. Our results also highlight that the EN-CD statistic is conservative with FDP lower on average than the target FDR level and the other three methods for <italic>q</italic> &lt; 0.5. On average, the four KO generation methods and statistics allow control of the number of false discoveries (see <xref ref-type="supplementary-material" rid="SD1">Supp. Fig. S3 (a)</xref>).</p><p id="P41">We also compare the performance obtained with the different statistics in the interaction case, to account for the interacting nature of biological processes. All methods, even the RF nonlinear statistic, demonstrate a clear lack of performance (see <xref ref-type="fig" rid="F2">Fig. 2</xref> and <xref ref-type="supplementary-material" rid="SD1">Supp. Mat. C.1.3</xref>).</p><p id="P42">We then conducted the same experiments on the AEGIS and BC datasets and found similar results (<xref ref-type="supplementary-material" rid="SD1">Supp. Mat. D.1 and D.2</xref>), although we note that the performance of the KO methods are generally better on the BC cohort than on the others. All our analyses, except for the BC data, exhibit large standard deviation intervals (<xref ref-type="fig" rid="F1">Fig. 1</xref>, <xref ref-type="supplementary-material" rid="SD1">Supp. Fig. S3 (b-d)</xref>, and <xref ref-type="supplementary-material" rid="SD1">Supp. Fig. S15</xref> and <xref ref-type="supplementary-material" rid="SD1">Fig. S16</xref>), indicating high variability in our results, which motivates the stability study below.</p><p id="P43">To conclude, this simulation study shows that all KO generation methods produce similar results, in contrast to other settings in which MVR or CI were shown to outperform SDP [<xref ref-type="bibr" rid="R19">19</xref>, <xref ref-type="bibr" rid="R20">20</xref>]. Concerning the choice of statistics, MLR and LCD have shown the best performance in the linear simulation setting. In what follows, we will use the LSCIP method for KO generation because it does not make assumptions about the data structure and the LCD statistic since it is the most widely used in the literature.</p><p id="P44">After determining which KO generation method and which statistic to use, we compared the performance of the KO framework using the aggregation method KOPI, and found that KOPI is as powerful as the standard KO procedure, and thus include KOPI in the following experiments (see <xref ref-type="supplementary-material" rid="SD1">Supp. Fig. S11</xref> and <xref ref-type="table" rid="T1">Tab. 1</xref>).</p></sec><sec id="S17"><title>Comparison to baselines</title><p id="P45">We investigate how KO methods compare to other standard variable selection procedures. We consider the linear simulation setting and compare KO methods to a LASSO-penalized logistic regression and Wilcoxon rank-sum tests (which has shown a promising reduction in of the number of false discoveries compared to some other methods for large sample sizes in differential expression analysis [<xref ref-type="bibr" rid="R30">30</xref>, <xref ref-type="bibr" rid="R32">32</xref>]).</p><p id="P46"><xref ref-type="table" rid="T1">Table 1</xref> summarizes the power and FDP values obtained for the different methods. It underlines that the Wilcoxon ranksum test and the LASSO computed with <italic>λ</italic><sub>min</sub> induce many false discoveries with an average FDP above 0.85. The two KO-based methods demonstrate better power over FDP ratio than other methods. In particular, we optimize <italic>a posteriori</italic> the LASSO penalization parameter to maximize the power over FDP ratio (called <italic>λ</italic><sub>oracle</sub>q, and found that KO-based methods outperformed the LASSO even in the LASSO most ideal case. As in our previous benchmarks, standard deviation values in Tab. 1 illustrate high variability.</p><p id="P47">To confirm our results, despite large standard deviation for both power and FDP, we compare the selection performance for larger sequences of false discoveries control parameters for the KO, KOPI and Wilcoxon rank-sum test and a sequence of penalization parameters for the LASSO. Regardless of the FDP interval considered after selection, the KO procedure and KOPI are more powerful than the LASSO and the Wilcoxon rank-sum test and show similar power (see <xref ref-type="supplementary-material" rid="SD1">Supp. Fig. S10</xref> and <xref ref-type="supplementary-material" rid="SD1">Fig. S11</xref>).</p><p id="P48">Overall, these comparisons with baselines models demonstrate the ability of KO-based methods to improve selection power and to control the number of false discoveries.</p></sec><sec id="S18"><title>Stability study</title><p id="P49">Our previous results show that the KO framework controls the number of false discoveries while maintaining good power in the linear simulation setting. However, these performance metrics do not give any information about the impact of the stochasticity of the KO framework and the confidence we can have in the resulting selected subsets of genes. To fill this gap, we now turn to the study of feature selection stability that we use as a measure of reliability. Stability is defined as the extent to which two subsets selected by the same method resemble each other when the sample to which it is applied is disturbed [<xref ref-type="bibr" rid="R4">4</xref>]. To study variable selection stability, we simulate perturbation by subsampling of the initial samples (see section 3.4.3 for details) and analyze the selection frequency of each variable.</p><p id="P50">The results are given in <xref ref-type="fig" rid="F3">Fig. 3</xref>. We find that LASSO-penalized logistic regressions, with penalization parameter chosen by cross-validation, selects too many variables and is unstable. In particular, out of a total of 749×10 genes, it has 2, 070 alternatively selected genes, <italic>i.e</italic>., genes that are neither always nor never selected–in other words, their selection frequency is not equal to 0 or 1. In addition, among the set of systematically selected genes, more than 40% are false discoveries.</p><p id="P51">Conversely, knockoff-based methods tend to limit the selection of alternatively selected features, 335 for the vanilla knockoff and 102 for KOPI with <italic>q</italic> = 0.2. Taking <italic>q</italic> = 0.5 and thus relaxing the constraint on the number of false discoveries enables the selection of true causal features at almost each iteration but also leads to an increase of the number of alternatively selected features (1 237 with vanilla knockoff and 470 with KOPI). KOPI leads to fewer alternatively selected genes as the KO procedure, showing the ability of the aggregation scheme to mitigate the effect of knockoff stochasticity while maintaining similar power (see <xref ref-type="fig" rid="F3">Fig. 3</xref> and <xref ref-type="supplementary-material" rid="SD1">Supp. Fig. S11</xref>.</p><p id="P52">Finally, the stability of the KOPI framework is equivalent to that of the oracle LASSO (set <italic>a posteriori</italic> to maximize the true-to-false discovery ratio), showing that KOPI is as good as the best possible selection set obtained with the lasso.</p><p id="P53"><xref ref-type="table" rid="T2">Table 2</xref> summarizes the number of true and false discoveries found in <xref ref-type="fig" rid="F3">Fig. 3</xref>. As expected, we find that the LASSO makes more true discoveries at the cost of a high number of false discoveries (proportion of true discoveries: 3.7%) while KOPI selects fewer features in order to control the FDR level (see <xref ref-type="table" rid="T2">Tab. 2</xref>). KOPI also demonstrates a higher proportion of true discoveries among the set of selected genes (for <italic>q</italic> = 0.2, 56.6% and for <italic>q</italic> = 0.5, 14.0%) than the LASSO (0.04%) and the oracle LASSO (13.7%).</p><p id="P54">Overall, our results illustrate both the conservative nature and the ability of knockoff-based methods to remove false discoveries specifically among systematically selected features. We also find that features selected by KOPI are more reliable than those selected by the KO framework or the LASSO.</p></sec><sec id="S19"><title>Influence of the correlation between features on performance</title><p id="P55">To study the causes of the differences in performance of the KO method between the different datasets (see <xref ref-type="supplementary-material" rid="SD1">Supp. Mat. D and Fig. 1</xref>), we compare the correlation matrices of the three cohorts (see <xref ref-type="supplementary-material" rid="SD1">Supp. Fig. S19</xref>) and find that the matrix data <italic>X</italic><sub>BC</sub> has a less correlated structure than both <italic>X</italic><sub>CRUKPAP</sub> and <italic>X</italic><sub>AEGIS</sub>. This suggests that correlation between causal genes may reduce power.</p><p id="P56">To test this hypothesis, we introduce difficulty categories (=easy”, =medium”, and =hard”) based on power criteria, which depict how easy it is for the KO framework to find the 10 true causal variables. We categorize in Tab. 3 each of the 100 simulations used in Section 3.4.1 (for additional details and <xref ref-type="supplementary-material" rid="SD1">Supp. Mat. D</xref>). <xref ref-type="table" rid="T3">Tab. 3</xref> shows that the selection is easier with the breast cancer cohort (54 =easy” cases) than with the other two cohorts (0 and 7 =easy” cases for CRUKPAP and AEGIS cohorts).</p><p id="P57">As an illustration, we compare (see <xref ref-type="fig" rid="F4">Fig. 4</xref>) the average power obtained over ten =easy” settings and ten =hard” settings for the CRUKPAP and the breast cancer cohorts. These curves point out that the CRUKPAP results demonstrate larger variability in performance than the breast cancer results. It also shows that the identity of the 10 non null features is the main source of performance variability. To quantify how correlation affects performance, we show in <xref ref-type="fig" rid="F5">Fig. 5</xref> the mean power as a function of the average pairwise correlation between the non null features. We find that the highest power values are reached for low pairwise correlations (&lt; 0.2). It also depicts a general tendency of the KO framework to be more powerful on average for lower pairwise correlations. Ultimately, higher correlation in the data matrix makes selection harder in the KO framework. We have shown that selection performance and variability depend partly on the correlation between true causal features.</p></sec><sec id="S20"><title>Application to real data</title><p id="P58">To evaluate the applicability of KO-based procedures in real settings, we apply KOPI to perform variable selection with real transcriptomic data from CRUKPAP, AEGIS and the breast cancer data sets, but this time <bold>y</bold> is set to real outcomes— for which we thus do not know the true causal genes. For CRUKPAP and AEGIS, we aim to select variables involved in cancer diagnosis (cancer/no cancer). In the breast cancer study, we are looking for genes that can discriminate breast cancer biomarkers: ER status (positive/negative), PgR status (positive/negative), and HER2 status (amplified/normal).</p><p id="P59">We find that for lung cancer predictions, ER and PgR status predictions, no genes are selected with <italic>q =</italic> {0.1, 0.2, 0.3}. For <italic>q</italic> set to 0.5, only 3 genes are selected in the CRUKPAP cohort and 7 in the AEGIS cohort (see <xref ref-type="supplementary-material" rid="SD1">Supp. Tab. S1</xref>). For the prediction of ER status in the breast cancer cohort, only one gene, the ESR1 gene (coding for the estrogen receptor) is selected. Likewise, in the sets of genes selected (6 genes for <italic>q</italic> = 0.3 and 13 genes for <italic>q</italic> = 0.5) as involved in HER2 status, we retrieve the gene coding for the HER2 protein (ERBB2). The fact that no genes are selected for the PgR status prediction is consistent with the results of the original breast cancer study where the PgR status was shown to be the hardest to predict from PGR expression level [<xref ref-type="bibr" rid="R28">28</xref>].</p><p id="P60">Finally, we also note that the ERBB2 gene is less correlated to other genes as compared to ESR1 and PGR (see <xref ref-type="supplementary-material" rid="SD1">Supp. Fig. S20</xref> and <xref ref-type="supplementary-material" rid="SD1">Fig. S21</xref>). This lower level of correlation might in part explain why the KO-framework selects more genes in the HER2 prediction case, in good agreement with our results of Section 4.4. Overall, these results illustrate the conservative nature of KOPI in real settings.</p></sec></sec><sec id="S21" sec-type="discussion"><title>Discussion</title><p id="P61">In this paper, we studied the applicability of the KO framework to variable selection problems on transcriptomic data. We focused on a common use case where the goal is to find genes associated with a phenotype of interest—such as a disease. This is in contrast to most applications of KO which are conducted in a regression setting. We benchmark different statistics and KO generation methods, and found that all KO generation methods yield similar results in our simulations. MLR and LCD statistics showed the best performance in the linear simulation setting while being, like other statistics, powerless in the interaction simulation setting. Our extensive simulation study shows that KO succeed in significantly reducing the false discovery rate compared to standard feature selection procedures. While using KO can also reduce statistical power, the KO framework exhibits an improved power over false discovery ratio as compared to standard approaches in our simulations. Finally, we also show that using KO aggregation strategies allows to mitigate the instability of gene selection, a problem often disregarded when comparing feature selection procedures, but nevertheless of major importance.</p><p id="P62">Applying KO to real transcriptomic data also demonstrates the limitations of the framework. First, the KO framework tends to be very conservative, leading in several of the real examples we studied to very few or sometimes no single discovery. However, these real life examples are taken from previous research where the authors select tens or hundreds of genes to construct clinical classifiers that achieve good performance, suggesting that there is signal in the data and features to be selected. While highlighting the conservative nature of the KO procedure, this discrepancy also indicates that the genes selected in the original studies potentially contain many false discoveries.</p><p id="P63">Second, we show that while KO performs well when features are moderately correlated, its power decreases when the correlation between the causal features reaches very high values. While KO are constructed precisely to perform variable selection for correlated features, our analyses show this is effective only up to a point. Interestingly, we find that the level of correlation between the true causal features impacts the performance of the KO selection procedure more strongly than the correlation between causal features and non-causal features.</p><p id="P64">Third, we focused mostly on the case of <italic>k</italic> = 10 true causal features in our simulations. Qualitatively, we obtain similar results with <italic>k</italic> = 30 (<italic>i.e</italic>. KO still outperforms other feature selection methods, see <xref ref-type="supplementary-material" rid="SD1">Supp. Fig. S14</xref>). However, we note that the performance of the KO methods deteriorates when the number of causal features increases, see <xref ref-type="supplementary-material" rid="SD1">Supp. Fig. S13</xref>. Similarly, how the KO framework behaves for larger values of the number of features <italic>p</italic> and of samples <italic>n</italic> remains to be investigated. One expected difficulty lies in the computational cost of the generation of the KO matrix, which increases rapidly as the dataset size increases [<xref ref-type="bibr" rid="R17">17</xref>].</p><p id="P65">Finally we find that variable selection performance collapses when the link between gene expression and outcome is not linear. It is however very likely that for many biological processes, the dependency between features and the outcome is not linear. This difficulty is not specific to KO, as all of the classical methods we compare have very low performances in our simulations (see <xref ref-type="supplementary-material" rid="SD1">Supp. Fig.S12</xref>). Our results thus highlight the need for novel methodological developments to tackle feature selection for correlated data in nonlinear classification settings.</p><p id="P66">Another interesting result of our study is to underline the instability of classical gene selection procedures, as previously observed [<xref ref-type="bibr" rid="R4">4</xref>]. While standard KO do not improve selection stability—notably because of the inherent stochasticity of the KO framework—we find that using KO aggregation methods such as KOPI [<xref ref-type="bibr" rid="R12">12</xref>] greatly improves the stability of the selection procedure (<xref ref-type="fig" rid="F3">Fig. 3</xref>), while maintaining similar statistical power. We note however that KOPI calibration is not ideal and the FDR control appears very conservative (see Tab. 2), such that very few discoveries are obtained with a target FDR rate of 0.2. In our simulations, we obtain a low number of false discoveries (&lt; 5%) at the cost of limited power (41%) for a target rate of 0.2. Yet, these results demonstrate a better power-to-FDP ratio than other approaches.</p><p id="P67">Overall, our study further demonstrates the difficulty of classification tasks with transcriptomic data. We nevertheless show that KO can constitute an interesting strategy, in particular to improve gene selection stability. Our study shows how KO can be applied to transcriptomic dataset, leading the way to more sophisticated approaches. One approach could be to group genes that belong to similar biological pathway, a strategy that was already implemented successfully many times [<xref ref-type="bibr" rid="R33">33</xref>], but never in combination with KO.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary_material</label><media xlink:href="EMS207198-supplement-Supplementary_material.pdf" mimetype="application" mime-subtype="pdf" id="d32aAcEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S22"><title>Acknowledgments</title><p>This work was carried out with the financial support of Carnot M.I.N.E.S, as part of its leading project CARINGS project, dedicated to health engineering. This work was also supported by the French Agence Nationale de la Recherche (ANR-19-P3IA-0001). The authors would like to thank the Cancer Research UK, Royal Papworth Hospital and the co-authors of De Biase et al [<xref ref-type="bibr" rid="R26">26</xref>] for access to the CRUKPAP cohort data (EGA project number: EGAD50000000333).</p></ack><sec id="S23" sec-type="data-availability"><title>Data Availability</title><p id="P68">The code for all experiments is available in the following Github repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/Julie-cartier/RNAKnockoffs">https://github.com/Julie-cartier/RNAKnockoffs</ext-link>). The breast cancer gene expression data are available from the NCBI Gene Expression Omnibus under accession GSE96058. The AEGIS gene expression data are also available in the NCBI Gene Expression Omnibus under accession number GSE80796.</p></sec><fn-group><fn fn-type="con" id="FN1"><p id="P69"><bold>Author contributions statement</bold></p><p id="P70">A.F., C.A.A and F.M. conceived the experiment(s), J.C. and J.L. conducted the experiment(s), and J.C., A.F., C.A.A and F.M. analysed the results. J.C. wrote the manuscript, A.F., C.A.A and F.M. reviewed the manuscript.</p></fn><fn fn-type="conflict" id="FN2"><p id="P71"><bold>Competing interests</bold></p><p id="P72">No competing interest is declared.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uhlen</surname><given-names>M</given-names></name><name><surname>Zhang</surname><given-names>C</given-names></name><name><surname>Lee</surname><given-names>S</given-names></name><etal/></person-group><article-title>A pathology atlas of the human cancer transcriptome</article-title><source>Science</source><year>2017</year><volume>357</volume><issue>6352</issue><pub-id pub-id-type="pmid">28818916</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Govaere</surname><given-names>O</given-names></name><name><surname>Cockell</surname><given-names>S</given-names></name><name><surname>Tiniakos</surname><given-names>D</given-names></name><etal/></person-group><article-title>Transcriptomic profiling across the nonalcoholic fatty liver disease spectrum reveals gene signatures for steatohepatitis and fibrosis</article-title><source>Science Translational Medicine</source><year>2020</year><volume>12</volume><issue>572</issue><pub-id pub-id-type="pmid">33268509</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacob</surname><given-names>L</given-names></name><name><surname>Speed</surname><given-names>T</given-names></name></person-group><article-title>The healthy ageing gene expression signature for Alzheimer’s disease diagnosis: a random sampling perspective</article-title><source>Genome Biology</source><year>2018</year><volume>19</volume><issue>97</issue><pub-id pub-id-type="doi">10.1186/s13059-018-1481-6</pub-id><pub-id pub-id-type="pmcid">PMC6060554</pub-id><pub-id pub-id-type="pmid">30045771</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haury</surname><given-names>AC</given-names></name><name><surname>Gestraud</surname><given-names>P</given-names></name><name><surname>Vert</surname><given-names>JP</given-names></name></person-group><article-title>The influence of feature selection methods on accuracy, stability and interpretability of molecular signatures</article-title><source>PLoS One</source><year>2011</year><volume>6</volume><issue>12</issue><pub-id pub-id-type="doi">10.1371/journal.pone.0028210</pub-id><pub-id pub-id-type="pmcid">PMC3244389</pub-id><pub-id pub-id-type="pmid">22205940</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ein-Dor</surname><given-names>L</given-names></name><name><surname>Kela</surname><given-names>I</given-names></name><name><surname>Getz</surname><given-names>G</given-names></name><name><surname>Givol</surname><given-names>D</given-names></name><etal/></person-group><article-title>Outcome signature genes in breast cancer: is there a unique set?</article-title><source>Bioinformatics</source><year>2004</year><volume>21</volume><issue>2</issue><fpage>171</fpage><lpage>178</lpage><pub-id pub-id-type="pmid">15308542</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnstone</surname><given-names>IM</given-names></name><name><surname>Titterington</surname><given-names>DM</given-names></name></person-group><article-title>Statistical challenges of high-dimensional data</article-title><source>Philosophical Transactions of the Royal Society A</source><year>2009</year><volume>367</volume><issue>1906</issue><fpage>4237</fpage><lpage>4253</lpage><pub-id pub-id-type="doi">10.1098/rsta.2009.0159</pub-id><pub-id pub-id-type="pmcid">PMC2865881</pub-id><pub-id pub-id-type="pmid">19805443</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clarke</surname><given-names>R</given-names></name><name><surname>Ressom</surname><given-names>H</given-names></name><name><surname>Wang</surname><given-names>A</given-names></name><etal/></person-group><article-title>The properties of high-dimensional data spaces: implications for exploring gene and protein expression data</article-title><source>Nature Reviews Cancer</source><year>2008</year><volume>8</volume><fpage>37</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1038/nrc2294</pub-id><pub-id pub-id-type="pmcid">PMC2238676</pub-id><pub-id pub-id-type="pmid">18097463</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barber</surname><given-names>RF</given-names></name><name><surname>Candès</surname><given-names>E</given-names></name></person-group><article-title>Controlling the false discovery rate via knockoffs</article-title><source>The Annals of Statistics</source><year>2015</year><volume>43</volume><issue>5</issue><fpage>2055</fpage><lpage>2085</lpage></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sesia</surname><given-names>M</given-names></name><name><surname>Sabatti</surname><given-names>C</given-names></name><name><surname>Candès</surname><given-names>E</given-names></name></person-group><article-title>Gene hunting with knockoffs for hidden markov models</article-title><source>Biometrika</source><year>2019</year><volume>106</volume><issue>1</issue><fpage>1</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1093/biomet/asy033</pub-id><pub-id pub-id-type="pmcid">PMC6373422</pub-id><pub-id pub-id-type="pmid">30799875</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sesia</surname><given-names>M</given-names></name><name><surname>Bates</surname><given-names>S</given-names></name><name><surname>Candès</surname><given-names>E</given-names></name><etal/></person-group><article-title>False discovery rate control in genome-wide association studies with population structure</article-title><source>Proceedings of the National Academy of Sciences</source><year>2021</year><volume>118</volume><issue>40</issue><pub-id pub-id-type="doi">10.1073/pnas.2105841118</pub-id><pub-id pub-id-type="pmcid">PMC8501795</pub-id><pub-id pub-id-type="pmid">34580220</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>T</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Motsinger-Reif</surname><given-names>AA</given-names></name></person-group><article-title>Knockoff boosted tree for model-free variable selection</article-title><source>Bioinformatics</source><year>2021</year><volume>37</volume><issue>7</issue><fpage>976</fpage><lpage>983</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btaa770</pub-id><pub-id pub-id-type="pmcid">PMC8128453</pub-id><pub-id pub-id-type="pmid">32966559</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Blain</surname><given-names>A</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Grisel</surname><given-names>O</given-names></name><etal/></person-group><source>False discovery proportion control for aggregated knockoffs</source><conf-name>Proceedings of the 37th International Conference on Neural Information Processing Systems</conf-name><year>2023</year><volume>3417</volume><issue>12</issue></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>A</given-names></name><name><surname>Fu</surname><given-names>H</given-names></name><name><surname>He</surname><given-names>K</given-names></name><etal/></person-group><article-title>False discovery rate control in cancer biomarker selection using knockoffs</article-title><source>Cancers</source><year>2019</year><volume>11</volume><issue>6</issue><fpage>744</fpage><pub-id pub-id-type="doi">10.3390/cancers11060744</pub-id><pub-id pub-id-type="pmcid">PMC6628039</pub-id><pub-id pub-id-type="pmid">31146393</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Nguyen</surname><given-names>T-B</given-names></name><name><surname>Chevalier</surname><given-names>J-A</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><etal/></person-group><source>Aggregation of multiple knockoffs</source><conf-name>Proceedings of the 37th International Conference on Machine Learning</conf-name><year>2020</year><volume>119</volume><fpage>7283</fpage><lpage>7293</lpage></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ren</surname><given-names>Z</given-names></name><name><surname>Wei</surname><given-names>Y</given-names></name><name><surname>Candès</surname><given-names>E</given-names></name></person-group><article-title>Derandomizing knock-offs</article-title><source>Journal of the American Statistical Association</source><year>2021</year><volume>118</volume><issue>542</issue><fpage>948</fpage><lpage>958</lpage></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ren</surname><given-names>Z</given-names></name><name><surname>Barber</surname><given-names>RF</given-names></name></person-group><article-title>Derandomised knockoffs: lever-aging e-values for false discovery rate control</article-title><source>Journal of the Royal Statistical Society Series B: Statistical Methodology</source><year>2023</year><volume>86</volume><issue>1</issue><fpage>122</fpage><lpage>154</lpage></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Candès</surname><given-names>E</given-names></name><name><surname>Fan</surname><given-names>Y</given-names></name><name><surname>Janson</surname><given-names>L</given-names></name><name><surname>Lv</surname><given-names>J</given-names></name></person-group><article-title>Panning for gold: ‘model-x’ knockoffs for high dimensional controlled variable selection</article-title><source>Journal of the Royal Statistical Society Series B: Statistical Methodology</source><year>2018</year><volume>80</volume><issue>3</issue><fpage>551</fpage><lpage>577</lpage></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Rigollet</surname><given-names>P</given-names></name></person-group><article-title>Power analysis of knockoff filters for correlated designs</article-title><source>Advances in Neural Information Processing Systems</source><year>2019</year><volume>32</volume></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spector</surname><given-names>A</given-names></name><name><surname>Janson</surname><given-names>L</given-names></name></person-group><article-title>Powerful knockoffs via minimizing reconstructability</article-title><source>The Annals of Statistics</source><year>2022</year><volume>50</volume><issue>1</issue><fpage>252</fpage><lpage>276</lpage></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ke</surname><given-names>ZT</given-names></name><name><surname>Liu</surname><given-names>JS</given-names></name><name><surname>Ma</surname><given-names>Y</given-names></name></person-group><article-title>Power of knockoff: The impact of ranking algorithm, augmented design, and symmetric statistic</article-title><source>Journal of Machine Learning Research</source><year>2024</year><volume>25</volume><issue>3</issue><fpage>1</fpage><lpage>67</lpage></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blain</surname><given-names>A</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Linhart</surname><given-names>J</given-names></name><etal/></person-group><article-title>When knockoffs fail: diagnosing and fixing non-exchangeability of knockoffs</article-title><source>arXiv, 2407.06892</source><year>2024</year></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Jordon</surname><given-names>J</given-names></name><name><surname>Yoon</surname><given-names>J</given-names></name><name><surname>van der Schaar</surname><given-names>M</given-names></name></person-group><source>KnockoffGAN: Generating knockoffs for feature selection using generative adversarial networks</source><conf-name>International Conference on Learning Representations</conf-name><year>2019</year></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Romano</surname><given-names>Y</given-names></name><name><surname>Sesia</surname><given-names>M</given-names></name><name><surname>Candès</surname><given-names>EJ</given-names></name></person-group><article-title>Deep knock-offs</article-title><source>Journal of the American Statistical Association</source><year>2020</year><volume>115</volume><issue>532</issue><fpage>1861</fpage><lpage>1872</lpage></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>Y</given-names></name><name><surname>Fan</surname><given-names>Y</given-names></name><name><surname>Lv</surname><given-names>J</given-names></name><etal/></person-group><article-title>DeepPINK: reproducible feature selection in deep neural networks</article-title><source>Advances in Neural Information Processing Systems</source><year>2018</year><volume>31</volume></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spector</surname><given-names>A</given-names></name><name><surname>Fithian</surname><given-names>W</given-names></name></person-group><article-title>Asymptotically optimal knock-off statistics via the masked likelihood ratio</article-title><source>arXiv, 2212.08766</source><year>2022</year></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Biase</surname><given-names>MS</given-names></name><name><surname>Massip</surname><given-names>F</given-names></name><name><surname>Wei</surname><given-names>TT</given-names></name><etal/></person-group><article-title>Smoking-associated gene expression alterations in nasal epithelium reveal immune impairment linked to lung cancer risk</article-title><source>Genome Medicine</source><year>2024</year><volume>16</volume><issue>1</issue><fpage>54</fpage><pub-id pub-id-type="doi">10.1186/s13073-024-01317-4</pub-id><pub-id pub-id-type="pmcid">PMC11000304</pub-id><pub-id pub-id-type="pmid">38589970</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perez-Rogers</surname><given-names>J</given-names></name><name><surname>Gerrein</surname><given-names>J</given-names></name><name><surname>Anderlind</surname><given-names>C</given-names></name><etal/></person-group><article-title>Shared gene expression alterations in nasal and bronchial epithelium for lung cancer detection</article-title><source>Journal of the National Cancer</source><year>2017</year><volume>109</volume><issue>7</issue><pub-id pub-id-type="doi">10.1093/jnci/djw327</pub-id><pub-id pub-id-type="pmcid">PMC6059169</pub-id><pub-id pub-id-type="pmid">28376173</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brueffer</surname><given-names>C</given-names></name><name><surname>Vallon-Christersson</surname><given-names>J</given-names></name><name><surname>Grabaut</surname><given-names>D</given-names></name><etal/></person-group><article-title>Clinical value of rna sequencing–based classifiers for prediction of the five conventional breast cancer biomarkers: a report from the population-based multicenter sweden cancerome analysis network—breast initiative</article-title><source>JCO Precision Oncology</source><year>2018</year><volume>2</volume><fpage>1</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1200/PO.17.00135</pub-id><pub-id pub-id-type="pmcid">PMC7446376</pub-id><pub-id pub-id-type="pmid">32913985</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friedman</surname><given-names>J</given-names></name><name><surname>Hastie</surname><given-names>T</given-names></name><name><surname>Tibshirani</surname><given-names>R</given-names></name></person-group><article-title>Regularization paths for generalized linear models via coordinate descent</article-title><source>Journal of Statistical Software</source><year>2010</year><volume>33</volume><issue>1</issue><fpage>1</fpage><lpage>22</lpage><pub-id pub-id-type="pmcid">PMC2929880</pub-id><pub-id pub-id-type="pmid">20808728</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Ge</surname><given-names>X</given-names></name><name><surname>Peng</surname><given-names>F</given-names></name><etal/></person-group><article-title>Exaggerated false positives by popular differential expression methods when analyzing human population samples</article-title><source>Genome Biology</source><year>2022</year><volume>23</volume><issue>79</issue><pub-id pub-id-type="doi">10.1186/s13059-022-02648-4</pub-id><pub-id pub-id-type="pmcid">PMC8922736</pub-id><pub-id pub-id-type="pmid">35292087</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname><given-names>Y</given-names></name><name><surname>Hochberg</surname><given-names>Y</given-names></name></person-group><article-title>Controlling the false discovery rate: A practical and powerful approach to multiple testing</article-title><source>Journal of the Royal Statistical Society Series B (Methodological)</source><year>1995</year><volume>57</volume><issue>1</issue><fpage>289</fpage><lpage>300</lpage></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hejblum</surname><given-names>PB</given-names></name><name><surname>Ba</surname><given-names>K</given-names></name><etal/></person-group><article-title>Neglecting the impact of normalization in semi-synthetic RNA-seq data simulations generates artificial false positives</article-title><source>Genome Biology</source><year>2024</year><month>October</month><volume>25</volume><issue>1</issue><fpage>281</fpage><pub-id pub-id-type="doi">10.1186/s13059-024-03231-9</pub-id><pub-id pub-id-type="pmcid">PMC11523660</pub-id><pub-id pub-id-type="pmid">39478633</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paull</surname><given-names>OE</given-names></name><name><surname>Aytes</surname><given-names>A</given-names></name><etal/></person-group><article-title>A modular master regulator landscape controls cancer transcriptional identity</article-title><source>Cell</source><year>2021</year><volume>184</volume><issue>2</issue><fpage>334</fpage><lpage>351</lpage><elocation-id>e20</elocation-id><pub-id pub-id-type="doi">10.1016/j.cell.2020.11.045</pub-id><pub-id pub-id-type="pmcid">PMC8103356</pub-id><pub-id pub-id-type="pmid">33434495</pub-id></element-citation></ref></ref-list></back><floats-group><boxed-text id="BX1" position="float" orientation="portrait"><caption><title>Keypoints</title></caption><list list-type="bullet" id="L1"><list-item><p>Knockoffs-based methods demonstrate a better power-to-false discovery proportion ratio than other commonly used variable selection methods in the context of high-dimensional classification from transcriptomic data.</p></list-item><list-item><p>The stability of the selection is improved by using an aggregation scheme that mitigates the effects of knockoffs stochasticity while maintaining the same power.</p></list-item><list-item><p>Both real-world applications and simulations highlight several limitations of the KO framework: its conservative nature, its sensitivity to the correlation structure and the number of true causal features, and its poor performance in nonlinear classification settings.</p></list-item></list></boxed-text><fig id="F1" position="float"><label>Figure 1</label><caption><title>Comparison of the KO generation methods (a) and of different KO statistics (b).</title><p>Outcomes are simulated from the CRUK-PAP feature matrix using the linear simulation setting with <italic>k</italic> = 10 and <italic>M</italic> = 100 repetitions. Power values are given as means over FDP bins of width 0.05. (a) Important variables are selected using the LCD statistics for all KO generation methods. CI: Conditional independence, LSCIP: Linear Sequential Conditional Independant Pairs algorithm, MVR: Minimum Variance-based reconstruction, SDP: semi-definite programming, see section 2.1 for details. (b) KO features are generated using the LSCIP method for all statistics. EN-CD: Elastic Net Coefficients Difference, LCD: Lasso Coefficients Difference, MLR: Maximum Likelihood Ratio, RF: Random Forest, see section 2.2 for details.</p></caption><graphic xlink:href="EMS207198-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Comparison of different KO statistics in the interaction simulation setting.</title><p>Outcomes are simulated from the CRUKPAP feature matrix with <italic>k</italic> = 10 and <italic>M</italic> = 10 repetitions. Power values are given as means over FDP bins of width 0.05. EN-CD: Elastic Net Coefficients Difference, LCD: Lasso Coefficients Difference, MLR: Maximum Likelihood Ratio, RF: Random Forest, see section 2.2 for details.</p></caption><graphic xlink:href="EMS207198-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Histograms of the frequency of features selected <italic>more than once</italic> in different variable selection frameworks.</title><p>Selection frequencies are computed over 10 iterations of ten-fold subsampling for each method and simulated outcome, in the linear simulation setting with <italic>k =</italic> 10 and the CRUKPAP features matrix. <italic>n</italic> gives the number of features among the 749 × 10 selected at least once across the 100 × 10 iterations. Features that were never selected across any iterations are not displayed. (a-b): Selection performed with LASSO-penalized logistic regressions, (a) for <italic>λ = λ</italic><sub><italic>min</italic></sub> or (b) <italic>λ = λ</italic><sub><italic>oracle</italic></sub> (c-d): Selection performed with knockoffs for FDR level <italic>q</italic> = 0.2 (c) and q = 0.5 (d). (e-f): Selection performed with KOPI with target FDR level <italic>q</italic> = 0.2 (e) and <italic>q</italic> = 0.5 (f).</p></caption><graphic xlink:href="EMS207198-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Illustration of the performance variation of the KO procedure (LSCIP + LCD)</title><p>The power is obtained for the 10 most difficult (continuous line) or the 10 easiest case (dotted line) out of <italic>M</italic> = 100 different outcomes simulations using the linear simulation setting with <italic>k</italic> = 10 and from the CRUKPAP (red) or the breast cancer (blue) feature matrix</p></caption><graphic xlink:href="EMS207198-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>KO variable selection power depending on the pairwise correlation between the 10 real important features.</title><p>Simulations are conducted with the linear simulation setting on the CRUKPAP feature matrix. The mean power is computed for two different FDP intervals: [0, 0.3] (purple dots) and [0, 0.5] (yellow dots). Each dot corresponds to a different simulation experiments. Continuous and dotted lines represent the linear regression fit of the mean power versus pairwise correlation for the two different FDP intervals.</p></caption><graphic xlink:href="EMS207198-f005"/></fig><table-wrap id="T1" orientation="portrait" position="float"><label>Table 1</label><caption><title>Comparison of the power and FDP obtained with different variable selection methods: the KO procedure and KOPI with <italic>q</italic> = 0.2 and <italic>q</italic> = 0.5, the LASSO with penalization parameters <italic>λ</italic><sub>min</sub> and <italic>λ</italic><sub>oracle</sub>, and the Wilcoxon rank-sum test with <italic>α</italic> = 10<sup>−6</sup> and <italic>α</italic> = 0.05.</title><p>The mean and the standard deviation (sd) are computed for <italic>M</italic> =100 linearly simulated <bold>y</bold> on the CRUKPAP features matrix.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="middle" align="left">Method</th><th valign="middle" align="left">Parameter</th><th valign="middle" align="left">FDP (mean)</th><th valign="middle" align="left">Power (mean)</th></tr></thead><tbody><tr><td valign="middle" align="left" rowspan="2">KO</td><td valign="middle" align="left"><italic>q</italic> = 0.2</td><td valign="middle" align="left">0.13 <italic>(sd: 0.15)</italic></td><td valign="middle" align="left">0.47 <italic>(sd: 0.31)</italic></td></tr><tr><td valign="middle" align="left"><italic>q</italic> = 0.5</td><td valign="middle" align="left">0.48 <italic>(sd: 0.27)</italic></td><td valign="middle" align="left">0.65 <italic>(sd: 0.14)</italic></td></tr><tr><td valign="middle" align="left" rowspan="2">KOPI</td><td valign="middle" align="left"><italic>q</italic> = 0.2</td><td valign="middle" align="left">0.03 <italic>(sd: 0.07)</italic></td><td valign="middle" align="left">0.41 <italic>(sd: 0.29)</italic></td></tr><tr><td valign="middle" align="left"><italic>q</italic> = 0.5</td><td valign="middle" align="left">0.35 <italic>(sd: 0.22)</italic></td><td valign="middle" align="left">0.66 <italic>(sd: 0.17)</italic></td></tr><tr><td valign="middle" align="left" rowspan="2">Wilcoxon</td><td valign="middle" align="left"><italic>α</italic> = 10<sup>−6</sup></td><td valign="middle" align="left">0.91 <italic>(sd: 0.21)</italic></td><td valign="middle" align="left">0.60 <italic>(sd: 0.23)</italic></td></tr><tr><td valign="middle" align="left"><italic>α</italic> = 0.05</td><td valign="middle" align="left">0.98 <italic>(sd: 0.01)</italic></td><td valign="middle" align="left">0.83 <italic>(sd: 0.16)</italic></td></tr><tr><td valign="middle" align="left" rowspan="2">LASSO</td><td valign="middle" align="left">λ<sub>oracle</sub></td><td valign="middle" align="left">0.37 <italic>(sd: 0.31)</italic></td><td valign="middle" align="left">0.28 <italic>(sd: 0.23)</italic></td></tr><tr><td valign="middle" align="left">λ<sub>min</sub></td><td valign="middle" align="left">0.88 <italic>(sd: 0.02)</italic></td><td valign="middle" align="left">0.76 <italic>(sd: 0.13)</italic></td></tr></tbody></table></table-wrap><table-wrap id="T2" orientation="portrait" position="float"><label>Table 2</label><caption><title>Number of true discoveries (TD) and false discoveries (FD) among systematically selected features (selection frequency = 1) and features selected at least once (freq &gt; 0) for different variable selection methods.</title><p>The selection frequency is computed over 10 iterations of ten-fold subsampling for each method and simulated outcome <bold>y</bold>. The outcomes <bold>y</bold> are simulated in the linear simulation setting with <italic>k</italic> = 10 and the CRUKPAP feature matrix.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="middle" align="left">Method</th><th valign="middle" align="left">Parameter</th><th valign="middle" align="center">TD (FD)<break/>(freq = 1)</th><th valign="middle" align="center">TD (FD)<break/>(freq &gt; 0)</th></tr></thead><tbody><tr><td valign="middle" align="left" rowspan="2">KO</td><td valign="middle" align="left"><italic>q</italic> = 0.2</td><td valign="middle" align="center">0 (0)</td><td valign="middle" align="center">74 (261)</td></tr><tr><td valign="middle" align="left"><italic>q</italic> = 0.5</td><td valign="middle" align="center">15 (0)</td><td valign="middle" align="center">75 (1177)</td></tr><tr><td valign="middle" align="left" rowspan="2">KOPI</td><td valign="middle" align="left"><italic>q</italic> = 0.2</td><td valign="middle" align="center">11 (0)</td><td valign="middle" align="center">64 (49)</td></tr><tr><td valign="middle" align="left"><italic>q</italic> = 0.5</td><td valign="middle" align="center">46 (4)</td><td valign="middle" align="center">73 (447)</td></tr><tr><td valign="middle" align="left" rowspan="2">LASSO</td><td valign="middle" align="left">λ<sub>oracle</sub></td><td valign="middle" align="center">11 (2)</td><td valign="middle" align="center">52 (329)</td></tr><tr><td valign="middle" align="left">λ<sub>min</sub></td><td valign="middle" align="center">60 (48)</td><td valign="middle" align="center">80 (2070)</td></tr></tbody></table></table-wrap><table-wrap id="T3" orientation="portrait" position="float"><label>Table 3</label><caption><title>Classification of the performance of variable selection using the KO framework for different transcriptomic data sets.</title><p>The outcomes <bold>y</bold> are simulated 100 times with, at each iteration, 10 randomly selected non null genes from a real feature matrix <italic>X</italic> (<italic>X</italic> ∈ {<italic>X</italic><sub>CRUKPAP</sub>, <italic>X</italic><sub>AEGIS</sub>, <italic>X</italic><sub>BC</sub>} see <xref ref-type="supplementary-material" rid="SD1">Supp. Mat. D</xref> for details). The power is given as the average power obtained over an FDP interval: <bold>FDP</bold> ∈ [0, 0.3].</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="bottom" align="left">Data sets</th><th valign="middle" align="left">Power</th><th valign="middle" align="center">≥ 0.7<break/>=easy”</th><th valign="middle" align="center">∈]0.25, 0.7[<break/>=medium”</th><th valign="middle" align="center">≤ 0.25<break/>=hard”</th></tr></thead><tbody><tr><td valign="middle" align="left" colspan="2">CRUKPAP</td><td valign="middle" align="center">0</td><td valign="middle" align="center">68</td><td valign="middle" align="center">32</td></tr><tr><td valign="middle" align="left" colspan="2">AEGIS</td><td valign="middle" align="center">7</td><td valign="middle" align="center">67</td><td valign="middle" align="center">26</td></tr><tr><td valign="middle" align="left" colspan="2">Breast cancer</td><td valign="middle" align="center">54</td><td valign="middle" align="center">43</td><td valign="middle" align="center">3</td></tr></tbody></table></table-wrap></floats-group></article>