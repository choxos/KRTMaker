<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS207568</article-id><article-id pub-id-type="doi">10.1101/2025.07.22.666089</article-id><article-id pub-id-type="archive">PPR1055090</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Assembly-based computations through contextual dendritic gating of plasticity</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Onasch</surname><given-names>Sebastian</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Miehl</surname><given-names>Christoph</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Miçkus</surname><given-names>M. Maurycy</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Gjorgjieva</surname><given-names>Julijana</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib></contrib-group><aff id="A1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02kkvpp62</institution-id><institution>Technical University of Munich</institution></institution-wrap>, School of Life Sciences, <city>Freising</city>, <country country="DE">Germany</country></aff><aff id="A2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02h1nk258</institution-id><institution>Max Planck Institute for Brain Research</institution></institution-wrap>, <city>Frankfurt am Main</city>, <country country="DE">Germany</country></aff><aff id="A3"><label>3</label>Department of Neurobiology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/024mw5h28</institution-id><institution>University of Chicago</institution></institution-wrap>, <city>Chicago</city>, <state>IL</state>, <country country="US">USA</country></aff><aff id="A4"><label>4</label>Grossman Center for Quantitative Biology and Human Behavior, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/024mw5h28</institution-id><institution>University of Chicago</institution></institution-wrap>, <city>Chicago</city>, <state>IL</state>, <country country="US">USA</country></aff><author-notes><corresp id="CR1">Correspondence: <email>sebastian.onasch@tum.de</email>, <email>cmiehl@uchicago.edu</email>, <email>gjorgjieva@tum.de</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>27</day><month>07</month><year>2025</year></pub-date><pub-date pub-type="preprint"><day>24</day><month>07</month><year>2025</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Neuronal assemblies – groups of strongly connected neurons – are considered the basic building blocks of perception and memory in the brain by encoding representations of specific concepts. Despite recent evidence for the biological basis behind the existence and formation of such assemblies, computational models often fall short of showing how assemblies can be flexibly learned and combined to perform real-world computations. A prominent problem is ‘catastrophic forgetting’, where learning a new assembly can disrupt existing connectivity structure and lead to forgetting previously learned assemblies. We propose a biologically plausible computational model, where dendritic compartments (instead of neurons) are the loci for learning and inhibition gates learning in a dendrite-specific manner, to flexibly learn new stimuli without forgetting of old ones. By learning stable projections from one brain region into another and associations between different brain regions, we demonstrate how the proposed assembly framework implements the basic building blocks for diverse computations. In a visual-auditory association task, we demonstrate how the context-specific assembly computations can be used to correctly separate ambiguous stimuli based on their dendritic representations. Our models provide unique insights and predictions for how hierarchically connected brain areas use their biological components to implement flexible yet robust learning.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">The brain can represent, flexibly learn, and stably store information. A long-standing hypothesis is that groups of neurons with highly correlated activity, known as neuronal ensembles, provide the basis for representations in the brain (<xref ref-type="bibr" rid="R10">Buzsáki, 2010</xref>; <xref ref-type="bibr" rid="R113">Yuste, 2015</xref>; <xref ref-type="bibr" rid="R21">Eichenbaum, 2018</xref>). Groups of strongly connected neurons called neuronal assemblies are hypothesized to be the structural basis of these correlated ensembles (<xref ref-type="bibr" rid="R65">Miehl et al., 2023</xref>). A core assumption is that each assembly represents a specific concept or feature, acting as a fundamental unit for storing memories (<xref ref-type="bibr" rid="R69">Neves et al., 2008</xref>). Even though many experimental studies have supported the idea of ensembles as the basic memory unit (<xref ref-type="bibr" rid="R113">Yuste, 2015</xref>; <xref ref-type="bibr" rid="R46">Josselyn and Tonegawa, 2020</xref>), past theoretical studies were either based on over-simplified network structures (<xref ref-type="bibr" rid="R40">Hopfield, 1982</xref>) or have mainly focused on how assembly structures can be learned via synaptic plasticity mechanisms (<xref ref-type="bibr" rid="R17">Clopath et al., 2010</xref>; <xref ref-type="bibr" rid="R58">Litwin-Kumar and Doiron, 2014</xref>; <xref ref-type="bibr" rid="R115">Zenke et al., 2015</xref>; <xref ref-type="bibr" rid="R108">Wu et al., 2020</xref>; <xref ref-type="bibr" rid="R67">Montangie et al., 2020</xref>). Only a few studies have shown that assemblies can be flexibly learned and combined to perform interesting computations (<xref ref-type="bibr" rid="R98">Tetzlaff et al., 2015</xref>; <xref ref-type="bibr" rid="R72">Papadimitriou et al., 2020</xref>; <xref ref-type="bibr" rid="R105">Weidel et al., 2021</xref>). Despite this interest and prior work on assemblies, several challenges remain: 1) overlaps in assemblies in terms of participating neurons often lead to the merging of assembly structures during ongoing plasticity, and 2) imprinting a new assembly into a network with an already existing connectivity structure can lead to ‘forgetting’ of stored representations.</p><p id="P3">At the same time, artificial neural networks (ANNs) have proved very successful and often exceed humans in solving specific tasks (<xref ref-type="bibr" rid="R94">Silver et al., 2017</xref>). However, ANNs mainly rely on assumptions that are not always biologically plausible, and learning new or additional tasks in ANNs leads to decreased performance and catastrophic forgetting in the worst case (<xref ref-type="bibr" rid="R74">Parisi et al., 2019</xref>). While multiple solutions have been suggested to counteract forgetting (<xref ref-type="bibr" rid="R50">Kirkpatrick et al., 2017</xref>; <xref ref-type="bibr" rid="R61">Masse et al., 2018</xref>; <xref ref-type="bibr" rid="R43">Jedlicka et al., 2022</xref>; <xref ref-type="bibr" rid="R63">Mei et al., 2022</xref>; <xref ref-type="bibr" rid="R115">Zenke and Laborieux, 2024</xref>), here we show how this problem can be solved in hierarchical recurrent neural networks using two specific biological components: nonlinear dendritic compartments and inhibitory context-dependent gating. We propose that dendritic compartments (instead of neurons) are the loci for learning, and inhibition gates learning in a dendrite-specific manner.</p><p id="P4">First, we introduce dendritic compartments with nonlinear integration properties to each neuron (<xref ref-type="fig" rid="F1">Fig. 1A</xref>). Recent computational studies have used dendrites for feature binding (<xref ref-type="bibr" rid="R56">Legenstein and Maass, 2011</xref>), multi-task learning (<xref ref-type="bibr" rid="R42">Iyer et al., 2022</xref>; <xref ref-type="bibr" rid="R109">Wybo et al., 2023</xref>), linking of memories (<xref ref-type="bibr" rid="R47">Kastellakis et al., 2016</xref>, 2023), solving the stability-plasticity problem (<xref ref-type="bibr" rid="R107">Wilmes and Clopath, 2023</xref>), or exploring the interplay of somatic and dendritic inhibition (<xref ref-type="bibr" rid="R77">Pedrosa and Clopath, 2020</xref>) (as reviewed in <xref ref-type="bibr" rid="R75">Payeur et al. (2019)</xref>; <xref ref-type="bibr" rid="R82">Poirazi and Papoutsi (2020)</xref>). Furthermore, dendritic compartments have been successfully implemented in machine learning applications to solve various tasks such as the credit assignment problem in deep neuronal networks (<xref ref-type="bibr" rid="R34">Guerguiev et al., 2017</xref>; <xref ref-type="bibr" rid="R87">Sacramento et al., 2018</xref>; <xref ref-type="bibr" rid="R75">Payeur et al., 2021</xref>; <xref ref-type="bibr" rid="R32">Greedy et al., 2022</xref>; <xref ref-type="bibr" rid="R26">Galloni et al., 2025</xref>), catastrophic forgetting (<xref ref-type="bibr" rid="R61">Masse et al., 2018</xref>; <xref ref-type="bibr" rid="R33">Grewal et al., 2021</xref>; <xref ref-type="bibr" rid="R42">Iyer et al., 2022</xref>; <xref ref-type="bibr" rid="R109">Wybo et al., 2023</xref>), parameter-efficient learning (<xref ref-type="bibr" rid="R13">Chavlis and Poirazi, 2025</xref>), have been proposed to solve classic machine learning tasks (<xref ref-type="bibr" rid="R45">Jones and Kording, 2020</xref>), and more efficient neuromorphic computing (<xref ref-type="bibr" rid="R6">Bhaduri et al., 2018</xref>) (as reviewed in <xref ref-type="bibr" rid="R1">Acharya et al. (2022)</xref>; <xref ref-type="bibr" rid="R70">Pagkalos et al. (2024)</xref>).</p><p id="P5">Second, we include inhibition as a mechanism for dendrite-specific gating. Cortical circuits contain a diversity of inhibitory cell types known to play specific roles in regulating cortical function and stability (<xref ref-type="bibr" rid="R79">Pfeffer et al., 2013</xref>). For example, parvalbumin (PV) expressing inhibitory neurons stabilize excitatory dynamics (<xref ref-type="bibr" rid="R101">Veit et al., 2017</xref>; <xref ref-type="bibr" rid="R88">Sanzeni et al., 2020</xref>; <xref ref-type="bibr" rid="R55">Lagzi et al., 2021</xref>; <xref ref-type="bibr" rid="R24">Festa et al., 2024</xref>; <xref ref-type="bibr" rid="R71">Palmigiano et al., 2023</xref>) while somatostatin (SOM) expressing inhibitory neurons modulate the circuit (<xref ref-type="bibr" rid="R2">Adesnik et al., 2012</xref>; <xref ref-type="bibr" rid="R9">Bos et al., 2025</xref>). Motivated by the top-down control of vasoactive intestinal peptide (VIP) expressing inhibitory neurons and the disinhibitory VIP-SOM circuit (<xref ref-type="bibr" rid="R80">Pi et al., 2013</xref>; <xref ref-type="bibr" rid="R12">Canto-Bustos et al., 2022</xref>; <xref ref-type="bibr" rid="R104">Waitzmann et al., 2024</xref>), we incorporate inhibition as a signal to gate dendrites ‘on’ or ‘off’. We interpret (dis-) inhibition as a ‘context’ signal, supporting dendrite-specific control of synaptic plasticity (<xref ref-type="fig" rid="F1">Fig. 1A</xref>). Context-dependent gating of activity has been utilized in ANNs for multitask learning (<xref ref-type="bibr" rid="R109">Wybo et al., 2023</xref>), extending the dynamical regime of ANNs (<xref ref-type="bibr" rid="R52">Krishnamurthy et al., 2022</xref>), and supporting high-capacity memory networks (<xref ref-type="bibr" rid="R81">Podlaski et al., 2025</xref>).</p><p id="P6">We show that adding two biological constraints, dendritic nonlinear integration and dendritic gating via inhibition, to hierarchically organized recurrent network models enables the learning of stable overlapping assemblies without forgetting previously learned features (<xref ref-type="fig" rid="F1">Fig. 1B</xref>). We demonstrate the potential of this framework to perform assembly computations based on projections and associations across multiple areas while amplifying information through recurrent connectivity and separating ambiguous stimuli (<xref ref-type="fig" rid="F1">Fig. 1C</xref>). Therefore, by providing a biologically grounded solution to the problem of catastrophic forgetting, we open the door to the mechanistic implementations of other forms of associative memory formation, and provide a basis for flexible yet robust learning in ANNs.</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>Inhibitory context gates dendrite-specific excitatory long-term plasticity</title><p id="P7">To investigate the learning of stable but flexible assemblies that encode distinct concepts in recurrent neural networks with spiking neurons, we identified two biologically motivated components as key: dendritic nonlinearities and context-specific inhibitory gating. We modeled each excitatory neuron as a multi-compartment unit, with a somatic leaky-integrate-and-fire compartment connected to multiple dendritic branches (<xref ref-type="fig" rid="F2">Fig. 2A</xref>). The neurons receive excitatory inputs to their dendrites via excitatory synapses with two types of ion channels, AMPA and NMDA, which integrate inputs linearly and nonlinearly, respectively (<xref ref-type="sec" rid="S10">Methods</xref>). Therefore, strong excitatory inputs can induce plateau potentials (‘NMDA spikes’), which last several milliseconds and have been observed experimentally (<xref ref-type="bibr" rid="R89">Schiller et al., 2000</xref>). We also modeled inhibitory synapses so that sufficiently strong inhibition can prevent these NMDA spikes, in line with experimental studies showing that inhibition can operate on single dendritic branches (<xref ref-type="bibr" rid="R30">Gidon and Segev, 2012</xref>; <xref ref-type="bibr" rid="R97">Stokes et al., 2014</xref>) and can control NMDA receptor activation (<xref ref-type="bibr" rid="R90">Schulz et al., 2018</xref>; <xref ref-type="bibr" rid="R19">Davenport et al., 2021</xref>).</p><p id="P8">To form stimulus-specific assemblies, we implemented long-term synaptic plasticity at excitatory synapses, which modifies excitatory synapses in an activity-dependent manner. Synaptic change follows a dendritic voltage-dependent plasticity rule where the sign of induced plasticity (long-term potentiation, LTP, vs. depression, LTD) depends on the low-pass filtered dendritic voltage traces relative to a plasticity threshold (<xref ref-type="bibr" rid="R17">Clopath et al., 2010</xref>; <xref ref-type="bibr" rid="R8">Bono and Clopath, 2017</xref>) (<xref ref-type="fig" rid="F2">Fig. 2B</xref>, <xref ref-type="sec" rid="S10">Methods</xref>). A synapse undergoes LTD if the dendritic voltage is slightly depolarized at the time of the presynaptic spike (above a threshold <italic>θ</italic><sub>-</sub>), whereas it undergoes LTP if depolarized strongly (above a second threshold <italic>θ</italic><sub>+</sub>). Under these conditions, the balance of excitatory and inhibitory inputs determines the sign and amount of synaptic plasticity at the dendrite (<xref ref-type="fig" rid="F2">Fig. 2C</xref>). For example, in the model, many active excitatory inputs can repeatedly evoke NMDA spikes when inhibition is weak, and therefore, induce LTP (<xref ref-type="fig" rid="F2">Fig. 2C, D</xref>). Decreasing the number of highly active excitatory inputs can shift the plasticity regime to LTD, and even to no plasticity if only few inputs are active (<xref ref-type="fig" rid="F2">Fig. 2D</xref>, left). Equivalently, increasing the inhibitory firing rate can also shift the plasticity regime to LTD and eventually to no plasticity (<xref ref-type="fig" rid="F2">Fig. 2D</xref>, right). An important factor enabling the dynamic switch between different regimes, LTP, LTD and no plasticity, is the nonlinearity of the NMDA synapses. With linear NMDA synapses, switches between plasticity regimes require much more drastic changes of excitatory and inhibitory input parameters (<xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S1</xref>). Each dendritic branch in our model can be considered an independent component, separate from the activity and plasticity dynamics at the other dendrites of the same neuron. Experimental studies have found such branch-specificity in the visual cortex (<xref ref-type="bibr" rid="R44">Jia et al., 2010</xref>), motor cortex (<xref ref-type="bibr" rid="R16">Cichon and Gan, 2015</xref>; <xref ref-type="bibr" rid="R49">Kerlin et al., 2019</xref>), and the hippocampus (<xref ref-type="bibr" rid="R84">Rashid et al., 2020</xref>; <xref ref-type="bibr" rid="R68">Moore et al., 2022</xref>).</p><p id="P9">Inhibitory neurons have been shown to modulate dendritic excitability (<xref ref-type="bibr" rid="R7">Bilash et al., 2023</xref>) and to gate synaptic plasticity (<xref ref-type="bibr" rid="R12">Canto-Bustos et al., 2022</xref>; <xref ref-type="bibr" rid="R80">Pi et al., 2013</xref>; <xref ref-type="bibr" rid="R2">Adesnik et al., 2012</xref>). Inspired by these experimental findings, we included inhibitory neurons that each represent a specific context (C1-C6), which can be switched ‘on’ one at a time (<xref ref-type="fig" rid="F2">Fig. 2E</xref>). We assumed that each dendritic compartment of the excitatory neurons receives inhibitory input from a distinct context. When the context is ‘off’, the inhibitory input is active, and the inputs targeting this dendrite experience no plasticity. When a given context is ‘on’, the corresponding inhibitory input is inactive, the dendrite is disinhibited and can be shifted into different plasticity regimes (LTP, LTD or no plasticity) depending on the number of highly active excitatory inputs it receives. Hence, the baseline state is one with highly active inhibitory neurons preventing ongoing plasticity. For example, when context C1 is ‘on’, there is barely any plasticity on the corresponding dendrite D1 if it receives few highly active inputs (<xref ref-type="fig" rid="F2">Fig. 2F</xref>). If context C2 is ‘on’ and the corresponding dendrite D2 receives many highly active excitatory inputs, then the connections to D2 undergo LTP. Finally, if context C3 is on, but the corresponding dendrite D3 receives an intermediate number of highly active inputs, the connections to D3 experience LTD. Importantly, even if a dendrite receives many strong excitatory inputs, it does not undergo any plasticity if the context signal remains ‘off’ (D4). Hence, while dendritic depolarization determines the direction of plasticity in our model, somatic spiking only minimally affects plasticity in line with experimental findings (<xref ref-type="bibr" rid="R27">Gambino et al., 2014</xref>; <xref ref-type="bibr" rid="R31">Golding et al., 2002</xref>; <xref ref-type="bibr" rid="R16">Cichon and Gan, 2015</xref>).</p><p id="P10">Hence, we have proposed a biologically grounded modeling framework to capture how excitatory plasticity induced by nonlinear NMDA spikes can be flexibly gated ‘on’ or ‘off’ in a dendrite-specific manner by inhibitory context signals. We demonstrated that if a dendrite is gated ‘on,’ the type of plasticity experienced by the synapses on the dendrite (no plasticity, LTD, or LTP) depends on the number of highly active excitatory inputs that arrive at the dendrite, providing a well controlled substrate for learning.</p></sec><sec id="S4"><title>Nonlinear dendrites and inhibitory context lead to the formation of dendrite-specific assemblies</title><p id="P11">We next combined the two biologically motivated components, nonlinear dendritic integration and inhibition-specific gating, with biologically inspired synaptic plasticity to stably learn assemblies that encode percepts or stimulus features in a recurrently connected network of excitatory neurons. To investigate the mutual potentiation of connections between neurons that underlie the assemblies, we connected 400 multi-compartment excitatory neurons in an all-to-all manner with initially weak but plastic connections on the dendrites (<xref ref-type="fig" rid="F3">Fig. 3A</xref>, <xref ref-type="sec" rid="S10">Methods</xref>). To stimulate the network, we drove each neuron with input from a random set of excitatory feedforward neurons on its dendrites. Thus, active feedforward inputs strongly activate a subset of neurons in the recurrent network (<xref ref-type="fig" rid="F3">Fig. 3B</xref>, top and middle). We implemented activity-dependent plasticity at the synaptic connections from feedforward and recurrent inputs at the dendrites. The type of plasticity at each synapse (LTP, LTD or no plasticity) depends on the number of highly active feedforward and recurrent excitatory inputs received at each dendrite and whether the inhibitory context for that dendrite is ‘on’ (<xref ref-type="fig" rid="F2">Fig. 2</xref>). Assuming the same context (C1) is always ‘on’, we observed the emergence of an assembly of strongly mutually connected neurons in the recurrent network, as seen in the neuron-neuron connectivity matrix (<xref ref-type="fig" rid="F3">Fig. 3C</xref>, left). Neurons with ‘on’-gated dendrites, which receive many active inputs, acquire the strongest mutual connections, as seen in the dendrite-neuron connectivity matrix (<xref ref-type="fig" rid="F3">Fig. 3C</xref>, right). With this plasticity mechanism, only the weights that are part of the assembly (‘within’) increase. In contrast, weights from other neurons to the assembly neurons (‘into’), weights from assembly neurons to other neurons (‘from’) and weights between other neurons (‘outside’) do not change or decrease, as they receive fewer excitatory inputs or too much dendritic inhibition (<xref ref-type="fig" rid="F3">Fig. 3B</xref>, bottom). In addition to the inhibitory context, the network receives feedforward and recurrent inhibition that, together with a biologically motivated dendrite-specific normalization mechanism, support the formation of refined assemblies while controlling assembly size (<xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S2</xref>, see <xref ref-type="supplementary-material" rid="SD1">Supplementary Text 1</xref> for details).</p><p id="P12">Next, we investigated how our assembly framework can be used to form multiple assemblies by training the weights in the same context (C1). To determine how many assemblies can be stored, we sequentially activated non-overlapping groups of some number of (e.g. 20) feedforward inputs and projected them one-after-another (e.g. 20 groups in total) into the downstream recurrent area (<xref ref-type="fig" rid="F3">Fig. 3D</xref>). Because of the random connectivity between the feedforward inputs and the recurrent neurons, it is unlikely that these non-overlapping feedforward inputs generate non-overlapping assemblies in the target area. Some neurons that have been part of a previously learned assembly might become recruited by a newly formed assembly, while other neurons in the recurrent area do not become part of an assembly because they never receive sufficiently strong excitatory inputs. For the choice of 20 groups with 20 input neurons each, ~ 3/4 of neurons in the recurrent downstream area became part of an assembly with later imprinted assemblies having more neurons (<xref ref-type="fig" rid="F3">Fig. 3D</xref>). The number of assemblies and the assembly size can be well described from the statistics of random connection probability, in which earlier imprinted assemblies will have smaller assembly sizes (<xref ref-type="sec" rid="S10">Methods</xref>, <xref ref-type="fig" rid="F3">Fig. 3E</xref>).</p><p id="P13">A key feature of assembly learning is that those previously learned assemblies can be reliably reactivated—or ‘recalled’ (<xref ref-type="bibr" rid="R40">Hopfield, 1982</xref>; <xref ref-type="bibr" rid="R102">Vogels et al., 2011</xref>; <xref ref-type="bibr" rid="R115">Zenke et al., 2015</xref>). Therefore, we next quantified to what extent the re-activation of assembly neurons in the recurrent network by their respective inputs after all assemblies have been imprinted reflects the original assemblies imprinted by the same inputs during training. We found that reactivating assemblies learned early in the training process have weaker firing rates (~4-6 Hz) and fewer active assembly neurons (~10-18) compared to those learned late in the training process (~8 Hz firing rates and ~20 active assembly neurons) (<xref ref-type="fig" rid="F3">Fig. 3F</xref>). The assemblies learned first are more affected by this reduction than the ones learned later, as their constituent neurons are likely to be recruited by newly learned assemblies. Nonetheless, both measures are clearly above the background activity of non-assembly neurons, suggesting that all learned assemblies can be reliably recalled. In addition, switching to a different context (C2) than the one used during training (C1) does not re-activate the assembly learned in the original context (C1) (<xref ref-type="fig" rid="F3">Fig. 3F</xref>, compare Asse. C1 and Asse. C2). Therefore, recall of assemblies in our framework depends on the context in which those assemblies are learned, a feature that we later use to perform assembly computations.</p><p id="P14">Storing memories in strongly recurrently connected neuron groups, which represent the assemblies, is known to amplify weak stimuli (<xref ref-type="bibr" rid="R78">Peron et al., 2020</xref>) and enable partial stimuli to evoke a full assembly response – this has been termed pattern completion (<xref ref-type="bibr" rid="R40">Hopfield, 1982</xref>; <xref ref-type="bibr" rid="R115">Zenke et al., 2015</xref>; <xref ref-type="bibr" rid="R102">Vogels et al., 2011</xref>; <xref ref-type="bibr" rid="R35">Guzman et al., 2016</xref>). We examined pattern completion by testing the response in the recurrent network to changing the properties of the input neurons used during training. First, we quantified pattern completion by reducing the number of active inputs compared to those used during training (<xref ref-type="fig" rid="F3">Fig. 3G</xref>). In the correct context (C1) for a given assembly, i.e., the context in which that assembly was learned, assembly firing rates and their number of active assembly neurons rapidly increase with the activation of more inputs (<xref ref-type="fig" rid="F3">Fig. 3G</xref>, middle and right). Even reducing the number of active inputs by more than half compared to those used during training reliably activates the whole assembly in the recurrent network. This amplification is a direct result of the strong recurrent within-assembly weights in the recurrent network. When presenting a different context (C2), far fewer assembly neurons compared to context C1 are active and at much lower firing rates (<xref ref-type="fig" rid="F3">Fig. 3G</xref>, gray lines). The network also exhibits pattern completion when reducing the input firing rate compared to those used during training (<xref ref-type="fig" rid="F3">Fig. 3H</xref>).</p><p id="P15">In summary, we find that a large number of assemblies can be learned in the same context and reliably recalled, but only in the context in which they were learned. Due to the strong within-assembly connectivity, it is sufficient to weakly activate the input neurons or activate only a subset of them while still ensuring reliable assembly reactivation. Hence, the dendritic-specific assemblies learned by gating inhibition in the appropriate context can perform pattern completion and recall.</p></sec><sec id="S5"><title>Learning assemblies in multi-area networks without catastrophic forgetting</title><p id="P16">A prominent hypothesis puts forward assemblies as the basic computational units in the brain (<xref ref-type="bibr" rid="R10">Buzsáki, 2010</xref>; <xref ref-type="bibr" rid="R11">Byrne and Huyck, 2010</xref>; <xref ref-type="bibr" rid="R38">Herpich and Tetzlaff, 2019</xref>). Thus, we next investigated how the formed assemblies in our recurrent networks can be combined to perform computations. Specifically, we focused on stably learning projections from one area into another as well as learning associations across areas, as two types of ‘assembly calculus’ operations with a clear biological correlate underlying many neural computations (<xref ref-type="bibr" rid="R72">Papadimitriou et al., 2020</xref>; <xref ref-type="bibr" rid="R73">Papadimitriou and Friederici, 2022</xref>). For example, projections of assemblies are suggested to generate directed sequences in hippocampus (<xref ref-type="bibr" rid="R39">Holtmaat and Caroni, 2016</xref>), and assembly associations are useful to combine information from multiple assemblies into a new one (Pokorny et al., 2020). We focused on hierarchically organized multi-area networks without reciprocal feedback connections between them because many brain circuits are organized in a hierarchical manner (<xref ref-type="bibr" rid="R37">Harris et al., 2019</xref>; <xref ref-type="bibr" rid="R93">Siegle et al., 2021</xref>) and have inspired deep neural networks in artificial intelligence research (<xref ref-type="bibr" rid="R110">Yamins et al., 2014</xref>).</p><p id="P17">In our proposed architecture, the context in which an assembly is learned plays a crucial role for the calculus operations, as it defines which dendrites are gated ‘on’. To learn a projection in a given context C1 from one area (X) to a downstream area (Y), we activated a subset of neurons in area X which randomly project to a subset of dendrites in area Y. Our activity-dependent plasticity rule leads to the formation of a projection assembly with strong recurrent connections between those neurons with ‘on’-gated dendrites in area Y (<xref ref-type="fig" rid="F4">Fig. 4A, B</xref>; light green).</p><p id="P18">In a second step, we aimed to learn an association of neurons from two different areas (X and X’) by forming an assembly in a common downstream area (Y) in the same context (C1) as the initial projection. Hence, we simultaneously coactivated a subset of neurons in area X and X’ (<xref ref-type="fig" rid="F4">Fig. 4A, B</xref>; dark green). As before, synaptic plasticity led to the formation of an association assembly in area Y by potentiating the recurrent weights between those neurons with ‘on’-gated dendrites in area Y. Due to the presence of feedforward inhibition, we ensured that plasticity during both, projection and association, generated assemblies of similar size (<xref ref-type="sec" rid="S10">Methods</xref>, <xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S2F, G</xref>).</p><p id="P19">A common problem with learning assemblies in recurrent networks is catastrophic forgetting, as previously learned assemblies become erased by new ones (<xref ref-type="bibr" rid="R58">Litwin-Kumar and Doiron, 2014</xref>; <xref ref-type="bibr" rid="R115">Zenke et al., 2015</xref>). Our proposed framework with dendrite-specific inhibitory gating provides a solution to this problem. We again took the same neurons in area X involved in the initial projection in context C1, but activated them in another context (C2). This activated a different subset of neurons in area Y due to the sparse and random feedforward connectivity and the distinct sets of ‘on’-gated dendrites for contexts C1 and C2 (<xref ref-type="fig" rid="F4">Fig. 4A</xref>; compare light green and light purple). Therefore, synaptic plasticity formed a different projection assembly in area Y in context C2 by potentiating the recurrent weights between a new subset of neurons (<xref ref-type="fig" rid="F4">Fig. 4C</xref>; compare light green and light purple). Following the same approach, a new association can also be learned in the new context (C2) (<xref ref-type="fig" rid="F4">Fig. 4A, C</xref>; dark purple).</p><p id="P20">Finally, to test if our framework supports learning with overlapping assemblies, we deliberately selected neurons in area X’ that project to those dendrites in area Y, which are gated ‘on’ in a new context (C3) and belong to the assembly neurons of the initial projection in C1 (light green). Activating these neurons (<xref ref-type="fig" rid="F4">Fig. 4A</xref>; light brown) leads to the formation of an assembly consisting of a large number of the same neurons as the assembly learned in the initial projection, despite a completely different set of presynaptic neurons in X (light green) and X’ (light brown). The same approach works for an association in context C3 (<xref ref-type="fig" rid="F4">Fig. 4A</xref>; dark brown). Therefore, we find that the same neurons can be part of multiple assemblies through their multiple dendrites, even with a large overlap between the assemblies (<xref ref-type="fig" rid="F4">Fig. 4B, C</xref>; light and dark brown). Crucially, the already learned assemblies in area Y are not forgotten, since previously learned synaptic weights are protected when learning new assemblies by changing context (<xref ref-type="fig" rid="F4">Fig. 4A</xref>; bottom).</p><p id="P21">In summary, dendrite-specific gating of synaptic plasticity via context-dependent inhibition enables the learning of multiple overlapping assemblies – projections and associations – without forgetting. While these assemblies are defined in the typical sense with strongly mutually connected neurons (<xref ref-type="bibr" rid="R65">Miehl et al., 2023</xref>), the dendrite-specific gating by context-dependent inhibition ensures that each neuron participates in an assembly only with its ‘on’-gated dendrite in a given context. Hence, imprinting multiple of these non-overlapping assemblies can support ‘assembly calculus’ operations underlying many neural computations (<xref ref-type="bibr" rid="R72">Papadimitriou et al., 2020</xref>; <xref ref-type="bibr" rid="R73">Papadimitriou and Friederici, 2022</xref>).</p></sec><sec id="S6"><title>Assembly computations in a visual-auditory association task</title><p id="P22">We designed a visual-auditory association task to demonstrate the applicability of the ‘assembly calculus’ framework in a real-world example. The goal of the task is to distinguish four different stimuli, two letters and two numbers, by their neural representation in a downstream area. We chose the numbers “0” and “1” and the letters “O” and “l” because both pairs have very similar visual appearances (“0” and “O” vs. “1” and “l”), but differ in their phonetics ([Zee-Ro] and [Ou] vs. [Wun] and [El]). Hence, it is much more difficult to distinguish the stimuli by only relying on the visual modality. We represented the sensory stimuli in the form of assemblies, first in a visual and an auditory sensory area, followed each by a downstream (auditory and visual) area (<xref ref-type="fig" rid="F5">Fig. 5A</xref>, left). We tasked the network to separate the two stimuli in a ‘concept’ area that is located downstream from the auditory and visual areas. We used two distinct contexts in the visual, auditory and concept areas to simulate the knowledge of perceiving letters vs. numbers.</p><p id="P23">Since all stimuli sound phonetically different, in the auditory sensory area, we assumed that each stimulus activates a potentially different assembly. In contrast, in the visual sensory area, similar stimuli (“0” and “O” vs. “1” and “l”) activate largely overlapping assemblies. To train the network with visual stimuli, we choose images from the extended MNIST dataset which were filtered by Gabor patches (<xref ref-type="sec" rid="S10">Methods</xref>). We implemented a learning protocol using our proposed plasticity framework with dendrite-specific inhibition encoded by context (numbers vs. letters): we projected each stimulus from the two sensory areas into assemblies in the downstream areas, associating the corresponding visual and auditory representations of the same stimulus, as can be seen in the activity (<xref ref-type="fig" rid="F5">Fig. 5A</xref>; right). Although largely overlapping assemblies in the visual area represent similar visual stimuli so they cannot be easily distinguished, due to the association with assemblies from the auditory area and in the presence of different contexts for letters vs. numbers, we found that they can be well separated in the downstream concept area, as reflected in non-overlapping assemblies (<xref ref-type="fig" rid="F5">Fig. 5B</xref>).</p><p id="P24">After learning, we tested the ability of the network to separate potentially confounding visual stimuli in the downstream concept area. We presented a new visual image not used during training in the absence of the corresponding auditory stimulus in each of the two contexts, and measured the assembly firing rates and the number of active assembly neurons in the downstream auditory/visual areas and the concept area (<xref ref-type="fig" rid="F5">Fig. 5C</xref>). Unsurprisingly, the downstream visual area fails to distinguish the two pairs of visually similar stimuli (“0” and “O” vs. “1” and “l”) (<xref ref-type="fig" rid="F5">Fig. 5C</xref>, top). However, due to the formed associations, the concept area can separate the two visually similar stimuli even without the presentation of an auditory stimuli, as represented by the high firing rate and higher number of active neurons of the corresponding assembly (<xref ref-type="fig" rid="F5">Fig. 5C</xref>, bottom). Even when only providing half of the input (e.g./visual stimulus only) to the concept area, strong recurrent connectivity among assembly neurons amplifies the firing rates and activates the entire assembly - consistent with pattern completion.</p><p id="P25">In summary, learning overlapping assemblies without forgetting enables context-specific assembly computations in the form of projections and associations, which can be used to correctly separate ambiguous stimuli in a given context by relying on their dendritic-specific representations.</p></sec><sec id="S7"><title>Recall and pattern completion are enhanced in downstream areas</title><p id="P26">The recall due to strong recurrent connectivity can prove useful in preventing information from being degraded as it propagates across multiple hierarchically organized areas. To investigate if this mechanism operates in our networks, we imprinted assemblies in several consecutive multi-area projections (from area X to area Y to area Z) through context-gated plasticity (<xref ref-type="fig" rid="F6">Fig. 6A</xref>). Given that neurons can drop out of an assembly without affecting the firing rates of the assembly (<xref ref-type="fig" rid="F3">Fig. 3G</xref>), we wondered if this dropout might impair the recall of assemblies in multi-area projections. Hence, we silenced a fraction of the neurons of the learned assembly in area Y and tested recall in area Z while reactivating the original set of neurons in area X (<xref ref-type="fig" rid="F6">Fig. 6B</xref>). We observed that despite decreased activity in area Y, assembly neurons in area Z remain highly active due to the strong recurrent within-assembly connections. Just like recall, pattern completion in area Z can be achieved when activating a subset of neurons in area X even if a fraction of neurons in area Y are silenced (<xref ref-type="fig" rid="F6">Fig. 6C</xref>). Therefore, the strong recurrent within-assembly weights effectively implement a chain of activity amplification as information propagates across multiple hierarchical areas due to pattern completion. Our results demonstrate that multi-area projections of neural assemblies can lead to reliable recall in downstream areas despite disruptions in earlier, upstream areas.</p></sec><sec id="S8"><title>Associative learning with existing assemblies</title><p id="P27">So far, we showed that our framework supports the learning of associations between different stimuli. However, these associations can be learned in different ways: ‘simultaneously’, where the assemblies are learned by activating them at the same time, or ‘sequentially’, where an association is learned utilizing existing assemblies. We compared how the identity of assembly neurons and pattern completion are implemented in these two types of associative learning as well as the case of learning two separate projections. Specifically, we studies three cases: (I) sequential projections of distinct assemblies, first from area X to Y to Z, followed by a second projection from area X’ to Y to Z; (II) simultaneous association of assemblies from areas X and X’ to Y to Z, and (III) sequential projection and association where the association of assemblies from areas X and X’ to Y to Z is learned on top of an existing projection from X to Y to Z (<xref ref-type="fig" rid="F7">Fig. 7A</xref>). We quantified how learned assemblies in areas Y and Z differ by comparing cases I and II (<xref ref-type="fig" rid="F7">Fig. 7B</xref>) and cases II and III (<xref ref-type="fig" rid="F7">Fig. 7C</xref>) based on three measures: the overlap in assembly neuron identity (Venn Diagram), the distribution of input synapses onto assembly neurons (Synapse distribution), and the response of assemblies to partial activation of input neurons (Pattern completion).</p><p id="P28">First, we compared the assemblies formed in areas Y and Z via sequential projection (case I) or via simultaneous association (case II). A large subset of neurons in the simultaneous association assembly in area Y is the same as the neurons learned via the respective projection only (on average 6.5 for the light green and 7.1 for the dark green assembly), while a smaller subset of neurons (3.7) is unique to the assembly learned via the simultaneous association (<xref ref-type="fig" rid="F7">Fig. 7B</xref>, top left). In the downstream area Z, the overlap decreases, with a smaller subset of neurons representing the learned association compared to learning projections alone (on average 4.5 for the light green and 5.7 for the dark green assembly), while the number of unique assembly neurons increases (11) (<xref ref-type="fig" rid="F7">Fig. 7B</xref>, bottom left). Therefore, the assemblies learned via simultaneous association (case II) become more independent from assemblies learned via projections (case I) in more downstream areas.</p><p id="P29">We next quantified how input synapses onto assemblies in areas Y and Z differ between cases I and II. To form an assembly in area Y, neurons need to receive a sufficiently high number of active inputs to drive LTP. As previously shown (<xref ref-type="fig" rid="F2">Fig. 2B</xref>), for the case of a projection from X to Y, most assembly neurons in the downstream area have to receive ≥ 4 highly active inputs from the upstream area (<xref ref-type="fig" rid="F7">Fig. 7B</xref>, middle, light green bars in area Y and Z). For the case of simultaneous association, there are twice as many highly active assembly neurons in area Y since both X and X’ are active, in contrast to the case of a projection when only X is active. Therefore, the distribution of input synapses from area X and X’ onto the assembly in area Y in the case of simultaneous association (II) is broader compared to the distribution in the case of a sequential projection (I) (<xref ref-type="fig" rid="F7">Fig. 7B</xref>, top middle, light and dark orange compared to light green). In the downstream area Z, the synapse distributions of the two cases become more similar again because the assembly in area Y (projection in case I and simultaneous association in case II) forms a projection to area Z in the two cases (<xref ref-type="fig" rid="F7">Fig. 7B</xref>, bottom middle, orange compared to light green). When activating either only the inputs in area X or area X’ in the case of simultaneous association, the downstream assemblies in areas Y and Z fire reliably at ≈ 50% of the rates when activating X and X’ simultaneously, significantly above background firing rate (<xref ref-type="fig" rid="F7">Fig. 7B</xref>, right, at 10 Hz input rates). Pattern completion is robust in both areas as observed before in <xref ref-type="fig" rid="F6">Fig. 6</xref>.</p><p id="P30">Next, we compared the assemblies formed in areas Y and Z via sequential projection (case I) vs. sequential association (case III). In contrast to the comparison between I and II, most assembly neurons involved in cases I and III are similar, in particular, the neurons in areas Y and Z after learning the association are almost the same as the neurons learned only from a projection in X (on average 18.2 neurons in area Y and 18.6 neurons in area Z, <xref ref-type="fig" rid="F7">Fig. 7C</xref>, top and bottom left). Therefore, when learning new associations with existing assemblies, most of the neurons that become part of the new assembly stay the same as the existing assembly. This is different from case II where the identity of assembly neurons differs, especially in downstream areas. The distribution of input synapses shows that most of the strong synapses onto the sequential association assembly in area Y come from previously imprinted area X (<xref ref-type="fig" rid="F7">Fig. 7C</xref>, top and bottom middle). This leads to a more reliable pattern completion (higher firing rates) when activating only area X compared to activating only area X’ (light orange is higher than dark orange, <xref ref-type="fig" rid="F7">Fig. 7C</xref>, top right). Moreover, when activating only X’, pattern completion is improved in the downstream area Z compared to area Y (dark orange is higher in the bottom than the top).</p><p id="P31">In summary, our framework allows us to learn two types of associations – either via simultaneously learned associations or by learning an association on top of existing projections. This suggests general principles of learning in the brain that might benefit from previously learned structures. While the learned assemblies differ in terms of which neurons and which synapses participate in intermediate areas along the hierarchy, their differences vanish in later downstream areas, where computational aspects like pattern completion become more reliable.</p></sec></sec><sec id="S9" sec-type="discussion"><title>Discussion</title><p id="P32">While assemblies have often been suggested as the basis for computations in the brain (<xref ref-type="bibr" rid="R10">Buzsáki, 2010</xref>; <xref ref-type="bibr" rid="R21">Eichenbaum, 2018</xref>; <xref ref-type="bibr" rid="R41">Huyck and Passmore, 2013</xref>; <xref ref-type="bibr" rid="R113">Yuste, 2015</xref>), few theoretical studies have shown how assemblies can be flexibly learned and combined to perform real-world complex computations. This has been mainly due to the challenge of learning overlapping assemblies without merging and the problem of catastrophic forgetting. Here, we propose two biologically-inspired mechanisms as a solution, specifically nonlinear dendritic compartments as the loci for learning, and inhibitory context-dependent gating allowing for dendrite-specific gating of synaptic plasticity via disinhibition (<xref ref-type="fig" rid="F2">Fig. 2</xref>). Together with inhibitory control of assembly size, these mechanisms support the learning of stable but flexible assemblies (<xref ref-type="fig" rid="F3">Fig. 3</xref>). These assemblies can be combined via projections or associations across multiple hierarchically organized areas without forgetting previously learned structures, and with assembly overlaps where the same neuron can participate in multiple assemblies through its different dendrites (<xref ref-type="fig" rid="F4">Fig. 4</xref>). We exemplify assembly computations in a visual-auditory association task, in which we show how assembly computations can be used to learn associations of related stimuli across different sensory pathways and how they can be separated according to their representations in downstream areas (<xref ref-type="fig" rid="F5">Fig. 5</xref>). We further show that assemblies can be reliably recalled, especially in downstream areas (<xref ref-type="fig" rid="F6">Fig. 6</xref>) and that associative learning in these downstream areas may benefit from existing representations (<xref ref-type="fig" rid="F7">Fig. 7</xref>).</p><p id="P33">The key underlying mechanism behind stable assembly formation and context-dependent gating in our framework is inhibition. We included inhibitory neurons with different roles in our modeling framework, in agreement with functions suggested by experimental studies. Recurrent inhibition prevents an assembly being learned from growing too large and taking over the entire network by suppressing excessive network excitation (<xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S2D, E</xref>). This mechanism is reminiscent of the ‘k-winner take-all’ concept (<xref ref-type="bibr" rid="R54">Kwon and Zervakis, 1995</xref>), where inhibition is proposed to keep roughly the same number of neurons active (<xref ref-type="bibr" rid="R72">Papadimitriou et al., 2020</xref>). Experimental support for this type of inhibition comes from the mouse hippocampus, where somatostatin-expressing (SST) inhibitory interneurons have been shown to control the size of co-active groups of neurons referred to as ensembles (<xref ref-type="bibr" rid="R95">Stefanelli et al., 2016</xref>). A second source of inhibition that acts in a feedforward manner in our model keeps the size of the assembly fixed when the excitatory input to the network changes (<xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S2F, G</xref>). This feedforward inhibition is consistent with that provided by parvalbumin-expressing (PV) interneurons (<xref ref-type="bibr" rid="R100">Tremblay et al., 2016</xref>). Changing the existing connectivity structure in our model implements learning, for which synaptic plasticity needs to be gated by decreased inhibition onto the dendrites, which are the loci of learning. Learning marks a departure of the model network from its baseline state where all dendrites are inhibited and synaptic plasticity is blocked. Learning occurs by selectively disinhibiting specific dendrites in a particular context. This disinhibitory mechanism is inspired by the disinhibitory influence of vasointestinal-peptide-expressing (VIP) interneurons onto SST interneurons, which together form the disinhibitory VIP-SST circuit. Hence, VIP interneuron input to SSTs provides a substrate for the context signal necessary for learning (<xref ref-type="bibr" rid="R51">Krabbe et al., 2019</xref>; <xref ref-type="bibr" rid="R12">Canto-Bustos et al., 2022</xref>). This disinhibition has been shown to enhance dendritic spikes (<xref ref-type="bibr" rid="R29">Gentet et al., 2012</xref>; <xref ref-type="bibr" rid="R59">Lovett-Barron, 2021</xref>) and flexibly route information (<xref ref-type="bibr" rid="R111">Yang et al., 2016</xref>). VIP interneurons are known to receive top-down inputs depending on the behavioral context of the animal and regulate population activity (<xref ref-type="bibr" rid="R80">Pi et al., 2013</xref>; <xref ref-type="bibr" rid="R25">Fu et al., 2014</xref>; <xref ref-type="bibr" rid="R28">Garcia Del Molino et al., 2017</xref>; <xref ref-type="bibr" rid="R20">Dipoppa et al., 2018</xref>).</p><p id="P34">Several prior computational studies have focused on how assembly structures can be learned with different plasticity rules (<xref ref-type="bibr" rid="R17">Clopath et al., 2010</xref>; <xref ref-type="bibr" rid="R58">Litwin-Kumar and Doiron, 2014</xref>; <xref ref-type="bibr" rid="R115">Zenke et al., 2015</xref>; <xref ref-type="bibr" rid="R90">Schulz et al., 2021</xref>; <xref ref-type="bibr" rid="R65">Miehl et al., 2023</xref>). However, many have faced the problem that any initial overlap between assemblies in terms of participating neurons usually leads to the merging or separation of the assemblies. A few notable example studies have demonstrated that assembly overlap is possible for specific choices of spike timing-dependent plasticity (<xref ref-type="bibr" rid="R60">Manz and Memmesheimer, 2023</xref>; <xref ref-type="bibr" rid="R111">Yang and Doiron, 2025</xref>), short-term plasticity (<xref ref-type="bibr" rid="R23">Fauth and van Rossum, 2019</xref>), or other mechanisms (<xref ref-type="bibr" rid="R81">Podlaski et al., 2025</xref>; <xref ref-type="bibr" rid="R5">Bergoin et al., 2025</xref>). A second related problem is forgetting previously learned assemblies where synaptic connections reinforcing one assembly decay while others grow as new assemblies are being learned. In our work, besides enabling learning at ‘on’-gated dendrites, the context signal also prevents synapses to ‘off’-gated dendrites from undergoing synaptic plasticity. This stabilizes the weights of already learned assemblies in different contexts, even if neurons of a previously learned assembly join a newly learned assembly (<xref ref-type="fig" rid="F3">Fig. 3</xref> - <xref ref-type="fig" rid="F5">5</xref>). Hence, the mechanism of context-dependent inhibition allows assemblies to overlap without merging or forgetting by sharing participating neurons through their different dendrites. This is similar to the idea that inhibition keeps memories in a quiescent state unless a context-dependent disinhibitory mechanism releases those previously learned memories (<xref ref-type="bibr" rid="R3">Barron et al., 2016</xref>, 2017). The proposal that inhibition might mediate a context signal has been implemented in several computational studies, studying the influence of surround information on visual computation (<xref ref-type="bibr" rid="R103">Voina et al., 2022</xref>), increasing capacity in memory networks (<xref ref-type="bibr" rid="R81">Podlaski et al., 2025</xref>), multitask and transfer learning (<xref ref-type="bibr" rid="R109">Wybo et al., 2023</xref>), and showing that sparse inhibitory context projects the network activity onto unique neural subspaces (<xref ref-type="bibr" rid="R57">Lehr et al., 2023</xref>).</p><p id="P35">Furthermore, our approach is similar to a recent proposal that error signals between sensory and expected information during learning are encoded in the dendritic excitatory/inhibitory balance. In these studies, top-down feedback signals arriving at the dendrites can disrupt this balance, gate local plasticity and, therefore, lead to the re-learning or updating of synaptic weights according to the error signal (<xref ref-type="bibr" rid="R87">Sacramento et al., 2018</xref>; <xref ref-type="bibr" rid="R85">Rossbroich and Zenke, 2025</xref>; <xref ref-type="bibr" rid="R26">Galloni et al., 2025</xref>). The contextual signal that gates dendrite-specific plasticity in our study could also be interpreted as such an error signal.</p><p id="P36">In artificial neural networks (ANNs), catastrophic forgetting has been known for a long time to hinder sequential learning (<xref ref-type="bibr" rid="R62">McCloskey and Cohen, 1989</xref>). Many different solutions have been suggested to counteract forgetting in ANNs, like metaplasticity (<xref ref-type="bibr" rid="R43">Jedlicka et al., 2022</xref>), gating via inhibition (<xref ref-type="bibr" rid="R61">Masse et al., 2018</xref>; <xref ref-type="bibr" rid="R92">Sezener et al., 2021</xref>; <xref ref-type="bibr" rid="R99">Tilley et al., 2023</xref>), dendritic compartments (<xref ref-type="bibr" rid="R13">Chavlis and Poirazi, 2021</xref>; <xref ref-type="bibr" rid="R70">Pagkalos et al., 2024</xref>), elastic weight consolidation (<xref ref-type="bibr" rid="R50">Kirkpatrick et al., 2017</xref>), neuromodulatory systems (<xref ref-type="bibr" rid="R63">Mei et al., 2022</xref>, 2025), and many more (<xref ref-type="bibr" rid="R74">Parisi et al., 2019</xref>; <xref ref-type="bibr" rid="R115">Zenke and Laborieux, 2024</xref>). Our model proposes the biologically motivated mechanisms of dendrite-specific gating via disinhibitory context pathways to counteract forgetting, which might be one of multiple strategies to achieve lifelong learning (<xref ref-type="bibr" rid="R36">Hadsell et al., 2020</xref>; <xref ref-type="bibr" rid="R53">Kudithipudi et al., 2022</xref>). Interestingly, the idea bears resemblance to a recent approach, where an additional module protects already learned connections in an artificial neural network by ‘rotating’ the inputs (<xref ref-type="bibr" rid="R114">Zeng et al., 2019</xref>).</p><p id="P37">In conclusion, this study presents a novel approach to assembly computations with two key biologically inspired mechanisms that put forward dendrites as the main place of learning: nonlinear dendritic compartments and inhibitory context-dependent gating. Our model demonstrates how assemblies can be flexibly learned and combined without merging or catastrophic forgetting. The important new assumptions in this framework align with multiple experimental findings on different roles of inhibition, offering many insights into complex brain computations that underlie flexible and stable learning. Our work also opens new avenues for how to make artificial neural networks that can do useful computations more biologically plausible and hence bridges the gap between biological realism and computational functionality in neural circuit modeling.</p></sec><sec id="S10" sec-type="methods"><title>Methods</title><sec id="S11"><title>The excitatory neuron model</title><p id="P38">The neuron model is a multi-compartment model adapted from <xref ref-type="bibr" rid="R111">Yang et al. (2016)</xref> and features <italic>D</italic> = 6 dendritic compartments that are each coupled to the soma.</p><sec id="S12"><title>The soma</title><p id="P39">The soma is a leaky integrate-and-fire compartment whose voltage dynamics follow: <disp-formula id="FD1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mtext>soma</mml:mtext></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mtext>d</mml:mtext><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mtext>soma</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mtext>leak</mml:mtext></mml:mrow><mml:mrow><mml:mtext>soma</mml:mtext></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mtext>soma</mml:mtext></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mtext>rest</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mtext>dends</mml:mtext></mml:mrow><mml:mrow><mml:mtext>soma</mml:mtext></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mtext>soma</mml:mtext></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mtext>dends</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mtext>syn</mml:mtext></mml:mrow><mml:mrow><mml:mtext>soma</mml:mtext></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>ξ</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>V</italic><sup>soma</sup> is the potential of the somatic compartment, <italic>C</italic><sup>soma</sup> = 50 pF is the membrane capacitance, <inline-formula><mml:math id="M2"><mml:mrow><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mtext>leak</mml:mtext></mml:mrow><mml:mrow><mml:mtext>soma</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>2.5</mml:mn><mml:mtext>nS</mml:mtext></mml:mrow></mml:math></inline-formula> is the leak conductance and the conductance <inline-formula><mml:math id="M3"><mml:mrow><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mtext>dends</mml:mtext></mml:mrow><mml:mrow><mml:mtext>soma</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>24</mml:mn><mml:mtext>nS</mml:mtext></mml:mrow></mml:math></inline-formula> determines the coupling strength between all dendritic and the somatic compartment. <italic>V</italic><sub>rest</sub> = –70 mV is the resting potential and 〈<italic>V</italic><sup>dends</sup>〉 describes the average potential of all 6 dendrites. In addition, the soma receives inhibitory gamma-aminobutyric acid (GABA) input via <inline-formula><mml:math id="M4"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mtext>syn</mml:mtext></mml:mrow><mml:mrow><mml:mtext>soma</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and somatic noise <italic>ξ</italic> is modeled as a Gaussian random variable with mean and standard deviation 110 ±7 pA to produce a baseline spiking behavior and model excitatory and inhibitory background inputs. Whenever the somatic potential <italic>V</italic><sup>soma</sup> exceeds the threshold <italic>V</italic><sub>thresh</sub> = –50 mV, the soma elicits a spike, and the somatic potential is set to <italic>V</italic><sub>reset</sub> = –55 mV. During refractory time <italic>τ</italic><sub>ref</sub> = 2 ms following the spike, <italic>V</italic><sup>soma</sup> can change, but crossing the threshold again does not evoke a spike.</p><p id="P40">The soma features inhibitory synapses modeling the linear dynamics of GABA<sub>A</sub> channels: <disp-formula id="FD2"><label>(2)</label><mml:math id="M5"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mtext>syn</mml:mtext></mml:mrow><mml:mrow><mml:mtext>soma</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mtext>GABA</mml:mtext></mml:mrow><mml:mrow><mml:mtext>soma</mml:mtext></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mtext>soma</mml:mtext></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mtext>GABA</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P41">The reversal potential for GABAergic synapses <italic>E</italic><sub>GABA</sub> is set at -75 mV. The synaptic conductance <inline-formula><mml:math id="M6"><mml:mrow><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mtext>GABA</mml:mtext></mml:mrow><mml:mrow><mml:mtext>soma</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> increases by 8 nS whenever a presynaptic spike arrives and decreases exponentially with time constant <italic>τ</italic><sub>GABA</sub> = 20 ms: <disp-formula id="FD3"><label>(3)</label><mml:math id="M7"><mml:mrow><mml:mfrac><mml:mrow><mml:mtext>d</mml:mtext><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mtext>GABA</mml:mtext></mml:mrow><mml:mrow><mml:mtext>soma</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mtext>GABA</mml:mtext></mml:mrow><mml:mrow><mml:mtext>soma</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>GABA</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p></sec><sec id="S13"><title>The dendrites</title><p id="P42">The dendrites are leaky integrator compartments, each with potential <italic>V</italic><sup>dend</sup> following <disp-formula id="FD4"><mml:math id="M8"><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mtext>dend</mml:mtext></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mtext>d</mml:mtext><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mtext>dend</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mtext>leak</mml:mtext></mml:mrow><mml:mrow><mml:mtext>dend</mml:mtext></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mtext>dend</mml:mtext></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mtext>rest</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mtext>soma</mml:mtext></mml:mrow><mml:mrow><mml:mtext>dend</mml:mtext></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mtext>dend</mml:mtext></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>V</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mtext>soma</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mtext>syn</mml:mtext></mml:mrow><mml:mrow><mml:mtext>dend</mml:mtext></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>C</italic><sup>dend</sup> = 20 pF is dendritic membrane capacitance, <inline-formula><mml:math id="M9"><mml:mrow><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mtext>leak</mml:mtext></mml:mrow><mml:mrow><mml:mtext>dend</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>4</mml:mn><mml:mtext>nS</mml:mtext></mml:mrow></mml:math></inline-formula> is the leak conductance and <italic>V</italic><sub>rest</sub> = –70 mV is the resting potential. The conductance <inline-formula><mml:math id="M10"><mml:mrow><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mtext>soma</mml:mtext></mml:mrow><mml:mrow><mml:mtext>dend</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>4</mml:mn><mml:mtext>nS</mml:mtext></mml:mrow></mml:math></inline-formula> determines the coupling strength of the dendrite to the soma. <inline-formula><mml:math id="M11"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>V</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mtext>soma</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is an auxiliary somatic potential, whose dynamics are identical to <italic>V</italic><sup>soma</sup> (Eq. 1 with exact same noise <italic>)</italic> except that it does not spike. The dendrites feature inhibitory GABA receptor synapses and excitatory synapses consisting of <italic>α</italic>-amino-3-hydroxy-5-methyl-4-isoxazolepropionic acid (AMPA) and non-linear N-methyl-D-aspartate (NMDA) receptor channels: <disp-formula id="FD5"><mml:math id="M12"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mtext>syn</mml:mtext></mml:mrow><mml:mrow><mml:mtext>dend</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mtext>GABA</mml:mtext></mml:mrow><mml:mrow><mml:mtext>dend</mml:mtext></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mtext>AMPA</mml:mtext></mml:mrow><mml:mrow><mml:mtext>dend</mml:mtext></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mtext>NMDA</mml:mtext></mml:mrow><mml:mrow><mml:mtext>dend</mml:mtext></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P43">The currents <inline-formula><mml:math id="M13"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mtext>GABA</mml:mtext></mml:mrow><mml:mrow><mml:mtext>dend</mml:mtext></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mtext>AMPA</mml:mtext></mml:mrow><mml:mrow><mml:mtext>dend</mml:mtext></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mtext>NMDA</mml:mtext></mml:mrow><mml:mrow><mml:mtext>dend</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> represent total currents flowing through the respective channels. The GABA and AMPA receptors follow linear dynamics analogous to (<xref ref-type="disp-formula" rid="FD2">Eq. 2</xref>) and (<xref ref-type="disp-formula" rid="FD3">Eq. 3</xref>): <disp-formula id="FD6"><mml:math id="M14"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mtext>GABA</mml:mtext></mml:mrow><mml:mrow><mml:mtext>dend</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mtext>GABA</mml:mtext></mml:mrow><mml:mrow><mml:mtext>dend</mml:mtext></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mtext>dend</mml:mtext></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mtext>GABA</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mtext>d</mml:mtext><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mtext>GABA</mml:mtext></mml:mrow><mml:mrow><mml:mtext>dend</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>g</mml:mi><mml:mrow><mml:mtext>GABA</mml:mtext></mml:mrow><mml:mrow><mml:mtext>dend</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>GABA</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula> <disp-formula id="FD7"><mml:math id="M15"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mtext>AMPA</mml:mtext></mml:mrow><mml:mrow><mml:mtext>dend</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mtext>AMPA</mml:mtext></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mtext>dend</mml:mtext></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mtext>d</mml:mtext><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mtext>AMPA</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mtext>AMPA</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>GABA</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p id="P44">The reversal potential for excitatory synapses is <italic>E<sub>exc</sub></italic> = 0 mV and the time constant is <italic>τ</italic><sub>AMPA</sub> = 2 ms. With each presynaptic spike, the conductance <italic>g</italic><sub>AMPA</sub> is increased by <inline-formula><mml:math id="M16"><mml:mrow><mml:mi>w</mml:mi><mml:msub><mml:mover accent="true"><mml:mi>g</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mtext>AMPA</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M17"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>g</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mtext>AMPA</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.4</mml:mn><mml:mtext>nS</mml:mtext></mml:mrow></mml:math></inline-formula> and the weight factor <italic>w</italic> may change due to synaptic plasticity. The conductance of GABAergic synapses <italic>g</italic><sub>GABA</sub> is increased with each presynaptic spike by a factor <inline-formula><mml:math id="M18"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>g</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mtext>GABA</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, which depends on the identity of the inhibitory synapse (see below for the description of different types of inhibition). NMDA receptors introduce a non-linear excitation mechanism which includes a voltage-dependent magnesium block <italic>f</italic><sub>Mg</sub> and a saturating gating variable <italic>s</italic><sub>NMDA</sub>: <disp-formula id="FD8"><mml:math id="M19"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mtext>NMDA</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>w</mml:mi><mml:msub><mml:mover accent="true"><mml:mi>g</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mtext>NMDA</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mtext>NMDA</mml:mtext></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mtext>dend</mml:mtext></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mtext>exc</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mtext>Mg</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mtext>dend</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P45">The conductance <inline-formula><mml:math id="M20"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>g</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mtext>NMDA</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2.5</mml:mn><mml:mtext>nS</mml:mtext></mml:mrow></mml:math></inline-formula>. The weight <italic>w</italic> is shared with AMPA receptors within the same excitatory synapse. The magnesium block is modeled using a sigmoid function <disp-formula id="FD9"><mml:math id="M21"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mtext>Mg</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mtext>dend</mml:mtext></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mtext>dend</mml:mtext></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mtext>half</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mtext>width</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula> with its inflection point at <italic>V</italic><sub>half</sub> = –19.9 mV and slope governed by <italic>V</italic><sub>width</sub> = 12.48 mV. The NMDAR gating variable dynamics follow: <disp-formula id="FD10"><mml:math id="M22"><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mrow><mml:mtext>d</mml:mtext><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mtext>NMDA</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mtext>NMDA</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>decay</mml:mtext></mml:mrow><mml:mrow><mml:mtext>NMDA</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mtext>NMDA</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mtext>NMDA</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mrow><mml:mtext>d</mml:mtext><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mtext>NMDA</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mtext>NMDA</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>rise</mml:mtext></mml:mrow><mml:mrow><mml:mtext>NMDA</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula> with <inline-formula><mml:math id="M23"><mml:mrow><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>decay</mml:mtext></mml:mrow><mml:mrow><mml:mtext>NMDA</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>100</mml:mn><mml:mtext>ms,</mml:mtext><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>rise</mml:mtext></mml:mrow><mml:mrow><mml:mtext>NMDA</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mtext>ms</mml:mtext></mml:mrow></mml:math></inline-formula>, and <italic>α</italic> — 0.3 ms<sup>−1</sup>. Whenever a presynaptic spike occurs, <italic>r</italic><sub>NMDA</sub> is set to 1. In the case of linear NMDA receptors, <italic>f</italic><sub>Mg</sub>(<italic>V</italic><sup>dend</sup>) = <italic>V</italic><sup>dend</sup> (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S1</xref>).</p></sec></sec><sec id="S14"><title>Homosynaptic plasticity</title><p id="P46">We implemented a phenomenologically derived synaptic plasticity rule, the so-called voltage rule (<xref ref-type="bibr" rid="R17">Clopath et al., 2010</xref>; <xref ref-type="bibr" rid="R8">Bono and Clopath, 2017</xref>). The rule is a form of spike-timing-dependent plasticity that considers the timing of presynaptic spikes and the postsynaptic membrane potential (here the potential of a dendrite). The amount by which the synaptic weight <italic>w</italic> changes follows <disp-formula id="FD11"><mml:math id="M24"><mml:mrow><mml:mfrac><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mtext>LTD</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>_</mml:mo></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mo>_</mml:mo></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo></mml:msub><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mtext>LTP</mml:mtext></mml:mrow></mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:msub><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mtext>dend</mml:mtext></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mo>+</mml:mo></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>+</mml:mo></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mo>_</mml:mo></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula> where <inline-formula><mml:math id="M25"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>−</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M26"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>+</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> is the dendritic potential, delayed and low-pass-filtered with different time constants: <disp-formula id="FD12"><mml:math id="M27"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mo>±</mml:mo></mml:msub><mml:mfrac><mml:mrow><mml:mtext>d</mml:mtext><mml:msub><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>±</mml:mo></mml:msub></mml:mrow><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>±</mml:mo></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mfrac><mml:mrow><mml:mtext>d</mml:mtext><mml:msub><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mtext>dend</mml:mtext></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p id="P47">The []<sub>+</sub> operator indicates rectification, that is [<italic>λ</italic>]<sub>+</sub> = <italic>λ</italic> for <italic>λ</italic> &gt; 0 and [<italic>λ</italic>]<sub>+</sub> = 0 otherwise. The spike trace <inline-formula><mml:math id="M28"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:math></inline-formula> follows <disp-formula id="FD13"><mml:math id="M29"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mtext>d</mml:mtext><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p id="P48">The Dirac <italic>δ</italic> represents presynaptic spikes occurring at times <italic>t<sub>i</sub></italic>. When a spike arrives at a plastic synapse at time <italic>ti</italic> and the dendrite has been sufficiently depolarized <inline-formula><mml:math id="M30"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>−</mml:mo></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mo>−</mml:mo></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> the synaptic weight gets immediately depressed by <inline-formula><mml:math id="M31"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mtext>LTD</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>−</mml:mo></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mo>−</mml:mo></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula>. The synaptic weights may also undergo potentiation at all times when <inline-formula><mml:math id="M32"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>+</mml:mo></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mo>−</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> and <italic>V</italic><sup>dend</sup> &gt; <italic>θ</italic><sub>+</sub>. The potentiation degree depends on the depolarization level and the time since the last presynaptic spikes. The amplitude parameters are <italic>A</italic><sub>LTD</sub> = 4 mV<sup>−1</sup>s<sup>−1</sup> and <italic>A</italic><sub>LTP</sub> = 1.4 · 10<sup>−3</sup> mV<sup>−2</sup>s<sup>−1</sup>, the threshold potentials are <italic>θ</italic><sub>−</sub> = –65 mV and <italic>θ</italic><sub>+</sub> = –30 mV, the time constants are <italic>τ</italic><sub>-</sub> = 15 ms, <italic>τ</italic><sub>+</sub> = 45 ms, <italic>τ</italic><sub>1</sub> = 5 ms and <italic>τ<sub>x</sub></italic> = 20 ms. We note that the values of <italic>θ</italic><sub>−</sub> and <italic>θ</italic><sub>+</sub> were slightly different from the ones used in (<xref ref-type="bibr" rid="R8">Bono and Clopath, 2017</xref>) to tune down synaptic depression. Both, feedforward and recurrent excitatory synapses undergo synaptic plasticity, and synaptic strength is bound within a specific range. For recurrent synapses the bounds are <italic>w</italic><sub>min,REC</sub> = 0 and <italic>w</italic><sub>max,REC</sub> = 5. For feedforward synapses, the maximum bound is given by <italic>w</italic><sub>max,FF</sub> = 26. The minimum bound is calculated for each dendrite individually to ensure that each dendrite can achieve a stable configuration when being part of an assembly: <disp-formula id="FD14"><mml:math id="M33"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mtext>min,FF</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mtext>in,Intial</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mtext>pot</mml:mtext></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mtext>max,FF</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mtext>max,REC</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mtext>somas</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mtext>min,REC</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mtext>syn,FF</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mtext>pot</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> with the total initial synaptic weight of the incoming synapses <italic>w</italic><sub>in,Initial</sub>, the average number of potentiated feedforward dendrites <italic>n</italic><sub>pot</sub>, the number of somas in the area <italic>n</italic><sub>somas</sub>, the average assembly size <italic>s<sub>a</sub></italic> and the number of feedforward connections <italic>n</italic><sub>syn,FF</sub>.</p></sec><sec id="S15"><title>Heterosynaptic plasticity</title><p id="P49">Besides weight changes induced by the voltage rule, excitatory synapses on each dendrite were subject to a normalization procedure. Every 5 ms, each excitatory synapse (both recurrent and feedforward) on a dendrite <italic>d</italic> is potentiated or depressed by <inline-formula><mml:math id="M34"><mml:mrow><mml:mtext>Δ</mml:mtext><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mtext>norm</mml:mtext></mml:mrow><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> depending on the weights of other excitatory synapses <italic>j</italic> on the same dendrite <disp-formula id="FD15"><mml:math id="M35"><mml:mrow><mml:mtext>Δ</mml:mtext><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mtext>norm</mml:mtext></mml:mrow><mml:mi>d</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>j</mml:mi><mml:mi>d</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mtext>tot</mml:mtext></mml:mrow><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> with <italic>η</italic> = 0.0025. The equilibrium value <inline-formula><mml:math id="M36"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mtext>tot</mml:mtext></mml:mrow><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is the sum of initial weights of all feedforward and recurrent excitatory synapses incoming to dendrite <italic>d</italic>.</p></sec><sec id="S16"><title>Single neuron simulations</title><p id="P50">For the simulations exemplifying the working of a single neuron (<xref ref-type="fig" rid="F2">Fig. 2</xref>), we built neurons with six dendrites. Each neuron is simulated with a combination of a set number of excitatory (active) synapses and a certain firing rate of the inhibitory neuron (as shown in <xref ref-type="fig" rid="F2">Fig. 2C</xref>). Each neuron receives 399 weak (<italic>w</italic> = <italic>w</italic><sub>0</sub>) excitatory connections representing recurrent input from other recurrent neurons and 32 stronger excitatory connections (<italic>w</italic> = <italic>w</italic><sub>ff</sub>) representing the feedforward drive. The excitatory connections project synapses to all six dendrites of a neuron. The feedforward synapses receive random Poisson spikes at rate 0.1 Hz, except a set number of connections called active (<xref ref-type="fig" rid="F2">Fig. 2C</xref>), which spike at rate 10 Hz. Additionally, each neuron’s dendrites receive GABAergic synapses from their inhibitory context neuron, which sends Poisson spikes at the indicated rates (<xref ref-type="fig" rid="F2">Fig. 2C</xref>; constant across each neuron).</p></sec><sec id="S17"><title>Network simulations</title><p id="P51">For simulations that involve multiple networks (areas) of neurons, each area consists of 400 neurons. Each soma projects excitatory synapses to all dendrites of the network apart from its own with an initial weight of <italic>w</italic> = 0.1. Input to each area either comes from other areas (e.g. area Y gets input from areas X and X’ in <xref ref-type="fig" rid="F4">Fig. 4</xref>) or two population 400excitatory neurons with Poisson neurons firing at the background rate <italic>r</italic><sub>bg</sub> = 0.1 Hz and a subset of 20 neurons with increased spiking rate of <italic>r</italic><sub>active</sub> = 10 Hz. The input neurons project to the network randomly with probability <italic>p<sub>FF</sub></italic> = 0.081. These connections underly homo- and heterosynaptic plasticity.</p><p id="P52">Apart from the recurrent and feedforward excitatory synapses, the neurons in the network receive inhibition from different sources, namely context inhibition, feedforward inhibition, and recurrent inhibition.</p><sec id="S18"><title>Context inhibition</title><p id="P53">Context is implemented as a population of inhibitory neurons, with each inhibitory population connecting to one dendrite of each neuron in the network. The context neurons inhibit the dendrite with a GABAergic synapse of static weight with the conductance increment <inline-formula><mml:math id="M37"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>g</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mtext>GABA</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>16</mml:mn><mml:mtext>nS</mml:mtext></mml:mrow></mml:math></inline-formula> (see above). While active, a context neuron generates spikes according to a Poisson distribution with the mean rate <italic>r</italic><sub>context</sub> = 400 Hz. The disinhibition of dendrites by the context signal is implemented by setting the firing rates of the respective context neurons to 0 Hz.</p></sec><sec id="S19"><title>Feedforward inhibition</title><p id="P54">Feedforward inhibition is implemented via a population of 400 inhibitory neurons projecting synapses to randomly selected dendrites in the network with a static weight (<inline-formula><mml:math id="M38"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>g</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mtext>GABA</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>4.45</mml:mn><mml:mtext>nS</mml:mtext></mml:mrow></mml:math></inline-formula>). Each dendrite in the network receives a connection from one and only one feedforward inhibitory neuron. The feedforward inhibitory neurons are modeled as Poisson neurons with a rate depending on the activity of excitatory feedforward inputs to the network: <disp-formula id="FD16"><mml:math id="M39"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mtext>ff</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mtext>input</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>v</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>γ</italic> = 0.34 Hz, v = 38.8 Hz, and <inline-formula><mml:math id="M40"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mtext>input</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is a trace of activity of the excitatory inputs with a time constant <italic>τ</italic><sub>input</sub> = 750 ms: <disp-formula id="FD17"><mml:math id="M41"><mml:mrow><mml:mfrac><mml:mrow><mml:mtext>d</mml:mtext><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mtext>input</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mtext>input</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>input</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P55">Whenever any feedforward excitatory neuron spikes, <inline-formula><mml:math id="M42"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mtext>input</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is increased by 1. The rectification []<sub>+</sub> is shifted to keep the firing rate not less than the minimal value of <italic>r</italic><sub>min</sub> = 36 Hz. Feedforward inhibition leads to a fixed assembly size when changing the number of excitatory inputs (<xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S2F, G</xref>).</p></sec><sec id="S20"><title>Recurrent inhibition</title><p id="P56">Recurrent inhibition controls excessive excitation in the network. For each excitatory neuron in the recurrent network, we computed a trace of its activity <italic>x</italic><sub>act</sub> low-pass-filtered with time constant <italic>τ</italic><sub>act</sub> = 750 ms: <disp-formula id="FD18"><mml:math id="M43"><mml:mrow><mml:mfrac><mml:mrow><mml:mtext>d</mml:mtext><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mtext>act</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mtext>act</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>act</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> with <italic>x</italic><sub>act</sub> being increased by 1 whenever the neuron spikes. Each area receives input from 400 recurrent inhibitory neurons. These inhibitory neurons receive random connections from the somas in the network with probability <italic>p</italic><sub>rec1</sub> = 0.15. Whenever at least <italic>n</italic><sub>act</sub> = 8 of its presynaptic excitatory neurons become activated enough (their <italic>x</italic><sub>act</sub> &gt; 2.2), the recurrent inhibitory neuron starts spiking in a Poisson manner at a rate <italic>r</italic><sub>rec</sub> = 60 Hz. The recurrent inhibitory neurons project randomly to the dendrites of the network with probability <italic>p</italic><sub>rec2</sub> = 0.06 and a static weight such that <inline-formula><mml:math id="M44"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>g</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mtext>GABA</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>16</mml:mn><mml:mtext>nS</mml:mtext></mml:mrow></mml:math></inline-formula>. This setup allows the recurrent inhibition to control activity and plasticity based on the network activity, preventing excessive excitation (<xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S2D, E</xref>).</p><p id="P57">In addition, the somas of excitatory neurons receive GABAergic connections of static weight (<inline-formula><mml:math id="M45"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>g</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mtext>GABA</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>8</mml:mn><mml:mtext>nS</mml:mtext></mml:mrow></mml:math></inline-formula>) from a population of 400 inhibitory neurons. These neurons are connected one-to-one to the network somas, and they spike according to the Poisson distribution with the rate <italic>r</italic><sub>som</sub> = 40 Hz.</p></sec><sec id="S21"><title>Sensory Stimulus Encoding</title><p id="P58">To model how different sensory areas process stimuli, we implemented distinct encoding strategies for the visual and auditory inputs (<xref ref-type="fig" rid="F5">Fig. 5</xref>). In both cases, stimuli were represented by assemblies of 20 co-active neurons, but with different patterns of overlap between similar stimuli.</p></sec><sec id="S22"><title>Visual Stimulus Processing</title><p id="P59">We selected four characters from the Extended MNIST (EMNIST) dataset (<xref ref-type="bibr" rid="R18">Cohen et al., 2017</xref>): two pairs of visually similar characters (“0”/’?” and “1”/”l”). For each character, we randomly selected 15 different sample images. These grayscale images (28×28 pixels) were processed using Gabor filtering to simulate early visual processing. We created a bank of 400 Gabor filters parameterized by: <disp-formula id="FD19"><mml:math id="M46"><mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>f</mml:mi><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula> where (<italic>x′, y′</italic>) = (<italic>x</italic> cos <italic>θ</italic> + <italic>y</italic> sin <italic>θ</italic>, –<italic>x</italic> sin <italic>θ</italic> + <italic>y</italic> cos <italic>θ</italic>) represents coordinates rotated by angle <italic>θ.</italic> We systematically varied the Gabor parameters: orientation <italic>θ</italic> ∈ 0°, 45°, 90°, 135°, phase <italic>ϕ</italic> ∈ 0°, 180°, frequency <italic>f</italic> ∈ 1,1.5, and scale <italic>σ</italic> ∈ 0.2,0.4.</p><p id="P60">Each Gabor filter was assigned a random center position (<italic>x</italic>, <italic>y</italic>) in the grid, while ensuring equal coverage of the whole image. The response of each filter was computed by convolving the image with the filter. To generate a sparse input pattern for a given stimuli, we assigned all inputs a base value of 0.1 Hz to mimic the baseline firing rate. The ids of the weakest filter responses were set to 0 Hz, while strongest filter responses were scaled to keep their proportions but achieve an average value of 10 Hz (values are clipped if they exceed 14 Hz during this process). The main purpose of this filtering process was to preserve the visual similarity between character pairs, resulting in partially overlapping assembly activations for similar characters (e.g., “0” and “O”), while generating similar input patterns that were used during the rest of the paper.</p></sec><sec id="S23"><title>Auditory Stimulus Processing</title><p id="P61">For the auditory inputs, we implemented a direct mapping where each stimulus type was associated with a distinct subset of neurons. This mapping ensured that phonetically distinct stimuli activated non-overlapping sets of neurons in the auditory sensory area. Each auditory index corresponded to a specific group of 20 consecutive neurons in the feedforward population (e.g., neurons 0-19 for “1”, 20-39 for “l”, etc.). By implementing these contrasting encoding strategies, we captured a key feature of biological sensory processing: visual stimuli with similar appearances produce overlapping neural representations, while auditorily distinct stimuli (different phonemes) activate largely separate neural populations.</p></sec></sec><sec id="S24"><title>Analysis</title><sec id="S25"><title>Identification of Neuronal Assemblies from Firing Rates and Synaptic Connectivity</title><p id="P62">To extract neurons that make up a neural assembly, we employed a two-stage clustering procedure that first uses firing rate statistics and subsequently refines the selection based on recurrent synaptic connectivity. First, we applied k-means clustering with k=2 to partition the neurons into two groups. The cluster whose centroid had the highest value was identified as the high firing rate group that forms the initial candidate assembly. In addition, we included the 10 neurons with the highest firing rate of the low firing rate group. Then, we applied k-means clustering with k=2 on the weight matrix that was reduced to contain only the candidate assembly neurons but included the summed input weights as an additional feature. Finally, we computed the mean intra-cluster connectivity for each cluster, and identified the cluster that exhibits the higher mean connectivity as the neuronal assembly.</p></sec><sec id="S26"><title>Theoretical Calculation of Assembly Storage Capacity</title><p id="P63">In our model, the feedforward projections into an area are randomized with probability p=0.081. With 400 neurons and a target assembly size of 20, there are at max 20 non-overlapping inputs possible.</p><p id="P64">Statistically, it is unlikely that these non-overlapping inputs generate non-overlapping assemblies in the target area. We compared the assembly sizes of subsequently imprinted assemblies in a single context (<xref ref-type="fig" rid="F3">Fig. 3E</xref>) with the theoretically determined sizes purely based on the statistics of how neurons in the target area are recruited.</p><p id="P65">We therefore repeated the following procedure 1000 times to sample the possible distributions of assembly sizes: <list list-type="order" id="L2"><list-item><p id="P66">Initialize 20 empty assembly groups <italic>A</italic><sub>1</sub>, <italic>A</italic><sub>2</sub>,…, <italic>A</italic><sub>20</sub>.</p></list-item><list-item><p id="P67">Sample an assembly size <italic>s<sub>i</sub></italic> from the empirical distribution of assembly sizes as observed in 500 network simulations with recurrent inhibition enabled (see <xref ref-type="supplementary-material" rid="SD1">Fig. S2E</xref>).</p></list-item><list-item><p id="P68">Randomly select <italic>s<sub>i</sub></italic> neurons from the pool of all neurons to form assembly <italic>A<sub>i</sub></italic>.</p></list-item><list-item><p id="P69">Remove neurons from assemblies <italic>A<sub>j</sub></italic> with <italic>j</italic> &lt; <italic>i</italic> for any neuron that appears in the new assembly <italic>A<sub>i</sub></italic>, modeling the recruitment of neurons from earlier assemblies.</p></list-item><list-item><p id="P70">Calculate the cumulative number of neurons <inline-formula><mml:math id="M47"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>Σ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for each <italic>i</italic> from 1 to 20, where |<italic>A<sub>j</sub></italic>| denotes the number of neurons in assembly <italic>A<sub>j</sub></italic> after all overlaps are accounted for.</p></list-item></list></p><p id="P71">This procedure captures the statistical properties of assembly formation in our network, particularly the effect of new assemblies recruiting neurons from previously formed assemblies. The calculated theoretical curve closely matches the results observed in the full network simulations (<xref ref-type="fig" rid="F3">Fig. 3E</xref>), validating our understanding of the mechanisms that constrain assembly storage capacity.</p></sec></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Material</label><media xlink:href="EMS207568-supplement-Supplementary_Material.pdf" mimetype="application" mime-subtype="pdf" id="d62aAcEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S27"><title>Acknowledgements</title><p>We thank all members of the ‘Computation in Neural Circuits’ group and Dylan Festa, Filippo Kiessler, Satchal Postlewaite, and Joey Stawyskyj for useful discussions and comments on the manuscript. This project has received funding from the European Research Council (ERC StG NeuroDevo, Grant agreement No. 804824 to JG), the Human Frontier Science Program Organization (RGP0062/2021 to JG), and a Human Frontier Science Program Postdoctoral Fellowship (LT0005/2024-L to CM).</p></ack><sec id="S28" sec-type="data-availability"><title>Code availability</title><p id="P72">All numerical computations were performed in Python using the Brian 2 simulator (<xref ref-type="bibr" rid="R96">Stimberget al., 2019</xref>). The code will be available on <ext-link ext-link-type="uri" xlink:href="https://github.com/comp-neural-circuits/contextual_gating">https://github.com/comp-neural-circuits/contextual_gating</ext-link> after publication.</p></sec><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Acharya</surname><given-names>J</given-names></name><name><surname>Basu</surname><given-names>A</given-names></name><name><surname>Legenstein</surname><given-names>R</given-names></name><name><surname>Limbacher</surname><given-names>T</given-names></name><name><surname>Poirazi</surname><given-names>P</given-names></name><name><surname>Wu</surname><given-names>X</given-names></name></person-group><article-title>Dendritic Computing: Branching Deeper into Machine Learning</article-title><source>Neuroscience</source><year>2022</year><volume>489</volume><fpage>275</fpage><lpage>289</lpage><pub-id pub-id-type="pmid">34656706</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adesnik</surname><given-names>H</given-names></name><name><surname>Bruns</surname><given-names>W</given-names></name><name><surname>Taniguchi</surname><given-names>H</given-names></name><name><surname>Huang</surname><given-names>ZJ</given-names></name><name><surname>Scanziani</surname><given-names>M</given-names></name></person-group><article-title>A neural circuit for spatial summation in visual cortex</article-title><source>Nature</source><year>2012</year><volume>490</volume><issue>7419</issue><fpage>226</fpage><lpage>230</lpage><pub-id pub-id-type="doi">10.1038/nature11526</pub-id><pub-id pub-id-type="pmcid">PMC3621107</pub-id><pub-id pub-id-type="pmid">23060193</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barron</surname><given-names>HC</given-names></name><name><surname>Vogels</surname><given-names>TP</given-names></name><name><surname>Behrens</surname><given-names>TE</given-names></name><name><surname>Ramaswami</surname><given-names>M</given-names></name></person-group><article-title>Inhibitory engrams in perception and memory</article-title><source>Proceedings of the National Academy of Sciences</source><year>2017</year><volume>114</volume><issue>26</issue><fpage>6666</fpage><lpage>6674</lpage><pub-id pub-id-type="doi">10.1073/pnas.1701812114</pub-id><pub-id pub-id-type="pmcid">PMC5495250</pub-id><pub-id pub-id-type="pmid">28611219</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barron</surname><given-names>HC</given-names></name><name><surname>Vogels</surname><given-names>TP</given-names></name><name><surname>Emir</surname><given-names>UE</given-names></name><name><surname>Makin</surname><given-names>TR</given-names></name><name><surname>O’Shea</surname><given-names>J</given-names></name><name><surname>Clare</surname><given-names>S</given-names></name><name><surname>Jbabdi</surname><given-names>S</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name></person-group><article-title>Unmasking Latent In-hibitory Connections in Human Cortex to Reveal Dormant Cortical Memories</article-title><source>Neuron</source><year>2016</year><volume>90</volume><issue>1</issue><fpage>191</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.02.031</pub-id><pub-id pub-id-type="pmcid">PMC4826438</pub-id><pub-id pub-id-type="pmid">26996082</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bergoin</surname><given-names>R</given-names></name><name><surname>Torcini</surname><given-names>A</given-names></name><name><surname>Deco</surname><given-names>G</given-names></name><name><surname>Quoy</surname><given-names>M</given-names></name><name><surname>Zamora-López</surname><given-names>G</given-names></name></person-group><article-title>Emergence and maintenance of modularity in neural networks with Hebbian and anti-Hebbian inhibitory STDP</article-title><source>PLOS Computational Biology</source><year>2025</year><volume>21</volume><issue>4</issue><elocation-id>e1012973</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1012973</pub-id><pub-id pub-id-type="pmcid">PMC12054933</pub-id><pub-id pub-id-type="pmid">40262082</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bhaduri</surname><given-names>A</given-names></name><name><surname>Banerjee</surname><given-names>A</given-names></name><name><surname>Roy</surname><given-names>S</given-names></name><name><surname>Kar</surname><given-names>S</given-names></name><name><surname>Basu</surname><given-names>A</given-names></name></person-group><article-title>Spiking Neural Classifier with Lumped Dendritic Nonlinearity and Binary Synapses: A Current Mode VLSI Implementation and Analysis</article-title><source>Neural Computation</source><year>2018</year><volume>30</volume><issue>3</issue><fpage>723</fpage><lpage>760</lpage><pub-id pub-id-type="pmid">29220305</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bilash</surname><given-names>OM</given-names></name><name><surname>Chavlis</surname><given-names>S</given-names></name><name><surname>Johnson</surname><given-names>CD</given-names></name><name><surname>Poirazi</surname><given-names>P</given-names></name><name><surname>Basu</surname><given-names>J</given-names></name></person-group><article-title>Lateral entorhinal cortex inputs modulate hippocampal dendritic excitability by recruiting a local disinhibitory microcircuit</article-title><source>Cell Reports</source><year>2023</year><volume>42</volume><elocation-id>111962</elocation-id><pub-id pub-id-type="doi">10.1016/j.celrep.2022.111962</pub-id><pub-id pub-id-type="pmcid">PMC10337264</pub-id><pub-id pub-id-type="pmid">36640337</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bono</surname><given-names>J</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name></person-group><article-title>Modeling somatic and dendritic spike mediated plasticity at the single neuron and network level</article-title><source>Nature Communications</source><year>2017</year><volume>8</volume><issue>1</issue><pub-id pub-id-type="doi">10.1038/s41467-017-00740-z</pub-id><pub-id pub-id-type="pmcid">PMC5615054</pub-id><pub-id pub-id-type="pmid">28951585</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bos</surname><given-names>H</given-names></name><name><surname>Miehl</surname><given-names>C</given-names></name><name><surname>Oswald</surname><given-names>AM</given-names></name><name><surname>Doiron</surname><given-names>B</given-names></name></person-group><article-title>Untangling stability and gain modulation in cortical circuits with multiple interneuron classes</article-title><source>eLife</source><year>2025</year><volume>13</volume><elocation-id>RP99808</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.99808</pub-id><pub-id pub-id-type="pmcid">PMC12043317</pub-id><pub-id pub-id-type="pmid">40304591</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname><given-names>G</given-names></name></person-group><article-title>Neural syntax: cell assemblies, synapsembles, and readers</article-title><source>Neuron</source><year>2010</year><volume>68</volume><fpage>362</fpage><lpage>385</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.09.023</pub-id><pub-id pub-id-type="pmcid">PMC3005627</pub-id><pub-id pub-id-type="pmid">21040841</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Byrne</surname><given-names>E</given-names></name><name><surname>Huyck</surname><given-names>CR</given-names></name></person-group><article-title>Processing with cell assemblies</article-title><source>Neurocomputing</source><year>2010</year><volume>74</volume><issue>1-3</issue><fpage>76</fpage><lpage>83</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2009.09.024</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Canto-Bustos</surname><given-names>M</given-names></name><name><surname>Friason</surname><given-names>FK</given-names></name><name><surname>Bassi</surname><given-names>C</given-names></name><name><surname>Oswald</surname><given-names>AMM</given-names></name></person-group><article-title>Disinhibitory circuitry gates associative synaptic plasticity in olfactory cortex</article-title><source>The Journal of Neuroscience</source><year>2022</year><volume>42</volume><issue>14</issue><fpage>2942</fpage><lpage>2950</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1369-21.2021</pub-id><pub-id pub-id-type="pmcid">PMC8985865</pub-id><pub-id pub-id-type="pmid">35181596</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chavlis</surname><given-names>S</given-names></name><name><surname>Poirazi</surname><given-names>P</given-names></name></person-group><article-title>Drawing inspiration from biological dendrites to empower artificial neural networks</article-title><source>Current Opinion in Neurobiology</source><year>2021</year><volume>70</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="pmid">34087540</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chavlis</surname><given-names>S</given-names></name><name><surname>Poirazi</surname><given-names>P</given-names></name></person-group><article-title>Dendrites endow artificial neural networks with accurate, robust and parameter-efficient learning</article-title><source>Nature Communications</source><year>2025</year><volume>16</volume><issue>1</issue><elocation-id>943</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-025-56297-9</pub-id><pub-id pub-id-type="pmcid">PMC11754790</pub-id><pub-id pub-id-type="pmid">39843414</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chistiakova</surname><given-names>M</given-names></name><name><surname>Bannon</surname><given-names>NM</given-names></name><name><surname>Chen</surname><given-names>JY</given-names></name><name><surname>Bazhenov</surname><given-names>M</given-names></name><name><surname>Volgushev</surname><given-names>M</given-names></name></person-group><article-title>Homeostatic role of heterosynaptic plasticity: models and experiments</article-title><source>Frontiers in Computational Neuroscience</source><year>2015</year><fpage>9</fpage><pub-id pub-id-type="doi">10.3389/fncom.2015.00089</pub-id><pub-id pub-id-type="pmcid">PMC4500102</pub-id><pub-id pub-id-type="pmid">26217218</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichon</surname><given-names>J</given-names></name><name><surname>Gan</surname><given-names>WB</given-names></name></person-group><article-title>Branch-specific dendritic Ca2+ spikes cause persistent synaptic plasticity</article-title><source>Nature</source><year>2015</year><volume>520</volume><issue>7546</issue><fpage>180</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1038/nature14251</pub-id><pub-id pub-id-type="pmcid">PMC4476301</pub-id><pub-id pub-id-type="pmid">25822789</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clopath</surname><given-names>C</given-names></name><name><surname>Büsing</surname><given-names>L</given-names></name><name><surname>Vasilaki</surname><given-names>E</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><article-title>Connectivity reflects coding: a model of voltage-based STDP with homeostasis</article-title><source>Nature Neuroscience</source><year>2010</year><volume>13</volume><issue>3</issue><fpage>344</fpage><lpage>352</lpage><pub-id pub-id-type="pmid">20098420</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>G</given-names></name><name><surname>Afshar</surname><given-names>S</given-names></name><name><surname>Tapson</surname><given-names>J</given-names></name><name><surname>Van Schaik</surname><given-names>A</given-names></name></person-group><source>EMNIST: Extending MNIST to handwritten letters</source><conf-name>2017 International Joint Conference on Neural Networks (IJCNN)</conf-name><year>2017</year><fpage>2921</fpage><lpage>2926</lpage><pub-id pub-id-type="doi">10.1109/IJCNN.2017.7966217</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davenport</surname><given-names>CM</given-names></name><name><surname>Rajappa</surname><given-names>R</given-names></name><name><surname>Katchan</surname><given-names>L</given-names></name><name><surname>Taylor</surname><given-names>CR</given-names></name><name><surname>Tsai</surname><given-names>Mc</given-names></name><name><surname>Smith</surname><given-names>CM</given-names></name><name><surname>deJong</surname><given-names>JW</given-names></name><name><surname>Arnold</surname><given-names>DB</given-names></name><name><surname>Lammel</surname><given-names>S</given-names></name><name><surname>Kramer</surname><given-names>RH</given-names></name></person-group><article-title>Relocation of an Extrasynaptic GABAA Receptor to Inhibitory Synapses Freezes Excitatory Synaptic Strength and Preserves Memory</article-title><source>Neuron</source><year>2021</year><fpage>109</fpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.09.037</pub-id><pub-id pub-id-type="pmcid">PMC7790995</pub-id><pub-id pub-id-type="pmid">33096025</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dipoppa</surname><given-names>M</given-names></name><name><surname>Ranson</surname><given-names>A</given-names></name><name><surname>Krumin</surname><given-names>M</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><article-title>Vision and Locomotion Shape the Interactions between Neuron Types in Mouse Visual Cortex</article-title><source>Neuron</source><year>2018</year><volume>98</volume><issue>3</issue><fpage>602</fpage><lpage>615</lpage><elocation-id>e8</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuron.2018.03.037</pub-id><pub-id pub-id-type="pmcid">PMC5946730</pub-id><pub-id pub-id-type="pmid">29656873</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eichenbaum</surname><given-names>H</given-names></name></person-group><article-title>Barlow versus Hebb: When is it time to abandon the notion of feature detectors and adopt the cell assembly as the unit of cognition?</article-title><source>Neuroscience Letters</source><year>2018</year><volume>680</volume><fpage>88</fpage><lpage>93</lpage><pub-id pub-id-type="doi">10.1016/j.neulet.2017.04.006</pub-id><pub-id pub-id-type="pmcid">PMC5628090</pub-id><pub-id pub-id-type="pmid">28389238</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>El-Boustani</surname><given-names>S</given-names></name><name><surname>Ip</surname><given-names>JPK</given-names></name><name><surname>Breton-Provencher</surname><given-names>V</given-names></name><name><surname>Knott</surname><given-names>GW</given-names></name><name><surname>Okuno</surname><given-names>H</given-names></name><name><surname>Bito</surname><given-names>H</given-names></name><name><surname>Sur</surname><given-names>M</given-names></name></person-group><article-title>Locally coordinated synaptic plasticity of visual cortex neurons in vivo</article-title><source>Science</source><year>2018</year><volume>360</volume><issue>6395</issue><fpage>1349</fpage><lpage>1354</lpage><pub-id pub-id-type="doi">10.1126/science.aao0862</pub-id><pub-id pub-id-type="pmcid">PMC6366621</pub-id><pub-id pub-id-type="pmid">29930137</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fauth</surname><given-names>MJ</given-names></name><name><surname>van Rossum</surname><given-names>MCW</given-names></name></person-group><article-title>Self-organized reactivation maintains and reinforces memories despite synaptic turnover</article-title><source>eLife</source><year>2019</year><volume>8</volume><elocation-id>e43717</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.43717</pub-id><pub-id pub-id-type="pmcid">PMC6546393</pub-id><pub-id pub-id-type="pmid">31074745</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Festa</surname><given-names>D</given-names></name><name><surname>Cusseddu</surname><given-names>C</given-names></name><name><surname>Gjorgjieva</surname><given-names>J</given-names></name></person-group><article-title>Structured stabilization in recurrent neural circuits through inhibitory synaptic plasticity</article-title><source>bioRxiv</source><year>2024</year><pub-id pub-id-type="doi">10.1101/2024.10.12.618014</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>Y</given-names></name><name><surname>Tucciarone</surname><given-names>JM</given-names></name><name><surname>Espinosa</surname><given-names>JS</given-names></name><name><surname>Sheng</surname><given-names>N</given-names></name><name><surname>Darcy</surname><given-names>DP</given-names></name><name><surname>Nicoll</surname><given-names>RA</given-names></name><name><surname>Huang</surname><given-names>ZJ</given-names></name><name><surname>Stryker</surname><given-names>MP</given-names></name></person-group><article-title>A cortical circuit for gain control by behavioral state</article-title><source>Cell</source><year>2014</year><volume>156</volume><issue>6</issue><fpage>1139</fpage><lpage>1152</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2014.01.050</pub-id><pub-id pub-id-type="pmcid">PMC4041382</pub-id><pub-id pub-id-type="pmid">24630718</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Galloni</surname><given-names>AR</given-names></name><name><surname>Peddada</surname><given-names>A</given-names></name><name><surname>Chennawar</surname><given-names>Y</given-names></name><name><surname>Milstein</surname><given-names>AD</given-names></name></person-group><article-title>Cellular and subcellular specialization enables biology-constrained deep learning</article-title><source>bioRxiv</source><year>2025</year><pub-id pub-id-type="doi">10.1101/2025.05.22.655599</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gambino</surname><given-names>F</given-names></name><name><surname>Pagès</surname><given-names>S</given-names></name><name><surname>Kehayas</surname><given-names>V</given-names></name><name><surname>Baptista</surname><given-names>D</given-names></name><name><surname>Tatti</surname><given-names>R</given-names></name><name><surname>Carleton</surname><given-names>A</given-names></name><name><surname>Holtmaat</surname><given-names>A</given-names></name></person-group><article-title>Sensory-evoked LTP driven by dendritic plateau potentials in vivo</article-title><source>Nature</source><year>2014</year><volume>515</volume><issue>7525</issue><fpage>116</fpage><lpage>119</lpage><pub-id pub-id-type="pmid">25174710</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garcia Del Molino</surname><given-names>LC</given-names></name><name><surname>Yang</surname><given-names>GR</given-names></name><name><surname>Mejias</surname><given-names>JF</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><article-title>Paradoxical response reversal of top-down modulation in cortical circuits with three interneuron types</article-title><source>eLife</source><year>2017</year><volume>6</volume><elocation-id>e29742</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.29742</pub-id><pub-id pub-id-type="pmcid">PMC5777826</pub-id><pub-id pub-id-type="pmid">29256863</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gentet</surname><given-names>LJ</given-names></name><name><surname>Kremer</surname><given-names>Y</given-names></name><name><surname>Taniguchi</surname><given-names>H</given-names></name><name><surname>Huang</surname><given-names>ZJ</given-names></name><name><surname>Staiger</surname><given-names>JF</given-names></name><name><surname>Petersen</surname><given-names>CCH</given-names></name></person-group><article-title>Unique functional properties of somatostatin-expressing GABAergic neurons in mouse barrel cortex</article-title><source>Nature Neuroscience</source><year>2012</year><volume>15</volume><issue>4</issue><fpage>607</fpage><lpage>612</lpage><pub-id pub-id-type="pmid">22366760</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gidon</surname><given-names>A</given-names></name><name><surname>Segev</surname><given-names>I</given-names></name></person-group><article-title>Principles Governing the Operation of Synaptic Inhibition in Dendrites</article-title><source>Neuron</source><year>2012</year><volume>75</volume><fpage>330</fpage><lpage>341</lpage><pub-id pub-id-type="pmid">22841317</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Golding</surname><given-names>NL</given-names></name><name><surname>Staff</surname><given-names>NP</given-names></name><name><surname>Spruston</surname><given-names>N</given-names></name></person-group><article-title>Dendritic spikes as a mechanism for cooperative long-term potentiation</article-title><source>Nature</source><year>2002</year><volume>418</volume><issue>6895</issue><fpage>326</fpage><lpage>331</lpage><pub-id pub-id-type="pmid">12124625</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Greedy</surname><given-names>W</given-names></name><name><surname>Zhu</surname><given-names>HW</given-names></name><name><surname>Pemberton</surname><given-names>J</given-names></name><name><surname>Mellor</surname><given-names>J</given-names></name><name><surname>Ponte Costa</surname><given-names>R</given-names></name></person-group><source>Single-phase deep learning in cortico-cortical networks</source><conf-name>36th Conference on Neural Information Processing Systems (NeurIPS)</conf-name><year>2022</year><comment><ext-link ext-link-type="uri" xlink:href="https://proceedings.neurips.cc/paper_files/paper/2022/file/99088dffd5eab0babebcda4bc58bbcea-Paper-Conference.pdf">https://proceedings.neurips.cc/paper_files/paper/2022/file/99088dffd5eab0babebcda4bc58bbcea-Paper-Conference.pdf</ext-link></comment></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grewal</surname><given-names>K</given-names></name><name><surname>Forest</surname><given-names>J</given-names></name><name><surname>Cohen</surname><given-names>BP</given-names></name><name><surname>Ahmad</surname><given-names>S</given-names></name></person-group><article-title>Going Beyond the Point Neuron: Active Dendrites and Sparse Representations for Continual Learning</article-title><source>bioRxiv</source><year>2021</year><pub-id pub-id-type="doi">10.1101/2021.10.25.465651</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guerguiev</surname><given-names>J</given-names></name><name><surname>Lillicrap</surname><given-names>TP</given-names></name><name><surname>Richards</surname><given-names>BA</given-names></name></person-group><article-title>Towards deep learning with segregated dendrites</article-title><source>eLife</source><year>2017</year><volume>6</volume><elocation-id>e22901</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.22901</pub-id><pub-id pub-id-type="pmcid">PMC5716677</pub-id><pub-id pub-id-type="pmid">29205151</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guzman</surname><given-names>SJ</given-names></name><name><surname>Schlögl</surname><given-names>A</given-names></name><name><surname>Frotscher</surname><given-names>M</given-names></name><name><surname>Jonas</surname><given-names>P</given-names></name></person-group><article-title>Synaptic mechanisms of pattern completion in the hippocampal CA3 network</article-title><source>Science</source><year>2016</year><volume>353</volume><issue>6304</issue><fpage>1117</fpage><lpage>1123</lpage><pub-id pub-id-type="pmid">27609885</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hadsell</surname><given-names>R</given-names></name><name><surname>Rao</surname><given-names>D</given-names></name><name><surname>Rusu</surname><given-names>AA</given-names></name><name><surname>Pascanu</surname><given-names>R</given-names></name></person-group><article-title>Embracing Change: Continual Learning in Deep Neural Networks</article-title><source>Trends in Cognitive Sciences</source><year>2020</year><volume>24</volume><issue>12</issue><fpage>1028</fpage><lpage>1040</lpage><pub-id pub-id-type="pmid">33158755</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>JA</given-names></name><name><surname>Mihalas</surname><given-names>S</given-names></name><name><surname>Hirokawa</surname><given-names>KE</given-names></name><name><surname>Whitesell</surname><given-names>JD</given-names></name><name><surname>Choi</surname><given-names>H</given-names></name><name><surname>Bernard</surname><given-names>A</given-names></name><name><surname>Bohn</surname><given-names>P</given-names></name><name><surname>Caldejon</surname><given-names>S</given-names></name><name><surname>Casal</surname><given-names>L</given-names></name><name><surname>Cho</surname><given-names>A</given-names></name><name><surname>Feiner</surname><given-names>A</given-names></name><etal/></person-group><article-title>Hierarchical organization of cortical and thalamic connectivity</article-title><source>Nature</source><year>2019</year><fpage>575</fpage><pub-id pub-id-type="doi">10.1038/s41586-019-1716-z</pub-id><pub-id pub-id-type="pmcid">PMC8433044</pub-id><pub-id pub-id-type="pmid">31666704</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herpich</surname><given-names>J</given-names></name><name><surname>Tetzlaff</surname><given-names>C</given-names></name></person-group><article-title>Principles underlying the input-dependent formation and organization of memories</article-title><source>Network Neuroscience</source><year>2019</year><volume>3</volume><issue>2</issue><fpage>606</fpage><lpage>634</lpage><pub-id pub-id-type="doi">10.1162/netn_a_00086</pub-id><pub-id pub-id-type="pmcid">PMC6542621</pub-id><pub-id pub-id-type="pmid">31157312</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holtmaat</surname><given-names>A</given-names></name><name><surname>Caroni</surname><given-names>P</given-names></name></person-group><article-title>Functional and structural underpinnings of neuronal assemblyformation in learning</article-title><source>Nature Neuroscience</source><year>2016</year><volume>19</volume><issue>11</issue><pub-id pub-id-type="pmid">27749830</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopfield</surname><given-names>JJ</given-names></name></person-group><article-title>Neural networks and physical systems with emergent collective computational abilities</article-title><source>Proceedings of the National Academy of Sciences</source><year>1982</year><volume>79</volume><fpage>2554</fpage><lpage>2558</lpage><pub-id pub-id-type="doi">10.1073/pnas.79.8.2554</pub-id><pub-id pub-id-type="pmcid">PMC346238</pub-id><pub-id pub-id-type="pmid">6953413</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huyck</surname><given-names>CR</given-names></name><name><surname>Passmore</surname><given-names>PJ</given-names></name></person-group><article-title>A review of cell assemblies</article-title><source>Biological Cybernetics</source><year>2013</year><volume>107</volume><issue>3</issue><fpage>263</fpage><lpage>288</lpage><pub-id pub-id-type="pmid">23559034</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iyer</surname><given-names>A</given-names></name><name><surname>Grewal</surname><given-names>K</given-names></name><name><surname>Velu</surname><given-names>A</given-names></name><name><surname>Souza</surname><given-names>LO</given-names></name><name><surname>Forest</surname><given-names>J</given-names></name><name><surname>Ahmad</surname><given-names>S</given-names></name></person-group><article-title>Avoiding Catastrophe: Active Dendrites Enable Multi-Task Learning in Dynamic Environments</article-title><source>Frontiers in Neurorobotics</source><year>2022</year><volume>16</volume><elocation-id>846219</elocation-id><pub-id pub-id-type="doi">10.3389/fnbot.2022.846219</pub-id><pub-id pub-id-type="pmcid">PMC9100780</pub-id><pub-id pub-id-type="pmid">35574225</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jedlicka</surname><given-names>P</given-names></name><name><surname>Tomko</surname><given-names>M</given-names></name><name><surname>Robins</surname><given-names>A</given-names></name><name><surname>Abraham</surname><given-names>WC</given-names></name></person-group><article-title>Contributions by metaplasticity to solving the Catastrophic Forgetting Problem</article-title><source>Trends in Neurosciences</source><year>2022</year><volume>45</volume><issue>9</issue><fpage>656</fpage><lpage>666</lpage><pub-id pub-id-type="pmid">35798611</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jia</surname><given-names>H</given-names></name><name><surname>Rochefort</surname><given-names>NL</given-names></name><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Konnerth</surname><given-names>A</given-names></name></person-group><article-title>Dendritic organization of sensory input to cortical neurons in vivo</article-title><source>Nature</source><year>2010</year><month>Apr</month><volume>464</volume><issue>7293</issue><fpage>1307</fpage><lpage>1312</lpage><pub-id pub-id-type="pmid">20428163</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>IS</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name></person-group><article-title>Can Single Neurons Solve MNIST?The Computational Power of Biological Dendritic Trees</article-title><source>arXiv</source><year>2020</year><comment><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2009.01269">http://arxiv.org/abs/2009.01269</ext-link></comment></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Josselyn</surname><given-names>SA</given-names></name><name><surname>Tonegawa</surname><given-names>S</given-names></name></person-group><article-title>Memory engrams: Recalling the past and imagining the future</article-title><source>Science</source><year>2020</year><volume>367</volume><issue>6473</issue><elocation-id>eaaw4325</elocation-id><pub-id pub-id-type="doi">10.1126/science.aaw4325</pub-id><pub-id pub-id-type="pmcid">PMC7577560</pub-id><pub-id pub-id-type="pmid">31896692</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kastellakis</surname><given-names>G</given-names></name><name><surname>Silva</surname><given-names>AJ</given-names></name><name><surname>Poirazi</surname><given-names>P</given-names></name></person-group><article-title>Linking Memories across Time via Neuronal and Dendritic Overlaps in Model Neurons with Active Dendrites</article-title><source>Cell Reports</source><year>2016</year><volume>17</volume><issue>6</issue><fpage>1491</fpage><lpage>1504</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2016.10.015</pub-id><pub-id pub-id-type="pmcid">PMC5149530</pub-id><pub-id pub-id-type="pmid">27806290</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kastellakis</surname><given-names>G</given-names></name><name><surname>Tasciotti</surname><given-names>S</given-names></name><name><surname>Pandi</surname><given-names>I</given-names></name><name><surname>Poirazi</surname><given-names>P</given-names></name></person-group><article-title>The dendritic engram</article-title><source>Frontiers in Behavioral Neuroscience</source><year>2023</year><volume>17</volume><elocation-id>1212139</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2023.1212139</pub-id><pub-id pub-id-type="pmcid">PMC10412934</pub-id><pub-id pub-id-type="pmid">37576932</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kerlin</surname><given-names>A</given-names></name><name><surname>Mohar</surname><given-names>B</given-names></name><name><surname>Flickinger</surname><given-names>D</given-names></name><name><surname>MacLennan</surname><given-names>BJ</given-names></name><name><surname>Dean</surname><given-names>MB</given-names></name><name><surname>Davis</surname><given-names>C</given-names></name><name><surname>Spruston</surname><given-names>N</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><article-title>Functional clustering of dendritic activity during decision-making</article-title><source>eLife</source><year>2019</year><volume>8</volume><elocation-id>e46966</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.46966</pub-id><pub-id pub-id-type="pmcid">PMC6821494</pub-id><pub-id pub-id-type="pmid">31663507</pub-id></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kirkpatrick</surname><given-names>J</given-names></name><name><surname>Pascanu</surname><given-names>R</given-names></name><name><surname>Rabinowitz</surname><given-names>N</given-names></name><name><surname>Veness</surname><given-names>J</given-names></name><name><surname>Desjardins</surname><given-names>G</given-names></name><name><surname>Rusu</surname><given-names>AA</given-names></name><name><surname>Milan</surname><given-names>K</given-names></name><name><surname>Quan</surname><given-names>J</given-names></name><name><surname>Ramalho</surname><given-names>T</given-names></name><name><surname>Grabska-Barwinska</surname><given-names>A</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><etal/></person-group><article-title>Overcoming catastrophic forgetting in neural networks</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2017</year><volume>114</volume><issue>13</issue><fpage>3521</fpage><lpage>3526</lpage><pub-id pub-id-type="doi">10.1073/pnas.1611835114</pub-id><pub-id pub-id-type="pmcid">PMC5380101</pub-id><pub-id pub-id-type="pmid">28292907</pub-id></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krabbe</surname><given-names>S</given-names></name><name><surname>Paradiso</surname><given-names>E</given-names></name><name><surname>D’Aquin</surname><given-names>S</given-names></name><name><surname>Bitterman</surname><given-names>Y</given-names></name><name><surname>Courtin</surname><given-names>J</given-names></name><name><surname>Xu</surname><given-names>C</given-names></name><name><surname>Yonehara</surname><given-names>K</given-names></name><name><surname>Markovic</surname><given-names>M</given-names></name><name><surname>Müller</surname><given-names>C</given-names></name><name><surname>Eichlisberger</surname><given-names>T</given-names></name><name><surname>Gründemann</surname><given-names>J</given-names></name><name><surname>Ferraguti</surname><given-names>F</given-names></name><name><surname>Lüthi</surname><given-names>A</given-names></name></person-group><article-title>Adaptive disinhibitory gating by VIP interneurons permits associative learning</article-title><source>Nature Neuroscience</source><year>2019</year><volume>22</volume><issue>11</issue><fpage>1834</fpage><lpage>1843</lpage><pub-id pub-id-type="pmid">31636447</pub-id></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krishnamurthy</surname><given-names>K</given-names></name><name><surname>Can</surname><given-names>T</given-names></name><name><surname>Schwab</surname><given-names>DJ</given-names></name></person-group><article-title>Theory of Gating in Recurrent Neural Networks</article-title><source>Physical Review X</source><year>2022</year><volume>12</volume><issue>1</issue><elocation-id>011011</elocation-id><pub-id pub-id-type="doi">10.1103/physrevx.12.011011</pub-id><pub-id pub-id-type="pmcid">PMC9762509</pub-id><pub-id pub-id-type="pmid">36545030</pub-id></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kudithipudi</surname><given-names>D</given-names></name><name><surname>Aguilar-Simon</surname><given-names>M</given-names></name><name><surname>Babb</surname><given-names>J</given-names></name><name><surname>Bazhenov</surname><given-names>M</given-names></name><name><surname>Blackiston</surname><given-names>D</given-names></name><name><surname>Bongard</surname><given-names>J</given-names></name><name><surname>Brna</surname><given-names>AP</given-names></name><name><surname>Chakravarthi Raja</surname><given-names>S</given-names></name><name><surname>Cheney</surname><given-names>N</given-names></name><name><surname>Clune</surname><given-names>J</given-names></name><name><surname>Daram</surname><given-names>A</given-names></name><etal/></person-group><article-title>Biological underpinnings for lifelong learning machines</article-title><source>Nature Machine Intelligence</source><year>2022</year><volume>4</volume><issue>3</issue><fpage>196</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1038/s42256-022-00452-0</pub-id></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kwon</surname><given-names>TM</given-names></name><name><surname>Zervakis</surname><given-names>M</given-names></name></person-group><article-title>KWTA networks and their applications</article-title><source>Multidimensional Systems and Signal Processing</source><year>1995</year><volume>6</volume><issue>4</issue><fpage>333</fpage><lpage>346</lpage><pub-id pub-id-type="doi">10.1007/BF00983559</pub-id></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lagzi</surname><given-names>F</given-names></name><name><surname>Canto Bustos</surname><given-names>M</given-names></name><name><surname>Oswald</surname><given-names>AMM</given-names></name><name><surname>Doiron</surname><given-names>B</given-names></name></person-group><article-title>Assembly formation is stabilized by Parvalbumin neurons and accelerated by Somatostatin neurons</article-title><source>bioRxiv</source><year>2021</year><pub-id pub-id-type="doi">10.1101/2021.09.06.459211</pub-id></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Legenstein</surname><given-names>R</given-names></name><name><surname>Maass</surname><given-names>W</given-names></name></person-group><article-title>Branch-specific plasticity enables self-organization of nonlinear computation in single neurons</article-title><source>Journal of Neuroscience</source><year>2011</year><volume>31</volume><issue>30</issue><fpage>10787</fpage><lpage>10802</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5684-10.2011</pub-id><pub-id pub-id-type="pmcid">PMC6623094</pub-id><pub-id pub-id-type="pmid">21795531</pub-id></element-citation></ref><ref id="R57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lehr</surname><given-names>AB</given-names></name><name><surname>Kumar</surname><given-names>A</given-names></name><name><surname>Tetzlaff</surname><given-names>C</given-names></name></person-group><article-title>Sparse clustered inhibition projects sequential activity onto unique neural subspaces</article-title><source>bioRxiv</source><year>2023</year><pub-id pub-id-type="doi">10.1101/2023.09.15.557865</pub-id></element-citation></ref><ref id="R58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litwin-Kumar</surname><given-names>A</given-names></name><name><surname>Doiron</surname><given-names>B</given-names></name></person-group><article-title>Formation and maintenance of neuronal assemblies through synaptic plasticity</article-title><source>Nature Communications</source><year>2014</year><volume>5</volume><elocation-id>5319</elocation-id><pub-id pub-id-type="pmid">25395015</pub-id></element-citation></ref><ref id="R59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lovett-Barron</surname><given-names>M</given-names></name></person-group><article-title>Learning-dependent neuronal activity across the larval zebrafish brain</article-title><source>Current Opinion in Neurobiology</source><year>2021</year><volume>67</volume><fpage>42</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2020.07.006</pub-id><pub-id pub-id-type="pmcid">PMC7907282</pub-id><pub-id pub-id-type="pmid">32861055</pub-id></element-citation></ref><ref id="R60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manz</surname><given-names>P</given-names></name><name><surname>Memmesheimer</surname><given-names>RM</given-names></name></person-group><article-title>Purely STDP-based assembly dynamics: stability, learning, overlaps, drift and aging</article-title><source>PLoS Computational Biology</source><year>2023</year><volume>19</volume><issue>4</issue><elocation-id>e1011006</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1011006</pub-id><pub-id pub-id-type="pmcid">PMC10124856</pub-id><pub-id pub-id-type="pmid">37043481</pub-id></element-citation></ref><ref id="R61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Masse</surname><given-names>NY</given-names></name><name><surname>Grant</surname><given-names>GD</given-names></name><name><surname>Freedman</surname><given-names>DJ</given-names></name></person-group><article-title>Alleviating catastrophic forgetting using contextdependent gating and synaptic stabilization</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2018</year><volume>115</volume><issue>44</issue><fpage>E104657</fpage><lpage>E104675</lpage><pub-id pub-id-type="doi">10.1073/pnas.1803839115</pub-id><pub-id pub-id-type="pmcid">PMC6217392</pub-id><pub-id pub-id-type="pmid">30315147</pub-id></element-citation></ref><ref id="R62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCloskey</surname><given-names>M</given-names></name><name><surname>Cohen</surname><given-names>NJ</given-names></name></person-group><article-title>Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem</article-title><source>Psychology of Learning and Motivation</source><year>1989</year><volume>24</volume><fpage>109</fpage><lpage>165</lpage><pub-id pub-id-type="doi">10.1016/S0079-7421(08)60536-8</pub-id></element-citation></ref><ref id="R63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mei</surname><given-names>J</given-names></name><name><surname>Muller</surname><given-names>E</given-names></name><name><surname>Ramaswamy</surname><given-names>S</given-names></name></person-group><article-title>Informing deep neural networks by multiscale principles of neuromodulatory systems</article-title><source>Trends in Neurosciences</source><year>2022</year><volume>45</volume><issue>3</issue><fpage>237</fpage><lpage>250</lpage><pub-id pub-id-type="pmid">35074219</pub-id></element-citation></ref><ref id="R64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mei</surname><given-names>J</given-names></name><name><surname>Rodriguez-Garcia</surname><given-names>A</given-names></name><name><surname>Takeuchi</surname><given-names>D</given-names></name></person-group><article-title>Improving the adaptive and continuous learning capabilities of artificial neural networks: Lessons from multi-neuromodulatory dynamics</article-title><source>arXiv</source><year>2025</year><pub-id pub-id-type="doi">10.48550/arXiv.2501.06762</pub-id></element-citation></ref><ref id="R65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miehl</surname><given-names>C</given-names></name><name><surname>Gjorgjieva</surname><given-names>J</given-names></name></person-group><article-title>Stability and learning in excitatory synapses by nonlinear inhibitory plasticity</article-title><source>PLoS Computational Biology</source><year>2022</year><volume>18</volume><issue>12</issue><elocation-id>e1010682</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1010682</pub-id><pub-id pub-id-type="pmcid">PMC9718420</pub-id><pub-id pub-id-type="pmid">36459503</pub-id></element-citation></ref><ref id="R66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miehl</surname><given-names>C</given-names></name><name><surname>Onasch</surname><given-names>S</given-names></name><name><surname>Festa</surname><given-names>D</given-names></name><name><surname>Gjorgjieva</surname><given-names>J</given-names></name></person-group><article-title>Formation and computational implications of assemblies in neural circuits</article-title><source>Journal of Physiology</source><year>2023</year><volume>601</volume><fpage>3071</fpage><lpage>3090</lpage><pub-id pub-id-type="pmid">36068723</pub-id></element-citation></ref><ref id="R67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montangie</surname><given-names>L</given-names></name><name><surname>Miehl</surname><given-names>C</given-names></name><name><surname>Gjorgjieva</surname><given-names>J</given-names></name></person-group><article-title>Autonomous emergence of connectivity assemblies via spike triplet interactions</article-title><source>PLoS Computational Biology</source><year>2020</year><volume>16</volume><issue>5</issue><elocation-id>e1007835</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007835</pub-id><pub-id pub-id-type="pmcid">PMC7239496</pub-id><pub-id pub-id-type="pmid">32384081</pub-id></element-citation></ref><ref id="R68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>JJ</given-names></name><name><surname>Robert</surname><given-names>V</given-names></name><name><surname>Rashid</surname><given-names>SK</given-names></name><name><surname>Basu</surname><given-names>J</given-names></name></person-group><article-title>Assessing Local and Branch-specific Activity in Dendrites</article-title><source>Neuroscience</source><year>2022</year><volume>489</volume><fpage>143</fpage><lpage>164</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2021.10.022</pub-id><pub-id pub-id-type="pmcid">PMC9125998</pub-id><pub-id pub-id-type="pmid">34756987</pub-id></element-citation></ref><ref id="R69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neves</surname><given-names>G</given-names></name><name><surname>Cooke</surname><given-names>SF</given-names></name><name><surname>Bliss</surname><given-names>TVP</given-names></name></person-group><article-title>Synaptic plasticity, memory and the hippocampus: a neural network approach to causality</article-title><source>Nature Reviews Neuroscience</source><year>2008</year><volume>9</volume><fpage>65</fpage><lpage>75</lpage><pub-id pub-id-type="pmid">18094707</pub-id></element-citation></ref><ref id="R70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pagkalos</surname><given-names>M</given-names></name><name><surname>Makarov</surname><given-names>R</given-names></name><name><surname>Poirazi</surname><given-names>P</given-names></name></person-group><article-title>Leveraging dendritic properties to advance machine learning and neuro-inspired computing</article-title><source>Current Opinion in Neurobiology</source><year>2024</year><volume>85</volume><elocation-id>102853</elocation-id><pub-id pub-id-type="pmid">38394956</pub-id></element-citation></ref><ref id="R71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palmigiano</surname><given-names>A</given-names></name><name><surname>Fumarola</surname><given-names>F</given-names></name><name><surname>Mossing</surname><given-names>DP</given-names></name><name><surname>Kraynyukova</surname><given-names>N</given-names></name><name><surname>Adesnik</surname><given-names>H</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name></person-group><article-title>Common rules underlying optogenetic and behavioral modulation of responses in multi-cell-type V1 circuits</article-title><source>bioRxiv</source><year>2023</year><pub-id pub-id-type="doi">10.1101/2020.11.11.378729</pub-id></element-citation></ref><ref id="R72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Papadimitriou</surname><given-names>CH</given-names></name><name><surname>Friederici</surname><given-names>AD</given-names></name></person-group><article-title>Bridging the Gap Between Neurons and Cognition Through Assemblies of Neurons</article-title><source>Neural Computation</source><year>2022</year><volume>34</volume><fpage>291</fpage><lpage>306</lpage><pub-id pub-id-type="pmid">34915560</pub-id></element-citation></ref><ref id="R73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Papadimitriou</surname><given-names>CH</given-names></name><name><surname>Vempala</surname><given-names>SS</given-names></name><name><surname>Mitropolsky</surname><given-names>D</given-names></name><name><surname>Collins</surname><given-names>M</given-names></name><name><surname>Maass</surname><given-names>W</given-names></name></person-group><article-title>Brain computation by assemblies of neurons</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2020</year><volume>117</volume><issue>25</issue><fpage>14464</fpage><lpage>14472</lpage><pub-id pub-id-type="doi">10.1073/pnas.2001893117</pub-id><pub-id pub-id-type="pmcid">PMC7322080</pub-id><pub-id pub-id-type="pmid">32518114</pub-id></element-citation></ref><ref id="R74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parisi</surname><given-names>GI</given-names></name><name><surname>Kemker</surname><given-names>R</given-names></name><name><surname>Part</surname><given-names>JL</given-names></name><name><surname>Kanan</surname><given-names>C</given-names></name><name><surname>Wermter</surname><given-names>S</given-names></name></person-group><article-title>Continual lifelong learning with neural networks: A review</article-title><source>Neural Networks</source><year>2019</year><volume>113</volume><fpage>54</fpage><lpage>71</lpage><pub-id pub-id-type="pmid">30780045</pub-id></element-citation></ref><ref id="R75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Payeur</surname><given-names>A</given-names></name><name><surname>Béïque</surname><given-names>JC</given-names></name><name><surname>Naud</surname><given-names>R</given-names></name></person-group><article-title>Classes of dendritic information processing</article-title><source>Current Opinion in Neurobiology</source><year>2019</year><volume>58</volume><fpage>78</fpage><lpage>85</lpage><pub-id pub-id-type="pmid">31419712</pub-id></element-citation></ref><ref id="R76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Payeur</surname><given-names>A</given-names></name><name><surname>Guerguiev</surname><given-names>J</given-names></name><name><surname>Zenke</surname><given-names>F</given-names></name><name><surname>Richards</surname><given-names>BA</given-names></name><name><surname>Naud</surname><given-names>R</given-names></name></person-group><article-title>Burst-dependent synaptic plasticity can coordinate learning in hierarchical circuits</article-title><source>Nature Neuroscience</source><year>2021</year><volume>24</volume><issue>7</issue><fpage>1010</fpage><lpage>1019</lpage><pub-id pub-id-type="pmid">33986551</pub-id></element-citation></ref><ref id="R77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedrosa</surname><given-names>V</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name></person-group><article-title>Interplay between somatic and dendritic inhibition promotes the emergence and stabilization of place fields</article-title><source>PLoS Computational Biology</source><year>2020</year><volume>16</volume><issue>7</issue><elocation-id>e1007955</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007955</pub-id><pub-id pub-id-type="pmcid">PMC7386595</pub-id><pub-id pub-id-type="pmid">32649658</pub-id></element-citation></ref><ref id="R78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peron</surname><given-names>S</given-names></name><name><surname>Pancholi</surname><given-names>R</given-names></name><name><surname>Voelcker</surname><given-names>B</given-names></name><name><surname>Wittenbach</surname><given-names>JD</given-names></name><name><surname>Ólafsdóttir</surname><given-names>HF</given-names></name><name><surname>Freeman</surname><given-names>J</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><article-title>Recurrent interactions in local cortical circuits</article-title><source>Nature</source><year>2020</year><volume>579</volume><issue>7798</issue><fpage>256</fpage><lpage>259</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2062-x</pub-id><pub-id pub-id-type="pmcid">PMC8092186</pub-id><pub-id pub-id-type="pmid">32132709</pub-id></element-citation></ref><ref id="R79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfeffer</surname><given-names>CK</given-names></name><name><surname>Xue</surname><given-names>M</given-names></name><name><surname>He</surname><given-names>M</given-names></name><name><surname>Huang</surname><given-names>ZJ</given-names></name><name><surname>Scanziani</surname><given-names>M</given-names></name></person-group><article-title>Inhibition of inhibition in visual cortex: The logic of connections between molecularly distinct interneurons</article-title><source>Nature Neuroscience</source><year>2013</year><volume>16</volume><issue>8</issue><fpage>1068</fpage><lpage>1076</lpage><pub-id pub-id-type="doi">10.1038/nn.3446</pub-id><pub-id pub-id-type="pmcid">PMC3729586</pub-id><pub-id pub-id-type="pmid">23817549</pub-id></element-citation></ref><ref id="R80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pi</surname><given-names>HJ</given-names></name><name><surname>Hangya</surname><given-names>B</given-names></name><name><surname>Kvitsiani</surname><given-names>D</given-names></name><name><surname>Sanders</surname><given-names>JI</given-names></name><name><surname>Huang</surname><given-names>ZJ</given-names></name><name><surname>Kepecs</surname><given-names>A</given-names></name></person-group><article-title>Cortical interneurons that specialize in disinhibitory control</article-title><source>Nature</source><year>2013</year><volume>503</volume><fpage>521</fpage><lpage>524</lpage><pub-id pub-id-type="doi">10.1038/nature12676</pub-id><pub-id pub-id-type="pmcid">PMC4017628</pub-id><pub-id pub-id-type="pmid">24097352</pub-id></element-citation></ref><ref id="R81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Podlaski</surname><given-names>WF</given-names></name><name><surname>Agnes</surname><given-names>EJ</given-names></name><name><surname>Vogels</surname><given-names>TP</given-names></name></person-group><article-title>High Capacity and Dynamic Accessibility in Associative Memory Networks with Context-Dependent Neuronal and Synaptic Gating</article-title><source>Physical Review X</source><year>2025</year><volume>15</volume><issue>1</issue><elocation-id>011057</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevX.15.011057</pub-id></element-citation></ref><ref id="R82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poirazi</surname><given-names>P</given-names></name><name><surname>Papoutsi</surname><given-names>A</given-names></name></person-group><article-title>Illuminating dendritic function with computational models</article-title><source>Nature Reviews Neuroscience</source><year>2020</year><pub-id pub-id-type="pmid">32393820</pub-id></element-citation></ref><ref id="R83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pokorny</surname><given-names>C</given-names></name><name><surname>Ison</surname><given-names>MJ</given-names></name><name><surname>Rao</surname><given-names>A</given-names></name><name><surname>Legenstein</surname><given-names>R</given-names></name><name><surname>Papadimitriou</surname><given-names>C</given-names></name><name><surname>Maass</surname><given-names>W</given-names></name></person-group><article-title>STDP Forms Associations between Memory Traces in Networks of Spiking Neurons</article-title><source>Cerebral Cortex</source><year>2020</year><volume>30</volume><issue>3</issue><fpage>952</fpage><lpage>968</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhz140</pub-id><pub-id pub-id-type="pmcid">PMC7132978</pub-id><pub-id pub-id-type="pmid">31403679</pub-id></element-citation></ref><ref id="R84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rashid</surname><given-names>SK</given-names></name><name><surname>Pedrosa</surname><given-names>V</given-names></name><name><surname>Dufour</surname><given-names>MA</given-names></name><name><surname>Moore</surname><given-names>JJ</given-names></name><name><surname>Chavlis</surname><given-names>S</given-names></name><name><surname>Delatorre</surname><given-names>RG</given-names></name><name><surname>Poirazi</surname><given-names>P</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name><name><surname>Basu</surname><given-names>J</given-names></name></person-group><article-title>The dendritic spatial code: branch-specific place tuning and its experience-dependent decoupling</article-title><source>bioRxiv</source><year>2020</year><pub-id pub-id-type="doi">10.1101/2020.01.24.916643</pub-id></element-citation></ref><ref id="R85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rossbroich</surname><given-names>J</given-names></name><name><surname>Zenke</surname><given-names>F</given-names></name></person-group><article-title>Breaking Balance: Encoding local error signals in perturbations of excitation-inhibition balance</article-title><source>bioRxiv</source><year>2025</year><pub-id pub-id-type="doi">10.1101/2025.05.12.653626</pub-id></element-citation></ref><ref id="R86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Royer</surname><given-names>S</given-names></name><name><surname>Paré</surname><given-names>D</given-names></name></person-group><article-title>Conservation of total synaptic weight through balanced synaptic depression and potentiation</article-title><source>Nature</source><year>2003</year><volume>422</volume><fpage>518</fpage><lpage>522</lpage><pub-id pub-id-type="pmid">12673250</pub-id></element-citation></ref><ref id="R87"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sacramento</surname><given-names>J</given-names></name><name><surname>Costa</surname><given-names>RP</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name></person-group><source>Dendritic cortical microcircuits approximate the backpropagation algorithm</source><conf-name>32nd Conference on Neural Information Processing Systems</conf-name><year>2018</year><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1810.11393">http://arxiv.org/abs/1810.11393</ext-link></comment></element-citation></ref><ref id="R88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanzeni</surname><given-names>A</given-names></name><name><surname>Akitake</surname><given-names>B</given-names></name><name><surname>Goldbach</surname><given-names>HC</given-names></name><name><surname>Leedy</surname><given-names>CE</given-names></name><name><surname>Brunel</surname><given-names>N</given-names></name><name><surname>Histed</surname><given-names>MH</given-names></name></person-group><article-title>Inhibition stabilization is a widespread property of cortical networks</article-title><source>eLife</source><year>2020</year><volume>9</volume><elocation-id>e54875</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.54875</pub-id><pub-id pub-id-type="pmcid">PMC7324160</pub-id><pub-id pub-id-type="pmid">32598278</pub-id></element-citation></ref><ref id="R89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schiller</surname><given-names>J</given-names></name><name><surname>Major</surname><given-names>G</given-names></name><name><surname>Koester</surname><given-names>HJ</given-names></name><name><surname>Schiller</surname><given-names>Y</given-names></name></person-group><article-title>NMDA spikes in basal dendrites of cortical pyramidal neurons</article-title><source>Nature</source><year>2000</year><volume>404</volume><issue>6775</issue><fpage>285</fpage><lpage>289</lpage><pub-id pub-id-type="pmid">10749211</pub-id></element-citation></ref><ref id="R90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schulz</surname><given-names>A</given-names></name><name><surname>Miehl</surname><given-names>C</given-names></name><name><surname>Berry</surname><given-names>MJ</given-names><suffix>II</suffix></name><name><surname>Gjorgjieva</surname><given-names>J</given-names></name></person-group><article-title>The generation of cortical novelty responses through inhibitory plasticity</article-title><source>eLife</source><year>2021</year><volume>10</volume><elocation-id>e65309</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.65309</pub-id><pub-id pub-id-type="pmcid">PMC8516419</pub-id><pub-id pub-id-type="pmid">34647889</pub-id></element-citation></ref><ref id="R91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schulz</surname><given-names>JM</given-names></name><name><surname>Knoflach</surname><given-names>F</given-names></name><name><surname>Hernandez</surname><given-names>MC</given-names></name><name><surname>Bischofberger</surname><given-names>J</given-names></name></person-group><article-title>Dendrite-targeting interneurons control synaptic NMDA-receptor activation via nonlinear 5-GABAA receptors</article-title><source>Nature Communications</source><year>2018</year><volume>9</volume><elocation-id>3576</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-06004-8</pub-id><pub-id pub-id-type="pmcid">PMC6120902</pub-id><pub-id pub-id-type="pmid">30177704</pub-id></element-citation></ref><ref id="R92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sezener</surname><given-names>E</given-names></name><name><surname>Grabska-Barwinska</surname><given-names>A</given-names></name><name><surname>Kostadinov</surname><given-names>D</given-names></name><name><surname>Beau</surname><given-names>M</given-names></name><name><surname>Krishnagopal</surname><given-names>S</given-names></name><name><surname>Budden</surname><given-names>D</given-names></name><name><surname>Hutter</surname><given-names>M</given-names></name><name><surname>Veness</surname><given-names>J</given-names></name><name><surname>Botvinick</surname><given-names>M</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name></person-group><article-title>A rapid and efficient learning rule for biological neural circuits</article-title><source>bioRxiv</source><year>2021</year><pub-id pub-id-type="doi">10.1101/2021.03.10.434756</pub-id></element-citation></ref><ref id="R93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siegle</surname><given-names>JH</given-names></name><name><surname>Jia</surname><given-names>X</given-names></name><name><surname>Durand</surname><given-names>S</given-names></name><name><surname>Gale</surname><given-names>S</given-names></name><name><surname>Bennett</surname><given-names>C</given-names></name><name><surname>Graddis</surname><given-names>N</given-names></name><name><surname>Heller</surname><given-names>G</given-names></name><name><surname>Ramirez</surname><given-names>TK</given-names></name><name><surname>Choi</surname><given-names>H</given-names></name><name><surname>Luviano</surname><given-names>JA</given-names></name><name><surname>Groblewski</surname><given-names>PA</given-names></name><name><surname>Ahmed</surname><given-names>R</given-names></name><name><surname>Arkhipov</surname><given-names>A</given-names></name><name><surname>Bernard</surname><given-names>A</given-names></name><name><surname>Billeh</surname><given-names>YN</given-names></name><name><surname>Brown</surname><given-names>D</given-names></name><name><surname>Buice</surname><given-names>MA</given-names></name><name><surname>Cain</surname><given-names>N</given-names></name><name><surname>Caldejon</surname><given-names>S</given-names></name><name><surname>Casal</surname><given-names>L</given-names></name><etal/></person-group><article-title>Survey of spiking in the mouse visual system reveals functional hierarchy</article-title><source>Nature</source><year>2021</year><volume>592</volume><issue>7852</issue><fpage>86</fpage><lpage>92</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-03171-x</pub-id><pub-id pub-id-type="pmcid">PMC10399640</pub-id><pub-id pub-id-type="pmid">33473216</pub-id></element-citation></ref><ref id="R94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silver</surname><given-names>D</given-names></name><name><surname>Schrittwieser</surname><given-names>J</given-names></name><name><surname>Simonyan</surname><given-names>K</given-names></name><name><surname>Antonoglou</surname><given-names>I</given-names></name><name><surname>Huang</surname><given-names>A</given-names></name><name><surname>Guez</surname><given-names>A</given-names></name><name><surname>Hubert</surname><given-names>T</given-names></name><name><surname>Baker</surname><given-names>L</given-names></name><name><surname>Lai</surname><given-names>M</given-names></name><name><surname>Bolton</surname><given-names>A</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Lillicrap</surname><given-names>T</given-names></name><name><surname>Hui</surname><given-names>F</given-names></name><name><surname>Sifre</surname><given-names>L</given-names></name><name><surname>Van Den Driessche</surname><given-names>G</given-names></name><name><surname>Graepel</surname><given-names>T</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name></person-group><article-title>Mastering the game of Go without human knowledge</article-title><source>Nature</source><year>2017</year><volume>550</volume><issue>7676</issue><fpage>354</fpage><lpage>359</lpage><pub-id pub-id-type="pmid">29052630</pub-id></element-citation></ref><ref id="R95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stefanelli</surname><given-names>T</given-names></name><name><surname>Bertollini</surname><given-names>C</given-names></name><name><surname>Lüscher</surname><given-names>C</given-names></name><name><surname>Muller</surname><given-names>D</given-names></name><name><surname>Mendez</surname><given-names>P</given-names></name></person-group><article-title>Hippocampal Somatostatin Interneurons Control the Size of Neuronal Memory Ensembles</article-title><source>Neuron</source><year>2016</year><volume>89</volume><issue>5</issue><fpage>1074</fpage><lpage>1085</lpage><pub-id pub-id-type="pmid">26875623</pub-id></element-citation></ref><ref id="R96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stimberg</surname><given-names>M</given-names></name><name><surname>Brette</surname><given-names>R</given-names></name><name><surname>Goodman</surname><given-names>DF</given-names></name></person-group><article-title>Brian 2, an intuitive and efficient neural simulator</article-title><source>eLife</source><year>2019</year><volume>8</volume><elocation-id>e47314</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.47314</pub-id><pub-id pub-id-type="pmcid">PMC6786860</pub-id><pub-id pub-id-type="pmid">31429824</pub-id></element-citation></ref><ref id="R97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stokes</surname><given-names>CCA</given-names></name><name><surname>Teeter</surname><given-names>CM</given-names></name><name><surname>Isaacson</surname><given-names>JS</given-names></name></person-group><article-title>Single dendrite-targeting interneurons generate branch-specific inhibition</article-title><source>Frontiers in Neural Circuits</source><year>2014</year><volume>8</volume><issue>139</issue><pub-id pub-id-type="doi">10.3389/fncir.2014.00139</pub-id><pub-id pub-id-type="pmcid">PMC4243555</pub-id><pub-id pub-id-type="pmid">25505385</pub-id></element-citation></ref><ref id="R98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tetzlaff</surname><given-names>C</given-names></name><name><surname>Dasgupta</surname><given-names>S</given-names></name><name><surname>Kulvicius</surname><given-names>T</given-names></name><name><surname>Wörgötter</surname><given-names>F</given-names></name></person-group><article-title>The Use of Hebbian Cell Assemblies for Nonlinear Computation</article-title><source>Scientific Reports</source><year>2015</year><volume>5</volume><elocation-id>12866</elocation-id><pub-id pub-id-type="doi">10.1038/srep12866</pub-id><pub-id pub-id-type="pmcid">PMC4650703</pub-id><pub-id pub-id-type="pmid">26249242</pub-id></element-citation></ref><ref id="R99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tilley</surname><given-names>MJ</given-names></name><name><surname>Miller</surname><given-names>M</given-names></name><name><surname>Freedman</surname><given-names>DJ</given-names></name></person-group><article-title>Artificial Neuronal Ensembles with Learned Context Dependent Gating</article-title><source>arXiv</source><year>2023</year><comment><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2301.07187">http://arxiv.org/abs/2301.07187</ext-link></comment></element-citation></ref><ref id="R100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tremblay</surname><given-names>R</given-names></name><name><surname>Lee</surname><given-names>S</given-names></name><name><surname>Rudy</surname><given-names>B</given-names></name></person-group><article-title>GABAergic Interneurons in the Neocortex: From Cellular Properties to Circuits</article-title><source>Neuron</source><year>2016</year><volume>91</volume><issue>2</issue><fpage>260</fpage><lpage>292</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.06.033</pub-id><pub-id pub-id-type="pmcid">PMC4980915</pub-id><pub-id pub-id-type="pmid">27477017</pub-id></element-citation></ref><ref id="R101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Veit</surname><given-names>J</given-names></name><name><surname>Hakim</surname><given-names>R</given-names></name><name><surname>Jadi</surname><given-names>MP</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name><name><surname>Adesnik</surname><given-names>H</given-names></name></person-group><article-title>Cortical gamma band synchronization through somatostatin interneurons</article-title><source>Nature Neuroscience</source><year>2017</year><volume>20</volume><issue>7</issue><fpage>951</fpage><lpage>959</lpage><pub-id pub-id-type="doi">10.1038/nn.4562</pub-id><pub-id pub-id-type="pmcid">PMC5511041</pub-id><pub-id pub-id-type="pmid">28481348</pub-id></element-citation></ref><ref id="R102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogels</surname><given-names>TP</given-names></name><name><surname>Sprekeler</surname><given-names>H</given-names></name><name><surname>Zenke</surname><given-names>F</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><article-title>Inhibitory Plasticity Balances Excitation and Inhibition in Sensory Pathways and Memory Networks</article-title><source>Science</source><year>2011</year><volume>334</volume><issue>6062</issue><fpage>1569</fpage><lpage>1573</lpage><pub-id pub-id-type="pmid">22075724</pub-id></element-citation></ref><ref id="R103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Voina</surname><given-names>D</given-names></name><name><surname>Recanatesi</surname><given-names>S</given-names></name><name><surname>Hu</surname><given-names>B</given-names></name><name><surname>Shea-Brown</surname><given-names>E</given-names></name><name><surname>Mihalas</surname><given-names>S</given-names></name></person-group><article-title>Single Circuit in V1 Capable of Switching Contexts During Movement Using an Inhibitory Population as a Switch</article-title><source>Neural Computation</source><year>2022</year><volume>34</volume><issue>3</issue><fpage>541</fpage><lpage>594</lpage><pub-id pub-id-type="pmid">35016220</pub-id></element-citation></ref><ref id="R104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waitzmann</surname><given-names>F</given-names></name><name><surname>Wu</surname><given-names>YK</given-names></name><name><surname>Gjorgjieva</surname><given-names>J</given-names></name></person-group><article-title>Top–down modulation in canonical cortical circuits with short-term plasticity</article-title><source>Proceedings of the National Academy of Sciences</source><year>2024</year><volume>121</volume><issue>16</issue><elocation-id>e2311040121</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2311040121</pub-id><pub-id pub-id-type="pmcid">PMC11032497</pub-id><pub-id pub-id-type="pmid">38593083</pub-id></element-citation></ref><ref id="R105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weidel</surname><given-names>P</given-names></name><name><surname>Duarte</surname><given-names>R</given-names></name><name><surname>Morrison</surname><given-names>A</given-names></name></person-group><article-title>Unsupervised Learning and Clustered Connectivity Enhance Reinforcement Learning in Spiking Neural Networks</article-title><source>Frontiers in Computational Neuroscience</source><year>2021</year><volume>15</volume><elocation-id>543872</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2021.543872</pub-id><pub-id pub-id-type="pmcid">PMC7970044</pub-id><pub-id pub-id-type="pmid">33746728</pub-id></element-citation></ref><ref id="R106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname><given-names>G</given-names></name><name><surname>Levy</surname><given-names>WB</given-names></name><name><surname>Steward</surname><given-names>O</given-names></name></person-group><article-title>Spatial overlap between populations of synapses determines the extent of their associative in-teraction during the induction of long-term potentiation and depression</article-title><source>Journal of Neurophysiology</source><year>1990</year><volume>64</volume><issue>4</issue><fpage>1186</fpage><lpage>1198</lpage><pub-id pub-id-type="pmid">2258741</pub-id></element-citation></ref><ref id="R107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilmes</surname><given-names>KA</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name></person-group><article-title>Dendrites help mitigate the plasticity-stability dilemma</article-title><source>Scientific Reports</source><year>2023</year><volume>13</volume><elocation-id>6543</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-023-32410-0</pub-id><pub-id pub-id-type="pmcid">PMC10121616</pub-id><pub-id pub-id-type="pmid">37085642</pub-id></element-citation></ref><ref id="R108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>YK</given-names></name><name><surname>Hengen</surname><given-names>KB</given-names></name><name><surname>Turrigiano</surname><given-names>GG</given-names></name><name><surname>Gjorgjieva</surname><given-names>J</given-names></name></person-group><article-title>Homeostatic mechanisms regulate distinct aspects of cortical circuit dynamics</article-title><source>Proceedings of the National Academy of Sciences</source><year>2020</year><volume>117</volume><issue>39</issue><fpage>24514</fpage><lpage>24525</lpage><pub-id pub-id-type="doi">10.1073/pnas.1918368117</pub-id><pub-id pub-id-type="pmcid">PMC7533694</pub-id><pub-id pub-id-type="pmid">32917810</pub-id></element-citation></ref><ref id="R109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wybo</surname><given-names>WAM</given-names></name><name><surname>Tsai</surname><given-names>MC</given-names></name><name><surname>Tran</surname><given-names>VAK</given-names></name><name><surname>Illing</surname><given-names>B</given-names></name><name><surname>Jordan</surname><given-names>J</given-names></name><name><surname>Morrison</surname><given-names>A</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name></person-group><article-title>NMDA-driven dendritic modulation enables multitask representation learning in hierarchical sensory processing pathways</article-title><source>PNAS</source><year>2023</year><volume>120</volume><issue>32</issue><elocation-id>e2300558120</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2300558120</pub-id><pub-id pub-id-type="pmcid">PMC10410730</pub-id><pub-id pub-id-type="pmid">37523562</pub-id></element-citation></ref><ref id="R110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Cadieu</surname><given-names>CF</given-names></name><name><surname>Solomon</surname><given-names>EA</given-names></name><name><surname>Seibert</surname><given-names>D</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title><source>Proceedings of the National Academy of Sciences</source><year>2014</year><volume>111</volume><issue>23</issue><fpage>8619</fpage><lpage>8624</lpage><pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id><pub-id pub-id-type="pmcid">PMC4060707</pub-id><pub-id pub-id-type="pmid">24812127</pub-id></element-citation></ref><ref id="R111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>GR</given-names></name><name><surname>Murray</surname><given-names>JD</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><article-title>A dendritic disinhibitory circuit mechanism for pathway-specific gating</article-title><source>Nature Communications</source><year>2016</year><volume>7</volume><elocation-id>12815</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms12815</pub-id><pub-id pub-id-type="pmcid">PMC5034308</pub-id><pub-id pub-id-type="pmid">27649374</pub-id></element-citation></ref><ref id="R112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>X</given-names></name><name><surname>Doiron</surname><given-names>B</given-names></name></person-group><article-title>Causal Spike Timing Dependent Plasticity Prevents Assembly Fusion in Recurrent Networks</article-title><source>bioRxiv</source><year>2025</year><pub-id pub-id-type="doi">10.1101/2025.01.14.633085</pub-id></element-citation></ref><ref id="R113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yuste</surname><given-names>R</given-names></name></person-group><article-title>From the neuron doctrine to neural networks</article-title><source>Nature Reviews Neuroscience</source><year>2015</year><volume>16</volume><fpage>487</fpage><lpage>497</lpage><pub-id pub-id-type="pmid">26152865</pub-id></element-citation></ref><ref id="R114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeng</surname><given-names>G</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Cui</surname><given-names>B</given-names></name><name><surname>Yu</surname><given-names>S</given-names></name></person-group><article-title>Continual learning of context-dependent processing in neural networks</article-title><source>Nature Machine Intelligence</source><year>2019</year><volume>1</volume><issue>8</issue><fpage>364</fpage><lpage>372</lpage><pub-id pub-id-type="doi">10.1038/s42256-019-0080-x</pub-id></element-citation></ref><ref id="R115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zenke</surname><given-names>F</given-names></name><name><surname>Agnes</surname><given-names>EJ</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><article-title>Diverse synaptic plasticity mechanisms orchestrated to form and retrieve memories in spiking neural networks</article-title><source>Nature Communications</source><year>2015</year><volume>6</volume><elocation-id>6922</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms7922</pub-id><pub-id pub-id-type="pmcid">PMC4411307</pub-id><pub-id pub-id-type="pmid">25897632</pub-id></element-citation></ref><ref id="R116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zenke</surname><given-names>F</given-names></name><name><surname>Laborieux</surname><given-names>A</given-names></name></person-group><article-title>Theories of synaptic memory consolidation and intelligent plasticity for continual learning</article-title><source>arXiv</source><year>2024</year><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2405.16922">http://arxiv.org/abs/2405.16922</ext-link></comment></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Bridging spatial scales from local dendritic properties to multi-area computations.</title><p><bold>A.</bold> Schematic of multi-compartment neurons with contextual dendrite-specific inhibitory control. A dendrite is gated ‘on’ if the inhibitory neurons specific to a given context are inhibited, and a dendrite is gated ‘off if the inhibitory neurons are not inhibited. <bold>B.</bold> Overlapping assembly structures can be learned in a recurrent network without forgetting previously learned ones, where a neuron can only participate in a given assembly with a dendrite that is gated ‘on’ in the appropriate context. <bold>C.</bold> Assembly computations in a multiarea network.</p></caption><graphic xlink:href="EMS207568-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Gating of excitatory plasticity via inhibitory context signals at nonlinear dendrites.</title><p><bold>A.</bold> Schematic of a single excitatory neuron with a somatic compartment and six dendritic compartments. Each dendrite receives excitatory and inhibitory input (only shown for one dendrite). <bold>B.</bold> Inhibitory and excitatory presynaptic spikes (top). When the low-pass filtered dendritic voltage (<italic>u</italic><sub>-</sub>, middle) at a synapse exceeds the LTD threshold (<italic>θ</italic><sub>-</sub>, middle) at the time of a presynaptic spike, the synaptic weight (bottom) undergoes LTD. If the low-pass filtered dendritic voltage (<italic>u</italic><sub>+</sub>, middle) with a longer time constant exceeds the threshold <italic>θ</italic><sub>-</sub> at the same time as the momentary dendritic voltage <italic>(V</italic>) surpasses a higher threshold (<italic>θ</italic><sub>+</sub>), the synaptic weight undergoes LTP at all times (<xref ref-type="sec" rid="S10">Methods</xref>). Mean (black) vs. individual (gray) synaptic weight change of highly active excitatory input synapses (bottom). <bold>C.</bold> Mean synaptic weight change Δ<italic>w</italic> as a function of the number of highly active excitatory synapses and the firing rate of inhibitory neurons. Long-term depression (LTD) is indicated by shades of blue, and long-term potentiation (LTP) by shades of red. <bold>D.</bold> Left: Filtered dendritic voltage <italic>u</italic><sub>+</sub> (top), and mean synaptic weight change Δ<italic>w</italic> (bottom) for a different number of highly active excitatory inputs (6 – red, 5 – blue, 1 – gray). Right: Same as left, for changing the inhibitory firing rate (40 Hz - red, 100 Hz - blue, 350 Hz - gray). Letters (a, b, c, a*, b*, c*) correspond to cases in panel C. <bold>E.</bold> Schematic of a single excitatory neuron with six dendritic compartments (D1-D6). Each dendrite receives a different number of highly active excitatory input synapses (indicated by the thickness of arrows) and a separate inhibitory input. Each inhibitory population is controlled by a separate inhibitory context signal (C1-C6). See also <xref ref-type="fig" rid="F1">Fig. 1A</xref>. <bold>F.</bold> Mean weight (<italic>w</italic>) change at dendrite D1, D2, D3, and D4. The activity of the respective inhibitory group is reduced by a context signal (C1, C2, and C3) for 4 seconds (gray dashed lines). D1 receives 1, D2 receives 6, D3 receives 4 and D4 receives 7 highly active excitatory inputs. The inhibitory firing rate is 400 Hz if a context is ‘off and 0 Hz if a context is ‘on.’ Dendrites color-coded in red undergo LTP, in blue LTD, and in gray no plasticity.</p></caption><graphic xlink:href="EMS207568-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Nonlinear dendrites and inhibitory context lead to dendrite-specific assemblies.</title><p><bold>A.</bold> Schematic of feedforward inputs and recurrent (RC) network neurons. Inset: Schematic of connections and depolarization levels at different dendrites when context 1 (C1) is ‘on’. Two neurons are within (left) and two neurons are outside (right) of the assembly. <bold>B.</bold> Feedforward input spikes (top), recurrent neuron spikes (middle), and mean recurrent weights (<italic>w</italic>) over time. Weights are separated into groups corresponding to whether they are between neurons within the assembly (green line), from the assembly, into the assembly, and outside the assembly (gray lines). <bold>C.</bold> The representation of a single assembly through connectivity matrices. Sorted neuron-neuron (left) and neuron-dendrite (right) connectivity matrix. Grayscale indicates connection strength. Inset: Subset of presynaptic neurons and dendrites. <bold>D.</bold> Neuron-neuron connectivity matrix of the recurrent neurons after imprinting 20 assemblies one after another. Neurons are sorted in descending order, starting from the first imprinted assembly. <bold>E.</bold> The dependence of the cumulative total number of neurons that are part of any assembly on the number of imprinted assemblies can be predicted from the statistics of random connection probability (<xref ref-type="sec" rid="S10">Methods</xref>). <bold>F.</bold> Recall in recurrent networks depends on the order in which assemblies were imprinted (left). Recall is measured via firing rates (middle) and number of active assembly neurons (right). Color indicates the order in which assemblies were imprinted (Asse. C1 - recall of an assembly in the learned context, Asse. C2 - recall of the same assembly in a different context, Bkgrd refers to the firing rate of non-assembly neurons). Recall is repeated 5 times for each assembly, and the dashed line represents the mean for the last imprinted assembly. <bold>G.</bold> Pattern completion in recurrent networks as a function of number of active input neurons measured via the firing rates of assembly neurons (middle) or number of active assembly neurons (right) in the context in which the assembly was learned (C1; black) or in a different context (C2; gray). The same 20 active neurons as the last imprinted assembly in panel F. <bold>H.</bold> Same as panel G as a function of the input firing rates. Input rate of 10 Hz corresponds to the recall in panel F.</p></caption><graphic xlink:href="EMS207568-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Learning without catastrophic forgetting.</title><p><bold>A.</bold> Top to bottom: Schematic of a network with three areas, spikes in area X, X’, and Y (spikes in area Y sorted to context C1), and mean within-assembly weight (<italic>w</italic>) change in different contexts (C1, C2, and C3). The firing rate of input spikes is 10 Hz. <bold>B.</bold> Neuron-neuron (left) and dendrite-neuron (right) connectivity matrix of area Y sorted according to context 1. Grayscale indicates connection strength. Inset: Subset of presynaptic neurons and dendrites with weights color-coded according to context. <bold>C.</bold> Neuron-neuron connectivity graph of area Y (only a subset of assembly neurons shown). Red circles indicate overlap neurons that are part of multiple assemblies.</p></caption><graphic xlink:href="EMS207568-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Assembly computations in a visual-auditory association task.</title><p><bold>A.</bold> Left: Schematic of visual-auditory association task, with two contexts: numbers (green) or letters (purple). Different auditory stimuli activate different assemblies in the auditory sensory area. Visual stimuli (taken from the extended MNIST dataset filtered by Gabor patches, see <xref ref-type="sec" rid="S10">Methods</xref>) are more similar and activate similar assemblies in the visual sensory area. Right: Spiking activity in each area. Spikes are sorted according to the number context (green) in the auditory, visual, and concept areas. <bold>B.</bold> Neuron-neuron connectivity graph of the concept area. <bold>C.</bold> Quantification of pattern completion via assembly firing rates (left) or number of active assembly neurons (right) in the visual area (top) or the concept area (bottom) for each type of visual stimulus (“0”, “O”, “1”, and “l”) alone. Orange dots correspond to values when both auditory and visual stimuli are presented together. Grey lines are single images, black line is the mean across images.</p></caption><graphic xlink:href="EMS207568-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><title>Enhanced recall and pattern completion in downstream areas despite disrupted recall in previous areas.</title><p><bold>A.</bold> Schematic of a network with three hierarchically connected areas X, Y, and Z. Assemblies are denoted by the green circled neurons which project across the areas. Shaded triangles denote silenced neurons in area Y. <bold>B.</bold> Quantification of recall for 0%, and 50% silenced assembly neurons in area Y (top) and area Z (bottom) via normalized assembly firing rates. ‘Bkgrd’ indicates background firing rates. <bold>C.</bold> Left: Quantifying pattern completion while silencing assembly neurons in area Y (gray-scale) via the normalized number of active neurons. Right: Same as the left panel for the assembly’s normalized firing rates in area Y (top) and area Z (bottom).</p></caption><graphic xlink:href="EMS207568-f006"/></fig><fig id="F7" position="float"><label>Figure 7</label><caption><title>Associative learning with existing assemblies.</title><p><bold>A.</bold> Schematics of three different assembly formation scenarios across multiple areas: Sequential projection (I), simultaneous association (II), and sequential association (III). For the comparison across all cases, we used the same connectivity matrices across and within areas, activated the same inputs in areas X and X’, and used the same context. <bold>B.</bold> Compare sequential projection (I) versus simultaneous association (II). Left: Venn diagram of the average number of neurons in area Y (top) or area Z (bottom), which is either unique to each assembly (light/dark green and light/dark orange) or part of two or more assemblies (overlaps in the diagram). Middle: Input synapse distribution from area X orX’ to area Y (top) and from area Y to area Z (bottom). The color of the bars indicates the presynaptic assembly (X or X’) and the projection/association into the downstream assembly (Y or Z). (e.g. light orange in area Y shows the synapse distribution from the assembly in area X to the assembly in area Y in the case of simultaneous association, and the light green shows the case of the sequential projection). Right: Quantification of pattern completion via the normalized rates when activating only X (light orange) or only X’(dark orange) for area Y (top) and area Z (bottom) for simultaneous association (case II). Rates were normalized to the response in area Y (top) and area Z (bottom) when activating both assemblies in X and X’ at the same time. <bold>C.</bold> Same as panel B, comparing sequential projection (I) versus sequential association (III).</p></caption><graphic xlink:href="EMS207568-f007"/></fig></floats-group></article>