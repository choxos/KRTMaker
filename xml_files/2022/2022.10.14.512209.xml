<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS155882</article-id><article-id pub-id-type="doi">10.1101/2022.10.14.512209</article-id><article-id pub-id-type="archive">PPR559202</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Deep learning based decoding of local field potential events</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Schilling</surname><given-names>Achim</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Gerum</surname><given-names>Richard</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Boehm</surname><given-names>Claudia</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Rasheed</surname><given-names>Jwan</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Metzner</surname><given-names>Claus</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Maier</surname><given-names>Andreas</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Reindl</surname><given-names>Caroline</given-names></name><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Hamer</surname><given-names>Hajo</given-names></name><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Krauss</surname><given-names>Patrick</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A4">4</xref><xref ref-type="aff" rid="A6">6</xref></contrib></contrib-group><aff id="A1"><label>1</label>Neuroscience Lab, University Hospital Erlangen, Germany</aff><aff id="A2"><label>2</label>Cognitive Computational Neuroscience Group, University Erlangen-Nürnberg, Germany</aff><aff id="A3"><label>3</label>Department of Physics and Center for Vision Research, York University, Toronto, Canada</aff><aff id="A4"><label>4</label>Pattern Recognition Lab, University Erlangen-Nürnberg, Germany</aff><aff id="A5"><label>5</label>Epilepsy Center, Department of Neurology, University Hospital Erlangen, Germany</aff><aff id="A6"><label>6</label>Linguistics Lab, University Erlangen-Nürnberg, Germany</aff><pub-date pub-type="nihms-submitted"><day>19</day><month>10</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>18</day><month>10</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">How is information processed in the cerebral cortex? To answer this question a lot of effort has been undertaken to create novel and to further develop existing neuroimaging techniques. Thus, a high spatial resolution of fMRI devices was the key to exactly localize cognitive processes. Furthermore, an increase in time-resolution and number of recording channels of electro-physiological setups has opened the door to investigate the exact timing of neural activity. However, in most cases the recorded signal is averaged over many (stimulus) repetitions, which erases the fine-structure of the neural signal. Here, we show that an unsupervised machine learning approach can be used to extract meaningful information from electro-physiological recordings on a single-trial base. We use an auto-encoder network to reduce the dimensions of single local field potential (LFP) events to create interpretable clusters of different neural activity patterns. Strikingly, certain LFP shapes correspond to latency differences in different recording channels. Hence, LFP shapes can be used to determine the direction of information flux in the cerebral cortex. Furthermore, after clustering, we decoded the cluster centroids to reverse-engineer the underlying prototypical LFP event shapes. To evaluate our approach, we applied it to both neural extra-cellular recordings in rodents, and intra-cranial EEG recordings in humans. Finally, we find that single channel LFP event shapes during spontaneous activity sample from the realm of possible stimulus evoked event shapes. A finding which so far has only been demonstrated for multi-channel population coding.</p></abstract><kwd-group><kwd>deep learning</kwd><kwd>local field potentials</kwd><kwd>auditory neuroscience</kwd><kwd>auditory cortex</kwd><kwd>speech perception</kwd><kwd>auto-encoder</kwd><kwd>embeddings</kwd><kwd>intracranial EEG (iEEG)</kwd><kwd>stereotactic EEG (sEEG)</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">How is sensory input and information processed in the cerebral cortex? To answer this question, a lot of effort has been undertaken to measure brain activity with growing temporal and spatial resolution. Indeed, nowadays due to the enormous progress of fMRI, we know very exactly were certain processes take place. Thus, it is possible to scan the whole brain on a millimeter-scale within seconds [<xref ref-type="bibr" rid="R1">1</xref>].</p><p id="P3">On the other hand, to understand what is exactly happening in the brain when sensory stimuli are processed, an in-depth analysis of temporal cues of neural activity in the brain is necessary. However, it is still not fully clear how to take advantage of the growing temporal resolution of modern neuro-imaging techniques such as MEG/EEG, and intra-cranial multi-unit recordings. Thus, surface based methods like EEG and MEG provide a poor spatial resolution and signal-to-noise ratio, as the source of the signal and the recording site are divided by the skull. The surface electrodes, or SQUIDs respectively, are used to record the sum-activity of millions of neurons [<xref ref-type="bibr" rid="R2">2</xref>, <xref ref-type="bibr" rid="R3">3</xref>]. Furthermore, the poor signal-to-noise ratio results in the issue that meaningful data can only be extracted, by averaging over many trials for evoked activity (for auditory evoked potentials <italic>≥</italic> 100, see [<xref ref-type="bibr" rid="R4">4</xref>, <xref ref-type="bibr" rid="R5">5</xref>]), or by extracting spectral cues through Fourier-filters [<xref ref-type="bibr" rid="R6">6</xref>, <xref ref-type="bibr" rid="R7">7</xref>]. However, as Waterstraat and co-workers state the brain is a <italic>”single trial processor”</italic>, that has been shaped by evolutionary processes [<xref ref-type="bibr" rid="R8">8</xref>].</p><p id="P4">The only way to increase the signal-to-noise ratio in order to extract meaningful data from single-trials are intra-cranial recordings. However, these experiments are limited to animal studies as the intra-cranial recording is an invasive method. However, in epilepsy diagnostics sometimes intra-cranial electrodes are implanted in human patients’ brains in order to exactly localize the epilepsy focus (iEEG, [<xref ref-type="bibr" rid="R9">9</xref>]). Usually, these rare recordings are additionally used to tackle scientific questions [<xref ref-type="bibr" rid="R10">10</xref>–<xref ref-type="bibr" rid="R12">12</xref>]. In contrast, to surface recordings on top of the skull such as EEG or MEG, LFP events from intra-cranial recordings have a far better resolution since they are mainly produced by three orders of magnitude smaller neuron ensembles [<xref ref-type="bibr" rid="R13">13</xref>]. Thus, the radius, within which neurons significantly contribute to the LFP is only in the range of a few hundreds of micrometers [<xref ref-type="bibr" rid="R14">14</xref>, <xref ref-type="bibr" rid="R15">15</xref>]. Furthermore, LFP shapes contain information on the proportions of the contributing sources of extra-cellular currents as well as on the conductive properties of the surrounding brain tissue [<xref ref-type="bibr" rid="R11">11</xref>]. Thus, the low frequency parts of the LFPs are induced by synaptic activity, inducing trans-membrane currents, which lead to electrical dipoles. These dipoles can be measured using intra-cranial electrodes [<xref ref-type="bibr" rid="R11">11</xref>]. Although fast action potentials contribute to the high-frequency components of LFPs, the main contribution comes from dendritic currents [<xref ref-type="bibr" rid="R16">16</xref>–<xref ref-type="bibr" rid="R18">18</xref>]. Therefore, LFPs are typically regarded as a measure of the input signal to the neurons to be measured [<xref ref-type="bibr" rid="R17">17</xref>]. It has to be stated that further effects such as calcium spiking, gap junctions, and neuron oscillations play a role for LFP events [<xref ref-type="bibr" rid="R11">11</xref>].</p><p id="P5">Since the spectral cues as well as the shape of LFP events contain a lot of information on geometry, properties and activity of the neuron ensembles, the investigation of LFPs in the sense of sensory processing and cognition is a promising approach. Thus, LFPs are for example used to implement brain computer interfaces for locked-in patients [<xref ref-type="bibr" rid="R19">19</xref>].</p><p id="P6">Despite the fact that single trial LFP events are a valid measure for local neural population analysis [<xref ref-type="bibr" rid="R20">20</xref>], studies on single-trial LFP events analysis are rare. Thus, a better understanding of LFP events could e.g. help to make major progress in understanding cognitive processes during sleep [<xref ref-type="bibr" rid="R21">21</xref>]. Unfortunately, in contrast to multi-unit activity (MUA), the shapes of LFP events are more versatile. Hence, it is far more difficult to automatically cluster, respectively classify, the different LFP shapes compared to spiking activity (for spike clustering see e.g. [<xref ref-type="bibr" rid="R22">22</xref>]). However, it has already been demonstrated that wavelet analysis could be used to extract meaningful features from single trial LFP events [<xref ref-type="bibr" rid="R23">23</xref>]. Furthermore, an algorithm to automatically detect and calculate three pre-defined features (latency, amplitude, and rebound) of single trial LFP events has been developed [<xref ref-type="bibr" rid="R24">24</xref>].</p><p id="P7">Nevertheless so far, more advanced evaluation techniques such as machine learning based pattern recognition, especially using deep neural networks, has been applied only rarely to detect and characterize single trial LFP events. Thus, more coarse-grained features such as spectral cues have been analyzed using machine learning [<xref ref-type="bibr" rid="R25">25</xref>]. Nurse and co-workers used a top-down approach to classify LFP data by training a convolutional neural network on raw data [<xref ref-type="bibr" rid="R26">26</xref>]. They argue that the bottleneck in processing LFPs using machine learning is to find the right hand-crafted features [<xref ref-type="bibr" rid="R26">26</xref>].</p><p id="P8">However, such top-down approaches like e.g. training classification networks on raw LFP data, respectively events, have also major drawbacks compared to parameter-free and data-driven bottom-up approaches [<xref ref-type="bibr" rid="R27">27</xref>]. Although, classifyer approaches approach are promising to develop e.g. brain computer interfaces, the neuroscientific value of these networks is poor: Whereas we can derive statements on certain features of the LFP signal regarding our pre-defined labels, all the other information, which is not needed for classification according to the pre-defined labels is lost.</p><p id="P9">Therefore, a valid approach to deal with high-dimensional electrophysiological data sets, is to apply unsupervised machine learning algorithms. These algorithms are used to extract recurring patterns from the signal and to reduce the dimensionality of the signal for the purpose of visualizations, being better interpretable for humans [<xref ref-type="bibr" rid="R28">28</xref>–<xref ref-type="bibr" rid="R30">30</xref>]. Indeed, recently some studies have been published, which applied unsupervised machine learning methods on electro-physiological data. For instance, Mackevicius and coworkers developed an un-supervised training algorithm to extract non-redundant sequences of the neurophysiological data [<xref ref-type="bibr" rid="R31">31</xref>]. In addition, Hardcastle and coworkers [<xref ref-type="bibr" rid="R32">32</xref>] applied an auto-encoder network on peripheral nervous system recordings (for further example see also [<xref ref-type="bibr" rid="R33">33</xref>]). Note that, some scientific approaches used the unsupervised trained networks the other way around, namely as models for the nervous system (see e.g. [<xref ref-type="bibr" rid="R34">34</xref>]).</p><p id="P10">Here, we use an auto-encoder network to cluster different LFP shapes. Therefore, we extract the LFP events by searching for local minima in the signal stream and applying advanced thresholding and filtering techniques. The extracted LFP events are encoded to identify certain clusters representing different LFP shapes. For the resulting low-dimensional representations the term embeddings has been coined [<xref ref-type="bibr" rid="R35">35</xref>–<xref ref-type="bibr" rid="R37">37</xref>]. To increase interpretability, we decode the embeddings after we have identified different clusters, thereby reverse-engineering the prototypical LFP shape of each cluster. Furthermore, we provide evidence that the LFP shape may be used to distinguish between different stimulus frequencies for evoked activity and is a highly reproducible marker for the direction of information flow within the cerebral cortex for both evoked and spontaneous activity. In addition, we demonstrate that the maximum of the cross-correlation is linked to the shape of the LFP events. Finally, we show that single channel LFP event shapes during spontaneous activity sample from the realm of possible stimulus evoked event shapes. A finding which so far has only been demonstrated for multi-channel population coding. To put it in a nutshell, the self-supervised auto-encoding approach can be used to extract valuable features from intra-cranial recordings in rodents and from iEEG data recorded in human epilepsy patients.</p></sec><sec id="S2" sec-type="methods"><title>Methods</title><sec id="S3"><title>Rodent Data Acquisition</title><sec id="S4"><title>Surgery</title><p id="P11">For the craniotomy, the Mongolian gerbils were put under deep Ketamin-Xylazine anesthesia and kept on a controlled heating pad to guarantee a constant body temperature. After hair removal using an electric razor the scalp was removed using sharp micro-scissor. The skull is cleaned with a drill and tweezers. Four screws (M2, 2 mm) are implanted frontally and caudally from bregma, which serve as base for the head fixation. As head-fixation a small elbow is glued with dental cement to the skull and the fixation screws. A rectangular trepanation of approximately 4 mm x 4 mm is opened located between ear and the eye contralaterally to the ear to be measured. The dura is removed using a fine needle and micro-tweezers. The trepanation is covered with NaCl solution to prevent the brain from drying out.</p></sec><sec id="S5"><title>Extracellular Recordings</title><p id="P12">After the craniotomy the animal is placed in an anechoic chamber on a controlled heating pad and the head is fixed by screwing the vertical part of the elbow to a aluminium rod. The distance of the loudspeaker and the ear of the animal is X cm. For extracellular recording we use 16 electrode arrays (Clunbury Scientific (Bloomfield Hills), 0.5 MΩ, layout:4 x 4, spacing; 500 <italic>μ</italic>m). The electrode is inserted in the auditory cortex of the animal using a manual micro-cotroller. The insertion depth of the electrode array is between 500 <italic>μ</italic>m and 1 mm to record signals from the granular and infra-granular layers, where we expect significant spiking activity and high field potential amplitudes. The neural signal is recorded using the CerePlex system from Blackrock Neurotech (Salt Lake City, USA). The neural signal is digitized directly at the recording site with the Cereplex <italic>μ</italic> headstage. We recorded all signals using the maximum sampling rate of 30 kHz. To divide between LFP and spike signal during recording, for LFPs a 250 Hz low-pass filter was applied, whereas spiking activity was assumed to occur in the range between 250 Hz and 2.5 kHz. The spikes were sorted online by manually set thresholds.</p></sec><sec id="S6"><title>Auditory Stimulation</title><p id="P13">For auditory stimulation we used a 3 way loudspeaker (MAC, Racer 320). We calibrated the loudspeaker by measuring the output loudness for frequencies between 500 Hz and 19 kHz (half octave steps). For each frequency a correction factor was calculated. The loudspeaker is driven by a 24 bit external sound card (ASUS Xonar MKII 7.1) connected to the computer via USB. The soundcard output is amplified using an audio amplifier (Amp75). To ensure exact alignment of the auditory stimuli with the recorded neural signal, the soundcard is also used as trigger pulse generator. Thus, a second soundcard channel sends trigger pulses to an analog input channel of the recording setup (cf. also [<xref ref-type="bibr" rid="R35">35</xref>, <xref ref-type="bibr" rid="R38">38</xref>]). To analyze the tuning of the neurons, we presented 50 ms pure tone stimuli with inter-stimulus intervals of 300 ms. Click sounds were prevented by adding sine<sup>2</sup>-ramps of 5 ms duration to the pure tone stimuli. The presented stimulus frequencies lie in the range of 500 Hz and 20 kHz (half octave steps). Stimulus intensities and frequencies were presented pseudo-randomly. Depending on the hearing ability of the animals, 5 different sound pressure levels were presented (10 dB steps) starting from 110 dB SPL (90 dB, 70 dB respectively).</p></sec></sec><sec id="S7"><title>Human Data Acquisition</title><p id="P14">The intracranial EEG (iEEG as EEG recorded via stereotactically implanted depth electrodes, SEEG) originated from a patient suffering from epilepsy, who underwent presurgical epilepsy evaluation due to a drug refractory focal epilepsy. Depths electrodes were implanted in the auditory cortex of the right temporal lobe solely because of clinical reasons. In this study, we show the data of 4 platin electrodes (RTB-1-4) with an impedance lower than 10 kΩ, which were measured against a reference electrode placed on top of the scalp (CPz). Due to the fact that there are not many patients, who receive electrodes in the auditory cortex we are data limited.</p></sec><sec id="S8"><title>Data Analysis</title><sec id="S9"><title>Computational Resources</title><p id="P15">The complete software used for the project was written in Python 3.8 using the NumPy library ([<xref ref-type="bibr" rid="R39">39</xref>]). The graphical user interface was designed and implemented using PyQt5 [<xref ref-type="bibr" rid="R40">40</xref>]. The neuronal recordings were imported and converted using the ‘brpylib’-library provided by Blackrock Neurotech [<xref ref-type="bibr" rid="R41">41</xref>]. Basic data processing and filtering was done using the SciPy-library and especially the ‘signal’-module [<xref ref-type="bibr" rid="R42">42</xref>]. Machine learning was implemented in Keras [<xref ref-type="bibr" rid="R43">43</xref>] with the TensorFlow backend [<xref ref-type="bibr" rid="R44">44</xref>]. All simulations and evaluations were performed on a stadard desktop computer. Plotting was done using the Matplotlib library [<xref ref-type="bibr" rid="R45">45</xref>] in combination with the pylustrator add-on [<xref ref-type="bibr" rid="R46">46</xref>].</p></sec><sec id="S10"><title>Neural Networks</title><p id="P16">The used neural networks in this study were all implemented using Keras with the tensorflow backend [<xref ref-type="bibr" rid="R43">43</xref>, <xref ref-type="bibr" rid="R44">44</xref>]. The auto-encoder was trained on single channel LFP events from a 1 h spontaneous activity recording of only one animal. Thus, first local minimum technique (descibed above) was used to find the LFP events. These LFP events were then used to train the auto-encoder. Test-data from other recordings and other animals were used to prove the validity of the training. Before used for training the LFP-event were down-sampled to 100 Hz and cut into 500 ms junks (50 values). The 50-dimensional input vector is expanded by a factor of 14 to 700 dimensions in the first hidden layer of the network. It has be shown that dimensionality expansion can lead to a huge benefit for data processing in neural networks [<xref ref-type="bibr" rid="R47">47</xref>, <xref ref-type="bibr" rid="R48">48</xref>]. The bottleneck layer has only 3 dimensions. This number was chosen as it is the highest dimensionality, which can be easily visualized (for details see <xref ref-type="table" rid="T1">Tab. 1</xref>). The batch-size for training was 500 and the training iterations were 1000. During training the data was randomly shuffled.</p><p id="P17">In order to measure the loss of meaningful information caused by the auto-encoding procedure, we trained an additional feed-forward network. In particular, we used a 1D convolutional neural network consisting of two convolutional layers, one max-pooling layer, one fully connected (dense) layer, and finally a softmax layer for classification (for details see <xref ref-type="table" rid="T2">Tab. 2</xref>). This network was trained on the classification of LFP events according to 4 different stimulus frequencies, hence the output vector is 4-dimensional. The number of iteration was set to 10,000, but for the classifier the early stopping technique was applied.</p></sec><sec id="S11"><title>Cross-Correlation Analysis</title><p id="P18">For correlation analysis we used the normalized cross correlation [<xref ref-type="bibr" rid="R49">49</xref>]. Thus, the LFP event of the reference channel and the LFP-events of the neighbouring channels to be tested were z-scored individually (subtract mean and divide by standard-deviation). After that, the cross-correlation was calculated using the correlate function from numpy [<xref ref-type="bibr" rid="R39">39</xref>]. The sum is then divided by <italic>n<sub>o</sub></italic> representing the number of summands. Thus, this definition results in a normed cross-correlation and an auto-correlation of one for no lag time (time delay) <italic>τ</italic> = 0. <disp-formula id="FD1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:munderover><mml:mstyle><mml:mi>∑</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mtext>Δ</mml:mtext><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mtext>Δ</mml:mtext><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P19">(<italic>C</italic>(<italic>τ</italic>): zero-normed cross correlation, <italic>n<sub>o</sub></italic>: number of summands which is the number of overlapping samples, <italic>r</italic>(<italic>i</italic>Δ<italic>t</italic>): reference channel value at time point <italic>i</italic>Δ<italic>t</italic>, <italic>i ∈</italic> {−<italic>n<sub>o</sub></italic>, <italic>n<sub>o</sub></italic> + 1…, <italic>n<sub>o</sub></italic> −1, <italic>n<sub>o</sub></italic>}, Δ<italic>t</italic> = 1 ms, <italic>μ<sub>r</sub></italic>: mean of all values only for one LFP event in reference channel, <italic>τ</italic>: time delay, <italic>τ</italic> ∈ [-50 ms, 50 ms], <italic>e</italic>(<italic>i</italic>Δ<italic>t</italic>+<italic>τ</italic>): value of tested channel at time point <italic>e</italic>(<italic>i</italic>Δ<italic>t</italic>+<italic>τ</italic>), <italic>μ<sub>e</sub></italic>: mean of test channel)</p></sec></sec></sec><sec id="S12" sec-type="results"><title>Results</title><p id="P20">In the following paragraphs, we describe how to identify and group LFP events in a continuous LFP data stream.</p><sec id="S13"><title>Detection of LFP events</title><p id="P21">In standard evaluation pipelines and especially in settings, where sensory stimuli are presented, LFP responses are averaged over several trials to remove the measurement noise. However, averaging over trials has several drawbacks such as the effect that inter-trial differences vanish and that always a well-defined stimulus and a time trigger is needed to group and align the recorded data. However, when the signal-to-noise ratio is large enough, then it is not necessary to average over several trials in intra-cranial recordings as the single trail events are already good enough to extract meaningful information. Thus, as shown in <xref ref-type="fig" rid="F1">Fig. 1a</xref>, single pure-tone stimuli of varying frequency and loudness do already evoke clear LFP responses. In most studies, the LFP responses as well as the spike rates are averaged to calculate spectro-receptive fields (STRFs, <xref ref-type="fig" rid="F1">Fig 1b</xref>) for different stimulus intensities, which then are used to calculate tuning curves for the respective neurons (examples of tuning curves for 3 neighbouring electrodes are shown in <xref ref-type="fig" rid="F1">Fig. 1</xref>). As described above, this approach is only possible in cases where the stimulus onset is known. However, similar LFP events also occur spontaneously without any external stimulus (see <xref ref-type="fig" rid="F1">Fig. 1a</xref> before stimulus onset). These LFP events are usually ignored. Nevertheless, the analysis of spontaneous activity plays a crucial role in several neuroscientific fields such as e.g. the investigation of ongoing phantom perception (e.g. tinnitus).</p><p id="P22">Indeed, LFP events also occur spontaneously, without any external stimulus (spontaneous activity measured at three neighbouring positions shown in <xref ref-type="fig" rid="F2">Fig. 2a</xref>). As a first step, these LFP events can be detected e.g. by an algorithm searching for local minima in the ongoing data stream. Additionally, invalid events may be filtered out by defining a threshold and removing events, where the latency between the events is too low. Thus, only the lowest minimum is taken into account (detected LFP events marked by black crosses in <xref ref-type="fig" rid="F2">Fig. 2a</xref>). A huge advantage of the local-minima-search-algorithm is the fact that the detected minimum already defines an exact time-point, which could for example be used to align and average multiple events. Furthermore, the extracted LFP events can be analyzed in terms of shape, size, time of occurrence, etc. (for an example of extracted LFP events see <xref ref-type="fig" rid="F2">Fig. 2</xref>). Thus, the data of the exemplary animal indicates that the inter-event-intervals (time between occurrence of two subsequent events) are distributed in an asymmetric (e.g. log-normal distributed) way, whereas the peak-to-peak amplitudes are more Gaussian-like distributed. Furthermore, the inter-event-intervals show a bi-modal distribution with 2 peaks at different time-points (approx. 1 s and 2 s). Besides these basic measures of the LFP events such as amplitude or inter-event-interval duration, the LFP events contain much more information. For instance, the shape of the events can provide further information on how information is processed in the cortex.</p></sec><sec id="S14"><title>Calculation of embeddings (latent space encodings) of LFP events</title><p id="P23">In order to extract meaningful information from the LFP events (e.g. the shapes of LFP events) it is necessary to find an efficient representation of these shapes.</p><p id="P24">Therefore, we calculated so called embeddings by setting up an auto-encoder, i.e. an encoder-decoder-network. This type of artificial neural network has a special network architecture, where the input layer and the output layer are of the same dimension, i.e. consist of equal number of neurons. The network is trained in a self-supervised way, i.e. on reproducing the input in the output layer. Thus, the cost function is the mean-squared error between input and output (schematic drawing of an auto-encoder in <xref ref-type="fig" rid="F3">Fig. 3a</xref>). A further important property of the auto-encoder is the so called bottleneck layer (green layer in <xref ref-type="fig" rid="F3">Fig. 3a</xref>). This means that the intermediate layer is smaller, i.e. has fewer neurons, than the input and output layer. Hence, the auto-encoder has to compress (encode) the input, and subsequently to decompress (decode) the activity in the bottleneck layer in order to reconstruct the input as output. The activation patterns of the bottleneck layer are called latent space embeddings (encodings). The idea behind this processing principle is, that the network has to reduce the dimensionality of the input without loosing relevant information for subsequently reconstructing the input as output again (cyan neurons in 3a). The performance of a given auto-encoder can be assessed by comparing input and output (<xref ref-type="fig" rid="F3">Fig. 3a</xref>). Over-fitting during training the network is prevented by adding dropout layers and testing the trained network) with an unknown test-data set (see <xref ref-type="fig" rid="F3">Fig. 3b</xref>).</p><p id="P25">However, auto-encoders are black boxes and in general it is not known which features of the input space are used to create the lower-dimensional latent space, i.e. what is the meaning of a particular dimension (neuron activity). Therefore, in order to make the encodings more interpretable, we systematically analyzed the emerging latent space embeddings. To this end, we directly activated the neurons in the bottleneck layer one by one without providing any input to the network. In particular, the activation vectors of the bottleneck layer were ((x,0,0), (0,x,0), or (0,0,x) for x∈ {0, 200, 400, 800, 1000, 1200, 1400}). These activation vectors were further propagated through the network to the output layer. The resulting outputs (<xref ref-type="fig" rid="F3">Fig. 3c1-c3</xref>) indicate that each encoding neuron is specialized to certain LFP shapes. For example, neuron 2 seems to encode low-frequency LFP events, whereas neuron 3 is associated with high-amplitude W-shaped signals.</p><p id="P26">The dimensionality reduction through the auto-encoding can be used to analyze large electrophysiological data sets such as recordings of spontaneous activity for e.g. 1 hour (<xref ref-type="fig" rid="F4">Fig. 4a-c</xref>). Note that, the encodings do neither depend on prior knowledge nor any labeled data. Actually, the auto-encoder only uses statistical features of the input such as the frequency of occurrence of certain LFP events. However, the encodings do not contain the full information of the underlying LFP event shapes, as certain information which is crucial to recosntruct the input is also stored in the decoding part of the neural network itself. Nevertheless, when certain clusters are identified, the shapes of this LFP events can be reconstructed using the decoder (examples of decoded inputs in <xref ref-type="fig" rid="F4">Fig. 4d-f</xref>). The two processing steps: (1) finding certain clusters in the embeddings, and (2) identifying the underlying LFP event shapes, provide efficient data processing on the one hand, minimize the information loss and preserve biological interpretability.</p><p id="P27">As the auto-encoder is exclusively trained on statistical features, the trained network can be applied to any kind of LFP data. Thus, we tested the auto-encoder trained on spontaneous activity to find clusters in evoked activity. Therefore, we used the evoked activity shown in <xref ref-type="fig" rid="F1">Fig. 1</xref> to check, if the auto-encoder actually helps to extract meaningful information. We analyzed 4 different sound pressure levels (110 dB–80 dB) and 3 frequencies (500 Hz, 1000 Hz, 2000 Hz). We could show that indeed the LFP responses induced by different stimulus frequencies form clusters (<xref ref-type="fig" rid="F5">Fig. 5a1-d1</xref>, for exemplary events and decoded LFP events see a-d 2-4). We quantified the quality of the clustering by calculating the generalized discrimination value (GDV) [<xref ref-type="bibr" rid="R50">50</xref>, <xref ref-type="bibr" rid="R51">51</xref>]. We could show that the the GDV is best for a stimulus intensity of 100 dB. Thus, too loud stimuli lead to unspecific activation due to recruiting of auditory nerve fibers. In contrast, too low stimulus intensities do not evoke clear responses. To put it in a nutshell, the auto-encoder trained on a different data set has proven to serve as universal tool to identify clusters of LFP events carrying the information on the stimulus frequency.</p></sec><sec id="S15"><title>Estimation of Information Loss Caused by Auto-Encoding using a Classifier Network</title><p id="P28">However, the dimensionality reduction by auto-encoding (<xref ref-type="fig" rid="F6">Fig. 6a</xref>) obviously causes a loss of information which needs to be quantified. In a previous study, we already introduced a classifier network as a tool to objectively quantify the amount of meaningful information of certain data [?]. Training a classifier network (<xref ref-type="fig" rid="F6">Fig. 6b</xref>) requires labeled data. We used a convolutional neural network which was trained on LFP responses induced by four different stimulus frequencies (500 Hz, 1000 Hz, 2000 Hz, 4000 Hz at 100 dB). To quantify the information loss due to auto-encoding/dimensionality reduction with respect to discriminability of different categories of LFP responses, two classifier networks were trained on stimulus frequencies. The original LFP events served as training input for the first classifier, whereas the second classifier was trained on the LFP events which were encoded and decoded by the auto-encoder. Note that, this auto-encoder was trained on data from a different animal (see <xref ref-type="fig" rid="F6">Fig. 6c</xref>). Indeed, the reconstructed input was of sufficient quality (see <xref ref-type="fig" rid="F6">Fig. 6d</xref>).</p><p id="P29">We could show that the classification accuracy (classes correspond to different stimulus frequencies) is slightly reduced for the reconstructed input compared to the unprocessed original input. Nevertheless, there is no large difference between validation accuracies (<xref ref-type="fig" rid="F6">Fig. 6f</xref>). Furthermore, the auto-encoded input data lead to less over-fitting, as the neural network is not trained on specific features or artifacts unique to certain trials (see <xref ref-type="fig" rid="F6">Fig. 6f</xref>). These features get lost during the process of auto-encoding since they are not relevant, i.e. carry no meaningful information, for reconstructing the input from the latent space embedding. Our analysis provides evidence that the information loss due to auto-encoding is acceptable and that the auto-encoder is a valid tool to reduce the dimensionality of the data. Up to now, the auto-encoding procedure was only applied to single trials of single channel data. In a next step, we show that the encoded data can be used to draw conclusions about information processing in the brain.</p></sec><sec id="S16"><title>The Relationship of Spontaneous and Evoked LFP events</title><p id="P30">We show that the embeddings of the LFP events evoked by pure tones form a subspace of a larger state space, which is spanned by the spontaneous LFP event embeddings (see <xref ref-type="fig" rid="F7">Fig. 7</xref>). Thus, during spontaneous activity the brain samples from the realm of possible stimulus evoked event shapes. This phenomenon was already described by Luczak and colleagues in 2009 [<xref ref-type="bibr" rid="R52">52</xref>] in the context of population coding. In their study, they analyzed multi-channel spiking activity of several neurons recorded with multi-electrode arrays. However, in our study, we confirm their findings by analyzing cortical activity patterns through the shape of single-channel LFP events.</p><p id="P31">So far, we restricted our analyses on LFP events from a single recording channel. In the following, we show that we can even derive information about the temporal dynamics of information flow from LFP events by taking into account two spatially separated recording channels.</p></sec><sec id="S17"><title>Auto-encoding, Clustering and Information Flow</title><p id="P32">Therefore, we detected spontaneously occurring LFP events in a certain reference channel and analyzed the activity in a spatially separated, neighboring channel (example for events in reference channel in <xref ref-type="fig" rid="F8">Fig. 8a</xref> blue curve and neighbouring channel green).</p><p id="P33">The the sign of the time difference between corresponding LFP events in two different channels indicates the direction of the information flow between the two recording sites. To quantify the time difference (latency) between corresponding events, the event-wise cross-correlation function between the two channels of interested was calculated (see <xref ref-type="fig" rid="F8">Fig. 8b</xref> black curve). For each pair of events in the two channels, the lag-time which leads to the maximum cross-correlation coefficient corresponds to the respective time difference between the occurrence of the same event at the two different channels. It turns out that there is no general lag-time/latency value that fits to all pairs of events. Instead, spontaneous activity is characterized by a distribution of latencies as shown in <xref ref-type="fig" rid="F8">Fig. 8c</xref>. This means that simply calculating the cross-correlation function over the complete LFP time series or spike trains of two recording channels as is frequently done as an ojective function of the synchrony between two channels is insufficient. For instance, an increased synchrony could either be caused by a more balanced distribution of negative and positive time delays (lag-times) between corresponding LFP events, or by smaller absolute values of the time delays. This information gets lost by simply calculating cross-correlation functions for the entire time series of both channels. Furthermore, it turns out that the resulting cross-correlation coefficients are also broadly distributed between 0 and 1 (<xref ref-type="fig" rid="F8">Fig. 8d</xref>), whereas there seems to be no clear dependence between resulting lag-time and maximum correlation coefficient. In <xref ref-type="fig" rid="F8">Figure 8e</xref>, a scatter plot of all pairs of cross-correlation coefficients and corresponding lag-times is shown. This indicates that the value of the cross-correlation coefficient is no reliable measure for the synchrony between two channels.</p><p id="P34">We took advantage of the previously calculated latent space embeddings of LFP event shapes to assess, if the time delays of LFP events between two channels correspond to the shape of the respective LFP event. Remarkably, the event shape embeddings, and consequently the event shapes, cluster according to the direction of information flow, i.e. the sign of the lag-time between two channels (<xref ref-type="fig" rid="F8">Fig. 8f</xref>). Finally, we decoded the embeddings again to visualize the corresponding LFP event shapes of the channel of interrest. We find that LFP event shapes characterized by larger amplitudes correspond to negative (blue) time delays, whereas broader shapes with smaller amplitudes correspond to positive (red) time delays, and hence indicate information flow in the opposite direction. Thus, it is possible to estimate the direction of information flow in the cortex by analyzing only a single recording channel. These findings indicate that it might be possible to assess changes in information flow direction, when the system is distorted e.g. by damages along the sensory pathway such as hearing loss caused by a noise trauma.</p><p id="P35">Indeed, noise trauma leads to changes of the information flow as seen in <xref ref-type="fig" rid="F9">Fig. 9</xref>. Depending on the electrode position, the embedding clusters changed when the silence condition (9a,b 1-2) is replaced by a 115 dB, 2 kHz auditory pure-tone trauma (9 a,b 3-4).</p></sec><sec id="S18"><title>Application of the method to human intra-cranial EEG (iEEG) data</title><p id="P36">The fact that the information flow can be determined by analyzing single channel LFP event shapes could be interesting for analyzing human intra-cranial EEG (iEEG) data. We applied the auto-encoding procedure on iEEG data recorded in the the auditory cortex of a human epilepsy patient. Indeed it is possible to use the local-minimum search algorithm on the recorded data to identify LFP events (see 10a,b, events are marked by x). Besides spontaneous activity also evoked activity was recorded: currents between 1 mA and 15 mA were applied to certain channels of the recording device. We distinguish between stimulation of channels, which are not our recording channels (see red markers in <xref ref-type="fig" rid="F10">Fig. 10a</xref>) and currents which cause artifacts (green markers <xref ref-type="fig" rid="F10">Fig. 10a</xref>). Note that, in principle, events labeled as spontaneous events might nevertheless be evoked events since we cannot fully exclude that there were no sounds or other auditory stimuli in the room during recording. Thus, for this analysis evoked events are defined as events which are evoked by an intra-cranially applied stimulation current. We find that auto-encoding is again a valuable technique to extract meaningful data from the iEEG data.</p><p id="P37">First, we demonstrate that the auto-encoder network produces valid reconstructions as output compared to the input data (see <xref ref-type="fig" rid="F10">Fig. 10c,d</xref>). Furthermore, evoked events lead to different LFP shapes than spontaneous LFP events. Even though, no distinct clusters in the embedding space could be observed, the median coordinate values of the three different conditions lead to three different reconstructed prototypical LFP event shapes (see <xref ref-type="fig" rid="F10">Fig. 10f</xref>).</p><p id="P38">The inter-channel cross-correlation analysis (<xref ref-type="fig" rid="F10">Fig. 10g,h</xref>) indicates that LFP event shape are correlated with lag-times between the channels. Thus, we find again a bi-modal lag-time distribution (<xref ref-type="fig" rid="F10">Fig. 10i</xref>). Furthermore, a correlation between LFP event shape and lag-time can be observed, at least for lag-times with an absolute value larger than 10 ms, (<xref ref-type="fig" rid="F10">Fig. 10j</xref>). Thus, the reconstruction of the average (prototypical) embeddings for lag-times larger than 10 ms and smaller than -10 ms indicate that at least the amplitude of the LFP events with negative lag times are increased (see also corresponding animal data in <xref ref-type="fig" rid="F8">Fig. 8g</xref> and <xref ref-type="fig" rid="F9">9a2, b4</xref>).</p></sec></sec><sec id="S19" sec-type="discussion"><title>Discussion</title><p id="P39">In the present study, we developed an analysis pipeline to identify, extract and characterize events from ongoing recordings of local field potentials (LFP). We applied a local minimum search algorithm in combination with a thresholding procedure to identify significant LFP events. In a next step, the dimensionality of the LFP event shapes is reduced using an auto-encoder network. In its bottleneck layer, the auto-encoder provides a low-dimensional representation (embedding) of the input data, which conserves relevant information to reconstruct the original shapes again as output. These embeddings are used to visualize the data and to identify potential stimulus-related clusters. Note that, the clusters result from the properties of the electrophysiological recordings and are not due to any pre-defined labels. The embeddings were further used to show that the shape of the LFP events is correlated with the direction of the information flow between different recording sites/channels (see <xref ref-type="fig" rid="F8">Fig. 8</xref>). In our example, sharp high-amplitude LFP events indicate that the source of the LFP event is located at the respective channel, whereas broad low-amplitude shapes indicate that the source is at the other channel. This means that LFP event shapes can be used to identify the location of sub-cortical input. However, auto-encoder based dimensionality reduction leads to worse interpretability of the underlying LFP event shapes, as the low-dimensional representations are highly abstract and auto-encoders, as deep learning in general, suffer from the so called black box problem [<xref ref-type="bibr" rid="R53">53</xref>]. Therefore, we calculated prototypical embeddings by averaging over all embeddings belonging to a certain stimulus conditions. Subsequently, we reconstructed the corresponding typical LFP shapes. These prototypical LFP event shapes could be used to make further assessments on cortical information processing. For instance, we obtained the remarkable result that during spontaneous activity without any stimulus, LFP event shapes ae sampled from the realm of possible stimulus evoked LFP event shapes. A phenomenon that has so far only been demonstrated in the context of multi-channel spike train population coding [<xref ref-type="bibr" rid="R52">52</xref>].</p><p id="P40">In summary, the benefit of using auto-encoders for processing LFP data can be divided into three major points. First, the dimensionality reduction allows for visualizing highly complex data sets. The fact that the embeddings can be reverse-engineered to prototypical LFP event shapes increases interpretability. Furthermore, there is another significant advantage of this procedure. The auto-encoder can be used to de-noise the data: as the auto-encoders are exclusively trained on statistical features of the LFP events, unique artifacts do not play a significant role. Thus, the encoding-decoding procedure can be used for artifact suppression (for some existing approaches see [<xref ref-type="bibr" rid="R32">32</xref>,<xref ref-type="bibr" rid="R54">54</xref>–<xref ref-type="bibr" rid="R56">56</xref>]). A further central finding of the study is, that lag-times of LFP events measured between two channels are broadly distributed and this distribution is bi-modal, i.e. with two maxima for positive and negative lag-times, respectively (see <xref ref-type="fig" rid="F8">Fig. 8c</xref> and <xref ref-type="fig" rid="F10">Fig. 10i</xref>). Our data indicates that the different LFP event shapes correspond to different information processing mechanisms. Since the analysis of neural synchrony is an important target to investigate auditory processing in the cortex, and is of particular meaning for different phantom perceptions such as tinnitus theories [<xref ref-type="bibr" rid="R57">57</xref>, <xref ref-type="bibr" rid="R58">58</xref>], this finding could be a starting point for further investigations. Indeed in most studies, synchrony between LFP streams from different channels is quantified by calculating the cross-correlation function between the entire signal streams, analyzing lag-times and the area under the cross-correlation curve [<xref ref-type="bibr" rid="R59">59</xref>]. However, our findings that lag-times correspond to different LFP event shapes and that lag-time histograms show two maxima indicate that standard synchrony analyses fail to provide the full picture. Applying the here presented novel approach to further investigate the functional plasticity after hearing loss, which is hypothesized to be the cause of tinnitus [<xref ref-type="bibr" rid="R60">60</xref>–<xref ref-type="bibr" rid="R66">66</xref>], might lead to a deeper understanding of the underlying processes.</p><p id="P41">However, analyzing LFP-events using auto-encoder networks has also some drawbacks, that have to be taken into account when interpreting the results. As we use the decoder part of the auto-encoder to generate reconstructions from average embeddings, we create novel LFP shapes and thus the decoder part of the network works as a generative model [<xref ref-type="bibr" rid="R67">67</xref>]. Overfitting is an important problem for all machine learning applications [<xref ref-type="bibr" rid="R68">68</xref>,<xref ref-type="bibr" rid="R69">69</xref>] and especially for generative models. When the auto-encoder network has too many trainable parameters negative side effects can occur. For instance, the network could store the whole information about different LFP shapes within the large weight matrix [<xref ref-type="bibr" rid="R70">70</xref>]. Therefore, the weight matrix would serve as some kind of list or look-up table for different LFP shapes, and the embedding layer would just learn random labels (indices) for the content of this list. This unwanted case would cause the effect that the embeddings alone do not contain any useful information about the LFP shape, because it would be possible to train further auto-encoder with arbitrary permutations of the embeddings, which perform equally good. However, in order to allow an interpretation of clusters in embedding space, neighbourhood relations between different LFP shapes should be conserved in the embedding layer [<xref ref-type="bibr" rid="R71">71</xref>]. Thus, in our study we used shallow-networks to reduce the number of parameters, and added drop-out layers to prevent the neural network from overfitting. We could show that the encoder does not simply add labels to the different LFP-shapes, because the self-organized emerging clusters actually correspond to different stimulus conditions (see <xref ref-type="fig" rid="F5">Fig. 5</xref>). As the neural network was not trained on these LFP event shapes yet from another data set and the clusters automatically emerge we could show that embeddings are not just random representations of the different LFP event shapes. Nevertheless, in a follow-up study, as a more sophisticated approach, variational auto-encoders [<xref ref-type="bibr" rid="R72">72</xref>] or transformer networks [<xref ref-type="bibr" rid="R73">73</xref>] could be used instead, which are further optimized to lead to better encondings and thus to more interpretable decodings [<xref ref-type="bibr" rid="R74">74</xref>,<xref ref-type="bibr" rid="R75">75</xref>].</p><p id="P42">Summing up, following the trend of integrating artificial intelligence and neuroscience [<xref ref-type="bibr" rid="R76">76</xref>–<xref ref-type="bibr" rid="R83">83</xref>], machine learning provides valuable tools to extract information from electrophysiological data [<xref ref-type="bibr" rid="R84">84</xref>–<xref ref-type="bibr" rid="R86">86</xref>]. As described above in most studies the data is averaged over many measurement trials to increase the signal to noise-ratio. However, the frequently performed averaging procedure erases any correlates of information processing taking place during recording of the ongoing continuous signal stream. Potentially, it is possible to translate that stream of voltage fluctuations into signs, which could be interpreted by humans using approaches from deep learning based natural language processing such as machine translation, or even from <italic>animal linguistics</italic>, where e.g. killer whale calls are identified, segmented, extracted and classified from ongoing continuous sound streams according to recurring feature patterns [<xref ref-type="bibr" rid="R87">87</xref>–<xref ref-type="bibr" rid="R90">90</xref>]. By that, we are convinced that our approach might further push the progress in neuroscience in order to extract meaningful information from continuous electrophysiological data streams.</p></sec></body><back><ack id="S20"><title>Acknowledgments</title><p id="P43">This work was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation): grant KR5148/2-1 (project number 436456810) to PK, and grant SCHI 1482/3-1 (project number 451810794) to AS. Furthermore, the research leading to these results has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (ERC Grant No. 810316 to AM).</p></ack><fn-group><fn id="FN1" fn-type="con"><p id="P44"><bold>Author contributions</bold></p><p id="P45">AS and PK conceived the study. PK supervised the study. AS, CB and JR performed the electrophysiological recordings in rodents. HH and CR provided human data. AS and RG wrote the evaluation software. AS, RG, AM, CM, HH and PK wrote the manuscript. All authors approved the final version of the manuscript.</p></fn></fn-group><ref-list><ref id="R1"><label>[1]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bollmann</surname><given-names>Saskia</given-names></name><name><surname>Barth</surname><given-names>Markus</given-names></name></person-group><article-title>New acquisition techniques and their prospects for the achievable resolution of fmri</article-title><source>Progress in neurobiology</source><year>2021</year><volume>207</volume><elocation-id>101936</elocation-id></element-citation></ref><ref id="R2"><label>[2]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Jing</given-names></name><name><surname>Yan</surname><given-names>Jiaqing</given-names></name><name><surname>Liu</surname><given-names>Xianzeng</given-names></name><name><surname>Ouyang</surname><given-names>Gaoxiang</given-names></name></person-group><article-title>Using permutation entropy to measure the changes in eeg signals during absence seizures</article-title><source>Entropy</source><year>2014</year><volume>16</volume><issue>6</issue><fpage>3049</fpage><lpage>3061</lpage></element-citation></ref><ref id="R3"><label>[3]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaiser</surname><given-names>David A</given-names></name></person-group><article-title>What is quantitative eeg?</article-title><source>Journal of Neurotherapy</source><year>2007</year><volume>10</volume><issue>4</issue><fpage>37</fpage><lpage>52</lpage></element-citation></ref><ref id="R4"><label>[4]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Başar</surname><given-names>E</given-names></name><name><surname>Rosen</surname><given-names>B</given-names></name><name><surname>Başar-Eroglu</surname><given-names>C</given-names></name><name><surname>Greitschus</surname><given-names>F</given-names></name></person-group><article-title>The associations between 40 hz-eeg and the middle latency response of the auditory evoked potential</article-title><source>International Journal of Neuroscience</source><year>1987</year><volume>33</volume><issue>1-2</issue><fpage>103</fpage><lpage>117</lpage></element-citation></ref><ref id="R5"><label>[5]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gajraj</surname><given-names>RJ</given-names></name><name><surname>Doi</surname><given-names>M</given-names></name><name><surname>Mantzaridis</surname><given-names>H</given-names></name><name><surname>Kenny</surname><given-names>GN</given-names></name></person-group><article-title>Analysis of the eeg bispectrum, auditory evoked potentials and the eeg power spectrum during repeated transitions from consciousness to unconsciousness</article-title><source>British Journal of Anaesthesia</source><year>1998</year><volume>80</volume><issue>1</issue><fpage>46</fpage><lpage>52</lpage></element-citation></ref><ref id="R6"><label>[6]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tonner</surname><given-names>PH</given-names></name><name><surname>Bein</surname><given-names>B</given-names></name></person-group><article-title>Classic electroencephalographic parameters: median frequency, spectral edge frequency etc</article-title><source>Best Practice &amp; Research Clinical Anaesthesiology</source><year>2006</year><volume>20</volume><issue>1</issue><fpage>147</fpage><lpage>159</lpage></element-citation></ref><ref id="R7"><label>[7]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Newson</surname><given-names>Jennifer J</given-names></name><name><surname>Thiagarajan</surname><given-names>Tara C</given-names></name></person-group><article-title>Eeg frequency bands in psychiatric disorders: a review of resting state studies</article-title><source>Frontiers in human neuroscience</source><year>2019</year><volume>12</volume><elocation-id>521</elocation-id></element-citation></ref><ref id="R8"><label>[8]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waterstraat</surname><given-names>Gunnar</given-names></name><name><surname>Körber</surname><given-names>Rainer</given-names></name><name><surname>Storm</surname><given-names>Jan-Hendrik</given-names></name><name><surname>Curio</surname><given-names>Gabriel</given-names></name></person-group><article-title>Noninvasive neuromagnetic single-trial analysis of human neocortical population spikes</article-title><source>Proceedings of the National Academy of Sciences</source><year>2021</year><volume>118</volume><issue>11</issue></element-citation></ref><ref id="R9"><label>[9]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kovac</surname><given-names>Stjepana</given-names></name><name><surname>Vakharia</surname><given-names>Vejay N</given-names></name><name><surname>Scott</surname><given-names>Catherine</given-names></name><name><surname>Diehl</surname><given-names>Beate</given-names></name></person-group><article-title>Invasive epilepsy surgery evaluation</article-title><source>Seizure</source><year>2017</year><volume>44</volume><fpage>125</fpage><lpage>136</lpage></element-citation></ref><ref id="R10"><label>[10]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Staresina</surname><given-names>Bernhard P</given-names></name><name><surname>Bergmann</surname><given-names>Til Ole</given-names></name><name><surname>Bonnefond</surname><given-names>Mathilde</given-names></name><name><surname>Van Der Meij</surname><given-names>Roemer</given-names></name><name><surname>Jensen</surname><given-names>Ole</given-names></name><name><surname>Deuker</surname><given-names>Lorena</given-names></name><name><surname>Elger</surname><given-names>Christian E</given-names></name><name><surname>Axmacher</surname><given-names>Nikolai</given-names></name><name><surname>Fell</surname><given-names>Juergen</given-names></name></person-group><article-title>Hierarchical nesting of slow oscillations, spindles and ripples in the human hippocampus during sleep</article-title><source>Nature neuroscience</source><year>2015</year><volume>18</volume><issue>11</issue><fpage>1679</fpage><lpage>1686</lpage></element-citation></ref><ref id="R11"><label>[11]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname><given-names>György</given-names></name><name><surname>Anastassiou</surname><given-names>Costas A</given-names></name><name><surname>Koch</surname><given-names>Christof</given-names></name></person-group><article-title>The origin of extracellular fields and currents—eeg, ecog, lfp and spikes</article-title><source>Nature reviews neuroscience</source><year>2012</year><volume>13</volume><issue>6</issue><fpage>407</fpage><lpage>420</lpage></element-citation></ref><ref id="R12"><label>[12]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mormann</surname><given-names>Florian</given-names></name><name><surname>Kornblith</surname><given-names>Simon</given-names></name><name><surname>Cerf</surname><given-names>Moran</given-names></name><name><surname>Ison</surname><given-names>Matias J</given-names></name><name><surname>Kraskov</surname><given-names>Alexander</given-names></name><name><surname>Tran</surname><given-names>Michelle</given-names></name><name><surname>Knieling</surname><given-names>Simeon</given-names></name><name><surname>Quiroga</surname><given-names>Rodrigo Quian</given-names></name><name><surname>Koch</surname><given-names>Christof</given-names></name><name><surname>Fried</surname><given-names>Itzhak</given-names></name></person-group><article-title>Scene-selective coding by single neurons in the human parahippocampal cortex</article-title><source>Proceedings of the National Academy of Sciences</source><year>2017</year><volume>114</volume><issue>5</issue><fpage>1153</fpage><lpage>1158</lpage></element-citation></ref><ref id="R13"><label>[13]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hagen</surname><given-names>Espen</given-names></name><name><surname>Dahmen</surname><given-names>David</given-names></name><name><surname>Stavrinou</surname><given-names>Maria L</given-names></name><name><surname>Lindén</surname><given-names>Henrik</given-names></name><name><surname>Tetzlaff</surname><given-names>Tom</given-names></name><name><surname>Van Albada</surname><given-names>Sacha J</given-names></name><name><surname>Grün</surname><given-names>Sonja</given-names></name><name><surname>Diesmann</surname><given-names>Markus</given-names></name><name><surname>Einevoll</surname><given-names>Gaute T</given-names></name></person-group><article-title>Hybrid scheme for modeling local field potentials from point-neuron networks</article-title><source>Cerebral cortex</source><year>2016</year><fpage>1</fpage><lpage>36</lpage></element-citation></ref><ref id="R14"><label>[14]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kajikawa</surname><given-names>Yoshinao</given-names></name><name><surname>Schroeder</surname><given-names>Charles E</given-names></name></person-group><article-title>How local is the local field potential?</article-title><source>Neuron</source><year>2011</year><volume>72</volume><issue>5</issue><fpage>847</fpage><lpage>858</lpage></element-citation></ref><ref id="R15"><label>[15]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lindén</surname><given-names>Henrik</given-names></name><name><surname>Tetzlaff</surname><given-names>Tom</given-names></name><name><surname>Potjans</surname><given-names>Tobias C</given-names></name><name><surname>Pettersen</surname><given-names>Klas H</given-names></name><name><surname>Grün</surname><given-names>Sonja</given-names></name><name><surname>Diesmann</surname><given-names>Markus</given-names></name><name><surname>Einevoll</surname><given-names>Gaute T</given-names></name></person-group><article-title>Modeling the spatial reach of the lfp</article-title><source>Neuron</source><year>2011</year><volume>72</volume><issue>5</issue><fpage>859</fpage><lpage>872</lpage></element-citation></ref><ref id="R16"><label>[16]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kraskov</surname><given-names>Alexander</given-names></name><name><surname>Quiroga</surname><given-names>Rodrigo Quian</given-names></name><name><surname>Reddy</surname><given-names>Leila</given-names></name><name><surname>Fried</surname><given-names>Itzhak</given-names></name><name><surname>Koch</surname><given-names>Christof</given-names></name></person-group><article-title>Local field potentials and spikes in the human medial temporal lobe are selective to image category</article-title><source>Journal of cognitive neuroscience</source><year>2007</year><volume>19</volume><issue>3</issue><fpage>479</fpage><lpage>492</lpage></element-citation></ref><ref id="R17"><label>[17]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kreiman</surname><given-names>Gabriel</given-names></name><name><surname>Hung</surname><given-names>Chou P</given-names></name><name><surname>Kraskov</surname><given-names>Alexander</given-names></name><name><surname>Quiroga</surname><given-names>Rodrigo Quian</given-names></name><name><surname>Poggio</surname><given-names>Tomaso</given-names></name><name><surname>DiCarlo</surname><given-names>James J</given-names></name></person-group><article-title>Object selectivity of local field potentials and spikes in the macaque inferior temporal cortex</article-title><source>Neuron</source><year>2006</year><volume>49</volume><issue>3</issue><fpage>433</fpage><lpage>445</lpage></element-citation></ref><ref id="R18"><label>[18]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Logothetis</surname><given-names>Nikos K</given-names></name></person-group><article-title>The underpinnings of the bold functional magnetic resonance imaging signal</article-title><source>Journal of Neuroscience</source><year>2003</year><volume>23</volume><issue>10</issue><fpage>3963</fpage><lpage>3971</lpage></element-citation></ref><ref id="R19"><label>[19]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jackson</surname><given-names>Andrew</given-names></name><name><surname>Hall</surname><given-names>Thomas M</given-names></name></person-group><article-title>Decoding local field potentials for neural interfaces</article-title><source>IEEE Transactions on Neural Systems and Rehabilitation Engineering</source><year>2016</year><volume>25</volume><issue>10</issue><fpage>1705</fpage><lpage>1714</lpage></element-citation></ref><ref id="R20"><label>[20]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Constantinou</surname><given-names>Maria</given-names></name><name><surname>Cogno</surname><given-names>Soledad Gonzalo</given-names></name><name><surname>Elijah</surname><given-names>Daniel H</given-names></name><name><surname>Kropff</surname><given-names>Emilio</given-names></name><name><surname>Gigg</surname><given-names>John</given-names></name><name><surname>Samengo</surname><given-names>Inés</given-names></name><name><surname>Montemurro</surname><given-names>Marcelo A</given-names></name></person-group><article-title>Bursting neurons in the hippocampal formation encode features of lfp rhythms</article-title><source>Frontiers in computational neuroscience</source><year>2016</year><volume>10</volume><elocation-id>133</elocation-id></element-citation></ref><ref id="R21"><label>[21]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bukhtiyarova</surname><given-names>Olga</given-names></name><name><surname>Chauvette</surname><given-names>Sylvain</given-names></name><name><surname>Seigneur</surname><given-names>Josée</given-names></name><name><surname>Timofeev</surname><given-names>Igor</given-names></name></person-group><article-title>Brain states in freely behaving marmosets</article-title><source>Sleep</source><year>2022</year></element-citation></ref><ref id="R22"><label>[22]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keshtkaran</surname><given-names>Mohammad Reza</given-names></name><name><surname>Yang</surname><given-names>Zhi</given-names></name></person-group><article-title>Noise-robust unsupervised spike sorting based on discriminative subspace learning with outlier handling</article-title><source>Journal of neural engineering</source><year>2017</year><volume>14</volume><issue>3</issue><elocation-id>036003</elocation-id></element-citation></ref><ref id="R23"><label>[23]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Zhisong</given-names></name><name><surname>Maier</surname><given-names>Alexander</given-names></name><name><surname>Leopold</surname><given-names>David A</given-names></name><name><surname>Logothetis</surname><given-names>Nikos K</given-names></name><name><surname>Liang</surname><given-names>Hualou</given-names></name></person-group><article-title>Single-trial evoked potential estimation using wavelets</article-title><source>Computers in Biology and Medicine</source><year>2007</year><volume>37</volume><issue>4</issue><fpage>463</fpage><lpage>473</lpage></element-citation></ref><ref id="R24"><label>[24]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mahmud</surname><given-names>Mufti</given-names></name><name><surname>Cecchetto</surname><given-names>Claudia</given-names></name><name><surname>Vassanelli</surname><given-names>Stefano</given-names></name></person-group><article-title>An automated method for characterization of evoked single-trial local field potentials recorded from rat barrel cortex under mechanical whisker stimulation</article-title><source>Cognitive Computation</source><year>2016</year><volume>8</volume><issue>5</issue><fpage>935</fpage><lpage>945</lpage></element-citation></ref><ref id="R25"><label>[25]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Golshan</surname><given-names>Hosein M</given-names></name><name><surname>Hebb</surname><given-names>Adam O</given-names></name><name><surname>Hanrahan</surname><given-names>Sara J</given-names></name><name><surname>Nedrud</surname><given-names>Joshua</given-names></name><name><surname>Mahoor</surname><given-names>Mohammad H</given-names></name></person-group><source>A multiple kernel learning approach for human behavioral task classification using stn-lfp signal</source><conf-name>2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</conf-name><conf-sponsor>IEEE</conf-sponsor><year>2016</year><fpage>1030</fpage><lpage>1033</lpage></element-citation></ref><ref id="R26"><label>[26]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Nurse</surname><given-names>Ewan</given-names></name><name><surname>Mashford</surname><given-names>Benjamin S</given-names></name><name><surname>Yepes</surname><given-names>Antonio Jimeno</given-names></name><name><surname>Kiral-Kornek</surname><given-names>Isabell</given-names></name><name><surname>Harrer</surname><given-names>Stefan</given-names></name><name><surname>Freestone</surname><given-names>Dean R</given-names></name></person-group><source>Decoding eeg and lfp signals using deep learning: heading truenorth</source><conf-name>Proceedings of the ACM international conference on computing frontiers</conf-name><year>2016</year><fpage>259</fpage><lpage>266</lpage></element-citation></ref><ref id="R27"><label>[27]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krauss</surname><given-names>Patrick</given-names></name><name><surname>Metzner</surname><given-names>Claus</given-names></name><name><surname>Lange</surname><given-names>Janina</given-names></name><name><surname>Lang</surname><given-names>Nadine</given-names></name><name><surname>Fabry</surname><given-names>Ben</given-names></name></person-group><article-title>Parameter-free binarization and skeletonization of fiber networks from confocal image stacks</article-title><source>PLoS One</source><year>2012</year><volume>7</volume><issue>5</issue><elocation-id>e36575</elocation-id></element-citation></ref><ref id="R28"><label>[28]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pang</surname><given-names>Rich</given-names></name><name><surname>Lansdell</surname><given-names>Benjamin J</given-names></name><name><surname>Fairhall</surname><given-names>Adrienne L</given-names></name></person-group><article-title>Dimensionality reduction in neuroscience</article-title><source>Current Biology</source><year>2016</year><volume>26</volume><issue>14</issue><fpage>R656</fpage><lpage>R660</lpage></element-citation></ref><ref id="R29"><label>[29]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cunningham</surname><given-names>John P</given-names></name><name><surname>Byron</surname><given-names>M Yu</given-names></name></person-group><article-title>Dimensionality reduction for large-scale neural recordings</article-title><source>Nature neuroscience</source><year>2014</year><volume>17</volume><issue>11</issue><fpage>1500</fpage><lpage>1509</lpage></element-citation></ref><ref id="R30"><label>[30]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Yasi</given-names></name><name><surname>Yao</surname><given-names>Hongxun</given-names></name><name><surname>Zhao</surname><given-names>Sicheng</given-names></name></person-group><article-title>Auto-encoder based dimensionality reduction</article-title><source>Neurocomputing</source><year>2016</year><volume>184</volume><fpage>232</fpage><lpage>242</lpage></element-citation></ref><ref id="R31"><label>[31]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mackevicius</surname><given-names>Emily L</given-names></name><name><surname>Bahle</surname><given-names>Andrew H</given-names></name><name><surname>Williams</surname><given-names>Alex H</given-names></name><name><surname>Gu</surname><given-names>Shijie</given-names></name><name><surname>Denisenko</surname><given-names>Natalia I</given-names></name><name><surname>Goldman</surname><given-names>Mark S</given-names></name><name><surname>Fee</surname><given-names>Michale S</given-names></name></person-group><article-title>Unsupervised discovery of temporal sequences in high-dimensional datasets, with applications to neuroscience</article-title><source>Elife</source><year>2019</year><volume>8</volume><elocation-id>e38471</elocation-id></element-citation></ref><ref id="R32"><label>[32]</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Hardcastle</surname><given-names>Thomas J</given-names></name><name><surname>Lee</surname><given-names>Susannah</given-names></name><name><surname>Wernisch</surname><given-names>Lorenz</given-names></name><name><surname>Fortier-Poisson</surname><given-names>Pascal</given-names></name><name><surname>Shunmugam</surname><given-names>Sudha</given-names></name><name><surname>Hewage</surname><given-names>Kalon</given-names></name><name><surname>Edwards</surname><given-names>Tris</given-names></name><name><surname>Armitage</surname><given-names>Oliver</given-names></name><name><surname>Hewage</surname><given-names>Emil</given-names></name></person-group><source>Coordinate-vae: Unsupervised clustering and de-noising of peripheral nervous system data</source><year>2019</year></element-citation></ref><ref id="R33"><label>[33]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>Ding</given-names></name><name><surname>Wei</surname><given-names>Xue-Xin</given-names></name></person-group><article-title>Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-vae</article-title><source>Advances in Neural Information Processing Systems</source><year>2020</year><volume>33</volume><fpage>7234</fpage><lpage>7247</lpage></element-citation></ref><ref id="R34"><label>[34]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ran</surname><given-names>Xuming</given-names></name><name><surname>Zhang</surname><given-names>Jie</given-names></name><name><surname>Ye</surname><given-names>Ziyuan</given-names></name><name><surname>Wu</surname><given-names>Haiyan</given-names></name><name><surname>Xu</surname><given-names>Qi</given-names></name><name><surname>Zhou</surname><given-names>Huihui</given-names></name><name><surname>Liu</surname><given-names>Quanying</given-names></name></person-group><article-title>Deep auto-encoder with neural response</article-title><source>arXiv preprint</source><year>2021</year><elocation-id>arXiv:2111.15309</elocation-id></element-citation></ref><ref id="R35"><label>[35]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schilling</surname><given-names>Achim</given-names></name><name><surname>Tomasello</surname><given-names>Rosario</given-names></name><name><surname>Henningsen-Schomers</surname><given-names>Malte R</given-names></name><name><surname>Zankl</surname><given-names>Alexandra</given-names></name><name><surname>Surendra</surname><given-names>Kishore</given-names></name><name><surname>Haller</surname><given-names>Martin</given-names></name><name><surname>Karl</surname><given-names>Valerie</given-names></name><name><surname>Uhrig</surname><given-names>Peter</given-names></name><name><surname>Maier</surname><given-names>Andreas</given-names></name><name><surname>Krauss</surname><given-names>Patrick</given-names></name></person-group><article-title>Analysis of continuous neuronal activity evoked by natural speech with computational corpus linguistics methods</article-title><source>Language, Cognition and Neuroscience</source><year>2021</year><volume>36</volume><issue>2</issue><fpage>167</fpage><lpage>186</lpage></element-citation></ref><ref id="R36"><label>[36]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Metzner</surname><given-names>Claus</given-names></name><name><surname>Schilling</surname><given-names>Achim</given-names></name><name><surname>Traxdorf</surname><given-names>Maximilian</given-names></name><name><surname>Schulze</surname><given-names>Holger</given-names></name><name><surname>Krauss</surname><given-names>Patrick</given-names></name></person-group><article-title>Sleep as a random walk: a super-statistical analysis of eeg data across sleep stages</article-title><source>Communications Biology</source><year>2021</year><volume>4</volume><issue>1</issue><fpage>1</fpage><lpage>11</lpage></element-citation></ref><ref id="R37"><label>[37]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krauss</surname><given-names>Patrick</given-names></name><name><surname>Metzner</surname><given-names>Claus</given-names></name><name><surname>Joshi</surname><given-names>Nidhi</given-names></name><name><surname>Schulze</surname><given-names>Holger</given-names></name><name><surname>Traxdorf</surname><given-names>Maximilian</given-names></name><name><surname>Maier</surname><given-names>Andreas</given-names></name><name><surname>Schilling</surname><given-names>Achim</given-names></name></person-group><article-title>Analysis and visualization of sleep stages based on deep neural networks</article-title><source>Neurobiology of sleep and circadian rhythms</source><year>2021</year><volume>10</volume><elocation-id>100064</elocation-id></element-citation></ref><ref id="R38"><label>[38]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garibyan</surname><given-names>Armine</given-names></name><name><surname>Schilling</surname><given-names>Achim</given-names></name><name><surname>Boehm</surname><given-names>Claudia</given-names></name><name><surname>Zankl</surname><given-names>Alexandra</given-names></name><name><surname>Krauss</surname><given-names>Patrick</given-names></name></person-group><article-title>Neural correlates of linguistic collocations during continuous speech perception</article-title><source>bioRxiv</source><year>2022</year></element-citation></ref><ref id="R39"><label>[39]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Der Walt</surname><given-names>Stefan</given-names></name><name><surname>Colbert</surname><given-names>S Chris</given-names></name><name><surname>Varoquaux</surname><given-names>Gael</given-names></name></person-group><article-title>The numpy array: a structure for efficient numerical computation</article-title><source>Computing in science &amp; engineering</source><year>2011</year><volume>13</volume><issue>2</issue><fpage>22</fpage><lpage>30</lpage></element-citation></ref><ref id="R40"><label>[40]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Meier</surname><given-names>Burkhard</given-names></name></person-group><source>Python GUI Programming Cookbook: Develop functional and responsive user interfaces with tkinter and PyQt5</source><publisher-name>Packt Publishing Ltd</publisher-name><year>2019</year></element-citation></ref><ref id="R41"><label>[41]</label><element-citation publication-type="web"><collab>BlackrockNeurotech</collab><year>2021</year><comment><ext-link ext-link-type="uri" xlink:href="https://github.com/BlackrockNeurotech/Python-Utilities">https://github.com/BlackrockNeurotech/Python-Utilities</ext-link></comment></element-citation></ref><ref id="R42"><label>[42]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Virtanen</surname><given-names>Pauli</given-names></name><name><surname>Gommers</surname><given-names>Ralf</given-names></name><name><surname>Oliphant</surname><given-names>Travis E</given-names></name><name><surname>Haberland</surname><given-names>Matt</given-names></name><name><surname>Reddy</surname><given-names>Tyler</given-names></name><name><surname>Cournapeau</surname><given-names>David</given-names></name><name><surname>Burovski</surname><given-names>Evgeni</given-names></name><name><surname>Peterson</surname><given-names>Pearu</given-names></name><name><surname>Weckesser</surname><given-names>Warren</given-names></name><name><surname>Bright</surname><given-names>Jonathan</given-names></name><etal/></person-group><article-title>Scipy 1.0: fundamental algorithms for scientific computing in python</article-title><source>Nature methods</source><year>2020</year><volume>17</volume><issue>3</issue><fpage>261</fpage><lpage>272</lpage></element-citation></ref><ref id="R43"><label>[43]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chollet</surname><given-names>Francois</given-names></name></person-group><source>Deep learning mit python und keras: das praxis-handbuch vom entwickler der keras-bibliothek</source><publisher-name>MITP-Verlags GmbH &amp; Co. KG</publisher-name><year>2018</year></element-citation></ref><ref id="R44"><label>[44]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pang</surname><given-names>Bo</given-names></name><name><surname>Nijkamp</surname><given-names>Erik</given-names></name><name><surname>Wu</surname><given-names>Ying Nian</given-names></name></person-group><article-title>Deep learning with tensorflow: A review</article-title><source>Journal of Educational and Behavioral Statistics</source><year>2020</year><volume>45</volume><issue>2</issue><fpage>227</fpage><lpage>248</lpage></element-citation></ref><ref id="R45"><label>[45]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunter</surname><given-names>John D</given-names></name></person-group><article-title>Matplotlib: A 2d graphics environment</article-title><source>Computing in science &amp; engineering</source><year>2007</year><volume>9</volume><issue>03</issue><fpage>90</fpage><lpage>95</lpage></element-citation></ref><ref id="R46"><label>[46]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gerum</surname><given-names>Richard</given-names></name></person-group><article-title>Pylustrator: code generation for reproducible figures for publication</article-title><source>arXiv preprint</source><year>2019</year><elocation-id>arXiv:1910.00279</elocation-id></element-citation></ref><ref id="R47"><label>[47]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dasgupta</surname><given-names>Sanjoy</given-names></name><name><surname>Stevens</surname><given-names>Charles F</given-names></name><name><surname>Navlakha</surname><given-names>Saket</given-names></name></person-group><article-title>A neural algorithm for a fundamental computing problem</article-title><source>Science</source><year>2017</year><volume>358</volume><issue>6364</issue><fpage>793</fpage><lpage>796</lpage></element-citation></ref><ref id="R48"><label>[48]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>Zijin</given-names></name><name><surname>Schilling</surname><given-names>Achim</given-names></name><name><surname>Maier</surname><given-names>Andreas</given-names></name><name><surname>Krauss</surname><given-names>Patrick</given-names></name></person-group><chapter-title>Neural networks with fixed binary random projections improve accuracy in classifying noisy data</chapter-title><source>Bildverarbeitung für die Medizin 2021</source><publisher-name>Springer</publisher-name><year>2021</year><fpage>211</fpage><lpage>216</lpage></element-citation></ref><ref id="R49"><label>[49]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoo</surname><given-names>Jae-Chern</given-names></name><name><surname>Han</surname><given-names>Tae Hee</given-names></name></person-group><article-title>Fast normalized cross-correlation</article-title><source>Circuits, systems and signal processing</source><year>2009</year><volume>28</volume><issue>6</issue><fpage>819</fpage><lpage>843</lpage></element-citation></ref><ref id="R50"><label>[50]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schilling</surname><given-names>Achim</given-names></name><name><surname>Maier</surname><given-names>Andreas</given-names></name><name><surname>Gerum</surname><given-names>Richard</given-names></name><name><surname>Metzner</surname><given-names>Claus</given-names></name><name><surname>Krauss</surname><given-names>Patrick</given-names></name></person-group><article-title>Quantifying the separability of data classes in neural networks</article-title><source>Neural Networks</source><year>2021</year><volume>139</volume><fpage>278</fpage><lpage>293</lpage></element-citation></ref><ref id="R51"><label>[51]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krauss</surname><given-names>Patrick</given-names></name><name><surname>Metzner</surname><given-names>Claus</given-names></name><name><surname>Schilling</surname><given-names>Achim</given-names></name><name><surname>Tziridis</surname><given-names>Konstantin</given-names></name><name><surname>Traxdorf</surname><given-names>Maximilian</given-names></name><name><surname>Wollbrink</surname><given-names>Andreas</given-names></name><name><surname>Rampp</surname><given-names>Stefan</given-names></name><name><surname>Pantev</surname><given-names>Christo</given-names></name><name><surname>Schulze</surname><given-names>Holger</given-names></name></person-group><article-title>A statistical method for analyzing and comparing spatiotemporal cortical activation patterns</article-title><source>Scientific reports</source><year>2018</year><volume>8</volume><issue>1</issue><fpage>1</fpage><lpage>9</lpage></element-citation></ref><ref id="R52"><label>[52]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luczak</surname><given-names>Artur</given-names></name><name><surname>Barthó</surname><given-names>Peter</given-names></name><name><surname>Harris</surname><given-names>Kenneth D</given-names></name></person-group><article-title>Spontaneous events outline the realm of possible sensory responses in neocortical populations</article-title><source>Neuron</source><year>2009</year><volume>62</volume><issue>3</issue><fpage>413</fpage><lpage>425</lpage></element-citation></ref><ref id="R53"><label>[53]</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Voosen</surname><given-names>Paul</given-names></name></person-group><source>The ai detectives</source><year>2017</year></element-citation></ref><ref id="R54"><label>[54]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nishio</surname><given-names>Mizuho</given-names></name><name><surname>Nagashima</surname><given-names>Chihiro</given-names></name><name><surname>Hirabayashi</surname><given-names>Saori</given-names></name><name><surname>Ohnishi</surname><given-names>Akinori</given-names></name><name><surname>Sasaki</surname><given-names>Kaori</given-names></name><name><surname>Sagawa</surname><given-names>Tomoyuki</given-names></name><name><surname>Hamada</surname><given-names>Masayuki</given-names></name><name><surname>Yamashita</surname><given-names>Tatsuo</given-names></name></person-group><article-title>Convolutional auto-encoder for image denoising of ultra-low-dose ct</article-title><source>Heliyon</source><year>2017</year><volume>3</volume><issue>8</issue><elocation-id>e00393</elocation-id></element-citation></ref><ref id="R55"><label>[55]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Xinyang</given-names></name><name><surname>Zhang</surname><given-names>Guoxun</given-names></name><name><surname>Wu</surname><given-names>Jiamin</given-names></name><name><surname>Zhang</surname><given-names>Yuanlong</given-names></name><name><surname>Zhao</surname><given-names>Zhifeng</given-names></name><name><surname>Lin</surname><given-names>Xing</given-names></name><name><surname>Qiao</surname><given-names>Hui</given-names></name><name><surname>Xie</surname><given-names>Hao</given-names></name><name><surname>Wang</surname><given-names>Haoqian</given-names></name><name><surname>Fang</surname><given-names>Lu</given-names></name><etal/></person-group><article-title>Reinforcing neuron extraction and spike inference in calcium imaging using deep self-supervised denoising</article-title><source>Nature Methods</source><year>2021</year><volume>18</volume><issue>11</issue><fpage>1395</fpage><lpage>1400</lpage></element-citation></ref><ref id="R56"><label>[56]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Coster</surname><given-names>T</given-names></name><name><surname>Kudryashova</surname><given-names>N</given-names></name><name><surname>Derevyanko</surname><given-names>G</given-names></name><name><surname>De Vries</surname><given-names>AAF</given-names></name><name><surname>Pijnappels</surname><given-names>DA</given-names></name><name><surname>Panfilov</surname><given-names>AV</given-names></name></person-group><article-title>Identification of electrical rotational activity in noisy cardiac tissue recordings using a deep neural network</article-title><source>Europace</source><year>2022</year><volume>24</volume><issue>Supplement 1</issue><elocation-id>euac053–620</elocation-id></element-citation></ref><ref id="R57"><label>[57]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tass</surname><given-names>Peter A</given-names></name><name><surname>Popovych</surname><given-names>Oleksandr V</given-names></name></person-group><article-title>Unlearning tinnitus-related cerebral synchrony with acoustic coordinated reset stimulation: theoretical concept and modelling</article-title><source>Biological Cybernetics</source><year>2012</year><volume>106</volume><issue>1</issue><fpage>27</fpage><lpage>36</lpage></element-citation></ref><ref id="R58"><label>[58]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eggermont</surname><given-names>Jos J</given-names></name><name><surname>Tass</surname><given-names>Peter A</given-names></name></person-group><article-title>Maladaptive neural synchrony in tinnitus: origin and restoration</article-title><source>Frontiers in neurology</source><year>2015</year><volume>6</volume><elocation-id>29</elocation-id></element-citation></ref><ref id="R59"><label>[59]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eggermont</surname><given-names>Jos J</given-names></name><name><surname>Munguia</surname><given-names>Raymundo</given-names></name><name><surname>Pienkowski</surname><given-names>Martin</given-names></name><name><surname>Shaw</surname><given-names>Greg</given-names></name></person-group><article-title>Comparison of lfp-based and spike-based spectro-temporal receptive fields and cross-correlation in cat primary auditory cortex</article-title><source>PloS one</source><year>2011</year><volume>6</volume><issue>5</issue><elocation-id>e20046</elocation-id></element-citation></ref><ref id="R60"><label>[60]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schilling</surname><given-names>Achim</given-names></name><name><surname>Sedley</surname><given-names>William</given-names></name><name><surname>Gerum</surname><given-names>Richard</given-names></name><name><surname>Metzner</surname><given-names>Claus</given-names></name><name><surname>Tziridis</surname><given-names>Konstantin</given-names></name><name><surname>Maier</surname><given-names>Andreas</given-names></name><name><surname>Schulze</surname><given-names>Holger</given-names></name><name><surname>Zeng</surname><given-names>Fan-Gang</given-names></name><name><surname>Friston</surname><given-names>Karl J</given-names></name><name><surname>Krauss</surname><given-names>Patrick</given-names></name></person-group><article-title>Predictive coding and stochastic resonance: Towards a unified theory of auditory (phantom) perception</article-title><source>arXiv preprint</source><year>2022</year><elocation-id>arXiv:2204.03354</elocation-id></element-citation></ref><ref id="R61"><label>[61]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krauss</surname><given-names>P</given-names></name><name><surname>Schilling</surname><given-names>A</given-names></name><name><surname>Tziridis</surname><given-names>K</given-names></name><name><surname>Schulze</surname><given-names>H</given-names></name></person-group><article-title>Models of tinnitus development: From cochlea to cortex</article-title><source>HNO</source><year>2019</year><volume>67</volume><issue>3</issue><fpage>172</fpage><lpage>177</lpage></element-citation></ref><ref id="R62"><label>[62]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schaette</surname><given-names>Roland</given-names></name><name><surname>McAlpine</surname><given-names>David</given-names></name></person-group><article-title>Tinnitus with a normal audiogram: physiological evidence for hidden hearing loss and computational model</article-title><source>Journal of Neuroscience</source><year>2011</year><volume>31</volume><issue>38</issue><fpage>13452</fpage><lpage>13457</lpage></element-citation></ref><ref id="R63"><label>[63]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schilling</surname><given-names>Achim</given-names></name><name><surname>Tziridis</surname><given-names>Konstantin</given-names></name><name><surname>Schulze</surname><given-names>Holger</given-names></name><name><surname>Krauss</surname><given-names>Patrick</given-names></name></person-group><article-title>The stochastic resonance model of auditory perception: A unified explanation of tinnitus development, zwicker tone illusion, and residual inhibition</article-title><source>Progress in brain research</source><year>2021</year><volume>262</volume><fpage>139</fpage><lpage>157</lpage></element-citation></ref><ref id="R64"><label>[64]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krauss</surname><given-names>Patrick</given-names></name><name><surname>Tziridis</surname><given-names>Konstantin</given-names></name><name><surname>Metzner</surname><given-names>Claus</given-names></name><name><surname>Schilling</surname><given-names>Achim</given-names></name><name><surname>Hoppe</surname><given-names>Ulrich</given-names></name><name><surname>Schulze</surname><given-names>Holger</given-names></name></person-group><article-title>Stochastic resonance controlled upregulation of internal noise after hearing loss as a putative cause of tinnitus-related neuronal hyperactivity</article-title><source>Frontiers in neuroscience</source><year>2016</year><volume>10</volume><elocation-id>597</elocation-id></element-citation></ref><ref id="R65"><label>[65]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schilling</surname><given-names>Achim</given-names></name><name><surname>Gerum</surname><given-names>Richard</given-names></name><name><surname>Zankl</surname><given-names>Alexandra</given-names></name><name><surname>Metzner</surname><given-names>Claus</given-names></name><name><surname>Maier</surname><given-names>Andreas</given-names></name><name><surname>Krauss</surname><given-names>Patrick</given-names></name></person-group><article-title>Intrinsic noise improves speech recognition in a computational model of the auditory pathway</article-title><source>bioRxiv</source><year>2020</year></element-citation></ref><ref id="R66"><label>[66]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schilling</surname><given-names>A</given-names></name><name><surname>Krauss</surname><given-names>P</given-names></name><name><surname>Hannemann</surname><given-names>R</given-names></name><name><surname>Schulze</surname><given-names>H</given-names></name><name><surname>Tziridis</surname><given-names>K</given-names></name></person-group><article-title>Reduktion der tinnituslautstärke: Pilotstudie zur abschwächung von tonalem tinnitus mit schwellennahem, individuell spektral optimiertem rauschen</article-title><source>Hno</source><year>2021</year><volume>69</volume><issue>11</issue><fpage>891</fpage></element-citation></ref><ref id="R67"><label>[67]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ullanat</surname><given-names>Varun</given-names></name></person-group><source>Variational autoencoder as a generative tool to produce de-novo lead compounds for biological targets</source><conf-name>2020 14th International Conference on Innovations in Information Technology (IIT)</conf-name><conf-sponsor>IEEE</conf-sponsor><year>2020</year><fpage>102</fpage><lpage>107</lpage></element-citation></ref><ref id="R68"><label>[68]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ying</surname><given-names>Xue</given-names></name></person-group><article-title>An overview of overfitting and its solutions</article-title><source>Journal of physics: Conference series</source><publisher-name>IOP Publishing</publisher-name><year>2019</year><volume>1168</volume><elocation-id>022022</elocation-id></element-citation></ref><ref id="R69"><label>[69]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gerum</surname><given-names>Richard C</given-names></name><name><surname>Erpenbeck</surname><given-names>André</given-names></name><name><surname>Krauss</surname><given-names>Patrick</given-names></name><name><surname>Schilling</surname><given-names>Achim</given-names></name></person-group><article-title>Sparsity through evolutionary pruning prevents neuronal networks from overfitting</article-title><source>Neural Networks</source><year>2020</year><volume>128</volume><fpage>305</fpage><lpage>312</lpage></element-citation></ref><ref id="R70"><label>[70]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>Wenjun</given-names></name><name><surname>Shao</surname><given-names>Siyu</given-names></name><name><surname>Zhao</surname><given-names>Rui</given-names></name><name><surname>Yan</surname><given-names>Ruqiang</given-names></name><name><surname>Zhang</surname><given-names>Xingwu</given-names></name><name><surname>Chen</surname><given-names>Xuefeng</given-names></name></person-group><article-title>A sparse auto-encoder-based deep neural network approach for induction motor faults classification</article-title><source>Measurement</source><year>2016</year><volume>89</volume><fpage>171</fpage><lpage>178</lpage></element-citation></ref><ref id="R71"><label>[71]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>Youpeng</given-names></name><name><surname>Li</surname><given-names>Xunkai</given-names></name><name><surname>Wang</surname><given-names>Yujie</given-names></name><name><surname>Wu</surname><given-names>Yixuan</given-names></name><name><surname>Zhao</surname><given-names>Yining</given-names></name><name><surname>Yan</surname><given-names>Chenggang</given-names></name><name><surname>Yin</surname><given-names>Jian</given-names></name><name><surname>Gao</surname><given-names>Yue</given-names></name></person-group><article-title>Adaptive hypergraph auto-encoder for relational data clustering</article-title><source>IEEE Transactions on Knowledge and Data Engineering</source><year>2021</year></element-citation></ref><ref id="R72"><label>[72]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>Diederik P</given-names></name><name><surname>Welling</surname><given-names>Max</given-names></name><etal/></person-group><article-title>An introduction to variational autoencoders</article-title><source>Foundations and Trends® in Machine Learning</source><year>2019</year><volume>12</volume><issue>4</issue><fpage>307</fpage><lpage>392</lpage></element-citation></ref><ref id="R73"><label>[73]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaderberg</surname><given-names>Max</given-names></name><name><surname>Simonyan</surname><given-names>Karen</given-names></name><name><surname>Zisserman</surname><given-names>Andrew</given-names></name><etal/></person-group><article-title>Spatial transformer networks</article-title><source>Advances in neural information processing systems</source><year>2015</year><volume>28</volume></element-citation></ref><ref id="R74"><label>[74]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doersch</surname><given-names>Carl</given-names></name></person-group><article-title>Tutorial on variational autoencoders</article-title><source>arXiv preprint</source><year>2016</year><elocation-id>arXiv:1606.05908</elocation-id></element-citation></ref><ref id="R75"><label>[75]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Girin</surname><given-names>Laurent</given-names></name><name><surname>Leglaive</surname><given-names>Simon</given-names></name><name><surname>Bie</surname><given-names>Xiaoyu</given-names></name><name><surname>Diard</surname><given-names>Julien</given-names></name><name><surname>Hueber</surname><given-names>Thomas</given-names></name><name><surname>Alameda-Pineda</surname><given-names>Xavier</given-names></name></person-group><article-title>Dynamical variational autoencoders: A comprehensive review</article-title><source>arXiv preprint</source><year>2020</year><elocation-id>arXiv:2008.12595</elocation-id></element-citation></ref><ref id="R76"><label>[76]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marblestone</surname><given-names>Adam H</given-names></name><name><surname>Wayne</surname><given-names>Greg</given-names></name><name><surname>Kording</surname><given-names>Konrad P</given-names></name></person-group><article-title>Toward an integration of deep learning and neuroscience</article-title><source>Frontiers in computational neuroscience</source><year>2016</year><elocation-id>94</elocation-id></element-citation></ref><ref id="R77"><label>[77]</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>De Schutter</surname><given-names>Erik</given-names></name></person-group><source>Deep learning and computational neuroscience</source><year>2018</year></element-citation></ref><ref id="R78"><label>[78]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richards</surname><given-names>Blake A</given-names></name><name><surname>Lillicrap</surname><given-names>Timothy P</given-names></name><name><surname>Beaudoin</surname><given-names>Philippe</given-names></name><name><surname>Bengio</surname><given-names>Yoshua</given-names></name><name><surname>Bogacz</surname><given-names>Rafal</given-names></name><name><surname>Christensen</surname><given-names>Amelia</given-names></name><name><surname>Clopath</surname><given-names>Claudia</given-names></name><name><surname>Costa</surname><given-names>Rui Ponte</given-names></name><name><surname>de Berker</surname><given-names>Archy</given-names></name><name><surname>Ganguli</surname><given-names>Surya</given-names></name><etal/></person-group><article-title>A deep learning framework for neuroscience</article-title><source>Nature neuroscience</source><year>2019</year><volume>22</volume><issue>11</issue><fpage>1761</fpage><lpage>1770</lpage></element-citation></ref><ref id="R79"><label>[79]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanaka</surname><given-names>Hidenori</given-names></name><name><surname>Nayebi</surname><given-names>Aran</given-names></name><name><surname>Maheswaranathan</surname><given-names>Niru</given-names></name><name><surname>McIntosh</surname><given-names>Lane</given-names></name><name><surname>Baccus</surname><given-names>Stephen</given-names></name><name><surname>Ganguli</surname><given-names>Surya</given-names></name></person-group><article-title>From deep learning to mechanistic understanding in neuroscience: the structure of retinal prediction</article-title><source>Advances in neural information processing systems</source><year>2019</year><volume>32</volume></element-citation></ref><ref id="R80"><label>[80]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krauss</surname><given-names>Patrick</given-names></name><name><surname>Maier</surname><given-names>Andreas</given-names></name></person-group><article-title>Will we ever have conscious machines?</article-title><source>Frontiers in computational neuroscience</source><year>2020</year><fpage>116</fpage></element-citation></ref><ref id="R81"><label>[81]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saxe</surname><given-names>Andrew</given-names></name><name><surname>Nelli</surname><given-names>Stephanie</given-names></name><name><surname>Summerfield</surname><given-names>Christopher</given-names></name></person-group><article-title>If deep learning is the answer, what is the question?</article-title><source>Nature Reviews Neuroscience</source><year>2021</year><volume>22</volume><issue>1</issue><fpage>55</fpage><lpage>67</lpage></element-citation></ref><ref id="R82"><label>[82]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schilling</surname><given-names>Achim</given-names></name><name><surname>Gerum</surname><given-names>Richard</given-names></name><name><surname>Metzner</surname><given-names>Claus</given-names></name><name><surname>Maier</surname><given-names>Andreas</given-names></name><name><surname>Krauss</surname><given-names>Patrick</given-names></name></person-group><article-title>Intrinsic noise improves speech recognition in a computational model of the auditory pathway</article-title><source>Frontiers in Neuroscience</source><year>2022</year><elocation-id>795</elocation-id></element-citation></ref><ref id="R83"><label>[83]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maier</surname><given-names>Andreas</given-names></name><name><surname>Köstler</surname><given-names>Harald</given-names></name><name><surname>Heisig</surname><given-names>Marco</given-names></name><name><surname>Krauss</surname><given-names>Patrick</given-names></name><name><surname>Yang</surname><given-names>Seung Hee</given-names></name></person-group><article-title>Known operator learning and hybrid machine learning in medical imaging—a review of the past, the present, and the future</article-title><source>Progress in Biomedical Engineering</source><year>2022</year></element-citation></ref><ref id="R84"><label>[84]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogt</surname><given-names>Nina</given-names></name></person-group><article-title>Machine learning in neuroscience</article-title><source>Nature Methods</source><year>2018</year><volume>15</volume><issue>1</issue><fpage>33</fpage></element-citation></ref><ref id="R85"><label>[85]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Storrs</surname><given-names>Katherine R</given-names></name><name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name></person-group><article-title>Deep learning for cognitive neuroscience</article-title><source>arXiv preprint</source><year>2019</year><elocation-id>arXiv:1903.01458</elocation-id></element-citation></ref><ref id="R86"><label>[86]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname><given-names>Mackenzie Weygandt</given-names></name><name><surname>Mathis</surname><given-names>Alexander</given-names></name></person-group><article-title>Deep learning tools for the measurement of animal behavior in neuroscience</article-title><source>Current opinion in neurobiology</source><year>2020</year><volume>60</volume><fpage>1</fpage><lpage>11</lpage></element-citation></ref><ref id="R87"><label>[87]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bergler</surname><given-names>Christian</given-names></name><name><surname>Schröter</surname><given-names>Hendrik</given-names></name><name><surname>Cheng</surname><given-names>Rachael Xi</given-names></name><name><surname>Barth</surname><given-names>Volker</given-names></name><name><surname>Weber</surname><given-names>Michael</given-names></name><name><surname>Nöth</surname><given-names>Elmar</given-names></name><name><surname>Hofer</surname><given-names>Heribert</given-names></name><name><surname>Maier</surname><given-names>Andreas</given-names></name></person-group><article-title>Orca-spot: An automatic killer whale sound detection toolkit using deep learning</article-title><source>Scientific reports</source><year>2019</year><volume>9</volume><issue>1</issue><fpage>1</fpage><lpage>17</lpage></element-citation></ref><ref id="R88"><label>[88]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Schröter</surname><given-names>Hendrik</given-names></name><name><surname>Nöth</surname><given-names>Elmar</given-names></name><name><surname>Maier</surname><given-names>Andreas</given-names></name><name><surname>Cheng</surname><given-names>Rachael</given-names></name><name><surname>Barth</surname><given-names>Volker</given-names></name><name><surname>Bergler</surname><given-names>Christian</given-names></name></person-group><source>Segmentation, classification, and visualization of orca calls using deep learning</source><conf-name>ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</conf-name><conf-sponsor>IEEE</conf-sponsor><year>2019</year><fpage>8231</fpage><lpage>8235</lpage></element-citation></ref><ref id="R89"><label>[89]</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Bergler</surname><given-names>Christian</given-names></name><name><surname>Schmitt</surname><given-names>Manuel</given-names></name><name><surname>Maier</surname><given-names>Andreas</given-names></name><name><surname>Smeele</surname><given-names>Simeon</given-names></name><name><surname>Barth</surname><given-names>Volker</given-names></name><name><surname>Nöth</surname><given-names>Elmar</given-names></name></person-group><article-title>Orca-clean: A deep denoising toolkit for killer whale communication</article-title><source>INTERSPEECH</source><year>2020</year><fpage>1136</fpage><lpage>1140</lpage><comment>In</comment></element-citation></ref><ref id="R90"><label>[90]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bergler</surname><given-names>Christian</given-names></name><name><surname>Schmitt</surname><given-names>Manuel</given-names></name><name><surname>Maier</surname><given-names>Andreas K</given-names></name><name><surname>Symonds</surname><given-names>Helena</given-names></name><name><surname>Spong</surname><given-names>Paul</given-names></name><name><surname>Ness</surname><given-names>Steven R</given-names></name><name><surname>Tzanetakis</surname><given-names>George</given-names></name><name><surname>Nöth</surname><given-names>Elmar</given-names></name></person-group><article-title>Orca-slang: An automatic multistage semi-supervised deep learning framework for large-scale killer whale call type identification</article-title><source>Interspeech</source><year>2021</year><fpage>2396</fpage><lpage>2400</lpage></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Tuning Curve</title><p>a: LFP stream of 3 electrodes (shades of blue) during 50 ms pure tone stimuli (pure tone intensity: 110 dB SPL-70 dB SPL, frequencies: 500 Hz-19027 Hz in half octave steps). The red curve shows the trigger channel (trigger: black vertical lines). b: Spectro-Temporal-Receptive-Fields (STRFs) of position/electrode 3 in terms of spiking activity and field potentials (sound intensities 110 dB SPL-60 dB SPL). c: Tuning-curves calculated from STRFs in b.</p></caption><graphic xlink:href="EMS155882-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>LFP events in spontaneous activity</title><p>a: Spontanious activity of 3 electrode channels. Black markers (X) show which events were detected as relevant LFP events. LFP events are detected via local minimum search combined with thresholding; b: Examples (n=24) of detected LFP events; The shape of the events differs. c: Distribution of intervals between detected events; d: Peak-to-peak amplitude distribution of events</p></caption><graphic xlink:href="EMS155882-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Auto-Encoder for LFP events</title><p>a: Scheme of the used auto-encoder for data compression with input layer (dark blue), hidden layers (gray), bottleneck/encoding layer (green), and output layer (cyan). b1: Exemplary input training data (blue) and corresponding outputs/reconstructions (cyan). b2: Exemplary input test data (red) and corresponding outputs/reconstructions (orange). c1-c3: Output of the auto-encoder, for different unity vector activations in the bottleneck/encoding layer, (c1: (x,0,0), c2: (0,x,0), c3: (0,0,x) x∈ {0, 200, 400, 800, 1000, 1200, 1400}). The resulting outputs (c1-c3) represent different degrees of expression of three fundamental complementary event shapes. Any concrete LFP event shape correspond to a weighted superposition of these three prototype shapes.</p></caption><graphic xlink:href="EMS155882-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Dimensionality reduction using the auto-encoder</title><p>a-c: 2-dimensional projections of the encoded 3-dimensional embeddings of LFP events for the training (blue-cyan) and the test data set (red-orange). Training and test data is spontaneous activity recorded for 1 h or 0.5 h respectively. The time of occurrence of the detected LFP events is color coded (blue to cyan for training data, red to yellow for test data). d-f: Histograms show the distribution of the encoded LFP events. The shapes of the LFP events for different encoding vectors (x,y,z-values ∈ {0, 500, 1000}). Note that, the plots do not show the exact reconstructed curve shapes since they are based on only 2 dimensions, whereas the embedding vector is 3-dimensional. For each column (a/d, b/e, c/f), we have set the respective missing third dimension to a constant medium range value of 500.</p></caption><graphic xlink:href="EMS155882-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Auto-Encoded LFPs from pure-tone stimulation</title><p>Column 1: LFP event shapes in 3-dimensional embedding space. Markers represent encoded LFP responses induced by pure tone stimuli (500 Hz, 1000 Hz, 2000 Hz, best frequency at 500 Hz, compare <xref ref-type="fig" rid="F1">Fig. 1</xref> electrode 3) for different stimulus intensities, i.e. sound pressure levels (a: 110 dB SPL, b: 100 dB SPL, c: 90 dB SPL, d: 80 dB SPL). The best separability, i.e. lowest generalized discrimination value (GDV, shown in red above plots) of the events can be observed for a optimum stimulus intensity of 100 dB. Louder stimuli cause further recruitment and the specificity to the stimuli is reduced, whereas lower stimulus intensities evoke a decreased signal intensity. Columns 2-4: Examples of input and corresponding reconstructed output shapes for the LFP events shown in a1-d1.</p></caption><graphic xlink:href="EMS155882-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><title>Measurement of the meaningful information of auto-encoded events using a classifier network</title><p>a, b: Schematic representation of the two artificial neural networks. Note that, depicted network architectures are sketches, for detailed network parameters see <xref ref-type="table" rid="T1">tab. 1</xref>. a: Auto-encoder network (compare <xref ref-type="fig" rid="F3">Figure 3a</xref>). b: The classifier network is trained on predicting the frequency (500 Hz, 1000 Hz, 2000 Hz, 4000 Hz) that was used as stimulus and evoked the LFP event shape provided as input to the network c: Reconstructed embeddings of one test data set (10 trials with 4 stimulation frequencies at a stimulation intensity of 100 dB SPL). d: Exemplary LFP responses for different stimulus frequencies (dashed line: measured signal, solid line: reconstructed signal from auto-encoder embeddings). e, f: Training accuracy (e) and validation accuracy (f) of the trained classifier network (dashed line: chance level, dark red/dark blue: average accuracy, red/blue: learning curves for 50 repetitions). The validation accuracy is slightly reduced due to the information loss caused by auto-encoding.</p></caption><graphic xlink:href="EMS155882-f006"/></fig><fig id="F7" position="float"><label>Figure 7</label><caption><title>Spontaneous LFP events outline the realm of possible evoked LFP events.</title><p>Shown are three 2D projections of the 3D embedding space (a: dim 1 and dim 2, b: dim 1 and dim 3, c: dim 2 and dim 3). Markers represent auto-encoder embeddings of LFP events evoked by auditory stimuli of different frequencies (500 Hz-16 kHz, half octave steps, 100 dB SPL, 10 repetitions for each frequency). Spontaneous activity is shown as contour plots (blue) representing the kernel density estimation of spontaneous LFP events.</p></caption><graphic xlink:href="EMS155882-f007"/></fig><fig id="F8" position="float"><label>Figure 8</label><caption><title>Event-wise cross-correlation of spontaneous LFP events</title><p>a: LFP events in reference (blue) and in a neighbouring channel (green). b: Auto-correlation of reference-channel (gray) and cross-correlation between channels (black). c: Histogram of detected lag-times. d: Histogram of maximum cross-correlation coefficients. e: Scatter plot of lag-times and corresponding cross-correlation coefficients. f: Encoding of all events of reference channel. Colors represent different lag times between reference channel and neighbouring channel. g: Reconstructed event shapes from embeddings shown in f. Again, colors represent different lag-times as shown in f (blue: negative lag-times, red: positive lag-times). The sign of the lag-time indicates direction of information flow. Thus, blue represents input from thalamus and red represents input from other cortical areas. The LFP event shape embeddings cluster according to lag-times. Sharp asymmetric curves are related to thalamic input (blue curves g).</p></caption><graphic xlink:href="EMS155882-f008"/></fig><fig id="F9" position="float"><label>Figure 9</label><caption><title>Event-wise cross-correlation of spontaneous LFP events before and during noise trauma.</title><p>a1: LFP event shape embeddings from an exemplary animal before noise trauma. a2: Reconstructed event shapes of embeddings from a1. a3-a4: Embeddings (a3) and corresponding reconstructions (a4) of LFP event shapes of the same animal shown in a1 and a2 but during application of an auditory noise trauma (2 kHz pure tone, 115 dB SPL, 60 min); b1-b4: Same as a1-a4 for a second animal.</p></caption><graphic xlink:href="EMS155882-f009"/></fig><fig id="F10" position="float"><label>Figure 10</label><caption><title>Application of auto-encoding approach on human iEEG data</title><p>a: Intracranial recording (iEEG) in the auditory cortex of a human epilepsy patient (shown are four channels: RTB1-4). Markers (black, green, red) indicate LFP events identified by the local minimum search algorithm (see <xref ref-type="sec" rid="S2">Methods</xref>). Red markers: Events evoked by stimulation currents induced in electrodes different from RTB1-4. Green markers:Events which were directly induced by current induction into the electrodes RTB1-4. Black markers: Spontaneous or auditory evoked events. b: Temporal zoom of time series shown in a. c, d: Examples of spontaneous LFP event shapes and corresponding reconstructions from auto-encoder embeddings of the training data set (blue: input, i.e. original recorded LFP events, cyan: reconstructed LFP events from embeddings) and the test data set (red: input LFP events, orange: reconstructions from embeddings). e: Embeddings of LFP events (blue: spontaneous, green: evoked by current in channels RTB1-4, red: evoked by current in different channel). f: Reconstructions calculated from median embeddings (from e) of the three different conditions. Note that, the reconstructions represent prototypical LFP shapes for the different conditions. g, h: Examplary LFP events of channel RTB4 (green, reference channel) and channel RTB3 (blue, test channel) and corresponding cross correlation (b, black curve). The gray curve shows the auto-correlation of the reference channel. i: Distribution of the lag-times between the two channels (position of black peak). j: Embeddings of all spontaneous LFP events with lag-times smaller than -10 ms (shades of blue) or larger than 10 ms (shades of red). Markers (x) represent the average embeddings (blue cross: average of LFP events with latency smaller than -10 ms, red: &gt; 10 ms). k: Reconstructions correspodning to the mean embedding coordinate values shown in j (solid line: mean of LFP events with lag-times <italic>in</italic>[<italic>−</italic>100, <italic>−</italic>10] ∪ [10, 100], dashed line: weighted mean over all events). l: Systematic reconstructions for different lag times.</p></caption><graphic xlink:href="EMS155882-f010"/></fig><table-wrap id="T1" orientation="portrait" position="float"><label>Table 1</label><caption><title>Auto-Encoder Network (epochs = 1000, batch-size = 500)</title></caption><table frame="void" rules="groups"><thead><tr><th align="center" valign="top" style="border-right: solid thin">Layer</th><th align="center" valign="top">Activation</th><th align="center" valign="top">Output Shapes</th><th align="center" valign="top">Parameters</th></tr></thead><tbody><tr><td align="center" valign="top" style="border-right: solid thin">Input Layer</td><td align="center" valign="top">ReLu</td><td align="center" valign="top">(None, 50)</td><td align="center" valign="top">0</td></tr><tr><td align="center" valign="top" style="border-right: solid thin">Dense</td><td align="center" valign="top">ReLu</td><td align="center" valign="top">(None, 700)</td><td align="center" valign="top">35700</td></tr><tr><td align="center" valign="top" style="border-right: solid thin">Dropout (0.3)</td><td align="left" valign="top"/><td align="center" valign="top">(None, 700)</td><td align="center" valign="top">0</td></tr><tr><td align="center" valign="top" style="border-right: solid thin">Dense</td><td align="center" valign="top">ReLu</td><td align="center" valign="top">(None, 3)</td><td align="center" valign="top">2103</td></tr><tr><td align="center" valign="top" style="border-right: solid thin">Dense</td><td align="center" valign="top">ReLu</td><td align="center" valign="top">(None, 700)</td><td align="center" valign="top">2800</td></tr><tr><td align="center" valign="top" style="border-right: solid thin">Dropout (0.3)</td><td align="left" valign="top"/><td align="center" valign="top">(None, 700)</td><td align="center" valign="top">0</td></tr><tr style="border-bottom: hidden"><td align="center" valign="top" style="border-right: solid thin">Dense</td><td align="center" valign="top">ReLu</td><td align="center" valign="top">(None, 50)</td><td align="center" valign="top">35050</td></tr></tbody></table></table-wrap><table-wrap id="T2" orientation="portrait" position="float"><label>Table 2</label><caption><title>Classifier Network</title></caption><table frame="void" rules="groups"><thead><tr><th align="center" valign="top" style="border-right: solid thin">Layer</th><th align="center" valign="top">Activation</th><th align="center" valign="top">Output Shapes</th><th align="center" valign="top">Parameters</th></tr></thead><tbody><tr><td align="center" valign="top" style="border-right: solid thin">Input Layer</td><td align="left" valign="top"/><td align="center" valign="top">(None, 50, 1)</td><td align="center" valign="top">0</td></tr><tr><td align="center" valign="top" style="border-right: solid thin">Convolution 1D</td><td align="center" valign="top">ReLu</td><td align="center" valign="top">(None, 41, 60)</td><td align="center" valign="top">330</td></tr><tr><td align="center" valign="top" style="border-right: solid thin">Dropout (0.3)</td><td align="left" valign="top"/><td align="center" valign="top">(None, 41, 60)</td><td align="center" valign="top">0</td></tr><tr><td align="center" valign="top" style="border-right: solid thin">Convolution 1D</td><td align="center" valign="top">ReLu</td><td align="center" valign="top">(None, 32, 30)</td><td align="center" valign="top">18030</td></tr><tr><td align="center" valign="top" style="border-right: solid thin">Max Pooling 1D</td><td align="left" valign="top"/><td align="center" valign="top">(None, 1, 30)</td><td align="center" valign="top">0</td></tr><tr><td align="center" valign="top" style="border-right: solid thin">Flatten</td><td align="left" valign="top"/><td align="center" valign="top">(None, 30)</td><td align="center" valign="top">0</td></tr><tr><td align="center" valign="top" style="border-right: solid thin">Dropout (0.3)</td><td align="left" valign="top"/><td align="center" valign="top">(None, 30)</td><td align="center" valign="top">0</td></tr><tr><td align="center" valign="top" style="border-right: solid thin">Dense</td><td align="center" valign="top">ReLu</td><td align="center" valign="top">(None, 20)</td><td align="center" valign="top">620</td></tr><tr style="border-bottom: hidden"><td align="center" valign="top" style="border-right: solid thin">Dense</td><td align="center" valign="top">Softmax</td><td align="center" valign="top">(None, 4)</td><td align="center" valign="top">84</td></tr></tbody></table></table-wrap></floats-group></article>