<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS157871</article-id><article-id pub-id-type="doi">10.1101/2022.11.28.517675</article-id><article-id pub-id-type="archive">PPR577358</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">2</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>ethoscopy &amp; ethoscope-lab: a framework for behavioural analysis to lower entrance barrier and aid reproducibility</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Blackhurst</surname><given-names>Laurence</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">#</xref></contrib><contrib contrib-type="author"><name><surname>Gilestro</surname><given-names>Giorgio F.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">#</xref></contrib></contrib-group><aff id="A1"><label>1</label>Department of Life Sciences, Imperial College London, London, UK</aff><author-notes><corresp id="CR1">
<label>#</label>To whom correspondence should be addressed. <email>l.blackhurst19@imperial.ac.uk</email>, <email>giorgio@gilest.ro</email>
</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>01</day><month>12</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>29</day><month>11</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><sec id="S1"><title>Summary</title><p id="P1">High-throughput analysis of behaviour is a pivotal instrument in modern neuroscience, allowing researchers to combine modern genetics breakthrough to unbiased, objective, reproducible experimental approaches. To this extent, we recently created an open-source hardware platform (ethoscope (<xref ref-type="bibr" rid="R4">Geissmann <italic>et al</italic>., 2017</xref>)) that allows for inexpensive, accessible, high-throughput analysis of behaviour in <italic>Drosophila</italic> or other animal models. Here we equip ethoscopes with a Python framework for data analysis, ethoscopy, designed to be a user-friendly yet powerful platform, meeting the requirements of researchers with limited coding expertise as well as experienced data scientists. Ethoscopy is best consumed in a prebaked Jupyter-based docker container, ethoscope-lab, to improve accessibility and to encourage the use of notebooks as a natural platform to share post-publication data analysis.</p></sec><sec id="S2"><title>Availability and implementation</title><p id="P2">Ethoscopy is a Python package available on GitHub and PyPi. Ethoscope-lab is a docker container available on DockerHub. A landing page aggregating all the code and documentation is available at <ext-link ext-link-type="uri" xlink:href="https://lab.gilest.ro/ethoscopy">https://lab.gilest.ro/ethoscopy</ext-link>.</p></sec></abstract></article-meta></front><body><sec id="S3" sec-type="intro"><title>Introduction</title><p id="P3">The use of open-source software in science is commended by researchers and funders alike, for it frees academic science from ties with possibly ephemeral third-parties; it creates a bottom-up environment in which researchers actively collaborate to build their own solutions based on their own demands; it maximises the value of public funding; it provides the transparency that is essential for reproducibility. At the same time, however, academic open-source software still encounters barriers in terms of ease of use, compatibility with multiple operating systems and constantly evolving software ecosystems, as well as often unmet good practices in post-publication data sharing. We recently created an open-source, well documented platform for high-throughput behavioural analysis in <italic>Drosophila</italic> that has been widely adopted by the community (ethoscope (<xref ref-type="bibr" rid="R4">Geissmann <italic>et al</italic>., 2017</xref>), <ext-link ext-link-type="uri" xlink:href="https://lab.gilest.ro/ethoscope">https://lab.gilest.ro/ethoscope</ext-link>) and we initially equipped it with a powerful and versatile selection of R packages (rethomics (<xref ref-type="bibr" rid="R5">Geissmann <italic>et al</italic>., 2019</xref>)). However, while R remains a language of first-choice for data scientists, its structure, syntax and philosophy can prove unwelcoming to newcomers and ultimately discourage adoption in the field. Moreover, data science is constantly increasing its ties with machine learning and Python is the environment of choice to combine the two. Here we describe ethoscopy and ethoscope-lab, two instruments whose genesis was motivated by our very experience in the past years. These tools are not simply the porting of rethomics into a more accessible programming language: in fact, they offer new functions and tools, integrating with the modern sleep and circadian literature and at the same time adopt the best practice in code accessibility and data sharing. Here we detail how.</p><sec id="S4"><title>Improved accessibility for the community at large</title><p id="P4">Software accessibility is a key aspect of bioinformatics, as even the best tools are useless if they are not adopted by the community. One way to improve accessibility is for authors to distribute Docker containers (<xref ref-type="bibr" rid="R1">Boettiger, 2015</xref>; <xref ref-type="bibr" rid="R11">NÃ¼st <italic>et al</italic>., 2020</xref>). Docker containers increase adoption because they guarantee the tool will always ship with the needed dependencies; they are truly multi-platform as they offer the same experience to any user irrespective of the operating system they prefer; they guarantee that all researchers will operate on the same version of code and underlying libraries, ultimately increasing reproducibility. Ethoscope-lab is a docker container distributed via the official DockerHub platform and providing at any time access to the latest versions of ethoscopy and rethomics in a Jupyter instance.</p></sec><sec id="S5"><title>Improved accessibility for users within the laboratory</title><p id="P5">Ethoscope-lab ships with a stable release of JupyterHub, the multiuser web-based notebook that rapidly became the tool of choice for most data scientists (<xref ref-type="bibr" rid="R12">Perkel, 2018</xref>). The typical scenario will be to deploy ethoscope-lab on a shared powerful workstation, accessible to all laboratory members via the intranet or the internet. Ideally, the same workstation will also store the raw data of the behavioural analysis, as extracted directly by the ethoscopes, so that ethoscopy can have local, read-only access to them. Users will connect to Jupyter-hub using their favourite web-browser on a computer or tablet, login using their own credentials, and work in the cloud, saving their notebooks, stats and figures on a remote folder on the workstation that can be backup-ed up and shared backed with themselves or other users (for instance, via Samba or the open source software ownCloud). This scenario offers multiple advantages: all the users in the laboratory can access ethoscopy as a SaaS (Software as a Service) which is the best way to improve accessibility to end users; the docker container and the workstation can be setup and maintained by one tech-savvy user or by the IT department, allowing all other end users to concentrate on the data analysis, not the tool maintenance; the only machine requiring powerful computing specifications will be the workstation hosting the docker container so that users will be able to perform big-data analysis from any device and any location, saving time and resources.</p></sec><sec id="S6"><title>Good practice in post-publication data sharing</title><p id="P6">At the time of publication, researchers will share information on which version of the docker container they used for their analysis and upload fully annotated Jupyter notebooks as <xref ref-type="supplementary-material" rid="SD1">supplementary data</xref>. We provide several example notebooks that can serve as step-by-step tutorials on the most common functions of ethoscopy (<xref ref-type="supplementary-material" rid="SD1">Supplementary Material</xref>) and, at the same time, offer direct evidence of why a Jupyter notebook is considered the best instrument to share post-publication data processing in research. A reader that has access to: 1) the raw behavioural data, 2) the metadata describing the experimental conditions, 3) a well documented Jupyter notebook and 4) the matching version of ethoscopelab, will be able to reproduce and re-analyse and figure of any publication, with no risk of obsolescence even after decades. All these can be shared on the journal platform or via third party scholarly services, such as Zenodo.</p></sec><sec id="S7"><title>Ethoscopy structure and features</title><p id="P7">Similarly to rethomics, ethoscopy will work out-of-the-box on ethoscope and DAMs data (<xref ref-type="bibr" rid="R13">Rosato and Kyriacou, 2006</xref>) but can in principle process any behavioural data as long as they provide the three necessary descriptors of any behaviour: an animal identification (1) and at least one quantification of a behavioural trait (2) over time (3). Two common examples of tabular inputs are: a comma separated list associating an animal identification (id column) to its Cartesian coordinates in space (x, y) over time (t); a spreadsheet associating animals id (id) to amount of food consumed per minute (f). As a proof of principle, we show how to import human actigraphy data collected through a commercially available device (FitBit Charge â Notebook 7 in the <xref ref-type="supplementary-material" rid="SD1">Supplementary Material</xref>). Once data are imported, the main structure in ethoscopy will build on a <italic>pandas</italic>.<italic>DataFrame</italic> (McKinney and others, 2010), the current handling standard in Python-based data science. A <italic>behavpy</italic> object combines two tabular structures, linking the raw behavioural data with their metadata and provides builtin custom methods and properties for handling data and plotting figures (<xref ref-type="fig" rid="F1">Figure 1a</xref>). Because <italic>behavpy</italic> classes inherit directly from <italic>pandas</italic>.<italic>DataFrame</italic>, they also carry all upstream properties and methods of the DataFrame, thus providing access to the entire pandas toolbox. Inbuilt plotting wrappers can output figures either using the graphing library plotly (<xref ref-type="bibr" rid="R14">Sievert, 2020</xref>) or Seaborn (<xref ref-type="bibr" rid="R15">Waskom, 2021</xref>), a Matplotlib (<xref ref-type="bibr" rid="R7">Hunter, 2007</xref>) front end. These two open-source widely adopted plotting libraries come each with different strengths and weaknesses (<xref ref-type="fig" rid="F1">Figure 1b</xref>). Plotly output is interactive by default and therefore particularly useful during exploratory data analysis: with a click of the mouse, users can zoom on smaller or larger windows of data or pick individual datasets, for instance hiding genotypes from a complex comparison to simplify the visualisation. Seaborn output, on the other hand, is static but lightweight and more easily customizable, and thus better suited for final data publication and sharing. In general, ethoscopy functions are mostly geared towards animalsâ activity data collected with ethoscopes, but they are not limited to sleep. Here, for instance, we provide tutorials for quantification of sleep (<xref ref-type="supplementary-material" rid="SD1">Notebook 1 in Supplementary Material</xref> and <xref ref-type="fig" rid="F2">Figure 2a-c</xref>), for circadian analysis (<xref ref-type="supplementary-material" rid="SD1">Notebook 3 in Supplementary Material</xref> and <xref ref-type="fig" rid="F3">Figure 3a,b</xref>), for an analysis of sleep-depth that uses a hidden Markov model to identify sleep stages in <italic>Drosophila</italic> (<xref ref-type="bibr" rid="R16">Wiggin <italic>et al</italic>., 2020</xref>) (<xref ref-type="supplementary-material" rid="SD1">Notebook 2 in Supplementary Materials</xref> and <xref ref-type="fig" rid="F1">Figure 1d,e</xref>) and, finally, for integration with highly comparative time-series analysis (HCTSA -(<xref ref-type="bibr" rid="R3">Fulcher and Jones, 2017</xref>)) either in its entirety (<xref ref-type="supplementary-material" rid="SD1">Notebook 5 in Supplementary Material</xref>) or its curated subset Catch22 (<xref ref-type="bibr" rid="R9">Lubba <italic>et al</italic>., 2019</xref>) (<xref ref-type="supplementary-material" rid="SD1">Notebook 6 in Supplementary Materials</xref> and <xref ref-type="fig" rid="F3">Figure 3c,d</xref>).</p></sec><sec id="S8"><title>A first example of ethoscopy new functions: Hidden Markov chain analysis of sleep stages</title><p id="P8">Wiggin <italic>et al</italic> first proposed an interesting computational paradigm to identify <italic>bona fide</italic> covert sleep stages using Hidden Markov Chains (HMC) (<xref ref-type="bibr" rid="R16">Wiggin <italic>et al</italic>., 2020</xref>). Given that their initial analyses were mostly based on data collected using <italic>Drosophila</italic> activity monitors (thus offering only limited resolution of activity (<xref ref-type="bibr" rid="R6">Gilestro, 2012</xref>)), we ought to explore how HMC would perform on activity data collected at higher resolution, employing ethoscope video tracking. While we found that the initial sleep characterisation in four hidden sleep stages generally holds true (<xref ref-type="fig" rid="F1">Figure 1c</xref>), we also identified a potentially interesting difference: using ethoscopes, we could detect deep-sleep not throughout the night as originally reported (<xref ref-type="bibr" rid="R16">Wiggin <italic>et al</italic>., 2020</xref>), but almost exclusively in the first half of the night, with a clear prominent peak at ZT15 (<xref ref-type="fig" rid="F1">Figure 1c</xref>). Interestingly, this specific window of time was previously shown to be the one associated to the highest arousal threshold using an essay of odour perception (<xref ref-type="bibr" rid="R2">French <italic>et al</italic>., 2021</xref>), nicely matching ethoscopyâs description of sleep stages.</p></sec><sec id="S9"><title>A second example of ethoscopy new functions: Integration with catch22 for unsupervised statistical analysis of time-series</title><p id="P9">Catch22 (CAnonical Time-series CHaracteristics) is a feature extraction library designed for time-series data analysis (<xref ref-type="bibr" rid="R9">Lubba <italic>et al</italic>., 2019</xref>). It provides a set of 22 carefully selected time-series features that capture important characteristics of the data. These features encompass a range of statistical, information-theoretic, and nonlinear measures, making them suitable for a wide variety of time-series applications. Catch22 can be used for feature extraction, comparison of time-series, anomaly detection, classification and regression. Behavioral timeseries are an excellent use case for massive, unbiased features extraction and ethoscope data reveal exceptional discerning power when coupled with these tools (<xref ref-type="bibr" rid="R8">Jones <italic>et al</italic>., 2023</xref>). In <xref ref-type="supplementary-material" rid="SD1">Notebook 5 and 6 of the Supplementary Material</xref> we provide two recipes for analyses on ethoscope. Notebook 5 will guide through integration with HCTSA, a toolbox for massive extraction of more than 7000 features (<xref ref-type="bibr" rid="R3">Fulcher and Jones, 2017</xref>), while Notebook 6 will show integration with Catch22, a refinement of the HCTSA set that narrows down to the 22 most commonly informative features (<xref ref-type="bibr" rid="R9">Lubba <italic>et al</italic>., 2019</xref>). HCTSA runs in Matlab only, therefore the notebook focuses on bridging ethoscope data to the Matlab code. Catch22, however, features a Python library that well integrates in the ethoscopy pipeline. Notebook 6 and <xref ref-type="fig" rid="F3">Figure 3c</xref> show an implementation of Catch22 combined with classical machine learning algorithms to discern among different circadian phenotypes using unsupervised approaches. This approach can be applied to any phenotype â even when invisible to the human eye â and takes full advantage of Pythonâs integration with the current machine learning toolbox.</p></sec><sec id="S10"><title>Conclusions</title><p id="P10">We built ethoscopy and ethoscope-lab to further improve accessibility and reproducibility in the field of <italic>Drosophila</italic> sleep. Besides offering state-of-the-art data analysis, we envision these tools will open new doors to behavioural scientists and introduce them to good sharing practice in terms of code accessibility and reproducibility.</p></sec></sec><sec id="S11" sec-type="methods"><title>Methods</title><sec id="S12"><title>Software creation</title><p id="P11">Ethoscopy was written in Python and packaged using Poetry, the Python dependency management and packaging framework. Ethoscopy can be installed using pip (via Pypi) or with the default python distutils. The Dockerfile used to create the ethoscopelab container is shared via github within the ethoscopy repository.</p></sec><sec id="S13"><title>Data acquisition and analysis</title><p id="P12">Ethoscope data are collected in single-file SQLite databases (<xref ref-type="bibr" rid="R4">Geissmann <italic>et al</italic>., 2017</xref>, <xref ref-type="bibr" rid="R5">2019</xref>) and processed using packages pandas and numpy. By default, sleep is defined as a period of complete inactivity lasting at least five minutes. The hidden Markov chain model is adapted from (<xref ref-type="bibr" rid="R16">Wiggin <italic>et al</italic>., 2020</xref>).</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Jupyter Notebooks</label><media xlink:href="EMS157871-supplement-Jupyter_Notebooks.zip" mimetype="application" mime-subtype="zip" id="d25aAdCbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S14"><title>Funding and acknowledgements</title><p>LB was funded by UKRI with a BBSRC DTP scholarship to Imperial College London (BB/M011178/1 proj 2283755). We thank Esteban Beckwith for providing the circadian data used in one of the Jupyter tutorials and all members of the Gilestro lab for discussion and useful feedback on ethoscopy and ethoscope-lab.</p></ack><sec id="S15" sec-type="data-availability"><title>Data availability</title><p id="P13">The data used to generate the figures and the relative code are available in the tutorial folder of the ethoscopy repository on github (<ext-link ext-link-type="uri" xlink:href="https://github.com/gilestrolab/ethoscopy">https://github.com/gilestrolab/ethoscopy</ext-link>).</p><sec id="S16" sec-type="data-availability"><title>Code availability</title><p id="P14">Code, documentation, and tutorials are available at <ext-link ext-link-type="uri" xlink:href="https://lab.gilest.ro/ethoscopy">https://lab.gilest.ro/ethoscopy</ext-link>
</p></sec></sec><fn-group><fn id="FN1" fn-type="con"><p id="P15"><bold>Author contributions</bold></p><p id="P16">LB wrote the ethoscopy software with input and guidance from GFG. GFG created the Docker container, wrote the manuscript and supervised the project. Both authors edited the manuscript.</p></fn><fn id="FN2" fn-type="conflict"><p id="P17"><bold>Competing interests</bold></p><p id="P18">The authors declare no competing interests.</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boettiger</surname><given-names>C</given-names></name></person-group><article-title>An introduction to Docker for reproducible research</article-title><source>ACM SIGOPS Oper Syst Rev</source><year>2015</year><volume>49</volume><fpage>71</fpage><lpage>79</lpage></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>French</surname><given-names>AS</given-names></name><etal/></person-group><article-title>Sensory processing during sleep in Drosophila melanogaster</article-title><source>Nature</source><year>2021</year><volume>598</volume><fpage>479</fpage><lpage>482</lpage><pub-id pub-id-type="pmid">34588694</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fulcher</surname><given-names>BD</given-names></name><name><surname>Jones</surname><given-names>NS</given-names></name></person-group><article-title>hctsa: A Computational Framework for Automated Time-Series Phenotyping Using Massive Feature Extraction</article-title><source>Cell Syst</source><year>2017</year><volume>5</volume><fpage>527</fpage><lpage>531</lpage><elocation-id>e3</elocation-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geissmann</surname><given-names>Q</given-names></name><etal/></person-group><article-title>Ethoscopes: An Open Platform For High-Throughput Ethomics</article-title><source>PLOS Biol</source><year>2017</year><volume>15</volume><elocation-id>e2003026</elocation-id><pub-id pub-id-type="pmcid">PMC5648103</pub-id><pub-id pub-id-type="pmid">29049280</pub-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2003026</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geissmann</surname><given-names>Q</given-names></name><etal/></person-group><article-title>Rethomics: An R framework to analyse high-throughput behavioural data</article-title><source>PLOS ONE</source><year>2019</year><volume>14</volume><elocation-id>e0209331</elocation-id><pub-id pub-id-type="pmcid">PMC6334930</pub-id><pub-id pub-id-type="pmid">30650089</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0209331</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilestro</surname><given-names>GF</given-names></name></person-group><article-title>Video tracking and analysis of sleep in Drosophila melanogaster</article-title><source>Nat Protoc</source><year>2012</year><volume>7</volume><fpage>995</fpage><lpage>1007</lpage><pub-id pub-id-type="pmid">22538850</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunter</surname><given-names>JD</given-names></name></person-group><article-title>Matplotlib: A 2D graphics environment</article-title><source>Comput Sci Eng</source><year>2007</year><volume>9</volume><fpage>90</fpage><lpage>95</lpage></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>H</given-names></name><etal/></person-group><article-title>A reductionist paradigm for high-throughput behavioural fingerprinting in Drosophila melanogaster</article-title><year>2023</year></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lubba</surname><given-names>CH</given-names></name><etal/></person-group><article-title>catch22: CAnonical Time-series CHaracteristics</article-title><source>Data Min Knowl Discov</source><year>2019</year><volume>33</volume><fpage>1821</fpage><lpage>1852</lpage></element-citation></ref><ref id="R10"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>McKinney</surname><given-names>W</given-names></name><etal/></person-group><source>Data structures for statistical computing in python</source><conf-name>Proceedings of the 9th Python in Science Conference Austin, TX</conf-name><year>2010</year><fpage>51</fpage><lpage>56</lpage></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>NÃ¼st</surname><given-names>D</given-names></name><etal/></person-group><article-title>Ten simple rules for writing Dockerfiles for reproducible data science</article-title><source>PLOS Comput Biol</source><year>2020</year><volume>16</volume><elocation-id>e1008316</elocation-id><pub-id pub-id-type="pmcid">PMC7654784</pub-id><pub-id pub-id-type="pmid">33170857</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008316</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perkel</surname><given-names>JM</given-names></name></person-group><article-title>Why Jupyter is data scientistsâ computational notebook of choice</article-title><source>Nature</source><year>2018</year><volume>563</volume><fpage>145</fpage><lpage>146</lpage><pub-id pub-id-type="pmid">30375502</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosato</surname><given-names>E</given-names></name><name><surname>Kyriacou</surname><given-names>CP</given-names></name></person-group><article-title>Analysis of locomotor activity rhythms in Drosophila</article-title><source>Nat Protoc</source><year>2006</year><volume>1</volume><fpage>559</fpage><lpage>568</lpage><pub-id pub-id-type="pmid">17406282</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sievert</surname><given-names>C</given-names></name></person-group><source>Interactive Web-Based Data Visualization with R, plotly, and shiny Chapman and Hall/CRC</source><publisher-loc>New York</publisher-loc><year>2020</year></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waskom</surname><given-names>ML</given-names></name></person-group><article-title>seaborn: statistical data visualization</article-title><source>J Open Source Softw</source><year>2021</year><volume>6</volume><elocation-id>3021</elocation-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiggin</surname><given-names>TD</given-names></name><etal/></person-group><article-title>Covert sleep-related biological processes are revealed by probabilistic analysis in Drosophila</article-title><source>Proc Natl Acad Sci</source><year>2020</year><pub-id pub-id-type="pmcid">PMC7211995</pub-id><pub-id pub-id-type="pmid">32303656</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1917573117</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Ethoscopy structure and flexibility.</title><p><bold>a)</bold> Schematic structure a typical <italic>behavpy</italic> object. All <italic>behavpy</italic> classes are children of a <italic>pandas</italic>.<italic>DataFrame</italic> object and carry the behavioural time series as their main content. The main property of the class is a second <italic>pandas</italic>.<italic>DataFrame</italic>, .meta, with the associated metadata linked to the behavioural data through the <italic>id</italic> column. Any <italic>behavpy</italic> object features bespoke methods for data transformation or handling, builtin methods for plotting figures, as well as all the upstream methods and properties inherited from <italic>pandas</italic>.<italic>DataFrame</italic>. <bold>b)</bold> Panoptic summary of the strengths and weaknesses of the two default plotting libraries, seaborn and plotly.</p></caption><graphic xlink:href="EMS157871-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Ethoscopy examples of sleep analysis.</title><p><bold>a)</bold> Overview of sleep and activity using the heatmap plotting method of ethoscopy. Overall activity of a group of wild type flies in control conditions (upper panel) or during a sleep deprivation experiment (lower panel). The blue to yellow colour gradient quantifies activity (blue) and sleep (yellow). <bold>b)</bold> Sleep profile of the two experimental conditions over 2.5 days, including the sleep deprivation day. <bold>c)</bold> Quantification of rebound sleep for the experiment shown in b). <bold>c)</bold> Hidden Markov chains modelling of four covert sleep stages over the 24 hours period in a sample group of wild-type CantonS flies. ZT: Zeitgeber. <bold>d)</bold> quantification of each state over the 24h.</p></caption><graphic xlink:href="EMS157871-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Advanced circadian analysis in ethoscopy.</title><p><bold>a)</bold> Double-plotted actograms showing averaged activity during an experiment for animal with short (left), wild-type (middle) or long (right) periods. Time is defined relative to the transition from LD to LL. <bold>b)</bold> Cumulative Ï2 periodograms of three populations of animals: short period mutants (<italic>per</italic><sup><italic>short</italic></sup>, light blue), wild type (CantonS, grey), and long period mutants (<italic>per</italic><sup><italic>long</italic></sup>, mustard). <bold>c)</bold> Decision surface (boundaries) of the best fit support vector machine (SVM) separating the data used in b), after performing a catch22 analysis. The three genotype are clustered without supervision. See Notebook 6 in the Supplementary Material for a detailed description of the procedure.</p></caption><graphic xlink:href="EMS157871-f003"/></fig></floats-group></article>