<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS157419</article-id><article-id pub-id-type="doi">10.1101/2022.11.17.516989</article-id><article-id pub-id-type="archive">PPR573633</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group><subj-group subj-group-type="europepmc-category"><subject>Covid-19</subject></subj-group></article-categories><title-group><article-title>Running ahead of evolution - AI based simulation for predicting future high-risk SARS-CoV-2 variants</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Chen</surname><given-names>Jie</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Nie</surname><given-names>Zhiwei</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Wang</surname><given-names>Yu</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Wang</surname><given-names>Kai</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Xu</surname><given-names>Fan</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Hu</surname><given-names>Zhiheng</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Zheng</surname><given-names>Bing</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Zhennan</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Song</surname><given-names>Guoli</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Jingyi</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Fu</surname><given-names>Jie</given-names></name><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Huang</surname><given-names>Xiansong</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Zhongqi</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Ren</surname><given-names>Zhixiang</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Qiankun</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Daixi</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A7">7</xref></contrib><contrib contrib-type="author"><name><surname>Wei</surname><given-names>Dongqing</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Zhou</surname><given-names>Bin</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A8">8</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Yang</surname><given-names>Chao</given-names></name><xref ref-type="aff" rid="A9">9</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Tian</surname><given-names>Yonghong</given-names></name><xref ref-type="aff" rid="A10">10</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Gao</surname><given-names>Wen</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib></contrib-group><aff id="A1"><label>1</label>Peng Cheng Laboratory, Shenzhen, China</aff><aff id="A2"><label>2</label>School of Electronic and Computer Engineering, Peking University, Shenzhen, China</aff><aff id="A3"><label>3</label>AI for Science (AI4S)-Preferred Program, Peking University Shenzhen Graduate School, China</aff><aff id="A4"><label>4</label>Shenzhen International Graduate School, Tsinghua University, Shenzhen, China</aff><aff id="A5"><label>5</label>Beijing Academy of Artificial Intelligence, Beijing, China</aff><aff id="A6"><label>6</label>State Key Laboratory of Microbial Metabolism, Joint International Research Laboratory of Metabolic &amp; Developmental Sciences and School of Life Sciences and Biotechnology, Shanghai Jiao Tong University, Shanghai, China</aff><aff id="A7"><label>7</label>School of Health Science and Engineering, University of Shanghai for Science and Technology, Shanghai, China</aff><aff id="A8"><label>8</label>School of Information Science and Engineering, Shandong University, Qingdao, China</aff><aff id="A9"><label>9</label>ICODE and School of Mathematical Sciences, Peking University, China</aff><aff id="A10"><label>10</label>School of Computer Science, Peking University, China</aff><author-notes><corresp id="CR1"><bold>Corresponding author:</bold> Wen Gao, Peng Cheng Laboratory, Shenzhen, China. <email>wgao@pku.edu.cn</email>, Yonghong Tian, School of Computer Science, Peking University, China. <email>yhtian@pku.edu.cn</email>, Chao Yang, ICODE and School of Mathematical Sciences, Peking University, China. <email>chao_yang@pku.edu.cn</email>, Bin Zhou, School of Information Science and Engineering, Shandong University, Qingdao, China. <email>binzhou@sdu.edu.cn</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>21</day><month>11</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>19</day><month>11</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license><license><ali:license_ref>https://europepmc.org/downloads/openaccess</ali:license_ref><license-p>This preprint is made available via the <ext-link ext-link-type="uri" xlink:href="https://europepmc.org/downloads/openaccess">Europe PMC open access subset</ext-link>, for unrestricted research re-use and secondary analysis in any form or by any means with acknowledgement of the original preprint source.</license-p></license></permissions><abstract><p id="P1">The never-ending emergence of SARS-CoV-2 variations of concern (VOCs) has challenged the whole world for pandemic control. In order to develop effective drugs and vaccines, one needs to efficiently simulate SARS-CoV-2 spike receptor binding domain (RBD) mutations and identify high-risk variants. We pretrain a large protein language model with approximately 408 million protein sequences and construct a high-throughput screening for the prediction of binding affinity and antibody escape. As the first work on SARS-CoV-2 RBD mutation simulation, we successfully identify mutations in the RBD regions of 5 VOCs and can screen millions of potential variants in seconds. Our workflow scales to 4096 NPUs with 96.5% scalability and 493.9× speedup in mixed precision computing, while achieving a peak performance of 366.8 PFLOPS (reaching 34.9% theoretical peak) on Pengcheng Cloudbrain-II. Our method paves the way for simulating coronavirus evolution in order to prepare for a future pandemic that will inevitably take place. Our models are released at <ext-link ext-link-type="uri" xlink:href="https://github.com/ZhiweiNiepku/SARS-CoV-2_mutation_simulation">https://github.com/ZhiweiNiepku/SARS-CoV-2_mutation_simulation</ext-link> to facilitate future related work.</p></abstract><kwd-group><kwd>COVID-19</kwd><kwd>Artificial Intelligence</kwd><kwd>protein language model</kwd><kwd>mutation simulation</kwd><kwd>high-risk variants prediction</kwd></kwd-group></article-meta></front><body><p id="P2">The virus that causes the pandemic is the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) (<xref ref-type="fig" rid="F1">Figure 1a</xref>), which belongs to the genus Betacoronavirus and has nearly 80% sequence similarity with the severe acute respiratory syndrome coronavirus (SARS-CoV) (<xref ref-type="bibr" rid="R25">Lamers and Haagmans 2022</xref>; <xref ref-type="bibr" rid="R7">Coronaviridae Study Group of the International Committee on Taxonomy of Viruses 2020</xref>; <xref ref-type="bibr" rid="R60">Zhou et al. 2020</xref>).</p><p id="P3">As the pandemic enters its third year, SARS-CoV-2 has been creating waves of infections around the world (<xref ref-type="fig" rid="F1">Figure 1b,c</xref>) (<xref ref-type="bibr" rid="R4">Callaway et al. 2022</xref>) due to the high mutation rate of this RNA virus. Which potential SARS-CoV-2 variants may become the next VOCs? Do we need to develop new vaccines to deal with new variants? In what direction will the virus evolve? Shall we just give up as a society and hope that the virus will finally fade away? These are the inconvenient questions that every country on this planet must answer.</p><p id="P4">Before the current pandemic, the best-known Betacoro-naviruses are SARS-CoV and Middle East respiratory syndrome coronavirus (MERS-CoV), which have relatively more severe clinical symptoms than most coronaviruses, which can infect humans but cause only mild symptoms (<xref ref-type="bibr" rid="R57">Yin and Wunderink 2018</xref>; <xref ref-type="bibr" rid="R10">Drosten et al. 2003</xref>; <xref ref-type="bibr" rid="R59">Zaki et al. 2012</xref>; <xref ref-type="bibr" rid="R48">Su et al. 2016</xref>; <xref ref-type="bibr" rid="R33">Lu et al. 2020</xref>). In the past two decades, the viruses mentioned above have led to two epidemics: SARS (2002) and MERS (2012)(<xref ref-type="bibr" rid="R33">Lu et al. 2020</xref>). SARS-CoV-2 can also infect the human respiratory system, but has a much higher infection rate than that of SARS-CoV or MERS-CoV (<xref ref-type="bibr" rid="R51">Walls et al. 2020</xref>; <xref ref-type="bibr" rid="R53">Wrapp et al. 2020</xref>).</p><p id="P5">Three sets of proteins, including structural proteins, nonstructural proteins, and accessory proteins, are encoded by SARS-CoV-2 (<xref ref-type="bibr" rid="R25">Lamers and Haagmans 2022</xref>) (<xref ref-type="fig" rid="F1">Figure 1a</xref>). There are four main classes of structural proteins, namely, spike protein (S), nucleocapsid protein (N), membrane protein (M), and envelope protein (E), which support the structure of the virus in terms of shape or function (<xref ref-type="bibr" rid="R54">Wu et al. 2020</xref>; <xref ref-type="bibr" rid="R25">Lamers and Haagmans 2022</xref>). In particular, in addition to their high similarity in sequences, SARS-CoV-2 and SARS-CoV have the same mechanism of infecting host cells, that is, binding to the host entry receptor angiotensin-converting enzyme 2 (hACE2) (<xref ref-type="bibr" rid="R60">Zhou et al. 2020</xref>; <xref ref-type="bibr" rid="R52">Wan et al. 2020</xref>; <xref ref-type="bibr" rid="R20">Hoffmann et al. 2020</xref>; <xref ref-type="bibr" rid="R29">Li et al. 2003</xref>). During infection, the trimeric S protein is cleaved by host proteases into the N-terminal S1 subunit and the C-terminal S2 subunit. The receptor-binding domain (RBD) is an important component of the S1 subunit (<xref ref-type="fig" rid="F1">Figure 1a</xref>) that is responsible for binding to hACE2, and is the primary binding target for neutralizing antibodies (NAbs) (<xref ref-type="bibr" rid="R3">Belouzard et al. 2009</xref>; <xref ref-type="bibr" rid="R53">Wrapp et al. 2020</xref>; <xref ref-type="bibr" rid="R32">Lu et al. 2015</xref>; <xref ref-type="bibr" rid="R6">Chi et al. 2020</xref>). Therefore, the S protein plays a key role in viral infection and the immune evasion process (<xref ref-type="bibr" rid="R16">Gallagher and Buchmeier 2001</xref>; <xref ref-type="bibr" rid="R44">Simmons et al. 2013</xref>).</p><p id="P6">SARS-CoV-2 continues to mutate with a high mutation rate (<xref ref-type="bibr" rid="R11">Duffy 2018</xref>) and has evolved into five main variants of concern (VOCs)<sup><xref ref-type="fn" rid="FN3">2</xref></sup> as of May 2022: B.1.1.7 (Alpha), B.1.351 (Beta), P.1 (Gamma), B.1.617.2 (Delta) and B.1.1.529 (Omicron) (<xref ref-type="fig" rid="F1">Figure 1b,c</xref>). These SARS-CoV-2 variants with novel spike protein mutations have created waves of infections and reinfections across the globe (<xref ref-type="fig" rid="F1">Figure 1d</xref>). It is vitally important to identify early (<xref ref-type="bibr" rid="R35">Obermeyer et al. 2022</xref>) or, even better, to predict dangerous viral mutations that may enhance viral fitness including binding affinity, viral infectivity, or immunity escape.</p><p id="P7">The Global Initiative on Sharing All Influenza Data (GISAID)<sup><xref ref-type="fn" rid="FN4">3</xref></sup> (<xref ref-type="bibr" rid="R43">Shu and McCauley 2017</xref>) has recorded more than 14 million SARS-CoV-2 genomes submitted by scientists around the world. This large number of genomic sequences presents an excellent opportunity to study the spread and evolution of SARS-CoV-2. Computational methods such as the Gillespie algorithms can be used to simulate realistic substitution patterns of closely related genomic large-scale datasets, e.g., simulators targeting gene trees, ancestral recombination graphs, or phylogenetic trees (<xref ref-type="bibr" rid="R2">Beiko and Charlebois 2007</xref>; <xref ref-type="bibr" rid="R22">Hudson 2002</xref>; <xref ref-type="bibr" rid="R27">Laval and Excoffier 2004</xref>; <xref ref-type="bibr" rid="R13">Ewing and Hermisson 2010</xref>; <xref ref-type="bibr" rid="R39">Rambaut and Grass 1997</xref>; <xref ref-type="bibr" rid="R15">Fletcher and Yang 2009</xref>; <xref ref-type="bibr" rid="R45">Sipos et al. 2011</xref>; <xref ref-type="bibr" rid="R8">De Maio et al. 2022</xref>; <xref ref-type="bibr" rid="R42">Shchur et al. 2022</xref>). Artificial Intelligence (AI) models can also learn hidden evolution patterns from the huge number of virus sequences submitted, prioritizing future potential viral mutations that could introduce the next VOCs (<xref ref-type="bibr" rid="R5">Chen et al. 2020</xref>; <xref ref-type="bibr" rid="R34">Mohamed et al. 2021</xref>).</p><p id="P8">As shown in <xref ref-type="fig" rid="F1">Figure 1a</xref>, the RBD region of the spike protein is an area of concern because it has a high mutation rate, which can significantly affect binding to hACE2, as well as antibodies. In this work, we simulate RBD mutations by learning, generating, screening, and fine-tuning based on pretrained protein language models as shown in <xref ref-type="fig" rid="F1">Figure 1e</xref>. A multi-constrains variation prediction (MCVP) framework is designed to learn from millions of RBD sequences and experimental measurements of binding affinity between single RBD mutations and hACE2/antibodies. MCVP utilizes active learning based on a pretrained protein language model. This high performance computing (HPC) driven work can evaluate RBD mutations based on protein expression, binding affinity, and antibody escape to ultimately provide assistance in the fight against SARS-CoV-2.</p><sec id="S1"><title>Current state of the art</title><sec id="S2"><title>Predictive modeling of SARS-CoV-2 variants</title><p id="P9">During the pandemic, studies have emerged with a variety of focuses and models to predict the mutation of SARS-CoV-2. For example, a renewal-equation-based model was used to describe the adaptive evolution among multiple variants of SARS-CoV-2 including R.1, Alpha, and Delta, and then to predict the dominant variants in Japan before the start of the Tokyo Olympic Games (<xref ref-type="bibr" rid="R23">Ito et al. 2021</xref>). Furthermore, some work sought to accurately predict the fitness of SARS-CoV-2 variants, which was used to characterize how efficiently the virus produces infectious progeny. A computational model named SpikePro (<xref ref-type="bibr" rid="R38">Pucci and Rooman 2021</xref>) was designed to predict the fitness of SARS-CoV-2 from the sequence and structure of the spike protein in order to allow the identification of new dangerous variants. PyR<sub>0</sub> (<xref ref-type="bibr" rid="R35">Obermeyer et al. 2022</xref>), a hierarchical Bayesian multinomial logistic regression model, was developed to infer relative transmissibility of lineages, forecast future lineage proportions, and identify mutations relevant to fitness. Deep Learning (DL) models have recently been shown to perform well in predicting variant adaptation. Specifically, a three-dimensional convolutional neural network (3D CNN) based on spike dinucleotide composition representation was used to learn the human adaptation of existing coronaviruses and predict the adaptation of SARS-CoV-2 VOCs (<xref ref-type="bibr" rid="R28">Li et al. 2022</xref>).</p><p id="P10">Language models have been used to decipher the genetic sequences of virus. For example, a Transformer-based discriminative model was trained with SARS-CoV-2 genetic sequences to predict potential mutations that may lead to enhanced virus transmissibility (<xref ref-type="bibr" rid="R55">Wu et al. 2021</xref>). Language models have also been applied for protein prediction tasks, as common protein motifs and domains can be analogized to words, phrases, and sentences in human language (<xref ref-type="bibr" rid="R36">Ofer et al. 2021</xref>; <xref ref-type="bibr" rid="R50">Trifonov 2009</xref>; <xref ref-type="bibr" rid="R47">Strait and Dewey 1996</xref>; <xref ref-type="bibr" rid="R58">Yu et al. 2019</xref>). Motivated by the success of masked language models such as BERT (<xref ref-type="bibr" rid="R9">Devlin et al. 2018</xref>), we design a pretrained protein language model for comprehensive variant prediction, aiming to simulate circulating viral mutation and predict potentially risky variants. In this work, we pretrain our protein language model on a large-scale set of protein sequences using a supercomputer with exascale AI training capabilities and further perform fine-tuning and multiconstraint screening on RBD sequences of the spike protein in SARS-CoV-2 to generate possible future variant branches.</p></sec><sec id="S3"><title>Large-scale language model training</title><p id="P11">The existing state-of-the-art language models, especially various BERT variations (<xref ref-type="bibr" rid="R9">Devlin et al. 2018</xref>; <xref ref-type="bibr" rid="R56">Yang et al. 2019</xref>; <xref ref-type="bibr" rid="R21">Howard and Ruder 2018</xref>; <xref ref-type="bibr" rid="R31">Liu et al. 2019</xref>; <xref ref-type="bibr" rid="R26">Lan et al. 2019</xref>) with Transformer as the core, have achieved outstanding performance in many fields. Recently, some works have emerged with a focus on transferring language models to large-scale protein representation learning, e.g., ESM (<xref ref-type="bibr" rid="R40">Rives et al. 2021</xref>) and ProtTrans (<xref ref-type="bibr" rid="R12">Elnaggar et al. 2022</xref>), which were trained on the Summit supercomputer, and demonstrated that large-scale pretrained language models can capture latent grammar of protein sequences to a certain degree (<xref ref-type="bibr" rid="R12">Elnaggar et al. 2022</xref>).</p><p id="P12">Mini-batch stochastic gradient descent has been found to be very effective for large-scale learning (<xref ref-type="bibr" rid="R18">He et al. 2021</xref>). However, updating the parameters in small batches makes the optimization unstable (<xref ref-type="bibr" rid="R30">Li et al. 2020</xref>). For large-scale datasets, large-batch training with data parallelism has found increasing popularity (<xref ref-type="bibr" rid="R31">Liu et al. 2019</xref>), as it can improve data communication and hardware utilization of a model. However, how to set the best batch size is a complex optimization problem. Some works (<xref ref-type="bibr" rid="R19">Hoffer et al. 2017</xref>; <xref ref-type="bibr" rid="R24">Keskar et al. 2016</xref>; <xref ref-type="bibr" rid="R17">Goyal et al. 2017</xref>; <xref ref-type="bibr" rid="R37">Osawa et al. 2022</xref>) have reported that increasing the batch size beyond a certain point can result in poor generalization performance.</p></sec></sec><sec id="S4"><title>Innovations realized</title><sec id="S5"><title>Overview of MCVP</title><p id="P13">Our proposed multi-constrains variation prediction (MCVP) framework is a heterogeneous system for simulating the effect of the RBD mutations on the fitness of SARS-COV-2 viruses. This system includes 1) a pretrained protein language generative model for RBD mutation generation, 2) an RBD and hACE2 binding affinity prediction model for selecting RBD mutants that have higher binding affinities than the wild type, and 3) an immune escape prediction model for selecting RBD mutants that are more likely to evade antibody attacks.</p><p id="P14">The training and validation data for the system are collected from various authoritative resources. We download protein sequences from the UniRef database (<xref ref-type="bibr" rid="R49">Suzek et al. 2007</xref>) for the training of the protein language model. We download data related to SARS-COV-2 from the GISAID database, which includes more than 14 million genome sequences of SARS-CoV-2 for rapidly sharing. The S protein sequences are obtained from GISAID, then the RBD region sequences are segmented for model fine-tuning and analyzed for the probability of the mutation rate at each position. SARS-COV-2 VOC defining mutations are obtained from <ext-link ext-link-type="uri" xlink:href="https://outbreak.info/">https://outbreak.info/</ext-link>.</p><sec id="S6"><title>The workflow of MCVP</title><p id="P15">We design the MCVP framework to follow the workflow as shown in <xref ref-type="fig" rid="F2">Figure 2a</xref>. The first module of MCVP is a Transformer-based language model, hereafter called ProtFound (Protein Foundation Model). ProtFound is trained with the UniRef90 dataset, including approximately 144 million protein sequences. All protein sequences are chopped into lengths of 256, as the RBD region of the spike protein S1 consists of 201 amino acids within the location range of 331-531 (<xref ref-type="bibr" rid="R46">Starr et al. 2020</xref>). The structure of ProtFound is similar to that of BERT, but there is no classification token. BERT is a bidirectional model for natural language processing that attempts to reconstruct corrupted tokens. For protein language modeling, 15% of each input protein sequence is masked. During the training process, ProtFound reconstructs the masked amino acids. After training, ProtFound can learn protein embeddings that captured some of the biophysical features of the protein sequences.</p><p id="P16">We use ProtFound in two ways. First, we design an RBD-variation-generating module. Specifically, we fine-tune ProtFound with RBD sequences truncated from the spike protein sequences which were downloaded from GISAID. Subsequently, we generate new RBD mutations by generating missing amino acids from a masked RBD sequence selected as the starting sequence. Second, as a protein embedding extractor, ProtFound provides meaningful vector representations of RBD mutations. These embeddings are used as the inputs to a binding affinity prediction model, and an immunity escape prediction model. The above models are essential in selecting RBD mutations that are more advantageous in the sense of virus fitness and survival because of higher binding affinities and immune evasion.</p><p id="P17">We employ ProtFound to generate millions of RBD mutations with Pengcheng Cloudbrain-II. Subsequently, the two AI filters are used to screen the various generated variants of the RBD based on hACE2 binding affinity and immunity escape respectively in a high-throughput manner. The <italic>in silico</italic> screening is designed to simulate the evolution of SARS-CoV-2 in nature. Therefore, the variants passing this screening could be considered evolutionarily more advantageous. After completing one round of mutation simulation, the selected variants are used as training samples to fine-tune the mutation model ProtFound, which forces the model to learn the characteristics of those variations that are more likely to survive the evolutionary selection. By repeating this procedure, ProtFound is guided to generate variants that are more likely to have evolutionary advantages, thus enabling the simulation of SARS-CoV-2 RBD mutation generation.</p><p id="P18">As shown in <xref ref-type="fig" rid="F2">Figure 2b</xref>, the protein embedding generation process starts with the tokenization of a protein sequence and the addition of the positional encoding. The resulting vectors pass through ProtFound to create context-aware embeddings for each amino acid, which are the last hidden state of the Transformer’s attention stack. Then these embeddings are concatenated and pooled along the length-dimension to obtain a fixed-size embedding irrespective of the sequence length. In MCVP, the two AI predictors are developed, based on the sequence embeddings extracted by ProtFound. The first is a binding affinity predictor designed for forecasting changes in binding affinity between the mutated RBD and hACE2. The second predictor can be used to evaluate the comprehensive antibody escape capability of the variants through antibody escape prediction.</p></sec><sec id="S7"><title>Generation of variants</title><p id="P19">A variant generation module is designed based on the ProtFound model. Essentially, the ProtFound model has learned the general properties of proteins through self-supervised learning on billions of protein sequences. Then, by fine-tuning ProtFound on millions of RBD sequences, the model is exposed to the subtle amino acid changes in the RBD region of the S1 proteins that are present in the GISAID submissions. We conclude that the final converged model should be able to generate RBD like sequences that would be very likely to new RBD mutations as long as proper constraints are satisfied, e.g., increased binding affinity to hACE2 and increased antibody evasion.</p><p id="P20">We generate RBD variants by performing the following steps. 1) Spike protein sequences are downloaded from the GISAID database, and the sequences in the RBD region are extracted. 2) Training datasets are created from the data processed in step 1. For each VOC, we create a training dataset using all RBD sequences from the Spike protein sequences that are submitted before the first appearance of that VOC. 3) The ProtFound model is fine-tuned using the training dataset. 4) A variation probability for each position in the RBD is calculated using the training dataset. 5) The variation probability is used to create masks for each position in the RBD. 6) The variant generation module is used to create amino acids at the masked positions.</p></sec><sec id="S8"><title>High-throughput screening</title><p id="P21">Once we have generated a large number of mutation sequences, the next step is to simulate the selection pressure faced by viruses through high-throughput screening. Two screening principles are adopted to perform the progressive filtering of the generated mutations. First, since the main receptor for entering human cells is hACE2, the affinity between the virus RBD and hACE2 is an important indicator for the viral entrance. In other words, future variants should maintain ideal binding affinity with hACE2. Second and more importantly, various studies have shown that VOCs can escape binding to antibodies. Therefore, we design a model to predict binding affinity and a model to predict the immunity escape of the variants. These two models are built with ProtFound as the backbone and are developed based on transfer learning.</p></sec><sec id="S9"><title>Simulation of circulating mutations</title><p id="P22">SAR-CoV-2 is constantly evolving within a host. As a result of evolutionary pressures, viruses tend to mutate to acquire stronger fitness, including better binding affinity, and stronger antibody escape capabilities. We simulate the mutation of SARS-CoV-2 through high-throughput screening and fine-tuning. In each round of stimulation, we use AI models to select those variants that are predicted to retain ideal binding affinity and stronger antibody escape capabilities. The screened variants will then be used for next round of fine-tuning of ProtFound. These steps complete the <italic>in silico</italic> mutational simulation of SARS-CoV-2 RBD.</p></sec></sec><sec id="S10"><title>HPC strategy design</title><p id="P23">For large-scale distributed AI training, the main goals are to optimize the throughput and speed up network convergence. Pengcheng Cloudbrain-II possesses 4096 pieces of AI processors with 512 server nodes. To efficiently train the language model on such a large cluster, we adopt multiple optimization strategies (<xref ref-type="fig" rid="F3">Figure 3</xref>), reaching a peak performance of 366.8 petaflops with mixed precision.</p><sec id="S11"><title>Operator fusion</title><p id="P24">We run the training task in graph mode and apply pattern-based operator fusion to accelerate the training in this mode. In this work, we perform fusion of the following operators to optimize the ProtFound model: 1) We fuse multiple operators for the forward/backward layer normalization operations and perform calculations on multiple neural processing units (NPU) cores. 2) We fuse the matrix multiplication (<italic>matmul</italic>) operator and the addition (<italic>add</italic>) operator. 3) We fuse the all-reduce operations for all gradients within one Transformer layer into a single operator. These optimizations account for more than 30% of the time consumption.</p></sec><sec id="S12"><title>Operator replacement</title><p id="P25">Operator replacement refers to the replacement of some operators in a model with new operators that are more amenable to online deployment. In this work, we use fast Gaussian Error Linear Unit (GeLU) in place of the original GeLU operator, since the later is not very friendly to NPUs. Such operator replacement can improve the model efficiency by about 10% while maintaining the accuracy performance.</p></sec><sec id="S13"><title>Operator auto-tuning</title><p id="P26">AI computing chips are usually composed of computing units, on-chip storage, data transmission, and other modules. The collaboration among these modules usually significantly affects the computation patterns of operators. The Auto Tune tool of Ascend uses reinforcement learning and genetic algorithm for tuning particular operators by identifying the optimal tiling policies. We use the Auto Tune tool to optimize the <italic>matmul</italic> operator, which accounts for more than 30% of the time consumption.</p></sec><sec id="S14"><title>Mixed precision</title><p id="P27">We further improve the speed performance by using mixed precision schedules. In dozens of layernorm operators, we schedule a reducing sum operation to the Ascend 910 cube core in FP16 and the other remaining operations to the Ascend 910 vector core in FP32 to avoid computation overflow and achieve higher performance. In addition, the embedding and loss calculations are performed in single precision, and the remaining operators are applied in half precision. The optimizer is implemented with single precision. This mixed-precision implementation greatly reduces the training latency at the cost of potential overflow due to the limited representation range of half precision.</p></sec></sec></sec><sec id="S15"><title>How performance was measured</title><p id="P28">We perform pretraining of our ProtFound model on Pengcheng Cloudbrain-II with the MindSpore<sup><xref ref-type="fn" rid="FN5">4</xref></sup> AI computation framework. We run tests with 8 NPUs per NPU Pod. The tests are scaled from (1 × 8) to (512 × 8) NPUs by powers of 2, and the largest one is assessed on (512 × 8) NPUs at full-scale. Our model reports timings, including epoch times, mini-batch times, and time-to-solution. We measure the full pretraining time-to-solution, scalability, and peak performance at full-scale. We measure the FLOPS for all precisions by using MindInsight, which is a module of MindSpore. We collect floating-point instructions of relevant flavors (that is, addition, multiplication, fused multiply-add, and tensor core operations for FP16, FP32, and FP64) and multiply them by corresponding weighting factors, respectively, to transform them into FLOPS counts. The sum of all these values for all precisions yields our overall mixed-precision FLOPS count. In summary, the criteria used to measure the performance of the ProtFound model are defined as follows: <list list-type="bullet" id="L1"><list-item><p id="P29"><bold>Time-to-solution</bold>, defined as the epoch times of strong scaling.</p></list-item><list-item><p id="P30"><bold>Mini-batch size</bold>, defined as the batch size on a single NPU.</p></list-item><list-item><p id="P31"><bold>Peak performance</bold>, defined as <inline-formula><mml:math id="M1"><mml:mrow><mml:mfrac><mml:mrow><mml:mtext>total</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>FLOPs</mml:mtext></mml:mrow><mml:mrow><mml:mtext>per</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>step</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>time</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>.</p></list-item></list></p></sec><sec id="S16"><title>Performance results</title><sec id="S17"><title>Strong scaling performance</title><p id="P32">The strong scalability of the pretraining process is measured in terms of the epoch times for 1 to 512 nodes of Pengcheng Cloudbrain-II, as shown in <xref ref-type="fig" rid="F4">Figure 4</xref>. For the strong scaling assessment, the total size of the problem remains the same, i.e., the number of protein sequences used for the ProtFound model pretraining is kept constant at approximately 408 million. The measured strong scaling, shown as a solid line, almost coincides with the optimal strong scaling, shown as a dotted line, which demonstrates that the strong scaling performance is nearly perfect for 1 to 512 nodes. With the performance for 1 node as the baseline, the parallel efficiency at 512 nodes is approximately 96.46%, and the speedup reaches about 493.9×. In addition, the peak performance reaches 366.81 PFLOPS, and the time-to-solution is 9.1 minutes when scaled to 512 nodes in mixed-precision, which enables rapid deployment and iteration of variant generation models.</p></sec><sec id="S18"><title>Weak scaling performance</title><p id="P33">As shown in <xref ref-type="fig" rid="F5">Figure 5</xref>, the weak scaling performance of pretraining the ProtFound model on Pengcheng Cloudbrain-II is also assessed. Unlike the strong scaling case, the problem size per node in the weak scaling test is kept constant at 640 thousand protein sequences. Here, the I/O operations are the saving of checkpoints and trained models. Even if the I/O time is included, the degradation in performance at high node is still slight. Specifically, the parallel efficiency for weak scaling from 1 to 512 nodes slightly reduces from 96.73% to 95.57%, and the utilization also remains stable, reducing from 34.99% to 33.54%. In addition, the peak performance reaches 366.86 PFLOPS (34.99% of Peak) when the I/O time is removed. In summary, for the pretraining of the ProtFound model on Pengcheng Cloudbrain-II, the optimized model scales well to the entire supercomputer.</p></sec><sec id="S19"><title><italic>In silico</italic> validation of RBD mutations of VOCs</title><p id="P34">The variations of concern (VOCs) that have emerged to date include B.1.1.7 (Alpha), B.1.351 (Beta), P.1 (Gamma), B.1.617.2 (Delta), and B.1.1.529 (Omicron). Omicron, the currently most widespread VOC, exhibits a several-fold accumulation of variants compared with the first four VOCs. Considering the significant difference between the variants before and after the appearance of Omicron, we simulate and verify the RBD mutation process with Omicron as the dividing line as shown in <xref ref-type="fig" rid="F6">Figure 6</xref>.</p><p id="P35">For SARS-CoV-2 mutation simulation before Omicron, we validate the predictive ability of MCVP by simulating the mutational changes from the wild type<sup><xref ref-type="fn" rid="FN6">5</xref></sup> to the four VOCs (Alpha, Beta, Gamma, and Delta). According to the pathogenic progression of SARS-CoV-2 (<xref ref-type="bibr" rid="R4">Callaway et al. 2022</xref>) based on the data from NextStrain<sup><xref ref-type="fn" rid="FN7">6</xref></sup>, these four VOCs have a parallel evolutionary relationship. Therefore, the starting sequence used to verify the evolutionary route is selected as wild type. The sequences used to fine-tune the model are chosen based on the time when each VOC was first detected. The first detected times and locations of the four VOCs before Omicron are identified via Wikipedia<sup><xref ref-type="fn" rid="FN8">7</xref></sup>. We segment the data downloaded from GISAID in accordance with the times corresponding to each VOC. For example, Alpha was first reported in September 2020, and we therefore take the data from those submitted before September 2020 as the training sequences for fine-tuning ProtFound to predict the emergence of Alpha. Next we adopt the wild type as the reference sequence for the mutation generation process. After the RBD mutation generation and high-throughput screening, we check the mutated sites to determine if the RBD of Alpha has appeared in the screened RBD mutations. If it appears, the mutation simulation from wild type to Alpha is complete. Otherwise, the filtered RBD mutations are used for iteratively fine-tuning of ProtFound until the RBD of Alpha is generated. Following this simulation method, we have successfully generated the RBDs of the four VOCs (Alpha, Beta, Gamma, and Delta) from the RBD of wild type.</p><p id="P36">To simulate the evolution of Omicron, we select Omicron BA.2 as the starting point to perform the virus evolving to generate BA.5 in accordance with the pathogenic progression of SARS-CoV-2 (<xref ref-type="bibr" rid="R4">Callaway et al. 2022</xref>). In this simulation, the sequences with submission times between BA.2 and BA.5 are selected to fine-tune ProtFound, and BA.2 is used as the reference sequence at the time of generation. Through fine-tuning and identification, BA.5 has been generated successfully by our workflow.</p><p id="P37"><xref ref-type="table" rid="T1">Table 1</xref> shows the proportion of remaining variants after each round of screening. Among the above five VOCs, the variants mutated towards Omicron BA.5 retain more than 80% of the proportion in both the hACE binding and antibody escape screening, which indicates that the Omicron sublineages tend to remain stable binding affinity and have stronger antibody escape capability.</p></sec><sec id="S20"><title>Potential high-risk mutation prediction</title><p id="P38">By simulating the mutation of the RBD, we have comprehensively demonstrated that the proposed MCVP can effectively evolve out the RBDs of the known VOCs. However, the real value of MCVP lies in its ability to predict potential future VOCs, thus assisting targeted drug design and vaccine development.</p><p id="P39">Omicron has been the dominant variant widely spreading around the world. The phenomenon of intra-VOC evolution has been significant due to the sustained transmission of VOCs, which leading to different descendent lineages. In view of this, a variant tracking system, termed ”Omicron subvariants under monitoring”, is added to remind us of lineages that need priority attention and monitoring<sup><xref ref-type="fn" rid="FN9">8</xref></sup>. In this tracking system, BA.5 sublineages (e.g. BF.7, BF.14, BQ.1), BA.2 sublineages (e.g. BA.2.75, BA.2.75.2), and BA.4 sublineage (BA.4.6) need to be focused at present<sup><xref ref-type="fn" rid="FN10">9</xref></sup>. In order to demonstrate the potential of MCVP to predict future high-risk variants, we simulate the mutational process of BF.7, BF.14, BQ.1, BA.2.75.2, and BA.4.6. As expected, we have successfully simulated these variants that WHO reminds public health authorities around the world to give priority to.</p><p id="P40">More importantly, as shown in <xref ref-type="fig" rid="F6">Figure 6f</xref>, we take the latest sublineage of Omicron, i.e. BA.5, as the reference sequence, then generate billions of variants in each round and conduct subsequent high-throughput screening. After evaluation of binding affinity and antibody escape capability, we use the screened sequences to fine-tune ProtFound. After several rounds of iterations, we select a number of potential RBD mutations with high risk that maintain a stable binding affinity with hACE2 and a high antibody escape capability. At this stage, to better evaluate potential VOCs, we calculate the relative risk factor based on mutations identified as being associated with fitness of PyR<sub>0</sub> (<xref ref-type="bibr" rid="R35">Obermeyer et al. 2022</xref>). A variant whose risk factor is greater than 0 may have greater risk than wild type, and a variant whose risk factor is less than 0 may have less risk. As a result, billions of variants can be evaluated quickly for the identification of potential high-risk mutations.</p></sec></sec><sec id="S21"><title>Implications</title><sec id="S22"><title>AI models can successfully generate and identify almost all VOCs</title><p id="P41">In our experiments, using genomic data submitted before the appearance of each VOCs, we successfully generate and identify all VOCs except Omicron. Given the original Omicron spike sequences, we could also generate the Omicron subvariants that are currently the dominant viral variants throughout the world.</p><p id="P42">During the iterative mutation generation process, the AI models can prioritize mutations based on their predicted binding affinity and antibody escape, two key factors for viral infectivity. Due to their combinatorial nature, it is impossible to experimentally measure the binding affinity changes among all possible RBD mutations (20<sup>201</sup>) and hACE2 or antibodies. Therefore, under the assumption that the deep mutational scanning (DMS) measurements of RBD single mutations might provide reasonable constraints for the RBD to hACE2/antibody binding affinity spaces, we approximate these binding affinity spaces using AI models for prediction of the binding affinities among multiple RBD mutations and hACE2 or antibodies. These AI models are key innovations of the whole workflow.</p><p id="P43">The fact that our workflow could not generate Omicron despite more than 20 rounds of iteration implies that the mutational features of Omicron are very different from those of other VOCs, since all other VOCs are found after a few rounds of generation.</p></sec><sec id="S23"><title>The simulation of SARS-CoV-2 spike mutation is an HPC application</title><p id="P44">The strategy we used to simulate SARS-CoV-2 spike mutation is dependent on the availability of large-scale genome data (more than 14 million viral genomes as provided by the GISAID database) and a large protein language generation model.</p><p id="P45">Recent progress in Transformer-based models has enabled the implementation of protein language models capable of generating de novo protein sequences following the principles of natural ones (<xref ref-type="bibr" rid="R14">Ferruz et al. 2022</xref>). Inspired by these successes, we pretrain a BERT-like model to learn from millions of viral spike proteins. Our mutation generation workflow heavily relies on the Pengcheng Cloudbrain-II: first, to train the protein language model; second, to iteratively generate new mutations; and third, to evaluate the variants based on AI predictors of: 1) the binding affinity between RBD and hACE2, 2) the antibody escape capability. All the processing steps require an HPC facility, as billions of RBD mutations must be generated in each round and evaluated accordingly.</p></sec><sec id="S24"><title>Simulating coronavirus evolution is a new challenge for HPC</title><p id="P46">The COVID-19 pandemic, caused by SARS-CoV-2, is a stark reminder that coronaviruses remain a major threat to humanity. It is crucial to study the evolution of Coronaviruses to be better prepared for the next pandemic.</p><p id="P47">SARS-CoV-2 has become the most sequenced virus ever in history, with 14 million SARS-CoV-2 genomes deposited in the GISAID database. The efficiency of simulating these extremely large numbers of closely related genomes to recreate potential histories of past and future virus evolution presents a new challenge for HPC. As proof of concept, in this study, we have initiated the first step toward elucidating the evolution of SARS-CoV-2 VOCs by using only RBD sequences of the SARS-CoV-2 S1 protein. Using all genomes of SARS-CoV-2 in the future, plus other coronavirus genomes, we will be able to perform more reliable simulations to study the evolution of coronaviruses in general and the dynamics of viral transmission across animal species. Meeting the computational requirements of such simulations will require some of the finest HPC systems built to date.</p></sec><sec id="S25"><title>SARS-CoV-2 mutation is a serious threat</title><p id="P48">It has been estimated that an infected person could carry 109 to 1012 SARS-CoV-2 virions (<xref ref-type="bibr" rid="R41">Sender et al. 2021</xref>). Since the initial outbreak of COVID-19, there have been more than 645 million infections as of December 2022<sup><xref ref-type="fn" rid="FN11">10</xref></sup>. The potential mutation space for SARS-CoV-2 is thus approximately 6 × 10<sup>17</sup> to 10<sup>20</sup>. The experimentally deduced spontaneous mutation rate of SARS-CoV-2 is 1.3 × 10<sup>−6</sup> ± 0.2 × 10<sup>−6</sup> per base per infection cycle (<xref ref-type="bibr" rid="R1">Amicone et al. 2022</xref>), which is heterogeneous throughout the genome. Taking all these numbers together, it is not too difficult to conclude that every single base mutation is being generated de-novo and transmitted to a new host every day (<xref ref-type="bibr" rid="R41">Sender et al. 2021</xref>). It is therefore extremely important to be able to simulate the viral mutation process and rapidly identify potential VOCs, which is essentially what we have demonstrated in this work through the state-of-the-art AI technology combined with the cutting-edge HPC hardware - the Pengcheng Cloudbrain-II. Any successful prediction of future VOCs of SARS-CoV-2 is not just good scientific research, but can prevent unnecessary deaths.</p><p id="P49">Further details of this paper will be published later.</p></sec></sec></body><back><ack id="S26"><title>Acknowledgements</title><p>We appreciate the useful discussions with Ming Li and Peng Zhou.</p><sec id="S27"><title>Funding</title><p>The author(s) disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: This work is supported by the Nature Science Foundation of China (No. 61972217, 62081360152, 62006133, 32071459, 12131002), Guangdong Basic and Applied Basic Research Foundation (No.2019B1515120049), Guangdong Science and Technology Department (No. 2020B1111340056), and the major key project of PCL(PCL2021A13).</p></sec></ack><fn-group><fn id="FN1" fn-type="conflict"><p id="P50"><bold>Declaration of conflicting interests</bold></p><p id="P51">The author(s) declared no potential conflicts of interests with respect to the research, authorship, and/or publication of this article.</p></fn><fn id="FN2"><label>1</label><p id="P52"><ext-link ext-link-type="uri" xlink:href="https://covid19.who.int/">https://covid19.who.int/</ext-link></p></fn><fn id="FN3"><label>2</label><p id="P53"><ext-link ext-link-type="uri" xlink:href="https://www.who.int/activities/tracking-SARS-CoV-2-variants/">https://www.who.int/activities/tracking-SARS-CoV-2-variants/</ext-link></p></fn><fn id="FN4"><label>3</label><p id="P54"><ext-link ext-link-type="uri" xlink:href="https://gisaid.org/">https://gisaid.org/</ext-link></p></fn><fn id="FN5"><label>4</label><p id="P55"><ext-link ext-link-type="uri" xlink:href="https://www.mindspore.cn/en">https://www.mindspore.cn/en</ext-link></p></fn><fn id="FN6"><label>5</label><p id="P56">EPI_ISL ID: EPI_ISL_402124</p></fn><fn id="FN7"><label>6</label><p id="P57"><ext-link ext-link-type="uri" xlink:href="https://nextstrain.org/">https://nextstrain.org/</ext-link></p></fn><fn id="FN8"><label>7</label><p id="P58"><ext-link ext-link-type="uri" xlink:href="https://en.wikipedia.org/wiki/SARS-CoV-2">https://en.wikipedia.org/wiki/SARS-CoV-2</ext-link></p></fn><fn id="FN9"><label>8</label><p id="P59"><ext-link ext-link-type="uri" xlink:href="https://www.who.int/activities/tracking-SARS-CoV-2-variants">https://www.who.int/activities/tracking-SARS-CoV-2-variants</ext-link></p></fn><fn id="FN10"><label>9</label><p id="P60"><ext-link ext-link-type="uri" xlink:href="https://www.cbsnews.com/news/covid-variants-ba46-bf7-ba275-rise-cdc-tracking">https://www.cbsnews.com/news/covid-variants-ba46-bf7-ba275-rise-cdc-tracking</ext-link>/</p></fn><fn id="FN11"><label>10</label><p id="P61"><ext-link ext-link-type="uri" xlink:href="https://covid19.who.int/">https://covid19.who.int/</ext-link></p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amicone</surname><given-names>M</given-names></name><name><surname>Borges</surname><given-names>V</given-names></name><name><surname>Alves</surname><given-names>MJ</given-names></name><name><surname>Isidro</surname><given-names>J</given-names></name><name><surname>Zé-Zé</surname><given-names>L</given-names></name><name><surname>Duarte</surname><given-names>S</given-names></name><name><surname>Vieira</surname><given-names>L</given-names></name><name><surname>Guiomar</surname><given-names>R</given-names></name><name><surname>Gomes</surname><given-names>JP</given-names></name><name><surname>Gordo</surname><given-names>I</given-names></name></person-group><article-title>Mutation rate of SARS-CoV-2 and emergence of mutators during experimental evolution</article-title><source>Evolution, Medicine, and Public Health</source><year>2022</year><volume>10</volume><issue>1</issue><fpage>142</fpage><lpage>155</lpage><pub-id pub-id-type="pmcid">PMC8996265</pub-id><pub-id pub-id-type="pmid">35419205</pub-id><pub-id pub-id-type="doi">10.1093/emph/eoac010</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beiko</surname><given-names>RG</given-names></name><name><surname>Charlebois</surname><given-names>RL</given-names></name></person-group><article-title>A simulation test bed for hypotheses of genome evolution</article-title><source>Bioinformatics</source><year>2007</year><volume>23</volume><issue>7</issue><fpage>825</fpage><lpage>831</lpage><pub-id pub-id-type="pmid">17267425</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Belouzard</surname><given-names>S</given-names></name><name><surname>Chu</surname><given-names>VC</given-names></name><name><surname>Whittaker</surname><given-names>GR</given-names></name></person-group><article-title>Activation of the SARS coronavirus spike protein via sequential proteolytic cleavage at two distinct sites</article-title><source>Proceedings of the National Academy of Sciences</source><year>2009</year><volume>106</volume><issue>14</issue><fpage>5871</fpage><lpage>5876</lpage><pub-id pub-id-type="pmcid">PMC2660061</pub-id><pub-id pub-id-type="pmid">19321428</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0809524106</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Callaway</surname><given-names>E</given-names></name><etal/></person-group><article-title>Are covid surges becoming more predictable?</article-title><source>Nature</source><year>2022</year><volume>605</volume><issue>7909</issue><fpage>204</fpage><lpage>206</lpage><pub-id pub-id-type="pmid">35523871</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>R</given-names></name><name><surname>Wang</surname><given-names>M</given-names></name><name><surname>Wei</surname><given-names>GW</given-names></name></person-group><article-title>Mutations strengthened SARS-CoV-2 infectivity</article-title><source>Journal of Molecular Biology</source><year>2020</year><volume>432</volume><issue>19</issue><fpage>5212</fpage><lpage>5226</lpage><pub-id pub-id-type="pmcid">PMC7375973</pub-id><pub-id pub-id-type="pmid">32710986</pub-id><pub-id pub-id-type="doi">10.1016/j.jmb.2020.07.009</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chi</surname><given-names>X</given-names></name><name><surname>Yan</surname><given-names>R</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>G</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Hao</surname><given-names>M</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>Fan</surname><given-names>P</given-names></name><name><surname>Dong</surname><given-names>Y</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name><etal/></person-group><article-title>A neutralizing human antibody binds to the n-terminal domain of the spike protein of SARS-CoV-2</article-title><source>Science</source><year>2020</year><volume>369</volume><issue>6504</issue><fpage>650</fpage><lpage>655</lpage><pub-id pub-id-type="pmcid">PMC7319273</pub-id><pub-id pub-id-type="pmid">32571838</pub-id><pub-id pub-id-type="doi">10.1126/science.abc6952</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><collab>Coronaviridae Study Group of the International Committee on Taxonomy of Viruses</collab><article-title>The species severe acute respiratory syndrome-related coronavirus: classifying 2019-nCoV and naming it SARS-CoV-2</article-title><source>Nature Microbiology</source><year>2020</year><volume>5</volume><issue>4</issue><fpage>536</fpage><lpage>544</lpage><pub-id pub-id-type="pmcid">PMC7095448</pub-id><pub-id pub-id-type="pmid">32123347</pub-id><pub-id pub-id-type="doi">10.1038/s41564-020-0695-z</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Maio</surname><given-names>N</given-names></name><name><surname>Boulton</surname><given-names>W</given-names></name><name><surname>Weilguny</surname><given-names>L</given-names></name><name><surname>Walker</surname><given-names>CR</given-names></name><name><surname>Turakhia</surname><given-names>Y</given-names></name><name><surname>Corbett-Detig</surname><given-names>R</given-names></name><name><surname>Goldman</surname><given-names>N</given-names></name></person-group><article-title>phastsim: efficient simulation of sequence evolution for pandemic-scale datasets</article-title><source>PLoS Computational Biology</source><year>2022</year><volume>18</volume><issue>4</issue><elocation-id>e1010056</elocation-id><pub-id pub-id-type="pmcid">PMC9094560</pub-id><pub-id pub-id-type="pmid">35486906</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1010056</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devlin</surname><given-names>J</given-names></name><name><surname>Chang</surname><given-names>MW</given-names></name><name><surname>Lee</surname><given-names>K</given-names></name><name><surname>Toutanova</surname><given-names>K</given-names></name></person-group><article-title>Bert: Pre-training of deep bidirectional transformers for language understanding</article-title><source>arXiv preprint</source><year>2018</year><elocation-id>arXiv:1810.04805</elocation-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drosten</surname><given-names>C</given-names></name><name><surname>Guünther</surname><given-names>S</given-names></name><name><surname>Preiser</surname><given-names>W</given-names></name><name><surname>Van Der Werf</surname><given-names>S</given-names></name><name><surname>Brodt</surname><given-names>HR</given-names></name><name><surname>Becker</surname><given-names>S</given-names></name><name><surname>Rabenau</surname><given-names>H</given-names></name><name><surname>Panning</surname><given-names>M</given-names></name><name><surname>Kolesnikova</surname><given-names>L</given-names></name><name><surname>Fouchier</surname><given-names>RA</given-names></name><etal/></person-group><article-title>Identification of a novel coronavirus in patients with severe acute respiratory syndrome</article-title><source>New England Journal of Medicine</source><year>2003</year><volume>348</volume><issue>20</issue><fpage>1967</fpage><lpage>1976</lpage><pub-id pub-id-type="pmid">12690091</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duffy</surname><given-names>S</given-names></name></person-group><article-title>Why are RNA virus mutation rates so damn high?</article-title><source>PLoS Biology</source><year>2018</year><volume>16</volume><issue>8</issue><elocation-id>e3000003</elocation-id><pub-id pub-id-type="pmcid">PMC6107253</pub-id><pub-id pub-id-type="pmid">30102691</pub-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3000003</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elnaggar</surname><given-names>A</given-names></name><name><surname>Heinzinger</surname><given-names>M</given-names></name><name><surname>Dallago</surname><given-names>C</given-names></name><name><surname>Rehawi</surname><given-names>G</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Jones</surname><given-names>L</given-names></name><name><surname>Gibbs</surname><given-names>T</given-names></name><name><surname>Feher</surname><given-names>T</given-names></name><name><surname>Angerer</surname><given-names>C</given-names></name><name><surname>Steinegger</surname><given-names>M</given-names></name><name><surname>Bhowmik</surname><given-names>D</given-names></name><etal/></person-group><article-title>ProtTrans: Toward understanding the language of life through self-supervised learning</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><year>2022</year><volume>44</volume><issue>10</issue><fpage>7112</fpage><lpage>7127</lpage><pub-id pub-id-type="pmid">34232869</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ewing</surname><given-names>G</given-names></name><name><surname>Hermisson</surname><given-names>J</given-names></name></person-group><article-title>MSMS: a coalescent simulation program including recombination, demographic structure and selection at a single locus</article-title><source>Bioinformatics</source><year>2010</year><volume>26</volume><issue>16</issue><fpage>2064</fpage><lpage>2065</lpage><pub-id pub-id-type="pmcid">PMC2916717</pub-id><pub-id pub-id-type="pmid">20591904</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btq322</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferruz</surname><given-names>N</given-names></name><name><surname>Schmidt</surname><given-names>S</given-names></name><name><surname>Höcker</surname><given-names>B</given-names></name></person-group><article-title>ProtGPT2 is a deep unsupervised language model for protein design</article-title><source>Nature Communications</source><year>2022</year><volume>13</volume><issue>1</issue><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="pmcid">PMC9329459</pub-id><pub-id pub-id-type="pmid">35896542</pub-id><pub-id pub-id-type="doi">10.1038/s41467-022-32007-7</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fletcher</surname><given-names>W</given-names></name><name><surname>Yang</surname><given-names>Z</given-names></name></person-group><article-title>Indelible: a flexible simulator of biological sequence evolution</article-title><source>Molecular Biology and Evolution</source><year>2009</year><volume>26</volume><issue>8</issue><fpage>1879</fpage><lpage>1888</lpage><pub-id pub-id-type="pmcid">PMC2712615</pub-id><pub-id pub-id-type="pmid">19423664</pub-id><pub-id pub-id-type="doi">10.1093/molbev/msp098</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallagher</surname><given-names>TM</given-names></name><name><surname>Buchmeier</surname><given-names>MJ</given-names></name></person-group><article-title>Coronavirus spike proteins in viral entry and pathogenesis</article-title><source>Virology</source><year>2001</year><volume>279</volume><issue>2</issue><fpage>371</fpage><lpage>374</lpage><pub-id pub-id-type="pmcid">PMC7133764</pub-id><pub-id pub-id-type="pmid">11162792</pub-id><pub-id pub-id-type="doi">10.1006/viro.2000.0757</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goyal</surname><given-names>P</given-names></name><name><surname>Dollár</surname><given-names>P</given-names></name><name><surname>Girshick</surname><given-names>R</given-names></name><name><surname>Noordhuis</surname><given-names>P</given-names></name><name><surname>Wesolowski</surname><given-names>L</given-names></name><name><surname>Kyrola</surname><given-names>A</given-names></name><name><surname>Tulloch</surname><given-names>A</given-names></name><name><surname>Jia</surname><given-names>Y</given-names></name><name><surname>He</surname><given-names>K</given-names></name></person-group><article-title>Accurate, large minibatch SGD: Training imagenet in 1 hour</article-title><source>arXiv preprint</source><year>2017</year><elocation-id>arXiv:1706.02677</elocation-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>X</given-names></name><name><surname>Xue</surname><given-names>F</given-names></name><name><surname>Ren</surname><given-names>X</given-names></name><name><surname>You</surname><given-names>Y</given-names></name></person-group><article-title>Large-scale deep learning optimizations: A comprehensive survey</article-title><source>arXiv preprint</source><year>2021</year><elocation-id>arXiv:2111.00856</elocation-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoffer</surname><given-names>E</given-names></name><name><surname>Hubara</surname><given-names>I</given-names></name><name><surname>Soudry</surname><given-names>D</given-names></name></person-group><article-title>Train longer, generalize better: closing the generalization gap in large batch training of neural networks</article-title><source>Advances in neural information processing systems</source><year>2017</year><volume>30</volume></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoffmann</surname><given-names>M</given-names></name><name><surname>Kleine-Weber</surname><given-names>H</given-names></name><name><surname>Schroeder</surname><given-names>S</given-names></name><name><surname>Krüger</surname><given-names>N</given-names></name><name><surname>Herrler</surname><given-names>T</given-names></name><name><surname>Erichsen</surname><given-names>S</given-names></name><name><surname>Schiergens</surname><given-names>TS</given-names></name><name><surname>Herrler</surname><given-names>G</given-names></name><name><surname>Wu</surname><given-names>NH</given-names></name><name><surname>Nitsche</surname><given-names>A</given-names></name><etal/></person-group><article-title>SARS-CoV-2 cell entry depends on ACE2 and TMPRSS2 and is blocked by a clinically proven protease inhibitor</article-title><source>Cell</source><year>2020</year><volume>181</volume><issue>2</issue><fpage>271</fpage><lpage>280</lpage><pub-id pub-id-type="pmcid">PMC7102627</pub-id><pub-id pub-id-type="pmid">32142651</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2020.02.052</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howard</surname><given-names>J</given-names></name><name><surname>Ruder</surname><given-names>S</given-names></name></person-group><article-title>Universal language model fine-tuning for text classification</article-title><source>arXiv preprint</source><year>2018</year><elocation-id>arXiv:1801.06146</elocation-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hudson</surname><given-names>RR</given-names></name></person-group><article-title>Generating samples under a Wright-Fisher neutral model of genetic variation</article-title><source>Bioinformatics</source><year>2002</year><volume>18</volume><issue>2</issue><fpage>337</fpage><lpage>338</lpage><pub-id pub-id-type="pmid">11847089</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ito</surname><given-names>K</given-names></name><name><surname>Piantham</surname><given-names>C</given-names></name><name><surname>Nishiura</surname><given-names>H</given-names></name></person-group><article-title>Predicted dominance of variant Delta of SARS-CoV-2 before Tokyo olympic games, Japan, July 2021</article-title><source>Eurosurveillance</source><year>2021</year><volume>26</volume><issue>27</issue><elocation-id>2100570</elocation-id><pub-id pub-id-type="pmcid">PMC8268651</pub-id><pub-id pub-id-type="pmid">34240695</pub-id><pub-id pub-id-type="doi">10.2807/1560-7917.ES.2021.26.27.2100570</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keskar</surname><given-names>NS</given-names></name><name><surname>Mudigere</surname><given-names>D</given-names></name><name><surname>Nocedal</surname><given-names>J</given-names></name><name><surname>Smelyanskiy</surname><given-names>M</given-names></name><name><surname>Tang</surname><given-names>PTP</given-names></name></person-group><article-title>On large-batch training for deep learning: Generalization gap and sharp minima</article-title><source>arXiv preprint</source><year>2016</year><elocation-id>arXiv:1609.04836</elocation-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lamers</surname><given-names>MM</given-names></name><name><surname>Haagmans</surname><given-names>BL</given-names></name></person-group><article-title>SARS-CoV-2 pathogenesis</article-title><source>Nature Reviews Microbiology</source><year>2022</year><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="pmid">35354968</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lan</surname><given-names>Z</given-names></name><name><surname>Chen</surname><given-names>M</given-names></name><name><surname>Goodman</surname><given-names>S</given-names></name><name><surname>Gimpel</surname><given-names>K</given-names></name><name><surname>Sharma</surname><given-names>P</given-names></name><name><surname>Soricut</surname><given-names>R</given-names></name></person-group><article-title>ALBERT: A lite BERT for self-supervised learning of language representations</article-title><source>arXiv preprint</source><year>2019</year><elocation-id>arXiv:1909.11942</elocation-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laval</surname><given-names>G</given-names></name><name><surname>Excoffier</surname><given-names>L</given-names></name></person-group><article-title>SIMCOAL 2.0: a program to simulate genomic diversity over large recombining regions in a subdivided population with a complex history</article-title><source>Bioinformatics</source><year>2004</year><volume>20</volume><issue>15</issue><fpage>2485</fpage><lpage>2487</lpage><pub-id pub-id-type="pmid">15117750</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Wu</surname><given-names>YN</given-names></name><name><surname>Zhang</surname><given-names>S</given-names></name><name><surname>Kang</surname><given-names>XP</given-names></name><name><surname>Jiang</surname><given-names>T</given-names></name></person-group><article-title>Deep learning based on biologically interpretable genome representation predicts two types of human adaptation of SARS-CoV-2 variants</article-title><source>Briefings in Bioinformatics</source><year>2022</year><volume>23</volume><issue>3</issue><elocation-id>bbac036</elocation-id><pub-id pub-id-type="pmcid">PMC9116219</pub-id><pub-id pub-id-type="pmid">35233612</pub-id><pub-id pub-id-type="doi">10.1093/bib/bbac036</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>W</given-names></name><name><surname>Moore</surname><given-names>MJ</given-names></name><name><surname>Vasilieva</surname><given-names>N</given-names></name><name><surname>Sui</surname><given-names>J</given-names></name><name><surname>Wong</surname><given-names>SK</given-names></name><name><surname>Berne</surname><given-names>MA</given-names></name><name><surname>Somasundaran</surname><given-names>M</given-names></name><name><surname>Sullivan</surname><given-names>JL</given-names></name><name><surname>Luzuriaga</surname><given-names>K</given-names></name><name><surname>Greenough</surname><given-names>TC</given-names></name><etal/></person-group><article-title>Angiotensin-converting enzyme 2 is a functional receptor for the SARS coronavirus</article-title><source>Nature</source><year>2003</year><volume>426</volume><issue>6965</issue><fpage>450</fpage><lpage>454</lpage><pub-id pub-id-type="pmcid">PMC7095016</pub-id><pub-id pub-id-type="pmid">14647384</pub-id><pub-id pub-id-type="doi">10.1038/nature02145</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Z</given-names></name><name><surname>Wallace</surname><given-names>E</given-names></name><name><surname>Shen</surname><given-names>S</given-names></name><name><surname>Lin</surname><given-names>K</given-names></name><name><surname>Keutzer</surname><given-names>K</given-names></name><name><surname>Klein</surname><given-names>D</given-names></name><name><surname>Gonzalez</surname><given-names>J</given-names></name></person-group><source>Train big, then compress: Rethinking model size for efficient training and inference of transformers</source><conf-name>International Conference on Machine Learning</conf-name><year>2020</year><fpage>5958</fpage><lpage>5968</lpage></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Ott</surname><given-names>M</given-names></name><name><surname>Goyal</surname><given-names>N</given-names></name><name><surname>Du</surname><given-names>J</given-names></name><name><surname>Joshi</surname><given-names>M</given-names></name><name><surname>Chen</surname><given-names>D</given-names></name><name><surname>Levy</surname><given-names>O</given-names></name><name><surname>Lewis</surname><given-names>M</given-names></name><name><surname>Zettlemoyer</surname><given-names>L</given-names></name><name><surname>Stoyanov</surname><given-names>V</given-names></name></person-group><article-title>RoBERTa: A robustly optimized BERT pretraining approach</article-title><source>arXiv preprint</source><year>2019</year><elocation-id>arXiv:1907.11692</elocation-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>G</given-names></name><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Gao</surname><given-names>GF</given-names></name></person-group><article-title>Bat-to-human: spike features determining ‘host jump’of coronaviruses SARS-CoV, MERS-CoV, and beyond</article-title><source>Trends in Microbiology</source><year>2015</year><volume>23</volume><issue>8</issue><fpage>468</fpage><lpage>478</lpage><pub-id pub-id-type="pmcid">PMC7125587</pub-id><pub-id pub-id-type="pmid">26206723</pub-id><pub-id pub-id-type="doi">10.1016/j.tim.2015.06.003</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>R</given-names></name><name><surname>Zhao</surname><given-names>X</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Niu</surname><given-names>P</given-names></name><name><surname>Yang</surname><given-names>B</given-names></name><name><surname>Wu</surname><given-names>H</given-names></name><name><surname>Wang</surname><given-names>W</given-names></name><name><surname>Song</surname><given-names>H</given-names></name><name><surname>Huang</surname><given-names>B</given-names></name><name><surname>Zhu</surname><given-names>N</given-names></name><etal/></person-group><article-title>Genomic characterisation and epidemiology of 2019 novel coronavirus: implications for virus origins and receptor binding</article-title><source>The Lancet</source><year>2020</year><volume>395</volume><issue>10224</issue><fpage>565</fpage><lpage>574</lpage><pub-id pub-id-type="pmcid">PMC7159086</pub-id><pub-id pub-id-type="pmid">32007145</pub-id><pub-id pub-id-type="doi">10.1016/S0140-6736(20)30251-8</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Mohamed</surname><given-names>T</given-names></name><name><surname>Sayed</surname><given-names>S</given-names></name><name><surname>Salah</surname><given-names>A</given-names></name><name><surname>Houssein</surname><given-names>EH</given-names></name></person-group><source>Next generation sequence prediction intelligent system for SARS-CoV-2 using deep learning neural network</source><conf-name>2021 17th International Computer Engineering Conference (ICENCO)</conf-name><conf-sponsor>IEEE</conf-sponsor><year>2021</year><fpage>88</fpage><lpage>93</lpage></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Obermeyer</surname><given-names>F</given-names></name><name><surname>Jankowiak</surname><given-names>M</given-names></name><name><surname>Barkas</surname><given-names>N</given-names></name><name><surname>Schaffner</surname><given-names>SF</given-names></name><name><surname>Pyle</surname><given-names>JD</given-names></name><name><surname>Yurkovetskiy</surname><given-names>L</given-names></name><name><surname>Bosso</surname><given-names>M</given-names></name><name><surname>Park</surname><given-names>DJ</given-names></name><name><surname>Babadi</surname><given-names>M</given-names></name><name><surname>MacInnis</surname><given-names>BL</given-names></name><etal/></person-group><article-title>Analysis of 6.4 million SARS-CoV-2 genomes identifies mutations associated with fitness</article-title><source>Science</source><year>2022</year><volume>376</volume><issue>6599</issue><fpage>1327</fpage><lpage>1332</lpage><pub-id pub-id-type="pmcid">PMC9161372</pub-id><pub-id pub-id-type="pmid">35608456</pub-id><pub-id pub-id-type="doi">10.1126/science.abm1208</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ofer</surname><given-names>D</given-names></name><name><surname>Brandes</surname><given-names>N</given-names></name><name><surname>Linial</surname><given-names>M</given-names></name></person-group><article-title>The language of proteins: Nlp, machine learning &amp; protein sequences</article-title><source>Computational and Structural Biotechnology Journal</source><year>2021</year><volume>19</volume><fpage>1750</fpage><lpage>1758</lpage><pub-id pub-id-type="pmcid">PMC8050421</pub-id><pub-id pub-id-type="pmid">33897979</pub-id><pub-id pub-id-type="doi">10.1016/j.csbj.2021.03.022</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Osawa</surname><given-names>K</given-names></name><name><surname>Tsuji</surname><given-names>Y</given-names></name><name><surname>Ueno</surname><given-names>Y</given-names></name><name><surname>Naruse</surname><given-names>A</given-names></name><name><surname>Foo</surname><given-names>C</given-names></name><name><surname>Yokota</surname><given-names>R</given-names></name></person-group><article-title>Scalable and practical natural gradient for large-scale deep learning</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><year>2022</year><volume>44</volume><issue>1</issue><fpage>404</fpage><lpage>415</lpage><pub-id pub-id-type="pmid">32750792</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pucci</surname><given-names>F</given-names></name><name><surname>Rooman</surname><given-names>M</given-names></name></person-group><article-title>Prediction and evolution of the molecular fitness of SARS-CoV-2 variants: Introducing SpikePro</article-title><source>Viruses</source><year>2021</year><volume>13</volume><issue>5</issue><fpage>935</fpage><pub-id pub-id-type="pmcid">PMC8158131</pub-id><pub-id pub-id-type="pmid">34070055</pub-id><pub-id pub-id-type="doi">10.3390/v13050935</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rambaut</surname><given-names>A</given-names></name><name><surname>Grass</surname><given-names>NC</given-names></name></person-group><article-title>Seq-Gen: an application for the Monte Carlo simulation of DNA sequence evolution along phylogenetic trees</article-title><source>Bioinformatics</source><year>1997</year><volume>13</volume><issue>3</issue><fpage>235</fpage><lpage>238</lpage><pub-id pub-id-type="pmid">9183526</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rives</surname><given-names>A</given-names></name><name><surname>Meier</surname><given-names>J</given-names></name><name><surname>Sercu</surname><given-names>T</given-names></name><name><surname>Goyal</surname><given-names>S</given-names></name><name><surname>Lin</surname><given-names>Z</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Guo</surname><given-names>D</given-names></name><name><surname>Ott</surname><given-names>M</given-names></name><name><surname>Zitnick</surname><given-names>CL</given-names></name><name><surname>Ma</surname><given-names>J</given-names></name><etal/></person-group><article-title>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</article-title><source>Proceedings of the National Academy of Sciences</source><year>2021</year><volume>118</volume><issue>15</issue><elocation-id>e2016239118</elocation-id><pub-id pub-id-type="pmcid">PMC8053943</pub-id><pub-id pub-id-type="pmid">33876751</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2016239118</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sender</surname><given-names>R</given-names></name><name><surname>Bar-On</surname><given-names>YM</given-names></name><name><surname>Gleizer</surname><given-names>S</given-names></name><name><surname>Bernshtein</surname><given-names>B</given-names></name><name><surname>Flamholz</surname><given-names>A</given-names></name><name><surname>Phillips</surname><given-names>R</given-names></name><name><surname>Milo</surname><given-names>R</given-names></name></person-group><article-title>The total number and mass of SARS-CoV-2 virions</article-title><source>Proceedings of the National Academy of Sciences</source><year>2021</year><volume>118</volume><issue>25</issue><elocation-id>e2024815118</elocation-id><pub-id pub-id-type="pmcid">PMC8237675</pub-id><pub-id pub-id-type="pmid">34083352</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2024815118</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shchur</surname><given-names>V</given-names></name><name><surname>Spirin</surname><given-names>V</given-names></name><name><surname>Sirotkin</surname><given-names>D</given-names></name><name><surname>Burovski</surname><given-names>E</given-names></name><name><surname>De Maio</surname><given-names>N</given-names></name><name><surname>Corbett-Detig</surname><given-names>R</given-names></name></person-group><article-title>Vgsim: scalable viral genealogy simulator for global pandemic</article-title><source>PLOS Computational Biology</source><year>2022</year><volume>18</volume><issue>8</issue><elocation-id>e1010409</elocation-id><pub-id pub-id-type="pmcid">PMC9447924</pub-id><pub-id pub-id-type="pmid">36001646</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1010409</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shu</surname><given-names>Y</given-names></name><name><surname>McCauley</surname><given-names>J</given-names></name></person-group><article-title>GISAID: Global initiative on sharing all influenza data–from vision to reality</article-title><source>Eurosurveillance</source><year>2017</year><volume>22</volume><issue>13</issue><fpage>30494</fpage><pub-id pub-id-type="pmcid">PMC5388101</pub-id><pub-id pub-id-type="pmid">28382917</pub-id><pub-id pub-id-type="doi">10.2807/1560-7917.ES.2017.22.13.30494</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simmons</surname><given-names>G</given-names></name><name><surname>Zmora</surname><given-names>P</given-names></name><name><surname>Gierer</surname><given-names>S</given-names></name><name><surname>Heurich</surname><given-names>A</given-names></name><name><surname>Pöhlmann</surname><given-names>S</given-names></name></person-group><article-title>Proteolytic activation of the SARS-coronavirus spike protein: cutting enzymes at the cutting edge of antiviral research</article-title><source>Antiviral Research</source><year>2013</year><volume>100</volume><issue>3</issue><fpage>605</fpage><lpage>614</lpage><pub-id pub-id-type="pmcid">PMC3889862</pub-id><pub-id pub-id-type="pmid">24121034</pub-id><pub-id pub-id-type="doi">10.1016/j.antiviral.2013.09.028</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sipos</surname><given-names>B</given-names></name><name><surname>Massingham</surname><given-names>T</given-names></name><name><surname>Jordan</surname><given-names>GE</given-names></name><name><surname>Goldman</surname><given-names>N</given-names></name></person-group><article-title>PhyloSim-Monte Carlo simulation of sequence evolution in the R statistical computing environment</article-title><source>BMC Bioinformatics</source><year>2011</year><volume>12</volume><issue>1</issue><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="pmcid">PMC3102636</pub-id><pub-id pub-id-type="pmid">21504561</pub-id><pub-id pub-id-type="doi">10.1186/1471-2105-12-104</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Starr</surname><given-names>TN</given-names></name><name><surname>Greaney</surname><given-names>AJ</given-names></name><name><surname>Hilton</surname><given-names>SK</given-names></name><name><surname>Ellis</surname><given-names>D</given-names></name><name><surname>Crawford</surname><given-names>KH</given-names></name><name><surname>Dingens</surname><given-names>AS</given-names></name><name><surname>Navarro</surname><given-names>MJ</given-names></name><name><surname>Bowen</surname><given-names>JE</given-names></name><name><surname>Tortorici</surname><given-names>MA</given-names></name><name><surname>Walls</surname><given-names>AC</given-names></name><etal/></person-group><article-title>Deep mutational scanning of SARS-CoV-2 receptor binding domain reveals constraints on folding and ACE2 binding</article-title><source>Cell</source><year>2020</year><volume>182</volume><issue>5</issue><fpage>1295</fpage><lpage>1310</lpage><pub-id pub-id-type="pmcid">PMC7418704</pub-id><pub-id pub-id-type="pmid">32841599</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2020.08.012</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strait</surname><given-names>BJ</given-names></name><name><surname>Dewey</surname><given-names>TG</given-names></name></person-group><article-title>The Shannon information entropy of protein sequences</article-title><source>Biophysical Journal</source><year>1996</year><volume>71</volume><issue>1</issue><fpage>148</fpage><lpage>155</lpage><pub-id pub-id-type="pmcid">PMC1233466</pub-id><pub-id pub-id-type="pmid">8804598</pub-id><pub-id pub-id-type="doi">10.1016/S0006-3495(96)79210-X</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Su</surname><given-names>S</given-names></name><name><surname>Wong</surname><given-names>G</given-names></name><name><surname>Shi</surname><given-names>W</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Lai</surname><given-names>AC</given-names></name><name><surname>Zhou</surname><given-names>J</given-names></name><name><surname>Liu</surname><given-names>W</given-names></name><name><surname>Bi</surname><given-names>Y</given-names></name><name><surname>Gao</surname><given-names>GF</given-names></name></person-group><article-title>Epidemiology, genetic recombination, and pathogenesis of coronaviruses</article-title><source>Trends in Microbiology</source><year>2016</year><volume>24</volume><issue>6</issue><fpage>490</fpage><lpage>502</lpage><pub-id pub-id-type="pmcid">PMC7125511</pub-id><pub-id pub-id-type="pmid">27012512</pub-id><pub-id pub-id-type="doi">10.1016/j.tim.2016.03.003</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suzek</surname><given-names>BE</given-names></name><name><surname>Huang</surname><given-names>H</given-names></name><name><surname>McGarvey</surname><given-names>P</given-names></name><name><surname>Mazumder</surname><given-names>R</given-names></name><name><surname>Wu</surname><given-names>CH</given-names></name></person-group><article-title>UniRef: comprehensive and non-redundant uniprot reference clusters</article-title><source>Bioinformatics</source><year>2007</year><volume>23</volume><issue>10</issue><fpage>1282</fpage><lpage>1288</lpage><pub-id pub-id-type="pmid">17379688</pub-id></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trifonov</surname><given-names>EN</given-names></name></person-group><article-title>The origin of the genetic code and of the earliest oligopeptides</article-title><source>Research in Microbiology</source><year>2009</year><volume>160</volume><issue>7</issue><fpage>481</fpage><lpage>486</lpage><pub-id pub-id-type="pmid">19524038</pub-id></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walls</surname><given-names>AC</given-names></name><name><surname>Park</surname><given-names>YJ</given-names></name><name><surname>Tortorici</surname><given-names>MA</given-names></name><name><surname>Wall</surname><given-names>A</given-names></name><name><surname>McGuire</surname><given-names>AT</given-names></name><name><surname>Veesler</surname><given-names>D</given-names></name></person-group><article-title>Structure, function, and antigenicity of the SARS-CoV-2 spike glycoprotein</article-title><source>Cell</source><year>2020</year><volume>181</volume><issue>2</issue><fpage>281</fpage><lpage>292</lpage><pub-id pub-id-type="pmcid">PMC7102599</pub-id><pub-id pub-id-type="pmid">32155444</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2020.02.058</pub-id></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wan</surname><given-names>Y</given-names></name><name><surname>Shang</surname><given-names>J</given-names></name><name><surname>Graham</surname><given-names>R</given-names></name><name><surname>Baric</surname><given-names>RS</given-names></name><name><surname>Li</surname><given-names>F</given-names></name></person-group><article-title>Receptor recognition by the novel coronavirus from Wuhan: an analysis based on decade-long structural studies of SARS coronavirus</article-title><source>Journal of Virology</source><year>2020</year><volume>94</volume><issue>7</issue><elocation-id>e00127–20</elocation-id><pub-id pub-id-type="pmcid">PMC7081895</pub-id><pub-id pub-id-type="pmid">31996437</pub-id><pub-id pub-id-type="doi">10.1128/JVI.00127-20</pub-id></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wrapp</surname><given-names>D</given-names></name><name><surname>Wang</surname><given-names>N</given-names></name><name><surname>Corbett</surname><given-names>KS</given-names></name><name><surname>Goldsmith</surname><given-names>JA</given-names></name><name><surname>Hsieh</surname><given-names>CL</given-names></name><name><surname>Abiona</surname><given-names>O</given-names></name><name><surname>Graham</surname><given-names>BS</given-names></name><name><surname>McLellan</surname><given-names>JS</given-names></name></person-group><article-title>Cryo-EM structure of the 2019-nCoV spike in the prefusion conformation</article-title><source>Science</source><year>2020</year><volume>367</volume><issue>6483</issue><fpage>1260</fpage><lpage>1263</lpage><pub-id pub-id-type="pmcid">PMC7164637</pub-id><pub-id pub-id-type="pmid">32075877</pub-id><pub-id pub-id-type="doi">10.1126/science.abb2507</pub-id></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>C</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>P</given-names></name><name><surname>Zhong</surname><given-names>W</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Xu</surname><given-names>Y</given-names></name><name><surname>Li</surname><given-names>M</given-names></name><name><surname>Li</surname><given-names>X</given-names></name><etal/></person-group><article-title>Analysis of therapeutic targets for SARS-CoV-2 and discovery of potential drugs by computational methods</article-title><source>Acta Pharmaceutica Sinica B</source><year>2020</year><volume>10</volume><issue>5</issue><fpage>766</fpage><lpage>788</lpage><pub-id pub-id-type="pmcid">PMC7102550</pub-id><pub-id pub-id-type="pmid">32292689</pub-id><pub-id pub-id-type="doi">10.1016/j.apsb.2020.02.008</pub-id></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>Xu</surname><given-names>S</given-names></name><name><surname>Yau</surname><given-names>ST</given-names></name><name><surname>Wu</surname><given-names>Y</given-names></name></person-group><article-title>Phylotransformer: A discriminative model for mutation prediction based on a multi-head self-attention mechanism</article-title><source>arXiv preprint</source><year>2021</year><elocation-id>arXiv:2111.01969</elocation-id></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>Z</given-names></name><name><surname>Dai</surname><given-names>Z</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>Carbonell</surname><given-names>J</given-names></name><name><surname>Salakhutdinov</surname><given-names>RR</given-names></name><name><surname>Le</surname><given-names>QV</given-names></name></person-group><article-title>XLNet: Generalized autoregressive pretraining for language understanding</article-title><source>Advances in neural information processing systems</source><year>2019</year><volume>32</volume></element-citation></ref><ref id="R57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yin</surname><given-names>Y</given-names></name><name><surname>Wunderink</surname><given-names>RG</given-names></name></person-group><article-title>MERS, SARS and other coronaviruses as causes of pneumonia</article-title><source>Respirology</source><year>2018</year><volume>23</volume><issue>2</issue><fpage>130</fpage><lpage>137</lpage><pub-id pub-id-type="pmcid">PMC7169239</pub-id><pub-id pub-id-type="pmid">29052924</pub-id><pub-id pub-id-type="doi">10.1111/resp.13196</pub-id></element-citation></ref><ref id="R58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>L</given-names></name><name><surname>Tanwar</surname><given-names>DK</given-names></name><name><surname>Penha</surname><given-names>EDS</given-names></name><name><surname>Wolf</surname><given-names>YI</given-names></name><name><surname>Koonin</surname><given-names>EV</given-names></name><name><surname>Basu</surname><given-names>MK</given-names></name></person-group><article-title>Grammar of protein domain architectures</article-title><source>Proceedings of the National Academy of Sciences</source><year>2019</year><volume>116</volume><issue>9</issue><fpage>3636</fpage><lpage>3645</lpage><pub-id pub-id-type="pmcid">PMC6397568</pub-id><pub-id pub-id-type="pmid">30733291</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1814684116</pub-id></element-citation></ref><ref id="R59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zaki</surname><given-names>AM</given-names></name><name><surname>Van Boheemen</surname><given-names>S</given-names></name><name><surname>Bestebroer</surname><given-names>TM</given-names></name><name><surname>Osterhaus</surname><given-names>AD</given-names></name><name><surname>Fouchier</surname><given-names>RA</given-names></name></person-group><article-title>Isolation of a novel coronavirus from a man with pneumonia in Saudi Arabia</article-title><source>New England Journal of Medicine</source><year>2012</year><volume>367</volume><issue>19</issue><fpage>1814</fpage><lpage>1820</lpage><pub-id pub-id-type="pmid">23075143</pub-id></element-citation></ref><ref id="R60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>P</given-names></name><name><surname>Yang</surname><given-names>XL</given-names></name><name><surname>Wang</surname><given-names>XG</given-names></name><name><surname>Hu</surname><given-names>B</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>W</given-names></name><name><surname>Si</surname><given-names>HR</given-names></name><name><surname>Zhu</surname><given-names>Y</given-names></name><name><surname>Li</surname><given-names>B</given-names></name><name><surname>Huang</surname><given-names>CL</given-names></name><etal/></person-group><article-title>A pneumonia outbreak associated with a new coronavirus of probable bat origin</article-title><source>Nature</source><year>2020</year><volume>579</volume><issue>7798</issue><fpage>270</fpage><lpage>273</lpage><pub-id pub-id-type="pmcid">PMC7095418</pub-id><pub-id pub-id-type="pmid">32015507</pub-id><pub-id pub-id-type="doi">10.1038/s41586-020-2012-7</pub-id></element-citation></ref></ref-list></back><floats-group><boxed-text id="BX1" position="float" orientation="portrait"><caption><title>Justification</title></caption><p>We develop a novel multi-constraint variation prediction framework to simulate SARS-CoV-2 RBD mutations, reaching a peak performance of 366.8 PFLOPS with 96.5% scalability and achieving 493.9× speedup. Our method facilitates the prediction and prioritization of future high-risk variants for the early deployment of drugs and vaccines.</p></boxed-text><boxed-text id="BX2" position="float" orientation="portrait"><caption><title>Performance attributes</title></caption><p><table-wrap id="T2" position="anchor" orientation="portrait"><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Performance attribute</th><th align="left" valign="top">Our submission</th></tr></thead><tbody><tr><td align="left" valign="top">Category of achievement</td><td align="left" valign="top">time-to-solution, scalability</td></tr><tr><td align="left" valign="top">Type of method used</td><td align="left" valign="top">machine learning</td></tr><tr><td align="left" valign="top">Results reported for</td><td align="left" valign="top">whole application using and except I/O</td></tr><tr><td align="left" valign="top">Precision reported</td><td align="left" valign="top">mixed precision</td></tr><tr><td align="left" valign="top">System scale</td><td align="left" valign="top">results measured on full-scale system</td></tr><tr><td align="left" valign="top">Measurement mechanism</td><td align="left" valign="top">timers, FLOP count, performance modeling</td></tr></tbody></table></table-wrap></p></boxed-text><boxed-text id="BX3" position="float" orientation="portrait"><caption><title>Overview of the problem</title></caption><p>Coronavirus Disease 2019 (COVID-19) has spread rapidly to more than 200 countries or regions since December 2019. Due to its high infectivity, there have been over 645 million confirmed cases, including approximately 6.6 million deaths, reported by the World Health Organization (WHO) as of December 2022<sup><xref ref-type="fn" rid="FN2">1</xref></sup>. In addition to being a serious threat to human health, COVID-19 has had a catastrophic impact on the global economy.</p></boxed-text><fig id="F1" position="float"><label>Figure 1</label><caption><title>Overview of the problem and our solution.</title><p>(a) The structural diagram of SARS-CoV-2, in which the RBD on the spike protein is an important region to which hACE2 and the majority of neutralizing antibodies bind. (b) The approximate detection time and places of the five VOCs (Alpha, Beta, Gamma, Delta, and Omicron). (c) Waves of infections caused by the five VOCs from the outbreak of COVID-19 to the present. (d) The phylogenetic tree of SARS-CoV-2 VOCs and the comparison of the variation sites of the five VOCs in the RBD regions. (e) Our methodology for simulating the viral mutation in the RBD. With the support of an HPC optimization strategy that integrates software and hardware, a protein language model (ProtFound) is efficiently pretrained for the generation of RBD mutations. With reference to the mutation frequency of each mutation site in the RBD in the real world, ProtFound can generate billions of RBD variants. These variants are sequentially screened by binding affinity with hACE2, and antibody escape capability. The screened variants are used to fine-tune the ProtFound generator. The fine-tuned ProtFound model is more likely to generate viral variants with higher binding affinity to hACE2 and better capability for antibody escape.</p></caption><graphic xlink:href="EMS157419-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><p>(a) The workflow of our multi-constrains variation prediction (MCVP) framework. It consists of four modules, i.e., pretraining, fine-tuning, generation, and high-throughput screening. (b) Two transfer-learning models for high-throughput screening. Three modules make up the whole processing workflow: a feature extractor module, a feature refinement module, and a downstream task module. The protein embeddings learned by ProtFound are further refined through the coupling of global and local features. Finally, neural networks are trained for two different downstream tasks.</p></caption><graphic xlink:href="EMS157419-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>An overview of the employed optimization strategies.</title><p>(a) Operator fusion. Op means operator. To reduce the redundant memory accesses incurred by the successive execution of many small operators, we integrate multiple transdata operators into one transdata operator. (b) Operator replacement. Conv means convolution. Concat means concatenate. We replace two operators with one simplified operator to reduce the computational cost and model size. (c) Operator auto-tuning. TBE means Tensor Boosting Engine. GA means Genetic Algorithm. We use a genetic algorithm for tuning particular operators by identifying the optimal tiling policies. A well-designed tiling schedule can fully utilize the computing power of the hardware. (d) Mixed precision. All parameters in the model and optimizer are stored in single precision (32-bit), but most of the calculations in this model are performed in half precision (16-bit) to accelerate the training process. This mixed-precision implementation greatly reduces the training latency at the cost of potential overflow due to the limited representation range of half precision.</p></caption><graphic xlink:href="EMS157419-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><p>Strong scaling performance of the ProtFound model pretraining for a constant total problem size of approximately 408 million protein sequences. Each data point is labeled with the PFLOPS and parallel efficiency for the corresponding node count. The black dotted line represents the optimal scaling performance for reference.</p></caption><graphic xlink:href="EMS157419-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><p>Weak scaling performance of the ProtFound model pretraining for a constant problem size of 640 thousand protein sequences per node. Each data point is labeled with the PFLOPS and utilization for the corresponding node count. Here, the I/O operations include the storage of checkpoints and trained models.</p></caption><graphic xlink:href="EMS157419-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><title>The validation scheme for RBD mutations of VOCs and potential high-risk variants prediction.</title><p>(a), (b), (c), (d) Four VOCs before Omicron, i.e., Alpha, Beta, Gamma, and Delta, are simulated from wild type to themselves. (e) Omicron BA.5, a latest subvariant of Omicron, is simulated from Omicron BA.2 to itself. (f)Potential high-risk variants prediction. Omicron BA.5 is adopted as the reference sequence. After the high-throughput screening of hACE2 binding and antibody binding, the risk factor is calculated based on mutations relevant to fitness.</p></caption><graphic xlink:href="EMS157419-f006"/></fig><table-wrap id="T1" orientation="portrait" position="float"><label>Table 1</label><caption><title>High throughput screening of various variants</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Variant</th><th align="left" valign="top">1st screening<xref ref-type="table-fn" rid="TFN1">*</xref></th><th align="left" valign="top">2st screening<xref ref-type="table-fn" rid="TFN2">**</xref></th></tr></thead><tbody><tr><td align="left" valign="top">Alpha</td><td align="left" valign="top">39.8%</td><td align="left" valign="top">2.0%</td></tr><tr><td align="left" valign="top">Beta</td><td align="left" valign="top">13.3%</td><td align="left" valign="top">51.3%</td></tr><tr><td align="left" valign="top">Gamma</td><td align="left" valign="top">45.2%</td><td align="left" valign="top">33.8%</td></tr><tr><td align="left" valign="top">Delta</td><td align="left" valign="top">46.7%</td><td align="left" valign="top">19.1%</td></tr><tr><td align="left" valign="top">Omicron BA.5</td><td align="left" valign="top">90.4%</td><td align="left" valign="top">80.2%</td></tr></tbody></table><table-wrap-foot><fn id="TFN1"><label>*</label><p id="P62">Proportion retained in hACE2 binding screening</p></fn><fn id="TFN2"><label>**</label><p id="P63">Proportion retained in antibody binding screening</p></fn></table-wrap-foot></table-wrap></floats-group></article>