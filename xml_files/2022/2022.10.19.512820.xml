<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS156221</article-id><article-id pub-id-type="doi">10.1101/2022.10.19.512820</article-id><article-id pub-id-type="archive">PPR562490</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Training stochastic stabilized supralinear networks by dynamics-neutral growth</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Soo</surname><given-names>Wayne W.M.</given-names></name><email>wmws2@cam.ac.uk</email><aff id="A1">Department of Engineering, University of Cambridge</aff></contrib><contrib contrib-type="author"><name><surname>Lengyel</surname><given-names>Máté</given-names></name><email>m.lengyel@eng.cam.ac.uk</email><aff id="A2">Department of Engineering, University of Cambridge; Department of Cognitive Science, Central European University</aff></contrib></contrib-group><pub-date pub-type="nihms-submitted"><day>27</day><month>10</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>21</day><month>10</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">There continues to be a trade-off between the biological realism and performance of neural networks. Contemporary deep learning techniques allow neural networks to be trained to perform challenging computations at (near) human-level, but these networks typically violate key biological constraints. More detailed models of biological neural networks can incorporate many of these constraints but typically suffer from subpar performance and trainability. Here, we narrow this gap by developing an effective method for training a canonical model of cortical neural circuits, the stabilized supralinear network (SSN), that in previous work had to be constructed manually or trained with undue constraints. SSNs are particularly challenging to train for the same reasons that make them biologically realistic: they are characterized by strongly-connected excitatory cells and expansive firing rate non-linearities that together make them prone to dynamical instabilities unless stabilized by appropriately tuned recurrent inhibition. Our method avoids such instabilities by initializing a small network and gradually increasing network size via the dynamics-neutral addition of neurons during training. We first show how SSNs can be trained to perform typical machine learning tasks by training an SSN on MNIST classification. We then demonstrate the effectiveness of our method by training an SSN on the challenging task of performing amortized Markov chain Monte Carlo-based inference under a Gaussian scale mixture generative model of natural image patches with a rich and diverse set of basis functions – something that was not possible with previous methods. These results open the way to training realistic cortical-like neural networks on challenging tasks at scale.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><label>1</label><title>Introduction</title><p id="P2">Biological neural network models of the brain have long been studied to understand some of the fundamental network mechanisms employed by the brain [<xref ref-type="bibr" rid="R1">1</xref>–<xref ref-type="bibr" rid="R4">4</xref>]. However, these models were not capable of actually achieving the brain’s performance on its wide range of complex computations. Conversely, artificial neural networks have been achieving competitive performance in a multitude of image, language, and speech-related tasks, but typically without regard to biological realism [<xref ref-type="bibr" rid="R5">5</xref>–<xref ref-type="bibr" rid="R8">8</xref>]. In systems neuroscience, neural networks have also been trained on laboratory tasks employed in typical experiments [<xref ref-type="bibr" rid="R9">9</xref>–<xref ref-type="bibr" rid="R16">16</xref>]. These approaches provided important insights into the contributions of experimentally found macroscopic neural dynamics and representations to the successful performance of such tasks. However, these networks did not incorporate some of the most salient constraints on the detailed organization of cortical circuits. For example, they were purely feedforward [<xref ref-type="bibr" rid="R10">10</xref>], utilized neuronal transfer functions that were either outright saturating (e.g. tanh) or at best lacked the superlinear characteristics of cortical neurons (as in ReLUs, or the rectified softplus) [<xref ref-type="bibr" rid="R9">9</xref>–<xref ref-type="bibr" rid="R11">11</xref>, <xref ref-type="bibr" rid="R15">15</xref>, <xref ref-type="bibr" rid="R16">16</xref>], had noiseless dynamics [<xref ref-type="bibr" rid="R12">12</xref>], no separation of excitatory (E) and inhibitory (I) cells [<xref ref-type="bibr" rid="R9">9</xref>, <xref ref-type="bibr" rid="R10">10</xref>, <xref ref-type="bibr" rid="R15">15</xref>], and sometimes even employed artificial gating mechanisms such as LSTMs [<xref ref-type="bibr" rid="R13">13</xref>]. Due to these properties, these networks remained abstracted away from biological neurons in key aspects, and hence offered limited insight into the neuron-level mechanisms that drive their computations.</p><p id="P3">To bring the modeling of neural circuit mechanisms underlying challenging computations at (or near) single-cell resolution within reach, we develop a novel method to train an experimentally-supported neural network model, the stabilized supralinear network (SSN) [<xref ref-type="bibr" rid="R17">17</xref>]. Critically, the SSN satisfies many of the key constraints of cortical circuits: it has separate but recurrent excitatory and inhibitory populations, expansive (rectified power-law) single neuron non-linearities, realistic single-neuron time constants, and no reliance on artificial gating mechanisms. In its original form, the SSN continues a venerable tradition of handcrafted excitatory-inhibitory (E-I) networks whose dynamics have been extensively studied [<xref ref-type="bibr" rid="R18">18</xref>, <xref ref-type="bibr" rid="R19">19</xref>], and that helped reveal key consequences of the cortex operating in a balanced E-I regime [<xref ref-type="bibr" rid="R3">3</xref>, <xref ref-type="bibr" rid="R20">20</xref>–<xref ref-type="bibr" rid="R22">22</xref>]. In particular, the SSN accounts for the experimentally-observed effects of stimulus tuning, sublinear response summation and surround suppression of neural responses in sensory cortices [<xref ref-type="bibr" rid="R4">4</xref>, <xref ref-type="bibr" rid="R23">23</xref>], as well as oscillations, bistable and persistent responses [<xref ref-type="bibr" rid="R24">24</xref>]. Furthermore, when extended to include noise, the resultant stochastic SSN produces realistic, stimulus-modulated population-wide patterns of noise variability [<xref ref-type="bibr" rid="R25">25</xref>].</p><p id="P4">However, the same features that make the SSN an attractive substrate for its biological realism also make it particularly challenging to train it. This is because, in SSNs, excitatory cells are typically strongly connected with one-another in order to implement non-trivial nonlinear transformations of their inputs, such as divisive normalization, which underlie many of the experimentally observed phenomena for which they account. In addition, single neuron non-linearities are expansive to reflect the experimental finding that cortical neurons almost exclusively use the convex (non-saturating) part of their firing rate non-linearities under physiological conditions, including for the strongest stimuli [<xref ref-type="bibr" rid="R26">26</xref>] (even if they can be trivially driven to saturation by direct current injection, due to the refractory period of the mechanism generating action potentials). These properties make SSNs particularly susceptible to dynamical instabilities resulting in run-away excitation, thus rendering their training highly challenging. Indeed, in the few cases in which the training of SSNs was attempted, either noiseless neurons were used [<xref ref-type="bibr" rid="R27">27</xref>, <xref ref-type="bibr" rid="R28">28</xref>], or the network was so heavily under-parameterized that it substantially limited its expressivity [<xref ref-type="bibr" rid="R29">29</xref>].</p><p id="P5">Here, we develop a new method to train SSNs that avoids dynamical instabilities during training and is able to train SSNs at scale on a variety of tasks. While standard approaches to training networks only optimize weights and keep the architecture of the network fixed throughout network training (but see recent work in Refs. 30–32), our method alternates between optimizing network weights with a fixed architecture, and changing the architecture by growing the network. Importantly, in each network growth step, new neurons are added such that they do not affect the dynamics of the network before weights are optimized again. We first demonstrate the effectiveness of our method by training an SSN on a standard machine learning benchmark, MNIST classification [<xref ref-type="bibr" rid="R33">33</xref>]. We achieve only slightly lower accuracies than state-of-the-art, despite restricting our network to be biologically plausible. For a direct comparison, we also train an SSN on a probabilistic inference task that has been shown to have neurobiological relevance but has proved to be a challenging target [<xref ref-type="bibr" rid="R29">29</xref>]. Our approach successfully trains an SSN that has three orders of magnitude more parameters than what was previously possible to optimize. The trained SSN performs accurate inference under a Gaussian scale mixture-model (GSM) [<xref ref-type="bibr" rid="R34">34</xref>] with a set of basis functions that is sufficiently rich for allowing high-quality reconstructions of CIFAR-10 [<xref ref-type="bibr" rid="R35">35</xref>] images. Inference under this GSM represents an unachievable target for previous, heavily constrained approaches for training SSNs. These results validate our approach and open the way to training SSNs on a large variety of different tasks, and to using the resulting networks to study the circuit mechanisms of neural computations at a single cell resolution.</p></sec><sec id="S2" sec-type="methods"><label>2</label><title>Methods</title><sec id="S3"><label>2.1</label><title>Stochastic supralinear stabilized network</title><p id="P6">The SSN is a canonical model of cortical circuits that has been shown to account for a wealth of neural response properties in the primary visual cortex [<xref ref-type="bibr" rid="R17">17</xref>, <xref ref-type="bibr" rid="R4">4</xref>, <xref ref-type="bibr" rid="R25">25</xref>]. Its dynamics are described by <disp-formula id="FD1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>τ</mml:mi></mml:mstyle><mml:mfrac><mml:mrow><mml:mtext>d</mml:mtext><mml:mtext mathvariant="bold">u</mml:mtext></mml:mrow><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mtext mathvariant="bold">u</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="bold">f</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold">h</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mtext mathvariant="bold">W</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>r</mml:mtext><mml:mo>+</mml:mo><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>η</mml:mi></mml:mstyle></mml:mrow></mml:math></disp-formula> where <italic><bold>τ</bold></italic> collects the neurons’ time constants, <bold>u</bold> denotes their membrane potentials, <bold>W</bold> is the symaptic weight matrix, <italic><bold>η</bold></italic> represents noise, correlated across time with time constant <italic>τ<sub>η</sub></italic>, as well as across neurons with covariance <bold>Σ</bold><sub><italic>η</italic></sub>, <bold>f</bold>(<bold>h</bold>) represents a non-linear transformation of the external input <bold>h</bold>, <disp-formula id="FD2"><label>(2)</label><mml:math id="M2"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula> with parameters <bold>θ</bold><sub><italic>h</italic></sub> = {<italic>θ</italic><sub>1</sub>, <italic>θ</italic><sub>2</sub>, <italic>θ</italic><sub>3</sub>}, and <bold>r</bold> denotes neural firing rates, given by a rectified expansive non-linearity, <disp-formula id="FD3"><label>(3)</label><mml:math id="M3"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:msubsup><mml:mrow><mml:mo>⌊</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⌋</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>γ</mml:mi></mml:msubsup></mml:mrow></mml:math></disp-formula> with <italic>γ</italic> &gt; 1 and <italic>k</italic> &gt; 0. Note that there is no built-in upper saturation to the model, which presents potential instability problems – excitations within the network can exponentially grow unbounded through mutual and self-excitation. The network relies purely on inhibition to stabilize, which therefore restricts the parameter regime in which the network is stable. The network comprises of distinct populations of excitatory and inhibitory neurons, imposing restrictions on the polarity of the elements of <bold>W</bold>. Specifically, the elements of each column of <bold>W</bold> must have the same sign (positive for excitatory neurons, negative for inhibitory neurons).</p></sec><sec id="S4"><label>2.2</label><title>Theoretical motivation</title><p id="P7">We note that under the assumption of reasonable noise characteristics, it is always possible to add a new neuron to an existing stable SSN while preserving stability in a non-trivial manner. We begin by defining the newly-added neuron as a “twin” of an existing neuron. The new twin has the same incoming recurrent weights as its original counterpart. This way, the newly-added neuron is guaranteed to receive the same input as the original neuron. Second, the newly-added neuron also inherits the outgoing weights of its original counterpart, but such that all outgoing weights of both neurons are halved. Thus, disregarding noise and external input, all other neurons in the network receive the same total input from these twin neurons as they used to receive from just the original neuron before its duplication. Therefore, still disregarding noise and external input, the activity of every neurons remains unchanged, and the newly-added neuron simply repeats the activity of its original counterpart. In other words, if the network was stable before adding the new neuron, it will also be stable after having added it. We refer to this process as the “dynamics-neutral” addition of a neuron to the network. This idea forms the mathematical foundation of our proposed training method.</p><p id="P8">Formally, let neuron <italic>n</italic> be the cloning candidate. The membrane potential vector <bold>u</bold>, and thus the firing rate vector <bold>r</bold> now each have an additional element: <disp-formula id="FD4"><label>(4)</label><mml:math id="M4"><mml:mtext mathvariant="bold">u</mml:mtext><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mtext mathvariant="bold">u</mml:mtext><mml:mrow><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo><mml:mo>→</mml:mo><mml:mo>[</mml:mo><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mtext mathvariant="bold">u</mml:mtext><mml:mrow><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mtext>and</mml:mtext><mml:mspace width="0.2em"/><mml:mtext mathvariant="bold">r</mml:mtext><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mtext mathvariant="bold">r</mml:mtext><mml:mrow><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo><mml:mo>→</mml:mo><mml:mo>[</mml:mo><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mtext mathvariant="bold">r</mml:mtext><mml:mrow><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:math></disp-formula> where ¬<italic>n</italic> represents the indices of all neurons that are not <italic>n</italic>. Similarly, we split the weight matrix <bold>W</bold> into four blocks, {<bold>W</bold>(¬<italic>n</italic>,¬<italic>n</italic>), <bold>W</bold>(¬<italic>n</italic>,¬<italic>n</italic>), <bold>W</bold>(¬<italic>n</italic>,¬<italic>n</italic>), <bold>W</bold>(¬<italic>n</italic>,¬<italic>n</italic>)}. We update the network weights according to <disp-formula id="FD5"><label>(5)</label><mml:math id="M5"><mml:mtext mathvariant="bold">W</mml:mtext><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:mtext mathvariant="bold">W</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext mathvariant="bold">W</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtext mathvariant="bold">W</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext mathvariant="bold">W</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo><mml:mo>→</mml:mo><mml:mo>[</mml:mo><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:mtext mathvariant="bold">W</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mtext mathvariant="bold">W</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mtext mathvariant="bold">W</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtext mathvariant="bold">W</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mtext mathvariant="bold">W</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mtext mathvariant="bold">W</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtext mathvariant="bold">W</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mtext mathvariant="bold">W</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mtext mathvariant="bold">W</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:math></disp-formula></p><p id="P9">The synaptic input into all neurons (the equivalent of the third term on the RHS of <xref ref-type="disp-formula" rid="FD1">Eq. 1</xref>) is now given by <disp-formula id="FD6"><label>(6)</label><mml:math id="M6"><mml:mrow><mml:mo>[</mml:mo><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:mtext mathvariant="bold">W</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mtext mathvariant="bold">W</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mtext mathvariant="bold">W</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtext mathvariant="bold">W</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mtext mathvariant="bold">W</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mtext mathvariant="bold">W</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtext mathvariant="bold">W</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mtext mathvariant="bold">W</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mtext mathvariant="bold">W</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo><mml:mo>[</mml:mo><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mtext mathvariant="bold">r</mml:mtext><mml:mrow><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:mtext mathvariant="bold">W</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mtext mathvariant="bold">r</mml:mtext><mml:mrow><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mtext mathvariant="bold">W</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtext mathvariant="bold">W</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mtext mathvariant="bold">r</mml:mtext><mml:mrow><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mtext mathvariant="bold">W</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtext mathvariant="bold">W</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mtext mathvariant="bold">r</mml:mtext><mml:mrow><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mtext mathvariant="bold">W</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:math></disp-formula> where the top rows of this vector are <disp-formula id="FD7"><label>(7)</label><mml:math id="M7"><mml:mrow><mml:mo>[</mml:mo><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mtext mathvariant="bold">W</mml:mtext><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mtext mathvariant="bold">r</mml:mtext><mml:mrow><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mtext mathvariant="bold">W</mml:mtext><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mtext mathvariant="bold">W</mml:mtext><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mtext mathvariant="bold">r</mml:mtext><mml:mrow><mml:mtext>¬</mml:mtext><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mtext mathvariant="bold">W</mml:mtext><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo><mml:mo>=</mml:mo><mml:mtext mathvariant="bold">W</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>r</mml:mtext></mml:mrow></mml:math></disp-formula> which is the same input as that received by the previously existing neurons, before the addition of the new neuron, as claimed (cf. <xref ref-type="disp-formula" rid="FD1">Eq. 1</xref>). Note that during optimization, symmetry between the twin neurons is broken due to them receiving statistically equal but nonetheless uniquely-generated noise inputs. As our test cases are constructed to have the same number of inputs as the number of neurons in the network, we also choose not to duplicate incoming weights from external inputs for newly added neurons, but instead to connect each newly added neuron to a single new input. (In future work, we will consider all-to-all connections between external inputs and existing neurons, with trainable weights, which are duplication at network growth.) These factors wear down the absolute guarantee of stability, but it is still reasonable to build on this intuitively high chance of stability.</p></sec><sec id="S5"><label>2.3</label><title>Training method</title><p id="P10">Even with a reliable way of growing a network, we still explicitly require an existing (smaller) network as a starting point. By “backward induction”, we recognize that the simplest and most reliable solution is to initialize the whole process by generating the smallest SSN possible – a network with only one excitatory and one inhibitory neuron. This can be achieved by brute-force or by some targeted initialization specific to the training objective in order to get a head-start on performance optimization. The full network can then be gradually constructed by adding neurons in a dynamics-neutral way as previously described. The entire training method is summarized in <xref ref-type="fig" rid="F1">Fig. 1</xref>.</p></sec><sec id="S6"><label>2.4</label><title>Training reliability</title><p id="P11">We calculate three different metrics that together characterize how reliably our method protects against instability problems typically encountered in training SSNs. <list list-type="order" id="L1"><list-item><p id="P12"><bold>Proportion of unstable networks during training.</bold> We repeat the entire training method for a total of 100 separate networks and count the number of networks that become unstable over training (i.e. runaway activity for any of the training inputs).</p></list-item><list-item><p id="P13"><bold>Proportion of unstable trials with shuffled weights.</bold> For each of the 100 fully trained networks (see above), we shuffle the elements within each quadrant of their weight matrix, defined by the excitatory (E) or inhibitory (I) nature of the pre- and postsynaptic neurons (E-E, E-I, I-E and I-I). We repeat this shuffling 100 times, run each shuffled network for 100 trials, each with a different input (from either MNIST or CIFAR-10, see below), and count the number of trials in which the network becomes unstable. We report the mean and standard error of the proportion of unstable trials across the 10,000 shuffled networks.</p></list-item><list-item><p id="P14"><bold>Proportion of unstable randomly generated networks.</bold> For each of the 100 fully trained networks (see above), we generate 100 random networks, such that each element of their weight matrix is generated from a zero-mean Gaussian with variance equal to the variance of all weights in the corresponding trained stable network. We then set the sign of each weight to comply with the E/I identity of the presynaptic neuron. We run each random network for 100 trials with different inputs, as above, count the number of unstable trials, and report the mean and standard error of the proportion of unstable trials across the 10,000 random networks.</p></list-item></list></p></sec></sec><sec id="S7"><label>3</label><title>MNIST classification task</title><p id="P15">We first train SSNs on MNIST digit classification [<xref ref-type="bibr" rid="R33">33</xref>]. Due to its recurrent dynamics (<xref ref-type="disp-formula" rid="FD1">Eq. 1</xref>), neural activities in an SSN evolve over time even in response to a fixed external input, instead of instantaneously producing a fixed output. Moreover, due to the presence of noise in the dynamics, a stochastic SSN never settles onto a single response. While these features are ever-present in experimental neural recordings, and thus contribute to the biological realism of stochastic SSNs, they also make their training challenging. Therefore, to define the network’s output for the purposes of training (and, more generally, for measuring its performance), we evolve neural activities until they reach a steady-state distribution, and pass the mean membrane potentials into a readout (softmax) layer.</p><p id="P16">An overview of the training approach is shown in <xref ref-type="fig" rid="F2">Fig. 2</xref>. We train an SSN with 80 excitatory and 20 inhibitory neurons (a biologically-realistic E:I ratio). We also train an SSN with 50 excitatory and 50 inhibitory neurons to compare with the performance of previous models [<xref ref-type="bibr" rid="R29">29</xref>]. Only the excitatory neurons receive external input (i.e. MNIST images) and are decoded in the readout layer, although activities in the entire network (including inhibitory neurons) evolve over time. In order to transform the MNIST images into inputs for the SSN, we optimize an autoencoder (also comprising of 3 layers) to reduce input dimensionality from 784 (number of pixels in the MNIST images) to 50 (number of excitatory neurons in the network). As a base comparison, we also train multi-layer perceptrons (MLPs) with 3 layers, and perform logistic regression on the encoded data from the autoencoder.</p><p id="P17">Training results are summarized in <xref ref-type="table" rid="T1">Table 1</xref>. None of our 200 trained networks encountered instability problems. This is highly non-trivial: at both E:I ratios, it is virtually impossible to initialize networks randomly so that they are stable, and even shuffling the weights of trained stable networks results in instability in nearly 80% of cases. In addition, our trained SSNs at both E:I ratios perform at only slightly lower levels than the corresponding MLPs, and also better than logistic regression despite having noise injected at every time step and also having to maintain dynamic stability. SSNs constrained to have a “ring” architecture (such that each E-I quadrant of the weight matrix is circulant) with Gaussian weight profiles, as in previous work [<xref ref-type="bibr" rid="R29">29</xref>], are not able to perform the task competently because of their highly-constrained parameterization designed to exploit rotational symmetries in their training images, which do not exist in the MNIST dataset. Nevertheless, their performance is still above chance (0.1).</p></sec><sec id="S8"><label>4</label><title>Amortized probabilistic inference</title><p id="P18">We next train an SSN on a challenging probabilistic inference task. This task requires the SSN to act as the recognition model for a probabilistic generative model of natural image patches, the Gaussian scale mixture (GSM) model [<xref ref-type="bibr" rid="R34">34</xref>, <xref ref-type="bibr" rid="R36">36</xref>], by performing amortized approximate Markov chain Monte Carlo (MCMC) inference [<xref ref-type="bibr" rid="R29">29</xref>] (<xref ref-type="fig" rid="F3">Fig. 3</xref>). That is, in response to an input (observation), the network needs to produce trajectories in the state space of its neurons (encoding latent variables of the GSM) such that the distribution of multi-neuron response patterns sampled by these trajectories approximately matches the joint posterior distribution of the GSM for corresponding input (hence performing approximate MCMC inference). Notably, the network needs to achieve this with a single set of parameters (prominently, synaptic weights) for a large number of inputs (hence the inference is “amortized”). This is a challenging task because the objective requires the network to maintain finite levels of variability while modulating the co-variability of its neurons in a stimulus-dependent way, which is only possible if stochasticity of neural noise is not suppressed and recurrent connections are sufficiently strong – a combination that pushes the network towards instability. This task has also been suggested to have neurobiological relevance. Previous work indicated that the stationary statistics of neural responses in V1 may be accounted for by a model assuming these responses represent statistical samples from a GSM posterior [<xref ref-type="bibr" rid="R36">36</xref>, <xref ref-type="bibr" rid="R37">37</xref>], and that SSNs producing such samples also account for other dynamical aspects of V1 responses such as oscillations and transient overshoots [<xref ref-type="bibr" rid="R29">29</xref>].</p><sec id="S9"><label>4.1</label><title>Gaussian scale mixture model</title><p id="P19">The GSM [<xref ref-type="bibr" rid="R34">34</xref>, <xref ref-type="bibr" rid="R36">36</xref>] was proposed as a generative model of natural image patches suitable for image processing, and as an internal model that the visual system (in particular, the primary visual cortex, V1) may implicitly use to process images. According to the GSM, an image <bold>x</bold> is generated as a linear combination of so-called “projective fields” (columns of a matrix, <bold>A</bold>, see below), with overall scaling (“global contrast”) determined by a scalar, <italic>z</italic>, and with some additive zero-mean, Gaussian white noise pixel noise, <inline-formula><mml:math id="M8"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>η</mml:mi></mml:mstyle><mml:mtext>x</mml:mtext></mml:msub><mml:mo>∼</mml:mo><mml:mi>𝒩</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mtext>x</mml:mtext><mml:mn>2</mml:mn></mml:msubsup><mml:mtext mathvariant="bold">I</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> (where <bold>I</bold> is the identity matrix): <disp-formula id="FD8"><label>(8)</label><mml:math id="M9"><mml:mrow><mml:mtext>x</mml:mtext><mml:mo>=</mml:mo><mml:mi>z</mml:mi><mml:mspace width="0.2em"/><mml:mtext mathvariant="bold">A</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>y</mml:mtext><mml:mo>+</mml:mo><mml:msub><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>η</mml:mi></mml:mstyle><mml:mtext>x</mml:mtext></mml:msub></mml:mrow></mml:math></disp-formula> where <bold>y</bold> are latent variables scaling the individual contribution of each projective field in <bold>A</bold>. The projective fields are 2D Gabor filters, each characterized by four parameters determining the orientation, size, and 2D location of the filter (see also <xref ref-type="supplementary-material" rid="SD1">Appendix A</xref> in the Supplementary material) such that each column of <bold>A</bold> corresponds to the pixels of a single vectorized 2D Gabor filter. An ideal observer, when presented with an image <bold>x</bold>, computes a posterior distribution over <bold>y</bold> and <italic>z</italic> by incorporating their respective priors: <disp-formula id="FD9"><label>(9)</label><mml:math id="M10"><mml:mrow><mml:mtext>y</mml:mtext><mml:mo>∼</mml:mo><mml:mi>𝒩</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>y</mml:mtext><mml:mo>;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mtext>C</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mtext>and</mml:mtext><mml:mspace width="0.2em"/><mml:mi>z</mml:mi><mml:mo>∼</mml:mo><mml:mtext>Gamma</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula> so that (by Bayes’ rule) <disp-formula id="FD10"><label>(10)</label><mml:math id="M11"><mml:mrow><mml:mtext>P</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold">y</mml:mtext><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:mi>𝒩</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo>;</mml:mo><mml:mi>z</mml:mi><mml:mtext mathvariant="bold">A</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>y</mml:mtext><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mtext>x</mml:mtext><mml:mn>2</mml:mn></mml:msubsup><mml:mtext mathvariant="bold">I</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mi>𝒩</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold">y</mml:mtext><mml:mo>;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mtext mathvariant="bold">C</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mtext>Gamma</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo>;</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula> after which the contrast level <italic>z</italic> is marginalized, resulting in a posterior distribution over <bold>y</bold>: <disp-formula id="FD11"><label>(11)</label><mml:math id="M12"><mml:mrow><mml:mtext>P</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold">y</mml:mtext><mml:mo stretchy="false">|</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:mtext>P</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold">y</mml:mtext><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mtext>d</mml:mtext><mml:mi>z</mml:mi></mml:mrow></mml:math></disp-formula></p><p id="P20">We construct our GSM by first maximizing the fraction of variance explained by the projective fields on CIFAR-10 images and then optimizing its remaining parameters by maximizing model (marginal) likelihood on the same images. Additional details can be found in <xref ref-type="supplementary-material" rid="SD1">Appendix A</xref>.</p></sec><sec id="S10"><label>4.2</label><title>Sampling-based probabilistic inference</title><p id="P21">Given an observed image as input, the membrane potential responses of the SSN, <bold>u</bold>, as determined by <xref ref-type="disp-formula" rid="FD1">Eq. 1</xref>, are required to (approximately) represent statistical samples from the posterior distribution over the latent variables of the GSM as computed by an ideal observer using <xref ref-type="disp-formula" rid="FD11">Eq. 11</xref> (assuming a one-to-one correspondence between the excitatory neurons of the SSN and the latent variables of the GSM). This inference method is therefore described as “sampling-based”. We achieve this by optimizing the SSN parameters with respect to the mean-squared errors (averaged over 50,000 training images) between the mean and (co)variance of its response distributions, <inline-formula><mml:math id="M13"><mml:mrow><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mtext>SSN</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mtext mathvariant="bold">u</mml:mtext><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="double-struck">V</mml:mi><mml:mrow><mml:mtext>SSN</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mtext mathvariant="bold">u</mml:mtext><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M14"><mml:mrow><mml:msub><mml:mi>ℂ</mml:mi><mml:mrow><mml:mtext>SSN</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mtext mathvariant="bold">u</mml:mtext><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> and of the corresponding target posterior distributions, <italic><bold>μ</bold></italic><sub>GSM</sub>, <inline-formula><mml:math id="M15"><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>σ</mml:mi></mml:mstyle><mml:mrow><mml:mtext>GSM</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> and <bold>Σ</bold><sub>GSM</sub>: <disp-formula id="FD12"><label>(12)</label><mml:math id="M16"><mml:mrow><mml:mi>ℒ</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mo>∥</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mtext>SSN</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mtext mathvariant="bold">u</mml:mtext><mml:mo stretchy="false">]</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle><mml:mrow><mml:mtext>GSM</mml:mtext></mml:mrow></mml:msub><mml:msubsup><mml:mo>∥</mml:mo><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msub><mml:mo>∥</mml:mo><mml:msub><mml:mi mathvariant="double-struck">V</mml:mi><mml:mrow><mml:mtext>SSN</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mtext mathvariant="bold">u</mml:mtext><mml:mo stretchy="false">]</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>σ</mml:mi></mml:mstyle><mml:mrow><mml:mtext>GSM</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:msubsup><mml:mo>∥</mml:mo><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mtext>Σ</mml:mtext></mml:msub><mml:mo>∥</mml:mo><mml:msub><mml:mi>ℂ</mml:mi><mml:mrow><mml:mtext>SSN</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mtext mathvariant="bold">u</mml:mtext><mml:mo stretchy="false">]</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>Σ</mml:mi></mml:mstyle><mml:mrow><mml:mtext>GSM</mml:mtext></mml:mrow></mml:msub><mml:msubsup><mml:mo>∥</mml:mo><mml:mtext>F</mml:mtext><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></disp-formula> for constant coefficients <italic>λ<sub>μ</sub>, λ</italic><sub><italic>σ</italic><sup>2</sup></sub> and <italic>λ</italic><sub>Σ</sub> (and where ∥·∥<sub>F</sub> denotes the Frobenius norm). In this case, both excitatory and inhibitory neurons receive external input corresponding to their respective receptive fields (such that an E-I pair shares the same receptive field) but, as before, only excitatory neurons are considered in the computation of the cost function.</p></sec><sec id="S11" sec-type="results"><label>4.3</label><title>Results</title><p id="P22">We train an SSN with circulant quadrants in the weight matrix (“ring SSN”) for a GSM whose projective fields only differ in their orientation (and thus form a ring topology), used in earlier work [<xref ref-type="bibr" rid="R29">29</xref>]. We also train an unconstrained SSN (“general SSN”) for the GSM with a richer set of projective fields we described above (<xref ref-type="sec" rid="S9">Section 4.1</xref>. For both GSMs, we also train ring SSNs that are constrained to have Gaussian weight profiles (Gaussian ring), as in previous work [<xref ref-type="bibr" rid="R29">29</xref>]. As target moments are different in the two GSMs, we compute a normalized cost function so that we can compare networks trained across different GSMs: <disp-formula id="FD13"><label>(13)</label><mml:math id="M17"><mml:mrow><mml:msup><mml:mi>ℒ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>μ</mml:mi></mml:msub><mml:mfrac><mml:mrow><mml:mo>∥</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mtext>SSN</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mtext mathvariant="bold">u</mml:mtext><mml:mo stretchy="false">]</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle><mml:mrow><mml:mtext>GSM</mml:mtext></mml:mrow></mml:msub><mml:msubsup><mml:mo>∥</mml:mo><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:mo>∥</mml:mo><mml:msub><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>μ</mml:mi></mml:mstyle><mml:mrow><mml:mtext>GSM</mml:mtext></mml:mrow></mml:msub><mml:msubsup><mml:mo>∥</mml:mo><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mo>∥</mml:mo><mml:msub><mml:mi mathvariant="double-struck">V</mml:mi><mml:mrow><mml:mtext>SSN</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mtext mathvariant="bold">u</mml:mtext><mml:mo stretchy="false">]</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>σ</mml:mi></mml:mstyle><mml:mrow><mml:mtext>GSM</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:msubsup><mml:mo>∥</mml:mo><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:mo>∥</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>σ</mml:mi></mml:mstyle><mml:mrow><mml:mtext>GSM</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:msubsup><mml:mo>∥</mml:mo><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mtext>Σ</mml:mtext></mml:msub><mml:mfrac><mml:mrow><mml:mo>∥</mml:mo><mml:msub><mml:mi>ℂ</mml:mi><mml:mrow><mml:mtext>SSN</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mtext mathvariant="bold">u</mml:mtext><mml:mo stretchy="false">]</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>Σ</mml:mi></mml:mstyle><mml:mrow><mml:mtext>GSM</mml:mtext></mml:mrow></mml:msub><mml:msubsup><mml:mo>∥</mml:mo><mml:mtext>F</mml:mtext><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:mo>∥</mml:mo><mml:msub><mml:mstyle mathvariant="bold-italic" mathsize="normal"><mml:mi>Σ</mml:mi></mml:mstyle><mml:mrow><mml:mtext>GSM</mml:mtext></mml:mrow></mml:msub><mml:msubsup><mml:mo>∥</mml:mo><mml:mtext>F</mml:mtext><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
</p><p id="P23">In order to fully appreciate these results, we briefly describe the GSM-SSN pairs in each row of <xref ref-type="table" rid="T2">Table 2</xref> and how they are trained. A full comparison can be found in <xref ref-type="supplementary-material" rid="SD1">Appendix B</xref>. <list list-type="order" id="L2"><list-item><p id="P24">The first row describes the Gaussian ring-SSN from [<xref ref-type="bibr" rid="R29">29</xref>] optimized for the ring GSM from the same study. This SSN is highly constrained; for example, the entire 100 × 100 weight matrix is parameterized by only 8 parameters. The same 5 rotationally-symmetric images are used for both training and testing, due to the high computational costs associated with the method. Optimization is first done using Adam for a small number of iterations, followed by a zero-variance but biased semi-analytical mean-field method described in [<xref ref-type="bibr" rid="R38">38</xref>] (and <xref ref-type="supplementary-material" rid="SD1">Appendix B</xref>). The normalized cost of this model is set to 1 by definition.</p></list-item><list-item><p id="P25">The ring SSN in the second row is trained by our method, with the same 5 images used as the training and test set (as above). The weight matrix is constrained to be block-symmetric-circulant in order to fully exploit the rotational symmetry of the images. Each symmetric-circulant 50 × 50 block is parameterized by 26 parameters, giving a total of 104 parameters for the weight matrix. The additional degrees of freedom has resulted in nearly an order of magnitude lower cost.</p></list-item><list-item><p id="P26">The third row shows the Gaussian ring SSN from [<xref ref-type="bibr" rid="R29">29</xref>] trained for our GSM. Due to its highly-constrained parameterization (see above) and built-in rotational symmetry assumptions, the model is not able to succeed in matching the moments of the GSM posterior moments when observing CIFAR-10 natural images and fails to converge during training.</p></list-item><list-item><p id="P27">The general SSN in the fourth row has all 10,000 parameters of its 100 × 100 weight matrix individually trained on our GSM. Despite the increase in difficulty of the task (matching a more complex GSM), our method produces a network that achieves a normalized cost lower than that achieved by a Gaussian ring SSN specifically designed to work for a ring GSM [<xref ref-type="bibr" rid="R29">29</xref>].</p></list-item></list></p><p id="P28">In summary, our method provides two advantages. First, it allows training networks with a large number of free parameters, which in turn achieves a much lower cost than the more highly constrained networks to which previous training methods were restricted (first two rows). Second, our stability-focused approach successfully trains networks to perform inference under a complex GSM of natural images, a feat which was previously impossible (last two rows). Once again, in all 100 independent training attempts, not a single network became unstable. Full-sized stable networks are also impossible to obtain by random generation, further justifying our approach.</p></sec><sec id="S12"><label>4.4</label><title>Analysis of the trained network</title><p id="P29">It is interesting to analyze the trained SSN and explore its dynamics. For this, we reverse-engineer the network in order to obtain the image that it “perceives” at a given time by interpreting the mean membrane potentials of its excitatory neurons as the latent activations of the GSM, from which an image is reconstructed using the GSM’s projective fields and generative process (<xref ref-type="disp-formula" rid="FD8">Eq. 8</xref>). We observe the phenomenon of percept morphing (<xref ref-type="fig" rid="F4">Fig. 4A</xref>), driven by membrane potential moments initially matching the GSM targets corresponding to the old stimulus, and gradually evolving over time to instead match those corresponding to the new stimulus (<xref ref-type="fig" rid="F4">Fig. 4B-C</xref>). While the target means are matched with high precision and within ∼40 ms (<xref ref-type="fig" rid="F4">Fig. 4B</xref>), variances are slightly less precisely matched and take longer to converge (<xref ref-type="fig" rid="F4">Fig. 4C</xref>), demonstrating the difficulty of matching higher-order moments. Furthermore, membrane potential means transition between the two posteriors faster than membrane potential variances do. From an MCMC perspective, this suggests a difference in mixing times for the two quantities.</p></sec></sec><sec id="S13" sec-type="discussion"><label>5</label><title>Discussion</title><p id="P30">Network stability is a problem faced in both artificial [<xref ref-type="bibr" rid="R39">39</xref>, <xref ref-type="bibr" rid="R40">40</xref>] and biological neural networks [<xref ref-type="bibr" rid="R17">17</xref>]. Typically, artificial neural networks (ANNs) can implement quick-fix solutions that will not impact their performance negatively, such as applying batch and layer normalizations [<xref ref-type="bibr" rid="R41">41</xref>–<xref ref-type="bibr" rid="R43">43</xref>]. Furthermore, artificial neurons may be designed to be stable, e.g. by simply choosing a saturating activation function (tanh, sigmoid) or by implementing other bounded-input-bounded-output designs.</p><p id="P31">In general, the concept of growing networks during training has recently been proposed in ANNs. Previous work combined both network growing and pruning to produce efficient networks [<xref ref-type="bibr" rid="R31">31</xref>], allowed the splitting of an existing neuron into multiple neurons [<xref ref-type="bibr" rid="R30">30</xref>], and used principles similar to ours for adding nodes without impacting the operation of the existing network [<xref ref-type="bibr" rid="R32">32</xref>]. Broadly speaking, the two primary goals of these algorithms were to reduce training load and improve performance. Here, given its particular relevance for biological networks, we instead focused on the even more basic requirement of staying within a stable dynamical regime.</p><p id="P32">Speed and scalability are two of the most important criteria that determine the efficiency of an algorithm. In our case, building a network gradually during optimization goes against these principles because it requires computationally expensive function retracing operations every time the network grows and thus changes in architecture. We justify this with two reasons. First, there is presently no other effective way of training SSNs. Second, the computational resources spent on optimization is still much greater than those spent on tracing, especially as there is only a finite number of tracing operations required (until the network reaches full size), but a much larger number of gradient computations and optimization iterations (which may need to continue even once the network reaches full size). Better balancing network building and its associated computational costs is an important future direction.</p><p id="P33">Our method may be further improved by fine-tuning its hyperparameters, such as the number of neurons to duplicate at the same time, or the criteria for selecting which neuron(s) to duplicate. In addition, our method may also be combined with methods based on fundamentally different approaches, such as adding regularization terms to the cost function explicitly encouraging networks to stay within a stable regime [<xref ref-type="bibr" rid="R27">27</xref>].</p><p id="P34">Throughout our analysis, we use backpropagation through time with Adam [<xref ref-type="bibr" rid="R44">44</xref>] to train SSNs (see <xref ref-type="supplementary-material" rid="SD1">Appendix C</xref> for additional information on the training procedure). This is because our primary objective is to build biologically-realistic networks so as to study their dynamics and network characteristics after they have been optimized. Future work will need to study how biologically plausible plasticity mechanisms may achieve the same goal.</p><p id="P35">Finally, as an example of a biologically relevant task for V1, we chose to train SSNs to perform sampling-based inference, as a particularly challenging test case [<xref ref-type="bibr" rid="R29">29</xref>]. Importantly, our method is readily applicable to training SSNs on other cognitive tasks engaging a number of different cortical areas, including the prefrontal cortex [<xref ref-type="bibr" rid="R16">16</xref>], opening the way to studying SSNs outside the realm of primary sensory cortices.</p></sec><sec id="S14" sec-type="conclusions"><label>6</label><title>Conclusion</title><p id="P36">We present an effective method for training SSNs, a canonical model of cortical circuits highly prone to exhibit dynamic instabilities. To demonstrate the generality of our method, we train SSNs to perform MNIST image classification. We also train SSNs to perform sampling-based inference under a GSM optimized for CIFAR-10 images, a previously unattainable task. Our contribution also makes it possible to train SSNs on other neurobiologically relevant cognitive tasks and thus study the network dynamics that underlie the complex computations performed by the brain.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Material</label><media xlink:href="EMS156221-supplement-Supplementary_Material.pdf" mimetype="application" mime-subtype="pdf" id="d38aAdGbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S15"><title>Acknowledgments and Disclosure of Funding</title><p>This work was supported by the Wellcome Trust (Investigator Award in Science 212262/Z/18/Z to M.L) and the Human Frontiers Science Program (research grant RGP0044/2018 to M.L.). All experiments were performed on the Cambridge Service for Data-Driven Discovery (CSD3), an EPSRC-funded Tier-2 HPC service.</p></ack><fn-group><title>Checklist</title><fn id="FN1"><p id="P37">
<list id="L3" list-type="order"><list-item><p>For all authors… <list list-type="simple" id="L4"><list-item><label>(a)</label><p id="P38">Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? [<styled-content style="color:#0000FF">Yes</styled-content>] In our abstract and introduction, we claim that we have developed a method to train supralinear stabilized networks. This method can be found in <xref ref-type="sec" rid="S2">Section 2</xref>. We also claim that we have successfully trained the network to perform sampling-based inference on an internal generative model of natural images. These results can be found summarized in <xref ref-type="sec" rid="S8">Section 4</xref>. Finally, we claim that our method is an order of magnitude better than previous approaches. This can be found in <xref ref-type="table" rid="T2">Table 2</xref>.</p></list-item><list-item><label>(b)</label><p id="P39">Did you describe the limitations of your work? [<styled-content style="color:#0000FF">Yes</styled-content>] We have highlighted some potential limitations in <xref ref-type="sec" rid="S13">Section 5</xref>.</p></list-item><list-item><label>(c)</label><p id="P40">Did you discuss any potential negative societal impacts of your work? [<styled-content style="color:#808080">N/A</styled-content>] We have not found any potential negative societal impacts.</p></list-item><list-item><label>(d)</label><p id="P41">Have you read the ethics review guidelines and ensured that your paper conforms to them? [<styled-content style="color:#0000FF">Yes</styled-content>]</p></list-item></list></p></list-item><list-item><p id="P42">If you are including theoretical results… <list list-type="simple" id="L5"><list-item><label>(a)</label><p id="P43">Did you state the full set of assumptions of all theoretical results? [<styled-content style="color:#0000FF">Yes</styled-content>] <xref ref-type="sec" rid="S4">Section 2.2</xref></p></list-item><list-item><label>(b)</label><p id="P44">Did you include complete proofs of all theoretical results? [<styled-content style="color:#0000FF">Yes</styled-content>] <xref ref-type="sec" rid="S4">Section 2.2</xref></p></list-item></list>
</p></list-item><list-item><p id="P45">If you ran experiments… <list list-type="simple" id="L6"><list-item><label>(a)</label><p id="P46">Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the <xref ref-type="supplementary-material" rid="SD1">supplemental material</xref> or as a URL)? [<styled-content style="color:#0000FF">Yes</styled-content>] The link to our code can be found in the <xref ref-type="supplementary-material" rid="SD1">supplementary material</xref>. Additional information required to replicate our results are also included.</p></list-item><list-item><label>(b)</label><p id="P47">Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [<styled-content style="color:#0000FF">Yes</styled-content>] All parameters of the model are clearly introduced early in the text. Numerical values assigned to all parameters can be found in the <xref ref-type="supplementary-material" rid="SD1">supplementary material</xref>.</p></list-item><list-item><label>(c)</label><p id="P48">Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [<styled-content style="color:#0000FF">Yes</styled-content>] <xref ref-type="table" rid="T1">Tables 1</xref> and <xref ref-type="table" rid="T2">2</xref></p></list-item><list-item><label>(d)</label><p id="P49">Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [<styled-content style="color:#0000FF">Yes</styled-content>] Computational resources and related information can be found in the <xref ref-type="supplementary-material" rid="SD1">supplementary material</xref>.</p></list-item></list></p></list-item><list-item><p id="P50">If you are using existing assets (e.g., code, data, models) or curating/releasing new assets… <list list-type="simple" id="L7"><list-item><label>(a)</label><p id="P51">If your work uses existing assets, did you cite the creators? [<styled-content style="color:#0000FF">Yes</styled-content>] We have used CIFAR-10 and MNIST and have cited the creators.</p></list-item><list-item><label>(b)</label><p id="P52">Did you mention the license of the assets? [<styled-content style="color:#0000FF">Yes</styled-content>] We have linked the licenses in the <xref ref-type="supplementary-material" rid="SD1">supplementary material</xref>.</p></list-item><list-item><label>(c)</label><p id="P53">Did you include any new assets either in the <xref ref-type="supplementary-material" rid="SD1">supplemental material</xref> or as a URL? [<styled-content style="color:#FF8000">No</styled-content>]</p></list-item><list-item><label>(d)</label><p id="P54">Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [<styled-content style="color:#808080">N/A</styled-content>] CIFAR-10 and MNIST are publicly available datasets.</p></list-item><list-item><label>(e)</label><p id="P55">Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [<styled-content style="color:#0000FF">Yes</styled-content>] This can be found in the <xref ref-type="supplementary-material" rid="SD1">supplementary</xref>.</p></list-item></list>
</p></list-item><list-item><p id="P56">If you used crowdsourcing or conducted research with human subjects… <list list-type="simple" id="L8"><list-item><label>(a)</label><p id="P57">Did you include the full text of instructions given to participants and screenshots, if applicable? [<styled-content style="color:#808080">N/A</styled-content>]</p></list-item><list-item><label>(b)</label><p id="P58">Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [<styled-content style="color:#808080">N/A</styled-content>]</p></list-item><list-item><label>(c)</label><p id="P59">Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [<styled-content style="color:#808080">N/A</styled-content>]</p></list-item></list>
</p></list-item></list></p></fn></fn-group><ref-list><ref id="R1"><label>[1]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCulloch</surname><given-names>WS</given-names></name><name><surname>Pitts</surname><given-names>W</given-names></name></person-group><article-title>A logical calculus of the ideas immanent in nervous activity</article-title><source>The Bulletin of Mathematical Biophysics</source><year>1943</year><volume>5</volume><fpage>115</fpage><lpage>133</lpage></element-citation></ref><ref id="R2"><label>[2]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hopfield</surname><given-names>JJ</given-names></name></person-group><source>Neural networks and physical systems with emergent collective computational abilities</source><conf-name>Proceedings of the National Academy of Sciences</conf-name><year>1982</year><volume>79</volume><fpage>2554</fpage><lpage>2558</lpage></element-citation></ref><ref id="R3"><label>[3]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Vreeswijk</surname><given-names>C</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><article-title>Chaos in neuronal networks with balanced excitatory and inhibitory activity</article-title><source>Science</source><year>1996</year><volume>274</volume><fpage>1724</fpage><lpage>1726</lpage></element-citation></ref><ref id="R4"><label>[4]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubin</surname><given-names>DB</given-names></name><name><surname>Hooser</surname><given-names>SDV</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name></person-group><article-title>The stabilized supralinear network: A unifying circuit motif underlying multi-input integration in sensory cortex</article-title><source>Neuron</source><year>2015</year><volume>85</volume><fpage>402</fpage><lpage>417</lpage></element-citation></ref><ref id="R5"><label>[5]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><article-title>Imagenet classification with deep convolutional neural networks</article-title><source>Communications of the ACM</source><year>2017</year><volume>60</volume><fpage>84</fpage><lpage>90</lpage></element-citation></ref><ref id="R6"><label>[6]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Goodfellow</surname><given-names>I</given-names></name><etal/></person-group><source>Generative adversarial nets</source><conf-name>Advances in Neural Information Processing Systems</conf-name><year>2014</year><volume>27</volume></element-citation></ref><ref id="R7"><label>[7]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Vaswani</surname><given-names>A</given-names></name><etal/></person-group><source>Attention is all you need</source><conf-name>Advances in Neural Information Processing Systems</conf-name><year>2017</year><volume>30</volume></element-citation></ref><ref id="R8"><label>[8]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ramesh</surname><given-names>A</given-names></name><etal/></person-group><source>Zero-shot text-to-image generation</source><conf-name>In Proceedings of the 38th International Conference on Machine Learning, vol. 139 of Proceedings of Machine Learning Research</conf-name><year>2021</year><fpage>8821</fpage><lpage>8831</lpage></element-citation></ref><ref id="R9"><label>[9]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mante</surname><given-names>V</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><article-title>Context-dependent computation by recurrent dynamics in prefrontal cortex</article-title><source>Nature</source><year>2013</year><volume>503</volume><fpage>78</fpage><lpage>84</lpage></element-citation></ref><ref id="R10"><label>[10]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DLK</given-names></name><etal/></person-group><source>Performance-optimized hierarchical models predict neural responses in higher visual cortex</source><conf-name>Proceedings of the National Academy of Sciences</conf-name><year>2014</year><volume>111</volume><fpage>8619</fpage><lpage>24</lpage></element-citation></ref><ref id="R11"><label>[11]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>HF</given-names></name><name><surname>Yang</surname><given-names>GR</given-names></name><name><surname>Wang</surname><given-names>X-J</given-names></name></person-group><article-title>Training excitatory-inhibitory recurrent neural networks for cognitive tasks: A simple and flexible framework</article-title><source>PLOS Computational Biology</source><year>2016</year><volume>12</volume><elocation-id>e1004792</elocation-id></element-citation></ref><ref id="R12"><label>[12]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orhan</surname><given-names>AE</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name></person-group><article-title>Efficient probabilistic inference in generic neural networks trained with non-probabilistic feedback</article-title><source>Nature Communications</source><year>2017</year><volume>8</volume></element-citation></ref><ref id="R13"><label>[13]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Banino</surname><given-names>A</given-names></name><etal/></person-group><article-title>Vector-based navigation using grid-like representations in artificial agents</article-title><source>Nature</source><year>2018</year><volume>557</volume><fpage>429</fpage><lpage>433</lpage></element-citation></ref><ref id="R14"><label>[14]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cueva</surname><given-names>CJ</given-names></name><name><surname>Wei</surname><given-names>X</given-names></name></person-group><source>Emergence of grid-like representations by training recurrent neural networks to perform spatial localization</source><conf-name>In International Conference on Learning Representations</conf-name><year>2018</year></element-citation></ref><ref id="R15"><label>[15]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Remington</surname><given-names>E</given-names></name><name><surname>Narain</surname><given-names>D</given-names></name><name><surname>Hosseini</surname><given-names>E</given-names></name><name><surname>Jazayeri</surname><given-names>M</given-names></name></person-group><article-title>Flexible sensorimotor computations through rapid reconfiguration of cortical dynamics</article-title><source>Neuron</source><year>2018</year><volume>98</volume><fpage>1005</fpage><lpage>1019</lpage></element-citation></ref><ref id="R16"><label>[16]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>GR</given-names></name><name><surname>Joglekar</surname><given-names>MR</given-names></name><name><surname>Song</surname><given-names>HF</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Wang</surname><given-names>X-J</given-names></name></person-group><article-title>Task representations in neural networks trained to perform many cognitive tasks</article-title><source>Nature Neuroscience</source><year>2019</year><volume>22</volume><fpage>297</fpage><lpage>306</lpage></element-citation></ref><ref id="R17"><label>[17]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahmadian</surname><given-names>Y</given-names></name><name><surname>Rubin</surname><given-names>DB</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name></person-group><article-title>Analysis of the stabilized supralinear network</article-title><source>Neural Computation</source><year>2013</year><volume>25</volume><fpage>1994</fpage><lpage>2037</lpage></element-citation></ref><ref id="R18"><label>[18]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>HR</given-names></name><name><surname>Cowan</surname><given-names>JD</given-names></name></person-group><article-title>Excitatory and inhibitory interactions in localized populations of model neurons</article-title><source>Biophys J</source><year>1972</year><volume>12</volume><fpage>1</fpage><lpage>24</lpage></element-citation></ref><ref id="R19"><label>[19]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Z</given-names></name><name><surname>Hopfield</surname><given-names>JJ</given-names></name></person-group><article-title>Modeling the olfactory bulb and its neural oscillatory processings</article-title><source>Biol Cybern</source><year>1989</year><volume>61</volume><fpage>379</fpage><lpage>392</lpage></element-citation></ref><ref id="R20"><label>[20]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Adini</surname><given-names>Y</given-names></name><name><surname>Sagi</surname><given-names>D</given-names></name><name><surname>Tsodyks</surname><given-names>M</given-names></name></person-group><source>Excitatory–inhibitory network in the visual cortex: Psychophysical evidence</source><conf-name>Proceedings of the National Academy of Sciences</conf-name><year>1997</year><volume>94</volume><fpage>10426</fpage><lpage>10431</lpage></element-citation></ref><ref id="R21"><label>[21]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litvak</surname><given-names>V</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name><name><surname>Segev</surname><given-names>I</given-names></name><name><surname>Abeles</surname><given-names>M</given-names></name></person-group><article-title>On the transmission of rate code in long feedforward networks with excitatory–inhibitory balance</article-title><source>Journal of neuroscience</source><year>2003</year><volume>23</volume><fpage>3006</fpage><lpage>3015</lpage></element-citation></ref><ref id="R22"><label>[22]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahmadian</surname><given-names>Y</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name></person-group><article-title>What is the dynamical regime of cerebral cortex?</article-title><source>Neuron</source><year>2021</year><volume>109</volume><fpage>3373</fpage><lpage>3391</lpage></element-citation></ref><ref id="R23"><label>[23]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Obeid</surname><given-names>D</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name></person-group><article-title>Stabilized supralinear network: Model of layer 2/3 of the primary visual cortex</article-title><source>bioRxiv</source><year>2021</year><elocation-id>2020.12.30.424892</elocation-id><pub-id pub-id-type="doi">10.1101/2020.12.30.424892</pub-id></element-citation></ref><ref id="R24"><label>[24]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kraynyukova</surname><given-names>N</given-names></name><name><surname>Tchumatchenko</surname><given-names>T</given-names></name></person-group><source>Stabilized supralinear network can give rise to bistable,oscillatory, and persistent activity</source><conf-name>Proceedings of the National Academy of Sciences</conf-name><year>2018</year><volume>115</volume><fpage>3464</fpage><lpage>3469</lpage></element-citation></ref><ref id="R25"><label>[25]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hennequin</surname><given-names>G</given-names></name><name><surname>Ahmadian</surname><given-names>Y</given-names></name><name><surname>Rubin</surname><given-names>DB</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name></person-group><article-title>The dynamical regime of sensory cortex: Stable dynamics around a single stimulus-tuned attractor account for patterns of noise variability</article-title><source>Neuron</source><year>2018</year><volume>98</volume><fpage>846</fpage><lpage>860</lpage><elocation-id>e5</elocation-id></element-citation></ref><ref id="R26"><label>[26]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Priebe</surname><given-names>NJ</given-names></name><name><surname>Ferster</surname><given-names>D</given-names></name></person-group><article-title>Inhibition, spike threshold, and stimulus selectivity in primary visual cortex</article-title><source>Neuron</source><year>2008</year><volume>57</volume><fpage>482</fpage><lpage>97</lpage></element-citation></ref><ref id="R27"><label>[27]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Festa</surname><given-names>D</given-names></name><name><surname>Hennequin</surname><given-names>G</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name></person-group><article-title>Analog memories in a balanced rate-based network of E-I neurons</article-title><source>Advances in Neural Information Processing Systems</source><year>2014</year><volume>27</volume><fpage>2231</fpage><lpage>2239</lpage></element-citation></ref><ref id="R28"><label>[28]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eckmann</surname><given-names>S</given-names></name><name><surname>Gjorgjieva</surname><given-names>J</given-names></name></person-group><article-title>Synapse-type-specific competitive Hebbian learning forms functional recurrent networks</article-title><source>bioRxiv</source><year>2022</year><elocation-id>2022.03.11.483899</elocation-id><pub-id pub-id-type="doi">10.1101/2022.03.11.483899</pub-id></element-citation></ref><ref id="R29"><label>[29]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Echeveste</surname><given-names>R</given-names></name><name><surname>Aitchison</surname><given-names>L</given-names></name><name><surname>Hennequin</surname><given-names>G</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name></person-group><article-title>Cortical-like dynamics in recurrent circuits optimized for sampling-based probabilistic inference</article-title><source>Nature Neuroscience</source><year>2020</year><volume>23</volume><fpage>1138</fpage><lpage>1149</lpage></element-citation></ref><ref id="R30"><label>[30]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>L</given-names></name><name><surname>Wang</surname><given-names>D</given-names></name><name><surname>Liu</surname><given-names>Q</given-names></name></person-group><article-title>Splitting steepest descent for growing neural architectures</article-title><source>Advances in Neural Information Processing Systems</source><year>2019</year><volume>32</volume></element-citation></ref><ref id="R31"><label>[31]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Yuan</surname><given-names>X</given-names></name><name><surname>Savarese</surname><given-names>PHP</given-names></name><name><surname>Maire</surname><given-names>M</given-names></name></person-group><source>Growing efficient deep networks by structured continuous sparsification</source><conf-name>International Conference on Learning Representations</conf-name><year>2021</year></element-citation></ref><ref id="R32"><label>[32]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Evci</surname><given-names>U</given-names></name><name><surname>van Merrienboer</surname><given-names>B</given-names></name><name><surname>Unterthiner</surname><given-names>T</given-names></name><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Vladymyrov</surname><given-names>M</given-names></name></person-group><source>Gradmax: Growing neural networks using gradient information</source><conf-name>International Conference on Learning Representations</conf-name><year>2022</year></element-citation></ref><ref id="R33"><label>[33]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>L</given-names></name></person-group><article-title>The MNIST database of handwritten digit images for machine learning research</article-title><source>IEEE Signal Processing Magazine</source><year>2012</year><volume>29</volume><fpage>141</fpage><lpage>142</lpage></element-citation></ref><ref id="R34"><label>[34]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wainwright</surname><given-names>MJ</given-names></name><name><surname>Simoncelli</surname><given-names>E</given-names></name></person-group><article-title>Scale mixtures of gaussians and the statistics of natural images</article-title><source>Advances in Neural Information Processing Systems</source><year>1999</year><volume>12</volume></element-citation></ref><ref id="R35"><label>[35]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><source>Learning multiple layers of features from tiny images. Tech Rep 0</source><year>2009</year><publisher-name>University of Toronto</publisher-name><publisher-loc>Toronto, Ontario</publisher-loc></element-citation></ref><ref id="R36"><label>[36]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orbán</surname><given-names>G</given-names></name><name><surname>Berkes</surname><given-names>P</given-names></name><name><surname>Fiser</surname><given-names>J</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name></person-group><article-title>Neural variability and sampling-based probabilistic representations in the visual cortex</article-title><source>Neuron</source><year>2016</year><volume>92</volume><fpage>530</fpage><lpage>543</lpage></element-citation></ref><ref id="R37"><label>[37]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Festa</surname><given-names>D</given-names></name><name><surname>Aschner</surname><given-names>A</given-names></name><name><surname>Davila</surname><given-names>A</given-names></name><name><surname>Kohn</surname><given-names>A</given-names></name><name><surname>Coen-Cagli</surname><given-names>R</given-names></name></person-group><article-title>Neuronal variability reflects probabilistic inference tuned to natural image statistics</article-title><source>Nature Communications</source><year>2021</year><volume>12</volume></element-citation></ref><ref id="R38"><label>[38]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hennequin</surname><given-names>G</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name></person-group><article-title>Characterizing variability in nonlinear recurrent neuronal networks</article-title><source>arXiv</source><year>2016</year><elocation-id>10.48550/1610.03110</elocation-id></element-citation></ref><ref id="R39"><label>[39]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bai</surname><given-names>S</given-names></name><name><surname>Kolter</surname><given-names>JZ</given-names></name><name><surname>Koltun</surname><given-names>V</given-names></name></person-group><article-title>Deep equilibrium models</article-title><source>Advances in Neural Information Processing Systems</source><year>2019</year><volume>32</volume><fpage>688</fpage><lpage>699</lpage></element-citation></ref><ref id="R40"><label>[40]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bai</surname><given-names>S</given-names></name><name><surname>Koltun</surname><given-names>V</given-names></name><name><surname>Kolter</surname><given-names>JZ</given-names></name></person-group><source>Stabilizing equilibrium models by jacobian regularization</source><conf-name>Proceedings of the 38th International Conference on Machine Learning</conf-name><year>2021</year><volume>139</volume><fpage>554</fpage><lpage>565</lpage></element-citation></ref><ref id="R41"><label>[41]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ioffe</surname><given-names>S</given-names></name><name><surname>Szegedy</surname><given-names>C</given-names></name></person-group><source>Batch normalization: Accelerating deep network training by reducing internal covariate shift</source><conf-name>Proceedings of the 32nd International Conference on Machine Learning</conf-name><year>2015</year><fpage>448</fpage><lpage>456</lpage></element-citation></ref><ref id="R42"><label>[42]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cooijmans</surname><given-names>T</given-names></name><name><surname>Ballas</surname><given-names>N</given-names></name><name><surname>Laurent</surname><given-names>C</given-names></name><name><surname>Gülçehre</surname><given-names>C</given-names></name><name><surname>Courville</surname><given-names>A</given-names></name></person-group><article-title>Recurrent batch normalization</article-title><source>arXiv</source><year>2016</year><pub-id pub-id-type="doi">10.48550/1603.09025</pub-id></element-citation></ref><ref id="R43"><label>[43]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Santurkar</surname><given-names>S</given-names></name><name><surname>Tsipras</surname><given-names>D</given-names></name><name><surname>Ilyas</surname><given-names>A</given-names></name><name><surname>Madry</surname><given-names>A</given-names></name></person-group><source>How does batch normalization help optimization?</source><conf-name>Advances in Neural Information Processing Systems</conf-name><year>2018</year><volume>31</volume></element-citation></ref><ref id="R44"><label>[44]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><article-title>Adam: A method for stochastic optimization</article-title><source>arXiv</source><year>2014</year><pub-id pub-id-type="doi">10.48550/1603.09025</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Method sketch for training SSNs by dynamics-neutral growth.</title><p>As randomly initializing large SSNs yields unstable or trivially stable (silent) networks with high probability [<xref ref-type="bibr" rid="R29">29</xref>], we start with a small network of two neurons, comprising of a single excitatory and inhibitory neuron, by generating network parameters until a set of parameters corresponding to a stable network is found (initialization phase). We subsequently add neurons in a dynamics-neutral manner while concurrently optimizing for the training objective until the desired network size is achieved (growth phase).</p></caption><graphic xlink:href="EMS156221-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Training an SSN to perform the MNIST classification task.</title><p>Right: the network consists of 40 excitatory and 10 inhibitory neurons (not shown). Left: before training the SSN, we optimize a 3-layer autoencoder (green) to reduce the dimensionality of each MNIST image to match the number of excitatory neurons in the network, receiving the encoded image as input after transformation by the input function, <italic>f</italic>. For each input, SSN activities, <italic>y<sub>i</sub></italic>, evolve over time until reaching their stationary distribution, after which the mean membrane potentials are passed through a feedforward readout layer to make a prediction (red). Trained parameters are highlighted on the two sides of the figure.</p></caption><graphic xlink:href="EMS156221-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Training an SSN to perform inference under a GSM.</title><p>The GSM (left) is a generative model of natural image patches proposed to underlie computations in the visual cortex. According to it, an image is constructed as a linear combination of (oriented Gabor filter-based) projective fields, each contributing according to its corresponding latent “activation”, <italic>y<sub>i</sub></italic>, scaled by a global scalar contrast level, <italic>z</italic>, and corrupted by white zero-mean Gaussian pixel noise, <italic>η<sub>x</sub></italic>. We train an SSN to perform approximate amortized MCMC inference under the GSM (right). Input the the SSN is an image patch linearly filtered by oriented receptive fields (taken to be identical to the projective fields of the GSM), elementwise transformed by a nonlinear “input function”, <italic>f</italic>. Excitatory neurons in the SSN are taken to correspond to latent variables of the GSM, such that their responses, <italic>u<sub>i</sub></italic>, given some input image, represents statistical samples from the posterior over the latent variables of the GSM given the same observed image. Trained parameters are highlighted on the two sides of the figure.</p></caption><graphic xlink:href="EMS156221-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Response statistics in the trained network rapidly adapt after a stimulus change.</title><p>The network is initially in steady state, with responses producing (approximate) samples from the posterior corresponding to some stimulus (red) as input. At <italic>t</italic> = 0 ms, a new stimulus (blue) replaces the old one as the input to the network. As a result, network responses evolve over time to represent the new posterior distribution. Each column represents the state of the network at some time <italic>t</italic> after new stimulus onset (top). <bold>A.</bold> Input image into the network (top left) and current image “perceived” by the network, as reconstructed from the neural responses (bottom right). <bold>B-C.</bold> Membrane potential means (<bold>B</bold>) and variances (<bold>C</bold>) of excitatory neurons (dots) versus target GSM posterior means corresponding the old (blue) and new (red) stimuli, respectively.</p></caption><graphic xlink:href="EMS156221-f004"/></fig><table-wrap id="T1" position="float" orientation="portrait"><label>Table 1</label><caption><title>Stability and performance of SSNs trained on MNIST classification.</title><p>Numbers separated by colons indicate the number of excitatory and inhibitory neurons in the SSN, respectively. The encoded data are labeled according to their reduced dimensionality (see <xref ref-type="fig" rid="F2">Fig. 2</xref>. For comparison, we separately train MLPs (with 3 layers) and perform logistic regression to classify the encoded data. Despite the presence of noise, our trained SSN performs only slightly worse than the MLP and better than logistic regression.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" colspan="3">stability</th></tr></thead><tbody><tr style="border-bottom: solid thin"><td align="left" valign="middle">SSN E:I ratio</td><td align="center" valign="middle">networks</td><td align="center" valign="middle">proportion unstable</td></tr><tr><td align="left" valign="middle" rowspan="3" style="border-bottom: solid thin">50:50</td><td align="center" valign="middle">trained networks</td><td align="center" valign="middle">0.0</td></tr><tr><td align="center" valign="middle">networks w/ shuffled weights</td><td align="center" valign="middle">0.790 (± 0.007)</td></tr><tr style="border-bottom: solid thin"><td align="center" valign="middle">random networks</td><td align="center" valign="middle">1.0 (± 0.0)</td></tr><tr><td align="left" valign="middle" rowspan="3" style="border-bottom: solid thin">80:20</td><td align="center" valign="middle">trained networks</td><td align="center" valign="middle">0.0</td></tr><tr><td align="center" valign="middle">networks w/ shuffled weights</td><td align="center" valign="middle">0.794 (± 0.008)</td></tr><tr style="border-bottom: solid thin"><td align="center" valign="middle">random networks</td><td align="center" valign="middle">1.0 (± 0.0)</td></tr><tr style="border-bottom: solid thin"><td align="center" valign="middle" colspan="3"><bold>performance</bold></td></tr><tr style="border-bottom: solid thin"><td align="left" valign="middle">model</td><td align="center" valign="middle">MNIST data</td><td align="center" valign="middle">test accuracy</td></tr><tr><td align="left" valign="middle">Logit</td><td align="center" valign="middle">encoded-50</td><td align="center" valign="middle">0.914</td></tr><tr><td align="left" valign="middle">MLP</td><td align="center" valign="middle">encoded-50</td><td align="center" valign="middle">0.974</td></tr><tr><td align="left" valign="middle">Gaussian ring SSN [<xref ref-type="bibr" rid="R29">29</xref>] (50:50)</td><td align="center" valign="middle">encoded-50</td><td align="center" valign="middle">0.223</td></tr><tr style="border-bottom: solid thin"><td align="left" valign="middle">SSN (50:50)</td><td align="center" valign="middle">encoded-50</td><td align="center" valign="middle"><bold>0.949</bold></td></tr><tr><td align="left" valign="middle">Logit</td><td align="center" valign="middle">encoded-80</td><td align="center" valign="middle">0.922</td></tr><tr><td align="left" valign="middle">MLP</td><td align="center" valign="middle">encoded-80</td><td align="center" valign="middle">0.976</td></tr><tr><td align="left" valign="middle">SSN (80:20)</td><td align="center" valign="middle">encoded-80</td><td align="center" valign="middle"><bold>0.952</bold></td></tr></tbody></table></table-wrap><table-wrap id="T2" position="float" orientation="portrait"><label>Table 2</label><caption><title>Stability and performance of SSNs trained on probabilistic inference under the GSM.</title><p>SSNs have been previously been trained for the same task [<xref ref-type="bibr" rid="R29">29</xref>], which serves as a comparison. We train two SSNs, one for the ring GSM in [<xref ref-type="bibr" rid="R29">29</xref>] (labeled as ring SSN), and another for our GSM which consists of a diverse set of basis functions optimized from CIFAR-10 images (general SSN). Since training targets are different for the two GSMs, we compute a modified cost (<xref ref-type="disp-formula" rid="FD13">Eq. 13</xref>) in order to directly compare optimization performance.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" colspan="3">stability</th></tr></thead><tbody><tr style="border-bottom: solid thin"><td align="left" valign="middle">model</td><td align="center" valign="middle">networks</td><td align="center" valign="middle">proportion unstable</td></tr><tr><td align="left" valign="middle" rowspan="3" style="border-bottom: solid thin">general SSN</td><td align="center" valign="middle">trained networks</td><td align="center" valign="middle">0.0</td></tr><tr><td align="center" valign="middle">networks w/ shuffled weights</td><td align="center" valign="middle">0.446 (± 0.008)</td></tr><tr style="border-bottom: solid thin"><td align="center" valign="middle">random networks</td><td align="center" valign="middle">1.0 (± 0.0)</td></tr><tr style="border-bottom: solid thin"><td align="center" valign="middle" colspan="3"><bold>performance</bold></td></tr><tr style="border-bottom: solid thin"><td align="left" valign="middle">network</td><td align="center" valign="middle">GSM target</td><td align="center" valign="middle">normalized cost</td></tr><tr><td align="left" valign="middle">Gaussian ring SSN [<xref ref-type="bibr" rid="R29">29</xref>]</td><td align="center" valign="middle">ring [<xref ref-type="bibr" rid="R29">29</xref>]</td><td align="center" valign="middle">1.0</td></tr><tr><td align="left" valign="middle">ring SSN</td><td align="center" valign="middle">ring [<xref ref-type="bibr" rid="R29">29</xref>]</td><td align="center" valign="middle"><bold>0.158</bold></td></tr><tr><td align="left" valign="middle">Gaussian ring SSN [<xref ref-type="bibr" rid="R29">29</xref>]</td><td align="center" valign="middle">optimized for CIFAR-10</td><td align="center" valign="middle">∼ 10 <sup>4</sup></td></tr><tr><td align="left" valign="middle">general SSN</td><td align="center" valign="middle">optimized for CIFAR-10</td><td align="center" valign="middle"><bold>0.745</bold></td></tr></tbody></table></table-wrap></floats-group></article>