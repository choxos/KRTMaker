<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS156325</article-id><article-id pub-id-type="doi">10.1101/2022.10.26.513822</article-id><article-id pub-id-type="archive">PPR563683</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>T-Rex: sTandalone Recorder of EXperiments; An easy and versatile neural recording platform</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Amigó-Vega</surname><given-names>Joaquín</given-names></name><email>joaquin.amigo@gssi.it</email><aff id="A1">Computer Science Department, Gran Sasso Science Institute, L’Aquila, Italy</aff></contrib></contrib-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Ottenhoff</surname><given-names>Maarten C.</given-names></name><email>m.ottenhoff@maastrichtuniversity.nl</email></contrib><contrib contrib-type="author"><name><surname>Verwoert</surname><given-names>Maxime</given-names></name></contrib><contrib contrib-type="author"><name><surname>Kubben</surname><given-names>Pieter</given-names></name></contrib><contrib contrib-type="author"><name><surname>Herff</surname><given-names>Christian</given-names></name></contrib><aff id="A5">Neurosurgery, School for Mental Health and Neuroscience, Maastricht University, Maastricht, Netherlands</aff></contrib-group><pub-date pub-type="nihms-submitted"><day>29</day><month>10</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>28</day><month>10</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Recording time in invasive neuroscientific empirical research is short and must be used as efficiently as possible. Time is often lost due to long setup times and errors by the researcher. Minimizing the number of manual actions reduces both and can be achieved by automating as much as possible. Importantly, automation should not reduce the flexibility of the system. Currently, recording setups are either custom-made by the researchers or provided as a module in comprehensive neuroscientific toolboxes, and no platforms exist focused explicitly on recording. Therefore, we developed a lightweight, flexible, platform- and measurement-independent recording system that can start and record experiments with a single press of a button. Data synchronization and recording are based on Lab Streaming Layer to ensure that all major programming languages and toolboxes can be used to develop and execute experiments. We have minimized the user restrictions as much as possible and imposed only two requirements on the experiment: The experiment should include a Lab Streaming Layer stream, and it should be able to run from a command line call. Further, we provided an easy-to-use interface that can be adjusted to specific measurement modalities, amplifiers, and participants. The presented system provides a new way of setting up and recording experiments for researchers and participants. Because of the automation and easy-to-use interface, the participant could even start and stop experiments by themselves, thus potentially providing data without the experimenter’s presence.</p></abstract><kwd-group><kwd>recording</kwd><kwd>platform</kwd><kwd>flexible</kwd><kwd>data recording</kwd><kwd>neurotechnology</kwd><kwd>experiments</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><label>1</label><title>Introduction</title><p id="P2">Measuring high-quality electrophysiological human brain activity in neuroscientific research is notoriously difficult. High-quality signals can for example be measured from invasive electrodes, providing high spatial and temporal resolution ([<xref ref-type="bibr" rid="R1">1</xref>], [<xref ref-type="bibr" rid="R2">2</xref>]). However, since implanting humans solely for neuroscientific research is ethically debatable, researchers often ’piggy-back’ on clinical treatments of patients that receive electrode implants ([<xref ref-type="bibr" rid="R3">3</xref>], [<xref ref-type="bibr" rid="R4">4</xref>]). For example in patients with medication-resistant epilepsy undergoing presurgical monitoring for resection surgery ([<xref ref-type="bibr" rid="R5">5</xref>]) or patients qualified for deep brain stimulation ([<xref ref-type="bibr" rid="R6">6</xref>]).</p><p id="P3">The time to record neuroscientific experiments with these patient groups is severely limited. For example, epilepsy patients are usually in a monitoring unit ranging from a few days to two weeks, and for patients with deep brain stimulation, measurement windows are either microelectrode recordings during surgery or the week after surgery up until the stimulator is turned on. Measurements must not interfere with the clinical treatment, and patients need time to sufficiently recover to be able to participate, further reducing the available time. It is also common that several groups want to perform their own measurements, resulting in a further division on the already limited time.</p><p id="P4">For this reason, it is essential that the brief time window that remains is used as efficiently as possible. This means the amount of time spent on recording should be maximized, and inversely the time spent on setup and solving errors should be minimized. Both setup time and the error rate can be significantly reduced by automating as many manual actions as possible (e.g., connecting to recording devices, starting experiments, selecting data streams, starting, stopping, and synchronizing the recording). However, as experiments or recording setups change over time, it is often not worthwhile for research groups to invest in developing a more sophisticated system. It takes human resources, technical knowledge, and notable time investment to move beyond the prototypical system, which is unavailable to most research labs.</p><p id="P5">Aside from the custom made setups, there are a few widely used measurement platforms, such as BCI2000 ([<xref ref-type="bibr" rid="R7">7</xref>]), OpenVIBE ([<xref ref-type="bibr" rid="R8">8</xref>]) and FieldTrip ([<xref ref-type="bibr" rid="R9">9</xref>]). These systems can record data from a large number of different amplifiers, as well as provide modules to design and analyze experiments. When considering only the recording functionality, these platforms do not all run on all operating systems, are restrictive in either hard- or software toolset, programming language, input or output, or are not open-source. For example, FieldTrip requires the researcher to use the proprietary platform MATLAB, and the BCI2000 and OpenVIBE require the researcher to use their tools and API. Additionally, the user must install the full software package on their system, even when only the recording functionality is needed. While we think these platforms are valuable in their own right, we identified that a standalone platform is missing solely focused on recording data. Such a system should be able to run on any operating system, should not restrict the users to a specific set of hard- or software, and should be open-source. Ashmaig et al. ([<xref ref-type="bibr" rid="R10">10</xref>]) developed and described a system exclusively focused on continuous data recording for neurosurgical patients. The system provides a good use case for naturalistic long-term recordings but has an extensive list of hardware requirements and limits the user to Linux. Additionally, not all research groups have the opportunity to perform long-term recordings.</p><p id="P6">Therefore, we developed sTandalone Recorder of EXperiments or <italic>T-Rex</italic> for short, purposely designed for easy setup and recording of neuroscientific experiments. We ensured <italic>T-Rex</italic> is lightweight, open-source, and independent of toolset or operating system. The system can be used in any lab with access to a laptop and provides the researcher with an easy-to-use user interface. Moreover, we automate all setups and the start and stop of experiments, allowing the researcher to start experiments with a single press of a button.</p></sec><sec id="S2"><label>2</label><title>T-Rex platform</title><p id="P7">As there are many differences between recording setup between labs, we set three criteria to make <italic>T-Rex</italic> applicable in most labs: first, <italic>T-Rex</italic> should be <bold>independent</bold> of the choice of tools, paradigms, operating systems and programming languages. Each lab has its preferred tool sets, and not restricting specific sets allows labs to port experiments into <italic>T-Rex</italic> easily. Secondly, <italic>T-Rex</italic> should be <bold>user-friendly</bold> for both researcher and participant. By increasing simplicity, error rates and time spent on setup can be reduced. Thirdly, the system should be <bold>robust</bold>, meaning that the experiment should always run, and in case of technical problems, retain the data up to that point and return to the <italic>Home</italic> screen.</p><sec id="S3"><label>2.1</label><title>System outline</title><p id="P8">In short, <italic>T-Rex</italic> acts as the middle man handling the experimental overhead for the researcher (<xref ref-type="fig" rid="F1">Figure 1</xref>). When using <italic>T-Rex</italic>, the researcher can select an experiment by simply pressing a button on the main menu screen (<xref ref-type="fig" rid="F2">Figure 2A</xref>). When an experiment is selected, <italic>T-Rex</italic> checks the availability of all required input devices for the selected experiment and establishes a connection. For example, a movement-tracking experiment requires input from a hand-tracking device and the amplifier measuring the participants’ neural activity. <italic>T-Rex</italic> will search for and connect to these devices. <italic>T-Rex</italic> will also start the experiment user interface (UI) that instructs the participant on what task to perform. It starts recording all data streams and saves them to a folder specified by the researcher. The synchronized data is saved inside this folder using the <monospace>.xdf</monospace> file format. Lastly, the UI prompts the participant on how the experiment went and returns to the <italic>Home</italic> screen, where the next experiment can be selected. The only action the researcher has to perform is to start the required device data stream(s) and select the experiment in the <italic>Home</italic> screen.</p><sec id="S4"><label>2.1.1</label><title>Materials, software, and technologies</title><p id="P9">The web interface (including the <italic>Home</italic> screen with experiment selection, see <xref ref-type="fig" rid="F2">Figure 2A-D</xref>) is built using Bootstrap5<sup><xref ref-type="fn" rid="FN1">1</xref></sup> for the front-end and the Python package Flask<sup><xref ref-type="fn" rid="FN2">2</xref></sup> for the back-end. The Controller, handling set up, start and stop of the experiments (<xref ref-type="sec" rid="S9">section 2.2.2</xref>), is built in Python 3.9+ and requires a few dependencies found in <monospace>requirements.txt</monospace> in the repository. Data synchronization is implemented using Lab Streaming Layer ([<xref ref-type="bibr" rid="R11">11</xref>]).</p><p id="P10"><italic>T-Rex</italic> is compatible with Windows, macOS, and Linux. As <italic>T-Rex</italic> is lightweight and primarily active before and after experiments, the hardware requirements are determined mainly by the recording of the experiment in Lab Streaming Layer, instead of <italic>T-Rex</italic>.</p><sec id="S5"><title>Lab streaming layer</title><p id="P11"><italic>T-Rex</italic> uses Lab Streaming Layer (LSL) to synchronize the data streams from different devices, such as a variety of EEG amplifiers, audio streams, movement trackers, and cameras. The service handles <italic>“networking, time-synchronization, (near-) real-time access and optionally the centralized collection and recording of data.”</italic> <sup><xref ref-type="fn" rid="FN3">3</xref></sup>. It is lightweight and has multi-language and platform support, including Unity and Android. LSL allows the user to send data via a data stream to a local network server which can be recorded. Basic usage involves instantiating a StreamOutlet object with a name, type, channel count, sample rate, data type, and source id. For example, the most basic application in Python is represented in the following code-block:</p><preformat preformat-type="computer code">1 <styled-content style="color:#00815c">from</styled-content> <styled-content style="color:#0000ff">pylsl</styled-content> <styled-content style="color:#00815c">import</styled-content> StreamOutlet, StreamInfo
2 outlet = StreamOutlet(StreamInfo(<styled-content style="color:#bb212b">‘my_marker_stream’</styled-content>, <styled-content style="color:#bb212b">‘markers’</styled-content>,
3                      <styled-content style="color:#666666">1, 0.0</styled-content>, <styled-content style="color:#00815c">str</styled-content>, <styled-content style="color:#bb212b">‘my_unique_id’</styled-content>))
4 outlet.push_sample(‘Experiment_start’)
</preformat><p id="P12">Inversely, to receive data one can instantiate a <monospace>StreamInlet</monospace> and use <monospace>inlet.pull_sample()</monospace>. For a comprehensive overview, see the official documentation.<sup><xref ref-type="fn" rid="FN4">4</xref></sup> <italic>T-Rex</italic> requires each device or experiment included to start a <monospace>StreamOutlet</monospace> similar to the example above. If no <monospace>StreamOutlet</monospace> is created, <italic>T-Rex</italic> will not be able to find the device and start the experiment.</p></sec><sec id="S6"><title>Trigger</title><p id="P13">In some recording setups, a trigger is used to mark the start and end of an experiment. In these setups, the participant’s clinical data is recorded continuously and stored on a server. During an experiment, the data cannot be streamed directly and needs to be retrieved afterwards by the responsible data steward. The data steward can locate the requested data files by sending a unique trigger pattern (usually represented as an on-off signal) directly to a separate channel in the amplifier. T-Rex includes specific functionality to send a trigger signal at the start and end of the experiment.</p></sec></sec></sec><sec id="S7"><label>2.2</label><title>Software components</title><p id="P14">The software consists of two main components: the web interface that handles the UI and the Controller that sets up, starts, and stops all experiments (<xref ref-type="fig" rid="F1">Figure 1B</xref>).</p><sec id="S8"><label>2.2.1</label><title>Web interface</title><p id="P15">The Web interface includes four windows: <italic>Home</italic>, <italic>Experiment Feedback</italic>, <italic>Admin Login</italic>, and <italic>Admin Configuration</italic> (<xref ref-type="fig" rid="F2">Figure 2</xref>).</p><p id="P16">The <italic>Home</italic> window (<xref ref-type="fig" rid="F2">Figure 2A</xref>) displays all the experiments in a grid. Experiments’ cards’ are shown on that grid with a title, description, and start button. When the button is pressed, the <italic>Controller</italic> executes a command that starts the selected experiment. The command is defined by the researcher and specified on the configuration of the experiment (more details in <xref ref-type="sec" rid="S14">section 2.3</xref>). During the experiment, the web interface is on stand-by awaiting the completion of the experiment.</p><p id="P17">After completion, the participant is redirected to the <italic>Experiment Feedback</italic> window where the question <italic>“How did the experiment go?”</italic> is prompted (<xref ref-type="fig" rid="F2">Figure 2B</xref>). The participants are required to select a feedback option to continue. This allows the researcher to save a brief experiment evaluation to assess data quality in later analysis. In potential future applications, the participant might perform the experiments by themselves. Then this feedback is useful to flag the researcher to be aware of a potential loss in data quality during later analysis. The feedback is stored under the file name feedback.txt in the same folder as the most recent <monospace>.xdf</monospace> file (that contains the data recorded from the experiment).</p><p id="P18">The <italic>Admin Configuration</italic> provides the researcher with a closed environment where the participant identifier and a selection of all available experiments can be chosen. To access the <italic>Admin Configuration</italic>, the researcher must first log in using the password that is configured in the main configuration file (<xref ref-type="fig" rid="F2">Figure 2C</xref>, <xref ref-type="sec" rid="S14">2.3</xref> for details). When logged in, the researcher can see the configuration of the active experimental session, composed of an alphanumeric participant identifier and their access to experiments. A list of all the experiments included in the platform is visible from this window, but only those with checked marks are visible to the participant. The changes on this window are only applied after pressing the <bold>“Save”</bold> button at the end of the page.</p></sec><sec id="S9"><label>2.2.2</label><title>Controller</title><p id="P19">The Controller handles everything related to running an experiment and has three main parts: 1) Setup, 2) Start, and 3) Stop (<xref ref-type="fig" rid="F3">Figure 3</xref>). The related code can be found in the <monospace>./libs directory</monospace>.</p><sec id="S10"><title>Setup</title><p id="P20">When an experiment is started by pressing the start button on its card, the <monospace>Controller</monospace> class in <italic>Controller.py</italic> (<xref ref-type="fig" rid="F3">Figure 3</xref>) is called, loads the main configuration file and extracts the information received from the UI about which experiment to run. With this information, an <monospace>Experiment</monospace> instance is created, and its loading function is called.</p><p id="P21"><monospace>Experiment</monospace> loads the experiment-specific information and completes the setup in three steps. First, it checks for all devices and their LSL streams as defined by the user in the experiment configuration under device_inputs.</p><p id="P22">Subsequently, <monospace>Experiment</monospace> initializes a Recorder instance and adds all streams to the list of streams it should record. For a movement experiment ([<xref ref-type="bibr" rid="R12">12</xref>], [<xref ref-type="bibr" rid="R13">13</xref>], [<xref ref-type="bibr" rid="R14">14</xref>], [<xref ref-type="bibr" rid="R15">15</xref>], [<xref ref-type="bibr" rid="R16">16</xref>]), the streams recorded could be the neural amplifier, experimental triggers, and a movement tracker could be potentially added ([<xref ref-type="bibr" rid="R17">17</xref>], [<xref ref-type="bibr" rid="R18">18</xref>], [<xref ref-type="bibr" rid="R19">19</xref>]) or a force sensor ([<xref ref-type="bibr" rid="R20">20</xref>]. For speech perception ([<xref ref-type="bibr" rid="R21">21</xref>], [<xref ref-type="bibr" rid="R22">22</xref>], [<xref ref-type="bibr" rid="R23">23</xref>]) or auditory perception ([<xref ref-type="bibr" rid="R24">24</xref>], [<xref ref-type="bibr" rid="R25">25</xref>]) the audio stream, experiment triggers and neural data. For speech production ([<xref ref-type="bibr" rid="R26">26</xref>], [<xref ref-type="bibr" rid="R27">27</xref>], [<xref ref-type="bibr" rid="R28">28</xref>], [<xref ref-type="bibr" rid="R29">29</xref>]), the streams could be neural data, microphone, and triggers. In <xref ref-type="sec" rid="S17">section 3</xref> we provide some example experiments.</p><p id="P23">The last step is to check if a trigger is required for the selected experiment. If positive, it will set up a trigger class that searches and connects to the trigger.</p><p id="P24">It is essential that all devices are connected and active before the <monospace>Experiment</monospace> instance is called. As all requested devices are essential for successful recording, <italic>T-Rex</italic> will raise an error and return to the UI if not all input devices are connected successfully.</p></sec><sec id="S11"><title>Start</title><p id="P25">A user-defined command is called using Python’s subprocess library to start the experiment UI. This command is set in the experiment-specific configuration and should be capable of being executed from the command line interface. Because the experiment UI likely contains a stream that sends out experiment-related markers, <monospace>Experiment</monospace> will start a loop on a user-defined timeout to search for the marker stream. Once found, usually, almost instantly, the Recorder will start recording all streams. Implementing the system this way does not restrict the research aside from using LSL. However, due to the timeout, the experiment may start before the recording starts. This can only happen if the time between the setup of the experiment StreamOutlet and sending of the first marker is shorter than the time that the Recorder can find the stream and start the recording. Usually, finding the StreamOutlet and start recording is in the order of milliseconds. However, to entirely prevent the possibility of this happening, we recommend including a waiting screen in the experiment UI (i.e., “Press button to start”) or built in sufficient time (longer than the timeout set in the experiment configuration) between the setup of a StreamOutlet and the start of the experiment. Once connected to the experiment StreamOutlet, the experiment UI should start and the <monospace>Experiment</monospace> instance will wait until the called command is terminated and returns, which usually happens when the experiment UI window is closed.</p></sec><sec id="S12"><title>Stop</title><p id="P26">Once the subprocess call is returned, <monospace>Experiment</monospace> sends the final trigger and stops the Recorder. The data is saved in the <monospace>./output/</monospace> folder, defined in the main configuration file (<xref ref-type="sec" rid="S14">section 2.3</xref>), an example of the created directory tree can be found in the <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S4</xref>.</p></sec></sec><sec id="S13"><label>2.2.3</label><title>Device inputs</title><p id="P27">Each experiment can have multiple input devices, such as the amplifier measuring the neural data, hand-tracking devices or a microphone. Any device can be included as long as it generates a StreamOutlet. Each device should send the data from the device to LSL, allowing it to be accessed by the other system components and to be recorded. The name, type or source_id supplied to the StreamOutlet will be the values that <italic>T-Rex</italic> will search for at experiment setup (<xref ref-type="sec" rid="S9">section 2.2.2</xref>). In practice, this means that either the name, type, or source_id needs to be supplied under device_inputs in the experiment configuration file (<xref ref-type="sec" rid="S16">section 2.3.2</xref>). Since devices can be used for multiple experiments, we included a separate destination for all device input files: <monospace>./exp_module/inputs</monospace>, although input devices can be stored anywhere as long as they generate a StreamOutlet.</p></sec></sec><sec id="S14"><label>2.3</label><title>User configuration</title><p id="P28">There are two types of configuration files that the user can set: the main configuration and experiment-specific configurations. All configuration files are formatted in Yet Another Markup Language (YAML).</p><sec id="S15"><label>2.3.1</label><title>Main configuration</title><p id="P29">The file <monospace>config.yaml</monospace> on the root folder contains the system-wide configurations. This configuration file contains information containing general settings. <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S1</xref> contains a description of the different available options and <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S2</xref> contains an example of the main configuration file. The <monospace>main</monospace> option under <monospace>path</monospace> is the path all relative paths will be anchored to and should be set to the root folder. Most parameters are preset, but out and <monospace>trigger</monospace> configurations may vary between different measurement setups and might need to be redefined.</p></sec><sec id="S16"><label>2.3.2</label><title>Experiment configuration</title><p id="P30">Each experiment included in <italic>T-Rex</italic> requires a separate folder in <monospace>./exp_module/experiments/</monospace> and must include at least two files: <monospace>config.yaml</monospace> and the entry file to start the experiment. In this experiment configuration file, <monospace>name</monospace> and <monospace>description</monospace> define the text shown in the UI. <monospace>command</monospace> sets the command line interface command made by the controller class to start the experiment. <monospace>exp_outlet</monospace> sets the name, type or source_id that the Experiment class (<xref ref-type="sec" rid="S9">section 2.2.2</xref>) will search for, for <monospace>timeout</monospace> seconds. For example, if your experiment UI is a Python script that will create a StreamOutlet named <italic>markers</italic>, then <monospace>command</monospace> would be python <monospace>.\exp_module\experiments\your_experiment_file.py and exp_outlet=</monospace>’markers’. A full description of all experiment configuration fields and options can be found in <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S5</xref>.</p></sec></sec></sec><sec id="S17"><label>3</label><title>Use cases</title><p id="P31">We have included three different example experiments to provide a more practical view of how to use <italic>T-Rex</italic>. The examples can also serve as a quick start for researchers to create new experiments or adapt the ones included. A step-by-step explanation of adding a new experiment is described in <xref ref-type="sec" rid="S22">section 3.4</xref>.</p><sec id="S18"><label>3.1</label><title>Case 1: Simple experiment in Python</title><p id="P32">This experiment is a simple text-based instruction for a grasping task (<xref ref-type="fig" rid="F4">Figure 4A</xref>). The participant is prompted by text in a Python tkinter<sup><xref ref-type="fn" rid="FN5">5</xref></sup> window to continuously open and close either their left or right hand, as used in [<xref ref-type="bibr" rid="R13">13</xref>]. The experiment requires neural data as the input device and generates a StreamOutlet to send markers that inform about the start and end of the experiment and of the trials. The neural data is acquired from a stream with <italic>name</italic> = <italic>Micromed</italic>, <italic>type</italic> = <italic>EEG</italic>, <italic>source</italic>_<italic>id</italic> = <italic>micm</italic>01. These values are all set by the user. As <italic>T-Rex</italic> will search for all three options (name, type or source_id), only one needs to be provided. Therefore, the option under <monospace>device\_inputs</monospace> in <monospace>grasping\config.yaml</monospace> is set to <monospace>eeg</monospace> (case-insensitive). Next, the Marker StreamOutlet that the experiment itself will generate has <italic>source</italic>_<italic>id</italic> = <italic>emuidw</italic>22. When the <monospace>Experiment</monospace> class runs the experiment command (<monospace>command</monospace> field in <monospace>grasping\config.yaml</monospace>) it will search for this streams. Therefore, the <monospace>exp_outlet</monospace> field is set to ’emuidw22’. Finally, since the grasping experiment is Python-based, the <monospace>command</monospace> should use Python to call the script with the command: <monospace>python .\exp_module\experiments\grasping\grasping.py</monospace>. The configuration file used can be found in <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S6</xref>.</p><p id="P33">When these options are set, the experiment is ready to go and can be started by pressing the start button on the <italic>Home</italic> window. The Tk window opens and waits for the spacebar to be pressed. Once pressed, the experiment starts and is locked on the topmost position upon completion. When the experiment is finished and closed (i.e., the command call ends and returns to the Experiment class), the <monospace>Experiment</monospace> instance stops the recording and saves the data. In-depth details on how experiments are started and stopped are described in <xref ref-type="sec" rid="S9">section 2.2.2</xref>.</p></sec><sec id="S19"><label>3.2</label><title>Case 2: Simple experiment in a WebUI</title><p id="P34">From the Web Interface, both participants and researchers can access the main functionalities of <italic>T-Rex</italic>, allowing the potential execution of the system on a headless server, different from the interface that interacts with the participants. The system that runs <italic>T-Rex</italic> could create a local network that serves the experiments to different devices (tablets, laptops, smartphones), removing the need for an active internet connection which could render the execution unsafe.</p><p id="P35">To illustrate this paradigm, we created the grasping web experiment, which mimics the behaviour of the “grasping” experiment (<xref ref-type="sec" rid="S18">section 3.1</xref>) but in a web format (<xref ref-type="fig" rid="F4">Figure 4B</xref>). Given that this experiment consists of a Single Page Application (SPA), it can be wholly executed on any device with access to a web browser, like laptops, tablets, and smartphones. The grasping web experiment also illustrates options other than a Tk window for experimenting.</p><p id="P36">For constructing the experiment, we used HTML, CSS (using Bootstrap5 for the responsiveness and other visual aspects), and JavaScript for the behavior. The device input is the same as in the Tk implementation as well as the StreamOutlet containing the markers, thus the <monospace>device_inputs</monospace> and <monospace>exp_outlet</monospace> are the same. The difference is in the command executed to start the experiment. In this case, <monospace>start .\exp_module\experiments\graspingWeb\index.html</monospace> is used. The configuration file used can be found in <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S7</xref>.</p><p id="P37">Once the experiment is started on the <italic>Home</italic> window, the <monospace>Experiment</monospace> instance opens another tab on the browser displaying the “grasping_web” experiment. The experiment starts when the participant presses the green “Start” button. When the experiment is finished, the participant or researcher is prompted to press a red button to close the experiment. The GraspingWeb command call is finished at button-press and returns to the <monospace>Experiment</monospace> instance, stopping the recording and saving the data.</p></sec><sec id="S20"><label>3.3</label><title>Case 3: Multiple devices</title><p id="P38">Lastly, we included a 3D hand-tracking experiment, where the goal is to hold the cursor (a black circle) on the target (a red circle). The cursor can be moved in 3d, where the third dimension controls the size of the circle (<xref ref-type="fig" rid="F4">Figure 4C</xref>). In this case, the hand-tracking is done by the LeapMotion controller<sup><xref ref-type="fn" rid="FN6">6</xref></sup>. We have provided a .exe that reads the data from the tracker and sends it to an LSL StreamOutlet with <italic>name</italic> = <italic>LeapLSL</italic>, <italic>type</italic> = <italic>Coordinates</italic>, <italic>source</italic>_<italic>id</italic> = <italic>LEAPLSL</italic>01. In addition to the hand-tracking information, we also need neural activity, for which we use the same StreamOutlet as described in case 2 (<xref ref-type="sec" rid="S18">section 3.1</xref>). Lastly, the experiment is implemented in a Python Tk window and generates a marker Stream similar to the streams described in the previous use case with <italic>Source</italic>_<italic>id</italic> = <italic>BUBBLE</italic>01. Thus, to set up the configuration for this experiment, we set command to <monospace>python .\exp_module\experiments\Bubbles\bubbles.py</monospace>, exp_outlet to <monospace>BUBBLE01</monospace> and device_inputs to <monospace>LEAPLSL01</monospace> (the tracking information stream) and eeg (the neural data stream). To run the experiment, the researcher should start the device stream before the experiment is started in the <italic>Home</italic> screen. (i.e. run the <monospace>.exe</monospace> first). The configuration file used can be found in <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S8</xref>.</p><sec id="S21"><label>3.3.1</label><title>Mix ’n Match</title><p id="P39">These are just three examples showing different possibilities. It is easy to include more devices by simply adding a StreamOutlet name, type, or source_id to the list of <monospace>device\_outputs</monospace>. However, it is required that the researcher writes a script to read the data from the device and send it to an LSL StreamOutlet. New experiments can also be built in Unity<sup><xref ref-type="fn" rid="FN7">7</xref></sup> or PyGame<sup><xref ref-type="fn" rid="FN8">8</xref></sup> to provide better graphical experiences. As mentioned before, we do not impose restrictions on which technologies to use or the specific type of experiments that can be performed, whether speech production, audio or speech perception, movement, or simple or more naturalistic tasks ([<xref ref-type="bibr" rid="R30">30</xref>], [<xref ref-type="bibr" rid="R31">31</xref>]).</p></sec></sec><sec id="S22"><label>3.4</label><title>Adding new experiments to the platform</title><p id="P40">The following steps describe how to add a new experiment from scratch to <italic>T-Rex</italic>:</p><list list-type="order" id="L1"><list-item><p id="P41">Create the experiment folder inside the directory <monospace>./exp_module/experiments/</monospace>. An example of the directory tree for different example experiments can be found in <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S3</xref>.</p></list-item><list-item><p id="P42">Create the experiment configuration file (<monospace>config.yaml</monospace>) inside the new folder. <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S9</xref> can be used as the base example for creating this file, and <xref ref-type="sec" rid="S16">section 2.3.2</xref> contains a detailed description of each parameter.</p></list-item><list-item><p id="P43">Adjust the fields to the specific experiment (<xref ref-type="sec" rid="S14">section 2.3</xref>).</p></list-item></list><p id="P44">After completing these initial steps, the experiment should be visible from the <italic>Admin Configuration</italic> panel. The researcher can set the experiment as “visible” from the admin panel by selecting its corresponding check mark. If configured as “visible”, it should appear on the <italic>Home</italic> window, and it can be executed by clicking on its respective button.</p><p id="P45">It is worth mentioning that when porting an already-configured version of <italic>T-Rex</italic> to a different OS, some parameters might need to be revised. For example, the parameter <monospace>command</monospace>, when used on Windows to start a python experiment, the definition is the following:</p><preformat preformat-type="computer code"><styled-content style="color:#00815c">command</styled-content>: python .\exp_module\experiments\example\example_experiment.py</preformat><p id="P46">However, when used on Unix or Unix-like systems, the definition changes to the following:</p><preformat preformat-type="computer code"><styled-content style="color:#00815c">command</styled-content>: python ./exp_module/experiments/example/example_experiment.py</preformat><p id="P47">The difference comes because “/” is the path separator on Unix and Unix-like systems, and Microsoft uses “\”.</p><p id="P48">There might be other scenarios where the parameter <monospace>command</monospace> might differ between OS, so we recommend revising each experiment configuration file when porting the platform to a different OS.</p><p id="P49">Even so, incorporating new experiments into <italic>T-Rex</italic> is seamless. Several experiments can be quickly integrated into the platform with just a few easy steps.</p></sec></sec><sec id="S23" sec-type="discussion"><label>4</label><title>Discussion</title><p id="P50">We presented <italic>T-Rex</italic>, an independent, user-friendly, and robust system that minimizes setup time and error rate. <italic>T-Rex</italic> provides a simple UI and reduces the experimental setup to a press of a button. The software merges features for automated continuous recordings and synchronization of neural data into an easily operated web interface. Once coupled to <italic>T-Rex</italic>, the experiments can be executed directly from the interface. It also includes functionalities for managing several participants with different access levels.</p><p id="P51">The simplicity of <italic>T-Rex</italic> reduces the amount actions that the researcher has to make to two: Starting the required device(s) and starting the experiment. This significantly reduces the room for error, saves time to set up and check all connections, improves reliability, and leaves more time for actual experiments. We aimed to make <italic>T-Rex</italic> as flexible as possible by making it available to all operating systems, most main programming languages (limited by LSL), and web browsers. So far, we can verify that it works with Firefox (version: 105.0.1), Chrome (version: 106), Safari (version: 16), and Edge (version: 106), although it should be compatible with higher versions and other mainstream browsers. Any software that provides a command line interface is compatible with <italic>T-Rex</italic> (for example, Psychopy<sup><xref ref-type="fn" rid="FN9">9</xref></sup>, PyGame, Unity). This also means that a system can easily be transferred to other devices by copying the root folder to the new device and installing the required dependencies. It is noteworthy that measurements with <italic>T-Rex</italic> are not limited to neuroscientific research. Lastly, as we used LSL to handle time series synchronization, there are also networking options, allowing for data recording over a network of devices instead connections to a single device.</p><p id="P52">Relying on a command line interface command to start an experiment also means that it does involve some knowledge of shell scripts. The commands or scripts are platform dependent and will likely need to be adjusted to the OS where the platform needs to be executed, even when executing the same experiments. Additionally, experiments need to include a StreamOutlet object in the code to be included in <italic>T-Rex</italic>, which requires some adjustment to existing experiments and programming knowledge. The same is true for the device inputs required for experiments. This usually means the researcher needs to create a basic script that reads data from the device and sends it to LSL. The researcher should also start these device scripts before starting the experiment.</p><p id="P53"><italic>T-Rex</italic> is in ongoing development, and starting the device scripts at the experiment start is likely to be added in future updates. We have designed <italic>T-Rex</italic> so that in future updates, it is not necessary anymore for the researcher to be present during experiments. If experiments are engaging enough, the participants may want to start an experiment themselves. <italic>T-Rex</italic> contains the core structure to automate the recording process entirely, which could increase the amount of experimental neural data gathered.</p><p id="P54">To summarize, we developed a new recording platform that is Operating System independent, user-friendly and robust. We provide researchers with a solution that can significantly increase time spent on recording instead of on the setup with its possible errors.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Document</label><media xlink:href="EMS156325-supplement-Supplementary_Document.pdf" mimetype="application" mime-subtype="pdf" id="d43aAdEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S24"><title>Funding</title><p>CH acknowledges funding by the Dutch Research Council (NWO) through the research project’ Decoding Speech In SEEG (DESIS)’ with project number VI.Veni.194.021.</p></ack><sec id="S25" sec-type="data-availability"><title>Code availability</title><p id="P55">The source code, installation guide, and example experiments can be found on GitHub <sup><xref ref-type="fn" rid="FN10">10</xref></sup>. <italic>T-Rex</italic> is available under the permissive MIT License. As <italic>T-Rex</italic> will be in ongoing development, we kindly invite users to provide feedback or contribute to this open-source project.</p></sec><fn-group><fn id="FN1"><label>1</label><p id="P56"><ext-link ext-link-type="uri" xlink:href="https://getbootstrap.com/docs/5.1/getting-started/introduction/">https://getbootstrap.com/docs/5.1/getting-started/introduction/</ext-link></p></fn><fn id="FN2"><label>2</label><p id="P57"><ext-link ext-link-type="uri" xlink:href="https://flask.palletsprojects.com/en/2.1.x/">https://flask.palletsprojects.com/en/2.1.x/</ext-link></p></fn><fn id="FN3"><label>3</label><p id="P58"><ext-link ext-link-type="uri" xlink:href="https://github.com/sccn/labstreaminglayer">https://github.com/sccn/labstreaminglayer</ext-link></p></fn><fn id="FN4"><label>4</label><p id="P59"><ext-link ext-link-type="uri" xlink:href="https://labstreaminglayer.readthedocs.io/info/getting_started.html">https://labstreaminglayer.readthedocs.io/info/getting_started.html</ext-link></p></fn><fn id="FN5"><label>5</label><p id="P60"><ext-link ext-link-type="uri" xlink:href="https://docs.python.org/3/library/tkinter.html">https://docs.python.org/3/library/tkinter.html</ext-link></p></fn><fn id="FN6"><label>6</label><p id="P61"><ext-link ext-link-type="uri" xlink:href="https://www.ultraleap.com/product/leap-motion-controller/">https://www.ultraleap.com/product/leap-motion-controller/</ext-link></p></fn><fn id="FN7"><label>7</label><p id="P62"><ext-link ext-link-type="uri" xlink:href="https://unity.com/">https://unity.com/</ext-link></p></fn><fn id="FN8"><label>8</label><p id="P63"><ext-link ext-link-type="uri" xlink:href="https://www.pygame.org/wiki/about">https://www.pygame.org/wiki/about</ext-link></p></fn><fn id="FN9"><label>9</label><p id="P64"><ext-link ext-link-type="uri" xlink:href="https://www.psychopy.org/">https://www.psychopy.org/</ext-link></p></fn><fn id="FN10"><label>10</label><p id="P65"><ext-link ext-link-type="uri" xlink:href="https://github.com/neuralinterfacinglab/t-rex">https://github.com/neuralinterfacinglab/t-rex</ext-link></p></fn><fn id="FN11" fn-type="conflict"><p id="P66"><bold>Conflict of Interest Statement</bold></p><p id="P67">The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be considered as a potential conflict of interest.</p></fn><fn id="FN12" fn-type="con"><p id="P68"><bold>Author Contributions</bold></p><p id="P69"><italic>Conceptualization</italic>: JAV, MCO, MV, PK, CH <italic>Data Curation</italic>: N/A <italic>Formal analysis</italic>: N/A <italic>Funding acquisition</italic>: N/A <italic>Investigation</italic>: JAV, MCO, PK <italic>Methodology</italic>: JAV, MCO, PK, CH <italic>Project administration</italic>: JAV, MCO <italic>Resources</italic>: PK, CH <italic>Software</italic>: JAV, MCO, PK <italic>Supervision</italic>: PK, CH <italic>Validation</italic>: JAV, MCO, MV <italic>Visualization</italic>: JAV, MCO <italic>Writing—Original Draft</italic>: JAV, MCO <italic>Writing—Review &amp; editing</italic>: JAV, MCO, MV, PK, CH</p></fn></fn-group><glossary><def-list><title>Abbreviations</title><def-item><term>T-Rex</term><def><p id="P70">sTandalone Recorder of EXperiments</p></def></def-item><def-item><term>UI</term><def><p id="P71">User interface</p></def></def-item><def-item><term>CLI</term><def><p id="P72">Command Line Interface</p></def></def-item><def-item><term>OS</term><def><p id="P73">Operating System</p></def></def-item><def-item><term>YAML</term><def><p id="P74">Yet Another Markup Language</p></def></def-item></def-list></glossary><ref-list><ref id="R1"><label>[1]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herff</surname><given-names>Christian</given-names></name><name><surname>Krusienski</surname><given-names>Dean J</given-names></name><name><surname>Kubben</surname><given-names>Pieter</given-names></name></person-group><article-title>The Potential of Stereotactic-EEG for Brain-Computer Interfaces: Current Progress and Future Directions</article-title><source>Frontiers in Neuroscience</source><year>2020</year><volume>14</volume></element-citation></ref><ref id="R2"><label>[2]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacobs</surname><given-names>Joshua</given-names></name><name><surname>Kahana</surname><given-names>Michael J</given-names></name></person-group><article-title>Direct brain recordings fuel advances in cognitive electrophysiology</article-title><source>Trends in Cognitive Sciences</source><year>2010</year><month>April</month><volume>14</volume><issue>4</issue><fpage>162</fpage><lpage>171</lpage></element-citation></ref><ref id="R3"><label>[3]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feinsinger</surname><given-names>Ashley</given-names></name><name><surname>Pouratian</surname><given-names>Nader</given-names></name><name><surname>Ebadi</surname><given-names>Hamasa</given-names></name><name><surname>Adolphs</surname><given-names>Ralph</given-names></name><name><surname>Andersen</surname><given-names>Richard</given-names></name><name><surname>Beauchamp</surname><given-names>Michael S</given-names></name><name><surname>Chang</surname><given-names>Edward F</given-names></name><name><surname>Crone</surname><given-names>Nathan E</given-names></name><name><surname>Collinger</surname><given-names>Jennifer L</given-names></name><name><surname>Fried</surname><given-names>Itzhak</given-names></name><etal/></person-group><article-title>Ethical commitments, principles, and practices guiding intracranial neuroscientific research in humans</article-title><source>Neuron</source><year>2022</year><volume>110</volume><issue>2</issue><fpage>188</fpage><lpage>194</lpage></element-citation></ref><ref id="R4"><label>[4]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mercier</surname><given-names>Manuel R</given-names></name><name><surname>Dubarry</surname><given-names>Anne-Sophie</given-names></name><name><surname>Tadel</surname><given-names>François</given-names></name><name><surname>Avanzini</surname><given-names>Pietro</given-names></name><name><surname>Axmacher</surname><given-names>Nikolai</given-names></name><name><surname>Cellier</surname><given-names>Dillan</given-names></name><name><surname>Vecchio</surname><given-names>Maria Del</given-names></name><name><surname>Hamilton</surname><given-names>Liberty S</given-names></name><name><surname>Hermes</surname><given-names>Dora</given-names></name><name><surname>Kahana</surname><given-names>Michael J</given-names></name><etal/></person-group><article-title>Advances in human intracranial electroencephalography research, guidelines and good practices</article-title><source>NeuroImage</source><year>2022</year><elocation-id>119438</elocation-id></element-citation></ref><ref id="R5"><label>[5]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chauvel</surname><given-names>Patrick</given-names></name><name><surname>Gonzalez-Martinez</surname><given-names>Jorge</given-names></name><name><surname>Bulacio</surname><given-names>Juan</given-names></name></person-group><chapter-title>Chapter 3 - Presurgical intracranial investigations in epilepsy surgery</chapter-title><person-group person-group-type="editor"><name><surname>Levin</surname><given-names>Kerry H</given-names></name><name><surname>Chauvel</surname><given-names>Patrick</given-names></name></person-group><source>Handbook of Clinical Neurology, volume 161 of Clinical Neurophysiology: Diseases and Disorders</source><publisher-name>Elsevier</publisher-name><year>2019</year><month>January</month><fpage>45</fpage><lpage>71</lpage></element-citation></ref><ref id="R6"><label>[6]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lozano</surname><given-names>Andres M</given-names></name><name><surname>Lipsman</surname><given-names>Nir</given-names></name><name><surname>Bergman</surname><given-names>Hagai</given-names></name><name><surname>Brown</surname><given-names>Peter</given-names></name><name><surname>Chabardes</surname><given-names>Stephan</given-names></name><name><surname>Chang</surname><given-names>Jin Woo</given-names></name><name><surname>Matthews</surname><given-names>Keith</given-names></name><name><surname>McIntyre</surname><given-names>Cameron C</given-names></name><name><surname>Schlaepfer</surname><given-names>Thomas E</given-names></name><name><surname>Schulder</surname><given-names>Michael</given-names></name><name><surname>Temel</surname><given-names>Yasin</given-names></name><etal/></person-group><article-title>Deep brain stimulation: current challenges and future directions</article-title><source>Nature Reviews Neurology</source><publisher-name>Springer US</publisher-name><year>2019</year><volume>15</volume><issue>3</issue><fpage>148</fpage><lpage>160</lpage></element-citation></ref><ref id="R7"><label>[7]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schalk</surname><given-names>G</given-names></name><name><surname>McFarland</surname><given-names>DJ</given-names></name><name><surname>Hinterberger</surname><given-names>T</given-names></name><name><surname>Birbaumer</surname><given-names>N</given-names></name><name><surname>Wolpaw</surname><given-names>JR</given-names></name></person-group><article-title>BCI2000: A General-Purpose Brain-Computer Interface (BCI) System</article-title><source>IEEE Transactions on Biomedical Engineering</source><year>2004</year><month>June</month><volume>51</volume><issue>6</issue><fpage>1034</fpage><lpage>1043</lpage></element-citation></ref><ref id="R8"><label>[8]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Renard</surname><given-names>Yann</given-names></name><name><surname>Lotte</surname><given-names>Fabien</given-names></name><name><surname>Gibert</surname><given-names>Guillaume</given-names></name><name><surname>Congedo</surname><given-names>Marco</given-names></name><name><surname>Maby</surname><given-names>Emmanuel</given-names></name><name><surname>Delannoy</surname><given-names>Vincent</given-names></name><name><surname>Bertrand</surname><given-names>Olivier</given-names></name><name><surname>Lécuyer</surname><given-names>Anatole</given-names></name></person-group><article-title>OpenViBE: An Open-Source Software Platform to Design, Test, and Use Brain–Computer Interfaces in Real and Virtual Environments</article-title><source>Presence</source><year>2010</year><month>February</month><volume>19</volume><issue>1</issue><fpage>35</fpage><lpage>53</lpage><comment>Conference Name: Presence</comment></element-citation></ref><ref id="R9"><label>[9]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Oostenveld</surname><given-names>Robert</given-names></name><name><surname>Fries</surname><given-names>Pascal</given-names></name><name><surname>Maris</surname><given-names>Eric</given-names></name><name><surname>Schoffelen</surname><given-names>Jan-Mathijs</given-names></name></person-group><article-title>FieldTrip: Open Source Software for Advanced Analysis of MEG, EEG, and Invasive Electrophysiological Data</article-title><source>Computational Intelligence and Neuroscience 2011</source><publisher-name>Hindawi</publisher-name><year>2010</year><month>December</month><elocation-id>e156869</elocation-id></element-citation></ref><ref id="R10"><label>[10]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashmaig</surname><given-names>Omer</given-names></name><name><surname>Hamilton</surname><given-names>Liberty S</given-names></name><name><surname>Modur</surname><given-names>Pradeep</given-names></name><name><surname>Buchanan</surname><given-names>Robert J</given-names></name><name><surname>Preston</surname><given-names>Alison R</given-names></name><name><surname>Watrous</surname><given-names>Andrew J</given-names></name></person-group><article-title>A Platform for Cognitive Monitoring of Neurosurgical Patients During Hospitalization</article-title><source>Frontiers in Human Neuroscience</source><year>2021</year><volume>15</volume></element-citation></ref><ref id="R11"><label>[11]</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Kothe</surname><given-names>C</given-names></name></person-group><source>Lab Streaming Layer (lsl)</source><year>2014</year></element-citation></ref><ref id="R12"><label>[12]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ottenhoff</surname><given-names>Maarten C</given-names></name><name><surname>Goulis</surname><given-names>Sophocles</given-names></name><name><surname>Wagner</surname><given-names>Louis</given-names></name><name><surname>Tousseyn</surname><given-names>Simon</given-names></name><name><surname>Colon</surname><given-names>Albert</given-names></name><name><surname>Kubben</surname><given-names>Pieter</given-names></name><name><surname>Herff</surname><given-names>Christian</given-names></name></person-group><source>Continuously Decoding Grasping Movements using Stereotactic Depth Electrodes</source><conf-name>2021 43rd Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC)</conf-name><year>2021</year><month>November</month><fpage>6098</fpage><lpage>6101</lpage><comment>ISSN: 2694-0604</comment></element-citation></ref><ref id="R13"><label>[13]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ottenhoff</surname><given-names>Maarten C</given-names></name><name><surname>Verwoert</surname><given-names>Maxime</given-names></name><name><surname>Goulis</surname><given-names>Sophocles</given-names></name><name><surname>Colon</surname><given-names>Albert J</given-names></name><name><surname>Wagner</surname><given-names>Louis</given-names></name><name><surname>Tousseyn</surname><given-names>Simon</given-names></name><name><surname>van Dijk</surname><given-names>Johannes P</given-names></name><name><surname>Kubben</surname><given-names>Pieter L</given-names></name><name><surname>Herff</surname><given-names>Christian</given-names></name></person-group><article-title>Executed and imagined grasping movements can be decoded from lower dimensional representation of distributed non-motor brain areas</article-title><source>bioRxiv [preprint]</source><year>2022</year><month>July</month></element-citation></ref><ref id="R14"><label>[14]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Guangye</given-names></name><name><surname>Jiang</surname><given-names>Shize</given-names></name><name><surname>Meng</surname><given-names>Jianjun</given-names></name><name><surname>Chai</surname><given-names>Guohong</given-names></name><name><surname>Wu</surname><given-names>Zehan</given-names></name><name><surname>Fan</surname><given-names>Zhen</given-names></name><name><surname>Hu</surname><given-names>Jie</given-names></name><name><surname>Sheng</surname><given-names>Xinjun</given-names></name><name><surname>Zhang</surname><given-names>Dingguo</given-names></name><name><surname>Chen</surname><given-names>Liang</given-names></name><name><surname>Zhu</surname><given-names>Xiangyang</given-names></name></person-group><article-title>Assessing differential representation of hand movements in multiple domains using stereo-electroencephalographic recordings</article-title><source>NeuroImage</source><year>2022</year><month>April</month><volume>250</volume><elocation-id>118969</elocation-id></element-citation></ref><ref id="R15"><label>[15]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Guangye</given-names></name><name><surname>Jiang</surname><given-names>Shize</given-names></name><name><surname>Paraskevopoulou</surname><given-names>Sivylla</given-names></name><name><surname>Chai</surname><given-names>Guohong</given-names></name><name><surname>Wei</surname><given-names>Zixuan</given-names></name><name><surname>Liu</surname><given-names>Shengjie</given-names></name><name><surname>Wang</surname><given-names>Meng</given-names></name><name><surname>Xu</surname><given-names>Yang</given-names></name><name><surname>Fan</surname><given-names>Zhen</given-names></name><name><surname>Wu</surname><given-names>Zehan</given-names></name><name><surname>Chen</surname><given-names>Liang</given-names></name><etal/></person-group><article-title>Detection of human white matter activation and evaluation of its function in movement decoding using stereo-electroencephalography (SEEG)</article-title><source>Journal of Neural Engineering</source><year>2021</year><month>July</month></element-citation></ref><ref id="R16"><label>[16]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Merk</surname><given-names>Timon</given-names></name><name><surname>Peterson</surname><given-names>Victoria</given-names></name><name><surname>Lipski</surname><given-names>Witold J</given-names></name><name><surname>Blankertz</surname><given-names>Benjamin</given-names></name><name><surname>Turner</surname><given-names>Robert S</given-names></name><name><surname>Li</surname><given-names>Ningfei</given-names></name><name><surname>Horn</surname><given-names>Andreas</given-names></name><name><surname>Richardson</surname><given-names>Robert Mark</given-names></name><name><surname>Neumann</surname><given-names>Wolf-Julian</given-names></name></person-group><article-title>Electrocorticography is superior to subthalamic local field potentials for movement decoding in Parkinson’s disease</article-title><source>eLife</source><publisher-name>eLife Sciences Publications, Ltd</publisher-name><year>2022</year><month>May</month><volume>11</volume><elocation-id>e75126</elocation-id></element-citation></ref><ref id="R17"><label>[17]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mondini</surname><given-names>Valeria</given-names></name><name><surname>Kobler</surname><given-names>Reinmar J</given-names></name><name><surname>Sburlea</surname><given-names>Andreea I</given-names></name><name><surname>Müller-Putz</surname><given-names>Gernot R</given-names></name></person-group><article-title>Continuous low-frequency EEG decoding of arm movement for closed-loop, natural control of a robotic arm</article-title><source>Journal of Neural Engineering</source><year>2020</year><volume>17</volume><issue>4</issue></element-citation></ref><ref id="R18"><label>[18]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coste</surname><given-names>Christine Azevedo</given-names></name><name><surname>William</surname><given-names>Lucie</given-names></name><name><surname>Fonseca</surname><given-names>Lucas</given-names></name><name><surname>Hiairrassary</surname><given-names>Arthur</given-names></name><name><surname>Andreu</surname><given-names>David</given-names></name><name><surname>Geffrier</surname><given-names>Antoine</given-names></name><name><surname>Teissier</surname><given-names>Jacques</given-names></name><name><surname>Fattal</surname><given-names>Charles</given-names></name><name><surname>Guiraud</surname><given-names>David</given-names></name></person-group><article-title>Activating effective functional hand movements in individuals with complete tetraplegia through neural stimulation</article-title><source>Scientific Reports</source><year>2022</year><volume>12</volume><issue>1</issue><fpage>1</fpage><lpage>17</lpage></element-citation></ref><ref id="R19"><label>[19]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hosseini</surname><given-names>Seyyed Moosa</given-names></name><name><surname>Shalchyan</surname><given-names>Vahid</given-names></name></person-group><article-title>Continuous Decoding of Hand Movement From EEG Signals Using Phase-Based Connectivity Features</article-title><source>Frontiers in Human Neuroscience</source><year>2022</year><volume>16</volume></element-citation></ref><ref id="R20"><label>[20]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Shah</surname><given-names>Syed A</given-names></name><name><surname>Tan</surname><given-names>Huiling</given-names></name><name><surname>Brown</surname><given-names>Peter</given-names></name></person-group><source>Continuous force decoding from deep brain local field potentials for Brain Computer Interfacing</source><conf-name>International IEEE/EMBS Conference on Neural Engineering, NER</conf-name><year>2017</year><fpage>371</fpage><lpage>374</lpage><comment>ISBN: 9781538619162</comment></element-citation></ref><ref id="R21"><label>[21]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patel</surname><given-names>Prachi</given-names></name><name><surname>Heijden</surname><given-names>Kiki van der</given-names></name><name><surname>Bickel</surname><given-names>Stephan</given-names></name><name><surname>Herrero</surname><given-names>Jose L</given-names></name><name><surname>Mehta</surname><given-names>Ashesh D</given-names></name><name><surname>Mesgarani</surname><given-names>Nima</given-names></name></person-group><article-title>Interaction of bottom-up and top-down neural mechanisms in spatial multi-talker speech perception</article-title><source>Current Biology</source><year>2022</year></element-citation></ref><ref id="R22"><label>[22]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prinsloo</surname><given-names>Kevin D</given-names></name><name><surname>Lalor</surname><given-names>Edmund C</given-names></name></person-group><article-title>General auditory and speech-specific contributions to cortical envelope tracking revealed using auditory chimeras</article-title><source>Journal of Neuroscience</source><year>2022</year></element-citation></ref><ref id="R23"><label>[23]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biau</surname><given-names>Emmanuel</given-names></name><name><surname>Schultz</surname><given-names>Benjamin G</given-names></name><name><surname>Gunter</surname><given-names>Thomas C</given-names></name><name><surname>Kotz</surname><given-names>Sonja A</given-names></name></person-group><article-title>Left motor <italic>δ</italic> oscillations reflect asynchrony detection in multisensory speech perception</article-title><source>Journal of Neuroscience</source><year>2022</year><volume>42</volume><issue>11</issue><fpage>2313</fpage><lpage>2326</lpage></element-citation></ref><ref id="R24"><label>[24]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hausfeld</surname><given-names>Lars</given-names></name><name><surname>Disbergen</surname><given-names>Niels R</given-names></name><name><surname>Valente</surname><given-names>Giancarlo</given-names></name><name><surname>Zatorre</surname><given-names>Robert J</given-names></name><name><surname>Formisano</surname><given-names>Elia</given-names></name></person-group><article-title>Modulating cortical instrument representations during auditory stream segregation and integration with polyphonic music</article-title><source>Frontiers in neuroscience</source><year>2021</year><volume>15</volume></element-citation></ref><ref id="R25"><label>[25]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hausfeld</surname><given-names>Lars</given-names></name><name><surname>Shiell</surname><given-names>Martha</given-names></name><name><surname>Formisano</surname><given-names>Elia</given-names></name><name><surname>Riecke</surname><given-names>Lars</given-names></name></person-group><article-title>Cortical processing of distracting speech in noisy auditory scenes depends on perceptual demand</article-title><source>Neuroimage</source><year>2021</year><volume>228</volume><elocation-id>117670</elocation-id></element-citation></ref><ref id="R26"><label>[26]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Angrick</surname><given-names>Miguel</given-names></name><name><surname>Herff</surname><given-names>Christian</given-names></name><name><surname>Mugler</surname><given-names>Emily</given-names></name><name><surname>Tate</surname><given-names>Matthew C</given-names></name><name><surname>Slutzky</surname><given-names>Marc W</given-names></name><name><surname>Krusienski</surname><given-names>Dean J</given-names></name><name><surname>Schultz</surname><given-names>Tanja</given-names></name></person-group><article-title>Speech synthesis from ECoG using densely connected 3D convolutional neural networks</article-title><source>Journal of Neural Engineering</source><year>2019</year><volume>16</volume><issue>3</issue></element-citation></ref><ref id="R27"><label>[27]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herff</surname><given-names>Christian</given-names></name><name><surname>Diener</surname><given-names>Lorenz</given-names></name><name><surname>Angrick</surname><given-names>Miguel</given-names></name><name><surname>Mugler</surname><given-names>Emily</given-names></name><name><surname>Tate</surname><given-names>Matthew C</given-names></name><name><surname>Goldrick</surname><given-names>Matthew A</given-names></name><name><surname>Krusienski</surname><given-names>Dean J</given-names></name><name><surname>Slutzky</surname><given-names>Marc W</given-names></name><name><surname>Schultz</surname><given-names>Tanja</given-names></name></person-group><article-title>Generating natural, intelligible speech from brain activity in motor, premotor, and inferior frontal cortices</article-title><source>Frontiers in neuroscience</source><year>2019</year><volume>13</volume><elocation-id>1267</elocation-id></element-citation></ref><ref id="R28"><label>[28]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Angrick</surname><given-names>Miguel</given-names></name><name><surname>Ottenhoff</surname><given-names>Maarten C</given-names></name><name><surname>Diener</surname><given-names>Lorenz</given-names></name><name><surname>Ivucic</surname><given-names>Darius</given-names></name><name><surname>Ivucic</surname><given-names>Gabriel</given-names></name><name><surname>Goulis</surname><given-names>Sophocles</given-names></name><name><surname>Saal</surname><given-names>Jeremy</given-names></name><name><surname>Colon</surname><given-names>Albert J</given-names></name><name><surname>Wagner</surname><given-names>Louis</given-names></name><name><surname>Krusienski</surname><given-names>Dean J</given-names></name><name><surname>Kubben</surname><given-names>Pieter L</given-names></name><etal/></person-group><article-title>Real-time synthesis of imagined speech processes from minimally invasive recordings of neural activity</article-title><source>Communications Biology</source><year>2021</year><month>September</month><volume>4</volume><issue>1</issue><fpage>1</fpage><lpage>10</lpage></element-citation></ref><ref id="R29"><label>[29]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Moses</surname><given-names>David A</given-names></name><name><surname>Metzger</surname><given-names>Sean L</given-names></name><name><surname>Liu</surname><given-names>Jessie R</given-names></name><name><surname>Anumanchipalli</surname><given-names>Gopala K</given-names></name><name><surname>Makin</surname><given-names>Joseph G</given-names></name><name><surname>Sun</surname><given-names>Pengfei F</given-names></name><name><surname>Chartier</surname><given-names>Josh</given-names></name><name><surname>Dougherty</surname><given-names>Maximilian E</given-names></name><name><surname>Liu</surname><given-names>Patricia M</given-names></name><name><surname>Abrams</surname><given-names>Gary M</given-names></name><name><surname>Tu-Chan</surname><given-names>Adelyn</given-names></name><etal/></person-group><article-title>Neuroprosthesis for Decoding Speech in a Paralyzed Person with Anarthria</article-title><source>New England Journal of Medicine</source><publisher-name>Massachusetts Medical Society _eprint</publisher-name><year>2021</year><month>July</month><volume>385</volume><issue>3</issue><fpage>217</fpage><lpage>227</lpage><pub-id pub-id-type="doi">10.1056/NEJMoa2027540</pub-id></element-citation></ref><ref id="R30"><label>[30]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sonkusare</surname><given-names>Saurabh</given-names></name><name><surname>Breakspear</surname><given-names>Michael</given-names></name><name><surname>Guo</surname><given-names>Christine</given-names></name></person-group><article-title>Naturalistic stimuli in neuroscience: critically acclaimed</article-title><source>Trends in cognitive sciences</source><year>2019</year><volume>23</volume><issue>8</issue><fpage>699</fpage><lpage>714</lpage></element-citation></ref><ref id="R31"><label>[31]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamilton</surname><given-names>Liberty S</given-names></name><name><surname>Huth</surname><given-names>Alexander G</given-names></name></person-group><article-title>The revolution will not be controlled: natural stimuli in speech neuroscience</article-title><source>Language, cognition and neuroscience</source><year>2020</year><volume>35</volume><issue>5</issue><fpage>573</fpage><lpage>582</lpage></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>The use of <italic>T-Rex</italic> imposes no limitations on the inputs, outputs, behavior, or frameworks used by the researchers for creating the experiments.</title><p><bold>(A)</bold> Represents a set of possible inputs. <bold>(B)</bold><italic>T-Rex</italic> sits in the middle handling the logic that connects the inputs and the outputs. The three main software components of the system are also illustrated (Web interface, Controller, and User configuration). <bold>(C)</bold> Depicts a set of possible outputs. It is worth emphasizing that the inputs and outputs are not limited to those represented.</p></caption><graphic xlink:href="EMS156325-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Representation of the main four windows of the Web interface.</title><p><bold>(A)</bold> The <italic>Home</italic> window contains all the experiments accessible to the user, represented on a grid configuration. <bold>(B)</bold> The <italic>Experiment Feedback</italic> window allows obtaining feedback from the participants about their experience with the experiment. It is achieved through the green <bold>(“All good”)</bold> and the red <bold>(“Not so good”)</bold> button. The participants can only continue after pressing one of these buttons. <bold>(C)</bold> The <italic>Admin Login</italic> window allows access to the administration panel by entering the password. <bold>(D)</bold> The <italic>Admin Configuration</italic> window, where the administrator can create new participants and modify their access to experiments.</p></caption><graphic xlink:href="EMS156325-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><p>Flow of running an experiment. When an experiment is started by pressing the start button on its card, the <monospace>Controller</monospace> is called, loading the main configuration file and extracting the information received from the UI about which experiment to run. Then an <monospace>Experiment</monospace> instance is created, loading the experiment-specific information and completing the setup in three steps. First, it checks for all devices and their LSL streams. Second, it initializes a Recorder instance and adds all streams to the list of streams it should record. Third and last, if a trigger is required for the selected experiment, it will set up a trigger class that searches and connects to the trigger. Once the subprocess call is returned, <monospace>Experiment</monospace> sends the final trigger and stops the Recorder. The data is saved in the <monospace>./output/</monospace> folder, and the user is redirected to the experiment assessment screen (<xref ref-type="fig" rid="F2">Figure 2B</xref>).</p></caption><graphic xlink:href="EMS156325-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>User interfaces for the three Use cases experiments included.</title><p><bold>(A)</bold> Grasping, simple text-based experiment built using the Python package Tkinter (Tk). <bold>(B)</bold> Grasping Web experiment, re-implementation of the Grasping experiment as a Single Page Application (SPA) to allow its execution on any device with access to a web browser. <bold>(C)</bold> 3D hand-tracking experiment, the hand-tracking is performed using the LeapMotion controller, and the experiment is implemented in Python using the package Tkinter (Tk).</p></caption><graphic xlink:href="EMS156325-f004"/></fig></floats-group></article>