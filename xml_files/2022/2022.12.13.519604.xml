<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS158904</article-id><article-id pub-id-type="doi">10.1101/2022.12.13.519604</article-id><article-id pub-id-type="archive">PPR584256</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Neural computations in prosopagnosia</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Faghel-Soubeyrand</surname><given-names>Simon</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Richoz</surname><given-names>Anne-Raphaelle</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Waeber</surname><given-names>Delphine</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Woodhams</surname><given-names>Jessica</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Gosselin</surname><given-names>Frédéric</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Caldara</surname><given-names>Roberto</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Charest</surname><given-names>Ian</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib></contrib-group><aff id="A1"><label>1</label>Département de psychologie, Université de Montréal, Canada</aff><aff id="A2"><label>2</label>School of Psychology and Centre for Human Brain Health, University of Birmingham, Birmingham, UK</aff><aff id="A3"><label>3</label>Départment of Psychology, Université de Fribourg, Switzerland</aff><aff id="A4"><label>4</label>School of Psychology, University of Birmingham, Birmingham, UK</aff><author-notes><corresp id="CR1">Corresponding authors: Prof. Ian Charest, <email>ian.charest.1@umontreal.ca</email> Simon Faghel-Soubeyrand, <email>simon.faghel-soubeyrand@umontreal.ca</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>23</day><month>12</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>13</day><month>12</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nd/4.0/">CC BY-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">We aimed to identify neural computations underlying the loss of face identification ability by modelling the brain activity of brain-lesioned patient PS, a well-documented case of acquired pure prosopagnosia. We collected a large dataset of high-density electrophysiological (EEG) recordings from PS and neurotypicals while they completed a one-back task on a stream of face, object, animal and scene images. We found reduced neural decoding of face identity around the N170 window in PS, and conjointly revealed normal <italic>non-face</italic> identification in this patient. We used Representational Similarity Analysis (RSA) to correlate human EEG representations with those of deep neural network (DNN) models of vision and caption-level semantics, offering a window into the neural computations at play in patient PS’s deficits. Brain representational dissimilarity matrices (RDMs) were computed for each participant at 4 ms steps using cross-validated classifiers. PS’s brain RDMs showed significant reliability across sessions, indicating meaningful measurements of brain representations with RSA even in the presence of significant lesions. Crucially, computational analyses were able to reveal PS’s representational deficits in high-level visual and semantic brain computations. Such multi-modal data-driven characterisations of prosopagnosia highlight the complex nature of processes contributing to face recognition in the human brain.</p></abstract><kwd-group><kwd>prosopagnosia</kwd><kwd>EEG</kwd><kwd>RSA</kwd><kwd>artificial neural networks</kwd><kwd>brain dynamics</kwd><kwd>semantic representations</kwd><kwd>visual representations</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">The human brain is equipped with sophisticated machinery optimised to quickly and effectively recognise faces in a series of computations unfolding within tens of milliseconds. A dramatic contrast to this typically efficient process has been revealed in brain-lesioned patients with an inability to recognise faces, individuals called acquired prosopagnosics (<xref ref-type="bibr" rid="R14">Bodamer, 1947</xref>). Findings from these patients have refined the functional role and the distributed nature of the face-sensitive brain regions in the ventral stream, such as the fusiform gyrus (FFA; <xref ref-type="bibr" rid="R13">Bobes et al., 2003</xref>; <xref ref-type="bibr" rid="R64">Kanwisher et al., 1997</xref>) and the lateral portion of the inferior occipital gyrus (Occipital Face Area, OFA; <xref ref-type="bibr" rid="R35">Dricot et al., 2008</xref>; <xref ref-type="bibr" rid="R47">Gauthier et al., 2000</xref>; <xref ref-type="bibr" rid="R104">Rossion et al., 2003</xref>; <xref ref-type="bibr" rid="R111">Sorger et al., 2007</xref>). This literature has generally contributed to the idea that specialised and category-selective neural modules are necessary for functional aspects of face processing (<xref ref-type="bibr" rid="R27">Cohen et al., 2019</xref>). Brain imaging findings from individuals born with deficits in face recognition (developmental prosopagnosics; (<xref ref-type="bibr" rid="R5">Avidan et al., 2005</xref>; <xref ref-type="bibr" rid="R60">Jiahui et al., 2018</xref>; <xref ref-type="bibr" rid="R63">Kaltwasser et al., 2014</xref>; <xref ref-type="bibr" rid="R83">McConachie, 1976</xref>; <xref ref-type="bibr" rid="R98">Rosenthal et al., 2017</xref>) have revealed finer-grained functional neural differences in the processes (<xref ref-type="bibr" rid="R60">Jiahui et al. 2018</xref>; <xref ref-type="bibr" rid="R98">Rosenthal et al. 2017</xref>; <xref ref-type="bibr" rid="R6">Avidan et al. 2014</xref>; Zhao et al. 2018) associated with deficits in face recognition. Overall, the cumulation of these neuropsychological, neuroanatomical and functional components of prosopagnosia (<xref ref-type="bibr" rid="R16">Busigny et al. 2010</xref>; <xref ref-type="bibr" rid="R35">Dricot et al. 2008</xref>; <xref ref-type="bibr" rid="R101">Rossion 2018</xref>; <xref ref-type="bibr" rid="R104">Rossion et al. 2003</xref>; <xref ref-type="bibr" rid="R36">Duchaine and Nakayama 2006</xref>) has significantly contributed to neural models of face perception in the last two decades (<xref ref-type="bibr" rid="R37">Duchaine &amp; Yovel, 2015</xref>; <xref ref-type="bibr" rid="R53">Haxby et al., 2000</xref>; <xref ref-type="bibr" rid="R121">White &amp; Mike Burton, 2022</xref>). Yet, very little is known on the nature of face representations of those patients (e.g., <xref ref-type="bibr" rid="R17">Caldara et al., 2005</xref>; <xref ref-type="bibr" rid="R43">Fiset et al., 2017</xref>), and next to nothing is known on the nature of brain dynamics and neural computations affected in prosopagnosia. Here, we report an investigation of the neural computations involved in the processing of faces and objects of patient PS, a well-documented case of pure acquired prosopagnosia (<xref ref-type="bibr" rid="R104">Rossion et al., 2003</xref>; <xref ref-type="bibr" rid="R111">Sorger et al., 2007</xref>), using Representational Similarity Analysis (RSA) applied to brain imaging and computational models.</p><p id="P3">Patient PS is a right-handed woman having sustained a closed head injury in 1992, leading to extensive bilateral occipitotemporal lesions encompassing the right OFA, left FFA, and a small region of the right middle temporal gyrus (<xref ref-type="bibr" rid="R35">Dricot et al., 2008</xref>; <xref ref-type="bibr" rid="R111">Sorger et al., 2007</xref>). She is perhaps the most studied case of acquired prosopagnosia, with more than 32 scientific publications in the last 20 years (see <xref ref-type="bibr" rid="R102">Rossion, 2022a</xref>, <xref ref-type="bibr" rid="R103">2022b</xref> for recent reviews on this patient). The particular attention given to this case can be attributed to the relatively focal aspect of her lesions in the face network and the resulting highly specific impairment for face-identification (<xref ref-type="bibr" rid="R16">Busigny et al., 2010</xref>). The neuro-anatomical/functional basis of PS has already been exhaustively reviewed elsewhere (<xref ref-type="bibr" rid="R103">Rossion, 2022b</xref>). Overall, while her condition has been shown to affect a wide array of perceptual mechanisms (e.g. holistic processes, <xref ref-type="bibr" rid="R95">Ramon et al., 2016</xref>; the visual content of face representations in <xref ref-type="bibr" rid="R17">Caldara et al., 2005</xref>; <xref ref-type="bibr" rid="R43">Fiset et al., 2017</xref>; see also <xref ref-type="bibr" rid="R102">Rossion, 2022a</xref>), a direct characterisation of the neural computations behind her deficits has, to the best of our knowledge, never been attempted. A traditional proxy to the nature and level of brain computations affected in this patient, and in prosopagnosia in general, has been to consider the temporal dynamics and face-selectivity of the underlying neural activity. Event-related potential differences occurring late, for example, are generally interpreted as representing higher level processes than those occurring earlier (<xref ref-type="bibr" rid="R2">Alonso Prieto et al., 2011</xref>; <xref ref-type="bibr" rid="R10">Bentin &amp; Deouell, 2000</xref>; <xref ref-type="bibr" rid="R40">Eimer et al., 2012</xref>; <xref ref-type="bibr" rid="R49">Gosling &amp; Eimer, 2011</xref>; <xref ref-type="bibr" rid="R56">Herzmann et al., 2004</xref>; <xref ref-type="bibr" rid="R78">Liu-Shuang et al., 2016a</xref>; <xref ref-type="bibr" rid="R109">Simon et al., 2011</xref>; <xref ref-type="bibr" rid="R113">Tanaka et al., 2006</xref>; <xref ref-type="bibr" rid="R122">Wiese et al., 2019a</xref>). Some associations have been revealed between prosopagnosia and typical neural correlates of face-processing, like the face-sensitive N170 (<xref ref-type="bibr" rid="R9">Bentin et al., 1996</xref>) and face-selective fMRI activation (<xref ref-type="bibr" rid="R115">Towler and Eimer 2012</xref>; <xref ref-type="bibr" rid="R13">Bobes et al. 2003</xref>; <xref ref-type="bibr" rid="R2">Alonso Prieto et al. 2011</xref>; <xref ref-type="bibr" rid="R46">Gao et al. 2019</xref>). Interestingly, however, despite important lesions and behavioural deficits in face-identification, PS still shows typical face-selectivity in spared-regions of the right-hemisphere (i.e. she displays a right FFA; e.g. see <xref ref-type="bibr" rid="R46">Gao et al., 2019</xref>; <xref ref-type="bibr" rid="R104">Rossion et al., 2003</xref>), as well as a typical N170 component in the right, but not left-hemisphere (<xref ref-type="bibr" rid="R2">Alonso Prieto, 2011</xref>; see also <xref ref-type="bibr" rid="R13">Bobes et al., 2003</xref>; <xref ref-type="bibr" rid="R30">Dalrymple et al., 2011</xref>). Similarly, developmental prosopagnosics show typical activation across the “core” (posterior) regions of the face-processing system (OFA and FFA, e.g. <xref ref-type="bibr" rid="R6">Avidan et al., 2014</xref>). More recently, robust experimental techniques such as fast periodic visual stimulation (<xref ref-type="bibr" rid="R79">Liu-Shuang et al., 2016b</xref>) have been able to shed light on the important deficits in neural face-individuation of PS. Characterising the underlying computations of these neural and perceptual processes, however, remains a challenging task. First, describing neural computations is generally arduous due to signal-to-noise ratio (SNR) issues, which is even more concerning when recording brain activity from brain-lesion patients (<xref ref-type="bibr" rid="R78">Liu-Shuang et al., 2016a</xref>). Brain damage, for example, can significantly alter the flow of brain activity compared to typical observers, which can potentially deform event-related potential components (<xref ref-type="bibr" rid="R2">Alonso Prieto et al., 2011</xref>) and require more repetitions of conditions. Second, using solely temporal evidence is limited in itself to reveal brain computations as it only partially indicates the nature of the computations that are relied on by the brain (<xref ref-type="bibr" rid="R76">Lamme &amp; Roelfsema, 2000</xref>; <xref ref-type="bibr" rid="R84">McDermott et al., 2002</xref>). Individuals relying on different neural computations in responses to faces, for example, could have identical activity at a given latency as indexed by univariate event-related potentials.</p><p id="P4">More explicit ways of revealing the nature of brain representations have recently gained traction with techniques associating functional and multivariate brain activity to computational models (<xref ref-type="bibr" rid="R38">Dwivedi et al. 2021</xref>; <xref ref-type="bibr" rid="R31">di Oleggio Castello et al. 2021</xref>; <xref ref-type="bibr" rid="R93">Popham et al. 2021</xref>; <xref ref-type="bibr" rid="R68">Kriegeskorte and Diedrichsen 2016</xref>; <xref ref-type="bibr" rid="R33">Doerig et al. 2022</xref>; <xref ref-type="bibr" rid="R42">Faghel-Soubeyrand et al. 2022</xref>). The aforementioned SNR concerns might explain why most work on prosopagnosia has relied on a limited set of stimuli conditions, block-designs, and univariate methods such as averaging of conditions and subtraction approaches. However, while significantly improving the statistical power of these studies, these approaches have prevented a thorough description of the brain computations underlying prosopagnosia (<xref ref-type="bibr" rid="R44">Friston et al., 2006</xref>). Investigating brain processing using condition-rich designs (<xref ref-type="bibr" rid="R1">Allen et al., 2022</xref>; <xref ref-type="bibr" rid="R22">Charest et al., 2014a</xref>; <xref ref-type="bibr" rid="R70">Kriegeskorte &amp; Kievit, 2013</xref>; <xref ref-type="bibr" rid="R87">Naselaris et al., 2021</xref>), and promoting a broad description of underlying brain mechanisms by testing diverse models on a whole-brain basis (<xref ref-type="bibr" rid="R38">Dwivedi et al., 2021</xref>; Kriegeskorte &amp; Diedrichsen, 2019; <xref ref-type="bibr" rid="R93">Popham et al., 2021</xref>) might provide a more comprehensive understanding of these processes.</p><p id="P5">Here, we take into account these temporal and computational aspects of brain processes by investigating prosopagnosia with fine-grained temporal recordings of brain activity (high-density electroencephalography; EEG), machine learning, and a proven multivariate method, i.e. Representational Similarity Analysis (<xref ref-type="bibr" rid="R72">Kriegeskorte, Mur, &amp; Bandettini, 2008</xref>). We recorded the brain activity of PS and neurotypical controls in responses to images of various categories. Using multivariate pattern analysis (time-resolved “decoding”; <xref ref-type="bibr" rid="R51">Grootswagers et al., 2017</xref>), we probe PS’ neural evidence for face and non-face identity representations. Using RSA, we produce functional brain representations in a format that provides straightforward comparisons between individuals differing in neuroanatomical structure (<xref ref-type="bibr" rid="R48">Golarai et al., 2015</xref>; <xref ref-type="bibr" rid="R92">Popal et al., 2019</xref>). This enabled us to compare human brain representations with those of artificial models characterising different types of computations, i.e. deep neural networks of vision and semantic classification, thereby offering a window into the neural computations at play in patient PS’s deficits.</p></sec><sec id="S2" sec-type="materials | methods"><title>Materials and Procedures</title><sec id="S3"><title>Patient PS and neurotypical participants</title><p id="P6">A total of 20 participants were recruited for this study. The first group consisted of 19 neurotypicals individuals that included 15 healthy controls (9 female, Mage = 22.9 years old) as well as 4 aged-matched (3 female, Mage = 67.5). This sample size was chosen according to the effects described in previous mvpa studies (<xref ref-type="bibr" rid="R20">Carlson et al., 2013</xref>; <xref ref-type="bibr" rid="R25">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="R42">Faghel-Soubeyrand et al., 2022</xref>; <xref ref-type="bibr" rid="R54">Hebart et al., 2018</xref>), as well as previous studies on prosopagnosia (<xref ref-type="bibr" rid="R46">Gao et al., 2019</xref>; <xref ref-type="bibr" rid="R57">Humphreys et al., 2007</xref>; <xref ref-type="bibr" rid="R78">Liu-Shuang et al., 2016a</xref>; <xref ref-type="bibr" rid="R96">Richoz et al., 2015</xref>). Data from 10 of these participants (healthy controls 1-10) have been reported in a previous study (Faghel-Soubeyrand et al., 2022b). One participant from the aged-matched group (aged-matched #2) was rejected due to faulty EEG recordings and poor behavioural performance during the one-back task and CFMT+. This study was approved by the Ethics and Research Committee of the University of Birmingham, The University of Fribourg, and informed consent was obtained from all participants.</p></sec><sec id="S4"><title>PS’s case report</title><p id="P7">Patient PS was born in 1950 and is a <italic>pure</italic> case of acquired prosopagnosia. She was hit by the side mirror of a London’s bus in 1992 while crossing the road. This closed head injury led to major lesions in the left middle fusiform gyrus, where the left Fusiform Face Area (lFFA) is typically located, and in the right inferior occipital gyrus, which typically locates the right Occipital Face Area (rOFA; see <xref ref-type="bibr" rid="R46">Gao et al. 2019</xref> for converging fMRI evidence). Both regions play a critical functional role within the face cortical network (<xref ref-type="bibr" rid="R102">Rossion, 2022a</xref>, <xref ref-type="bibr" rid="R103">2022b</xref>). She also reported minor damages in the right middle temporal gyrus and left posterior cerebellum (for an exhaustive anatomical description and an illustration of her brain damages see, <xref ref-type="bibr" rid="R111">Sorger et al., 2007</xref>, <xref ref-type="fig" rid="F2">Figures 2</xref> and <xref ref-type="fig" rid="F3">3</xref>). Patient PS is a very well-documented and described case of acquired prosopagnosia. She has been extensively studied over the last 20 years, leading to impactful scientific contributions that significantly enriched the theoretical models on human face perception (<xref ref-type="bibr" rid="R99">Rossion, 2008</xref>, <xref ref-type="bibr" rid="R62">2014</xref>; for a complete case report see, <xref ref-type="bibr" rid="R102">Rossion, 2022a</xref>, <xref ref-type="bibr" rid="R103">2022b</xref>; Rossion et al., 2003b).</p><p id="P8">Patient PS recovered remarkably well from initially significant cognitive deficits with the support of medical treatment and neuropsychological rehabilitation. A couple of months after her injury she performed within the normal range at different non-visual tasks for which she was slightly impaired after the accident (e.g., calculation, short and long-term memory, visual imagery). Yet, her fine-grained visual discrimination abilities remained slower compared to controls, and she also presented reduced contrast sensitivity to high spatial frequency information (&gt;22c/degree) and a profound prosopagnosia with massively impaired face recognition abilities (<xref ref-type="bibr" rid="R104">Rossion et al., 2003</xref>). The patient complains of a severe difficulty at recognizing faces, including the ones of close relatives (husband, children, friends), as well as her own face. PS can correctly categorise (and draw) faces as a unique visual object and discriminate faces from other non-face objects or scenes, even when the images are briefly presented (<xref ref-type="bibr" rid="R106">Schiltz et al., 2006</xref>). She shows no difficulty at object recognition, even for subordinate-level discriminations (<xref ref-type="bibr" rid="R104">Rossion et al., 2003</xref>; <xref ref-type="bibr" rid="R106">Schiltz et al., 2006</xref>). Patient PS is perfect at all tests from the Birmingham Object Recognition Battery (<xref ref-type="bibr" rid="R97">BORB – Riddoch &amp; Humphreys, 2022</xref>) showing preserved processing of low-level aspects of visual information (i.e., matching of basic elementary features), intact object matching from different viewpoints, and normal performance for object naming (<xref ref-type="bibr" rid="R104">Rossion et al., 2003</xref>; Table1). Her reading abilities are also well preserved although slightly slowed down, her visual acuity (0.8 bilaterally) is within the normal range, and her visual field almost intact apart from a small left paracentral scotoma. As reported by <xref ref-type="bibr" rid="R104">Rossion et al. (2003)</xref>, she is highly impaired on the Benton Face Matching Test (BFRT - <xref ref-type="bibr" rid="R11">Benton &amp; Van Allen, 1972</xref>) scoring 27/54 (percentile 1). She performs also poorly on the Warrington Recognition Memory Test (WRMT - <xref ref-type="bibr" rid="R120">Warrington &amp; Shallice, 1984</xref>), scoring 18/25 (percentile 3) a performance that characterises her as impaired compared to controls. Over the years, patient PS developed strategies to infer a person’s identity by relying on external cues such as haircut, clothes, beard, glasses, gait, posture, or a person’s voice. Moreover, as revealed by the <italic>Bubbles</italic> response classification technique, patient PS uses suboptimal diagnostic information to recognise familiar faces, relying on the lower part of the face (i.e., the mouth region and external contours) instead of the most informative eye area (<xref ref-type="bibr" rid="R17">Caldara et al., 2005</xref>). A similar bias towards the mouth has been observed for the recognition of static facial expressions (<xref ref-type="bibr" rid="R43">Fiset et al., 2017</xref>) for which she is strongly impaired. Her ability to recognize the dynamic versions of the same facial expressions is nevertheless preserved (<xref ref-type="bibr" rid="R96">Richoz et al., 2015</xref>). Overall, PS is a very cooperative patient with extraordinarily preserved cognitive functions, sensory and motor skills and without any attentional deficits. She therefore represents an exemplary case to investigate the functional models of typical face processing.</p></sec><sec id="S5"><title>Behavioural tasks</title><sec id="S6"><title>One-back task</title><p id="P9">The stimuli used in the main experiment consisted of 49 images of faces, animals (e.g., giraffe, monkey, puppy), plants, objects (e.g., car, computer monitor, flower, banana), and scenes (e.g., city landscape, kitchen, bedroom). The 24 faces (13 identities, 8 males, and 8 neutral, 8 happy, 8 fearful expressions) were taken from the Radboud Face dataset (<xref ref-type="bibr" rid="R77">Langner et al., 2010</xref>). For further details on stimulus processing steps, see <xref ref-type="bibr" rid="R42">Faghel-Soubeyrand et al. (2022)</xref>.</p><p id="P10">These stimuli were presented during a one-back task where we measured high-density electroencephalographic (EEG) activity (<xref ref-type="fig" rid="F1">Figure 1b,c</xref>). Participants performed ~3200 trials in two recording sessions, which were separated by at least one day and by a maximum of two weeks. Participants were asked to press a computer keyboard key on trials where the current image was identical to the previous one (repetitions occurring with a 0.1 probability). They were asked to respond as quickly and accurately as possible. Feedback about accuracy was given on each trial. A trial unravelled as follows: a white fixation point was presented on a grey background for 500 ms (with a jitter of ± 50 ms); followed by a stimulus presented on a grey background for 600 ms; and, finally, by a white fixation point on a grey background for 500 ms. Participants had a maximum of 1,100 ms following stimulus onset to respond. This interval, as well as the 200 ms preceding stimulus onset, constituted the epoch selected for our EEG analyses.</p></sec><sec id="S7"><title>Cambridge Face Memory Test +</title><p id="P11">All participants were administered the CFMT long-form, or CFMT+ (<xref ref-type="bibr" rid="R105">Russell et al., 2009</xref>). In the CFMT +, participants are required to memorise a series of face identities, and to subsequently identify the newly learned faces among three faces. It includes a total of 102 trials of increasing difficulty. The duration of this test is about 15 minutes. EEG was not recorded while participants completed this test.</p></sec></sec><sec id="S8"><title>EEG recording and preprocessing</title><p id="P12">High-density electroencephalographic data was continuously recorded at a sampling rate of 1024 Hz using a 128-channel BioSemi ActiveTwo headset (Biosemi B.V., Amsterdam, Netherlands). Electrodes’ impedance was kept below 20 μV. Data were collected at the University of Fribourg. Data was preprocessed using FieldTrip (<xref ref-type="bibr" rid="R90">Oostenveld et al., 2011</xref>) and in-house Matlab code: continuous raw signal was first re-referenced relative to A1 (Cz), filtered with a band-pass filter [.01-80 Hz], segmented into trial epochs from -200 ms to 1100 ms relative to image onset, and down-sampled at 256 Hz. These EEG recordings were completed during the one-back task only.</p></sec><sec id="S9"><title>Representational Similarity Analysis</title><p id="P13">We compared our participants’ brain representations to those from visual and caption deep neural networks using Representational Similarity Analysis (RSA; (<xref ref-type="bibr" rid="R23">Charest et al., 2014b</xref>; <xref ref-type="bibr" rid="R72">Kriegeskorte, Mur, &amp; Bandettini, 2008</xref>; <xref ref-type="bibr" rid="R72">Kriegeskorte, Mur, Ruff, et al., 2008</xref>; <xref ref-type="bibr" rid="R20">Kriegeskorte &amp; Kievit, 8/2013</xref>).</p><sec id="S10"><title>Brain Representational Dissimilarity Matrices</title><p id="P14">For every participant, we trained a Fisher linear discriminant (5-fold cross-validation, 5 repetitions; <xref ref-type="bibr" rid="R116">Treder, 2020</xref>) to distinguish pairs of stimuli from every 4-ms intervals of EEG response to these stimuli from -200 to 1100 ms after stimulus onset (<xref ref-type="bibr" rid="R24">Cichy&amp;Oliva, 2020</xref>; <xref ref-type="bibr" rid="R50">Graumann et al., 2022</xref>). All 128 channels served as features in these classifiers. Cross-validated area under the curve (AUC) served as pairwise classification dissimilarity metric. By repeating this process for all possible pairs (1176 for our 49 stimuli), we obtained a representational dissimilarity matrix (RDM; see also <xref ref-type="supplementary-material" rid="SD1">Supplementary figure 1a,b</xref>). RDMs are shown for selected time points in <xref ref-type="fig" rid="F1">Figure 1d</xref>.</p></sec><sec id="S11"><title>Visual Convolutional Neural Networks RDMs</title><p id="P15">We used a pre-trained AlexNet (<xref ref-type="bibr" rid="R74">Krizhevsky et al., 2012</xref>) as one model of the visual computations along the ventral stream (<xref ref-type="bibr" rid="R52">Güçlü &amp; van Gerven, 2015</xref>). Our 49 stimuli were input to AlexNet. Layer-wise RDMs were constructed comparing the unit activation patterns for each pair of images using Pearson correlations (see also <xref ref-type="supplementary-material" rid="SD1">Supplementary figure 1c</xref> for a visualisation). This CNN process visual features of gradually higher complexity and abstraction along their layers (<xref ref-type="bibr" rid="R52">Güçlü &amp; van Gerven, 2015</xref>), from low-level (i.e., orientation, edges in shallow layers) to high-level features (e.g., objects and object parts in deeper layers).</p></sec><sec id="S12"><title>Caption-level Semantic RDM</title><p id="P16">To derive a model of an higher level of computations than purely visual processes, we first asked 5 new participants to provide a sentence caption describing each stimulus (e.g., “a city seen from the other side of the forest”) using the Meadows online platform (<ext-link ext-link-type="uri" xlink:href="https://www.meadows-research.com">https://www.meadows-research.com</ext-link>). The sentence captions were fed as input in Google’s universal sentence encoder (GUSE; <xref ref-type="bibr" rid="R21">Cer et al., 2018</xref>) resulting in 512 dimensional sentence embeddings. GUSE has been trained to predict semantic textual similarity from human judgments, and its embeddings generalise to an array of other semantic judgement tasks (<xref ref-type="bibr" rid="R21">Cer et al., 2018</xref>). We then computed the dissimilarities (cosine distances) between the sentence embeddings across all pairs of captions, resulting in a caption-level semantic RDM for each participant. The average RDM was used for further analyses (see also <xref ref-type="supplementary-material" rid="SD1">Supplementary figure 1c</xref> for a visualisation).</p></sec></sec><sec id="S13"><title>Decoding analyses</title><sec id="S14"><title>Face-identity decoding</title><p id="P17">We trained multiclass linear discriminant classifiers to predict 8 face identities (5-fold cross-validation, 5 repetitions; <xref ref-type="bibr" rid="R51">Grootswagers et al., 2017</xref>; <xref ref-type="bibr" rid="R116">Treder, 2020</xref>), using all 128 channels single-trial EEG data as features. A sliding-average with a window of 39 ms was applied to EEG traces prior to training. Separate classifiers were trained on the resulting successive 4-ms EEG time intervals. The classifiers were trained on trials of EEG activity of participants viewing face-identities varying in facial-expressions (fear &amp; joy; ~525 observations per session per participant), similarly to previous studies of face-identity information using EEG signal (<xref ref-type="bibr" rid="R88">Nemrodov et al., 2016</xref>). The time courses of these decoders’ performance were averaged across sessions. Recall (defined as : <italic>true positives</italic> / [<italic>true positives</italic> + <italic>false negatives]</italic>) was used to assess decoding performance.</p></sec><sec id="S15"><title>Non-face identitydecoding</title><p id="P18">PS’s impairments for individuation of visual objects are restrained to faces (<xref ref-type="bibr" rid="R101">Rossion, 2018</xref>, <xref ref-type="bibr" rid="R102">2022a</xref>). As a standard of comparison to the neural face-identification decoders, we trained multiclass linear discriminant classifiers decoders to predict 8 within-category identities from <italic>non-face</italic> categories (i.e. objects, scenes, and animals). These classifiers were trained with identical parameters to the face-identity classifiers. Separate classifiers were trained for each category (e.g. a decoder for 8 identities within the animal category, another for 8 identities within the object category, another for 8 identities within the scene category; ~260 observations per session per participant). The time courses of these decoders’ performance were averaged across all three categories and sessions. Recall was again used to assess decoding performance.</p></sec></sec><sec id="S16"><title>Comparison of brain representations with computational models</title><p id="P19">We compared our participants’ brain RDMs to those from the vision (<xref ref-type="fig" rid="F3">Figure 3a</xref>) and caption-level description (<xref ref-type="fig" rid="F3">Figure 3b</xref>) models described in the previous section using Spearman correlations. Partial correlations were used where mentioned.</p></sec><sec id="S17"><title>Reliability of brain representations across recording days</title><p id="P20">We computed the reliability of brain representations in a similar way as in <xref ref-type="bibr" rid="R22">Charest et al., 2014</xref> (<xref ref-type="bibr" rid="R22">Charest et al., 2014a</xref>). For each participant, two RDMs were computed from two separate recording days, and compared using Spearman correlation in a time-resolved manner. Significance was assessed using permutation testing. Specifically, we created, for each participant and at each 4 ms step, a null distribution of 1000 brain-to-brain correlations using an RDM in which rows and columns indices were randomly shuffled.</p></sec><sec id="S18"><title>Group comparison and inferential statistics</title><p id="P21">All contrasts between PS and neurotypical controls were computed using Crawford-Howell modified t-tests for case-controls comparisons (<xref ref-type="bibr" rid="R28">Crawford &amp; Garthwaite, 2012</xref>; <xref ref-type="bibr" rid="R29">Crawford &amp; Howell, 1998</xref>). All time-resolved contrasts were computed from 0 to 1.1 s after image-onset. Spearman correlations were used for correlations within the control group, i.e. correlation analyses with behavioural performance.</p><p id="P22">Permutation testing was used throughout the paper to assess significance of EEG-RDM to computational model latencies. We created, for each participant and at each 4 ms step, a null distribution of 1000 brain-model correlations using model RDMs in which rows and columns indices were randomly shuffled (<xref ref-type="bibr" rid="R72">Kriegeskorte, Mur, &amp; Bandettini, 2008</xref>).</p><p id="P23">Permutation testing was also used to assess significance of identity decoding latencies. We created, for each participant, session, and at each 4 ms step, a null distribution of 500 decoding performances using identity labels which indices were randomly shuffled. The average of these distributions across sessions and participants were used to assess significance of the time-resolved decoding performance shown in <xref ref-type="fig" rid="F3">figure 3a</xref>.</p></sec></sec><sec id="S19" sec-type="results"><title>Results</title><sec id="S20"><title>One-back task</title><p id="P24">Accuracies did not differ between aged-matched and young controls sub-groups either for face stimuli (t(16) = -0.3099, <italic>p</italic> =.761; t(16) = -0.9607, p =.3510) or non-face stimuli (t(16) = 1.2925, <italic>p</italic> =.215; t(16) = -.09704, p =.3463). Therefore, their data have been aggregated into a single neurotypical control group. PS differed significantly from controls on a face vs. non-face performance score, computed as the first PCA component of face vs. non-face accuracies and response times (t(17) = -7.7157, p = 1.6053x10-6).</p></sec><sec id="S21"><title>Cambridge Face Memory test long-form (CFMT+)</title><p id="P25">Within the control participants, CFMT+ scores did not differ between aged-matched and young controls sub-groups (t(16) = -0.8058, <italic>p</italic> =.4322). Their data have been aggregated into a single control group. PS significantly differed from controls on this standard face identification ability score (t(17)=-2.7623, <italic>p</italic> = 0.0133).</p><p id="P26">To assess the face-specific performance of PS and controls in a single individual score across all behavioural measures, we combined performance in the one-back task (accuracies and response times of face and non-face trials) and face-memory performance (CFMT+) using Principal Component Analysis (PCA). Specifically, face-specific performance in the one-back tasks was computed as a face vs. non-face performance score ([face - nonface]./[face + nonface]) separately for accuracy and RTs, for each participant. We used PCA to extract projections explaining variance across these two variables as well as the CFMT+ (<xref ref-type="bibr" rid="R18">Calder et al., 2001</xref>; <xref ref-type="bibr" rid="R19">Calder &amp; Young, 2005</xref>). The first component, which explained 65.76% of the variance in performance across participants, is henceforth referred to as the face-specific performance score. PS significantly differed from neurotypical controls on this score (t(17) = -7.1966, <italic>p</italic> = 1.0691e-06; see <xref ref-type="fig" rid="F2">Figure 2a</xref>), indicating typical face-specific behavioural deficits in this patient.</p></sec><sec id="S22"><title>Reliability of neural dynamics measured with RSA</title><p id="P27">Measuring brain dynamics of brain-lesioned patients can be arduous using raw electrophysiological topographies (<xref ref-type="bibr" rid="R2">Alonso Prieto et al., 2011</xref>). We assessed whether we could measure reliable brain representations of brain-lesioned PS by computing inter-session reliability of brain Representational Dissimilarity Matrices (RDMs). We computed the correlation between EEG RDMs computed from recording day 1 and recording day 2 at every 4 ms steps. The timecourse of these correlations, shown in <xref ref-type="fig" rid="F2">figure 2a</xref>, indicates significant (<italic>ps</italic>&lt;.05, permutation testing) reliability of brain representations of both neurotypicals and PS across most time points after image onset, peaking in the N170 window (160 ms and 180 ms for controls and PS, respectively; r<sub>peak_ctrls</sub> =.4316; r<sub>peak_PS</sub> =.2638). PS showed surprisingly high SNR across sessions, her correlation time course being indistinguishable from the neurotypicals’ reliability scores across time points (i.e. no significant contrasts).</p></sec><sec id="S23"><title>Impaired neural decoding of face-identity</title><p id="P28">We first asked whether we could capture PS’s face identification deficits at the neural level (<xref ref-type="bibr" rid="R46">Gao et al., 2019</xref>; <xref ref-type="bibr" rid="R78">Liu-Shuang et al., 2016</xref>). For each participant, we decoded face-identity from brain activity by training a multiclass linear discriminant classifier from whole-brain high-density EEG patterns at each 4 ms steps from face onset (see <xref ref-type="fig" rid="F3">Figure 3a</xref>). In neurotypical controls, this resulted in weak but above chance decoding of face identity across time (from 138 ms after face onset, peaking at around 180 ms; <italic>ps</italic>&lt;.05, permutations; see <xref ref-type="fig" rid="F3">Figure 3a</xref>), as can be expected from this difficult classification task (<xref ref-type="bibr" rid="R69">Kriegeskorte et al. 2007</xref>; <xref ref-type="bibr" rid="R117">Tsantani et al. 2021</xref>; <xref ref-type="bibr" rid="R86">Muukkonen et al. 2020</xref>; <xref ref-type="bibr" rid="R32">Dobs et al. 2019</xref>). PS's neural face-identity decoding was continuously at chance level (no significant time points after face onset; <italic>ps</italic> &gt;.05, permutations). Time-resolved contrasts with controls confirmed the reduced neural identity decoding in PS around 200 ms (Howell-Crawford t-tests; <italic>ps</italic>&lt;.05). Neural decoding of <italic>non-face</italic> identities, on the other hand, appeared entirely spared in PS. Non-face identity classifiers resulted in above chance performance in both controls (from 84 ms to 620 ms after non-face image onset, peaking at around 140 ms; <italic>ps</italic>&lt;.05, permutations) <italic>and</italic> PS (from 80 ms to 290 ms, peaking at around 228 ms; <italic>ps</italic>&lt;.05, permutations). PS’s neural individuation for non-face objects was also within the normal range of controls across all time points after face onset (no significant contrasts PS &lt; controls; see <xref ref-type="fig" rid="F3">Figure3b</xref>). Peak individual neural decoding evidence for face identities (<xref ref-type="fig" rid="F3">Figure 3c</xref>) and non-face identities (<xref ref-type="fig" rid="F3">Figure 3d</xref>) across participants demonstrated the same face-specific pattern, with PS positioned at the low end of the spectrum of the neural face individuation scores (<italic>t</italic>(18) =-2.48, <italic>p</italic>&lt;.05), but showing her within typical scores on the neural <italic>non-face</italic> individuation evidence (<italic>t</italic>(18) = -0.73, <italic>p</italic> =.47).</p><p id="P29">We further tested the behavioural relevance of this decoded face identity evidence to the general population. We correlated the magnitude of neural decoding across participants (i.e. the neural individuation score) with behavioural face-identification abilities, as measured by the CFMT+ within our sample. Notably, this was done <italic>with individuals from the control group only</italic>. This showed that, indeed, neural face-individuation score correlated positively with face recognition abilities across neurotypicals (peak-<italic>r</italic> = 0.63, peak-<italic>r</italic><sub>time</sub> = 330 ms; <italic>ps</italic>&lt;.05, ~174-702 ms; <xref ref-type="fig" rid="F3">Figure 3c</xref>; <italic>p</italic>&lt;.05, permutations). Identical brain-behaviour correlation analyses using <italic>non-face</italic> neural individuation scores resulted in much reduced correlations with the CFMT+, with mid and late correlation windows disappearing and the only significant positive correlations appearing in a short window around 100 ms after non-face objects onset (<xref ref-type="fig" rid="F3">Figure 3d;</xref> <italic>p</italic>&lt;.05, permutations). Both the contrasts between PS vs. controls and these interindividual results were replicated using the brain RDMs of our participants and identity model RDMs (<xref ref-type="bibr" rid="R32">Dobs et al., 2019</xref>; see <xref ref-type="supplementary-material" rid="SD1">Supplementary material</xref>).</p><p id="P30">Thus, we were able to capture the poor neural identity representations of PS across time using multivariate EEG signal, showing her reduced neural distance between face identities around 200 ms. This impairment appeared specific to face individuation, and the magnitude of this representational distance, particularly around 330 ms, was predictive of neurotypical face identification abilities.</p><p id="P31">contrasts PS &lt; controls at <italic>p</italic> &lt;.05). Shaded error bar represents standard error within the control group. <bold>b)</bold> PS showed typical neural decoding of non-face identities across all time points, with no significant contrasts with controls. Peak individual neural decoding evidence for face identities <bold>(c)</bold> and non-face identities <bold>(d)</bold> have been ranked across participants, and demonstrated similar effects with PS at the lower end of the spectrum of the neural face individuation scores, but was within typical scores on the neural non-face individuation. <bold>e)</bold> Spearman correlation between individual neural decoding evidence and face recognition abilities (i.e. the CFMT+ scores across neurotypical participants only) was computed at each 4 ms step. Correlation time course and shaded error bar were computed using a jackknife procedure. Significant brain-behaviour correlations (permutation testing at <italic>p</italic>&lt;.05) were found in several time points for the face-identity decoding (between 174 ms and 702 ms, peaking at 330 ms). <bold>f)</bold> In contrast, brain-behaviour correlation with non-face identity decoding and CFMT+ scores were restrained to a small early window peaking at 100 ms (92-100 ms). Each panel shows a scatter plot of CFMT+ and neural decoding computed at peak correlation with corresponding least-square regression line.</p></sec><sec id="S24"><title>Similarity with visual and semantic computational models</title><p id="P32">Next, we characterised the specific neural computations underlying these deficits. We assessed visual brain computations in PS and neurotypical controls by comparing their brain RDMs to those of convolutional neural networks (CNNs) trained to categorise objects (<xref ref-type="bibr" rid="R52">Güçlü &amp; van Gerven, 2015</xref>; <xref ref-type="bibr" rid="R74">Krizhevsky et al., 2012</xref>; <xref ref-type="bibr" rid="R110">Simonyan &amp; Zisserman, 2014</xref>). The visual model RDMs were produced for all 8 layers of the CNN (see <xref ref-type="supplementary-material" rid="SD1">Supplementary figure 1</xref>). Controls showed significant brain-RDM correlations with the sixth layer of the visual CNN around 98 ms (CNN layer 6, <italic>p</italic>&lt;.05, permutations), continuing as late as around 500 ms after image onset (see <xref ref-type="fig" rid="F4">Figure 4a</xref>; CNN layer 3 : 59 - 746 ms; CNN layer 1 : 100 - 242 ms). PS showed significant correlations with the CNN layer 6 in a more restrained window between 153 and 204 ms (layer 1: ns; layer 3: 211 - 215 ms). Direct contrasts of PS’ correlation time courses with those of controls indicated reduced correlations with RDMs across the visual CNN (i.e., at layers 1, 3, 6 &amp; 7; <xref ref-type="fig" rid="F4">Figure 4a</xref>, <italic>p</italic>&lt;.05). These contrasts peaked at layer 6, which represent a higher proportion of high-level visual features (e.g., whole objects and object parts, <xref ref-type="bibr" rid="R52">Güçlü &amp; van Gerven, 2015</xref>; <xref ref-type="bibr" rid="R80">Long et al., 2018</xref>).</p><p id="P33">To reveal whether even higher-level semantic computations (<xref ref-type="bibr" rid="R8">Barton et al., 2009</xref>; <xref ref-type="bibr" rid="R107">Schweinberger &amp; Neumann, 2016</xref>) could be affected in the brain of PS, we used a deep averaging network (Google Universal Sentence Encoder, GUSE; <xref ref-type="bibr" rid="R21">Cer et al., 2018</xref>) to transform human-derived captions of our stimuli (e.g. “ a city seen from the other side of the forest “) into embeddings (points in a caption space). Then, we compared the RDMs computed from this semantic model to the brain RDMs of PS and control participants. Controls showed significant correlations with semantic computations from around 98 ms, continuing as late as 981 ms after image onset, and peaking at 164 ms (peak-<italic>r</italic>ctrls =.45; <italic>p</italic>&lt;.05, permutations, <xref ref-type="fig" rid="F4">Figure 4b</xref>). Again, PS showed significant correlations with this model in a more restrained window from around 176 to 200 ms, and peaking ~20 ms later than controls at 188 ms (peak-<italic>r</italic>PS =.173; <italic>p</italic>&lt;.05, permutations, <xref ref-type="fig" rid="F4">Figure 4a</xref>). Direct contrasts confirmed the reduced correlations with these semantic computations in the brain of PS compared to controls (<xref ref-type="fig" rid="F4">Figure 4b</xref>, <italic>p</italic>&lt;.05). Note that this comparison, as well as the one with the visual model, excluded the information shared between the semantic and visual models. Similar results were observed when comparing brains and the visual and semantic model without removing this shared information between brain and visual/semantic CNN (see <xref ref-type="supplementary-material" rid="SD1">Supplementary figure 3 &amp; Supplementary figure 4</xref>).</p><p id="P34">A summary of significant contrasts with all computational models (<xref ref-type="fig" rid="F4">Figure 4c</xref>) indicates that PS’s brain processing stream exhibited impairments peaking in higher-level visual (CNN layer 6) and semantic (caption-level) representations.</p></sec><sec id="S25"><title>Relationship with electrophysiological brain components</title><p id="P35">We further specified the time at which visual and semantic brain computations differed in PS by producing brain-to-model similarities in time windows corresponding to well-known Event-Related Potential (ERP) components indexing early, mid, and late neural processing, i.e. the P100 (<xref ref-type="bibr" rid="R81">Luck et al., 1990</xref>), N170 (<xref ref-type="bibr" rid="R9">Bentin et al., 1996</xref>) and N400 components (<xref ref-type="bibr" rid="R75">Kutas &amp; Federmeier, 2000</xref>), respectively (<xref ref-type="fig" rid="F5">Figure 5a</xref>). Within neurotypicals, we found EEG representations peaking in similarity with the visual CNN at mid-layers (fourth and fifth; <xref ref-type="bibr" rid="R59">Jiahui et al., 2022</xref>) around mid-level temporal windows. Similarity with semantic computations also peaked around mid-latencies. EEG correlations with the semantic model, however, far surpassed those with the visual CNN at mid and late latencies (paired t-tests comparing brain-semantic vs. brain-visual correlation, <italic>ps</italic> &lt;.05).</p><p id="P36">In contrast to this tendency, PS showed generally lower similarity to semantic computations across all time windows. Direct contrasts of brain-model similarity between controls and PS are shown in <xref ref-type="fig" rid="F5">Figure 5b</xref>, with higher values indicating stronger impairments in PS in visual/semantic computations. We observed reduced semantic brain computations for early and mid-processing windows around the P100 and N170 components (<xref ref-type="bibr" rid="R9">Bentin et al., 1996</xref>; <xref ref-type="bibr" rid="R26">Clark et al., 1994</xref>), respectively, as well as a marginally significant effect in late windows around the N400 component (<italic>p</italic> =.0536; <xref ref-type="bibr" rid="R75">Kutas &amp; Federmeier, 2000</xref>). Contrasts of similarity with the visual CNN layers were overall less pronounced after averaging RDMs in these ERP windows. Similarity to the CNN layer 6 showed the only marginally significant difference between PS and controls around the late processing window corresponding to the N400 (<italic>p</italic> =.0512). More fine-grained temporal contrasts using time windows of 60 ms steps (see <xref ref-type="supplementary-material" rid="SD1">Supplementary figure 5</xref>) confirmed these results.</p><p id="P37">As stated earlier, a predominant assumption in cognitive-neuroscience is that temporally early brain signal refers to low-level computations while later brain signal refers to higher-level computations (e.g. <xref ref-type="bibr" rid="R122">Wiese et al. 2019</xref>; DiCarlo et al. 2012). To assess whether such progression was present in the neural time course of our participants, we computed all pairwise correlations between brain RDMs, CNN-layers RDMs, and semantic RDMs, resulting in a second-level representational similarity matrix (RSM) of 12 x 12 correlation values (<xref ref-type="fig" rid="F6">Figure 6a</xref>). Comparing all RDMs in this manner is similar to the analyses described in the previous section, but allows two additional insights. First, it reveals how similar brain representations computed from different temporal windows are with one another (<xref ref-type="bibr" rid="R67">King &amp; Dehaene, 2014</xref>). In our setting, this showed that neural representations during the N170 are more similar to those during the N400 compared to earlier (P100) computations within controls. Second, this analysis reveals how similar the representations of computational models are along their putative hierarchical computational stream (i.e. layers 1-8 &amp; semantic processes; <xref ref-type="bibr" rid="R52">Güçlü &amp; van Gerven, 2015</xref>). This showed, reassuringly, that the representations of the CNN used here are also generally more similar in layers closer apart within the CNN’s architecture (e.g. layer 6 and 7 are more similar to one another than layer 1 and 7). Finally, both of these levels of analysis <italic>and their interaction</italic> can be summarised by computing the optimal two-dimensional solution of this multidimensional space with multidimensional scaling (MDS). This visual summary is shown in <xref ref-type="fig" rid="F6">Figure 6b</xref>. Because it produces 2D coordinates in a (representational) space as it unfolds across time, producing a path or progression of brain representations, it will be referred to as representational trajectory. In both PS and controls, this analysis confirmed a trajectory from early brain representations, relatively more similar to early layers of the visual CNN, to mid and late brain representations, relatively more similar to the deeper layer of the CNN and semantic model. Some differences, however, are noticeable. First, these second-level RSMs indicated the reduced magnitude of similarity to the models in PS outlined in the previous section, specifically to those of higher-level semantic computations. More importantly, these representational trajectories revealed that PS’ late neural code is both i) more similar to her early (brain) representations around the P100, and ii) more similar to low-level (visual) computations of the CNN. These results indicate that PS shows relatively less transformations in neural computations from early to late stages of her visual processing stream (<xref ref-type="bibr" rid="R24">Cichy &amp; Oliva, 2020</xref>).</p></sec></sec><sec id="S26" sec-type="discussion"><title>Discussion</title><p id="P38">Here, we characterised the neural computations of a well-studied single-case of acquired pure prosopagnosia, patient PS, using whole brain multivariate signal and computational models of vision and caption-level semantics. We were able to capture the impaired functional neural signatures of face individuation — as well as the absence of non-face individuation deficits — in the brain of PS. Most importantly, we show that PS’ inability to identify faces is associated with reduced high-level visual computations, including a significant reduction of even higher-level semantic brain computations. This reduction in semantic computations was present as soon as the P100, peaked around the well-known N170 component, and persisted later around the N400 window. Analyses of brain dynamics revealed that the neural code of patient PS appears to show reduced transformations in neural computations across time, with fewer changes from early to late brain representations.</p><p id="P39">Impairment for face individuation is the archetypal deficit of patient PS and prosopagnosia in general (<xref ref-type="bibr" rid="R14">Bodamer, 1947</xref>), but has been arduous to reveal solely from brain activity (<xref ref-type="bibr" rid="R4">Anzellotti et al. 2014</xref>; <xref ref-type="bibr" rid="R2">Alonso Prieto et al. 2011</xref>). Using a data-driven whole-brain approach and high-density EEG, we extend recent work having been able to reveal such deficits from the neural responses of PS with fast periodic visual stimulation (FPVS; <xref ref-type="bibr" rid="R78">Liu-Shuang et al., 2016</xref>) by describing the temporal unfolding of this deficit under normal passive viewing conditions. We found reduced neural evidence for face identity around the N170 window (<xref ref-type="bibr" rid="R9">Bentin et al., 1996</xref>) in PS, and, conjointly, revealed normal <italic>non-face</italic> individuation in this patient (<xref ref-type="bibr" rid="R35">Dricot et al., 2008</xref>; <xref ref-type="bibr" rid="R78">Liu-Shuang et al., 2016</xref>). We further discovered that variations in <italic>typical</italic> face recognition abilities, as indexed by gold-standard behavioural tests in control participants (<xref ref-type="bibr" rid="R105">Russell et al., 2009</xref>), is associated with this neural decoding of face identities from around the N170 onward, peaking around 350 ms in the N400 window (<xref ref-type="bibr" rid="R61">Johnston et al., 2016</xref>; <xref ref-type="bibr" rid="R107">Schweinberger &amp; Neumann, 2016</xref>; <xref ref-type="bibr" rid="R122">Wiese et al., 2019</xref>). Again, individual neural decoding of non-face individuation did not correlate with neurotypical face-recognition abilities in these time windows but was rather only associated with early neural activity around 100 ms after face onset.</p><p id="P40">Crucially, by associating electrophysiological signal and computational models, we found that the underlying neural computations of PS differed most with respect to the higher-level visual and semantic computations of deep neural networks models (DNNs). The late layers of visual DNNs have been previously linked to processing in human infero-temporal cortex (hIT; <xref ref-type="bibr" rid="R52">Güçlü &amp; van Gerven, 2015</xref>; <xref ref-type="bibr" rid="R59">Jiahui et al., 2022</xref>.; <xref ref-type="bibr" rid="R65">Khaligh-Razavi &amp; Kriegeskorte, 2014</xref>), peaking in the FFA (<xref ref-type="bibr" rid="R65">Khaligh-Razavi &amp; Kriegeskorte, 2014</xref>), and functionally to higher-level visual feature representations such as parts of objects, whole objects and viewpoint invariant representations (<xref ref-type="bibr" rid="R52">Güçlü &amp; van Gerven, 2015</xref>). These observations are consistent with the impaired whole face (<xref ref-type="bibr" rid="R95">Ramon et al., 2016</xref>) and feature representations (<xref ref-type="bibr" rid="R17">Caldara et al., 2005</xref>; <xref ref-type="bibr" rid="R43">Fiset et al., 2017</xref>) previously described in patient PS. Associations between brain activity and higher-level semantic computations have only been attempted much more recently (<xref ref-type="bibr" rid="R38">Dwivedi et al., 2021</xref>; <xref ref-type="bibr" rid="R42">Faghel-Soubeyrand et al., 2022</xref>; <xref ref-type="bibr" rid="R93">Popham et al., 2021</xref>). The computations revealed here are arguably closer to visuo-semantic representations, which have been shown to be represented both within the visual ventral stream (including the FFA and OFA; <xref ref-type="bibr" rid="R33">Doerig, Kietzmann, et al., 2022</xref>; <xref ref-type="bibr" rid="R93">Popham et al., 2021</xref>) and outside (<xref ref-type="bibr" rid="R33">Doerig et al. 2022</xref>). Using a model of caption-level semantics (<xref ref-type="bibr" rid="R21">Cer et al., 2018</xref>; <xref ref-type="bibr" rid="R42">Faghel-Soubeyrand et al., 2022</xref>; <xref ref-type="bibr" rid="R33">Doerig et al. 2022</xref>), we show that PS’s representational geometry displays significant — but greatly reduced — correlation with these semantic computations compared to controls. These findings demonstrate a clear link between semantic brain computations and important changes in the ability to recognise faces (<xref ref-type="bibr" rid="R15">Bruce &amp; Young, 1986</xref>; <xref ref-type="bibr" rid="R37">Duchaine &amp; Yovel, 2015</xref>).</p><p id="P41">Together with similar computational characterisation of brain representations in individuals with “super-recognition” of faces (<xref ref-type="bibr" rid="R42">Faghel-Soubeyrand et al., 2022</xref>; <xref ref-type="bibr" rid="R105">Russell et al., 2009</xref>), our results suggest <italic>some</italic> gradient of neural computations from the low-end to high-end of face-recognition abilities. Indeed, super-recognisers in <xref ref-type="bibr" rid="R42">Faghel-Soubeyrand et al., (2022)</xref> showed <italic>enhanced</italic> similarity with mid-level visual and semantic computations around the N170 and P600 windows, respectively. Here, while PS showed reduced similarity with visual and semantic computations, obvious differences are also apparent. PS’s neural computations, particularly semantic computations, appear to be impacted much sooner and on a larger extent than those of super-recognisers. Whereas semantic brain computations were enhanced around the P600 in super-recognisers, here PS displayed reduced similarity as early as the P100, continuing throughout the N170 and N400 windows. In fact, PS showed no significant correlation with either the visual or semantic-level computations at any point during late processing windows (i.e. after ~300 ms). These observations suggest that PS’s deficits in neural computations start relatively early along the typical processing stream. Some differences, however, are to be expected given the important lesions to the face processing network (i.e. including the right-OFA and left-FFA) of this patient. Indeed, the OFA is not only involved in the extraction of featural information about faces (<xref ref-type="bibr" rid="R91">Pitcher et al., 2007</xref>), but has also been causally linked to higher-level identity (<xref ref-type="bibr" rid="R3">Ambrus et al., 2017</xref>; <xref ref-type="bibr" rid="R62">Jonas et al., 2014</xref>) and semantic information (<xref ref-type="bibr" rid="R39">Eick et al., 2020</xref>) processing. Both these associations and the fact that the Anterior Temporal Lobe (ATL) — an important hub for the processing of semantic information — is preserved in PS (<xref ref-type="bibr" rid="R102">Rossion 2022</xref>; <xref ref-type="bibr" rid="R46">Gao et al. 2019</xref>) points to a similar involvement of the OFA/FFA complex in the semantic neural computations revealed here. Future work using brain imaging with higher spatial resolution will be required to address these questions.</p><p id="P42">While the present study offers insights on the neural dynamics and computations of prosopagnosia, it is also limited in a number of ways. For example, even though we recorded a sizable data set of high-density EEG comprising numerous trials and stimulus conditions, signal-to-noise ratio was still small when probing specific neural processes. Characterising face identity representations across time is a notoriously difficult task (<xref ref-type="bibr" rid="R4">Anzellotti et al., 2014</xref>; <xref ref-type="bibr" rid="R32">Dobs et al., 2019</xref>) that could have benefited from recordings of even higher density imaging (M/EEG in <xref ref-type="bibr" rid="R32">Dobs et al., 2019</xref>; <xref ref-type="bibr" rid="R118">Vida et al., 2017</xref>; fMRI in <xref ref-type="bibr" rid="R89">Nestor et al., 2011</xref>), as well as from the use of experimental tasks recruiting more directly this process (<xref ref-type="bibr" rid="R88">Nemrodov et al., 2016</xref>). Another related and potentially fruitful approach to reveal brain computations and representational geometry in prosopagnosia would be to produce EEG-fMRI “fusion” (<xref ref-type="bibr" rid="R25">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="R24">Cichy &amp; Oliva, 2020</xref>). Indeed, the spatio-temporal description procured by linking EEG <italic>and</italic> fMRI data with RSA would be ideal to test hypotheses regarding transformations in neural code such as the one hinted in our findings. Employing this technique on PS’s EEG and additional fMRI data could reveal the putative hurdles in the transformation of neural code across the different lesions of PS’s, and thereby attribute causal links between OFA/FFA and impairments in brain computations. Finally, comparison of brain representations with computational models in general also has certain limits (<xref ref-type="bibr" rid="R33">Doerig, Sommers, et al., 2022</xref>). Recent advances in developing more interpretable (<xref ref-type="bibr" rid="R108">Schyns et al., 2022</xref>; <xref ref-type="bibr" rid="R112">Soulos &amp; Isik, 2020</xref>) and ecological models (<xref ref-type="bibr" rid="R66">Kietzmann et al., 2019</xref>; <xref ref-type="bibr" rid="R85">Mehrer et al., 2021</xref>) of human cognition will be helpful to address them.</p><p id="P43">Notwithsanding these limitations, this work offers, to our knowledge, the first description of the fine-grained temporal processes combined with a state-of-the-art computational characterisation of the brain representations in prosopagnosia. Understanding the very nature of defective perceptual representations offers new routes for patient rehabilitation. Indeed, we believe that the recent technological advances that permitted us to reveal these findings offer new promising ways to understand, and could perhaps even help diagnose the fine-grained deficits in perception and cognition in diverse clinical populations.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Material</label><media xlink:href="EMS158904-supplement-Supplementary_Material.pdf" mimetype="application" mime-subtype="pdf" id="d87aAdEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S27"><title>Acknowledgements</title><p>We sincerely thank PS for her precious contribution and participation in this study.</p><sec id="S28"><title>Funding</title><p>Funding for this project was supported by an ERC Starting Grant [ERC-StG-759432] to I.C, an ERSC-IAA grant to J.W., I.C. and S.F.S., by a Swiss National Science Foundation grant (10001C_201145) to A.-R.R. and R.C., and by a NSERC and IVADO graduate scholarships to S.F.S.</p></sec></ack><sec id="S29" sec-type="data-availability"><title>Data availability</title><p id="P44">Data are available from the corresponding authors upon request.</p></sec><sec id="S30" sec-type="data-availability"><title>Code availability</title><p id="P45">The MATLAB and Python codes used in this study will be available upon request.</p></sec><fn-group><fn id="FN1" fn-type="con"><p id="P46"><bold>Author contributions</bold></p><p id="P47">(CRediT standardised author statement)</p><p id="P48"><bold>S.F-S.</bold> : conceptualisation, methodology, software, formal analysis, investigation, data curation, writing - original draft, visualisation, supervision, project administration, funding acquisition. <bold>A-R.R.</bold> : project administration, investigation, writing - editing and reviewing. <bold>J.W. :</bold> funding acquisition. <bold>D.W. :</bold> Investigation <bold>R.C. :</bold> resources, project administration, writing - editing and reviewing. <bold>F.G. :</bold> supervision, writing - editing and reviewing. <bold>I.C.</bold> : supervision, methodology, resources, formal analysis, writing - editing and reviewing, project administration, funding acquisition.</p></fn><fn id="FN2" fn-type="conflict"><p id="P49"><bold>Competing interests</bold></p><p id="P50">The authors declare no competing interests.</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>EJ</given-names></name><name><surname>St-Yves</surname><given-names>G</given-names></name><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>Breedlove</surname><given-names>JL</given-names></name><name><surname>Prince</surname><given-names>JS</given-names></name><name><surname>Dowdle</surname><given-names>LT</given-names></name><name><surname>Nau</surname><given-names>M</given-names></name><name><surname>Caron</surname><given-names>B</given-names></name><name><surname>Pestilli</surname><given-names>F</given-names></name><name><surname>Charest</surname><given-names>I</given-names></name><name><surname>Hutchinson</surname><given-names>JB</given-names></name><etal/></person-group><article-title>A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence</article-title><source>Nature Neuroscience</source><year>2022</year><volume>25</volume><issue>1</issue><fpage>116</fpage><lpage>126</lpage><pub-id pub-id-type="pmid">34916659</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alonso Prieto</surname><given-names>E</given-names></name><name><surname>Caharel</surname><given-names>S</given-names></name><name><surname>Henson</surname><given-names>R</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><article-title>Early (N170/M170) Face-Sensitivity Despite Right Lateral Occipital Brain Damage in Acquired Prosopagnosia</article-title><source>Frontiers in Human Neuroscience</source><year>2011</year><volume>5</volume><fpage>138</fpage><pub-id pub-id-type="pmcid">PMC3257870</pub-id><pub-id pub-id-type="pmid">22275889</pub-id><pub-id pub-id-type="doi">10.3389/fnhum.2011.00138</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ambrus</surname><given-names>GG</given-names></name><name><surname>Windel</surname><given-names>F</given-names></name><name><surname>Mike Burton</surname><given-names>A</given-names></name><name><surname>Kovács</surname><given-names>G</given-names></name></person-group><article-title>Causal evidence of the involvement of the right occipital face area in face-identity acquisition</article-title><source>NeuroImage</source><year>2017</year><volume>148</volume><fpage>212</fpage><lpage>218</lpage><pub-id pub-id-type="pmid">28110089</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anzellotti</surname><given-names>S</given-names></name><name><surname>Fairhall</surname><given-names>SL</given-names></name><name><surname>Caramazza</surname><given-names>A</given-names></name></person-group><article-title>Decoding representations of face identity that are tolerant to rotation</article-title><source>Cerebral Cortex</source><year>2014</year><volume>24</volume><issue>8</issue><fpage>1988</fpage><lpage>1995</lpage><pub-id pub-id-type="pmid">23463339</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avidan</surname><given-names>G</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name><name><surname>Behrmann</surname><given-names>M</given-names></name></person-group><article-title>Detailed Exploration of Face-related Processing in Congenital Prosopagnosia: 2. Functional Neuroimaging Findings</article-title><source>Journal of Cognitive Neuroscience</source><year>2005</year><volume>17</volume><issue>7</issue><fpage>1150</fpage><lpage>1167</lpage><pub-id pub-id-type="pmid">16102242</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avidan</surname><given-names>G</given-names></name><name><surname>Tanzer</surname><given-names>M</given-names></name><name><surname>Hadj-Bouziane</surname><given-names>F</given-names></name><name><surname>Liu</surname><given-names>N</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name><name><surname>Behrmann</surname><given-names>M</given-names></name></person-group><article-title>Selective dissociation between core and extended regions of the face processing network in congenital prosopagnosia</article-title><source>Cerebral Cortex</source><year>2014</year><volume>24</volume><issue>6</issue><fpage>1565</fpage><lpage>1578</lpage><pub-id pub-id-type="pmcid">PMC4064011</pub-id><pub-id pub-id-type="pmid">23377287</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bht007</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barton</surname><given-names>JJS</given-names></name><name><surname>Corrow</surname><given-names>SL</given-names></name></person-group><article-title>The problem of being bad at faces</article-title><source>Neuropsychologia</source><year>2016</year><volume>89</volume><fpage>119</fpage><lpage>124</lpage><pub-id pub-id-type="pmcid">PMC4996721</pub-id><pub-id pub-id-type="pmid">27312748</pub-id><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2016.06.008</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barton</surname><given-names>JJS</given-names></name><name><surname>Hanif</surname><given-names>H</given-names></name><name><surname>Ashraf</surname><given-names>S</given-names></name></person-group><article-title>Relating visual to verbal semantic knowledge: the evaluation of object recognition in prosopagnosia</article-title><source>Brain: A Journal of Neurology</source><year>2009</year><volume>132</volume><issue>Pt 12</issue><fpage>3456</fpage><lpage>3466</lpage><pub-id pub-id-type="pmcid">PMC2800384</pub-id><pub-id pub-id-type="pmid">19805494</pub-id><pub-id pub-id-type="doi">10.1093/brain/awp252</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bentin</surname><given-names>S</given-names></name><name><surname>Allison</surname><given-names>T</given-names></name><name><surname>Puce</surname><given-names>A</given-names></name><name><surname>Perez</surname><given-names>E</given-names></name><name><surname>McCarthy</surname><given-names>G</given-names></name></person-group><article-title>Electrophysiological Studies of Face Perception in Humans</article-title><source>Journal of Cognitive Neuroscience</source><year>1996</year><volume>8</volume><issue>6</issue><fpage>551</fpage><lpage>565</lpage><pub-id pub-id-type="pmcid">PMC2927138</pub-id><pub-id pub-id-type="pmid">20740065</pub-id><pub-id pub-id-type="doi">10.1162/jocn.1996.8.6.551</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bentin</surname><given-names>S</given-names></name><name><surname>Deouell</surname><given-names>LY</given-names></name></person-group><article-title>Structural encoding and identification in face processing: erp evidence for separate mechanisms</article-title><source>Cognitive Neuropsychology</source><year>2000</year><volume>17</volume><issue>1</issue><fpage>35</fpage><lpage>55</lpage><pub-id pub-id-type="pmid">20945170</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benton</surname><given-names>AL</given-names></name><name><surname>Van Allen</surname><given-names>MW</given-names></name></person-group><article-title>Prosopagnosia and facial discrimination</article-title><source>Journal of the Neurological Sciences</source><year>1972</year><volume>15</volume><issue>2</issue><fpage>167</fpage><lpage>172</lpage><pub-id pub-id-type="pmid">5010102</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bobak</surname><given-names>AK</given-names></name><name><surname>Parris</surname><given-names>BA</given-names></name><name><surname>Gregory</surname><given-names>NJ</given-names></name><name><surname>Bennetts</surname><given-names>RJ</given-names></name><name><surname>Bate</surname><given-names>S</given-names></name></person-group><article-title>Eye-Movement Strategies in Developmental Prosopagnosia and “Super” Face Recognition</article-title><source>Quarterly Journal of Experimental Psychology</source><year>2017</year><volume>70</volume><issue>2</issue><fpage>201</fpage><lpage>217</lpage><pub-id pub-id-type="pmid">26933872</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bobes</surname><given-names>MA</given-names></name><name><surname>Lopera</surname><given-names>F</given-names></name><name><surname>Garcia</surname><given-names>M</given-names></name><name><surname>Díaz-Comas</surname><given-names>L</given-names></name><name><surname>Galan</surname><given-names>L</given-names></name><name><surname>Valdes-Sosa</surname><given-names>M</given-names></name></person-group><article-title>Covert matching of unfamiliar faces in a case of prosopagnosia: an ERP study</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><year>2003</year><volume>39</volume><issue>1</issue><fpage>41</fpage><lpage>56</lpage><pub-id pub-id-type="pmid">12627752</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bodamer</surname><given-names>J</given-names></name></person-group><article-title>Die Prosop-Agnosie</article-title><source>Archiv für Psychiatrie und Nervenkrankheiten</source><year>1947</year><volume>179</volume><issue>1</issue><fpage>6</fpage><lpage>53</lpage><pub-id pub-id-type="pmid">18901747</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruce</surname><given-names>V</given-names></name><name><surname>Young</surname><given-names>A</given-names></name></person-group><article-title>Understanding face recognition</article-title><source>British Journal of Psychology</source><year>1986</year><volume>77</volume><issue>Pt 3</issue><fpage>305</fpage><lpage>327</lpage><pub-id pub-id-type="pmid">3756376</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Busigny</surname><given-names>T</given-names></name><name><surname>Graf</surname><given-names>M</given-names></name><name><surname>Mayer</surname><given-names>E</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><article-title>Acquired prosopagnosia as a face-specific disorder: Ruling out the general visual similarity account</article-title><source>Neuropsychologia</source><year>2010</year><volume>48</volume><issue>7</issue><fpage>2051</fpage><lpage>2067</lpage><pub-id pub-id-type="pmid">20362595</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caldara</surname><given-names>R</given-names></name><name><surname>Schyns</surname><given-names>P</given-names></name><name><surname>Mayer</surname><given-names>E</given-names></name><name><surname>Smith</surname><given-names>ML</given-names></name><name><surname>Gosselin</surname><given-names>F</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><article-title>Does Prosopagnosia Take the Eyes Out of Face Representations? Evidence for a Defect in Representing Diagnostic Facial Information following Brain Damage</article-title><source>Journal of Cognitive Neuroscience</source><year>2005</year><volume>17</volume><issue>10</issue><fpage>1652</fpage><lpage>1666</lpage><pub-id pub-id-type="pmid">16269103</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calder</surname><given-names>AJ</given-names></name><name><surname>Burton</surname><given-names>AM</given-names></name><name><surname>Miller</surname><given-names>P</given-names></name><name><surname>Young</surname><given-names>AW</given-names></name><name><surname>Akamatsu</surname><given-names>S</given-names></name></person-group><article-title>A principal component analysis of facial expressions</article-title><source>Vision Research</source><year>2001</year><volume>41</volume><issue>9</issue><fpage>1179</fpage><lpage>1208</lpage><pub-id pub-id-type="pmid">11292507</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calder</surname><given-names>AJ</given-names></name><name><surname>Young</surname><given-names>AW</given-names></name></person-group><article-title>Understanding the recognition of facial identity and facial expression</article-title><source>Nature Reviews Neuroscience</source><year>2005</year><volume>6</volume><issue>8</issue><fpage>641</fpage><lpage>651</lpage><pub-id pub-id-type="pmid">16062171</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlson</surname><given-names>TA</given-names></name><name><surname>Tovar</surname><given-names>DA</given-names></name><name><surname>Alink</surname><given-names>A</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Representational dynamics of object vision: The first 1000 ms</article-title><source>Journal of Vision</source><year>2013</year><volume>13</volume><issue>10</issue><comment>1-1</comment><pub-id pub-id-type="pmid">23908380</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cer</surname><given-names>D</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>Kong</surname><given-names>S-Y</given-names></name><name><surname>Hua</surname><given-names>N</given-names></name><name><surname>Limtiaco</surname><given-names>N</given-names><prefix>St</prefix></name><name><surname>John</surname><given-names>R</given-names></name><name><surname>Constant</surname><given-names>N</given-names></name><name><surname>Guajardo-Cespedes</surname><given-names>M</given-names></name><name><surname>Yuan</surname><given-names>S</given-names></name><name><surname>Tar</surname><given-names>C</given-names></name><name><surname>Sung</surname><given-names>Y-H</given-names></name><etal/></person-group><year>2018</year><article-title>Universal Sentence Encoder</article-title><source>arXiv [csCL] arXiv</source><comment><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1803.11175">http://arxiv.org/abs/1803.11175</ext-link></comment></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Charest</surname><given-names>I</given-names></name><name><surname>Kievit</surname><given-names>RA</given-names></name><name><surname>Schmitz</surname><given-names>TW</given-names></name><name><surname>Deca</surname><given-names>D</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Unique semantic space in the brain of each beholder predicts perceived similarity</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2014a</year><volume>111</volume><issue>40</issue><fpage>14565</fpage><lpage>14570</lpage><pub-id pub-id-type="pmcid">PMC4209976</pub-id><pub-id pub-id-type="pmid">25246586</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1402594111</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Charest</surname><given-names>I</given-names></name><name><surname>Kievit</surname><given-names>RA</given-names></name><name><surname>Schmitz</surname><given-names>TW</given-names></name><name><surname>Deca</surname><given-names>D</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Unique semantic space in the brain of each beholder predicts perceived similarity</article-title><source>Proceedings of the National Academy of Sciences</source><year>2014b</year><volume>111</volume><issue>40</issue><fpage>14565</fpage><lpage>14570</lpage></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><article-title>A M/EEG-fMRI Fusion Primer: Resolving Human Brain Responses in Space and Time</article-title><source>Neuron</source><year>2020</year><pub-id pub-id-type="pmcid">PMC7612024</pub-id><pub-id pub-id-type="pmid">32721379</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2020.07.001</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><article-title>Resolving human object recognition in space and time</article-title><source>Nature Neuroscience</source><year>2014</year><volume>17</volume><issue>3</issue><fpage>455</fpage><lpage>462</lpage><pub-id pub-id-type="pmcid">PMC4261693</pub-id><pub-id pub-id-type="pmid">24464044</pub-id><pub-id pub-id-type="doi">10.1038/nn.3635</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>VP</given-names></name><name><surname>Fan</surname><given-names>S</given-names></name><name><surname>Hillyard</surname><given-names>SA</given-names></name></person-group><article-title>Identification of early visual evoked potential generators by retinotopic and topographic analyses</article-title><source>Human Brain Mapping</source><year>1994</year><volume>2</volume><issue>3</issue><fpage>170</fpage><lpage>187</lpage><pub-id pub-id-type="doi">10.1002/hbm.460020306</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>AL</given-names></name><name><surname>Soussand</surname><given-names>L</given-names></name><name><surname>Corrow</surname><given-names>SL</given-names></name><name><surname>Martinaud</surname><given-names>O</given-names></name><name><surname>Barton</surname><given-names>JJS</given-names></name><name><surname>Fox</surname><given-names>MD</given-names></name></person-group><article-title>Looking beyond the face area: lesion network mapping of prosopagnosia</article-title><source>Brain: A Journal of Neurology</source><year>2019</year><volume>142</volume><issue>12</issue><fpage>3975</fpage><lpage>3990</lpage><pub-id pub-id-type="pmcid">PMC6906597</pub-id><pub-id pub-id-type="pmid">31740940</pub-id><pub-id pub-id-type="doi">10.1093/brain/awz332</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crawford</surname><given-names>JR</given-names></name><name><surname>Garthwaite</surname><given-names>PH</given-names></name></person-group><article-title>Single-case research in neuropsychology: a comparison of five forms of t-test for comparing a case to controls</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><year>2012</year><volume>48</volume><issue>8</issue><fpage>1009</fpage><lpage>1016</lpage><pub-id pub-id-type="pmid">21843884</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crawford</surname><given-names>JR</given-names></name><name><surname>Howell</surname><given-names>DC</given-names></name></person-group><article-title>Comparing an Individual’s Test Score Against Norms Derived from Small Samples</article-title><source>The Clinical Neuropsychologist</source><year>1998</year><volume>12</volume><issue>4</issue><fpage>482</fpage><lpage>486</lpage><pub-id pub-id-type="doi">10.1076/clin.12.4.482.7241</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dalrymple</surname><given-names>KA</given-names></name><name><surname>Oruç</surname><given-names>I</given-names></name><name><surname>Duchaine</surname><given-names>B</given-names></name><name><surname>Pancaroglu</surname><given-names>R</given-names></name><name><surname>Fox</surname><given-names>CJ</given-names></name><name><surname>Iaria</surname><given-names>G</given-names></name><name><surname>Handy</surname><given-names>TC</given-names></name><name><surname>Barton</surname><given-names>JJS</given-names></name></person-group><article-title>The anatomic basis of the right face-selective N170 IN acquired prosopagnosia: a combined ERP/fMRI study</article-title><source>Neuropsychologia</source><year>2011</year><volume>49</volume><issue>9</issue><fpage>2553</fpage><lpage>2563</lpage><pub-id pub-id-type="pmid">21601585</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>di Oleggio Castello</surname><given-names>MV</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name><name><surname>Ida Gobbini</surname><given-names>M</given-names></name></person-group><article-title>Shared neural codes for visual and semantic information about familiar faces in a common representational space</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2021</year><volume>118</volume><issue>45</issue><pub-id pub-id-type="pmcid">PMC8609335</pub-id><pub-id pub-id-type="pmid">34732577</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2110474118</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dobs</surname><given-names>K</given-names></name><name><surname>Isik</surname><given-names>L</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><article-title>How face perception unfolds over time</article-title><source>Nature Communications</source><year>2019</year><volume>10</volume><issue>1</issue><elocation-id>1258</elocation-id><pub-id pub-id-type="pmcid">PMC6425020</pub-id><pub-id pub-id-type="pmid">30890707</pub-id><pub-id pub-id-type="doi">10.1038/s41467-019-09239-1</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doerig</surname><given-names>A</given-names></name><name><surname>Kietzmann</surname><given-names>TC</given-names></name><name><surname>Allen</surname><given-names>E</given-names></name><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>Naselaris</surname><given-names>T</given-names></name><name><surname>Kay</surname><given-names>K</given-names></name><name><surname>Charest</surname><given-names>I</given-names></name></person-group><article-title>Semantic scene descriptions as an objective of human vision</article-title><source>arXiv [csCV] arXiv</source><year>2022</year><comment><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2209.11737">http://arxiv.org/abs/2209.11737</ext-link></comment></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doerig</surname><given-names>A</given-names></name><name><surname>Sommers</surname><given-names>R</given-names></name><name><surname>Seeliger</surname><given-names>K</given-names></name><name><surname>Richards</surname><given-names>B</given-names></name><name><surname>Ismael</surname><given-names>J</given-names></name><name><surname>Lindsay</surname><given-names>G</given-names></name><name><surname>Kording</surname><given-names>K</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name><name><surname>Van Gerven</surname><given-names>MAJ</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Kietzmann</surname><given-names>TC</given-names></name></person-group><year>2022</year><article-title>The neuroconnectionist research programme</article-title><source>arXiv [q-bioNC] arXiv</source><comment><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2209.03718">http://arxiv.org/abs/2209.03718</ext-link></comment></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dricot</surname><given-names>L</given-names></name><name><surname>Sorger</surname><given-names>B</given-names></name><name><surname>Schiltz</surname><given-names>C</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><article-title>The roles of “face” and “non-face” areas during individual face perception: Evidence by fMRI adaptation in a brain-damaged prosopagnosic patient</article-title><source>NeuroImage</source><year>2008</year><volume>40</volume><issue>1</issue><fpage>318</fpage><lpage>332</lpage><pub-id pub-id-type="pmid">18164628</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duchaine</surname><given-names>BC</given-names></name><name><surname>Nakayama</surname><given-names>K</given-names></name></person-group><article-title>Developmental prosopagnosia: a window to content-specific face processing</article-title><source>Current Opinion in Neurobiology</source><year>2006</year><volume>16</volume><issue>2</issue><fpage>166</fpage><lpage>173</lpage><pub-id pub-id-type="pmid">16563738</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duchaine</surname><given-names>B</given-names></name><name><surname>Yovel</surname><given-names>G</given-names></name></person-group><article-title>A Revised Neural Framework for Face Processing</article-title><source>Annual Review of Vision Science</source><year>2015</year><volume>1</volume><fpage>393</fpage><lpage>416</lpage><pub-id pub-id-type="pmid">28532371</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dwivedi</surname><given-names>K</given-names></name><name><surname>Bonner</surname><given-names>MF</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Roig</surname><given-names>G</given-names></name></person-group><article-title>Unveiling functions of the visual cortex using task-specific deep neural networks</article-title><source>PLoS Computational Biology</source><year>2021</year><volume>17</volume><issue>8</issue><elocation-id>e1009267</elocation-id><pub-id pub-id-type="pmcid">PMC8407579</pub-id><pub-id pub-id-type="pmid">34388161</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1009267</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eick</surname><given-names>CM</given-names></name><name><surname>Kovács</surname><given-names>G</given-names></name><name><surname>Rostalski</surname><given-names>S-M</given-names></name><name><surname>Röhrig</surname><given-names>L</given-names></name><name><surname>Ambrus</surname><given-names>GG</given-names></name></person-group><article-title>The occipital face area is causally involved in identity-related visual-semantic associations</article-title><source>Brain Structure Function</source><year>2020</year><volume>225</volume><issue>5</issue><fpage>1483</fpage><lpage>1493</lpage><pub-id pub-id-type="pmcid">PMC7286950</pub-id><pub-id pub-id-type="pmid">32342226</pub-id><pub-id pub-id-type="doi">10.1007/s00429-020-02068-9</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eimer</surname><given-names>M</given-names></name><name><surname>Gosling</surname><given-names>A</given-names></name><name><surname>Duchaine</surname><given-names>B</given-names></name></person-group><article-title>Electrophysiological markers of covert face recognition in developmental prosopagnosia</article-title><source>Brain: A Journal of Neurology</source><year>2012</year><volume>135</volume><issue>Pt 2</issue><fpage>542</fpage><lpage>554</lpage><pub-id pub-id-type="pmid">22271660</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faghel-Soubeyrand</surname><given-names>S</given-names></name><name><surname>Alink</surname><given-names>A</given-names></name><name><surname>Bamps</surname><given-names>E</given-names></name><name><surname>Gervais</surname><given-names>R-M</given-names></name><name><surname>Gosselin</surname><given-names>F</given-names></name><name><surname>Charest</surname><given-names>I</given-names></name></person-group><article-title>The two-faces of recognition ability: better face recognizers extract different physical content from left and right sides of face stimuli</article-title><source>Journal of Vision</source><year>2019</year><volume>19</volume><issue>10</issue><comment>136d-136d</comment><pub-id pub-id-type="doi">10.1167/19.10.136d</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faghel-Soubeyrand</surname><given-names>S</given-names></name><name><surname>Ramon</surname><given-names>M</given-names></name><name><surname>Bamps</surname><given-names>E</given-names></name><name><surname>Zoia</surname><given-names>M</given-names></name><name><surname>Woodhams</surname><given-names>J</given-names></name><name><surname>Richoz</surname><given-names>A-R</given-names></name><name><surname>Caldara</surname><given-names>R</given-names></name><name><surname>Gosselin</surname><given-names>F</given-names></name><name><surname>Charest</surname><given-names>I</given-names></name></person-group><article-title>The neural code behind face recognition abilities</article-title><source>bioRxiv</source><year>2022</year><elocation-id>2022.03.19.484245</elocation-id><pub-id pub-id-type="doi">10.1101/2022.03.19.484245</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiset</surname><given-names>D</given-names></name><name><surname>Blais</surname><given-names>C</given-names></name><name><surname>Royer</surname><given-names>J</given-names></name><name><surname>Richoz</surname><given-names>A-R</given-names></name><name><surname>Dugas</surname><given-names>G</given-names></name><name><surname>Caldara</surname><given-names>R</given-names></name></person-group><article-title>Mapping the impairment in decoding static facial expressions of emotion in prosopagnosia</article-title><source>Social Cognitive and Affective Neuroscience</source><year>2017</year><volume>12</volume><issue>8</issue><fpage>1334</fpage><lpage>1341</lpage><pub-id pub-id-type="pmcid">PMC5597863</pub-id><pub-id pub-id-type="pmid">28459990</pub-id><pub-id pub-id-type="doi">10.1093/scan/nsx068</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Rotshtein</surname><given-names>P</given-names></name><name><surname>Geng</surname><given-names>JJ</given-names></name><name><surname>Sterzer</surname><given-names>P</given-names></name><name><surname>Henson</surname><given-names>RN</given-names></name></person-group><article-title>A critique of functional localisers</article-title><source>NeuroImage</source><year>2006</year><volume>30</volume><issue>4</issue><fpage>1077</fpage><lpage>1087</lpage><pub-id pub-id-type="pmid">16635579</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fysh</surname><given-names>MC</given-names></name><name><surname>Stacchi</surname><given-names>L</given-names></name><name><surname>Ramon</surname><given-names>M</given-names></name></person-group><article-title>Differences between and within individuals, and subprocesses of face cognition: implications for theory, research and personnel selection</article-title><source>Royal Society Open Science</source><year>2020</year><volume>7</volume><issue>9</issue><elocation-id>200233</elocation-id><pub-id pub-id-type="pmcid">PMC7540753</pub-id><pub-id pub-id-type="pmid">33047013</pub-id><pub-id pub-id-type="doi">10.1098/rsos.200233</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>X</given-names></name><name><surname>Vuong</surname><given-names>QC</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><article-title>The cortical face network of the prosopagnosic patient PS with fast periodic stimulation in fMRI</article-title><source>Cortex</source><year>2019</year><volume>119</volume><fpage>528</fpage><lpage>542</lpage><pub-id pub-id-type="pmid">30545601</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gauthier</surname><given-names>I</given-names></name><name><surname>Tarr</surname><given-names>MJ</given-names></name><name><surname>Moylan</surname><given-names>J</given-names></name><name><surname>Skudlarski</surname><given-names>P</given-names></name><name><surname>Gore</surname><given-names>JC</given-names></name><name><surname>Anderson</surname><given-names>AW</given-names></name></person-group><article-title>The fusiform “face area” is part of a network that processes faces at the individual level</article-title><source>Journal of Cognitive Neuroscience</source><year>2000</year><volume>12</volume><issue>3</issue><fpage>495</fpage><lpage>504</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://direct.mit.edu/jocn/article-abstract/12/3/495/3437">https://direct.mit.edu/jocn/article-abstract/12/3/495/3437</ext-link></comment><pub-id pub-id-type="pmid">10931774</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Golarai</surname><given-names>G</given-names></name><name><surname>Liberman</surname><given-names>A</given-names></name><name><surname>Grill-Spector</surname><given-names>K</given-names></name></person-group><article-title>Experience Shapes the Development of Neural Substrates of Face Processing in Human Ventral Temporal Cortex</article-title><source>Cerebral Cortex</source><year>2015</year><volume>27</volume><issue>2</issue><pub-id pub-id-type="pmcid">PMC6161183</pub-id><pub-id pub-id-type="pmid">26683171</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhv314</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gosling</surname><given-names>A</given-names></name><name><surname>Eimer</surname><given-names>M</given-names></name></person-group><article-title>An event-related brain potential study of explicit face recognition</article-title><source>Neuropsychologia</source><year>2011</year><volume>49</volume><issue>9</issue><fpage>2736</fpage><lpage>2745</lpage><pub-id pub-id-type="pmid">21679721</pub-id></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graumann</surname><given-names>M</given-names></name><name><surname>Ciuffi</surname><given-names>C</given-names></name><name><surname>Dwivedi</surname><given-names>K</given-names></name><name><surname>Roig</surname><given-names>G</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><article-title>The spatiotemporal neural dynamics of object location representations in the human brain</article-title><source>Nature Human Behaviour</source><year>2022</year><pub-id pub-id-type="pmcid">PMC9225954</pub-id><pub-id pub-id-type="pmid">35210593</pub-id><pub-id pub-id-type="doi">10.1038/s41562-022-01302-0</pub-id></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grootswagers</surname><given-names>T</given-names></name><name><surname>Wardle</surname><given-names>SG</given-names></name><name><surname>Carlson</surname><given-names>TA</given-names></name></person-group><article-title>Decoding Dynamic Brain Patterns from Evoked Responses: A Tutorial on Multivariate Pattern Analysis Applied to Time Series Neuroimaging Data</article-title><source>Journal of Cognitive Neuroscience</source><year>2017</year><volume>29</volume><issue>4</issue><fpage>677</fpage><lpage>697</lpage><pub-id pub-id-type="pmid">27779910</pub-id></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Güçlü</surname><given-names>U</given-names></name><name><surname>van Gerven</surname><given-names>MAJ</given-names></name></person-group><article-title>Deep Neural Networks Reveal a Gradient in the Complexity of Neural Representations across the Ventral Stream</article-title><source>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</source><year>2015</year><volume>35</volume><issue>27</issue><fpage>10005</fpage><lpage>10014</lpage><pub-id pub-id-type="pmcid">PMC6605414</pub-id><pub-id pub-id-type="pmid">26157000</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5023-14.2015</pub-id></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname><given-names>JV</given-names></name><name><surname>Hoffman</surname><given-names>EA</given-names></name><name><surname>Gobbini</surname><given-names>MI</given-names></name></person-group><article-title>The distributed human neural system for face perception</article-title><source>Trends in Cognitive Sciences</source><year>2000</year><volume>4</volume><issue>6</issue><fpage>223</fpage><lpage>233</lpage><pub-id pub-id-type="pmid">10827445</pub-id></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Bankson</surname><given-names>BB</given-names></name><name><surname>Harel</surname><given-names>A</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><article-title>The representational dynamics of task and object processing in humans</article-title><source>eLife</source><year>2018</year><volume>7</volume><pub-id pub-id-type="pmcid">PMC5811210</pub-id><pub-id pub-id-type="pmid">29384473</pub-id><pub-id pub-id-type="doi">10.7554/eLife.32816</pub-id></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hendel</surname><given-names>RK</given-names></name><name><surname>Starrfelt</surname><given-names>R</given-names></name><name><surname>Gerlach</surname><given-names>C</given-names></name></person-group><article-title>The good, the bad, and the average: Characterizing the relationship between face and object processing across the face recognition spectrum</article-title><source>Neuropsychologia</source><year>2019</year><volume>124</volume><fpage>274</fpage><lpage>284</lpage><pub-id pub-id-type="pmid">30529245</pub-id></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herzmann</surname><given-names>G</given-names></name><name><surname>Schweinberger</surname><given-names>SR</given-names></name><name><surname>Sommer</surname><given-names>W</given-names></name><name><surname>Jentzsch</surname><given-names>I</given-names></name></person-group><article-title>What’s special about personally familiar faces? A multimodal approach</article-title><source>Psychophysiology</source><year>2004</year><volume>41</volume><issue>5</issue><fpage>688</fpage><lpage>701</lpage><pub-id pub-id-type="pmid">15318875</pub-id></element-citation></ref><ref id="R57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Humphreys</surname><given-names>K</given-names></name><name><surname>Avidan</surname><given-names>G</given-names></name><name><surname>Behrmann</surname><given-names>M</given-names></name></person-group><article-title>A detailed investigation of facial expression processing in congenital prosopagnosia as compared to acquired prosopagnosia. Experimental Brain Research. Experimentelle Hirnforschung</article-title><source>Experimentation Cerebrale</source><year>2007</year><volume>176</volume><issue>2</issue><fpage>356</fpage><lpage>373</lpage><pub-id pub-id-type="pmid">16917773</pub-id></element-citation></ref><ref id="R58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiahui</surname><given-names>G</given-names></name><name><surname>Feilong</surname><given-names>M</given-names></name><name><surname>di Oleggio Castello</surname><given-names>MV</given-names></name><name><surname>Nastase</surname><given-names>SA</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name><name><surname>Ida Gobbini</surname><given-names>M</given-names></name></person-group><article-title>Modeling naturalistic face processing in humans with deep convolutional neural networks</article-title><pub-id pub-id-type="doi">10.1101/2021.11.17.469009</pub-id></element-citation></ref><ref id="R59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiahui</surname><given-names>G</given-names></name><name><surname>Feilong</surname><given-names>M</given-names></name><name><surname>di Oleggio Castello</surname><given-names>MV</given-names></name><name><surname>Nastase</surname><given-names>SA</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name><name><surname>Ida Gobbini</surname><given-names>M</given-names></name></person-group><article-title>Modeling naturalistic face processing in humans with deep convolutional neural networks</article-title><source>bioRxiv</source><year>2022</year><elocation-id>2021.11.17.469009</elocation-id><pub-id pub-id-type="doi">10.1101/2021.11.17.469009</pub-id></element-citation></ref><ref id="R60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiahui</surname><given-names>G</given-names></name><name><surname>Yang</surname><given-names>H</given-names></name><name><surname>Duchaine</surname><given-names>B</given-names></name></person-group><article-title>Developmental prosopagnosics have widespread selectivity reductions across category-selective visual cortex</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2018</year><volume>115</volume><issue>28</issue><fpage>E6418</fpage><lpage>E6427</lpage><pub-id pub-id-type="pmcid">PMC6048498</pub-id><pub-id pub-id-type="pmid">29941554</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1802246115</pub-id></element-citation></ref><ref id="R61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnston</surname><given-names>P</given-names></name><name><surname>Overell</surname><given-names>A</given-names></name><name><surname>Kaufman</surname><given-names>J</given-names></name><name><surname>Robinson</surname><given-names>J</given-names></name><name><surname>Young</surname><given-names>AW</given-names></name></person-group><article-title>Expectations about person identity modulate the face-sensitive N170</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><year>2016</year><volume>85</volume><fpage>54</fpage><lpage>64</lpage><pub-id pub-id-type="pmid">27837657</pub-id></element-citation></ref><ref id="R62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jonas</surname><given-names>J</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name><name><surname>Krieg</surname><given-names>J</given-names></name><name><surname>Koessler</surname><given-names>L</given-names></name><name><surname>Colnat-Coulbois</surname><given-names>S</given-names></name><name><surname>Vespignani</surname><given-names>H</given-names></name><name><surname>Jacques</surname><given-names>C</given-names></name><name><surname>Vignal</surname><given-names>J-P</given-names></name><name><surname>Brissart</surname><given-names>H</given-names></name><name><surname>Maillard</surname><given-names>L</given-names></name></person-group><article-title>Intracerebral electrical stimulation of a face-selective area in the right inferior occipital cortex impairs individual face discrimination</article-title><source>NeuroImage</source><year>2014</year><volume>99</volume><fpage>487</fpage><lpage>497</lpage><pub-id pub-id-type="pmid">24936686</pub-id></element-citation></ref><ref id="R63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaltwasser</surname><given-names>L</given-names></name><name><surname>Hildebrandt</surname><given-names>A</given-names></name><name><surname>Recio</surname><given-names>G</given-names></name><name><surname>Wilhelm</surname><given-names>O</given-names></name><name><surname>Sommer</surname><given-names>W</given-names></name></person-group><article-title>Neurocognitive mechanisms of individual differences in face cognition: a replication and extension</article-title><source>Cognitive, Affective Behavioral Neuroscience</source><year>2014</year><volume>14</volume><issue>2</issue><fpage>861</fpage><lpage>878</lpage><pub-id pub-id-type="pmid">24379165</pub-id></element-citation></ref><ref id="R64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>McDermott</surname><given-names>J</given-names></name><name><surname>Chun</surname><given-names>MM</given-names></name></person-group><article-title>The fusiform face area: a module in human extrastriate cortex specialized for face perception</article-title><source>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</source><year>1997</year><volume>17</volume><issue>11</issue><fpage>4302</fpage><lpage>4311</lpage><pub-id pub-id-type="pmcid">PMC6573547</pub-id><pub-id pub-id-type="pmid">9151747</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.17-11-04302.1997</pub-id></element-citation></ref><ref id="R65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khaligh-Razavi</surname><given-names>S-M</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Deep supervised, but not unsupervised, models may explain IT cortical representation</article-title><source>PLoS Computational Biology</source><year>2014</year><volume>10</volume><issue>11</issue><elocation-id>e1003915</elocation-id><pub-id pub-id-type="pmcid">PMC4222664</pub-id><pub-id pub-id-type="pmid">25375136</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003915</pub-id></element-citation></ref><ref id="R66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kietzmann</surname><given-names>TC</given-names></name><name><surname>Spoerer</surname><given-names>CJ</given-names></name><name><surname>Sörensen</surname><given-names>LKA</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Hauk</surname><given-names>O</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Recurrence is required to capture the representational dynamics of the human visual system</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2019</year><volume>116</volume><issue>43</issue><fpage>21854</fpage><lpage>21863</lpage><pub-id pub-id-type="pmcid">PMC6815174</pub-id><pub-id pub-id-type="pmid">31591217</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1905544116</pub-id></element-citation></ref><ref id="R67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname><given-names>J-R</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><article-title>Characterizing the dynamics of mental representations: the temporal generalization method</article-title><source>Trends in Cognitive Sciences</source><year>2014</year><volume>18</volume><issue>4</issue><fpage>203</fpage><lpage>210</lpage><pub-id pub-id-type="pmcid">PMC5635958</pub-id><pub-id pub-id-type="pmid">24593982</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2014.01.002</pub-id></element-citation></ref><ref id="R68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Diedrichsen</surname><given-names>J</given-names></name></person-group><article-title>Inferring brain-computational mechanisms with models of activity measurements</article-title><source>Philosophical Transactions of the Royal Society of London, Series B, Biological Sciences</source><year>2016</year><volume>371</volume><issue>1705</issue><pub-id pub-id-type="pmcid">PMC5003864</pub-id><pub-id pub-id-type="pmid">27574316</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2016.0278</pub-id></element-citation></ref><ref id="R69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Formisano</surname><given-names>E</given-names></name><name><surname>Sorger</surname><given-names>B</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name></person-group><article-title>Individual faces elicit distinct response patterns in human anterior temporal cortex</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2007</year><volume>104</volume><issue>51</issue><fpage>20600</fpage><lpage>20605</lpage><pub-id pub-id-type="pmcid">PMC2154477</pub-id><pub-id pub-id-type="pmid">18077383</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0705654104</pub-id></element-citation></ref><ref id="R70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Kievit</surname><given-names>RA</given-names></name></person-group><article-title>Representational geometry: integrating cognition, computation, and the brain</article-title><source>Trends in Cognitive Sciences</source><year>2013</year><day>8</day><volume>17</volume><issue>8</issue><fpage>401</fpage><lpage>412</lpage><pub-id pub-id-type="pmcid">PMC3730178</pub-id><pub-id pub-id-type="pmid">23876494</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2013.06.007</pub-id></element-citation></ref><ref id="R71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Kievit</surname><given-names>RA</given-names></name></person-group><article-title>Representational geometry: integrating cognition, computation, and the brain</article-title><source>Trends in Cognitive Sciences</source><year>2013</year><volume>17</volume><issue>8</issue><fpage>401</fpage><lpage>412</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2013.06.007</pub-id></element-citation></ref><ref id="R72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Bandettini</surname><given-names>P</given-names></name></person-group><article-title>Representational similarity analysis-connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><year>2008</year><volume>2</volume><fpage>4</fpage><pub-id pub-id-type="pmcid">PMC2605405</pub-id><pub-id pub-id-type="pmid">19104670</pub-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id></element-citation></ref><ref id="R73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Ruff</surname><given-names>DA</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Bodurka</surname><given-names>J</given-names></name><name><surname>Esteky</surname><given-names>H</given-names></name><name><surname>Tanaka</surname><given-names>K</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name></person-group><article-title>Matching categorical object representations in inferior temporal cortex of man and monkey</article-title><source>Neuron</source><year>2008</year><volume>60</volume><issue>6</issue><fpage>1126</fpage><lpage>1141</lpage><pub-id pub-id-type="pmcid">PMC3143574</pub-id><pub-id pub-id-type="pmid">19109916</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2008.10.043</pub-id></element-citation></ref><ref id="R74"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><chapter-title>ImageNet Classification with Deep Convolutional Neural Networks</chapter-title><person-group person-group-type="editor"><name><surname>Pereira</surname><given-names>F</given-names></name><name><surname>Burges</surname><given-names>CJC</given-names></name><name><surname>Bottou</surname><given-names>L</given-names></name><name><surname>Weinberger</surname><given-names>KQ</given-names></name></person-group><source>Advances in Neural Information Processing Systems</source><publisher-name>Curran Associates, Inc</publisher-name><year>2012</year><volume>25</volume><fpage>1097</fpage><lpage>1105</lpage><comment><ext-link ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</ext-link></comment></element-citation></ref><ref id="R75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kutas</surname><given-names>M</given-names></name><name><surname>Federmeier</surname><given-names>KD</given-names></name></person-group><article-title>Electrophysiology reveals semantic memory use in language comprehension</article-title><source>Trends in Cognitive Sciences</source><year>2000</year><volume>4</volume><issue>12</issue><fpage>463</fpage><lpage>470</lpage><pub-id pub-id-type="pmid">11115760</pub-id></element-citation></ref><ref id="R76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lamme</surname><given-names>VA</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><article-title>The distinct modes of vision offered by feedforward and recurrent processing</article-title><source>Trends in Neurosciences</source><year>2000</year><volume>23</volume><issue>11</issue><fpage>571</fpage><lpage>579</lpage><pub-id pub-id-type="pmid">11074267</pub-id></element-citation></ref><ref id="R77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langner</surname><given-names>O</given-names></name><name><surname>Dotsch</surname><given-names>R</given-names></name><name><surname>Bijlstra</surname><given-names>G</given-names></name><name><surname>Wigboldus</surname><given-names>DHJ</given-names></name><name><surname>Hawk</surname><given-names>ST</given-names></name><name><surname>van Knippenberg</surname><given-names>A</given-names></name></person-group><article-title>Presentation and validation of the Radboud Faces Database</article-title><source>Cognition and Emotion</source><year>2010</year><volume>24</volume><issue>8</issue><fpage>1377</fpage><lpage>1388</lpage><pub-id pub-id-type="doi">10.1080/02699930903485076</pub-id></element-citation></ref><ref id="R78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu-Shuang</surname><given-names>J</given-names></name><name><surname>Torfs</surname><given-names>K</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><article-title>An objective electrophysiological marker of face individualisation impairment in acquired prosopagnosia with fast periodic visual stimulation</article-title><source>Neuropsychologia</source><year>2016a</year><volume>83</volume><fpage>100</fpage><lpage>113</lpage><pub-id pub-id-type="pmid">26318239</pub-id></element-citation></ref><ref id="R79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu-Shuang</surname><given-names>J</given-names></name><name><surname>Torfs</surname><given-names>K</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><article-title>An objective electrophysiological marker of face individualisation impairment in acquired prosopagnosia with fast periodic visual stimulation</article-title><source>Neuropsychologia</source><year>2016b</year><volume>83</volume><fpage>100</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2015.08.023</pub-id></element-citation></ref><ref id="R80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Long</surname><given-names>B</given-names></name><name><surname>Yu</surname><given-names>C-P</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name></person-group><article-title>Mid-level visual features underlie the high-level categorical organization of the ventral stream</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2018</year><volume>115</volume><issue>38</issue><fpage>E9015</fpage><lpage>E9024</lpage><pub-id pub-id-type="pmcid">PMC6156638</pub-id><pub-id pub-id-type="pmid">30171168</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1719616115</pub-id></element-citation></ref><ref id="R81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luck</surname><given-names>SJ</given-names></name><name><surname>Heinze</surname><given-names>HJ</given-names></name><name><surname>Mangun</surname><given-names>GR</given-names></name><name><surname>Hillyard</surname><given-names>SA</given-names></name></person-group><article-title>Visual event-related potentials index focused attention within bilateral stimulus arrays. II. Functional dissociation of P1 and N1 components</article-title><source>Electroencephalography and Clinical Neurophysiology</source><year>1990</year><volume>75</volume><issue>6</issue><fpage>528</fpage><lpage>542</lpage><pub-id pub-id-type="pmid">1693897</pub-id></element-citation></ref><ref id="R82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maguire</surname><given-names>EA</given-names></name><name><surname>Valentine</surname><given-names>ER</given-names></name><name><surname>Wilding</surname><given-names>JM</given-names></name><name><surname>Kapur</surname><given-names>N</given-names></name></person-group><article-title>Routes to remembering: the brains behind superior memory</article-title><source>Nature Neuroscience</source><year>2003</year><volume>6</volume><issue>1</issue><fpage>90</fpage><lpage>95</lpage><pub-id pub-id-type="pmid">12483214</pub-id></element-citation></ref><ref id="R83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McConachie</surname><given-names>HR</given-names></name></person-group><article-title>Developmental prosopagnosia. A single case report</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><year>1976</year><volume>12</volume><issue>1</issue><fpage>76</fpage><lpage>82</lpage><pub-id pub-id-type="pmid">1261287</pub-id></element-citation></ref><ref id="R84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McDermott</surname><given-names>J</given-names></name><name><surname>Schiller</surname><given-names>PH</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><article-title>Spatial frequency and orientation tuning dynamics in area V1</article-title><source>Proceedings of the</source><year>2002</year><comment><ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/content/99/3/1645.short">https://www.pnas.org/content/99/3/1645.short</ext-link></comment><pub-id pub-id-type="pmcid">PMC122244</pub-id><pub-id pub-id-type="pmid">11818532</pub-id><pub-id pub-id-type="doi">10.1073/pnas.022638499</pub-id></element-citation></ref><ref id="R85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehrer</surname><given-names>J</given-names></name><name><surname>Spoerer</surname><given-names>CJ</given-names></name><name><surname>Jones</surname><given-names>EC</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Kietzmann</surname><given-names>TC</given-names></name></person-group><article-title>An ecologically motivated image dataset for deep learning yields better models of human vision</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2021</year><volume>118</volume><issue>8</issue><pub-id pub-id-type="pmcid">PMC7923360</pub-id><pub-id pub-id-type="pmid">33593900</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2011417118</pub-id></element-citation></ref><ref id="R86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muukkonen</surname><given-names>I</given-names></name><name><surname>Ölander</surname><given-names>K</given-names></name><name><surname>Numminen</surname><given-names>J</given-names></name><name><surname>Salmela</surname><given-names>VR</given-names></name></person-group><article-title>Spatio-temporal dynamics of face perception</article-title><source>NeuroImage</source><year>2020</year><volume>209</volume><elocation-id>116531</elocation-id><pub-id pub-id-type="pmid">31931156</pub-id></element-citation></ref><ref id="R87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naselaris</surname><given-names>T</given-names></name><name><surname>Allen</surname><given-names>E</given-names></name><name><surname>Kay</surname><given-names>K</given-names></name></person-group><article-title>Extensive sampling for complete models of individual brains</article-title><source>Current Opinion in Behavioral Sciences</source><year>2021</year><volume>40</volume><fpage>45</fpage><lpage>51</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2020.12.008</pub-id></element-citation></ref><ref id="R88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nemrodov</surname><given-names>D</given-names></name><name><surname>Niemeier</surname><given-names>M</given-names></name><name><surname>Mok</surname><given-names>JNY</given-names></name><name><surname>Nestor</surname><given-names>A</given-names></name></person-group><article-title>The time course of individual face recognition: A pattern analysis of ERP signals</article-title><source>NeuroImage</source><year>2016</year><volume>132</volume><fpage>469</fpage><lpage>476</lpage><pub-id pub-id-type="pmid">26973169</pub-id></element-citation></ref><ref id="R89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nestor</surname><given-names>A</given-names></name><name><surname>Plaut</surname><given-names>DC</given-names></name><name><surname>Behrmann</surname><given-names>M</given-names></name></person-group><article-title>Unraveling the distributed neural code of facial identity through spatiotemporal pattern analysis</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2011</year><volume>108</volume><issue>24</issue><fpage>9998</fpage><lpage>10003</lpage><pub-id pub-id-type="pmcid">PMC3116398</pub-id><pub-id pub-id-type="pmid">21628569</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1102433108</pub-id></element-citation></ref><ref id="R90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Schoffelen</surname><given-names>J-M</given-names></name></person-group><article-title>FieldTrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Computational Intelligence and Neuroscience</source><year>2011</year><volume>2011</volume><elocation-id>156869</elocation-id><pub-id pub-id-type="pmcid">PMC3021840</pub-id><pub-id pub-id-type="pmid">21253357</pub-id><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id></element-citation></ref><ref id="R91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitcher</surname><given-names>D</given-names></name><name><surname>Walsh</surname><given-names>V</given-names></name><name><surname>Yovel</surname><given-names>G</given-names></name><name><surname>Duchaine</surname><given-names>B</given-names></name></person-group><article-title>TMS evidence for the involvement of the right occipital face area in early face processing</article-title><source>Current Biology: CB</source><year>2007</year><volume>17</volume><issue>18</issue><fpage>1568</fpage><lpage>1573</lpage><pub-id pub-id-type="pmid">17764942</pub-id></element-citation></ref><ref id="R92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Popal</surname><given-names>H</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Olson</surname><given-names>IR</given-names></name></person-group><article-title>A Guide to Representational Similarity Analysis for Social Neuroscience</article-title><source>Social Cognitive and Affective Neuroscience</source><year>2019</year><volume>14</volume><issue>11</issue><fpage>1243</fpage><lpage>1253</lpage><pub-id pub-id-type="pmcid">PMC7057283</pub-id><pub-id pub-id-type="pmid">31989169</pub-id><pub-id pub-id-type="doi">10.1093/scan/nsz099</pub-id></element-citation></ref><ref id="R93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Popham</surname><given-names>SF</given-names></name><name><surname>Huth</surname><given-names>AG</given-names></name><name><surname>Bilenko</surname><given-names>NY</given-names></name><name><surname>Deniz</surname><given-names>F</given-names></name><name><surname>Gao</surname><given-names>JS</given-names></name><name><surname>Nunez-Elizalde</surname><given-names>AO</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year>2021</year><article-title>Visual and linguistic semantic representations are aligned at the border of human visual cortex</article-title><source>Nature Neuroscience</source><volume>24</volume><issue>11</issue><fpage>1628</fpage><lpage>1636</lpage><pub-id pub-id-type="pmid">34711960</pub-id></element-citation></ref><ref id="R94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Price</surname><given-names>CJ</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><article-title>Degeneracy and cognitive anatomy</article-title><source>Trends in Cognitive Sciences</source><year>2002</year><volume>6</volume><issue>10</issue><fpage>416</fpage><lpage>421</lpage><pub-id pub-id-type="pmid">12413574</pub-id></element-citation></ref><ref id="R95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramon</surname><given-names>M</given-names></name><name><surname>Busigny</surname><given-names>T</given-names></name><name><surname>Gosselin</surname><given-names>F</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><article-title>All new kids on the block? Impaired holistic processing of personally familiar faces in a kindergarten teacher with acquired prosopagnosia</article-title><source>Visual Cognition</source><year>2016</year><volume>24</volume><issue>5–6</issue><fpage>321</fpage><lpage>355</lpage><pub-id pub-id-type="doi">10.1080/13506285.2016.1273985</pub-id></element-citation></ref><ref id="R96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richoz</surname><given-names>A-R</given-names></name><name><surname>Jack</surname><given-names>RE</given-names></name><name><surname>Garrod</surname><given-names>OGB</given-names></name><name><surname>Schyns</surname><given-names>PG</given-names></name><name><surname>Caldara</surname><given-names>R</given-names></name></person-group><article-title>Reconstructing dynamic mental models of facial expressions in prosopagnosia reveals distinct representations for identity and expression</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><year>2015</year><volume>65</volume><fpage>50</fpage><lpage>64</lpage><pub-id pub-id-type="pmid">25638352</pub-id></element-citation></ref><ref id="R97"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Riddoch</surname><given-names>JM</given-names></name><name><surname>Humphreys</surname><given-names>GW</given-names></name></person-group><source>BORB: Birmingham Object Recognition Battery</source><publisher-name>Psychology Press</publisher-name><year>2022</year><comment><ext-link ext-link-type="uri" xlink:href="https://play.google.com/store/books/details?id=kLhmEAAAQBAJ">https://play.google.com/store/books/details?id=kLhmEAAAQBAJ</ext-link></comment></element-citation></ref><ref id="R98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenthal</surname><given-names>G</given-names></name><name><surname>Tanzer</surname><given-names>M</given-names></name><name><surname>Simony</surname><given-names>E</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Behrmann</surname><given-names>M</given-names></name><name><surname>Avidan</surname><given-names>G</given-names></name></person-group><article-title>Altered topology of neural circuits in congenital prosopagnosia</article-title><source>eLife</source><year>2017</year><volume>6</volume><pub-id pub-id-type="pmcid">PMC5565317</pub-id><pub-id pub-id-type="pmid">28825896</pub-id><pub-id pub-id-type="doi">10.7554/eLife.25069</pub-id></element-citation></ref><ref id="R99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><article-title>Constraining the cortical face network by neuroimaging studies of acquired prosopagnosia</article-title><source>NeuroImage</source><year>2008</year><volume>40</volume><issue>2</issue><fpage>423</fpage><lpage>426</lpage><pub-id pub-id-type="pmid">18086537</pub-id></element-citation></ref><ref id="R100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><article-title>Understanding face perception by means of prosopagnosia and neuroimaging</article-title><source>Frontiers in Bioscience</source><year>2014</year><volume>6</volume><issue>2</issue><fpage>258</fpage><lpage>307</lpage><pub-id pub-id-type="pmid">24896206</pub-id></element-citation></ref><ref id="R101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><article-title>Damasio’s error-Prosopagnosia with intact within-category object recognition</article-title><source>Journal of Neuropsychology</source><year>2018</year><pub-id pub-id-type="pmid">29845731</pub-id></element-citation></ref><ref id="R102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><article-title>Twenty years of investigation with the case of prosopagnosia PS to understand human face identity recognition. Part I: Function</article-title><source>Neuropsychologia</source><year>2022a</year><volume>173</volume><elocation-id>108278</elocation-id><pub-id pub-id-type="pmid">35690112</pub-id></element-citation></ref><ref id="R103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><article-title>Twenty years of investigation with the case of prosopagnosia PS to understand human face identity recognition. Part II: Neural basis</article-title><source>Neuropsychologia</source><year>2022b</year><elocation-id>108279</elocation-id><pub-id pub-id-type="pmid">35667496</pub-id></element-citation></ref><ref id="R104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rossion</surname><given-names>B</given-names></name><name><surname>Caldara</surname><given-names>R</given-names></name><name><surname>Seghier</surname><given-names>M</given-names></name><name><surname>Schuller</surname><given-names>A</given-names></name><name><surname>Lazeyras</surname><given-names>F</given-names></name><name><surname>Mayer</surname><given-names>E</given-names></name></person-group><article-title>A network of occipito-temporal face-sensitive areas besides the right middle fusiform gyrus is necessary for normal face processing</article-title><source>Brain: A Journal of Neurology</source><year>2003</year><volume>126</volume><issue>11</issue><fpage>2381</fpage><lpage>2395</lpage><pub-id pub-id-type="pmid">12876150</pub-id></element-citation></ref><ref id="R105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russell</surname><given-names>R</given-names></name><name><surname>Duchaine</surname><given-names>B</given-names></name><name><surname>Nakayama</surname><given-names>K</given-names></name></person-group><article-title>Super-recognizers: people with extraordinary face recognition ability</article-title><source>Psychonomic Bulletin Review</source><year>2009</year><volume>16</volume><issue>2</issue><fpage>252</fpage><lpage>257</lpage><pub-id pub-id-type="pmcid">PMC3904192</pub-id><pub-id pub-id-type="pmid">19293090</pub-id><pub-id pub-id-type="doi">10.3758/PBR.16.2.252</pub-id></element-citation></ref><ref id="R106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schiltz</surname><given-names>C</given-names></name><name><surname>Sorger</surname><given-names>B</given-names></name><name><surname>Caldara</surname><given-names>R</given-names></name><name><surname>Ahmed</surname><given-names>F</given-names></name><name><surname>Mayer</surname><given-names>E</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><article-title>Impaired face discrimination in acquired prosopagnosia is associated with abnormal response to individual faces in the right middle fusiform gyrus</article-title><source>Cerebral Cortex</source><year>2006</year><volume>16</volume><issue>4</issue><fpage>574</fpage><lpage>586</lpage><pub-id pub-id-type="pmid">16033923</pub-id></element-citation></ref><ref id="R107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schweinberger</surname><given-names>SR</given-names></name><name><surname>Neumann</surname><given-names>MF</given-names></name></person-group><article-title>Repetition effects in human ERPs to faces</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><year>2016</year><volume>80</volume><fpage>141</fpage><lpage>153</lpage><pub-id pub-id-type="pmid">26672902</pub-id></element-citation></ref><ref id="R108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schyns</surname><given-names>PG</given-names></name><name><surname>Snoek</surname><given-names>L</given-names></name><name><surname>Daube</surname><given-names>C</given-names></name></person-group><article-title>Degrees of algorithmic equivalence between the brain and its DNN models</article-title><source>Trends in Cognitive Sciences</source><year>2022</year><pub-id pub-id-type="pmid">36216674</pub-id></element-citation></ref><ref id="R109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simon</surname><given-names>SR</given-names></name><name><surname>Khateb</surname><given-names>A</given-names></name><name><surname>Darque</surname><given-names>A</given-names></name><name><surname>Lazeyras</surname><given-names>F</given-names></name><name><surname>Mayer</surname><given-names>E</given-names></name><name><surname>Pegna</surname><given-names>AJ</given-names></name></person-group><article-title>When the brain remembers, but the patient doesn’t: Converging fMRI and EEG evidence for covert recognition in a case of prosopagnosia</article-title><source>Cortex</source><year>2011</year><volume>47</volume><issue>7</issue><fpage>825</fpage><lpage>838</lpage><pub-id pub-id-type="pmid">20850714</pub-id></element-citation></ref><ref id="R110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group><article-title>Very Deep Convolutional Networks for Large-Scale Image Recognition</article-title><source>arXiv [csCV] arXiv</source><year>2014</year><comment><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1409.1556">http://arxiv.org/abs/1409.1556</ext-link></comment></element-citation></ref><ref id="R111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sorger</surname><given-names>B</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>Schiltz</surname><given-names>C</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><article-title>Understanding the functional neuroanatomy of acquired prosopagnosia</article-title><source>NeuroImage</source><year>2007</year><volume>35</volume><issue>2</issue><fpage>836</fpage><lpage>852</lpage><pub-id pub-id-type="pmid">17303440</pub-id></element-citation></ref><ref id="R112"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Soulos</surname><given-names>Isik</given-names></name></person-group><chapter-title>Disentangled face representations in deep generative models and the human brain</chapter-title><source>NeurIPS</source><year>2020</year><conf-name>Workshop SVRHM</conf-name><comment><ext-link ext-link-type="uri" xlink:href="https://openreview.net/forum?id=ME5Uh_tyld5">https://openreview.net/forum?id=ME5Uh_tyld5</ext-link></comment></element-citation></ref><ref id="R113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanaka</surname><given-names>JW</given-names></name><name><surname>Curran</surname><given-names>T</given-names></name><name><surname>Porterfield</surname><given-names>AL</given-names></name><name><surname>Collins</surname><given-names>D</given-names></name></person-group><article-title>Activation of preexisting and acquired face representations: the N250 event-related potential as an index of face familiarity</article-title><source>Journal of Cognitive Neuroscience</source><year>2006</year><volume>18</volume><issue>9</issue><fpage>1488</fpage><lpage>1497</lpage><pub-id pub-id-type="pmid">16989550</pub-id></element-citation></ref><ref id="R114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tardif</surname><given-names>J</given-names></name><name><surname>Morin Duchesne</surname><given-names>X</given-names></name><name><surname>Cohan</surname><given-names>S</given-names></name><name><surname>Royer</surname><given-names>J</given-names></name><name><surname>Blais</surname><given-names>C</given-names></name><name><surname>Fiset</surname><given-names>D</given-names></name><name><surname>Duchaine</surname><given-names>B</given-names></name><name><surname>Gosselin</surname><given-names>F</given-names></name></person-group><article-title>Use of face information varies systematically from developmental prosopagnosics to super-recognizers</article-title><source>Psychological Science</source><year>2019</year><volume>30</volume><issue>2</issue><fpage>300</fpage><lpage>308</lpage><pub-id pub-id-type="pmid">30452304</pub-id></element-citation></ref><ref id="R115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Towler</surname><given-names>J</given-names></name><name><surname>Eimer</surname><given-names>M</given-names></name></person-group><article-title>Electrophysiological studies of face processing in developmental prosopagnosia: neuropsychological and neurodevelopmental perspectives</article-title><source>Cognitive Neuropsychology</source><year>2012</year><volume>29</volume><issue>5–6</issue><fpage>503</fpage><lpage>529</lpage><pub-id pub-id-type="pmid">23066851</pub-id></element-citation></ref><ref id="R116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treder</surname><given-names>MS</given-names></name></person-group><article-title>MVPA-Light: A Classification and Regression Toolbox for Multi-Dimensional Data</article-title><source>Frontiers in Neuroscience</source><year>2020</year><volume>14</volume><fpage>289</fpage><pub-id pub-id-type="pmcid">PMC7287158</pub-id><pub-id pub-id-type="pmid">32581662</pub-id><pub-id pub-id-type="doi">10.3389/fnins.2020.00289</pub-id></element-citation></ref><ref id="R117"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsantani</surname><given-names>M</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Storrs</surname><given-names>K</given-names></name><name><surname>Williams</surname><given-names>AL</given-names></name><name><surname>McGettigan</surname><given-names>C</given-names></name><name><surname>Garrido</surname><given-names>L</given-names></name></person-group><article-title>FFA and OFA Encode Distinct Types of Face Identity Information</article-title><source>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</source><year>2021</year><volume>41</volume><issue>9</issue><fpage>1952</fpage><lpage>1969</lpage><pub-id pub-id-type="pmcid">PMC7939092</pub-id><pub-id pub-id-type="pmid">33452225</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1449-20.2020</pub-id></element-citation></ref><ref id="R118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vida</surname><given-names>MD</given-names></name><name><surname>Nestor</surname><given-names>A</given-names></name><name><surname>Plaut</surname><given-names>DC</given-names></name><name><surname>Behrmann</surname><given-names>M</given-names></name></person-group><article-title>Spatiotemporal dynamics of similarity-based neural representations of facial identity</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2017</year><volume>114</volume><issue>2</issue><fpage>388</fpage><lpage>393</lpage><pub-id pub-id-type="pmcid">PMC5240702</pub-id><pub-id pub-id-type="pmid">28028220</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1614763114</pub-id></element-citation></ref><ref id="R119"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogel</surname><given-names>EK</given-names></name><name><surname>McCollough</surname><given-names>AW</given-names></name><name><surname>Machizawa</surname><given-names>MG</given-names></name></person-group><article-title>Neural measures reveal individual differences in controlling access to working memory</article-title><source>Nature</source><year>2005</year><volume>438</volume><issue>7067</issue><fpage>500</fpage><lpage>503</lpage><pub-id pub-id-type="pmid">16306992</pub-id></element-citation></ref><ref id="R120"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warrington</surname><given-names>EK</given-names></name><name><surname>Shallice</surname><given-names>T</given-names></name></person-group><article-title>Category specific semantic impairments</article-title><source>Brain: A Journal of Neurology</source><year>1984</year><volume>107</volume><issue>Pt 3</issue><fpage>829</fpage><lpage>854</lpage><pub-id pub-id-type="pmid">6206910</pub-id></element-citation></ref><ref id="R121"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname><given-names>D</given-names></name><name><surname>Mike Burton</surname><given-names>A</given-names></name></person-group><article-title>Individual differences and the multidimensional nature of face perception</article-title><source>Nature Reviews Psychology</source><year>2022</year><pub-id pub-id-type="doi">10.1038/s44159-022-00041-3</pub-id></element-citation></ref><ref id="R122"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiese</surname><given-names>H</given-names></name><name><surname>Tüttenberg</surname><given-names>SC</given-names></name><name><surname>Ingram</surname><given-names>BT</given-names></name><name><surname>Chan</surname><given-names>CYX</given-names></name><name><surname>Gurbuz</surname><given-names>Z</given-names></name><name><surname>Burton</surname><given-names>AM</given-names></name><name><surname>Young</surname><given-names>AW</given-names></name></person-group><article-title>A Robust Neural Index of High Face Familiarity</article-title><source>Psychological Science</source><year>2019a</year><volume>30</volume><issue>2</issue><fpage>261</fpage><lpage>272</lpage><pub-id pub-id-type="pmid">30557087</pub-id></element-citation></ref><ref id="R123"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiese</surname><given-names>H</given-names></name><name><surname>Tüttenberg</surname><given-names>SC</given-names></name><name><surname>Ingram</surname><given-names>BT</given-names></name><name><surname>Chan</surname><given-names>CYX</given-names></name><name><surname>Gurbuz</surname><given-names>Z</given-names></name><name><surname>Burton</surname><given-names>AM</given-names></name><name><surname>Young</surname><given-names>AW</given-names></name></person-group><article-title>A Robust Neural Index of High Face Familiarity</article-title><source>Psychological Science</source><year>2019b</year><volume>30</volume><issue>2</issue><fpage>261</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1177/0956797618813572</pub-id></element-citation></ref><ref id="R124"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yan</surname><given-names>X</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><article-title>A robust neural familiar face recognition response in a dynamic (periodic) stream of unfamiliar faces</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><year>2020</year><volume>132</volume><fpage>281</fpage><lpage>295</lpage><pub-id pub-id-type="pmid">33007641</pub-id></element-citation></ref><ref id="R125"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zadelaar</surname><given-names>JN</given-names></name><name><surname>Weeda</surname><given-names>WD</given-names></name><name><surname>Waldorp</surname><given-names>LJ</given-names></name><name><surname>Van Duijvenvoorde</surname><given-names>ACK</given-names></name><name><surname>Blankenstein</surname><given-names>NE</given-names></name><name><surname>Huizenga</surname><given-names>HM</given-names></name></person-group><article-title>Are individual differences quantitative or qualitative? An integrated behavioral and fMRI MIMIC approach</article-title><source>NeuroImage</source><year>2019</year><volume>202</volume><elocation-id>116058</elocation-id><pub-id pub-id-type="pmid">31352125</pub-id></element-citation></ref></ref-list></back><floats-group><boxed-text id="BX1" position="float" orientation="portrait"><caption><title>Highlights</title></caption><list list-type="bullet" id="L1"><list-item><p>We assess the neural computations in the prosopagnosic patient PS using EEG, RSA, and deep neural networks</p></list-item><list-item><p>Neural dynamics of brain-lesioned PS are reliably captured using RSA</p></list-item><list-item><p>Neural decoding shows normal evidence for non-face individuation in PS</p></list-item><list-item><p>Neural decoding shows abnormal neural evidence for face individuation in PS</p></list-item><list-item><p>PS shows impaired high-level visual and semantic neural computations</p></list-item></list></boxed-text><fig id="F1" position="float"><label>Figure 1</label><caption><title>Overview of the experimental procedure.</title><p><bold>a)</bold> The histogram shows the Cambridge Face Memory Test long-form (CFMT+, <xref ref-type="bibr" rid="R105">Russell et al., 2009</xref>) scores of PS, typical recognisers (black bars), and an additional 332 neurotypical observers from three independent studies for comparison (<xref ref-type="bibr" rid="R41">Faghel-Soubeyrand et al., 2019</xref>; <xref ref-type="bibr" rid="R45">Fysh et al., 2020</xref>; <xref ref-type="bibr" rid="R114">Tardif et al., 2019</xref>). <bold>b)</bold> Participants engaged in a one-back task while their brain activity was recorded with high-density electroencephalography. The objects depicted in the stimuli belonged to various categories, such as faces, objects, and scenes. Note that the face drawings shown here are an anonymised substitute to the experimental face stimuli presented to our participants. <bold>c)</bold> Brain Representational dissimilarity matrices (RDM) were computed for PS and controls using cross-validated decoding performance between the EEG topographies from each pair of stimuli at every 4 ms time-point. Brain RDMs are shown for specific time points.</p></caption><graphic xlink:href="EMS158904-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Behavioural performance and reliability of brain representations across time.</title><p><bold>(a)</bold> A behavioural face-specific performance score was computed for all participants and constrasted between PS and controls, indicating strong and archetypical face-sensitive deficits (<italic>p</italic>&lt;.0001). <bold>(b)</bold> Brain Representational Dissimilarity Matrices (RDMs) computed on different recording days were cross-correlated within participants for patient PS (red line) and neurotypical participants (grey lines indicate individuals participants, black line indicate control-averaged). PS brain RDMs showed significant inter-session reliability across most time windows (<italic>p</italic>&lt;.05, permutations). For comparison, the percentage of control participants with significant cross-session correlations is shown on top for every time point, with darker points indicating higher percentage of neurotypicals with reliable brain RDMs. PS had similar reliability coefficients compared to controls.</p></caption><graphic xlink:href="EMS158904-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Neural decoding of face and non-face identities and association with individual face recognition abilities.</title><p>Multivariate classifiers were trained to predict 8 face identities <bold>(a)</bold> as well as 8 identities from non-face categories <bold>(b)</bold> from time-resolved EEG patterns. <bold>a)</bold> Neural face-identity decoding performance (recall, 5-fold, 5-repetition cross-validation; <xref ref-type="bibr" rid="R116">Treder, 2020</xref>) is shown for PS (red line) and the control group (black line, representing control-average decoding time course; grey dots show significant identity decoding at <italic>p</italic>&lt;.05, permutations). PS had overall below chance decoding for the face-identity decoding across time and showed significantly lower identity decoding compared to controls around 200 ms after face onset (black dots indicate significant</p></caption><graphic xlink:href="EMS158904-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Comparison of brain representations with those of artificial neural networks of visual and semantic processing.</title><p><bold>a</bold>) Partial correlation between brain RDMs and AlexNet RDMs (removing shared correlation between brain and semantic model) is shown for PS (coloured curve) and controls (grey curve). Each column shows different layer RDMs in ascending order from left to right. We found overall lower similarity of visual computations within the brain of PS compared to controls (black dots indicate significant contrasts, Howell-Crawford modified t-tests, <italic>p</italic>&lt;.05), with differences peaking in higher-level CNN layer 6. Similar results were observed when comparing brains and CNN models without removing the shared information between brains and the semantic (caption-level) model (see <xref ref-type="supplementary-material" rid="SD1">supplementary figure 3</xref>). <bold>b</bold>) Partial correlation with RDMs of the semantic model (excluding shared information between brain and AlexNet) was significantly lower in the brain of PS compared to controls (cyan curve; black dots indicates significant contrasts, <italic>p</italic>&lt;.05). The shaded areas of all curves represent the standard error for the controls. <bold>c</bold>) Proportion of all time points after EEG onset showing significant contrasts (PS &lt; controls, <italic>p</italic>&lt;.05, uncorrected) is shown on a Venn diagram as a summary of PS’s impaired brain computations for visual computations (blue colours, where deeper blue represents deeper CNN layers) and semantic computations (cyan). The total surface area for the “brain” section represents all possible time points after onset (0-1100 ms in 4 ms steps), while the surface of each categories represents a portion of these time points. Overlap indicates significance over the same time points.</p></caption><graphic xlink:href="EMS158904-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Comparison of visual and semantic computations along early, mid and late brain processing steps.</title><p><bold>(a)</bold> Correlations of brain RDMs to visual model (blue plots, with deeper blue corresponding to deeper CNN layers) and semantic model (green plot) was assessed in PS and controls for brain RDMs computed in temporal windows corresponding to the P100 (80-130 ms), N170 (130-230 ms) and N400 (250-500 ms) ERP components. <bold>(b)</bold> Crawford-Howell t-statistics comparing controls vs. PS on these correlations are shown for the visual CNN (blue bars, with deeper blue corresponding to deeper CNN layers) and the semantic model (green bars; significant comparisons are indicated with an asterisk, marginally significant comparisons with the letter <italic>t</italic>).</p></caption><graphic xlink:href="EMS158904-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><title>Representational trajectories of brain and computation models.</title><p><bold>(a)</bold> All pairwise correlations across brain and computational models’ RDMs resulted in second-order similarity matrices (RSMs) for controls (upper panel) and PS (lower panel). <bold>(b)</bold> A summary of group RSMs was obtained by computing the 2D solution of these RSMs using MDS. Points corresponding to brain representations of controls (upper panel) and PS (lower panel) were linked with grey and red lines, respectively.</p></caption><graphic xlink:href="EMS158904-f006"/></fig></floats-group></article>