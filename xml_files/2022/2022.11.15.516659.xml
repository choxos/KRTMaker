<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS157286</article-id><article-id pub-id-type="doi">10.1101/2022.11.15.516659</article-id><article-id pub-id-type="archive">PPR572570</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Viewpoint-Dependence and Scene Context Effects Generalize to Depth Rotated 3D Objects</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Kallmayer</surname><given-names>Aylin</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Võ</surname><given-names>Melissa L.-H.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Draschkow</surname><given-names>Dejan</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib></contrib-group><aff id="A1"><label>1</label>Department of Psychology, Goethe University Frankfurt, Theodor-W.-Adorno-Platz 6, Frankfurt am Main 60323, Germany</aff><aff id="A2"><label>2</label>Department of Experimental Psychology, University of Oxford, Oxford, UK</aff><aff id="A3"><label>3</label>Oxford Centre for Human Brain Activity, Wellcome Centre for Integrative Neuroimaging, Department of Psychiatry, University of Oxford, Oxford, UK</aff><author-notes><corresp id="CR1">Corresponding author contact information: Aylin Kallmayer, Department of Psychology, Scene Grammar Lab, Goethe University, Frankfurt, Germany, Phone: +491778445631, <email>kallmayer@psych.uni-frankfurt.de</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>18</day><month>11</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>15</day><month>11</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Viewpoint effects on object recognition interact with object-scene consistency effects. While recognition of objects seen from “accidental” viewpoints (e.g., a cup from below) is typically impeded compared to processing of objects seen from canonical viewpoints (e.g., the string-side of a guitar), this effect is reduced by meaningful scene context information. In the present study we investigated if these findings established by using photographic images, generalise to 3D models of objects. Using 3D models further allowed us to probe a broad range of viewpoints and empirically establish accidental and canonical viewpoints. In Experiment 1, we presented 3D models of objects from six different viewpoints (0°, 60°, 120°, 180° 240°, 300°) in colour (1a) and grayscaled (1b) in a sequential matching task. Viewpoint had a significant effect on accuracy and response times. Based on the performance in Experiments 1a and 1b, we determined canonical (0°-rotation) and non-canonical (120°-rotation) viewpoints for the stimuli. In Experiment 2, participants again performed a sequential matching task, however now the objects were paired with scene backgrounds which could be either consistent (e.g., a cup in the kitchen) or inconsistent (e.g., a guitar in the bathroom) to the object. Viewpoint interacted significantly with scene consistency in that object recognition was less affected by viewpoint when consistent scene information was provided, compared to inconsistent information. Our results show that viewpoint-dependence and scene context effects generalize to depth rotated 3D objects. This supports the important role object-scene processing plays for object constancy.</p></abstract><kwd-group><kwd>Object recognition</kwd><kwd>Viewpoint dependence</kwd><kwd>Scene context effects</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Object recognition happens fast, automatic, and in most cases seems effortless to us. Since our environment is highly dynamic, especially when interacting with it, one and the same object will produce a range of different images on the retina. In fact, it is very unlikely that an object would produce the same retinal image twice due to changes in viewpoint, lighting, reflections, or viewing distance. Still, our visual system is able to flexibly transform this variable visual input in a way that object identity can successfully be read out from the resulting abstract representations in higher areas of visual cortex (see <xref ref-type="bibr" rid="R13">DiCarlo &amp; Cox, 2007</xref>).</p><p id="P3">Whether object recognition is viewpoint-dependent (recognition performance is sensitive to changes in viewpoints as indicated by accuracy and response-time (RT) data) or viewpoint-invariant (recognition performance is largely unaffected by changes in viewpoint) has been a debated topic (<xref ref-type="bibr" rid="R4">Biederman &amp; Gerhardstein, 1993</xref>; <xref ref-type="bibr" rid="R9">Bülthoff &amp; Edelman, 1992</xref>; <xref ref-type="bibr" rid="R10">Burgund &amp; Marsolek, 2000</xref>; <xref ref-type="bibr" rid="R11">Charles Leek &amp; Johnston, 2006</xref>; <xref ref-type="bibr" rid="R17">Edelman, 1995</xref>; <xref ref-type="bibr" rid="R20">Graf, 2006</xref>; <xref ref-type="bibr" rid="R22">Hayward, 2003</xref>; <xref ref-type="bibr" rid="R23">Hayward &amp; Tarr, 1997</xref>; <xref ref-type="bibr" rid="R27">Jolicoeur, 1990</xref>; <xref ref-type="bibr" rid="R33">Leek et al., 2007</xref>; <xref ref-type="bibr" rid="R36">Lowe, 1987</xref>; <xref ref-type="bibr" rid="R37">Marr et al., 1978</xref>; <xref ref-type="bibr" rid="R44">Ratan Murty &amp; Arun, 2015</xref>; <xref ref-type="bibr" rid="R46">Stankiewicz, 2002</xref>; <xref ref-type="bibr" rid="R47">Tarr &amp; Bülthoff, 1995</xref>; <xref ref-type="bibr" rid="R48">Tarr &amp; Pinker, 1989</xref>; <xref ref-type="bibr" rid="R56">Wilson &amp; Farah, 2003</xref>). Since the early debates, there has been overwhelming consensus that object recognition is neither solely viewpoint-dependent nor solely viewpoint-invariant and that evidence for both can be observed depending on experimental task and stimuli (<xref ref-type="bibr" rid="R18">Foster &amp; Gilson, 2002</xref>; <xref ref-type="bibr" rid="R21">Hamm &amp; McMullen, 1998</xref>; <xref ref-type="bibr" rid="R27">Jolicoeur, 1990</xref>; <xref ref-type="bibr" rid="R33">Leek et al., 2007</xref>; <xref ref-type="bibr" rid="R44">Ratan Murty &amp; Arun, 2015</xref>; <xref ref-type="bibr" rid="R45">Sastyin et al., 2015</xref>; <xref ref-type="bibr" rid="R46">Stankiewicz, 2002</xref>; <xref ref-type="bibr" rid="R49">Vanrie et al., 2002</xref>).</p><p id="P4">Past research has made great advances towards understanding the mechanisms that underly invariant object recognition, when objects are presented in isolation (i.e., <xref ref-type="bibr" rid="R13">DiCarlo &amp; Cox, 2007</xref>). More recently, however, researchers have started to investigate the viewpoint problem in the context of object-scene processing. Object recognition rarely occurs in isolation where the only available information are the objects’ features. In our everyday lives, we encounter objects within certain contexts, which provides us with a pool of complex visual and multimodal information that is integrated during object recognition. Past research has shown that context facilitates object recognition (<xref ref-type="bibr" rid="R5">Biederman et al., 1982</xref>; <xref ref-type="bibr" rid="R40">Oliva &amp; Torralba, 2007</xref>; for a recent review see <xref ref-type="bibr" rid="R32">Lauer et al., 2021</xref>). Evidence from behavioral as well as neurophysiological studies (e.g., <xref ref-type="bibr" rid="R8">Brandman &amp; Peelen, 2017</xref>) suggest an interactive processing of objects and scenes. For instance, objects placed in semantically consistent contexts are recognized faster and more accurately, often referred to as the <italic>scene-consistency effect</italic> (<xref ref-type="bibr" rid="R12">Davenport &amp; Potter, 2004</xref>; <xref ref-type="bibr" rid="R41">Palmer, 1975</xref>). Accordingly, models of object recognition have been updated to incorporate the integration of contextual information (<xref ref-type="bibr" rid="R2">Bar, 2004</xref>). Further, frameworks incorporating object-scene and object-object relations (e.g., the so-called <italic>scene-grammar</italic>) describe a set of internalized rules based on regularities found in real-world scenes that facilitate scene and object perception and guide our attention during different visual cognitive tasks (<xref ref-type="bibr" rid="R16">Draschkow &amp; Võ, 2017</xref>; <xref ref-type="bibr" rid="R28">Josephs et al., 2016</xref>; <xref ref-type="bibr" rid="R50">Võ, 2021</xref>; <xref ref-type="bibr" rid="R51">Võ et al., 2019</xref>; <xref ref-type="bibr" rid="R52">Võ &amp; Henderson, 2009</xref>; <xref ref-type="bibr" rid="R53">Võ &amp; Wolfe, 2013a</xref>, <xref ref-type="bibr" rid="R54">2013b</xref>).</p><p id="P5"><xref ref-type="bibr" rid="R45">Sastyin and clleagues (2015)</xref> conducted a series of experiments investigating the interaction between viewpoint and scene-consistency on object and scene recognition. They used photographic images of objects shown from canonical and accidental viewpoints and paired them with consistent or inconsistent scenes. They found a significant interaction between viewpoint and consistency where the viewpoint effect was weaker when consistent scene information was provided. From this they concluded that object recognition relied more on context information if the object was presented from an accidental viewpoint.</p><p id="P6">Here, in order to increase the external validity of these findings (<xref ref-type="bibr" rid="R14">Draschkow, 2022</xref>), we aimed to generalize the insights from 2D photographic images to 3D models of objects (<xref ref-type="bibr" rid="R4">Biederman &amp; Gerhardstein, 1993</xref>; <xref ref-type="bibr" rid="R19">Gauthier et al., 2002</xref>; <xref ref-type="bibr" rid="R35">Logothetis et al., 1994</xref>; <xref ref-type="bibr" rid="R42">Poggio &amp; Edelman, 1990</xref>; <xref ref-type="bibr" rid="R57">Zisserman et al., 1995</xref>). Recent work using 3D immersive environments has highlighted the importance of studying vision under more naturalistic constraints in order to investigate cognitive processes in the context of natural behavior (<xref ref-type="bibr" rid="R15">Draschkow et al., 2021</xref>; <xref ref-type="bibr" rid="R26">Helbing et al., 2020</xref>, <xref ref-type="bibr" rid="R25">2022</xref>; <xref ref-type="bibr" rid="R30">Kristjánsson &amp; Draschkow, 2021</xref>). An additional benefit of using 3D models is that we could probe a broad range of viewpoints and empirically establish accidental and canonical viewpoints, allowing for a broader representation of the viewpoints we encounter in our natural environment.</p><p id="P7">In the present study, we conducted three behavioral experiments. In our first two experiments, (Experiment 1a and 1b) we presented 3D models of real-world objects from six different angles (0°, 60°, 180°, 120°, 240°, 300°) rotated around the pitch axis in a word-picture verification task. Because rotating the objects around the pitch axis results in highly atypical viewpoints, we expected to find viewpoint-dependent recognition indicated by lower accuracy and slower RTs. In Experiment 1b, we wanted to replicate Experiment 1a with grayscale versions of the images, expecting similar effects of viewpoint as for Experiment 1a (<xref ref-type="bibr" rid="R24">Hayward &amp; Williams, 2000</xref>). Experiments 1a and 1b also served to identify viewpoints which produced highest (canonical) and lowest (non-canonical) recognition performance which we then used in Experiment 2.</p><p id="P8">In Experiment 2, we paired 3D objects presented in canonical (0° rotation) and non-canonical (120° rotation) viewpoints with semantically consistent and inconsistent scenes. Our aim was to test if viewpoint-dependence and object-scene processing effects (<xref ref-type="bibr" rid="R45">Sastyin et al., 2015</xref>) generalize to depth rotated 3D models of objects.</p></sec><sec id="S2"><title>General Method</title><sec id="S3" sec-type="subjects"><title>Participants</title><p id="P9">Participants were recruited at Goethe-University Frankfurt am Main. The sample consisted of 12 participants who completed Experiment 1a (6 women, <italic>M</italic> = 23.92, range = 19–29), 12 different participants who completed Experiment 1b (8 women, <italic>M</italic> = 19, range = 18–22), and another set of 32 participants who completed Experiment 2 (25 women, <italic>M</italic> = 24.28, range = 18–51). The sample size of Experiment 2 was a priori chosen to be higher compared to previous studies which found reliable effects across multiple experiments with 20 participants (e.g., <xref ref-type="bibr" rid="R45">Sastyin et al., 2015</xref>). In Experiment 1a, all except for six participants were psychology students that were compensated with course credits, while the remaining participants volunteered for the experiment without any compensation. All had normal or corrected-to-normal vision, were native German speakers, and were unfamiliar with the stimulus materials. Written informed consent was obtained before participation, data collection and analysis were carried out according to guidelines approved by the Human Research Ethics Committee of the Goethe University Frankfurt.</p></sec><sec id="S4"><title>Stimulus Material</title><p id="P10">For Experiment 1a and Experiment 1b, we collected 100 3D models of objects from a broad range of categories such as furniture, foods, vehicles, plants, and electrical devices. Eighty-two of the 3D models were purchased from CG Axis Complete packages I, II, III, and V, 18 additional models were obtained free of charge from sources like TurboSquid and free3D. Each model was rotated around its pitch axis by 0°, 60°, 120°, 180°, 240°, and 300° degrees and sized to fit a 60cm x 60cm x 60cm box using the free 3D animation software Blender. A snapshot from each angle was systematically recorded in front of a gray background using the virtual reality software Vizward5 to create our final stimulus set of 600 images. Additionally, we created grey-scaled versions of these images for Experiment 1b using the GrayscaleEffect function in Vizard5 (<ext-link ext-link-type="uri" xlink:href="https://docs.worldviz.com/vizard/latest/postprocess_color.htm">https://docs.worldviz.com/vizard/latest/postprocess_color.htm</ext-link>).</p><p id="P11">For Experiment 2, we used the same 3D models as in Experiment 1 adding an additional 56 models collected from the CGAxis packages, resulting in a total of 156 models. Instead of creating snapshots of all six angles, we chose the two viewpoints that had previously produced the highest (canonical viewpoint, 0°) and lowest (non-canonical viewpoint, 120°) recognition performance averaged over Experiment 1a and Experiment 1b. We gray-scaled the images using the method described above.</p><p id="P12">Additionally, we collected 312 photographic images of scenes, one consistent and one inconsistent scene for each object. We defined a consistent scene as one in which we would expect the object to appear naturally. In both cases, the target object was not present in the scene. Most of the photographs were obtained from the SCEGRAM database (<xref ref-type="bibr" rid="R39">Öhlschläger &amp; Võ, 2017</xref>) as well as from Google images.</p></sec><sec id="S5" sec-type="methods"><title>Procedure</title><p id="P13">To investigate the speed and accuracy of object recognition, while keeping the procedure comparable with previous studies, a word-picture verification task was employed for all experiments (<xref ref-type="fig" rid="F1">Figure 1</xref>). Participants were instructed on screen as well as through standardized verbal instructions to decide as quickly and accurately as possible whether the object on screen matched the basic level category label presented to them at the beginning of the trial using a corresponding “match” or “mismatch” key. Participants were not made aware of the different viewpoint conditions beforehand. Each experiment consisted of three practice trials during which the instructor stayed in the room with the participant. More detailed procedure and trial sequences will be described in the individual Procedure sections of each experiment. Experiments 1a and 1b lasted approximately 30 minutes, Experiment 2 lasted approx. 12 minutes.</p></sec><sec id="S6"><title>Design</title><p id="P14">Experiments 1a and 1b consisted of six blocks with 100 trials each. In each block, the object was presented from a different angle (0°, 60°, 120°, 180°, 240°, 300°) chosen randomly and counterbalanced between participants. The order of objects within each block was randomized. Each object appeared three times in the match condition (object image matched basic level category label) and three times in the mismatch condition (object image did not match basic level category label), randomized between blocks.</p><p id="P15">In the mismatch condition, the basic level category label stemmed from a different superordinate category than the object image (e.g., the label “chair” as part of the superordinate category “furniture” was paired with an image of a “car” as part of the superordinate category “vehicle”).</p><p id="P16">Because there was no effect of viewpoint in the mismatch condition in Experiment 1a and 1b, most trials in Experiment 2 were match trials (N = 120) with 23% mismatch trials (N = 36) that were later excluded from analysis. In Experiment 2, each object was presented to each participant once, and we counterbalanced consistency (consistent vs. inconsistent) and viewpoint (canonical vs. non-canonical) between participants.</p></sec><sec id="S7"><title>Data Analysis</title><p id="P17">In Experiments 1a and 1b, we were interested in the effects of viewpoint (how far the object was rotated away from its canonical 0° angle) and match (whether the object matched the basic level category label as part of the experimental design) on reaction times (time between the onset of the object image and keypress response) and accuracy. In Experiment 2, we were interested in the interaction between viewpoint (canonical versus non-canonical viewpoint), and scene consistency (consistent scene versus inconsistent scene) on reaction times and accuracy.</p><p id="P18">Raw data was pre-processed and analysed using R (<xref ref-type="bibr" rid="R43">R Core Team, 2021</xref>). Objects that produced accuracy ratings that deviated more than 2.5 SD from the mean (computed for each condition separately) were excluded from analysis. Based on this, we excluded four objects in Experiment 1a, one in Experiment 1b, and two in Experiment 2. We based our reaction time analysis on correctly matched trials only (percent trials removed: Experiment 1a = 4.45%, Experiment 1b = 10.16%, Experiment 2 = 8.55%).</p><p id="P19">In our data analysis, we employed (generalized) linear mixed-effects models ((G)LMMs) using the lme4 package (<xref ref-type="bibr" rid="R3">Bates et al., 2015</xref>). We chose this approach because of its potential advantages over analysis of variance (ANOVA) as it allows us to simultaneously estimate by-participant and by-stimulus variance (<xref ref-type="bibr" rid="R1">Baayen et al., 2008</xref>; Bates et al., 2014; <xref ref-type="bibr" rid="R29">Kliegl et al., 2011</xref>). The random effects structure of each model was determined using a drop-one procedure starting with the full model including by-participant and by-stimulus varying intercepts and slopes for the main effects in our design. We then subsequently removed random slopes that did not contribute significantly to the goodness of fit as determined by likelihood ratio tests. This allowed us to avoid overparameterization and produce converging models that are supported by the data. Details about the individual analysis and models are described in the Data Analyses sections of each experiment. For each GLMM we report β regression coefficients together with the <italic>z</italic> statistic and apply a two-tailed 5% error criterion for significance testing. <italic>P</italic>-values for the binary accuracy variable are based on asymptotic Wald tests. Additionally, reaction times were transformed following the Box-Cox procedure (<xref ref-type="bibr" rid="R7">Box &amp; Cox, 1964</xref>) to correct for deviation from normality as to better meet LMM assumptions (see individual <xref ref-type="sec" rid="S11">Data Analysis</xref> sections for further details). For the LMMs regression coefficients are reported with the t-statistic and p-values were calculated with the lmerTest package (<xref ref-type="bibr" rid="R31">Kuznetsova et al., 2017</xref>). We defined sum contrasts for match (match vs. mismatch), and consistency (consistent vs. inconsistent) where slope coefficients represent differences between factor levels and the intercept is equal to the grand mean.</p><p id="P20">We used the ggplot2 package (<xref ref-type="bibr" rid="R55">Wickham, 2016</xref>) for graphics and emmeans (<xref ref-type="bibr" rid="R34">Lenth, 2022</xref>) for post-hoc comparisons. Data and code are openly available at <ext-link ext-link-type="uri" xlink:href="https://github.com/aylinsgl/2022-Viewpoint_and_Context">https://github.com/aylinsgl/2022-Viewpoint_and_Context</ext-link>.</p></sec><sec id="S8"><title>Apparatus</title><p id="P21">All experimental sessions were carried out in the same six experimental cabins of the department of psychology at Goethe-University Frankfurt am Main, containing the same experimental set up (computers running OS Windows 10). Stimulus presentation, response-times (RT) and accuracy were systematically controlled and recorded by OpenSesame (<xref ref-type="bibr" rid="R38">Mathôt et al., 2012</xref>), presented on a 19-in monitor (resolution = 1680 × 1050, refresh rate = 60 Hz, viewing distance = approx. 65 cm, subtending approx. 11.13 °× 9.28° of visual angle for the object images and approx. 19° × 15.84° of visual angle for the background images).</p></sec></sec><sec id="S9"><title>Experiment 1a &amp; 1b</title><p id="P22">In Experiments 1a and 1b, we investigated the effect of viewpoint on object recognition RT and accuracy using 3D models of objects rotated around the pitch axis (0°, 60°, 120°, 180°, 240°, 300°). The only difference between the experiments was that 3D models were presented either in color (Experiment 1a) or a grayscale version of the model was used (Experiment 1b). Participants had to indicate whether the object matched the previously presented basic level category label.</p><sec id="S10" sec-type="methods"><title>Procedure</title><p id="P23">Participants were presented with a fixation point in the middle of the screen followed by a basic level object category label (in German, font: Droid Sans Mono; font size: 26; color: black). This was followed by the target object presented in the middle of the screen, which could either match or mismatch the label, until the participant gave a response (<xref ref-type="fig" rid="F1">Figure 1A</xref>). Participants were given feedback on screen if their answer was incorrect. The next trial automatically started with a new fixation point.</p></sec><sec id="S11"><title>Data Analysis</title><p id="P24">After data preprocessing, we employed a binomial GLMM to examine the effects of viewpoint and match on accuracy. As fixed effects we included viewpoint (0°, 60°, 120°, 180°, 240°, 300°) as a first and second-degree polynomial, the match vs mismatch comparison, and the interactions between these terms. The second-degree polynomial viewpoint term was added as we expected viewpoint to affect recognition in a non-linear manner (symmetry around 180°). Our final model included random intercepts for participants and stimuli, as well as a by-stimuli random slope for the match vs. mismatch effect for Experiment 1a, and random intercepts for participants and stimuli, as well as a by-stimuli and by-participant random slope for the match effect for Experiment 1b.</p><p id="P25">Based on the power coefficient output of the Box-Cox procedure (λ = 0.22), RTs were log-transformed. We employed the same fixed effects structure for the RT-LMMs as for the accuracy-GLMMs. As random effects, we entered random intercepts for participants and stimuli, as well as by-participant and by-stimuli random slopes for the effect of match for Experiment 1a and 1b.</p></sec><sec id="S12" sec-type="results"><title>Results</title><sec id="S13"><title>Accuracy</title><p id="P26">The average accuracy in Experiment 1a was quite high (<italic>M</italic> = 0.95, <italic>SD</italic> = 0.21) and slightly lower in Experiment 1b (<italic>M</italic> = 0.9, <italic>SD</italic> = 0.3). In line with our hypothesis, the GLMM yielded a significant main effect for the second-degree polynomial viewpoint term in both experiments (Experiment 1a: β = 16.67, <italic>SD</italic> = 5.61, <italic>z</italic> = 2.97, <italic>p</italic> = 0.003; Experiment 1b: β = 18.82, <italic>SE</italic> = 3.79, <italic>z</italic> = 4.97, <italic>p</italic> &lt; 0.001), meaning that the effect of viewpoint on accuracy can be well described by a quadratic function (<xref ref-type="fig" rid="F2">Figure 2A and 2C</xref>). There was also a significant interaction between the second-degree polynomial of viewpoint and the match condition in both experiments, Experiment 1a: β = 23.62, <italic>SE</italic> = 5.69, <italic>z</italic> = 4.15, <italic>p</italic> &lt; 0.001; Experiment 1b: β = 15.23, <italic>SE</italic> = 3.82, <italic>z</italic> = 3.98, <italic>p</italic> &lt; 0.001. Comparing the viewpoint trend for the match and mismatch conditions, we found that the second-degree viewpoint trend was significant in the match condition (Experiment 1a: β = 0.19, <italic>SE</italic> = 0.03, <italic>CI</italic>95% = [0.13, 0.25]; Experiment 1b: β = 0.16, <italic>SE</italic> = 0.02, <italic>CI</italic>95% = [0.12, 0.21), but not in the mismatch condition, Experiment 1a: β = -0.03, <italic>SE</italic> = 0.04, <italic>CI</italic>95% = [-0.12, 0.05]; Experiment 1b: β = -0.02, <italic>SE</italic> = 0.03, <italic>CI</italic>95% = [-0.04, 0.07].</p></sec><sec id="S14"><title>Response-times (RT)</title><p id="P27">Participants were slightly faster on average in Experiment 1b (<italic>M</italic> = 685 ms, <italic>SD</italic> = 358 ms) than Experiment 1a (<italic>M</italic> = 738 ms, <italic>SD</italic> = 299 ms). In line with our hypothesis, the LMM revealed a significant main effect for the second-degree polynomial viewpoint term in both experiments, Experiment 1a: β = -2.2, <italic>SE</italic> = 0.29, <italic>t</italic> = -7.48, <italic>p</italic> &lt; 0.001; Experiment 1b: β = -1.42, <italic>SE</italic> = 0.29, <italic>t</italic> = -4.99, <italic>p</italic> &lt; 0.001 (<xref ref-type="fig" rid="F2">Figure 2B and 2D</xref>). In both experiments there was no interaction between viewpoint and match, Experiment 1a: β = -0.12, <italic>SE</italic> = 0.29, <italic>t</italic> = -0.4, <italic>p</italic> = 0.69; Experiment 1b: β = -0.38, <italic>SE</italic> = 0.29, <italic>t</italic> = -1.34, <italic>p</italic> = 0.18.</p></sec></sec></sec><sec id="S15" sec-type="discussion"><title>Discussion</title><p id="P28">In Experiment 1a, we found viewpoint-dependent object recognition for objects rotated around the pitch axis. This effect can best be described by a quadratic curve that approximates symmetry around 120° rotation. We also found that in our sequential matching task, only the match condition produced viewpoint-dependent behavior, while mismatch trials seemed unaffected by viewpoint. Finding a mismatch might rely more on the analysis of global, viewpoint-invariant features, whereas matching might be more dependent on the analysis of local, viewpoint-dependent features (e.g., Jolicoeur, 1990a) (e.g., deciding a shape is not a car might require less viewpoint-dependent information than identifying the shape as a chair). In Experiment 1b, we were able to replicate our results from Experiment 1a. Grayscaling the images seemed to have made the overall task slightly more difficult while still producing similarly viewpoint-dependent behavior. The canonical (0°) and non-canonical (120°) viewpoints we used in Experiment 2 represented viewpoints that produced the best and worst recognition performance derived from average accuracy ratings obtained from Experiment 1a and 1b.</p></sec><sec id="S16"><title>Experiment 2</title><p id="P29">In Experiment 2, we paired canonical (0°) and non-canonical (120°) viewpoints with consistent and inconsistent scene contexts. We were specifically interested in the interaction between viewpoint and consistency with the expectation that meaningful scene context information would reduce the effect of viewpoint on object recognition.</p><sec id="S17" sec-type="methods"><title>Procedure</title><p id="P30">In Experiment 2, we used the same word-picture verification task as in Experiments 1a and 1b (<xref ref-type="fig" rid="F1">Figure 1B</xref>). Scene context was provided by first previewing the consistent or inconsistent scene for 300ms and then overlaying the target object on top of the scene background until a response was given.</p></sec><sec id="S18"><title>Data Analysis</title><p id="P31">For both the accuracy-GLMM and response time (RT) LMM we entered interaction terms between viewpoint and consistency as fixed effects. The GLMM included random intercepts for participants and stimuli, as well as a by-stimuli random slope for the effect of viewpoint. Response time data was log transformed.</p><p id="P32">For the RT-LMM we had random intercepts for participants and stimuli, and a by-participant random slope for the effect of viewpoint and by-stimuli random slopes for the effects of viewpoint and consistency.</p></sec><sec id="S19" sec-type="results"><title>Results</title><sec id="S20"><title>Accuracy</title><p id="P33">Accuracy was significantly higher for canonical viewpoints than for non-canonical viewpoints as revealed by the GLMM (β = 0.68, <italic>SE</italic> = 0.14, <italic>z</italic> = 4.82, <italic>p</italic> &lt; 0.001) but there was no significant main effect for consistency, β = 0.06, <italic>SE</italic> = 0.07, <italic>z</italic> = 0.75, <italic>p</italic> = 0.45. Critically, there was a significant interaction between viewpoint and consistency, β = -0.21, <italic>SE</italic> = 0.07, <italic>z</italic> = -2.84, <italic>p</italic> = 0.004 (<xref ref-type="fig" rid="F3">Figure 3A</xref>). Post-hoc interaction contrasts revealed that the viewpoint-dependence effect was significantly stronger in the inconsistent scene condition compared to the consistent scene condition, β = -0.84, <italic>SE</italic> = 0.3, <italic>z</italic> = -2.84, <italic>p</italic> = 0.005. This is in line with our hypothesis that providing meaningful scene context can reduce the effects of viewpoint on object recognition. Additionally, the scene-consistency effect was only significant in the non-canonical condition (β = 0.53, <italic>SE</italic> = 0.15, <italic>z</italic> = 3.45, <italic>p</italic> &lt; 0.001), but not in the canonical condition, β = -0.31, <italic>SE</italic> = 0.25, <italic>z</italic> = -1.22, <italic>p</italic> = 0.22.</p></sec><sec id="S21"><title>Response-Times (RT)</title><p id="P34">The LMM yielded a significant main effect for viewpoint (β = -0.07, <italic>SE</italic> = 0.01, <italic>t</italic> = -7.26, <italic>p</italic> &lt; 0.001), where RTs were faster for canonical (<italic>M</italic> = 558ms, <italic>SD</italic> = 255ms) than for non-canonical viewpoints (<italic>M</italic> = 645 ms, <italic>SD</italic> = 333 ms) (<xref ref-type="fig" rid="F3">Figure 3B</xref>). There was no significant interaction between viewpoint and consistency, β = 0.004, <italic>SE</italic> = 0.005, <italic>t</italic> = 0.83, <italic>p</italic> = 0.41.</p></sec></sec><sec id="S22" sec-type="discussion"><title>Discussion</title><p id="P35">In general, object recognition accuracy was viewpoint dependent, however, there was a significant interaction between viewpoint and consistency. In line with our hypothesis, the viewpoint effect was significantly weaker for consistent scenes and the scene consistency effect was only observed for non-canonical viewpoints (<xref ref-type="fig" rid="F3">Figure 3A</xref>). Non-canonical viewpoints were recognized significantly slower than canonical viewpoints. However, this was unaffected by scene consistency.</p></sec></sec><sec id="S23"><title>General Discussion</title><p id="P36">In the present study, we investigated how scene context information modulates viewpoint-dependent object recognition using 3D models of everyday objects. While providing meaningful context did not eradicate the viewpoint effect fully, it significantly reduced recognition accuracy costs. In line with previous findings (<xref ref-type="bibr" rid="R45">Sastyin et al., 2015</xref>) this supports a model of object recognition that incorporates context (e.g., <xref ref-type="bibr" rid="R2">Bar, 2004</xref>) while dynamically adapting to the amount of available information based not only on visual features of the object (<xref ref-type="bibr" rid="R10">Burgund &amp; Marsolek, 2000</xref>; <xref ref-type="bibr" rid="R23">Hayward &amp; Tarr, 1997</xref>; <xref ref-type="bibr" rid="R27">Jolicoeur, 1990</xref>), but also context. It further motivates models of object constancy - the visual system’s ability to produce representations that are robust to changes in e.g., viewpoint or lighting (e.g., <xref ref-type="bibr" rid="R13">DiCarlo &amp; Cox, 2007</xref>) – that efficiently integrate contextual information and can lead to both viewpoint-dependent and invariant behavior based on available information and the task at hand.</p><p id="P37">A key component of the present study was to generalize previous findings on object-scene processing effects and viewpoint-dependence to depth rotated 3D objects. We want to highlight the importance of generalizing findings from traditional 2D settings to more naturalistic settings and stimuli. <xref ref-type="bibr" rid="R30">Kristjánsson and Draschkow., (2021)</xref> have shown very illustratively for a variety of phenomena that given more naturalistic constraints, a system is able to circumvent e.g., capacity limits by drawing on the rich visual experience of natural environments. While we did not use fully immersive environments, using 3D models offers a more realistic encounter of everyday objects and therefore a more precise measure of viewpoint-dependence in real-world object recognition. It should be noted, however, that there is a trade-off between naturalistic <italic>looking</italic> stimuli (i.e., photographs) and stimuli that more precisely capture naturalistic properties (i.e., 3D structure of objects from different viewpoints) in a highly controlled manner while not <italic>looking</italic> as naturalistic. Here, we opted for providing more naturalistic 3D properties of the displayed objects.</p><p id="P38">From the present study it is unclear what kind of information contained in the scenes was responsible for reducing the viewpoint costs. Rapidly accessed global information such as the gist of the scene (<xref ref-type="bibr" rid="R40">Oliva &amp; Torralba, 2007</xref>) could be the main factor. At the same time, more local information such as the detection and recognition of certain objects in the scene preview could provide information about related possible target objects based on internalized scene-object and object-object regularities (<xref ref-type="bibr" rid="R51">Võ et al., 2019</xref>). Revealing the time course of when what kind of contextual information is integrated to buffer viewpoint effects would provide new insights into how the visual system so effortlessly achieves invariant object recognition.</p><p id="P39">Varying what information is presented during the task (i.e., providing meaningful context vs. showing objects in isolation) is one way to probe the visual system’s ability to overcome processing limitations in viewpoint-dependent object recognition. Alternatively, one could keep the visual input constant but vary the level at which participants have to perform the matching task (<xref ref-type="bibr" rid="R21">Hamm &amp; McMullen, 1998</xref>). If there are object representations that contain more or less viewpoint-dependent or invariant information how does this interact with the integration of contextual information in the form of scene context?</p><p id="P40">Finally, we would like to address that on average performance was high in the matching task throughout all our experiments. These ceiling effects are probably due to the type of task we chose - different from the tasks usually employed to study scene consistency effects (<xref ref-type="bibr" rid="R12">Davenport &amp; Potter, 2004</xref>; <xref ref-type="bibr" rid="R45">Sastyin et al., 2015</xref>). Despite these differences in difficulty, we were able to demonstrate a significant reduction in viewpoint costs by providing meaningful scene context.</p><p id="P41">Past research has made strong advances towards understanding the computations that underly invariant object recognition (<xref ref-type="bibr" rid="R13">DiCarlo &amp; Cox, 2007</xref>). Understanding these mechanisms in isolation is key to understanding object recognition in general. We argue that understanding how the visual system is able to make use of richly structured naturalistic environments to circumvent computational bottlenecks will ultimately lead to better, more robust models of object recognition and inspire approaches in fields such as computer vision (e.g., <xref ref-type="bibr" rid="R6">Bomatter et al., 2021</xref>).</p><p id="P42">To conclude, in the present study we built upon previous findings on object-scene processing and viewpoint dependence by generalizing these effects to depth rotated 3D objects. We highlight the importance of testing capacity limits of object recognition in more naturalistic frameworks in order to build more robust and flexible models and move towards a better understanding of vision under naturalistic constraints.</p></sec></body><back><ack id="S24"><title>Acknowledgements</title><p>This work was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)—project number 222641018—SFB/TRR 135, sub-project C7 to M.L.-H.V., and by the Main-Campus-doctus scholarship of the Stiftung Polytechnische Gesellschaft Frankfurt a. M. to A.K.</p><p>The Wellcome Centre for Integrative Neuroimaging is supported by core funding from the Wellcome Trust (203139/Z/16/Z). The work is supported by the NIHR Oxford Health Biomedical Research Centre. The funders had no role in the decision to publish or in the preparation of the manuscript.</p></ack><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baayen</surname><given-names>RH</given-names></name><name><surname>Davidson</surname><given-names>DJ</given-names></name><name><surname>Bates</surname><given-names>DM</given-names></name></person-group><article-title>Mixed-effects modeling with crossed random effects for subjects and items</article-title><source>Journal of Memory and Language</source><year>2008</year><volume>59</volume><issue>4</issue><fpage>390</fpage><lpage>412</lpage><pub-id pub-id-type="doi">10.1016/j.jml.2007.12.005</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar</surname><given-names>M</given-names></name></person-group><article-title>Visual objects in context</article-title><source>Nature Reviews Neuroscience</source><year>2004</year><volume>5</volume><issue>8</issue><comment>Article 8</comment><pub-id pub-id-type="pmid">15263892</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bates</surname><given-names>D</given-names></name><name><surname>Mächler</surname><given-names>M</given-names></name><name><surname>Bolker</surname><given-names>B</given-names></name><name><surname>Walker</surname><given-names>S</given-names></name></person-group><article-title>Fitting Linear Mixed Effects Models Using lme4</article-title><source>Journal of Statistical Software</source><year>2015</year><volume>67</volume><issue>1</issue><fpage>1</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.18637/jss.v067.i01</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biederman</surname><given-names>I</given-names></name><name><surname>Gerhardstein</surname><given-names>PC</given-names></name></person-group><article-title>Recognizing depth-rotated objects: Evidence and conditions for three-dimensional viewpoint invariance</article-title><source>Journal of Experimental Psychology:Human Perception and Performance</source><year>1993</year><volume>19</volume><issue>6</issue><fpage>1162</fpage><lpage>1182</lpage><pub-id pub-id-type="pmid">8294886</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biederman</surname><given-names>I</given-names></name><name><surname>Mezzanotte</surname><given-names>RJ</given-names></name><name><surname>Rabinowitz</surname><given-names>JC</given-names></name></person-group><article-title>Scene perception: Detecting and judging objects undergoing relational violations</article-title><source>Cognitive Psychology</source><year>1982</year><volume>14</volume><issue>2</issue><fpage>143</fpage><lpage>177</lpage><pub-id pub-id-type="pmid">7083801</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Bomatter</surname><given-names>P</given-names></name><name><surname>Zhang</surname><given-names>M</given-names></name><name><surname>Karev</surname><given-names>D</given-names></name><name><surname>Madan</surname><given-names>S</given-names></name><name><surname>Tseng</surname><given-names>C</given-names></name><name><surname>Kreiman</surname><given-names>G</given-names></name></person-group><source>When Pigs Fly: Contextual Reasoning in Synthetic and Natural Scenes</source><year>2021</year><fpage>255</fpage><lpage>264</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://openaccess.thecvf.com/content/ICCV2021/html/Bomatter_When_Pigs_Fly_Contextual_Reasoning_in_Synthetic_and_Natural_Scenes_ICCV_2021_paper.html">https://openaccess.thecvf.com/content/ICCV2021/html/Bomatter_When_Pigs_Fly_Contextual_Reasoning_in_Synthetic_and_Natural_Scenes_ICCV_2021_paper.html</ext-link></comment></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Box</surname><given-names>GEP</given-names></name><name><surname>Cox</surname><given-names>DR</given-names></name></person-group><article-title>An Analysis of Transformations</article-title><source>Journal of the Royal Statistical Society: Series B (Methodological)</source><year>1964</year><volume>26</volume><issue>2</issue><fpage>211</fpage><lpage>243</lpage><pub-id pub-id-type="doi">10.1111/j.2517-6161.1964.tb00553.x</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brandman</surname><given-names>T</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><article-title>Interaction between Scene and Object Processing Revealed by Human fMRI and MEG Decoding</article-title><source>Journal of Neuroscience</source><year>2017</year><volume>37</volume><issue>32</issue><fpage>7700</fpage><lpage>7710</lpage><pub-id pub-id-type="pmcid">PMC6596648</pub-id><pub-id pub-id-type="pmid">28687603</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0582-17.2017</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bülthoff</surname><given-names>HH</given-names></name><name><surname>Edelman</surname><given-names>S</given-names></name></person-group><article-title>Psychophysical support for a two-dimensional view interpolation theory of object recognition</article-title><source>Proceedings of the National Academy of Sciences</source><year>1992</year><volume>89</volume><issue>1</issue><fpage>60</fpage><lpage>64</lpage><pub-id pub-id-type="pmcid">PMC48175</pub-id><pub-id pub-id-type="pmid">1729718</pub-id><pub-id pub-id-type="doi">10.1073/pnas.89.1.60</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burgund</surname><given-names>ED</given-names></name><name><surname>Marsolek</surname><given-names>CJ</given-names></name></person-group><article-title>Viewpoint-invariant and viewpoint-dependent object recognition in dissociable neural subsystems</article-title><source>Psychonomic Bulletin &amp; Review</source><year>2000</year><volume>7</volume><issue>3</issue><fpage>480</fpage><lpage>489</lpage><pub-id pub-id-type="pmid">11082854</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Charles Leek</surname><given-names>E</given-names></name><name><surname>Johnston</surname><given-names>SJ</given-names></name></person-group><article-title>A polarity effect in misoriented object recognition: The role of polar features in the computation of orientation-invariant shape representations</article-title><source>Visual Cognition</source><year>2006</year><volume>13</volume><issue>5</issue><fpage>573</fpage><lpage>600</lpage><pub-id pub-id-type="doi">10.1080/13506280544000048</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davenport</surname><given-names>JL</given-names></name><name><surname>Potter</surname><given-names>MC</given-names></name></person-group><article-title>Scene Consistency in Object and Background Perception</article-title><source>Psychological Science</source><year>2004</year><volume>15</volume><issue>8</issue><fpage>559</fpage><lpage>564</lpage><pub-id pub-id-type="pmid">15271002</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Cox</surname><given-names>DD</given-names></name></person-group><article-title>Untangling invariant object recognition</article-title><source>Trends in Cognitive Sciences</source><year>2007</year><volume>11</volume><issue>8</issue><fpage>333</fpage><lpage>341</lpage><pub-id pub-id-type="pmid">17631409</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Draschkow</surname><given-names>D</given-names></name></person-group><article-title>Remote virtual reality as a tool for increasing external validity</article-title><source>Nature Reviews Psychology</source><year>2022</year><volume>1</volume><issue>8</issue><comment>Article 8</comment><pub-id pub-id-type="doi">10.1038/s44159-022-00082-8</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Draschkow</surname><given-names>D</given-names></name><name><surname>Kallmayer</surname><given-names>M</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name></person-group><article-title>When Natural Behavior Engages Working Memory</article-title><source>Current Biology</source><year>2021</year><volume>31</volume><issue>4</issue><fpage>869</fpage><lpage>874</lpage><elocation-id>e5</elocation-id><pub-id pub-id-type="pmcid">PMC7902904</pub-id><pub-id pub-id-type="pmid">33278355</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2020.11.013</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Draschkow</surname><given-names>D</given-names></name><name><surname>Võ</surname><given-names>ML-H</given-names></name></person-group><article-title>Scene grammar shapes the way we interact with objects, strengthens memories, and speeds search</article-title><source>Scientific Reports</source><year>2017</year><volume>7</volume><issue>1</issue><comment>Article 1</comment><pub-id pub-id-type="pmcid">PMC5705766</pub-id><pub-id pub-id-type="pmid">29184115</pub-id><pub-id pub-id-type="doi">10.1038/s41598-017-16739-x</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Edelman</surname><given-names>S</given-names></name></person-group><article-title>Class similarity and viewpoint invariance in the recognition of 3D objects</article-title><source>Biological Cybernetics</source><year>1995</year><volume>72</volume><issue>3</issue><fpage>207</fpage><lpage>220</lpage><pub-id pub-id-type="doi">10.1007/BF00201485</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foster</surname><given-names>DH</given-names></name><name><surname>Gilson</surname><given-names>SJ</given-names></name></person-group><article-title>Recognizing novel three-dimensional objects by summing signals from parts and views</article-title><source>Proceedings of the Royal Society of London Series B: Biological Sciences</source><year>2002</year><volume>269</volume><issue>1503</issue><fpage>1939</fpage><lpage>1947</lpage><pub-id pub-id-type="pmcid">PMC1691113</pub-id><pub-id pub-id-type="pmid">12350257</pub-id><pub-id pub-id-type="doi">10.1098/rspb.2002.2119</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gauthier</surname><given-names>I</given-names></name><name><surname>Hayward</surname><given-names>WG</given-names></name><name><surname>Tarr</surname><given-names>MJ</given-names></name><name><surname>Anderson</surname><given-names>AW</given-names></name><name><surname>Skudlarski</surname><given-names>P</given-names></name><name><surname>Gore</surname><given-names>JC</given-names></name></person-group><article-title>BOLD Activity during Mental Rotation and Viewpoint-Dependent Object Recognition</article-title><source>Neuron</source><year>2002</year><volume>34</volume><issue>1</issue><fpage>161</fpage><lpage>171</lpage><pub-id pub-id-type="pmid">11931750</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graf</surname><given-names>M</given-names></name></person-group><article-title>Coordinate transformations in object recognition</article-title><source>Psychological Bulletin</source><year>2006</year><volume>132</volume><issue>6</issue><fpage>920</fpage><lpage>945</lpage><pub-id pub-id-type="pmid">17073527</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamm</surname><given-names>JP</given-names></name><name><surname>McMullen</surname><given-names>PA</given-names></name></person-group><article-title>Effects of orientation on the identification of rotated objects depend on the level of identity</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>1998</year><volume>24</volume><issue>2</issue><fpage>413</fpage><lpage>426</lpage><pub-id pub-id-type="pmid">9606109</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayward</surname><given-names>WG</given-names></name></person-group><article-title>After the viewpoint debate: Where next in object recognition?</article-title><source>Trends in Cognitive Sciences</source><year>2003</year><volume>7</volume><issue>10</issue><fpage>425</fpage><lpage>427</lpage><pub-id pub-id-type="pmid">14550482</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayward</surname><given-names>WG</given-names></name><name><surname>Tarr</surname><given-names>MJ</given-names></name></person-group><article-title>Testing conditions for viewpoint invariance in object recognition</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>1997</year><volume>23</volume><issue>5</issue><fpage>1511</fpage><lpage>1521</lpage><pub-id pub-id-type="pmid">9411023</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayward</surname><given-names>WG</given-names></name><name><surname>Williams</surname><given-names>P</given-names></name></person-group><article-title>Viewpoint Dependence and Object Discriminability</article-title><source>Psychological Science</source><year>2000</year><volume>11</volume><issue>1</issue><fpage>7</fpage><lpage>12</lpage><pub-id pub-id-type="pmid">11228847</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Helbing</surname><given-names>J</given-names></name><name><surname>Draschkow</surname><given-names>D</given-names></name><name><surname>L-H Võ</surname><given-names>M</given-names></name></person-group><article-title>Auxiliary Scene-Context Information Provided by Anchor Objects Guides Attention and Locomotion in Natural Search Behavior</article-title><source>Psychological Science</source><year>2022</year><volume>33</volume><issue>9</issue><fpage>1463</fpage><lpage>1476</lpage><pub-id pub-id-type="pmid">35942922</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Helbing</surname><given-names>J</given-names></name><name><surname>Draschkow</surname><given-names>D</given-names></name><name><surname>Võ</surname><given-names>ML-H</given-names></name></person-group><article-title>Search superiority: Goal-directed attentional allocation creates more reliable incidental identity and location memory than explicit encoding in naturalistic virtual environments</article-title><source>Cognition</source><year>2020</year><volume>196</volume><elocation-id>104147</elocation-id><pub-id pub-id-type="pmid">32004760</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jolicoeur</surname><given-names>P</given-names></name></person-group><article-title>Identification of Disoriented Objects: A Dual-systems Theory</article-title><source>Mind &amp; Language</source><year>1990</year><volume>5</volume><issue>4</issue><fpage>387</fpage><lpage>410</lpage><pub-id pub-id-type="doi">10.1111/j.1468-0017.1990.tb00170.x</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Josephs</surname><given-names>EL</given-names></name><name><surname>Draschkow</surname><given-names>D</given-names></name><name><surname>Wolfe</surname><given-names>JM</given-names></name><name><surname>Võ</surname><given-names>ML-H</given-names></name></person-group><article-title>Gist in time: Scene semantics and structure enhance recall of searched objects</article-title><source>Acta Psychologica</source><year>2016</year><volume>169</volume><fpage>100</fpage><lpage>108</lpage><pub-id pub-id-type="pmcid">PMC4987188</pub-id><pub-id pub-id-type="pmid">27270227</pub-id><pub-id pub-id-type="doi">10.1016/j.actpsy.2016.05.013</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kliegl</surname><given-names>R</given-names></name><name><surname>Wei</surname><given-names>P</given-names></name><name><surname>Dambacher</surname><given-names>M</given-names></name><name><surname>Yan</surname><given-names>M</given-names></name><name><surname>Zhou</surname><given-names>X</given-names></name></person-group><article-title>Experimental Effects and Individual Differences in Linear Mixed Models: Estimating the Relationship between Spatial, Object, and Attraction Effects in Visual Attention</article-title><source>Frontiers in Psychology</source><year>2011</year><volume>1</volume><comment><ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/article/10.3389/fpsyg.2010.00238">https://www.frontiersin.org/article/10.3389/fpsyg.2010.00238</ext-link></comment><pub-id pub-id-type="pmcid">PMC3153842</pub-id><pub-id pub-id-type="pmid">21833292</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2010.00238</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kristjánsson</surname><given-names>Á</given-names></name><name><surname>Draschkow</surname><given-names>D</given-names></name></person-group><article-title>Keeping it real: Looking beyond capacity limits in visual cognition</article-title><source>Attention, Perception, &amp; Psychophysics</source><year>2021</year><volume>83</volume><issue>4</issue><fpage>1375</fpage><lpage>1390</lpage><pub-id pub-id-type="pmcid">PMC8084831</pub-id><pub-id pub-id-type="pmid">33791942</pub-id><pub-id pub-id-type="doi">10.3758/s13414-021-02256-7</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuznetsova</surname><given-names>A</given-names></name><name><surname>Brockhoff</surname><given-names>PB</given-names></name><name><surname>Christensen</surname><given-names>RHB</given-names></name></person-group><article-title>lmerTest Package: Tests in Linear Mixed Effects Models</article-title><source>Journal of Statistical Software</source><year>2017</year><volume>82</volume><issue>13</issue><fpage>1</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.18637/jss.v082.i13</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lauer</surname><given-names>T</given-names></name><name><surname>Schmidt</surname><given-names>F</given-names></name><name><surname>Võ</surname><given-names>ML-H</given-names></name></person-group><article-title>The role of contextual materials in object recognition</article-title><source>Scientific Reports</source><year>2021</year><volume>11</volume><issue>1</issue><comment>Article 1</comment><pub-id pub-id-type="pmcid">PMC8578445</pub-id><pub-id pub-id-type="pmid">34753999</pub-id><pub-id pub-id-type="doi">10.1038/s41598-021-01406-z</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leek</surname><given-names>EC</given-names></name><name><surname>Atherton</surname><given-names>CJ</given-names></name><name><surname>Thierry</surname><given-names>G</given-names></name></person-group><article-title>Computational mechanisms of object constancy for visual recognition revealed by event-related potentials</article-title><source>Vision Research</source><year>2007</year><volume>47</volume><issue>5</issue><fpage>706</fpage><lpage>713</lpage><pub-id pub-id-type="pmid">17267003</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Lenth</surname><given-names>RV</given-names></name></person-group><source>emmeans: Estimated Marginal Means, aka Least-Squares Means. R package version 1.7.2</source><year>2022</year><comment><ext-link ext-link-type="uri" xlink:href="https://CRAN.R-project.org/package=emmeans">https://CRAN.R-project.org/package=emmeans</ext-link></comment></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Logothetis</surname><given-names>NK</given-names></name><name><surname>Pauls</surname><given-names>J</given-names></name><name><surname>Bülthoff</surname><given-names>HH</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name></person-group><article-title>View-dependent object recognition by monkeys</article-title><source>Current Biology</source><year>1994</year><volume>4</volume><issue>5</issue><fpage>401</fpage><lpage>414</lpage><pub-id pub-id-type="pmid">7922354</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lowe</surname><given-names>DG</given-names></name></person-group><article-title>Three-dimensional object recognition from single two-dimensional images</article-title><source>Artificial Intelligence</source><year>1987</year><volume>31</volume><issue>3</issue><fpage>355</fpage><lpage>395</lpage><pub-id pub-id-type="doi">10.1016/0004-3702(87)90070-1</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Marr</surname><given-names>D</given-names></name><name><surname>Nishihara</surname><given-names>HK</given-names></name><name><surname>Brenner</surname><given-names>S</given-names></name></person-group><article-title>Representation and recognition of the spatial organization of three-dimensional shapes</article-title><source>Proceedings of the Royal Society of London Series B Biological Sciences</source><year>1978</year><volume>200</volume><issue>1140</issue><fpage>269</fpage><lpage>294</lpage><pub-id pub-id-type="doi">10.1098/rspb.1978.0020</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathôt</surname><given-names>S</given-names></name><name><surname>Schreij</surname><given-names>D</given-names></name><name><surname>Theeuwes</surname><given-names>J</given-names></name></person-group><article-title>OpenSesame: An open-source, graphical experiment builder for the social sciences</article-title><source>Behavior Research Methods</source><year>2012</year><volume>44</volume><issue>2</issue><fpage>314</fpage><lpage>324</lpage><pub-id pub-id-type="pmcid">PMC3356517</pub-id><pub-id pub-id-type="pmid">22083660</pub-id><pub-id pub-id-type="doi">10.3758/s13428-011-0168-7</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Öhlschläger</surname><given-names>S</given-names></name><name><surname>Võ</surname><given-names>ML-H</given-names></name></person-group><article-title>SCEGRAM: An image database for semantic and syntactic inconsistencies in scenes</article-title><source>Behavior Research Methods</source><year>2017</year><volume>49</volume><issue>5</issue><fpage>1780</fpage><lpage>1791</lpage><pub-id pub-id-type="pmid">27800578</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oliva</surname><given-names>A</given-names></name><name><surname>Torralba</surname><given-names>A</given-names></name></person-group><article-title>The role of context in object recognition</article-title><source>Trends in Cognitive Sciences</source><year>2007</year><volume>11</volume><issue>12</issue><fpage>520</fpage><lpage>527</lpage><pub-id pub-id-type="pmid">18024143</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palmer</surname><given-names>tephen E</given-names></name></person-group><article-title>The effects of contextual scenes on the identification of objects</article-title><source>Memory &amp; Cognition</source><year>1975</year><volume>3</volume><issue>5</issue><fpage>519</fpage><lpage>526</lpage><pub-id pub-id-type="pmid">24203874</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poggio</surname><given-names>T</given-names></name><name><surname>Edelman</surname><given-names>S</given-names></name></person-group><article-title>A network that learns to recognize three-dimensional objects</article-title><source>Nature</source><year>1990</year><volume>343</volume><issue>6255</issue><comment>Article 6255</comment><pub-id pub-id-type="pmid">2300170</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="book"><collab>R Core Team</collab><source>R: A language and environment for statistical computing</source><year>2021</year><publisher-name>R Foundation for Statistical Computing</publisher-name><publisher-loc>Vienna, Austria</publisher-loc><comment>URL<ext-link ext-link-type="uri" xlink:href="https://www.R-project.org/">https://www.R-project.org/</ext-link></comment></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratan Murty</surname><given-names>NA</given-names></name><name><surname>Arun</surname><given-names>SP</given-names></name></person-group><article-title>Dynamics of 3D view invariance in monkey inferotemporal cortex</article-title><source>Journal of Neurophysiology</source><year>2015</year><volume>113</volume><issue>7</issue><fpage>2180</fpage><lpage>2194</lpage><pub-id pub-id-type="pmcid">PMC4416554</pub-id><pub-id pub-id-type="pmid">25609108</pub-id><pub-id pub-id-type="doi">10.1152/jn.00810.2014</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sastyin</surname><given-names>G</given-names></name><name><surname>Niimi</surname><given-names>R</given-names></name><name><surname>Yokosawa</surname><given-names>K</given-names></name></person-group><article-title>Does object view influence the scene consistency effect?</article-title><source>Attention, Perception, &amp; Psychophysics</source><year>2015</year><volume>77</volume><issue>3</issue><fpage>856</fpage><lpage>866</lpage><pub-id pub-id-type="pmid">25522833</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stankiewicz</surname><given-names>BJ</given-names></name></person-group><article-title>Empirical evidence for independent dimensions in the visual representation of three-dimensional shape</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>2002</year><volume>28</volume><issue>4</issue><fpage>913</fpage><lpage>932</lpage><pub-id pub-id-type="pmid">12190258</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tarr</surname><given-names>MJ</given-names></name><name><surname>Bülthoff</surname><given-names>HH</given-names></name></person-group><article-title>Is human object recognition better described by geon structural descriptions or by multiple views? Comment on <xref ref-type="bibr" rid="R4">Biederman and Gerhardstein (1993)</xref></article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>1995</year><volume>21</volume><issue>6</issue><fpage>1494</fpage><lpage>1505</lpage><pub-id pub-id-type="pmid">7490590</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tarr</surname><given-names>MJ</given-names></name><name><surname>Pinker</surname><given-names>S</given-names></name></person-group><article-title>Mental rotation and orientation-dependence in shape recognition</article-title><source>Cognitive Psychology</source><year>1989</year><volume>21</volume><issue>2</issue><fpage>233</fpage><lpage>282</lpage><pub-id pub-id-type="pmid">2706928</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vanrie</surname><given-names>J</given-names></name><name><surname>Béatse</surname><given-names>E</given-names></name><name><surname>Wagemans</surname><given-names>J</given-names></name><name><surname>Sunaert</surname><given-names>S</given-names></name><name><surname>Van Hecke</surname><given-names>P</given-names></name></person-group><article-title>Mental rotation versus invariant features in object perception from different viewpoints: An fMRI study</article-title><source>Neuropsychologia</source><year>2002</year><volume>40</volume><issue>7</issue><fpage>917</fpage><lpage>930</lpage><pub-id pub-id-type="pmid">11900744</pub-id></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Võ</surname><given-names>ML-H</given-names></name></person-group><article-title>The meaning and structure of scenes</article-title><source>Vision Research</source><year>2021</year><volume>181</volume><fpage>10</fpage><lpage>20</lpage><pub-id pub-id-type="pmid">33429218</pub-id></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Võ</surname><given-names>ML-H</given-names></name><name><surname>Boettcher</surname><given-names>SE</given-names></name><name><surname>Draschkow</surname><given-names>D</given-names></name></person-group><article-title>Reading scenes: How scene grammar guides attention and aids perception in real-world environments</article-title><source>Current Opinion in Psychology</source><year>2019</year><volume>29</volume><fpage>205</fpage><lpage>210</lpage><pub-id pub-id-type="pmid">31051430</pub-id></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Võ</surname><given-names>ML-H</given-names></name><name><surname>Henderson</surname><given-names>JM</given-names></name></person-group><article-title>Does gravity matter? Effects of semantic and syntactic inconsistencies on the allocation of attention during scene perception</article-title><source>Journal of Vision</source><year>2009</year><volume>9</volume><issue>3</issue><fpage>24</fpage><pub-id pub-id-type="pmid">19757963</pub-id></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Võ</surname><given-names>ML-H</given-names></name><name><surname>Wolfe</surname><given-names>JM</given-names></name></person-group><article-title>The interplay of episodic and semantic memory in guiding repeated search in scenes</article-title><source>Cognition</source><year>2013a</year><volume>126</volume><issue>2</issue><fpage>198</fpage><lpage>212</lpage><pub-id pub-id-type="pmcid">PMC3928147</pub-id><pub-id pub-id-type="pmid">23177141</pub-id><pub-id pub-id-type="doi">10.1016/j.cognition.2012.09.017</pub-id></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Võ</surname><given-names>ML-H</given-names></name><name><surname>Wolfe</surname><given-names>JM</given-names></name></person-group><article-title>Differential Electrophysiological Signatures of Semantic and Syntactic Scene Processing</article-title><source>Psychological Science</source><year>2013b</year><volume>24</volume><issue>9</issue><fpage>1816</fpage><lpage>1823</lpage><pub-id pub-id-type="pmcid">PMC4838599</pub-id><pub-id pub-id-type="pmid">23842954</pub-id><pub-id pub-id-type="doi">10.1177/0956797613476955</pub-id></element-citation></ref><ref id="R55"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wickham</surname><given-names>H</given-names></name></person-group><source>ggplot2: Elegant Graphics for Data Analysis</source><year>2016</year><publisher-name>Springer-Verlag</publisher-name><publisher-loc>New York</publisher-loc><comment>2016</comment></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>KD</given-names></name><name><surname>Farah</surname><given-names>MJ</given-names></name></person-group><article-title>When does the visual system use viewpoint-invariant representations during recognition?</article-title><source>Cognitive Brain Research</source><year>2003</year><volume>16</volume><issue>3</issue><fpage>399</fpage><lpage>415</lpage><pub-id pub-id-type="pmid">12706220</pub-id></element-citation></ref><ref id="R57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zisserman</surname><given-names>A</given-names></name><name><surname>Forsyth</surname><given-names>D</given-names></name><name><surname>Mundy</surname><given-names>J</given-names></name><name><surname>Rothwell</surname><given-names>C</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Pillow</surname><given-names>N</given-names></name></person-group><article-title>3D object recognition using invariance</article-title><source>Artificial Intelligence</source><year>1995</year><volume>78</volume><issue>1</issue><fpage>239</fpage><lpage>288</lpage><pub-id pub-id-type="doi">10.1016/0004-3702(95)00023-2</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><p>Trial procedures for the matching task in Experiment 1a and 1b (A) and Experiment 2 (B). The object was presented in colour in Experiment 1a and greyscaled in Experiment 1b. Note that the depicted labels are in English for visualization purpose. Feedback was only provided in case of incorrect responses.</p></caption><graphic xlink:href="EMS157286-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><p>Partial effect plots of the interactions of viewpoint (0°, 60°, 120°, 180° 240°, 300°) and match (match vs. mismatch) on accuracy for Experiment 1a (coloured; A), and Experiment 1b (greyscaled; C), and the effect of viewpoint on RT for Experiment.</p></caption><graphic xlink:href="EMS157286-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><p>Experiment 2 accuracy difference scores per participant (canonical vs. non-canonical) for consistent and inconsistent scene backgrounds (A). Adjusted response times (B) were obtained with the remef package (Hohenstein &amp; Kliegl, 2021). *p &lt; .05. ***p &lt; .001.</p></caption><graphic xlink:href="EMS157286-f003"/></fig></floats-group></article>