<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS158004</article-id><article-id pub-id-type="doi">10.1101/2022.12.01.518716</article-id><article-id pub-id-type="archive">PPR578770</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">2</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Visualization &amp; Quality Control Tools for Large-scale Multiplex Tissue Analysis in TissUUmaps 3</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Behanova</surname><given-names>Andrea</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Avenel</surname><given-names>Christophe</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Andersson</surname><given-names>Axel</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Chelebian</surname><given-names>Eduard</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Klemm</surname><given-names>Anna</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Wik</surname><given-names>Lina</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Östman</surname><given-names>Arne</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Wählby</surname><given-names>Carolina</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib></contrib-group><aff id="A1"><label>1</label>Department of Information Technology and SciLifeLab BioImage Informatics Facility, Uppsala University, Uppsala, Sweden</aff><aff id="A2"><label>2</label>Department of Oncology-Pathology, Karolinska Institutet, Solna, Sweden</aff><pub-date pub-type="nihms-submitted"><day>04</day><month>12</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>02</day><month>12</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Large-scale multiplex tissue analysis aims to understand processes such as development and tumor formation by studying the occurrence and interaction of cells in local environments in e.g. tissue samples from patient cohorts. A typical procedure in the analysis is to delineate individual cells, classify them into cell types, and analyze their spatial relationships. All steps come with a number of challenges, and to address them and identify the bottlenecks of the analysis, it is necessary to include quality control tools in the analysis workflow. This makes it possible to optimize the steps and adjust settings in order to get better and more precise results. Additionally, the development of automated approaches for tissue analysis requires visual verification to reduce skepticism with regard to the accuracy of the results. Quality control tools could be used to build users’ trust in automated approaches. In this paper, we present three plugins for visualization and quality control in large-scale multiplex tissue analysis of microscopy images. The first plugin focuses on the quality of cell staining, the second one was made for interactive evaluation and comparison of different cell classification results, and the third one serves for reviewing interactions of different cell types.</p></abstract><kwd-group><kwd>spatial omics</kwd><kwd>cell classification</kwd><kwd>visualization</kwd><kwd>quality control</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><label>1</label><title>Introduction</title><p id="P2">Understanding cell distributions and interactions in tissue plays a crucial role in order to fully comprehend organism development, healing, and homeostasis [<xref ref-type="bibr" rid="R1">1</xref>]. Particularly important for this is the spatial distribution of different cell types, which can be identified by various spatial omics techniques, such as multiplexed spatially resolved analysis of gene expression [<xref ref-type="bibr" rid="R2">2</xref>, <xref ref-type="bibr" rid="R3">3</xref>, <xref ref-type="bibr" rid="R4">4</xref>, <xref ref-type="bibr" rid="R5">5</xref>] or multiplex immunohistochemical (IHC) staining microscopy [<xref ref-type="bibr" rid="R6">6</xref>, <xref ref-type="bibr" rid="R7">7</xref>, <xref ref-type="bibr" rid="R8">8</xref>, <xref ref-type="bibr" rid="R9">9</xref>, <xref ref-type="bibr" rid="R10">10</xref>]. Analysis of gene expression and IHC should be considered complementary techniques, where gene expression is related to the function of the cells and IHC approaches provide a direct understanding of marker protein expression, post-translational modifications, and sub-cellular localization [<xref ref-type="bibr" rid="R11">11</xref>]. It has also been shown that multiplex IHC has significantly higher performance than gene expression profiling for predicting objective response to a certain therapy [<xref ref-type="bibr" rid="R12">12</xref>]. Multiplex IHC makes it possible to identify multiple cell types in parallel and enables the study of cell-cell interactions and local cell environments. The first choice of analysis is often semi-automated visual/manual cell classification by intensity thresholding. There is a trend to move towards fully automated tools, not only to speed up analysis but also to reduce bias. However, the development of automated processing steps requires visual inspection and quality control tools to optimize settings and confirm the correctness. These tools help to increase the user’s trust in automated systems, especially if compared to some kind of validated ground truth.</p><p id="P3">More steps in an analysis approach bring more challenges, and there are several essential steps required in order to be able to quantify cell-cell interaction. The first step is IHC staining, coming with challenges including weak staining, high background intensity, over-staining or nonspecific staining [<xref ref-type="bibr" rid="R13">13</xref>, <xref ref-type="bibr" rid="R14">14</xref>]. The second step, microscopy imaging, bears obstacles such as nonuniform illumination or low/high contrast [<xref ref-type="bibr" rid="R15">15</xref>]. Cell classification is the next step and brings challenges such as misclassification, and false negative classification, especially if cells are crowded and partially overlapping. The last step is the quantification of interaction, which can be done by various methods [<xref ref-type="bibr" rid="R16">16</xref>], and selecting which one to use might not be trivial. To conclude, there are many challenges in the analysis workflow and all these steps have a large impact on the final result. Careful quality control combined with visual assessment is necessary to compare and validate different options.</p><p id="P4">We present here three plugins for quality control and visual assessment of the analysis intermediate and final results. The first one is for quality control and comparison of cell staining. The second one is for quality control and comparison of cell classification. And the third one is for quality control and visualization of cell-cell interactions. We also provide a new approach to quantify cell-cell interactions that takes local tissue structures into account. Other tools for evaluating cell-cell interaction exist, such as ImaCytE [<xref ref-type="bibr" rid="R17">17</xref>], which can highlight the interaction of protein expression profiles in microenvironments. However, ImaCytE was developed for Imaging Mass Cytometry data, unlike TissUUmaps 3 [<xref ref-type="bibr" rid="R18">18</xref>] which is suitable for any type of marker data.</p><p id="P5">In this paper, present each of the plugins together with example data, and share the plugins via <ext-link ext-link-type="uri" xlink:href="https://tissuumaps.github.io/">https://tissuumaps.github.io/</ext-link>.</p></sec><sec id="S2"><label>2</label><title>Software and methods</title><p id="P6">In this section, we present newly developed plugins for the free and open-source software TissUUmaps 3 [<xref ref-type="bibr" rid="R18">18</xref>]. TissUUmaps 3 is a browser-based tool for fast visualization and exploration of millions of data points overlaying gigapixel-sized multi-layered images. TissUUmaps 3 can be used as a web service or locally on your computer and allows users to share regions of interest and local statistics. The three plugins are Visualization comparison and quality control of cell staining (StainV&amp;QC), Visualization comparison and quality control of cell classification (ClassV&amp;QC) and Visualization and quality control of cell-cell interactions (InteractionV&amp;QC).</p><sec id="S3"><label>2.1</label><title>Overview</title><p id="P7">The typical steps from multiplex microscopy image data collection to quantitative analysis of cell-cell interactions are shown in <xref ref-type="fig" rid="F1">Figure 1</xref>, together with the necessary quality controls and corresponding plugins. Initial steps, such as cell segmentation, feature extraction, and cell classification, require external tools such as CellProfiler [<xref ref-type="bibr" rid="R19">19</xref>, <xref ref-type="bibr" rid="R20">20</xref>], CellProfiler Analyst [<xref ref-type="bibr" rid="R21">21</xref>] and QuPath [<xref ref-type="bibr" rid="R22">22</xref>]. Accumulation scores quantifying cell-cell interactions can be calculated by tools such as Squidpy [<xref ref-type="bibr" rid="R23">23</xref>] and histoCAT [<xref ref-type="bibr" rid="R24">24</xref>]. The results are visually inspected and checked for quality by the three plugins. Plugin <italic>StainV&amp;QC</italic> compares raw data from different cores and the effect of image preprocessing methods. Plugin <italic>ClassV&amp;QC</italic> visually verifies/discards classification results or compares classification results achieved by different manual and/or automated classification approaches. Plugin <italic>InteractionV&amp;QC</italic> visually verifies/discards and investigates non-random tissue patterns and spatial cell-cell interactions.</p></sec><sec id="S4"><label>2.2</label><title>Plugins</title><sec id="S5"><label>2.2.1</label><title>Visualization comparison and quality control of cell staining (StainV&amp;QC)</title><p id="P8">Our plugin for visualization comparison and quality control of cell staining, StainV&amp;QC, can visualize a feature space or compare several feature spaces of different samples. With feature space, we refer to the space spanned by all the different measurements extracted from individual cells, such as mean intensity per cell and image channel, but also more advanced features such as measurements of texture and shape. <xref ref-type="fig" rid="F2">Figure 2</xref> shows the workflow of required steps in order to use the StainV&amp;QC plugin. The first step is to segment individual cells from the multiplexed microscopy data. The second step is to use a feature extractor on segmented cells to extract features. Then either visualize two features at the same time in the plugin or use a dimensionality reduction technique, such as UMAP [<xref ref-type="bibr" rid="R25">25</xref>], to visualize all the features but in a lower dimension. It is also possible to visualize several samples at the same time to investigate if their feature spaces match or if some pre-processing steps are necessary to add before further steps. Hypothetically, feature spaces of the same tissue type should approximately match, assuming that at least some portion of the cells in the tissue should be of the same type, and therefore have similar feature spaces. Shifts in feature spaces are typically due to variations in staining and can be corrected by normalization [<xref ref-type="bibr" rid="R26">26</xref>]. Such correction is vital for downstream cell classification.</p><p id="P9">The main screen of the StainV&amp;QC plugin can be seen in subfigure 2 - c). The left side shows a microscopy image of two tissue samples overlaid by cell markers with different colors per sample, this element is called the Spatial viewport. The right side displays the feature space, where the user can interactively select markers and instantaneously see the corresponding markers in the Spatial viewport. The color of the markers in the feature space is the same as the color of the markers in the Spatial viewport. A detailed description of the plugin settings can be found in the Supplementary material - <xref ref-type="supplementary-material" rid="SD1">Figure S1.1</xref>. This plugin can be found at <ext-link ext-link-type="uri" xlink:href="https://tissuumaps.github.io/TissUUmaps/plugins/">https://tissuumaps.github.io/TissUUmaps/plugins/</ext-link>.</p><p id="P10">StainV&amp;QC can be very useful to identify upstream analysis challenges such as weak staining, high background intensity, over-staining, nonspecific staining, nonuniform illumination in imaging, or low/high contrast.</p></sec><sec id="S6"><label>2.2.2</label><title>Visualization comparison and quality control of cell classification (ClassV&amp;QC)</title><p id="P11">The plugin for visualization, comparison and quality control of cell classification, ClassV&amp;QC, can visualize and set side-by-side results of various techniques for cell classification. <xref ref-type="fig" rid="F3">Figure 3</xref> shows that the workflow requires that cell classification has first been applied to the cells represented by the feature space. Cell identification (segmentation) followed by cell classification can be done by several approaches, such as traditional manual annotations by an expert, or by semi-automated or automated categorization of the cells by specific tools (e.g. CellProfiler [<xref ref-type="bibr" rid="R19">19</xref>, <xref ref-type="bibr" rid="R20">20</xref>] or QuPath [<xref ref-type="bibr" rid="R22">22</xref>]). Subsequently, the results are visualized in the ClassV&amp;QC plugin, and different approaches for detection and classification (e.g. manual vs automated or two different automated methods) can be compared and evaluated also in relation to original input data, shown as a mosaic of cut-outs cropped from a region around a selected cell across the channels of the raw data.</p><p id="P12">The main screen of the plugin can be seen in <xref ref-type="fig" rid="F3">figure 3 - c</xref>). The left side shows the Spatial viewport. When comparing two approaches, the result of each approach is presented as a different shape and size of markers. The first approach has bigger circular markers and the second approach has smaller star markers on top of the bigger circular markers. Hence, differences of classification between approaches can be detected easily. The right side of the Spatial viewport shows image patches cropped around a selected cell (red square in Spatial viewport) in all the stain channels of the dataset. This tool can help to investigate potential staining issues associated with cells that are assigned the wrong class. The right side of <xref ref-type="fig" rid="F3">figure 3 - c</xref>) shows an interactive confusion matrix when comparing two cell classification approaches. A confusion matrix is a way to compare the result of two classification results, typically manually or semi-manually annotated cells (expected class), and the results of a fully automated classification method (predicted class). It is also possible to compare the performance of two different automated classification methods. Each row of the matrix represents the elements in an expected class while each column represents the elements in a predicted class. The user can click on the elements of the confusion matrix and only cells counted in that matrix element are displayed on the Spatial viewport. This function requires that the two approaches that are compared have the same cell segmentation/identification as input so that the order of the cells matches. Non-matching cell IDs only enable visualization of cell type distributions and patches of microscopy data. A detailed description of the plugin settings can be found in the Supplementary material - <xref ref-type="supplementary-material" rid="SD1">Figure S2.1</xref>. This plugin can be found at <ext-link ext-link-type="uri" xlink:href="https://tissuumaps.github.io/TissUUmaps/plugins/">https://tissuumaps.github.io/TissUUmaps/plugins/</ext-link>.</p><p id="P13">When using the patches to visually evaluate the quality of the classification, we expect that the objects in the original image data always correspond to the true biological structures which were meant to be imaged. For example, if the stain is supposed to bind to nuclei, we expect to see exclusively nuclei in the final image. The visualization of the patches of the microscopy data then points out false negative classification or wrong classification.</p></sec><sec id="S7"><label>2.2.3</label><title>Visualization and quality control of cell-cell interactions (InteractionV&amp;QC)</title><p id="P14">Cell-cell interaction can be defined as two (or more) cell types with a certain distance to each other that appear with a higher frequency than what would be expected by random distribution. A spatial distribution pattern between two cell types that is statistically significant can indicate involvement in some kind of interaction. For example, immune cells appearing non-randomly close to tumor cells may indicate some kind of interaction. The amount of interaction, and its significance as compared to interaction by chance, can be quantified by several approaches as summarized in the review [<xref ref-type="bibr" rid="R16">16</xref>].</p><p id="P15">Apart from the visualization and quality control plugins, we also present an approach to quantify interactions, using a neighborhood enrichment test (NET) that compares observed cell neighborhoods to randomized patterns. A previous approach to analyzing neighborhood enrichment was presented by Palla et al in SquidPy [<xref ref-type="bibr" rid="R23">23</xref>]. The NET presented here automatically compensates for tissue structure variation and can identify cell types that are distributed non-randomly in relation to one another, independent of the intrinsic tissue patterns. A cell neighborhood is defined as all cells within a distance <italic>k</italic> defined by the user. The same distance <italic>k</italic> is used across the whole tissue, independent of cell density. The NET score compares the observed neighborhood relationship between two cell types <italic>A</italic> and <italic>B</italic>, to what could be observed if one of the cell types was randomly distributed. First, the average number of cells of type <italic>B</italic> in the local neighborhood of cell type <italic>A</italic> (defined by the distance <italic>k</italic>) is measured. This gives <italic>n<sub>AB</sub></italic>. Next, the cells of type <italic>B</italic> are randomized to the positions of all other cell types (excluding <italic>A),</italic> while all the cells of type <italic>A</italic> are kept in their original positions. The randomization process is repeated many times (specified by the user, e.g. 1000 times), and for each randomization, the average number of cells of type <italic>B</italic> in the local neighborhood of cell type <italic>A</italic> is measured. Finally, the mean (<italic>μ<sub>AB</sub></italic>) and the standard deviation (<italic>σ<sub>AB</sub></italic>) of cell count averages over all randomizations is calculated, and the NET score is defined as <disp-formula id="FD1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:mi>N</mml:mi><mml:mi>E</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P16">A NET score close to zero indicates that cell types A and B are randomly distributed to one another, while a positive score means that they are attracted to one another, and a negative score indicates that there is a repulsion between the cell types. The output is saved as a csv-file. The code for running the NET analysis can be found as a Jupyter notebook at <ext-link ext-link-type="uri" xlink:href="https://github.com/BIIFSweden/accScore">https://github.com/BIIFSweden/accScore</ext-link>.</p><p id="P17">Our plugin for visualization and quality control of cell-cell interactions, InteractionV&amp;QC, serves as a tool to visualize and understand non-random tissue patterns and spatial cell-cell interactions, such as those quantified by NET. As presented in <xref ref-type="fig" rid="F4">figure 4</xref>, firstly, cells in microscopy images are classified and once the classification is regarded as valid (e.g. using the ClassV&amp;QC plugin), accumulation scores are calculated by spatial statistic tools, such as NET, Squidpy [<xref ref-type="bibr" rid="R23">23</xref>], or histoCAT [<xref ref-type="bibr" rid="R24">24</xref>]. The resulting matrix has to be saved as a .csv file for uploading to the InteractionV&amp;QC plugin.</p><p id="P18">The main screen of the plugin can be seen in <xref ref-type="fig" rid="F4">figure 4 - c</xref>). The left side shows the Spatial viewport and the right side represents a visualization of the neighborhood enrichment test NET. The axes of the matrix are colored based on different cell types as can be seen in the legend, and the color code is the same as the corresponding markers in the Spatial viewport. This matrix is interactive and the user can click on the elements of the matrix and only those two corresponding cell types are displayed on the Spatial viewport. If the interaction between two cell types is quantified as significant, this tool helps to visually explore the interactions and build an understanding of why the interaction may be significant. A detailed description of the plugin settings can be found in the Supplementary material - <xref ref-type="supplementary-material" rid="SD1">Figure S3.1</xref>. This plugin can be found at <ext-link ext-link-type="uri" xlink:href="https://tissuumaps.github.io/TissUUmaps/plugins/">https://tissuumaps.github.io/TissUUmaps/plugins/</ext-link>.</p><p id="P19">InteractionV&amp;QC plugin helps the user to visualize possible non-random interaction between two cell types in the space. This can be done for several different methods of calculating the NET score and the user can visually access which methods’ results make more sense from the spatial point of view.</p></sec></sec></sec><sec id="S8"><label>3</label><title>Experimental Validation and Results</title><p id="P20">In this section, we present the application results of the plugins on real-life datasets. We show the plugins’ versatility and usefulness when investigating multiplexed microscopy images.</p><sec id="S9"><label>3.1</label><title>Results of StainV&amp;QC plugin</title><p id="P21">For testing the utility of the StainV&amp;QC plugin, we used a dataset from a tissue microarray (TMA) of tumor cores with a diameter of 1.2 mm constructed at the Human Protein Atlas, Department of Immunology, Genetics and Pathology, Uppsala University, Sweden, previously presented in [<xref ref-type="bibr" rid="R27">27</xref>]. The TMA underwent octaplex immunofluorescence staining using a 7-plex Opal kit and additional Opal 480 and Opal 780 reagent kits (Akoya Biosciences, Marlborough, US) and DAPI counterstain of cell nuclei. Images were obtained after scanning the TMA through Vectra Polaris (Akoya Biosciences) at 20× magnification. For this experiment, we selected two TMA cores and used their corresponding DAPI images as input for QuPath to segment all cell nuclei. We also created an approximate cell segmentation by dilation of these cell nuclei. These areas were used to extract features per cell by QuPath. The features are basic statistics of intensities (mean, min, max, and SD). The description of an example file can be found in the Supplementary material - <xref ref-type="supplementary-material" rid="SD1">Figure S1.2</xref>.</p><p id="P22">Before proceeding to cell classification, we loaded the data to the StainV&amp;QC plugin. As can be seen in <xref ref-type="fig" rid="F5">figure 5</xref> - middle, the feature spaces are distinct from each other which means very strong differences in the image intensities between the two cores. These differences may be due to variations in tissue fixation, leading to variations in antibody binding properties. Next, we applied normalization of extracted features through Winsorization [<xref ref-type="bibr" rid="R28">28</xref>], defined as <disp-formula id="FD2"><label>(2)</label><mml:math id="M2"><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>m</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>90</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="P23">Here, <italic>pct</italic><sub>10</sub> and <italic>pct</italic><sub>90</sub> are the 10th and 90th percentiles of the corresponding feature measurements <italic>x.</italic> The normalized data was loaded to the StainV&amp;QC plugin and we got aligned feature spaces as can be seen in <xref ref-type="fig" rid="F5">figure 5</xref> - right. Using the StainV&amp;QC plugin, we can quickly confirm that the aim of the normalization step, to make the feature distribution of the two cores more equal, has been reached. This is important for the following steps involving automated cell classification.</p></sec><sec id="S10"><label>3.2</label><title>Results of ClassV&amp;QC plugin</title><p id="P24">For showcasing the ClassV&amp;QC plugin, we used the same dataset presented above, and the ClassV&amp;QC plugin was used to compare two different cell classification results. The first classification result (referred to as <italic>Expected class</italic>) consists of manual annotations by an expert and is described in the Materials section of [<xref ref-type="bibr" rid="R27">27</xref>], and the second result (referred to as <italic>Predicted class</italic>) is the result of a fully automated classification result based on a fully convolutional neural network (FCNN) model [<xref ref-type="bibr" rid="R27">27</xref>]. Both classification approaches start from the same cell segmentation/identification step as presented above. The description of the example files can be found in the Supplementary material - <xref ref-type="supplementary-material" rid="SD1">Figure S2.2</xref>.</p><p id="P25"><xref ref-type="fig" rid="F6">Figure 6</xref> - left shows the classification results of both approaches, circles represent the results of manual annotations colored by cell type and stars represent the results of FCNN classification colored by cell type. This visualization helps to investigate where selected methods match and mismatch from the spatial point of view. In cases where the methods mismatch, the ClassV&amp;QC plugin makes it easy to investigate which result is correct by clicking on the cell marker to visualize patches from all the image channels from the raw microscopy data. These patches are shown on the right side of the Spatial viewport. For instance, the selected cell was manually classified as a Glioma cell, corresponding to high-intensity values in the Opal650 image channel marking mutIDH1. However, the FCNN algorithm classified the same cell as Astrocyte, meaning there should be higher intensity in the Opal780 channel marking GFAP and no signal from mutIDH1. Then the user can use the ClassV&amp;QC plugin to visually evaluate which method has a more reliable classification result. The right side of <xref ref-type="fig" rid="F6">figure 6</xref> shows an interactive confusion matrix that visualizes a comparison of the classification results by the two approaches. The user can interactively click on the matrix’s elements to view only specific disagreements or only specific agreements between the cells on the Spatial viewport. This can be used, for example, to investigate if certain disagreements are significant only for a particular area of the tissue.</p><p id="P26">In order to further verify the value of the ClassV&amp;QC plugin, we also tested it on fluorescence microscopy images from the open dataset originally provided by Ilya Ravkin and made publicly available via the Broad Bioimage Benchmark Collection [<xref ref-type="bibr" rid="R29">29</xref>]. The images are from a drug screening experiment, where human U2OS cells were grown in a 96-well plate with varying doses of two drugs. As the drug dose increases, a protein tagged with the green fluorescent protein GFP is translocated from the cytoplasm to the nucleus, and thus the amount of GFP expressed in the nuclei increases and GFP expressed in the cytoplasm decreases. The goal of the analysis is to quantify this translocation of GFP, or more specifically, to measure the fraction of cells in an image that have nuclear or cytoplasmic GFP expression. In the evaluation of the ClassV&amp;QC plugin, we compared results achieved by two different, fully automated classification methods; SimSearch [<xref ref-type="bibr" rid="R30">30</xref>] and CellProfiler [<xref ref-type="bibr" rid="R19">19</xref>, <xref ref-type="bibr" rid="R20">20</xref>]. CellProfiler classifies cells by first identifying individual cell nuclei, and then extracting measurements such as staining intensities from the surrounding area to assign cell classes. SimSearch is based on deep learning, and searches for image patches that fit a pattern learned from examples of cells of different classes. This means that there may not be a 1:1 match between cell IDs in SimSearch and CellProfiler. More detailed instructions on how to use the plugin, as well as illustrations of example files can be found in the Supplementary material - <xref ref-type="supplementary-material" rid="SD1">Figure S2.3</xref>.</p><p id="P27"><xref ref-type="fig" rid="F7">Figure 7</xref> shows the results of these two classification methods; discs represent CellProfiler results colored by the cell category and stars stand for SimSearch results colored by its proposed cell class category. In this case, there are three cell categories: GFP in the nucleus (purple), GFP in the cytoplasm (orange), and no GFP (green). Cropped patches from each staining image, specifically the distribution of GFP (top) and a nuclear stain (bottom) are displayed in the right corner of the viewport. Using the ClassV&amp;QC plugin, it is easy to see that the selected cell has high GFP intensities localized to the cytoplasm, so it has been correctly classified as having GFP in the cytoplasm by SimSearch (orange star), while CellProfiler incorrectly classified it as having GFP in the nucleus (purple disc).</p></sec><sec id="S11"><label>3.3</label><title>Results of InteractionV&amp;QC plugin</title><p id="P28">Finally, we evaluated the usefulness of the InteractionV&amp;QC plugin. For this experiment, we once again used the multiplexed immunofluorescence dataset presented above. We selected one TMA core containing all five possible cell types and then we used the NET score Jupyter Notebook described above to calculate the neighborhood enrichment between all combinations of the cell type pairs and saved it as a .csv file. The description of the example file can be found in the Supplementary material - <xref ref-type="supplementary-material" rid="SD1">Figure S3.2</xref>. Subsequently, the .csv file was loaded into the InteractionV&amp;QC plugin to visualize the matrix, as can be seen in <xref ref-type="fig" rid="F8">figure 8 - a</xref>). The user can interactively click on any element of the matrix and it displays two corresponding cell types in the Spatial viewport. In <xref ref-type="fig" rid="F8">figure 8</xref> we selected the test results between Glioma (blue) and tumor-associated macrophages/microglia (TAMM) (green) cells since the test results show significant interaction (positive NET score). The InteractionV&amp;QC makes it possible to instantaneously observe the corresponding cell types in the Spatial viewport, highlighting the spatial location of the interaction of these two cell types.</p><p id="P29">To illustrate the utility of the tool, we compare neighborhood enrichments calculated by the NET score and Squidpy as shown in <xref ref-type="fig" rid="F8">figure 8 - b</xref>). The difference is that when using our NET method for calculating the accumulation score, we keep the locations and labels of one cell type and shuffle all the other ones. In the implementation of Squidpy, all the cell locations are shuffled, which is faster. However, we would like to argue that in the case of the NET method presented here, we automatically compensate for the presence of tissue structures, such as vessels, or tissue samples section containing more than one tissue type. By keeping the locations of one cell type which is appearing only in one part of the tissue (one tissue type), we do not create false indications of repulsion or interaction when randomizing the remaining cell types in the computation of the accumulation score. If we look at the highlighted matrix elements in <xref ref-type="fig" rid="F8">figure 8 - a) and b</xref>), we can see that our NET score is asymmetrical indicating that the green TAMM cells are repulsed in relation to the Glioma cells (the TAMM cell are clustered and the NET score is negative), while the Glioma cells are randomly distributed in relation to the TAMM cells (NET score close to zero). The Squidpy implementation does not pick up these differences.</p></sec></sec><sec id="S12" sec-type="conclusions"><label>4</label><title>Conclusion</title><p id="P30">It is important to note that it is not possible to extract quantitative results of quality using these plugins. The plugins serve as a means of visual quality control, and not as a processing step. Quantitative quality control relies on a set of problem-specific definitions of what is considered ‘good enough’. For example, in <xref ref-type="fig" rid="F7">figure 7</xref>, one could argue that the result is ‘good enough’ if the cell count of the two methods is within a certain error range, or one could argue that the distance between the location of each cell of a given class must be within a certain minimum distance of cells of the same class as presented by the other method. And in the end, one may want to compare to visual annotations, which are very expensive to produce. A fast and efficient tool, such as TissUUmaps, for visually presenting results, can give the user quick insights into quality, without tedious annotations. Visual assessment can also be a valuable tool in designing metrics for quantitative evaluation.</p><p id="P31">To conclude, the visualization and quality control tools presented here have the potential to function as an important bridge between visual/manual assessment and fully automated approaches to quantitatively extract information from large-scale multiplex microscopy experiments. Since truly validated ground truth is lacking in this type of assay, tools for interactive quality control are necessary for methods comparison, optimization, and validation. The plugins presented here are part of the free and open-source TissUUmaps 3 project, designed to scale to very large datasets. Thanks to access via a web browser, it enables easy sharing of the plugins and data in multi-disciplinary projects across different labs, without having to transfer data. We believe these visualization and quality control plugins will forward the field through efficient optimization and building of trust in automated analysis, enabling large-scale studies advancing our understanding of medicine and biology.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Suplementary material</label><media xlink:href="EMS158004-supplement-Suplementary_material.pdf" mimetype="application" mime-subtype="pdf" id="d33aAdEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S13"><title>Acknowledgements</title><p>We are grateful for sharing access to data and technical assistance of Anja Smits, Thomas Olsson Bontell and Asgeir Jakola.</p><sec id="S14"><title>Funding Statement</title><p>This research was funded by the European Research Council via ERC Consolidator grant CoG 682810 to C.W. and the SciLifeLab BioImage Informatics Facility.</p></sec></ack><sec id="S15" sec-type="data-availability"><title>Data Availability Statement</title><p id="P32">The code can be found at <ext-link ext-link-type="uri" xlink:href="https://tissuumaps.github.io/TissUUmaps/plugins/">https://tissuumaps.github.io/TissUUmaps/plugins/</ext-link>.</p></sec><fn-group><fn id="FN1" fn-type="conflict"><p id="P33"><bold>Competing Interests</bold> The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></fn><fn id="FN2"><p id="P34"><bold>Ethical Standards</bold> The research meets all ethical guidelines, including adherence to the legal requirements of the study country.</p></fn><fn id="FN3" fn-type="con"><p id="P35"><bold>Author Contributions</bold> Conceptualization: A.B., C.W.; Data curation, L.W., A.O.; Funding acquisition, C.W., A.O.; Investigation, A.B., E.C., A.A., C.W.; Methodology, A.B., C.A., A.A., E.C., A.K., C.W.; Project administration, C.W.; Resources, C.W., A.O.; Software, A.B., C.A., E.C.; Supervision, C.W., A.O.; Visualization, A.B., C.A.; Writing—original draft, A.B.; Writing—review &amp; editing, A.B., C.A., A.A., E.C., A.K., L.W., A.O., C.W. All authors approved the final submitted draft.</p></fn></fn-group><ref-list><ref id="R1"><label>[1]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Armingol</surname><given-names>Erick</given-names></name><etal/></person-group><article-title>Deciphering cell–cell interactions and communication from gene expression</article-title><source>Nature Reviews Genetics</source><year>2020</year><month>nov</month><volume>22</volume><issue>2</issue><fpage>71</fpage><lpage>88</lpage><pub-id pub-id-type="pmcid">PMC7649713</pub-id><pub-id pub-id-type="pmid">33168968</pub-id><pub-id pub-id-type="doi">10.1038/s41576-020-00292-x</pub-id></element-citation></ref><ref id="R2"><label>[2]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ke</surname><given-names>Rongqin</given-names></name><etal/></person-group><article-title>In situ sequencing for RNA analysis in preserved tissue and cells</article-title><source>Nature Methods</source><year>2013</year><month>July</month><volume>10</volume><issue>9</issue><fpage>857</fpage><lpage>860</lpage><pub-id pub-id-type="pmid">23852452</pub-id></element-citation></ref><ref id="R3"><label>[3]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Xiao</given-names></name><etal/></person-group><article-title>Three-dimensional intact-tissue sequencing of single-cell transcriptional states</article-title><source>Science</source><year>2018</year><month>June</month><volume>361</volume><issue>6400</issue><elocation-id>eaat5691</elocation-id><pub-id pub-id-type="pmcid">PMC6339868</pub-id><pub-id pub-id-type="pmid">29930089</pub-id><pub-id pub-id-type="doi">10.1126/science.aat5691</pub-id></element-citation></ref><ref id="R4"><label>[4]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moffitt</surname><given-names>Jeffrey R</given-names></name><etal/></person-group><article-title>Molecular, spatial, and functional single-cell profiling of the hypothalamic preoptic region</article-title><source>Science</source><year>2018</year><month>November</month><volume>362</volume><issue>6416</issue><elocation-id>eaau5324</elocation-id><pub-id pub-id-type="pmcid">PMC6482113</pub-id><pub-id pub-id-type="pmid">30385464</pub-id><pub-id pub-id-type="doi">10.1126/science.aau5324</pub-id></element-citation></ref><ref id="R5"><label>[5]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Codeluppi</surname><given-names>Simone</given-names></name><etal/></person-group><article-title>Spatial organization of the somatosensory cortex revealed by osmFISH</article-title><source>Nature Methods</source><year>2018</year><month>October</month><volume>15</volume><issue>11</issue><fpage>932</fpage><lpage>935</lpage><pub-id pub-id-type="pmid">30377364</pub-id></element-citation></ref><ref id="R6"><label>[6]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glass</surname><given-names>George</given-names></name><etal/></person-group><article-title>Simple: A sequential immunoperoxidase labeling and erasing method</article-title><source>Journal of Histochemistry &amp; Cytochemistry</source><year>2009</year><month>apr</month><volume>57</volume><issue>10</issue><fpage>899</fpage><lpage>905</lpage><pub-id pub-id-type="pmcid">PMC2746723</pub-id><pub-id pub-id-type="pmid">19365090</pub-id><pub-id pub-id-type="doi">10.1369/jhc.2009.953612</pub-id></element-citation></ref><ref id="R7"><label>[7]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stack</surname><given-names>Edward C</given-names></name><etal/></person-group><article-title>Multiplexed immunohistochemistry, imaging, and quantitation: A review, with an assessment of tyramide signal amplification, multispectral imaging and multiplex analysis</article-title><source>Methods</source><year>2014</year><month>November</month><volume>70</volume><issue>1</issue><fpage>46</fpage><lpage>58</lpage><pub-id pub-id-type="pmid">25242720</pub-id></element-citation></ref><ref id="R8"><label>[8]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Remark</surname><given-names>Romain</given-names></name><etal/></person-group><article-title>In-depth tissue profiling using multiplexed immunohistochemical consecutive staining on single slide</article-title><source>Science Immunology</source><year>2016</year><month>jul</month><volume>1</volume><issue>1</issue><pub-id pub-id-type="pmid">28783673</pub-id></element-citation></ref><ref id="R9"><label>[9]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsujikawa</surname><given-names>Takahiro</given-names></name><etal/></person-group><article-title>Quantitative multiplex immunohistochemistry reveals myeloid-inflamed tumor-immune complexity associated with poor prognosis</article-title><source>Cell Reports</source><year>2017</year><month>April</month><volume>19</volume><issue>1</issue><fpage>203</fpage><lpage>217</lpage><pub-id pub-id-type="pmcid">PMC5564306</pub-id><pub-id pub-id-type="pmid">28380359</pub-id><pub-id pub-id-type="doi">10.1016/j.celrep.2017.03.037</pub-id></element-citation></ref><ref id="R10"><label>[10]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorris</surname><given-names>Mark AJ</given-names></name><etal/></person-group><article-title>Eight-color multiplex immunohistochemistry for simultaneous detection of multiple immune checkpoint molecules within the tumor microenvironment</article-title><source>The Journal of Immunology</source><year>2017</year><month>November</month><volume>200</volume><issue>1</issue><fpage>347</fpage><lpage>354</lpage><pub-id pub-id-type="pmid">29141863</pub-id></element-citation></ref><ref id="R11"><label>[11]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hickey</surname><given-names>John W</given-names></name><etal/></person-group><article-title>Spatial mapping of protein composition and tissue organization: a primer for multiplexed antibody-based imaging</article-title><source>Nature Methods</source><year>2021</year><month>November</month><volume>19</volume><issue>3</issue><fpage>284</fpage><lpage>295</lpage><pub-id pub-id-type="pmcid">PMC9264278</pub-id><pub-id pub-id-type="pmid">34811556</pub-id><pub-id pub-id-type="doi">10.1038/s41592-021-01316-y</pub-id></element-citation></ref><ref id="R12"><label>[12]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>Steve</given-names></name><etal/></person-group><article-title>Comparison of biomarker modalities for predicting response to PD-1/PD-l1 checkpoint blockade</article-title><source>JAMA Oncology</source><year>2019</year><month>August</month><volume>5</volume><issue>8</issue><fpage>1195</fpage><pub-id pub-id-type="pmcid">PMC6646995</pub-id><pub-id pub-id-type="pmid">31318407</pub-id><pub-id pub-id-type="doi">10.1001/jamaoncol.2019.1549</pub-id></element-citation></ref><ref id="R13"><label>[13]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haines</surname><given-names>Deborah M</given-names></name><name><surname>Chelack</surname><given-names>Brian J</given-names></name></person-group><article-title>Technical considerations for developing enzyme immunohisto-chemical staining procedures on formalin-fixed paraffin-embedded tissues for diagnostic pathology</article-title><source>Journal of Veterinary Diagnostic Investigation</source><year>1991</year><volume>3</volume><issue>1</issue><fpage>101</fpage><lpage>112</lpage><pub-id pub-id-type="pmid">2039784</pub-id></element-citation></ref><ref id="R14"><label>[14]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gan</surname><given-names>David</given-names></name></person-group><article-title>Troubleshooting immunohistochemistry</article-title><source>Immunohistochemistry</source><year>2022</year><fpage>183</fpage></element-citation></ref><ref id="R15"><label>[15]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lichtman</surname><given-names>Jeff W</given-names></name><name><surname>Conchello</surname><given-names>José-Angel</given-names></name></person-group><article-title>Fluorescence microscopy</article-title><source>Nature Methods</source><year>2005</year><month>nov</month><volume>2</volume><issue>12</issue><fpage>910</fpage><lpage>919</lpage><pub-id pub-id-type="pmid">16299476</pub-id></element-citation></ref><ref id="R16"><label>[16]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behanova</surname><given-names>Andrea</given-names></name><etal/></person-group><article-title>Spatial statistics for understanding tissue organization</article-title><source>Frontiers in Physiology</source><year>2022</year><month>jan</month><volume>13</volume><pub-id pub-id-type="pmcid">PMC8837270</pub-id><pub-id pub-id-type="pmid">35153840</pub-id><pub-id pub-id-type="doi">10.3389/fphys.2022.832417</pub-id></element-citation></ref><ref id="R17"><label>[17]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Somarakis</surname><given-names>Antonios</given-names></name><etal/></person-group><article-title>ImaCytE: Visual exploration of cellular micro-environments for imaging mass cytometry data</article-title><source>IEEE Transactions on Visualization and Computer Graphics</source><year>2021</year><month>January</month><volume>27</volume><issue>1</issue><fpage>98</fpage><lpage>110</lpage><pub-id pub-id-type="pmid">31369380</pub-id></element-citation></ref><ref id="R18"><label>[18]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pielawski</surname><given-names>Nicolas</given-names></name><etal/></person-group><article-title>Tissuumaps 3: Improvements in interactive visualization, exploration, and quality assessment of large-scale spatial omics data</article-title><source>bioRxiv</source><year>2023</year></element-citation></ref><ref id="R19"><label>[19]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carpenter</surname><given-names>Anne E</given-names></name><etal/></person-group><article-title>Cellprofiler: image analysis software for identifying and quantifying cell pheno-types</article-title><source>Genome biology</source><year>2006</year><volume>7</volume><issue>10</issue><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="pmcid">PMC1794559</pub-id><pub-id pub-id-type="pmid">17076895</pub-id><pub-id pub-id-type="doi">10.1186/gb-2006-7-10-r100</pub-id></element-citation></ref><ref id="R20"><label>[20]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McQuin</surname><given-names>Claire</given-names></name><etal/></person-group><article-title>Cellprofiler 3.0: Next-generation image processing for biology</article-title><source>PLoS biology</source><year>2018</year><volume>16</volume><issue>7</issue><elocation-id>e2005970</elocation-id><pub-id pub-id-type="pmcid">PMC6029841</pub-id><pub-id pub-id-type="pmid">29969450</pub-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2005970</pub-id></element-citation></ref><ref id="R21"><label>[21]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stirling</surname><given-names>David R</given-names></name><etal/></person-group><article-title>CellProfiler analyst 3.0: accessible data exploration and machine learning for image analysis</article-title><source>Bioinformatics</source><year>2021</year><month>September</month><volume>37</volume><issue>21</issue><fpage>3992</fpage><lpage>3994</lpage><pub-id pub-id-type="pmid">34478488</pub-id></element-citation></ref><ref id="R22"><label>[22]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bankhead</surname><given-names>Peter</given-names></name><etal/></person-group><article-title>Qupath: Open source software for digital pathology image analysis</article-title><source>Scientific reports</source><year>2017</year><volume>7</volume><issue>1</issue><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="pmcid">PMC5715110</pub-id><pub-id pub-id-type="pmid">29203879</pub-id><pub-id pub-id-type="doi">10.1038/s41598-017-17204-5</pub-id></element-citation></ref><ref id="R23"><label>[23]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palla</surname><given-names>Giovanni</given-names></name><etal/></person-group><article-title>Squidpy: a scalable framework for spatial omics analysis</article-title><source>Nature Methods</source><year>2022</year><month>jan</month><volume>19</volume><issue>2</issue><fpage>171</fpage><lpage>178</lpage><pub-id pub-id-type="pmcid">PMC8828470</pub-id><pub-id pub-id-type="pmid">35102346</pub-id><pub-id pub-id-type="doi">10.1038/s41592-021-01358-2</pub-id></element-citation></ref><ref id="R24"><label>[24]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schapiro</surname><given-names>Denis</given-names></name><etal/></person-group><article-title>histoCAT: analysis of cell phenotypes and interactions in multiplex image cytometry data</article-title><source>Nature Methods</source><year>2017</year><month>aug</month><volume>14</volume><issue>9</issue><fpage>873</fpage><lpage>876</lpage><pub-id pub-id-type="pmcid">PMC5617107</pub-id><pub-id pub-id-type="pmid">28783155</pub-id><pub-id pub-id-type="doi">10.1038/nmeth.4391</pub-id></element-citation></ref><ref id="R25"><label>[25]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McInnes</surname><given-names>L</given-names></name><name><surname>Healy</surname><given-names>John</given-names></name></person-group><article-title>Umap: Uniform manifold approximation and projection for dimension reduction</article-title><source>ArXiv</source><year>2018</year><comment>abs/1802.03426</comment></element-citation></ref><ref id="R26"><label>[26]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Janowczyk</surname><given-names>Andrew</given-names></name><etal/></person-group><article-title>Stain normalization using sparse AutoEncoders (StaNoSA): Application to digital pathology</article-title><source>Computerized Medical Imaging and Graphics</source><year>2017</year><month>April</month><volume>57</volume><fpage>50</fpage><lpage>61</lpage><pub-id pub-id-type="pmcid">PMC5112159</pub-id><pub-id pub-id-type="pmid">27373749</pub-id><pub-id pub-id-type="doi">10.1016/j.compmedimag.2016.05.003</pub-id></element-citation></ref><ref id="R27"><label>[27]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Solorzano</surname><given-names>Leslie</given-names></name><etal/></person-group><article-title>Machine learning for cell classification and neighborhood analysis in glioma tissue</article-title><source>Cytometry Part A</source><year>2021</year><month>June</month><pub-id pub-id-type="pmid">34089228</pub-id></element-citation></ref><ref id="R28"><label>[28]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hastings</surname><given-names>Cecil</given-names></name><etal/></person-group><article-title>Low moments for small samples: A comparative study of order statistics</article-title><source>The Annals of Mathematical Statistics</source><year>1947</year><month>September</month><volume>18</volume><issue>3</issue><fpage>413</fpage><lpage>426</lpage></element-citation></ref><ref id="R29"><label>[29]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ljosa</surname><given-names>Vebjorn</given-names></name><etal/></person-group><article-title>Annotated high-throughput microscopy image sets for validation</article-title><source>Nature Methods</source><year>2012</year><month>June</month><volume>9</volume><issue>7</issue><fpage>637</fpage><pub-id pub-id-type="pmcid">PMC3627348</pub-id><pub-id pub-id-type="pmid">22743765</pub-id><pub-id pub-id-type="doi">10.1038/nmeth.2083</pub-id></element-citation></ref><ref id="R30"><label>[30]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gupta</surname><given-names>Ankit</given-names></name><etal/></person-group><article-title>Simsearch: A human-in-the-loop learning framework for fast detection of regions of interest in microscopy images</article-title><source>IEEE Journal of Biomedical and Health Informatics</source><year>2022</year><volume>26</volume><issue>8</issue><fpage>4079</fpage><lpage>4089</lpage><pub-id pub-id-type="pmid">35609108</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Diagram of the workflow.</title><p>The left column describes the steps of the analysis, while the center column describes some of the associated quality control questions, and the right column lists the associated visualization and quality control plugins: a) - Plugin for visualization comparison and quality control of cell staining. b) Plugin for visualization comparison and quality control of cell classification. c) - Plugin for visualization and quality control of cell-cell interactions</p></caption><graphic xlink:href="EMS158004-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Workflow before using plugin StainV&amp;QC.</title><p>a) - Multiplexed microscopy data with segmented cells, b) Table of extracted features from all the segmented cells, c) - Plugin StainV&amp;QC</p></caption><graphic xlink:href="EMS158004-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Workflow for using plugin ClassV&amp;QC.</title><p>a) - Features extracted from the microscopy images are input to manual or automated classification step, b) - Manual, semiautomated, or automated cell classification, c) - Plugin ClassV&amp;QC containing interactive confusion matrix where the user can click on the elements of the matrix and only cells counted in that matrix element are displayed on the Spatial viewport</p></caption><graphic xlink:href="EMS158004-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Workflow for using the InteractionV&amp;QC plugin.</title><p>a) - Cell classification results, b) - Accumulation scores (as quantified using NET), blue bars represent the distribution of the randomized counts of connections and the black line represent the actual count of connections, c) - Plugin InteractionV&amp;QC containing interactive matrix where the user can click on the elements of the matrix and only those two corresponding cell types are displayed on the Spatial viewport</p></caption><graphic xlink:href="EMS158004-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><p>Main screen of StainV&amp;QC plugin, comparing two cores with corresponding data in feature space before and after normalization. Colors represent individual tissue cores</p></caption><graphic xlink:href="EMS158004-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><p>Main screen of the ClassV&amp;QC plugin, comparing two classification techniques in the Spatial viewport with a corresponding confusion matrix. Circles represent the results of manual annotations colored by cell type and stars stand for FCNN classification colored by cell type</p></caption><graphic xlink:href="EMS158004-f006"/></fig><fig id="F7" position="float"><label>Figure 7</label><caption><p>Main screen of ClassV&amp;QC plugin, comparing two classification techniques applied to human U2OS cell cultures. Discs represent CellProfiler results colored by the cell category and stars stand for SimSearch results colored by the cell category</p></caption><graphic xlink:href="EMS158004-f007"/></fig><fig id="F8" position="float"><label>Figure 8</label><caption><title>The main screen of the InteractionV&amp;QC plugin.</title><p>a) - neighborhood enrichment calculated by NET, b) - neighborhood enrichment calculated by Squidpy</p></caption><graphic xlink:href="EMS158004-f008"/></fig></floats-group></article>