<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS158019</article-id><article-id pub-id-type="doi">10.1101/2022.12.01.518732</article-id><article-id pub-id-type="archive">PPR578926</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Visiomode: an open-source platform for building rodent touchscreen-based behavioral assays</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Eleftheriou</surname><given-names>Constantinos</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Clarke</surname><given-names>Thomas</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Poon</surname><given-names>Victoriana</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Zechner</surname><given-names>Marie</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Duguid</surname><given-names>Ian</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A4">4</xref><xref ref-type="fn" rid="FN1">5</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>Simons Initiative for the Developing Brain, University of Edinburgh, Edinburgh, EH8 9XD, UK</aff><aff id="A2"><label>2</label>Centre for Discovery Brain Sciences and Patrick Wild Centre, Edinburgh Medical School: Biomedical Sciences, University of Edinburgh, Edinburgh, EH8 9XD, UK</aff><aff id="A3"><label>3</label>The Roslin Institute, University of Edinburgh, Easter Bush, Midlothian, EH25 9RG, Scotland, UK</aff><aff id="A4"><label>4</label>Twitter: @Ian_Duguid</aff><author-notes><corresp id="CR1">
<label>*</label>Correspondence: <email>Constantinos.Eleftheriou@ed.ac.uk</email>, <email>Ian.Duguid@ed.ac.uk</email></corresp><fn id="FN1"><label>5</label><p id="P1">Lead contact</p></fn></author-notes><pub-date pub-type="nihms-submitted"><day>04</day><month>12</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>01</day><month>12</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><sec id="S1"><title>Background</title><p id="P2">Touchscreen-based behavioral assays provide a robust method for assessing cognitive behavior in rodents, offering great flexibility and translational potential. The development of touchscreen assays presents a significant programming and mechanical engineering challenge, where commercial solutions can be prohibitively expensive and open-source solutions are underdeveloped, with limited adaptability.</p></sec><sec id="S2"><title>New method</title><p id="P3">Here, we present Visiomode (<ext-link ext-link-type="uri" xlink:href="http://www.visiomode.org">www.visiomode.org</ext-link>), an open-source platform for building rodent touchscreen-based behavioral tasks. Visiomode leverages the inherent flexibility of touchscreens to offer a simple yet adaptable software and hardware platform. The platform is built on the Raspberry Pi computer combining a web-based interface and powerful plug-in system with an operant chamber that can be adapted to generate a wide range of behavioral tasks.</p></sec><sec id="S3"><title>Results</title><p id="P4">As a proof of concept, we use Visiomode to build both simple stimulus-response and more complex visual discrimination tasks, showing that mice display rapid sensorimotor learning including switching between different motor responses (i.e., nose poke versus reaching).</p></sec><sec id="S4"><title>Comparison with existing methods</title><p id="P5">Commercial solutions are the ‘go to’ for rodent touchscreen behaviors, but the associated costs can be prohibitive, limiting their uptake by the wider neuroscience community. While several open-source solutions have been developed, efforts so far have focused on reducing the cost, rather than promoting ease of use and adaptability. Visiomode addresses these unmet needs providing a low-cost, extensible platform for creating touchscreen tasks.</p></sec><sec id="S5"><title>Conclusions</title><p id="P6">Developing an open-source, rapidly scalable and low-cost platform for building touchscreen-based behavioral assays should increase uptake across the science community and accelerate the investigation of cognition, decision-making and sensorimotor behaviors both in health and disease.</p></sec></abstract></article-meta></front><body><sec id="S6" sec-type="intro"><title>Introduction</title><p id="P7">Since their introduction to biomedical research, touchscreens have become an increasingly popular tool for assessing cognitive function in rodents (<xref ref-type="bibr" rid="R12">Bussey et al., 1997</xref>; <xref ref-type="bibr" rid="R18">Dumont et al., 2021</xref>; <xref ref-type="bibr" rid="R34">Markham et al., 1996</xref>). Their appeal lies with their remarkable flexibility, supporting a vast array of visual stimuli coupled with quantifiable motor responses (<xref ref-type="bibr" rid="R44">Seitz et al., 2021</xref>), both of which are necessary for designing tasks to investigate complex cognitive processes such as category learning (<xref ref-type="bibr" rid="R9">Broschard et al., 2021</xref>; <xref ref-type="bibr" rid="R31">Kim et al., 2018</xref>), spatial attention (<xref ref-type="bibr" rid="R25">Haddad et al., 2021</xref>), cognitive flexibility (<xref ref-type="bibr" rid="R23">Groman et al., 2012</xref>), and visual perception (<xref ref-type="bibr" rid="R34">Markham et al., 1996</xref>). Their use has transformed studies of neurological disorders by providing a sensitive assay of sensorimotor behaviors (<xref ref-type="bibr" rid="R4">Arulsamy et al., 2019</xref>; <xref ref-type="bibr" rid="R15">Copping et al., 2017</xref>; <xref ref-type="bibr" rid="R32">Leach and Crawley, 2018</xref>; <xref ref-type="bibr" rid="R33">Leach et al., 2016</xref>; <xref ref-type="bibr" rid="R35">Morton et al., 2006</xref>; <xref ref-type="bibr" rid="R37">Norris et al., 2019</xref>; <xref ref-type="bibr" rid="R51">Yang et al., 2015</xref>), revealing subtle phenotypes that were not detected by more conventional assays (<xref ref-type="bibr" rid="R50">Van den Broeck et al., 2019</xref>; <xref ref-type="bibr" rid="R52">Zeleznikow-Johnston et al., 2018</xref>). This sensitive readout of changes in behavior holds great translational promise (<xref ref-type="bibr" rid="R48">Talpos and Steckler, 2013</xref>), where tasks designed for animals can be directly translated to human subjects (<xref ref-type="bibr" rid="R14">Chow et al., 2020</xref>; <xref ref-type="bibr" rid="R28">Hvoslef-Eide et al., 2015</xref>; <xref ref-type="bibr" rid="R36">Nithianantharajah et al., 2015</xref>). Despite their increasing popularity, the use of touchscreen-based behaviors is somewhat limited in rodent research. Uptake has been hampered either by the prohibitive up-front costs of commercial systems or the considerable ‘in-house’ development required to create bespoke touchscreen-based behaviors (<xref ref-type="bibr" rid="R18">Dumont et al., 2021</xref>).</p><p id="P8">Commercially available touchscreen behavioral arenas provide researchers with a simple turnkey solution requiring minimal setup time. These systems have dominated the touchscreen landscape in biomedicine over the past two decades (<xref ref-type="bibr" rid="R4">Arulsamy et al., 2019</xref>; <xref ref-type="bibr" rid="R7">Brasted et al., 2002</xref>; <xref ref-type="bibr" rid="R8">Brigman et al., 2010</xref>; <xref ref-type="bibr" rid="R11">Bussey et al., 1998</xref>; <xref ref-type="bibr" rid="R13">Bussey et al., 2008</xref>; <xref ref-type="bibr" rid="R17">Delotterie et al., 2014</xref>; <xref ref-type="bibr" rid="R22">Glover et al., 2020</xref>; <xref ref-type="bibr" rid="R25">Haddad et al., 2021</xref>; <xref ref-type="bibr" rid="R27">Heath et al., 2019</xref>; <xref ref-type="bibr" rid="R39">Odland et al., 2021</xref>; <xref ref-type="bibr" rid="R41">Piantadosi et al., 2019</xref>; <xref ref-type="bibr" rid="R47">Stirman et al., 2016</xref>; <xref ref-type="bibr" rid="R49">Talpos et al., 2008</xref>), and have played an important role in popularizing their use in rodent research (<xref ref-type="bibr" rid="R18">Dumont et al., 2021</xref>). However, the prohibitive costs associated with commercial systems (i.e., &gt; 10,000 USD) provides a rate limiting step for their widespread adoption (<xref ref-type="bibr" rid="R18">Dumont et al., 2021</xref>). In contrast, developing touchscreen tasks ‘in-house’ is a particularly challenging programming and engineering problem. While most traditional open-field (<xref ref-type="bibr" rid="R26">Hall and Ballachey, 1932</xref>) or operant chamber (<xref ref-type="bibr" rid="R45">Skinner, 1938</xref>) tasks can be implemented with a simple microcontroller device (<xref ref-type="bibr" rid="R2">Akam et al., 2022</xref>), the introduction of a touchscreen interface requires complex hardware and software integration to control the generation and display of graphics, as well as registering behavioral interactions with the screen. Utilizing graphics libraries available on most Operating Systems (e.g., Microsoft Windows, Linux and MacOS) requires extensive programming knowledge (<xref ref-type="bibr" rid="R30">Kessenich et al., 2016</xref>), and while open-source initiatives such as PsychoPy greatly simplify the task of generating visual stimuli (<xref ref-type="bibr" rid="R40">Peirce, 2007</xref>), they still require significant development to be adapted for touchscreen tasks (<xref ref-type="bibr" rid="R44">Seitz et al., 2021</xref>). This is further complicated by the choice of touchscreen hardware, where heterogeneity in compact touchscreen systems results in variable touch sensitivities, requiring the developer to test and validate a range of screens before final implementation (<xref ref-type="bibr" rid="R18">Dumont et al., 2021</xref>).</p><p id="P9">In our view, an open-source, community-driven touchscreen solution would solve both problems by distributing the development effort across multiple research groups, while also reducing overall costs (<xref ref-type="bibr" rid="R19">Fortunato and Galassi, 2021</xref>; <xref ref-type="bibr" rid="R20">Freeman, 2015</xref>). Openscience initiatives have continued to grow in the past few years, with projects like Mouse-Bytes (<xref ref-type="bibr" rid="R6">Beraldo et al., 2019</xref>) and the advice sharing platform touchscreencognition.org (<xref ref-type="bibr" rid="R18">Dumont et al., 2021</xref>). To date, no opensource community-based solution exists. While open-source touchscreen-based operant chambers have been developed (<xref ref-type="bibr" rid="R24">Gurley, 2019</xref>; <xref ref-type="bibr" rid="R38">O’Leary et al., 2018</xref>; <xref ref-type="bibr" rid="R42">Pineno, 2014</xref>), this has not led to increased uptake due a lack of code availability and the primary focus being on reducing costs, rather than enhancing the user experience, scalability, adaptability, and ease of use.</p><p id="P10">To address these unmet needs, we have developed Visiomode (<ext-link ext-link-type="uri" xlink:href="http://www.visiomode.org">www.visiomode.org</ext-link>), a complete open-source software and hardware platform for developing touchscreen-based behavioral tasks for rodents. Visiomode combines a sophisticated web-based user interface (UI) and powerful plug-in system, with an affordable hardware configuration that can accommodate a wide variety of visuomotor tasks (<xref ref-type="fig" rid="F1">Figure 1</xref>). Our solution has been designed to maximize adaptability, while providing a consistent, standardized user experience and output data format. While several visual stimuli and task structures have been provided out-of-the-box, including drifting gratings, symbols and natural images, users can upload or simply programmatically define their own visual stimuli. Using simple stimulus-response and more complex visual discrimination tasks as exemplars, we show that mice display rapid sensorimotor learning, switching between both nose poke and visually guided reaching depending on task requirements. In addition, we discuss Visiomode’s Application Programming Interface (API), how it can be scaled to parallelize data acquisition using a single personal device and the build components.</p></sec><sec id="S7" sec-type="methods"><title>Methods</title><sec id="S8"><title>API design principles</title><p id="P11">We first designed Visiomode’s API which encapsulates all the software functionality required to design and run touchscreen behavioral tasks, including stimulus generation, trial structure definition, response recording and interfacing with external hardware via the USB. The API is written in Python, lever-aging its popularity, wide availability of libraries and ease of use. The PyGame and PySDL libraries are used for handling graphics and task timing, while the Flask library is used for rendering Visiomode’s web interface. In addition to the behavioral tasks provided out-of-the-box, the API can be used to implement additional task components such as protocols, stimuli or hardware integrations using user-defined task files.</p><p id="P12">The API is broadly divided into three parts: stimulus interface, protocol interface and device interface. Each constituent part represents a Python abstract base class (Hunt, 2019), which defines a programmatic interface which all user-defined stimuli, protocols and devices must be derived from. Each interface allows users to integrate custom HTML forms for dynamically setting component-specific parameters within Visiomode’s web interface.</p><p id="P13">The stimulus interface allows for user-defined visual stimuli to be integrated into behavioral tasks, independently from task structure. For example, the same user-defined stimulus, such as a solid color or a drifting grating, can be reused across all available task protocols without having to be redefined. Each class derived from the stimulus interface inherits several functions that are core to the presentation of the visual stimuli within a protocol, such as functions that control the appearance and removal of stimuli, updating of stimulus position, or modification of stimuli between trials. Additionally, stimuli can integrate peripheral USB devices to yield multi-sensory stimuli (i.e., simultaneous presentation of an auditory tone and drifting grating).</p><p id="P14">Task structure definition classes are inherited from the protocol interface, which controls the timing for the presentation of stimuli and processes touchscreen and external device events during an experimental session. User-defined tasks must specify, as a minimum, a target stimulus parameter, which may be set dynamically via the web interface. The web interface will pass the session duration, intertrial interval (ITI) and duration of stimulus presentation to every protocol-derived class, which by default will iterate through calls to a trial_block function until the session duration expires. The trial_block function monitors the application’s touch event queue during the ITI and while a stimulus is present, and assigns correct, incorrect, uncued or miss responses accordingly. The protocol interface implements several functions corresponding to different trial outcome conditions, such as on_ correct, on_incorrect and on_uncued, as well as trial epochs such as on_trial_start or on_ stimulus_start, which can be overridden by users to build complex trial structures with fewer lines of code and without requiring an extensive understanding of Visiomode’s core functionality.</p><p id="P15">While most behavioral protocols will be defined before each session, Visiomode can support dynamic, on-the-fly changes to stimulus and protocol settings depending on performance. For example, the contrast of a stimulus could be decreased following a user-defined number of correct responses within a single training session. This would be achieved by implementing a new protocol file, which can then be uploaded to Visiomode via the web interface. For more information, we encourage users to visit <ext-link ext-link-type="uri" xlink:href="http://www.visiomode.org">www.visiomode.org</ext-link> for the latest guidance on implementing new protocols.</p><p id="P16">While Visiomode’s web interface can be accessed via a browser, instances of Visiomode do not need an active internet connection as they contain all necessary code libraries for running tasks and monitoring performance, including stimulus generation and live plotting of behavioral data. While the recommended installation route requires an internet connection, we provide alternative means of installing the software on Raspberry Pis if internet connectivity is not possible.</p><p id="P17">Finally, in addition to the touchscreen itself, Visiomode can integrate external USB devices connected to the Raspberry Pi through the device interface. For example, a water reward mechanism driven by an Arduino microcontroller can be used to dispense rewards following correct task responses. The device interface is subdivided into Input and Output interfaces, supporting both sensors that can feed into a task structure as well as actuators providing additional sensory stimuli or dispensing rewards. While the Raspberry Pi’s own GPIO ports could also be used to integrate sensors or actuators in Visiomode tasks, supporting USB devices can be advantageous. Touchscreen displays for the Raspberry Pi use all the available GPIO ports, which leads to a more compact design, but necessitates the use of USB-connected microcontrollers to integrate external hardware. Given the ubiquity of microcontroller devices in neuroscience, this plug-and-play approach allows the user to easily incorporate USB-connected devices from existing setups facilitating friction-free development of novel behavioral tasks.</p><p id="P18">While Visiomode has been designed as a flexible platform that can be readily extended programmatically, it offers several common experimental paradigms out-of-the-box providing utility for users with no prior programming experience. Single target, two-alternative forced choice (2AFC) and Go/NoGo protocols are included with every installation which support correction trials, pseudo-randomization of stimulus presentation, and any arbitrary external reward devices. Sinusoidal gratings with user-defined characteristics, as well as a range of symbols and natural scenes are also included, which can be incorporated into any protocol. Finally, Visiomode supports a range of external reward devices, such as reward spouts and food hoppers, with microcontroller code compatible with Visiomode available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zen-odo.6877795">https://doi.org/10.5281/zen-odo.6877795</ext-link>. For any USB connected devices for which the Rasberry Pi operating systems does not provide an out-of-the-box solution for integration/calibration, Visiomode supports the use of custom written “driver” code within its devices interface.</p><p id="P19">To facilitate synchronization of behavior and physiological recordings, Visiomode time-stamps behavioral epochs using the system clock, which can be synched with a local Network Time Protocol server to provide sub-millisecond time synchronization between the Raspberry Pi computer running Visiomode and the computer acquiring physiological data. For physiological recordings that require higher temporal precision (i.e., electrophysiological recordings at 20 KHz), Visiomode can provide a time synchronization signal using the host Raspberry Pi’s GPIO ports or a microcontroller device connected to the USB (<xref ref-type="bibr" rid="R2">Akam et al., 2022</xref>).</p><p id="P20">Visiomode,’s source code is openly available and is publicly hosted at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.6877795">https://doi.org/10.5281/zenodo.6877795</ext-link> under the terms of the MIT license.</p></sec><sec id="S9"><title>Web interface</title><p id="P21">Each instance of Visiomode exposes a web interface that enables the user to set up, run and monitor touchscreen experiments (<xref ref-type="fig" rid="F2">Figure 2</xref>), accessible via any web browser that is connected to the same local network as the Raspberry Pi. The web interface is designed to cater to both novice and expert users. An accessible user interface enables the uptake of Visiomode by users with no prior programming knowledge, while offering a fully customizable user interface for users that wish to extend Visiomode’s capabilities. The primary function of the web interface is to set up and run touchscreen behavioral tasks. Task, stimulus, and device parameters can be set on-the-fly, allowing users to control all aspects of the setup without the need to modify the code. For example, Visiomode provides out-of-the-box support for drifting gratings, where parameters such as the cycles per degree, contrast and drift frequency can be adjusted online. Each component, derived from the Protocol, Stimulus and Device interfaces described in the previous section, can optionally implement a webform via the form_path attribute that passes parameters from the web interface to the Visiomode API (for an example, see <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.6877795">https://doi.org/10.5281/zenodo.6877795</ext-link>) (<xref ref-type="fig" rid="F2">Figure 2</xref>). These webforms are loaded dynamically when the user selects a particular component and can interface with multiple components to build complex tasks (e.g., protocols can have multiple stimuli integrated with multiple input and/or output devices). The parameters set on the web interface are serialized and converted to asynchronous calls to the Visiomode API, which assembles the different components into a behavioral protocol that runs on the touchscreen.</p><p id="P22">In addition to task control, the Visiomode interface plots real-time task analytics that are protocol-specific and can be further customized or extended via an analytics_path attribute, see <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zen-odo.6877795">https://doi.org/10.5281/zen-odo.6877795</ext-link>. Visiomode analytics use the Graphs.js Javascript library for plotting, however other popular Javascript graphics libraries such as D3 or Plotly can also be used. Visiomode’s web interface pools data from the API running the behavior at 4 second intervals, updating only the analytics components visible to the user.</p></sec><sec id="S10"><title>A flexible, scalable platform for building touchscreen tasks</title><p id="P23">The web interface design ensures that each Visiomode setup is self-contained, and that there is virtually no upper limit to parallelization, providing all devices are connected to the same local network. Each instance of the web interface is designed to be asynchronous to the running of the task, such that if the web interface is disconnected (e.g., by an intermittent network issue, or accidental closing of the device‘s browser) the Visiomode API running the task will be unaffected. Decoupling the web interface from the API running the experimental session yields a resilient and easily scalable core platform, which users can customize and extend to suit their individual experimental needs.</p><p id="P24">The web interface can also be used to export session data in a variety of different formats. By default, Visiomode session data are stored as human-readable Javascript Object Notation (JSON) files which include metadata relating to the animal, the protocol and stimulation parameters as well as information on the host device and any other USB peripherals. Session files can additionally be exported to comma-separated value (CSV), hierarchical data format (HDF5) as well as Neurodata Without Borders (NWB) files (<xref ref-type="bibr" rid="R43">Rübel et al., 2019</xref>). The NWB format is a particularly useful tool in integrating behavioral and neurophysiological data in a standardized and widely accessible format. To ease the onboarding of users with Visiomode’s behavioural data, we provide an example Jupyter notebook at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.6877795">https://doi.org/10.5281/zenodo.6877795</ext-link>, which can be used as a template.</p></sec><sec id="S11"><title>Touchscreen behavioral arena</title><p id="P25">Next, we designed a behavioral arena that conforms to a rectangular design common across many operant chambers, measuring 200 mm (L) x 200 mm (W) x 400 mm (H). The walls of the arena were constructed from red transparent acrylic panels (ER Plastics, UK), allowing for easy monitoring of animal behavior, while minimizing confounding external visual stimuli. Individual panels were mounted on aluminum struts (RS Components, UK) creating the exterior shape of the arena and enabling rapid assembly and disassembly during cleaning. The touchscreen (Hyperpixel 4.0, 58 mm x 97 mm, Pimoroni, RS Components, UK) was positioned on one side of the arena, accessed via a 90 mm X 115 mm hole cut in the back wall of the arena and mounted vertically across the GPIO ports of a Raspberry Pi computer (Revision 4, RS Components). To limit access to the touchscreen, a clear, transparent acyclic panel (100 mm x 125 mm x 2mm) was positioned directly in front of the screen using magnetic mounting strips attached to the arena wall. For nose poke experiments we used a panel with a 40 mm x 20 mm cut out, positioned 10 mm above the floor of the arena, while for forelimb reaching, we designed a divider with two 35 mm x 4 mm vertical slits positioned 15 mm apart (<xref ref-type="fig" rid="F3">Figure 3a-b</xref>). The narrow width of the slits and position of the screen 6 mm from the arena wall prevents screen touches using the snout or tongue.</p><p id="P26">For mouse behavior, the size of the capacitance touchscreen is in general inversely proportional to the sensitivity, so care must be taken to select a touchscreen with a sensitivity range that is compatible with mouse touch pressures. A variety of off-the-shelf touchscreens are available for the Raspberry Pi, however, we found that the Hyperpixel display offers a good compromise between size and sensitivity. Visiomode interfaces with touchscreen devices connected to its Raspberry Pi host via the host’s operating system. This allows for Visiomode to work with the wide range of touchscreen devices with no additional configuration. Consequently, touchscreen calibration, which should not be required in most instances, would not be performed within the Visiomode interface but rather via the operating system’s own tools. To use touchscreen devices for which the Raspberry Pi operating system offers no out-of-the-box solution for calibration, users should, in the first instance, refer to the manufacturer’s specification for device drivers compatible with the Linux kernel. Alternatively, Visiomode supports the integration of custom-written hardware drivers through its devices interface, which allows for the custom integration of any external touchscreen device.</p><p id="P27">To record the behavior of the mouse in the arena (e.g., open field locomotion, rearing, grooming and task engagement) a webcam (Logitech, UK) was mounted on the wall of the arena opposite the touchscreen using a custom 3D printed support arm (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.6877081">https://doi.org/10.5281/zenodo.6877081</ext-link>).</p><p id="P28">Upon successful completion of a trial, we delivered a water reward via a 2 mm diameter clear acrylic spout mounted directly below the touchscreen. The reward spout was gravity fed via a water reservoir positioned 30 cm above the arena and dispensation was controlled by a 5 V solenoid valve (RS Components, UK) connected to an Arduino Nano microcontroller (RS Components, UK). Solenoid opening times and the height of the reservoir were adjusted to reproducibly release 10 μl of water per rewarded trial (measured using a calibrated pipette). To prevent mice from chewing or pulling the spout after reward delivery it was mounted to a 5 V servomotor which rotated the spout by 10 degrees after a 1 s delay effectively retracting the spout. While we used a solenoid valve to dispense water, the reward mechanisms could be replaced by a syringe pump allowing for finer and dynamic reward size control (<xref ref-type="bibr" rid="R3">Amarante et al., 2019</xref>), or by an automated food hopper mounted to the side of the touchscreen module (<xref ref-type="bibr" rid="R1">Acosta-Rodríguez et al., 2017</xref>). Full behavioral arena and touchscreen module build details can be found at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.6877081">https://doi.org/10.5281/zenodo.6877081</ext-link>.</p></sec><sec id="S12"><title>Animals, habituation, and water control</title><p id="P29">For all behavioral experiments, male adult C57BL/6J wild-type mice (8-10 weeks old, 20-30g, 3-4 animals per cage) were maintained on a reversed 12:12 hour light:dark cycle and provided ad libitum access to food and water as well as environmental enrichment. All experiments and procedures were approved by the University of Edinburgh local ethical review committee and performed under license from the UK Home Office in accordance with the Animal (Scientific Procedures) Act 1986. Mice were handled extensively for at least 5 days prior to any behavioral training and were trained once per day for 30 mins. To increase task engagement, mice were placed on a water control regime (1 ml / day) and weighed daily to ensure body weight remained above 80% of baseline (<xref ref-type="bibr" rid="R16">Dacre et al., 2021</xref>).</p></sec><sec id="S13"><title>Data Analysis &amp; Statistics</title><p id="P30">Data were analyzed using custom scripts written in Python v3.7. Data are reported as mean ±95% bootstrapped confidence interval (95% CI) (10,000 bootstrap samples, 50 replicates per sample) unless otherwise stated. To assess the ability of mice to discriminate between the two visual stimuli in the 2AFC task, we calculated a discriminability index (d’), defined as <disp-formula id="FD1"><mml:math id="M1"><mml:mrow><mml:mi>d</mml:mi><mml:mo>’</mml:mo><mml:mo>=</mml:mo><mml:mfrac bevelled="true"><mml:mn>1</mml:mn><mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:mi>A</mml:mi><mml:mo>))</mml:mo></mml:mrow></mml:math></disp-formula> where Z(x) is the inverse-normal transformation of x, and H and FA correspond to the hit and false alarm rates, respectively (<xref ref-type="bibr" rid="R46">Stanislaw &amp; Todorov, 1999</xref>). All analysis is available in the form of Jupyter notebooks at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.6877795">https://doi.org/10.5281/zenodo.6877795</ext-link>.</p></sec></sec><sec id="S14" sec-type="results"><title>Results</title><sec id="S15"><title>Phase I: simple stimulus-response task</title><p id="P31">In the first phase of training mice had to learn a simple stimulus-response behavior. Each trial began with a fixed-length (3 s) inter-trial-interval (ITI), where the touchscreen was left blank (default is a solid black screen), before presentation of a 10 s drifting grating stimulus (100% contrast, 1 Hz sinusoid at 30 cycles / degree). During this shaping phase, mice explored the behavioral arena and responded to the presentation of a drifting grating by nose poking the touchscreen to receive a 10 μl water reward. The touchscreen rests behind a clear, transparent insert which restricts the touchable surface thus centralizing contact points. Any contact with the screen during the ITI was deemed a ‘uncued touch’ and resulted in a reset and commencement of a subsequent ITI. To allow time for reward consumption during successful trials we implemented a 1 s delay prior to the start of the next trial (<xref ref-type="fig" rid="F4">Figure 4a-b</xref>). Mice rapidly learned the association between presentation of the target stimulus, response and reward displaying an increased number of successful trials and fewer miss trials (i.e., no touch response during stimulus) (<xref ref-type="fig" rid="F4">Figure 4c-d</xref>). Miss trials and uncued touches were not punished. On average, mice required two behavioral sessions to reach our experimenter-defined threshold of &gt;70 successful trials for 2 consecutive days, before they transferred to Phase II of the behavior (mean = 1.9 days [1.7, 2.1] 95% CI, N = 9 mice) (<xref ref-type="fig" rid="F4">Figure 4e</xref>).</p></sec><sec id="S16"><title>Phase II: 2-AFC visual discrimination nose poke task.</title><p id="P32">After successful completion of behavioral shaping in Phase I, mice progressed to the 2-alternative forced choice (2-AFC) version of the task which incorporated a pseudo-ran-domized (4-6 s) ITI, with the target stimulus (drifting grating, 10 s) presented alongside an isoluminant grey distractor stimulus. The 10 mm ‘dead zone’ separating the stimulus and distractor was an area where touch events would not be registered (black vertical line, <xref ref-type="fig" rid="F4">Figure 4f</xref>). The left versus right positioning of the target and distractor stimuli were pseudo-randomly ordered per trial. Mice learned to nose poke the target stimulus to receive a 10 μl water reward, while ignoring the distractor stimulus. If mice incorrectly chose the distractor stimulus, the subsequent trial was a correction trial, whereby the same stimulus placement was presented until the mouse correctly touched the target stimulus (<xref ref-type="fig" rid="F4">Figure 4g</xref>). Mice rapidly learned to discriminate between target and distractor stimuli, with reliable discrimination (discrimination index d’ &gt; 1.5) after an average of 9 training sessions (mean = 8.7, [7.4, 10.0] 95% CI, N = 9 mice) with peak discrimination reflecting very high performance (d’ mean = 2.4 [2.2, 2.6], N = 9 mice) (<xref ref-type="fig" rid="F5">Figure 5h-i</xref>). To ensure an unbiased measure of discriminability, correction trials were excluded from d’ calculations. In addition, task engagement (i.e., hit trials / total trials) was consistently high across behavioral sessions (bootstrap mean = 92.3% [84.3, 100.0] 95% CI response rate across trials, N = 9 mice) despite mice routinely receiving more than their daily allowance of water during the task (cumulative volume of rewards &gt; 1 ml per session) (data not shown). After ~12 sessions, d’ became asymptotic and at 20 sessions mice were transferred to Phase III of the task.</p></sec><sec id="S17"><title>Phase III: 2-AFC visual discrimination reaching task.</title><p id="P33">To explore the use of forelimb reaching as an alternative readout in our 2-AFC task, we replaced the transparent ‘nose poke’ insert with an insert containing two guide slits spaced 15 mm apart. We encouraged reaching by placing the touchscreen 8 mm from the front of the slit (2 mm deep insert + 6 mm space between insert and touchscreen) which restricted access such that mice could neither nose poke nor contact the screen using their tongue (<xref ref-type="fig" rid="F5">Figure 5a</xref>). Mice had to learn to reach through the slit corresponding to the target stimulus to gain a 10 μl water reward, while ignoring the slit associated with the distractor stimulus. This task setup allows investigation of sensory perception, decision-making and skilled motor control with quantifiable metrics for each component. Given the increase in complexity of the movement when switching from nose poke to reaching, mice inevitably made more mistakes resulting in a reduction in d’ immediately after the transition. This was not due to inactivity as mice displayed many cue-triggered reaches but with a high proportion of misses (mean = 168.8 reaches [160.9, 176.6] 95% CI, N = 9 mice), comparable to the number of nose pokes in Phase II (mean = 180.5 nose pokes [169.3, 191.8] 95% CI; median difference = 16.6 responses [-9.7, 34.6] 95% CI, p = 0.14; N = 9 mice). Mice also explored using nose pokes and licking as a strategy before associating reaching with reward. The exploration phase lasted for only a short period of time with d’ recovering to &gt; 1.5 within 2 training sessions (mean =2.3 days [2.0, 2.7] 95% CI, N = 9 mice) (<xref ref-type="fig" rid="F5">Figure 5b-c</xref>). A hallmark of rodent motor learning is the development of reproducible, stereotyped reach trajectories (<xref ref-type="bibr" rid="R5">Becker and Person, 2019</xref>; <xref ref-type="bibr" rid="R21">Galinanes et al., 2018</xref>; <xref ref-type="bibr" rid="R29">Kawai et al., 2015</xref>). By comparing the average pairwise distance between paw touch positions across learning, we could demonstrate the rapid decrease in pairwise distance across training, resulting in highly clustered touch positions after ~10 training sessions (early, mean = 6.3 [5.9, 6.7] 95% CI; late, mean = 2.6 [2.5, 2.8] 95% CI; median difference = 4.1 [3.8, 4.4] 96% CI, p = 2.5x10,<sup>-128</sup>, N = 9 mice) (<xref ref-type="fig" rid="F5">Figure 5d-f</xref>). Together, our results show that mice rapidly accommodate the switch in task structure, transferring from nose poke to visually guided reaching with minimal extra training. To facilitate uptake, we have generated a detailed, step-by-step protocol for behavioral training that can be found at <ext-link ext-link-type="uri" xlink:href="https://dx.doi.org/10.17504/protocols.io.bumgnu3w">https://dx.doi.org/10.17504/protocols.io.bumgnu3w</ext-link>.</p></sec></sec><sec id="S18" sec-type="discussion"><title>Discussion</title><p id="P34">Here we have developed Visiomode (<ext-link ext-link-type="uri" xlink:href="http://www.visiomode.org">www.visiomode.org</ext-link>), a complete open-source software and hardware platform for building touchscreen-based behavioral tasks for rodents. As a key design principle, our aim was to develop a platform that was low cost and as close to turnkey as possible without sacrificing flexibility.</p><p id="P35">Visiomode’s goal is to empower users with little or no programming experience to run their own touchscreen tasks without the up-front cost of a commercial solution. After setting up a behavioral arena to the specifications we describe in this paper, the typical Visiomode user is four clicks away from their own battery of touchscreen-based behavioral tasks. First, a user would navigate to our website at <ext-link ext-link-type="uri" xlink:href="http://www.visiomode.org">www.visiomode.org</ext-link>, download and install the software with the instructions provided, choose from a wide selection of pre-programmed task paradigms, and click start. In contrast with currently available open-source solutions (<xref ref-type="bibr" rid="R24">Gurley, 2019</xref>; <xref ref-type="bibr" rid="R38">O’Leary et al., 2018</xref>; <xref ref-type="bibr" rid="R42">Pineno, 2014</xref>; <xref ref-type="bibr" rid="R10">Buscher et al 2020</xref>), Visiomode is openly available online and with no programming experience required due to its web interface that encapsulates all the functionality required to design tasks, as well as acquire and export behavioural data. The platform can be parallelized with no additional effort; users can follow the same steps for adding additional arenas, all of which can then be controlled from the same web browser. Thus, Visiomode is a unique, turkey solution which addresses many of the shortcomings of currently available open-source touchscreen solutions (i.e. ease of use, paradigm flexibility, code accessibility), and addresses many of the unmet needs of the user community (i.e. easy parallelization, low cost, adaptable).</p><p id="P36">Visiomode has been designed to be a community-driven project. Project development takes place on GitHub with a transparent development roadmap (<ext-link ext-link-type="uri" xlink:href="https://github.com/DuguidLab/visiomode">https://github.com/DuguidLab/visiomode</ext-link>). Members of the community can contribute plugins for new protocols, stimuli and external USB devices, report bugs or suggest improvements in the software, help with documenting Visiomode or contribute to the development of the core API. The project will follow the “fork and pull” model of open-source software development for contributions, whereby any user can obtain their own copy of the source code to make changes, avoiding the need for change-related permissions. Submitted changes will be audited by the host lab to ensure all code conforms to the core project style. The “fork and pull” model empowers any user to become a contributor by eliminating the need for individual contributions to be coordinated by the project’s core team, while the review process ensures Visiomode’s stability and compatibility across behaviors. Our vision is that community-based contributions (protocols, stimuli etc) will eventually generate a comprehensive ‘go-to’ solution for any form of touchscreen-based behavior.</p><p id="P37">The Visiomode platform can accommodate a wide range of hardware configurations, including the addition of multiple USB-connected peripherals acting as input or output devices and various sized touchscreen to suit both rat and mouse behavioral arenas (<xref ref-type="bibr" rid="R4">Arulsamy et al., 2019</xref>; <xref ref-type="bibr" rid="R7">Brasted et al., 2002</xref>; <xref ref-type="bibr" rid="R8">Brigman et al., 2010</xref>; <xref ref-type="bibr" rid="R11">Bussey et al., 1998</xref>; <xref ref-type="bibr" rid="R13">Bussey et al., 2008</xref>; <xref ref-type="bibr" rid="R17">Delotterie et al., 2014</xref>; <xref ref-type="bibr" rid="R22">Glover et al., 2020</xref>; <xref ref-type="bibr" rid="R24">Gurley, 2019</xref>; <xref ref-type="bibr" rid="R25">Haddad et al., 2021</xref>; <xref ref-type="bibr" rid="R27">Heath et al., 2019</xref>; <xref ref-type="bibr" rid="R38">O’Leary et al., 2018</xref>; <xref ref-type="bibr" rid="R39">Odland et al., 2021</xref>; <xref ref-type="bibr" rid="R41">Piantadosi et al., 2019</xref>; <xref ref-type="bibr" rid="R42">Pineno, 2014</xref>; <xref ref-type="bibr" rid="R47">Stirman et al., 2016</xref>; <xref ref-type="bibr" rid="R49">Talpos et al., 2008</xref>). In addition, it permits rapid scaling to parallelize multiple behavioral arenas, each hosting its own web interface that is accessible via any personal device web browser connected to the same network. Unlike parallelized behavioral setups that are controlled by a single GUI running via a centralized host, Visiomode’s decentralized web interface design allows for multiple behavioral setups running separate tasks to be controlled in parallel. This parallelization model will facilitate high-throughput behavioral testing, enabling large-scale behavioral assays, efficient drug screening, or disease model phenotyping. This combined with the easy-to-use USB plug-and-play design permits rapid scaling up and scaling down of experiments on-the-fly depending on experimental requirements.</p><p id="P38">By developing an open-source, rapidly scalable and low-cost platform our aim is to increase uptake of touchscreen-based behavioral assays across our community, accelerating the investigation of cognition, decisionmaking and sensorimotor behaviors both in health and disease.</p></sec></body><back><ack id="S19"><title>Acknowledgements</title><p>We are grateful to members of the Duguid lab for experimental discussions and comments on the manuscript. This study was supported by the Simons Initiative for the Developing Brain PhD studentship (C.E.) and a Wellcome Senior Research Fellowship (UK) (110131/Z/ 15/Z) to I.D.</p></ack><sec id="S20" sec-type="data-availability"><title>Data Availability</title><p id="P43">All code and data are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/DuguidLab/visiomode/">https://github.com/DuguidLab/visiomode/</ext-link>.</p></sec><fn-group><fn id="FN2" fn-type="con"><p id="P39"><bold>Author Contributions</bold></p><p id="P40">Conceptualization and design, C.E., and I.D.; methodology &amp; investigation, C.E., V.P., T.C., and I.D.; resources, C.E., T.C.; graphic design, M.Z.; writing, reviewing &amp; editing, all authors.</p></fn><fn id="FN3" fn-type="conflict"><p id="P41"><bold>Declaration of Competing Interest</bold></p><p id="P42">The authors declare no declarations of competing interest.</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Acosta-Rodríguez</surname><given-names>VA</given-names></name><name><surname>de Groot</surname><given-names>MH</given-names></name><name><surname>Rno-Fer-reira</surname><given-names>F</given-names></name><name><surname>Green</surname><given-names>CB</given-names></name><name><surname>Takahashi</surname><given-names>JS</given-names></name></person-group><article-title>Mice under caloric restriction self-impose a temporal restriction of food intake as revealed by an automated feeder system</article-title><source>Cell metabolism</source><year>2017</year><volume>26</volume><fpage>267</fpage><lpage>277</lpage><elocation-id>e262</elocation-id><pub-id pub-id-type="pmcid">PMC5576447</pub-id><pub-id pub-id-type="pmid">28683292</pub-id><pub-id pub-id-type="doi">10.1016/j.cmet.2017.06.007</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akam</surname><given-names>T</given-names></name><name><surname>Lustig</surname><given-names>A</given-names></name><name><surname>Rowland</surname><given-names>JM</given-names></name><name><surname>Kapanaiah</surname><given-names>SK</given-names></name><name><surname>Esteve-Agraz</surname><given-names>J</given-names></name><name><surname>Panniello</surname><given-names>M</given-names></name><name><surname>Marquez</surname><given-names>C</given-names></name><name><surname>Kohl</surname><given-names>MM</given-names></name><name><surname>Katzel</surname><given-names>D</given-names></name><name><surname>Costa</surname><given-names>RM</given-names></name><etal/></person-group><article-title>Open-source, Python-based, hardware and software for controlling behavioural neuroscience experiments</article-title><source>eLife</source><year>2022</year><volume>11</volume><pub-id pub-id-type="pmcid">PMC8769647</pub-id><pub-id pub-id-type="pmid">35043782</pub-id><pub-id pub-id-type="doi">10.7554/eLife.67846</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amarante</surname><given-names>LM</given-names></name><name><surname>Newport</surname><given-names>J</given-names></name><name><surname>Mitchell</surname><given-names>M</given-names></name><name><surname>Wilson</surname><given-names>J</given-names></name><name><surname>Laubach</surname><given-names>M</given-names></name></person-group><article-title>An Open Source Syringe Pump Controller for Fluid Delivery of Multiple Volumes</article-title><source>eNeuro</source><year>2019</year><volume>6</volume><pub-id pub-id-type="pmcid">PMC6734045</pub-id><pub-id pub-id-type="pmid">31416819</pub-id><pub-id pub-id-type="doi">10.1523/ENEURO.0240-19.2019</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arulsamy</surname><given-names>A</given-names></name><name><surname>Corrigan</surname><given-names>F</given-names></name><name><surname>Collins-Praino</surname><given-names>LE</given-names></name></person-group><article-title>Age, but not severity of injury, mediates decline in executive function: Validation of the rodent touchscreen paradigm for preclinical models of traumatic brain injury</article-title><source>Behav Brain Res</source><year>2019</year><volume>368</volume><elocation-id>111912</elocation-id><pub-id pub-id-type="pmid">30998995</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Becker</surname><given-names>MI</given-names></name><name><surname>Person</surname><given-names>AL</given-names></name></person-group><article-title>Cerebellar control of reach kinematics for endpoint precision</article-title><source>Neuron</source><year>2019</year><volume>103</volume><fpage>335</fpage><lpage>348</lpage><elocation-id>e335</elocation-id><pub-id pub-id-type="pmcid">PMC6790131</pub-id><pub-id pub-id-type="pmid">31174960</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2019.05.007</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beraldo</surname><given-names>FH</given-names></name><name><surname>Palmer</surname><given-names>D</given-names></name><name><surname>Memar</surname><given-names>S</given-names></name><name><surname>Wasserman</surname><given-names>DI</given-names></name><name><surname>Lee</surname><given-names>WV</given-names></name><name><surname>Liang</surname><given-names>S</given-names></name><name><surname>Creighton</surname><given-names>SD</given-names></name><name><surname>Kolisnyk</surname><given-names>B</given-names></name><name><surname>Cowan</surname><given-names>MF</given-names></name><name><surname>Mels</surname><given-names>J</given-names></name><etal/></person-group><article-title>MouseBytes, an open-access high-throughput pipeline and database for rodent touchscreen-based cognitive assessment</article-title><source>eLife</source><year>2019</year><volume>8</volume><pub-id pub-id-type="pmcid">PMC6934379</pub-id><pub-id pub-id-type="pmid">31825307</pub-id><pub-id pub-id-type="doi">10.7554/eLife.49630</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brasted</surname><given-names>PJ</given-names></name><name><surname>Bussey</surname><given-names>TJ</given-names></name><name><surname>Murray</surname><given-names>EA</given-names></name><name><surname>Wise</surname><given-names>SP</given-names></name></person-group><article-title>Fornix transection impairs conditional visuomotor learning in tasks involving nonspatially differentiated responses</article-title><source>J Neurophysiol</source><year>2002</year><volume>87</volume><fpage>631</fpage><lpage>633</lpage><pub-id pub-id-type="pmid">11784778</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brigman</surname><given-names>JL</given-names></name><name><surname>Graybeal</surname><given-names>C</given-names></name><name><surname>Holmes</surname><given-names>A</given-names></name></person-group><article-title>Predictably irrational: assaying cognitive ility in mouse models of schizophrenia</article-title><source>Front Neurosci</source><year>2010</year><volume>4</volume><pub-id pub-id-type="pmcid">PMC2938983</pub-id><pub-id pub-id-type="pmid">20859447</pub-id><pub-id pub-id-type="doi">10.3389/neuro.01.013.2010</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Broschard</surname><given-names>MB</given-names></name><name><surname>Kim</surname><given-names>J</given-names></name><name><surname>Love</surname><given-names>BC</given-names></name><name><surname>Freeman</surname><given-names>JH</given-names></name></person-group><article-title>Category learning in rodents using touchscreen-based tasks</article-title><source>Genes Brain Behav</source><year>2021</year><volume>20</volume><elocation-id>e12665</elocation-id><pub-id pub-id-type="pmid">32383519</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buscher</surname><given-names>N</given-names></name><name><surname>Ojeda</surname><given-names>A</given-names></name><name><surname>Francoeur</surname><given-names>M</given-names></name><name><surname>Hulyalkar</surname><given-names>S</given-names></name><name><surname>Claros</surname><given-names>C</given-names></name><name><surname>Tang</surname><given-names>T</given-names></name><name><surname>Terry</surname><given-names>A</given-names></name><name><surname>Gupta</surname><given-names>A</given-names></name><name><surname>Fakhraei</surname><given-names>L</given-names></name><name><surname>Ramanathan</surname><given-names>DS</given-names></name></person-group><article-title>Open-source raspberry Pi-based operant box for translational behavioral testing in rodents</article-title><source>Journal of Neuroscience Methods</source><year>2020</year><volume>342</volume><elocation-id>108761</elocation-id><pub-id pub-id-type="pmid">32479970</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bussey</surname><given-names>TJ</given-names></name><name><surname>Clea Warburton</surname><given-names>E</given-names></name><name><surname>Aggleton</surname><given-names>JP</given-names></name><name><surname>Muir</surname><given-names>JL</given-names></name></person-group><article-title>Fornix lesions can facilitate acquisition of the transverse patterning task: a challenge for “configural” theories of hippocampal function</article-title><source>J Neurosci</source><year>1998</year><volume>18</volume><fpage>1622</fpage><lpage>1631</lpage><pub-id pub-id-type="pmcid">PMC6792739</pub-id><pub-id pub-id-type="pmid">9454867</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.18-04-01622.1998</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bussey</surname><given-names>TJ</given-names></name><name><surname>Muir</surname><given-names>JL</given-names></name><name><surname>Everitt</surname><given-names>BJ</given-names></name><name><surname>Robbins</surname><given-names>TW</given-names></name></person-group><article-title>Triple dissociation of anterior cingulate, posterior cingulate, and medial frontal cortices on visual discrimination tasks using a touchscreen testing procedure for the rat</article-title><source>Behav Neu-rosci</source><year>1997</year><volume>111</volume><fpage>920</fpage><lpage>936</lpage><pub-id pub-id-type="pmid">9383514</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bussey</surname><given-names>TJ</given-names></name><name><surname>Padain</surname><given-names>TL</given-names></name><name><surname>Skillings</surname><given-names>EA</given-names></name><name><surname>Winters</surname><given-names>BD</given-names></name><name><surname>Morton</surname><given-names>AJ</given-names></name><name><surname>Saksida</surname><given-names>LM</given-names></name></person-group><article-title>The touchscreen cognitive testing method for rodents: how to get the best out of your rat</article-title><source>Learn Mem</source><year>2008</year><volume>15</volume><fpage>516</fpage><lpage>523</lpage><pub-id pub-id-type="pmcid">PMC2505319</pub-id><pub-id pub-id-type="pmid">18612068</pub-id><pub-id pub-id-type="doi">10.1101/lm.987808</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chow</surname><given-names>WZ</given-names></name><name><surname>Ong</surname><given-names>LK</given-names></name><name><surname>Kluge</surname><given-names>MG</given-names></name><name><surname>Gyawali</surname><given-names>P</given-names></name><name><surname>Walker</surname><given-names>FR</given-names></name><name><surname>Nilsson</surname><given-names>M</given-names></name></person-group><article-title>Similar cognitive deficits in mice and humans in the chronic phase post-stroke identified using the touchscreen-based paired-associate learning task</article-title><source>Sci Rep</source><year>2020</year><volume>10</volume><elocation-id>19545</elocation-id><pub-id pub-id-type="pmcid">PMC7658221</pub-id><pub-id pub-id-type="pmid">33177588</pub-id><pub-id pub-id-type="doi">10.1038/s41598-020-76560-x</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Copping</surname><given-names>NA</given-names></name><name><surname>Berg</surname><given-names>EL</given-names></name><name><surname>Foley</surname><given-names>GM</given-names></name><name><surname>Schaffler</surname><given-names>MD</given-names></name><name><surname>Onaga</surname><given-names>BL</given-names></name><name><surname>Buscher</surname><given-names>N</given-names></name><name><surname>Silverman</surname><given-names>JL</given-names></name><name><surname>Yang</surname><given-names>M</given-names></name></person-group><article-title>Touchscreen learning deficits and normal social approach behavior in the Shank3B model of Phelan-McDermid Syndrome and autism</article-title><source>Neuroscience</source><year>2017</year><volume>345</volume><fpage>155</fpage><lpage>165</lpage><pub-id pub-id-type="pmcid">PMC5108683</pub-id><pub-id pub-id-type="pmid">27189882</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroscience.2016.05.016</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dacre</surname><given-names>J</given-names></name><name><surname>Colligan</surname><given-names>M</given-names></name><name><surname>Clarke</surname><given-names>T</given-names></name><name><surname>Ammei</surname><given-names>JJ</given-names></name><name><surname>Schiemann</surname><given-names>J</given-names></name><name><surname>Chamosa-Pino</surname><given-names>V</given-names></name><name><surname>Claudi</surname><given-names>F</given-names></name><name><surname>Harston</surname><given-names>JA</given-names></name><name><surname>Eleftheriou</surname><given-names>C</given-names></name><name><surname>Pakan</surname><given-names>JM</given-names></name></person-group><article-title>A cerebellar-thalamocortical pathway drives behavioral context-dependent movement initiation</article-title><source>Neuron</source><year>2021</year><volume>109</volume><fpage>2326</fpage><lpage>2338</lpage><elocation-id>e2328</elocation-id><pub-id pub-id-type="pmcid">PMC8315304</pub-id><pub-id pub-id-type="pmid">34146469</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2021.05.016</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Delotterie</surname><given-names>D</given-names></name><name><surname>Mathis</surname><given-names>C</given-names></name><name><surname>Cassel</surname><given-names>JC</given-names></name><name><surname>Dorner-Ciossek</surname><given-names>C</given-names></name><name><surname>Marti</surname><given-names>A</given-names></name></person-group><article-title>Optimization of touchscreen-based behavioral paradigms in mice: implications for building a battery of tasks taxing learning and memory functions</article-title><source>PLoS One</source><year>2014</year><volume>9</volume><elocation-id>e100817</elocation-id><pub-id pub-id-type="pmcid">PMC4069170</pub-id><pub-id pub-id-type="pmid">24960028</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0100817</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dumont</surname><given-names>JR</given-names></name><name><surname>Salewski</surname><given-names>R</given-names></name><name><surname>Beraldo</surname><given-names>F</given-names></name></person-group><article-title>Critical mass: The rise of a touchscreen technology community for rodent cognitive testing</article-title><source>Genes Brain Behav</source><year>2021</year><volume>20</volume><elocation-id>e12650</elocation-id><pub-id pub-id-type="pmid">32141694</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fortunato</surname><given-names>L</given-names></name><name><surname>Galassi</surname><given-names>M</given-names></name></person-group><article-title>The case for free and open source software in research and scholarship</article-title><source>Philos Trans A Math Phys Eng Sci</source><year>2021</year><volume>379</volume><elocation-id>20200079</elocation-id><pub-id pub-id-type="pmid">33775148</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freeman</surname><given-names>J</given-names></name></person-group><article-title>Open source tools for large-scale neuroscience</article-title><source>Current opinion in neurobiology</source><year>2015</year><volume>32</volume><fpage>156</fpage><lpage>163</lpage><pub-id pub-id-type="pmid">25982977</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Galinanes</surname><given-names>GL</given-names></name><name><surname>Bonardi</surname><given-names>C</given-names></name><name><surname>Huber</surname><given-names>D</given-names></name></person-group><article-title>Directional Reaching for Water as a Cortex-Dependent Behavioral Framework for Mice</article-title><source>Cell reports</source><year>2018</year><volume>22</volume><fpage>2767</fpage><lpage>2783</lpage><pub-id pub-id-type="pmcid">PMC5863030</pub-id><pub-id pub-id-type="pmid">29514103</pub-id><pub-id pub-id-type="doi">10.1016/j.celrep.2018.02.042</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glover</surname><given-names>LR</given-names></name><name><surname>Postle</surname><given-names>AF</given-names></name><name><surname>Holmes</surname><given-names>A</given-names></name></person-group><article-title>Touchscreen-based assessment of risky-choice in mice</article-title><source>Behav Brain Res</source><year>2020</year><volume>393</volume><elocation-id>112748</elocation-id><pub-id pub-id-type="pmcid">PMC7423716</pub-id><pub-id pub-id-type="pmid">32531231</pub-id><pub-id pub-id-type="doi">10.1016/j.bbr.2020.112748</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Groman</surname><given-names>SM</given-names></name><name><surname>Lee</surname><given-names>B</given-names></name><name><surname>Seu</surname><given-names>E</given-names></name><name><surname>James</surname><given-names>AS</given-names></name><name><surname>Feiler</surname><given-names>K</given-names></name><name><surname>Mandelkern</surname><given-names>MA</given-names></name><name><surname>London</surname><given-names>ED</given-names></name><name><surname>Jentsch</surname><given-names>JD</given-names></name></person-group><article-title>Dysregulation of D(2)-mediated dopamine transmission in monkeys after chronic escalating methamphetamine exposure</article-title><source>J Neurosci</source><year>2012</year><volume>32</volume><fpage>5843</fpage><lpage>5852</lpage><pub-id pub-id-type="pmcid">PMC3353813</pub-id><pub-id pub-id-type="pmid">22539846</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0029-12.2012</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gurley</surname><given-names>K</given-names></name></person-group><article-title>Two open source designs for a low-cost operant chamber using Raspberry Pi</article-title><source>J Exp Anal Behav</source><year>2019</year><volume>111</volume><fpage>508</fpage><lpage>518</lpage><pub-id pub-id-type="pmid">31038195</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haddad</surname><given-names>FL</given-names></name><name><surname>Ghahremani</surname><given-names>M</given-names></name><name><surname>De Oliveira</surname><given-names>C</given-names></name><name><surname>Doornaert</surname><given-names>EE</given-names></name><name><surname>Johnston</surname><given-names>KD</given-names></name><name><surname>Everling</surname><given-names>S</given-names></name><name><surname>Schmid</surname><given-names>S</given-names></name></person-group><article-title>A Novel Three-Choice Touchscreen Task to Examine Spatial Attention and Orienting Responses in Rodents</article-title><source>eNeuro</source><year>2021</year><volume>8</volume><pub-id pub-id-type="pmcid">PMC8272401</pub-id><pub-id pub-id-type="pmid">33789926</pub-id><pub-id pub-id-type="doi">10.1523/ENEURO.0032-20.2021</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hall</surname><given-names>C</given-names></name><name><surname>Ballachey</surname><given-names>EL</given-names></name></person-group><article-title>A study of the rat,s behavior in a field. A contribution to method in comparative psychology</article-title><source>University of California Publications in Psychology</source><year>1932</year></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heath</surname><given-names>CJ</given-names></name><name><surname>O’Callaghan</surname><given-names>C</given-names></name><name><surname>Mason</surname><given-names>SL</given-names></name><name><surname>Phillips</surname><given-names>BU</given-names></name><name><surname>Saksida</surname><given-names>LM</given-names></name><name><surname>Robbins</surname><given-names>TW</given-names></name><name><surname>Barker</surname><given-names>RA</given-names></name><name><surname>Bussey</surname><given-names>TJ</given-names></name><name><surname>Sahakian</surname><given-names>BJ</given-names></name></person-group><article-title>A Touchscreen Motivation Assessment Evaluated in Huntington,s Disease Patients and R6/1 Model Mice</article-title><source>Front Neurol</source><year>2019</year><volume>10</volume><fpage>858</fpage><pub-id pub-id-type="pmcid">PMC6696591</pub-id><pub-id pub-id-type="pmid">31447770</pub-id><pub-id pub-id-type="doi">10.3389/fneur.2019.00858</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hvoslef-Eide</surname><given-names>M</given-names></name><name><surname>Nilsson</surname><given-names>S</given-names></name><name><surname>Saksida</surname><given-names>L</given-names></name><name><surname>Bussey</surname><given-names>T</given-names></name></person-group><article-title>Cognitive translation using the rodent touchscreen testing approach</article-title><source>Translational Neuropsychopharmacology</source><year>2015</year><fpage>423</fpage><lpage>447</lpage><pub-id pub-id-type="pmid">27305921</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kawai</surname><given-names>R</given-names></name><name><surname>Markman</surname><given-names>T</given-names></name><name><surname>Poddar</surname><given-names>R</given-names></name><name><surname>Ko</surname><given-names>R</given-names></name><name><surname>Fan-tana</surname><given-names>AL</given-names></name><name><surname>Dhawale</surname><given-names>AK</given-names></name><name><surname>Kampff</surname><given-names>AR</given-names></name><name><surname>Olveczky</surname><given-names>BP</given-names></name></person-group><article-title>Motor cortex is required for learning but not for executing a motor skill</article-title><source>Neuron</source><year>2015</year><volume>86</volume><fpage>800</fpage><lpage>812</lpage><pub-id pub-id-type="pmcid">PMC5939934</pub-id><pub-id pub-id-type="pmid">25892304</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2015.03.024</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kessenich</surname><given-names>J</given-names></name><name><surname>Sellers</surname><given-names>G</given-names></name><name><surname>Shreiner</surname><given-names>D</given-names></name></person-group><source>OpenGL Programming Guide: The official guide to learning OpenGL, version 4.5 with SPIR-V</source><publisher-name>Addison -Wesley Professional</publisher-name><year>2016</year></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>J</given-names></name><name><surname>Castro</surname><given-names>L</given-names></name><name><surname>Wasserman</surname><given-names>EA</given-names></name><name><surname>Freeman</surname><given-names>JH</given-names></name></person-group><article-title>Dorsal hippocampus is necessary for visual categorization in rats</article-title><source>Hippocampus</source><year>2018</year><volume>28</volume><fpage>392</fpage><lpage>405</lpage><pub-id pub-id-type="pmcid">PMC5992064</pub-id><pub-id pub-id-type="pmid">29473984</pub-id><pub-id pub-id-type="doi">10.1002/hipo.22839</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leach</surname><given-names>PT</given-names></name><name><surname>Crawley</surname><given-names>JN</given-names></name></person-group><article-title>Touchscreen learning deficits in Uoe3a, Ts65Dn and Mecp2 mouse models of neurodevelopmental disorders with intellectual disabilities</article-title><source>Genes Brain Behav</source><year>2018</year><volume>17</volume><elocation-id>e12452</elocation-id><pub-id pub-id-type="pmcid">PMC6013336</pub-id><pub-id pub-id-type="pmid">29266714</pub-id><pub-id pub-id-type="doi">10.1111/gbb.12452</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leach</surname><given-names>PT</given-names></name><name><surname>Hayes</surname><given-names>J</given-names></name><name><surname>Pride</surname><given-names>M</given-names></name><name><surname>Silverman</surname><given-names>JL</given-names></name><name><surname>Crawley</surname><given-names>JN</given-names></name></person-group><article-title>Normal Performance of Fmr1 Mice on a Touchscreen Delayed Nonmatching to Position Working Memory Task</article-title><source>eNeuro</source><year>2016</year><volume>3</volume><pub-id pub-id-type="pmcid">PMC4800045</pub-id><pub-id pub-id-type="pmid">27022628</pub-id><pub-id pub-id-type="doi">10.1523/ENEURO.0143-15.2016</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markham</surname><given-names>MR</given-names></name><name><surname>Butt</surname><given-names>AE</given-names></name><name><surname>Dougher</surname><given-names>MJ</given-names></name></person-group><article-title>A computer touch-screen apparatus for training visual discriminations in rats</article-title><source>J Exp Anal Behav</source><year>1996</year><volume>65</volume><fpage>173</fpage><lpage>182</lpage><pub-id pub-id-type="pmcid">PMC1350070</pub-id><pub-id pub-id-type="pmid">8583196</pub-id><pub-id pub-id-type="doi">10.1901/jeab.1996.65-173</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morton</surname><given-names>AJ</given-names></name><name><surname>Skillings</surname><given-names>E</given-names></name><name><surname>Bussey</surname><given-names>TJ</given-names></name><name><surname>Sak-sida</surname><given-names>LM</given-names></name></person-group><article-title>Measuring cognitive deficits in disabled mice using an automated interactive touchscreen system</article-title><source>Nat Methods</source><year>2006</year><volume>3</volume><fpage>767</fpage><pub-id pub-id-type="pmid">16990806</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nithianantharajah</surname><given-names>J</given-names></name><name><surname>McKechanie</surname><given-names>AG</given-names></name><name><surname>Stewart</surname><given-names>TJ</given-names></name><name><surname>Johnstone</surname><given-names>M</given-names></name><name><surname>Blackwood</surname><given-names>DH</given-names></name><name><surname>St Clair</surname><given-names>D</given-names></name><name><surname>Grant</surname><given-names>SG</given-names></name><name><surname>Bussey</surname><given-names>TJ</given-names></name><name><surname>Saksida</surname><given-names>LM</given-names></name></person-group><article-title>Bridging the translational divide: identical cognitive touchscreen testing in mice and humans carrying mutations in a disease-relevant homologous gene</article-title><source>Sci Rep</source><year>2015</year><volume>5</volume><elocation-id>14613</elocation-id><pub-id pub-id-type="pmcid">PMC4589696</pub-id><pub-id pub-id-type="pmid">26423861</pub-id><pub-id pub-id-type="doi">10.1038/srep14613</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norris</surname><given-names>RHC</given-names></name><name><surname>Churilov</surname><given-names>L</given-names></name><name><surname>Hannan</surname><given-names>AJ</given-names></name><name><surname>Nithianantharajah</surname><given-names>J</given-names></name></person-group><article-title>Mutations in neuroli-gin-3 in male mice impact behavioral flexibility but not relational memory in a touchscreen test of visual transitive inference</article-title><source>Mol Autism</source><year>2019</year><volume>10</volume><fpage>42</fpage><pub-id pub-id-type="pmcid">PMC6889473</pub-id><pub-id pub-id-type="pmid">31827744</pub-id><pub-id pub-id-type="doi">10.1186/s13229-019-0292-2</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Leary</surname><given-names>JD</given-names></name><name><surname>O’Leary</surname><given-names>OF</given-names></name><name><surname>Cryan</surname><given-names>JF</given-names></name><name><surname>Nolan</surname><given-names>YM</given-names></name></person-group><article-title>A low-cost touchscreen operant chamber using a Raspberry Pi</article-title><source>Behav Res Methods</source><year>2018</year><volume>50</volume><fpage>2523</fpage><lpage>2530</lpage><pub-id pub-id-type="pmid">29520633</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Odland</surname><given-names>AU</given-names></name><name><surname>Sandahl</surname><given-names>R</given-names></name><name><surname>Andreasen</surname><given-names>JT</given-names></name></person-group><article-title>Sequential reversal learning: a new touchscreen schedule for assessing cognitive flexibility in mice</article-title><source>Psychopharmacology (Berl)</source><year>2021</year><volume>238</volume><fpage>383</fpage><lpage>397</lpage><pub-id pub-id-type="pmid">33123820</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peirce</surname><given-names>JW</given-names></name></person-group><article-title>PsychoPy--Psychophysics software in Python</article-title><source>J Neurosci Methods</source><year>2007</year><volume>162</volume><fpage>8</fpage><lpage>13</lpage><pub-id pub-id-type="pmcid">PMC2018741</pub-id><pub-id pub-id-type="pmid">17254636</pub-id><pub-id pub-id-type="doi">10.1016/j.jneumeth.2006.11.017</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piantadosi</surname><given-names>PT</given-names></name><name><surname>Lieberman</surname><given-names>AG</given-names></name><name><surname>Pickens</surname><given-names>CL</given-names></name><name><surname>Bergstrom</surname><given-names>HC</given-names></name><name><surname>Holmes</surname><given-names>A</given-names></name></person-group><article-title>A novel multichoice touchscreen paradigm for assessing cognitive flexibility in mice</article-title><source>Learn Mem</source><year>2019</year><volume>26</volume><fpage>24</fpage><lpage>30</lpage><pub-id pub-id-type="pmcid">PMC6298539</pub-id><pub-id pub-id-type="pmid">30559117</pub-id><pub-id pub-id-type="doi">10.1101/lm.048264.118</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pineno</surname><given-names>O</given-names></name></person-group><article-title>ArduiPod Box: a low-cost and open-source Skinner box using an iPod Touch and an Arduino microcontroller</article-title><source>Behav Res Methods</source><year>2014</year><volume>46</volume><fpage>196</fpage><lpage>205</lpage><pub-id pub-id-type="pmid">23813238</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rübel</surname><given-names>O</given-names></name><name><surname>Tritt</surname><given-names>A</given-names></name><name><surname>Dichter</surname><given-names>B</given-names></name><name><surname>Braun</surname><given-names>T</given-names></name><name><surname>Cain</surname><given-names>N</given-names></name><name><surname>Clack</surname><given-names>N</given-names></name><name><surname>Davidson</surname><given-names>T</given-names></name><name><surname>Dougherty</surname><given-names>M</given-names></name><name><surname>FillionRobin</surname><given-names>J</given-names></name><name><surname>Graddis</surname><given-names>N</given-names></name></person-group><article-title>NWB: N 2.0: An Accessible Data Standard for Neurophysiology</article-title><source>bioRxiv</source><year>2019</year><elocation-id>523035</elocation-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seitz</surname><given-names>BM</given-names></name><name><surname>McCune</surname><given-names>K</given-names></name><name><surname>MacPherson</surname><given-names>M</given-names></name><name><surname>Bergeron</surname><given-names>L</given-names></name><name><surname>Blaisdell</surname><given-names>AP</given-names></name><name><surname>Logan</surname><given-names>CJ</given-names></name></person-group><article-title>Using touchscreen equipped operant chambers to study animal cognition. Benefits, limitations, and advice</article-title><source>PLoS One</source><year>2021</year><volume>16</volume><elocation-id>e0246446</elocation-id><pub-id pub-id-type="pmcid">PMC7894864</pub-id><pub-id pub-id-type="pmid">33606723</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0246446</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Skinner</surname><given-names>B</given-names></name></person-group><chapter-title>The Behavior of Organisms: An Experimental Analysis</chapter-title><source>Department of Psychology Virginia Polytechnic Institute Blacksburg, Virginia</source><publisher-name>Appleton-Century</publisher-name><publisher-loc>New York</publisher-loc><year>1938</year><elocation-id>24061</elocation-id></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stanislaw</surname><given-names>H</given-names></name><name><surname>Todorov</surname><given-names>N</given-names></name></person-group><article-title>Calculation of signal detection theory measures</article-title><source>Behavior Research Methods, Instruments, &amp; Computers</source><year>1999</year><volume>31</volume><fpage>137</fpage><lpage>149</lpage><pub-id pub-id-type="pmid">10495845</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stirman</surname><given-names>J</given-names></name><name><surname>Townsend</surname><given-names>LB</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name></person-group><article-title>A touchscreen based global motion perception task for mice</article-title><source>Vision Res</source><year>2016</year><volume>127</volume><fpage>74</fpage><lpage>83</lpage><pub-id pub-id-type="pmcid">PMC5035629</pub-id><pub-id pub-id-type="pmid">27497283</pub-id><pub-id pub-id-type="doi">10.1016/j.visres.2016.07.006</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Talpos</surname><given-names>J</given-names></name><name><surname>Steckler</surname><given-names>T</given-names></name></person-group><article-title>Touching on translation</article-title><source>Cell Tissue Res</source><year>2013</year><volume>354</volume><fpage>297</fpage><lpage>308</lpage><pub-id pub-id-type="pmid">23949375</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Talpos</surname><given-names>JC</given-names></name><name><surname>Dias</surname><given-names>R</given-names></name><name><surname>Bussey</surname><given-names>TJ</given-names></name><name><surname>Saksida</surname><given-names>LM</given-names></name></person-group><article-title>Hippocampal lesions in rats impair learning and memory for locations on a touch-sensitive computer screen: the “ASAT” task</article-title><source>Behav Brain Res</source><year>2008</year><volume>192</volume><fpage>216</fpage><lpage>225</lpage><pub-id pub-id-type="pmid">18499279</pub-id></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van den Broeck</surname><given-names>L</given-names></name><name><surname>Hansquine</surname><given-names>P</given-names></name><name><surname>Callaerts-Vegh</surname><given-names>Z</given-names></name><name><surname>D’Hooge</surname><given-names>R</given-names></name></person-group><article-title>Impaired Reversal Learning in APPPS1-21 Mice in the Touchscreen Visual Discrimination Task</article-title><source>Front Behav Neurosci</source><year>2019</year><volume>13</volume><fpage>92</fpage><pub-id pub-id-type="pmcid">PMC6521801</pub-id><pub-id pub-id-type="pmid">31143103</pub-id><pub-id pub-id-type="doi">10.3389/fnbeh.2019.00092</pub-id></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>M</given-names></name><name><surname>Lewis</surname><given-names>FC</given-names></name><name><surname>Sarvi</surname><given-names>MS</given-names></name><name><surname>Foley</surname><given-names>GM</given-names></name><name><surname>Crawley</surname><given-names>JN</given-names></name></person-group><article-title>16p11.2 Deletion mice display cognitive deficits in touchscreen learning and novelty recognition tasks</article-title><source>Learn Mem</source><year>2015</year><volume>22</volume><fpage>622</fpage><lpage>632</lpage><pub-id pub-id-type="pmcid">PMC4749736</pub-id><pub-id pub-id-type="pmid">26572653</pub-id><pub-id pub-id-type="doi">10.1101/lm.039602.115</pub-id></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeleznikow-Johnston</surname><given-names>AM</given-names></name><name><surname>Renoir</surname><given-names>T</given-names></name><name><surname>Churilov</surname><given-names>L</given-names></name><name><surname>Li</surname><given-names>S</given-names></name><name><surname>Burrows</surname><given-names>EL</given-names></name><name><surname>Hannan</surname><given-names>AJ</given-names></name></person-group><article-title>Touchscreen testing reveals clinically relevant cognitive abnormalities in a mouse model of schizophrenia lacking metabotropic glutamate receptor 5</article-title><source>Sci Rep</source><year>2018</year><volume>8</volume><elocation-id>16412</elocation-id><pub-id pub-id-type="pmcid">PMC6219561</pub-id><pub-id pub-id-type="pmid">30401923</pub-id><pub-id pub-id-type="doi">10.1038/s41598-018-33929-3</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Visiomode: a flexible, scalable platform for building touchscreen-based tasks.</title><p><italic>Left,</italic> Image of the mouse behavioral arena with interactive touchscreen controlled by Visiomode. <italic>Right,</italic> Visiomode is a web-based interface that controls, via WiFi, a Raspberry Pi computer and any coupled USB devices (e.g. loudspeaker, USB microcontroller for reward delivery, touchscreen). Visual stimuli can be generated programmatically via Visiomode’s API or loaded as images / animation files on-the-fly and presented via any touchscreen supported by Raspberry Pi, including integrated touchscreen displays such as the Pimoroni Hyperpixel. Visiomode also supports real-time analysis with data being exported in a variety of formats, including JSON, HDF5 and NWB.</p></caption><graphic xlink:href="EMS158019-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Visiomode: web-based Graphical User Interface (GUI).</title><p>Visiomode GUI which provides session information for an individual mouse (mouse552) and experiment (single drifting grating target, nose poke). The ‘More Options’ tab allows the user to define advanced stimulus parameter options such as frequency, contrast, inter-trial-interval duration and separator size (pixels).</p></caption><graphic xlink:href="EMS158019-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Behavioral arena with touchscreen module.</title><p>(a) 3D reconstruction of the behavioral arena and touchscreen module consisting of a Hyperpixel Display 4.0, servo controlled reward spout, servo motor, solenoid, water reservoir and transparent Perspex screen divider with either nose poke or reaching slit cutouts. The behavioral arena and touchscreen module can be custom designed to fit the needs of each individual experiment.</p><p>(b) Schematic diagrams showing 2-AFC nose poke <italic>(left)</italic> and forelimb reaching <italic>(right)</italic> configurations. Note the reward spout retracts after each trial using a servomotor which rotates by 10 degrees.</p></caption><graphic xlink:href="EMS158019-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Using Visiomode to shape stimulus-response associations and 2-alternative forced choice task learning.</title><p>(a) Schematic showing a mouse engaged in a simple stimulus-response behavior (Phase I).</p><p>(b) Training flowchart showing Phase I of the behavioral task (simple stimulus-response association): presentation of a target stimulus (moving grating) after a fixed-length inter-trial-interval (3s, ITI) requires mice to nose poke the touchscreen to receive a water reward, failure to touch the screen results in the initiation of a subsequent ITI.</p><p>(c) Number of successful trials per 30 minute training session (blue lines, data from individual mice, N = 9 mice). Gray dotted line, threshold of &gt;70 rewards / session. Mice progress to Phase II after achieving &gt;70 rewards / session for two consecutive sessions.</p><p>(d) Number of missed trials per 30 minute training session (blue lines, data from individual mice, N = 9 mice). Note, decrease in the number of missed trials is reflected in the increase in the number of successful trials shown in (c).</p><p>(d) Number of missed trials per 30 minute training session (blue lines, data from individual mice, N = 9 mice). Note, decrease in the number of missed trials is reflected in the increase in the number of successful trials shown in (c).</p><p>(e) Box-and-whisker plot showing median, interquartile range, and range of the number of sessions required to reach &gt;70 rewards (N = 9 mice).</p><p>(f) Schematic showing a mouse engaged in a 2-AFC nose-poke task (Phase II).</p><p>(g) Training flowchart showing Phase II of the behavioral task (2-AFC): presentation of a pair of stimuli (target stimulus = moving grating; distractor stimulus = isoluminescent gray screen) after a pseudo-random inter-trial-interval (4-6s, ITI) requires mice to nose poke the target area of the touchscreen to receive a water reward, failure to touch the screen results in the initiation of a subsequent ITI. Nose poking the distractor area of the touchscreen results in a correction trial, where the same pair or stimuli are presented until a correct response has been achieved.</p><p>(h) Discriminability index (d’) as a function of the number of training sessions. Blue line, average d’ ± 95% CI (N = 9 mice). Gray dotted line, d’ threshold of 1.5.</p><p>(i) Box-and-whisker plot showing median, interquartile range, and range of the number of training sessions required to reach d’ &gt; 1.5 (N = 9 mice).</p></caption><graphic xlink:href="EMS158019-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>A 2-AFC visual discrimination reaching task for freely moving mice.</title><p>(a) Schematic showing a mouse engaged in a 2-AFC reaching task (Phase III).</p><p>(b) Discriminability index (d’) as a function of the number of training sessions. Blue line, average d’ ± 95% CI (N = 9 mice). Gray dotted line,d’ threshold of 1.5.</p><p>(c) Box-and-whisker plot showing median, interquartile range, and range of the number of training sessions required to reach d’ &gt; 1.5 (N = 9 mice).</p><p>(d) Schematic showing paw placement distributions during an early <italic>(left)</italic> and late <italic>(right)</italic> training session. Red dots depict individual paw placements within the open slit.</p><p>(e) Pairwise distance between individual paw placements as a function of the number of training sessions. Blue line, average pairwise distance (mm) ± 95% CI (N = 9 mice).</p><p>(f) Box-and-whisker plot showing median, interquartile range, and range of the pairwise distance between individual paw placements during an early and late training session (N = 9 mice). Red cross denotes identified outlier. <italic>p</italic> = 2.5x10<sup>-182</sup>.</p></caption><graphic xlink:href="EMS158019-f005"/></fig></floats-group></article>