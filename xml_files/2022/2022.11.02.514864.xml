<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS156653</article-id><article-id pub-id-type="doi">10.1101/2022.11.02.514864</article-id><article-id pub-id-type="archive">PPR566738</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Modelling surface color discrimination under different lighting environments using image chromatic statistics and convolutional neural networks</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Ponting</surname><given-names>Samuel</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Morimoto</surname><given-names>Takuma</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Smithson</surname><given-names>Hannah</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib></contrib-group><aff id="A1"><label>1</label>Department of Experimental Psychology, University of Oxford, Oxford, UK</aff><aff id="A2"><label>2</label>Department of General Psychology, Justus-Liebig-Universitat-Giessen, Giessen, Germany</aff><author-notes><corresp id="CR1">
<label>*</label><email>takuma.morimoto@psy.ox.ac.uk</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>05</day><month>11</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>03</day><month>11</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">We modeled discrimination thresholds for object colors under different lighting environments [<xref ref-type="bibr" rid="R1">1</xref>]. Firstly we built models based on chromatic statistics, testing 60 models in total. Secondly we trained convolutional neural networks (CNNs), using 160,280 images labeled either by the ground-truth or by human responses. No single chromatic statistics model was sufficient to describe human discrimination thresholds across conditions, while human-response-trained CNNs nearly perfectly predicted human thresholds. Guided by region-of-interest analysis of the network, we modified the chromatic statistics models to use only the lower regions of the objects, which substantially improved performance.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><label>1</label><title>Introduction</title><p id="P2">Humans can discriminate very subtle color differences. Mechanisms underpinning color discrimination have been a major focus of past color vision studies, and the non-uniformity of chromatic sensitivities across color spaces has been painstakingly specified in remarkable detail (e.g., [<xref ref-type="bibr" rid="R2">2</xref>–<xref ref-type="bibr" rid="R5">5</xref>]). These efforts not only informed psychophysical research but have also been used for applications in the display, printing, or textiles industries where knowledge about the discriminability of a given color pair is highly beneficial. However, these early works on color discrimination used colored lights or spatially-uniform color patches as experimental stimuli, and did not directly address the problem of object colour perception. For pigmented objects, spatial variation in pigmentation across the object’s surface introduces spatial colour variation, and objects cannot therefore be characterized by a single color [<xref ref-type="bibr" rid="R6">6</xref>]. Acknowledging this limitation, there have been recent attempts to investigate how well our visual system discriminates among natural objects by their color. Hansen et al. [<xref ref-type="bibr" rid="R7">7</xref>] directly measured discrimination thresholds using natural objects such as fruits and vegetables and chromatic textures that exhibit spatial color variation. It was found that discrimination ellipses were highly elongated along the axes of maximum variation in the chromatic distributions of the stimuli. Giesel et al. [<xref ref-type="bibr" rid="R8">8</xref>] additionally measured thresholds for chromatically variegated stimuli, and they found that discrimination models with eight parameters (four cardinal directions and four intermediate directions) better account for the obtained threshold patterns than four parameter models (cardinal mechanisms alone). In addition to these studies on color discrimination a few studies investigated the color appearance of chromatically variegated objects. One of these color matching experiments showed that humans match to different colors depending on the region of the objects that was specified to be judged [<xref ref-type="bibr" rid="R9">9</xref>]. Humans can effectively take the spatial average of hue [<xref ref-type="bibr" rid="R10">10</xref>], and mean hue value may indeed determine the overall impression of color for inhomogeneous objects [<xref ref-type="bibr" rid="R11">11</xref>].</p><p id="P3">These works emphasized the importance of specifically considering object-based color discrimination. Moreover, since most objects are visible only by virtue of the light they reflect, and diffuse and specular surface reflections have different imaging geometries, there is additional spatial variation across the proximal image of an illuminated object that derives from the lighting environment and viewing geometry. Natural environments introduce additional complexity because the proximal images of identical objects can exhibit very different color distributions when placed under different lighting environments. Moreover, even within a single object, different regions of the object will receive different illumination, and it is a curious empirical question whether we might correct them differently [<xref ref-type="bibr" rid="R12">12</xref>]. Yet the influence of lighting environments on color discrimination has been little studied, especially in the presence of complex object properties (e.g., three-dimensionality and specular reflection). To tackle this, Morimoto &amp; Smithson [<xref ref-type="bibr" rid="R1">1</xref>] measured chromatic thresholds using glossy and matte objects, placed under one of three lighting environments. Results showed that discrimination ability is strongly influenced by the chromatic distributions of the lighting environments in which objects are placed (<xref ref-type="fig" rid="F1">Figure 1</xref>). Also, discrimination thresholds were generally higher for glossy objects than matte objects. However, the 2018 paper did not directly address what kind of signals in the proximal images of the objects lead to these different discrimination performances.</p><p id="P4">Thus, the aim of this study was to uncover potential strategies participants took in performing surface color discrimination of complex objects. We tackled this using two distinct computational modeling approaches. For the first set of models, each model uses one of 60 pre-defined chromatic statistics directly computable from the proximal image of a given object and makes discriminations by comparing these statistics across objects. In a second approach, instead of specifying what the model should do, we trained convolutional neural networks (CNNs) on chromatic discrimination tasks using 160,280 images. Importantly, for each CNN images were labeled either by their physical ground-truth labels or by perceptual labels. We formally evaluated the performance of each model (i.e. those based on chromatic statistics, and each of the trained networks) using the adaptive staircase procedure that tested human participants in the previous study. Finally, we used occlusion sensitivity maps to reveal the potential strategies used by the CNNs to solve the discrimination task. Then, guided by this suggested strategy, we developed models that segmented the images before computing specified chromatic statistics.</p></sec><sec id="S2" sec-type="methods"><label>2</label><title>Method</title><sec id="S3"><label>2.1</label><title>Modeled datasets and experimental task from the previous study</title><p id="P5">The psychophysical data modeled in this paper were all collected in a previous study [<xref ref-type="bibr" rid="R1">1</xref>]. Details are provided in the previous study, but here we briefly introduce the experimental design and stimuli. Thresholds were measured along eight hue directions from the equal energy white point, using an adaptive staircase procedure [<xref ref-type="bibr" rid="R14">14</xref>]. Test images were generated by rendering bumpy spheroid stimuli placed under one of three lighting environments (shown in <xref ref-type="fig" rid="F1">Figure 1</xref>, obtained from a publicly available database, <ext-link ext-link-type="uri" xlink:href="http://dativ.at/lightprobes/">http://dativ.at/lightprobes/</ext-link>). The first two lighting environments featured chromatic distributions that broadly corresponded with the daylight locus [<xref ref-type="bibr" rid="R15">15</xref>], while the third environment was created by chromatically inverting the first. Surface roughness of the object was fixed at 0.2, and the object was either matte or glossy (specularity 0.20 in the Ward reflectance model [<xref ref-type="bibr" rid="R16">16</xref>]). All experimental stimuli were generated by a physically based renderer Mitsuba [<xref ref-type="bibr" rid="R17">17</xref>] and RenderToolBox3 [<xref ref-type="bibr" rid="R18">18</xref>]. We performed multispectral rendering from 400 nm to 700 nm with a 10 nm step size. The resultant spectral images were converted to LMS images (corresponding to the signals in the long-, middle- and short-wave sensitive human cones) using Stockman &amp; Sharpe’s cone fundamentals [<xref ref-type="bibr" rid="R19">19</xref>].</p><p id="P6">There were six experimental conditions (two levels of glossiness × three lighting environments). For each trial, we simultaneously presented four objects rendered under the same lighting environment in a square configuration. The three distractor objects were assigned an achromatic surface colour (flat reflectance), while the fourth stimulus – the target – was assigned a reflectance biased in one of the eight directions of hue angle. Participants were asked to select the odd-one-out object in terms of surface color. In every trial the camera angle for each object was randomly assigned from six possible angles (0° to 300° in 60° steps), and we also ensured that within each trial, no two stimuli shared the same camera angle.</p></sec><sec id="S4"><label>2.2</label><title>Computational modeling</title><sec id="S5"><label>2.2.1</label><title>Chromatic statistics models</title><p id="P7">As shown in <xref ref-type="fig" rid="F2">Figure 2 (a)</xref>, we constructed model observers that base their judgment on specified chromatic statistics. We selected 60 candidate statistics that are directly computable either in the MacLeod-Boynton chromaticity diagram [<xref ref-type="bibr" rid="R20">20</xref>] or in <italic>L*a*b*</italic> color space, which are physiology-based or perception-based spaces, respectively. Chromatic coordinates <italic>L</italic>/(<italic>L</italic>+<italic>M</italic>) and <italic>S</italic>/(<italic>L+M</italic>) in the MacLeod-Boynton diagram and photopic luminance are collectively termed MB color space in this study.</p><p id="P8">We used two types of chromatic statistics models. The first models summarize variation of chromaticity over the surface by taking (1) mean chromaticity [<xref ref-type="bibr" rid="R21">21</xref>], (2) mean chromaticity weighted by luminance or lightness [<xref ref-type="bibr" rid="R22">22</xref>] or (3) the color of the brightest pixel [<xref ref-type="bibr" rid="R23">23</xref>] in MB color space or <italic>L*a*b*</italic> color space. Thus, a single chromaticity that consists of two scalar values was taken as an estimate of surface color for a given object. These six models are referred to as 2-D models. The second type of model focuses on a single dimension in a color space, namely (1) <italic>L</italic>/(<italic>L</italic>+<italic>M</italic>), (2) <italic>S</italic>/(<italic>L</italic>+<italic>M</italic>), (3) <italic>a*,</italic> (4) <italic>b</italic>*, (5) <italic>C*<sub>ab</sub>,</italic> (6) saturation (<italic>C*<sub>ab</sub></italic> / <italic>L</italic>*) and summarizes the spatial variations of these by taking (1) mean (2) median, (3) 25th percentile value, (4) 75th percentile value, (5) standard deviation, (6) skewness, (7) kurtosis, (8) minimum value or (9) maximum value. The combination of these (six dimensions × nine statistics) resulted in 54 models which were termed 1-D models in this study. Some of these 1-D statistics have been used as candidate statistics to estimate surface color from natural objects [<xref ref-type="bibr" rid="R11">11</xref>]. To convert XYZ to <italic>L*a*b*,</italic> we defined a white point separately for each lighting environment. We first rendered 12 white matte objects whose diffuse reflectance are 100% across visible wavelengths from 0° to 330° in 30° steps, and then we computed average XYZ values over the object and over 12 camera angles. By defining the white point per environment in this way, the <italic>L</italic>*<italic>a</italic>*<italic>b</italic>* values (and statistics based on them) largely compensate for differences in the global means of the chromatic statistics in the proximal images of the objects. We also note that it is unlikely that participants actually make judgments based only on, for example, <italic>a</italic>* values, but these simulations are intended to test the extent to which such reduced models can perform the discrimination task.</p><p id="P9">In each trial a given model calculated the specified statistic for each of the four stimuli and picked the odd-one-out by selecting the object that has the maximally different chromatic statistic from the other three objects, assessed by a sum of three differences. For the models based on 2-D statistics, we used the 2-D distance on the chromatic plane of MB space or <italic>L*a*b*</italic> space (see lower left plot in <xref ref-type="fig" rid="F2">Figure 2 (a)</xref>). When calculating the distance in MB space, the two axes were scaled by the discrimination thresholds (previously measured using homogenous colored discs) to roughly equate their scales. Threshold estimates from each model were derived using an adaptive staircase to test the modeled ‘observer’ as human observers had previously been tested psychophysically (see below).</p><p id="P10">Previous studies that used computational observer models typically introduced noise in the estimation of the color of the surface or illuminant (e.g. [<xref ref-type="bibr" rid="R24">24</xref>]), but in this study no noise was added to estimation of chromatic statistics for two reasons. First, we generally found that thresholds obtained from these models were larger than thresholds found from humans. Thus, adding noise did not improve the model’s ability to predict human thresholds. Second, we used chromatic statistics derived from different color spaces, and it was difficult to find the equivalent strength of noise across the color spaces, which would likely confound the effectiveness of each chromatic statistic.</p></sec><sec id="S6"><label>2.2.2</label><title>CNN-based models</title><p id="P11">The second set of models we tested were convolutional neural networks (CNNs) as shown in <xref ref-type="fig" rid="F2">Figure 2 (b)</xref>. We selected three widely used CNN architectures for object recognition tasks: AlexNet [<xref ref-type="bibr" rid="R25">25</xref>], ResNet18, and ResNet50 [<xref ref-type="bibr" rid="R26">26</xref>]. Each model was trained in two ways, either with the ground-truth labels (to optimize objective performance of the models), or with human perceptual labels (to train the model to reproduce human-like chromatic thresholds). The second approach relied on data from the previous study in which a large number of images were labeled by the object location that human participants selected during each trial of the experiment. The previous study used an adaptive staircase procedure, requiring 10 to 40 trials for each threshold estimate, with stimulus magnitude typically clustered around the threshold. Putting all repetitions, conditions and participants together, the total number of images presented during the experiment summed to 20,070 images. To increase the training set we applied data augmentation by a factor of eight, based on two assumptions: participants give the identical response even when (1) a given image is horizontally mirrored, and (2) the target object was presented at any of the other three positions. The resulting total of 160,280 images were labeled either by human responses (the location selected by human participants in the previous study) or by physical ground-truth (correct location of target object from 1 to 4). Each pixel stored LMS cone signals instead of RGB values to make sure that networks will receive the same sensory inputs as humans. Each network was trained from scratch using a stochastic gradient descent with momentum algorithm with a fixed learning rate of 0.0003 for 10 epochs by which point performance of the network plateaued. The batch size was 64, and the Deep Learning Toolbox in MATLAB was used.</p></sec></sec><sec id="S7"><label>2.3</label><title>Estimation of chromatic thresholds from models</title><p id="P12">We formally estimated chromatic thresholds from all models using new image datasets in which all stimuli were rendered from different camera angles (30° to 330°, every 60° steps) from the training dataset. Thresholds were estimated using a Palamedes adaptive staircase [<xref ref-type="bibr" rid="R14">14</xref>], starting from large differences in saturation between the target and distractor stimuli, and decreasing the saturation difference if the algorithm correctly identified the odd-one-out. The staircase algorithm increased the target stimulus’ saturation if the algorithm identified an incorrect stimulus. Staircases consisted of a minimum of 10 and maximum of 40 trials to improve reliability, and to match the human paradigm. One session consisted of 8 interleaved staircases to measure thresholds for all hue angles. 20 sessions were carried out, such that 20 thresholds were obtained for each data point.</p></sec></sec><sec id="S8" sec-type="results"><label>3</label><title>Results</title><sec id="S9"><label>3.1</label><title>Model performances</title><p id="P13"><xref ref-type="fig" rid="F3">Figure 3</xref> summarizes the root-mean-square-error (RMSE) between model thresholds and human thresholds (averaged across three participants) for all conditions calculated in scaled MB chromaticity space. On the left-hand plots, we show RMSE for all CNN-based models and for the top-10 chromatic statistics models. An RMSE value of 0 denotes a perfect match to human thresholds, with higher values showing a worse match. On the right-hand plots, we show thresholds of the best CNN models (upper-right) and best chromatic statistics (lower-right) by colored symbols and fitted ellipses, along with human thresholds (black symbols and ellipses). These best models are ringed by red circles on the left plot.</p><p id="P14">Looking across panels, it is notable that ResNet50 (or ResNet 18 for (e)) trained by human response labels shows nearly perfect match to human thresholds in all conditions. In contrast, for the chromatic statistics models, the best performance was derived from different models for different conditions.</p><p id="P15">For environment 1 and matte stimuli (panel (a)), we see that, of the chromatic statistics models, the color of the brightest pixel in <italic>L</italic>*<italic>a</italic>*<italic>b</italic>* color space best predicts human performance. However, there was a significant difference in performance averaged over 20 sessions between this model and ResNet50 (non-paired two-tailed t-test, <italic>t</italic>(38) = 3.20, <italic>p</italic> = 0.0028). The brightest color in MB color space is ranked second. Both of these summary statistics suggest that humans used color at high intensity points on the image of the object’s surface to perform discrimination for matte stimuli. However, these brightest-point-based models are not listed within the top-10 models for the corresponding glossy condition (panel (b)). Instead, the mean chromaticity or weighted mean chromaticity model in <italic>L</italic>*<italic>a</italic>*<italic>b</italic>* color space shows the best agreement with human thresholds. Importantly, the thresholds are still substantially larger than human thresholds and ResNet50.</p><p id="P16">The ranking of chromatic statistics models in environment 2 is very different from environment 1. It is shown that saturation-based methods were ranked first for both matte (panel (c)) and glossy (panel (d)) stimuli. RMSE error for this model is not significantly different from ResNet50 trained on human-labels for the matte condition (non-paired, two-tailed t-test, <italic>t</italic>(38) = 1.54, <italic>p</italic> = 0.132) but is significantly different for the glossy condition (non-paired, two-tailed t-test, <italic>t</italic>(38) = 4.07, <italic>p</italic> = 2.26×10-4).</p><p id="P17">Data from environment 3 show a similar trend to environment 1 where the brightest color in <italic>L</italic>*<italic>a</italic>*<italic>b</italic>* color space is the best predictor for matte stimuli (panel (e)), and for glossy stimuli (panel (f)) the mean chromaticity model in MB color space is the best.</p><p id="P18">CNNs trained on ground-truth labels consistently showed higher RMSE values (with respect to human thresholds) than CNNs trained by human response labels. As shown in <xref ref-type="fig" rid="F4">Figure 4</xref>, this is because these networks can perform the task nearly perfectly, leading to large RMSE values in comparison to the less perfect human thresholds. Since the CNNs were given the correct answer for each trial, it is not surprising that CNNs produce lower discrimination thresholds than CNNs trained by human labels. Importantly, the low thresholds for ground-truth CNNs confirm that the stimuli contain information that allows for near-perfect discrimination, though this information is not available to, or is not correctly utilized by, humans or the chromatic statistics models.</p><p id="P19">In summary, these modeling approaches reveal potential strategies that human participants might have used when discriminating surface colors of complex objects within lighting environments. For matte objects presented under environments 1 and 3, human performance is correlated with the model that bases its judgment on the color of the brightest region of the stimuli. For gloss objects, human performance is closer to that predicted from more global statistics such as mean chromaticity or weighted mean chromaticity, since, for these stimuli, the brightest point corresponds to specular reflection, which does not convey information about the surface color of the object.</p><p id="P20">It is curious that different trends were observed in environment 2, but this could be explained by the relatively small chromatic variation across the proximal image of objects under environment 2. In other words, surface color discrimination in environment 2 was closer to simple saturation discrimination of a uniform surface, which might be why saturation-based metrics were better predictors of the empirical thresholds. To confirm this, we analyzed the standard deviation of L/(L+M) and S/(L+M) values across the proximal images of matte and glossy achromatic objects (same as distractor stimuli) under environment 1 and 2. This calculation was repeated over six different camera angles (from 30 degree to 330 degrees, with 60 steps). For matte objects, the averaged standard deviation of <italic>L</italic>/(<italic>L</italic>+<italic>M</italic>) was significantly lower for the six camera angles in environment 2 than for those in environment 1 (non-paired two-tailed t-test; <italic>t</italic>(10)=14.4, <italic>p</italic> = 5.05×10<sup>-8</sup>), but not for <italic>S</italic>/(<italic>L</italic>+<italic>M</italic>) (non-paired two-tailed t-test; <italic>t</italic>(10)=0.1458, <italic>p</italic> = 0.8870). For glossy objects, the variation in both chromatic axes was significantly lower for environment 2 than for environment 1 (non-paired two-tailed t-test; <italic>t</italic>(10) = 30.9, <italic>p</italic> = 2.95×10<sup>-11</sup> for <italic>L</italic>/(<italic>L</italic>+<italic>M</italic>); <italic>t</italic>(10) = 5.14, <italic>p</italic> = 4.34×10-4 for <italic>S</italic>/(<italic>L</italic>+<italic>M</italic>)). Thus, the image statistics are broadly consistent with an explanation based on chromatic variation. In addition, it is worth noting that, as seen in <xref ref-type="fig" rid="F2">Figure 2</xref> of the 2018 paper, the mean chromaticity of environment 2 is biased away from equal energy white towards bluish-green. However, to compute <italic>L*a*b*</italic> coordinates, we set the white points for each environment based on the mean chromaticity of objects rendered under that environment, and thus we expect that the chromatic bias of environment 2 was largely discounted in the saturation metric, which thus directly provided information about the saturation of the objects without much interference from the color of the lighting environment.</p><p id="P21">For the gloss conditions for environment 1 and 3, we found limited applicability of the implemented chromatic statistics models. Nevertheless, ResNet50 trained on human responses provided good fits to the human psychophysical thresholds for these conditions as well. Thus, we thought that if we could understand the potential strategy of this trained CNN to perform the task, it might give us a direct hint as to the strategy adopted by human participants. This notion encouraged us to analyze the regions of interest for this network trained on human responses.</p></sec><sec id="S10"><label>3.2</label><title>Occlusion sensitivity</title><p id="P22">We used occlusion sensitivity [<xref ref-type="bibr" rid="R27">27</xref>] to reveal regions of the stimuli that were significant to the networks in making their classifications. The basic idea of this technique is to hide a specific region in the image with a square mask and record the change of classification results. This mask is moved across the whole image, and if the area causes a strong change of response, that region can be deemed critical for the discrimination task.</p><p id="P23"><xref ref-type="fig" rid="F5">Figure 5</xref> shows occlusion sensitivity maps derived from ResNet50 trained by the human labels and ground-truth labels, for the glossy conditions in each environment. During the analysis, the images included four objects, arranged in a square configuration, but only the target region is shown here. For a given hue angle, stimulus strength (saturation of the surface reflectance) was fixed at twice the ResNet50 thresholds measured in each condition, and separate strength was used for human ResNet50 and ground-truth ResNet50. The results shown here are for hue value 0 degree but we found similar results for other hue angles. It seems that both CNNs – when classifying correctly – were pulling information from specific regions on the lower part of the target stimulus. The region deemed to be useful by the ground truth-trained models was also typically larger than that of the human-trained models, while still being biased towards lower regions of the stimuli. This pattern was also present in ResNet18 which was also relatively successful in predicting human thresholds. Furthermore, we confirmed that matte objects lead to similar maps.</p><p id="P24">To summarize, these analyses suggest that humans might have extracted information from the lower regions of the stimuli to get reliable estimates of surface colour. This observation encouraged us to create a further image-segmentation model which computes chromatic statistics from a restricted region of a given object image.</p></sec><sec id="S11"><label>3.3</label><title>Image segmentation models</title><p id="P25">Guided by what we learned from occlusion sensitivity analysis of CNNs, we decided to further implement chromatic statistics models that operate only on pre-determined interesting regions of a given object, which we call image segmentation models. The occlusion sensitivity analysis identified lower regions of the objects. The first set of models simply calculate image statistics from the lower p% height (p= 25, 50, and 75) of the object’s region in the proximal image. An additional characteristic of the lower regions of the objects is that they contain fewer specular highlights. To test whether this was the critical feature of the lower parts of the objects in improving performance, we made a second set of models that take image statistics from pixels that have intensity below p% of the highest intensity pixel over the object’s surface (p = 25, 50 and 75). Since the mean chromaticity and weighted mean chromaticity models were the best predictors of human thresholds for glossy conditions in environment 1 and 3 (<xref ref-type="fig" rid="F3">Figure 3</xref>), we implemented only these chromatic statistics for the image segmentation models.</p><p id="P26">We found that models that compute statistics only from the lower 25% region of the objects are indeed better predictors of human behavior. <xref ref-type="fig" rid="F6">Figure 6</xref> is a summary plot showing discrimination thresholds of the best overall models in this study. We found that mean chromaticity or weighted mean chromaticity from the lower 25% of the object region best predicted human thresholds in environments 1 and 3. In <xref ref-type="supplementary-material" rid="SD1">supplementary material</xref>, we report RMSE values for all models implemented in this study.</p><p id="P27">It is interesting that a model that computes statistics from the lower part of the object showed more similar thresholds to those of human observers than a model that uses only dim pixels. Computing the statistics on the dim pixels would remove specular highlights. Similarly, there are fewer specular highlights in the lower parts of the object images. But, computing statistics from lower regions of the object is different from using dim pixels since the lower part is shadowed, meaning that it does not receive light from the brighter regions of the environment maps - i.e. the sky above for environment 1 (and also necessarily environment 3). The main source of within-environment variance that makes the odd-one-out task in this study challenging is the variation introduced from changing camera angle. Thus, the success of models looking at the lower part of the object implies that the lower region of the lighting environments is more ‘rotationally’ symmetric, giving more reliable information to perform the odd-one-out task. To validate this idea, we computed the variation of chromatic statistics across camera angles for the upper part and lower part of the object as summarized in <xref ref-type="fig" rid="F7">Figure 7</xref>. To get these variations, for each condition (e.g. glossy and environment 1), we first computed a lightness-weighted mean chromaticity of a gray object (distractor stimuli) for each of 6 camera angles in <italic>L*a*b*</italic> space (white circles in right inserted plot) and computed the average across 6 values (red dot). Then, we calculated the average deviation between this averaged color and the lightness-weighted mean of each camera angle (average length across 6 black six lines between white circles and red dot), which gives a measure of the variation of lightness-weighted mean across camera angles. As seen, the upper 75% of the object exhibits larger variation across camera angles than the lower 25% region for both matte and glossy conditions in environment 1 and 3. In contrast, in environment 2, variation is nearly the same for upper and lower regions of the object and also for matte and gloss conditions. These are the variation of lightness-weighted mean chromaticity, but we confirmed similar trends for the variation of mean chromaticity. To sum, these analyses support the idea that the lower region of the lighting environments 1 and 3 is more symmetric across camera angles than the upper region. It is important to note that this trend depends on the content of the specific lighting environment, and to conclude how generally this observation holds in the real world would require systematic analysis of other types of lighting environments (e.g. indoor environment or different weather conditions). Furthermore, this analysis confirmed that when the lower 25% of the object exhibits smaller chromatic variation for glossy objects, it does so also for matte objects, which might be a potential reason why occlusion sensitivity maps were similar between matte and glossy objects.</p></sec><sec id="S12"><label>3.4</label><title>Evaluation of CNNs in novel lighting environments</title><p id="P28">One limitation in the CNN implementation so far is that CNNs were trained under only three lighting environments, and in the model evaluation stage, chromatic thresholds were estimated under the environments that the CNNs saw during the training process, even though we did use novel, unseen object images presented from different camera angles. This was a necessary choice because the major purpose in this study was to train CNNs using images labeled by human responses and the number of human responses was limited. Nevertheless, it would be desirable to test how well CNNs generalize to novel lighting environments. To test this idea, we first trained neural networks under two of the three environments, and tested under the third environment. However, under such training, CNNs trained by ground-truth and by human responses showed poor performances, and chromatic thresholds could not be estimated in most conditions. Secondly, we further rendered new datasets under 15 novel lighting environments ([<xref ref-type="bibr" rid="R28">28</xref>, <xref ref-type="bibr" rid="R13">13</xref>]; and <ext-link ext-link-type="uri" xlink:href="https://hdrmaps.com/freebies/">https://hdrmaps.com/freebies/</ext-link>) that cover a diverse range of natural lighting environments. Then, we evaluated the CNNs trained by human responses and ground-truth (ResNet50 presented in the <xref ref-type="sec" rid="S8">Results section</xref>) using the procedure detailed in <xref ref-type="sec" rid="S7">subsection 2.3</xref>. For four environments (labelled A-D) for which it was possible to estimate thresholds, the areas of the resultant discrimination ellipses are summarized in <xref ref-type="fig" rid="F8">Figure 8</xref>. For environments A and B, the human-trained CNN showed better performance (smaller area), showing better generalization to novel environments for both matte and glossy conditions. For the environment C, however, the ground-truth trained CNN showed better performance. For the environment D, the human-trained CNN performed better than the ground-truth CNN for the matte condition, but the trend was reversed for the glossy condition. These results suggest that the level of generalization of CNNs to novel lighting environments depends strongly on the lighting environment, and there are differences in the way that human-trained and ground-truth CNNs generalize to novel environments.</p></sec></sec><sec id="S13"><label>4</label><title>General Discussion</title><p id="P29">The major purpose of this study was to model surface color discrimination of matte and glossy objects within each of three lighting environments. Under these conditions, the rendered objects exhibit large color variations over the proximal image of the surface. Two approaches were taken: the first approach used a variety of chromatic statistics extracted from the object region of the proximal image, and another utilized the recent advances in machine learning algorithms to produce trained CNNs to perform the task. Hand-crafted models provided us with several insights. First, none of the tested chromatic statistics models are individually sufficient to achieve human-like discrimination behavior across all conditions, though heuristic effectiveness is condition-dependent. For example, the brightest model generally predicted human thresholds better than the other models in matte conditions, however it performed much worse than the other models and humans in glossy conditions. This was to be expected, as glossy objects feature specular reflections which are chromatically unmodified representations of the environmental illumination. It follows that under this model bright regions of specular reflection, unrepresentative of object color, were selected for comparison, leading to high discrimination thresholds. Both the mean chromaticity and mean chromaticity weighted by lightness or luminance models were shown to be effective in predicting thresholds in environments 1 and 3. The machine learning approach revealed that CNNs trained by human responses can reproduce human thresholds nearly perfectly whilst CNNs trained by groundtruth labels showed much better performance than humans, leading to high RMSE values. We further used a technique to visualize the region of an object that drives a network to perform the discrimination task and identified that the lower region of the object is key to performing surface color discrimination for both matte and glossy objects. When chromatic statistics models were also restricted to this region of the objects, they more closely predicted human performance.</p><p id="P30">One major bottleneck in hypothesis-based modeling approaches is that researchers have to predict in advance what information could be used by humans. In contrast, behaviors observed in some visual experiments could be complex, as in the case of this study, making it hard to predefine potential strategies. In this regard, a hypothesis-free data-driven approach based on CNNs provides a powerful alternative as CNNs automatically learn to use information useful in reproducing human responses. Because of its computational power, the performance can match human thresholds with minimum error, and reverse engineering of such CNNs could provide insight into underlying computations. CNNs have been used presumably in almost all domains for computer vision by now, but in these engineering applications networks are exclusively trained on ground-truth labels. Recent years have seen increasing use of CNNs in perceptual studies, such as gloss perception [<xref ref-type="bibr" rid="R29">29</xref>–<xref ref-type="bibr" rid="R31">31</xref>], translucency perception [<xref ref-type="bibr" rid="R32">32</xref>], and physical properties of cloth [<xref ref-type="bibr" rid="R33">33</xref>]. To our knowledge, however, this is the first time human color discrimination has been modeled based on CNNs trained on human behavioral patterns. Identifying the surface color of an object requires perceptual separation of contributions from lighting and surface properties, but this is a hard ill-posed problem. Our approaches revealed potential strategies humans might take to separate contributions of lighting and materials from the proximal image of an object in a simplistic way that utilizes the regularities of natural environments. Occlusion sensitivity analysis revealed a stronger influence of the lower part of the object’s surface on classification behavior of the CNN. This might be beneficial for judging glossy objects since specular reflection tends to locate around the upper part of the object’s surface. In addition, such strategy was shown to be effective to perform an odd-one-out task specifically in this study because chromatic variation across camera angles for the lower part of objects is lower than that of the upper part of objects in environments 1 and 3. Past studies have used simplistic stimuli in color discrimination. While these stimuli helped researchers to precisely characterize fundamental abilities to discriminate two homogeneously colored matte surfaces, our findings strongly suggest that the discrimination of real-world objects requires additional strategies to disentangle the contributions of light and material.</p><p id="P31">One caveat of this study is to acknowledge the difficulty in interpreting internal computations in CNNs. Several researchers have discussed the validity of network architecture regression to human cytoarchitecture, such as Hasson et al. [<xref ref-type="bibr" rid="R34">34</xref>], who draw a comparison between macroevolution across generations of animals and backpropagation across trials for machine learning algorithms; with functional adaptations in the former being analogous to optimization in the latter. Kriegseskorte [<xref ref-type="bibr" rid="R35">35</xref>] has pointed out the emergence of simple orientation-based filters in early layers of CNNs, and more complex object-based filters in later layers of networks trained on visual discrimination and classification tasks that are analogous to simple cells early in the visual system and the inferior temporal lobe in the later parts of the ventral stream. This correspondence is particularly pronounced when stimuli and networks are sufficiently complex, as is the case in our task. These proposals seem to suggest that network architecture, when appropriately trained, organizes itself like neural structures, and therefore can be used as an analogy to the brain when investigating human behavior. Saxe et al. [<xref ref-type="bibr" rid="R36">36</xref>] has been skeptical of this, noting the way in which data-driven models can serve to shift the focus of study away from developing specific theories of neural function. Machine learning, they argue, is just one approach in a broader toolkit that psychologists and neuroscientists can use to examine neural functions and should be combined with hypothesis-based testing in humans. Accordingly, while the results of the present study are insufficient to make resolute conclusions about what information humans extract from the proximal image of an object, they provide a foundation for future hypothesis-based testing in humans. The exploration of other techniques to formalize the strategies of CNNs is rapidly expanding, which allows for a further analysis of regions of interest at different layers throughout a network. Future studies using CNNs can benefit from these innovations and may inspire novel hypothesis-based testing.</p><p id="P32">Overfitting of CNNs to the properties of the training dataset is a well known issue. In this study, we used unseen images, from novel camera angles, to test the model performance. However, given the limited availability of human psychophysical responses for training, we were limited to a small number of lighting environments, and it is possible that features of the models were driven by specific features of these environments. The fact that the human-trained and ground-truth trained networks differed in their patterns of generalization, again suggests that they encoded different regularities that drove their responses. Tests of generalization provide a further method by which to uncover the statistical regularities in the training set that are used by human and machine-learning algorithms.</p><p id="P33">This study prompts several future research questions. For example, removing regions of interest in a psychophysical study would allow us to confirm a correspondence between the regions significant to the network and to humans during color discrimination. In such an ‘Image Perturbation’ paradigm [<xref ref-type="bibr" rid="R37">37</xref>, <xref ref-type="bibr" rid="R38">38</xref>], noise is added to regions of interest according to occlusion sensitivity and thresholds are measured in both pre-trained networks and human participants. Observing meaningful deficits in performance, and concordance between human and network performance after perturbation would help confirm that the networks are performing the task using information that is also important to human observers.</p><p id="P34">To summarize, the present study demonstrated that image chromatic statistics, especially computed after image segmentation, could be useful in predicting human color discrimination, although their effectiveness is dependent on the types of surfaces being observed, and the types of environments in which they are seen. This study also showed the effectiveness of deep learning techniques in modeling color vision functions, especially when trained using human responses. Networks trained on ground-truth labels perform better than humans, and this makes them less useful for modeling human behavior. Analysis of the most successful networks identifies image regions that are key to reproducing human discrimination thresholds, which prompts future study to empirically test this dependence via psychophysics.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary material</label><media xlink:href="EMS156653-supplement-Supplementary_material.pdf" mimetype="application" mime-subtype="pdf" id="d15aAdEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S14"><title>Acknowledgments</title><p>TM is supported by a Sir Henry Wellcome Postdoctoral Fellowship from Wellcome Trust (218657/Z/19/Z) and a Junior Research Fellowship from Pembroke College, University of Oxford. SP is supported by an Oxford-MRC Doctoral Training Partnership, Hoare Lea, LLC and Pembroke College, Oxford. For the purpose of open access, the author has applied a CC BY public copyright license to any Author Accepted Manuscript version arising from this submission.</p></ack><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morimoto</surname><given-names>T</given-names></name><name><surname>Smithson</surname><given-names>HE</given-names></name></person-group><article-title>Discrimination of spectral reflectance under environmental illumination</article-title><source>Journal of the Optical Society of America</source><year>2018</year><volume>35</volume><issue>4</issue><fpage>244</fpage><lpage>255</lpage><pub-id pub-id-type="pmcid">PMC5894873</pub-id><pub-id pub-id-type="pmid">29603985</pub-id><pub-id pub-id-type="doi">10.1364/JOSAA.35.00B244</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wright</surname><given-names>WD</given-names></name></person-group><article-title>The sensitivity of the eye to small colour differences</article-title><source>Proceedings of the Physical Society</source><year>1941</year><volume>53</volume><fpage>93</fpage></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacAdam</surname><given-names>DL</given-names></name></person-group><article-title>Visual Sensitivities to Color Differences in Daylight</article-title><source>JOSA</source><year>1942</year><volume>32</volume><issue>5</issue><fpage>247</fpage><lpage>274</lpage></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boynton</surname><given-names>RM</given-names></name><name><surname>Kambe</surname><given-names>N</given-names></name></person-group><article-title>Chromatic Difference Steps of Moderate Size Measured along Theoretically Critical Axes</article-title><source>Color Res Appl</source><year>1980</year><volume>5</volume><fpage>13</fpage><lpage>23</lpage></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gegenfurtner</surname><given-names>KR</given-names></name><name><surname>Kiper</surname><given-names>DC</given-names></name></person-group><article-title>Contrast detection in luminance and chromatic noise</article-title><source>Journal of the Optical Society of America A</source><year>1992</year><volume>9</volume><fpage>1880</fpage><lpage>1888</lpage><pub-id pub-id-type="pmid">1432339</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ennis</surname><given-names>R</given-names></name><name><surname>Toscani</surname><given-names>M</given-names></name><name><surname>Schiller</surname><given-names>F</given-names></name><name><surname>Gegenfurtner</surname><given-names>KR</given-names></name></person-group><article-title>Hyperspectral database of fruits and vegetables</article-title><source>Journal of the Optical Society of America A</source><year>2018</year><volume>35</volume><issue>4</issue><fpage>B256</fpage><lpage>B266</lpage><pub-id pub-id-type="pmid">29603941</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hansen</surname><given-names>T</given-names></name><name><surname>Giesel</surname><given-names>M</given-names></name><name><surname>Gegenfurtner</surname><given-names>KR</given-names></name></person-group><article-title>Chromatic discrimination of natural objects</article-title><source>Journal of vision</source><year>2008</year><volume>8</volume><issue>1</issue><fpage>1</fpage><lpage>19</lpage><pub-id pub-id-type="pmid">18318605</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giesel</surname><given-names>M</given-names></name><name><surname>Hansen</surname><given-names>T</given-names></name><name><surname>Gegenfurtner</surname><given-names>KR</given-names></name></person-group><article-title>The discrimination of chromatic textures</article-title><source>Journal of Vision</source><year>2009</year><volume>9</volume><issue>9</issue><fpage>1</fpage><lpage>28</lpage><comment>11</comment><pub-id pub-id-type="pmid">19761344</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiao</surname><given-names>B</given-names></name><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><article-title>Surface gloss and color perception of 3D objects</article-title><source>Visual Neuroscience</source><year>2008</year><volume>25</volume><fpage>371</fpage><lpage>385</lpage><pub-id pub-id-type="pmcid">PMC2538579</pub-id><pub-id pub-id-type="pmid">18598406</pub-id><pub-id pub-id-type="doi">10.1017/S0952523808080267</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Virtanen</surname><given-names>LS</given-names></name><name><surname>Olkkonen</surname><given-names>M</given-names></name><name><surname>Saarela</surname><given-names>TP</given-names></name></person-group><article-title>Color ensembles: Sampling and averaging spatial hue distributions</article-title><source>Journal of Vision</source><year>2020</year><volume>20</volume><issue>5</issue><fpage>1</fpage><lpage>14</lpage><comment>1</comment><pub-id pub-id-type="pmcid">PMC7409613</pub-id><pub-id pub-id-type="pmid">32392284</pub-id><pub-id pub-id-type="doi">10.1167/jov.20.5.1</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Milojevic</surname><given-names>Z</given-names></name><name><surname>Ennis</surname><given-names>R</given-names></name><name><surname>Toscani</surname><given-names>M</given-names></name><name><surname>Gegenfurtner</surname><given-names>KR</given-names></name></person-group><article-title>Categorizing natural color distributions</article-title><source>Vision Research</source><year>2018</year><volume>151</volume><fpage>18</fpage><lpage>30</lpage><pub-id pub-id-type="pmid">29555302</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>RJ</given-names></name><name><surname>Smithson</surname><given-names>HE</given-names></name></person-group><article-title>Context-dependent judgments of color that might allow color constancy in scenes with multiple regions of illumination</article-title><source>Journal of the Optical Society of America A, Optics, image science, and vision</source><year>2012</year><volume>29</volume><issue>2</issue><fpage>A247</fpage><lpage>A257</lpage><pub-id pub-id-type="pmcid">PMC3287284</pub-id><pub-id pub-id-type="pmid">22330386</pub-id><pub-id pub-id-type="doi">10.1364/JOSAA.29.00A247</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Debevec</surname><given-names>P</given-names></name></person-group><source>Rendering synthetic objects into real scenes: Bridging traditional and image-based graphics with global illumination and high dynamic range photography</source><conf-name>SIGGRAPH98 Proceedings of the 25th Annual Conference on Computer Graphics and Interactive Techniques</conf-name><conf-sponsor>Association for Computing Machinery</conf-sponsor><conf-loc>New York</conf-loc><year>1998</year><fpage>189</fpage><lpage>198</lpage></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prins</surname><given-names>N</given-names></name><name><surname>Kingdom</surname><given-names>FAA</given-names></name></person-group><article-title>Applying the Model-Comparison Approach to Test Specific Research Hypotheses in Psychophysical Research Using the Palamedes Toolbox</article-title><source>Frontiers in psychology</source><year>2018</year><volume>9</volume><elocation-id>1250</elocation-id><comment>F.A.A</comment><pub-id pub-id-type="pmcid">PMC6064978</pub-id><pub-id pub-id-type="pmid">30083122</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2018.01250</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Judd</surname><given-names>DB</given-names></name><name><surname>MacAdam</surname><given-names>DL</given-names></name><name><surname>Wyszecki</surname><given-names>G</given-names></name><name><surname>Budde</surname><given-names>HW</given-names></name><name><surname>Condit</surname><given-names>HR</given-names></name><name><surname>Henderson</surname><given-names>ST</given-names></name><name><surname>Simonds</surname><given-names>JL</given-names></name></person-group><article-title>Spectral distribution of typical daylight as a function of correlated color temperature</article-title><source>J Opt Soc Am</source><year>1964</year><volume>54</volume><issue>8</issue><fpage>1031</fpage><lpage>1040</lpage></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ward</surname><given-names>GJ</given-names></name></person-group><source>Measuring and modeling anisotropic reflection</source><person-group person-group-type="editor"><name><surname>Glassner</surname><given-names>A</given-names></name></person-group><conf-name>SIGGRAPH 92: Proceedings of the 19th Annual Conference on Computer Graphics and Interactive Techniques</conf-name><conf-loc>New York</conf-loc><conf-sponsor>ACM, ACM Press</conf-sponsor><fpage>459</fpage><lpage>472</lpage><year>1992</year></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Jakob</surname><given-names>W</given-names></name></person-group><source>Mitsuba: Physically Based Renderer</source><year>2010</year><comment><ext-link ext-link-type="uri" xlink:href="https://www.mitsuba-renderer.org/download.html">https://www.mitsuba-renderer.org/download.html</ext-link></comment></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heasly</surname><given-names>BS</given-names></name><name><surname>Cottaris</surname><given-names>NP</given-names></name><name><surname>Lichtman</surname><given-names>DP</given-names></name><name><surname>Xiao</surname><given-names>B</given-names></name><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><article-title>RenderToolbox3: MATLAB tools that facilitate physically based stimulus rendering for vision research</article-title><source>Journal of vision</source><year>2014</year><volume>14</volume><issue>2</issue><fpage>6</fpage><pub-id pub-id-type="pmcid">PMC3919102</pub-id><pub-id pub-id-type="pmid">24511145</pub-id><pub-id pub-id-type="doi">10.1167/14.2.6</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stockman</surname><given-names>A</given-names></name><name><surname>Sharpe</surname><given-names>LT</given-names></name></person-group><article-title>The spectral sensitivities of the middle- and long-wavelength-sensitive cones derived from measurements in observers of known genotype</article-title><source>Vision Research</source><year>2000</year><volume>40</volume><issue>13</issue><fpage>1711</fpage><lpage>1737</lpage><pub-id pub-id-type="pmid">10814758</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacLeod</surname><given-names>DIA</given-names></name><name><surname>Boynton</surname><given-names>RM</given-names></name></person-group><article-title>Chromaticity diagram showing cone excitation by stimuli of equal luminance</article-title><source>Journal of the Optical Society of America A</source><year>1979</year><volume>69</volume><issue>8</issue><fpage>1183</fpage><lpage>1186</lpage><pub-id pub-id-type="pmid">490231</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buchsbaum</surname><given-names>G</given-names></name></person-group><article-title>A spatial processor model for object color perception</article-title><source>Journal of the Franklin Institute</source><year>1980</year><volume>310</volume><issue>1</issue><fpage>1</fpage><lpage>26</lpage></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khang</surname><given-names>BG</given-names></name><name><surname>Zaidi</surname><given-names>Q</given-names></name></person-group><article-title>Illuminant color perception of spectrally filtered spotlights</article-title><source>Journal of Vision</source><year>2004</year><volume>4</volume><issue>9</issue><fpage>680</fpage><lpage>692</lpage><pub-id pub-id-type="pmid">15493963</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Land</surname><given-names>EH</given-names></name></person-group><article-title>The retinex theory of color vision</article-title><source>Scientific American</source><year>1977</year><volume>237</volume><issue>6</issue><fpage>108</fpage><lpage>128</lpage><pub-id pub-id-type="pmid">929159</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cottaris</surname><given-names>NP</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name><name><surname>Reike</surname><given-names>F</given-names></name><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><article-title>A computational observer model of spatial contrast sensitivity: Effects of photocurrent encoding, fixational eye movements, and inference engine</article-title><source>J Vis</source><year>2020</year><volume>20</volume><issue>7</issue><fpage>17</fpage><pub-id pub-id-type="pmcid">PMC7424933</pub-id><pub-id pub-id-type="pmid">32692826</pub-id><pub-id pub-id-type="doi">10.1167/jov.20.7.17</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><source>ImageNet classification with deep convolutional neural networks</source><conf-name>Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1 (NIPS’12)</conf-name><conf-sponsor>Curran Associates Inc</conf-sponsor><conf-loc>Red Hook, NY, USA</conf-loc><year>2012</year><fpage>1097</fpage><lpage>1105</lpage></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><source>Deep Residual Learning for Image Recognition</source><conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),2016</conf-name><year>2016</year><fpage>770</fpage><lpage>778</lpage></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zeiler</surname><given-names>MD</given-names></name><name><surname>Fergus</surname><given-names>R</given-names></name></person-group><chapter-title>Visualizing and Understanding Convolutional Networks</chapter-title><person-group person-group-type="editor"><name><surname>Fleet</surname><given-names>D</given-names></name><name><surname>Pajdla</surname><given-names>T</given-names></name><name><surname>Schiele</surname><given-names>B</given-names></name><name><surname>Tuytelaars</surname><given-names>T</given-names></name></person-group><source>Computer Vision – ECCV 2014. ECCV 2014 Lecture Notes in Computer Science</source><publisher-name>Springer</publisher-name><publisher-loc>Cham</publisher-loc><year>2014</year><volume>8689</volume></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adams</surname><given-names>WJ</given-names></name><name><surname>Elder</surname><given-names>JH</given-names></name><name><surname>Graf</surname><given-names>EW</given-names></name><name><surname>Leyland</surname><given-names>J</given-names></name><name><surname>Lugtigheid</surname><given-names>AJ</given-names></name><name><surname>Murry</surname><given-names>A</given-names></name></person-group><article-title>The Southampton-York Natural Scenes (SYNS) dataset: Statistics of surface attitude</article-title><source>Sci Rep</source><year>2016</year><volume>6</volume><elocation-id>35805</elocation-id><pub-id pub-id-type="pmcid">PMC5080654</pub-id><pub-id pub-id-type="pmid">27782103</pub-id><pub-id pub-id-type="doi">10.1038/srep35805</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Storrs</surname><given-names>KR</given-names></name><name><surname>Anderson</surname><given-names>BL</given-names></name><name><surname>Fleming</surname><given-names>RW</given-names></name></person-group><article-title>Unsupervised learning predicts human perception and misperception of gloss</article-title><source>Nat Hum Behav</source><year>2021</year><volume>5</volume><fpage>1402</fpage><lpage>1417</lpage><pub-id pub-id-type="pmcid">PMC8526360</pub-id><pub-id pub-id-type="pmid">33958744</pub-id><pub-id pub-id-type="doi">10.1038/s41562-021-01097-6</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tamura</surname><given-names>H</given-names></name><name><surname>Prokott</surname><given-names>KE</given-names></name><name><surname>Fleming</surname><given-names>RW</given-names></name></person-group><article-title>Distinguishing mirror from glass: A “big data” approach to material perception</article-title><source>Journal of Vision</source><year>2022</year><volume>22</volume><issue>4</issue><fpage>1</fpage><lpage>22</lpage><comment>4</comment><pub-id pub-id-type="pmcid">PMC8934559</pub-id><pub-id pub-id-type="pmid">35266961</pub-id><pub-id pub-id-type="doi">10.1167/jov.22.4.4</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prokott</surname><given-names>KE</given-names></name><name><surname>Tamura</surname><given-names>H</given-names></name><name><surname>Fleming</surname><given-names>RW</given-names></name></person-group><article-title>Gloss perception: Searching for a deep neural network that behaves like humans</article-title><source>Journal of Vision</source><year>2021</year><volume>21</volume><issue>12</issue><fpage>1</fpage><lpage>20</lpage><comment>14</comment><pub-id pub-id-type="pmcid">PMC8626854</pub-id><pub-id pub-id-type="pmid">34817568</pub-id><pub-id pub-id-type="doi">10.1167/jov.21.12.14</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liao</surname><given-names>C</given-names></name><name><surname>Sawayama</surname><given-names>M</given-names></name><name><surname>Xiao</surname><given-names>B</given-names></name></person-group><article-title>Translucency perception emerges in deep generative representations for natural image synthesis</article-title><source>bioRxiv</source><year>2022</year><elocation-id>2022.08.12.503662</elocation-id><pub-id pub-id-type="doi">10.1101/2022.08.12.503662</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bi</surname><given-names>W</given-names></name><name><surname>Jin</surname><given-names>PP</given-names></name><name><surname>Nienborg</surname><given-names>H</given-names></name><name><surname>Bei</surname><given-names>X</given-names></name></person-group><article-title>Estimating mechanical properties of cloth from videos using dense motion trajectories: Human psychophysics and machine learning</article-title><source>J Vis</source><year>2018</year><volume>18</volume><issue>5</issue><elocation-id>12</elocation-id><pub-id pub-id-type="pmid">29904787</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Nastase</surname><given-names>SA</given-names></name><name><surname>Golstein</surname><given-names>A</given-names></name></person-group><article-title>Direct-fit to nature: an evolutionary perspective on biological (and artificial) neural networks</article-title><source>Neuron</source><year>2020</year><volume>105</volume><issue>3</issue><fpage>416</fpage><lpage>434</lpage><pub-id pub-id-type="pmcid">PMC7096172</pub-id><pub-id pub-id-type="pmid">32027833</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2019.12.002</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Deep neural networks: a new framework for modeling biological vision and brain information processing</article-title><source>Annual Review of Vision Science</source><year>2015</year><volume>1</volume><fpage>417</fpage><lpage>446</lpage><pub-id pub-id-type="pmid">28532370</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saxe</surname><given-names>A</given-names></name><name><surname>Nelli</surname><given-names>S</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><article-title>If deep learning is the answer, what is the question?</article-title><source>Nature Reviews Neuroscience</source><year>2021</year><volume>22</volume><fpage>55</fpage><lpage>67</lpage><pub-id pub-id-type="pmid">33199854</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gosselin</surname><given-names>F</given-names></name><name><surname>Schyns</surname><given-names>PG</given-names></name></person-group><article-title>Bubbles: a technique to reveal the use of information in recognition tasks</article-title><source>Vision Research</source><year>2001</year><volume>41</volume><issue>17</issue><fpage>2261</fpage><lpage>2271</lpage><comment>2001</comment><pub-id pub-id-type="pmid">11448718</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Fong</surname><given-names>RC</given-names></name><name><surname>Vedaldi</surname><given-names>A</given-names></name></person-group><source>Interpretable explanations of black boxes by meaning perturbation</source><conf-name>Proceedings of the IEEE International Conference on Computer Vision</conf-name><year>2017</year><fpage>3429</fpage><lpage>3437</lpage></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russakovsky</surname><given-names>O</given-names></name><name><surname>Deng</surname><given-names>J</given-names></name><name><surname>Su</surname><given-names>H</given-names></name><name><surname>Krause</surname><given-names>J</given-names></name><name><surname>Satheesh</surname><given-names>S</given-names></name><name><surname>Ma</surname><given-names>S</given-names></name><name><surname>Huang</surname><given-names>Z</given-names></name><name><surname>Karpathy</surname><given-names>A</given-names></name><name><surname>Khosla</surname><given-names>A</given-names></name><name><surname>Bernstein</surname><given-names>M</given-names></name><name><surname>Berg</surname><given-names>A</given-names></name><etal/></person-group><article-title>ImageNet large scale visual recognition challenge</article-title><source>International Journal of Computer Vision</source><year>2015</year><volume>115</volume><fpage>211</fpage><lpage>252</lpage></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Fig. 1</label><caption><p>Human discrimination thresholds collected in a previous study [<xref ref-type="bibr" rid="R1">1</xref>] for (a) environment 1, (b) environment 2, and (c) environment 3. Left pictures show environmental illumination [<xref ref-type="bibr" rid="R13">13</xref>], and the two plots show discrimination thresholds for matte (left) and glossy (right) objects, averaged across three participants. The small colored dots in the background of each plot show the color distributions of each lighting environment. Environment 3 was generated by inverting the chromatic distribution of environment 1 along the S/(L+M) axis.</p></caption><graphic xlink:href="EMS156653-f001"/></fig><fig id="F2" position="float"><label>Fig. 2</label><caption><title>Two modeling approaches used in this study.</title><p>For (a) chromatic statistics models, we predefined 60 chromatic statistics and for each trial the model extracted the specified statistics from each object (C1 to C4). The model compared the statistics and identified the odd-one-out. In contrast (b) CNN-based models were simply given 160,280 training images labeled either by ground-truth or human response labels. Three network architectures were used, generating six trained CNN models in total.</p></caption><graphic xlink:href="EMS156653-f002"/></fig><fig id="F3" position="float"><label>Fig. 3</label><caption><p>Prediction error (RMSE) between human thresholds and model thresholds for all CNN-based models (light blue and dark blue diamonds in left plot) and for the top-10 chromatic statistics models (orange and red circles) for each condition (a) - (f). Lower RMSE shows better model performance. Estimated discrimination thresholds for the best models (ringed data points) are shown on the right-hand plots.</p></caption><graphic xlink:href="EMS156653-f003"/></fig><fig id="F4" position="float"><label>Fig. 4</label><caption><p>Thresholds derived from ResNet50 trained on ground-truth labels (light blue circles). It is shown that the ground-truth CNN shows much lower thresholds than human psychophysical data (black circles and ellipses).</p></caption><graphic xlink:href="EMS156653-f004"/></fig><fig id="F5" position="float"><label>Fig. 5</label><caption><p>Occlusion sensitivity maps from ResNet50 obtained for objects used in the glossy conditions for different viewpoints under three different lighting environments. For these maps, the hue angle was 0 degrees, and the magnitude of stimuli was fixed at twice the ResNet50 thresholds (separate strength was set for human ResNet50 and ground-truth ResNet50). The input image to the network was four objects located in square configuration (as shown in <xref ref-type="fig" rid="F2">Figure 2</xref>), but only the target region is shown here. For each environment, the leftmost column shows the target object in sRGB format, and the center and rightmost columns show the strength of activation overlaid on the image of the object obtained from ResNet50 trained on human labels and the ground-truth labels, respectively.</p></caption><graphic xlink:href="EMS156653-f005"/></fig><fig id="F6" position="float"><label>Fig. 6</label><caption><p>Overall best models for each condition. Using chromatic statistics computed from lower part of the object yielded the best prediction in (a) environment 1 and (c) environment 3 while a saturation metric (C*ab / L*) yielded the best prediction in (b) environment 2, in which chromatic variation around the white point introduced by the lighting environment was smaller than in the other two environments.</p></caption><graphic xlink:href="EMS156653-f006"/></fig><fig id="F7" position="float"><label>Fig. 7</label><caption><p>Variation of lightness-weighted mean chromaticity over 6 camera angles for the upper 75% region of the object and for the lower 25 % region of the object. For each condition (e.g. glossy, environment 1 and upper 75%), we computed a lightness-weighted mean color across 6 camera angles, and then calculated the average color difference between this mean color and the mean at each camera angle.</p></caption><graphic xlink:href="EMS156653-f007"/></fig><fig id="F8" position="float"><label>Fig. 8</label><caption><p>Performance of the human-trained and ground-truth trained CNNs when tested in 15 novel lighting environments. Thresholds could be estimated in 4 environments (A-D), for which the areas of the discrimination ellipses are plotted; in the remaining 11 environments discrimination thresholds fell beyond the gamut defined by realizable natural reflectance samples (shown by red line). Light blue data points show human-trained CNNs, and dark blue data points show ground-truth CNNs. Error bars show ± 1.0 S.E. across 20 sessions. For comparison, average ellipse areas estimated from ResNet18 trained and tested under the same lighting environments are plotted as dotted lines (light blue for human labels and dark blue for ground-truth labels, with the associated shaded regions showing ± 1.0 S.E. across 6 conditions (2 gloss levels × 3 environments)).</p></caption><graphic xlink:href="EMS156653-f008"/></fig></floats-group></article>