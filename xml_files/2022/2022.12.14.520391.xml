<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS158907</article-id><article-id pub-id-type="doi">10.1101/2022.12.14.520391</article-id><article-id pub-id-type="archive">PPR585462</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">3</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A spatial code for temporal cues is necessary for sensory learning</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Bagur</surname><given-names>Sophie</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Bourg</surname><given-names>Jacques</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Kempf</surname><given-names>Alexandre</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Tarpin</surname><given-names>Thibault</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Bergaoui</surname><given-names>Khalil</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Guo</surname><given-names>Yin</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Ceballo</surname><given-names>Sebastian</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Schwenkgrub</surname><given-names>Joanna</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Verdier</surname><given-names>Antonin</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Puel</surname><given-names>Jean Luc</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Bourien</surname><given-names>Jérôme</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Bathellier</surname><given-names>Brice</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>Institut Pasteur, Université Paris-Cité, INSERM, Institut de l’Audition, 63 rue de Charenton, F-75012 Paris, France</aff><aff id="A2"><label>2</label>Institut des Neurosciences de Montpellier, Université de Montpellier, INSERM, Montpellier, France</aff><author-notes><corresp id="CR1">
<label>*</label>Corresponding authors : <email>brice.bathellier@pasteur.fr</email>, <email>sophie.bagur@pasteur.fr</email>
</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>23</day><month>12</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>15</day><month>12</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">The temporal structure of sensory inputs contains essential information for their interpretation by the brain<sup><xref ref-type="bibr" rid="R1">1</xref>–<xref ref-type="bibr" rid="R9">9</xref></sup>. Sensory systems represent these temporal cues through two codes: the temporal sequences of neuronal activity and the spatial patterns of neuronal firing rate<sup><xref ref-type="bibr" rid="R3">3</xref>,<xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R10">10</xref>–<xref ref-type="bibr" rid="R20">20</xref></sup>. However, it is still unknown which of these two coexisting codes causally drives sensory decisions<sup><xref ref-type="bibr" rid="R3">3</xref>,<xref ref-type="bibr" rid="R10">10</xref>,<xref ref-type="bibr" rid="R20">20</xref>,<xref ref-type="bibr" rid="R21">21</xref></sup>. To separate their contributions, we designed an optogenetic stimulation paradigm in the mouse auditory cortex to generate neuronal activity patterns differing exclusively along their temporal or spatial dimensions. Training mice to discriminate these patterns shows that they efficiently learn to discriminate spatial but not temporal patterns, indicating that spatial representations are necessary for sensory learning. In line with this result, we observed, based on large-scale neuronal recordings of the auditory system, that the auditory cortex is the first region in which spatial patterns efficiently represent temporal auditory cues varying over several hundred milliseconds. This feature is shared by the deep layers of neural networks trained to categorise time-varying sounds. Therefore, the emergence of a spatial code for temporal sensory cues is a necessary condition to associate temporally structured stimuli to decisions. We expect this constraint to be crucial for re-engineering perception by cortical stimulation.</p></abstract></article-meta></front><body><p id="P2">Many stimuli which drive selective behavioural decisions, such as phonemes and vocalisations<sup><xref ref-type="bibr" rid="R5">5</xref>,<xref ref-type="bibr" rid="R10">10</xref></sup>, tactile textures and shapes<sup><xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R20">20</xref></sup> or the coherent motion of a moving animal<sup><xref ref-type="bibr" rid="R4">4</xref></sup> result from temporally evolving sensory inputs. In the brain, this temporal structure is associated with changes both in when neurons fire and which neurons fire. On the one hand, temporal stimuli drive neuronal activity sequences, observed throughout the visual<sup><xref ref-type="bibr" rid="R3">3</xref>,<xref ref-type="bibr" rid="R11">11</xref></sup>, auditory<sup><xref ref-type="bibr" rid="R12">12</xref>,<xref ref-type="bibr" rid="R13">13</xref>,<xref ref-type="bibr" rid="R22">22</xref></sup>, tactile<sup><xref ref-type="bibr" rid="R14">14</xref>,<xref ref-type="bibr" rid="R15">15</xref></sup> and olfactory<sup><xref ref-type="bibr" rid="R6">6</xref>,<xref ref-type="bibr" rid="R16">16</xref></sup> systems, including sensory cortex. In single cortical neurons, these activity sequences carry information which is not available in the neuron’s mean firing rate<sup><xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R11">11</xref>,<xref ref-type="bibr" rid="R13">13</xref>,<xref ref-type="bibr" rid="R15">15</xref>,<xref ref-type="bibr" rid="R16">16</xref></sup>. On the other hand, several studies have established that the time-averaged firing rate of many neurons is tuned to specific temporal cues, such as the speed or direction of motion in visual stimuli<sup><xref ref-type="bibr" rid="R9">9</xref>,<xref ref-type="bibr" rid="R23">23</xref></sup>, the dynamics of tactile contacts<sup><xref ref-type="bibr" rid="R14">14</xref>,<xref ref-type="bibr" rid="R19">19</xref></sup> or amplitude and frequency modulations in sounds<sup><xref ref-type="bibr" rid="R17">17</xref>,<xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R22">22</xref>,<xref ref-type="bibr" rid="R24">24</xref>,<xref ref-type="bibr" rid="R25">25</xref></sup>. This tuning generates a spatial representation of the temporal structure of sensory inputs<sup><xref ref-type="bibr" rid="R10">10</xref>,<xref ref-type="bibr" rid="R20">20</xref></sup> based on the identity of the set of activated neurons. Although these spatial representations do not necessarily form anatomical maps and can be widely distributed across a sensory area, they constitute a code for temporal sensory cues that depend on the neuronal space rather than on time. The functional importance of these spatial and temporal codes for behaviour is a long standing issue in sensory neuroscience given that they coexist at all levels of sensory systems including the cortex<sup><xref ref-type="bibr" rid="R10">10</xref>,<xref ref-type="bibr" rid="R11">11</xref>,<xref ref-type="bibr" rid="R15">15</xref>–<xref ref-type="bibr" rid="R17">17</xref>,<xref ref-type="bibr" rid="R19">19</xref>,<xref ref-type="bibr" rid="R20">20</xref>,<xref ref-type="bibr" rid="R22">22</xref>,<xref ref-type="bibr" rid="R26">26</xref></sup>. Therefore, direct manipulation of neural activity patterns in space and time is necessary to evaluate which code is actually deciphered by downstream areas to drive perceptual decisions and behavioural output. While activation of spatial patterns in sensory cortex has been shown to drive discriminative behaviour<sup><xref ref-type="bibr" rid="R27">27</xref>–<xref ref-type="bibr" rid="R30">30</xref></sup>, the role of temporal cues has so far only been causally addressed at the level of sensory receptor neurons<sup><xref ref-type="bibr" rid="R26">26</xref>,<xref ref-type="bibr" rid="R31">31</xref></sup> or of peripheral sensory networks<sup><xref ref-type="bibr" rid="R32">32</xref></sup>. It is therefore unknown whether, at the cortical stage, temporal codes are exploited by the downstream motor centres within which associations between stimuli and behavioural decisions are learnt<sup><xref ref-type="bibr" rid="R33">33</xref></sup>.</p><sec id="S1"><title>Engineering spatial and temporal codes</title><p id="P3">To address this question, we engineered optogenetically-driven activity patterns in the auditory cortex (AC) that can be distinguished either specifically from their spatial structure or from their temporal structure. We focused on the auditory system because natural sounds contain rich temporal information<sup><xref ref-type="bibr" rid="R2">2</xref></sup> that is at the basis of speech recognition<sup><xref ref-type="bibr" rid="R5">5</xref></sup> and influence several perceptual properties important for sound identification and characterization such as timber or loudness<sup><xref ref-type="bibr" rid="R1">1</xref>,<xref ref-type="bibr" rid="R34">34</xref></sup>. We used Emx1-Cre x flex-ChR2 mice expressing channelrhodopsin in a large population of pyramidal neurons and optogenetically activated a low (A spot) and a high (B spot) frequency region of primary AC using light spots delivered with a video-projector through a chronic cranial window<sup><xref ref-type="bibr" rid="R27">27</xref>,<xref ref-type="bibr" rid="R29">29</xref></sup> (<xref ref-type="fig" rid="F1">Fig. 1a</xref>, <xref ref-type="fig" rid="F5">Extended Data Fig. 1a-d</xref>). To generate spatially distinct but temporally identical neural patterns, we stimulated either the A or B spot with the same train of ten pulses at 20 Hz (<xref ref-type="fig" rid="F1">Fig. 1b</xref>). Generating spatially identical but temporally distinct patterns is more challenging due to two confounding phenomena. First, cortical neurons adapt their firing rate to stimulation frequency<sup><xref ref-type="bibr" rid="R35">35</xref></sup> and adaptation varies across neurons. This leads to different firing rates for different neurons, thus generating spurious spatial patterns. We therefore excluded temporal stimulations differing by stimulation frequency and focused on temporal cues based on the relative timing of the stimulation of the two spots<sup><xref ref-type="bibr" rid="R36">36</xref></sup> (<xref ref-type="fig" rid="F1">Fig. 1b</xref>). However, based on earlier observations<sup><xref ref-type="bibr" rid="R26">26</xref>,<xref ref-type="bibr" rid="R31">31</xref>,<xref ref-type="bibr" rid="R37">37</xref></sup> and neuronal simulations (<xref ref-type="fig" rid="F6">Extended Data Fig. 2a-c</xref>), we reasoned that a recurrent circuit, like the cortex, can convert relative timing cues in the stimulation into spatial patterns in the neuronal population (<xref ref-type="fig" rid="F1">Fig. 1c</xref>), which would prevent us from generating purely temporal activity patterns in the network. To quantify these effects, we used silicon probes in awake mice to record the light-evoked activity during optogenetic stimulation of the A and B spots, while shifting the delay between each spot stimulation (<xref ref-type="fig" rid="F1">Fig. 1d-f</xref>). Stimulation of A or B alone elevated the time-averaged firing rate in different sets of neurons (<xref ref-type="fig" rid="F1">Fig. 1e</xref>), hence producing distinct spatial patterns. Optogenetically driven firing rates were in the same range as those naturally evoked by sound presentation (<xref ref-type="fig" rid="F6">Extended Data Fig. 2d</xref>). Stimulating the A and B spots successively in the two possible orders (A-B or B-A) elevated time-averaged firing rates similarly, suggesting much weaker spatial information (<xref ref-type="fig" rid="F1">Fig. 1e</xref>). To verify this, we used a population decoder measuring if it is possible to discriminate on a trial-by-trial basis between A-B and B-A sequences only based on the time-averaged firing rate of recorded neurons. We found that this spatial decoder could discriminate temporal stimulation order above chance if the interval between A and B stimulations was lower or equal to 13 ms, but could not for an interval of 25 ms (<xref ref-type="fig" rid="F1">Fig. 1f</xref>). Hence, consistent with synaptic interactions integrating over the short membrane time constant of cortical neurons <italic>in vivo</italic><sup><xref ref-type="bibr" rid="R38">38</xref>,<xref ref-type="bibr" rid="R39">39</xref></sup>, only stimulus sequences separated by a sufficiently long time interval generate cortical network patterns that contain robust temporal information (quantified in <xref ref-type="fig" rid="F6">Extended Data Fig. 2e</xref>) but lack spatial information (<xref ref-type="fig" rid="F1">Fig. 1f</xref>). We therefore selected the successive activations of A and B separated by 25ms and in opposite orders as a protocol to assess discriminability of purely temporal patterns in a behavioural task.</p></sec><sec id="S2"><title>A spatial code is necessary for learning</title><p id="P4">Mice were trained to discriminate the spatial (A vs B) and the temporal (A-B vs B-A) pairs of patterns in two Go/Nogo tasks. The tasks consisted in licking within a 1.5s opportunity window after the Go pattern onset to get a reward provided by medial forebrain bundle stimulation, which leads to identical learning rates as water rewards<sup><xref ref-type="bibr" rid="R40">40</xref></sup>. Licking for the NoGo pattern was punished by a timeout (<xref ref-type="fig" rid="F1">Fig. 1g</xref>). Mice were trained on both tasks, counterbalancing task order. Consistent with previous results<sup><xref ref-type="bibr" rid="R29">29</xref></sup>, mice rapidly learned to discriminate in the spatial task, reaching 70% accuracy within 69+/-385 trials (<xref ref-type="fig" rid="F1">Fig. 1g-i</xref>, <xref ref-type="fig" rid="F5">Extended Data Fig. 1b</xref>). By contrast, learning was extremely slow and inefficient for the temporal task. After 3000 training trials, none of the mice could discriminate the temporal patterns, while most of them could discriminate the spatial patterns (<xref ref-type="fig" rid="F1">Fig. 1g-i</xref>). Pushing training to even higher trial numbers, only two of seven mice reached slightly above chance levels for the temporal task (<xref ref-type="fig" rid="F5">Extended Data Fig. 1e</xref>). This shows that activity patterns of cortical activity which contain temporal but no spatial information are hard to access for downstream learning processes for sensory-motor associations, while spatial patterns are easily exploited.</p></sec><sec id="S3"><title>Cortex elaborates a spatial code</title><p id="P5">We reasoned that this important constraint on AC output activity must have consequences for sound encoding across the auditory system. Many sounds differ only by temporal cues. For example a word and its time-reversed rendition are perceptually distinct in humans<sup><xref ref-type="bibr" rid="R5">5</xref></sup>. Rodents, including mice, are also able to discriminate between two sounds which mirror each other in time, an ability that depends on AC<sup><xref ref-type="bibr" rid="R41">41</xref>,<xref ref-type="bibr" rid="R42">42</xref></sup>. We therefore hypothesised that the inefficiency of circuits downstream of the cortex in learning only from temporal patterns constrains the auditory system to re-encode auditory temporal cues as spatial activity patterns. To test this hypothesis, we performed large-scale recordings in the awake mouse in three successive regions of the auditory system: the inferior colliculus (IC), the auditory thalamus (TH) and the AC (<xref ref-type="fig" rid="F2">Fig. 2a</xref>, <xref ref-type="table" rid="T1">Extended Data Table 1</xref>), combined with auditory nerve (AN) responses simulated with a detailed biophysical model. In each region, we measured the responses to a set of 140 sounds, mainly of 500 ms duration, which covered simple, spectral and temporal features (<xref ref-type="fig" rid="F2">Fig. 2b</xref>, <xref ref-type="table" rid="T2">Extended Data Table 2</xref>). In the IC, we used silicon probe electrophysiology to record 563 single units in the primary IC (central nucleus of IC) and 2-photon calcium imaging to record 13.132 ROIs from the more superficial secondary IC (dorsal cortex of IC). We also imaged 39.191 TH axonal boutons spread throughout AC and recorded 498 single units directly in TH. Finally, we imaged 60.822 ROIS throughout all subregions of the AC down to layer V. Calcium signals were linearly deconvolved <sup><xref ref-type="bibr" rid="R24">24</xref></sup>, providing a temporal resolution of ∼150 ms sufficient to follow slow temporal patterns produced by our 500ms sounds. Full details of the dataset are provided in the <xref ref-type="supplementary-material" rid="SD1">Supplementary Information</xref> and <xref ref-type="fig" rid="F7">Extended Data Figs. 3</xref> and <xref ref-type="fig" rid="F8">4</xref>.</p><p id="P6">Contrasting neural responses to a frequency sweep and its time-reversed rendition (<xref ref-type="fig" rid="F2">Fig. 2c</xref>) provides qualitative insight into the transformation of temporal cue representations that we observed throughout the auditory system. In the IC, the two sounds are represented by spatio-temporal activity patterns that involve the same neurons and mirror each other in time (<xref ref-type="fig" rid="F2">Fig. 2c</xref>). In the AC, the temporal symmetry is no longer apparent and each sound is instead encoded by spatio-temporal activity patterns involving different neurons. This suggests that in AC but not in IC, sounds that differ only temporally are encoded by activity patterns that also differ in the spatial domain (<xref ref-type="fig" rid="F2">Fig. 2c</xref>). This would make temporal information necessary in IC but dispensable in AC to discriminate between these sounds. We quantified this using two types of population activity decoders. To exhaustively extract the information contained in activity patterns, we used a spatio-temporal decoder that classifies sound responses with the full temporal sequence of population vectors (<xref ref-type="fig" rid="F2">Fig. 2d</xref>). To extract information contained exclusively in spatial patterns, we used a spatial decoder that classifies sound responses with the population vectors obtained after time-averaging neuronal activity over the sound response (<xref ref-type="fig" rid="F2">Fig. 2d</xref>). In subcortical areas, the spatio-temporal decoder clearly outperformed the spatial decoder. By contrast, in the cortex, the spatio-temporal and spatial decoder accuracies reached almost the same level (<xref ref-type="fig" rid="F2">Fig. 2e-f</xref>). This result holds when the numbers of neurons are matched across datasets (<xref ref-type="fig" rid="F9">Extended Data Fig 5a</xref>). Improved spatial decoding accuracy in AC could either result from a change of the representation as suggested in <xref ref-type="fig" rid="F2">Fig. 2c</xref>, or from a change in the signal-to-noise ratio which varies across datasets (<xref ref-type="fig" rid="F9">Extended Data Fig. 5b</xref>). To rule out the latter possibility, we quantified the similarity between population vectors evoked by a given sound pair. We used a numerically and analytically validated noise-corrected version of the Pearson correlation<sup><xref ref-type="bibr" rid="R43">43</xref></sup> (<xref ref-type="fig" rid="F9">Extended Data Fig. 5c</xref>, see <xref ref-type="supplementary-material" rid="SD1">Supplementary Information</xref> for mathematical derivations) to estimate similarity across representations in absence of noise.</p><p id="P7">Resulting representational similarity analysis (RSA) matrices summarise the relations between sound representations based either on the spatial or on the spatio-temporal information (<xref ref-type="fig" rid="F2">Fig. 2g</xref>, <xref ref-type="fig" rid="F10">Extended Data Fig. 6a</xref>). We first observed that, subcortically, the mean similarity between representations of different sounds (mean of RSA matrix) is higher for the spatio-temporal than for the spatial code, indicating that temporal patterns help segregating sounds in distinct representations. However, this difference is very small in AC contrary to subcortical structures (<xref ref-type="fig" rid="F2">Fig. 2h,i</xref>). In addition RSA matrices for spatial and spatio-temporal representations were very similar in AC, while they were much dissimilar subcortically (<xref ref-type="fig" rid="F2">Fig. 2j</xref>). These results hold in all subfields of AC (<xref ref-type="fig" rid="F2">Fig. 2h</xref>) and are robust to the number of neurons included in the analysis (<xref ref-type="fig" rid="F9">Extended Data Fig. 5d</xref>). This together demonstrates that spatio-temporal and spatial representations of sounds in AC are very similar, explaining why decoders perform almost equally with both representations. Together, these results show that time-varying sounds are accurately represented by purely spatial patterns in the AC but not in subcortical structures. Interestingly, this transformation makes temporal sensory information available to learning mechanisms requiring a spatial representation, reconciling our optogenetic results with the ability to discriminate temporal cues in sounds.</p></sec><sec id="S4"><title>Spatial and temporal codes co-exist</title><p id="P8">We then tested if the convergence between spatio-temporal and spatial representations in AC is the consequence of a decrease in temporal resolution in the cortex<sup><xref ref-type="bibr" rid="R22">22</xref>,<xref ref-type="bibr" rid="R44">44</xref></sup> or occurs without loss of temporal information. We decomposed neural population activity using Fourier analysis and measured classifier accuracy at each specific timescale (<xref ref-type="fig" rid="F2">Fig. 2k</xref>, <xref ref-type="fig" rid="F9">Extended Data Fig. 5e</xref>). This analysis showed that relevant temporal resolution is preserved on the time scales considered in our study. In all datasets, classifier accuracy is maximal at ∼1.5 Hz resolution (<xref ref-type="fig" rid="F9">Extended Data Fig. 5e</xref>). Thus all datasets contain temporally structured neural activity that is sufficient to identify the relevant temporal cues in our sounds. Moreover, accumulating information from low to high temporal resolutions shows a saturation of classifier accuracy at around 3 Hz for all datasets (<xref ref-type="fig" rid="F2">Fig. 2k</xref>). Therefore all temporal information needed to discriminate our sounds is available below 3 Hz, which is much lower than the putative 30 Hz cutoff for temporal resolution in AC <sup><xref ref-type="bibr" rid="R22">22</xref></sup>. We observed sound-related information at fast timescales, in particular in electrophysiological recordings (<xref ref-type="fig" rid="F9">Extended Data Fig. 5e</xref>) but it was redundant to information at slower time scales (<xref ref-type="fig" rid="F2">Fig. 2k</xref>). Corroborating the spatial and spatio-temporal decoders (<xref ref-type="fig" rid="F2">Fig. 2e,f</xref>), the time-averaged activity of neurons (0 Hz) reached a level similar to that of the full cumulative spatio-temporal information in AC but not subcortically (<xref ref-type="fig" rid="F2">Fig. 2k</xref>). Therefore, in AC, almost all information, including that present in neural temporal patterns, is also accessible from the identity of active neurons, i.e. from a purely spatial code. Accordingly, each sound is represented by small sets of highly active, and highly specific neurons in AC as shown by the high population and lifetime sparseness (<xref ref-type="fig" rid="F9">Extended Data Fig. 5f,g</xref>)<sup><xref ref-type="bibr" rid="R45">45</xref></sup>. Notably, this property evolved non-monotonically along the auditory system, with much less sparse representations in TH (<xref ref-type="fig" rid="F9">Extended Data Fig. 5f,g</xref>), paralleling the increase in representation similarity in this area (<xref ref-type="fig" rid="F2">Fig. 2h</xref>). In contrast to sparseness measures, the level of tuning to simple, individual temporal features (e.g. frequency modulation direction) was stable from IC to AC (<xref ref-type="fig" rid="F11">Extended Data Fig. 7</xref>), consistent with previous reports<sup><xref ref-type="bibr" rid="R46">46</xref>,<xref ref-type="bibr" rid="R47">47</xref></sup>. This suggests that the cortical transformation of sound representations does not correspond to a sharpening of particular tuning properties but to the emergence of more complex tuning properties<sup><xref ref-type="bibr" rid="R48">48</xref></sup>.</p></sec><sec id="S5"><title>The spatial code sets learning speed</title><p id="P9">We next investigated in a computational model if known learning mechanisms downstream of cortex can capture the advantages of the cortical elaboration of a spatial code for temporal information. We used a feedforward neural network model which simulates discrimination learning in an auditory Go/NoGo task based on reinforcement learning principles<sup><xref ref-type="bibr" rid="R49">49</xref></sup> (<xref ref-type="fig" rid="F3">Fig. 3a</xref>). We upgraded this model by complementing its Hebbian synaptic learning rule with an eligibility trace mechanism<sup><xref ref-type="bibr" rid="R50">50</xref>,<xref ref-type="bibr" rid="R51">51</xref></sup> parameterized with data from the striatum<sup><xref ref-type="bibr" rid="R52">52</xref></sup>, a structure receiving AC projections and implicated in sound discrimination learning<sup><xref ref-type="bibr" rid="R33">33</xref></sup>. The eligibility trace flags active synapses with a signal that decays over ∼1-3s. This mechanism allows even delayed post-synaptic activity driven by the reward to gate plasticity based on presynaptic input (<xref ref-type="fig" rid="F3">Fig. 3a</xref>). However, when associated with Hebbian plasticity, the long decay of the eligibility trace averages out the precise timing of pre- and postsynaptic activity coincidences. Thus, when we trained the model to discriminate between the population responses to pairs of sounds taken from the AC, TH or IC datasets, we observed that the model learned from the spatial patterns but ignored the temporal patterns. This can be evidenced by plotting learning duration as a function of the noise-corrected similarity (correlation in RSA matrix) between discriminated representations, which shows that learning duration is much more correlated to the spatial than to the spatio-temporal similarity (<xref ref-type="fig" rid="F3">Fig. 3b</xref>). Moreover, learning duration rises in a steep and non-linear manner for high spatial representation similarity (<xref ref-type="fig" rid="F3">Fig. 3c</xref>). This non-linear relationship matches the result of our optogenetic experiments (<xref ref-type="fig" rid="F1">Fig. 1</xref>), in which temporal sequences with maximal spatial representation similarity yielded extremely slow learning. The model also provides an explanation for the long standing observation that discrimination of pure tones does not require AC and can be performed via subcortical sensory-motor projections, whereas discrimination of temporal cues requires AC<sup><xref ref-type="bibr" rid="R29">29</xref>,<xref ref-type="bibr" rid="R41">41</xref>,<xref ref-type="bibr" rid="R42">42</xref>,<xref ref-type="bibr" rid="R53">53</xref></sup> (<xref ref-type="fig" rid="F3">Fig. 3d</xref>). Simple sound pairs, such as pure tones differing enough in frequency (e.g. &gt; 0.33 octave), have low spatial representation similarity at all stages of the auditory (e.g. correlation &lt; 0.75, <xref ref-type="fig" rid="F3">Fig. 3e</xref>, <xref ref-type="fig" rid="F10">Extended Data Fig. 6b-d</xref>). For this range of low correlation values, our model shows that learning occurs quickly and the impact of representation similarity on learning speed is marginal (<xref ref-type="fig" rid="F3">Fig. 3c</xref>). Hence, the model predicts similar learning speeds whether it is based on thalamic or cortical representations (<xref ref-type="fig" rid="F3">Fig. 3e</xref>), as observed for pure tone discriminations with intact or ablated AC<sup><xref ref-type="bibr" rid="R41">41</xref></sup>. Contrariwise, sounds that differ only in their temporal structure, such as time-symmetric frequency modulations, have spatial representations that are highly correlated subcortically (&gt;0.9, <xref ref-type="fig" rid="F3">Fig. 3f</xref>) and clearly less in the cortex (0.74, <xref ref-type="fig" rid="F3">Fig. 3f</xref>, similar results for other temporal cues, <xref ref-type="fig" rid="F10">Extended Data Fig. 6e-g</xref>). Based on these values, our model predicts a ∼3-fold decrease in learning duration with cortical compared to thalamic representations (<xref ref-type="fig" rid="F3">Fig. 3f</xref>). This is in line with the observation that pre-training AC ablation severely prolongs discrimination learning for time-reversed frequency sweeps<sup><xref ref-type="bibr" rid="R41">41</xref></sup>. Also, if one postulates that learning speed determines which auditory system stage is recruited for solving a sound discrimination task, the relationship between spatial representation and learning duration in our model can explain the strong impact of post-training AC inactivation for discrimination of temporal cues, but not for pure tones<sup><xref ref-type="bibr" rid="R29">29</xref>,<xref ref-type="bibr" rid="R42">42</xref>,<xref ref-type="bibr" rid="R53">53</xref></sup>. This together indicates that the properties of learning rules in the striatum can quantitatively explain the necessity of an upstream spatial code for sensory-motor learning and recapitulate effects of causal manipulations of AC on auditory discrimination learning.</p></sec><sec id="S6"><title>Spatial codes emerge from categorisation</title><p id="P10">The emergence of a spatial code for temporal information in the cortex may more generally reflect computations related to the resolution of common perceptual tasks such as stimulus identification and categorization. To explore this theoretically, we analysed representations in convolutional neural networks (CNNs) trained for different sound processing tasks (<xref ref-type="fig" rid="F4">Fig. 4</xref>). A first network was trained at categorising key features of the stimuli presented to our mice: the frequency and intensity range, and the type of frequency and amplitude modulations present in the sounds (<xref ref-type="fig" rid="F4">Fig. 4a</xref>, <xref ref-type="fig" rid="F12">Extended Data Fig. 8a</xref>). This network generated a spatial code for temporal cues in its deep layers after training, as shown by the convergence of spatial and spatio-temporal similarity (<xref ref-type="fig" rid="F4">Fig. 4a</xref>, <xref ref-type="fig" rid="F12">Extended Data Fig. 8b</xref>). Like typical CNNs, our network implemented pooling mechanisms which increase the size of sensory receptive fields and shrink the temporal and spatial dimensions in deeper layers<sup><xref ref-type="bibr" rid="R54">54</xref></sup>. To rule out that the convergence between spatio-temporal and spatial codes is due to this temporal shrinking, we trained a second CNN without pooling over the temporal dimension (<xref ref-type="fig" rid="F4">Fig. 4b</xref>). We observed that temporal shrinking accelerates learning but is not necessary for the emergence of a spatial code for temporal cues in the CNNs (<xref ref-type="fig" rid="F4">Fig. 4b</xref>, <xref ref-type="fig" rid="F12">Extended Data Fig. 8a</xref>). This network displayed properties similar to the auditory system (compare <xref ref-type="fig" rid="F4">Fig. 4c,d</xref> and <xref ref-type="fig" rid="F2">Fig. 2f,j</xref>) and in particular the spatial code for temporal cues in deep CNN layers did not involve a decreased temporal resolution of the representation (compare <xref ref-type="fig" rid="F4">Fig. 4e</xref>, <xref ref-type="fig" rid="F12">Extended Data Fig. 8c</xref> and <xref ref-type="fig" rid="F2">Fig. 2k</xref>, <xref ref-type="fig" rid="F9">Extended Data Fig. 5e</xref>). These results extended to a previously published CNN trained to classify words and musical styles<sup><xref ref-type="bibr" rid="R55">55</xref></sup> (<xref ref-type="fig" rid="F12">Extended DataFig. 8d</xref>) and to networks trained to perform single sound identification in noise (<xref ref-type="fig" rid="F12">Extended DataFig. 8e</xref>). However, when we trained another CNN network, with an auto-encoder architecture, to compress and denoise sound representations without assigning specific labels to sounds, we did not observe the emergence of a spatial code for temporal cues (<xref ref-type="fig" rid="F4">Fig. 4f</xref>, <xref ref-type="fig" rid="F12">Extended Data Fig. 8f,g</xref>). Therefore, CNN models support the view that the emergence of spatial representations for temporal cues in the cortex is driven by the computational constraints of classifying sounds into perceptual objects assigned with meaning.</p></sec><sec id="S7" sec-type="discussion"><title>Discussion</title><p id="P11">Our results highlight the spatial encoding of temporal sound cues as an important function of the sensory cortical network. This is in line with the proposed role of AC in the encoding of auditory objects<sup><xref ref-type="bibr" rid="R56">56</xref></sup> such as phonemes, vocalisations or musical notes and previous observations of spatial representations for speech or natural sounds in the AC of humans and animals<sup><xref ref-type="bibr" rid="R10">10</xref>,<xref ref-type="bibr" rid="R55">55</xref>,<xref ref-type="bibr" rid="R57">57</xref></sup>. Our results demonstrate that this spatial encoding is necessary for rapid learning of sensory-motor associations. Based on a reduced model of sensory discrimination learning, we propose that this constraint arises from time-averaging properties of plasticity rules implemented in associative centres such as the striatum or the amygdala. These brain regions link information from a wide range of sensory areas to behavioural responses and environmental outcomes that have their own, unrelated temporal structure. Our results suggest that the challenge of associating the distinct temporalities of sensory signals and motor responses is resolved via the use of spatial representations for temporal cues.</p><p id="P12">Several computational models<sup><xref ref-type="bibr" rid="R58">58</xref>–<xref ref-type="bibr" rid="R60">60</xref></sup> and experimental findings<sup><xref ref-type="bibr" rid="R61">61</xref>,<xref ref-type="bibr" rid="R62">62</xref></sup> have identified plasticity mechanisms by which temporal sequences can be associated to a specific neuronal output, by combining spike-timing-dependent synaptic plasticity (STDP)<sup><xref ref-type="bibr" rid="R63">63</xref></sup> and neuronal integration mechanisms. Several factors could explain why such mechanisms are not efficiently recruited in auditory sensory-motor learning. First, many auditory objects, like our optogenetic stimuli, evolve on timescales of hundreds of milliseconds that are not suited for the short timescales of STDP<sup><xref ref-type="bibr" rid="R64">64</xref></sup>. Second, different models indicate that under irregular spike train statistics as observed <italic>in vivo</italic>, STDP rules behave as standard Hebbian rules<sup><xref ref-type="bibr" rid="R60">60</xref>,<xref ref-type="bibr" rid="R65">65</xref>,<xref ref-type="bibr" rid="R66">66</xref></sup>. While temporally precise sound responses in AC were often reported under anaesthesia, more irregular spike trains are observed in the awake state<sup><xref ref-type="bibr" rid="R67">67</xref></sup>. Likewise, our mild optogenetic stimulations calibrated to yield realistic firing rates (<xref ref-type="fig" rid="F5">Extended Data Fig. 1d</xref>) are likely too weak to overcome cortical noise<sup><xref ref-type="bibr" rid="R39">39</xref></sup> and generate high temporal precision (<xref ref-type="fig" rid="F1">Fig. 1e</xref>). Finally, eligibility traces gate plasticity based on neuromodulatory feedback which, in the case of striatal dopamine, can occur up to 3s after synaptic activity. This slow timescale of integration averages out the precise timing of pre- and postsynaptic activity coincidences and could explain why fine temporal information is inaccessible to plasticity mechanisms that are driven by delayed environmental feedback.</p><p id="P13">A previous study showed that rats can discriminate short time intervals between two electrical stimulations at two different AC locations<sup><xref ref-type="bibr" rid="R36">36</xref></sup>. We expect these results to reflect both the acute precision of electrical stimulation and the conversion of fast temporal information into a spatial code via synaptic interactions in the cortical networks (<xref ref-type="fig" rid="F6">Extended Data Fig. 2</xref>), as we observed when decreasing the time interval in our paradigm (<xref ref-type="fig" rid="F1">Fig. 1f</xref>). Network interactions in AC may underlie the transfer of temporal information received from thalamus into the spatial domain (<xref ref-type="fig" rid="F2">Fig. 2</xref>). CNNs are also based on local computations that iteratively detect spectrotemporal features and recapitulate the emergence of the spatial code (<xref ref-type="fig" rid="F4">Fig. 4</xref>). Hence, simple local computations whose implementation in the local circuit motifs of the auditory system remain to be defined, are sufficient to make temporal information accessible through a spatial code in AC. Our study also revealed two intriguing aspects of sound information processing in the auditory system. First, contrary to what is observed in CNNs (<xref ref-type="fig" rid="F4">Fig. 4</xref>), representations from IC to AC are transformed non-monotonically with denser, more correlated representations in TH compared to AC and IC (<xref ref-type="fig" rid="F2">Fig. 2</xref>). Second, it is remarkable that neural temporal information is largely preserved in the AC despite the accessibility of temporal information through a spatial code (<xref ref-type="fig" rid="F2">Fig. 2</xref>). This coexistence of temporal and spatial coding schemes could serve to combine object-like representations, useful for categorical decisions, with an explicit representation of the temporal details that are also perceived together with the object.</p></sec><sec sec-type="methods" specific-use="web-only" id="S8"><title>Methods</title><sec id="S9"><title>Subjects and authorizations</title><p id="P14">All mice used for imaging and electrophysiology were 6 to 14 weeks old male and female C57Bl6J mice that had not undergone any other procedures. For optogenetic stimulation, we used Emx1-IRES-Cre (Jax #005628) crossed with Ai27 (Jax #012567) mice. Mice were group-housed (2–6 per cage) before and after surgery, had <italic>ad libitum</italic> access to food and water and enrichment (running wheel, cotton bedding and wooden logs) and were maintained on a 12-hour light-dark cycle in controlled humidity and temperature conditions (21-23°C, 45-55% humidity). All experiments were performed during the light phase. All experimental and surgical procedures were carried out in accordance with the French Ethical Committee the French Ethical Committees #59 and #89 (authorizations APAFIS#9714-2018011108392486 v2 and APAFIS#27040-2020090316536717 v1).</p></sec><sec id="S10"><title>Surgery</title><p id="P15">Mice were injected with buprenorphine (Vétergesic, 0,05-0,1 mg/kg) 30 min prior to surgery. Surgical procedures were carried out using either intraperitoneal ketamine (Ketasol) and medetomidine (Domitor) which was antagonised with atipamezole (Antisedan, Orion pharma) at the end of the surgery) or 3% isoflurane delivered via a mask. After induction, mice were kept on a thermal blanket during the whole procedure and their eyes were protected with Ocrygel (TVM Lab). Lidocaine was injected under the skin of the skull 5 minutes prior to incision.</p><p id="P16">For calcium imaging, craniotomies of 3 (IC) or 5 (AC) mm were performed above the IC or the AC. Injections of 150nL of AAV1.Syn.GCaMP6s.WPRE (Vector Core, Philadelphia, PA; 10^13 viral particles per ml; used pure for TH and diluted 30x for AC and IC) were made at 30 nL/min with pulled glass pipettes at a depth of 500µm and spaced every 500 µm to cover the a large surface of the IC or AC. The craniotomy was sealed with a circular glass coverslip. The coverslip and head post were fixed to the skull using cyanolit glue and dental cement (Ortho-Jet, Lang).</p><p id="P17">For electrophysiology recordings, the skull above the IC or above the cortex dorsal to the TH was exposed for ulterior craniotomy. A well was formed around it using dental cement in order to retain saline solution during recordings and the head post was fixed to the skull using cyanolit glue and dental cement. To protect the skull, the well was filled with a waterproof silicone elastomer (Kwikcast, WPI) that could be removed prior to recording. The head post was fixed to the skull using cyanolit glue and dental cement (Ortho-Jet, Lang).</p><p id="P18">For patterned optogenetic stimulation of the cortex, a cranial window was placed above the AC as for calcium imaging but without viral injection. For MFB stimulation, a bipolar stimulation electrode (60-µm-diameter twisted stainless steel, PlasticsOne) was implanted using stereotaxic coordinates (AP -1.4, ML +1.2, DV +4.8). It was then fixed along with the headplate to the skull using dental cement (Ortho-Jet, Lang).</p><p id="P19">After surgery, mice received a subcutaneous injection of 30% glucose and metacam (1 mg/kg). Mice were subsequently housed for one week with metacam delivered via drinking water or dietgel (ClearH20). Mice were given one week to recover from surgery without any manipulation. Then, for four days before recording, mice were habituated to head restraint for increasing periods of time (30 min - 2 hours). For electrophysiological experiments, the day before recording animals were briefly anaesthetised using isoflurane anaesthesia (2%) in order to perform craniotomy and durectomy for electrode descent.</p></sec><sec id="S11"><title>Two photon calcium imaging in the awake mouse</title><p id="P20">Imaging was performed using a two-photon microscope (Femtonics, Budapest, Hungary) equipped with an 8kHz resonant scanner combined with a pulsed laser (MaiTai-DS, SpectraPhysics, Santa Clara, CA) set at 900 nm. We used a 10x Olympus objective (XLPLN10XSVMP), which provided a field of view of up to 1×1 mm. For AC, a 1×1mm field of view was used. For IC, the field of view was adjusted to the size of the structure (∼0.5×0.5 mm). For thalamic axons, the field of view was reduced to 0.22×0.22 mm. Images were acquired at 31.5 Hz.</p></sec><sec id="S12"><title>Electrophysiology in the awake mouse</title><p id="P21">Electrophysiology was performed using Neuronexus probes : (1×32 linear probe for IC and 4*8 comb for TH). For track reconstruction, the electrodes were dipped in diI, diO or diD (Vybrant™ Multicolor Cell-Labelling Kit, Thermofisher) prior to recording and allowed to dry at least 15 min before insertion. Recordings were performed using warmed saline filling the cyanolit glue well and in contact with the reference electrode. After each recording the well was amply flushed and then refilled with Kwickast. A maximum of three recordings were performed per site. Data was sampled at 20kHz using an Intan RHD2000 amplifier board.</p><p id="P22">For recordings during optogenetic stimulation, a small hole was drilled in the coverglass and a 1×64 linear probe was inserted into the stimulated region. During these recordings, optogenetic stimuli and sounds were presented randomly. Recordings were performed using warmed saline filling the cyanolit glue well and with a reference electrode chronically implanted into the brain. After each recording the well was amply flushed and then refilled with Kwickast.</p></sec><sec id="S13"><title>Sound delivery</title><p id="P23">Sounds were generated with Matlab (The Mathworks, Natick, MA) and were delivered at 192 kHz with a NI-PCI-6221 card (National Instruments) driven by the software Elphy (G. Sadoc, UNIC, France) and feeding an amplified free-field loudspeaker (SA1 and MF1-S, Tucker-Davis Technologies, Alachua, FL) positioned 15 to 20 cm from the mouse ear. Sound intensity was cosine-ramped over 10 ms at the onset and offset to avoid spectral splatter. The head fixed mouse was isolated from external noise sources by sound-proof boxes (custom-made by Femtonics, Budapest, Hungary or Decibel France, Miribel, France) providing 30 dB attenuation above 1 kHz. Sounds were calibrated in intensity at the location of the mouse ear using a probe microphone (Bruel &amp; Kjaer, type 4939-L-002). For two-photon calcium imaging, the resonant scanner generated a harmonic background noise at 8 kHz (intensity at the mouse ear, 45 dB SPL).</p><p id="P24">During a recording session, each of the 140 sounds (sketched in <xref ref-type="fig" rid="F2">Fig. 2b</xref>) was presented 15 times in random order. In order to be compatible with 2-photon image acquisition, sounds were presented in 120 blocks of 32s each, interleaved by a 15s pause in a 94 min protocol. The list of all sound parameters can be found in <xref ref-type="table" rid="T2">Extended Data Table 2</xref>.</p></sec><sec id="S14"><title>Intrinsic optical imaging recordings in anaesthetised mice</title><p id="P25">Intrinsic imaging was performed to localise AC in mice under light isoflurane anaesthesia (1% delivered with SomnoSuite, Kent Scientific) on a thermal blanket. Images were acquired at 20Hz using a 50mm objective (1.2 NA, NIKKOR, Nikon) with a CCDcamera (GC651MP, Smartek Vision) equipped with a 50 mm objective (Fujinon, HF50HA-1B, Fujifilm) through the cranial window implanted 1-2 weeks before the experiment (4-pixel binning, field of view between 3.7 x 2.8 mm or 164 x 124 pixels at 5.58 mm/pixel). Signals were obtained under 780 nm LED illumination (M780D2, Thorlabs). Images of the vasculature over the same field of view were taken under 530 nm LED illumination (NSPG310B, Conrad). Sequences of short pure tones at 80 dB SPL were repeated for 2 s every 30 s with 10 trials per sound. Acquisition was triggered and synchronised using a custom-made GUI in MATLAB. For each sound, we computed baseline and response images, 3 s before and 3 s after sound onset, respectively. The change in light reflectance ΔR/R was calculated for each repetition of each sound frequency (4, 8, 16, 32 kHz, white noise) as the difference between the baseline and response image and was then averaged across all repetitions of a given tone frequency. Response images were smoothed applying a 2D Gaussian filter (sd = 3 pixels). Auditory cortex activity appeared as regions with reduced light reflectance changing with frequency, revealing the tonotopic maps of its different subfields. To align intrinsic imaging responses across different animals, the 4 kHz response was used as a functional landmark. The spatial locations of maximal amplitude responses in the 4 kHz response map for the A1, A2 and AAF (three points) was extracted for each mouse and a Euclidean transformation matrix was calculated by minimising the sum of squared deviations (RMSD) for the distance between the three landmarks across mice. This procedure yielded a matrix of rotation and translation for each mouse that was applied to compute intrinsic imaging responses averaged across a population of mice.</p></sec><sec id="S15"><title>Histology and immunostainings</title><p id="P26">In order to extract the brain for histology, mice were deeply anaesthetised using a ketamine- medetomidine mixture and perfused intracardially with 4% buffered paraformaldehyde fixative. The brains were dissected and left in paraformaldehyde overnight and then sliced into fifty micrometre sections using a vibratome. Slices were either stained with cytochrome oxidase or directly mounted using a mounting medium with DAPI. Analysis of the fluorescence band diI, diO or diD allowed isolating up to 3 tracks per mouse for electrophysiological experiments.</p><p id="P27">For Vglut2 immunostainings, after fixation, tissues were rinsed in PBS and blocked in Tris-Buffered Saline (TBS) supplemented with 5 % (vol/vol) Normal Donkey Serum (Jackson Immunoresearch) and 0.3 % (wg/vol) Triton X-100. Then, sections were incubated for 48h at 4°C while rocking with a primary antibody: guinea pig anti-Vglut2 (1:500, Synaptic Systems #135404), followed by a 4 h incubation with a secondary donkey anti-guinea pig IgG [F(ab’)2fragments] (1:500, Jackson ImmunoResearch #706606148). Tissues were rinsed and mounted using Prolong diamond antifade (Life Technologies). Pictures of the brain sections were taken with LSM 900 confocal microscope (Zeiss Microsystems) using 20x objective, whereas the magnified view of the thalamocortical boutons was obtained with Airyscan acquisition and 63x objective.</p><p id="P28">The labelled boutons (GCaMP alone in green; GCaMP with Vglut2 in yellow) were counted manually using ZEISS ZEN 2 microscope software in 12 sample regions selected within layer 1 AC in 3 different Airyscan images. The number of boutons was then calculated per volume tissue.</p></sec><sec id="S16"><title>Behavioural discrimination of patterned optogenetic stimuli</title><p id="P29">For patterned optogenetic activation in the mouse AC, we used a video projector (DLP LightCrafter, Texas Instruments) powered by a blue LED (centre wavelength 460 nm). To project a two-dimensional image onto the AC surface. The image of the micromirror chip was collimated through a 150 mm cylindrical lens (Thorlabs, diameter: 2 inches) and focused through a 50 mm objective (NIKKOR, Nikon). Light collected by the objective passes through a dichroic beam splitter (long pass, &gt; 640nm, FF640-FDi01, Semrock) and is collected by a CCD camera (GC651MP, Smartek Vision) equipped with a 50 mm objective (Fujinon, HF50HA-1B, Fujifilm).</p><p id="P30">The behavioural task aimed to teach mice to discriminate between two optogenetically induced patterns of activity in AC. The reinforcement used for the task used medial forebrain bundle (MFB) stimulation in non-deprived mice. This protocol leads to similar learning speed, motor response timing and psychometric measurements as water rewards in deprived animals<sup><xref ref-type="bibr" rid="R40">40</xref></sup>. In the “spatial discrimination task”, the two stimuli were composed of 500 ms illumination of 300 µm diameter spots placed at two different locations of AC. In the “temporal discrimination task”, the two stimuli were composed of a succession of two 250 ms illuminations of 300 µm diameter spots at different locations in the cortex in one order (AB) or in the reversed order (BA). All light stimuli were temporally modulated at 20 Hz (25 ms ON, 25 ms OFF). To prevent visual perception of the optogenetic stimuli a constant and strong background illumination provided by a white LED lamp was used and a cache was placed in front and close to the eyes to limit visual inputs. Mice were trained on both tasks. 4 mice were first trained on the temporal, and then on the spatial task. 3 mice were first trained on the spatial and then on the temporal task. The spots used in the task they first learnt were positioned at the two extremes of the tonotopic axis of A1. In order to minimise interference between the two subsequent tasks, the spots in the second task were positioned at two different locations, on the axis orthogonal to the tonotopic axis of A1 keeping the inter-spot distance equal. In both cases, spot position was adjusted to avoid placing them above major vessels which could lead to reduced illumination of neurons (<xref ref-type="fig" rid="F5">Extended Data Fig. 1a,b</xref>). Alignment of optogenetic stimulus locations across days was done using blood vessel patterns at the surface of the brain manually aligned to a reference blood vessel image taken at the beginning of the experiment.</p><p id="P31">Behavioural experiments were monitored and controlled using a custom Matlab software controlling an input-output board (PCIe-6351, National Instruments) and the images delivered by the video projector. Mice performed behaviour for one hour per day. During the entire behavioural training period, food and water were available <italic>ad libitum</italic> as rewards were provided through the stimulation of the medial forebrain bundle (MFB).</p><p id="P32">MFB stimulation was delivered via a pulse train generator (PulsePal V2, Sanworks) that produced 2ms biphasic pulses at 50Hz for 100ms at a voltage calibrated for each individual mouse to the minimal level that evoked sustained responding, using the protocol in <sup><xref ref-type="bibr" rid="R40">40</xref></sup>. The stimulation was controlled with a solenoid valve (LVM10R1-6B-1-Q, SMC). A voltage of 5V was applied through an electric circuit joining the lick tube and an aluminium foil on which the mouse was sitting. Lick events could be monitored by measuring the voltage across a series resistor in this circuit.</p><p id="P33">Training was broken down into three phases. (i) Lick training: On the first day, mice were presented with the lick tube and any licking was rewarded with immediate MFB stimulation. Mice generally began licking at high rates after 1-2 minutes and the session was continued until mice reliably collected around 300 rewards. (ii) Go training: On the following day, Go trials were presented with 80% probability, while the remaining trials were blank trials (no stimulus). A trial consisted of a random inter-trial interval (ITI : 0.5 to 1 s), a random ‘no lick’ period (duration adjusted, see below) and a fixed response window of 1.5 s. The first lick occuring during the response window on a Go trial was scored as a ‘hit’ and triggered immediate MFB stimulation. During initial go training the ‘no lick’ period was between 2 and 5 s in order to discourage non-specific licking. When mice achieved &gt;80% accuracy for the Go stimulus, a final Go session was performed during which a cache was placed over the window to verify that animals were not licking to remnant visual cues from the video projector (<xref ref-type="fig" rid="F5">Extended Data Fig. 1c,d</xref>). On this day and for subsequent Go/NoGo sessions, the no lick period was shortened to 1.5 to 3 s in order to obtain more trials per session. (iii) Go/NoGo training: After Go training, the second stimulus (NoGo) was introduced. During presentation of the NoGo sound, the absence of licking for the full response window was scored as a ‘correct rejection’ (CR) and the next trial immediately followed. Any licking during NoGo trials was scored as a ‘false alarm’ (FA), no stimulation was given, and the animal was punished with a random time-out period between 5 and 7 s. Each session contained 45% Go stimuli, 45% NoGo stimuli and 10% blank stimuli. Note that the Go training was used to ensure high motivation of the animal during the Go/Nogo training by establishing an association between the optogenetic stimulus and the reward. For the time-independent task, this association was generalised to the NoGo stimulus, as seen through very high false alarm rates at the beginning of the Go/NoGo training (e.g. <xref ref-type="fig" rid="F5">Extended Data Fig. 1b</xref>). This indicates that faster learning for the time-independent task is not due to an absence of generalisation between the Go and NoGo stimulus when transitioning from the Go to the Go/NoGo training phases.</p><p id="P34">Learning curves were obtained by calculating the fraction of correct responses over blocks of 150 trials. Discrimination performance over one session was calculated as (hits + correct rejections)/total trials.</p></sec><sec id="S17"><title>Data pre-processing</title><p id="P35">For calcium imaging, regions of interest corresponding to putative neurons (AC and IC) or axons and boutons (TH) were identified by using Autocell <sup><xref ref-type="bibr" rid="R24">24</xref></sup> (<ext-link ext-link-type="uri" xlink:href="https://github.com/thomasdeneux/Autocell">https://github.com/thomasdeneux/Autocell</ext-link>). Briefly, each frame of the recording was corrected for horizontal motion using rigid body registration.This step was visually controlled and all sessions with visible z motion were eliminated. A hierarchical clustering algorithm, based on pixel covariance over time, agglomerated pixels up to a user-selected number of clusters corresponding to regions of the size of neurons of axons. Clusters were automatically filtered according to size and shape criteria. This step was controlled by a detailed visual inspection of selected regions of interest (ROIs) during which ROIs without visually identifiable cell body shape were discarded.</p><p id="P36">For each region of interest, the mean fluorescence signal F(t) was extracted together with the local neuropil signal F<sub>np</sub>(t). Then 70% of the neuropil signal was subtracted from the neuron signal to limit neuropil contamination. Baseline fluorescence F<sub>0</sub> was calculated with a sliding window computing the 3rd percentile of a Gaussian-filtered trace over the imaging blocks. Fluorescence variations were then computed as f(t) = ΔF/F = (F(t) - F<sub>0</sub>)/F<sub>0</sub>. An estimate of firing rate variations r(t) was then obtained by linear temporal deconvolution of f(t): r(t) = f’(t) + f(t)/τ, f’(t) being the first derivative of f(t) and τ = 2s, the estimated decay of the GCAMP6s fluorescent transients. This simple method efficiently corrects the strong discrepancy between fluorescence and firing rate time courses due to the slow decay of spike-triggered calcium transients. It does not correct for the rise time of GCAMP6s, leading to remnant low pass filtering of the firing rate estimate and a delay of ∼100ms between the firing rate peaks and the peaks of the deconvolved signal. Finally, response traces were smoothed with a Gaussian filter (σ = 31ms).</p><p id="P37">Electrophysiological signals were high-pass filtered and spike sorting was performed using the CortexLab suite (<ext-link ext-link-type="uri" xlink:href="https://github.com/cortex-lab">https://github.com/cortex-lab</ext-link>, UCL, London, England). Single unit clusters were identified using kilosort 2.5 followed by manual corrections based on the interspike-interval histogram and the inspection of the spike waveform using Phy (<ext-link ext-link-type="uri" xlink:href="https://github.com/cortex-lab/phy">https://github.com/cortex-lab/phy</ext-link>).</p><p id="P38">Both for imaging and electrophysiology data, single trial sound responses were extracted (0.5s before and 1s after sound onset) and the average activity over the prestimulus period (0.5s - 0s before sound onset) was subtracted for each trial.</p></sec><sec id="S18"><title>Reproducibility index and cell selection</title><p id="P39">To quantify the noise levels in the data, we calculated the mean inter-trial correlation across all pairs of trials. The single neuron reproducibility is then defined for each neuron as the average of the inter-trial correlation for that neuron’s response to all 140 sounds. The population response reproducibility for each sound is defined as the average of the inter-trial correlations of the full sequence of response of the whole neural population to that sound. Region of interests (ROIs) or single units with reproducibility below 0.12 were classified as non-responsive and were excluded from all analyses except population sparseness. As detailed in the <xref ref-type="table" rid="T1">Extended Data Table 1</xref>, the number of responsive units and the corresponding fraction of the total number of units/ROIs recorded are: AC, 19414 (32%), TH, 3969 (12%), THE, 484 (97%), 5936 (39%), 442 (78%).</p></sec><sec id="S19"><title>Noise-corrected correlation</title><p id="P40">For each dataset, population representations were estimated after pooling all recording sessions in a virtual population. We used the correlation between population vectors as a metric of similarity between representations. The areas and techniques used to estimate neuronal ensemble representations yielded different levels of trial-to-trial variability due to intrinsic neuronal response variability and measurement noise. Most representation metrics are biassed by variability, even after trial averaging, due to variability residues. For example, the correlation between two population representations (population vectors) will tend to decrease with respect to a variability-free estimate <sup><xref ref-type="bibr" rid="R43">43</xref></sup>. When multiple observations of the same representations are available, it is possible to account for the impact of variability, by using specific estimators <sup><xref ref-type="bibr" rid="R43">43</xref></sup>. Here we showed analytically (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Information</xref>) that the value of the Pearson correlation coefficient <inline-formula><mml:math id="M1"><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>ν</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mi>s</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>ν</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> between population vectors for two sounds <inline-formula><mml:math id="M2"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>ν</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M3"><mml:msub><mml:mrow><mml:mover><mml:mi>ν</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:math></inline-formula> in absence of variability can be exactly estimated from noise-corrupted single-trial observations <inline-formula><mml:math id="M4"><mml:msub><mml:mrow><mml:mover><mml:mi>ν</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="M5"><mml:msub><mml:mrow><mml:mover><mml:mi>ν</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub></mml:math></inline-formula> of <inline-formula><mml:math id="M6"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>ν</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M7"><mml:msub><mml:mrow><mml:mover><mml:mi>ν</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:math></inline-formula> when their dimension N approaches infinity, based on the formula: <disp-formula id="FD1"><mml:math id="M8"><mml:mstyle displaystyle="true"><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>ν</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mi>s</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>ν</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mfrac><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>ν</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>ν</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:msqrt><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>R</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo></mml:mrow><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>≠</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>ν</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>ν</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo></mml:mrow><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>≠</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>ν</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>ν</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msqrt></mml:mfrac></mml:mstyle></mml:math></disp-formula> in which r and r’ are single trial indices and R is the total number of trials. This analytical result is confirmed by simulations for finite N, indicating that our estimator converges to the correlation value of the noise-free vectors (<xref ref-type="fig" rid="F9">Extended Data Fig. 5c</xref>). Code for calculating this estimator is provided with the online data set.</p><p id="P41">Simulations for finite N show as expected that the estimator displays substantial deviations around the true correlation which however average to zero. This leads to values of the estimator that can be outside [-1,1] in some cases. Our estimator displays extremely large deviations when <inline-formula><mml:math id="M9"><mml:mstyle displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>≠</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>ν</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>ν</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> approaches 0, i.e. for representations that are dominated by noise. This occurred more often in datasets obtained by imaging, in particular in the thalamic axonal boutons dataset (TH). To limit imprecisions from these extreme values we excluded from all datasets sounds for which <inline-formula><mml:math id="M10"><mml:mstyle displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>≠</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>ν</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:math></inline-formula> In typical neural data, there are significant noise correlations across simultaneously recorded neurons within a trial. Therefore, the effective N can be much lower than the number of neurons. We minimised this contribution by shuffling trial identity for each neuron independently.</p><p id="P42">To evaluate the significance of mean correlation differences across all sound pairs for temporal and rate representations, we used a bootstrap procedure over the independently recorded sessions. This procedure had the advantage of providing a statistical assessment for biological replicability based on strictly independent measurements (neurons of the same recording are not fully independent statistically). The noise-corrected correlation measure was estimated 100 times after a random resampling of sessions with replacement. Based on this distribution, we measured the standard deviation and calculated p-values down to 0.01.</p><p id="P43">Sequence correlation was measured on vectors formed by concatenating the responses of all neurons throughout time (vector dimension = N<sub>Neurons</sub> x N<sub>TimeBins</sub>). Rate correlation was measured first by time-averaging the responses of each neuron and then concatenating these values for all neurons (vector dimension = N<sub>Neurons</sub>). In both cases, we used data from the sound onset to 250ms after the sound offset. To normalise the difference between temporal and rate correlation when comparing between areas we use the formula : <disp-formula id="FD2"><mml:math id="M11"><mml:mstyle displaystyle="true"><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>a</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>a</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:math></disp-formula></p></sec><sec id="S20"><title>Noise-corrected sparseness measure</title><p id="P44">There exist several sparseness measures which are all biassed by variability in neuronal activity measurements <sup><xref ref-type="bibr" rid="R45">45</xref>,<xref ref-type="bibr" rid="R68">68</xref>–<xref ref-type="bibr" rid="R70">70</xref></sup>. The most classical measure as defined in <sup><xref ref-type="bibr" rid="R68">68</xref></sup> is not appropriate for baseline-corrected, linearly deconvolved calcium data because it requires positive response values. We show in the <xref ref-type="supplementary-material" rid="SD1">Supplementary Information</xref> that kurtosis, the 4th order moment of a distribution, is a sparseness measure which can be corrected for variability-related biases and is appropriate for all our datasets. This metric quantifies the “long-tailedness” of the distribution. Sparse response properties correspond to rare and strong responses which generate long-tailed response distributions as opposed to dense response properties which correspond to more compact response distributions. For lifetime sparseness, measured for each neuron separately, Kurtosis is defined as: <disp-formula id="FD3"><mml:math id="M12"><mml:mstyle displaystyle="true"><mml:msub><mml:mi>K</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>&lt;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo>&gt;</mml:mo><mml:mi>s</mml:mi></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>4</mml:mn></mml:msup><mml:msub><mml:mo>&gt;</mml:mo><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo>&gt;</mml:mo><mml:mi>s</mml:mi></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mo>&gt;</mml:mo><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mstyle></mml:math></disp-formula> in which &lt;&gt;<sub>s</sub> indicates averaging over sounds and <italic>ν</italic><sub><italic>n</italic>,<italic>s</italic></sub> is the noise-free response of neuron <italic>n</italic> to sound <italic>s</italic>. In the case of population sparseness, which is measured for each sound separately, &lt;&gt;<sub>s</sub> should be replaced by &lt;&gt;<sub>n</sub> which indicates averaging over neurons. The Kurtosis formula can be developed into the moments of order 1 to 4 of <italic>ν</italic><sub><italic>n</italic>,<italic>s</italic></sub>. <disp-formula id="FD4"><mml:math id="M13"><mml:mstyle displaystyle="true"><mml:msub><mml:mi>K</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>&lt;</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mn>4</mml:mn></mml:msubsup><mml:msub><mml:mo>&gt;</mml:mo><mml:mi>s</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>4</mml:mn><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo>&gt;</mml:mo><mml:mi>s</mml:mi></mml:msub><mml:mo>&lt;</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mn>3</mml:mn></mml:msubsup><mml:msub><mml:mo>&gt;</mml:mo><mml:mi>s</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>6</mml:mn><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mo>&gt;</mml:mo><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>&lt;</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mo>&gt;</mml:mo><mml:mi>s</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mo>&gt;</mml:mo><mml:mi>s</mml:mi><mml:mn>4</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>&lt;</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mo>&gt;</mml:mo><mml:mi>s</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mo>&gt;</mml:mo><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mstyle></mml:math></disp-formula></p><p id="P45">Starting from the second order, estimates of these moments based on trial-averaged response include noise-related bias terms, which skew the kurtosis estimates for limited trial counts. We analytically demonstrated and numerically verified that these biases can be suppressed using noise corrected formulae of all moments that are detailed in the <xref ref-type="supplementary-material" rid="SD1">Supplementary Information</xref>. Code for these calculations is provided with the online data set.</p><p id="P46">When calculating population sparseness, we analysed all neurons including non-responsive neurons. Non-responsive neurons with aberrant response levels (&gt;5 times the maximal value of responsive neurons) were excluded. Based on this, the percentages of units used were : ICE : 92%, IC: 80%, TH: 61%, THE: 97%, AC:92%).</p></sec><sec id="S21"><title>Population activity classifiers</title><p id="P47">To evaluate the accuracy of sound identification based on single-trial population responses, we trained a nearest-neighbour classifier on a subset of trials and cross-validated it on a distinct subset of trials. Training and testing sets were constructed by randomly selecting half of the trials for each unit. For each sound, we correlated the population response averaged over the training trials for this sound with the population response averaged over the testing trials for all the other sounds. The sound with the highest correlation was assigned as the prediction. Decoding accuracy is defined as the proportion of correctly assigned sounds.</p><p id="P48">Spatial and spatio-temporal were defined as for the correlation measures. Statistical significance was evaluated using the same bootstrap procedure as for the correlation measures. Importantly, decoding depends inherently on trial-to-trial noise which limits the possibility of comparing between areas. This analysis serves to contrast spatial and spatio-temporal codes within an area.</p><p id="P49">To measure the information contained at different timescales, the temporal sequence of population activity was decomposed into its Fourier coefficients corresponding to a discrete set of timescales ranging from <italic>T</italic>, the 750 ms sound response duration, down to <italic><monospace>2Δt,</monospace></italic> where <monospace><italic>Δt</italic></monospace> is the discretization time of the dataset (1/2<monospace><italic>Δt = f</italic> the Nyquist frequency; Δt = <italic>T</italic>/24 = 31.25 ms for 2P-imaging data and Δt =</monospace> <italic>T</italic>/96 = 7.81 ms for electrophysiology data).</p><p id="P50">The Fourier coefficient <italic>C</italic><sub><italic>n</italic>,<italic>r</italic></sub> for frequency <italic>n/T</italic> and neuron <italic>r</italic> is defined as <disp-formula id="FD5"><mml:math id="M14"><mml:mstyle displaystyle="true"><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>ν</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mrow><mml:mi>i</mml:mi><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:mstyle></mml:math></disp-formula> where <italic>ν<sub>r</sub>(k)</italic> is the activity of neuron <italic>r</italic> at timestep <italic>k</italic>, <italic>i</italic> = <inline-formula><mml:math id="M15"><mml:msqrt><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:msqrt></mml:math></inline-formula> and <italic>K</italic> = <italic>Tf</italic>. Each coefficient is a complex number or, equivalently, a two-dimensional vector. Hence the activity sequence for a given neuron is either represented by a vector of <italic>2K</italic> data points or of <italic>2K</italic> Fourier coefficients.</p><p id="P51">To measure the information present at a given time scale, we applied the population activity classifier on the population vector containing the 2N Fourier coefficients for this time scale for the N neurons of the dataset (<xref ref-type="fig" rid="F9">Extended Data Fig. 5e</xref>). To measure information present above a particular time scale T<sub>max</sub>, we used the Fourier coefficients from 1 to T<sub>max</sub> for each neuron and concatenated them into a 2NT<sub>max</sub> population vector (<xref ref-type="fig" rid="F2">Fig. 2k</xref>). Of note, when evaluating information at particular time scales, we did not apply any temporal filtering steps to avoid artefacts due to the finite size of the filter and preserve the full bandwidth of the data.</p></sec><sec id="S22"><title>Tuning analysis</title><p id="P52">To quantify the number of neurons significantly tuned to a specific property, we first performed a parametric ANOVA test to identify the neurons which respond significantly more to one of the sounds of interest (e.g. 60, 70 or 80 dB levels across all pure tones for intensity tuning, up vs down modulations in a given frequency range for frequency modulation). We used a threshold of p=0.05. We do not compare the absolute number of neurons tuned to a given property between areas since this will largely reflect the different levels of noise in the data sets and we focus on the properties of significantly tuned neurons.</p><p id="P53">To measure the tuning of individual units to classes of stimuli (for example up chirps vs down chirps) we used the following modulation index: <disp-formula id="FD6"><mml:math id="M16"><mml:mstyle displaystyle="true"><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mover><mml:msub><mml:mi>ν</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>−</mml:mo><mml:mover><mml:msub><mml:mi>ν</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0.5</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mover><mml:msub><mml:mi>ν</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mover><mml:msub><mml:mi>ν</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:math></disp-formula></p></sec><sec id="S23"><title>Reinforcement learning model</title><p id="P54">We adjusted a previously published reinforcement learning model <sup><xref ref-type="bibr" rid="R27">27</xref>,<xref ref-type="bibr" rid="R49">49</xref></sup>, to learn discriminations between pairs of temporal inputs. The model receives as inputs the temporal responses for two sounds: (<italic>X</italic><sub>Go</sub>(<italic>t</italic>)) for the rewarded sound and (<italic>X</italic><sub>NoGo</sub>(<italic>t</italic>)) for the non-rewarded sound. The model learns the synaptic weights between these input representations and a downstream decision circuit (<xref ref-type="fig" rid="F3">Fig. 3a</xref>). This circuit is composed of a Go-unit which outputs the decision (synaptic weights : <italic>w<sub>E</sub></italic>) and an inhibitory neuron that provides immediate linear inhibition to the reward neuron (synaptic weights : <italic>W<sub>I</sub></italic>). The temporal output, <italic>y(t)</italic>, of the model can therefore be described as :</p><p id="P55"><italic>y</italic>(<italic>t</italic>) = <italic>θ</italic>(<italic>w<sub>E</sub></italic><italic>X</italic>(<italic>t</italic>) − <italic>w</italic>(<italic>I</italic>)<italic>X</italic>(<italic>t</italic>) − <italic>ξ</italic>) where <italic>θ</italic> is the Heaviside step function, ξ is a time - independent Gaussian random noise process that models stochasticity of behavioural choices. The decision to go is made if the mean activity of the Go-unit within the response window &lt; <italic>y</italic>(<italic>t</italic>) &gt;<sub><italic>t</italic></sub> is larger than 0.2 (&lt; · &gt;<italic><sub>t</sub></italic> denotes time averaging over 0.5s).</p><p id="P56">The synaptic weights are updated according to a learning rule which compares the reward prediction to the actual reward, assuming that reward prediction corresponds to the mean input received by the Go-unit. The learning rule has three particularities that have been previously shown to be important to account for mouse behaviour <sup><xref ref-type="bibr" rid="R49">49</xref></sup> and compatible with our knowledge of synaptic plasticity rules. First, it is asymmetric : the learning rate is larger when an unexpected reward occurs than when an expected reward does not. Second, it is multiplicative: the learning rate at a given synapse depends on the current weight of that synapse. Finally, it takes into account the known dynamics of the eligibility trace in the striatum <sup><xref ref-type="bibr" rid="R52">52</xref></sup> which is a key target of both AC and TH in discrimination learning <sup><xref ref-type="bibr" rid="R33">33</xref></sup>. The eligibility trace is a key mechanism in the “neo-hebbian framework” that aims to explain how synaptic plasticity can accommodate delays between action initiation and environmental feedback. This theory proposes that synapses that undergo pre-post coincidence prior to feedback are tagged via a long-lasting (∼ few seconds) eligibility trace. Weight changes will only occur at these tagged synapses if they are subsequently exposed to neuromodulatory feedback before this eligibility trace decays. In line with this, in the striatum, potentiation of synapses is conditioned on dopamine release within a ∼3s time window following coincidence of pre- and post-synaptic activity <sup><xref ref-type="bibr" rid="R52">52</xref></sup>. To implement this in our model, the temporal signal for the model input is convolved with a kernel corresponding to the temporal profile of dopaminergic plasticity gating taken from Yagishita et al <sup><xref ref-type="bibr" rid="R52">52</xref></sup> before calculation of the weight update.</p><p id="P57">The learning rule is implemented as : <disp-formula id="FD7"><mml:math id="M17"><mml:mtable><mml:mtr><mml:mtd><mml:mi>δ</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>E</mml:mi><mml:mi>l</mml:mi><mml:mi>T</mml:mi><mml:mi>r</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>δ</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>E</mml:mi><mml:mi>l</mml:mi><mml:mi>T</mml:mi><mml:mi>r</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p id="P58">Where λ the learning rate, <italic>R</italic> is the action outcome (<italic>R</italic> = 1 for reward, <italic>R</italic> = -1 for no reward, σ is the behavioural noise level parameter that sets the models peak performance, <italic>f</italic>() is the function that implements asymmetric learning such that <disp-formula id="FD8"><mml:math id="M18"><mml:mtable><mml:mtr><mml:mtd><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mtext>if</mml:mtext><mml:mspace width="0.2em"/><mml:mi>u</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>ν</mml:mi><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mtext>if</mml:mtext><mml:mspace width="0.2em"/><mml:mi>u</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula> <italic>ν</italic> &gt; 1 is the learning rate asymmetry ratio, <disp-formula id="FD9"><mml:math id="M19"><mml:mstyle displaystyle="true"><mml:mi>E</mml:mi><mml:mi>l</mml:mi><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>l</mml:mi><mml:mi>T</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mi>X</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>u</mml:mi></mml:mstyle></mml:math></disp-formula> where <italic>D</italic>(<italic>u</italic>) is the temporal function shown in <xref ref-type="fig" rid="F3">Fig. 3a</xref> and taken from Yagishita et al <sup><xref ref-type="bibr" rid="R52">52</xref></sup> and <italic>T<sub>ElTr</sub></italic> = 0.5s.</p><p id="P59">In order to estimate the speed at which the model learns to discriminate between different neural representations, we used as input the population vector time series for two different sounds from a given area. For calcium imaging, we first performed clustering of the response to reduce dimensionality. The model was then run for three independent simulations to average out the stochastic contribution and we evaluated the number of trials to reach 80% based on the average learning curve over these three repeats.</p><p id="P60">For dimensionality reduction of the population vector, we performed agglomerative hierarchical clustering based on the euclidean distance between each neuron’s full temporal response to all stimuli. The number of clusters was established by increasing the number of clusters until the sound-pair RSA matrix constructed from the clusters explained 95% of the variance of the matrix constructed from the full neural population. Clustering was performed independently for each data set and yielded approximately 150 clusters in all areas. AC data displayed in <xref ref-type="fig" rid="F2">Fig. 2c</xref> represents clusters rather than single neurons.</p></sec><sec id="S24"><title>Convolutional neural networks</title><sec id="S25"><title>Augmented sound set</title><p id="P61">In order to train deep neural networks, we created an augmented sound set that covered all the basic parameters explored by the original 140 sound set used in experiments. We first augmented the basic sounds composing the sound set from 140 to 2169. This first step generated the sounds by independently varying all features defining the sounds (frequency, intensity, amplitude modulation direction or period, frequency modulation direction, chord composition). Thereby, a given feature cannot be predicted based on other features as in the experimental sound set. We further augmented the sound set using the approach from <sup><xref ref-type="bibr" rid="R55">55</xref></sup>. Each 500ms sound is embedded at a random time in a randomly chosen 1.5 s snippet taken from an auditory scene (bus station, park, street…) with a random intensity (average : 53db, std : 7dB). We thus generated a total of 150.000 sounds for the test (6.000), train (110.000) and validation (34.000) sets respectively.</p></sec><sec id="S26"><title>Task definitions</title><p id="P62">The multi-category task required the network to output a 14-element binary category vector in which 1 indicates that the sound presented belongs to one of 14 categories, divided into 4 groups within which categories are mutually exclusive: frequency range, intensity range, frequency modulation type, and amplitude modulation type. However, all sounds had to receive one label from each group. The group structure was not provided to the network which therefore had to learn that a sound could not be simultaneously high and mid frequency for example. The categories were defined as follows:</p><list list-type="simple" id="L1"><list-item><label>-</label><p id="P63">Frequency range group: high frequency (4-8 kHz) / mid frequency (9-17 kHz) / low frequency 18-38 kHz) / broadband (white noise only). For chords and frequency modulated chirps, the frequency value used for categorization was the average of all frequencies (i.e. middle of the chirp).</p></list-item><list-item><label>-</label><p id="P64">Intensity range group: high time-averaged intensity (80dB) / mid time-averaged intensity (70dB) / and low time averaged intensity (60 dB). Amplitude modulated sounds were assigned to their closest time-averaged range group. We obtained different overall intensities by ramping sounds sublinearly, linearly or supralinearly.</p></list-item><list-item><label>-</label><p id="P65">Amplitude-modulation group: Up-ramping/ down-ramping / sinusoidal modulation / no modulation.</p></list-item><list-item><label>-</label><p id="P66">Frequency-modulation group: Up chirp / Down chirp / no modulation.</p></list-item></list><p id="P67">The sound identification task required the network to output the identity of each of the 2169 different sounds without any category.</p><p id="P68">The convolutional autoencoder is a network trained to reproduce with minimal loss its input with the constraint of passing all information through a small, central bottleneck layer. It is composed of an encoder sub-network that processes the input to allow for compression in the bottleneck layer and a decoder sub-network that reconstructs the output from the low-dimensional bottleneck representation.</p></sec><sec id="S27"><title>Architecture definition and training</title><p id="P69">All networks take as input a 2D (time x frequency) matrix of the log-scaled spectrogram of the sound and must produce as output the labels described above. In order to achieve this, a series of convolutional blocks is applied to transform the input. All classification networks were built from a series of 6 blocks composed of the same layers :</p><list list-type="simple" id="L2"><list-item><label>-</label><p id="P70">convolution : the input is convolved by a filter whose weights the network must learn, each layer applies multiple filters, generating a 3D matrix (time x frequency channel) from the initial 2D input (free parameters : kernel size, kernel stride, channel number)</p></list-item><list-item><label>-</label><p id="P71">activation : the output of the convolution is passed through a Relu non-linear activation function</p></list-item><list-item><label>-</label><p id="P72">maxpooling : the output of activation is downsampled by taking the maximal value of neighbouring values (free parameters : pool size, pool stride)</p></list-item><list-item><label>-</label><p id="P73">dropout : in order to improve the robustness of training, during each training batch a random 50% selection of connections are eliminated. During testing and validation, all connections are active.</p></list-item></list><p id="P74">After these convolutional blocks, a final 64-node fully connected layer with a Relu non-linearity allows to aggregate information across time, frequency and channel dimensions. The output layer is obtained for the multilabel task by applying a sigmoid function to the fully connected output and for the identification task by applying a softmax function.</p><p id="P75">The output of the last layer allowed us to calculate the value of the loss function that comprises the error the network makes (categorical cross entropy loss function) and a L1 regularisation term in order to improve network robustness. This loss was then back-propagated during training in order to optimise the weights of the connections using the Adam optimizer.</p><p id="P76">Any given architecture requires arbitration across a wide range of free parameters, most notably the kernel and max pooling size and stride as well as the number of channels in each block. One approach to this problem is to perform a search across architectures to obtain optimal performance on the task. This has allowed optimization on ecologically-relevant tasks to be proposed as a criteria for building deep networks that function like the brain. However we focused on general properties of CNNs and were using a simple task without natural sounds. We therefore chose to assess the generality of our results on various architectures instead of performing an exhaustive search. We also verified the reliability of our results for a given architecture by using 2 different initialization weights per architecture. The four architectures we evaluated are defined as follows (CV : convolution layer, MP : max pooling layer, FC : fully connected layer, Ker : kernel size) :</p><list list-type="simple" id="L3"><list-item><label>(1)</label><p id="P77">Input : 109 x 150; Cv1 : 109 x 150 x 18, Ker(3,3); MP; CV2 : 55 x 75 x 20, Ker(5,5); CV3 : 55 x 75 x 24, Ker(6,6); MP; CV4 : 28 x 38x 28, Ker(7,7); CV5 : 28 x 38 x 32, Ker(8,8); MP; CV6 : 14 x 19 x 32, Ker(9,9); FC : 64</p></list-item><list-item><label>(2)</label><p id="P78">Input : 109 x 150; Cv1 : 55 x 75 x 18, Ker(3,3); CV2 : 55 x 75 x 20, Ker(5,5); CV3 : 28 x 38 x 24, Ker(6,6); CV4 : 28 x 38x 28, Ker(7,7); CV5 : 14 x 19 x 32, Ker(8,8); CV6 : 14 x 19 x 32, Ker(9,9); FC : 64</p></list-item><list-item><label>(3)</label><p id="P79">Input : 109 x 150; Cv1 : 55 x 75 x 1, Ker(7,7)8; CV2 : 55 x 75 x 20, Ker(7,7); CV3 : 28 x 38 x 24, Ker(7,7); CV4 : 28 x 38x 28, Ker(7,7); CV5 : 14 x 19 x 32, Ker(7,7); CV6 : 14 x 19 x 32, Ker(7,7); FC : 64</p></list-item><list-item><label>(4)</label><p id="P80">Input : 109 x 150; Cv1 : 55 x 75 x 24, Ker(3,3); CV2 : 55 x 75 x 24, Ker(5,5); CV3 : 28 x 38 x 24, Ker(6,6); CV4 : 28 x 38x 24, Ker(7,7); CV5 : 14 x 19 x 24, Ker(8,8); CV6 : 14 x 19 x 24, Ker(9,9); FC : 64</p></list-item></list><p id="P81">One prominent consequence of the choice of CNN architecture is the way in which the input volume evolves throughout the network. Choosing a large stride in the convolutional or a large window size in the max pooling layer will lead to a shrinkage of the input dimensions (time and frequency). Given that the temporal dimension is preserved in the brain, we examined an architecture in which there is no shrinkage at all of the temporal dimension. To do this, we used the 4 same architectures described above, with the temporal dimension kept constant by setting all strides to 1 and eliminating max pooling. This results in a large expansion of the parameters in the network and affects training speed although asymptotic performance remains the same (<xref ref-type="fig" rid="F4">Fig. 4b</xref>).</p><p id="P82">The convolutional autoencoder receives as input the 2D spectrogram and must output a denoised spectrogram (spectrogram of the central sound without the background noise). The autoencoder was composed of 4 convolutional blocks as previously described in the encoding part and decoding networks, the bottleneck is a fully-connected, 20 node layer. Training was performed with an Adam optimizer, L1 and L2 regularisation and MSE as a loss function.</p><p id="P83">The convolutional neural network trained on word and musical genre recognition was previously published<sup><xref ref-type="bibr" rid="R55">55</xref></sup> and parameters have been made available at (<ext-link ext-link-type="uri" xlink:href="https://github.com/mcdermottLab/kelletal2018">https://github.com/mcdermottLab/kelletal2018</ext-link>). This network is composed of a central branch that splits into two branches, with one branch trained to identify musical genres and the other branch trained to identify words. In the original paper, the network was shown to achieve human-like performance and to qualitatively reproduce psychophysical measures during these tasks.</p></sec><sec id="S28"><title>Analysis of CNN activations</title><p id="P84">Once the networks had been trained, we analysed the responses of all nodes in each activation layer to the 140 sounds that were presented during experimental sessions. Each sound generates at a given layer a 3D matrix (time x frequency x channels). By considering the temporal response of each frequency x channel combination we obtained analogs to the temporal response of individual neurons. We then applied the same analysis techniques to these artificial responses as described above for neural recordings. In order to perform decoding which requires multiple presentations of the same sound, we presented to the network multiple copies of each sound embedded in different noise backgrounds.</p></sec><sec id="S29"><title>Cochlear model</title><p id="P85">A computational model was implemented by adapting the seminal model of Meddis <sup><xref ref-type="bibr" rid="R71">71</xref>,<xref ref-type="bibr" rid="R72">72</xref></sup> to the mouse cochlea and validating it with mouse auditory nerve recordings <sup><xref ref-type="bibr" rid="R73">73</xref></sup>. The model consists of a cascade of six stages recapitulating stapes velocity, basilar membrane velocity, inner hair cell (IHC) receptor potential, IHC presynaptic calcium currents, transmitter release events at the ribbon synapse, and firing response in auditory nerve fibres (ANFs) including refractory effects. The input model is a sound stimulus (in Pascals). The output is a train of spiking events (in spikes/s) in 590 ANFs innervating 40 IHCs with a characteristic frequency (CF) distributed at regular intervals along the cochlear tonotopic from 5 to 50 kHz, 12 IHCs per octave. This distribution covered 82.8% of the basilar membrane length from 1.2% (apex) to 83.9% (base) in 2.07% increments. According to experimental data, the number of ANFs per IHC (N) was controlled by the relationship N=−0.0038x^2+0.375×+7.9 where x is the IHC location along the basilar membrane such that x=−56.5+82.5 log⁡(CF), with x in percent from the apex and CF in kHz. By adjusting the time constant of the calcium clearance τ_Ca within each IHC synapse, ANFs with different spontaneous discharge rate (SR=91.1 <italic>τ</italic><sub>Ca</sub><sup><xref ref-type="bibr" rid="R2">2</xref></sup><sup>.<xref ref-type="bibr" rid="R66">66</xref></sup>, with <italic>τ</italic><sub>Ca</sub> in ms and SR in spikes/s) were simulated from 0.5 to 95 spikes/s (21 ± 19.8 spikes/s, mean ± SD) to match the SR distribution reported in mouse auditory nerve.</p></sec><sec id="S30"><title>Fast time-scale temporal to rate conversion in an excitation / inhibition model</title><p id="P86">We modeled the response of two integrate-and-fire neurons connected by a single inhibitory synapse that receive inputs A (In<sub>A</sub>) and B (In<sub>B</sub>) respectively. These inputs represent the A and B driven populations in the optogenetics experiment of <xref ref-type="fig" rid="F1">Fig 1</xref>. The A and B inputs exactly reproduced our temporal-coded stimulation (225ms duration, 20Hz stimulation, 25ms between flashes) but we systematically varied the interval between A and B which in the experiment was set to 250ms. The excitatory neuron received excitatory input A input of synaptic strength (J<sub>A</sub>=0.09) and inhibitory input from the inhibitory neuron (In<sub>I</sub>) of synaptic strength (J<sub>I</sub>=0.04), delayed relative to inhibitory spiking (<italic>τ</italic><sub>I</sub>=2ms). The inhibitory neuron received excitatory input B of synaptic strength (J<sub>B</sub>=0.09). Both neurons decayed to their resting membrane voltage (V<sub>m</sub>=-65mV) with membrane constant (<italic>τ</italic><sub>M</sub>=10ms), consistent with <italic>in vivo</italic> findings <sup><xref ref-type="bibr" rid="R38">38</xref></sup> and contained a white noise term (In<sub>noise</sub>). They emitted a single spike when they reached the threshold voltage (V<sub>T</sub>=-50mV) and their voltage was then reset to (V<sub>R</sub>=-70mV).</p><p id="P87">The equation of the voltage for each neuron is given by : <disp-formula id="FD10"><mml:math id="M20"><mml:mtable><mml:mtr><mml:mtd><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mi>I</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mi>I</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>I</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mtext>noise </mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mi>I</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>I</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mtext>noise </mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p></sec><sec id="S31"><title>Statistical analysis</title><p id="P88">Statistical results (degrees of freedom, p-values and statistical values) are reported in figure legends or in <xref ref-type="table" rid="T3">Extended Data Table 3</xref>. For statistical analysis of neural data, we performed a bootstrap analysis as detailed above. For statistical analysis of behavioural data provided in the manuscript, the Kolmogorov–Smirnov normality test was first performed on the data. If the data failed to meet the normality criterion, statistics relied on non-parametric tests. We therefore represent the median and quartiles of data in boxplots in all figures, in accordance with the use of non-parametric tests. Ranksum and signed rank: we report the signed rank statistic if the number of replicates is too weak to provide the normal Z statistic.</p></sec></sec></sec><sec sec-type="extended-data" id="S32"><title>Extended Data</title><fig id="F5" position="anchor"><label>Extended Data Fig. 1</label><caption><title>Details of behavioural learning in optogenetic cortical stimulation protocol.</title><p id="P89"><bold>a.</bold> Population average intrinsic imaging map of tonotopic areas in AC showing the localization of all spots used for optogenetic stimulation (n=7 mice). <bold>b.</bold> Intrinsic maps and spots used for stimulation with learning curves from two example mice in both tasks. <bold>c-d.</bold> Control experiment showing that response to optogenetic stimulation is specific to cortical activation : mice ceased responding to light stimulation when the cranial was blocked by a small cache that left all other light cues intact. Note also that the lick probability for temporal and rate patterns is identical during this initial phase. (paired Wilcoxon test, p = 0.0156, signed rank value = 28, n=7) <bold>e.</bold> Accuracy over the last 300 trials for all mice. (paired Wilcoxon test, p = 0.015, signed rank value = 28, n=7).</p></caption><graphic xlink:href="EMS158907-f005"/></fig><fig id="F6" position="anchor"><label>Extended Data Fig. 2</label><caption><title>Synaptic integration converts temporal information into firing rates at short time scales.</title><p id="P90">Neurons integrate synaptic inputs over the timescale of their membrane time constant. Hence the number of output spikes depends not only on the number and size of synaptic inputs but also on their relative timing. This property is reinforced by synaptic connectivity such that simple circuits can detect time shifts between incoming inputs and thereby transform temporal information into firing rate information. <bold>a.b</bold> To illustrate this point and evaluate in which conditions temporal information injected in the AC may give rise to salient firing rate representations, we simulated the activity of an integrate-and-fire neuron E connected to a neuron I by an inhibitory synapse. The inputs received by E and I are trains of 5 current pulses at 20Hz (pulse duration 25ms, total duration 225ms) simulating Chr2 activations by light as in <xref ref-type="fig" rid="F1">Fig. 1</xref>. Input onsets to neurons E and I are shifted by a time Δt to generate temporal information. Exemple simulations indicate that when the time shift is small and the inputs to E and I overlap in time (middle panels), neuron E emits fewer action potentials than when the inputs are well separated in time (right panels). <bold>c.</bold> Quantification of the effect exemplified in panels <bold>a</bold> and <bold>b</bold>, showing the firing rate of neuron E for a range of time shifts Δt. For |Δt| &lt; 250ms, neuron E emits fewer action potentials than for |Δt| &gt; 250ms and the number of action potentials depends on Δt. Hence, temporal information is converted partially to rate information for |Δt| &lt; 250ms but not for |Δt| &gt; 250ms. The boundary value for |Δt| (here ∼250ms) depends on the membrane time constant of the neurons (here set to 10ms, similar to values reported <italic>in vivo</italic> <sup><xref ref-type="bibr" rid="R38">38</xref></sup>). In general, this simulation indicates that in order to avoid the conversion of temporal information into rate information, the temporal sequences injected in cortex must avoid temporal contiguities over the time scale of the membrane time constant. Moreover, the use of time-reversed sequences tends to ensure smaller firing rate differences across sequences, compared to time sequences that are not symmetric of each other. <bold>d</bold>. Mean firing rate during the optogenetic stimulation and sound stimulus that evoked the highest firing rate in each neuron (n=321 units). <bold>e.</bold> Accuracy of neural decoder trained to discriminate the patterns used in the task with all spatial and temporal information available in the population vectors. (n=321 units, bootstrap over units, p-value of accuracy vs chance level of 0.5: 0.01, 0.01)</p></caption><graphic xlink:href="EMS158907-f006"/></fig><fig id="F7" position="anchor"><label>Extended Data Fig. 3</label><caption><title>Extensive neural recordings throughout the auditory system.</title><p id="P91"><bold>a.</bold> (i) Schematic of imaging strategy, (ii) sample field of view, and (iii) raw (black) or deconvolved (blue) calcium traces (grey bar: sound presentation) for a sample neuron in AC. (iv) Location of all recorded neurons, colour-coded according to their preferred frequency at 60dB, overlayed with the tonotopic gradients obtained from intrinsic imaging. (v) Response of 3 neurons to 3Hz amplitude modulated white noise. <bold>b.</bold> Same as in <bold>a</bold> for thalamic axon imaging. <bold>c.</bold> (i) Schematic of recording strategy, (ii) sample histology with di-I strained electrode track, (iii) average waveforms and auto-correlograms of three single units, (iv) response latencies of all single units, (v) raster plot of 5 trials from 3 sample units in response to 3Hz modulated white noise. <bold>d.</bold> Same as <bold>a</bold> for dorsal IC except for (iv): view of the cranial window and intrinsic imaging response to white noise. Inset histogram shows distribution recording depths. <bold>e.</bold> Same as <bold>c</bold> for central IC, except for (iv): reconstructed of IC tonotopy from single units. <bold>Ff</bold> (i) Schematic of the cochlea and (ii) of the biophysical model taking a sound as input and providing the responses of auditory nerve fibres. (iii) Response to 3Hz amplitude-modulated white noise. A1 : primary auditory cortex, DP: dorsal posterior field, AAF: anterior auditory field, VPAF : ventral posterior auditory field, SRAF : suprarhinal auditory field.</p></caption><graphic xlink:href="EMS158907-f007"/></fig><fig id="F8" position="anchor"><label>Extended Data Fig. 4</label><caption><title>Details of auditory system sampling.</title><p id="P92"><bold>a.</bold> Mean intrinsic imaging responses (n=32 mice) for 4, 16 and 32 kHz sounds (black) and the subtraction of 32kHz and 4kHz maps (colour). This extended data set allowed us to construct a consensus map to align mice included in the study. <bold>b.</bold> Illustration of method used to identify AC subregions based on the tonotopic gradients established in <sup><xref ref-type="bibr" rid="R74">74</xref></sup>. <bold>c.</bold> Localization of all recorded ROIs on the consensus tonotopic map with AC subregions. <bold>d.</bold> Localization of responsive neurons to increasing frequency and intensity. Note the larger recruitment with stronger intensity and the spatial shift with frequency. <bold>e.</bold> Proportion of units per subarea. <bold>f.</bold> Depth distribution of units per subarea. <bold>g</bold>. Example thalamocortical axon expressing GCaMP6s merged with Vglut2. Thalamic axonal boutons expressing Vglut2 appear yellow as shown in the magnified region (right). <bold>h.</bold> Density of labelled boutons (Vglut2<sup>+</sup>;GCaMP6s-expressing in yellow; GCaMP6s alone in green) in layer 1 of the AC (12 sample regions; 4 regions per confocal image; means and STD: 0.0122±0.0052, 0.0005±0.0008, density of co-labelled and green only boutons, respectively). <bold>i</bold>. Peristimulus time histogram of an auditory nerve fibre (ANF) with a characteristic frequency equal to that of the presented 12-kHz tone burst (10-ms rise/fall, 500-ms duration) with increasing level from 60, 70 and 80 dB SPL. Note the rapid adaptation of the firing. <bold>j</bold>. Basilar membrane velocity and sound-activated auditory nerve fibres per inner hair cell (IHC) along the tonotopic axis. Note the reduced frequency selectivity with the increasing intensity. Gray dashed line shows the mouse synaptic cochleogram. The criterion for sound-activated auditory nerve fibres was 10 spikes/s above the spontaneous rate.</p></caption><graphic xlink:href="EMS158907-f008"/></fig><fig id="F9" position="anchor"><label>Extended Data Fig. 5</label><caption><title>Robustness of accuracy and similarity measures.</title><p id="P93"><bold>a.</bold> Decoding accuracy for spatio-temporal and spatial codes in each area with varying numbers of sub-selected neurons. <bold>b.</bold> Reproducibility of single neuron (left) or population (right) responses measured as the mean inter-trial correlation between responses across sounds (left : n=number of neurons per area, right : n=140 sounds, error bars are quantiles). <bold>c.</bold> Measured correlation of simulated data with low to high response reproducibility before (orange) or after (blue) noise-correction. <bold>d.</bold> Noise-corrected correlation for spatio-temporal and spatial code in each area with varying numbers of sub-selected neurons. <bold>e.</bold> Sketch illustrating the decomposition of population responses by timescale and mean decoding accuracy based on successive Fourier coefficients of neural responses. 0 Hz = spatial code. As expected, 2-photon data only contained information up to whereas electrophysiology data was informative even up to 30 Hz. <bold>f-g.</bold> Noise-corrected sparseness measured using kurtosis. n = 140 sounds for population kurtosis (<bold>f</bold>) and n = ‘all neurons’ for lifetime kurtosis (<bold>g</bold>).</p></caption><graphic xlink:href="EMS158907-f009"/></fig><fig id="F10" position="anchor"><label>Extended Data Fig. 6</label><caption><title>Time-independent rate representations of time-symmetric sounds decorrelate in AC.</title><p id="P94"><bold>a.</bold> Noise-corrected RSA matrices for all sound pairs for temporal (left) or rate (right) codes. <bold>b.</bold> Illustration of method to calculate population tuning curves shown in B from RSA matrix. <bold>c.</bold> Mean noise-corrected correlation between pure tones as a function of their frequency separation. <bold>d–g.</bold> Mean noise-corrected correlation between sound pairs differing by only one acoustic property. <bold>d</bold>. Pure tones at the same frequency differing by intensity, <bold>e</bold>. amplitude ramps at same frequency differing by direction. <bold>f.</bold> frequency sweeps with identical frequency content and duration at 60dB vs 80dB, <bold>g</bold>. frequency sweeps with identical frequency content of different duration, For sounds without temporal structure, correlation of representations are similar in AC and IC, whereas for time-symmetric sounds, all brain areas show larger rate correlations than in the cortex, except for TH2P in <bold>e</bold> likely due to the high variability of thalamic responses. p-value for 100 bootstraps comparing rate correlation of each region to AC, error bars are S.D. Statistical test details are given in the <xref ref-type="table" rid="T3">Extended Data Table 3</xref>.</p></caption><graphic xlink:href="EMS158907-f010"/></fig><fig id="F11" position="anchor"><label>Extended Data Fig. 7</label><caption><title>Single cell tuning to diverse acoustic features from cochlea to auditory cortex.</title><p id="P95"><bold>a-e. Right:</bold> For each tuning property we show the responses of example neurons from the IC, TH and AC to sounds that differ according to that property and provide the tuning strength (TS) and best frequency (BF) for that neuron. Asterisks indicate significant tuning of the neuron to a specific value, for example the leftmost neuron in <bold>a</bold> is an IC neuron that is significantly tuned to frequency modulation speed with a maximum response for decreasing frequency at 3oct/s. (left) Boxplot giving the distribution of tuning strengths across the whole population and pie charts showing the proportion of neurons maximally tuned to each parameter value for significantly tuned neurons.</p></caption><graphic xlink:href="EMS158907-f011"/></fig><fig id="F12" position="anchor"><label>Extended Data Fig. 8</label><caption><title>Extended range of CNN networks and details of performance.</title><p id="P96"><bold>a</bold>. Category by category performance of CNNs trained without shrinking of the temporal dimension (left) or with (right) (n=8, error bars are sem). <bold>b.</bold> Mean response correlations from RSA matrices from untrained networks with the same architecture as those trained on the multi-category task (n=8, error bars are sem). <bold>c.</bold> Mean decoding accuracy based on successive Fourier coefficients of CNN responses. 0Hz = spatial code (n=8, shaded areas are sem). <bold>d.</bold> Mean correlations from the network trained on natural sounds from Kell et al <sup><xref ref-type="bibr" rid="R55">55</xref></sup> for musical snippets (left) or words (centre). <bold>e.</bold> Mean correlations from CNNs trained to identify all 2169 sounds individually (left, n=8) and accuracy for each sound (right). <bold>f.</bold> Example sounds provided as input to the autoencoder and their reconstructions at the output. <bold>g.</bold> Representation Similarity Analysis matrix of original sounds and reconstructed sounds showing that the autoencoder fully preserved the relations between all the sounds.</p></caption><graphic xlink:href="EMS158907-f012"/></fig><table-wrap id="T1" position="anchor" orientation="portrait"><label>Extended Data Table 1</label><caption><title>Details of dataset</title></caption><table frame="box" rules="all"><thead><tr><th align="center" valign="top"><italic>Brain region</italic></th><th align="center" valign="top"><italic>Recording method</italic></th><th align="center" valign="top"><italic>Units recorded</italic></th><th align="center" valign="top"><italic>Responsive units</italic></th><th align="center" valign="top"><italic>Animal number</italic></th><th align="center" valign="top"><italic>Session number</italic></th><th align="center" valign="top"><italic>Units per animal (min, mean, max)</italic></th><th align="center" valign="top"><italic>Units per session (min, mean, max)</italic></th></tr></thead><tbody><tr><td align="left" valign="top">Auditory cortex</td><td align="left" valign="top">Cell body 2 photon calcium imaging</td><td align="left" valign="top">60822</td><td align="left" valign="top">19414 (32%)</td><td align="left" valign="top">7</td><td align="left" valign="top">60</td><td align="left" valign="top">2164 / 8688 / 20631</td><td align="left" valign="top">57 /1013 / 1782</td></tr><tr><td align="left" valign="top" rowspan="2">Auditory thalamus</td><td align="left" valign="top">Axonal bouton 2 photon calcium imaging</td><td align="left" valign="top">39191</td><td align="left" valign="top">3969 (12%)</td><td align="left" valign="top">4</td><td align="left" valign="top">24</td><td align="left" valign="top">1280 / 9287 / 19870</td><td align="left" valign="top">477 / 1632 / 3120</td></tr><tr><td align="left" valign="top">Single unit electrophysiology</td><td align="left" valign="top">498</td><td align="left" valign="top">484 (97%)</td><td align="left" valign="top">10</td><td align="left" valign="top">33</td><td align="left" valign="top">4 / 49 / 113</td><td align="left" valign="top">2/15 / 32</td></tr><tr><td align="left" valign="top" rowspan="2">Inferior colliculus</td><td align="left" valign="top">Cell body 2 photon calcium imaging</td><td align="left" valign="top">15312</td><td align="left" valign="top">5936 (39%)</td><td align="left" valign="top">30</td><td align="left" valign="top">101</td><td align="left" valign="top">25 /510 / 2975</td><td align="left" valign="top">25 / 151 / 495</td></tr><tr><td align="left" valign="top">Single unit electrophysiology</td><td align="left" valign="top">563</td><td align="left" valign="top">442 (78%)</td><td align="left" valign="top">11</td><td align="left" valign="top">30</td><td align="left" valign="top">10 / 56 / 119</td><td align="left" valign="top">4/18 / 54</td></tr></tbody></table></table-wrap><table-wrap id="T2" position="anchor" orientation="portrait"><label>Extended Data Table 2</label><caption><title>Sound parameters</title></caption><table frame="box" rules="all"><thead><tr><th align="center" valign="middle"/><th align="center" valign="middle"/><th align="center" valign="middle"/><th align="center" valign="middle">Start freq. (kHz)</th><th align="center" valign="middle">Stop freq. (kHz)</th><th align="center" valign="middle">Start int. (dB)</th><th align="center" valign="middle">Stop int. (dB)</th><th align="center" valign="middle">Dur. (ms)</th></tr></thead><tbody><tr><td align="center" valign="middle">1</td><td align="center" valign="middle"/><td align="center" valign="middle">blank</td><td align="center" valign="middle">NaN</td><td align="center" valign="middle">NaN</td><td align="center" valign="middle">NaN</td><td align="center" valign="middle">NaN</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">2</td><td align="center" valign="middle" rowspan="33">Pure tones</td><td align="center" valign="middle">tono60dB_4kHz</td><td align="center" valign="middle">4</td><td align="center" valign="middle">4</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">3</td><td align="center" valign="middle">tono60dB_5kHz</td><td align="center" valign="middle">5</td><td align="center" valign="middle">5</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">4</td><td align="center" valign="middle">tono60dB_6kHz</td><td align="center" valign="middle">6</td><td align="center" valign="middle">6</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">5</td><td align="center" valign="middle">tono60dB_7kHz</td><td align="center" valign="middle">7</td><td align="center" valign="middle">7</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">6</td><td align="center" valign="middle">tono60dB_9kHz</td><td align="center" valign="middle">9</td><td align="center" valign="middle">9</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">7</td><td align="center" valign="middle">tono60dB_12kHz</td><td align="center" valign="middle">12</td><td align="center" valign="middle">12</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">8</td><td align="center" valign="middle">tono60dB_15kHz</td><td align="center" valign="middle">15</td><td align="center" valign="middle">15</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">9</td><td align="center" valign="middle">tono60dB_19kHz</td><td align="center" valign="middle">19</td><td align="center" valign="middle">19</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">10</td><td align="center" valign="middle">tono60dB_24kHz</td><td align="center" valign="middle">24</td><td align="center" valign="middle">24</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">11</td><td align="center" valign="middle">tono60dB_29kHz</td><td align="center" valign="middle">29</td><td align="center" valign="middle">29</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">12</td><td align="center" valign="middle">tono60dB_37kHz</td><td align="center" valign="middle">37</td><td align="center" valign="middle">37</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">13</td><td align="center" valign="middle">tono70dB_4kHz</td><td align="center" valign="middle">4</td><td align="center" valign="middle">4</td><td align="center" valign="middle">70</td><td align="center" valign="middle">70</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">14</td><td align="center" valign="middle">tono70dB_5kHz</td><td align="center" valign="middle">5</td><td align="center" valign="middle">5</td><td align="center" valign="middle">70</td><td align="center" valign="middle">70</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">15</td><td align="center" valign="middle">tono70dB_6kHz</td><td align="center" valign="middle">6</td><td align="center" valign="middle">6</td><td align="center" valign="middle">70</td><td align="center" valign="middle">70</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">16</td><td align="center" valign="middle">tono70dB_7kHz</td><td align="center" valign="middle">7</td><td align="center" valign="middle">7</td><td align="center" valign="middle">70</td><td align="center" valign="middle">70</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">17</td><td align="center" valign="middle">tono70dB_9kHz</td><td align="center" valign="middle">9</td><td align="center" valign="middle">9</td><td align="center" valign="middle">70</td><td align="center" valign="middle">70</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">18</td><td align="center" valign="middle">tono70dB_12kHz</td><td align="center" valign="middle">12</td><td align="center" valign="middle">12</td><td align="center" valign="middle">70</td><td align="center" valign="middle">70</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">19</td><td align="center" valign="middle">tono70dB_15kHz</td><td align="center" valign="middle">15</td><td align="center" valign="middle">15</td><td align="center" valign="middle">70</td><td align="center" valign="middle">70</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">20</td><td align="center" valign="middle">tono70dB_19kHz</td><td align="center" valign="middle">19</td><td align="center" valign="middle">19</td><td align="center" valign="middle">70</td><td align="center" valign="middle">70</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">21</td><td align="center" valign="middle">tono70dB_24kHz</td><td align="center" valign="middle">24</td><td align="center" valign="middle">24</td><td align="center" valign="middle">70</td><td align="center" valign="middle">70</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">22</td><td align="center" valign="middle">tono70dB_29kHz</td><td align="center" valign="middle">29</td><td align="center" valign="middle">29</td><td align="center" valign="middle">70</td><td align="center" valign="middle">70</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">23</td><td align="center" valign="middle">tono70dB_37kHz</td><td align="center" valign="middle">37</td><td align="center" valign="middle">37</td><td align="center" valign="middle">70</td><td align="center" valign="middle">70</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">24</td><td align="center" valign="middle">tono80dB_4kHz</td><td align="center" valign="middle">4</td><td align="center" valign="middle">4</td><td align="center" valign="middle">80</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">25</td><td align="center" valign="middle">tono80dB_5kHz</td><td align="center" valign="middle">5</td><td align="center" valign="middle">5</td><td align="center" valign="middle">80</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">26</td><td align="center" valign="middle">tono80dB_6kHz</td><td align="center" valign="middle">6</td><td align="center" valign="middle">6</td><td align="center" valign="middle">80</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">27</td><td align="center" valign="middle">tono80dB_7kHz</td><td align="center" valign="middle">7</td><td align="center" valign="middle">7</td><td align="center" valign="middle">80</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">28</td><td align="center" valign="middle">tono80dB_9kHz</td><td align="center" valign="middle">9</td><td align="center" valign="middle">9</td><td align="center" valign="middle">80</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">29</td><td align="center" valign="middle">tono80dB_12kHz</td><td align="center" valign="middle">12</td><td align="center" valign="middle">12</td><td align="center" valign="middle">80</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">30</td><td align="center" valign="middle">tono80dB_15kHz</td><td align="center" valign="middle">15</td><td align="center" valign="middle">15</td><td align="center" valign="middle">80</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">31</td><td align="center" valign="middle">tono80dB_19kHz</td><td align="center" valign="middle">19</td><td align="center" valign="middle">19</td><td align="center" valign="middle">80</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">32</td><td align="center" valign="middle">tono80dB_24kHz</td><td align="center" valign="middle">24</td><td align="center" valign="middle">24</td><td align="center" valign="middle">80</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">33</td><td align="center" valign="middle">tono80dB_29kHz</td><td align="center" valign="middle">29</td><td align="center" valign="middle">29</td><td align="center" valign="middle">80</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">34</td><td align="center" valign="middle">tono80dB_37kHz</td><td align="center" valign="middle">37</td><td align="center" valign="middle">37</td><td align="center" valign="middle">80</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">35</td><td align="center" valign="middle" rowspan="5">Pure up ramps</td><td align="center" valign="middle">Up4kHz</td><td align="center" valign="middle">4</td><td align="center" valign="middle">4</td><td align="center" valign="middle">60</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">36</td><td align="center" valign="middle">Up6kHz</td><td align="center" valign="middle">6</td><td align="center" valign="middle">6</td><td align="center" valign="middle">60</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">37</td><td align="center" valign="middle">Up9kHz</td><td align="center" valign="middle">9</td><td align="center" valign="middle">9</td><td align="center" valign="middle">60</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">38</td><td align="center" valign="middle">Up15kHz</td><td align="center" valign="middle">15</td><td align="center" valign="middle">15</td><td align="center" valign="middle">60</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">39</td><td align="center" valign="middle">Up24kHz</td><td align="center" valign="middle">24</td><td align="center" valign="middle">24</td><td align="center" valign="middle">60</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">40</td><td align="center" valign="middle" rowspan="16">Chord up ramps</td><td align="center" valign="middle">Up4+6kHz</td><td align="center" valign="middle">4, 6</td><td align="center" valign="middle">4, 6</td><td align="center" valign="middle">60</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">41</td><td align="center" valign="middle">Up4+9kHz</td><td align="center" valign="middle">4, 9</td><td align="center" valign="middle">4, 9</td><td align="center" valign="middle">60</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">42</td><td align="center" valign="middle">Up4+15kHz</td><td align="center" valign="middle">4, 15</td><td align="center" valign="middle">4, 15</td><td align="center" valign="middle">60</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">43</td><td align="center" valign="middle">Up4+24kHz</td><td align="center" valign="middle">4, 24</td><td align="center" valign="middle">4, 24</td><td align="center" valign="middle">60</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">44</td><td align="center" valign="middle">Up6+9kHz</td><td align="center" valign="middle">6, 9</td><td align="center" valign="middle">6, 9</td><td align="center" valign="middle">60</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">45</td><td align="center" valign="middle">Up6+15kHz</td><td align="center" valign="middle">6, 15</td><td align="center" valign="middle">6, 15</td><td align="center" valign="middle">60</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">46</td><td align="center" valign="middle">Up6+24kHz</td><td align="center" valign="middle">6, 24</td><td align="center" valign="middle">6, 24</td><td align="center" valign="middle">60</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">47</td><td align="center" valign="middle">Up9+15kHz</td><td align="center" valign="middle">9, 15</td><td align="center" valign="middle">9, 15</td><td align="center" valign="middle">60</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">48</td><td align="center" valign="middle">Up9+24kHz</td><td align="center" valign="middle">9, 24</td><td align="center" valign="middle">9, 24</td><td align="center" valign="middle">60</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">49</td><td align="center" valign="middle">Up15+24kHz</td><td align="center" valign="middle">15, 24</td><td align="center" valign="middle">15, 24</td><td align="center" valign="middle">60</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">50</td><td align="center" valign="middle">Up4+6+9+15kHz</td><td align="center" valign="middle">4, 6, 9, 15</td><td align="center" valign="middle">4, 6, 9, 15</td><td align="center" valign="middle">60</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">51</td><td align="center" valign="middle">Up4+6+9+24kHz</td><td align="center" valign="middle">4, 6, 9, 15, 24</td><td align="center" valign="middle">4, 6, 9, 15, 24</td><td align="center" valign="middle">60</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">52</td><td align="center" valign="middle">Up4+6+15+24kHz</td><td align="center" valign="middle">4, 6, 15, 24</td><td align="center" valign="middle">4, 6, 15, 24</td><td align="center" valign="middle">60</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">53</td><td align="center" valign="middle">Up4+9+15+24kHz</td><td align="center" valign="middle">4, 9, 15, 24</td><td align="center" valign="middle">4, 9, 15, 24</td><td align="center" valign="middle">60</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">54</td><td align="center" valign="middle">Up6+9+15+24kHz</td><td align="center" valign="middle">6, 9, 15, 24</td><td align="center" valign="middle">6, 9, 15, 24</td><td align="center" valign="middle">60</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">55</td><td align="center" valign="middle">UpmultiHz</td><td align="center" valign="middle">4, 6, 9, 15, 24</td><td align="center" valign="middle">4, 6, 9, 15, 24</td><td align="center" valign="middle">60</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">56</td><td align="center" valign="middle" rowspan="5">Pure down ramps</td><td align="center" valign="middle">Down4kHz</td><td align="center" valign="middle">4</td><td align="center" valign="middle">4</td><td align="center" valign="middle">80</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">57</td><td align="center" valign="middle">Down6kHz</td><td align="center" valign="middle">6</td><td align="center" valign="middle">6</td><td align="center" valign="middle">80</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">58</td><td align="center" valign="middle">Down9kHz</td><td align="center" valign="middle">9</td><td align="center" valign="middle">9</td><td align="center" valign="middle">80</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">59</td><td align="center" valign="middle">Down15kHz</td><td align="center" valign="middle">15</td><td align="center" valign="middle">15</td><td align="center" valign="middle">80</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">60</td><td align="center" valign="middle">Down24kHz</td><td align="center" valign="middle">24</td><td align="center" valign="middle">24</td><td align="center" valign="middle">80</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">61</td><td align="center" valign="middle" rowspan="16">Chord down ramps</td><td align="center" valign="middle">Down4+6kHz</td><td align="center" valign="middle">4, 6</td><td align="center" valign="middle">4, 6</td><td align="center" valign="middle">80</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">62</td><td align="center" valign="middle">Down4+9kHz</td><td align="center" valign="middle">4, 9</td><td align="center" valign="middle">4, 9</td><td align="center" valign="middle">80</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">63</td><td align="center" valign="middle">Down4+15kHz</td><td align="center" valign="middle">4, 15</td><td align="center" valign="middle">4, 15</td><td align="center" valign="middle">80</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">64</td><td align="center" valign="middle">Down4+24kHz</td><td align="center" valign="middle">4, 24</td><td align="center" valign="middle">4, 24</td><td align="center" valign="middle">80</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">65</td><td align="center" valign="middle">Down6+9kHz</td><td align="center" valign="middle">6, 9</td><td align="center" valign="middle">6, 9</td><td align="center" valign="middle">80</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">66</td><td align="center" valign="middle">Down6+15kHz</td><td align="center" valign="middle">6, 15</td><td align="center" valign="middle">6, 15</td><td align="center" valign="middle">80</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">67</td><td align="center" valign="middle">Down6+24kHz</td><td align="center" valign="middle">6, 24</td><td align="center" valign="middle">6, 24</td><td align="center" valign="middle">80</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">68</td><td align="center" valign="middle">Down9+15kHz</td><td align="center" valign="middle">9, 15</td><td align="center" valign="middle">9, 15</td><td align="center" valign="middle">80</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">69</td><td align="center" valign="middle">Down9+24kHz</td><td align="center" valign="middle">9, 24</td><td align="center" valign="middle">9, 24</td><td align="center" valign="middle">80</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">70</td><td align="center" valign="middle">Down15+24kHz</td><td align="center" valign="middle">15, 24</td><td align="center" valign="middle">15, 24</td><td align="center" valign="middle">80</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">71</td><td align="center" valign="middle">Down4+6+9+15kHz</td><td align="center" valign="middle">4, 6, 9, 15</td><td align="center" valign="middle">4, 6, 9, 15</td><td align="center" valign="middle">80</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">72</td><td align="center" valign="middle">Down4+6+9+24kHz</td><td align="center" valign="middle">4, 6, 9, 15, 24</td><td align="center" valign="middle">4, 6, 9, 15, 24</td><td align="center" valign="middle">80</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">73</td><td align="center" valign="middle">Down4+6+15+24kHz</td><td align="center" valign="middle">4, 6, 15, 24</td><td align="center" valign="middle">4, 6, 15, 24</td><td align="center" valign="middle">80</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">74</td><td align="center" valign="middle">Down4+9+15+24kHz</td><td align="center" valign="middle">4, 9, 15, 24</td><td align="center" valign="middle">4, 9, 15, 24</td><td align="center" valign="middle">80</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">75</td><td align="center" valign="middle">Down6+9+15+24kHz</td><td align="center" valign="middle">6, 9, 15, 24</td><td align="center" valign="middle">6, 9, 15, 24</td><td align="center" valign="middle">80</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">76</td><td align="center" valign="middle">DownmultiHz</td><td align="center" valign="middle">4, 6, 9, 15, 24</td><td align="center" valign="middle">4, 6, 9, 15, 24</td><td align="center" valign="middle">80</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">77</td><td align="center" valign="middle" rowspan="12">Sinusoid AM modulation</td><td align="center" valign="middle">Sin1Hz9kHz</td><td align="center" valign="middle">9</td><td align="center" valign="middle">9</td><td align="center" valign="middle">60 - 80</td><td align="center" valign="middle">60 - 80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">78</td><td align="center" valign="middle">Sin3Hz9kHz</td><td align="center" valign="middle">9</td><td align="center" valign="middle">9</td><td align="center" valign="middle">60 - 80</td><td align="center" valign="middle">60 - 80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">79</td><td align="center" valign="middle">Sin7Hz9kHz</td><td align="center" valign="middle">9</td><td align="center" valign="middle">9</td><td align="center" valign="middle">60 - 80</td><td align="center" valign="middle">60 - 80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">80</td><td align="center" valign="middle">Sin20Hz9kHz</td><td align="center" valign="middle">9</td><td align="center" valign="middle">9</td><td align="center" valign="middle">60 - 80</td><td align="center" valign="middle">60 - 80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">81</td><td align="center" valign="middle">Sin1Hz24kHz</td><td align="center" valign="middle">24</td><td align="center" valign="middle">24</td><td align="center" valign="middle">60 - 80</td><td align="center" valign="middle">60 - 80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">82</td><td align="center" valign="middle">Sin3Hz24kHz</td><td align="center" valign="middle">24</td><td align="center" valign="middle">24</td><td align="center" valign="middle">60 - 80</td><td align="center" valign="middle">60 - 80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">83</td><td align="center" valign="middle">Sin7Hz24kHz</td><td align="center" valign="middle">24</td><td align="center" valign="middle">24</td><td align="center" valign="middle">60 - 80</td><td align="center" valign="middle">60 - 80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">84</td><td align="center" valign="middle">Sin20Hz24kHz</td><td align="center" valign="middle">24</td><td align="center" valign="middle">24</td><td align="center" valign="middle">60 - 80</td><td align="center" valign="middle">60 - 80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">85</td><td align="center" valign="middle">Sin1Hz White noise</td><td align="center" valign="middle">WN</td><td align="center" valign="middle">WN</td><td align="center" valign="middle">60 - 80</td><td align="center" valign="middle">60 - 80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">86</td><td align="center" valign="middle">Sin3HzWhitenoise</td><td align="center" valign="middle">WN</td><td align="center" valign="middle">WN</td><td align="center" valign="middle">60 - 80</td><td align="center" valign="middle">60 - 80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">87</td><td align="center" valign="middle">Sin7HzWhitenoise</td><td align="center" valign="middle">WN</td><td align="center" valign="middle">WN</td><td align="center" valign="middle">60 - 80</td><td align="center" valign="middle">60 - 80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">88</td><td align="center" valign="middle">Sin20Hz Whitenoise</td><td align="center" valign="middle">WN</td><td align="center" valign="middle">WN</td><td align="center" valign="middle">60 - 80</td><td align="center" valign="middle">60 - 80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">89</td><td align="center" valign="middle" rowspan="3">Up chirp varying speed</td><td align="center" valign="middle">ChirpUp4kHz60dB100ms</td><td align="center" valign="middle">4</td><td align="center" valign="middle">9</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">100</td></tr><tr><td align="center" valign="middle">90</td><td align="center" valign="middle">ChirpUp4kHz60dB250ms</td><td align="center" valign="middle">4</td><td align="center" valign="middle">9</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">250</td></tr><tr><td align="center" valign="middle">91</td><td align="center" valign="middle">ChirpUp4kHz60dB500ms</td><td align="center" valign="middle">4</td><td align="center" valign="middle">9</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">92</td><td align="center" valign="middle" rowspan="3"/><td align="center" valign="middle">ChirpUp24kHz60dB100ms</td><td align="center" valign="middle">9</td><td align="center" valign="middle">24</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">100</td></tr><tr><td align="center" valign="middle">93</td><td align="center" valign="middle">ChirpUp24kHz60dB250ms</td><td align="center" valign="middle">9</td><td align="center" valign="middle">24</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">250</td></tr><tr><td align="center" valign="middle">94</td><td align="center" valign="middle">ChirpUp24kHz60dB500ms</td><td align="center" valign="middle">9</td><td align="center" valign="middle">24</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">95</td><td align="center" valign="middle" rowspan="6">Down chirp varying speed</td><td align="center" valign="middle">ChirpDown4kHz60dB100ms</td><td align="center" valign="middle">9</td><td align="center" valign="middle">4</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">100</td></tr><tr><td align="center" valign="middle">96</td><td align="center" valign="middle">ChirpDown4kHz60dB250ms</td><td align="center" valign="middle">9</td><td align="center" valign="middle">4</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">250</td></tr><tr><td align="center" valign="middle">97</td><td align="center" valign="middle">ChirpDown4kHz60dB500ms</td><td align="center" valign="middle">9</td><td align="center" valign="middle">4</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">98</td><td align="center" valign="middle">ChirpDown24kHz60dB100ms</td><td align="center" valign="middle">24</td><td align="center" valign="middle">9</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">100</td></tr><tr><td align="center" valign="middle">99</td><td align="center" valign="middle">ChirpDown24kHz60dB250ms</td><td align="center" valign="middle">24</td><td align="center" valign="middle">9</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">250</td></tr><tr><td align="center" valign="middle">100</td><td align="center" valign="middle">ChirpDown24kHz60dB500ms</td><td align="center" valign="middle">24</td><td align="center" valign="middle">9</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">101</td><td align="center" valign="middle" rowspan="10">Up chirp - 60 dB</td><td align="center" valign="middle">ChirpUpclose4kHz60dB</td><td align="center" valign="middle">4</td><td align="center" valign="middle">6</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">102</td><td align="center" valign="middle">ChirpUpclose4to9kHz60dB</td><td align="center" valign="middle">4</td><td align="center" valign="middle">9</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">103</td><td align="center" valign="middle">ChirpUpclose4to15kHz60dB</td><td align="center" valign="middle">4</td><td align="center" valign="middle">15</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">104</td><td align="center" valign="middle">ChirpUpclose4to24kHz60dB</td><td align="center" valign="middle">4</td><td align="center" valign="middle">24</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">105</td><td align="center" valign="middle">ChirpUpclose6kHz60dB</td><td align="center" valign="middle">6</td><td align="center" valign="middle">9</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">106</td><td align="center" valign="middle">ChirpUpclose6to15kHz60dB</td><td align="center" valign="middle">6</td><td align="center" valign="middle">15</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">107</td><td align="center" valign="middle">ChirpUpclose6to24kHz60dB</td><td align="center" valign="middle">6</td><td align="center" valign="middle">24</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">108</td><td align="center" valign="middle">ChirpUpclose9kHz60dB</td><td align="center" valign="middle">9</td><td align="center" valign="middle">15</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">109</td><td align="center" valign="middle">ChirpUpclose9to24kHz60dB</td><td align="center" valign="middle">9</td><td align="center" valign="middle">24</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">110</td><td align="center" valign="middle">ChirpUpclose15kHz60dB</td><td align="center" valign="middle">15</td><td align="center" valign="middle">24</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">111</td><td align="center" valign="middle" rowspan="10">Down chirp - 60 dB</td><td align="center" valign="middle">ChirpDownclose6kHz60dB</td><td align="center" valign="middle">6</td><td align="center" valign="middle">4</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">112</td><td align="center" valign="middle">ChirpDownclose9to4kHz60dB</td><td align="center" valign="middle">9</td><td align="center" valign="middle">4</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">113</td><td align="center" valign="middle">ChirpDownclose 15to4kHz60dB</td><td align="center" valign="middle">15</td><td align="center" valign="middle">4</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">114</td><td align="center" valign="middle">ChirpDownclose24to4kHz60dB</td><td align="center" valign="middle">24</td><td align="center" valign="middle">4</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">115</td><td align="center" valign="middle">ChirpDownclose9kHz60dB</td><td align="center" valign="middle">9</td><td align="center" valign="middle">6</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">116</td><td align="center" valign="middle">ChirpDownclose15to6kHz60dB</td><td align="center" valign="middle">15</td><td align="center" valign="middle">6</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">117</td><td align="center" valign="middle">ChirpDownclose24to6kHz60dB</td><td align="center" valign="middle">24</td><td align="center" valign="middle">6</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">118</td><td align="center" valign="middle">ChirpDownclose15kHz60dB</td><td align="center" valign="middle">15</td><td align="center" valign="middle">9</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">119</td><td align="center" valign="middle">ChirpDownclose24to9kHz60dB</td><td align="center" valign="middle">24</td><td align="center" valign="middle">9</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">120</td><td align="center" valign="middle">ChirpDownclose24kHz60dB</td><td align="center" valign="middle">24</td><td align="center" valign="middle">15</td><td align="center" valign="middle">60</td><td align="center" valign="middle">60</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">121</td><td align="center" valign="middle" rowspan="3">Up chirp - 80 dB</td><td align="center" valign="middle">ChirpUpclose4kHz80dB</td><td align="center" valign="middle">4</td><td align="center" valign="middle">6</td><td align="center" valign="middle">80</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">122</td><td align="center" valign="middle">ChirpUpclose4to9kHz80dB</td><td align="center" valign="middle">4</td><td align="center" valign="middle">9</td><td align="center" valign="middle">80</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">123</td><td align="center" valign="middle">ChirpUpclose4to15kHz80dB</td><td align="center" valign="middle">4</td><td align="center" valign="middle">15</td><td align="center" valign="middle">80</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">124</td><td align="center" valign="middle" rowspan="7"/><td align="center" valign="middle">ChirpUpclose4to24kHz80dB</td><td align="center" valign="middle">4</td><td align="center" valign="middle">24</td><td align="center" valign="middle">80</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">125</td><td align="center" valign="middle">ChirpUpclose 6kHz80dB</td><td align="center" valign="middle">6</td><td align="center" valign="middle">9</td><td align="center" valign="middle">80</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">126</td><td align="center" valign="middle">ChirpUpclose6to15kHz80dB</td><td align="center" valign="middle">6</td><td align="center" valign="middle">15</td><td align="center" valign="middle">80</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">127</td><td align="center" valign="middle">ChirpUpclose6to24kHz80dB</td><td align="center" valign="middle">6</td><td align="center" valign="middle">24</td><td align="center" valign="middle">80</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">128</td><td align="center" valign="middle">ChirpUpclose9kHz80dB</td><td align="center" valign="middle">9</td><td align="center" valign="middle">15</td><td align="center" valign="middle">80</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">129</td><td align="center" valign="middle">ChirpUpclose9to24kHz80dB</td><td align="center" valign="middle">9</td><td align="center" valign="middle">24</td><td align="center" valign="middle">80</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">130</td><td align="center" valign="middle">ChirpUpclose15kHz80dB</td><td align="center" valign="middle">15</td><td align="center" valign="middle">24</td><td align="center" valign="middle">80</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">131</td><td align="center" valign="middle" rowspan="10">Down chirp - 80 dB</td><td align="center" valign="middle">ChirpDownclose6kHz80dB</td><td align="center" valign="middle">6</td><td align="center" valign="middle">4</td><td align="center" valign="middle">80</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">132</td><td align="center" valign="middle">ChirpDownclose9to4kHz80dB</td><td align="center" valign="middle">9</td><td align="center" valign="middle">4</td><td align="center" valign="middle">80</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">133</td><td align="center" valign="middle">ChirpDownclose 15to4kHz80dB</td><td align="center" valign="middle">15</td><td align="center" valign="middle">4</td><td align="center" valign="middle">80</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">134</td><td align="center" valign="middle">ChirpDownclose24to4kHz80dB</td><td align="center" valign="middle">24</td><td align="center" valign="middle">4</td><td align="center" valign="middle">80</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">135</td><td align="center" valign="middle">ChirpDownclose9kHz80dB</td><td align="center" valign="middle">9</td><td align="center" valign="middle">6</td><td align="center" valign="middle">80</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">136</td><td align="center" valign="middle">ChirpDownclose 15to6kHz80dB</td><td align="center" valign="middle">15</td><td align="center" valign="middle">6</td><td align="center" valign="middle">80</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">137</td><td align="center" valign="middle">ChirpDownclose24to6kHz80dB</td><td align="center" valign="middle">24</td><td align="center" valign="middle">6</td><td align="center" valign="middle">80</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">138</td><td align="center" valign="middle">ChirpDownclose15kHz80dB</td><td align="center" valign="middle">15</td><td align="center" valign="middle">9</td><td align="center" valign="middle">80</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">139</td><td align="center" valign="middle">ChirpDownclose24to9kHz80dB</td><td align="center" valign="middle">24</td><td align="center" valign="middle">9</td><td align="center" valign="middle">80</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr><tr><td align="center" valign="middle">140</td><td align="center" valign="middle">ChirpDownclose24kHz80dB</td><td align="center" valign="middle">24</td><td align="center" valign="middle">15</td><td align="center" valign="middle">80</td><td align="center" valign="middle">80</td><td align="center" valign="middle">500</td></tr></tbody></table></table-wrap><table-wrap id="T3" position="anchor" orientation="portrait"><label>Extended Data Table 3</label><caption><title>Detail of statistical comparisons</title></caption><table frame="box" rules="all"><thead><tr><th align="left" valign="top" colspan="7"><xref ref-type="fig" rid="F2">Fig. 2f</xref>. Bootstrap comparison of difference between temporal and rate accuracy in structure X vs in AC</th></tr></thead><tbody><tr><td align="center" valign="middle"/><td align="left" valign="top">AN</td><td align="left" valign="top">ICE</td><td align="left" valign="top">IC</td><td align="left" valign="top">THE</td><td align="left" valign="top">TH</td><td align="left" valign="top"/></tr><tr><td align="left" valign="middle">(Temp-Rate) norm</td><td align="left" valign="top">&lt;0.01</td><td align="left" valign="top">&lt;0.01</td><td align="left" valign="top">0.01</td><td align="left" valign="top">0.04</td><td align="left" valign="top">0.15</td><td align="left" valign="top"/></tr><tr><td align="center" valign="middle" colspan="7"/></tr><tr><td align="center" valign="middle" colspan="7"/></tr><tr><td align="left" valign="top" colspan="7"><xref ref-type="fig" rid="F2">Fig. 2h,i</xref><bold>. <italic>Bootstrap comparison of temporal and rate mean correlations in structure X vs in AC</italic></bold></td></tr><tr><td align="center" valign="middle"/><td align="left" valign="top">AN</td><td align="left" valign="top">ICE</td><td align="left" valign="top">IC</td><td align="left" valign="top">THE</td><td align="left" valign="top">TH</td><td align="left" valign="top"/></tr><tr><td align="left" valign="top">Temporal</td><td align="left" valign="top">&lt;0.01</td><td align="left" valign="top">0.43</td><td align="left" valign="top">0.12</td><td align="left" valign="top">0.01</td><td align="left" valign="top">&lt;0.01</td><td align="left" valign="top"/></tr><tr><td align="left" valign="top">Rate</td><td align="left" valign="top">&lt;0.01</td><td align="left" valign="top">&lt;0.01</td><td align="left" valign="top">&lt;0.01</td><td align="left" valign="top">&lt;0.01</td><td align="left" valign="top">&lt;0.01</td><td align="left" valign="top"/></tr><tr><td align="left" valign="middle">(Temp-Rate) norm</td><td align="left" valign="top">&lt;0.01</td><td align="left" valign="top">&lt;0.01</td><td align="left" valign="top">0.032</td><td align="left" valign="top">0.01</td><td align="left" valign="top">0.086</td><td align="left" valign="top"/></tr><tr><td align="center" valign="middle"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/></tr><tr><td align="left" valign="middle" colspan="7"><xref ref-type="fig" rid="F2">Fig. 2j</xref><bold>. <italic>Bootstrap comparison of RSA matrix similarity in structure X vs in AC</italic></bold></td></tr><tr><td align="center" valign="middle"/><td align="left" valign="top">AN</td><td align="left" valign="top">ICE</td><td align="left" valign="top">IC</td><td align="left" valign="top">THE</td><td align="left" valign="top">TH</td><td align="left" valign="top"/></tr><tr><td align="left" valign="top">Temp vs Rate</td><td align="left" valign="top">&lt;0.01</td><td align="left" valign="top">&lt;0.01</td><td align="left" valign="top">0.01</td><td align="left" valign="top">&lt;0.01</td><td align="left" valign="top">0.01</td><td align="left" valign="top"/></tr><tr><td align="center" valign="middle"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/></tr><tr><td align="left" valign="top" colspan="7"><xref ref-type="fig" rid="F3">Fig. 3</xref>. <bold><italic>Bootstrap comparison of rate mean correlations in structure X vs in AC</italic></bold></td></tr><tr><td align="center" valign="middle"/><td align="left" valign="top">AN</td><td align="left" valign="top">ICE</td><td align="left" valign="top">IC</td><td align="left" valign="top">THE</td><td align="left" valign="top">TH</td><td align="left" valign="top"/></tr><tr><td align="left" valign="top">3e - freq</td><td align="left" valign="top">&lt;0.01</td><td align="left" valign="top">0.25</td><td align="left" valign="top">0.61</td><td align="left" valign="top">&lt;0.01</td><td align="left" valign="top">0.27</td><td align="left" valign="top"/></tr><tr><td align="left" valign="top">3f - FM direction</td><td align="left" valign="top">&lt;0.01</td><td align="left" valign="top">&lt;0.01</td><td align="left" valign="top">&lt;0.01</td><td align="left" valign="top">&lt;0.01</td><td align="left" valign="top">0.04</td><td align="left" valign="top"/></tr><tr><td align="center" valign="middle"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/></tr><tr><td align="left" valign="top" colspan="7"><xref ref-type="fig" rid="F3">Fig. 3</xref>. <bold><italic>Bootstrap comparison of learning rates with TH vs AC representations</italic></bold></td></tr><tr><td align="left" valign="top">3e - freq</td><td align="left" valign="top">0.31</td><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/></tr><tr><td align="left" valign="top">3f - FM direction</td><td align="left" valign="top">&lt;0.01</td><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/></tr><tr><td align="center" valign="middle"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/></tr><tr><td align="left" valign="top" colspan="7"><xref ref-type="fig" rid="F10">Extended Data Fig. 6 d-g</xref> <bold><italic>Bootstrap comparison of rate correlations in structure X vs AC</italic></bold></td></tr><tr><td align="left" valign="top">ED6d-int PT</td><td align="left" valign="top">&lt;0.01</td><td align="left" valign="top">0.27</td><td align="left" valign="top">0.71</td><td align="left" valign="top">0.03</td><td align="left" valign="top">0.52</td><td align="left" valign="top"/></tr><tr><td align="left" valign="top">ED6e-AM direction</td><td align="left" valign="top">&lt;0.01</td><td align="left" valign="top">&lt;0.01</td><td align="left" valign="top">&lt;0.01</td><td align="left" valign="top">&lt;0.01</td><td align="left" valign="top">0.1</td><td align="left" valign="top"/></tr><tr><td align="left" valign="top">ED6f-int FM</td><td align="left" valign="top">&lt;0.01</td><td align="left" valign="top">&lt;0.01</td><td align="left" valign="top">&lt;0.01</td><td align="left" valign="top">&lt;0.01</td><td align="left" valign="top">&lt;0.01</td><td align="left" valign="top"/></tr><tr><td align="left" valign="top">ED6g-FM speed</td><td align="left" valign="top">&lt;0.01</td><td align="left" valign="top">0.04</td><td align="left" valign="top">&lt;0.01</td><td align="left" valign="top">0.12</td><td align="left" valign="top">0.22</td><td align="left" valign="top"/></tr></tbody></table></table-wrap></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary text</label><media xlink:href="EMS158907-supplement-Supplementary_text.pdf" mimetype="application" mime-subtype="pdf" id="d53aAdKbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S33"><title>Acknowledgments</title><p>We thank Maia Brunstein of the Hearing Institute Bioimaging Core Facility of C2RT/C2RA for help in acquiring Airyscan images of thalamocortical boutons in the auditory cortex and Alexander Kell for help implementing the word and music classification network. We also thank Yves Boubenec, Yves Frégnac, Andrew King, Srdjan Ostojic and Christine Petit for their feedback on the manuscript. We acknowledge the support of the Fondation pour l’Audition to the Institut de l’Audition.</p><p>We acknowledge the support of the following funding sources: Fondation pour l’Audition, FPA IDA02 (BB) and APA 2016-03 (BB) European Research Council, ERC CoG 770841 DEEPEN, (BB) Fondation pour la Recherche Médicale SPF202005011970 (SB) European Union’s Horizon 2020 research and innovation programme under grant agreement No 964568, project Hearlight (BB)</p></ack><fn-group><fn id="FN1" fn-type="con"><p id="P97"><bold>Author contributions:</bold> S.B., Ja.B., A.K. and B.B. conceived experiments, designed the study and interpreted data. S.B., Ja.B., A.K., T.T., J.S., A.V. and B.B. collected data and S.B., Ja.B., A.K., S.C. and B.B. performed data analysis. Je.B. and J.L.P conceived and implemented the cochlear model. B.B. and S.B. implemented the reinforcement learning model. S.B., K.B. and Y.G. implemented the deep learning models. S.B., Ja.B. and B.B. prepared figures. S.B. and B.B. wrote the manuscript. S.B and B.B. managed the project.</p></fn><fn id="FN2" fn-type="conflict"><p id="P98"><bold>Competing interests:</bold> Authors declare that they have no competing interests.</p></fn><fn id="FN3"><p id="P99"><bold>Materials &amp; Correspondence.</bold></p><p id="P100">All datasets are freely available at 10.12751/g-node.sz67di, hosted by G-Node Infrastructure. Custom codes used in this study are freely available at 10.12751/g-node.sz67di, hosted by G-Node Infrastructure. Further requests should be addressed to <email>brice.bathellier@pasteur.fr</email></p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neuhoff</surname><given-names>JG</given-names></name></person-group><article-title>Perceptual bias for rising tones</article-title><source>Nature</source><year>1998</year><volume>395</volume><fpage>123</fpage><lpage>4</lpage><pub-id pub-id-type="pmid">9744266</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nelken</surname><given-names>I</given-names></name><name><surname>Rotman</surname><given-names>Y</given-names></name><name><surname>Bar Yosef</surname><given-names>O</given-names></name></person-group><article-title>Responses of auditory-cortex neurons to structural features of natural sounds</article-title><source>Nature</source><year>1999</year><volume>397</volume><fpage>154</fpage><lpage>7</lpage><pub-id pub-id-type="pmid">9923676</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Butts</surname><given-names>DA</given-names></name><etal/></person-group><article-title>Temporal precision in the neural code and the timescales of natural vision</article-title><source>Nature</source><year>2007</year><volume>449</volume><fpage>92</fpage><lpage>95</lpage><pub-id pub-id-type="pmid">17805296</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johansson</surname><given-names>G</given-names></name></person-group><article-title>Visual perception of biological motion and a model for its analysis</article-title><source>Percept Psychophys</source><year>1973</year><volume>14</volume><fpage>201</fpage><lpage>211</lpage></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saberi</surname><given-names>K</given-names></name><name><surname>Perrott</surname><given-names>DR</given-names></name></person-group><article-title>Cognitive restoration of reversed speech</article-title><source>Nature</source><year>1999</year><volume>398</volume><fpage>760</fpage><pub-id pub-id-type="pmid">10235257</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ackels</surname><given-names>T</given-names></name><etal/></person-group><article-title>Fast odour dynamics are encoded in the olfactory system and guide behaviour</article-title><source>Nature</source><year>2021</year><volume>593</volume><fpage>558</fpage><lpage>563</lpage><pub-id pub-id-type="pmcid">PMC7611658</pub-id><pub-id pub-id-type="pmid">33953395</pub-id><pub-id pub-id-type="doi">10.1038/s41586-021-03514-2</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jadhav</surname><given-names>SP</given-names></name><name><surname>Wolfe</surname><given-names>J</given-names></name><name><surname>Feldman</surname><given-names>DE</given-names></name></person-group><article-title>Sparse temporal coding of elementary tactile features during active whisker sensation</article-title><source>Nat Neurosci</source><year>2009</year><volume>12</volume><fpage>792</fpage><lpage>800</lpage><pub-id pub-id-type="pmid">19430473</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosen</surname><given-names>S</given-names></name><name><surname>Carlyon</surname><given-names>RP</given-names></name><name><surname>Darwin</surname><given-names>CJ</given-names></name><name><surname>Russell</surname><given-names>IJ</given-names></name></person-group><article-title>Temporal information in speech: acoustic, auditory and linguistic aspects</article-title><source>Philos Trans R Soc Lond B Biol Sci</source><year>1992</year><volume>336</volume><fpage>367</fpage><lpage>373</lpage><pub-id pub-id-type="pmid">1354376</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cynader</surname><given-names>M</given-names></name><name><surname>Chernenko</surname><given-names>G</given-names></name></person-group><article-title>Abolition of Direction Selectivity in the Visual Cortex of the Cat</article-title><source>Science</source><year>1976</year><volume>193</volume><fpage>504</fpage><lpage>505</lpage><pub-id pub-id-type="pmid">941025</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fox</surname><given-names>NP</given-names></name><name><surname>Leonard</surname><given-names>M</given-names></name><name><surname>Sjerps</surname><given-names>MJ</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name></person-group><article-title>Transformation of a temporal speech cue to a spatial neural code in human auditory cortex</article-title><source>eLife</source><year>2020</year><volume>9</volume><elocation-id>e53051</elocation-id><pub-id pub-id-type="pmcid">PMC7556862</pub-id><pub-id pub-id-type="pmid">32840483</pub-id><pub-id pub-id-type="doi">10.7554/eLife.53051</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buračas</surname><given-names>GT</given-names></name><name><surname>Zador</surname><given-names>AM</given-names></name><name><surname>DeWeese</surname><given-names>MR</given-names></name><name><surname>Albright</surname><given-names>TD</given-names></name></person-group><article-title>Efficient Discrimination of Temporal Patterns by Motion-Sensitive Neurons in Primate Visual Cortex</article-title><source>Neuron</source><year>1998</year><volume>20</volume><fpage>959</fpage><lpage>969</lpage><pub-id pub-id-type="pmid">9620700</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nelken</surname><given-names>I</given-names></name><name><surname>Chechik</surname><given-names>G</given-names></name><name><surname>Mrsic-Flogel</surname><given-names>TD</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name><name><surname>Schnupp</surname><given-names>JWH</given-names></name></person-group><article-title>Encoding Stimulus Information by Spike Numbers and Mean Response Time in Primary Auditory Cortex</article-title><source>J Comput Neurosci</source><year>2005</year><volume>19</volume><fpage>199</fpage><lpage>221</lpage><pub-id pub-id-type="pmid">16133819</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walker</surname><given-names>KMM</given-names></name><name><surname>Bizley</surname><given-names>JK</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name><name><surname>Schnupp</surname><given-names>JWH</given-names></name></person-group><article-title>Multiplexed and Robust Representations of Sound Features in Auditory Cortex</article-title><source>J Neurosci</source><year>2011</year><volume>31</volume><fpage>14565</fpage><lpage>14576</lpage><pub-id pub-id-type="pmcid">PMC3272412</pub-id><pub-id pub-id-type="pmid">21994373</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2074-11.2011</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>JL</given-names></name><name><surname>Carta</surname><given-names>S</given-names></name><name><surname>Soldado-Magraner</surname><given-names>J</given-names></name><name><surname>Schneider</surname><given-names>BL</given-names></name><name><surname>Helmchen</surname><given-names>F</given-names></name></person-group><article-title>Behaviour-dependent recruitment of long-range projection neurons in somatosensory cortex</article-title><source>Nature</source><year>2013</year><volume>499</volume><fpage>336</fpage><lpage>340</lpage><pub-id pub-id-type="pmid">23792559</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Long</surname><given-names>KH</given-names></name><name><surname>Lieber</surname><given-names>JD</given-names></name><name><surname>Bensmaia</surname><given-names>SJ</given-names></name></person-group><article-title>Texture is encoded in precise temporal spiking patterns in primate somatosensory cortex</article-title><source>Nat Commun</source><year>2022</year><volume>13</volume><elocation-id>1311</elocation-id><pub-id pub-id-type="pmcid">PMC8921276</pub-id><pub-id pub-id-type="pmid">35288570</pub-id><pub-id pub-id-type="doi">10.1038/s41467-022-28873-w</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shusterman</surname><given-names>R</given-names></name><name><surname>Smear</surname><given-names>MC</given-names></name><name><surname>Koulakov</surname><given-names>AA</given-names></name><name><surname>Rinberg</surname><given-names>D</given-names></name></person-group><article-title>Precise olfactory responses tile the sniff cycle</article-title><source>Nat Neurosci</source><year>2011</year><volume>14</volume><fpage>1039</fpage><lpage>1044</lpage><pub-id pub-id-type="pmid">21765422</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>X</given-names></name><name><surname>Wehr</surname><given-names>M</given-names></name></person-group><article-title>A coding transformation for temporally structured sounds within auditory cortical neurons</article-title><source>Neuron</source><year>2015</year><volume>86</volume><fpage>292</fpage><lpage>303</lpage><pub-id pub-id-type="pmcid">PMC4393373</pub-id><pub-id pub-id-type="pmid">25819614</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2015.03.004</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scholl</surname><given-names>B</given-names></name><name><surname>Gao</surname><given-names>X</given-names></name><name><surname>Wehr</surname><given-names>M</given-names></name></person-group><article-title>Nonoverlapping sets of synapses drive on responses and off responses in auditory cortex</article-title><source>Neuron</source><year>2010</year><volume>65</volume><fpage>412</fpage><lpage>21</lpage><pub-id pub-id-type="pmcid">PMC3800047</pub-id><pub-id pub-id-type="pmid">20159453</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2010.01.020</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salinas</surname><given-names>E</given-names></name><name><surname>Hernández</surname><given-names>A</given-names></name><name><surname>Zainos</surname><given-names>A</given-names></name><name><surname>Romo</surname><given-names>R</given-names></name></person-group><article-title>Periodicity and Firing Rate As Candidate Neural Codes for the Frequency of Vibrotactile Stimuli</article-title><source>J Neurosci</source><year>2000</year><volume>20</volume><fpage>5503</fpage><lpage>5515</lpage><pub-id pub-id-type="pmcid">PMC6772326</pub-id><pub-id pub-id-type="pmid">10884334</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.20-14-05503.2000</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weber</surname><given-names>AI</given-names></name><etal/></person-group><article-title>Spatial and temporal codes mediate the tactile perception of natural textures</article-title><source>Proc Natl Acad Sci</source><year>2013</year><volume>110</volume><fpage>17107</fpage><lpage>17112</lpage><pub-id pub-id-type="pmcid">PMC3800989</pub-id><pub-id pub-id-type="pmid">24082087</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1305509110</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chong</surname><given-names>E</given-names></name><name><surname>Rinberg</surname><given-names>D</given-names></name></person-group><article-title>Behavioral readout of spatio-temporal codes in olfaction</article-title><source>Curr Opin Neurobiol</source><year>2018</year><volume>52</volume><fpage>18</fpage><lpage>24</lpage><pub-id pub-id-type="pmid">29694923</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Lu</surname><given-names>T</given-names></name><name><surname>Bendor</surname><given-names>D</given-names></name><name><surname>Bartlett</surname><given-names>E</given-names></name></person-group><article-title>Neural coding of temporal information in auditory thalamus and cortex</article-title><source>Neuroscience</source><year>2008</year><volume>154</volume><fpage>294</fpage><lpage>303</lpage><pub-id pub-id-type="pmid">19143093</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hillier</surname><given-names>D</given-names></name><etal/></person-group><article-title>Causal evidence for retina dependent and independent visual motion computations in mouse cortex</article-title><source>Nat Neurosci</source><year>2017</year><volume>20</volume><fpage>960</fpage><lpage>968</lpage><pub-id pub-id-type="pmcid">PMC5490790</pub-id><pub-id pub-id-type="pmid">28530661</pub-id><pub-id pub-id-type="doi">10.1038/nn.4566</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deneux</surname><given-names>T</given-names></name><name><surname>Kempf</surname><given-names>A</given-names></name><name><surname>Daret</surname><given-names>A</given-names></name><name><surname>Ponsot</surname><given-names>E</given-names></name><name><surname>Bathellier</surname><given-names>B</given-names></name></person-group><article-title>Temporal asymmetries in auditory coding and perception reflect multi-layered nonlinearities</article-title><source>Nat Commun</source><year>2016</year><volume>7</volume><elocation-id>12682</elocation-id><pub-id pub-id-type="pmcid">PMC5025791</pub-id><pub-id pub-id-type="pmid">27580932</pub-id><pub-id pub-id-type="doi">10.1038/ncomms12682</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aponte</surname><given-names>DA</given-names></name><etal/></person-group><article-title>Recurrent network dynamics shape direction selectivity in primary auditory cortex</article-title><source>Nat Commun</source><year>2021</year><volume>12</volume><fpage>314</fpage><pub-id pub-id-type="pmcid">PMC7804939</pub-id><pub-id pub-id-type="pmid">33436635</pub-id><pub-id pub-id-type="doi">10.1038/s41467-020-20590-6</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haddad</surname><given-names>R</given-names></name><etal/></person-group><article-title>Olfactory cortical neurons read out a relative time code in the olfactory bulb</article-title><source>Nat Neurosci</source><year>2013</year><volume>16</volume><fpage>949</fpage><lpage>957</lpage><pub-id pub-id-type="pmcid">PMC3695490</pub-id><pub-id pub-id-type="pmid">23685720</pub-id><pub-id pub-id-type="doi">10.1038/nn.3407</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ceballo</surname><given-names>S</given-names></name><etal/></person-group><article-title>Cortical recruitment determines learning dynamics and strategy</article-title><source>Nat Commun</source><year>2019</year><volume>10</volume><elocation-id>1479</elocation-id><pub-id pub-id-type="pmcid">PMC6443669</pub-id><pub-id pub-id-type="pmid">30931939</pub-id><pub-id pub-id-type="doi">10.1038/s41467-019-09450-0</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marshel</surname><given-names>JH</given-names></name><etal/></person-group><article-title>Cortical layer-specific critical dynamics triggering perception</article-title><source>Science</source><year>2019</year><volume>365</volume><pub-id pub-id-type="pmcid">PMC6711485</pub-id><pub-id pub-id-type="pmid">31320556</pub-id><pub-id pub-id-type="doi">10.1126/science.aaw5202</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ceballo</surname><given-names>S</given-names></name><name><surname>Piwkowska</surname><given-names>Z</given-names></name><name><surname>Bourg</surname><given-names>J</given-names></name><name><surname>Daret</surname><given-names>A</given-names></name><name><surname>Bathellier</surname><given-names>B</given-names></name></person-group><article-title>Targeted Cortical Manipulation of Auditory Perception</article-title><source>Neuron</source><year>2019</year><volume>104</volume><fpage>1168</fpage><lpage>1179</lpage><elocation-id>e5</elocation-id><pub-id pub-id-type="pmcid">PMC6926484</pub-id><pub-id pub-id-type="pmid">31727548</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2019.09.043</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carrillo-Reid</surname><given-names>L</given-names></name><name><surname>Han</surname><given-names>S</given-names></name><name><surname>Yang</surname><given-names>W</given-names></name><name><surname>Akrouh</surname><given-names>A</given-names></name><name><surname>Yuste</surname><given-names>R</given-names></name></person-group><article-title>Controlling Visually Guided Behavior by Holographic Recalling of Cortical Ensembles</article-title><source>Cell</source><year>2019</year><volume>178</volume><fpage>447</fpage><lpage>457</lpage><elocation-id>e5</elocation-id><pub-id pub-id-type="pmcid">PMC6747687</pub-id><pub-id pub-id-type="pmid">31257030</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2019.05.045</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smear</surname><given-names>M</given-names></name><name><surname>Shusterman</surname><given-names>R</given-names></name><name><surname>O’Connor</surname><given-names>R</given-names></name><name><surname>Bozza</surname><given-names>T</given-names></name><name><surname>Rinberg</surname><given-names>D</given-names></name></person-group><article-title>Perception of sniff phase in mouse olfaction</article-title><source>Nature</source><year>2011</year><volume>479</volume><fpage>397</fpage><lpage>400</lpage><pub-id pub-id-type="pmid">21993623</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gill</surname><given-names>JV</given-names></name><etal/></person-group><article-title>Precise Holographic Manipulation of Olfactory Circuits Reveals Coding Features Determining Perceptual Detection</article-title><source>Neuron</source><year>2020</year><volume>108</volume><fpage>382</fpage><lpage>393</lpage><elocation-id>e5</elocation-id><pub-id pub-id-type="pmcid">PMC8289117</pub-id><pub-id pub-id-type="pmid">32841590</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2020.07.034</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Znamenskiy</surname><given-names>P</given-names></name><name><surname>Zador</surname><given-names>AM</given-names></name></person-group><article-title>Corticostriatal neurons in auditory cortex drive decisions during auditory discrimination</article-title><source>Nature</source><year>2013</year><volume>497</volume><fpage>482</fpage><lpage>485</lpage><pub-id pub-id-type="pmcid">PMC3670751</pub-id><pub-id pub-id-type="pmid">23636333</pub-id><pub-id pub-id-type="doi">10.1038/nature12077</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berger</surname><given-names>KW</given-names></name></person-group><article-title>Some Factors in the Recognition of Timbre</article-title><source>J Acoust Soc Am</source><year>1964</year><volume>36</volume><fpage>1888</fpage><lpage>1891</lpage></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Musall</surname><given-names>S</given-names></name><etal/></person-group><article-title>Tactile frequency discrimination is enhanced by circumventing neocortical adaptation</article-title><source>Nat Neurosci</source><year>2014</year><volume>17</volume><fpage>1567</fpage><lpage>73</lpage><pub-id pub-id-type="pmid">25242306</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>DeWeese</surname><given-names>MR</given-names></name><name><surname>Otazu</surname><given-names>GH</given-names></name><name><surname>Zador</surname><given-names>AM</given-names></name></person-group><article-title>Millisecond-scale differences in neural activity in auditory cortex can drive decisions</article-title><source>Nat Neurosci</source><year>2008</year><volume>11</volume><fpage>1262</fpage><lpage>1263</lpage><pub-id pub-id-type="pmcid">PMC4062077</pub-id><pub-id pub-id-type="pmid">18849984</pub-id><pub-id pub-id-type="doi">10.1038/nn.2211</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>A</given-names></name><name><surname>Gire</surname><given-names>DH</given-names></name><name><surname>Bozza</surname><given-names>T</given-names></name><name><surname>Restrepo</surname><given-names>D</given-names></name></person-group><article-title>Precise Detection of Direct Glomerular Input Duration by the Olfactory Bulb</article-title><source>J Neurosci</source><year>2014</year><volume>34</volume><fpage>16058</fpage><lpage>16064</lpage><pub-id pub-id-type="pmcid">PMC4244471</pub-id><pub-id pub-id-type="pmid">25429146</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3382-14.2014</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Léger</surname><given-names>J-F</given-names></name><name><surname>Stern</surname><given-names>EA</given-names></name><name><surname>Aertsen</surname><given-names>A</given-names></name><name><surname>Heck</surname><given-names>D</given-names></name></person-group><article-title>Synaptic Integration in Rat Frontal Cortex Shaped by Network Activity</article-title><source>J Neurophysiol</source><year>2005</year><volume>93</volume><fpage>281</fpage><lpage>293</lpage><pub-id pub-id-type="pmid">15306631</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>London</surname><given-names>M</given-names></name><name><surname>Roth</surname><given-names>A</given-names></name><name><surname>Beeren</surname><given-names>L</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name></person-group><article-title>Sensitivity to perturbations in vivo implies high noise and suggests rate coding in cortex</article-title><source>Nature</source><year>2010</year><volume>466</volume><fpage>123</fpage><lpage>127</lpage><pub-id pub-id-type="pmcid">PMC2898896</pub-id><pub-id pub-id-type="pmid">20596024</pub-id><pub-id pub-id-type="doi">10.1038/nature09086</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verdier</surname><given-names>A</given-names></name><etal/></person-group><article-title>Enhanced perceptual task performance without deprivation in mice using medial forebrain bundle stimulation</article-title><source>Cell Rep Methods</source><year>2022</year><elocation-id>100355</elocation-id><pub-id pub-id-type="pmcid">PMC9795331</pub-id><pub-id pub-id-type="pmid">36590697</pub-id><pub-id pub-id-type="doi">10.1016/j.crmeth.2022.100355</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ohl</surname><given-names>FW</given-names></name><name><surname>Wetzel</surname><given-names>W</given-names></name><name><surname>Wagner</surname><given-names>T</given-names></name><name><surname>Rech</surname><given-names>A</given-names></name><name><surname>Scheich</surname><given-names>H</given-names></name></person-group><article-title>Bilateral Ablation of Auditory Cortex in Mongolian Gerbil Affects Discrimination of Frequency Modulated Tones but not of Pure Tones</article-title><source>Learn Mem</source><year>1999</year><volume>6</volume><fpage>347</fpage><lpage>362</lpage><pub-id pub-id-type="pmcid">PMC311295</pub-id><pub-id pub-id-type="pmid">10509706</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dalmay</surname><given-names>T</given-names></name><etal/></person-group><article-title>A Critical Role for Neocortical Processing of Threat Memory</article-title><source>Neuron</source><year>2019</year><volume>104</volume><fpage>1180</fpage><lpage>1194</lpage><elocation-id>e7</elocation-id><pub-id pub-id-type="pmid">31727549</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spearman</surname><given-names>C</given-names></name></person-group><article-title>The Proof and Measurement of Association between Two Things</article-title><source>Am J Psychol</source><year>1904</year><volume>15</volume><fpage>72</fpage><pub-id pub-id-type="pmid">3322052</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Asokan</surname><given-names>MM</given-names></name><name><surname>Williamson</surname><given-names>RS</given-names></name><name><surname>Hancock</surname><given-names>KE</given-names></name><name><surname>Polley</surname><given-names>DB</given-names></name></person-group><article-title>Inverted central auditory hierarchies for encoding local intervals and global temporal patterns</article-title><source>Curr Biol</source><year>2021</year><volume>31</volume><fpage>1762</fpage><lpage>1770</lpage><elocation-id>e4</elocation-id><pub-id pub-id-type="pmcid">PMC8085059</pub-id><pub-id pub-id-type="pmid">33609455</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2021.01.076</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willmore</surname><given-names>B</given-names></name><name><surname>Tolhurst</surname><given-names>DJ</given-names></name></person-group><article-title>Characterizing the sparseness of neural codes</article-title><source>Netw Bristol Engl</source><year>2001</year><volume>12</volume><fpage>255</fpage><lpage>270</lpage><pub-id pub-id-type="pmid">11563529</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kopp-Scheinpflug</surname><given-names>C</given-names></name><name><surname>Sinclair</surname><given-names>JL</given-names></name><name><surname>Linden</surname><given-names>JF</given-names></name></person-group><article-title>When Sound Stops: Offset Responses in the Auditory System</article-title><source>Trends Neurosci</source><year>2018</year><volume>41</volume><fpage>712</fpage><lpage>728</lpage><pub-id pub-id-type="pmid">30274606</pub-id></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname><given-names>AJ</given-names></name><name><surname>Nelken</surname><given-names>I</given-names></name></person-group><article-title>Unraveling the principles of auditory cortical processing: can we learn from the visual system?</article-title><source>Nat Neurosci</source><year>2009</year><volume>12</volume><fpage>698</fpage><lpage>701</lpage><pub-id pub-id-type="pmcid">PMC3657701</pub-id><pub-id pub-id-type="pmid">19471268</pub-id><pub-id pub-id-type="doi">10.1038/nn.2308</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>deCharms</surname><given-names>RC</given-names></name><name><surname>Blake</surname><given-names>DT</given-names></name><name><surname>Merzenich</surname><given-names>MM</given-names></name></person-group><article-title>Optimizing Sound Features for Cortical Neurons</article-title><source>Science</source><year>1998</year><volume>280</volume><fpage>1439</fpage><lpage>1444</lpage><pub-id pub-id-type="pmid">9603734</pub-id></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bathellier</surname><given-names>B</given-names></name><name><surname>Tee</surname><given-names>SP</given-names></name><name><surname>Hrovat</surname><given-names>C</given-names></name><name><surname>Rumpel</surname><given-names>S</given-names></name></person-group><article-title>A multiplicative reinforcement learning model capturing learning dynamics and interindividual variability in mice</article-title><source>Proc Natl Acad Sci</source><year>2013</year><volume>110</volume><fpage>19950</fpage><lpage>19955</lpage><pub-id pub-id-type="pmcid">PMC3856837</pub-id><pub-id pub-id-type="pmid">24255115</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1312125110</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>W-X</given-names></name><name><surname>Schmidt</surname><given-names>R</given-names></name><name><surname>Wickens</surname><given-names>JR</given-names></name><name><surname>Hyland</surname><given-names>BI</given-names></name></person-group><article-title>Dopamine Cells Respond to Predicted Events during Classical Conditioning: Evidence for Eligibility Traces in the Reward-Learning Network</article-title><source>J Neurosci</source><year>2005</year><volume>25</volume><fpage>6235</fpage><lpage>6242</lpage><pub-id pub-id-type="pmcid">PMC6725057</pub-id><pub-id pub-id-type="pmid">15987953</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1478-05.2005</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gerstner</surname><given-names>W</given-names></name><name><surname>Lehmann</surname><given-names>M</given-names></name><name><surname>Liakoni</surname><given-names>V</given-names></name><name><surname>Corneil</surname><given-names>D</given-names></name><name><surname>Brea</surname><given-names>J</given-names></name></person-group><article-title>Eligibility Traces and Plasticity on Behavioral Time Scales: Experimental Support of NeoHebbian Three-Factor Learning Rules</article-title><source>Front Neural Circuits</source><year>2018</year><volume>12</volume><pub-id pub-id-type="pmcid">PMC6079224</pub-id><pub-id pub-id-type="pmid">30108488</pub-id><pub-id pub-id-type="doi">10.3389/fncir.2018.00053</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yagishita</surname><given-names>S</given-names></name><etal/></person-group><article-title>A critical time window for dopamine actions on the structural plasticity of dendritic spines</article-title><source>Science</source><year>2014</year><volume>345</volume><fpage>1616</fpage><lpage>1620</lpage><pub-id pub-id-type="pmcid">PMC4225776</pub-id><pub-id pub-id-type="pmid">25258080</pub-id><pub-id pub-id-type="doi">10.1126/science.1255514</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>H</given-names></name><etal/></person-group><article-title>Phasic Off responses of auditory cortex underlie perception of sound duration</article-title><source>Cell Rep</source><year>2021</year><volume>35</volume><pub-id pub-id-type="pmcid">PMC8154544</pub-id><pub-id pub-id-type="pmid">33882311</pub-id><pub-id pub-id-type="doi">10.1016/j.celrep.2021.109003</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><article-title>Deep learning</article-title><source>Nature</source><year>2015</year><volume>521</volume><fpage>436</fpage><lpage>44</lpage><pub-id pub-id-type="pmid">26017442</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kell</surname><given-names>AJE</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Shook</surname><given-names>EN</given-names></name><name><surname>Norman-Haignere</surname><given-names>SV</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name></person-group><article-title>A Task-Optimized Neural Network Replicates Human Auditory Behavior, Predicts Brain Responses, and Reveals a Cortical Processing Hierarchy</article-title><source>Neuron</source><year>2018</year><volume>98</volume><fpage>630</fpage><lpage>644</lpage><elocation-id>e16</elocation-id><pub-id pub-id-type="pmid">29681533</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Slonina</surname><given-names>ZA</given-names></name><name><surname>Poole</surname><given-names>KC</given-names></name><name><surname>Bizley</surname><given-names>JK</given-names></name></person-group><article-title>What can we learn from inactivation studies? Lessons from auditory cortex</article-title><source>Trends Neurosci</source><year>2022</year><volume>45</volume><fpage>64</fpage><lpage>77</lpage><pub-id pub-id-type="pmcid">PMC8897194</pub-id><pub-id pub-id-type="pmid">34799134</pub-id><pub-id pub-id-type="doi">10.1016/j.tins.2021.10.005</pub-id></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Landemard</surname><given-names>A</given-names></name><etal/></person-group><article-title>Distinct higher-order representations of natural sounds in human and ferret auditory cortex</article-title><source>eLife</source><year>2021</year><volume>10</volume><elocation-id>e65566</elocation-id><pub-id pub-id-type="pmcid">PMC8601661</pub-id><pub-id pub-id-type="pmid">34792467</pub-id><pub-id pub-id-type="doi">10.7554/eLife.65566</pub-id></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drew</surname><given-names>PJ</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><article-title>Extending the effects of spike-timing-dependent plasticity to behavioral timescales</article-title><source>Proc Natl Acad Sci</source><year>2006</year><volume>103</volume><fpage>8876</fpage><lpage>8881</lpage><pub-id pub-id-type="pmcid">PMC1470971</pub-id><pub-id pub-id-type="pmid">16731625</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0600676103</pub-id></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gutig</surname><given-names>R</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><article-title>The tempotron: a neuron that learns spike timing-based decisions</article-title><source>Nat Neurosci</source><year>2006</year><volume>9</volume><fpage>420</fpage><lpage>8</lpage><pub-id pub-id-type="pmid">16474393</pub-id></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gjorgjieva</surname><given-names>J</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name><name><surname>Audet</surname><given-names>J</given-names></name><name><surname>Pfister</surname><given-names>JP</given-names></name></person-group><article-title>A triplet spike-timing-dependent plasticity model generalizes the Bienenstock-Cooper-Munro rule to higher-order spatiotemporal correlations</article-title><source>Proc Natl Acad Sci U A</source><year>2011</year><volume>108</volume><fpage>19383</fpage><lpage>8</lpage><pub-id pub-id-type="pmcid">PMC3228426</pub-id><pub-id pub-id-type="pmid">22080608</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1105933108</pub-id></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Branco</surname><given-names>T</given-names></name><name><surname>Clark</surname><given-names>BA</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name></person-group><article-title>Dendritic Discrimination of Temporal Input Sequences in Cortical Neurons</article-title><source>Science</source><year>2010</year><volume>329</volume><fpage>1671</fpage><lpage>1675</lpage><pub-id pub-id-type="pmcid">PMC6354899</pub-id><pub-id pub-id-type="pmid">20705816</pub-id><pub-id pub-id-type="doi">10.1126/science.1189664</pub-id></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sivyer</surname><given-names>B</given-names></name><name><surname>Williams</surname><given-names>SR</given-names></name></person-group><article-title>Direction selectivity is computed by active dendritic integration in retinal ganglion cells</article-title><source>Nat Neurosci</source><year>2013</year><volume>16</volume><fpage>1848</fpage><lpage>1856</lpage><pub-id pub-id-type="pmid">24162650</pub-id></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markram</surname><given-names>H</given-names></name><name><surname>Lubke</surname><given-names>J</given-names></name><name><surname>Frotscher</surname><given-names>M</given-names></name><name><surname>Sakmann</surname><given-names>B</given-names></name></person-group><article-title>Regulation of synaptic efficacy by coincidence of postsynaptic APs and EPSPs</article-title><source>Science</source><year>1997</year><volume>275</volume><fpage>213</fpage><lpage>5</lpage><pub-id pub-id-type="pmid">8985014</pub-id></element-citation></ref><ref id="R64"><label>64</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singh</surname><given-names>NC</given-names></name><name><surname>Theunissen</surname><given-names>FE</given-names></name></person-group><article-title>Modulation spectra of natural sounds and ethological theories of auditory processing</article-title><source>J Acoust Soc Am</source><year>2003</year><volume>114</volume><fpage>3394</fpage><lpage>3411</lpage><pub-id pub-id-type="pmid">14714819</pub-id></element-citation></ref><ref id="R65"><label>65</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Toyoizumi</surname><given-names>T</given-names></name><name><surname>Pfister</surname><given-names>JP</given-names></name><name><surname>Aihara</surname><given-names>K</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><article-title>Generalized Bienenstock-Cooper-Munro rule for spiking neurons that maximizes information transmission</article-title><source>Proc Natl Acad Sci U A</source><year>2005</year><volume>102</volume><fpage>5239</fpage><lpage>44</lpage><pub-id pub-id-type="pmcid">PMC555686</pub-id><pub-id pub-id-type="pmid">15795376</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0500495102</pub-id></element-citation></ref><ref id="R66"><label>66</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graupner</surname><given-names>M</given-names></name><name><surname>Wallisch</surname><given-names>P</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name></person-group><article-title>Natural Firing Patterns Imply Low Sensitivity of Synaptic Plasticity to Spike Timing Compared with Firing Rate</article-title><source>J Neurosci</source><year>2016</year><volume>36</volume><fpage>11238</fpage><lpage>11258</lpage><pub-id pub-id-type="pmcid">PMC5148241</pub-id><pub-id pub-id-type="pmid">27807166</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0104-16.2016</pub-id></element-citation></ref><ref id="R67"><label>67</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hromadka</surname><given-names>T</given-names></name><name><surname>Deweese</surname><given-names>MR</given-names></name><name><surname>Zador</surname><given-names>AM</given-names></name></person-group><article-title>Sparse representation of sounds in the unanesthetized auditory cortex</article-title><source>PLoS Biol</source><year>2008</year><volume>6</volume><fpage>e16</fpage><pub-id pub-id-type="pmcid">PMC2214813</pub-id><pub-id pub-id-type="pmid">18232737</pub-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0060016</pub-id></element-citation></ref></ref-list><ref-list><title>Method references</title><ref id="R68"><label>68</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friedrich</surname><given-names>RW</given-names></name><name><surname>Laurent</surname><given-names>G</given-names></name></person-group><article-title>Dynamic optimization of odor representations by slow temporal patterning of mitral cell activity</article-title><source>Science</source><year>2001</year><volume>291</volume><fpage>889</fpage><lpage>94</lpage><pub-id pub-id-type="pmid">11157170</pub-id></element-citation></ref><ref id="R69"><label>69</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>S</given-names></name><etal/></person-group><article-title>Large-scale two-photon imaging revealed super-sparse population codes in the V1 superficial layer of awake monkeys</article-title><source>eLife</source><year>2018</year><volume>7</volume><elocation-id>e33370</elocation-id><pub-id pub-id-type="pmcid">PMC5953536</pub-id><pub-id pub-id-type="pmid">29697371</pub-id><pub-id pub-id-type="doi">10.7554/eLife.33370</pub-id></element-citation></ref><ref id="R70"><label>70</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolls</surname><given-names>ET</given-names></name><name><surname>Tovee</surname><given-names>MJ</given-names></name></person-group><article-title>Sparseness of the neuronal representation of stimuli in the primate temporal visual cortex</article-title><source>J Neurophysiol</source><year>1995</year><volume>73</volume><fpage>713</fpage><lpage>726</lpage><pub-id pub-id-type="pmid">7760130</pub-id></element-citation></ref><ref id="R71"><label>71</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bourien</surname><given-names>J</given-names></name><etal/></person-group><article-title>Contribution of auditory nerve fibers to compound action potential of the auditory nerve</article-title><source>J Neurophysiol</source><year>2014</year><volume>112</volume><fpage>1025</fpage><lpage>1039</lpage><pub-id pub-id-type="pmid">24848461</pub-id></element-citation></ref><ref id="R72"><label>72</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meddis</surname><given-names>R</given-names></name></person-group><article-title>Auditory-nerve first-spike latency and auditory absolute threshold: A computer model</article-title><source>J Acoust Soc Am</source><year>2006</year><volume>119</volume><fpage>406</fpage><lpage>417</lpage><pub-id pub-id-type="pmid">16454295</pub-id></element-citation></ref><ref id="R73"><label>73</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taberner</surname><given-names>AM</given-names></name><name><surname>Liberman</surname><given-names>MC</given-names></name></person-group><article-title>Response Properties of Single Auditory Nerve Fibers in the Mouse</article-title><source>J Neurophysiol</source><year>2005</year><volume>93</volume><fpage>557</fpage><lpage>569</lpage><pub-id pub-id-type="pmid">15456804</pub-id></element-citation></ref><ref id="R74"><label>74</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Romero</surname><given-names>S</given-names></name><etal/></person-group><article-title>Cellular and Widefield Imaging of Sound Frequency Organization in Primary and Higher Order Fields of the Mouse Auditory Cortex</article-title><source>Cereb Cortex</source><year>2020</year><volume>30</volume><fpage>1603</fpage><lpage>1622</lpage><pub-id pub-id-type="pmcid">PMC7132909</pub-id><pub-id pub-id-type="pmid">31667491</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhz190</pub-id></element-citation></ref><ref id="R75"><label>75</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nahmani</surname><given-names>M</given-names></name><name><surname>Erisir</surname><given-names>A</given-names></name></person-group><article-title>VGluT2 immunochemistry identifies thalamocortical terminals in layer 4 of adult and developing visual cortex</article-title><source>J Comp Neurol</source><year>2005</year><volume>484</volume><fpage>458</fpage><lpage>473</lpage><pub-id pub-id-type="pmid">15770654</pub-id></element-citation></ref><ref id="R76"><label>76</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yaksi</surname><given-names>E</given-names></name><name><surname>Friedrich</surname><given-names>RW</given-names></name></person-group><article-title>Reconstruction of firing rate changes across neuronal populations by temporally deconvolved Ca2+ imaging</article-title><source>Nat Methods</source><year>2006</year><volume>3</volume><fpage>377</fpage><lpage>383</lpage><pub-id pub-id-type="pmid">16628208</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Fig. 1</label><caption><title>Sensory-motor learning requires spatial representations.</title><p><bold>a.</bold> Sketch of patterned optogenetic experiment in AC (MFB: medial forebrain bundle) and cranial window from an example mouse showing the location of the stimulation spots in the tonotopic axis of the primary auditory field. <bold>b.</bold> Sketch of the optogenetic stimulation time courses for each discrimination task. <bold>c.</bold> Sketch illustrating the conversion of a temporal code into a spatial code in the cortical network. <bold>d.</bold> AC window with 64 channel silicone probe inserted via a hole in the coverglass (top right) to record single unit responses to light patterns used in the behavioural task and illustrative data from 3 channels. <bold>e.</bold> Z-scored responses of 321 neurons to A, B, AB and BA stimulations, ordered by preference for A vs B stimulation and difference in average firing rate between A and B and between AB and BA stimulations. <bold>f.</bold> Accuracy of a neural decoder trained to discriminate the temporal and spatial optogenetic patterns based only on spatial information, i.e. time-averaged firing rates of each neuron (n=321 units, bootstrap over units, p-value of accuracy vs chance level of 0.5: 0.01, 0.01, 0.01, 0.43). <bold>g.</bold> Sample lick traces (top) and mean lick signal (bottom) for Go and NoGo trials in the rate-coded (left) and temporal-coded (right) discrimination tasks. <bold>h.</bold> Learning curves for all mice performing each task (n=7, error bars are sem). <bold>i.</bold> Accuracy at 3000 trials for all mice. (paired Wilcoxon test, p = 0.032, signed rank value = 27, n=7).</p></caption><graphic xlink:href="EMS158907-f001"/></fig><fig id="F2" position="float"><label>Fig. 2</label><caption><title>A spatial code for temporal cues emerges in the auditory cortex.</title><p><bold>a.</bold> Sketch of the auditory system and sample sizes at each level. <bold>b.</bold> Spectrograms of the sound set. <bold>c.</bold> Sample responses to up and down frequency sweeps from IC and AC neurons ordered by response amplitude. <bold>d.</bold> Responses of 4 AC neurons to different up and down frequency sweeps illustrating how spatio-temporal and spatial codes are extracted. <bold>e-f.</bold> Mean sound decoding accuracy for spatial-temporal and spatial codes in each area (<bold>e</bold>) and normalised difference between the two (<bold>f</bold>). (p-value for 100 bootstraps, error bars are S.D). <bold>g.</bold> Noise-corrected RSA matrices for all sound pairs for spatio-temporal (left) or spatial (right) codes in IC and AC. <bold>h.</bold> Mean noise-corrected correlation by area. (p-value for 100 bootstraps comparing rate correlation of each region to AC, error bars are bootstrapped S.D). <bold>i.</bold> Normalised difference between mean noise-corrected correlation for spatio-temporal and spatial codes. (p-value for 100 bootstraps, error bars are S.D). <bold>j.</bold> Noise-corrected dissimilarity between RSA matrix structure of spatio-temporal and spatial codes. (p-value for 100 bootstraps, error bars are S.D). <bold>k.</bold> (top) Sketch illustrating the decomposition of population responses by timescale and the concatenation of successive Fourier coefficients to accumulate increasingly fine timescales. (bottom) Mean decoding accuracy based on cumulative Fourier coefficients of neural responses. Full statistics are reported in <xref ref-type="table" rid="T3">Extended Data Table 3</xref>.</p></caption><graphic xlink:href="EMS158907-f002"/></fig><fig id="F3" position="float"><label>Fig. 3</label><caption><title>A spatial code is necessary for reinforcement learning with a bio-inspired eligibility trace mechanism.</title><p><bold>a.</bold> Sketch of the reinforcement learning model (bottom left), eligibility trace dynamics (top left) and example learning curves for two recorded representations that have similar spatio-temporal correlations but different spatial correlations. <bold>b.</bold> Heatmap of the number of trials needed to reach 80% accuracy at discriminating between a pair of sounds as a function of the correlations of their spatio-temporal and spatial representations. The colour map indicates learning duration averaged over all pairs of representations for all brain regions. <bold>c.</bold> Number of trials to 80% accuracy as a function of the correlations of their spatial representations. Large square dots show the mean correlation and learning time for time-symmetric frequency sweeps in IC, TH and AC and the black line shows the fit to data. <bold>d.</bold> Sketch showing the thalamic and cortical pathways for auditory learning. <bold>e.</bold> Mean noise-corrected correlation between representations of sound pairs differing only by frequency (0.33 octave difference) and predicted duration for learning a pure tone discrimination task based on thalamic (average of THe and TH2P) and cortical representations. <bold>f.</bold> Mean noise-corrected correlation between representations of sound pairs differing only by the direction of the frequency sweep and predicted duration for learning to discriminate the two frequency sweep directions based on thalamic (average of THe and TH2P) and cortical representations. Full statistics are reported in <xref ref-type="table" rid="T3">Extended Data Table 3</xref>.</p></caption><graphic xlink:href="EMS158907-f003"/></fig><fig id="F4" position="float"><label>Fig. 4</label><caption><title>Categorization deep networks implement a spatial code for temporal cues in deeper layers.</title><p><bold>a-b.</bold> (Left) Schematic of CNN architectures and target categories. (Right) Mean response correlations for the spatial and spatio-temporal codes from RSA matrices constructed with the set of 140 sounds presented to mice (line) and difference between the two codes (bars). <bold>a.</bold> Multi-category CNN (n=8 networks). <bold>b.</bold> Multi-category CNN without shrinking of the temporal dimension (n=8 networks). Inset shows learning curves from training epochs for networks in A and B. <bold>c-e.</bold> All graphs refer to the categorization CNN without temporal pooling and reproduce analysis shown in <xref ref-type="fig" rid="F2">Fig. 2</xref> for neural data. <bold>c.</bold> Normalised difference between mean sound decoding accuracy for spatio-temporal and spatial codes. (error bars are sem over trained networks). <bold>d.</bold> Noise-corrected dissimilarity between RSA matrix structure of spatial and spatio-temporal codes. <bold>e.</bold> Mean decoding accuracy based on cumulative Fourier coefficients of neural responses. <bold>f.</bold> Autoencoder CNN performing sound compression and denoising through a 20-unit bottleneck. (cv : convolution block, d-cv : deconvolution block - see <xref ref-type="sec" rid="S8">methods</xref> for architecture details).</p></caption><graphic xlink:href="EMS158907-f004"/></fig></floats-group></article>