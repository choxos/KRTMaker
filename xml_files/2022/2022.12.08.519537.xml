<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS158469</article-id><article-id pub-id-type="doi">10.1101/2022.12.08.519537</article-id><article-id pub-id-type="archive">PPR583194</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Graph neural networks learn emergent tissue properties from spatial molecular profiles</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Fischer</surname><given-names>David S.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Ali</surname><given-names>Mayar</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Richter</surname><given-names>Sabrina</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Ertürk</surname><given-names>Ali</given-names></name><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Theis</surname><given-names>Fabian</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A5">5</xref><xref ref-type="corresp" rid="CR1">+</xref></contrib></contrib-group><aff id="A1"><label>1</label>Institute of Computational Biology, Helmholtz Zentrum München, 85764 Neuherberg, Germany</aff><aff id="A2"><label>2</label>TUM School of Life Sciences Weihenstephan, Technical University of Munich, 85354 Freising, Germany</aff><aff id="A3"><label>3</label>Institute for Tissue Engineering and Regenerative Medicine, Helmholtz Zentrum München, 85764 Neuherberg, Germany</aff><aff id="A4"><label>4</label>Graduate School of Systemic Neurosciences, Ludwig Maximilian University of Munich, 80539 Munich, Germany</aff><aff id="A5"><label>5</label>Department of Mathematics, Technical University of Munich, 85748 Garching bei München, Germany</aff><author-notes><corresp id="CR1"><label>+</label> correspondence to <email>fabian.theis@helmholtz-muenchen.de</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>13</day><month>12</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>11</day><month>12</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Tissue phenotypes such as metabolic states, inflammation, and tumor properties are functions of molecular states of cells that constitute the tissue. Recent spatial molecular profiling assays measure tissue architecture motifs in a molecular and often unbiased way and thus can explain some aspects of emergence of these phenotypes. Here, we characterize the ability of graph neural networks to model tissue-level emergent phenotypes based on spatial data by evaluating phenotype prediction across model complexities. First, we show that immune cell dispersion in colorectal tumors, which is known to be predictive of disease outcome, can be captured by graph neural networks. Second, we show that breast cancer tumor classes can be predicted from gene expression alone without spatial information and are thus too simplistic a phenotype to require a complex model of emergence. Third, we show that representation learning approaches for spatial graphs of molecular profiles are limited by overfitting in the prevalent regime of up to 100s of images per study. We address overfitting with within-graph self-supervision and illustrate its promise for tissue representation learning as a constraint for node representations.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">The high molecular resolution provided by single-cell RNA-seq (scRNA-seq) has put the cell as a functional unit in the focus of recent advances in tissue biology<sup><xref ref-type="bibr" rid="R1">1</xref></sup>. However, interactions between cells and emergent properties of the tissue beyond the length scale of a cell are largely lost in assays that are based on dissociated tissues. Spatial molecular profiling assays with single-cell resolution can fill this gap between cell and tissue biology by providing descriptions of tissues as graphs of cells<sup><xref ref-type="bibr" rid="R2">2</xref></sup>. For example, emergent properties of cells in tissue niches can be modeled in such spatial graphs<sup><xref ref-type="bibr" rid="R3">3</xref></sup>. Sample-level labels often reflect complex tissue phenotypes, such as metabolic properties or disease states. In the absence of spatial information, a supervised model on sample-level labels can detect cell states and frequencies thereof that correlate with the tissue phenotype. Spatial information and graphs of cells extend the capability of such supervised models to not only detect cellular phenotypes that correlate with the labels, but also motifs of tissue architecture, such as cellular niches<sup><xref ref-type="bibr" rid="R4">4</xref></sup>. Because of their explicit representation of cells as constituent building blocks of a tissue, graph neural networks promise to be more interpretable with respect to niches than convolutional neural networks on tissue images and, indeed, have been recently successfully deployed for tumor phenotype prediction from spatial proteomics data<sup><xref ref-type="bibr" rid="R5">5</xref>–<xref ref-type="bibr" rid="R7">7</xref></sup>. Here, we perform a comparative ablation study over spatial features and single-cell resolution for graph neural networks that predict tumor phenotypes from spatial graphs with gene expression or categorical cell type node features. We use this analysis to show that spatial motives of cell types that are predictive of tumor labels are encoded in cell-wise gene expression, thus delineating whether spatial edges need to be modeled.</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>Graph neural networks model tissue phenotypes</title><p id="P3">We introduce graph neural networks over spatial graphs of cells with an aggregation function across node states to model graph-level labels that represent tissue phenotypes (<xref ref-type="fig" rid="F1">Fig. 1a</xref>). The input node features can either be gene expression vectors per cell or coarsened gene expression features, such as categorical cell type labels (<xref ref-type="fig" rid="F1">Fig. 1b</xref>). This model class captures both molecular information of individual cells as input node feature vectors and the architecture of the tissue through the spatial proximity graph. Previously, such graph neural network architectures have been used to model tumor grades based on H&amp;E images<sup><xref ref-type="bibr" rid="R8">8</xref></sup> and spatial proteomics<sup><xref ref-type="bibr" rid="R5">5</xref>–<xref ref-type="bibr" rid="R7">7</xref></sup>. We consider examples of graph-level supervision on three data sets: Two large cohorts of imaging mass cytometry (IMC) of breast cancer biopsy data stratified by tumor grade and other tumor characteristics (<italic>IMC - Jackson</italic><sup><xref ref-type="bibr" rid="R9">9</xref></sup> with 559 images from 350 patients and <italic>IMC - METABRIC</italic><sup><xref ref-type="bibr" rid="R9">9</xref></sup> with 500 images from 454 patients) and a cohort of CODEX samples of colorectal cancer biopsies stratified by tumor classification and disease outcome (<italic>CODEX - colorectal cancer</italic><sup><xref ref-type="bibr" rid="R10">10</xref></sup>, 140 images from 35 patients). We focussed on tumor grade prediction on the <italic>IMC - Jackson</italic> and <italic>IMC - METABRIC</italic> datasets. In these published breast cancer data sets, the grade label was previously assigned by a pathologist to the tumors based on standard histology and the tumor section assayed with IMC were chosen to represent this decision. In both cases, we considered the problem of distinguishing grade 1, 2 and 3 tumors without introducing class weighting. We considered the prediction of binary tumor classes defined based on the presence of diffuse inflammatory infiltration in the <italic>CODEX - colorectal cancer</italic> data set (<xref ref-type="fig" rid="F1">Fig. 1c</xref>). In all data sets, we performed training, validation and test data splits by patients and grouped all images of a patient into one partition. We chose neighborhood sizes for spatial graphs (resolution) based on the average node degree distribution<sup><xref ref-type="bibr" rid="R3">3</xref></sup> (<xref ref-type="supplementary-material" rid="SD1">Supp. Fig. 1</xref>). We standardized all features globally across all data as we found this to best reflect cell type and condition information (<xref ref-type="supplementary-material" rid="SD1">Supp. Fig. 2</xref>).</p></sec><sec id="S4"><title>Predictive features of tumor architecture are defined by an ablation study</title><p id="P4">When applying the GNN models to the three tumor settings described above, we found graph neural networks with node state aggregation to be predictive of the considered tissue labels (<xref ref-type="fig" rid="F2">Fig. 2</xref>). We performed an ablation study over single-cell and spatial information for a tissue phenotype to investigate if these models require the spatial proximity graph, thus testing if they capture emergent properties of tissue architecture. We designed the ablation based on two baseline experiments, omitting spatial structure and cell resolution from spatial graphs of single cells (<xref ref-type="fig" rid="F2">Fig. 2b</xref>). The first baseline model uses pseudo bulk data, which consists of mean gene expression vectors across an entire image. We used fully connected dense neural networks including linear models (MLP) to predict tissue-level phenotypes on pseudo-bulk data (Online Methods). The second baseline model uses <italic>in silico</italic> dissociated single-cell data, which corresponds to removing the observed cellular gene expression vectors from their spatial context to yield independent observations. We modeled this <italic>in silico</italic> dissociated single-cell data with a multi-instance (MI) model that aggregate cell-wise embeddings (Online Methods). We compared the performance of these baseline models with spatial graph convolutional networks (GCN) that leverages spatial information (<xref ref-type="fig" rid="F1">Fig. 1c</xref>). We considered GCNs trained with cross entropy loss functions on the tumor phenotype. In addition, we trained GCNs with an additional self-supervision task (GCN-SS). This self-supervision task consists of predicting cell type composition in adjacent regions in the graph from local graph embeddings, in analogy to predicting held-out patches in self-supervision in images (Online Methods).</p><p id="P5">First, we considered one-hot encoded cell type labels as input node features. MI models did not outperform pseudo-bulk models on coarse binary cell type labels (immune and non-immune cells, <xref ref-type="fig" rid="F2">Fig. 2b</xref>) and on more fine-grained author-supplied labels (<xref ref-type="supplementary-material" rid="SD1">Supp. Fig. 3a-d</xref>). It was previously reported that the spatial distribution of immune cells in colorectal cancer is predictive of disease outcome and is used to stratify tumors<sup><xref ref-type="bibr" rid="R5">5</xref>,<xref ref-type="bibr" rid="R10">10</xref></sup>. Indeed, we found that an MLP trained on an immune cell dispersion estimate per image was more predictive of tumor class in test images of colorectal cancer than the baseline pseudo-bulk model that only reflects composition but not architecture (<xref ref-type="fig" rid="F2">Fig. 2c</xref>, Online Methods). This performance was matched by GCN and GCN-SS (<xref ref-type="fig" rid="F2">Fig. 2c</xref>), demonstrating that these models can independently detect this tissue architecture feature. In contrast, the spatial distribution of immune cells in breast cancer was not predictive of tumor class (<xref ref-type="fig" rid="F2">Fig. 2c</xref>), potentially reflecting differences in tumor biology and measurement characteristics.</p><p id="P6">Cell type labels are a coarsening of cell-wise gene expression measurements. Within-cell type gene expression variation is in parts explained by spatial niches<sup><xref ref-type="bibr" rid="R3">3</xref></sup>. We considered gene expression as input feature vectors to ablate over spatial features in the absence of confounding by cell type classification choices. First, we found that MI models outperform pseudo-bulk models on test images of the <italic>IMC - Jackson</italic> dataset significantly (average accuracy difference 0.105), not significantly on the <italic>IMC - METABRIC</italic> and <italic>CODEX - colorectal cancer</italic> datasets (average accuracy difference 0.081 and 0.094, respectively). This performance of MI models indicates that information on tissue-level phenotypes is captured by single-cell-resolved data (<xref ref-type="fig" rid="F2">Fig. 2c</xref>). Second, we found that GCN models do not outperform MI models in the same setting (<xref ref-type="fig" rid="F2">Fig. 2c</xref>). This result shows that predictors of the considered tissue phenotypes are sufficiently captured by non-spatial single-cell gene expression states. One interpretation of this result is that the predictive tissue architecture motives are already encoded in cell-wise gene expression vectors, which would be consistent with the previously demonstrated ability of spatial graphs to explain parts of within-cell-type gene expression variation<sup><xref ref-type="bibr" rid="R3">3</xref></sup>. Indeed, MI models were significantly better when trained on gene expression vectors as opposed to one-hot encoded cell type node representations on the <italic>IMC - Jackson</italic> data and <italic>IMC - METABRIC</italic> data (<xref ref-type="supplementary-material" rid="SD1">Supp. Fig. 3e</xref>), demonstrating that this information inherent single-cell resolution is lost upon coarse graining gene expression to cell types.</p><p id="P7">We noticed that the test performance of the optimal hyperparameter set depended on whether we selected models based on training data loss or validation data loss and deteriorated if hyperparameters were chosen on validation data (<xref ref-type="supplementary-material" rid="SD1">Supp. Fig. 4d</xref>). The two breast cancer data sets were big enough to include a validation data set. We used 10% of the total data as validation data. The small validation dataset size of a few held-out images may result in stochastic effects in model selection that are detrimental to finding good hyperparameters and we continued to investigate models selected on train data performance. We noticed that the training loss plateaued early in many model fits (<xref ref-type="supplementary-material" rid="SD1">Supp. Fig. 5</xref>) and hypothesized that GCNs overfit patients during training because of the low number of total samples and the large content of information in the input data. We added further sample-level labels in a multi-task set-up to the GCNs trained on the <italic>IMC - breast cancer</italic> and <italic>IMC - METABRIC</italic> data sets but did not find this to improve the prediction accuracy on test data (<xref ref-type="supplementary-material" rid="SD1">Supp. Fig. 6</xref>).</p></sec><sec id="S5"><title>Sample embeddings reflect tumor phenotypes</title><p id="P8">We sought to interpret the learnt tissue representations of GCNs through sample embeddings (<xref ref-type="fig" rid="F1">Fig. 1c</xref>). After node-pooling, graph-based neural networks yield spatially-aware embeddings of an entire graph that can be used to compare different samples. Interestingly, the graph-embeddings show a continuous manifold of tumor grades ranging from grade 1 via grade 2 to grade 3 in the <italic>IMC - Jackson</italic> and <italic>IMC - METABRIC</italic> datasets (<xref ref-type="fig" rid="F3">Fig. 3a,b</xref>). Note that the order of tumor grades is not encoded in the categorical multi-class prediction problem and was learned correctly by the model, with grade 2 lying between grade 1 and 3 (<xref ref-type="fig" rid="F3">Fig. 3a,b</xref>). The graph-embeddings of grade 1 and 2 tend to overlap in the <italic>IMC - METABRIC</italic> dataset which reflects their anatomic similarity (<xref ref-type="fig" rid="F3">Fig. 3b</xref>). The sample embedding separates both tumor classes in the <italic>CODEX - colorectal cancer</italic> data sets (<xref ref-type="fig" rid="F3">Fig. 3c</xref>).</p></sec><sec id="S6"><title>Self-supervision mitigates overfitting on the node level</title><p id="P9">Not having found anomalies on the level of sample embeddings that could explain overfitting, we next considered node embeddings. We considered the distribution of cell types and images of each node in a UMAP fit to the embedding of 52 test images in both the input layer and the final node embedding layer of a GCN and a GCN-SS (<xref ref-type="fig" rid="F4">Fig. 4</xref>). Indeed, we found traces of overfitting to images on the level of node embeddings as a strong separation of nodes of each image in the last node embedding layer of the GCN. We found that the self-supervision loss constrains node embeddings by increasing integration of node embeddings across images (<xref ref-type="fig" rid="F4">Fig. 4a,b</xref>). We quantified this domain correction through self-supervision using data integration metrics from scRNA-seq analysis<sup><xref ref-type="bibr" rid="R11">11</xref></sup>. We found image integration to improve as measured by data integration metrics<sup><xref ref-type="bibr" rid="R11">11</xref></sup> to decrease when adding a self-supervision loss in a GCN to a graph-level label loss (<xref ref-type="fig" rid="F4">Fig. 4c</xref>, <xref ref-type="supplementary-material" rid="SD1">Supp. Fig. 8</xref>). Overfitting was less prominent for MI models when compared to GCNs as evaluated by data integration metrics (<xref ref-type="supplementary-material" rid="SD1">Supp. Fig. 8</xref>), demonstrating that the expressiveness of GCNs poses a challenge here. In summary, we showed that self-supervision in graph representation learning is a powerful mechanism to constrain node embeddings and a promising avenue to improve predictive performance in the future.</p></sec></sec><sec id="S7" sec-type="discussion"><title>Discussion</title><p id="P10">We benchmarked the ability of graph neural networks to predict tissue-level phenotypes based on single-cell-resolved spatial molecular profiling data sets. We detected spatial patterns of immune cells in colorectal cancer related to tumor class with graph neural networks. In breast cancer, the spatial information was sufficiently well represented in the cell-wise gene expression observations so that the predictive performance of GCNs is matched by non-spatial baseline models. This may be, in part, due to the simplicity of the currently available tissue phenotype labels and to the small number of images per study that limits the complexity of representations learned. We discussed self-supervision as a means to improve graph-level representation learning in this regime of low number of data points where overfitting is hard to avoid. We validated our findings in two comparable breast cancer cohorts and a colorectal cancer cohort, using tumor phenotypes as sample labels. Different tissue phenotypes may provide harder supervision problems and thus may give rise to a stronger relative performance of GCNs with respect to baseline models. Metabolic tissue properties and tissue response to perturbation may provide for such complex physiological read-outs.</p><p id="P11">Beyond the choice of tissue phenotype, there are many model architecture design choices to be considered in the future: First, we considered GCN kernels here. Graph attention<sup><xref ref-type="bibr" rid="R12">12</xref></sup> and other graph kernels with more degrees of freedom may be more sensitive to complex tissue niche motives. Second, pooling across the graph may be performed globally or hierarchically, as previously also discussed by Wu <italic>et al</italic>.<sup><xref ref-type="bibr" rid="R5">5</xref></sup>. Aggregation of information across a graph becomes even more relevant if larger graphs are considered, which may for example become available in the context of tissue clearing<sup><xref ref-type="bibr" rid="R13">13</xref></sup>. Integration of spatial profiling data with deep molecular profiles from scRNA-seq data may provide for higher resolution in the gene expression input feature space to GCNs in the future<sup><xref ref-type="bibr" rid="R14">14</xref></sup>.</p></sec><sec id="S8"><title>Online methods</title><sec id="S9"><title>Data</title><sec id="S10"><title>IMC - breast cancer (Jackson)</title><p id="P12">The breast cancer dataset (Jackson <italic>et al</italic>.<sup><xref ref-type="bibr" rid="R9">9</xref>,<xref ref-type="bibr" rid="R15">15</xref></sup> with 559 images from 350 patients) was measured with IMC. The dataset consists of samples from three breast cancer grades, grade 1 (114 images), grade 2 (214 images) and grade 3 (231 images). Here, 34 proteins in a panel specific to breast cancer microenvironment were simultaneously measured. We used the segmentation provided by Jackson <italic>et al.</italic>. We used the following channels: 1021522Tm169Di EGFR, 1031747Er167Di ECadhe, 112475Gd156Di Estroge, 117792Dy163Di GATA3, 1261726In113Di Histone, 1441101Er168Di Ki67, 174864Nd148Di SMA, 1921755Sm149Di Vimenti, 198883Yb176Di cleaved, 201487Eu151Di cerbB, 207736Tb159Di p53, 234832Lu175Di panCyto, 3111576Nd143Di Cytoker, Nd145Di Twist, 312878Gd158Di Progest, 322787Nd150Di cMyc, 3281668Nd142Di Fibrone, 346876Sm147Di Keratin, 3521227Gd155Di Slug, 361077Dy164Di CD20, 378871Yb172Di vWF, 473968La139Di Histone, 651779Pr141Di Cytoker, 6967Gd160Di CD44, 71790Dy162Di CD45, 77877Nd146Di CD68, 8001752Sm152Di CD3epsi, 92964Er166Di Carboni, 971099Nd144Di Cytoker, 98922Yb174Di Cytoker, phospho Histone, phospho S6, phospho mTOR, Area. Jackon <italic>et al</italic>. annotated the following cell types: B cells, T and B cells, T cells, macrophages, T cells, macrophages, endothelial, vimentin hi stromal cell, small circular stromal cell, small elongated stromal cell, fibronectin hi stromal cell, large elongated stromal cell, SMA hi vimentin hi stromal cell, hypoxic tumor cell, apoptotic tumor cell, proliferative tumor cell, p53+ EGFR+ tumor cell, basal CK tumor cell, CK7+ CK hi cadherin hi tumor cell, CK7+ CK+ tumor cell, epithelial low tumor cell, CK low HR low tumor cell, CK+ HR hi tumor cell, CK+ HR+ tumor cell, CK+ HR low tumor cell, CK low HR hi p53+ tumor cell and myoepithelial tumor cell. We coarsened the cell types into B cells, T and B cells, T cells, macrophages, T cells, macrophages, endothelial, stromal cells (vimentin hi stromal cell, small circular stromal cell, small elongated stromal cell, fibronectin hi stromal cell, large elongated stromal cell, SMA hi vimentin hi stromal cell) and tumor cells (hypoxic tumor cell, apoptotic tumor cell, proliferative tumor cell, p53+ EGFR+ tumor cell, basal CK tumor cell, CK7+ CK hi cadherin hi tumor cell, CK7+ CK+ tumor cell, epithelial low tumor cell, CK low HR low tumor cell, CK+ HR hi tumor cell, CK+ HR+ tumor cell, CK+ HR low tumor cell, CK low HR hi p53+ tumor cell, myoepithelial tumor cell).</p></sec><sec id="S11"><title>IMC - breast cancer (METABRIC)</title><p id="P13">The breast cancer METABRIC cohort (Ali et al.<sup><xref ref-type="bibr" rid="R15">15</xref></sup> with 500 images from 467 patients) was collected with IMC. Here, 37 proteins in formalin-fixed, paraffin-embedded breast tumor samples were measured. METABRIC dataset consists of images from three breast cancer grades, grade 1 (50 images), grade 2 (181 images) and grade 3 (269 images). Ali <italic>et al</italic>. segmented the single cells in the images using random forest classifier and then the expression of proteins in single cells was quantified. The mean protein expression of the segmented cells are used as the node features of the spatial graph with edge weights between cells defined using decaying kernels at the center of the reference cells. We used the following channels: HH3_total, CK19, CK8_18, Twist, CD68, CK14, SMA, Vimentin, c_Myc, HER2, CD3, HH3_ph, Erk1_2, Slug, ER, PR, p53, CD44, EpCAM, CD45, GATA3, CD20, Beta_catenin, CAIX, E_cadherin, Ki67, EGFR, pS6, Sox9, vWF_CD31, pmTOR, CK7, panCK, c_PARP_c_Casp3, DNA1, DNA2, H3K27me3, CK5, Fibronectin. Ali <italic>et al.</italic> annotated the following cell types: B cells, Basal CKlow, Endothelial, Fibroblasts, Fibroblasts CD68+, HER2+, HR+ CK7-, HR+ CK7- Ki67+, HR+ CK7- Slug+, HR- CK7+, HR- CK7-, HR- CKlow CK5+, HR- Ki67+, HRlow CKlow, Hypoxia, Macrophages Vim+ CD45low, Macrophages Vim+ Slug+, Macrophages Vim+ Slug-, Myoepithelial, Myofibroblasts and T cells, Vascular SMA+. We coarsened the cell types into B cells, Endothelial, Fibroblasts (Fibroblasts, Fibroblasts CD68+), Macrophages (Macrophages Vim+ CD45low, Macrophages Vim+ Slug+, Macrophages Vim+ Slug-), Myoepithelial, Myofibroblasts, T cells, Vascular SMA+ and Tumor cells (HER2+, HR+ CK7-, HR+ CK7- Ki67+, HR+ CK7- Slug+, HR- CK7+, HR- CK7-, HR- CKlow CK5+, HR- Ki67+, HRlow CKlow, Hypoxia).</p></sec><sec id="S12"><title>CODEX - colorectal cancer</title><p id="P14">The colorectal cancer dataset (Schürch <italic>et al</italic>.<sup><xref ref-type="bibr" rid="R7">7</xref></sup> with 140 images from 35 patients) was measured with CODEX. The dataset consists of two patient groups, one group with Crohn’s-like reaction (CLR) represented in 68 images and one group with diffuse inflammatory infiltration (DII) represented in 72 images. Here, 57 proteins specific to the tumor microenvironment were measured. We used the segmentation previously performed by Schürch <italic>et al</italic>.. The molecular abundance per cell segment and the coordinates of the center of each cell were used to construct the spatial graph. We used the following channels: CD44, FOXP3, CD8A, TP53, GATA3, PTPRC, TBX21, CTNNB1, HLA-DR, CD274, MKI67, PTPRC, CD4, CR2, MUC1, TNFRSF8, CD2, VIM, MS4A1, LAG3, ATP1A1, CD5, IDO1, KRT1, ITGAM, NCAM1, ACTA1, BCL2, IL2RA, ITGAX, PDCD1, GZMB, EGFR, VISTA, FUT4, ICOS, SYP, GFAP, CD7, CD247, CHGA, CD163, PTPRC, CD68, PECAM1, PDPN, CD34, CD38, SDC1, HOECHST1:Cyc_1_ch_1, CDX2, COL6A1, CCR4, MMP9, TFRC, B3GAT1, MMP12. Schürch <italic>et al</italic>. annotated the following cell types: B cells, CD11b+ monocytes, CD11b+CD68+ macrophages, CD11c+ DCs, CD163+ macrophages, CD3+ T cells, CD4+ T cells, CD4+ T cells CD45RO+, CD4+ T cells GATA3+, CD68+ macrophages, CD68+ macrophages GzmB+, CD68+CD163+ macrophages, CD8+ T cells, NK cells, Tregs, adipocytes, dirt, granulocytes, immune cells, immune cells / vasculature, lymphatics, nerves, plasma cells, smooth muscle, stroma, tumor cells, tumor cells / immune cells and undefined, vasculature. We binarized the cell types into <italic>immune cells</italic> (B cells, CD11b+ monocytes, CD11b+CD68+ macrophages, CD11c+ DCs, CD163+ macrophages, CD3+ T cells, CD4+ T cells, CD4+ T cells CD45RO+, CD4+ T cells GATA3+, CD68+ macrophages, CD68+ macrophages GzmB+, CD68+CD163+ macrophages, CD8+ T cells, NK cells, Tregs, granulocytes, immune cells, immune cells / vasculature, lymphatics and tumor cells / immune cells) and <italic>other</italic> (adipocytes, dirt, nerves, plasma cells, smooth muscle, stroma, tumor cells, undefined and vasculature).</p></sec><sec id="S13"><title>Processing</title><p id="P15">We considered molecular feature spaces and cell type feature spaces. We compared molecular feature quantification per cell as provided by the authors<sup><xref ref-type="bibr" rid="R9">9</xref>,<xref ref-type="bibr" rid="R16">16</xref></sup> (no feature transformation), image-wise feature standardization and global feature standardization. The cell type feature space was assembled as a one-hot encoding of the categorical cell type labels as described in the methods sections specific to each dataset. We used patient identifiers as the domain label for all datasets.</p></sec><sec id="S14"><title>Spatial proximity graphs</title><p id="P16">We considered neighborhood graphs built with fixed kernel radii across all images. In all datasets considered here, pixel dimensions are fixed across images so that radii defined on pixels correspond to consistent spatial distances across images. We defined a raw adjacency matrix <italic>A</italic> for each image with entries <italic>a<sub>ij</sub></italic> based on a radius <italic>r</italic> of a kernel between the position of two cells <italic>i</italic>, <italic>j</italic> in 2D space <italic>z<sub>i</sub></italic>, <italic>z<sub>j</sub></italic>: <disp-formula id="FD1"><mml:math id="M1"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mspace width="0.2em"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:msub><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>&lt;</mml:mo><mml:mi>r</mml:mi><mml:mspace width="0.2em"/><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.2em"/><mml:mn>0.</mml:mn></mml:mrow></mml:math></disp-formula></p></sec><sec id="S15"><title>Spectral clustering</title><p id="P17">We computed spectral clustering for the spatial graphs, where we divided the graph into a certain number of subgraphs (clusters) based on distances between the nodes of the graph. We first compute <italic>k</italic> nearest neighbors matrices, where <italic>k</italic> is a hyperparameter representing the number of neighbors used for kNN graph construction. We then computed the <italic>n</italic> spectral clusters in the graph using the <italic>SpectralClustering</italic> model, where <italic>n</italic> is a hyperparameter of the desired number of subgraphs and we perform a one-hot encoded assignment of the graph nodes to the nearest cluster. We also calculate the adjacency matrices within each of the clusters.</p></sec></sec><sec id="S16"><title>Models</title><p id="P18">All models presented are feed-forward networks that take graph data (or reductions thereof) as input and produce a graph-level classification as output.</p><sec id="S17"><title>Pseudobulk multi-layer perceptron networks (MLP)</title><p id="P19">For the pseudobulk reference model, we used an aggregated mean of the feature space across the entire image and it as input, <inline-formula><mml:math id="M2"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mi>i</mml:mi><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M3"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula> is the vector of average expression of input features, <italic>x</italic> is the node feature matrix (number of nodes <italic>N x</italic> input features). The input is then fed to a dense fully connected network to obtain the graph-level prediction <inline-formula><mml:math id="M4"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="S18"><title>Multi-instance networks (MI)</title><p id="P20">For the multi-instance reference model, we used a stack of fully connected layers to obtain node-wise feature embeddings. Each layer <italic>l</italic> transformed the set of node features as: <disp-formula id="FD2"><mml:math id="M5"><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:msup><mml:mi>W</mml:mi><mml:mi>l</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where σ is an activation function, <italic>H<sup>l</sup></italic> is a node feature matrix of dimensions (number of nodes x input features) and <italic>W<sup>l</sup></italic> is a weight matrix of dimensions (input features x output features). To then obtain graph-level predictions, the node feature embeddings were aggregated by a pooling layer and further transformed by two dense layers.</p></sec><sec id="S19"><title>Graph convolutional networks (GCN)</title><p id="P21">The node embedding layers for the Graph Convolutional Network are defined as: <disp-formula id="FD3"><mml:math id="M6"><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>A</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:msup><mml:mi>H</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:msup><mml:mi>W</mml:mi><mml:mi>l</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where σ is an activation function, <italic>H<sup>l</sup></italic> is a n input node feature matrix of dimensions (number of nodes x input features), <italic>W<sup>l</sup></italic> is a weight matrix of dimensions (input features x output features) and <italic>A<sup>*</sup></italic> is the spectral normalized adjacency matrix: <disp-formula id="FD4"><mml:math id="M7"><mml:mrow><mml:msup><mml:mi>A</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:mi>A</mml:mi><mml:mspace width="0.2em"/><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula> where <italic>A</italic> is the raw adjacency matrix and <italic>D</italic> is the degree matrix of A. Node embeddings were then aggregated by a pooling layer and two dense layers were then used to obtain the graph-level predictions.</p></sec><sec id="S20"><title>Graph convolutional networks with self-supervision (GCN-SS)</title><p id="P22">We introduced a self-supervision auxiliary task to the graph neural network. The self-supervision loss was added to the supervised loss which in this case acts as a regularization factor to the loss function. Here, we chose relative cell type proportions as our self-supervision task. Spectral clustering was performed on the spatial graph. For each node, the relative proportions of cell types in the same cluster are predicted.</p><p id="P23"><disp-formula id="FD5"><mml:math id="M8"><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>C</mml:mi><mml:mo>*</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mi>i</mml:mi><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mi>c</mml:mi><mml:mi>C</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula> where <italic>y</italic> ∈ <italic>R<sup>KxC</sup></italic>, <inline-formula><mml:math id="M9"><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>x</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, where <italic>K</italic> is the number of spectral clusters and <italic>k</italic>(<italic>i</italic>) the cluster assignment of cell <italic>i</italic>, and <italic>C</italic> is the number of distinct cell types.</p></sec><sec id="S21"><title>Dispersion model</title><p id="P24">As for deriving self-supervision labels, we performed spectral clustering, calculated the relative proportions of cell types per cluster, and finally aggregated the mean of relative proportions of cell types across the clusters, <inline-formula><mml:math id="M10"><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mi>k</mml:mi><mml:mi>K</mml:mi></mml:munderover><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula>, a vector of length <italic>C</italic>, where <italic>K</italic> is the number of spectral clusters and <inline-formula><mml:math id="M11"><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:math></inline-formula> is the fraction of cell types of type <italic>c</italic> in cluster <italic>k</italic>. We then predicted the graph-level label based on <inline-formula><mml:math id="M12"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:math></inline-formula> with a fully connected network: <inline-formula><mml:math id="M13"><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mover accent="true"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:math></inline-formula>. In this manuscript, we used the dispersion model only on binarized cell type labels.</p></sec><sec id="S22"><title>Node-wise pooling</title><p id="P25">We aggregated information across all observations in an image using a mean aggregation for the IMC datasets and used aggregation in spectral clusters on the CODEX data. For spectral pooling, we first aggregated nodes within the same cluster followed by aggregating across clusters. The spectral clusters are subgraphs (clusters) of adjacent nodes assigned k-nearest neighbors clustering algorithm. The number of clusters in a graph is a hyperparameter ranging between 5 and 20 clusters.</p></sec><sec id="S23"><title>Integration metrics</title><p id="P26">To quantify the domain correction across the different models, we adapted integration metrics from scRNA-seq, namely the <italic>graph connectivity</italic> metric and <italic>iLISI graph</italic> metric from the scIB<sup><xref ref-type="bibr" rid="R9">9</xref></sup> package. The <italic>graph connectivity</italic> and <italic>iLISI graph</italic> metrics are both calculated from the kNN graph (k=50) of the node embeddings of all images and assess the connectivity between the nodes from the same images, the higher the scores the better the images are integrated. We further trained a linear regression model using the node embeddings from the last layer of the models for the prediction of the image identities and used its accuracy to quantify the strength of the signal still coming from the image identity. For the <italic>prediction accuracy</italic>, the lower the score the better the images are integrated.</p></sec></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Figure 1-9</label><media xlink:href="EMS158469-supplement-Supplementary_Figure_1_9.pdf" mimetype="application" mime-subtype="pdf" id="d20aAdEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S24"><title>Acknowledgements</title><p>We would like to thank Jana R. Fischer, Jonas Windhager, and Prof. Dr. Bernd Bodenmiller for assistance with the discussed breast cancer data sets and for discussion on the topic of graph convolutional networks and cancer grade prediction. We would like to thank Eeshit Dhaval Vaishnav Prof. Dr. Aviv Regev for discussion on spatial molecular profiling data. We would like to thank Anna Schaar for discussions about the model and the code base.</p><p>This work was supported by the German Federal Ministry of Education and Research (BMBF) under Grant No. 01IS18036B and No. 01IS18053A, by the Wellcome Trust Grant 108413/A/15/D and by the Helmholtz Association’s Initiative and Networking Fund through Helmholtz AI [grant number: ZT-I-PF-5-01]. D.S.F. acknowledges support from a German Research Foundation (DFG) fellowship through the Graduate School of Quantitative Biosciences Munich (QBM) [GSC 1006 to D.S.F.] and by the Joachim Herz Foundation. S. R. is supported by the Helmholtz Association under the joint research school “Munich School for Data Science” —MUDS.</p></ack><sec id="S25" sec-type="data-availability"><title>Code and data availability</title><p id="P27">We used published datasets provided in the original studies. We summarized all models, training and interpretation mechanisms discussed here in a python package centered around graph-level supervision on spatial single-cell graphs, <ext-link ext-link-type="uri" xlink:href="https://github.com/theislab/tissue">https://github.com/theislab/tissue</ext-link>. We are also providing all the analysis and the model evaluation notebooks that were presented throughout the paper, <ext-link ext-link-type="uri" xlink:href="https://github.com/theislab/tissue_reproducibility">https://github.com/theislab/tissue_reproducibility</ext-link>.</p></sec><fn-group><fn id="FN1" fn-type="con"><p id="P28"><bold>Author contributions</bold></p><p id="P29">DSF and FJT conceived the study. DSF, MA and SR implemented the overall software and performed the analyses. DSF, MA, SR, AE and FJT wrote the manuscript.</p></fn><fn id="FN2" fn-type="conflict"><p id="P30"><bold>Conflicts of interest</bold></p><p id="P31">F.J.T consults for Immunai Inc., Singularity Bio B.V., CytoReason Ltd, and Omniscope Ltd, and has ownership interest in Dermagnostix GmbH and Cellarity.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wagner</surname><given-names>A</given-names></name><name><surname>Regev</surname><given-names>A</given-names></name><name><surname>Yosef</surname><given-names>N</given-names></name></person-group><article-title>Revealing the vectors of cellular identity with single-cell genomics</article-title><source>Nat Biotechnol</source><year>2016</year><volume>34</volume><fpage>1145</fpage><lpage>1160</lpage><pub-id pub-id-type="pmcid">PMC5465644</pub-id><pub-id pub-id-type="pmid">27824854</pub-id><pub-id pub-id-type="doi">10.1038/nbt.3711</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palla</surname><given-names>G</given-names></name><name><surname>Fischer</surname><given-names>DS</given-names></name><name><surname>Regev</surname><given-names>A</given-names></name><name><surname>Theis</surname><given-names>FJ</given-names></name></person-group><article-title>Spatial components of molecular tissue biology</article-title><source>Nat Biotechnol</source><year>2022</year><pub-id pub-id-type="pmid">35132261</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischer</surname><given-names>DS</given-names></name><name><surname>Schaar</surname><given-names>AC</given-names></name><name><surname>Theis</surname><given-names>FJ</given-names></name></person-group><article-title>Modeling intercellular communication in tissues using spatial graphs of cells</article-title><source>Nat Biotechnol</source><year>2022</year><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="pmid">36302986</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hetzel</surname><given-names>L</given-names></name><name><surname>Fischer</surname><given-names>DS</given-names></name><name><surname>Günnemann</surname><given-names>S</given-names></name><name><surname>Theis</surname><given-names>FJ</given-names></name></person-group><article-title>Graph Representation Learning for Single Cell Biology</article-title><source>Current Opinion in Systems Biology</source><year>2021</year><pub-id pub-id-type="doi">10.1016/j.coisb.2021.05.008</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>Z</given-names></name><etal/></person-group><article-title>Graph deep learning for the characterization of tumour microenvironments from spatial protein profiles in tissue specimens</article-title><source>Nat Biomed Eng</source><year>2022</year><pub-id pub-id-type="pmid">36357512</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Learning predictive models of tissue cellular neighborhoods from cell phenotypes with graph pooling</article-title><source>bioRxiv</source><year>2022</year><elocation-id>2022.11.06.515344</elocation-id><pub-id pub-id-type="doi">10.1101/2022.11.06.515344</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Cell graph neural networks enable the precise prediction of patient survival in gastric cancer</article-title><source>NPJ Precis Oncol</source><year>2022</year><volume>6</volume><elocation-id>45</elocation-id><pub-id pub-id-type="pmcid">PMC9226174</pub-id><pub-id pub-id-type="pmid">35739342</pub-id><pub-id pub-id-type="doi">10.1038/s41698-022-00285-5</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graham</surname></name><name><surname>Zhou</surname></name></person-group><article-title>Cgc-net: Cell graph convolutional network for grading of colorectal cancer histology images</article-title><source>Proc Estonian Acad Sci Biol Ecol</source></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jackson</surname><given-names>HW</given-names></name><etal/></person-group><article-title>The single-cell pathology landscape of breast cancer</article-title><source>Nature</source><year>2020</year><volume>578</volume><fpage>615</fpage><lpage>620</lpage><pub-id pub-id-type="pmid">31959985</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schürch</surname><given-names>CM</given-names></name><etal/></person-group><article-title>Coordinated Cellular Neighborhoods Orchestrate Antitumoral Immunity at the Colorectal Cancer Invasive Front</article-title><source>Cell</source><year>2020</year><volume>182</volume><fpage>1341</fpage><lpage>1359</lpage><elocation-id>e19</elocation-id><pub-id pub-id-type="pmcid">PMC7479520</pub-id><pub-id pub-id-type="pmid">32763154</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2020.07.005</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luecken</surname><given-names>MD</given-names></name><etal/></person-group><article-title>Benchmarking atlas-level data integration in single-cell genomics</article-title><source>Nat Methods</source><year>2022</year><volume>19</volume><fpage>41</fpage><lpage>50</lpage><pub-id pub-id-type="pmcid">PMC8748196</pub-id><pub-id pub-id-type="pmid">34949812</pub-id><pub-id pub-id-type="doi">10.1038/s41592-021-01336-8</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Veličković</surname><given-names>P</given-names></name><etal/></person-group><article-title>Graph Attention Networks</article-title><source>arXiv [statML]</source><year>2017</year></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bhatia</surname><given-names>HS</given-names></name><etal/></person-group><article-title>Proteomics of spatially identified tissues in whole organs</article-title><pub-id pub-id-type="doi">10.1101/2021.11.02.466753</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biancalani</surname><given-names>T</given-names></name><etal/></person-group><article-title>Deep learning and alignment of spatially resolved single-cell transcriptomes with Tangram</article-title><source>Nat Methods</source><year>2021</year><volume>18</volume><fpage>1352</fpage><lpage>1362</lpage><pub-id pub-id-type="pmcid">PMC8566243</pub-id><pub-id pub-id-type="pmid">34711971</pub-id><pub-id pub-id-type="doi">10.1038/s41592-021-01264-7</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ali</surname><given-names>HR</given-names></name><etal/></person-group><article-title>Imaging mass cytometry and multiplatform genomics define the phenogenomic landscape of breast cancer</article-title><source>Nat Cancer</source><year>2020</year><volume>1</volume><fpage>163</fpage><lpage>175</lpage><pub-id pub-id-type="pmid">35122013</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jerby-Arnon</surname><given-names>L</given-names></name><etal/></person-group><article-title>A Cancer Cell Program Promotes T Cell Exclusion and Resistance to Checkpoint Blockade</article-title><source>Cell</source><year>2018</year><volume>175</volume><fpage>984</fpage><lpage>997</lpage><elocation-id>e24</elocation-id><pub-id pub-id-type="pmcid">PMC6410377</pub-id><pub-id pub-id-type="pmid">30388455</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2018.09.006</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>A graph neural networks model of tissue phenotypes.</title><p><bold>(a)</bold> Tissue-level phenotypes are functions of the architecture of the tissue. In this case, two colorectal tumor classes, Crohn’s-like reaction (CLR) and diffuse inflammatory infiltration (DII), can be distinguished based on the spatial distribution of immune cells. This tumor label cannot be inferred based on frequencies of cell types that would be available in dissociation-based protocols, but only based on the spatial distribution of cells<sup><xref ref-type="bibr" rid="R10">10</xref></sup>. One example image from the <italic>CODEX - colorectal cancer</italic> dataset for each class. <bold>(b)</bold> A graphical representation of a tissue graph with different node features, molecular gene expression profiles (left) and one-hot encoding of node cell type (right). <bold>(c)</bold> The spatial context of each cell can be formally represented by a graph in which edges are weighted based on the distance between nodes. Each sample can be represented as one such graph, where nodes are colored by the measured cell features. We perform prediction with a model that consists of graph neural network layers to produce node embeddings, followed by pooling over nodes and a final classification network. In addition, the node embeddings of connected components of nodes on the spatial proximity graph can be aggregated for local self-supervision tasks, such as reconstruction of adjacent clusters’ cell type composition. <italic>dotted line</italic>: connected component of nodes on spatial proximity graph. The spatially-aware graph embedding can be visualized with a UMAP in which each point reflects one graph (image) and depicts separation of samples by the tumor class (CLR and DII) here.</p></caption><graphic xlink:href="EMS158469-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Graph neural networks predict whole-slide tumor phenotypes from cellular architecture.</title><p><bold>(a)</bold> Design of the ablation study. MLP models only have access to the average node feature vector of the graph. MI models have access to single-cell-resolved but <italic>in silico</italic> dissociated data from the observed spatial graph. Spatially aware models, DISP, GCN, and GCN-SS, have access to node features and the spatial proximity graph. <italic>MLP</italic>: Multi-layer perceptron based on graph-wide summary statistics on features. <italic>MI</italic>: Multi-instance model on nodes of graph. <italic>DISP</italic>: Dispersion multi-layer perceptron based on average graph niches features. <italic>GCN</italic>: Graph convolutional network. <italic>GCN-SS</italic>: Graph convolutional network with additional self-supervision loss. <bold>(b, c)</bold> Three separate applications of graph neural networks to predict tumor phenotypes on the IMC - breast cancer (Jackson), IMC - breast cancer (METABRIC) and CODEX - colorectal cancer datasets. Model complexity ablation study on classification performance on breast cancer grade prediction and colorectal cancer group prediction using two feature spaces, molecular features and cell types. Shown is the area-under-curve of the receiver-operator characteristic curve (AUC ROC) using the binary cell type features (immune and non-immune) <bold>(b)</bold> and molecular profile features <bold>(c)</bold> across six-fold cross-validation for the best performing hyper-parameter set for each model class for best models selected on train loss.</p></caption><graphic xlink:href="EMS158469-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Graph neural networks interpolate tumor states.</title><p>UMAP of graph embedding of GCN-SS of training and test data with class label superimposed for <bold>(a)</bold> IMC - breast cancer (Jackson), <bold>(b)</bold> IMC - breast cancer (METABRIC) and <bold>(c)</bold> CODEX - colorectal cancer datasets. Each point is one graph. <bold>(d, e)</bold> The euclidean distances between the different class embeddings of the <bold>(d)</bold> IMC - breast cancer (Jackson) and <bold>(e)</bold> IMC - breast cancer (METABRIC).</p></caption><graphic xlink:href="EMS158469-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Auxiliary self-supervision loss stabilizes node embeddings.</title><p><bold>(a,b)</bold> UMAPs of node embeddings by layer: input, last GCN layer in a GCN model and last GCN layer in a GCN-SS with two GCN layers trained on the <italic>IMC - breast cancer</italic> data (<italic>Jackson</italic>). Superimposed are <bold>(a)</bold> the cancer grade of the corresponding graph and <bold>(b)</bold> the image (graph) to which the node belongs. <bold>(c)</bold> Focusing only on GCN and GCN-SS here, we show the <italic>iLISI and graph connectivity</italic> integration metrics on the three datasets (N=3 cross validations). For <italic>iLISI</italic> and <italic>1 - graph connectivity</italic>, higher values imply better integration, while low values result from distribution differences in the considered batches.</p></caption><graphic xlink:href="EMS158469-f004"/></fig><table-wrap id="T1" position="float" orientation="portrait"><label>Table 1</label><caption><title>Hyper-parameters screened in grid search for each data set.</title></caption><table frame="box" rules="all"><tbody><tr><td align="left" valign="top"><bold>dataset</bold></td><td align="left" valign="top">learning</td><td align="left" valign="top">l2</td><td align="left" valign="top">radius</td><td align="left" valign="top">aggregation</td><td align="left" valign="top">depth</td><td align="left" valign="top">number of clusters</td><td align="left" valign="top">width</td></tr><tr><td align="left" valign="top"><bold>IMC - breast cancer (Jackson)</bold></td><td align="left" valign="top">{<bold>0.05</bold>, 5e-3, 5e-4}</td><td align="left" valign="top">{<bold>0</bold>, 1e-6, 1e-3}</td><td align="left" valign="top">{<bold>10</bold>,50}</td><td align="left" valign="top">mean</td><td align="left" valign="top">{1,<bold>2</bold>,3}</td><td align="left" valign="top">{5,<bold>10</bold>, 20}</td><td align="left" valign="top">{<bold>16</bold>,64}</td></tr><tr><td align="left" valign="top"><bold>IMC - breast cancer (METABRIC)</bold></td><td align="left" valign="top">{<bold>5e-3</bold>, 5e-4}</td><td align="left" valign="top">{<bold>0</bold>, 1e-6}</td><td align="left" valign="top">{10, <bold>20</bold>, 55}</td><td align="left" valign="top">mean</td><td align="left" valign="top">3</td><td align="left" valign="top">{<bold>5</bold>, 10}</td><td align="left" valign="top">{16, <bold>64</bold>}</td></tr><tr><td align="left" valign="top"><bold>CODEX - colorectal cancer</bold></td><td align="left" valign="top">{<bold>5e-3</bold>, 5e-4}</td><td align="left" valign="top">{<bold>0</bold>, 1e-6}</td><td align="left" valign="top">{25,<bold>50</bold>, 120}</td><td align="left" valign="top">spectral</td><td align="left" valign="top">{2,<bold>3</bold>}</td><td align="left" valign="top">{<bold>5</bold>,10}</td><td align="left" valign="top">{4,8,<bold>16</bold>, 32}</td></tr></tbody></table></table-wrap></floats-group></article>