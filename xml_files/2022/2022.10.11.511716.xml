<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS155803</article-id><article-id pub-id-type="doi">10.1101/2022.10.11.511716</article-id><article-id pub-id-type="archive">PPR558287</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Emergence of the cortical encoding of phonetic features in the first year of life</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><name><surname>Di Liberto</surname><given-names>Giovanni M.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Attaheri</surname><given-names>Adam</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Cantisani</surname><given-names>Giorgia</given-names></name><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Reilly</surname><given-names>Richard B.</given-names></name><xref ref-type="aff" rid="A4">4</xref><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Choisdealbha</surname><given-names>Áine Ní</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Rocha</surname><given-names>Sinead</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Brusini</surname><given-names>Perrine</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Goswami</surname><given-names>Usha</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib></contrib-group><aff id="A1"><label>1</label>ADAPT Centre, School of Computer Science and Statistics, Trinity College, The University of Dublin, Ireland; Trinity College Institute of Neuroscience</aff><aff id="A2"><label>2</label>Centre for Neuroscience in Education, Department of Psychology, University of Cambridge, United Kingdom</aff><aff id="A3"><label>3</label>Laboratoire des systémes perceptifs, Département d’études cognitives, école normale supérieure, PSL University, CNRS, 75005 Paris, France</aff><aff id="A4"><label>4</label>School of Engineering, Trinity Centre for Biomedical Engineering, Trinity College, The University of Dublin. Trinity College Institute of Neuroscience</aff><aff id="A5"><label>5</label>School of Medicine, Trinity College, The University of Dublin, Ireland</aff><author-notes><corresp id="CR1">Correspondence: <email>diliberg@tcd.ie</email>
</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>18</day><month>10</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>14</day><month>10</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Even prior to producing their first words, infants are developing a sophisticated speech processing system, with robust word recognition present by 4-6 months of age. These emergent linguistic skills, observed with behavioural investigations, are likely to rely on increasingly sophisticated neural underpinnings. The infant brain is known to robustly track the speech envelope, however to date no cortical tracking study could investigate the emergence of phonetic feature encoding. Here we utilise temporal response functions computed from electrophysiological responses to nursery rhymes to investigate the cortical encoding of phonetic features in a longitudinal cohort of infants when aged 4, 7 and 11 months, as well as adults. The analyses reveal an increasingly detailed and acoustically-invariant phonetic encoding over the first year of life, providing the first direct evidence that the pre-verbal human cortex learns phonetic categories. By 11 months of age, however, infants still did not exhibit adult-like encoding.</p></abstract></article-meta></front><body><p id="P2">The human ability to understand speech relies on a complex neural system, whose foundations develop over the first few years of life. A wealth of evidence on the developmental progression of speech perception is available from infant behavioural studies, including with neonates, augmented by studies of speech production from around the second year of life<sup><xref ref-type="bibr" rid="R1">1</xref>,<xref ref-type="bibr" rid="R2">2</xref></sup>. Yet our understanding of speech perception in the first year of life is largely dependent on tasks relying on simple behaviours (e.g., head turn preference procedure). Direct investigation of the neural encoding of phonetic information in continuous natural speech across the first year of life has not previously been possible. Experiments using behavioural measures enable the assessment of valuable factors such as the familiarity of a particular speaker, the phonetic features that can be discriminated, and sensitivity to native versus non-native speech contrasts, thereby providing a time-line for the development of speech perception in the first year of life<sup><xref ref-type="bibr" rid="R1">1</xref></sup>. However, behavioural methods can only serve as an indirect index of the emergence of linguistic skills, and cannot reveal when the phonetic encoding in the human cortex becomes invariant across different instantiations. Previous behavioural studies focused on sound discrimination due to methodological constraints, and made use of targeted experimental paradigms involving simple stimuli. Although this behavioural timeline has been complemented by neurophysiological investigations, these studies have employed similar targeted paradigms, with the most widely-used neurophysiological measure with infants being the mismatch negativity (MMN, or mismatch response, MMR). The MMR is a neurophysiological signature of automatic change detection<sup><xref ref-type="bibr" rid="R3">3</xref>–<xref ref-type="bibr" rid="R5">5</xref></sup> typically used to measure the ability to discriminate particular speech contrasts. However, previous studies showed that such mismatch responses in infants can sometimes be positive<sup><xref ref-type="bibr" rid="R6">6</xref></sup>, causing inconsistencies that can complicate or limit their use in infants. This leaves us with a number of key open questions: 1) How do infants perceive and encode the phonological units such as syllables and phonemes in <bold>continuous natural speech?</bold> 2) How are these speech sounds <bold>encoded</bold> in the infant brain? And 3) how does that encoding <bold>develop across the first year of life?</bold></p><p id="P3">This study is the first to address these research questions directly. Non-invasive electroencephalography signals (EEG) were recorded as infants listened to 18 nursery rhymes (vocals only with no instruments involved) through video recordings of a native English speaker. EEG recordings were carried out at 4, 7 and 11 months of age from the first 50 participants in a longitudinal cohort involving 122 infants (the same subjects were tested in the three subsequent sessions and only participants with all sessions were selected). Three participants were excluded due to excessive EEG noise (see <xref ref-type="sec" rid="S5">Methods</xref>). We then measured how the infant brain encodes acoustic and phonetic information by means of the multivariate Temporal Response Function analysis (TRF), a neurophysiology framework enabling the study of how neural signals encode continuous sensory stimuli<sup><xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R8">8</xref></sup>. TRF analyses were also carried out on recordings from adult participants listening to the same stimuli. We targeted one key aspect for speech perception, the perception of phonetic features. We do not assume here that encoding phonetic features equates to encoding phonemes, as there is a large psychoacoustic and developmental literature showing that phonemes are only represented by literate brains<sup><xref ref-type="bibr" rid="R9">9</xref>,<xref ref-type="bibr" rid="R10">10</xref></sup>. Our core hypothesis was rather that phonetic feature encoding (invariant to acoustic changes) would emerge in the neural responses to natural speech during the first year of life.</p><p id="P4">Speech TRFs reflect the neural tracking of (or neural entrainment to, in the broad sense<sup><xref ref-type="bibr" rid="R11">11</xref></sup>) natural speech features (e.g., acoustic envelope), offering a direct window into human perception during natural listening without imposing any particular task other than listening. In recent years, neural tracking measures have played a growing role in the study of speech comprehension and auditory processing in general. Many TRF studies have assessed the neural tracking of the acoustic envelope<sup><xref ref-type="bibr" rid="R12">12</xref>–<xref ref-type="bibr" rid="R15">15</xref></sup>, which is an important property of speech that co-varies with a number of key properties of interest (e.g., syllable stress patterns, syllables, phonemes). Neural tracking of the speech envelope (or envelope tracking) was shown to reflect both bottom-up and top-down cortical processes in adult listeners, encompassing fundamental functions such as selective attention<sup><xref ref-type="bibr" rid="R15">15</xref>–<xref ref-type="bibr" rid="R17">17</xref></sup>, working memory processing load<sup><xref ref-type="bibr" rid="R18">18</xref></sup>, and prediction<sup><xref ref-type="bibr" rid="R19">19</xref>,<xref ref-type="bibr" rid="R20">20</xref></sup>. While robust envelope tracking has also been demonstrated in infants<sup><xref ref-type="bibr" rid="R21">21</xref>–<xref ref-type="bibr" rid="R26">26</xref></sup>, envelope measures only reveal some of the cortical mechanisms underlying speech perception. Recent work with adults and children has demonstrated that TRFs can be extended to isolate the neural encoding of targeted speech properties of interest, starting from phonetic features<sup><xref ref-type="bibr" rid="R27">27</xref></sup>. Phonetic encoding was measured in multiple studies from different research teams<sup><xref ref-type="bibr" rid="R27">27</xref>–<xref ref-type="bibr" rid="R31">31</xref></sup>, and the neural tracking was shown to correlate with phonemic awareness skills in school-aged children between 6 and 12 years of age<sup><xref ref-type="bibr" rid="R32">32</xref></sup> and with second language proficiency in adults<sup><xref ref-type="bibr" rid="R33">33</xref></sup>.</p><p id="P5">Here, we employed TRFs to test the hypothesis that the neural encoding of phonetic features during natural speech listening is already developing during the first-year of life. Current behavioural data indicate that infant perception becomes more selective towards native than non-native speech contrasts around 9-12 months of age<sup><xref ref-type="bibr" rid="R34">34</xref></sup> (see footnote<sup><xref ref-type="fn" rid="FN2">i</xref></sup>), with perceptual “magnet” effects helping to isolate native from non-native phonetic contrasts already by 6 months<sup><xref ref-type="bibr" rid="R35">35</xref></sup>. We hypothesised that these phenomena may be underpinned by a progressively more precise and acoustically-invariant neural encoding of phonetic features across the first year of life. This encoding would be expected to emerge as a neural response to speech that reflects a growing invariance towards phonetic categories, where the limit case would be to have neural responses to phonetic categories that are fully invariant to acoustic changes. This longitudinal investigation offers the first view into phonetic feature encoding in the first year of life, while accounting for the full complexity of speech in naturalistic listening environments. In the discussion, we connect our encoding analyses with previous work on phonetic discrimination and consider the key role of perceptual invariance. Our results provide a promising new avenue for developmental research with both infants and children. Precise measures of how and when phonetic feature encoding evolves could serve as a complementary set of risk factors for developmental language disorders, as well as illuminating the phonological trajectories experienced by both typically- and atypically-developing children.</p><sec id="S1" sec-type="results"><title>Results</title><sec id="S2"><title>Robust neural tracking of acoustic and phonetic features in infants</title><p id="P6">A multivariate TRF analysis was carried out to assess the low-frequency (1-15 Hz) neural encoding of speech across the first year of life. Acoustic and phonetic features were extracted from the stimulus. Acoustic features consisted of the 8-band acoustic spectrogram of speech (<italic>S</italic>) sound and the half-way rectified envelope derivative (<italic>D</italic>). Fourteen phonetic features were included to mark the categorical occurrence of speech sounds, according to articulatory features describing voicing as well as manner and place of articulation. To account for possible differences in the encoding of stressed and unstressed sounds, each phonetic feature produced to two distinct vectors, leading to a 28-dimensional phonetic features matrix (<italic>F</italic>; see <xref ref-type="sec" rid="S5">Methods</xref>). A nuisance regressor was also included to capture EEG variance related to visual motion (<italic>V</italic>). Single-subject TRFs were derived for each experimental session to assess the cortical encoding of acoustic and phonetic features by fitting a multivariate lagged regression model with all such features simultaneously (<xref ref-type="fig" rid="F1">Figure 1A</xref>).</p><p id="P7">EEG prediction correlations, calculated with leave-one-out cross-validation and averaged across all EEG channels, were <bold>greater than zero for all age groups</bold> (one-sample Wilcoxon rank sum test, FDR-corrected for multiple comparisons; 4mo: <italic>p</italic>=9.5*10<sup>−6</sup>; 7mo: <italic>p</italic>=9.5*10<sup>−6</sup>; 11mo: <italic>p</italic>=9.5*10<sup>−6</sup>; adults: <italic>p</italic>=2.2*10<sup>−4</sup>; <xref ref-type="fig" rid="F1">Figure 1B</xref>). Consistent with previous work, this analysis was carried out by considering speech-EEG lags from 0 to 400ms, which were shown to largely capture the cortical acoustic-phonetic response in adults. The TRF analysis was also repeated when considering a 100-500ms lag window, aiming to control for possible responses with longer latencies in infants, while keeping the same model complexity (i.e., same window size). This analysis also led to significant EEG predictions (one-sample Wilcoxon rank sum test, FDR-corrected; 4mo: <italic>p</italic>=3.6*10<sup>−6</sup>; 7mo: <italic>p</italic>=3.6*10<sup>−6</sup>; 11mo: <italic>p</italic>=1.7*10<sup>−6</sup>; adults: <italic>p</italic>=0.001; <xref ref-type="fig" rid="F1">Figure 1B</xref>), indicating a consistent speech-EEG relationship involving acoustic and phonetic features in both time windows.</p><p id="P8">Topographic differences were expected both across participants and by age group due to major anatomical changes during infancy<sup><xref ref-type="bibr" rid="R36">36</xref></sup>. Larger EEG prediction correlations were measured in centro-frontal electrodes for all age groups (<xref ref-type="fig" rid="F1">Figure 1C</xref>), with <bold>topographies becoming progressively more similar to those for adults</bold> with age in both the 0-400ms lag window (bootstrap with group size = 17 and 1000 iterations; average correlation with adults: <italic>r</italic> = 0.43, 0.51, 0.54 for 4mo, 7mo, and 11mo respectively; repeated measures ANOVA on infant data with age as the repeated factor: <italic>F</italic>(2,1998) = 172.8, p = 6.1*10<sup>−70</sup>) and 100-500ms window (bootstrap; average correlation with adults: <italic>r</italic> = 0.32, 0.55, 0.52 for 4mo, 7mo, and 11mo respectively; repeated measures ANOVA: <italic>F</italic>(2,1998) = 900.6, p = 1.5*10<sup>−279</sup>). TRF models corresponding to spectrogram features are reported in <xref ref-type="fig" rid="F1">Figure 1D</xref>, where weights were averaged across fourteen centro-frontal electrodes (25% of all channels) and all participants (see <xref ref-type="sec" rid="S5">Methods</xref>).</p></sec><sec id="S3"><title>Emergence of phonetic feature encoding in the first year of life</title><p id="P9">The analyses that follow aim to determine if and when cortical signals encode acoustically-invariant phonetic features during the first year of life. In line with previous behavioural work<sup><xref ref-type="bibr" rid="R35">35</xref>,<xref ref-type="bibr" rid="R37">37</xref>–<xref ref-type="bibr" rid="R41">41</xref></sup> and current developmental theories<sup><xref ref-type="bibr" rid="R1">1</xref>,<xref ref-type="bibr" rid="R34">34</xref></sup>, we expected categorical phonetic feature encoding to emerge from 6 months on (i.e., from the 7mo recording session, in the present study), with progressively stronger encoding across the first year of life visible by 11 months of age. To test this hypothesis (see Hp<sub>2</sub> in <xref ref-type="fig" rid="F2">Figure 2A</xref>), phonetic feature encoding was assessed based on the multivariate TRF models described in the previous section. Neural activity linearly reflecting phonetic feature categories but not sound acoustics was accounted by subtracting EEG prediction correlations corresponding to acoustic-only TRFs (which did not include phonetic features; see <xref ref-type="sec" rid="S5">Methods</xref>) from those corresponding to acoustic-phonetic TRFs<sup><xref ref-type="fn" rid="FN3">ii</xref></sup>. For consistency with previous work<sup><xref ref-type="bibr" rid="R27">27</xref>,<xref ref-type="bibr" rid="R29">29</xref>,<xref ref-type="bibr" rid="R32">32</xref>,<xref ref-type="bibr" rid="R33">33</xref>,<xref ref-type="bibr" rid="R42">42</xref>–<xref ref-type="bibr" rid="R44">44</xref></sup>, this metric is referred to as FS-S (F: phonetic features; S: spectrogram and envelope derivative). As expected (Hp1-3), FS-S values were progressively larger with age (average across all EEG channels; <xref ref-type="fig" rid="F2">Figure 2B</xref>), with values greater than zero emerging from 11 months of age for the time-latency window 0-400ms (one-sample Wilcoxon rank sum test, FDR-corrected; 4mo: <italic>p</italic>=0.237; 7mo: <italic>p</italic>=0.237; <bold>11mo: <italic>p</italic>=0.037</bold>; <bold>adults: <italic>p</italic>=0.037</bold>; black bars indicate <italic>p</italic>&lt;0.05), and <bold>from 7 months</bold> of age for the time-latency window 100-500ms (one-sample Wilcoxon rank sum test, FDR-corrected; 4mo: <italic>p</italic>=0.167; <bold>7mo: <italic>p</italic>=0.044</bold>; <bold>11mo: <italic>p</italic>=0.023</bold>; <bold>adults: <italic>p</italic>=0.044</bold>). This latter finding is consistent with hypothesis 2 (Hp<sub>2</sub>), as depicted in <xref ref-type="fig" rid="F2">Figure 2A</xref>. Furthermore, the 4mo group did not show significant FS-S values for subsequent latency windows (200-600ms and 300-700ms).</p><p id="P10">While the previous analysis identified significant phonetic encoding within individual age groups, the analysis that follows explicitly assessed if phonetic encoding increased across the first year of life. TRF models and the corresponding EEG prediction correlations showed large between-subject variability, which was expected due to the noisy single-subject data. Under the assumption that participants in the same age group present EEG responses to nursery rhymes with similar temporal patterns, a Multiway Canonical Correlation Analysis (MCCA)<sup><xref ref-type="bibr" rid="R45">45</xref></sup> was carried out to isolate EEG components that are consistent within each group, substantially improving the signal-to-noise ratio of the single-subject EEG. A repeated measures ANOVA test was then carried out to determine if FS-S increased with age by considering the first MCCA component only (MCC1) i.e., the EEG component with highest temporal correlation across subjects within a given age group. A significant increasing trend emerged for the 100-500ms window (<italic>F</italic>(2,138)=3.19, <italic>p</italic>=0.044) but not for the 0-400ms latency window (<italic>F</italic>(2,138)=1.37, <italic>p</italic>=0.257; see coloured panel <xref ref-type="fig" rid="F2">Figure 2B</xref>). The test was also run when considering an increasing number of MCCs, showing significant results when considering up to five components for the 100-500ms latency window. The EEG encoding of phonetic features was also studied at individual electrodes, revealing robust encoding of phonetic features on large clusters of EEG channels in adults as well as infants from 7 months of age, both when considering 0-400ms and 100-500ms windows (<xref ref-type="fig" rid="F2">Figure 2C</xref>; FDR-corrected one-sample Wilcoxon rank sum tests were run on each EEG channel; colours indicate significant results with <italic>p</italic>&lt;0.05).</p><p id="P11">Further analyses were carried out to assess the phonetic feature encoding at a fine-grained level, by studying the TRF weights of the acoustic-phonetic TRF (weights are shown in <xref ref-type="fig" rid="F2">Figure 2D</xref>). Phonetic distance maps<sup><xref ref-type="bibr" rid="R33">33</xref></sup> were calculated by using a multidimensional scaling analysis (MDS; see <xref ref-type="sec" rid="S5">Methods</xref>) on the TRF weights corresponding to phonetic features. This approach allows to quantify and visualise the level of similarity or distance between datapoints by accounting simultaneously for multiple EEG channels and peri-stimulus time latencies. By selecting the two most relevant MDS dimensions, the infant-adult Euclidean distance was calculated for each age group (<xref ref-type="fig" rid="F3">Figure 3A</xref>; bars indicate mean and SE calculated across 27 adult phonemes computed as a linear combination of the corresponding phonetic features), showing that <bold>the infant-adult distance decreases with infant age</bold> in the first year of life (repeated measures ANOVA, <italic>F</italic>(2,52) = 18.2, <italic>p</italic> = 5.0*10<sup>−7</sup>). <italic>F</italic>-score measures were derived quantifying the discriminability of specific phonetic feature groups in the individual-subject TRFs using a <italic>k</italic>-means analysis (mean and SE were calculated on the <italic>F</italic>-scores resulting from the 100 repetitions of <italic>k</italic>-means). The phonetic feature groupings considered for this analysis were place of articulation, voicing, and manner of articulation (Figure 3B,C). As a validation step, the stability of the resulting <italic>F</italic>-scores for the infants in the three longitudinal sessions was assessed over 100 repetitions of the <italic>£</italic>-means procedure (repeated measures ANOVA, voicing: <italic>F</italic>(2,198) = 155.8, <italic>p</italic> &lt; 10<sup>−12</sup>; place of articulation: <italic>F</italic>(2,198) = 377.2, <italic>p</italic> &lt; 10<sup>−12</sup>; manner of articulation: <italic>F</italic>(2,198) = 29.4, <italic>p</italic> = 6.84*10<sup>−12</sup>; Figure 3B). Next, statistical analyses were carried out to determine the significance of the result across participants. As expected (due to factors such as low-SNR, limited data, and inter-subject variability), single-subject phonetic feature maps did not lead to significant results, even though results for place of articulation were trending towards significance (repeated measures ANOVA, voicing: <italic>F</italic>(2,92) = 0.6, <italic>p</italic> = 0.54; place of articulation: <italic>F</italic>(2,92) = 2.6, <italic>p</italic> = 0.08; manner of articulation: <italic>F</italic>(2,92) = 0.5, <italic>p</italic> = 0.61). To compensate for the limited single-subject data, we ran a bootstrap analysis with 100 repetitions, each derived by averaging 17 subjects i.e., the same number of participants in the adult group. This analysis revealed that phonetic feature encoding increased with age for place of articulation and voicing, but not manner of articulation (repeated measures ANOVA, <bold>voicing</bold>: <italic>F</italic>(2,198) = 21.3, <bold><italic>p</italic> = 4.2*10<sup>−9</sup></bold>; <bold>place of articulation</bold>: <italic>F</italic>(2,198) = 17.6, <bold><italic>p</italic> = 9.0*10<sup>−8</sup></bold>; manner of articulation: <italic>F</italic>(2,198) = 1.5, <italic>p</italic> = 0.22).</p></sec></sec><sec id="S4" sec-type="discussion"><title>Discussion</title><p id="P12">The present investigation offers the first direct evidence that the human cortex encodes phonetic categories during the first year of life, demonstrating significant phonetic encoding from 7 months of age and progressively stronger encoding thereafter. A fine-grained and longitudinal understanding of the development of phonetic feature encoding by the same infants listening to continuous speech was previously absent from the literature. The behavioural and MMR infant speech processing literature has used targeted experimental contrasts, focused largely on the perception of syllable stress and speech rhythm and on phonetic category formation. As rhythm and stress patterns aid in identifying word boundaries, and phonetic categories aid in comprehension (e.g., distinguishing ‘doggy’ from ‘daddy’), this prior work has been important, showing that infants are sensitive to differences in speech rhythm from birth<sup><xref ref-type="bibr" rid="R46">46</xref>,<xref ref-type="bibr" rid="R47">47</xref></sup>, and are sensitive to some phonetic information as neonates<sup><xref ref-type="bibr" rid="R48">48</xref></sup>. Nevertheless, no prior study has used continuous speech as a basis for studying phonetic encoding. Consequently, our findings have several implications for understanding of the development of speech processing.</p><p id="P13">Currently, it remains unclear how and at what stage of development phonetic category encoding is learnt. This question remains open largely because of methodological constraints. The present study offers, for the first time, direct evidence on the ‘when’ of phonetic category learning. There is a consensus in the literature that discriminating phonetic categories is a key processing step regarding speech comprehension by adults<sup><xref ref-type="bibr" rid="R49">49</xref></sup>, although see Feldman et al., 2021<sup><xref ref-type="bibr" rid="R50">50</xref></sup> for recent caveats regarding infants. While adult studies used direct invasive recordings to measure the cortical encoding of phonetic categories<sup><xref ref-type="bibr" rid="R51">51</xref></sup>, recent methodological developments (i.e., the TRF framework<sup><xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R8">8</xref></sup>) allowed us to circumvent some of the major challenges encountered by previous infant studies, thereby providing more precise developmental information.</p><p id="P14">The assessment of phonetic encoding as operationalised here fulfils three main elements of novelty that go beyond any previous investigation. First, we studied the cortical encoding of phonetic categories in infants with <bold>direct neural measurements</bold> based on EEG and as part of an unprecedented targeted longitudinal investigation. Second, the use of the forward TRF framework allowed us to assess phonetic category <bold>encoding</bold>, rather than relying on the typical sound discrimination metrics used in prior behavioural<sup><xref ref-type="bibr" rid="R35">35</xref>,<xref ref-type="bibr" rid="R37">37</xref>–<xref ref-type="bibr" rid="R41">41</xref></sup> and neurophysiology studies (e.g., MMR)<sup><xref ref-type="bibr" rid="R4">4</xref>,<xref ref-type="bibr" rid="R52">52</xref>–<xref ref-type="bibr" rid="R56">56</xref></sup>. Third, the TRF framework allowed us to study the perception of <bold>natural speech</bold> in infants, instead of focusing on selected phonetic or word contrasts, as in the past literature. This is a crucial step forward, as the discriminatory skills that infants exhibit in simplified laboratory settings (e.g., isolated syllable discrimination measured via a head-turn or looking procedure or with MMRs) may not be sufficient for detecting phonetic categories in naturalistic settings.</p><p id="P15">The present study indicates that phonetic category encoding during natural speech listening emerges between 5 and 7 months of age. This provides the literature with new and fundamental insights into the development of speech processing in neurotypical infants. Further, the TRF approach yields novel information on <italic>which</italic> specific phonetic contrasts evolve with age, demonstrating a natural progression toward adult phonetic encoding. This insight is further reinforced by the observation that acoustic encoding did not increase with age. Consequently, the enhanced phonetic encoding with age observed here could not simply be due to stronger acoustic encoding, as acoustic encoding showed the opposite pattern (a non-significant decreasing trend with age). Interestingly, our results suggest that 4mo pre-babbling infants, despite being equipped with the fundamental combinatorial code for speech analysis<sup><xref ref-type="bibr" rid="R57">57</xref></sup>, do not yet exhibit categorical phonetic encoding. Based on these results, we can speculate that prior demonstrations of infant behavioural and MMR discrimination between syllables like “pa” and “ba” probably have an acoustic basis but do not reflect categorical phonetic encoding. In other words, the ability to distinguish two sounds does not necessarily mean that those sounds are encoded as separate categories. Our study is instead probing that categorical encoding directly.</p><p id="P16">One challenge with longitudinal neurophysiology studies in infants is the substantial anatomical change that occurs with age, meaning that while macroscopic patterns are likely to remain consistent (e.g., temporal vs. occipital), there cannot be a channel-by-channel correspondence between age groups, even when considering the same participants. For this reason, the majority of this investigation focused on measures combining multiple EEG channels simultaneously (e.g., <xref ref-type="fig" rid="F1">Figure 1D</xref> was an average of 14 centro-frontal channels). These considerations make the topographical distribution of phonetic encoding strength shown in <xref ref-type="fig" rid="F2">Figure 2C</xref> even more remarkable, as a centro-frontal cluster of EEG channels was shown to reflect phonetic encoding across all age groups with the exception of 4 months, where phonetic encoding as assessed by the TRF was not significant.</p><p id="P17">Our phonetic encoding results showed topographical patterns and TRF weights for adults that differ from the prior adult EEG literature on natural speech listening TRFs<sup><xref ref-type="bibr" rid="R27">27</xref>,<xref ref-type="bibr" rid="R29">29</xref></sup>. While part of the discrepancy may be due to the use of a different EEG acquisition device and to the use of audio-visual stimuli, the primary explanation is likely to be the choice of stimuli. This is the first TRF investigation of phonetic processing with adults involving a nursery rhyme listening task. Nursery rhymes are indeed a form of natural speech which is more suited to infants. The rhythmic cues and exaggerated stress patterns characterising nursery rhymes have been demonstrated to be important elements supporting speech perception and language learning<sup><xref ref-type="bibr" rid="R58">58</xref>,<xref ref-type="bibr" rid="R59">59</xref></sup>, accordingly they were ideal stimuli for the Cambridge UK BabyRhythm study. In prior TRF work, we have demonstrated similar envelope entrainment to these nursery rhymes by adults and infants<sup><xref ref-type="bibr" rid="R26">26</xref></sup>. Nevertheless, it is important to note that the regular rhythms and melodic properties of nursery rhymes makes the different from the typical speech TRF stimuli used with adults, such as audio-books and podcasts. As such, the TRF results were expected to show different spatio-temporal patterns for adult listeners compared to previous TRF work.</p><p id="P18">The results of this study add to the growing literature on cortical speech tracking<sup><xref ref-type="bibr" rid="R21">21</xref>,<xref ref-type="bibr" rid="R27">27</xref>,<xref ref-type="bibr" rid="R31">31</xref>,<xref ref-type="bibr" rid="R33">33</xref>,<xref ref-type="bibr" rid="R44">44</xref>,<xref ref-type="bibr" rid="R60">60</xref>–<xref ref-type="bibr" rid="R62">62</xref></sup>. While the literature typically focuses on the cortical tracking of the speech envelope<sup><xref ref-type="bibr" rid="R16">16</xref>,<xref ref-type="bibr" rid="R24">24</xref>,<xref ref-type="bibr" rid="R63">63</xref>–<xref ref-type="bibr" rid="R66">66</xref></sup> (including previous analyses of this dataset<sup><xref ref-type="bibr" rid="R21">21</xref>,<xref ref-type="bibr" rid="R26">26</xref></sup>), the present investigation enriches our understanding of phonetic feature TRFs. Prior TRF studies of phonetic encoding in adults and children have revealed that phonetic processing is affected by speech clarity<sup><xref ref-type="bibr" rid="R43">43</xref></sup>, selective attention<sup><xref ref-type="bibr" rid="R29">29</xref></sup>, and proficiency in a second language<sup><xref ref-type="bibr" rid="R33">33</xref></sup>, and shows correlations with psychometric measures of phonemic awareness<sup><xref ref-type="bibr" rid="R32">32</xref></sup>. The present study demonstrates that emergent phonetic TRFs can also be measured in pre-verbal infants, providing a novel window into infant perception and cognition. Whilst recent developments have started to use neural tracking to predict language development in infants<sup><xref ref-type="bibr" rid="R67">67</xref></sup>, further research will also determine whether a robust relationship exists between speech TRFs and other related aspects of cognition (e.g., selective attention, prediction) in infants, and when such related aspects come on-line. Further research with infants at family risk for disorders of language learning may also reveal when and how developmental trajectories are impacted by developmental disorders that are carried genetically, such as developmental dyslexia and developmental language disorder. Such work could be very valuable regarding early detection and improved mechanistic understanding of these disorders.</p><p id="P19">In summary, this study demonstrated the emergence of phonetic encoding from 7 months of age using direct neural measurements during natural speech listening. The data provide clear-cut evidence of the emergence of phonetic categories that contributes to the current debate regarding their role in the development of speech processing. Our demonstration that phonetic encoding can be assessed with nursery rhyme stimuli in ecologically-valid conditions opens the door to cross-language work using TRFs that investigates the interaction between characteristics of natural language such as phonological complexity and the development of phonetic encoding. It also provides opportunities for novel mechanistic investigations of the development of bi-lingual and multi-lingual lexicons during language acquisition.</p></sec><sec id="S5" sec-type="methods"><title>Online Methods</title><sec id="S6"><title>Subjects and experimental procedure</title><p id="P20">The present study carried out a re-analysis of an EEG dataset involving a speech listening task in a longitudinal cohort of fifty infants (first part of a larger cohort of 122 subjects<sup><xref ref-type="bibr" rid="R21">21</xref></sup>). Participants were infants born full term (37-42 gestational weeks) and had no diagnosed developmental disorder, recruited from a medium sized city in the United Kingdom and surrounding areas via multiple means (e.g., flyers in hospitals, schools, and antenatal classes, research presentations at maternity classes, online advertising). The study was approved by the Psychology Research Ethics Committee of the University of Cambridge. Parents gave written informed consent after a detailed explanation of the study and families were repeatedly reminded that they could withdraw from the study at any point during the repeated appointment. The experiment involved three EEG recording sessions when the infants (24 male and 26 female) were 4 months old (4mo; 115.6 ± 5.3 days), 7 months old (7mo; 212.5 ± 7.2 days) and 11 months old (11mo; 333.0 ± 5.5 days) [mean ± standard deviation (<italic>SD</italic>)]. A bilingualism questionnaire (collected from 45 out of the 50 infants) ascertained that 38 of the infants were exposed to a monolingual environment and 12 were exposed multilingual environment, of these 93.5% (43 infants) reported English as the primary language exposed to the infant. Note that this was a longitudinal investigation, meaning that the same 50 infants were tested at 4, 7, and 11 months of age. In addition to the 150 EEG sessions from the infant dataset, this study also analysed EEG data from twenty-two monolingual, English-speaking adult participants performing the same listening task (11 male, aged 18-30, mean age: 21). Data from four adult participant was excluded due to inconsistencies with the synchronisation triggers, leaving seventeen participants data for the analysis.</p><p id="P21">Infant participants were seated in a highchair (one metre in front of their primary caregiver) in a sound-proof acoustic chamber, while adult participants were seated in a normal chair. All participants were seated 650mm away from the presentation screen. EEG data were recorded at a sampling rate of 1 kHz using a GES 300 amplifier using a Geodesic Sensor Net (Electrical Geodesics Inc., Eugene, OR, United States). 64 and 128 channels were used for infants and adults respectively. Sounds were presented at 60 dB from speakers placed either side of the screen (Q acoustics 2020i driven by a Cambridge Audio Topaz AM5 Stereo amplifier). Participants were presented with eighteen nursery rhyme videos played sequentially, each repeated 3 times (54 videos with a presentation time of 20’ 33” in total). Adult participants were asked to attend to the audio-visual stimulus while minimising their motor movements. All adult participants completed the full experiment. Infants listened to at least two repetitions of each nursery rhyme (minimum of 36 nursery rhymes lasting 13’ 42”). The experiment included other elements that were not relevant to the present study (e.g., resting state EEG; please refer to the previous papers on this dataset for further information<sup><xref ref-type="bibr" rid="R21">21</xref>,<xref ref-type="bibr" rid="R26">26</xref></sup>).</p></sec><sec id="S7"><title>Stimuli</title><p id="P22">A selection of eighteen typical English language nursery rhymes was chosen as the stimuli. Audio-visual stimuli of a singing person (upper-body only) were recorded using a Canon XA20 video camera at 1080p, 50fps and with audio at 4800 Hz. A native female speaker of British English used infant-directed speech to melodically sing (for example “Mary Quite Contrary”) or rhythmically chant (for nursery rhymes like “There was an old woman who lived in a shoe”) the nursery rhymes whilst listening to a 120 bpm metronome through an intra-auricular headphone (e.g., allowing for 1Hz and 2Hz beat rates; see Figs. S2 and S4 from Attaheri et al.<sup><xref ref-type="bibr" rid="R21">21</xref></sup>). The metronome’s beat was not present on the stimulus audios and videos, but it ensured that a consistent rhythmic production was maintained throughout the 18 nursery rhymes. To ensure natural vocalisations, the nursery rhyme videos were recorded sung, or rhythmically chanted, live to an alert infant.</p></sec><sec id="S8"><title>Data preprocessing</title><p id="P23">Analyses were conducted with MATLAB 2021a by using custom scripts developed starting from publicly available scripts shared by the CNSP initiative (Cognition and Natural Sensory Processing; <ext-link ext-link-type="uri" xlink:href="https://cnspworkshop.net/">https://cnspworkshop.net</ext-link>; see section Data and Code Availability for further details).</p><p id="P24">In order to carry out the same preprocessing and analysis pipeline on infants and adult EEG data, the adult 128-channel EEG data was transformed into a 64-channel dataset via spline interpolation, with the relative channel locations corresponding to those of the infant participants. All subsequent analyses on infants and adult were identical.</p><p id="P25">The four facial electrodes (channels 61-64) were excluded from all analyses, as they are not part of the specific infant-sized EGI Geodesic sensor net. The EEG data from the remaining 60 channels was band-pass filtered between 1 and 15 Hz by means of zero-phase shift Butterworth filters with order 2 (by using the filtering functions in the CNSP resources). EEG signals were downsampled to 50 Hz. Next, Artifact Subspace Reconstruction (ASR; <italic>clean</italic>_<italic>asr</italic> function from EEGLAB<sup><xref ref-type="bibr" rid="R68">68</xref></sup>) was used to clean noise artifacts from the EEG signals. Channels with excessive noise (which could not be corrected with ASR) were identified via probability and kurtosis and were interpolated via spherical interpolation, if they were three standard deviations away from the mean. EEG signals were then re-referenced to the average of the two mastoid channels, which were then removed from the data, producing a preprocessed EEG dataset with 58 channels. Data from repeated trials was then averaged. Three infant subjects were removed because of excessive noise in at least one of their three recording sessions.</p></sec><sec id="S9"><title>Sung speech representations</title><p id="P26">The present study involved the measurement of the coupling between EEG data and various properties of the sung speech stimuli. These properties were extracted from the stimulus data based on methodologies developed in previous research. First, we defined a set of descriptors summarising low-level acoustic properties of the speech stimuli. Acoustic features consisted of an 8-band acoustic spectrogram (S) and a half-way rectified broadband envelope derivative (D)<sup><xref ref-type="bibr" rid="R33">33</xref>,<xref ref-type="bibr" rid="R60">60</xref></sup>. S was obtained by filtering the sound waveform into eight frequency bands between 250 and 8 kHz that were logarithmically spaced according to the Greenwood equation<sup><xref ref-type="bibr" rid="R69">69</xref></sup>. The broadband envelope was calculated as the sum across the eight frequency bands of S. The D signal was then derived by calculating the derivative of the broadband envelope, and by half-way rectifying the resulting signal. Second, fourteen phonetic features were then selected to mark the categorical occurrence of speech sounds, according to articulatory features describing voicing, manner, and place of articulation<sup><xref ref-type="bibr" rid="R70">70</xref>,<xref ref-type="bibr" rid="R71">71</xref></sup>: voiced consonant, unvoiced consonant, plosive, fricative, nasal, strident, labial, coronal, dorsal, anterior, front, back, high, low. To account for possible differences in the encoding of stressed and unstressed sounds, each phonetic feature was assigned to two distinct vectors, leading to a 28-dimensional phonetic features matrix (<italic>F</italic>). The precise timing of the phonetic units was identified in three steps. First, syllable and phoneme sequences were obtained from the transcripts of the nursery rhymes. Second, an initial alignment was derived by identifying the syllabic rate and syllable onsets for each piece, and then assigning the phonemes in a syllable starting from the corresponding onset time. This automatic alignment was stored according to the TextGrid format<sup><xref ref-type="bibr" rid="R72">72</xref></sup>. Third, the phoneme alignments were manually adjusted using Praat software<sup><xref ref-type="bibr" rid="R72">72</xref></sup>. Phonetic feature vectors were produced in MATLAB software to categorically mark the occurrence of phonetic units from start to finish with unit rectangular pulses<sup><xref ref-type="bibr" rid="R27">27</xref></sup>. Finally, a nuisance regressor was also included to capture EEG variance related with visual motion (V), which was derived as the frame-to-frame luminance change, averaged across all pixels.</p></sec><sec id="S10"><title>Multivariate Temporal Response Function (mTRF)</title><p id="P27">A single input event at time <italic>t<sub>0</sub></italic> affects the neural signals for a certain time window [<italic>t<sub>1</sub>, t<sub>1</sub></italic>+<italic>t</italic><sub>win</sub>], with <italic>t<sub>1</sub></italic> ≥ 0 and <italic>t<sub>win</sub></italic> &gt; 0. Temporal response functions (TRFs) describe this relationship at the level of individual subject and EEG channel. In this study, TRFs were estimated by means of a multivariate lagged regression, which determines the optimal linear transformation from stimulus features to EEG (forward model)<sup><xref ref-type="bibr" rid="R13">13</xref>,<xref ref-type="bibr" rid="R73">73</xref></sup>. A multivariate TRF model (mTRF) was fit for each subject by considering all features simultaneously (S, D, F, and V; <xref ref-type="fig" rid="F1">Figure 1A</xref>) with the mTRF-Toolbox<sup><xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R8">8</xref></sup>. While a time-lag window of 0-400 ms was considered sufficient to largely capture the acoustic-phonetic/EEG relationship with a single-speaker listening task in adults, based on previous studies (e.g., Di Liberto et al.<sup><xref ref-type="bibr" rid="R27">27</xref></sup>), the relevant latencies in infants were unknown. To account for possible slower or delayed response in infants, a time-latency window of 100-500 ms (i.e., same duration but with longer latency) was also included in the analysis for all groups. The reliability of the TRF models was assessed using a leave-one-out cross-validation procedure (across trials i.e., nursery rhymes), which quantified the EEG prediction correlation (Pearson’s <italic>r</italic>) on unseen data while controlling for overfitting. The TRF model calculation included a Tikhonov regularization, which involves the tuning of a regularization parameter (λ) that was conducted by means of an exhaustive search of a logarithmic parameter space from 0.01 to 10<sup>6</sup> on the training fold of each cross-validation iteration<sup><xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R8">8</xref></sup>. Note that the correlation values were calculated with the noisy EEG signal; therefore, the <italic>r</italic>-scores could be highly significant even though they have low absolute values (<italic>r</italic> ~ 0.1 for sensor-space low-frequency EEG<sup><xref ref-type="bibr" rid="R27">27</xref>,<xref ref-type="bibr" rid="R30">30</xref>,<xref ref-type="bibr" rid="R60">60</xref></sup>).</p></sec><sec id="S11"><title>Phonetic distance maps</title><p id="P28">We sought to study the effect of development on phonetic perception by projecting the TRF weights corresponding with the phonetic features onto a space in which distance represents the perceptual separation between phonological units. The TRF weights for the phonetic features, which were represented in a 28-dimensional space, were projected to the phoneme space. To do so, the TRF for a given phoneme was calculated as the sum of the TRF weights of the corresponding features. This produced a 54-dimensional matrix: 27 stressed and 27 unstressed phonemes. The weights corresponding to the two versions of a phoneme (stressed and unstressed) were then combined when projecting to the phonetic feature map, obtaining a 27-dimensional space. Specifically, a classical multidimensional scaling (MDS) was used to project the phoneme TRF weights (phonemes were considered as objects and time latencies were considered as dimensions) onto a multidimensional space for each age group, in which distances represented the discriminability of particular phonetic contrasts in the EEG signal. The result for each infant group was then mapped to the average adult MDS space by means of a Procrustes analysis (MATLAB function <italic>procrustes</italic>). This analysis allowed us to project the infant phonetic feature maps for different proficiency levels to a common multidimensional space where they could be compared quantitatively; we call these maps phonetic feature distance maps. Note that while this transformation does not assume nor imply categorical phonemic encoding, it indeed allows to quantify and visualise the encoding of phonetic features across age, by using familiar phonemic units.</p><p id="P29">While Figure 3A shows the infant-adult distances in the MDS space, the effect of age on phonetic feature encoding was also quantified with a clustering analysis. Specifically, the randomised clustering algorithm <italic>k</italic>-means was run on the MDS maps for each group to determine whether the phonetic feature groups could be deduced from the data without supervision. We performed 100 repetitions of <italic>k</italic>-means and a classification <italic>F</italic>-Score (or F<sub>1</sub>-Score) was calculated on each of them, obtained as the harmonic mean of precision and recall. We performed this procedure for the three feature sets of interest: a) the three-class feature-set with ‘vowels’, ‘voiced consonants’, and ‘unvoiced consonants’; b) the three-class feature-set describing the place of articulation, with features ‘labial’, ‘coronal’, and ‘dorsal’; and c) the three-class feature-set describing the manner of articulation, with features ‘fricative’, ‘stop’, and ‘nasal’. Since all feature-sets were three-dimensional, <italic>k</italic> had always value three. Note that <italic>k</italic>-means is an unsupervised clustering algorithm, meaning that there was no direct correspondence between the clusters and the classes of interest. As such, <italic>F</italic>-scores were selected for the best matching assignment of classes on each execution of <italic>k</italic>-means. A large <italic>F</italic>-score corresponded to a strong encoding of the feature-set of interest in the EEG data.</p></sec><sec id="S12"><title>Multiway Canonical Correlation Analysis (MCCA)</title><p id="P30">EEG data is notorious for its low signal-to-noise ratio (SNR), which represent one of the core challenges when analysing this kind of data. One approach to improve the SNR is multiway canonical correlation analysis (MCCA), a tool that identifies EEG components that are most correlated across subjects. Under the assumption that a stimulus would produce consistent cortical responses across subjects in the same age-group, MCCA identifies such consistent responses accepting that they may originate from distinct sources (i.e., distinct topographical patterns) for different subjects. MCCA is an extension of canonical correlation analysis<sup><xref ref-type="bibr" rid="R74">74</xref></sup> to the case of multiple (&gt; 2) subjects. Given N multichannel datasets Y<sub>i</sub> with size T x J<sub>i</sub>, 1 ≤ i ≤ N (time x channels), MCCA finds a linear transform W<sub>i</sub> (sizes J<sub>i</sub> x J<sub>0</sub>, where J<sub>0</sub> &lt; min(J<sub>i</sub>)<sub>1≤i≤N</sub>), which, when applied to the corresponding data matrices, aligns them to common coordinates and reveals shared patterns<sup><xref ref-type="bibr" rid="R45">45</xref></sup>. These patterns can be derived by summing the transformed data matrices as follows: <inline-formula><mml:math id="M1"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msub><mml:mi>Y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>W</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:math></inline-formula> The columns of the matrix Y, which are mutually orthogonal, are referred to as summary components (SC). The first components are signals that most strongly reflect the shared information across the several input datasets, thus minimising subject-specific and channel-specific noise. Here, MCCA was run within each age group (4mo, 7mo, 11mo, and adults). After fitting the MCCA mapping and projecting the data to the SC space, a given number of component was retained (e.g., only the first component) before performing the inverse mapping and obtain a denoised version of the EEG signal for each subject. This denoising procedure was repeated by retaining a progressive number of components (<xref ref-type="fig" rid="F2">Figure 2B</xref>).</p></sec><sec id="S13"><title>Statistical analysis</title><p id="P31">All statistical analyses directly comparing the groups were performed using a repeated measures ANOVA. One-sample Wilcoxon signed-rank tests were used for post hoc tests. Correction for multiple comparisons was applied where necessary via the false discovery rate (FDR) approach. In that case, the FDR adjusted <italic>p</italic>-value was reported. Descriptive statistics for the neurophysiology results are reported as a combination of mean and standard error (SE).</p></sec></sec></body><back><ack id="S14"><title>Acknowledgements</title><p>We thank Dimitris Panayiotou, Alessia Philips, Natasha Mead, Helen Olawole-Scott, Panagiotis Boutris, Samuel Gibbon, Isabel Williams, Sheila Flanagan, and Christina Grey who helped collecting the data as well as all the families of the infant participants. We thank Dr. Susan Richards for her assistance on the phoneme transcription. We thank the CogHear workshop organisers (Mounya Elhilali, Malcolm Slaney, and Shihab Shamma) and participants for their useful feedback on the early results of this study.</p><sec id="S15"><title>Funding sources</title><p>This project received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (Grant Agreement No. 694786) (A.A., U.G.). This research was conducted with the financial support of Science Foundation Ireland under Grant Agreement No. 13/RC/2106_P2 at the ADAPT SFI Research Centre at Trinity College Dublin (G.D.L., G.C.). ADAPT, the SFI Research Centre for AI-Driven Digital Content Technology, is funded by Science Foundation Ireland through the SFI Research Centres Programme. This work was also supported by the Science Foundation Ireland Career Development Award 15/CDA/3316 (G.D.L., R.R.). G.C. was supported by an Advanced European Research Council grant (NEUME, 787836) and by the FrontCog grant ANR-17-EURE-0017.</p></sec></ack><sec sec-type="data-availability" id="S16"><title>Data and code availability</title><p id="P32">Analyses were conducted by using custom MATLAB scripts developed starting from publicly available scripts shared by the CNSP initiative (Cognition and Natural Sensory Processing; <ext-link ext-link-type="uri" xlink:href="https://cnspworkshop.net/">https://cnspworkshop.net</ext-link>). Such analysis scripts avail of external publicly available libraries: the mTRF-Toolbox (<ext-link ext-link-type="uri" xlink:href="https://github.com/mickcrosse/mTRF-Toolbox">https://github.com/mickcrosse/mTRF-Toolbox</ext-link>)<sup><xref ref-type="bibr" rid="R8">8</xref></sup>, EEGLAB<sup><xref ref-type="bibr" rid="R68">68</xref></sup>; and the NoiseTools library (<ext-link ext-link-type="uri" xlink:href="http://audition.ens.fr/adc/NoiseTools">http://audition.ens.fr/adc/NoiseTools</ext-link>)<sup><xref ref-type="bibr" rid="R45">45</xref></sup>. Data was converted to the CND data structure (Continuous-event Neural Data - <ext-link ext-link-type="uri" xlink:href="https://cnspworkshop.net/">https://cnspworkshop.net</ext-link>), allowing to carry out the analyses with the CNSP analysis scripts, which provided a platform for bringing together all the necessary libraries. We commit to publicly share the EEG data in the first half of 2023. Study data were collected and managed using REDCap (Research Electronic Data Capture) electronic data capture tools hosted at Cambridge university<sup><xref ref-type="bibr" rid="R75">75</xref>,<xref ref-type="bibr" rid="R76">76</xref></sup>.</p></sec><fn-group><fn id="FN1" fn-type="conflict"><p id="P33">Conflicts of interest: none declared.</p></fn><fn id="FN2"><label>i</label><p id="P34">This should not be intended as a hard boundary, as this is likely a gradual phenomenon that changes over large time windows, with differences between easy and more difficult speech contrasts</p></fn><fn id="FN3"><label>ii</label><p id="P35">Please note that results did not change when acoustic-only TRFs consisted of acoustic vectors concatenated with shuffled phonetic information.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuhl</surname><given-names>PK</given-names></name></person-group><article-title>Early language acquisition: cracking the speech code</article-title><source>Nat Rev Neurosci</source><year>2004</year><volume>5</volume><fpage>831</fpage><lpage>843</lpage><pub-id pub-id-type="doi">10.1038/nrn1533</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuhl</surname><given-names>P</given-names></name><name><surname>Rivera-Gaxiola</surname><given-names>M</given-names></name></person-group><article-title>Neural Substrates of Language Acquisition</article-title><source>Annual Review of Neuroscience</source><year>2008</year><volume>31</volume><fpage>511</fpage><lpage>534</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.30.051606.094321</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuhl</surname><given-names>PK</given-names></name></person-group><article-title>Brain mechanisms in early language acquisition</article-title><source>Neuron</source><year>2010</year><volume>67</volume><fpage>713</fpage><lpage>727</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.08.038</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>YJ</given-names></name><name><surname>Hou</surname><given-names>X</given-names></name><name><surname>Peng</surname><given-names>C</given-names></name><name><surname>Yu</surname><given-names>W</given-names></name><name><surname>Oppenheim</surname><given-names>GM</given-names></name><name><surname>Thierry</surname><given-names>G</given-names></name><name><surname>Zhang</surname><given-names>D</given-names></name></person-group><article-title>Rapid learning of a phonemic discrimination in the first hours of life</article-title><source>Nature Human Behaviour</source><year>2022</year><volume>6</volume><fpage>1169</fpage><lpage>1179</lpage><pub-id pub-id-type="doi">10.1038/s41562-022-01355-1</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene-Lambertz</surname><given-names>G</given-names></name><name><surname>Gliga</surname><given-names>T</given-names></name></person-group><article-title>Common neural basis for phoneme processing in infants and adults</article-title><source>J Cogn Neurosci</source><year>2004</year><volume>16</volume><fpage>1375</fpage><lpage>1387</lpage><pub-id pub-id-type="doi">10.1162/0898929042304714</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Csibra</surname><given-names>G</given-names></name><name><surname>Kushnerenko</surname><given-names>E</given-names></name><name><surname>Grossmann</surname><given-names>T</given-names></name></person-group><article-title>Electrophysiological methods in studying infant cognitive development</article-title><year>2008</year></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crosse</surname><given-names>MJ</given-names></name><name><surname>Zuk</surname><given-names>NJ</given-names></name><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Nidiffer</surname><given-names>AR</given-names></name><name><surname>Molholm</surname><given-names>S</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><article-title>Linear Modeling of Neurophysiological Responses to Speech and Other Continuous Stimuli: Methodological Considerations for Applied Research</article-title><source>Frontiers in neuroscience</source><year>2021</year><volume>15</volume><elocation-id>705621</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2021.705621</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crosse</surname><given-names>MJ</given-names></name><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Bednar</surname><given-names>A</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><article-title>The multivariate temporal response function (mTRF) toolbox: A MATLAB toolbox for relating neural signals to continuous stimuli</article-title><source>Frontiers in Human Neuroscience</source><year>2016</year><volume>10</volume><pub-id pub-id-type="doi">10.3389/fnhum.2016.00604</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Port</surname><given-names>R</given-names></name></person-group><article-title>How are words stored in memory? Beyond phones and phonemes</article-title><source>New Ideas in Psychology</source><year>2007</year><volume>25</volume><fpage>143</fpage><lpage>170</lpage><pub-id pub-id-type="doi">10.1016/j.newideapsych.2007.02.001</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ziegler</surname><given-names>JC</given-names></name><name><surname>Goswami</surname><given-names>U</given-names></name></person-group><article-title>Reading acquisition, developmental dyslexia, and skilled reading across languages: a psycholinguistic grain size theory</article-title><source>Psychol Bull</source><year>2005</year><volume>131</volume><fpage>3</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1037/0033-2909.131.1.3</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Obleser</surname><given-names>J</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name></person-group><article-title>Neural Entrainment and Attentional Selection in the Listening Brain</article-title><source>Trends in Cognitive Sciences</source><publisher-name>Elsevier Ltd</publisher-name><year>2019</year></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lalor</surname><given-names>EC</given-names></name><name><surname>Foxe</surname><given-names>JJ</given-names></name></person-group><article-title>Neural responses to uninterrupted natural speech can be extracted with precise temporal resolution</article-title><source>European Journal of Neuroscience</source><year>2010</year><volume>31</volume><fpage>189</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2009.07055.x</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lalor</surname><given-names>EC</given-names></name><name><surname>Power</surname><given-names>AJ</given-names></name><name><surname>Reilly</surname><given-names>RB</given-names></name><name><surname>Foxe</surname><given-names>JJ</given-names></name></person-group><article-title>Resolving Precise Temporal Processing Properties of the Auditory System Using Continuous Stimuli</article-title><source>Journal of Neurophysiology</source><year>2009</year><volume>102</volume><fpage>349</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1152/jn.90896.2008</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aiken</surname><given-names>SJ</given-names></name><name><surname>Picton</surname><given-names>TW</given-names></name></person-group><article-title>Human cortical responses to the speech envelope</article-title><source>Ear and Hearing</source><year>2008</year><volume>29</volume><fpage>139</fpage><lpage>157</lpage></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>N</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><article-title>Emergence of neural encoding of auditory objects while listening to competing speakers</article-title><source>Proc Natl Acad Sci U S A</source><year>2012</year><volume>109</volume><fpage>11854</fpage><lpage>11859</lpage><pub-id pub-id-type="doi">10.1073/pnas.1205381109</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Sullivan</surname><given-names>JA</given-names></name><name><surname>Power</surname><given-names>AJ</given-names></name><name><surname>Mesgarani</surname><given-names>N</given-names></name><name><surname>Rajaram</surname><given-names>S</given-names></name><name><surname>Foxe</surname><given-names>JJ</given-names></name><name><surname>Shinn-Cunningham</surname><given-names>BG</given-names></name><name><surname>Slaney</surname><given-names>M</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><article-title>Attentional Selection in a Cocktail Party Environment Can Be Decoded from Single-Trial EEG</article-title><source>Cerebral Cortex</source><year>2014</year><elocation-id>bht355</elocation-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mesgarani</surname><given-names>N</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name></person-group><article-title>Selective cortical representation of attended speaker in multi-talker speech perception</article-title><source>Nature</source><year>2012</year><volume>485</volume><fpage>233</fpage><lpage>U118</lpage><pub-id pub-id-type="doi">10.1038/nature11020</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hjortkjaer</surname><given-names>J</given-names></name><name><surname>Märcher-Rørsted</surname><given-names>J</given-names></name><name><surname>Fuglsang</surname><given-names>SA</given-names></name><name><surname>Dau</surname><given-names>T</given-names></name></person-group><article-title>Cortical oscillations and entrainment in speech processing during working memory load</article-title><source>Eur J Neurosci</source><year>2020</year><volume>51</volume><fpage>1279</fpage><lpage>1289</lpage><pub-id pub-id-type="doi">10.1111/ejn.13855</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leonard</surname><given-names>MK</given-names></name><name><surname>Baud</surname><given-names>MO</given-names></name><name><surname>Sjerps</surname><given-names>MJ</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name></person-group><article-title>Perceptual restoration of masked speech in human cortex</article-title><source>Nature Communications</source><year>2016</year><volume>7</volume><elocation-id>13619</elocation-id><comment><ext-link ext-link-type="uri" xlink:href="http://www.nature.com/articles/ncomms13619#supplementary-information">http://www.nature.com/articles/ncomms13619#supplementary-information</ext-link></comment><pub-id pub-id-type="doi">10.1038/ncomms13619</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name><name><surname>Millman</surname><given-names>RE</given-names></name></person-group><article-title>Causal cortical dynamics of a predictive enhancement of speech intelligibility</article-title><source>NeuroImage</source><year>2018</year><volume>166</volume><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.10.066</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Attaheri</surname><given-names>A</given-names></name><name><surname>Choisdealbha</surname><given-names>ÁN</given-names></name><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Rocha</surname><given-names>S</given-names></name><name><surname>Brusini</surname><given-names>P</given-names></name><name><surname>Mead</surname><given-names>N</given-names></name><name><surname>Olawole-Scott</surname><given-names>H</given-names></name><name><surname>Boutris</surname><given-names>P</given-names></name><name><surname>Gibbon</surname><given-names>S</given-names></name><name><surname>Williams</surname><given-names>I</given-names></name><etal/></person-group><article-title>Delta-and theta-band cortical tracking and phase-amplitude coupling to sung speech by infants</article-title><source>Neuroimage</source><year>2022</year><volume>247</volume><elocation-id>118698</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2021.118698</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jessen</surname><given-names>S</given-names></name><name><surname>Fiedler</surname><given-names>L</given-names></name><name><surname>Münte</surname><given-names>TF</given-names></name><name><surname>Obleser</surname><given-names>J</given-names></name></person-group><article-title>Quantifying the individual auditory and visual brain response in 7-month-old infants watching a brief cartoon movie</article-title><source>NeuroImage</source><year>2019</year><volume>202</volume><elocation-id>116060</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.116060</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jessica Tan</surname><given-names>SH</given-names></name><name><surname>Kalashnikova</surname><given-names>M</given-names></name><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Crosse</surname><given-names>MJ</given-names></name><name><surname>Burnham</surname><given-names>D</given-names></name></person-group><article-title>Seeing a talking face matters: The relationship between cortical tracking of continuous auditory - visual speech and gaze behaviour in infants, children and adults</article-title><source>NeuroImage</source><year>2022</year><volume>256</volume><elocation-id>119217</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119217</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalashnikova</surname><given-names>M</given-names></name><name><surname>Peter</surname><given-names>V</given-names></name><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name><name><surname>Burnham</surname><given-names>D</given-names></name></person-group><article-title>Infant-directed speech facilitates seven-month-old infants’ cortical tracking of speech</article-title><source>Scientific Reports</source><year>2018</year><volume>8</volume><pub-id pub-id-type="doi">10.1038/s41598-018-32150-6</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ortiz Barajas</surname><given-names>MC</given-names></name><name><surname>Guevara</surname><given-names>R</given-names></name><name><surname>Gervain</surname><given-names>J</given-names></name></person-group><article-title>The origins and development of speech envelope tracking during the first months of life</article-title><source>Developmental Cognitive Neuroscience</source><year>2021</year><volume>48</volume><elocation-id>100915</elocation-id><pub-id pub-id-type="doi">10.1016/j.dcn.2021.100915</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Attaheri</surname><given-names>A</given-names></name><name><surname>Panayiotou</surname><given-names>D</given-names></name><name><surname>Phillips</surname><given-names>A</given-names></name><name><surname>Choisdealbha</surname><given-names>ÁN</given-names></name><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Rocha</surname><given-names>S</given-names></name><name><surname>Brusini</surname><given-names>P</given-names></name><name><surname>Mead</surname><given-names>N</given-names></name><name><surname>Flanagan</surname><given-names>S</given-names></name><name><surname>Olawole-Scott</surname><given-names>H</given-names></name></person-group><article-title>Cortical Tracking of Sung Speech in Adults vs Infants: A Developmental Analysis</article-title><source>Frontiers in neuroscience</source><year>2022</year><volume>16</volume></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>O’Sullivan</surname><given-names>JA</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><article-title>Low-frequency cortical entrainment to speech reflects phoneme-level processing</article-title><source>Current Biology</source><year>2015</year><volume>25</volume><pub-id pub-id-type="doi">10.1016/j.cub.2015.08.030</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mesgarani</surname><given-names>N</given-names></name><name><surname>Cheung</surname><given-names>C</given-names></name><name><surname>Johnson</surname><given-names>K</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name></person-group><article-title>Phonetic Feature Encoding in Human Superior Temporal Gyrus</article-title><source>Science</source><year>2014</year><volume>343</volume><fpage>1006</fpage><lpage>1010</lpage><pub-id pub-id-type="doi">10.1126/science.1245994</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teoh</surname><given-names>ES</given-names></name><name><surname>Ahmed</surname><given-names>F</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><article-title>Attention Differentially Affects Acoustic and Phonetic Feature Encoding in a Multispeaker Environment</article-title><source>The Journal of Neuroscience</source><year>2022</year><volume>42</volume><fpage>682</fpage><lpage>691</lpage><pub-id pub-id-type="doi">10.1523/jneurosci.1455-20.2021</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lesenfants</surname><given-names>D</given-names></name><name><surname>Vanthornhout</surname><given-names>J</given-names></name><name><surname>Verschueren</surname><given-names>E</given-names></name><name><surname>Francart</surname><given-names>T</given-names></name></person-group><article-title>Data-driven spatial filtering for improved measurement of cortical tracking of multiple representations of speech</article-title><source>Journal of Neural Engineering</source><year>2019</year><pub-id pub-id-type="doi">10.1088/1741-2552/ab3c92</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Hong</surname><given-names>LE</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><article-title>Rapid Transformation from Auditory to Linguistic Representations of Continuous Speech</article-title><source>Current Biology</source><year>2018</year><volume>28</volume><fpage>3976</fpage><lpage>3983</lpage><elocation-id>e3975</elocation-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Peter</surname><given-names>V</given-names></name><name><surname>Kalashnikova</surname><given-names>M</given-names></name><name><surname>Goswami</surname><given-names>U</given-names></name><name><surname>Burnham</surname><given-names>D</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><article-title>Atypical cortical entrainment to speech in the right hemisphere underpins phonemic deficits in dyslexia</article-title><source>NeuroImage</source><year>2018</year><fpage>70</fpage><lpage>79</lpage><comment>NIMG-17-29</comment><pub-id pub-id-type="doi">10.1016/J.NEUROIMAGE.2018.03.072</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Nie</surname><given-names>J</given-names></name><name><surname>Yeaton</surname><given-names>J</given-names></name><name><surname>Khalighinejad</surname><given-names>B</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name><name><surname>Mesgarani</surname><given-names>N</given-names></name></person-group><article-title>Neural representation of linguistic feature hierarchy reflects second-language proficiency</article-title><source>NeuroImage</source><year>2021</year><volume>227</volume><elocation-id>117586</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117586</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kuhl</surname><given-names>PK</given-names></name><name><surname>Conboy</surname><given-names>BT</given-names></name><name><surname>Coffey-Corina</surname><given-names>S</given-names></name><name><surname>Padden</surname><given-names>D</given-names></name><name><surname>Rivera-Gaxiola</surname><given-names>M</given-names></name><name><surname>Nelson</surname><given-names>T</given-names></name></person-group><article-title>Phonetic learning as a pathway to language: New data and native language magnet theory expanded (NLM-e)</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><publisher-name>Royal Society</publisher-name><year>2008</year></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuhl</surname><given-names>PK</given-names></name><name><surname>Williams</surname><given-names>KA</given-names></name><name><surname>Lacerda</surname><given-names>F</given-names></name><name><surname>Stevens</surname><given-names>KN</given-names></name><name><surname>Lindblom</surname><given-names>B</given-names></name></person-group><article-title>Linguistic experience alters phonetic perception in infants by 6 months of age</article-title><source>Science</source><year>1992</year><volume>255</volume><fpage>606</fpage><lpage>608</lpage></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene-Lambertz</surname><given-names>G</given-names></name><name><surname>Spelke</surname><given-names>ES</given-names></name></person-group><article-title>The infancy of the human brain</article-title><source>Neuron</source><year>2015</year><volume>88</volume><fpage>93</fpage><lpage>109</lpage></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsao</surname><given-names>F-M</given-names></name><name><surname>Liu</surname><given-names>H-M</given-names></name><name><surname>Kuhl</surname><given-names>PK</given-names></name></person-group><article-title>Speech Perception in Infancy Predicts Language Development in the Second Year of Life: A Longitudinal Study</article-title><source>Child Development</source><year>2004</year><volume>75</volume><fpage>1067</fpage><lpage>1084</lpage></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuhl</surname><given-names>PK</given-names></name><name><surname>Stevens</surname><given-names>E</given-names></name><name><surname>Hayashi</surname><given-names>A</given-names></name><name><surname>Deguchi</surname><given-names>T</given-names></name><name><surname>Kiritani</surname><given-names>S</given-names></name><name><surname>Iverson</surname><given-names>P</given-names></name></person-group><article-title>Infants show a facilitation effect for native language phonetic perception between 6 and 12 months</article-title><source>Dev Sci</source><year>2006</year><volume>9</volume><fpage>F13</fpage><lpage>f21</lpage><pub-id pub-id-type="doi">10.1111/j.1467-7687.2006.00468.x</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eilers</surname><given-names>RE</given-names></name><name><surname>Wilson</surname><given-names>WR</given-names></name><name><surname>Moore</surname><given-names>JM</given-names></name></person-group><article-title>Developmental changes in speech discrimination in infants</article-title><source>J Speech Hear Res</source><year>1977</year><volume>20</volume><fpage>766</fpage><lpage>780</lpage><pub-id pub-id-type="doi">10.1044/jshr.2004.766</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuhl</surname><given-names>PK</given-names></name><name><surname>Conboy</surname><given-names>BT</given-names></name><name><surname>Padden</surname><given-names>D</given-names></name><name><surname>Nelson</surname><given-names>T</given-names></name><name><surname>Pruitt</surname><given-names>J</given-names></name></person-group><article-title>Early Speech Perception and Later Language Development: Implications for the “Critical Period”</article-title><source>Language Learning and Development</source><year>2005</year><volume>1</volume><fpage>237</fpage><lpage>264</lpage><pub-id pub-id-type="doi">10.1080/15475441.2005.9671948</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polka</surname><given-names>L</given-names></name><name><surname>Colantonio</surname><given-names>C</given-names></name><name><surname>Sundara</surname><given-names>M</given-names></name></person-group><article-title>A cross-language comparison of /d/-/th/ perception: evidence for a new developmental pattern</article-title><source>J Acoust Soc Am</source><year>2001</year><volume>109</volume><fpage>2190</fpage><lpage>2201</lpage><pub-id pub-id-type="doi">10.1121/1.1362689</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><article-title>Indexing cortical entrainment to natural speech at the phonemic level: Methodological considerations for applied research</article-title><source>Hearing Research</source><year>2017</year><volume>348</volume><fpage>70</fpage><lpage>77</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2017.02.015</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Crosse</surname><given-names>MJ</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><article-title>Cortical Measures of Phoneme-Level Speech Encoding Correlate with the Perceived Clarity of Natural Speech</article-title><source>Eneuro</source><year>2018</year><volume>5</volume><elocation-id>ENEURO.0084-0018.2018</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0084-18.2018</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Wong</surname><given-names>D</given-names></name><name><surname>Melnik</surname><given-names>GA</given-names></name><name><surname>de Cheveigne</surname><given-names>A</given-names></name></person-group><article-title>Low-frequency cortical responses to natural speech reflect probabilistic phonotactics</article-title><source>NeuroImage</source><year>2019</year><volume>196</volume><fpage>237</fpage><lpage>247</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.04.037</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Cheveigné</surname><given-names>A</given-names></name><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Arzounian</surname><given-names>D</given-names></name><name><surname>Wong</surname><given-names>DDE</given-names></name><name><surname>Hjortkjær</surname><given-names>J</given-names></name><name><surname>Fuglsang</surname><given-names>S</given-names></name><name><surname>Parra</surname><given-names>LC</given-names></name></person-group><article-title>Multiway canonical correlation analysis of brain data</article-title><source>NeuroImage</source><year>2019</year><volume>186</volume><fpage>728</fpage><lpage>740</lpage><pub-id pub-id-type="doi">10.1016/J.NEUROIMAGE.2018.11.026</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cooper</surname><given-names>RP</given-names></name><name><surname>Aslin</surname><given-names>RN</given-names></name></person-group><article-title>Preference for infant-directed speech in the first month after birth</article-title><source>Child Dev</source><year>1990</year><volume>61</volume><fpage>1584</fpage><lpage>1595</lpage></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gasparini</surname><given-names>L</given-names></name><name><surname>Langus</surname><given-names>A</given-names></name><name><surname>Tsuji</surname><given-names>S</given-names></name><name><surname>Boll-Avetisyan</surname><given-names>N</given-names></name></person-group><article-title>Quantifying the role of rhythm in infants’ language discrimination abilities: A meta-analysis</article-title><source>Cognition</source><year>2021</year><volume>213</volume><elocation-id>104757</elocation-id><pub-id pub-id-type="doi">10.1016/j.cognition.2021.104757</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jusczyk</surname><given-names>PW</given-names></name><name><surname>Aslin</surname><given-names>RN</given-names></name></person-group><article-title>Infants’ Detection of the Sound Patterns of Words in Fluent Speech</article-title><source>Cognitive Psychology</source><year>1995</year><volume>29</volume><fpage>1</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1006/cogp.1995.1010</pub-id></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liberman</surname><given-names>AM</given-names></name><name><surname>Harris</surname><given-names>KS</given-names></name><name><surname>Hoffman</surname><given-names>HS</given-names></name><name><surname>Griffith</surname><given-names>BC</given-names></name></person-group><article-title>The discrimination of speech sounds within and across phoneme boundaries</article-title><source>J Exp Psychol</source><year>1957</year><volume>54</volume><fpage>358</fpage><lpage>368</lpage><pub-id pub-id-type="doi">10.1037/h0044417</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feldman</surname><given-names>NH</given-names></name><name><surname>Goldwater</surname><given-names>S</given-names></name><name><surname>Dupoux</surname><given-names>E</given-names></name><name><surname>Schatz</surname><given-names>T</given-names></name></person-group><article-title>Do Infants Really Learn Phonetic Categories?</article-title><source>Open Mind</source><year>2021</year><volume>5</volume><fpage>113</fpage><lpage>131</lpage><pub-id pub-id-type="doi">10.1162/opmi_a_00046</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>EF</given-names></name><name><surname>Rieger</surname><given-names>JW</given-names></name><name><surname>Johnson</surname><given-names>K</given-names></name><name><surname>Berger</surname><given-names>MS</given-names></name><name><surname>Barbaro</surname><given-names>NM</given-names></name><name><surname>Knight</surname><given-names>RT</given-names></name></person-group><article-title>Categorical speech representation in human superior temporal gyrus</article-title><source>Nat Neurosci</source><year>2010</year><volume>13</volume><fpage>1428</fpage><lpage>1432</lpage><comment><ext-link ext-link-type="uri" xlink:href="http://www.nature.com/neuro/journal/v13/n11/abs/nn.2641.html#supplementary-information">http://www.nature.com/neuro/journal/v13/n11/abs/nn.2641.html#supplementary-information</ext-link></comment></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huotilainen</surname><given-names>M</given-names></name><name><surname>Kujala</surname><given-names>A</given-names></name><name><surname>Hotakainen</surname><given-names>M</given-names></name><name><surname>Parkkonen</surname><given-names>L</given-names></name><name><surname>Taulu</surname><given-names>S</given-names></name><name><surname>Simola</surname><given-names>J</given-names></name><name><surname>Nenonen</surname><given-names>J</given-names></name><name><surname>Karjalainen</surname><given-names>M</given-names></name><name><surname>Näätänen</surname><given-names>R</given-names></name></person-group><article-title>Short-term memory functions of the human fetus recorded with magnetoencephalography</article-title><source>NeuroReport</source><year>2005</year><volume>16</volume><fpage>81</fpage><lpage>84</lpage></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheour</surname><given-names>M</given-names></name><name><surname>Alho</surname><given-names>K</given-names></name><name><surname>Čeponiené</surname><given-names>R</given-names></name><name><surname>Reinikainen</surname><given-names>K</given-names></name><name><surname>Sainio</surname><given-names>K</given-names></name><name><surname>Pohjavuori</surname><given-names>M</given-names></name><name><surname>Aaltonen</surname><given-names>O</given-names></name><name><surname>Näätänen</surname><given-names>R</given-names></name></person-group><article-title>Maturation of mismatch negativity in infants</article-title><source>International Journal of Psychophysiology</source><year>1998</year><volume>29</volume><fpage>217</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1016/S0167-8760(98)00017-8</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Näätänen</surname><given-names>R</given-names></name><name><surname>Lehtokoski</surname><given-names>A</given-names></name><name><surname>Lennes</surname><given-names>M</given-names></name><name><surname>Cheour</surname><given-names>M</given-names></name><name><surname>Huotilainen</surname><given-names>M</given-names></name><name><surname>Iivonen</surname><given-names>A</given-names></name><name><surname>Vainio</surname><given-names>M</given-names></name><name><surname>Alku</surname><given-names>P</given-names></name><name><surname>Ilmoniemi</surname><given-names>RJ</given-names></name><name><surname>Luuk</surname><given-names>A</given-names></name><etal/></person-group><article-title>Language-specific phoneme representations revealed by electric and magnetic brain responses</article-title><source>Nature</source><year>1997</year><volume>385</volume><fpage>432</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1038/385432a0</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheour</surname><given-names>M</given-names></name><name><surname>Martynova</surname><given-names>O</given-names></name><name><surname>Näätänen</surname><given-names>R</given-names></name><name><surname>Erkkola</surname><given-names>R</given-names></name><name><surname>Sillanpää</surname><given-names>M</given-names></name><name><surname>Kero</surname><given-names>P</given-names></name><name><surname>Raz</surname><given-names>A</given-names></name><name><surname>Kaipio</surname><given-names>M-L</given-names></name><name><surname>Hiltunen</surname><given-names>J</given-names></name><name><surname>Aaltonen</surname><given-names>O</given-names></name></person-group><article-title>Speech sounds learned by sleeping newborns</article-title><source>Nature</source><year>2002</year><volume>415</volume><fpage>599</fpage><lpage>600</lpage></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Choi</surname><given-names>D</given-names></name><name><surname>Dehaene-Lambertz</surname><given-names>G</given-names></name><name><surname>Peña</surname><given-names>M</given-names></name><name><surname>Werker</surname><given-names>JF</given-names></name></person-group><article-title>Neural indicators of articulator-specific sensorimotor influences on infant speech perception</article-title><source>Proceedings of the National Academy of Sciences</source><year>2021</year><volume>118</volume><elocation-id>e2025043118</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2025043118</pub-id></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gennari</surname><given-names>G</given-names></name><name><surname>Marti</surname><given-names>S</given-names></name><name><surname>Palu</surname><given-names>M</given-names></name><name><surname>Fló</surname><given-names>A</given-names></name><name><surname>Dehaene-Lambertz</surname><given-names>G</given-names></name></person-group><article-title>Orthogonal neural codes for speech in the infant brain</article-title><source>Proceedings of the National Academy of Sciences</source><year>2021</year><volume>118</volume><elocation-id>e2020410118</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2020410118</pub-id></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leong</surname><given-names>V</given-names></name><name><surname>Goswami</surname><given-names>U</given-names></name></person-group><article-title>Acoustic-Emergent Phonology in the Amplitude Envelope of Child-Directed Speech</article-title><source>PLOS ONE</source><year>2015</year><volume>10</volume><elocation-id>e0144411</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0144411</pub-id></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leong</surname><given-names>V</given-names></name><name><surname>Kalashnikova</surname><given-names>M</given-names></name><name><surname>Burnham</surname><given-names>D</given-names></name><name><surname>Goswami</surname><given-names>U</given-names></name></person-group><article-title>The Temporal Modulation Structure of Infant-Directed Speech</article-title><source>Open Mind</source><year>2017</year><volume>1</volume><fpage>78</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1162/OPMI_a_00008</pub-id></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daube</surname><given-names>C</given-names></name><name><surname>Ince</surname><given-names>RAA</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name></person-group><article-title>Simple Acoustic Features Can Explain Phoneme-Based Predictions of Cortical Responses to Speech</article-title><source>Current Biology</source><year>2019</year><volume>29</volume><fpage>1924</fpage><lpage>1937</lpage><elocation-id>e1929</elocation-id><pub-id pub-id-type="doi">10.1016/J.CUB.2019.04.067</pub-id></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gillis</surname><given-names>M</given-names></name><name><surname>Vanthornhout</surname><given-names>J</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name><name><surname>Francart</surname><given-names>T</given-names></name><name><surname>Brodbeck</surname><given-names>C</given-names></name></person-group><article-title>Neural Markers of Speech Comprehension: Measuring EEG Tracking of Linguistic Speech Representations, Controlling the Speech Acoustics</article-title><source>The Journal of Neuroscience</source><year>2021</year><volume>41</volume><elocation-id>10316</elocation-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0812-21.2021</pub-id></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Broderick</surname><given-names>MP</given-names></name><name><surname>Anderson</surname><given-names>AJ</given-names></name><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Crosse</surname><given-names>MJ</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><article-title>Electrophysiological Correlates of Semantic Dissimilarity Reflect the Comprehension of Natural, Narrative Speech</article-title><source>Current Biology</source><year>2018</year><pub-id pub-id-type="doi">10.1016/j.cub.2018.01.080</pub-id></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Decruy</surname><given-names>L</given-names></name><name><surname>Vanthornhout</surname><given-names>J</given-names></name><name><surname>Francart</surname><given-names>T</given-names></name></person-group><article-title>Hearing impairment is associated with enhanced neural tracking of the speech envelope</article-title><source>Hearing Research</source><year>2020</year><volume>393</volume><elocation-id>107961</elocation-id><pub-id pub-id-type="doi">10.1016/j.heares.2020.107961</pub-id></element-citation></ref><ref id="R64"><label>64</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Presacco</surname><given-names>A</given-names></name><name><surname>Anderson</surname><given-names>S</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><source>Over-representation of speech in older adults originates from early response in higher order auditory cortex. 2018/9//</source><publisher-name>S. Hirzel Verlag GmbH</publisher-name><year>2018</year><fpage>774</fpage><lpage>777</lpage></element-citation></ref><ref id="R65"><label>65</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vanthornhout</surname><given-names>J</given-names></name><name><surname>Decruy</surname><given-names>L</given-names></name><name><surname>Wouters</surname><given-names>J</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name><name><surname>Francart</surname><given-names>T</given-names></name></person-group><article-title>Speech Intelligibility Predicted from Neural Entrainment of the Speech Envelope</article-title><source>Journal of the Association for Research in Otolaryngology</source><year>2018</year><volume>19</volume><fpage>181</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.1007/s10162-018-0654-z</pub-id></element-citation></ref><ref id="R66"><label>66</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crosse</surname><given-names>MJ</given-names></name><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><article-title>Eye can hear clearly now: Inverse effectiveness in natural audiovisual speech processing relies on long-term crossmodal temporal integration</article-title><source>Journal of Neuroscience</source><year>2016</year><volume>36</volume><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1396-16.2016</pub-id></element-citation></ref><ref id="R67"><label>67</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Menn</surname><given-names>KH</given-names></name><name><surname>Ward</surname><given-names>EK</given-names></name><name><surname>Braukmann</surname><given-names>R</given-names></name><name><surname>van den Boomen</surname><given-names>C</given-names></name><name><surname>Buitelaar</surname><given-names>J</given-names></name><name><surname>Hunnius</surname><given-names>S</given-names></name><name><surname>Snijders</surname><given-names>TM</given-names></name></person-group><article-title>Neural Tracking in Infancy Predicts Language Development in Children With and Without Family History of Autism</article-title><source>Neurobiology of Language</source><year>2022</year><volume>3</volume><fpage>495</fpage><lpage>514</lpage><pub-id pub-id-type="doi">10.1162/nol_a_00074</pub-id></element-citation></ref><ref id="R68"><label>68</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Delorme</surname><given-names>A</given-names></name><name><surname>Makeig</surname><given-names>S</given-names></name></person-group><article-title>EEGLAB: an open source toolbox for analysis of single-trial EEG dynamics including independent component analysis</article-title><source>J Neurosci Methods</source><year>2004</year><volume>134</volume><fpage>9</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2003.10.009</pub-id></element-citation></ref><ref id="R69"><label>69</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greenwood</surname><given-names>DD</given-names></name></person-group><article-title>Auditory Masking and the Critical Band</article-title><source>The Journal of the Acoustical Society of America</source><year>1961</year><volume>33</volume><fpage>484</fpage><lpage>502</lpage><pub-id pub-id-type="doi">10.1121/1.1908699</pub-id></element-citation></ref><ref id="R70"><label>70</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Chomsky</surname><given-names>N</given-names></name><name><surname>Halle</surname><given-names>M</given-names></name></person-group><source>The sound pattern of English</source><year>1968</year></element-citation></ref><ref id="R71"><label>71</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Ladefoged</surname><given-names>P</given-names></name><name><surname>Johnson</surname><given-names>K</given-names></name></person-group><source>A course in phonetics</source><year>2014</year></element-citation></ref><ref id="R72"><label>72</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Boersma</surname><given-names>P</given-names></name><name><surname>Weenink</surname><given-names>D</given-names></name></person-group><source>Praat: doing phonetics by computer (Version 5.1. 05)[Computer program]</source><year>2009</year><comment>Retrieved May 1, 2009</comment></element-citation></ref><ref id="R73"><label>73</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>N</given-names></name><name><surname>Chatterjee</surname><given-names>M</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><article-title>Robust cortical entrainment to the speech envelope relies on the spectro-temporal fine structure</article-title><source>NeuroImage</source><year>2014</year><volume>88</volume><fpage>41</fpage><lpage>46</lpage></element-citation></ref><ref id="R74"><label>74</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hotelling</surname><given-names>H</given-names></name></person-group><article-title>Relations Between Two Sets of Variates</article-title><source>Biometrika</source><year>1936</year><volume>28</volume><fpage>321</fpage><pub-id pub-id-type="doi">10.2307/2333955</pub-id></element-citation></ref><ref id="R75"><label>75</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>PA</given-names></name><name><surname>Taylor</surname><given-names>R</given-names></name><name><surname>Minor</surname><given-names>BL</given-names></name><name><surname>Elliott</surname><given-names>V</given-names></name><name><surname>Fernandez</surname><given-names>M</given-names></name><name><surname>O’Neal</surname><given-names>L</given-names></name><name><surname>McLeod</surname><given-names>L</given-names></name><name><surname>Delacqua</surname><given-names>G</given-names></name><name><surname>Delacqua</surname><given-names>F</given-names></name><name><surname>Kirby</surname><given-names>J</given-names></name><name><surname>Duda</surname><given-names>SN</given-names></name></person-group><article-title>The REDCap consortium: Building an international community of software platform partners</article-title><source>J Biomed Inform</source><year>2019</year><volume>95</volume><elocation-id>103208</elocation-id><pub-id pub-id-type="doi">10.1016/j.jbi.2019.103208</pub-id></element-citation></ref><ref id="R76"><label>76</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>PA</given-names></name><name><surname>Taylor</surname><given-names>R</given-names></name><name><surname>Thielke</surname><given-names>R</given-names></name><name><surname>Payne</surname><given-names>J</given-names></name><name><surname>Gonzalez</surname><given-names>N</given-names></name><name><surname>Conde</surname><given-names>JG</given-names></name></person-group><article-title>Research electronic data capture (REDCap)--a metadata-driven methodology and workflow process for providing translational research informatics support</article-title><source>J Biomed Inform</source><year>2009</year><volume>42</volume><fpage>377</fpage><lpage>381</lpage><pub-id pub-id-type="doi">10.1016/j.jbi.2008.08.010</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>EEG tracking of acoustic and phonetic features in infants and adults.</title><p><bold>(A)</bold> Schematic diagram of the analysis paradigm. Multivariate Temporal Response Function (TRF) models were fit to describe the forward relationship between speech features and the EEG signal recorded from adults and infants (4, 7, and 11mo). Speech features included the 8-band acoustic spectrogram (S), half-way rectified envelope derivative (D), visual motion (V), and phonetic features (F). <bold>(B)</bold> EEG prediction correlations of the multivariate TRF model were significant within each group for both the time-lag windows 0-400ms and 100-500ms. <bold>(C)</bold> Topographical patterns of the EEG prediction correlations in infants (shown for the TRF window 0-400ms) became progressively more similar to adults responses with age. <bold>(D)</bold> TRF weights corresponding to the S features averaged across centro-frontal electrodes.</p></caption><graphic xlink:href="EMS155803-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Cortical encoding of phonetic features in the first year of life.</title><p><bold>(A)</bold> Hypotheses: The cortical encoding of phonetic feature categories was expected to emerge and progressively increase across the first year of life. Hypothesis 0 (Hp0): No phonetic encoding in the first year of life; Hp1-3: phonetic encoding from 11, 7, and 4 months of age respectively. <bold>(B)</bold> Phonetic feature encoding measured as the EEG prediction correlation gain when including phonetic features in the TRF (mean and SE across participants, for the 0-400ms and 100-500ms lag windows). Black bars indicate significance (p&lt;0.05 after FDR-correction). The right panel indicates the F-statistics (repeated measures ANOVA) when using MCCA denoising (retaining 1, 2, 3, 4, 5, and 10 components) and without MCCA denoising (‘all’). A main effect of age emerged for the 100-500ms TRF when retaining up to 5 components (filled dots indicate significance; p&lt;0.05). <bold>(C)</bold> Phonetic feature encoding (EEG prediction correlation gain) across all electrodes. Coloured areas indicate significance (p&lt;0.05, t-test with FDR correction). <bold>(D)</bold> TRF weights corresponding to phonetic features for the 100-500ms TRF.</p></caption><graphic xlink:href="EMS155803-f002"/></fig><fig id="F3" position="float"><label>Figure 2</label><caption><title>Sensitivity to phonetic feature groups in the first year of life.</title><p><bold>(A)</bold> Distance between infant and adult TRF weights (mean and SE). <bold>(B,C)</bold> Multidimensional scaling maps (MDS) were calculated on the phonetic features TRFs as a function of peri-stimulus time lag and electrode. By carrying out 100 repeated k-means classification, F-score measures were derived representing the discriminability of specific phonetic feature groups in the TRFs (mean and SE across repetitions) i.e., place of articulation, voicing, and manner of articulation (B). Individual MDS maps are shown in (C), where dots correspond to adult phonemes.</p></caption><graphic xlink:href="EMS155803-f003"/></fig></floats-group></article>