<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS158082</article-id><article-id pub-id-type="doi">10.1101/2022.12.03.518965</article-id><article-id pub-id-type="archive">PPR579552</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Visual homogeneity computations in the brain enable solving generic visual tasks</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Jacob</surname><given-names>Georgin</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Pramod</surname><given-names>R. T.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Arun</surname><given-names>S. P.</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>Department of Electrical Communication Engineering, Bangalore 560012</aff><aff id="A2"><label>2</label>Centre for Neuroscience Indian Institute of Science, Bangalore 560012</aff><author-notes><corresp id="CR1"><label>*</label>Correspondence to <email>sparun@iisc.ac.in</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>05</day><month>12</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>03</day><month>12</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Many visual tasks involve looking for specific object features. But we also often solve generic tasks where we look for a specific property, such as finding an odd item, deciding if two items are same, or if an object has symmetry. How do we solve such tasks? Building on simple neural rules, we show that displays with repeating elements can be distinguished from heterogeneous displays using a property we denote visual homogeneity. In behavior, visual homogeneity predicted response times on visual search and symmetry tasks. Brain imaging during these tasks revealed that visual homogeneity in both tasks is highly localized to a region in the object-selective cortex. Thus, a novel image property, visual homogeneity, is encoded in a localized brain region, to solve generic visual tasks.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Many visual tasks involve looking for specific objects or features, such as a friend in a crowd or selecting vegetables in the market. In such tasks, which have been studied extensively, we form a template in our brain that helps guide eye movements and locate the target (<xref ref-type="bibr" rid="R27">Peelen and Kastner, 2014</xref>). However, we also easily perform tasks that do not involve any specific feature but finding a property or relation between items. Examples of these generic tasks include finding an odd item, deciding if two items are same and judging if an object is symmetric. While machine vision algorithms are extremely successful in solving feature-based tasks like object categorization (<xref ref-type="bibr" rid="R39">Serre, 2019</xref>), they struggle to solve these generic tasks (<xref ref-type="bibr" rid="R18">Kim et al., 2018</xref>; <xref ref-type="bibr" rid="R35">Ricci et al., 2021</xref>).</p><p id="P3">At first glance, these tasks appear completely different. Indeed, visual search (<xref ref-type="bibr" rid="R49">Verghese, 2001</xref>; <xref ref-type="bibr" rid="R53">Wolfe and Horowitz, 2017</xref>), same-different judgments (<xref ref-type="bibr" rid="R25">Nickerson, 1969</xref>; <xref ref-type="bibr" rid="R29">Petrov, 2009</xref>) and symmetry detection (<xref ref-type="bibr" rid="R50">Wagemans, 1997</xref>; <xref ref-type="bibr" rid="R6">Bertamini and Makin, 2014</xref>) have all been studied extensively, but always separately. However, at a deeper level, these tasks are similar because they all involve discriminating between items with repeating features from those without repeating features. We reasoned that if images with repeating features are somehow represented differently in the brain, this difference could be used to solve all these tasks without requiring separate computations for each task. Here we provide evidence for this hypothesis through behavioural and brain imaging experiments on humans.</p><p id="P4">Our key predictions are depicted in <xref ref-type="fig" rid="F1">Figure 1</xref>. Consider a visual search task where participants have to indicate if a display contains an oddball target (<xref ref-type="fig" rid="F1">Figure 1A</xref>) or contains no oddball targets (<xref ref-type="fig" rid="F1">Figure 1B</xref>). According the well-known principle of divisive normalization in high-level visual cortex (<xref ref-type="bibr" rid="R55">Zoccolan et al., 2005</xref>; <xref ref-type="bibr" rid="R1">Agrawal et al., 2020</xref>; <xref ref-type="bibr" rid="R16">Katti and Arun, 2022</xref>), the neural response to multiple objects is the average of the single object responses. Accordingly, the response to an array of identical items will be the same as the response to the single item. Moreover, the response to an array containing a target among distractors would lie along the line joining the target and distractor in the (neural) representational space. These possibilities are shown for all possible arrays made from three objects in <xref ref-type="fig" rid="F1">Figure 1C</xref>. It can be seen that the homogeneous (target-absent) arrays stand apart since they do not mix multiple items, whereas the heterogeneous (target-present) arrays come closer since they contain a mixture of items. Since displays with repeating items are further away from the center of this space, this distance can be used to discriminate them from heterogeneous displays (<xref ref-type="fig" rid="F1">Figure 1C</xref>, <italic>inset</italic>).</p><p id="P5">We reasoned similarly for symmetry detection: here, participants have to decide if an object is asymmetric (<xref ref-type="fig" rid="F1">Figure 1D</xref>) or symmetric (<xref ref-type="fig" rid="F1">Figure 1E</xref>). According to multiple object normalization, objects with two different parts would lie along the line joining objects containing the two repeated parts (<xref ref-type="fig" rid="F1">Figure 1F</xref>). Indeed, both symmetric and asymmetric objects show part summation in their neural responses (<xref ref-type="bibr" rid="R32">Pramod and Arun, 2018</xref>). Consequently, symmetric objects will be further away from the centre of this space compared to asymmetric objects, and this can be the basis for discriminating them (<xref ref-type="fig" rid="F1">Figure 1F</xref>, <italic>inset</italic>).</p><p id="P6">We define this distance from the center for each image as its <italic>visual homogeneity (VH)</italic>. We made two key experimental predictions for behavioural and brain imaging data. First, if visual homogeneity is being used to solve visual search and symmetry detection tasks, then responses should be slowest for displays with VH close to the decision boundary and faster for displays with VH far away (<xref ref-type="fig" rid="F1">Figure 1G</xref>). This predicts opposite correlations between response time and VH: for target-present arrays and asymmetric objects, the response time should be positively correlated with VH. By contrast, for target-absent arrays and symmetric objects, response time should be negatively correlated with VH. Importantly, because response times can be positively or negatively correlated with VH, the net correlation between response time and VH will be close to zero. Second, if VH is encoded by a dedicated brain region, then brain activity in that region will be positively correlated with VH (<xref ref-type="fig" rid="F1">Figure 1H</xref>). Such a positive correlation cannot be explained easily by cognitive processes linked to response time such as attention or task difficulty, since response times have a net zero correlation with the mean activity of this region.</p></sec><sec id="S2" sec-type="results"><title>Results</title><p id="P7">In <xref ref-type="sec" rid="S20">Experiments 1</xref>-<xref ref-type="sec" rid="S21">2</xref>, we investigated whether visual homogeneity computations could explain decisions about targets being present or absent in an array. Since visual homogeneity requires measuring distance in perceptual space, we set out to first characterize the underlying representation of a set of natural objects using measurements of perceptual dissimilarity.</p><sec id="S3"><title>Measuring perceptual space for natural objects</title><p id="P8">In <xref ref-type="sec" rid="S20">Experiment 1</xref>, 16 human participants viewed arrays made from a set of 32 grayscale natural objects, with an oddball on the left or right (<xref ref-type="fig" rid="F2">Figure 2A</xref>), and had to indicate the side on which the oddball appeared using a key press. Participants were highly accurate and consistent in their responses during this task (accuracy, mean ± sd: 98.8 ± 0.9%; correlation between mean response times of even- and odd-numbered participants: r = 0.91, p &lt; 0.0001 across all <sup>32</sup>C<sub>2</sub> = 496 object pairs). The reciprocal of response time is a measure of perceptual distance (or dissimilarity) between the two images (<xref ref-type="bibr" rid="R2">Arun, 2012</xref>). To visualize the underlying object representation, we performed a multidimensional scaling analysis, which embeds objects in a multidimensional space such that their pairwise dissimilarities match the experimentally observed dissimilarities (see Methods). The resulting two-dimensional embedding of all objects is shown in <xref ref-type="fig" rid="F2">Figure 2B</xref>. In the resulting plot, nearby objects correspond to hard searches, and far away objects correspond to easy searches. Such representations reconstructed from behavioural data closely match population neural responses in high-level visual areas (<xref ref-type="bibr" rid="R26">Op de Beeck et al., 2001</xref>; <xref ref-type="bibr" rid="R40">Sripati and Olson, 2010</xref>). To capture the object representation accurately, we took the multidimensional embedding of all objects and treated the values along each dimension as the responses of an individual artificial neuron. We selected the number of dimensions in the multidimensional embedding so that the correlation between the observed and embedding dissimilarities matches the noise ceiling in the data. Subsequently, we averaged these single object responses to obtain responses to larger visual search arrays, as detailed below.</p></sec><sec id="S4"><title>Visual homogeneity predicts target present/absent judgments (<xref ref-type="sec" rid="S20">Experiments 1</xref>-<xref ref-type="sec" rid="S21">2</xref>)</title><p id="P9">Having characterized the underlying perceptual representation for single objects, we set out to investigate whether target present/absent responses during visual search can be explained using this representation. In <xref ref-type="sec" rid="S21">Experiment 2</xref>, 16 human participants viewed an array of items on each trial, and indicated using a key press whether there was an oddball target present or not (<xref ref-type="fig" rid="F2">Figure 2C</xref>). This task was performed inside an MRI scanner to simultaneously observe both brain activity and behaviour. Participants were highly accurate and consistent in their responses (accuracy, mean ± sd: 95 ± 3%; correlation between average response times of even- and odd-numbered participants: r = 0.86, p &lt; 0.0001 across 32 target-present searches, r = 0.63, p &lt; 0.001 across 32 target-absent searches).</p><p id="P10">Next we set out to predict the responses to target-present and target-absent search displays containing these objects. We first took the object coordinates returned by multidimensional scaling in <xref ref-type="sec" rid="S20">Experiment 1</xref> as neural responses of multiple neurons. We then used a well-known principle of object representations in high-level visual areas: the response to multiple objects is the average of the single object responses (<xref ref-type="bibr" rid="R55">Zoccolan et al., 2005</xref>; <xref ref-type="bibr" rid="R1">Agrawal et al., 2020</xref>). Thus, we took the response vector for a target-present array to be the average of the response vectors of the target and distractor (<xref ref-type="fig" rid="F2">Figure 2D</xref>). Likewise, we took the response vector for a target-absent array to be equal to the response vector of the single item. We then asked if there is any point in this multidimensional representation such that distances from this point to the target-present and target-absent response vectors can accurately predict the target-present and target-absent response times with a positive and negative correlation respectively (see Methods). We note that this model has as many free parameters as the coordinates of this unknown point or center in multidimensional space. We used nonlinear optimization to find the coordinates of the center to best match the data (see Methods).</p><p id="P11">We denoted the distance of each display to the optimized center as the visual homogeneity. As expected, the visual homogeneity of target-present arrays was significantly smaller than target-absent arrays (<xref ref-type="fig" rid="F2">Figure 2E</xref>). The resulting model predictions are shown in <xref ref-type="fig" rid="F2">Figure 2F-G</xref>. The response times for target-present searches were positively correlated with visual homogeneity (r = 0.68, p &lt; 0.0001; <xref ref-type="fig" rid="F2">Figure 2F</xref>). By contrast, the response times for target-absent searches were negatively correlated with visual homogeneity (r = −0.71, p &lt; 0.0001; <xref ref-type="fig" rid="F2">Figure 2G</xref>). This is exactly as predicted if visual homogeneity is the underlying decision variable (<xref ref-type="fig" rid="F1">Figure 1G</xref>). We note that the range of visual homogeneity values for target-present and target-absent searches do overlap, suggesting that visual homogeneity contributes but does not fully determine task performance. Rather, we suggest that visual homogeneity provides a useful and initial first guess at the presence or absence of a target, which can be refined further through detailed scrutiny.</p><p id="P12">To confirm that the above model fits are not due to overfitting, we performed a leave-one-out cross validation analysis, where we left out all target-present and target-absent searches involving a particular image, and then predicted these searches by calculating visual homogeneity. This too yielded similar correlations (r = 0.63, p &lt; 0.0001 for target-present, r = −0.63, p &lt; 0.001 for target-absent).</p><p id="P13">These findings are non-trivial for several reasons. First, it suggests that there are highly specific computations that can be performed on perceptual space to solve oddball tasks. This result is by no means straightforward from the mere measurement of perceptual dissimilarities. Second, while target-present response times are known to be driven by target-distractor similarity, target-absent response times are known to vary systematically but the reasons have been unclear. To the best of our knowledge our model provides the first unified mechanistic explanation for the systematic variations in both target-present and target-absent responses.</p><p id="P14">We performed several additional analyses to validate these results and confirm their generality. First, if heterogeneous displays elicit similar neural responses due to mixing, then their average distance to other objects must be related to their visual homogeneity. We confirmed that this was indeed the case, suggesting that the average distance of an object from all other objects is an useful estimate of visual homogeneity (<xref ref-type="supplementary-material" rid="SD1">Section S1</xref>). Second, the above analysis was based on taking the neural response to oddball arrays to be the average of the target and distractor responses. To confirm that averaging was indeed optimal, we repeated the above analysis by assuming a range of relative weights between the target and distractor. The best correlation was obtained for almost equal weights in LO, consistent with averaging and its role in the underlying perceptual representation (<xref ref-type="supplementary-material" rid="SD1">Section S1</xref>). Third, we performed several additional experiments on a larger set of natural objects as well as on silhouette shapes. In all cases, present/absent responses were explained using visual homogeneity (<xref ref-type="supplementary-material" rid="SD1">Section S2</xref>).</p><p id="P15">In sum, we conclude that visual homogeneity can explain oddball target present/absent judgements during visual search.</p></sec><sec id="S5"><title>Visual homogeneity predicts same/different responses</title><p id="P16">We have proposed that visual homogeneity can be used to solve any task that requires discriminating between homogeneous and heterogeneous displays. In <xref ref-type="sec" rid="S20">Experiments 1</xref>-<xref ref-type="sec" rid="S21">2</xref>, we have shown that visual homogeneity predicts target present/absent responses in visual search. We performed an additional experiment to assess whether visual homogeneity can be used to solve an entirely different task, namely a same-different task. In this task, participants have to indicate whether two items are the same or different. We note that instructions to participants for the same/different task (“you have to indicate if the two items are same or different”) are quite different from the visual search task (“you have ot indicate if there’s an odd-one-out target present or absent”). Yet both tasks involve discriminating between homogeneous and heterogeneous displays. We therefore predicted that “same” responses would be correlated with target-absent judgements and “different” responses would be correlated with target-present judgements. Remarkably, this was indeed the case (<xref ref-type="supplementary-material" rid="SD1">Section S3</xref>), demonstrating that same/different responses can also be predicted using visual homogeneity.</p></sec><sec id="S6"><title>Visual homogeneity is independent of experimental context</title><p id="P17">In the above analyses, visual homogeneity was calculated for each display as its distance from an optimum center in perceptual space. This raises the possibility that visual homogeneity could be modified depending on experimental context since it could depend on the set of objects relative to which the visual homogeneity is computed. We performed a number of experiments to evaluate this possibility: we found that target-absent response times, which index visual homogeneity, are unaffected by a variety of experimental context manipulations (<xref ref-type="supplementary-material" rid="SD1">Section S4</xref>). We therefore propose that visual homogeneity is an image-computable property that remains stable across tasks.</p></sec><sec id="S7"><title>A localized brain region encodes visual homogeneity (<xref ref-type="sec" rid="S21">Experiment 2</xref>)</title><p id="P18">So far, we have found that target present/absent response times had opposite correlations with visual homogeneity (<xref ref-type="fig" rid="F2">Figure 2F-G</xref>), suggesting that visual homogeneity is a possible decision variable for this task. Therefore, we reasoned that visual homogeneity may be localized to specific brain regions, such as in the visual or prefrontal cortices. Since the task in <xref ref-type="sec" rid="S21">Experiment 2</xref> was performed by participants inside an MRI scanner, we set out to investigate this issue by analyzing their brain activations.</p><p id="P19">We estimated brain activations in each voxel for individual target-present and target-absent search arrays (see Methods). To identify the brain regions whose activations correlated with visual homogeneity, we performed a whole-brain searchlight analysis. For each voxel, we calculated the mean activity in a 3×3×3 volume centered on that voxel (averaged across voxels and participants) for each present/absent search display, and calculated its correlation with visual homogeneity predictions derived from behavior (see Methods). The resulting map is shown in <xref ref-type="fig" rid="F3">Figure 3A</xref>. Visual homogeneity was encoded in a highly localized region just anterior of the lateral occipital (LO) region, with additional weak activations in the parietal and frontal regions. To compare these trends across key visual regions, we calculated the correlation between mean activation and visual homogeneity for each region. This revealed visual homogeneity to be encoded strongly in this region VH, and only weakly in other visual regions (<xref ref-type="fig" rid="F3">Figure 3D</xref>).</p><p id="P20">To ensure that the high match between visual homogeneity and neural activations in the VH region is not due to an artefact of voxel selection, we performed subject-level analysis (<xref ref-type="supplementary-material" rid="SD1">Section S5</xref>). We repeated the searchlight analysis for each subject and defined VH region for each subject. We find this VH region consistently anterior to the LO region in each subject. Next, we divided participants into two groups, and repeated the brain-wide searchlight analysis. Importantly, the match between mean activation and visual homogeneity remained significant even when the VH region was defined using one group of participants and the correlation was calculated using the mean activations of the other group (<xref ref-type="supplementary-material" rid="SD1">Section S5</xref>).</p><p id="P21">To confirm that neural activations in VH region are not driven by other cognitive processes linked to response time, such as attention, we performed a whole-brain searchlight analysis using response times across both target-present and target-absent searches. Proceeding as before, we calculated the correlation between mean activations to the target-present, target-absent and all displays with the respective response times. The resulting maps show that mean activations in the VH region are uncorrelated with response times overall (<xref ref-type="supplementary-material" rid="SD1">Section S5</xref>). By contrast, activations in EVC and LO are negatively correlated with response times, suggesting that faster responses are driven by higher activation of these areas. Finally, mean activation of parietal and prefrontal regions were strongly correlated with response times, consistent with their role in attentional modulation (<xref ref-type="supplementary-material" rid="SD1">Section S5</xref>).</p></sec><sec id="S8"><title>Object representations in LO match with visual search dissimilarities</title><p id="P22">To investigate the neural space on which visual homogeneity is being computed, we performed a dissimilarity analysis. Since target-absent displays contain multiple instances of a single item, we took the neural response to target-absent displays as a proxy for the response to single items. For each pair of objects, we took the neural activations in a 3×3×3 neighborhood centered around a given voxel and calculated the Euclidean distance between the two 27-dimensional response vectors (averaged across participants). In this manner, we calculated the neural dissimilarity for all <sup>32</sup>C<sub>2</sub> = 496 pairs of objects used in the experiment, and calculated the correlation between the neural dissimilarity in each local neighborhood and the perceptual dissimilarities for the same objects measured using oddball search in <xref ref-type="sec" rid="S20">Experiment 1</xref>. The resulting map is shown in <xref ref-type="fig" rid="F3">Figure 3B</xref>. It can be seen that perceptual dissimilarities from visual search are best correlated in the lateral occipital region, consistent with previous studies (<xref ref-type="fig" rid="F3">Figure 3E</xref>). To compare these trends across key visual regions, we performed this analysis for early visual cortex (EVC), area V4, LO and for the newly identified region VH (average MNI coordinates (x, y, z): (−48, −59, −6) with 111 voxels in the left hemisphere; (49, −56, −7) with 60 voxels in the right hemisphere). Perceptual dissimilarities matched best with neural dissimilarities in LO compared to the other visual regions (<xref ref-type="fig" rid="F3">Figure 3E</xref>). We conclude that neural representations in LO match with perceptual space. This is concordant with many previous studies (<xref ref-type="bibr" rid="R13">Haushofer et al., 2008</xref>; <xref ref-type="bibr" rid="R19">Kriegeskorte et al., 2008</xref>; <xref ref-type="bibr" rid="R1">Agrawal et al., 2020</xref>; <xref ref-type="bibr" rid="R42">Storrs et al., 2021</xref>; <xref ref-type="bibr" rid="R4">Ayzenberg et al., 2022</xref>).</p></sec><sec id="S9"><title>Equal weights for target and distractor in target-present array responses</title><p id="P23">In the preceding sections, visual homogeneity was calculated using behavioural experiments assuming a neural representation that gives equal weights to the target and distractor. We tested this assumption experimentally by asking whether neural responses to target-present displays can be predicted using the response to the target and distractor items separately. The resulting maps revealed that target-present arrays were accurately predicted as a linear sum of the constituent items, with roughly equal weights for the target and distractor (<xref ref-type="supplementary-material" rid="SD1">Section S5</xref>).</p></sec><sec id="S10"><title>Visual homogeneity predicts symmetry perception (<xref ref-type="sec" rid="S25">Experiments 3</xref>-<xref ref-type="sec" rid="S26">4</xref>)</title><p id="P24">The preceding sections show that visual homogeneity predicts target present/absent responses as well same/different responses. We have proposed that visual homogeneity can be used to solve any task that involves discriminating homogeneous and heterogeneous displays. In <xref ref-type="sec" rid="S25">Experiments 3</xref> &amp; <xref ref-type="sec" rid="S26">4</xref>, we extend the generality of these findings to an entirely different task, namely symmetry perception. Here, asymmetric objects are akin to heterogeneous displays whereas symmetric objects are homogeneous displays. In <xref ref-type="sec" rid="S25">Experiment 3</xref>, we measured perceptual dissimilarities for a set of 64 objects (32 symmetric, 32 asymmetric objects) made from a common set of parts. On each trial, participants viewed a search array with identical items except for one oddball, and had to indicate the side (left/right) on which the oddball appeared using a key press. An example search array is shown in <xref ref-type="fig" rid="F4">Figure 4A</xref>. Participants performed searches involving all possible <sup>64</sup>C<sub>2</sub> = 2,016 pairs of objects. Participants made highly accurate and consistent responses on this task (accuracy, mean ± sd: 98.5 ± 1.33%; correlation between average response times from even- and odd-numbered subjects: r = 0.88, p &lt; 0.0001 across 2,016 searches). As before, we took the perceptual dissimilarity between each pair of objects to be the reciprocal of the average response time for displays with either item as target and the other as distractor. To visualize the underlying object representation, we performed a multidimensional scaling analysis, which embeds objects in a multidimensional space such that their pairwise dissimilarities match the experimentally observed dissimilarities. The resulting plot for two dimensions is shown in <xref ref-type="fig" rid="F4">Figure 4B</xref>, where nearby objects correspond to similar searches. It can be seen that symmetric objects are generally more spread apart than asymmetric objects, suggesting that visual homogeneity could discriminate between symmetric and asymmetric objects.</p><p id="P25">In <xref ref-type="sec" rid="S26">Experiment 4</xref>, we tested this prediction experimentally using a symmetry detection task that was performed by participants inside an MRI scanner. On each trial, participants viewed a briefly presented object, and had to indicate whether the object was symmetric or asymmetric using a key press (<xref ref-type="fig" rid="F4">Figure 4C</xref>). Participants made accurate and consistent responses in this task (accuracy, mean ± sd: 97.7 ± 1.7%; correlation between response times of odd- and even-numbered participants: r = 0.47, p &lt; 0.0001).</p><p id="P26">We next wondered whether visual homogeneity can be used to predict symmetry judgments. To this end, we took the embedding of all objects from <xref ref-type="sec" rid="S25">Experiment 3</xref>, and asked whether there was a center in this multidimensional space such that the distance of each object to this center would be oppositely correlated with response times for symmetric and asymmetric objects (see Methods). Model predictions are shown in <xref ref-type="fig" rid="F4">Figure 4E-G</xref>. As predicted, visual homogeneity was significantly larger for symmetric compared to asymmetric objects (visual homogeneity, mean ± sd: 0.60 ± 0.24 s<sup>-1</sup> for asymmetric objects; 0.76 ± 0.29 s<sup>-1</sup> for symmetric objects; p &lt; 0.05, rank-sum test; <xref ref-type="fig" rid="F4">Figure 4E</xref>). For asymmetric objects, symmetry detection response times were positively correlated with visual homogeneity (r = 0.56, p &lt; 0.001; <xref ref-type="fig" rid="F4">Figure 4F</xref>). By contrast, for symmetric objects, response times were negatively correlated with visual homogeneity (r = −0.39, p &lt; 0.05; <xref ref-type="fig" rid="F4">Figure 4G</xref>). These patterns are exactly as expected if visual homogeneity was the underlying decision variable for symmetry detection. However, we note that the range of visual homogeneity values for asymmetric and symmetric objects do overlap, suggesting that visual homogeneity contributes but does not fully determine task performance. Rather, we suggest that visual homogeneity provides a useful and initial first guess at symmetry in an image, which can be refined further through detailed scrutiny.</p><p id="P27">To confirm that these model fits are not due to overfitting, we performed a leave-one-out cross validation analysis, where we left out one object at a time, and then calculated its visual homogeneity. This too yielded similar correlations (r = 0.44 for asymmetric, r = −0.39 for symmetric objects, p &lt; 0.05 in both cases).</p><p id="P28">In sum, we conclude that visual homogeneity can predict symmetry perception. Taken together, these experiments demonstrate that the same computation (distance from a center) explains two disparate generic visual tasks: symmetry perception and visual search.</p></sec><sec id="S11"><title>Visual homogeneity is encoded by the VH region during symmetry detection</title><p id="P29">If visual homogeneity is a decision variable for symmetry detection, it could be localized to specific regions in the brain. To investigate this issue, we analyzed the brain activations of participants in <xref ref-type="sec" rid="S26">Experiment 4</xref>.</p><p id="P30">To investigate the neural substrates of visual homogeneity, we performed a searchlight analysis. For each voxel, we calculated the correlation between mean activations in a 3×3×3 voxel neighborhood and visual homogeneity. This revealed a localized region in the visual cortex as well as some parietal regions where this correlation attained a maximum (<xref ref-type="fig" rid="F5">Figure 5A</xref>). This VH region (average MNI coordinates (x, y, z): (−57, −56, −8) with 93 voxels in the left hemisphere; (58, −50, −8) with 73 voxels in the right hemisphere) overlaps with VH region defined during visual search in <xref ref-type="sec" rid="S25">Experiment 3</xref> (for a detailed comparison, see <xref ref-type="supplementary-material" rid="SD1">Section S7</xref>). We note that it is not straightforward to interpret the activation differences here, since the objects in this experiment were present foveally whereas the visual search arrays contained no central item with items exclusively in the periphery.</p><p id="P31">To confirm that neural activations in VH region are not driven by other cognitive processes linked to response time, such as attention, we performed a whole-brain searchlight analysis using response times across both symmetric and asymmetric objects. This revealed that mean activations in the VH region were poorly correlated with response times overall, whereas other parietal and prefrontal regions strongly correlated with response times consistent with their role in attentional modulation (<xref ref-type="supplementary-material" rid="SD1">Section S6</xref>).</p><p id="P32">To investigate the perceptual representation that is being used for visual homogeneity computations, we performed a neural dissimilarity analysis. For each pair of objects, we took the neural activations in a 3×3×3 neighborhood centered around a given voxel and calculated the Euclidean distance between the two 27-dimensional response vectors. In this manner, we calculated the neural dissimilarity for all <sup>64</sup>C<sub>2</sub> = 2,016 pairs of objects used in the experiment, and calculated the correlation between the neural dissimilarity in each local neighborhood and the perceptual dissimilarities for the same objects measured using oddball search in <xref ref-type="sec" rid="S25">Experiment 3</xref>. The resulting map is shown in <xref ref-type="fig" rid="F5">Figure 5B</xref>. The match between neural and perceptual dissimilarity was maximum in the lateral occipital region (<xref ref-type="fig" rid="F5">Figure 5B</xref>).</p><p id="P33">To compare these trends for key visual regions, we repeated this analysis for anatomically defined regions of interest in the visual cortex: early visual cortex (EVC), area V4, the lateral occipital (LO) region, and the VH region defined based on the searchlight map in <xref ref-type="fig" rid="F5">Figure 5A</xref>. These regions are depicted in <xref ref-type="fig" rid="F5">Figure 5C</xref>. We then asked how mean activations in each of these regions is correlated with visual homogeneity. This revealed that the match with visual homogeneity is best in the VH region compared to the other regions (<xref ref-type="fig" rid="F5">Figure 5D</xref>). To further confirm that visual homogeneity is encoded in a localized region in the symmetry task, we repeated the searchlight analysis on two independent subgroups of participants. This revealed highly similar regions in both groups (<xref ref-type="supplementary-material" rid="SD1">Section S6</xref>).</p><p id="P34">Finally, we compared neural dissimilarities and perceptual dissimilarities in each region as before. This revealed that perceptual dissimilarities (measured from <xref ref-type="sec" rid="S25">Experiment 3</xref>, during visual search) matched best with the LO region (<xref ref-type="fig" rid="F5">Figure 5E</xref>), suggesting that object representations in LO are the basis for visual homogeneity computations in the VH region.</p><p id="P35">In sum, our results suggest that visual homogeneity is encoded by the VH region, using object representations present in the adjoining LO region.</p></sec><sec id="S12"><title>Target-absent responses predict symmetry detection</title><p id="P36">So far, we have shown that visual homogeneity predicts target present/absent responses in visual search as well as symmetry detection responses. These results suggest a direct empirical link between these two tasks. Specifically, since target-absent response time is inversely correlated with visual homogeneity, we can take its reciprocal as an estimate of visual homogeneity. This in turn predicts opposite correlations between symmetry detection times and reciprocal of target-absent response time: in other words, we should see a positive correlation for asymmetric objects, and a negative correlation for symmetric objects. We confirmed these predictions using additional experiments (<xref ref-type="supplementary-material" rid="SD1">Section S8</xref>). These results reconfirm that a common decision variable, visual homogeneity, drives both target present/absent and symmetry judgements.</p></sec><sec id="S13"><title>Visual homogeneity explains animate categorization</title><p id="P37">Since visual homogeneity is always calculated relative to a location in perceptual space, we reasoned that shifting this center towards a particular object category would make it a decision variable for object categorization. To test this prediction, we reanalyzed data from a previous study in which participants had to categorize images as belonging to three hierarchical categories: animals, dogs or Labradors (<xref ref-type="bibr" rid="R22">Mohan and Arun, 2012</xref>). By adjusting the center of the perceptual space measured using visual search, we were able to predict categorization responses for all three categories (<xref ref-type="supplementary-material" rid="SD1">Section S9</xref>). We further reasoned that, if the optimum center for animal/dog/Labrador categorization is close to the default center in perceptual space that predicts target present/absent judgements, then even the default visual homogeneity, as indexed by the reciprocal of target-absent search time, should predict categorization responses. Interestingly, this was indeed the case (<xref ref-type="supplementary-material" rid="SD1">Section S9</xref>). We conclude that, at least for the categories tested, visual homogeneity computations can serve as a decision variable for object categorization.</p></sec></sec><sec id="S14" sec-type="discussion"><title>Discussion</title><p id="P38">Here, we investigated three disparate visual tasks: detecting whether an oddball is present in a search array, deciding if two items are same or different, and judging whether an object is symmetric/asymmetric. Although these tasks are superficially quite different, they all involve discriminating between homogeneous and heterogeneous displays. We defined a new image property computable from the underlying perceptual representation, namely visual homogeneity, that can be used to solve these tasks. We found that visual homogeneity can predict response times in all three tasks. Finally we have found that visual homogeneity is encoded in a highly localized brain region in both visual search and symmetry tasks. Below we discuss these findings in relation to the existing literature.</p><sec id="S15"><title>Visual homogeneity unifies visual search, same-different and symmetry tasks</title><p id="P39">Our main finding, that a single decision variable (visual homogeneity) can be used to solve three disparate visual tasks (visual search, same/different and symmetry detection) is novel to the best of our knowledge. This finding is interesting and important because it establishes a close correspondence between all three tasks, and explains some unresolved puzzles in each of these tasks, as detailed below.</p><p id="P40">First, with regards to visual search, theoretical accounts of search are based on signal detection theory (<xref ref-type="bibr" rid="R49">Verghese, 2001</xref>; <xref ref-type="bibr" rid="R53">Wolfe and Horowitz, 2017</xref>), but define the signal only for specific target-distractor pairs. By contrast, the task of detecting whether an oddball item is present requires a more general decision rule that has not been identified. Our results suggest that visual homogeneity is the underlying decision variable. In visual search, target-present search times are widely believed to be driven by target-distractor similarity (<xref ref-type="bibr" rid="R10">Duncan and Humphreys, 1989</xref>; <xref ref-type="bibr" rid="R52">Wolfe and Horowitz, 2004</xref>). But target-absent search times also vary systematically for reasons that have not been elucidated previously. The slope of target-absent search times as a function of set size are typically twice the slope of target present searches (<xref ref-type="bibr" rid="R51">Wolfe, 1998</xref>). However this observation is based on averaging across many target-present searches. Since there is only one unique item in a target-absent search array, there must be some image property that causes systematic variations. Our results elucidate this puzzle by showing that this systematic variation is driven by visual homogeneity. We speculate that visual homogeneity might explain many other search phenomena, such as search asymmetries. Finally, our findings also help explain why we sometimes know a target is present without knowing its exact location – this is because the underlying decision variable, visual homogeneity, arises in high-level visual areas with relatively coarse spatial information.</p><p id="P41">Second, with regards to same-different tasks, most theoretical accounts use signal detection theory but usually with reference to specific stimulus pairs (<xref ref-type="bibr" rid="R25">Nickerson, 1969</xref>; <xref ref-type="bibr" rid="R29">Petrov, 2009</xref>). It has long been observed that “different” responses become faster with increasing target-distractor dissimilarity but this trend logically predicts that “same” responses, which have zero difference, should be the slowest (<xref ref-type="bibr" rid="R24">Nickerson, 1967</xref>, <xref ref-type="bibr" rid="R25">1969</xref>). But in fact, “same” responses are faster than “different” responses. This puzzle has been resolved by assuming a separate decision rule for “same” judgements, making the overall decision process more complex (<xref ref-type="bibr" rid="R29">Petrov, 2009</xref>; <xref ref-type="bibr" rid="R12">Goulet, 2020</xref>). Our findings resolve this puzzle by identifying a novel variable, visual homogeneity, which can be used to implement a simple decision rule for making same/different responses. Our findings also explain why some images elicit faster “same” responses than others: this is due to image-to-image differences in visual homogeneity.</p><p id="P42">Third, with regard to symmetry detection, most theoretical accounts assume that symmetry is explicitly detected using symmetry detectors along particular axes (<xref ref-type="bibr" rid="R50">Wagemans, 1997</xref>; <xref ref-type="bibr" rid="R6">Bertamini and Makin, 2014</xref>). By contrast, our findings indicate an indirect mechanism for symmetry detection that does not invoke any special symmetry computations. We show that visual homogeneity computations can easily discriminate between symmetric and asymmetric objects. This is because symmetric objects have high visual homogeneity since they have repeated parts, whereas asymmetric objects have low visual homogeneity since they have disparate parts (<xref ref-type="bibr" rid="R32">Pramod and Arun, 2018</xref>). In a recent study, symmetry detection was explained by the average distance of objects relative to other objects (<xref ref-type="bibr" rid="R32">Pramod and Arun, 2018</xref>). This finding is consistent with ours since visual homogeneity is correlated with the average distance to other objects (<xref ref-type="supplementary-material" rid="SD1">Section S1</xref>). However, there is an important distinction between these two quantities. Visual homogeneity is an intrinsic image property, whereas the average distance of an object to other objects depends on the set of other objects on which the average is being computed. Indeed, we have confirmed through additional experiments that visual homogeneity is independent of experimental context (<xref ref-type="supplementary-material" rid="SD1">Section S4</xref>). We speculate that visual homogeneity can explain many other aspects of symmetry perception, such as the relative strength of symmetries.</p></sec><sec id="S16"><title>Visual homogeneity in other visual tasks</title><p id="P43">Our finding that visual homogeneity explains generic visual tasks has several important implications for visual tasks in general. First, we note that visual homogeneity can be easily extended to explain other generic tasks such as delayed match-to-sample tasks or n-back tasks, by taking the response to the test stimulus as being averaged with the sample-related information in working memory. In such tasks, visual homogeneity will be larger for sequences with repeated compared to non-repeated stimuli, and can easily be used to solve the task. Testing these possibilities will require comparing systematic variations in response times in these tasks across images, and measurements of perceptual space for calculating visual homogeneity.</p><p id="P44">Second, we note that visual homogeneity can also be extended to explain object categorization, if one assumes that the center in perceptual space for calculating visual homogeneity can be temporarily shifted to the center of an object category. In such tasks, visual homogeneity relative to the category center will be small for objects belonging to a category and large for objects outside the category, and can be used as a decision variable to solve categorization tasks. This idea is consistent with prevalent accounts of object categorization (<xref ref-type="bibr" rid="R41">Stewart and Morin, 2007</xref>; <xref ref-type="bibr" rid="R3">Ashby and Maddox, 2011</xref>; <xref ref-type="bibr" rid="R22">Mohan and Arun, 2012</xref>). Indeed, categorization response times can be explained using perceptual distances to category and non-category items (<xref ref-type="bibr" rid="R22">Mohan and Arun, 2012</xref>). By reanalyzing data from this study, we have found that, at least for the animate categories tested, visual homogeneity can explain categorization responses (<xref ref-type="supplementary-material" rid="SD1">Section S9</xref>). However, this remains to be tested in a more general fashion across multiple object categories.</p></sec><sec id="S17"><title>Neural encoding of visual homogeneity</title><p id="P45">We have found that visual homogeneity is encoded in a specific region of the brain, which we denote as region VH, in both visual search and symmetry detection tasks (<xref ref-type="fig" rid="F3">Figure 3D</xref>, <xref ref-type="fig" rid="F5">5D</xref>). This finding is consistent with observations of norm-based encoding in IT neurons (<xref ref-type="bibr" rid="R20">Leopold et al., 2006</xref>) and in face recognition (<xref ref-type="bibr" rid="R45">Valentine, 1991</xref>; <xref ref-type="bibr" rid="R34">Rhodes and Jeffery, 2006</xref>; <xref ref-type="bibr" rid="R9">Carlin and Kriegeskorte, 2017</xref>). However our finding is significant because it reveals a dedicated region in high-level visual cortex for solving generic visual tasks.</p><p id="P46">We have found that the VH region is located just anterior to the lateral occipital (LO) region, where neural dissimilarities match closely with perceptual dissimilarities (<xref ref-type="fig" rid="F3">Figure 3E</xref>, <xref ref-type="fig" rid="F5">5E</xref>). Based on this proximity, we speculate that visual homogeneity computations are based on object representations in LO. However, confirming this prediction will require fine-grained recordings of neural activity in VH and LO. An interesting possibility for future studies would be to causally perturb brain activity separately in VH or LO using magnetic or electrical stimulation, if at all possible. A simple prediction would be that perturbing LO would distort the underlying representation, whereas perturbing VH would distort the underlying decision process. We caution however that the results might not be so easily interpretable if visual homogeneity computations in VH are based on object representations in LO.</p><p id="P47">Recent observations from neural recordings in monkeys suggest that perceptual dissimilarities and visual homogeneity need not be encoded in separate regions. For instance, the overall magnitude of the population neural response of monkey inferior temporal (IT) cortex neurons was found to correlate with memorability (<xref ref-type="bibr" rid="R15">Jaegle et al., 2019</xref>). We speculate that this quantity might be related to visual homogeneity. At the same time, the neural responses of IT neurons predict perceptual dissimilarities (<xref ref-type="bibr" rid="R26">Op de Beeck et al., 2001</xref>; <xref ref-type="bibr" rid="R40">Sripati and Olson, 2010</xref>; <xref ref-type="bibr" rid="R54">Zhivago and Arun, 2014</xref>; <xref ref-type="bibr" rid="R1">Agrawal et al., 2020</xref>). Taken together, these findings suggest that visual homogeneity computations and the underlying perceptual representation could be interleaved within a single neural population. Indeed, in our study, the mean activations of the LO region were also correlated with visual homogeneity for symmetry detection (<xref ref-type="fig" rid="F5">Figure 5A</xref>), but not for target present/absent search (<xref ref-type="fig" rid="F3">Figure 3A</xref>). We speculate that perhaps visual homogeneity might be intermingled into the object representation in monkeys but separated into a dedicated region in humans. These are interesting possibilities for future work.</p><p id="P48">Although many previous studies have reported brain activations in the vicinity of the VH region, we are unaware of any study that has ascribed a specific function to this region. The localized activations in our study match closely with the location of the recently reported ventral stream attention module in both humans and monkeys (<xref ref-type="bibr" rid="R37">Sani et al., 2021</xref>). Previous studies have observed important differences in brain activations in this region, which we can be explained using visual homogeneity, as detailed below.</p><p id="P49">First, previous studies have observed larger brain activations for animate compared to inanimate objects in higher visual areas which have typically included our newly defined VH region (<xref ref-type="bibr" rid="R7">Bracci and Op de Beeck, 2015</xref>; <xref ref-type="bibr" rid="R33">Proklova et al., 2016</xref>; <xref ref-type="bibr" rid="R44">Thorat et al., 2019</xref>). These observations would be consistent with our findings if visual homogeneity is smaller for animate compared to inanimate objects, which would result in weaker activations for animate objects in region VH. Indeed, visual homogeneity, as indexed by the reciprocal of target-absent search time, is smaller for animate objects compared to inanimate objects (<xref ref-type="supplementary-material" rid="SD1">Section S9</xref>). Likewise, brain activations were weaker for animate objects compared to inanimate objects in region VH (average VH activations, mean ± sd across participants: 0.50 ± 0.61 for animate target-absent displays, 0.64 ± 0.59 for inanimate target-absent displays, p &lt; 0.05, sign-rank test across participants). Based on this we speculate that visual homogeneity may partially explain the animacy organization of human ventral temporal cortex. Establishing this will require controlling animate/inanimate stimuli not only for shape but also for visual homogeneity.</p><p id="P50">Second, previous studies have reported larger brain activations for symmetric objects compared to asymmetric objects in the vicinity of this region (<xref ref-type="bibr" rid="R38">Sasaki et al., 2005</xref>; <xref ref-type="bibr" rid="R48">Van Meel et al., 2019</xref>). This can be explained by our finding that symmetric objects have larger visual homogeneity (<xref ref-type="fig" rid="F4">Figure 4E</xref>), leading to activation of the VH region (<xref ref-type="fig" rid="F5">Figure 5A</xref>). But the increased activations in previous studies were located in the V4 &amp; LO regions, whereas we have found greater activations more anteriorly in the VH region. This difference could be due to the stimulus-related differences: both previous studies used dot patterns, which could appear more object-like when symmetric, leading to more widespread differences in brain activation due to other visual processes like figure-ground segmentation (<xref ref-type="bibr" rid="R48">Van Meel et al., 2019</xref>). By contrast, both symmetric and asymmetric objects in our study are equally object-like. Resolving these discrepancies will require measuring visual homogeneity as well as behavioural performance during symmetry detection for dot patterns.</p></sec><sec id="S18"><title>Relation to image memorability</title><p id="P51">We have defined a novel image property, visual homogeneity, which refers to the distance of a visual image to a central point in the underlying perceptual representation. It can be reliably estimated for each image as the inverse of the target-absent response time in a visual search task (<xref ref-type="fig" rid="F2">Figure 2</xref>) and is independent of the immediate experimental context (<xref ref-type="supplementary-material" rid="SD1">Section S4</xref>).</p><p id="P52">The above observations raise the question of whether and how visual homogeneity might be related to image memorability. It has long been noted that faces that are rated as being distinctive or unusual are also easier to remember (<xref ref-type="bibr" rid="R23">Murdock, 1960</xref>; <xref ref-type="bibr" rid="R46">Valentine and Bruce, 1986a</xref>, <xref ref-type="bibr" rid="R47">1986b</xref>; <xref ref-type="bibr" rid="R45">Valentine, 1991</xref>). Recent studies have elucidated this observation by showing that there are specific image properties that predict image memorability (<xref ref-type="bibr" rid="R5">Bainbridge et al., 2017</xref>; <xref ref-type="bibr" rid="R21">Lukavský and Děchtěrenko, 2017</xref>; <xref ref-type="bibr" rid="R36">Rust and Mehrpour, 2020</xref>). However, image memorability, as elegantly summarized in a recent review (<xref ref-type="bibr" rid="R36">Rust and Mehrpour, 2020</xref>), could be driven by a number of both intrinsic and extrinsic factors. It would therefore be interesting to characterize how well visual homogeneity, empirically measured using target-absent visual search, can predict image memorability.</p></sec></sec><sec id="S19" sec-type="materials | methods"><title>Materials and Methods</title><p id="P53">All participants had a normal or corrected-to-normal vision and gave informed consent to an experimental protocol approved by the Institutional Human Ethics Committee of the Indian Institute of Science (IHEC # 6-15092017). Participants provided written informed consent before each experiment and were monetarily compensated.</p><sec id="S20"><title>Experiment 1. Oddball detection for perceptual space (natural objects)</title><p id="P54"><italic>Participants.</italic> A total of 16 participants (8 males, 22 ± 2.8 years) participated in this experiment.</p><p id="P55"><italic>Stimuli.</italic> The stimulus set comprised a set of 32 grayscale natural objects (16 animate, 16 inanimate) presented against a black background.</p><p id="P56"><italic>Procedure.</italic> Participants performed an oddball detection task with a block of practice trials involving unrelated stimuli followed by the main task. Each trial began with a red fixation cross (diameter 0.5°) for 500 ms, followed by a 4 x 4 search array measuring 30° x 30° for 5 seconds or until a response was made. The search array always contained one oddball target and 15 identical distractors, with the target appearing equally often on the left or right. A vertical red line divided the screen equally into two halves to facilitate responses. Participants were asked to respond as quickly and as accurately as possible using a key press to indicate the side of the screen containing the target (’Z’ for left, M’ for right). Incorrect trials were repeated later after a random number of other trials. Each participant completed 992 correct trials (<sup>32</sup>C<sub>2</sub> object pairs x 2 repetitions with either image as target). The experiment was created using PsychoPy (<xref ref-type="bibr" rid="R28">Peirce et al., 2019</xref>) and ported to the online platform Pavlovia for collecting data.</p><p id="P57">Since stimulus size could vary with the monitor used by the online participants, we equated the stimulus size across participants using the ScreenScale function (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/8FHQK">https://doi.org/10.17605/OSF.IO/8FHQK</ext-link>). Each participant adjusted the size of a rectangle on the screen such that its size matched the physical dimensions of a credit card. All the stimuli presented were then scaled with the estimated scaling function to obtain the desired size in degrees of visual angle, assuming an average distance to screen of 60 cm.</p><p id="P58"><italic>Data Analysis.</italic> Response times faster than 0.3 seconds or slower than 3 seconds were removed from the data. This step removed only 1.25% of the data and improved the overall response time consistency, but did not qualitatively alter the results.</p><p id="P59"><italic>Characterizing perceptual space using multidimensional scaling.</italic> To characterize the perceptual space on which present/absent decisions are made, we took the inverse of the average response times (across trials and participants) for each image pair. This inverse of response time (i.e. 1/RT) represents the dissimilarity between the target and distractor (<xref ref-type="bibr" rid="R2">Arun, 2012</xref>), indexes the underlying salience signal in visual search (<xref ref-type="bibr" rid="R43">Sunder and Arun, 2016</xref>) and combines linearly across a variety of factors (<xref ref-type="bibr" rid="R30">Pramod and Arun, 2014</xref>, <xref ref-type="bibr" rid="R31">2016</xref>; <xref ref-type="bibr" rid="R14">Jacob and Arun, 2020</xref>). Since there were 32 natural objects in the experiment and all possible (<sup>32</sup>C<sub>2</sub> = 496) pairwise searches in the experiment, we obtained 496 pairwise dissimilarities overall. To calculate target-present and target-absent array responses, we embedded these objects into a multidimensional space using multidimensional scaling analysis (<italic>mdscale</italic> function; MATLAB 2019). This analysis finds the n-dimensional coordinates for each object such that pairwise distances between objects best matches with the experimentally observed pairwise distances. We then treated the activations of objects along each dimension as the responses of a single artificial neuron, so that the response to target-present arrays could be computed as the average of the target and distractor responses.</p></sec><sec id="S21"><title>Experiment 2. Target present-absent search during fMRI</title><p id="P60"><italic>Participants.</italic> A total of 16 subjects (11 males; age, mean ± sd: 25 ± 2.9 years) participated in this experiment. Participants with history of neurological or psychiatric disorders, or with metal implants or claustrophobia were excluded through screening questionnaires.</p><p id="P61"><italic>Procedure.</italic> Inside the scanner, participants performed a single run of a one-back task for functional localizers (block design, object vs scrambled objects), eight runs of the present-absent search task (event-related design), and an anatomical scan. The experiment was deployed using custom MATLAB scripts written using Psychophysics Toolbox (<xref ref-type="bibr" rid="R8">Brainard, 1997</xref>).</p><p id="P62"><italic>Functional localizer runs.</italic> Participants had to view a series of images against a black background and press a response button whenever an item was repeated. On each trial, 16 images were presented (0.8 s on, 0.2 s off), containing one repeat of an image that could occur at random. Trials were combined into blocks of 16 trials each containing either only objects or only scrambled objects. A single run of the functional localizers contained 12 such blocks (6 object blocks &amp; 6 scrambled-object blocks). Stimuli in each block were chosen randomly from a larger pool of 80 naturalistic objects with the corresponding phase-scrambled objects (created by taking the 2D Fourier transform of each image, randomly shuffling the Fourier phase, and performing the Fourier inverse transform). This is a widely used method for functional localization of object-selective cortex. In practice, however, we observed no qualitative differences in our results upon using voxels activated during these functional localizer runs to further narrow down the voxels selected using anatomical masks. As a result, we did not use the functional localizer data, and all the analyses presented here are based on anatomical masks only.</p><p id="P63"><italic>Visual search task.</italic> In the present-absent search task, participants reported the presence or absence of an oddball target by pressing one of two buttons using their right hand. The response buttons were fixed for a given participant and counterbalanced across participants. Each search array had eight items, measuring 1.5° along the longer dimension, arranged in a 3 x 3 grid, with no central element to avoid fixation biases (as shown in <xref ref-type="fig" rid="F2">Figure 2C</xref>). The entire search array measured 6.5°, with an average inter-item spacing of 2.5°. Item positions were jittered randomly on each trial according to a uniform distribution with range ± 0.2°. Each trial lasted 4 s (1 s ON time and 3 s OFF time), and participants had to respond within 4 s. Each run had 64 unique searches (32 present, 32 absent) presented in random order, using the natural objects from <xref ref-type="sec" rid="S20">Experiment 1</xref>. Target-present searches were chosen randomly from all possible searches such that all 32 images appeared equally often. Target-absent searches included all 32 objects. The location of the target in the target-present searches was chosen such that all eight locations were sampled equally often. In this manner, participants performed 8 such runs of 64 trials each.</p><p id="P64"><italic>Data acquisition.</italic> Participants viewed images projected on a screen through a mirror placed above their eyes. Functional MRI (fMRI) data were acquired using a 32-channel head coil on a 3T Skyra (Siemens, Mumbai, India) at the HealthCare Global Hospital, Bengaluru. Functional scans were performed using a T2*-weighted gradient-echo-planar imaging sequence with the following parameters: repetition time (TR) = 2s, echo time (TE) = 28 ms, flip angle = 79°, voxel size = 3 × 3 × 3 mm<sup>3</sup>, field of view = 192 × 192 mm<sup>2</sup>, and 33 axial-oblique slices for whole-brain coverage. Anatomical scans were performed using T1-weighted images with the following parameters: TR = 2.30 s, TE = 1.99 ms, flip angle = 9°, voxel size = 1 × 1 × 1 mm<sup>3</sup>, field of view = 256 × 256 × 176 mm<sup>3</sup>.</p><p id="P65"><italic>Data preprocessing.</italic> The raw fMRI data were preprocessed using Statistical Parametric Mapping (SPM) software (Version12; Welcome Center for Human Neuroimaging; <ext-link ext-link-type="uri" xlink:href="https://www.fil.ion.ucl.ac.uk/spm/software/spm12/">https://www.fil.ion.ucl.ac.uk/spm/software/spm12/</ext-link>), running on MATLAB 2019b. Raw images were realigned, slice-time corrected, co-registered to the anatomical image, segmented, and normalized to the Montreal Neurological Institute (MNI) 305 anatomical template. Repeating the key analyses with voxel activations estimated from individual subjects yielded qualitatively similar results. Smoothing was performed only on the functional localizer blocks using a Gaussian kernel with a full-width half-maximum of 5 mm. Default SPM parameters were used, and voxel size after normalization was kept at 3×3×3 mm<sup>3</sup>. The data were further processed using GLMdenoise (<xref ref-type="bibr" rid="R17">Kay et al., 2013</xref>). GLMdenoise improves the signal-to-noise ratio in the data by regressing out the noise estimated from task-unrelated voxels. The denoised time-series data were modeled using generalized linear modeling in SPM after removing low-frequency drift using a high-pass filter with a cutoff of 128 s. In the main experiment, the activity of each voxel was modeled using 83 regressors (68 stimuli + 1 fixation + 6 motion regressors + 8 runs). In the localizer block, each voxel was modeled using 14 regressors (6 stimuli + 1 fixation + 6 motion regressors + 1 run).</p><p id="P66"><italic>ROI definitions.</italic> The regions of interest (ROI) of Early Visual Cortex (EVC) and Lateral Occipital (LO) regions were defined using anatomical masks from the SPM anatomy toolbox (<xref ref-type="bibr" rid="R11">Eickhoff et al., 2005</xref>). All brain maps were visualized on the inflated brain using Freesurfer (<ext-link ext-link-type="uri" xlink:href="https://surfer.nmr.mgh.harvard.edu/fswiki/">https://surfer.nmr.mgh.harvard.edu/fswiki/</ext-link>).</p><p id="P67"><italic>Behavioral data analysis.</italic> Response times faster than 0.3 seconds or slower than 3 seconds were removed from the data. This step removed only 0.75% of the data and improved the overall response time consistency, but did not qualitatively alter the results.</p><sec id="S22"><title>Model fitting for visual homogeneity</title><p id="P68">We took the multi-dimensional embedding returned by the perceptual space experiment (<xref ref-type="sec" rid="S20">Experiment 1</xref>) in 5 dimensions as the responses of 5 artificial neurons to the entire set of objects. For each target-present array, we calculated the neural response as the average of the responses elicited by these 5 neurons to the target and distractor items. Likewise, for target-absent search arrays, the neural response was simply the response elicited by these 5 neurons to the distractor item in the search array. To estimate the visual homogeneity of the target-present and target-absent search arrays, we calculated the distance of each of these arrays from a single point in the multidimensional representation. We then calculated the correlation between the visual homogeneity calculated relative to this point and the response times for the target-present and target-absent search arrays. The 5 coordinates of this center point was adjusted using constrained nonlinear optimization to maximize the difference between correlations with the target-present &amp; target-absent response times respectively. This optimum center remained stable across many random starting points, and our results were qualitatively similar upon varying the number of embedding dimensions.</p><p id="P69">Additionally, we performed a leave-one-out cross-validation analysis to validate the number of dimensions or neurons used for the multidimensional scaling analysis in the visual homogeneity model fits. For each choice of number of dimensions, we estimated the optimal centre for visual homogeneity calculations while leaving out all searches involving a single image. We then calculated the visual homogeneity for all the target-present and target-absent searches involving the left-out image. Compiling these predictions by leaving out all images by turn results in a leave-one-out predicted visual homogeneity, which we correlated with the target-present and target-absent response times. We found that the absolute sum of the correlations between visual homogeneity and present/absent reaction times increased monotonically from 1 to 5 neurons, remained at a steady level from 5 to 9 neurons and decreased beyond 9 neurons. Furthermore, the visual homogeneity using the optimal center is highly correlated for 5-9 neurons. We therefore selected 5 neurons or dimensions for reporting visual homogeneity computations.</p></sec><sec id="S23"><title>Searchlight maps for mean activation (<xref ref-type="fig" rid="F3">Figure 3A</xref>, <xref ref-type="fig" rid="F5">Figure 5A</xref>)</title><p id="P70">To characterize the brain regions that encode visual homogeneity, we performed a whole-brain searchlight analysis. For each voxel, we took the voxels in a 3×3×3 neighborhood and calculated the mean activations across these voxels across all participants. To avoid overall actiaviton level differences between target-present and target-absent searches, we z-scored the mean activations separately across target-present and target-absent searches. Similarly, we calculated the visual homogeneity model predictions from behaviour, and z-scored the visual homogeneity values for target-present and target-absent seaches separately. We then calculated the correlation between the normalized mean activations and the normalized visual homogeneity for each voxel, and displayed this as a colormap on the flattened MNI brain template in <xref ref-type="fig" rid="F3">Figures 3A</xref> &amp; <xref ref-type="fig" rid="F5">5A</xref>.</p><p id="P71">Note that the z-scoring of mean activations and visual homogeneity removes any artefactual correlation between mean activation and visual homogeneity arising simply due to overall level differences in mean activation or visual homogeneity itself, but does not alter the overall positive correlation between the visual homogeneity and mean activation across individual search displays.</p></sec><sec id="S24"><title>Searchlight maps for neural and behavioural dissimilarity (<xref ref-type="fig" rid="F3">Figure 3B</xref>, <xref ref-type="fig" rid="F5">Figure 5B</xref>)</title><p id="P72">To characterize the brain regions that encode perceptual dissimilarity, we performed a whole-brain searchlight analysis. For each voxel, we took the voxel activations in a 3×3×3 neighborhood to target-absent displays as a proxy for the neural response to the single image. For each image pair, we calculated the pair-wise Euclidean distance between the 27-dimensional voxel activations evoked by the two images, and averaged this distance across participants to get a single average distance. For 32 target-absent displays in the experiment, taking all possible pairwise distances results in <sup>32</sup>C<sub>2</sub> = 496 pairwise distances. Similarly, we obtained the same 496 pairwise perceptual dissimilarities between these items from the oddball detection task (<xref ref-type="sec" rid="S20">Experiment 1</xref>). We then calculated the correlation between the mean neural dissimilarities at each voxel with perceptual dissimilarities, and displayed this as a colormap on the flattened MNI brain template in <xref ref-type="fig" rid="F3">Figures 3B</xref> &amp; <xref ref-type="fig" rid="F5">5B</xref>.</p></sec></sec><sec id="S25"><title>Experiment 3. Oddball detection for perceptual space (Symmetric/Asymmetric objects)</title><p id="P73"><italic>Participants.</italic> A total of 15 participants (11 males, 22.8 ± 4.3 years) participated in this experiment.</p><p id="P74"><italic>Paradigm.</italic> Participants performed an oddball visual search task. Participants completed 4032 correct trials (<sup>64</sup>C<sub>2</sub> shape pairs x 2 repetitions) as two sessions in two days. We used a total of 64 baton shapes (32 symmetric and 32 asymmetric), and all shapes were presented against a black background. We created 32 unique parts with the vertical line as part of the contour. We created 32 symmetric by joining the part and its mirror-filled version, and 32 asymmetric objects were created by randomly pairing the left part and mirror flipped version of another left part. All parts were occurring equally likely. All other task details are the same as <xref ref-type="sec" rid="S20">Experiment 1</xref>.</p></sec><sec id="S26"><title>Experiment 4 Symmetry judgment task (fMRI &amp; behavior)</title><p id="P75"><italic>Participants.</italic> A total of 15 subjects participated in this study. Participants had normal or corrected to normal vision. Participants had no history of neurological or psychiatric impairment. We excluded participants with metal implants or claustrophobia from the study.</p><p id="P76"><italic>Paradigm.</italic> Inside the scanner, participants performed two runs of one-back identity task (functional localizer), eight runs of symmetry judgment task (event-related design), and one anatomical scan. We excluded the data from one participant due to poor accuracy and long response times.</p><p id="P77"><italic>Symmetry Task.</italic> On each trial, participants had to report whether a briefly presented object was symmetric or not using a keypress. Objects measured 4° and were presented against a black background. Response keys were counterbalanced across participants. Each trial lasted 4 s, with the object displayed for 200 ms followed by a blank period of 3800 ms. Participants could respond at any time following appearance of the object, up to a time out of 4 s after which the next trial began. Each run had 64 unique conditions (32 symmetric and 32 asymmetric).</p><p id="P78"><italic>1-back task for functional localizers.</italic> Stimuli were presented as blocks, and participants reported repeated stimuli with a keypress. Each run had blocks of either silhouettes (asymmetric/symmetric), dot patterns (asymmetric/symmetric), combination of baton and dot patterns (asymmetric/symmetric) and natural objects (intact/scrambled).</p></sec><sec id="S27"><title>Data Analysis</title><p id="P79"><italic>Noise Removal in RT.</italic> Very fast (&lt; 100 ms) reaction times were removed. We also discarded all reaction times to an object if participant’s accuracy was less than 80%. This process removed 3.6% of RT data.</p><p id="P80"><italic>Model fitting for visual homogeneity.</italic> We proceeded as before by embedding the oddball detection response times into multidimensional space with 3 dimensions. For each image, the visual homogeneity was defined as its distance from an optimum center. We then calculated the correlation between the visual homogeneity calculated relative to this optimum center and the response times for the target-present and target-absent search arrays separately. This optimum center was estimated using a constrained nonlinear optimization to maximize the difference between the correlations for asymmetric object response times and symmetric object response times. Other details were the same as in <xref ref-type="sec" rid="S21">Experiment-2</xref>.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Text</label><media xlink:href="EMS158082-supplement-Supplementary_Text.pdf" mimetype="application" mime-subtype="pdf" id="d2aAdEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S28"><title>Acknowledgements</title><p>We thank Divya Gulati for help with data collection of <xref ref-type="supplementary-material" rid="SD1">Experiments S4 &amp; S5</xref>. This research was supported by the DBT/Wellcome Trust India Alliance Senior Fellowship awarded to SPA (Grant# IA/S/17/1/503081). GJ was supported by a Senior Research Fellowship from MHRD, Government of India.</p></ack><sec id="S29" sec-type="data-availability"><title>Data Availability</title><p id="P81">All data and code required to reproduce the results will be made available on OSF.</p></sec><fn-group><fn fn-type="con" id="FN1"><p id="P82"><bold>Author Contributions</bold></p><p id="P83">GJ and SPA designed the visual search experiments; GJ collected behavioral and fMRI data; GJ &amp; SPA analyzed and interpreted the data; PRT &amp; SPA designed the symmetry experiments; PRT collected fMRI data; GJ, PRT &amp; SPA analyzed and interpreted the data; GJ and SPA wrote the manuscript with inputs from PRT.</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Agrawal</surname><given-names>A</given-names></name><name><surname>Hari</surname><given-names>K</given-names></name><name><surname>Arun</surname><given-names>SP</given-names></name></person-group><article-title>A compositional neural code in high-level visual cortex can explain jumbled word reading</article-title><source>Elife</source><year>2020</year><volume>9</volume><elocation-id>e54846</elocation-id><pub-id pub-id-type="pmcid">PMC7272193</pub-id><pub-id pub-id-type="pmid">32369017</pub-id><pub-id pub-id-type="doi">10.7554/eLife.54846</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arun</surname><given-names>SP</given-names></name></person-group><article-title>Turning visual search time on its head</article-title><source>Vision Res</source><year>2012</year><volume>74</volume><fpage>86</fpage><lpage>92</lpage><pub-id pub-id-type="pmcid">PMC6087462</pub-id><pub-id pub-id-type="pmid">22561524</pub-id><pub-id pub-id-type="doi">10.1016/j.visres.2012.04.005</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashby</surname><given-names>FG</given-names></name><name><surname>Maddox</surname><given-names>WT</given-names></name></person-group><article-title>Human category learning 2.0</article-title><source>Ann N Y Acad Sci</source><year>2011</year><volume>1224</volume><fpage>147</fpage><lpage>161</lpage><pub-id pub-id-type="pmcid">PMC3076539</pub-id><pub-id pub-id-type="pmid">21182535</pub-id><pub-id pub-id-type="doi">10.1111/j.1749-6632.2010.05874.x</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ayzenberg</surname><given-names>V</given-names></name><name><surname>Kamps</surname><given-names>FS</given-names></name><name><surname>Dilks</surname><given-names>DD</given-names></name><name><surname>Lourenco</surname><given-names>SF</given-names></name></person-group><article-title>Skeletal representations of shape in the human visual cortex</article-title><source>Neuropsychologia</source><year>2022</year><volume>164</volume><elocation-id>108092</elocation-id><pub-id pub-id-type="pmid">34801519</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bainbridge</surname><given-names>WA</given-names></name><name><surname>Dilks</surname><given-names>DD</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><article-title>Memorability: A stimulus-driven perceptual neural signature distinctive from memory</article-title><source>Neuroimage</source><year>2017</year><volume>149</volume><fpage>141</fpage><lpage>152</lpage><pub-id pub-id-type="pmid">28132932</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bertamini</surname><given-names>M</given-names></name><name><surname>Makin</surname><given-names>ADJ</given-names></name></person-group><article-title>Brain Activity in Response to Visual Symmetry</article-title><source>Symmetry (Basel)</source><year>2014</year><volume>6</volume><fpage>975</fpage><lpage>996</lpage></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bracci</surname><given-names>S</given-names></name><name><surname>Op de Beeck</surname><given-names>H</given-names></name></person-group><article-title>Dissociations and associations between shape and category representations in the two visual pathways</article-title><source>J Neurosci</source><year>2015</year><volume>36</volume><fpage>432</fpage><lpage>444</lpage><pub-id pub-id-type="pmcid">PMC6602035</pub-id><pub-id pub-id-type="pmid">26758835</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2314-15.2016</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><article-title>The Psychophysics Toolbox</article-title><source>Spat Vis</source><year>1997</year><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlin</surname><given-names>JD</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Adjudicating between face-coding models with individual-face fMRI responses</article-title><source>PLoS Comput Biol</source><year>2017</year><volume>13</volume><elocation-id>e1005604</elocation-id><pub-id pub-id-type="pmcid">PMC5550004</pub-id><pub-id pub-id-type="pmid">28746335</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005604</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duncan</surname><given-names>J</given-names></name><name><surname>Humphreys</surname><given-names>GW</given-names></name></person-group><article-title>Visual search and stimulus similarity</article-title><source>Psychol Rev</source><year>1989</year><volume>96</volume><fpage>433</fpage><lpage>458</lpage><pub-id pub-id-type="pmid">2756067</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eickhoff</surname><given-names>SB</given-names></name><name><surname>Stephan</surname><given-names>KE</given-names></name><name><surname>Mohlberg</surname><given-names>H</given-names></name><name><surname>Grefkes</surname><given-names>C</given-names></name><name><surname>Fink</surname><given-names>GR</given-names></name><name><surname>Amunts</surname><given-names>K</given-names></name><name><surname>Zilles</surname><given-names>K</given-names></name></person-group><article-title>A new SPM toolbox for combining probabilistic cytoarchitectonic maps and functional imaging data</article-title><source>Neuroimage</source><year>2005</year><volume>25</volume><fpage>1325</fpage><lpage>1335</lpage><pub-id pub-id-type="pmid">15850749</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Goulet</surname><given-names>M-A</given-names></name></person-group><source>Investigation of the Cognitive Mechanisms of Same and Different Judgments</source><year>2020</year></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haushofer</surname><given-names>J</given-names></name><name><surname>Livingstone</surname><given-names>MS</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><article-title>Multivariate patterns in object-selective cortex dissociate perceptual and physical shape similarity</article-title><source>PLoS Biol</source><year>2008</year><volume>6</volume><elocation-id>e187</elocation-id><pub-id pub-id-type="pmcid">PMC2486311</pub-id><pub-id pub-id-type="pmid">18666833</pub-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0060187</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacob</surname><given-names>G</given-names></name><name><surname>Arun</surname><given-names>SP</given-names></name></person-group><article-title>How the forest interacts with the trees: Multiscale shape integration explains global and local processing</article-title><source>J Vis</source><year>2020</year><volume>20</volume><fpage>20</fpage><pub-id pub-id-type="pmcid">PMC7594584</pub-id><pub-id pub-id-type="pmid">33107916</pub-id><pub-id pub-id-type="doi">10.1167/jov.20.10.20</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaegle</surname><given-names>A</given-names></name><name><surname>Mehrpour</surname><given-names>V</given-names></name><name><surname>Mohsenzadeh</surname><given-names>Y</given-names></name><name><surname>Meyer</surname><given-names>T</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name><name><surname>Rust</surname><given-names>N</given-names></name></person-group><article-title>Population response magnitude variation in inferotemporal cortex predicts image memorability</article-title><source>Elife</source><year>2019</year><volume>8</volume><elocation-id>e47596</elocation-id><pub-id pub-id-type="pmcid">PMC6715346</pub-id><pub-id pub-id-type="pmid">31464687</pub-id><pub-id pub-id-type="doi">10.7554/eLife.47596</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Katti</surname><given-names>H</given-names></name><name><surname>Arun</surname><given-names>SP</given-names></name></person-group><article-title>A separable neural code in monkey IT enables perfect CAPTCHA decoding</article-title><source>J Neurophysiol</source><year>2022</year><volume>127</volume><fpage>869</fpage><lpage>884</lpage><pub-id pub-id-type="pmcid">PMC8957334</pub-id><pub-id pub-id-type="pmid">35196158</pub-id><pub-id pub-id-type="doi">10.1152/jn.00160.2021</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname><given-names>KN</given-names></name><name><surname>Rokem</surname><given-names>A</given-names></name><name><surname>Winawer</surname><given-names>J</given-names></name><name><surname>Dougherty</surname><given-names>RF</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><article-title>GLMdenoise: A fast, automated technique for denoising task-based fMRI data</article-title><source>Front Neurosci</source><year>2013</year><volume>7</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="pmcid">PMC3865440</pub-id><pub-id pub-id-type="pmid">24381539</pub-id><pub-id pub-id-type="doi">10.3389/fnins.2013.00247</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>J</given-names></name><name><surname>Ricci</surname><given-names>M</given-names></name><name><surname>Serre</surname><given-names>T</given-names></name></person-group><article-title>Not-So-CLEVR: Learning same-different relations strains feedforward neural networks</article-title><source>Interface Focus</source><year>2018</year><volume>8</volume><pub-id pub-id-type="pmcid">PMC6015812</pub-id><pub-id pub-id-type="pmid">29951191</pub-id><pub-id pub-id-type="doi">10.1098/rsfs.2018.0011</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Ruff</surname><given-names>DA</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Bodurka</surname><given-names>J</given-names></name><name><surname>Esteky</surname><given-names>H</given-names></name><name><surname>Tanaka</surname><given-names>K</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name></person-group><article-title>Matching categorical object representations in inferior temporal cortex of man and monkey</article-title><source>Neuron</source><year>2008</year><volume>60</volume><fpage>1126</fpage><lpage>1141</lpage><pub-id pub-id-type="pmcid">PMC3143574</pub-id><pub-id pub-id-type="pmid">19109916</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2008.10.043</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leopold</surname><given-names>Da</given-names></name><name><surname>Bondar</surname><given-names>IV</given-names></name><name><surname>Giese</surname><given-names>Ma</given-names></name></person-group><article-title>Norm-based face encoding by single neurons in the monkey inferotemporal cortex</article-title><source>Nature</source><year>2006</year><volume>442</volume><fpage>572</fpage><lpage>575</lpage><pub-id pub-id-type="pmid">16862123</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lukavský</surname><given-names>J</given-names></name><name><surname>Děchtěrenko</surname><given-names>F</given-names></name></person-group><article-title>Visual properties and memorising scenes: Effects of image-space sparseness and uniformity. Attention, Perception</article-title><source>Psychophys</source><year>2017</year><volume>79</volume><fpage>2044</fpage><lpage>2054</lpage><pub-id pub-id-type="pmid">28707123</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mohan</surname><given-names>K</given-names></name><name><surname>Arun</surname><given-names>SP</given-names></name></person-group><article-title>Similarity relations in visual search predict rapid visual categorization</article-title><source>J Vis</source><year>2012</year><volume>12</volume><fpage>19</fpage><pub-id pub-id-type="pmcid">PMC3586997</pub-id><pub-id pub-id-type="pmid">23092947</pub-id><pub-id pub-id-type="doi">10.1167/12.11.19</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murdock</surname><given-names>BB</given-names></name></person-group><article-title>The distinctiveness of stimuli</article-title><source>Psychol Rev</source><year>1960</year><volume>67</volume><fpage>16</fpage><lpage>31</lpage><pub-id pub-id-type="pmid">14425343</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nickerson</surname><given-names>RS</given-names></name></person-group><article-title>“Same”-”different” response times with multi-attribute stimulus differences</article-title><source>Percept Mot Skills</source><year>1967</year><volume>24</volume><fpage>543</fpage><lpage>554</lpage><pub-id pub-id-type="pmid">6040229</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nickerson</surname><given-names>RS</given-names></name></person-group><article-title>‘Same’-‘different’ response times: A model and a preliminary test</article-title><source>Acta Psychol (Amst)</source><year>1969</year><volume>30</volume><fpage>257</fpage><lpage>275</lpage></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Op de Beeck</surname><given-names>H</given-names></name><name><surname>Wagemans</surname><given-names>J</given-names></name><name><surname>Vogels</surname><given-names>R</given-names></name></person-group><article-title>Inferotemporal neurons represent low-dimensional configurations of parameterized shapes</article-title><source>Nat Neurosci</source><year>2001</year><volume>4</volume><fpage>1244</fpage><lpage>1252</lpage><pub-id pub-id-type="pmid">11713468</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelen</surname><given-names>MV</given-names></name><name><surname>Kastner</surname><given-names>S</given-names></name></person-group><article-title>Attention in the real world: Toward understanding its neural basis</article-title><source>Trends Cogn Sci</source><year>2014</year><volume>18</volume><fpage>242</fpage><lpage>250</lpage><pub-id pub-id-type="pmcid">PMC4908952</pub-id><pub-id pub-id-type="pmid">24630872</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2014.02.004</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peirce</surname><given-names>J</given-names></name><name><surname>Gray</surname><given-names>JR</given-names></name><name><surname>Simpson</surname><given-names>S</given-names></name><name><surname>MacAskill</surname><given-names>M</given-names></name><name><surname>Höchenberger</surname><given-names>R</given-names></name><name><surname>Sogo</surname><given-names>H</given-names></name><name><surname>Kastman</surname><given-names>E</given-names></name><name><surname>Lindeløv</surname><given-names>JK</given-names></name></person-group><article-title>PsychoPy2: Experiments in behavior made easy</article-title><source>Behav Res Methods</source><year>2019</year><volume>51</volume><fpage>195</fpage><lpage>203</lpage><pub-id pub-id-type="pmcid">PMC6420413</pub-id><pub-id pub-id-type="pmid">30734206</pub-id><pub-id pub-id-type="doi">10.3758/s13428-018-01193-y</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petrov</surname><given-names>AA</given-names></name></person-group><article-title>Symmetry-based methodology for decision-rule identification in same-different experiments</article-title><source>Psychon Bull Rev</source><year>2009</year><volume>16</volume><fpage>1011</fpage><lpage>1025</lpage><pub-id pub-id-type="pmid">19966250</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pramod</surname><given-names>RT</given-names></name><name><surname>Arun</surname><given-names>SP</given-names></name></person-group><article-title>Features in visual search combine linearly</article-title><source>J Vis</source><year>2014</year><volume>14</volume><fpage>1</fpage><lpage>20</lpage><pub-id pub-id-type="pmcid">PMC3980647</pub-id><pub-id pub-id-type="pmid">24715328</pub-id><pub-id pub-id-type="doi">10.1167/14.4.6</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pramod</surname><given-names>RT</given-names></name><name><surname>Arun</surname><given-names>SP</given-names></name></person-group><article-title>Object attributes combine additively in visual search</article-title><source>J Vis</source><year>2016</year><volume>16</volume><fpage>8</fpage><pub-id pub-id-type="pmcid">PMC4790416</pub-id><pub-id pub-id-type="pmid">26967014</pub-id><pub-id pub-id-type="doi">10.1167/16.5.8</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pramod</surname><given-names>RT</given-names></name><name><surname>Arun</surname><given-names>SP</given-names></name></person-group><article-title>Symmetric Objects Become Special in Perception Because of Generic Computations in Neurons</article-title><source>Psychol Sci</source><year>2018</year><volume>29</volume><fpage>95</fpage><lpage>109</lpage><pub-id pub-id-type="pmcid">PMC5772447</pub-id><pub-id pub-id-type="pmid">29219748</pub-id><pub-id pub-id-type="doi">10.1177/0956797617729808</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Proklova</surname><given-names>D</given-names></name><name><surname>Kaiser</surname><given-names>D</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><article-title>Disentangling Representations of Object Shape and Object Category in Human Visual Cortex: The Animate-Inanimate Distinction</article-title><source>J Cogn Neurosci</source><year>2016</year><volume>28</volume><fpage>680</fpage><lpage>692</lpage><pub-id pub-id-type="pmid">26765944</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rhodes</surname><given-names>G</given-names></name><name><surname>Jeffery</surname><given-names>L</given-names></name></person-group><article-title>Adaptive norm-based coding of facial identity</article-title><source>Vision Res</source><year>2006</year><volume>46</volume><fpage>2977</fpage><lpage>2987</lpage><pub-id pub-id-type="pmid">16647736</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ricci</surname><given-names>M</given-names></name><name><surname>Cadène</surname><given-names>R</given-names></name><name><surname>Serre</surname><given-names>T</given-names></name></person-group><article-title>Same-different conceptualization: a machine vision perspective</article-title><source>Curr Opin Behav Sci</source><year>2021</year><volume>37</volume><fpage>47</fpage><lpage>55</lpage><pub-id pub-id-type="pmcid">PMC8192071</pub-id><pub-id pub-id-type="pmid">34124319</pub-id><pub-id pub-id-type="doi">10.1016/j.cobeha.2020.06.004</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rust</surname><given-names>NC</given-names></name><name><surname>Mehrpour</surname><given-names>V</given-names></name></person-group><article-title>Understanding Image Memorability</article-title><source>Trends Cogn Sci</source><year>2020</year><volume>24</volume><fpage>557</fpage><lpage>568</lpage><pub-id pub-id-type="pmcid">PMC8818973</pub-id><pub-id pub-id-type="pmid">32386889</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2020.04.001</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sani</surname><given-names>I</given-names></name><name><surname>Stemmann</surname><given-names>H</given-names></name><name><surname>Caron</surname><given-names>B</given-names></name><name><surname>Bullock</surname><given-names>D</given-names></name><name><surname>Stemmler</surname><given-names>T</given-names></name><name><surname>Fahle</surname><given-names>M</given-names></name><name><surname>Pestilli</surname><given-names>F</given-names></name><name><surname>Freiwald</surname><given-names>WA</given-names></name></person-group><article-title>The human endogenous attentional control network includes a ventro-temporal cortical node</article-title><source>Nat Commun</source><year>2021</year><volume>12</volume><fpage>360</fpage><pub-id pub-id-type="pmcid">PMC7810878</pub-id><pub-id pub-id-type="pmid">33452252</pub-id><pub-id pub-id-type="doi">10.1038/s41467-020-20583-5</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sasaki</surname><given-names>Y</given-names></name><name><surname>Vanduffel</surname><given-names>W</given-names></name><name><surname>Knutsen</surname><given-names>T</given-names></name><name><surname>Tyler</surname><given-names>C</given-names></name><name><surname>Tootell</surname><given-names>R</given-names></name></person-group><article-title>Symmetry activates extrastriate visual cortex in human and nonhuman primates</article-title><source>Proc Natl Acad Sci U S A</source><year>2005</year><volume>102</volume><fpage>3159</fpage><lpage>3163</lpage><pub-id pub-id-type="pmcid">PMC549500</pub-id><pub-id pub-id-type="pmid">15710884</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0500319102</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serre</surname><given-names>T</given-names></name></person-group><article-title>Deep Learning: The Good, the Bad, and the Ugly</article-title><source>Annu Rev Vis Sci</source><year>2019</year><volume>5</volume><fpage>399</fpage><lpage>426</lpage><pub-id pub-id-type="pmid">31394043</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sripati</surname><given-names>AP</given-names></name><name><surname>Olson</surname><given-names>CR</given-names></name></person-group><article-title>Global Image Dissimilarity in Macaque Inferotemporal Cortex Predicts Human Visual Search Efficiency</article-title><source>J Neurosci</source><year>2010</year><volume>30</volume><fpage>1258</fpage><lpage>1269</lpage><pub-id pub-id-type="pmcid">PMC2847854</pub-id><pub-id pub-id-type="pmid">20107054</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1908-09.2010</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stewart</surname><given-names>N</given-names></name><name><surname>Morin</surname><given-names>C</given-names></name></person-group><article-title>Dissimilarity is used as evidence of category membership in multidimensional perceptual categorization: a test of the similarity-dissimilarity generalized context model</article-title><source>Q J Exp Psychol (Hove)</source><year>2007</year><volume>60</volume><fpage>1337</fpage><lpage>1346</lpage><pub-id pub-id-type="pmid">17853242</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Storrs</surname><given-names>KR</given-names></name><name><surname>Kietzmann</surname><given-names>TC</given-names></name><name><surname>Walther</surname><given-names>A</given-names></name><name><surname>Mehrer</surname><given-names>J</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Diverse Deep Neural Networks All Predict Human Inferior Temporal Cortex Well, After Training and Fitting</article-title><source>J Cogn Neurosci</source><year>2021</year><volume>33</volume><fpage>2044</fpage><lpage>2064</lpage><pub-id pub-id-type="pmid">34272948</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sunder</surname><given-names>S</given-names></name><name><surname>Arun</surname><given-names>SP</given-names></name></person-group><article-title>Look before you seek: Preview adds a fixed benefit to all searches</article-title><source>J Vis</source><year>2016</year><volume>16</volume><fpage>3</fpage><pub-id pub-id-type="pmcid">PMC5142796</pub-id><pub-id pub-id-type="pmid">27919099</pub-id><pub-id pub-id-type="doi">10.1167/16.15.3</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thorat</surname><given-names>S</given-names></name><name><surname>Proklova</surname><given-names>D</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><article-title>The nature of the animacy organization in human ventral temporal cortex</article-title><source>Elife</source><year>2019</year><volume>8</volume><pub-id pub-id-type="pmcid">PMC6733573</pub-id><pub-id pub-id-type="pmid">31496518</pub-id><pub-id pub-id-type="doi">10.7554/eLife.47142</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Valentine</surname><given-names>T</given-names></name></person-group><article-title>A Unified Account of the Effects of Distinctiveness, Inversion, and Race in Face Recognition</article-title><source>Q J Exp Psychol Sect A</source><year>1991</year><volume>43</volume><fpage>161</fpage><lpage>204</lpage><pub-id pub-id-type="pmid">1866456</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Valentine</surname><given-names>T</given-names></name><name><surname>Bruce</surname><given-names>V</given-names></name></person-group><article-title>Recognizing familiar faces: The role of distinctiveness and familiarity</article-title><source>Can J Psychol Can Psychol</source><year>1986a</year><volume>40</volume><fpage>300</fpage><lpage>305</lpage><pub-id pub-id-type="pmid">3768805</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Valentine</surname><given-names>T</given-names></name><name><surname>Bruce</surname><given-names>V</given-names></name></person-group><article-title>The Effects of Distinctiveness in Recognising and Classifying Faces</article-title><source>Perception</source><year>1986b</year><volume>15</volume><fpage>525</fpage><lpage>535</lpage><pub-id pub-id-type="pmid">3588212</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Meel</surname><given-names>C</given-names></name><name><surname>Baeck</surname><given-names>A</given-names></name><name><surname>Gillebert</surname><given-names>CR</given-names></name><name><surname>Wagemans</surname><given-names>J</given-names></name><name><surname>Op de Beeck</surname><given-names>HP</given-names></name></person-group><article-title>The representation of symmetry in multi-voxel response patterns and functional connectivity throughout the ventral visual stream</article-title><source>Neuroimage</source><year>2019</year><pub-id pub-id-type="pmid">30771448</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verghese</surname><given-names>P</given-names></name></person-group><article-title>Visual search and attention: A signal detection theory approach</article-title><source>Neuron</source><year>2001</year><volume>31</volume><fpage>523</fpage><lpage>535</lpage><pub-id pub-id-type="pmid">11545712</pub-id></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wagemans</surname><given-names>J</given-names></name></person-group><article-title>Characteristics and models of human symmetry detection</article-title><source>Trends Cogn Sci</source><year>1997</year><volume>1</volume><fpage>346</fpage><lpage>352</lpage><pub-id pub-id-type="pmid">21223945</pub-id></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolfe</surname><given-names>JM</given-names></name></person-group><article-title>What Can 1 Million Trials Tell Us About Visual Search?</article-title><source>Psychol Sci</source><year>1998</year><volume>9</volume><fpage>33</fpage><lpage>39</lpage></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolfe</surname><given-names>JM</given-names></name><name><surname>Horowitz</surname><given-names>TS</given-names></name></person-group><article-title>What attributes guide the deployment of visual attention and how do they do it?</article-title><source>Nat Rev Neurosci</source><year>2004</year><volume>5</volume><fpage>495</fpage><lpage>501</lpage><pub-id pub-id-type="pmid">15152199</pub-id></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolfe</surname><given-names>JM</given-names></name><name><surname>Horowitz</surname><given-names>TS</given-names></name></person-group><article-title>Five factors that guide attention in visual search</article-title><source>Nat Hum Behav</source><year>2017</year><volume>1</volume><fpage>0058</fpage></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhivago</surname><given-names>KA</given-names></name><name><surname>Arun</surname><given-names>SP</given-names></name></person-group><article-title>Texture discriminability in monkey inferotemporal cortex predicts human texture perception</article-title><source>J Neurophysiol</source><year>2014</year><volume>112</volume><fpage>2745</fpage><lpage>2755</lpage><pub-id pub-id-type="pmcid">PMC4254883</pub-id><pub-id pub-id-type="pmid">25210165</pub-id><pub-id pub-id-type="doi">10.1152/jn.00532.2014</pub-id></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zoccolan</surname><given-names>D</given-names></name><name><surname>Cox</surname><given-names>DD</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><article-title>Multiple Object Response Normalization in Monkey Inferotemporal Cortex</article-title><source>J Neurosci</source><year>2005</year><volume>25</volume><fpage>8150</fpage><lpage>8164</lpage><pub-id pub-id-type="pmcid">PMC6725538</pub-id><pub-id pub-id-type="pmid">16148223</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2058-05.2005</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Solving visual search and symmetry tasks using visual homogeneity.</title><p>(A) Example target-present search display, containing a single oddball target (horse) among identical distractors (dog). Participants in such tasks have to indicate whether the display contains an oddball or not, without knowing the features of the target or distractor. This means they have to perform this task by detecting some property of each display rather than some feature contained in it.</p><p>(B) Example target-absent search display containing no oddball target.</p><p>(C) Hypothesized neural computation for target present/absent judgements. According to multiple object normalization, the response to multiple items is an average of the responses to the individual items. Thus, the response to a target-absent array will be identical to the individual items, whereas the response to a target-present array will lie along the line joining the corresponding target-absent arrays. This causes the target-absent arrays to stay apart (<italic>red lines</italic>), and the target-present arrays to come closer due to mixing (<italic>blue lines</italic>). If we calculate the distance (VH, for visual homogeneity) for each display, then target-absent arrays will have a larger distance to the center (VH<sub>a</sub>) compared to target-present arrays (VH<sub>p</sub>), and this distance can be used to distinguish between them. <italic>Inset:</italic> Schematic distance from center for target-absent arrays (red) and target-present arrays (blue).</p><p>(D) Example asymmetric object in a symmetry detection task. Here too, participants have to indicate if the display contains a symmetric object or not, without knowing the features of the object itself. This means they have to perform this task by detecting some property in the display.</p><p>(E) Example symmetric object in a symmetry detection task.</p><p>(F) Hypothesized neural computations for symmetry detection. Following multiple object normalization, the response to an object containing repeated parts is equal the response to the individual part, whereas the response to an object containing two different parts will lie along the line joining the objects with the two parts repeating. This causes symmetric objects to stand apart (red lines) and asymmetric objects to come closer due to mixing (<italic>blue lines</italic>). Thus, the visual homogeneity for symmetric objects (VH<sub>s</sub>) will be larger than for asymmetric objects (VH<sub>a</sub>). <italic>Inset:</italic> Schematic distance from center for symmetric objects (red) and asymmetric objects (blue).</p><p>(G)<italic>Behavioral predictions for VH.</italic> If visual homogeneity (VH) is a decision variable in visual search and symmetry detection tasks, then response times (RT) must be largest for displays with VH close to the decision boundary. This predicts opposite correlations between response time and VH for the present/absent or symmetry/asymmetry judgements. It also predicts zero overall correlation between VH and RT.</p><p>(H) <italic>Neural predictions for VH. Left:</italic> Correlation between brain activations and VH for two hypothetical brain regions. In the VH-encoding region, brain activations should be positively correlated with VH. In any region that encodes task difficulty as indexed by response time, brain activity should show no correlation since VH itself is uncorrelated with RT (see Panel G). <italic>Right:</italic> Correlation between brain activations and RT. Since VH is uncorrelated with RT overall, the region VH should show little or no correlation, whereas the regions encoding task difficulty would show a positive correlation.</p></caption><graphic xlink:href="EMS158082-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Visual homogeneity predicts target present/absent responses</title><p>(A) Example search array in an oddball search task (<xref ref-type="sec" rid="S20">Experiment 1</xref>). Participants viewed an array containing identical items except for an oddball present either on the left or right side, and had to indicate using a key press which side the oddball appeared. The reciprocal of average search time was taken as the perceptual distance between the target and distractor items. We measured all possible pairwise distances for 32 grayscale natural objects in this manner.</p><p>(B) Perceptual space reconstructed using multidimensional scaling performed on the pairwise perceptual dissimilarities. In the resulting plot, nearby objects represent hard searches, and far away objects represent easy searches. Some images are shown at a small size due to space constraints; in the actual experiment, all objects were equated to have the same longer dimension. The correlation on the top right indicates the match between the distances in the 2D plot with the observed pairwise distances (**** is p &lt; 0.00005).</p><p>(C) Example display from <xref ref-type="sec" rid="S21">Experiment 2</xref>. Participants performed this task inside the scanner. On each trial, they had to indicate whether an oddball target is present or absent using a key press.</p><p>(D) Predicted response to target-present and target-absent arrays, using the principle that the neural response to multiple items is the average of the individual item responses. This predicts that target-present arrays become similar due to mixing of responses, whereas target-absent arrays stand apart. Consequently, these two types of displays can be distinguished using their distance to a central point in this space. We define this distance as visual homogeneity.</p><p>(E) Mean visual homogeneity relative to the optimum center for target-present and target-absent displays. Error bars represent s.e.m across all displays. Asterisks represent statistical significance (**** is p &lt; 0.00005, unpaired rank-sum test comparing visual homogeneity for 32 target-absent and 32 target-present arrays).</p><p>(F) Response time for target-present searches in <xref ref-type="sec" rid="S21">Experiment 2</xref> plotted against visual homogeneity calculated from <xref ref-type="sec" rid="S20">Experiment 1</xref>. Asterisks represent statistical significance of the correlation (**** is p &lt; 0.00005).</p><p>(G)Response time for target-absent searches in <xref ref-type="sec" rid="S21">Experiment 2</xref> plotted against visual homogeneity calculated from <xref ref-type="sec" rid="S20">Experiment 1</xref>. Asterisks represent statistical significance of the correlation (**** is p &lt; 0.00005).</p></caption><graphic xlink:href="EMS158082-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>A localized brain region encodes visual homogeneity</title><p>A. Searchlight map showing the correlation between mean activation in each 3×3×3 voxel neighborhood and visual homogeneity.</p><p>B. Searchlight map showing the correlation between neural dissimilarity in each 3×3×3 voxel neighborhood and perceptual dissimilarity measured in visual search.</p><p>C. Key visual regions identified using standard anatomical masks: early visual cortex (EVC), area V4, lateral occipital (LO) region. The visual homogeneity (VH) region was identified using the searchlight map in Panel A.</p><p>D. Correlation between the mean activation and visual homogeneity in key visual regions EVC, V4, LO and VH. Error bars represent standard deviation of the correlation obtained using a boostrap process, by repeatedly sampling participants with replacement for 10,000 times. Asterisks represent statistical significance, estimated by calculating the fraction of bootstrap samples in which the observed trend was violated (* is p &lt; 0.05, ** is p&lt; 0.01, **** is p &lt; 0.0001).</p><p>E. Correlation between neural dissimilarity in key visual regions with perceptual dissimilarity. Error bars represent the standard deviation of correlation obtained using a bootstrap process, by repeatedly sampling participants with replacement 10,000 times. Asterisks represent statistical significance, estimated by calculating the fraction of bootstrap samples in which the observed trend was violated (** is p &lt; 0.001).</p></caption><graphic xlink:href="EMS158082-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Visual homogeneity predicts symmetry perception</title><p>(A) Example search array in <xref ref-type="sec" rid="S25">Experiment 3</xref>. Participants viewed an array containing identical items except for an oddball present either on the left or right side, and had to indicate using a key press which side the oddball appeared. The reciprocal of average search time was taken as the perceptual distance between the target and distractor items. We measured all possible pairwise distances for 64 objects (32 symmetric, 32 asymmetric) in this manner.</p><p>(B) Perceptual space reconstructed using multidimensional scaling performed on the pairwise perceptual dissimilarities. In the resulting plot, nearby objects represent hard searches, and far away objects represent easy searches. Some images are shown at a small size due to space constraints; in the actual experiment, all objects were equated to have the same longer dimension. The correlation on the <italic>top right</italic> indicates the match between the distances in the 2D plot with the observed pairwise distances (**** is p &lt; 0.00005).</p><p>(C) Two example displays from <xref ref-type="sec" rid="S26">Experiment 4</xref>. Participants had to indicate whether the object is symmetric or asymmetric using a key press.</p><p>(D) Using the perceptual representation of symmetric and asymmetric objects from <xref ref-type="sec" rid="S25">Experiment 3</xref>, we reasoned that they can be distinguished using their distance to a center in perceptual space. The coordinates of this center are optimized to maximize the match to the observed symmetry detection times.</p><p>(E) Visual homogeneity relative to the optimum center for asymmetric and symmetric objects. Error bar represents s.e.m. across images. Asterisks represent statistical significance (* is p &lt; 0.05, unpaired rank-sum test comparing visual homogeneity for 32 symmetric and 32 asymmetric objects).</p><p>(F) Response time for asymmetric objects in <xref ref-type="sec" rid="S26">Experiment 4</xref> plotted against visual homogeneity calculated from <xref ref-type="sec" rid="S25">Experiment 3</xref>. Asterisks represent statistical significance of the correlation (** is p &lt; 0.001).</p><p>(G)Response time for symmetric objects in <xref ref-type="sec" rid="S26">Experiment 4</xref> plotted against visual homogeneity calculated from <xref ref-type="sec" rid="S25">Experiment 3</xref>. Asterisks represent statistical significance of the correlation (* is p &lt; 0.05).</p></caption><graphic xlink:href="EMS158082-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Brain region encoding visual homogeneity during symmetry detection</title><p>(A) Searchlight map showing the correlation between mean activation in each 3×3×3 voxel neighborhood and visual homogeneity.</p><p>(B) Searchlight map showing the correlation between neural dissimilarity in each 3×3×3 voxel neighborhood and perceptual dissimilarity measured in visual search.</p><p>(C) Key visual regions identified using standard anatomical masks: early visual cortex (EVC), area V4, Lateral occipital (LO) region. The visual homogeneity (VH) region was identified using searchlight map in Panel A.</p><p>(D) Correlation between the mean activation and visual homogeneity in key visual regions EVC, V4, LO and VH. Error bars represent standard deviation of the correlation obtained using a boostrap process, by repeatedly sampling participants with replacement for 10,000 times. Asterisks represent statistical significance, estimated by calculating the fraction of bootstrap samples in which the observed trend was violated (* is p &lt; 0.05, ** is p&lt; 0.01, **** is p &lt; 0.0001).</p><p>(E) Correlation between neural dissimilarity in key visual regions with perceptual dissimilarity. Error bars represent the standard deviation of correlation obtained using a bootstrap process, by repeatedly sampling participants with replacement 10,000 times. Asterisks represent statistical significance, estimated by calculating the fraction of bootstrap samples in which the observed trend was violated (** is p &lt; 0.001).</p></caption><graphic xlink:href="EMS158082-f005"/></fig></floats-group></article>