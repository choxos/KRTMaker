<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS158077</article-id><article-id pub-id-type="doi">10.1101/2022.11.30.518466</article-id><article-id pub-id-type="archive">PPR578713</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group><subj-group subj-group-type="europepmc-category"><subject>Covid-19</subject></subj-group></article-categories><title-group><article-title>Predicting Immune Escape with Pretrained Protein Language Model Embeddings</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Swanson</surname><given-names>Kyle</given-names></name><email>swansonk@stanford.edu</email><aff id="A1">Department of Computer Science, Stanford University</aff></contrib><contrib contrib-type="author"><name><surname>Chang</surname><given-names>Howard</given-names></name><email>howchang@stanford.edu</email><aff id="A2">Center for Personal Dynamic Regulomes, Howard Hughes Medical Institute, Stanford University</aff><xref ref-type="fn" rid="FN1">*</xref></contrib><contrib contrib-type="author"><name><surname>Zou</surname><given-names>James</given-names></name><email>jamesz@stanford.edu</email><aff id="A3">Department of Biomedical Data Science, Stanford University</aff><xref ref-type="fn" rid="FN1">*</xref></contrib></contrib-group><author-notes><fn id="FN1"><label>*</label><p id="P1">Denotes co-senior author.</p></fn></author-notes><pub-date pub-type="nihms-submitted"><day>05</day><month>12</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>02</day><month>12</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P2">Assessing the severity of new pathogenic variants requires an understanding of which mutations enable escape of the human immune response. Even single point mutations to an antigen can cause immune escape and infection by disrupting antibody binding. Recent work has modeled the effect of single point mutations on proteins by leveraging the information contained in large-scale, pretrained protein language models (PLMs). PLMs are often applied in a zero-shot setting, where the effect of each mutation is predicted based on the output of the language model with no additional training. However, this approach cannot appropriately model immune escape, which involves the interaction of two proteins—antibody and antigen—instead of one protein and requires making different predictions for the same antigenic mutation in response to different antibodies. Here, we explore several methods for predicting immune escape by building models on top of embeddings from PLMs. We evaluate our methods on a SARS-CoV-2 deep mutational scanning dataset and show that our embedding-based methods significantly outperform zero-shot methods, which have almost no predictive power. We also highlight insights gained into how best to use embeddings from PLMs to predict escape. Despite these promising results, simple statistical and machine learning baseline models that do not use pretraining perform comparably, showing that computationally expensive pretraining approaches may not be beneficial for escape prediction. Furthermore, all models perform relatively poorly, indicating that future work is necessary to improve escape prediction with or without pretrained embeddings<sup><xref ref-type="fn" rid="FN2">1</xref></sup>.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><label>1</label><title>Introduction</title><p id="P3">Pathogens are constantly evolving in their search to evade the immune system and infect host organisms [<xref ref-type="bibr" rid="R1">1</xref>]. In many organisms, including humans, this evolutionary battle occurs in the context of antibody-antigen interactions [<xref ref-type="bibr" rid="R2">2</xref>]. Antibodies are proteins produced by the immune system that are designed to bind to antigens, which are pathogenic proteins that induce an immune response. Antibodies that effectively bind to an antigen and neutralize the pathogen put evolutionary pressure on the pathogen to mutate its antigen in a process known as immune escape [<xref ref-type="bibr" rid="R3">3</xref>]. Predicting which mutations cause escape is crucial to identifying dangerous pathogenic variants that can cause infection and disease even in the presence of antibodies from prior infection, vaccination, or therapies [<xref ref-type="bibr" rid="R3">3</xref>–<xref ref-type="bibr" rid="R5">5</xref>]. Machine learning models have been developed that can predict the effect of protein mutations on various protein functions [<xref ref-type="bibr" rid="R4">4</xref>, <xref ref-type="bibr" rid="R6">6</xref>–<xref ref-type="bibr" rid="R9">9</xref>]. Recent approaches to mutation effect prediction have leveraged large protein language models (PLMs) that have been trained in an unsupervised manner on huge databases with hundreds of millions to billions of protein sequences [<xref ref-type="bibr" rid="R10">10</xref>, <xref ref-type="bibr" rid="R11">11</xref>]. PLMs learn the underlying statistics of naturally occurring protein sequences and can predict the likelihood that a given amino acid appears at a position in a protein. Prior work has shown that the relative likelihoods of a mutated and wildtype amino acid at a given position are predictive of the effect of that mutation in a zero-shot manner (i.e., without additional training) [<xref ref-type="bibr" rid="R6">6</xref>, <xref ref-type="bibr" rid="R7">7</xref>, <xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R13">13</xref>].</p><p id="P4">However, a major limitation of the zero-shot likelihood approach is that it predicts the same likelihood for a mutation regardless of the protein function in question [<xref ref-type="bibr" rid="R7">7</xref>]. Since proteins can have multiple functions that are affected differently by the same mutation, one likelihood cannot model the effect of a mutation on all of these functions simultaneously. Additionally, the likelihood only accounts for the protein that is mutated, which means that it ignores any interacting proteins such as antibodies.</p><p id="P5">We propose to overcome these limitations by modeling immune escape using antibody and antigen embeddings produced by a PLM. These embeddings encode information about the protein, including aspects of 3D structure, that can inform the effect of protein mutations [<xref ref-type="bibr" rid="R14">14</xref>]. We build a lightweight neural model that learns to extract information from the embeddings to predict escape in an antibody-dependent manner. We develop several variants of this embedding-based approach and evaluate them on a SARS-CoV-2 deep mutational scanning dataset from Cao et al. [<xref ref-type="bibr" rid="R5">5</xref>]. We show that embeddings significantly outperform zero-shot likelihoods, which have almost no predictive power. We discuss insights gained from our experiments about how best to use embeddings from PLMs to predict escape. We also develop two statistical baseline models and a machine learning model that do not rely on the pretrained models. These models perform comparably to the embedding models, indicating that pretrained embeddings may not be beneficial for predicting escape. Furthermore, the relatively poor performance of all models demonstrates that future work is necessary to improve escape prediction with or without pretrained embeddings.</p></sec><sec id="S2" sec-type="methods"><label>2</label><title>Methods</title><p id="P6">Our goal is to design a model that can predict immune escape. Specifically, we model escape by predicting how single point mutations to an antigen affect the binding ability of antibodies. Below, we first outline notation used throughout this section. Then, we describe several models to predict immune escape using either simple statistics (mutation and site models), a machine learning model trained from scratch (RNN), PLM likelihoods, or PLM embeddings.</p><sec id="S3"><label>2.1</label><title>Notation</title><p id="P7">An antigen is a sequence of amino acids <italic>A</italic> = {<italic>A</italic><sub>1</sub>, <italic>A</italic><sub>2</sub>, …, <italic>A</italic><sub><italic>n</italic></sub>} with each <italic>A</italic><sub><italic>i</italic></sub> ∈ <bold>P</bold> where <bold>P</bold> is the set of 20 naturally occurring amino acids. Each location <italic>s</italic> ∈ {1, 2, …, <italic>n</italic>} in the antigen is called a site. The original, unmutated sequence of antigen amino acids is referred to as the wildtype sequence. Due to the abundance of single point mutation data and the relative lack of multi-mutation data, we only consider single point mutations, where a single site <italic>s</italic> in the antigen has its amino acid mutated from the wildtype amino acid <italic>A</italic><sub><italic>s</italic></sub> to the mutant amino acid <italic>M</italic> ∈ <bold>P \</bold> <italic>A</italic><sub><italic>s</italic></sub>, which is one of the other 19 possible amino acids. The mutated antigen sequence then becomes <italic>A</italic><sup><italic>s</italic>→<italic>M</italic></sup> = {<italic>A</italic><sub>1</sub>, …, <italic>A</italic><sub><italic>s</italic>−1</sub>, <italic>M, A</italic><sub><italic>s</italic>+1</sub>, …, <italic>A</italic><sub><italic>n</italic></sub>}. In this paper, we consider a single antigen <italic>A</italic> and all possible <italic>n</italic> × 19 single point mutations across the <italic>n</italic> antigen sites.</p><p id="P8">An antibody <italic>B</italic> is a protein that consists of four chains, where each chain is a sequence of amino acids. Among the four chains, two are identical chains called the heavy chain with the sequence <italic>H</italic> = {<italic>H</italic><sub>1</sub>, <italic>H</italic><sub>2</sub>, …, <italic>H</italic><sub><italic>h</italic></sub>} and two are identical chains called the light chain with the sequence <italic>L</italic> = {<italic>L</italic><sub>1</sub>, <italic>L</italic><sub>2</sub>, …, <italic>L</italic><sub><italic>l</italic></sub>}. The antibody as a whole is represented by the pair of unique chains, <italic>B</italic> = (<italic>H, L</italic>). Here, we consider many different antibodies <italic>B</italic> ∈ <bold>B</bold> where <bold>B</bold> is a set of antibodies, all of which bind to the same antigen <italic>A</italic>.</p><p id="P9">When an antibody comes into contact with an antigen, interactions between the amino acids of the antibody and antigen can result in binding and subsequent neutralization of the pathogen. Mutations to the antigen may inhibit antibody binding. The degree to which antibody binding is reduced by an antigenic mutation is called the escape score. For antigen <italic>A</italic> with amino acid <italic>M</italic> at site <italic>s</italic> in the presence of antibody <italic>B</italic> that binds the wildtype antigen, the experimentally determined escape score is <italic>E</italic>(<italic>A, s, M, B</italic>) ≥ 0, with larger numbers indicating more escape (less antibody binding with the mutation). If <italic>M</italic> is the wildtype amino acid at site <italic>s</italic>, i.e., <italic>A</italic><sub><italic>s</italic></sub> = <italic>M</italic>, then the escape score is zero since the antigen is unchanged. If there is a mutation so that <italic>A</italic><sub><italic>s</italic></sub> <italic>≠ M</italic>, then the escape score may be zero or non-zero depending on whether and to what degree the mutation affects antibody binding.</p><p id="P10">Given a set of training data points <bold>T</bold> = {(<italic>A, s, M, B</italic>)} <sub><italic>s</italic>∈{1,2,…,<italic>n</italic>}, <italic>M</italic>∈<bold>P</bold>, <italic>B</italic>∈<bold>B</bold></sub> consisting of one fixed antigen with many site, mutation, and antibody combinations, along with their known escape scores given by <italic>E</italic>, our goal is to build a model that can predict the escape score for a site, mutation, and antibody combination not in the training set.</p></sec><sec id="S4"><label>2.2</label><title>Mutation Model</title><p id="P11">The mutation model <monospace>MM</monospace> models escape as a function of the change in amino acid from wildtype to mutant. This assumes that amino acid changes have consistent escape effects regardless of the site and antibody. The model is fitted by computing the average escape score in the training set for each pair of wildtype (wt) and mutant (mut) amino acids across all sites and antibodies, i.e., <disp-formula id="FD1"><label>(1)</label><mml:math id="M1"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true"><mml:mi>MM</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>Z</mml:mi></mml:mfrac><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:mn>𝟙</mml:mn></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>]</mml:mo></mml:mrow><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mi>E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mn>𝟙</mml:mn></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula> where <italic>n</italic> is the number of antigen sites, 𝟙 is an indicator variable, and <disp-formula id="FD2"><label>(2)</label><mml:math id="M2"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true"><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:mn>𝟙</mml:mn></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>]</mml:mo></mml:mrow><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mn>𝟙</mml:mn></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula> is the total number of data points in the training set where site <italic>s</italic> is mutated from <italic>M</italic><sup><italic>wt</italic></sup> to <italic>M</italic><sup><italic>mut</italic></sup>. To make a prediction for a new site <italic>s</italic>, mutation <italic>M</italic>, and antibody <italic>B</italic>, the model simply outputs <monospace>MM</monospace>(<italic>A, s, M, B</italic>) = <monospace>MM</monospace>(<italic>A</italic><sub><italic>s</italic></sub>, <italic>M</italic>), thereby ignoring the site, the rest of the antigen sequence, and the antibody sequences. Since there are 20 amino acids, this model has 20 × 20 = 400 parameters.</p></sec><sec id="S5"><label>2.3</label><title>Site Model</title><p id="P12">The site model <monospace>SM</monospace> models escape as a function of the antigen site. This assumes that sites have consistent escape effects regardless of the wildtype and mutant amino acids and the antibody. The model is fitted by computing the average escape score in the training set for each antigen site across all mutant amino acids and across all antibodies, i.e., <disp-formula id="FD3"><label>(3)</label><mml:math id="M3"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true"><mml:mi>SM</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>Z</mml:mi></mml:mfrac><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="bold">P</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mspace width="0.3em"/><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mrow><mml:mn>𝟙</mml:mn></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula> where 𝟙 is an indicator variable and <disp-formula id="FD4"><label>(4)</label><mml:math id="M4"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true"><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="bold">P</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mspace width="0.3em"/><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mn>𝟙</mml:mn></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula> is the total number of data points in the training set for site <italic>s</italic>. To make a prediction for a new site <italic>s</italic>, mutation <italic>M</italic>, and antibody <italic>B</italic>, the model simply outputs <monospace>SM</monospace>(<italic>A, s, M, B</italic>) = <monospace>SM</monospace>(<italic>s</italic>), thereby ignoring the entire antigen sequence, including wildtype and mutant amino acids, and the antibody sequences. The model has one parameter for each antigen site for a total of <italic>n</italic> parameters.</p></sec><sec id="S6"><label>2.4</label><title>RNN</title><p id="P13">We train a recurrent neural network (RNN) from scratch as a non-pretrained baseline machine learning model. Given antigen <italic>A</italic>, site <italic>s</italic>, and mutated amino acid <italic>M</italic> at that site, we construct the mutated antigen sequence <italic>A</italic><sup><italic>s</italic>→<italic>M</italic></sup> and embed it using a bidirectional LSTM [<xref ref-type="bibr" rid="R15">15</xref>]. We then extract an embedding in one of two ways and pass that embedding through a small multilayer perceptron to predict escape. In the model we call RNN Seq, this embedding is the final LSTM hidden state embedding representing the whole sequence. In the model we call RNN Res, this embedding is the LSTM output embedding corresponding to the mutated site <italic>s</italic>. Since the RNN model ignores the antibody sequences, it computes <monospace>RNN</monospace>(<italic>A, s, M, B</italic>) = <monospace>RNN</monospace>(<italic>A, s, M</italic>).</p></sec><sec id="S7"><label>2.5</label><title>Likelihood Model</title><p id="P14">Our likelihood model <monospace>L</monospace> adopts the zero-shot mutation prediction framework of Meier et al. [<xref ref-type="bibr" rid="R7">7</xref>]. In this framework, a PLM is applied to the antigen sequence <italic>A</italic><sup><italic>s</italic>→</sup><monospace><sup>&lt;mask&gt;</sup></monospace>, where the amino acid at site <italic>s</italic> is replaced with a <monospace>&lt;mask&gt;</monospace> token. The escape score is predicted as the model’s log odds ratio of the mutated amino acid <italic>M</italic> versus the wildtype amino acid <italic>A</italic><sub><italic>s</italic></sub> at site <italic>s</italic>. Specifically, the model computes <disp-formula id="FD5"><label>(5)</label><mml:math id="M5"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true"><mml:mrow><mml:mtext mathvariant="bold">L</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>M</mml:mi><mml:mo>∣</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mo fence="false" stretchy="false">&lt;</mml:mo><mml:mtext mathvariant="bold">mask</mml:mtext><mml:mo>&gt;</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mo fence="false" stretchy="false">&lt;</mml:mo><mml:mtext mathvariant="bold">mask</mml:mtext><mml:mo>&gt;</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula> where <italic>p</italic><sub><italic>s</italic></sub>(·|·) is the probability the model assigns to an amino acid at a site <italic>s</italic> within the given sequence. The likelihood model does not require any additional training. It does not incorporate the antibody sequences so <monospace>L</monospace>(<italic>A, s, M, B</italic>) = <monospace>L</monospace>(<italic>A, s, M</italic>).</p></sec><sec id="S8"><label>2.6</label><title>Embedding Models</title><p id="P15">PLM embeddings provide a more flexible way of modeling immune escape than PLM likelihoods since it is possible to combine embeddings of multiple protein sequences in a single model. In our PLM embedding models, we train a small multilayer perceptron to use some form of PLM embedding as input to predict the escape score. All of the models use an embedding of the mutated antigen sequence <italic>A</italic><sup><italic>s</italic>→<italic>M</italic></sup>, and some additionally use an embedding of the wildtype antigen sequence <italic>A</italic> and/or embeddings of the antibody heavy and light chains. The embedding variants are described below and illustrated in <xref ref-type="fig" rid="F1">Figure 1</xref>.</p><sec id="S9"><title>Antigen Sequence Mutant</title><p id="P16">The PLM is given the mutated antigen sequence <italic>A</italic><sup><italic>s</italic>→<italic>M</italic></sup> and computes the embedding matrix <italic>R</italic><sup><italic>mut</italic></sup> = <monospace>PLM</monospace>(<italic>A</italic><sup><italic>s</italic>→<italic>M</italic></sup>) with <italic>R</italic><sup><italic>mut</italic></sup> ∈ ℝ<sup><italic>n</italic>×<italic>d</italic></sup> containing a <italic>d</italic>-dimensional embedding for each amino acid in the antigen sequence. The PLM embedding at each site encodes the identity of the amino acid at that site as well as its role in the context of the antigen sequence. The PLM embeddings of the amino acids are averaged to form a single embedding for the full antigen sequence, <inline-formula id="FD6"><mml:math id="M6"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msubsup><mml:mi>R</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:math></inline-formula> with <italic>R</italic><sup><italic>seq,mut</italic></sup> ∈ ℝ<sup><italic>d</italic></sup>. We refer to this embedding as Antigen Seq Mut.</p></sec><sec id="S10"><title>Antigen Residue Mutant</title><p id="P17">As above, the PLM computes the embedding matrix <italic>R</italic><sup><italic>mut</italic></sup> = <monospace>PLM</monospace>(<italic>A</italic><sup><italic>s</italic>→<italic>M</italic></sup>). Here, the embedding <inline-formula id="FD7"><mml:math id="M7"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true"><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></inline-formula> of the mutated residue at site <italic>s</italic> is used instead of the sequence average. We refer to this embedding as Antigen Res Mut.</p></sec><sec id="S11"><title>Antigen Difference</title><p id="P18">Embeddings are computed for both the mutated antigen sequence <italic>A</italic><sup><italic>s</italic>→<italic>M</italic></sup> and the wildtype antigen sequence <italic>A</italic> as <italic>R</italic><sup><italic>mut</italic></sup> = <monospace>PLM</monospace>(<italic>A</italic><sup><italic>s</italic>→<italic>M</italic></sup>) and <italic>R</italic><sup><italic>wt</italic></sup> = <monospace>PLM</monospace>(<italic>A</italic>). The difference between these embeddings is then computed, either at the sequence level or at the residue level, as <italic>R</italic><sup><italic>seq,diff</italic></sup> = <italic>R</italic><sup><italic>seq,mut</italic></sup>− <italic>R</italic><sup><italic>seq,wt</italic></sup> or <italic>R</italic><sup><italic>res,diff</italic></sup> = <italic>R</italic><sup><italic>res,mut</italic></sup> − <italic>R</italic><sup><italic>res,wt</italic></sup>. We refer to these embeddings as Antigen Seq Diff and Antigen Res Diff embeddings. We also concatenate the mutant and difference embeddings at either the sequence or residue level to form what we call Antigen Seq MutDiff and Antigen Res MutDiff embeddings, which are 2<italic>d</italic>-dimensional.</p></sec><sec id="S12"><title>Antibody</title><p id="P19">We develop four methods of incorporating antibody information into the model. <list list-type="simple" id="L1"><list-item><label>1)</label><p id="P20"><bold>Antibody One-Hot:</bold> We concatenate the antigen embedding (of any form) with a one-hot encoding of the antibody to provide the model with the antibody identity but without embedding information.</p></list-item><list-item><label>2)</label><p id="P21"><bold>Antibody Emb:</bold> We use the PLM to embed the heavy and light chains of the antibody, and we concatenate those two embeddings with the antigen embedding to create a 3<italic>d</italic>-dimensional embedding.</p></list-item><list-item><label>3)</label><p id="P22"><bold>Antibody Att:</bold> We use attention to merge antigen and antibody embeddings. Due to slow training and poor performance, we reserve this model’s description and results for <xref ref-type="supplementary-material" rid="SD1">Appendices C</xref> and <xref ref-type="supplementary-material" rid="SD1">D</xref>.</p></list-item><list-item><label>4)</label><p id="P23"><bold>Antigen Linker Antibody:</bold> We create embeddings of combined antibody-antigen sequences. For each antibody, we create two sequences, both including the mutated antigen <italic>A</italic><sup><italic>s</italic>→<italic>M</italic></sup> and one with the heavy chain <italic>H</italic> and the other with the light chain <italic>L</italic>. In both cases, the antigen and antibody sequences are joined by seven repeats of a glycine-glycine-serine linker. Both linked sequences are embedded by the PLM, sequence averaged, and then concatenated to form a single 2<italic>d</italic>-dimensional embedding for the antigen and antibody. This design is inspired by the use of linkers to enable protein complex prediction from single-chain protein structure prediction models like AlphaFold2 [<xref ref-type="bibr" rid="R16">16</xref>].</p></list-item></list></p></sec></sec></sec><sec id="S13"><label>3</label><title>Experiments</title><p id="P24">Here, we describe the data we use to train and evaluate our model as well as the data splits, tasks, metrics, and models that we use.</p><sec id="S14"><label>3.1</label><title>Data</title><p id="P25">We use SARS-CoV-2 deep mutational scanning data from Cao et al. [<xref ref-type="bibr" rid="R5">5</xref>]. This data consists of 247 antibodies that are known to bind the original strain of SARS-CoV-2 by binding to the receptor binding domain (RBD) of the spike protein. The binding ability of each antibody is measured for the wildtype RBD antigen as well as for all 3,819 single point mutations to the antigen (201 sites in the RBD with 19 amino acid substitutions at each site). For each antibody and each antigen mutation, an escape score is computed as a normalized measure of the reduction in antibody binding compared to the wildtype antigen (see <xref ref-type="fig" rid="F2">Figure 2</xref>). Of the 943,293 escape scores in the dataset, 30,658 (3.2%) are non-zero, all in the range (0, 1] except for 74 outliers above 1 with a max of 3.6 (see <xref ref-type="supplementary-material" rid="SD1">Figure S.1</xref>). Cao et al. [<xref ref-type="bibr" rid="R5">5</xref>] clustered the 247 antibodies into six groups based on their escape scores (see <xref ref-type="supplementary-material" rid="SD1">Figure S.1</xref>).</p></sec><sec id="S15"><label>3.2</label><title>Data Splits</title><p id="P26">The practical usefulness of an escape prediction model, as well as the difficulty of learning such a model, depends on how the data is split. Below we describe and motivate the data splits we use.</p><sec id="S16"><title>Mutation</title><p id="P27">Mutations are randomly split between train and test. This assumes that for a new antibody, we already know escape scores for some but not all mutations across all antigen sites. This split corresponds to a scenario in which we have a significant amount of escape data, either from laboratory mutation experiments or real-world infections by mutated pathogens, across antigen sites for a particular antibody. However, we do not have escape data for the complete set of mutations, so a model trained in this setting would be able to fill in the escape effect of any missing mutations.</p></sec><sec id="S17"><title>Site</title><p id="P28">Antigen sites are randomly split between train and test. This assumes that for a new antibody, we already know escape scores for some but not all antigen sites. In this split, we are more conservative and assume that we only have escape data for some antigen sites and need to make predictions for other antigen sites. This still requires knowing some escape scores for the antibody, but we no longer need to know escape scores across all sites, making it possible to use any available escape data.</p></sec><sec id="S18"><title>Antibody</title><p id="P29">Antibodies are randomly split between train and test. This assumes that we do not know any escape scores for a new antibody. This split models a situation in which some antibodies have already been experimentally evaluated and have escape data, and we want to make predictions for a new antibody for which we do not yet have any escape data.</p></sec><sec id="S19"><title>Antibody group</title><p id="P30">Antibody groups, as defined by a clustering of escape scores, are randomly split between train and test. This assumes that we do not know any escape scores for a new antibody, and furthermore, no antibody in the train set has a similar pattern of escape to this antibody. This split is especially relevant since new groups of antibodies may continue to bind the antigen and eliminate the pathogen even in the presence of antigenic mutations that prevent binding to other antibody groups.</p><p id="P31">In general, the antibody and antibody group splits are more practically useful because they demonstrate the effectiveness of escape prediction for antibodies that have not undergone any experimental escape measurements. This means that new antibodies can be evaluated entirely <italic>in silico</italic>. Escape prediction models that are effective under these data splits could thus be used to guide the selection or design of antibodies that are robust to antigenic mutations that escape other antibodies, providing an avenue for designing effective new antibody treatments against mutating pathogens.</p><p id="P32">For all four splits, we train and test the models across all antibodies (cross-antibody setting) using five-fold cross-validation. For the mutation and site splits where each antibody can appear in both the train and test sets, we also build separate models for each of the 247 antibodies (per-antibody setting). This makes it possible to compare the ability of a single model learned across antibodies to separate models learned for each antibody individually.</p></sec></sec><sec id="S20"><label>3.3</label><title>Tasks and Metrics</title><p id="P33">For all of the models except for the likelihood model, which doesn’t require training, we train the model either for a regression task, where escape scores are real values, or for a classification task, where escape scores are binarized into zero or non-zero escape. All models are evaluated with the metrics ROC-AUC (area under the receiver operating characteristic curve) and PRC-AUC (area under the precision-recall curve), and regression models are additionally evaluated with the metrics MSE (mean squared error) and R<sup>2</sup> (coefficient of determination).</p></sec><sec id="S21"><label>3.4</label><title>Models</title><p id="P34">Below we describe the implementation details of the models we developed. All models were built using PyTorch version 1.12.1 [<xref ref-type="bibr" rid="R17">17</xref>].</p><sec id="S22"><label>3.4.1</label><title>RNN</title><p id="P35">The RNN model is a bidirectional LSTM [<xref ref-type="bibr" rid="R15">15</xref>] with a hidden dimensionality of 100. The input to the RNN is the mutated antigen sequence with amino acids encoded using trainable embeddings with a dimensionality of 100. The output of the RNN is an embedding for each amino acid with a dimensionality of 200 (100 for each direction of the RNN). Either the final hidden state embedding (RNN Seq) or the output embedding of the mutated amino acid (RNN Res) is used as input to a small multilayer perceptron (see below), which makes escape predictions. The amino acid embeddings, RNN, and multilayer perceptron are trained end-to-end.</p></sec><sec id="S23"><label>3.4.2</label><title>Protein Language Model</title><p id="P36">For the likelihood and embedding models, we use the pretrained protein language model ESM2 [<xref ref-type="bibr" rid="R14">14</xref>]. We specifically use the <monospace>esm2_t33_650M_UR50D</monospace> version of the model consisting of 33 layers and 650M parameters that was trained on the UniRef50 database [<xref ref-type="bibr" rid="R18">18</xref>]. The embeddings produced by this model have a dimensionality of 1,280 and are used as fixed input to a small multilayer perceptron.</p></sec><sec id="S24"><label>3.4.3</label><title>Multilayer Perceptron</title><p id="P37">The multilayer perceptron (MLP) model that we use with the RNN and with all of the pretrained embeddings has two hidden layers with 100 neurons in each layer and ReLU activation followed by a single linear output. For classification tasks, we apply a sigmoid activation to the output.</p></sec><sec id="S25"><label>3.4.4</label><title>Training</title><p id="P38">The RNN and the embedding models were trained with mean squared error loss for regression and binary cross entropy loss for classification using the Adam optimizer [<xref ref-type="bibr" rid="R19">19</xref>]. Per-antibody models were trained for 50 epochs while cross-antibody models were trained for one epoch. The RNN model was trained on a single GPU, with training taking about 3 minutes for a cross-antibody model (one fold) and about 30 seconds for each per-antibody model. The embedding models were trained on a single CPU, with training taking about 1 minute for a cross-antibody model (one fold) and about 15 seconds for each per-antibody model.</p></sec></sec></sec><sec id="S26" sec-type="results"><label>4</label><title>Results</title><p id="P39">In this section, we highlight some of the key results from our experiments (see <xref ref-type="fig" rid="F3">Figure 3</xref>). We only show classification model results since the regression models performed poorly. Additionally, since the relative ranking of models was similar between ROC-AUC and PRC-AUC but the differences in PRC-AUC scores were more noticeable, we only present PRC-AUC results. We show results for all data splits and for a subset of the models, leaving out embedding models whose performance was not insightful for space. The complete set of results across all 168 experiments is in <xref ref-type="supplementary-material" rid="SD1">Appendix D</xref>.</p><sec id="S27"><label>4.1</label><title>Mutation Model</title><p id="P40">The mutation model is a very weak model. On the mutation and site splits in the per-antibody setting, the model has essentially no predictive power, and on all four splits in the cross-antibody setting, the model performs poorly. This is to be expected since the model ignores the mutation site even though the mutation site is very informative of immune escape due to the consistent interaction of key antigen sites with binding antibodies.</p></sec><sec id="S28"><label>4.2</label><title>Site Model</title><p id="P41">The site model is strong across most splits with the exception of the site split where the model has no information about unseen sites. The site model is frequently competitive with the best embedding models despite containing only 201 parameters instead of 650M parameters. The site model is significantly more effective in the per-antibody mutation split than in any of the cross-antibody splits since escape is highly consistent at a given antigen site for an antibody across amino acid mutations. Even so, the fact that the model retains some predictive power across antibodies and antibody groups indicates that patterns of escape at specific sites are conserved.</p></sec><sec id="S29"><label>4.3</label><title>RNN</title><p id="P42">The RNN Res model performs comparatively well across all splits. It is only outperformed by the embedding models that include antibody embeddings, which is reasonable given that the RNN only processes the antigen sequence and has no knowledge of the antibody. Notably, the model performs on par with or better than most of the embedding models, even on the site split, despite training in just a couple of minutes with no expensive pretraining needed. This shows that existing pretraining methods and models may not be particularly beneficial for escape prediction, at least for this dataset. Interestingly, the RNN Seq model performs very poorly (see <xref ref-type="supplementary-material" rid="SD1">Appendix D</xref>), which may indicate that sequence averaging obscures the relevant information from the mutated amino acid. A similar phenomenon occurs in the Antigen Seq versus Antigen Res embeddings, meaning that it may be preferable to use residue rather than sequence averaged embeddings across model types.</p></sec><sec id="S30"><label>4.4</label><title>Likelihood Model</title><p id="P43">The likelihood model has virtually no predictive power across all data splits. This is in contrast to examples in the literature where likelihoods achieve reasonable mutation effect prediction performance [<xref ref-type="bibr" rid="R7">7</xref>]. This finding demonstrates a fundamental limitation of the zero-shot prediction framework since likelihoods derived from models trained to recreate naturally occurring proteins may not be calibrated to predict the probability of antigen escape.</p></sec><sec id="S31"><label>4.5</label><title>Embedding Models</title><p id="P44">The embedding models significantly outperform the likelihood model across all data splits. This indicates that PLMs do contain information that is useful for mutation effect prediction but require that their representations are adapted to the task rather than used in a zero-shot manner. However, the strength of the RNN model indicates that pretrained embeddings are not necessary for escape prediction. Even so, the embedding model results still provide several interesting takeaways regarding how best to use pretrained embeddings to predict escape in cases where they may be useful.</p><sec id="S32"><title>Mutant vs Difference</title><p id="P45">The Antigen Seq Diff embedding consistently outperforms the Antigen Seq Mut embedding, which indicates that the change in embedding from wildtype to mutant is more informative than the mutant embedding in isolation. The concatenation of the mutant and difference embeddings (MutDiff) does not improve performance further, indicating that the mutant embeddings do not contribute information beyond that contained in the difference embeddings.</p></sec><sec id="S33"><title>Sequence vs Residue</title><p id="P46">The Antigen Res Mut embedding outperforms the Antigen Seq embeddings, perhaps because the sequence embeddings contain largely irrelevant information from the non-mutated residues. Interestingly, using embedding differences (Antigen Res Diff) instead of mutant embeddings (Antigen Res Mut) does not improve performance at the residue level (see <xref ref-type="supplementary-material" rid="SD1">Appendix D</xref>).</p></sec><sec id="S34"><title>Antibody</title><p id="P47">Including antibody information alongside the antigen embeddings generally provides a benefit in all cross-antibody splits, where each model sees more than one antibody. The Antigen Res Mut Antibody One-Hot encodings provide a particularly large benefit in the mutation and site cross-antibody splits, where the same antibody can appear in train and test and the one-hot encoding makes it easy for the model to associate patterns of escape with particular antibodies. Interestingly, in the mutation splits, the one-hot antibody encoding does not allow the cross-antibody model to recover the performance of the per-antibody models, which may suggest a benefit to training separate models for each antibody, even though each model will be trained on less data.</p><p id="P48">Although the antibody embedding in the Antigen Res Mut Antibody Emb model should also indicate the antibody identity, the model is not able to use this information as well as the one-hot embedding in the mutation and site cross-antibody splits. However, in the antibody and antibody group splits, no antibodies are shared between train and test and the one-hot encoding provides no useful information at test time. In contrast, the antibody embedding provides a small performance boost in the antibody split, but this effect disappears in the harder antibody group split where no similar antibodies are present in the test set. This indicates that the model struggles to learn how to extract useful information from the embeddings of antibodies with diverse antigen binding behavior.</p><p id="P49">The Antigen Linker Antibody embeddings do provide some benefit over antibody-agnostic models in the site cross-antibody split, but otherwise they do not help and sometimes hurt performance, as in the antibody split. This is likely because the PLM was not designed to use linkers and may not provide particularly useful embeddings for such artificially linked sequences.</p></sec></sec></sec><sec id="S35" sec-type="conclusions"><label>5</label><title>Conclusion</title><p id="P50">We present several methods for predicting immune escape using pretrained protein language model embeddings. We performed a comprehensive set of experiments on a SARS-CoV-2 deep mutational scanning dataset and showed that embeddings from PLMs are much more effective at predicting escape than zero-shot likelihoods. The Antigen Res Mut Antibody Emb model was particularly powerful among the embedding models, indicating that escape is best modeled at the residue level with both antigen and antibody embeddings. Although these results are promising, the relatively strong performance of the site model and the RNN, neither of which rely on computationally expensive pretraining, show that PLM embeddings may not be particularly beneficial for tasks such as escape prediction. Furthermore, the overall poor performance of all models across most splits demonstrates that significant future work is needed to make accurate and useful escape predictions. Notably, the results here are limited to a single antibody-antigen escape prediction task and dataset. Although the comprehensive nature of this data, which includes every possible single point mutation of the antigen for every antibody, gives our conclusions strength, experimentation on additional datasets is necessary to validate whether the conclusions drawn here generalize to other escape prediction tasks.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Appendices</label><media xlink:href="EMS158077-supplement-Appendices.pdf" mimetype="application" mime-subtype="pdf" id="d27aAdFbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S36"><title>Acknowledgments</title><p>We would like to thank Mert Yuksekgonul, Mirac Suzgun, Jeremy Wohlwend, and Kirk Swanson for their insightful comments, suggestions, and feedback. We would also like to thank the members of the Chang lab and the Zou lab for their helpful discussions. K.S. gratefully acknowledges the support of the Knight-Hennessy Scholarship.</p></ack><fn-group><fn id="FN2"><label>1</label><p id="P51">Our code, data, embeddings, and results are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/swansonk14/escape_embeddings">https://github.com/swansonk14/escape_embeddings</ext-link></p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rai</surname><given-names>KR</given-names></name><etal/></person-group><article-title>Acute Infection of Viral Pathogens and Their Innate Immune Escape</article-title><source>Frontiers in Microbiology</source><year>2021</year><volume>12</volume><comment>ISSN: 1664-302X</comment><pub-id pub-id-type="pmcid">PMC8258165</pub-id><pub-id pub-id-type="pmid">34239508</pub-id><pub-id pub-id-type="doi">10.3389/fmicb.2021.672026</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kapingidza</surname><given-names>AB</given-names></name><name><surname>Kowal</surname><given-names>K</given-names></name><name><surname>Chruszcz</surname><given-names>M</given-names></name></person-group><chapter-title>Vertebrate and Invertebrate Respiratory Proteins</chapter-title><person-group person-group-type="editor"><name><surname>Hoeger</surname><given-names>U</given-names></name><name><surname>Harris</surname><given-names>JR</given-names></name></person-group><source>Lipoproteins and other Body Fluid Proteins</source><year>2020</year><fpage>465</fpage><lpage>497</lpage><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham</publisher-loc><comment>ISBN 978-3-030-41769-7</comment><pub-id pub-id-type="doi">10.1007/978-3-030-41769-7_19</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Starr</surname><given-names>TN</given-names></name><etal/></person-group><article-title>Prospective mapping of viral mutations that escape antibodies used to treat COVID-19</article-title><source>Science</source><year>2021</year><volume>371</volume><fpage>850</fpage><lpage>854</lpage><pub-id pub-id-type="pmcid">PMC7963219</pub-id><pub-id pub-id-type="pmid">33495308</pub-id><pub-id pub-id-type="doi">10.1126/science.abf9302</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hie</surname><given-names>B</given-names></name><name><surname>Zhong</surname><given-names>ED</given-names></name><name><surname>Berger</surname><given-names>B</given-names></name><name><surname>Bryson</surname><given-names>B</given-names></name></person-group><article-title>Learning the language of viral evolution and escape</article-title><source>Science</source><year>2021</year><volume>371</volume><fpage>284</fpage><lpage>288</lpage><comment>eprint: <ext-link ext-link-type="uri" xlink:href="https://www.science.org/doi/abs/10.1126/science.abd7331">https://www.science.org/doi/abs/10.1126/science.abd7331</ext-link></comment><pub-id pub-id-type="pmid">33446556</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cao</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Omicron escapes the majority of existing SARS-CoV-2 neutralizing antibodies</article-title><source>Nature</source><year>2022</year><month>Feb</month><volume>602</volume><fpage>657</fpage><lpage>663</lpage><comment>ISSN: 1476-4687</comment><pub-id pub-id-type="pmcid">PMC8866119</pub-id><pub-id pub-id-type="pmid">35016194</pub-id><pub-id pub-id-type="doi">10.1038/s41586-021-04385-3</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frazer</surname><given-names>J</given-names></name><etal/></person-group><article-title>Disease variant prediction with deep generative models of evolutionary data</article-title><source>Nature</source><year>2021</year><month>Nov</month><volume>599</volume><fpage>91</fpage><lpage>95</lpage><comment>ISSN: 1476-4687</comment><pub-id pub-id-type="pmid">34707284</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meier</surname><given-names>J</given-names></name><etal/></person-group><article-title>Language models enable zero-shot prediction of the effects of mutations on protein function</article-title><source>bioRxiv</source><year>2021</year><comment>eprint: <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/early/2021/11/17/2021.07.09.450648.full.pdf">https://www.biorxiv.org/content/early/2021/11/17/2021.07.09.450648.full.pdf</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/early/2021/11/17/2021.07.09.450648">https://www.biorxiv.org/content/early/2021/11/17/2021.07.09.450648</ext-link></comment></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taft</surname><given-names>JM</given-names></name><etal/></person-group><article-title>Deep mutational learning predicts ACE2 binding and antibody escape to combinatorial mutations in the SARS-CoV-2 receptor-binding domain</article-title><source>Cell</source><year>2022</year><comment>ISSN: 0092-8674. <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0092867422011199">https://www.sciencedirect.com/science/article/pii/S0092867422011199</ext-link></comment><pub-id pub-id-type="pmcid">PMC9428596</pub-id><pub-id pub-id-type="pmid">36150393</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2022.08.024</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shan</surname><given-names>S</given-names></name><etal/></person-group><article-title>Deep learning guided optimization of human antibody against SARS-CoV-2 variants with broad neutralization</article-title><source>Proceedings of the National Academy of Sciences</source><year>2022</year><volume>119</volume><elocation-id>e2122954119</elocation-id><comment>eprint: <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/doi/abs/10.1073/pnas.2122954119">https://www.pnas.org/doi/abs/10.1073/pnas.2122954119</ext-link></comment><pub-id pub-id-type="pmcid">PMC8931377</pub-id><pub-id pub-id-type="pmid">35238654</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2122954119</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rives</surname><given-names>A</given-names></name><etal/></person-group><article-title>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</article-title><source>Proceedings of the National Academy of Sciences</source><year>2021</year><volume>118</volume><elocation-id>e2016239118</elocation-id><comment>eprint: <ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/doi/abs/10.1073/pnas.2016239118">https://www.pnas.org/doi/abs/10.1073/pnas.2016239118</ext-link></comment><pub-id pub-id-type="pmcid">PMC8053943</pub-id><pub-id pub-id-type="pmid">33876751</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2016239118</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Nijkamp</surname><given-names>E</given-names></name><name><surname>Ruffolo</surname><given-names>J</given-names></name><name><surname>Weinstein</surname><given-names>EN</given-names></name><name><surname>Naik</surname><given-names>N</given-names></name><name><surname>Madani</surname><given-names>A</given-names></name></person-group><source>ProGen2: Exploring the Boundaries of Protein Language Models</source><year>2022</year><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2206.13517">https://arxiv.org/abs/2206.13517</ext-link></comment></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riesselman</surname><given-names>AJ</given-names></name><name><surname>Ingraham</surname><given-names>JB</given-names></name><name><surname>Marks</surname><given-names>DS</given-names></name></person-group><article-title>Deep generative models of genetic variation capture the effects of mutations</article-title><source>Nature Methods</source><year>2018</year><month>Oct</month><volume>15</volume><fpage>816</fpage><lpage>822</lpage><comment>ISSN: 1548-7105</comment><pub-id pub-id-type="pmcid">PMC6693876</pub-id><pub-id pub-id-type="pmid">30250057</pub-id><pub-id pub-id-type="doi">10.1038/s41592-018-0138-4</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shin</surname><given-names>J-E</given-names></name><etal/></person-group><article-title>Protein design and variant prediction using autoregressive generative models</article-title><source>Nature Communications</source><year>2021</year><month>Apr</month><volume>12</volume><elocation-id>2403</elocation-id><comment>ISSN: 2041-1723</comment><pub-id pub-id-type="pmcid">PMC8065141</pub-id><pub-id pub-id-type="pmid">33893299</pub-id><pub-id pub-id-type="doi">10.1038/s41467-021-22732-w</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>Z</given-names></name><etal/></person-group><article-title>Language models of protein sequences at the scale of evolution enable accurate structure prediction</article-title><source>bioRxiv</source><year>2022</year><comment>eprint: <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/early/2022/07/21/2022.07.20.500902.full.pdf">https://www.biorxiv.org/content/early/2022/07/21/2022.07.20.500902.full.pdf</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/early/2022/07/21/2022.07.20.500902">https://www.biorxiv.org/content/early/2022/07/21/2022.07.20.500902</ext-link></comment></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochreiter</surname><given-names>S</given-names></name><name><surname>Schmidhuber</surname><given-names>J</given-names></name></person-group><article-title>Long Short-Term Memory</article-title><source>Neural Computation</source><year>1997</year><month>Nov</month><volume>9</volume><fpage>1735</fpage><lpage>1780</lpage><comment>ISSN: 0899-7667 eprint: <ext-link ext-link-type="uri" xlink:href="https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf">https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf</ext-link></comment><pub-id pub-id-type="pmid">9377276</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Evans</surname><given-names>R</given-names></name><etal/></person-group><article-title>Protein complex prediction with AlphaFold-Multimer</article-title><source>bioRxiv</source><year>2022</year><comment>eprint: <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/early/2022/03/10/2021.10.04.463034.full.pdf">https://www.biorxiv.org/content/early/2022/03/10/2021.10.04.463034.full.pdf</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/early/2022/03/10/2021.10.04.463034">https://www.biorxiv.org/content/early/2022/03/10/2021.10.04.463034</ext-link></comment></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A</given-names></name><etal/></person-group><conf-name>in Proceedings of the 33rd International Conference on Neural Information Processing Systems</conf-name><year>2019</year><conf-sponsor>Curran Associates Inc</conf-sponsor><conf-loc>Red Hook, NY, USA</conf-loc></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suzek</surname><given-names>BE</given-names></name><etal/></person-group><article-title>UniRef clusters: a comprehensive and scalable alternative for improving sequence similarity searches</article-title><source>Bioinformatics</source><year>2014</year><month>Nov</month><volume>31</volume><fpage>926</fpage><lpage>932</lpage><comment>ISSN: 1367-4803. eprint: <ext-link ext-link-type="uri" xlink:href="https://academic.oup.com/bioinformatics/article-pdf/31/6/926/569379/btu739.pdf">https://academic.oup.com/bioinformatics/article-pdf/31/6/926/569379/btu739.pdf</ext-link></comment><pub-id pub-id-type="pmcid">PMC4375400</pub-id><pub-id pub-id-type="pmid">25398609</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btu739</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><conf-name>Adam: A Method for Stochastic Optimization in 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings</conf-name><person-group person-group-type="editor"><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>LeCun</surname><given-names>Y</given-names></name></person-group><year>2015</year><comment><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1412.6980">http://arxiv.org/abs/1412.6980</ext-link></comment></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><p>An illustration of the various PLM embedding models for predicting immune escape. The different embedding types are described in detail in <xref ref-type="sec" rid="S8">Section 2.6</xref>. Note that ⊕ indicates concatenation, ⊖ indicates elementwise difference, and <inline-graphic xlink:href="EMS158077-i001.jpg"/> indicates exclusive or (i.e., only one vector is used).</p></caption><graphic xlink:href="EMS158077-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><p>SARS-CoV-2 immune escape data from Cao et al. [<xref ref-type="bibr" rid="R5">5</xref>] and associated statistical models. (Left) The average escape score across all amino acid mutations for each antibody and each antigen site in the receptor binding domain (RBD) of the SARS-CoV-2 spike protein. (Middle) A mutation model fit on the full dataset, showing the escape score for each wildtype to mutant amino acid change averaged across all antigen sites and antibodies. (Right) A site model fit on the full dataset, showing the escape score for each antigen site averaged across all amino acid mutations and antibodies. Note: In all figures, the 74 escape scores greater than 1 (max 3.6) are truncated to 1.</p></caption><graphic xlink:href="EMS158077-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><p>Classification model results with the PRC-AUC metric across data splits (x-axis) and models (color-coded bars). Error bars indicate the standard deviation across 247 antibodies for per-antibody splits and across five-fold cross-validation for cross-antibody splits.</p></caption><graphic xlink:href="EMS158077-f003"/></fig></floats-group></article>