<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS159004</article-id><article-id pub-id-type="doi">10.1101/2022.12.23.521425</article-id><article-id pub-id-type="archive">PPR588403</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Bird song comparison using deep learning trained from avian perceptual judgments</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zandberg</surname><given-names>Lies</given-names></name><xref ref-type="aff" rid="A1">a</xref><xref ref-type="aff" rid="A2">b</xref></contrib><contrib contrib-type="author"><name><surname>Morfi</surname><given-names>Veronica</given-names></name><xref ref-type="aff" rid="A3">c</xref></contrib><contrib contrib-type="author"><name><surname>George</surname><given-names>Julia</given-names></name><xref ref-type="aff" rid="A2">b</xref><xref ref-type="aff" rid="A4">d</xref></contrib><contrib contrib-type="author"><name><surname>Clayton</surname><given-names>David F.</given-names></name><xref ref-type="aff" rid="A2">b</xref><xref ref-type="aff" rid="A5">e</xref></contrib><contrib contrib-type="author"><name><surname>Stowell</surname><given-names>Dan</given-names></name><xref ref-type="aff" rid="A3">c</xref><xref ref-type="aff" rid="A6">f</xref><xref ref-type="aff" rid="A7">g</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Lachlan</surname><given-names>Robert F.</given-names></name><xref ref-type="aff" rid="A1">a</xref><xref ref-type="aff" rid="A2">b</xref></contrib></contrib-group><aff id="A1"><label>a</label>Department of Psychology, Royal Holloway University of London, UK</aff><aff id="A2"><label>b</label>Department of Psychology, Queen Mary University of London, UK</aff><aff id="A3"><label>c</label>Machine Listening Lab, Centre for Digital Music (CfDM), Queen Mary University of London, UK</aff><aff id="A4"><label>d</label>Department of Biological Sciences, Clemson University, Clemson, SC, USA</aff><aff id="A5"><label>e</label>Department of Genetics and Biochemistry, Clemson University, Clemson, SC, USA</aff><aff id="A6"><label>f</label>Department of Cognitive Science and AI, Tilburg University, Netherlands</aff><aff id="A7"><label>g</label>Naturalis Biodiversity Centre, Leiden, Netherlands</aff><author-notes><corresp id="CR1"><bold>Corresponding author</bold>: Robert Lachlan, Department of Psychology, Royal Holloway, University of London, Egham Hill, Egham, TW20 0EX, United Kingdom. <email>robert.lachlan@rhul.ac.uk</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>24</day><month>12</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>23</day><month>12</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Our understanding of bird song, a model system for animal communication and the neurobiology of learning, depends critically on making reliable, validated comparisons between the complex multidimensional syllables that are used in songs. However, most assessments of song similarity are based on human inspection of spectrograms, or computational methods developed from human intuitions. Using a novel automated operant conditioning system, we collected a large corpus of zebra finches’ (<italic>Taeniopygia guttata</italic>) decisions about song syllable similarity. We use this dataset to compare and externally validate similarity algorithms in widely-used publicly available software (Raven, Sound Analysis Pro, Luscinia). Although these methods all perform better than chance, they do not closely emulate the avian assessments. We then introduce a novel deep learning method that can produce perceptual similarity judgements trained on such avian decisions. We find that this new method outperforms the established methods in accuracy and more closely approaches the avian assessments. Inconsistent (hence ambiguous) decisions are a common occurrence in animal behavioural data; we show that a modification of the deep learning training that accommodates these leads to the strongest performance. We argue this approach is the best way to validate methods to compare song similarity, that our dataset can be used to validate novel methods, and that the general approach can easily be extended to other species.</p></abstract><kwd-group><kwd>Vocal learning</kwd><kwd>Audio</kwd><kwd>Machine learning</kwd><kwd>Bird song</kwd><kwd>Acoustic perception</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Bird song is an important model system for several related fields: the neurobiology of learning, animal communication, sexual selection and cultural evolution (<xref ref-type="bibr" rid="R7">Catchpole &amp; Slater, 2003</xref>; <xref ref-type="bibr" rid="R39">Whiten, 2019</xref>; <xref ref-type="bibr" rid="R2">Bolhuis <italic>et al</italic>., 2010</xref>). Similarities in development, neurobiology and neurogenomics have further led song to become a model system for understanding human speech (<xref ref-type="bibr" rid="R2">Bolhuis <italic>et al</italic>., 2010</xref>; <xref ref-type="bibr" rid="R33">ten Cate <italic>et al</italic>., 2013</xref>). Its importance largely results from the fact that songbirds memorise songs they hear from adult conspecifics during a sensitive phase early in life, and then produce imitations of these songs (<xref ref-type="bibr" rid="R7">Catchpole &amp; Slater, 2003</xref>), but this flexibility also leads to bird song being an unusually variable animal signal.</p><p id="P3">All these fields of bird song research depend on reliable comparisons of song recordings. This task is not straightforward because bird songs, like many acoustic signals, are very high dimensional—with many potential spectral features varying dynamically through the course of each song unit. In fact, when animals themselves compare two vocal signals, they must subjectively integrate numerous differences in frequency and timing. This leads to the realisation that the comparison methods we use need to be validated against animals’ own perception: there is no comparison method that can be objectively “correct” (<xref ref-type="bibr" rid="R19">Janik, 1999</xref>; <xref ref-type="bibr" rid="R33">ten Cate <italic>et al</italic>., 2013</xref>).</p><p id="P4">The first studies comparing bird song were performed by comparing songs by ear or transcribing songs into onomatopoetic descriptions (<xref ref-type="bibr" rid="R25">Marler, 1952</xref>). The invention of the sonograph revolutionized the study of animal communication (<xref ref-type="bibr" rid="R35">Thorpe, 1954</xref>), making it possible to produce a visual representation of the time-frequency structure of the songs on basis of which spectrograms of songs can be compared visually: human visual assessment of spectrographic similarity (HVA). To assess similarity more repeatably, <xref ref-type="bibr" rid="R9">Clark <italic>et al.</italic> (1987)</xref> proposed spectrographic cross correlation (SPCC) as a computational measure of similarity. Two spectrograms are superimposed and then shifted temporally to find the peak correlation coefficient, which is used as a measure of similarity between the sounds. A recent implementation of this method can be found in software such as Raven Pro (<xref ref-type="bibr" rid="R8">Center for Conservation Bioacoustics, 2019</xref>). SPCC relies heavily on the spectrographic representation of the signal, for example being intolerant of often minor differences in the relative duration of spectral components. An alternative approach is to extract contours of acoustic features that are believed to be perceptually relevant, and then to align the contours of two signals and integrate the differences between them in these features. Sound Analysis Pro (SAP) is a widely used tool that employs this approach, measuring several features (e.g. Wiener entropy, spectral continuity, pitch and frequency modulation), that can be tuned to the study species (<xref ref-type="bibr" rid="R32">Tchernichovski <italic>et al</italic>., 2000</xref>). SAP allows for timing differences by linearly warping time by up to 30%. Another software package, Luscinia (<xref ref-type="bibr" rid="R22">Lachlan, 2020</xref>), also uses multiple acoustic features to generate a dissimilarity measure, but uses dynamic time-warping (DTW) to align features. DTW allows non-linear warping of time to find the optimal alignment of features. SAP and DTW both require decisions about which acoustic features to use and how to weight them.</p><p id="P5">Recently, data-driven methods have been proposed for analysis of song similarity based on machine learning (<xref ref-type="bibr" rid="R26">Mets &amp; Brainard, 2018</xref>; <xref ref-type="bibr" rid="R15">Goffinet <italic>et al</italic>., 2021</xref>; <xref ref-type="bibr" rid="R31">Sethi <italic>et al</italic>., 2020</xref>). While not yet widely used in bird song research, such methods reduce the dependence on engineering good features, by using spectrograms or waveforms directly as input. Deep learning can achieve impressive accuracy on various tasks, but this neither implies nor demands that their recognition strategies are similar to those of animals (<xref ref-type="bibr" rid="R13">Fel <italic>et al</italic>., 2022</xref>). All comparison methods make assumptions about what constitutes similarity in acoustic signals. This is true even in the case of unsupervised deep learning, which (like principal components analysis before it) derives a representation automatically from unlabelled data: in such a case, assumptions about similarity emerge implicitly from the choice of training data as well as the neural net structure.</p><p id="P6">The most fundamental problem facing all comparison methods is how their assumptions are validated (<xref ref-type="bibr" rid="R19">Janik, 1999</xref>). The design of the algorithms relies on the intuition of the scientists, although particular decisions can be justified on the basis of perceptual research. But, typically, they have only been validated by comparison with the previous “gold standard”, HVA. However, HVA is based on human, not avian, perception, in the visual, rather than auditory domain, of a spectrographic representation of a sound, which differs in known ways from how sounds are perceived (e.g. the linear rather than logarithmic scaling of frequency (<xref ref-type="bibr" rid="R11">Dooling &amp; Prior, 2017a</xref>)).</p><p id="P7">To overcome these problems, in the present work we have: 1) developed a novel method to test zebra finches for their perception of sound similarity and deployed it to collect a large data-set of avian sound assessments, 2) developed a novel algorithm to train a deep neural network to assess song similarity, and 3) validated this algorithm as well as current conventional methods of measuring song similarity on basis of the birds’ assessments. By using an automated operant feeder in combination with radio-frequency identification (RFID) technology we were able to individually train and test group-living zebra finches continuously, enabling us to collect over 900,000 responses to stimuli. Such response data typically involve some amount of inconsistent or unclear responses, so we further adapted our algorithm to train using both the unambiguous and the ambiguous operant response data. Although here we tested birds for their assessments of similarity in zebra finch syllables, this method can be applied to measure similarity between arbitrary new song unit recordings. Unlike previous methods, this algorithm is not validated by HVA, but by a measure of the birds’ own perceptions of sound similarity. Here we found that the algorithm we developed on basis of the birds’ assessments outperformed all widely used methods.</p></sec><sec id="S2" sec-type="methods"><title>Methods</title><sec id="S3"><title>Assessments of song similarity</title><sec id="S4"><title>Birds</title><p id="P8">We experimentally tested group-living zebra finches for their perception of sound similarity using a novel operant conditioning system. Experiments were performed using 26 domesticated zebra finches from an outbred colony at Queen Mary University of London. All birds used in the experiment were hatched in the population between December 2015 and May 2018 and reared in a flock in a large free-flight room. The birds were fitted with a RFID (radio frequency identification) tag leg band (Eccel Technology Ltd., Leicester, UK) in addition to the standard metal leg ring and a colour band. Throughout the experiment all birds were kept at a 12.00:12.00 light:dark schedule (lights on at 7:00 GMT), in two separate aviary rooms (one room with two aviaries of approx. 100x200x200cm and one room with two aviaries of approx. 200x200x200cm). Each experimental aviary held 4-6 individuals in both single sex and mixed groups. The birds received water <italic>ad libitum</italic>, and a commercial tropical seed mixture from the operant feeders. Each aviary was outfitted with 2 operant feeders. Birds could feed <italic>ad libitum</italic> for a half hour after lights on, and a half hour before lights off. Operant feeders were operational from 7:30 to 18:30, which meant that the birds had to interact with the feeder to gain access to the food.</p></sec><sec id="S5"><title>Operant device</title><p id="P9">We used two different operant feeder designs (see <xref ref-type="fig" rid="F1">Figure 1a</xref> for design 1). Each feeder device consisted of a PIT tag detection system, a motor activated feeding tube, two capacitive touch perches registering responses, speaker system, a Raspberry Pi 3B+ and a Hall effect sensor. In design 1 all electronics are integrated in a weatherproof electronics box, whereas in design 2 these components are integrated in drainpipe elements. Additionally, in design 2 the stimuli are played through two speakers, whereas in design 1 there is only one speaker. Other than the external casing and the number of speakers the devices work identically. The Raspberry Pi computer receives information from the PIT tag system and the response perches, and on basis of this input controls the speaker output and the running of the motor activating the reward mechanism. A PIT antenna was positioned at 20cm from the feeding tube, which registered the bird passing through. On opposite sides of the feeding tube two aluminium response perches were placed, which registered the presence of a bird on the perch through a capacitive touch sensor. The feeding mechanism consisted of a PVC outer tube with an opening on both the left and the right side, and inside this tube a transparent acrylic inner tube with only one opening. This inner tube held all the seeds used as a reward. By turning the inner tube so that the opening lined up with the opening in the outer tube, birds were able to access the seeds in the inner tube. To prevent misalignment of the inner tube a Hall effect sensor was built in together with a magnet attached to the inner tube, to check the position of the inner tube after each reward, and to readjust when needed.</p></sec><sec id="S6"><title>Sound stimuli</title><p id="P10">Sound stimuli used were 887 syllables extracted from zebra finch song recordings (<xref ref-type="bibr" rid="R3">Boogert <italic>et al</italic>., 2018</xref>). These song recordings were made in a different population from the focal population to avoid any potential confounding effects in case of recognition of singer identity. All songs were recorded at 48kHz and divided into separate syllables, which were high pass filtered (100Hz), normalised and a 20ms fade in and out was added. When a stimulus was played it was repeated 4 times in quick succession.</p></sec><sec id="S7"><title>Test paradigm</title><p id="P11">We tested group-living captive zebra finches for their perception of sound similarity using an operant feeder with a 2 alternative forced choice (2AFC) paradigm: forced-choice ‘AXB’ judgments (see <xref ref-type="fig" rid="F1">Figure 1a</xref>). In short, this meant that we trained birds to discriminate between two sound stimuli, ‘A’ and ‘B’ and associate each of them with one of the choice perches. Once the birds were accurate in their responses to these training stimuli we introduced 10–15% probe stimuli ‘X’, which in terms of similarity lies somewhere in between the two sets of training stimuli, for them to assess for their perception of similarity. When a bird was presented with a probe stimulus we expected it to respond by choosing the perch of the training stimulus that it perceived to be most similar to the presented probe. We infer that the choice made in response to the AXB (probe-stimulus) triplet represents a perception of categorical similarity. A trial was started by a bird passing through the PIT antenna and registering its identity on basis of the PIT tag. Depending on the identity of the bird, the software selected a stimulus sound to be played through the speaker(s) on the top of the device.</p><p id="P12">Each stimulus sound is either a training stimulus, and thus associated with a ‘correct’ side of the device, left or right, or is a probe sound. Responses to training sounds were rewarded only when the bird responded by hopping onto the perch associated with that side. In case of an incorrect response, a short fragment of white noise was played and the feeder remained closed. All responses to probe sounds were rewarded by the inner feeding tube opening, through which the birds had access to the seeds for 2 seconds. For a more detailed description of the steps involved in pre-training, training and probe presentation see below and <xref ref-type="fig" rid="F1">Figure 1b</xref>).</p></sec><sec id="S8"><title>Selection of stimuli</title><p id="P13">Using Luscinia sound software (<xref ref-type="bibr" rid="R22">Lachlan, 2020</xref>) we compared all syllables with each other and extracted similarity measures for each combination of syllables. On basis of these similarity measures, we subsequently constructed a matrix with, for each syllable, a ranking of all other syllables ranging from the most to the least similar syllable. From all syllables a random syllable was chosen as the first training stimulus, stimulus ‘A’, and a second training stimulus, stimulus ‘B’, was selected to be ranked between 50 and 150 similar to the first training stimulus. For each stimulus A and B, their 7 most similar syllables were selected as additional stimuli. Probe stimuli were selected to be ranked between 20 and 200 of both training stimuli (see <xref ref-type="supplementary-material" rid="SD1">Figure S1</xref>).</p></sec><sec id="S9"><title>Pre-training</title><p id="P14"><italic>Step 1</italic> – Habituation to the feeder: Feeder opening moves from left to right side, every 30 minutes. Birds can feed <italic>ad libitum</italic> from the feeder. <italic>Step 2</italic> - Birds have to go through the antenna, which triggers a playback of a stimulus. After the playback the feeder immediately opens to the side of the feeder associated with the sound. The feeder remains open for 8 seconds during which the bird can feed. <italic>Step 3</italic> - The time delay to open after playback is increased. When a bird hops onto a choice perch within the time delay period it receives either a correct (stimulus is repeated and the feeder opens immediately) or an incorrect response (playback of 3s white noise, and an added time delay of 30s before the next trial can be initiated). If the bird does not make a choice within the time delay period the feeder opens automatically after the time delay. Step-by-step the time delay period is increased to encourage the birds to make a choice, rather than to wait for the feeder to open automatically. At the same time, the reward time of the automatic opening is reduced from 8 seconds to 3 seconds, whereas the choice-reward time is kept at 8 seconds.</p></sec><sec id="S10"><title>Training</title><p id="P15"><italic>Step 4a</italic> – The time delay to open automatically is removed and from this phase onward the feeder does not open automatically anymore: the birds have to make a correct choice to receive a food reward and the the choice-reward time is reduced to 2 seconds. Similar to the previous phase an incorrect response by the bird is followed by a playback of 3s white noise, and an added time delay of 30s before the next trial can be initiated. In this training phase each bird is trained with its own set of stimuli: one stimulus is rewarded on the left side, one stimulus on the right side. <italic>Step 4b</italic> - When on average the group makes 70% accurate decision we increased the number of stimuli on each side to 4. These stimuli are selected to be the original stimuli plus the 3 syllables most similar to each of the original stimuli. <italic>Step 4c</italic> - When on average the group makes 70% accurate decision we increased to 8 stimuli on each side (original stimulus and the 7 most similar stimuli).</p></sec><sec id="S11"><title>Probe testing</title><p id="P16"><italic>Step 5</italic> - After the group reaches 70% accurate decisions with 8 stimuli on each side we add 10-15% probes. The responses to probe sounds are always rewarded.</p></sec><sec id="S12"><title>Assessing birds’ assessments</title><p id="P17">To assess whether birds made choices we expected, we compared the birds’ assessments of similarity with assessments made by Luscinia. For each stimulus we ranked all other stimuli on basis or their similarity, and determined the rank of the each probe stimulus. For each stimuli-probe triplet we calculated the probe rank difference between the left and the right stimulus: a negative rank difference suggests that, according to Luscinia, the probe is more similar (closer in rank) to the left stimulus, and a positive rank difference suggests that the probe stimulus is more similar to the right stimulus. To account for training stimuli that were closer together or further apart we adjusted the rank difference by dividing it by the mean rank difference. To test whether the birds choices were more similar to Luscinia than expected by chance we compared the birds’ decisions to the distribution of random choice by randomising their decisions 10,000 times. We calculated this for birds with a cycle accuracy of &gt; 65% (overall accuracy in its responses to the training stimuli in a test period).</p></sec></sec><sec id="S13"><title>Deep learning embeddings</title><p id="P18">We next built a deep learning model that can learn from the birds’ judgements about similarity of sounds and use it to create a perceptual space that can mirror birds’ perception. This perceptual space is a learnt vector representation of data referred to as an <italic>embedding</italic> space and can be used for classification, verification and other similarity-based tasks (<xref ref-type="bibr" rid="R6">Bromley <italic>et al</italic>., 1993</xref>; <xref ref-type="bibr" rid="R5">Bredin, 2017</xref>; <xref ref-type="bibr" rid="R31">Sethi <italic>et al</italic>., 2020</xref>). Our deep learning approach follows that of <xref ref-type="bibr" rid="R27">Morfi et <italic>al.</italic> (2021)</xref>, but is adapted to the specific situation in which with animal behavioural data there can be a high proportion of uncertain (ambiguous) data points, due to the limited accuracy and consistency of measured animal judgments. In common with <xref ref-type="bibr" rid="R21">Kumari <italic>et al.</italic> (2019)</xref>, our method is distinct from most deep learning embedding methods (including triplet-based methods) in that no semantic “classification” labels are used. It is also distinct from purely unsupervised methods which are guided only by the signal variation in the dataset, and thus have no necessary link to perceptual or communicative relevance (<xref ref-type="bibr" rid="R15">Goffinet <italic>et al.</italic>, 2021</xref>).</p><p id="P19">In order to infer a data-driven similarity algorithm from our probe responses, it was required to fit a highly non-linear function to the data, where the algorithm input is the audio data, and the fit is conditioned on the responses to our probes. Deep learning has made great advances in creating learnt <italic>embeddings</italic> by methods such as triplet networks which rely on distance metrics rather than classification as the training objective (<xref ref-type="bibr" rid="R30">Schultz &amp; Joachims, 2004</xref>; <xref ref-type="bibr" rid="R38">Weinberger &amp; Saul, 2009</xref>; <xref ref-type="bibr" rid="R37">Wang <italic>et al.</italic>, 2014</xref>; <xref ref-type="bibr" rid="R17">Hoffer &amp; Ailon, 2015</xref>). Triplet networks are trained on triplets of data points consisting of an anchor, a positive sample, and a negative sample. Through the training procedure an embedding space is learnt so that the anchor and the positive samples are featured close to each other, while at the same time the anchor and negative samples are separated as much as possible. However, most previous successes in representation learning are driven by datasets with explicit class labels, which are used to provide a strong signal of semantic distance even for triplet networks (<xref ref-type="bibr" rid="R34">Thakur <italic>et al.</italic>, 2019</xref>). In other words, the distance metric is often derived from an underlying measure of classification correctness, rather than general similarity.</p><p id="P20">Our operant AXB experiments were designed so a probe X can correspond to a triplet anchor. Based on the A/B decision a bird made for that probe, all training stimuli rewarded on the selected side correspond to positive exemplars for that anchor, while all training stimuli rewarded on the opposite side correspond to negative exemplars in the triplet. This makes it possible for us to create a dataset of triplets out of the decisions made by the zebra finches in order to train a triplet network to learn an embedding space for the recordings used in the experiment. We are able to then project new stimuli into this learnt embedding space to measure their similarity to the training stimuli. This is done without any use of labels or class knowledge for the stimuli. The goal is for the learnt embedding space to make the same A/B decisions as birds would make for the recordings.</p></sec><sec id="S14"><title>Network architecture</title><p id="P21">An overview of the deep learning model architecture is depicted in <xref ref-type="supplementary-material" rid="SD1">Figure S2</xref>. We compute the log scaled power mel spectrogram from the zebra finch recordings, of 150 mel bands and 170 time frames, and use it as input to our model. The selection of log mel spectrogram as input was based on prior work which found these useful for deep learning applied to birdsong data ((<xref ref-type="bibr" rid="R27">Morfi <italic>et al.</italic>, 2021</xref>)). The architecture of our model consists of a shared convolutional part followed by two parallel parts of attention pooling and max pooling, the predictions of which are combined at the last layers and projected to an embedding space of <italic>d</italic> dimensions (see <xref ref-type="bibr" rid="R27">Morfi <italic>et al.</italic> (2021)</xref> for more information).</p><p id="P22">For our triplet model, the anchor, positive and negative examples are each passed separately through this network, to estimate the coordinates of each stimulus in the perceptual embedding space. The distances between these coordinates are then used as input to the triplet loss function, described next, which will be used to update the weights of the network.</p><p id="P23">We reiterate that, in common with <xref ref-type="bibr" rid="R21">Kumari <italic>et al.</italic> (2019)</xref>, our method is distinct from most deep learning embedding methods (including triplet-based methods) in that no semantic “classification” labels are used. Our algorithm training is driven by the data coming from two-alternative similarity decisions, using both the ambiguous and unambiguous behavioural data.</p></sec><sec id="S15"><title>Metric learning with triplet loss</title><p id="P24">Let <italic>f</italic> (<italic>x</italic>) ∈ ℝ<sup><italic>d</italic></sup> represent an embedding that maps a sample <italic>x</italic> (an audio clip) into a <italic>d</italic>-dimensional space. Often <italic>f</italic> (<italic>x</italic>) is normalised to have unit length for training stability (<xref ref-type="bibr" rid="R29">Schroff <italic>et al.</italic>, 2015</xref>). We want to ensure that an anchor sample <italic>x<sub>a</sub></italic> is closer to positive samples <italic>x<sub>p</sub></italic> that are more perceptually similar to it than negative samples <italic>x<sub>n</sub></italic> without having any information about the class of each sample. We define the distance between any two points <italic>x<sub>i</sub></italic> and <italic>x<sub>j</sub></italic> as <italic>D<sub>ij</sub></italic>. In our experiments, this distance denotes the squared the Euclidean distance. For a triplet <inline-formula><mml:math id="M1"><mml:mrow><mml:mi mathvariant="script">T</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> of example stimuli the loss is given by
<disp-formula id="FD1"><label>(1)</label><mml:math id="M2"><mml:mrow><mml:mi>ℒ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="script">T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mi>δ</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula>
where [·]<sub>+</sub> is standard hinge loss and <italic>δ</italic> is a non-negative margin hyper-parameter. The larger the margin, the further the negative example should be from the anchor in the embedding space (<xref ref-type="bibr" rid="R17">Hoffer &amp; Ailon, 2015</xref>; <xref ref-type="bibr" rid="R5">Bredin, 2017</xref>; <xref ref-type="bibr" rid="R34">Thakur <italic>et al.</italic>, 2019</xref>).</p></sec><sec id="S16"><title>Unambiguous triplet setting</title><p id="P25">In order to build a system that can model the way birds perceive sounds, we require a set of easily distin-guishable signals to avoid perceptual ambiguity, which in turn requires a way to estimate the perceptual similarity of such signals. Bird decisions for sound similarity can contain a lot of noise; this can be due to the cycle accuracy of the individual, scrounging interactions or the consistency of side decisions for probes caused by perceptual ambiguity of sounds. In triplet learning it can be important to create a dataset without any ambiguity in it, and in our experiments we try to limit this noise from the above sources as much as possible.</p><p id="P26">In order to reduce decision noise and make our dataset unambiguous we perform the following pre-processing steps on the bird decision data:
<list list-type="bullet" id="L1"><list-item><p id="P27">Discard individual’s decisions if cycle accuracy for them is less than 65% on the training stimuli. (We use training stimuli to estimate this, since we cannot calculate it from the probes: we do not make any assumptions about their positive/negative association with the training stimuli.)</p></list-item><list-item><p id="P28">Discard decisions if there were any technical issues with the device for that day.</p></list-item><list-item><p id="P29">Discard inconsistent probe decisions: a consistent probe decision is one where the same side was chosen for that probe during a cycle by an individual over 70% of the times it was played to them.</p></list-item></list>
</p><p id="P30">This unambiguous dataset of decisions formulates 2,099 triplets. We split these into training and evaluation sets of 1,239 and 860 triplets, respectively. The training triplets are used as input to our deep learning model in order for it to learn a space that can satisfy the triplet metric of <xref ref-type="disp-formula" rid="FD1">Equation 1</xref>. The evaluation set only consists of triplets acquired from birds with accuracy of 77% and higher to make it as unambiguous as possible and to properly evaluate the performance of our model.</p></sec><sec id="S17"><title>Ambiguous triplet setting</title><p id="P31">In <xref ref-type="bibr" rid="R21">Kumari et <italic>al.</italic> (2019)</xref> a method is introduced to learn perceptual embeddings for haptic signals, gathered from human participants. Similarly to our data, theirs was based on two-alternative decisions about texture similarity between triplets of items. Their model is trained on both unambiguous and ambiguous triplets and they show an improvement over training only on unambiguous data.</p><p id="P32">To make use of the ambiguous data points for training our model, we collect bird decisions that had inconsistency in side selection. Decisions with 50% to 70% side consistency were chosen from individuals with cycle accuracy of 65% and higher to produce a dataset of “ambiguous” triplets. This ambiguous dataset of decisions totals 1,073 triplets. We split these into training and evaluation sets of 668 and 405 triplets, respectively. Unlike common triplet-based learning approaches which tend to ignore these type of triplets as uninformative, our approach treats both triplet types (ambiguous and unambiguous) as informative, based on the work in <xref ref-type="bibr" rid="R21">Kumari <italic>et al.</italic> (2019)</xref>.</p><p id="P33">Depending on whether a triplet is ambiguous or unambiguous, a different condition needs to be satisfied. For an unambiguous triplet <inline-formula><mml:math id="M3"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>:</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>≥</mml:mo><mml:mi>δ</mml:mi></mml:mrow></mml:math></inline-formula>; on the other hand for an ambiguous triplet <inline-formula><mml:math id="M4"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>:</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, where <italic>D<sub>ij</sub></italic> is the Euclidean distance between any two points <italic>x<sub>i</sub></italic> and <italic>x<sub>j</sub></italic> and <italic>δ</italic> is a non-negative margin hyper-parameter. These conditions are defined as:
<disp-formula id="FD2"><label>(2)</label><mml:math id="M5"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:mi>δ</mml:mi></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD3"><label>(3)</label><mml:math id="M6"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></disp-formula>
</p><p id="P34">In order to facilitate training with both ambiguous and unambiguous triplets, for a triplet <inline-formula><mml:math id="M7"><mml:mrow><mml:mi mathvariant="script">T</mml:mi><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> we adjust the loss function of <xref ref-type="disp-formula" rid="FD1">Equation 1</xref> to:
<disp-formula id="FD4"><label>(4)</label><mml:math id="M8"><mml:mrow><mml:mi>ℒ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="script">T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>exp</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>exp</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula>
where [·]<sub>+</sub>is standard hinge loss and <italic>u</italic> ∈ [0,1] denotes if a triplet is ambiguous (<italic>u</italic> = 0) or unambiguous (<italic>u</italic> = 1).</p><p id="P35">In some of our machine learning tests we use only the unambiguous decisions; we also test the use of ambiguous and unambiguous data together. Furthermore, we explore models that use triplets created by Luscinia after tuning the algorithms parameters (as explained in the following section). In either case the training data are treated as a single training dataset, to train the network by backpropagation using the Adam optimiser, following standard practice in deep learning.</p></sec><sec id="S18"><title>Evaluating deep learning</title><p id="P36">To explore the sensitivity of our deep learning method to hyperparameter settings, we evaluated its performance at different dimensionalities of embedding space, and different ways of using bird decisions and Luscinia-U decisions in training (<xref ref-type="table" rid="T1">Table 1</xref>). To evaluate whether an algorithm can produce similarity measures compatible with the birds’ judgments recorded in the operant devices, we measured the degree to which each algorithm produced the same triplet decision as did the birds. We focussed our evaluation on a held-out set of unambiguous sounds. Due to the inherent variability of behavioural decision data, the gold standard is not 100% but is an accuracy rate matching that of the birds. This rate cannot be measured directly since the ground truth is unknown for the test probes; we estimated it from their cycle accuracy as <italic>estimated maximum attainable accuracy (cycle accuracy)</italic>. We also calculated an upper bound based on the birds’ self- and inter-rater agreement on the test probes. This is an optimistic upper bound since it relies on a consistency assumption (namely, that each bird’s majority decision is the correct decision for a AXB triplet) as <italic>estimated maximum attainable accuracy (consistency assumption).</italic></p></sec><sec id="S19"><title>Luscinia parameter tuning</title><p id="P37">Separately, we used the bird decisions (from the unambiguous triplets) to tune the parameters of the Luscinia software (<xref ref-type="bibr" rid="R22">Lachlan, 2020</xref>). We refer to this tuned software as Luscinia-U.</p><p id="P38">Luscinia uses dynamic time warping (DTW) to align syllables, based on the trajectories of several acoustic features (<xref ref-type="bibr" rid="R22">Lachlan, 2020</xref>). In this case, these features were: fundamental frequency, peak frequency and mean frequency (log-transformed), fundamental and peak frequency change (the arcsin transform of the slope of these features on the spectrogram), normalized fundamental frequency (where the syllable-wide mean of fundamental frequency is subtracted from the fundamental frequency), Wiener entropy and harmonicity (measures of spectral complexity), and vibrato amplitude (a measure of sinusoidal signal in the fundamental frequency). Finally, time itself is included as a feature in order to penalize time warping. When two syllables are compared using this DTW process, a Euclidean distance is calculated over each of these features for each point in one syllable compared with each point in the other syllable. A dynamic algorithm then searches for an efficient alignment between the two syllables using these distances, and an overall dissimilarity is then calculated by averaging dissimilarities over the alignment.</p><p id="P39">A key challenge is to decide how to weight the sound features relative to each other. We developed a data-driven approach that used the same training data that we used to train neural networks. For each triplet in the dataset, we calculated the likelihood that the DTW algorithm would make the same choice as the birds. We then integrated log-likelihoods over the entire training dataset. We then used a Monte Carlo Markov Chain approach to find weightings that would maximise likelihood. The MCMC chain was initiated with equal weightings for each feature. We ran it for 10,000 generations, and treated the first 1,000 generations as burn-in. New parameter values were sampled from a log-Gaussian distribution with standard deviation 0.1. In each generation, parameters were normalized to sum to 1. We then estimated the mean parameter weightings from the last 9,000 generations of the MCMC and used those in a DTW analysis to measure the dissimilarity between each pair of syllables in the dataset. This dissimilarity matrix was then used for further analysis and comparison of different methods. The weightings of acoustic features before and after training are shown in <xref ref-type="supplementary-material" rid="SD1">Table S1</xref>.</p><p id="P40">We evaluated Luscinia-U directly, but we also experimented with using triplet decisions from that system as additional training data for deep learning, to supplement the bird decisions dataset. For this, we used the tuned Luscinia-U to generate additional AXB triplet decisions. These were then used as additional data, either to pretrain the dep learning network or simply pooled with the natural bird decisions during the main training phase. For this we filtered the Luscinia-U triplets down to only the coarse-level (‘easy’) distinctions using a minimum distance threshold, to minimise the risk of contradicting birds’ judgments.</p></sec><sec id="S20"><title>Other software</title><p id="P41">We compared the performance of our deep learning methods with current software used for similarity measures: Raven (<xref ref-type="bibr" rid="R8">Center for Conservation Bioacoustics, 2019</xref>), Sound Analysis Pro (<xref ref-type="bibr" rid="R32">Tchernichovski <italic>et al.</italic>, 2000</xref>) and Luscinia (<xref ref-type="bibr" rid="R22">Lachlan, 2020</xref>) (with and without tuning). For each software package we extracted the (dis)similarity matrix of the evaluation set and used this to determine their AXB decisions. From this we calculated their accuracy at predicting bird AXB decisions.</p></sec><sec id="S21"><title>Song features</title><p id="P42">It is hypothetically possible that our trained deep learning algorithm would rely entirely on just one or a few basic underlying acoustic features, such as fundamental frequency. To test whether this happened, we first calculated, using Luscinia, for each syllable in the data-set, the syllable length, as well as the mean, maximum, minimum, start and end value of each of the following spectral acoustic features: fundamental frequency, peak frequency, median frequency, fundamental frequency change and harmonicity (<xref ref-type="bibr" rid="R18">Holveck <italic>et al.</italic>,2008</xref>). We then carried out an multiple regression on distance matrices (MRM) analysis using the r package <italic>ecodist</italic> (<xref ref-type="bibr" rid="R16">Goslee &amp; Urban, 2007</xref>), a permutation based regression technique, predicting the dissimilarity matrix derived from the output of the machine learning algorithm Emb-LUA.</p><p id="P43">We included six predictor dissimilarity matrices based on the Euclidean distance for each of the five spectral acoustic features plus syllable length (10,000 permutations). The frequency parameters and length were log transformed, and all measures were normalised by their standard deviation.</p></sec></sec><sec id="S22" sec-type="results"><title>Results</title><sec id="S23"><title>Training and Test outcomes</title><p id="P44">Using RFID technology we were able to continuously train and test each bird individually within its home cage and social group, enabling us to collect sound assessments over long time periods and on a much larger scale than conventional operant experiments. We trained 23 birds in 4 aviaries with in total 99 sets of stimuli. Over a period of 11 months we presented the birds with 1,116,214 trials, for which we received 927,158 responses. Of these trials, 25,999 trials were probe trials from which we collected 22,048 sound assessments from the birds. For each bird and each testing period we calculated a cycle accuracy, which is the overall accuracy in its responses to the training stimuli during a specific test period. Out of 99 stimulus sets, for 46 sets the birds reached a cycle accuracy of 65% or higher (<italic>mean</italic> ± <italic>SE</italic> = 72.07% ± 0.63%).</p><p id="P45">To assess whether birds made choices we expected we compared the birds’ assessments of similarity with assessments made by Luscinia. Birds with a cycle accuracy of at least 65% were more likely to choose the perch corresponding to the most-similar probe according to Luscinia, than in a random scenario (10,000 permutations) (<xref ref-type="fig" rid="F2">Figure 2</xref>): random choice 95% confidence interval adjusted rank difference: Left choice = [0.005, 0.03]; median adjusted rank difference birds = -0.06; right choice =[0.07, 0.08]; median adjusted rank difference birds = 0.10; N = 15,141 responses to probe-stimulus triplets; 10,000 permutations.</p></sec><sec id="S24"><title>Deep learning</title><p id="P46">To explore the sensitivity of our deep learning method to hyperparameter settings, we evaluated its performance at different dimensionalities of embedding space (<xref ref-type="supplementary-material" rid="SD1">Figure S3</xref>), and different ways of using bird decisions and Luscinia-U decisions in training (<xref ref-type="table" rid="T1">Table 1</xref> and <xref ref-type="fig" rid="F3">Figure 3</xref>). Good performance was obtained with embeddings of 16 dimensions or more, peaking at 64 dimensions (<xref ref-type="supplementary-material" rid="SD1">Figure S3</xref>).</p><p id="P47">We evaluated our algorithms by measuring the degree to which each algorithm produced the same triplet decisions as birds. We found that the deep learning performance is improved by our interventions, especially Emb-LUA—which used the unambiguous and ambiguous triplets as well as the additional Luscinia-U triplets—but not by pretraining (<xref ref-type="fig" rid="F3">Figure 3</xref>). Using a mix of Luscinia-U decisions made on triplets of sounds along with perceptual decisions from the birds provided the best result.</p><p id="P48">We furthermore investigated the similarities between all models by projecting them in a 2-D space by using multi-dimensional scaling (MDS) (<xref ref-type="bibr" rid="R4">Borg <italic>et al.</italic>, 2013</xref>) (<xref ref-type="fig" rid="F5">Figure 5</xref>). In this visualisation, algorithms are closer together if they produce similar decisions, irrespective of whether those decisions are right or wrong. It illustrates that all three pretrained deep learning models remain very similar, and indeed very similar to Luscinia-U, the source of their pretraining, whereas our Emb-LUA model produces a different pattern of judgments.</p></sec><sec id="S25"><title>Other software</title><p id="P49">When comparing the performance of our deep learning model with current software used for similarity measures—Raven (<xref ref-type="bibr" rid="R8">Center for Conservation Bioacoustics, 2019</xref>), Sound Analysis Pro (<xref ref-type="bibr" rid="R32">Tchernichovski <italic>et al.</italic>, 2000</xref>) and Luscinia (<xref ref-type="bibr" rid="R22">Lachlan, 2020</xref>) (with and without tuning)—we found that deep learning Emb-LU trained using standard unambiguous triplets along with Luscinia-U triplets was unable to outperform these specialist tools (<xref ref-type="fig" rid="F4">Figure 4</xref>). However, our deep learning model Emb-LUA trained on both unambiguous and ambiguous triplets along with Luscinia-U triplets can learn an embedding space that can reflect the birds’ perceptual judgements better than any current software publicly available. Comparison between Emb-LU and Emb-LUA leads us to conclude that even AXB triplets that yield inconsistent side decisions can provide useful information about perceived sound similarity, which helps to constrain the optimisation search space for deep learning. The difference in accuracy between the two best performing models, Emb-LUA and Luscinia-U, is 2.9 percentage points. We added all software methods to the 2-D space by using multi-dimensional scaling (MDS) (<xref ref-type="fig" rid="F5">Figure 5</xref>).</p><p id="P50">When evaluating systems, we found that Sound Analysis Pro would yield similarities of precisely zero for some comparisons (in the case of high dissimilarity). When both pairs of similarities have zero values, we assume the three exemplars to be so dissimilar to each other that no similarity judgement can be computed from Sound Analysis Pro; this was the case for 103 triplets. To inspect whether these highly dissimilar triplets led to an unfair penalty for some systems, we repeated our evaluation with those 103 triplets excluded (<xref ref-type="supplementary-material" rid="SD1">Figure S4</xref>). This did lead to improved performance of Sound Analysis Pro, though the overall pattern of outcomes remained similar, with our Emb-LUA model maintaining the best performance over all other systems.</p></sec><sec id="S26"><title>Song features</title><p id="P51">The set of 26 acoustic features together explained 60.6% of variation in the machine learning dissimilarities (<xref ref-type="supplementary-material" rid="SD1">Figure S5</xref>). All features except fundamental frequency change were significant predictors of the machine learning dissimilarity matrix; fundamental frequency was the strongest predictor. A second analysis including just the single most informative acoustic feature, mean fundamental frequency, explained 56% of variation in the machine learning dissimilarities. Thus while a comprehensive set of acoustic features was, as expected, clearly related to the decisions that the machine learning algorithm made, much variation was left unexplained.</p></sec></sec><sec id="S27" sec-type="discussion"><title>Discussion</title><p id="P52">Making reliable comparisons of song recordings is essential for the field of bird song research. However, so far most comparison methods use HVA for validation, rather than the birds’ perception itself. We compiled a large corpus of zebra finches’ perceptual decisions of syllable similarities, which allowed us to validate existing methods, and also to train a deep neural network to assess song similarity.</p><p id="P53">Training and testing birds in a group setting allowed us to collect a dataset with over 22,000 probe assessments; to our knowledge no similar dataset has been generated and its size allows training of deep learning algorithms as we demonstrate. The social context of training, combined with relying on behavioural rather than neural responses potentially adds additional sources of noise compared with other approaches (e.g. <xref ref-type="bibr" rid="R1">Bell <italic>et al.</italic> (2015)</xref>). While in our experiments some birds reached accuracy levels of over 80%, some other birds’ performance remained consistently low in accuracy. One reason for these differences in accuracy may be that the group setting allowed some birds to scrounge others’ rewards (<xref ref-type="bibr" rid="R14">Giraldeau &amp; Dubois, 2008</xref>), or to interfere with other individuals’ trials. In future experiments our method could be refined by adding an extra RFID antenna to the choice perches to limit choice registration to only the bird that started the trial. But even in social isolation, however, similar operant conditioning designs do not lead to birds approaching 100% accuracy: birds continue to frequently explore unrewarded options. These sources of error are substantially higher than when human observers are asked to generate perceptual training data-sets.</p><p id="P54">The birds’ choices in operant tests were, however, much more similar to computational methods than expected by chance. This suggests that overall the birds make decisions that correspond to acoustic characteristics. The differences between computational assessments and the birds’ decisions is difficult to interpret: we might expect that it can be explained partly by the noise in the data due to the birds’ behavioural variation in responses, and partly by differences between the algorithm and birds underlying perception. We have made the assumption that, taking into account our large sample size, our inclusion only of birds that performed to a reasonably high level on training choices, and evidence of agreement between trials when making the same choice on probes, that higher agreement between birds’ choices and computational judgement reflects a greater similarity between bird perception and the algorithm.</p><p id="P55">Using a mix of Luscinia-U decisions made on triplets of sounds along with perceptual decisions from the birds provided the best correspondence to bird decisions (<xref ref-type="fig" rid="F3">Figure 3</xref>). We noticed that a pretraining phase using Luscinia-U triplet decisions led to worse performance. This finding is in contrast to much recent work in deep learning that makes heavy use of pretraining, especially when target datasets are small (<xref ref-type="bibr" rid="R20">Kong <italic>et al.</italic>,2020</xref>; <xref ref-type="bibr" rid="R24">Lasseck, 2018</xref>). It can be attributed to the fact that the pretrained space (driven by Luscinia’s similarity algorithm) is not appropriate, because it initialises the model such that its fine judgments follow Luscinia rather than bird judgments. Our alternative successful approach is to use both data sources simultaneously in the main training phase of deep learning.</p><p id="P56">The new algorithm cannot be explained well by standard acoustic features of the syllables. Several features together explained only around 60% of the variation in the machine learning dissimilarities. Moreover the relative contribution of different song features are similar to the parameter weight settings that resulted from tuning Luscinia with the birds’ decisions (<xref ref-type="supplementary-material" rid="SD1">Table S1</xref>). Although this wasn’t the primary goal of our study, these two sources of data also provide an indication of how birds integrate different perceptual features when assessing songs. It appears that mean fundamental frequency plays a particularly important role to zebra finches and that other temporal aspects of the pitch trajectory are, perhaps surprisingly, less critical. The birds (and deep learning algorithms) make use of spectro-temporal differences that are not reflected in standard acoustic features.</p><p id="P57">The two algorithms that had been trained with our training data set (Luscinia-U and our new machine learning algorithm) outperformed other methods of song comparison on our testing data-set. This supports the idea that data-informed methods of comparison may be more reliable than ones that rely solely on human intuition. Of the remaining algorithms that we tested, the untrained version of Luscinia’s dynamic time warping algorithm outperformed Sound Analysis Pro and the implementation of cross-correlation in Raven. Aspects of our study system and task may go some way to explaining these differences. Zebra finch song is highly spectrally complex compared to many birds’ song. Spectrographic cross correlation was originally employed on swamp sparrow song, where all spectral energy is concentrated at one frequency at any one time point, and it would seem logical that it might be harder to interpret spectrographic overlap in more complex scenarios. The algorithm used by Sound Analysis Pro was similarly designed for a particular task: to assess similarity between shared song syllables - i.e. pairs of syllables that share some degree of similarity. It was not designed to measure similarity between all pairs of notes, and beyond a certain level of dissimilarity, a floor of 0 is reached. This directly impacted its performance in our test: when removing affected triplets, its performance did not exceed that of Luscinia and our machine learning algorithm, but the three were relatively close in their performance. Most research on zebra finch song over the last two decades has used Sound Analysis Pro and our results would suggest that for tasks involving the assessment of the precision of song learning among relatively similar groups of syllables, these analyses have a reasonable degree of validity.</p><p id="P58">Although the algorithm we introduced here outperforms other commonly used methods and presents a major step forward in the measurement of song similarity, some care has to be taken. In this experiment we have presented birds with single song syllables. Although these isolated syllables, rather than full song, may have sounded less natural to the birds, and also limited the possibilities of measuring higher levels of song perceptions, they made it possible to study sound perception using sounds with features they are naturally attuned to. Furthermore, although all syllables used originated from a single population (different from the experimental population), since the variation in syllables within a zebra finch colony is large compared to that between colonies (<xref ref-type="bibr" rid="R23">Lachlan <italic>et al.</italic>, 2016</xref>) (but see <xref ref-type="bibr" rid="R36">Wang <italic>et al.</italic> (2022)</xref>), we do not expect that this limits the generalisability of the algorithm for different zebra finch colonies. Finally, the current algorithm is only based on zebra finch perceptual judgements which may not be the same for all bird species. Different species have different hearing sensitivities, and may differ in how they weight different sound features (<xref ref-type="bibr" rid="R12">Dooling &amp; Prior, 2017b</xref>; <xref ref-type="bibr" rid="R10">Dooling <italic>et al.</italic>, 1992</xref>; <xref ref-type="bibr" rid="R28">Okanoya &amp; Dooling, 1988</xref>). Collecting similar perceptual judgements from a range of other species will enable us to compare differences in perception and will improve our overall ability to accurately measure song similarity.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Information</label><media xlink:href="EMS159004-supplement-Supplementary_Information.pdf" mimetype="application" mime-subtype="pdf" id="d37aAdEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S28"><title>Acknowledgements</title><p>This research was supported by BBSRC research Grant. No. BB/R008736/1 “Machine Learning for Bird Song Learning.”</p></ack><fn-group><fn id="FN1" fn-type="con"><p id="P59"><bold>Author contributions</bold></p><p id="P60">LZ, VM, DFC, DS, RFL designed the research; LZ, VM, JG, DS, RFL performed the research; LZ, VM, DS, RFL analysed data; LZ, VM, DS, RFL wrote the paper. All authors contributed critically to the drafts and gave final approval for publication.</p></fn></fn-group><sec id="S29" sec-type="data-availability"><title>Data accessibility</title><p id="P61">Both the code and the data will be stored on Zenodo</p></sec><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bell</surname><given-names>BA</given-names></name><name><surname>Phan</surname><given-names>ML</given-names></name><name><surname>Vicario</surname><given-names>DS</given-names></name></person-group><article-title>Neural responses in songbird forebrain reflect learning rates, acquired salience, and stimulus novelty after auditory discrimination training</article-title><source>Journal of neurophysiology</source><year>2015</year><volume>113</volume><fpage>1480</fpage><lpage>1492</lpage><pub-id pub-id-type="pmcid">PMC4346724</pub-id><pub-id pub-id-type="pmid">25475353</pub-id><pub-id pub-id-type="doi">10.1152/jn.00611.2014</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bolhuis</surname><given-names>JJ</given-names></name><name><surname>Okanoya</surname><given-names>K</given-names></name><name><surname>Scharff</surname><given-names>C</given-names></name></person-group><article-title>Twitter evolution: converging mechanisms in birdsong and human speech</article-title><source>Nature Reviews Neuroscience</source><year>2010</year><volume>11</volume><fpage>747</fpage><lpage>759</lpage><pub-id pub-id-type="pmid">20959859</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boogert</surname><given-names>NJ</given-names></name><name><surname>Lachlan</surname><given-names>RF</given-names></name><name><surname>Spencer</surname><given-names>KA</given-names></name><name><surname>Templeton</surname><given-names>CN</given-names></name><name><surname>Farine</surname><given-names>DR</given-names></name></person-group><article-title>Stress hormones, social associations and song learning in zebra finches</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><year>2018</year><volume>373</volume><elocation-id>20170290</elocation-id><pub-id pub-id-type="pmcid">PMC6107560</pub-id><pub-id pub-id-type="pmid">30104435</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2017.0290</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Borg</surname><given-names>I</given-names></name><name><surname>Groenen</surname><given-names>PJF</given-names></name><name><surname>Mair</surname><given-names>P</given-names></name></person-group><source>The Purpose of MDS</source><fpage>7</fpage><lpage>19</lpage><publisher-name>Springer Berlin Heidelberg</publisher-name><publisher-loc>Berlin, Heidelberg</publisher-loc><year>2013</year></element-citation></ref><ref id="R5"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bredin</surname><given-names>H</given-names></name></person-group><source>Tristounet: Triplet loss for speaker turn embedding</source><conf-name>2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)</conf-name><year>2017</year><fpage>5430</fpage><lpage>5434</lpage></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bromley</surname><given-names>J</given-names></name><name><surname>Bentz</surname><given-names>JW</given-names></name><name><surname>Bottou</surname><given-names>L</given-names></name><name><surname>Guyon</surname><given-names>I</given-names></name><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Moore</surname><given-names>C</given-names></name><name><surname>Sackinger</surname><given-names>E</given-names></name><name><surname>Shah</surname><given-names>R</given-names></name></person-group><article-title>Signature verification using a “siamese” time delay neural network</article-title><source>IJPRAI</source><year>1993</year><volume>7</volume><fpage>669</fpage><lpage>688</lpage></element-citation></ref><ref id="R7"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Catchpole</surname><given-names>CK</given-names></name><name><surname>Slater</surname><given-names>PJ</given-names></name></person-group><source>Bird song: biological themes and variations</source><publisher-name>Cambridge university press</publisher-name><year>2003</year></element-citation></ref><ref id="R8"><element-citation publication-type="book"><collab>Center for Conservation Bioacoustics</collab><source>Raven Pro: Interactive Sound Analysis Software</source><year>2019</year><publisher-loc>Ithaca, NY</publisher-loc><publisher-name>The Cornell Lab of Ornithology</publisher-name></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>CW</given-names></name><name><surname>Marler</surname><given-names>P</given-names></name><name><surname>Beeman</surname><given-names>K</given-names></name></person-group><article-title>Quantitative analysis of animal vocal phonology: an application to swamp sparrow song</article-title><source>Ethology</source><year>1987</year><volume>76</volume><fpage>101</fpage><lpage>115</lpage></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dooling</surname><given-names>RJ</given-names></name><name><surname>Brown</surname><given-names>SD</given-names></name><name><surname>Klump</surname><given-names>GM</given-names></name><name><surname>Okanoya</surname><given-names>K</given-names></name></person-group><article-title>Auditory perception of conspecific and heterospecific vocalizations in birds: evidence for special processes</article-title><source>Journal of Comparative Psychology</source><year>1992</year><volume>106</volume><fpage>20</fpage><pub-id pub-id-type="pmid">1555398</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dooling</surname><given-names>RJ</given-names></name><name><surname>Prior</surname><given-names>NH</given-names></name></person-group><article-title>Do we hear what birds hear in birdsong?</article-title><source>Animal behaviour</source><year>2017a</year><volume>124</volume><fpage>283</fpage><lpage>289</lpage><pub-id pub-id-type="pmcid">PMC5884127</pub-id><pub-id pub-id-type="pmid">29628517</pub-id><pub-id pub-id-type="doi">10.1016/j.anbehav.2016.10.012</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dooling</surname><given-names>RJ</given-names></name><name><surname>Prior</surname><given-names>NH</given-names></name></person-group><article-title>Do we hear what birds hear in birdsong?</article-title><source>Animal behaviour</source><year>2017b</year><volume>124</volume><fpage>283</fpage><lpage>289</lpage></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fel</surname><given-names>T</given-names></name><name><surname>Felipe</surname><given-names>I</given-names></name><name><surname>Linsley</surname><given-names>D</given-names></name><name><surname>Serre</surname><given-names>T</given-names></name></person-group><article-title>Harmonizing the object recognition strategies of deep neural networks with humans</article-title><source>arXiv e-prints</source><year>2022</year><elocation-id>arXiv:2211.04533</elocation-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giraldeau</surname><given-names>LA</given-names></name><name><surname>Dubois</surname><given-names>F</given-names></name></person-group><article-title>Social foraging and the study of exploitative behavior</article-title><source>Advances in the Study of Behavior</source><year>2008</year><volume>38</volume><fpage>59</fpage><lpage>104</lpage></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goffinet</surname><given-names>J</given-names></name><name><surname>Brudner</surname><given-names>S</given-names></name><name><surname>Mooney</surname><given-names>R</given-names></name><name><surname>Pearson</surname><given-names>J</given-names></name></person-group><article-title>Low-dimensional learned feature spaces quantify individual and group differences in vocal repertoires</article-title><source>Elife</source><year>2021</year><volume>10</volume><elocation-id>e67855</elocation-id><pub-id pub-id-type="pmcid">PMC8213406</pub-id><pub-id pub-id-type="pmid">33988503</pub-id><pub-id pub-id-type="doi">10.7554/eLife.67855</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goslee</surname><given-names>SC</given-names></name><name><surname>Urban</surname><given-names>DL</given-names></name></person-group><article-title>The ecodist package for dissimilarity-based analysis of ecological data</article-title><source>Journal of Statistical Software</source><year>2007</year><volume>22</volume><fpage>1</fpage><lpage>19</lpage></element-citation></ref><ref id="R17"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hoffer</surname><given-names>E</given-names></name><name><surname>Ailon</surname><given-names>N</given-names></name></person-group><chapter-title>Deep metric learning using triplet network</chapter-title><person-group person-group-type="editor"><name><surname>Feragen</surname><given-names>A</given-names></name><name><surname>Pelillo</surname><given-names>M</given-names></name><name><surname>Loog</surname><given-names>M</given-names></name></person-group><source>Similarity-Based Pattern Recognition</source><fpage>84</fpage><lpage>92</lpage><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham</publisher-loc><year>2015</year></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holveck</surname><given-names>MJ</given-names></name><name><surname>Vieira de Castro</surname><given-names>AC</given-names></name><name><surname>Lachlan</surname><given-names>RF</given-names></name><name><surname>ten Cate</surname><given-names>C</given-names></name><name><surname>Riebel</surname><given-names>K</given-names></name></person-group><article-title>Accuracy of song syntax learning and singing consistency signal early condition in zebra finches</article-title><source>Behavioral Ecology</source><year>2008</year><volume>19</volume><fpage>1267</fpage><lpage>1281</lpage></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Janik</surname><given-names>VM</given-names></name></person-group><article-title>Pitfalls in the categorization of behaviour: a comparison of dolphin whistle classification methods</article-title><source>Animal Behaviour</source><year>1999</year><volume>57</volume><fpage>133</fpage><lpage>143</lpage><pub-id pub-id-type="pmid">10053080</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kong</surname><given-names>Q</given-names></name><name><surname>Cao</surname><given-names>Y</given-names></name><name><surname>Iqbal</surname><given-names>T</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>W</given-names></name><name><surname>Plumbley</surname><given-names>MD</given-names></name></person-group><article-title>PANNs: Large-scale pretrained audio neural networks for audio pattern recognition</article-title><source>IEEE/ACM Transactions on Audio, Speech, and Language Processing</source><year>2020</year><volume>28</volume><fpage>2880</fpage><lpage>2894</lpage></element-citation></ref><ref id="R21"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kumari</surname><given-names>P</given-names></name><name><surname>Chaudhuri</surname><given-names>S</given-names></name><name><surname>Chaudhuri</surname><given-names>S</given-names></name></person-group><source>Perceptnet: Learning perceptual similarity of haptic textures in presence of unorderable triplets</source><conf-name>2019 IEEE World Haptics Conference (WHC)</conf-name><year>2019</year><fpage>163</fpage><lpage>168</lpage></element-citation></ref><ref id="R22"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Lachlan</surname><given-names>RF</given-names></name></person-group><source>Luscinia: Bioacoustic analysis of field recordings (version 2.20.03.11.01)</source><year>2020</year><comment>available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/rflachlan/luscinia/">https://github.com/rflachlan/luscinia/</ext-link></comment></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lachlan</surname><given-names>RF</given-names></name><name><surname>Van Heijningen</surname><given-names>CA</given-names></name><name><surname>Ter Haar</surname><given-names>SM</given-names></name><name><surname>Ten Cate</surname><given-names>C</given-names></name></person-group><article-title>Zebra finch song phonology and syntactical structure across populations and continents—a computational comparison</article-title><source>Frontiers in psychology</source><year>2016</year><volume>7</volume><fpage>980</fpage><pub-id pub-id-type="pmcid">PMC4935685</pub-id><pub-id pub-id-type="pmid">27458396</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2016.00980</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lasseck</surname><given-names>M</given-names></name></person-group><article-title>Audio-based bird species identification with deep convolutional neural networks</article-title><source>Working Notes of CLEF</source><year>2018</year><volume>2018</volume></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marler</surname><given-names>P</given-names></name></person-group><article-title>Variation in the song of the chaffinch fringilla coelebs</article-title><source>Ibis</source><year>1952</year><volume>94</volume><fpage>458</fpage><lpage>472</lpage></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mets</surname><given-names>DG</given-names></name><name><surname>Brainard</surname><given-names>MS</given-names></name></person-group><article-title>An automated approach to the quantitation of vocalizations and vocal learning in the songbird</article-title><source>PLoS computational biology</source><year>2018</year><volume>14</volume><elocation-id>e1006437</elocation-id><pub-id pub-id-type="pmcid">PMC6136806</pub-id><pub-id pub-id-type="pmid">30169523</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006437</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morfi</surname><given-names>V</given-names></name><name><surname>Lachlan</surname><given-names>RF</given-names></name><name><surname>Stowell</surname><given-names>D</given-names></name></person-group><article-title>Deep perceptual embeddings for unlabelled animal sound events</article-title><source>The Journal of the Acoustical Society of America</source><year>2021</year><volume>150</volume><fpage>2</fpage><lpage>11</lpage><pub-id pub-id-type="pmid">34340499</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Okanoya</surname><given-names>K</given-names></name><name><surname>Dooling</surname><given-names>RJ</given-names></name></person-group><article-title>Hearing in the swamp sparrow, melospiza georgiana, and the song sparrow, melospiza melodia</article-title><source>Animal Behaviour</source><year>1988</year><volume>36</volume><fpage>726</fpage><lpage>732</lpage></element-citation></ref><ref id="R29"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Schroff</surname><given-names>F</given-names></name><name><surname>Kalenichenko</surname><given-names>D</given-names></name><name><surname>Philbin</surname><given-names>J</given-names></name></person-group><source>Facenet: A unified embedding for face recognition and clustering</source><conf-name>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><year>2015</year><fpage>815</fpage><lpage>823</lpage></element-citation></ref><ref id="R30"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schultz</surname><given-names>M</given-names></name><name><surname>Joachims</surname><given-names>T</given-names></name></person-group><chapter-title>Learning a distance metric from relative comparisons</chapter-title><person-group person-group-type="editor"><name><surname>Thrun</surname><given-names>S</given-names></name><name><surname>Saul</surname><given-names>LK</given-names></name><name><surname>Scholkopf</surname><given-names>B</given-names></name></person-group><source>Advances in Neural Information Processing Systems</source><volume>16</volume><fpage>41</fpage><lpage>48</lpage><publisher-name>MIT Press</publisher-name><year>2004</year></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sethi</surname><given-names>SS</given-names></name><name><surname>Jones</surname><given-names>NS</given-names></name><name><surname>Fulcher</surname><given-names>BD</given-names></name><name><surname>Picinali</surname><given-names>L</given-names></name><name><surname>Clink</surname><given-names>DJ</given-names></name><name><surname>Klinck</surname><given-names>H</given-names></name><name><surname>Orme</surname><given-names>CD</given-names></name><name><surname>Wrege</surname><given-names>LPH</given-names></name><name><surname>Ewers</surname><given-names>RM</given-names></name></person-group><article-title>Characterizing soundscapes across diverse ecosystems using a universal acoustic feature set</article-title><source>Proceedings of the National Academy of Sciences</source><year>2020</year><volume>117</volume><fpage>17049</fpage><lpage>17055</lpage><pub-id pub-id-type="pmcid">PMC7382238</pub-id><pub-id pub-id-type="pmid">32636258</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2004702117</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tchernichovski</surname><given-names>O</given-names></name><name><surname>Nottebohm</surname><given-names>F</given-names></name><name><surname>Ho</surname><given-names>CE</given-names></name><name><surname>Pesaran</surname><given-names>B</given-names></name><name><surname>Mitra</surname><given-names>PP</given-names></name></person-group><article-title>A procedure for an automated measurement of song similarity</article-title><source>Animal behaviour</source><year>2000</year><volume>59</volume><fpage>1167</fpage><lpage>1176</lpage><pub-id pub-id-type="pmid">10877896</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>ten Cate</surname><given-names>C</given-names></name><name><surname>Lachlan</surname><given-names>R</given-names></name><name><surname>Zuidema</surname><given-names>W</given-names></name></person-group><chapter-title>Analyzing the structure of bird vocalizations and language: Finding common ground</chapter-title><person-group person-group-type="editor"><name><surname>Bolhuis</surname><given-names>JJ</given-names></name><name><surname>Everaert</surname><given-names>M</given-names></name></person-group><source>Birdsong, speech, and language: exploring the evolution of mind and brain</source><fpage>243</fpage><lpage>260</lpage><publisher-name>MIT Press</publisher-name><year>2013</year></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thakur</surname><given-names>A</given-names></name><name><surname>Thapar</surname><given-names>D</given-names></name><name><surname>Rajan</surname><given-names>P</given-names></name><name><surname>Nigam</surname><given-names>A</given-names></name></person-group><article-title>Deep metric learning for bioacoustic classification: Overcoming training data scarcity using dynamic triplet loss</article-title><source>The Journal of the Acoustical Society of America</source><year>2019</year><volume>146</volume><fpage>534</fpage><lpage>547</lpage><pub-id pub-id-type="pmid">31370640</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thorpe</surname><given-names>WH</given-names></name></person-group><article-title>The process of song-learning in the chaffinch as studied by means of the sound spectrograph</article-title><source>Nature</source><year>1954</year><volume>173</volume><fpage>465</fpage><lpage>469</lpage></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>D</given-names></name><name><surname>Forstmeier</surname><given-names>W</given-names></name><name><surname>Farine</surname><given-names>DR</given-names></name><name><surname>Maldonado-Chaparro</surname><given-names>AA</given-names></name><name><surname>Martin</surname><given-names>K</given-names></name><name><surname>Pei</surname><given-names>Y</given-names></name><name><surname>Alarcon-Nieto</surname><given-names>G</given-names></name><name><surname>Klarevas-Irby</surname><given-names>JA</given-names></name><name><surname>Ma</surname><given-names>S</given-names></name><name><surname>Aplin</surname><given-names>LM</given-names></name><etal/></person-group><article-title>Machine learning reveals cryptic dialects that explain mate choice in a songbird</article-title><source>Nature communications</source><year>2022</year><volume>13</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="pmcid">PMC8960899</pub-id><pub-id pub-id-type="pmid">35347115</pub-id><pub-id pub-id-type="doi">10.1038/s41467-022-28881-w</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Song</surname><given-names>Y</given-names></name><name><surname>Leung</surname><given-names>T</given-names></name><name><surname>Rosenberg</surname><given-names>C</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Philbin</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>B</given-names></name><name><surname>Wu</surname><given-names>Y</given-names></name></person-group><source>Learning fine-grained image similarity with deep ranking</source><conf-name>2014 IEEE Conference on Computer Vision and Pattern Recognition</conf-name><year>2014</year><fpage>1386</fpage><lpage>1393</lpage></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weinberger</surname><given-names>KQ</given-names></name><name><surname>Saul</surname><given-names>LK</given-names></name></person-group><article-title>Distance metric learning for large margin nearest neighbor classification</article-title><source>Journal of Machine Learning Research</source><year>2009</year><volume>10</volume><fpage>207</fpage><lpage>244</lpage></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whiten</surname><given-names>A</given-names></name></person-group><article-title>Cultural evolution in animals</article-title><source>Annual Review of Ecology, Evolution, and Systematics</source><year>2019</year><volume>50</volume><fpage>27</fpage><lpage>48</lpage></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><p><bold>a)</bold> Schematic image of operant feeder design 1. <bold>b)</bold> Bird training scheme with the different training steps.</p></caption><graphic xlink:href="EMS159004-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><p>Birds’ choices in operant tests were more similar to computational methods than expected by chance. The figure shows the adjusted rank differences between probes and stimuli and the standardised density of choices for each of the sides of the apparatus. 95% confidence intervals of 1000 permutations of random responses are shown in light red and blue, and the birds’ actual assessments are shown in the dark red and dark blue lines. Random choice 95% confidence interval adjusted rank difference: Left choice =[0.005, 0.03]; median adjusted rank difference birds = -0.06; right choice =[0.07, 0.08]; median adjusted rank difference birds = 0.10</p></caption><graphic xlink:href="EMS159004-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><p>Deep learning performance is improved by our interventions, but not by pretraining. Accuracy performance of the different variants of our deep learning model, each model being described in <xref ref-type="table" rid="T1">Table 1</xref>. Error bars show 95% confidence intervals (bootstrap estimate). The upper lines indicate two different ways to estimate the maximum accuracy attainable in principle; 100% is not attainable since birds’ choices are not always consistent for the same stimulus set. An accuracy of 50% represents chance level.</p></caption><graphic xlink:href="EMS159004-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><p>Accuracy of computational methods at predicting bird AXB decisions, evaluated on the evaluation set. Details are the same as in <xref ref-type="fig" rid="F4">figure 4</xref>.</p></caption><graphic xlink:href="EMS159004-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><p>Different algorithms differ from birds’ judgements of similarity in different ways. 2-D multidimensional scaling (MDS) projection of the similarity between different algorithms based on their predictions. <italic>BIRDS</italic> is the ground truth provided by the bird decisions. Current software methods: <italic>SAP</italic> (Sound Analysis Pro). <italic>Raven,Lusc</italic> (Luscinia). <italic>Lusc-U</italic> represents the Luscinia software with tuned Luscinia parameters. Deep learning embedding models: <italic>Emb-*</italic> are as described in <xref ref-type="table" rid="T1">Table 1</xref>.</p></caption><graphic xlink:href="EMS159004-f005"/></fig><table-wrap id="T1" position="float" orientation="portrait"><label>Table 1</label><caption><title>Table of attributes for all learnt embedding space through our deep learning model.</title></caption><table frame="void" rules="cols"><thead><tr style="border-bottom: solid thin"><th align="center" valign="middle"/><th align="center" valign="middle">Emb-U</th><th align="center" valign="middle">Emb-LU</th><th align="center" valign="middle">Emb-LUA</th><th align="center" valign="middle">Emb-Pre</th><th align="center" valign="middle">Emb-PreUA</th><th align="center" valign="middle">Emb-PreLUA</th></tr></thead><tbody><tr><td align="right" valign="middle">pretrained (Pre)</td><td align="center" valign="middle">-</td><td align="center" valign="middle">-</td><td align="center" valign="middle">-</td><td align="center" valign="middle">X</td><td align="center" valign="middle">X</td><td align="center" valign="middle">X</td></tr><tr><td align="right" valign="middle">Luscinia triplets (L)</td><td align="center" valign="middle">-</td><td align="center" valign="middle">X</td><td align="center" valign="middle">X</td><td align="center" valign="middle">-</td><td align="center" valign="middle">-</td><td align="center" valign="middle">X</td></tr><tr><td align="right" valign="middle">unambiguous bird decisions (U)</td><td align="center" valign="middle">X</td><td align="center" valign="middle">X</td><td align="center" valign="middle">X</td><td align="center" valign="middle">-</td><td align="center" valign="middle">X</td><td align="center" valign="middle">X</td></tr><tr><td align="right" valign="middle">ambiguous bird decisions (A)</td><td align="center" valign="middle">-</td><td align="center" valign="middle">-</td><td align="center" valign="middle">X</td><td align="center" valign="middle">-</td><td align="center" valign="middle">X</td><td align="center" valign="middle">X</td></tr></tbody></table><table-wrap-foot><fn id="TFN1"><p id="P62">In the table, <italic>pretrained (Pre)</italic> refers to models that used Luscinia-U triplets to pre-train with and initialise the weights of the deep learning model; <italic>Luscinia triplets (L)</italic> refers to models that used Luscinia-U triplets for training; <italic>unambiguous bird decisions (U)</italic> refers to models that used the triplet decisions created by the bird judgements through our operant experiments; <italic>ambiguous bird decisions (A)</italic> refers to models that used the triplet decisions created by the bird judgements through our operant experiments. Note that some models used more than one source of triplets to train.</p></fn></table-wrap-foot></table-wrap></floats-group></article>