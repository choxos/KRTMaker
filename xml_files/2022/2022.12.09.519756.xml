<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS158401</article-id><article-id pub-id-type="doi">10.1101/2022.12.09.519756</article-id><article-id pub-id-type="archive">PPR582443</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Color and gloss constancy under diverse lighting environments</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Morimoto</surname><given-names>Takuma</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Akbarinia</surname><given-names>Arash</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Storrs</surname><given-names>Katherine</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Cheeseman</surname><given-names>Jacob R.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Smithson</surname><given-names>Hannah E.</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Gegenfurtner</surname><given-names>Karl R.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Fleming</surname><given-names>Roland W.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib></contrib-group><aff id="A1"><label>1</label>Department of Psychology, Justus-Liebig-Universität Gießen, Germany</aff><aff id="A2"><label>2</label>Department of Experimental Psychology, University of Oxford, Oxford, UK</aff><aff id="A3"><label>3</label>School of Psychology, University of Auckland, New Zealand</aff><author-notes><corresp id="CR1"><label>*</label>Corresponding author: <email>takuma.morimoto@psy.ox.ac.uk</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>11</day><month>12</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>09</day><month>12</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">When we look at an object, we simultaneously see how glossy or matte it is, how light or dark, and what color. Yet, at each point on the object’s surface, both diffuse and specular reflections are mixed in different proportions, resulting in substantial spatial chromatic and luminance variations. To further complicate matters, this pattern changes radically when the object is viewed under different lighting conditions. The purpose of this study was to simultaneously measure our ability to judge color and gloss using an image set capturing diverse object and illuminant properties. Participants adjusted the hue, lightness, chroma, and specular reflectance of a reference object so that it appeared to be made of the same material as a test object. Critically, the two objects were presented under different lighting environments. We found that hue matches were highly accurate, except for under a chromatically atypical illuminant. Chroma and lightness constancy were generally poor, but these failures correlated well with simple image statistics. Gloss constancy was particularly poor, and these failures were only partially explained by reflection contrast. Importantly, across all measures, participants were highly consistent with one another in their deviations from constancy. Although color and gloss constancy hold well in simple conditions, the variety of lighting and shape in the real world presents significant challenges to our visual system’s ability to judge intrinsic material properties.</p></abstract><kwd-group><kwd>color</kwd><kwd>gloss</kwd><kwd>perceptual constancy</kwd><kwd>directional lighting environments</kwd><kwd>asymmetric matching</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><label>1</label><title>Introduction</title><p id="P2">In our everyday lives, we often identify and interact with objects across major changes in lighting—for example, returning at sunset to a car we parked in the morning, or carrying a familiar coffee mug from the kitchen to the balcony. In such circumstances, we are not usually struck by the impression that the color of the car’s paint, or the gloss of the mug’s glaze, have changed. Yet judging the color and gloss of a surface—especially across different lighting conditions—poses hard computational challenges. Firstly, these perceptual quantities result from two physically distinct aspects of the surface’s reflectance, which are confounded in the retinal image. Surface color typically depends primarily on diffuse reflection in which the incident illumination is modified through interaction with pigments, whilst gloss stems from specular reflection, which is a direct reflection of the illuminant. At each point on the object’s surface, diffuse and specular reflections are mixed in different proportions, resulting in substantial spatial chromatic and luminance variations. In the demonstration in <xref ref-type="fig" rid="F1">Figure 1 (a)</xref>, pixel colors dramatically vary among three selected points on the surface of the object, showing that there is no simple mapping between the cone excitations at a single pixel and the color of the object. Furthermore, when we created this image (using computer graphics rendering techniques), we placed an object under a specific lighting environment, applied specific diffuse and specular reflectance properties to the object, and fixed the camera at a specific viewpoint (panel (a) upper-right). Changing any of these underlying scene parameters would alter the color distribution across the same object’s surface in the resultant image. As shown in <xref ref-type="fig" rid="F1">Figure 1 (b)</xref>, pixel colors of the same three surface points change substantially when the same object is viewed in the same pose, but in different lighting environments. The reader may also have the impression that the color and glossiness of the object appear somewhat different, even though the material properties are identical across the three objects.</p><p id="P3">Color constancy has been a core domain of human color vision research (<xref ref-type="bibr" rid="R23">Foster, 2011</xref>; <xref ref-type="bibr" rid="R76">Smithson, 2005</xref>; <xref ref-type="bibr" rid="R33">Hurlbert, 2007</xref>). Many of the early studies used simplistic visual stimuli that were flat, matte, and uniformly illuminated, but more recent studies have begun to use more complex stimuli that better represent natural visual environments (<xref ref-type="bibr" rid="R10">Brainard &amp; Maloney, 2004</xref>; <xref ref-type="bibr" rid="R86">Witzel &amp; Gegenfurtner, 2018</xref>; <xref ref-type="bibr" rid="R47">Mizokami 2019</xref>). One common research question is whether the use of real three-dimensional objects increases the degree of color constancy, compared to flat and uniformly-colored surfaces or images presented on a computer monitor (<xref ref-type="bibr" rid="R28">Hedrich, Bloj &amp; Ruppertsberg 2009</xref>; <xref ref-type="bibr" rid="R3">de Almeida, Fiadeiro &amp; Nascimento, 2010</xref>; <xref ref-type="bibr" rid="R49">Morimoto et al. 2017</xref>). Color and lightness constancy in the presence of specular reflection have also been well-studied, and several studies have found that the presence of specular reflections can improve color constancy (<xref ref-type="bibr" rid="R32">Hurlbert et al. 1991</xref>; <xref ref-type="bibr" rid="R90">Yang &amp; Maloney, 2001</xref>; <xref ref-type="bibr" rid="R91">Yang &amp; Shevell, 2003</xref>; <xref ref-type="bibr" rid="R77">Snyder, Doerschner &amp; Maloney, 2005</xref>; <xref ref-type="bibr" rid="R26">Granzier, Vergne &amp; Gegenfurtner 2014</xref>; <xref ref-type="bibr" rid="R41">Lee &amp; Smithson, 2016</xref>; <xref ref-type="bibr" rid="R53">Nagai, et al. 2017</xref>) while other works found the effect only in limited conditions (<xref ref-type="bibr" rid="R89">Xiao et al. 2012</xref>; <xref ref-type="bibr" rid="R82">Wedge-Roberts et al. 2020</xref>).</p><p id="P4">The effect of object and lighting properties on perceived gloss has also been the subject of several studies, although the term “constancy” is not always explicitly stated. Many factors that can alter perceived gloss have been identified: e.g., object motion (<xref ref-type="bibr" rid="R17">Doerschner et al., 2011</xref>), body color (<xref ref-type="bibr" rid="R84">Wendt et al., 2010</xref>), surface curvature (<xref ref-type="bibr" rid="R30">Ho, Landy, &amp; Maloney, 2008</xref>), and lighting environment statistics (<xref ref-type="bibr" rid="R20">Fleming, Dror &amp; Adelson, 2003</xref>; <xref ref-type="bibr" rid="R58">Obein, Knoblauch, &amp; Viéot, 2004</xref>; <xref ref-type="bibr" rid="R62">Pont &amp; te Pas, 2006</xref>; <xref ref-type="bibr" rid="R1">Adams et al., 2018</xref>). Furthermore, systematic perturbation of lighting and material properties have been used to quantify their influences on perceived gloss level (<xref ref-type="bibr" rid="R92">Zhang, Ridder, &amp; Pont, 2018</xref>; <xref ref-type="bibr" rid="R93">Zhang et al. 2019</xref>; <xref ref-type="bibr" rid="R94">Zhang et al., 2020</xref>). <xref ref-type="bibr" rid="R52">Motoyoshi and Matoba (2012)</xref> used scenes containing a statue along with other objects. They showed failures of gloss constancy and that surrounding context information had virtually no effect on matching results, implying that participants do not use contextual information to discount the influence of illumination.</p><p id="P5">In addition to these constancy perspectives, specific visual computations that may underlie gloss perception have been proposed in recent decades (<xref ref-type="bibr" rid="R40">Landy, 2007</xref>; <xref ref-type="bibr" rid="R12">Chadwick &amp; Kentridge, 2015</xref>). One primary discussion point is whether the visual system makes use of summary statistics extracted from a given image, rather than reconstructing the entire optical input (<xref ref-type="bibr" rid="R21">Fleming, 2014</xref>; <xref ref-type="bibr" rid="R22">Fleming, 2017</xref>; <xref ref-type="bibr" rid="R57">Nishida, 2019</xref>). Candidate cues to gloss include skewness of the luminance histogram (<xref ref-type="bibr" rid="R51">Motoyoshi et al., 2007</xref>; <xref ref-type="bibr" rid="R73">Sharan, et al. 2008</xref>; <xref ref-type="bibr" rid="R4">Anderson &amp; Kim, 2009</xref>; <xref ref-type="bibr" rid="R38">Kim &amp; Anderson, 2010</xref>), its standard deviation (<xref ref-type="bibr" rid="R85">Wiebel, Toscani &amp; Gegenfurtner, 2015</xref>), the magnitude of luminance gradients in an image (<xref ref-type="bibr" rid="R69">Sawayama &amp; Nishida, 2018</xref>), and more complex image metrics computed from specular reflection patterns (<xref ref-type="bibr" rid="R44">Marlow, Kim &amp; Anderson 2012</xref>; <xref ref-type="bibr" rid="R45">Marlow &amp; Anderson, 2013</xref>). Perceived gloss is non-linearly related to underlying physical quantities such as the proportion of light reflected in a specular manner, and the discriminability of gloss as a function of specular reflectance has been recently investigated (<xref ref-type="bibr" rid="R13">Cheeseman et al. 2021</xref>).</p><p id="P6">Despite these extensive investigations into color constancy and gloss constancy as separate domains, very few studies have simultaneously measured color <italic>and</italic> gloss constancy (lightness and gloss, <xref ref-type="bibr" rid="R59">Olkkonen &amp; Brainard, 2010</xref>, <xref ref-type="bibr" rid="R27">Hansmann-Roth &amp; Mamassian, 2017</xref>; gloss and hue variation between green and blue, <xref ref-type="bibr" rid="R65">Radonjić et al. 2018</xref>, <xref ref-type="bibr" rid="R11">Brainard, Cottaris &amp; Radonjić 2018</xref>, <xref ref-type="bibr" rid="R66">Radonjić et al. 2019</xref>). No study has measured all dimensions of perceived color (hue, chroma and lightness) and gloss at the same time. Yet, in daily life, when we look at an object, both percepts naturally occur together, and we do not judge each separately unless explicitly asked to do so. The purpose of this study was to directly measure our ability to judge color and gloss using a synthetic image set produced using physically-based ray-tracing techniques from computer graphics that captures large variations of object properties and illuminant properties. For this, we used a well-established asymmetric matching task in which two images were presented side-by-side on a computer screen. Participants were asked to adjust the color and gloss of a right reference object so that it appears to be made of the same material as a left test object. The critical manipulation is that the two objects were presented under different lighting environments, and thus participants needed to take this difference into account to achieve accurate matching of physical reflectance parameters. This methodology has been extensively used in perceptual constancy studies and shown to be an effective constancy task (<xref ref-type="bibr" rid="R5">Arend &amp; Reeves 1986</xref>; <xref ref-type="bibr" rid="R8">Brainard &amp; Wandell, 1992</xref>). For instance, <xref ref-type="bibr" rid="R59">Olkkonen &amp; Brainard (2010)</xref> jointly measured perceived lightness and gloss of smooth spheres placed under real-world lighting environments. They showed that participants accurately judged lightness, but perceived gloss varied substantially across lighting environments. There are a few other studies using the asymmetric matching task which did not consider the influence of illuminant changes, but which are nonetheless relevant to the present study. <xref ref-type="bibr" rid="R88">Xiao and Brainard (2008)</xref> showed that humans do not simply use the mean color across the whole object to determine the overall color impression of a three-dimensional glossy object. Strong interactions between specular reflection, chroma and lightness were found (<xref ref-type="bibr" rid="R31">Honson et al. 2020</xref>; <xref ref-type="bibr" rid="R34">Isherwood et al., 2021</xref>). Color matching using different types of materials (papers, sponges, wool, candles and porcelain) revealed that hue perception is highly stable, while chroma and lightness were influenced by material types (<xref ref-type="bibr" rid="R25">Giesel &amp; Gegenfurtner 2010</xref>).</p><p id="P7">This study built upon these past efforts in two ways: (i) participants were asked to judge three color dimensions (hue, lightness and chroma) and gloss at the same time, (ii) we used a substantially wider variety of lighting environments and object shapes to capture the complex behavior of constancy mechanisms (including successes and errors) in a large stimulus space. Two psychophysical experiments were performed. The first experiment measured perceived color and gloss from single objects, under different lighting environments, where each object was randomly assigned a unique color and gloss level. The second experiment followed up the failure of gloss constancy observed in the first experiment by systematically exploring the effects of object shape and lighting environment on perceived gloss.</p></sec><sec id="S2"><label>2</label><title>General Method</title><sec id="S3"><label>2.1</label><title>Apparatus</title><p id="P8">Both experiments were computer-controlled and all images were displayed on a 24-inch LCD monitor (ColorEdge CG2420, 1920×1200 pixels, frame rate 60 Hz; EIZO, Ishikawa, Japan) in 10 bits per color channel (red, green and blue). We performed gamma correction and spectral calibration using a spectroradiometer (CS-2000; KONICA MINOLTA, Inc., Tokyo, Japan). Experimental code was written in MATLAB using custom-built functions as well as functions provided in PsychToolbox-3 (<xref ref-type="bibr" rid="R9">Brainard, 1997</xref>).</p></sec><sec id="S4" sec-type="subjects"><label>2.2</label><title>Participants</title><p id="P9">Ten naive participants were recruited for <xref ref-type="sec" rid="S1">Experiments 1</xref> and <xref ref-type="sec" rid="S2">2</xref>. The ratio of female to male was 2.33 for <xref ref-type="sec" rid="S1">Experiment 1</xref> and 1.50 for <xref ref-type="sec" rid="S2">Experiment 2</xref>. The age of participants ranged from 21 to 32; their mean ± 1.0 S.D. was 25.6 ± 2.83 for <xref ref-type="sec" rid="S1">Experiment 1</xref> and 24.0 ± 3.33 for <xref ref-type="sec" rid="S2">Experiment 2</xref>. Four participants completed both experiments. The experiments were approved by a local ethics committee at Justus Liebig University Giessen in accordance with the Helsinki Declaration (6th revision, 2008). Before the experiments, all participants were screened for normal color vision using Ishihara pseudoisochromatic testing plates (<xref ref-type="bibr" rid="R35">Ishihara, 1973</xref>). All participants were undergraduate students at Justus Liebig University Giessen, Germany and paid for their time.</p></sec><sec id="S5"><label>2.3</label><title>Task</title><p id="P10">We used an asymmetric matching task. As shown in <xref ref-type="fig" rid="F2">Figure 2</xref>, a test image and a reference image were presented side by side, separated by 7.1° of visual angle. Participants were asked to adjust the color and gloss of the right reference object until it appeared to be made of the same material as the left test object. The adjustment of color and gloss were done in the “object” space rather than the “image” space. In other words, for the color setting, participants changed an underlying diffuse reflectance of the reference object by adjusting hue, lightness (<italic>L</italic>*) and chroma (<italic>C</italic>*<sub>ab</sub>) (as defined in the <italic>L</italic>*<italic>a*b*</italic> color space, for the reflectance under equal energy white) which updated the weights of reflectance basis functions (described below) to produce a composite reflectance with the desired color properties. For the gloss setting, participants changed an underlying specular reflectance by adjusting a parameter <italic>c</italic>, a perceptually linear gloss scale developed by <xref ref-type="bibr" rid="R61">Pellacini et al. (2000)</xref> (‘<italic>Pellacini’s c</italic>’ hereafter). We used these approximately perceptually linear scales for color and gloss adjustment because we predicted that participants might find it easier if increasing or decreasing a single step would have approximately equal perceptual effect at any point on each parameter range. Participants used eight buttons, each corresponding to increasing or decreasing values of one of the four parameters. A beep was provided when the value reached the limit of the prepared range for lightness, chroma, and <italic>Pellacini’s c</italic>. Hue is a circular variable, and thus there was no range limit. Participants binocularly viewed stimuli presented on a flat screen. During the matching task, participants were allowed to move their eyes freely between the test image and reference image. There was no time limit for each trial. In this study the right reference object served as a scale to quantify participants’ subjective experiences, and thus the shape and the lighting environment in the reference image stayed the same during the whole experiment, and only the left test image changed from one trial to the next. A key feature of this task was that the two objects were placed under different lighting environments. Thus participants needed to discount the effect of lighting on appearance to make accurate color and gloss matches. Pixel colors have a complex relationship to the physical parameters applied to the test object because they are influenced by the lighting environment, as shown in <xref ref-type="fig" rid="F1">Figure 1</xref>. However, a perfectly color- and gloss-constant observer should be able to estimate physical parameters from the test image regardless of the lighting environment and assign the same parameters to the reference image. Details of test images and reference images will be explained in the subsequent section.</p></sec><sec id="S6"><label>2.4</label><title>Experimental stimuli</title><sec id="S7"><label>2.4.1</label><title>Rendering</title><p id="P11">All images used in this study were generated using the physically-based renderer Mitsuba (<xref ref-type="bibr" rid="R36">Jakob, 2010</xref>). A single image contained one floating object shape, to which we applied a diffuse reflectance and a specular reflectance using the Ward reflectance model (<xref ref-type="bibr" rid="R81">Ward, 1992</xref>). For the illumination we used image-based lighting (as shown in <xref ref-type="fig" rid="F1">Figure 1</xref>), which we call ‘environmental illumination’ in this study, where each pixel in the environmental illumination map conveys information about a light ray that reaches a single point in the scene from a particular direction. The map thereby depicts light coming from every possible direction in the environment. Environmental illuminations used in this study were all originally RGB images, and Mitsuba was used to convert these to hyperspectral images during the rendering process (<xref ref-type="bibr" rid="R74">Smith, 2000</xref>). To maximize the accuracy of the rendering process, all images were spectrally rendered from 400 nm to 720 nm in 10 nm steps (31 spectral channels). All images were rendered in 512×512 pixel resolution. Rendered hyperspectral images were converted to the XYZ color space to calculate image statistics, and to the calibrated RGB color space such that displayed images reproduced the XYZ values from the rendered images.</p></sec><sec id="S8"><label>2.4.2</label><title>Reference images</title><p id="P12">Measuring perceived color and gloss using the asymmetric matching task required a reference scene in which participants can adjust the physical reflectance parameters of the reference object in a continuous fashion. To construct the reference scene, we selected (i) environmental illumination, (ii) shape of the reference object, and (iii) diffuse and specular reflectances as follows.</p><p id="P13">In selecting the reference environment, we wanted an environment in which chromatic variation is low and a sufficient amount of light directly hits the objects embedded in this environment producing highly visible specular reflections, which we expected could help participants to easily judge both color and gloss of the reference object. We also wanted to choose an environment in which the reference object appears highly glossy, as otherwise participants may not find a satisfactory match with a test object even at the highest gloss level allowed for the reference object. To establish this, we plotted diagnostic metrics, for publicly available maps, as shown in <xref ref-type="fig" rid="F3">Figure 3</xref>. Panel (a) shows the selected environmental illumination map (‘Uffizi Gallery, Italy’) downloaded from a publicly available database (<xref ref-type="bibr" rid="R14">Debevec, 1998</xref>; <ext-link ext-link-type="uri" xlink:href="https://vgl.ict.usc.edu/Data/HighResProbes/">https://vgl.ict.usc.edu/Data/HighResProbes/</ext-link>, accessed on 15th March 2022) along with statistical characterization of the illumination. The upper-right plot shows a luminance histogram of all pixels together with some basic statistics: mean, standard deviation (s.d.), skewness (skew.), kurtosis and Xia’s diffuseness metric (Xia. et al. 2017) which quantifies how much the lighting environment is directionally uniform from 0 (point light source) to 1 (fully uniform). This lighting environment has low diffuseness because upper and lower hemifields have largely different directional lighting patterns. The lower-left plot shows 10% pixel distribution on <italic>a</italic>*<italic>b</italic>* chromatic plane where equal energy white is set to the origin. The lower-right panel shows a power spectrum, analyzed by spherical harmonic decomposition, with a negative slope of the regression line about -2 that was shown to be typical for outdoor environmental illuminations (Ron et al. 2004).</p><p id="P14">For the reference shape, we chose a simple bumpy sphere whose surface roughness was fixed at 0.05 as defined in Mitsuba. We chose this shape as a standard shape again because some degree of bumpiness increases the spatial contrast of specular reflection between concave and convex regions, helping participants to detect where the specular reflections are, and high spatial-frequency geometries have been associated with better material recognition accuracy (<xref ref-type="bibr" rid="R39">Lagunas et al., 2021</xref>).</p><p id="P15">For a continuous adjustment of color and gloss, we needed to systematically control the underlying physical reflectances, separately for diffuse reflectance and specular reflectance. Our approach was to separately render diffuse and specular images which were linearly summed to produce a colored glossy object, which provided a good enough approximation for the geometry considered here.</p><p id="P16">We first generated a specular image that has only a specular reflection component by setting the specular reflectance to 1.0 and diffuse reflectance to zero across all wavelengths. To modulate the gloss level of test images, this specular image was multiplicatively scaled up and down by a single factor. As already described, participants adjusted the gloss level by changing Pellacini’s c, and this parameter was directly converted to specularity <italic>p</italic><sub><italic>s</italic></sub>, a parameter defined in the Ward reflectance model. The scalar quantity <italic>p</italic><sub><italic>s</italic></sub> was used to scale the specular image at each wavelength. We sampled Pellacini’s c from 0 (matte) to 0.149 (highly glossy), which corresponded to 0 and 0.0999 in specularity <italic>p</italic><sub><italic>s</italic></sub>, respectively. Pellacini’s c captures the lightness-dependent nature of perceived gloss and thus requires a lightness value for the conversion. For this, we used a value of 50, which is roughly equivalent to the mid value of the prepared lightness range (detailed below).</p><p id="P17">Our approach to define diffuse reflectances was to first specify colors in terms of hue, lightness and chroma in <italic>L</italic>*<italic>a</italic>*<italic>b</italic>* color space, and find a spectral reflectance, by combining basis reflectance functions extracted from the Munsell color system, that produces the desired hue, lightness and chroma when placed under an equal energy white light (X = Y = Z = 100).</p><p id="P18">We first applied non-negative matrix factorization to 1,269 matte Munsell color chips (<ext-link ext-link-type="uri" xlink:href="https://sites.uef.fi/spectral/munsell-colors-matt-spectrofotometer-measured/">https://sites.uef.fi/spectral/munsell-colors-matt-spectrofotometer-measured/</ext-link>, accessed on 15th March 2022) to obtain 6 basis spectral reflectance functions as shown in the left panel of <xref ref-type="fig" rid="F3">Figure 3 (b)</xref>. Then, we sampled 21,600 colors in <italic>L</italic>*<italic>a</italic>*<italic>b</italic>* color space (subset is shown in the right plot of <xref ref-type="fig" rid="F3">Figure 3 (b</xref>)): 90 hue values from 0° to 356° in 4° steps; 24 lightness values from 28 to 74 in 2 steps; 10 chroma values from 8 to 26 in 2 steps. Then, for each color, we searched for the optimal weights for 6 basis reflectances so that the weighted summation of the basis functions produced the desired <italic>L</italic>*<italic>a</italic>*b* values under equal energy white light (X = Y = Z = 100). For the optimization we set a condition that the resultant reflectance value should be between zero and one at any wavelength to meet physical constraints. This process generated 21,600 spectral diffuse reflectances, and using these we rendered 21,600 matte reference images. We used 36 of these reflectances to generate test images in <xref ref-type="sec" rid="S1">Experiment 1</xref> as described in the subsequent section. These diffuse images along with the aforementioned single specular image enabled continuous adjustment of the color and gloss of the objects in such a way as to achieve approximately uniform coverage of the perceptual space, at least as expected under reference viewing conditions. <xref ref-type="fig" rid="F3">Figure 3 (c)</xref> shows some example reference images.</p><p id="P19">When we rendered scenes in the Mitsuba renderer, the object was placed at the center of the scene and environmental illumination was applied to the scene. Then, the camera was set at the same height as the object and looked directly at the object. Environmental illumination generally had high dynamic range, and had we selected the viewpoint (i.e. camera position) at random it would have often created an image in which the object region was too dark to see. Thus, we rendered a mirrored sphere from different viewpoints (0° to 330° in 30° steps) and picked the viewpoint at which the mean luminance over the sphere was the highest. We also made sure that there were no pixels with particularly intense lights in the surrounding non-object region of the image. Pixel values in the raw hyperspectral images from Mitsuba were arbitrarily defined because pixel values in the environmental illumination have no units. Thus, after converting the raw hyperspectral image to a linear monitor RGB image, we scaled the whole image by the 99th percentile pixel value across all reference images. We applied the same scaling value to all reference images to equate the light level across images. Because of these selection processes, no tone mapping was applied to any of the reference images. These scaled linear RGB images were gamma-corrected and used for the experiment.</p></sec><sec id="S9"><label>2.4.3</label><title>Test images</title><p id="P20">We first gathered environmental illuminations from multiple online databases (including <xref ref-type="bibr" rid="R2">Adams et al. 2016</xref>; <xref ref-type="bibr" rid="R14">Debevec, 1998</xref>; and <ext-link ext-link-type="uri" xlink:href="https://hdrmaps.com/freebies/">https://hdrmaps.com/freebies/</ext-link>) from which we selected 12 lighting environments that cover a diverse variation of natural lighting. <xref ref-type="fig" rid="F4">Figure 4</xref> shows selected lighting environments along with the luminance histogram of all pixels and 10% pixel chromatic distributions on <italic>a</italic>*<italic>b</italic>* plane for (a) 6 outdoor scenes including sunny days (left 4 scenes) and cloudy days (right 2 scenes) and (b) 6 indoor scenes.</p><p id="P21">To expand the diversity of the selected lighting environments to unnatural domains, we applied two independent manipulations to each lighting environment. Firstly, we rotated the <italic>a</italic>*<italic>b</italic>* chromatic distribution of the original lighting environment by +90° while keeping the <italic>L</italic>* distribution the same, producing ‘gamut-rotated lighting environment’. Chromaticities in natural lighting environments tend to cluster along a blue-yellow axis known as the CIE daylight locus (<xref ref-type="bibr" rid="R37">Judd et al., 1964</xref>; <xref ref-type="bibr" rid="R29">Hernández-Andrés et al. 2001</xref>), and this manipulation made the distribution run along an orthogonal red-green axis. If our visual system uses a prior about a typical illuminant to achieve perceptual constancy (<xref ref-type="bibr" rid="R15">Delahunt &amp; Brainard, 2004</xref>; <xref ref-type="bibr" rid="R60">Pearce et al. 2014</xref>; <xref ref-type="bibr" rid="R83">Weiss, Witzel &amp; Gegenfurtner 2017</xref>), we may observe higher errors in participants’ settings under these chromatically atypical environments. After the rotation, some pixels in the map went outside the chromatic gamut of the experimental monitor and were thus likely to produce out-of-gamut pixels in the rendered images. For those pixels, the chroma value was decreased until the colors came inside the chromatic gamut. When selecting the original lighting environments we made sure that such pixels were always less than 5%. The second manipulation was to distort the directional structure of each lighting environment using a phase scrambling technique via spherical harmonic decomposition. This manipulation normally changes the color distribution of the image, but we kept the distribution of chromaticity and luminance by histogram matching the phase-scrambled environment maps to the originals. Again, if our visual system uses a prior about the directional structure, such as ‘light from above’ (e.g. <xref ref-type="bibr" rid="R67">Ramachandran, 1988</xref>; <xref ref-type="bibr" rid="R48">Morgenstern et al. 2011</xref>), we might observe poor perceptual constancy under these environments because intense lights could hit the object from every direction. The resultant 36 lighting environments (12 natural, 12 gamut-rotated, 12 phase-scrambled) were used in <xref ref-type="sec" rid="S1">Experiment 1</xref>. Twelve natural lighting environments were used in <xref ref-type="sec" rid="S2">Experiment 2</xref>.</p><p id="P22">For <xref ref-type="sec" rid="S1">Experiment 1</xref>, for each of the 36 lighting environments, we placed in the scene a randomly-selected object from a dataset of 3-D meshes of everyday objects (purchased from Evermotion, <ext-link ext-link-type="uri" xlink:href="https://evermotion.org/">https://evermotion.org/</ext-link>), and physical reflectance parameters (hue, chroma, lightness and Pellacini’s c), and selected the viewpoint in the same way we selected the viewpoint for the reference image. The viewpoint was shared between natural environments and gamut-rotated environments, but different camera angles were used for phase-scrambled environments (as the scene geometry was different). No tone-mapping was applied to any of the test images. <xref ref-type="fig" rid="F5">Figure 5</xref> shows 36 example test images together with an example lighting environment and its a*b* chromatic distribution. Mean luminance over the object region was 22.6 ± 3.44 cd/m<sup>2</sup> (mean ± 1.0 S.E. across 36 objects). For each test image, the underlying physical reflectance parameters were defined as ground-truth values in the analysis.</p><p id="P23">For <xref ref-type="sec" rid="S2">Experiment 2</xref>, we generated 216 test images, a factorial combination of 12 natural lighting environments and 18 shapes sampled from shapes used in <xref ref-type="sec" rid="S1">Experiment 1</xref>. These images are shown in a later section for <xref ref-type="sec" rid="S2">Experiment 2</xref>.</p></sec></sec><sec id="S10" sec-type="methods"><label>2.5</label><title>Procedure</title><p id="P24">Prior to the experiment, participants were given instructions both orally and by written text on the screen: <italic>“Your task is to adjust glossiness and color of the right ‘reference’ object in terms of specularity, lightness, chroma, and hue until the reference object appears to be made of the same material as the left ‘target’ object. Each parameter changes the appearance of the reference object as shown here</italic>.<italic>”</italic> Regarding the final sentence, we presented some example reference images similar to <xref ref-type="fig" rid="F3">Figure 3 (c)</xref> to explain how changing each parameter influences the appearance of the reference object. Three practice trials immediately followed the instructions, using example test images that were not used in the actual experiment. During the practice trials, participants were asked to explore the full range of each parameter to familiarize themselves with the range of each parameter. After the practice trials, participants adapted for one minute to 20 Hz random-dot dynamic color noise whose mean chromaticity was equal to the chromaticity of equal energy white, then the experiment began. Initial physical reflectance parameters for the reference object were randomized for each trial. Viewing distance was kept constant at 49 cm from the LCD monitor. Participants signaled when the matching was completed by a button press and the selected underlying physical reflectance parameters of the reference object were recorded as the participant’s response.</p><p id="P25">In <xref ref-type="sec" rid="S1">Experiment 1</xref>, there were 36 test images and 2 control images which contained the same bumpy sphere as the reference objects presented under the reference lighting environment (symmetric matching). The reflectance parameter in control images was fixed at hue 120, chroma 22, lightness 40, and <italic>Pellacini’s</italic> c 0.1305 for the control image 1, and hue 320, chroma 14, lightness 60, <italic>Pellacini’s</italic> c 0.0152 for the control image 2. One session thus consisted of 38 settings, and all participants completed 3 sessions in total. One setting took 66.4 ± 15.5, 53.5 ± 13.0, and 41.1 ± 7.9 seconds (mean ± 1.0 S.D. across 10 participants) for sessions 1, 2 and 3, respectively.</p><p id="P26">In <xref ref-type="sec" rid="S2">Experiment 2</xref>, one session consisted of 216 trials (216 test images), and 2 sessions were completed. One setting lasted 11.5 ± 3.23 and 8.42 ± 2.18 seconds (mean ± 1.0 S.D. across 10 participants) for sessions 1 and 2, respectively.</p><p id="P27">For both experiments, there was a break between sessions.</p></sec></sec><sec id="S11"><label>3</label><title>Experiment 1</title><sec id="S12" sec-type="results"><label>3.1</label><title>Results</title><sec id="S13"><label>3.1.1</label><title>Control condition</title><p id="P28">We first show participants’ settings for the control image 1 (symmetric matching) in <xref ref-type="fig" rid="F6">Figure 6</xref>. It is clear that participants can highly accurately set the reflectance parameters close to ground-truth values when the shape and lighting environment are identical between test and reference images. The accuracy of the setting was similar for the control image 2. This is not surprising but these error values can be usefully taken as a measure of matching precision for each parameter.</p></sec><sec id="S14"><label>3.1.2</label><title>Main conditions</title><p id="P29">
<xref ref-type="fig" rid="F7">Figure 7</xref> shows the main results for <xref ref-type="sec" rid="S1">Experiment 1</xref>. Each data point shows the averaged setting across 10 participants for one test image. From left to right, each subplot shows results for hue, lightness, chroma and Pellacini’s c, respectively. Black numbers at the upper left corner of each subpanel show the correlation coefficient between ground-truth value and participants’ settings, calculated separately for each observer first and averaged across 10 participants.</p><p id="P30">Note that in an asymmetric matching experiment participants’ settings should be interpreted relative to the reference image used in this study. Taking the Pellacini’s c as an example, data points falling exactly on the diagonal unity line shows that participants set the Pellacini’s c of the reference object so that it is the same as the test object. This means that the perceived gloss level of the test object and reference objects are the same when both objects have the same Pellacini’s c. In an alternative case, data points located below the diagonal line mean that those test objects would appear less glossy than the reference object if the test objects’ Pellacini’s c (ground-truth) were applied to the reference object (which is why participants needed to lower the Pellacini’s c of the reference object to achieve matching). For this reason, absolute error values are likely to vary by the choice of reference image, and thus we evaluated the accuracy of the settings using correlation coefficients between human settings and ground-truth values which are not affected by additive shift or multiplicative scaling of the data points which might have been introduced by the choice of the reference image. This relative measure also allows comparison across different parameters.</p><p id="P31">At first glance, data points in <xref ref-type="fig" rid="F7">Figure 7</xref> show large scatter, except for hue settings, and the data seem noisy. However, we emphasize that these settings were highly consistent across participants as illustrated in <xref ref-type="fig" rid="F8">Figure 8</xref>, meaning that all participants systematically showed similar deviations from ground truth. Left bars show the averaged correlation coefficient between human settings and ground-truth (the same as the black numbers in the upper left corners of plots in <xref ref-type="fig" rid="F7">Figure 7</xref>), and small circles show individual participants. Right bars show the averaged correlation among participants (inter-participant correlation). To compute this, for each participant, we calculated the correlation coefficient between the participant’s settings and settings averaged across all participants (<xref ref-type="bibr" rid="R55">Nili et al., 2014</xref>). This reveals that human-human correlations are generally high regardless of human-groundtruth correlation, making it clear that the scattered data patterns in <xref ref-type="fig" rid="F7">Figure 7</xref> are not due to noise.</p><p id="P32">Returning to <xref ref-type="fig" rid="F7">Figure 7</xref>, hue settings in (a) natural environments show a significant correlation with ground-truth values, indicating that hue judgment is stable regardless of test lighting environments. This observation holds well for (c) phase-scrambled environments and less so for (b) gamut-rotated environments. This generally high degree of hue constancy may not be very surprising because the circular correlation between mean hue value over the object region and ground-truth values across 36 test images was 0.92. This means that basing a judgment on the mean hue value over the object region (<xref ref-type="bibr" rid="R46">Milojevic et al. 2018</xref>) in each trial would lead to high correlation with ground-truth, but this might also suggest that hue is physically a relatively stable quantity at least for the environmental illuminations we tested here. To evaluate the influence of the type of lighting environment (natural, gamut-rotated, and phase-scrambled) on the correlation coefficient between human settings and ground-truth, we performed one-way repeated measures ANOVA which confirmed a significant effect of illuminant type (<italic>F</italic>(2,18) = 11.9, <italic>p</italic> = 5.14 × 10<sup>-4</sup>). Post-hoc multiple comparisons (Bonferroni’s corrected <italic>p</italic> = 0.05) showed significantly higher correlation for (a) natural environments than (b) gamut-rotated environments and (c) phase-scrambled than (b) gamut-rotated environments, suggesting worse hue constancy in chromatically atypical lighting environments which implies the role of a daylight prior in judging the illuminant influence (<xref ref-type="bibr" rid="R60">Pearce et al. 2014</xref>; <xref ref-type="bibr" rid="R83">Weiss, Witzel &amp; Gegenfurtner 2017</xref>).</p><p id="P33">In contrast, lightness and chroma settings are scattered in a disorderly way, leading to generally lower correlation with ground-truth than for hue settings. The correlation coefficient was statistically significant only for the chroma setting in (c) phase-scrambled lighting environments. Similarly, settings of Pellacini’s c were not highly correlated with ground-truth though the correlation is significant for (c) phase-scrambled environments. Here most data points fall below the diagonal unit line, meaning that the perceived gloss level of test objects was generally lower than the reference object. This might reflect either the more even sampling of surface normals for the reference geometry, or a tendency of the Uffizi probe to make objects appear particularly glossy.</p></sec></sec><sec id="S15" sec-type="discussion"><label>3.2</label><title>Discussion</title><p id="P34">Subsequent subsections discuss potential underlying reasons for highly consistent error patterns for chroma, lightness and gloss judgments.</p><sec id="S16"><label>3.2.1</label><title>Interaction between diffuse reflectance and specular reflectance</title><p id="P35">Firstly, we considered the possibility that specular reflections may have contaminated the perception of body color. If so, we should observe that deviations between lightness and chroma settings by participants and ground-truth values become larger as a function of physical (or perceived) gloss level. Accordingly, we computed correlation coefficients between the ground-truth Pellacini’s c and lightness error (human setting minus ground-truth) and between the ground-truth Pellacini’s c and chroma error across 12 test images, separately for each type of lighting environment. However, we found no significant correlation under any type of lighting environment. We repeated this analysis using Pellacini’s c set by participants (i.e. perceived gloss) instead of the ground-truth Pellacini’s c, but again there was no significant correlation under any type of lighting environment. Thus, large errors in chroma and lightness observed in <xref ref-type="fig" rid="F7">Figure 7</xref> are unlikely to be due to masking or intrusion by the specular reflection.</p><p id="P36">Similarly, we next asked whether diffuse reflection contaminated the perception of gloss. In other words, were participants more likely to make errors in gloss settings for a certain range of body colors? <xref ref-type="fig" rid="F9">Figure 9</xref> visualizes the magnitude of errors (Pellacini’s c set by participants minus ground-truth Pellacini’s c) as a function of body color. However, we found no significant correlation between errors in gloss settings and each of the color parameters (i.e. hue, chroma and lightness) under any type of lighting environment after the correction of significance level (by Bonferroni’s correction), showing that there are no noteworthy interactions between failures of gloss constancy and body color.</p><p id="P37">Overall, these analyses confirmed that error patterns in color settings were independent from specular reflectance. Similarly errors in gloss judgements were not systematically affected by diffuse reflectance. Evidently judgments of the components of reflectance are based on distinct image information or internal representations, not a single composite representation of the entire bidirectional reflectance distribution function (BRDF; Nicodemus et al. 1977).</p></sec><sec id="S17"><label>3.2.2</label><title>Interaction between lightness and chroma</title><p id="P38">We wondered whether there might be an interaction between parameters related to color, which affected participants’ perceptual judgment and matching. One such candidate was the interaction between chroma and lightness as a past observation suggested that perceived saturation is relatively well predicted by <italic>C</italic><sub><italic>ab</italic></sub>* / <italic>L</italic>* (<xref ref-type="bibr" rid="R18">Fairchild, 2013</xref>; <xref ref-type="bibr" rid="R71">Schiller et al. 2018</xref>). In other words, it is possible that participants judged the chroma and lightness match by simply judging the match in perceived saturation between test and reference objects. If that’s the case, ground-truth values and participants’ settings when represented in <italic>C</italic><sub><italic>ab</italic></sub>* / <italic>L</italic>* should correlate well. As shown in <xref ref-type="fig" rid="F10">Figure 10</xref>, we found significant correlations for all three types of lighting environment. This explains at least partially the scattered setting patterns for chroma and lightness in <xref ref-type="fig" rid="F7">Figure 7</xref>, especially for the (c) phase-scrambled environments, although a substantial portion of the variance remains unaccounted for with the (a) natural and (b) gamut-rotated environments</p></sec><sec id="S18"><label>3.2.3</label><title>Image statistics</title><p id="P39">Another candidate account would be that participants based their judgements on various summary statistics directly accessible from test images. Indeed, since there is no direct way for participants to access the ground-truth values of gloss and body color, such a strategy could be a reasonable alternative approach to performing the task. To predict participants’ lightness and chroma settings, we calculated mean, median, standard deviation, skewness, kurtosis, first quartile (Q1), third quantile (Q3), minimum and maximum values of lightness and chroma across the object region in each test image. Surrounding context was excluded from the computation.</p><p id="P40">Additionally, to understand why gloss constancy was poor in our task, we selectively looked at test images that were rated particularly high (discrepancy from ground-truth, +0.0411, +0.0469) and low gloss (discrepancy, -0.129, -0.136) as well as images where ground-truth and human settings well matched. <xref ref-type="fig" rid="F11">Figure 11 (a)</xref> shows example images (these images are labeled by colored arrows in <xref ref-type="fig" rid="F7">Figure 7</xref>). It is evident that specular reflection patterns are visibly different across images. Objects in high gloss images (surrounded by red squares) seem to receive strong directional lights in the environment and consequently have a readily visible specular reflection pattern though physically the specularity is around the middle of the range (see <xref ref-type="fig" rid="F7">Figure 7</xref>). In contrast, objects in low gloss images (surrounded by blue squares) are placed under a dim and cloudy lighting environment or indoor scene. Specular reflection is present in these cases too, but it spreads across the surface and that’s why when mixed with diffuse images, the specular reflection patterns are hard to detect even though both objects have nearly the highest possible specular reflectance. Finally, for the images where human settings and ground-truth were well correlated, specular reflections are moderately visible. In sum it visually makes sense that these images were rated by participants in these ways, which encouraged us to compute image metrics based on patterns of specular reflection.</p><p id="P41">Thus, in addition to basic descriptive statistics, we used metrics computed from the structure of the specular reflection image (<xref ref-type="bibr" rid="R44">Marlow et al. 2012</xref>; <xref ref-type="bibr" rid="R72">Schmid et al. 2021</xref>) to predict participants’ gloss settings. As shown in <xref ref-type="fig" rid="F11">Figure 11 (b)</xref>, we first converted the original image to a luminance image, masked out the object region, and subtracted the diffuse component from the image, which resulted in test images with specular reflection alone. Then, we extracted pixels whose intensity is higher than <italic>k</italic> % value of the highest intensity across this specular image, where <italic>k</italic> took the following values 0, 1, 3, 5, 10, 20 and 40 to get rid of the region of specular reflection that stems from secondary and higher-order inter-reflections. Using this thresholded ‘highlight image’ we calculated the following three metrics. The first metric is <italic>coverage</italic>, corresponding to the proportion of area covered by the highlight relative to the whole object area as depicted in the upper-left part of panel (c). Second, we calculated the <italic>sharpness</italic>. Using a spatial convolution, this metric emphasizes the region where luminance rapidly changes and sharpness is defined as a mean value of the convoluted sharpness map (<xref ref-type="bibr" rid="R80">Vu et al. 2012</xref>) as shown in upper-right part of panel (c). For coverage and sharpness, model predictions were affected by the cut-off percentage to threshold the highlight regions and thus we selected an optimal value of <italic>k</italic> that produced the highest correlation value with human settings. Finally, the third metric was <italic>contrast</italic>, which essentially measures the spatial luminance variation over the surface. The standard way would be to calculate a contrast from the raw highlight image directly. However, considering a previous observation that perceived gloss is affected by the modulation of a specific frequency channel (<xref ref-type="bibr" rid="R7">Boyadzhiev et al. 2015</xref>), we first decomposed the raw highlight image into 8 sub-band images using a Gaussian band-pass filter (upper and lower cut-off frequencies: 1.5 to 3.0, 3.0 to 6.0, 6.0 to 12.0, 12.0 to 24.0, 24.0 to 48.0, 48.0 to 96.0, 96.0 to 192, and 192 to 384 cycles/image) and a subset of sub-band images are shown in the lower part of panel (c). We calculated the RMS contrast, equivalent to standard deviation of the pixel intensities, for each sub-band image as well as for an aggregated image across all frequencies. This means that unlike coverage and sharpness which has one parameter, the sub-band contrast metric has two free parameters (i.e. cut-off pixel intensity and cut-off spatial frequency band), and optimal values producing the highest correlation with human settings were selected. For all three metrics, searching for the best parameters was performed separately for each type of lighting environment (natural, gamut-rotated, phase-scrambled).</p><p id="P42">Each bar in <xref ref-type="fig" rid="F12">Figure 12</xref> shows the correlation coefficient between image statistics and human settings for (a) natural lighting environment, (b) gamut-rotated lighting environment, and (c) phase-scrambled lighting environment. Higher values indicate that the models capture participants’ settings better. The magenta shaded region shows the inter-participant noise ceiling, between upper bounds (same as magenta values in <xref ref-type="fig" rid="F7">Figure 7</xref>) and lower bounds, computed by a similar procedure as the upper bound, but by calculating correlation coefficient between the left-out participant’s settings and settings averaged across all other participants. This range effectively defines a bound on how well any image-computable model could perform. These image statistics were computed from test objects that include both diffuse and specular reflections. For lightness and chroma, we also considered image statistics computed directly from the diffuse component only (without specular reflection) to test the idea that humans might have effectively discounted the specular reflection from the test image. The red diamonds show the correlation between image statistics calculated directly from the diffuse image and participants’ settings. If participants took such a strategy, the red diamonds should come higher than the green bars.</p><p id="P43">For lightness and chroma, it is clear that although the correlation between human settings and ground truth are remarkably low (as shown in <xref ref-type="fig" rid="F7">Figure 7</xref>), participant’s settings are highly correlated with simple statistics such as mean lightness and mean chroma over the object region, nearly touching the noise ceiling level in some cases. Interestingly, for lightness, maximum luminance of diffuse components (red diamonds) predicts human settings better than maximum luminance of the original image (bars), consistent with a past observation (<xref ref-type="bibr" rid="R25">Giesel &amp; Gegenfurtner, 2010</xref>).</p><p id="P44">Looking at Pellacini’s c, overall image statistics correlate with human settings less than for chroma and lightness, and maximum luminance values are generally good predictors for any type of lighting environment. Yet, there are still significant disparities between the predictors and human performance. Thus these models explain the failure of gloss constancy to some extent, but not enough to capture the complexity of gloss perception.</p><p id="P45">To summarize the results from <xref ref-type="sec" rid="S1">Experiment 1</xref>, both success and failures of color constancy were well captured. Hue constancy holds remarkably well. Failures of chroma and lightness constancy were systematic and largely predicted by simple metrics such as mean chroma or mean lightness over the object surface. Gloss constancy was very poor for our stimuli, but matches were highly consistent across participants, and the simple image statistics we investigated explained human behavior to a limited extent.</p><p id="P46">To grasp the complex nature of gloss perception better, we felt that the 36 test images used in <xref ref-type="sec" rid="S1">Experiment 1</xref> may not be enough. Thus we conducted a follow-up Experiment 2 with 216 test images using a factorial combination between 12 lighting environments and 18 shapes, and the perceived gloss was again measured using the asymmetric matching task.</p></sec></sec></sec><sec id="S19"><label>4</label><title>Experiment 2</title><sec id="S20"><label>4.1</label><title>Test images and procedure</title><p id="P47">We generated 216 images using 18 shapes sampled from those used in <xref ref-type="sec" rid="S1">Experiment 1</xref>, and 12 lighting environments (<xref ref-type="fig" rid="F13">Figure 13</xref>). Hue, lightness and chroma of test objects and matching objects were fixed at 188.2°, 33.9 and 11.9, respectively. We picked this greenish color as we speculated that visibility of specular reflection would be higher when the body color falls on the axis orthogonal to daylight locus (i.e. red-green axis) because as seen in <xref ref-type="fig" rid="F4">Figure 4</xref> colors of specular reflection are mainly distributed along the blue-yellow axis. The task of participants and procedure were identical to <xref ref-type="sec" rid="S1">Experiment 1</xref>, except that participants’ adjusted only Pellacini’s c in <xref ref-type="sec" rid="S2">Experiment 2</xref> and the color of the reference object was fixed to the same green as the test object. For each test image, we applied a random Pellacini’s c ranging from 0 to 0.224, which corresponds to the range from 0 to 0.0999 in Ward’s specularity. The maximum value of Pellacini’s c here differs from the value in <xref ref-type="sec" rid="S1">Experiment 1</xref> (0.149) because the lightness value of the object for the conversion between Pellacini’s c and Ward’s specularity was 33.9 instead of 50. One session consisted of 216 trials, and all participants completed two sessions in total.</p></sec><sec id="S21" sec-type="results | discussion"><label>4.2</label><title>Results and Discussion</title><p id="P48">
<xref ref-type="fig" rid="F14">Figure 14</xref> (a) shows participants’ settings (averaged across 10 participants) in <xref ref-type="sec" rid="S2">Experiment 2</xref> grouped by the test lighting environment. Each data point corresponds to one shape, and thus there are 18 data points in each subplot. Globally looking through subplots, it is evident that data points deviate substantially from the unity line (diagonal dotted line), showing that human settings and ground-truth strongly disagree. It is also noticeable that the slope of the red regression line differs from one lighting environment to another (min 0.34 and max 0.90). Higher slope means that on average under that lighting environment objects appear more glossy. Also, the correlation coefficients between human settings and ground-truth values (black numbers at upper-left) were significant in all lighting environments, unlike <xref ref-type="sec" rid="S1">Experiment 1</xref>, but the values vary from 0.46 to 0.78 showing that the variability due to the objects’ shape also differs from one lighting environment to another. Thus, simple transformations are unlikely to equate perceived gloss level in one lighting environment to another environment. This is inconsistent with past research using smooth spheres (<xref ref-type="bibr" rid="R20">Fleming et al, 2003</xref>; <xref ref-type="bibr" rid="R16">Doerschner et al. 2010</xref>), and the use of a variety of shapes in this study is likely to be a reason. Overall, these results suggest that perceived gloss somewhat correlates with underlying physical specular reflectance, but also differs due to the change of lighting environment, showing failures of gloss constancy.</p><p id="P49">We next asked whether image statistics explain these human behaviors. The lefthand bar graph in <xref ref-type="fig" rid="F14">Figure 14 (b)</xref> shows the correlation coefficient (averaged over 10 participants) between each statistic and human settings over 216 images. Unlike the results of <xref ref-type="sec" rid="S1">Experiment 1</xref> (<xref ref-type="fig" rid="F12">Figure 12</xref>), basic image statistics showed fairly low correlation, while the sub-band contrast metric showed relatively high correlation. However, this correlation was significantly lower than the correlation between human settings and ground-truth shown by the blue line (two-tailed paired t-test; <italic>t</italic>(18)=3.25, <italic>p</italic> = 0.0045). Moreover, it is worth noting that this contrast metric (and others) also correlates with ground-truth value to some extent, and this may be a part of the reason why these metrics correlated with human settings. Thus, to remove the influence of ground-truth, we additionally computed a partial correlation coefficient between the image statistics and human settings (gray filled circles), removing the correlation between image statistics and ground-truth values. We found that they are lower than the original correlation coefficient especially for image metrics computed from specular reflections (left three gray points). This result suggests that these models correlate well with human settings mainly simply because they correlate with ground-truth, not necessarily capturing systematic failures of gloss constancy consistently exhibited by participants. For other simple image statistics, partial correlation and original correlation coefficients are similar to each other, likely because those simple metrics do not correlate with ground-truth values well. Finally, we tested what happens if we assume observers combined multiple statistics instead of using a single cue independently. To test this idea we ran multiple regression analysis using all 12 metrics as independent variables to explain the participants’ settings. When we did this, we trained a regressor using the averaged settings across 9 participants to find optimized weights for 12 metrics, and then using these optimized weights we calculated the correlation between the regressor’s prediction and the settings by the left-out participant. We repeated this 10 times (leave-one-out cross-validation), and the average across the 10 correlation coefficients is shown by the horizontal dashed red line. The right-hand bar plot shows standardized weights for each statistic, averaged across 10 validations. However, the improvement due to integration of multiple cues was marginal, and consequently there is still much room between this level and the noise ceiling level.</p></sec></sec><sec id="S22"><label>5</label><title>General Discussion</title><p id="P50">How do we overcome huge variations in the proximal image to create a stable percept of the color and gloss of objects? To address this question we measured color and gloss constancy together using an asymmetric matching task under a diverse set of lighting environments. Our results revealed a strong asymmetry across hue, chroma and lightness constancy; the degree of hue constancy was generally high, although it slightly decreased when the lighting environment had atypical chromatic properties, while lightness and chroma constancy were in general severely limited (except for chroma settings under phase-scrambled environments). These failures of chroma and lightness constancy were well captured by the saturation metric (chroma/lightness) and simple image statistics over the object region in the image. In contrast, gloss constancy was generally poor (i.e., gloss ratings depended on lighting environment and shape), but phase-scrambling directional lighting geometry did not additionally impair gloss constancy. Image statistics explained those failures of gloss constancy only to a limited extent. One major finding in this study is that, although there have been observations that simplistic image metrics can account for large variations in human gloss perception, when the diversity of shape and lighting approaches that seen in real world environments a significant amount of consistent variance in gloss judgments remains unexplained.</p><p id="P51">This study presented constancy errors that were remarkably consistent across tested participants, but there have been reports that the degree of color constancy could vary substantially from one experimental condition and paradigm to another (<xref ref-type="bibr" rid="R23">Foster 2011</xref>). Our findings are in fact largely in line with findings from several past studies. <xref ref-type="bibr" rid="R62">Pont &amp; te Pas (2006)</xref> reported that participants showed significant failures of constancy in a task where two presented spheres illuminated differently have either the same reflectance properties or not. <xref ref-type="bibr" rid="R56">Nishida &amp; Shinya (1998)</xref> used a reflectance matching paradigm using both albedo and specular reflectance and showed that participants’ lightness and glossiness judgment were heavily influenced by object shape. <xref ref-type="bibr" rid="R59">Olkkonen and Brainard (2010)</xref> showed systematic failures of gloss constancy due to changes of lighting environment. However, it is worth noting that all these experiments reporting constancy failures—including our own—were conducted using computer monitors, and whether this finding applies to real-world scenarios needs careful investigation. For example, one challenge associated with performing the asymmetric matching paradigm on monitors is limited adaptation to test lighting environments, which is a key contributor in human color constancy (<xref ref-type="bibr" rid="R75">Smithson, 2004</xref>). Although the participants were allowed to move their eyes freely during a trial, it is reasonable to assume that the participant looked at the reference image for most of the time during the trial to complete the task. Moreover, images presented on the monitor only occupied a small part of the visual field, and the surrounding region in test images that provides cues to the lighting environments was even smaller (on average, 26.3 % of the whole image for <xref ref-type="sec" rid="S1">Experiment 1</xref> and 18.3 % for <xref ref-type="sec" rid="S2">Experiment 2</xref>), which would have made it harder for observers to infer the illuminant influence. However, it is worth noting that the conditions in our experiment were sufficient to enable relatively good hue matches. Finally, we presented only one object in each test image, and presenting multiple objects with various colors in the same scene may have increased the degree of color constancy.</p><p id="P52">We observed that changing the lighting environment had different effects on different color dimensions. One reason for the superior hue stability could be that the lighting environments we selected did not produce extreme chromatic shifts in the proximal image and consequently pixel hue values did not change severely enough to cause poor constancy. In fact, as shown in <xref ref-type="fig" rid="F4">Figure 4</xref>, although we tried to select environmental illuminations whose color distributions are different from each other, the mean color (shown by a black cross symbol) is still located relatively close to the white point. In contrast, chroma and lightness values shifted enough that mean chroma and lightness values were decorrelated from ground-truth values. However, it is generally true that in natural environments extreme chromatic shifts are rare (<xref ref-type="bibr" rid="R50">Morimoto et al. 2022</xref>). It is thus interesting to ask how much our selection of lighting environments reflects the true variation of actual lighting environments. If the physical hue values of an object do not vary substantially across scenes in the real world, hue serves as a useful perceptual anchor for object identification under different environments (<xref ref-type="bibr" rid="R46">Milojevic et al. 2018</xref>; <xref ref-type="bibr" rid="R19">Ennis 2018</xref>).</p><p id="P53">We showed that a contrast metric computed directly from the specular images showed highest correlation with human settings in <xref ref-type="sec" rid="S2">Experiment 2</xref>, but this model implicitly assumes that humans are capable of separating diffuse and specular reflections from a given image. Such a separation is an ill-posed problem and it is an empirical question how accurately humans can perform the decomposition (e.g. Lee &amp; Smithson 2018). A complete model of human gloss perception should predict the perceived gloss level from a raw image where diffuse and specular reflections are confounded. A recent effort used deep neural networks trained to output a specular image from a raw image and showed that such a network outperformed a simple alternative highlight-detection model based on thresholding and showed relatively high overall similarity to human judgements (<xref ref-type="bibr" rid="R64">Prokott et al. 2022</xref>). Another trained unsupervised deep neural networks to model the high-level statistics of images of glossy and matte surfaces, and found that these predicted human gloss judgements better than supervised networks or a range of simpler image statistics (<xref ref-type="bibr" rid="R78">Storrs et al., 2021</xref>).</p><p id="P54">A good perceptual model should reproduce both the successes and error patterns that humans make on an image-by-image basis beyond predicting the overall performance level (<xref ref-type="bibr" rid="R24">Geirhos et al. 2020</xref>; <xref ref-type="bibr" rid="R78">Storrs et al., 2021</xref>). In this sense, systematic error patterns in <xref ref-type="sec" rid="S2">Experiment 2</xref> are a potentially useful feature of the dataset as a window into underlying constancy mechanisms. However, we found that our hand-selected features accounted for a limited extent for gloss percepts, and it is a common shortcoming that researchers must select in advance or hand-engineer candidate features. In recent years ‘big data’ approaches (often coupled with deep neural networks) have been opening a new avenue to overcome such limitations as networks can learn to extract useful image features by themselves (<xref ref-type="bibr" rid="R63">Prokott et al. 2021</xref>; Tamura et al. 2021; <xref ref-type="bibr" rid="R43">Liao, 2022</xref>; <xref ref-type="bibr" rid="R70">Sawayama, 2022</xref>), and this study might also benefit from such approaches. The fact that human judgments can deviate substantially but consistently from ground truth—as we found here—suggests that training a neural network with human responses would potentially yield quite different internal representations than training with ground truth specular reflectance values.</p></sec></body><back><ack id="S23"><title>Acknowledgement</title><p>Authors thank Alexandra C. Schmid for providing source codes to calculate coverage, sharpness, and contrast metrics from a specular image and Wiebke Siedentop and Annika Zentel for assisting with data collection. TM is supported by a Sir Henry Wellcome Postdoctoral Fellowship from Wellcome Trust (218657/Z/19/Z) and a Junior Research Fellowship from Pembroke College, University of Oxford. This research was also supported by “The Adaptive Mind”, funded by the Excellence Program of the Hessian Ministry of Higher Education, Science, Research, by the DFG SFB-TRR-135 “Cardinal Mechanisms of Perception” (project No. 222641018, TPs C1 and C2), and by a Marsden Fast Start Grant to KS from the Royal Society of New Zealand (project MFP-UOA2109). For the purpose of open access, the author has applied a CC BY public copyright license to any Author Accepted Manuscript version arising from this submission.</p></ack><fn-group><fn id="FN1"><p id="P55">Data access</p><p id="P56">The raw experimental data and source codes to analyze the data and reproduce figures will be available in a data repository at the time of publication.</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adams</surname><given-names>WJ</given-names></name><name><surname>Kucukoglu</surname><given-names>G</given-names></name><name><surname>Landy</surname><given-names>MS</given-names></name><name><surname>Mantiuk</surname><given-names>RK</given-names></name></person-group><article-title>Naturally glossy: gloss perception, illumination statistics, and tone mapping</article-title><source>J Vis</source><year>2018</year><volume>18</volume><fpage>4</fpage><pub-id pub-id-type="pmcid">PMC6279370</pub-id><pub-id pub-id-type="pmid">30508429</pub-id><pub-id pub-id-type="doi">10.1167/18.13.4</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adams</surname><given-names>WJ</given-names></name><name><surname>Elder</surname><given-names>JH</given-names></name><name><surname>Graf</surname><given-names>EW</given-names></name><name><surname>Leyland</surname><given-names>J</given-names></name><name><surname>Lugtigheid</surname><given-names>AJ</given-names></name><name><surname>Muryy</surname><given-names>A</given-names></name></person-group><article-title>The Southampton-York Natural Scenes (SYNS) dataset: Statistics of surface attitude</article-title><source>Scientific Reports</source><year>2016</year><volume>6</volume><elocation-id>35805</elocation-id><pub-id pub-id-type="pmcid">PMC5080654</pub-id><pub-id pub-id-type="pmid">27782103</pub-id><pub-id pub-id-type="doi">10.1038/srep35805</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Almeida</surname><given-names>VM</given-names></name><name><surname>Fiadeiro</surname><given-names>PT</given-names></name><name><surname>Nascimento</surname><given-names>SM</given-names></name></person-group><article-title>Effect of scene dimensionality on colour constancy with real three-dimensional scenes and objects</article-title><source>Perception</source><year>2010</year><volume>39</volume><issue>6</issue><fpage>770</fpage><lpage>779</lpage><pub-id pub-id-type="pmid">20698472</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>BL</given-names></name><name><surname>Kim</surname><given-names>J</given-names></name></person-group><article-title>Image statistics do not explain the perception of gloss and lightness</article-title><source>Journal of Vision</source><year>2009</year><volume>9</volume><issue>11</issue><fpage>1</fpage><lpage>17</lpage><comment>10</comment><pub-id pub-id-type="pmid">20053073</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arend</surname><given-names>L</given-names></name><name><surname>Reeves</surname><given-names>A</given-names></name></person-group><article-title>Simultaneous color constancy</article-title><source>Journal of the Optical Society of America A</source><year>1986</year><volume>3</volume><issue>10</issue><fpage>1743</fpage><lpage>1751</lpage><pub-id pub-id-type="pmid">3772637</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borgonovo</surname><given-names>E</given-names></name><name><surname>Plischke</surname><given-names>E</given-names></name></person-group><article-title>Sensitivity analysis: A review of recent advances</article-title><source>European Journal of Operational Research</source><year>2016</year><volume>248</volume><issue>3</issue><fpage>869</fpage><lpage>887</lpage></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boyadzhiev</surname><given-names>I</given-names></name><name><surname>Bala</surname><given-names>K</given-names></name><name><surname>Paris</surname><given-names>S</given-names></name><name><surname>Adelson</surname><given-names>E</given-names></name></person-group><article-title>Band-sifting decomposition for image-based material editing</article-title><source>ACM Transactions on Graphics</source><year>2015</year><volume>34</volume><issue>5</issue><fpage>1</fpage><lpage>16</lpage></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><article-title>Asymmetric color matching: How color appearance depends on the illuminant</article-title><source>Journal of the Optical Society of America A</source><year>1992</year><volume>9</volume><fpage>1433</fpage><lpage>1448</lpage><pub-id pub-id-type="pmid">1527647</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><article-title>The psychophysics toolbox</article-title><source>Spatial vision</source><year>1997</year><volume>10</volume><issue>4</issue><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name><name><surname>Maloney</surname><given-names>LT</given-names></name></person-group><article-title>Perception of color and material properties in complex scenes</article-title><source>Journal of Vision</source><year>2004</year><volume>4</volume><issue>9</issue><pub-id pub-id-type="pmid">15493961</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name><name><surname>Cottaris</surname><given-names>NP</given-names></name><name><surname>Radonjić</surname><given-names>A</given-names></name></person-group><article-title>The perception of color and material in natural tasks</article-title><source>Royal Society Interface Focus</source><year>2018</year><volume>8</volume><issue>4</issue><pub-id pub-id-type="pmcid">PMC6015813</pub-id><pub-id pub-id-type="pmid">29951192</pub-id><pub-id pub-id-type="doi">10.1098/rsfs.2018.0012</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chadwick</surname><given-names>AC</given-names></name><name><surname>Kentridge</surname><given-names>RW</given-names></name></person-group><article-title>The perception of gloss: a review</article-title><source>Vision research</source><year>2015</year><volume>109</volume><fpage>221</fpage><lpage>235</lpage><pub-id pub-id-type="pmid">25448119</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheeseman</surname><given-names>JR</given-names></name><name><surname>Ferwerda</surname><given-names>JA</given-names></name><name><surname>Maile</surname><given-names>FJ</given-names></name><name><surname>Fleming</surname><given-names>RWFleming</given-names></name></person-group><article-title>Scaling and discriminability of perceived gloss</article-title><source>J Opt Soc Am A</source><year>2021</year><volume>38</volume><fpage>203</fpage><lpage>210</lpage><pub-id pub-id-type="pmid">33690530</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Debevec</surname><given-names>P</given-names></name></person-group><source>Rendering synthetic objects into real scenes: Bridging traditional and image-based graphics with global illumination and high dynamic range photography</source><conf-name>SIGGRAPH98 Proceedings of the 25th Annual Conference on Computer Graphics and Interactive Techniques</conf-name><conf-sponsor>Association for Computing Machinery</conf-sponsor><conf-loc>New York</conf-loc><year>1998</year><fpage>189</fpage><lpage>198</lpage></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Delahunt</surname><given-names>PB</given-names></name><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><article-title>Does human color constancy incorporate the statistical regularity of natural daylight?</article-title><source>Journal of Vision</source><year>2004</year><volume>4</volume><issue>2</issue><fpage>57</fpage><lpage>81</lpage><comment>1</comment><pub-id pub-id-type="pmid">15005648</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doerschner</surname><given-names>K</given-names></name><name><surname>Boyaci</surname><given-names>H</given-names></name><name><surname>Maloney</surname><given-names>LT</given-names></name></person-group><article-title>Estimating the glossiness transfer function induced by illumination change and testing its transitivity</article-title><source>Journal of vision</source><year>2010</year><volume>10</volume><issue>4</issue><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="pmcid">PMC4454136</pub-id><pub-id pub-id-type="pmid">20465328</pub-id><pub-id pub-id-type="doi">10.1167/10.4.8</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doerschner</surname><given-names>K</given-names></name><name><surname>Fleming</surname><given-names>RW</given-names></name><name><surname>Yilmaz</surname><given-names>O</given-names></name><name><surname>Schrater</surname><given-names>PR</given-names></name><name><surname>Hartung</surname><given-names>B</given-names></name><name><surname>Kersten</surname><given-names>D</given-names></name></person-group><article-title>Visual motion and the perception of surface material</article-title><source>Current Biology</source><year>2011</year><volume>21</volume><issue>23</issue><fpage>2010</fpage><lpage>2016</lpage><pub-id pub-id-type="pmcid">PMC3246380</pub-id><pub-id pub-id-type="pmid">22119529</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2011.10.036</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fairchild</surname><given-names>MD</given-names></name></person-group><source>Color Appearance Models</source><publisher-name>John Wiley &amp; Sons, Ltd</publisher-name><year>2013</year></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ennis</surname><given-names>R</given-names></name><name><surname>Schiller</surname><given-names>F</given-names></name><name><surname>Toscani</surname><given-names>M</given-names></name><name><surname>Gegenfurtner</surname><given-names>KR</given-names></name></person-group><article-title>Hyperspectral database of fruits and vegetables</article-title><source>J Opt Soc Am A</source><year>2018</year><volume>35</volume><fpage>B256</fpage><lpage>B266</lpage><pub-id pub-id-type="pmid">29603941</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fleming</surname><given-names>RW</given-names></name><name><surname>Dror</surname><given-names>RO</given-names></name><name><surname>Adelson</surname><given-names>EH</given-names></name></person-group><article-title>Real-world illumination and the perception of surface reflectance properties</article-title><source>Journal of Vision</source><year>2003</year><volume>3</volume><issue>5</issue><fpage>347</fpage><lpage>368</lpage><comment>3</comment><pub-id pub-id-type="pmid">12875632</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fleming</surname><given-names>RW</given-names></name></person-group><article-title>Visual perception of materials and their properties</article-title><source>Vision Research</source><year>2014</year><volume>94</volume><fpage>62</fpage><lpage>75</lpage><pub-id pub-id-type="pmid">24291494</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fleming</surname><given-names>RW</given-names></name></person-group><article-title>Material perception</article-title><source>Annual Review of Vision Science</source><year>2017</year><volume>3</volume><fpage>365</fpage><lpage>388</lpage><pub-id pub-id-type="pmid">28697677</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foster</surname><given-names>DH</given-names></name></person-group><article-title>Color constancy</article-title><source>Vision Res</source><year>2011</year><volume>51</volume><fpage>674</fpage><lpage>700</lpage><pub-id pub-id-type="pmid">20849875</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geirhos</surname><given-names>R</given-names></name><name><surname>Meding</surname><given-names>K</given-names></name><name><surname>Wichmann</surname><given-names>FA</given-names></name></person-group><article-title>Beyond accuracy: quantifying trial-by-trial behaviour of CNNs and humans by measuring error consistency</article-title><source>arXiv</source><year>2020</year></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giesel</surname><given-names>M</given-names></name><name><surname>Gegenfurtner</surname><given-names>KR</given-names></name></person-group><article-title>Color appearance of real objects varying in material, hue, and shape</article-title><source>Journal of Vision</source><year>2010</year><volume>10</volume><issue>9</issue><fpage>10</fpage><pub-id pub-id-type="pmid">20884608</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Granzier</surname><given-names>J</given-names></name><name><surname>Vergne</surname><given-names>R</given-names></name><name><surname>Gegenfurtner</surname><given-names>KR</given-names></name></person-group><article-title>The effects of surface gloss and roughness on color constancy for real 3-D objects</article-title><source>Journal of Vision</source><year>2014</year><volume>14</volume><issue>2</issue><fpage>16</fpage><pub-id pub-id-type="pmid">24563527</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hansmann-Roth</surname><given-names>S</given-names></name><name><surname>Mamassian</surname><given-names>P</given-names></name></person-group><article-title>A Glossy Simultaneous Contrast: Conjoint Measurements of Gloss and Lightness</article-title><source>I-Perception</source><year>2017</year><volume>8</volume><issue>1</issue><pub-id pub-id-type="pmcid">PMC5298473</pub-id><pub-id pub-id-type="pmid">28203352</pub-id><pub-id pub-id-type="doi">10.1177/2041669516687770</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hedrich</surname><given-names>M</given-names></name><name><surname>Bloj</surname><given-names>M</given-names></name><name><surname>Ruppertsberg</surname><given-names>AI</given-names></name></person-group><article-title>Color constancy improves for real 3D objects</article-title><source>Journal of vision</source><year>2009</year><volume>9</volume><issue>4</issue><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="pmid">19757925</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hernández-Andrés</surname><given-names>J</given-names></name><name><surname>Romero</surname><given-names>J</given-names></name><name><surname>Nieves</surname><given-names>JL</given-names></name><name><surname>Lee</surname><given-names>RL</given-names></name></person-group><article-title>Color and spectral analysis of daylight in southern Europe</article-title><source>J Opt Soc Am</source><year>2001</year><volume>18</volume><issue>6</issue><fpage>1325</fpage><lpage>1335</lpage><pub-id pub-id-type="pmid">11393625</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ho</surname><given-names>Y-X</given-names></name><name><surname>Landy</surname><given-names>MS</given-names></name><name><surname>Maloney</surname><given-names>LT</given-names></name></person-group><article-title>Conjoint Measurement of Gloss and Surface Texture</article-title><source>Psychological Science</source><year>2008</year><volume>19</volume><issue>2</issue><fpage>196</fpage><lpage>204</lpage><pub-id pub-id-type="pmcid">PMC2679902</pub-id><pub-id pub-id-type="pmid">18271869</pub-id><pub-id pub-id-type="doi">10.1111/j.1467-9280.2008.02067.x</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Honson</surname><given-names>V</given-names></name><name><surname>Huynh-Thu</surname><given-names>Q</given-names></name><name><surname>Arnison</surname><given-names>M</given-names></name><name><surname>Monaghan</surname><given-names>D</given-names></name><name><surname>Isherwood</surname><given-names>ZJ</given-names></name><name><surname>Kim</surname><given-names>J</given-names></name></person-group><article-title>Effects of Shape, Roughness and Gloss on the Perceived Reflectance of Colored Surfaces</article-title><source>Front Psychol</source><year>2020</year><volume>11</volume><fpage>485</fpage><pub-id pub-id-type="pmcid">PMC7101081</pub-id><pub-id pub-id-type="pmid">32265792</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2020.00485</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hurlbert</surname><given-names>A</given-names></name><name><surname>Cumming</surname><given-names>BG</given-names></name><name><surname>Parker</surname><given-names>AJ</given-names></name></person-group><article-title>Recognition and perceptual use of specular reflections</article-title><source>Investigative Ophthalmology &amp; Visual Science</source><year>1991</year><volume>32</volume><fpage>1278</fpage></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hurlbert</surname><given-names>A</given-names></name></person-group><article-title>Colour constancy</article-title><source>Curr Biol</source><year>2007</year><volume>17</volume><fpage>R906</fpage><lpage>R907</lpage><pub-id pub-id-type="pmid">17983562</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isherwood</surname><given-names>ZJ</given-names></name><name><surname>Huynh-Thu</surname><given-names>Q</given-names></name><name><surname>Arnison</surname><given-names>M</given-names></name><name><surname>Monaghan</surname><given-names>D</given-names></name><name><surname>Toscani</surname><given-names>M</given-names></name><name><surname>Perry</surname><given-names>S</given-names></name><name><surname>Honson</surname><given-names>V</given-names></name><name><surname>Kim</surname><given-names>J</given-names></name></person-group><article-title>Surface properties and the perception of color</article-title><source>Journal of Vision</source><year>2021</year><volume>21</volume><issue>2</issue><fpage>1</fpage><lpage>22</lpage><comment>7</comment><pub-id pub-id-type="pmcid">PMC7888285</pub-id><pub-id pub-id-type="pmid">33576764</pub-id><pub-id pub-id-type="doi">10.1167/jov.21.2.7</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ishihara</surname><given-names>S</given-names></name></person-group><source>The series of plates designed as a test for colour-blindness</source><publisher-name>Kanehara Shuppan Co. Ltd</publisher-name><publisher-loc>Tokyo Japan</publisher-loc><year>1973</year></element-citation></ref><ref id="R36"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Jakob</surname><given-names>W</given-names></name></person-group><source>Mitsuba: Physically Based Renderer</source><year>2010</year><comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="https://www.mitsuba-renderer.org/download.html">https://www.mitsuba-renderer.org/download.html</ext-link></comment></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Judd</surname><given-names>DB</given-names></name><name><surname>MacAdam</surname><given-names>DL</given-names></name><name><surname>Wyszecki</surname><given-names>G</given-names></name><name><surname>Budde</surname><given-names>HW</given-names></name><name><surname>Condit</surname><given-names>HR</given-names></name><name><surname>Henderson</surname><given-names>ST</given-names></name><name><surname>Simonds</surname><given-names>JL</given-names></name></person-group><article-title>Spectral distribution of typical daylight as a function of correlated color temperature</article-title><source>J Opt Soc Am</source><year>1964</year><volume>54</volume><issue>8</issue><fpage>1031</fpage><lpage>1040</lpage></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>J</given-names></name><name><surname>Anderson</surname><given-names>BL</given-names></name></person-group><article-title>Image statistics and the perception of surface gloss and lightness</article-title><source>Journal of Vision</source><year>2010</year><volume>10</volume><issue>9</issue><fpage>1</fpage><lpage>17</lpage><comment>3</comment><pub-id pub-id-type="pmid">20884601</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lagunas</surname><given-names>M</given-names></name><name><surname>Serrano</surname><given-names>A</given-names></name><name><surname>Gutierrez</surname><given-names>D</given-names></name><name><surname>Masia</surname><given-names>B</given-names></name></person-group><article-title>The joint role of geometry and illumination on material recognition</article-title><source>Journal of Vision</source><year>2021</year><volume>21</volume><issue>2</issue><fpage>1</fpage><lpage>18</lpage><comment>2</comment><pub-id pub-id-type="pmcid">PMC7862729</pub-id><pub-id pub-id-type="pmid">33533879</pub-id><pub-id pub-id-type="doi">10.1167/jov.21.2.2</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Landy</surname><given-names>MS</given-names></name></person-group><article-title>Visual perception—A gloss on surface properties</article-title><source>Nature</source><year>2007</year><volume>447</volume><issue>7141</issue><fpage>158</fpage><lpage>159</lpage><pub-id pub-id-type="pmcid">PMC2745613</pub-id><pub-id pub-id-type="pmid">17443194</pub-id><pub-id pub-id-type="doi">10.1038/nature05714</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>RJ</given-names></name><name><surname>Smithson</surname><given-names>HE</given-names></name></person-group><article-title>Low levels of specularity support operational color constancy, particularly when surface and illumination geometry can be inferred</article-title><source>J Opt Soc Am A</source><year>2016</year><volume>33</volume><fpage>A306</fpage><lpage>A318</lpage><pub-id pub-id-type="pmcid">PMC4805180</pub-id><pub-id pub-id-type="pmid">26974938</pub-id><pub-id pub-id-type="doi">10.1364/JOSAA.33.00A306</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>RJ</given-names></name><name><surname>Smithson</surname><given-names>HE</given-names></name></person-group><article-title>Motion of glossy objects does not promote separation of lighting and surface colour</article-title><source>R Soc open sci</source><year>2017</year><elocation-id>4171290171290</elocation-id><pub-id pub-id-type="pmcid">PMC5717688</pub-id><pub-id pub-id-type="pmid">29291113</pub-id><pub-id pub-id-type="doi">10.1098/rsos.171290</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liao</surname><given-names>C</given-names></name><name><surname>Sawayama</surname><given-names>M</given-names></name><name><surname>Xiao</surname><given-names>B</given-names></name></person-group><article-title>Translucency perception emerges in deep generative representations for natural image synthesis</article-title><source>bioRxiv</source><year>2022</year><elocation-id>2022.08.12.503662</elocation-id><pub-id pub-id-type="doi">10.1101/2022.08.12.503662</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marlow</surname><given-names>PJ</given-names></name><name><surname>Kim</surname><given-names>J</given-names></name><name><surname>Anderson</surname><given-names>BL</given-names></name></person-group><article-title>The perception and misperception of specular surface reflectance</article-title><source>Current Biology</source><year>2012</year><volume>22</volume><issue>20</issue><fpage>1909</fpage><lpage>1913</lpage><pub-id pub-id-type="pmid">22959347</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marlow</surname><given-names>PJ</given-names></name><name><surname>Anderson</surname><given-names>BL</given-names></name></person-group><article-title>Generative constraints on image cues for perceived gloss</article-title><source>J Vis</source><year>2013</year><volume>13</volume><fpage>2</fpage><pub-id pub-id-type="pmid">24297776</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Milojevic</surname><given-names>Z</given-names></name><name><surname>Ennis</surname><given-names>R</given-names></name><name><surname>Toscani</surname><given-names>M</given-names></name><name><surname>Gegenfurtner</surname><given-names>KR</given-names></name></person-group><article-title>Categorizing natural color distributions</article-title><source>Vision Research</source><year>2018</year><volume>151</volume><fpage>18</fpage><lpage>30</lpage><pub-id pub-id-type="pmid">29555302</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mizokami</surname><given-names>Y</given-names></name></person-group><article-title>Three-dimensional stimuli and environment for studies of color constancy</article-title><source>Current Opinion in Behavioral Sciences</source><year>2019</year><volume>30</volume><fpage>217</fpage><lpage>222</lpage></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morgenstern</surname><given-names>Y</given-names></name><name><surname>Murray</surname><given-names>RF</given-names></name><name><surname>Harris</surname><given-names>LR</given-names></name></person-group><article-title>The human visual system’s assumption that light comes from above is weak</article-title><source>Proceedings of the National Academy of Sciences of the USA</source><year>2011</year><volume>108</volume><issue>30</issue><fpage>12551</fpage><lpage>12553</lpage><pub-id pub-id-type="pmcid">PMC3145687</pub-id><pub-id pub-id-type="pmid">21746935</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1100794108</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morimoto</surname><given-names>T</given-names></name><name><surname>Mizokami</surname><given-names>Y</given-names></name><name><surname>Yaguchi</surname><given-names>H</given-names></name><name><surname>Buck</surname><given-names>SL</given-names></name></person-group><article-title>Color Constancy in Two-Dimensional and Three-Dimensional Scenes: Effects of Viewing Methods and Surface Texture</article-title><source>I-Perception</source><year>2017</year><pub-id pub-id-type="pmcid">PMC5721973</pub-id><pub-id pub-id-type="pmid">29238513</pub-id><pub-id pub-id-type="doi">10.1177/2041669517743522</pub-id></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morimoto</surname><given-names>T</given-names></name><name><surname>Zhang</surname><given-names>C</given-names></name><name><surname>Fukuda</surname><given-names>K</given-names></name><name><surname>Uchikawa</surname><given-names>K</given-names></name></person-group><article-title>Spectral measurement of daylights and surface properties of natural objects in Japan</article-title><source>Opt Express</source><year>2022</year><volume>30</volume><fpage>3183</fpage><lpage>3204</lpage><pub-id pub-id-type="pmcid">PMC7612288</pub-id><pub-id pub-id-type="pmid">35106053</pub-id><pub-id pub-id-type="doi">10.1364/OE.441063</pub-id></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Motoyoshi</surname><given-names>I</given-names></name><name><surname>Nishida</surname><given-names>S</given-names></name><name><surname>Sharan</surname><given-names>L</given-names></name><name><surname>Adelson</surname><given-names>EH</given-names></name></person-group><article-title>Image statistics and the perception of surface qualities</article-title><source>Nature</source><year>2007</year><volume>447</volume><fpage>206</fpage><lpage>209</lpage><pub-id pub-id-type="pmid">17443193</pub-id></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Motoyoshi</surname><given-names>I</given-names></name><name><surname>Matoba</surname><given-names>H</given-names></name></person-group><article-title>Variability in constancy of the perceived surface reflectance across different illumination statistics</article-title><source>Vision Research</source><year>2012</year><volume>53</volume><issue>1</issue><fpage>30</fpage><lpage>39</lpage><pub-id pub-id-type="pmid">22138530</pub-id></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nagai</surname><given-names>T</given-names></name><name><surname>Kaneko</surname><given-names>S</given-names></name><name><surname>Kawashima</surname><given-names>Y</given-names></name><name><surname>Yamauchi</surname><given-names>Y</given-names></name></person-group><article-title>Do specular highlights and the daylight locus act as cues for estimating illumination color from a single object?</article-title><source>Opt Rev</source><year>2017</year><volume>24</volume><fpage>47</fpage><lpage>61</lpage></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nicodemus</surname><given-names>FE</given-names></name><name><surname>Richmond</surname><given-names>JC</given-names></name><name><surname>Hsia</surname><given-names>JJ</given-names></name><name><surname>Ginsburg</surname><given-names>IW</given-names></name><name><surname>Limperis</surname><given-names>T</given-names></name></person-group><article-title>Geometrical considerations and nomenclature for reflectance</article-title><source>National Bureau of Standards, NBS monograph</source><year>1997</year><volume>160</volume></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nili</surname><given-names>H</given-names></name><name><surname>Wingfield</surname><given-names>C</given-names></name><name><surname>Walther</surname><given-names>A</given-names></name><name><surname>Su</surname><given-names>L</given-names></name><name><surname>Marslen-Wilson</surname><given-names>W</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>A Toolbox for Representational Similarity Analysis</article-title><source>PLoS Comput Biol</source><year>2014</year><volume>10</volume><issue>4</issue><elocation-id>e1003553</elocation-id><pub-id pub-id-type="pmcid">PMC3990488</pub-id><pub-id pub-id-type="pmid">24743308</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003553</pub-id></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nishida</surname><given-names>S</given-names></name><name><surname>Shinya</surname><given-names>M</given-names></name></person-group><article-title>Use of image-based information in judgments of surface-reflectance properties</article-title><source>Journal of the Optical Society of America A</source><year>1998</year><volume>15</volume><issue>12</issue><fpage>2951</fpage><lpage>2965</lpage><pub-id pub-id-type="pmid">9857525</pub-id></element-citation></ref><ref id="R57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nishida</surname><given-names>S</given-names></name></person-group><article-title>Image statistics for material perception</article-title><source>Current Opinion in Behavioral Sciences</source><year>2019</year><volume>30</volume><fpage>94</fpage><lpage>99</lpage></element-citation></ref><ref id="R58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knoblauch</surname><given-names>ObeinG</given-names></name><name><surname>KViéot</surname><given-names>F</given-names></name></person-group><article-title>Difference scaling of gloss: Nonlinearity, binocularity, and constancy</article-title><source>Journal of Vision</source><year>2004</year><volume>4</volume><issue>9</issue><fpage>4</fpage><pub-id pub-id-type="pmid">15493965</pub-id></element-citation></ref><ref id="R59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olkkonen</surname><given-names>M</given-names></name><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><article-title>Perceived glossiness and lightness under real-world illumination</article-title><source>Journal of Vision</source><year>2010</year><volume>10</volume><issue>9</issue><fpage>1</fpage><lpage>19</lpage><comment>5, <ext-link ext-link-type="uri" xlink:href="http://www.journalofvision.org/content/10/9/5">http://www.journalofvision.org/content/10/9/5</ext-link></comment><pub-id pub-id-type="pmcid">PMC2981171</pub-id><pub-id pub-id-type="pmid">20884603</pub-id><pub-id pub-id-type="doi">10.1167/10.9.5</pub-id></element-citation></ref><ref id="R60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearce</surname><given-names>B</given-names></name><name><surname>Crichton</surname><given-names>S</given-names></name><name><surname>Mackiewicz</surname><given-names>M</given-names></name><name><surname>Finlayson</surname><given-names>GD</given-names></name><name><surname>Hurlbert</surname><given-names>A</given-names></name></person-group><article-title>Chromatic illumination discrimination ability reveals that human colour constancy is optimised for blue daylight illuminations</article-title><source>PloS one</source><year>2014</year><volume>9</volume><issue>2</issue><elocation-id>e87989</elocation-id><pub-id pub-id-type="pmcid">PMC3929610</pub-id><pub-id pub-id-type="pmid">24586299</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0087989</pub-id></element-citation></ref><ref id="R61"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Pellacini</surname><given-names>F</given-names></name><name><surname>Ferwerda</surname><given-names>JA</given-names></name><name><surname>Greenberg</surname><given-names>DP</given-names></name></person-group><source>Toward a psychophysically-based light reflection model for image synthesis</source><conf-name>Proceedings of the 27th annual conference on Computer graphics and interactive techniques (SIGGRAPH ‘00)</conf-name><conf-sponsor>ACM Press/Addison-Wesley Publishing Co</conf-sponsor><conf-loc>USA</conf-loc><year>2000</year><fpage>55</fpage><lpage>64</lpage><comment>ACM Press/Addison-Wesley Publishing Co</comment><pub-id pub-id-type="doi">10.1145/344779.344812</pub-id></element-citation></ref><ref id="R62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pont</surname><given-names>SC</given-names></name><name><surname>te Pas</surname><given-names>SF</given-names></name></person-group><article-title>Material-illumination ambiguities and the perception of solid objects</article-title><source>Perception</source><year>2006</year><volume>35</volume><fpage>1331</fpage><lpage>1350</lpage><pub-id pub-id-type="pmid">17214380</pub-id></element-citation></ref><ref id="R63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prokott</surname><given-names>KE</given-names></name><name><surname>Tamura</surname><given-names>H</given-names></name><name><surname>Fleming</surname><given-names>RW</given-names></name></person-group><article-title>Gloss perception: Searching for a deep neural network that behaves like humans</article-title><source>Journal of Vision</source><year>2021</year><volume>21</volume><issue>12</issue><fpage>1</fpage><lpage>20</lpage><comment>14</comment><pub-id pub-id-type="pmcid">PMC8626854</pub-id><pub-id pub-id-type="pmid">34817568</pub-id><pub-id pub-id-type="doi">10.1167/jov.21.12.14</pub-id></element-citation></ref><ref id="R64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prokott</surname><given-names>E</given-names></name><name><surname>Fleming</surname><given-names>RW</given-names></name></person-group><article-title>Identifying specular highlights: Insights from deep learning</article-title><source>Journal of Vision</source><year>2022</year><volume>22</volume><issue>7</issue><fpage>1</fpage><lpage>19</lpage><comment>6</comment><pub-id pub-id-type="pmcid">PMC9206496</pub-id><pub-id pub-id-type="pmid">35713928</pub-id><pub-id pub-id-type="doi">10.1167/jov.22.7.6</pub-id></element-citation></ref><ref id="R65"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Radonjić</surname><given-names>A</given-names></name><name><surname>Cottaris</surname><given-names>NP</given-names></name><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><source>Quantifying how humans trade off color and material in object identification</source><conf-name>Proceedings of Electronic Imaging 2018, Burlingame</conf-name><conf-loc>CA</conf-loc><year>2018</year></element-citation></ref><ref id="R66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Radonjić</surname><given-names>A</given-names></name><name><surname>Cottaris</surname><given-names>NP</given-names></name><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><article-title>The relative contribution of color and material in object selection</article-title><source>PLoS Computational Biology</source><year>2019</year><volume>15</volume><issue>4</issue><elocation-id>e1006950</elocation-id><pub-id pub-id-type="pmcid">PMC6490924</pub-id><pub-id pub-id-type="pmid">30978187</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006950</pub-id></element-citation></ref><ref id="R67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramachandran</surname><given-names>VS</given-names></name></person-group><article-title>Perception of shape from shading</article-title><source>Nature</source><year>1988</year><volume>331</volume><issue>6152</issue><fpage>163</fpage><lpage>166</lpage><pub-id pub-id-type="pmid">3340162</pub-id></element-citation></ref><ref id="R68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dror</surname><given-names>Ron O</given-names></name><name><surname>Willsky</surname><given-names>Alan S</given-names></name><name><surname>Adelson</surname><given-names>Edward H</given-names></name></person-group><article-title>Statistical characterization of real-world illumination</article-title><source>Journal of Vision</source><year>2004</year><volume>4</volume><issue>9</issue><fpage>11</fpage><pub-id pub-id-type="pmid">15493972</pub-id></element-citation></ref><ref id="R69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sawayama</surname><given-names>M</given-names></name><name><surname>Nishida</surname><given-names>SY</given-names></name></person-group><article-title>Material and shape perception based on two types of intensity gradient information</article-title><source>PLoS Computational Biology</source><year>2018</year><volume>14</volume><issue>4</issue><elocation-id>e1006061</elocation-id><pub-id pub-id-type="pmcid">PMC5963816</pub-id><pub-id pub-id-type="pmid">29702644</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006061</pub-id></element-citation></ref><ref id="R70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sawayama</surname><given-names>M</given-names></name><name><surname>Dobashi</surname><given-names>Y</given-names></name><name><surname>Okabe</surname><given-names>M</given-names></name><name><surname>Hosokawasa</surname><given-names>K</given-names></name><name><surname>Koumura</surname><given-names>T</given-names></name><name><surname>Saarela</surname><given-names>T</given-names></name><name><surname>Olkkonen</surname><given-names>M</given-names></name><name><surname>Nishida</surname><given-names>S</given-names></name></person-group><article-title>Visual discrimination of optical material properties: a large-scale study</article-title><source>Journal of Vision</source><year>2022</year><volume>22</volume><issue>2</issue><fpage>1</fpage><lpage>24</lpage><comment>17</comment><pub-id pub-id-type="pmcid">PMC8883156</pub-id><pub-id pub-id-type="pmid">35195670</pub-id><pub-id pub-id-type="doi">10.1167/jov.22.2.17</pub-id></element-citation></ref><ref id="R71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schiller</surname><given-names>F</given-names></name><name><surname>Valsecchi</surname><given-names>M</given-names></name><name><surname>Gegenfurtner</surname><given-names>KR</given-names></name></person-group><article-title>An evaluation of different measures of color saturation</article-title><source>Vision Research</source><year>2018</year><volume>151</volume><fpage>117</fpage><lpage>134</lpage><pub-id pub-id-type="pmid">28551362</pub-id></element-citation></ref><ref id="R72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmid</surname><given-names>AC</given-names></name><name><surname>Barla</surname><given-names>P</given-names></name><name><surname>Doerschner</surname><given-names>K</given-names></name></person-group><article-title>Material category of visual objects computed from specular image structure</article-title><source>bioRxiv</source><year>2021</year><elocation-id>2019.12.31.892083</elocation-id><pub-id pub-id-type="doi">10.1101/2019.12.31.892083</pub-id></element-citation></ref><ref id="R73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharan</surname><given-names>L</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Motoyoshi</surname><given-names>I</given-names></name><name><surname>Nishida</surname><given-names>S</given-names></name><name><surname>Adelson</surname><given-names>EH</given-names></name></person-group><article-title>Image statistics for surface reflectance perception</article-title><source>Journal of the Optical Society of America A</source><year>2008</year><volume>25</volume><fpage>846</fpage><lpage>865</lpage><pub-id pub-id-type="pmid">18382484</pub-id></element-citation></ref><ref id="R74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>B</given-names></name></person-group><article-title>An RGB to spectrum conversion for reflectances</article-title><source>J Graph Tools</source><year>2000</year><volume>4</volume><fpage>11</fpage><lpage>22</lpage></element-citation></ref><ref id="R75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smithson</surname><given-names>H</given-names></name><name><surname>Zaidi</surname><given-names>Q</given-names></name></person-group><article-title>Colour constancy in context: roles for local adaptation and levels of reference</article-title><source>J Vis</source><year>2004</year><volume>4</volume><fpage>693</fpage><lpage>710</lpage><pub-id pub-id-type="pmid">15493964</pub-id></element-citation></ref><ref id="R76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smithson</surname><given-names>HE</given-names></name></person-group><article-title>Sensory, computational, and cognitive components of human colour constancy</article-title><source>Phil Trans R Soc B</source><year>2005</year><volume>360</volume><fpage>1329</fpage><lpage>1346</lpage><pub-id pub-id-type="pmcid">PMC1609194</pub-id><pub-id pub-id-type="pmid">16147525</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2005.1633</pub-id></element-citation></ref><ref id="R77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snyder</surname><given-names>JL</given-names></name><name><surname>Doerschner</surname><given-names>K</given-names></name><name><surname>Maloney</surname><given-names>LT</given-names></name></person-group><article-title>Illumination estimation in three-dimensional scenes with and without specular cues</article-title><source>Journal of Vision</source><year>2005</year><volume>5</volume><issue>10</issue><fpage>863</fpage><lpage>877</lpage><comment>8</comment><pub-id pub-id-type="pmid">16441190</pub-id></element-citation></ref><ref id="R78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Storrs</surname><given-names>KR</given-names></name><name><surname>Anderson</surname><given-names>BL</given-names></name><name><surname>Fleming</surname><given-names>RW</given-names></name></person-group><article-title>Unsupervised learning predicts human perception and misperception of gloss</article-title><source>Nat Hum Behav</source><year>2021</year><volume>5</volume><fpage>1402</fpage><lpage>1417</lpage><pub-id pub-id-type="pmcid">PMC8526360</pub-id><pub-id pub-id-type="pmid">33958744</pub-id><pub-id pub-id-type="doi">10.1038/s41562-021-01097-6</pub-id></element-citation></ref><ref id="R79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tamura</surname><given-names>H</given-names></name><name><surname>Prokott</surname><given-names>KE</given-names></name><name><surname>Fleming</surname><given-names>RW</given-names></name></person-group><article-title>Distinguishing mirror from glass: A “big data” approach to material perception</article-title><source>Journal of Vision</source><year>2022</year><volume>22</volume><issue>4</issue><fpage>1</fpage><lpage>22</lpage><comment>4</comment><pub-id pub-id-type="pmcid">PMC8934559</pub-id><pub-id pub-id-type="pmid">35266961</pub-id><pub-id pub-id-type="doi">10.1167/jov.22.4.4</pub-id></element-citation></ref><ref id="R80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vu</surname><given-names>CT</given-names></name><name><surname>Phan</surname><given-names>TD</given-names></name><name><surname>Chandler</surname><given-names>DM</given-names></name></person-group><article-title>S3: A Spectral and Spatial Measure of Local Perceived Sharpness in Natural Images</article-title><source>IEEE Transactions on Image Processing</source><year>2012</year><volume>21</volume><issue>3</issue><fpage>934</fpage><lpage>945</lpage><pub-id pub-id-type="pmid">21965207</pub-id></element-citation></ref><ref id="R81"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ward</surname><given-names>GJ</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Glassner</surname><given-names>A</given-names></name></person-group><source>Measuring and modeling anisotropic reflection</source><conf-name>SIGGRAPH 92: Proceedings of the 19th Annual Conference on Computer Graphics and Interactive Techniques</conf-name><conf-sponsor>ACM, ACM Press</conf-sponsor><conf-loc>New York</conf-loc><year>1992</year><fpage>459</fpage><lpage>472</lpage></element-citation></ref><ref id="R82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wedge-Roberts</surname><given-names>R</given-names></name><name><surname>Aston</surname><given-names>S</given-names></name><name><surname>Beierholm</surname><given-names>U</given-names></name><name><surname>Kentridge</surname><given-names>R</given-names></name><name><surname>Hurlbert</surname><given-names>A</given-names></name><name><surname>Nardini</surname><given-names>M</given-names></name><name><surname>Olkkonen</surname><given-names>M</given-names></name></person-group><article-title>Specular highlights improve color constancy when other cues are weakened</article-title><source>Journal of Vision</source><year>2020</year><volume>20</volume><issue>12</issue><fpage>1</fpage><lpage>22</lpage><comment>4</comment><pub-id pub-id-type="pmcid">PMC7674000</pub-id><pub-id pub-id-type="pmid">33170203</pub-id><pub-id pub-id-type="doi">10.1167/jov.20.12.4</pub-id></element-citation></ref><ref id="R83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weiss</surname><given-names>D</given-names></name><name><surname>Witzel</surname><given-names>C</given-names></name><name><surname>Gegenfurtner</surname><given-names>KR</given-names></name></person-group><article-title>Determinants of colour constancy and the blue bias</article-title><source>i-Perception</source><year>2017</year><volume>8</volume><issue>6</issue><elocation-id>2041669517739635</elocation-id><pub-id pub-id-type="pmcid">PMC5768282</pub-id><pub-id pub-id-type="pmid">29348910</pub-id><pub-id pub-id-type="doi">10.1177/2041669517739635</pub-id></element-citation></ref><ref id="R84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wendt</surname><given-names>G</given-names></name><name><surname>Faul</surname><given-names>F</given-names></name><name><surname>Ekroll</surname><given-names>V</given-names></name><name><surname>Mausfeld</surname><given-names>R</given-names></name></person-group><article-title>Disparity, motion, and color information improve gloss constancy performance</article-title><source>Journal of Vision</source><year>2010</year><volume>10</volume><issue>9</issue><fpage>7</fpage><pub-id pub-id-type="pmid">20884605</pub-id></element-citation></ref><ref id="R85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiebel</surname><given-names>C</given-names></name><name><surname>Toscani</surname><given-names>M</given-names></name><name><surname>Gegenfurtner</surname><given-names>KR</given-names></name></person-group><article-title>Statistical correlates of perceived gloss in natural images</article-title><source>Vision Research</source><year>2015</year><volume>115B</volume><fpage>175</fpage><lpage>187</lpage><pub-id pub-id-type="pmid">25937518</pub-id></element-citation></ref><ref id="R86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Witzel</surname><given-names>C</given-names></name><name><surname>Gegenfurtner</surname><given-names>KR</given-names></name></person-group><article-title>Color perception: objects, constancy, and categories</article-title><source>Annual Review of Vision Science</source><year>2018</year><volume>4</volume><fpage>475</fpage><lpage>499</lpage><pub-id pub-id-type="pmid">30004833</pub-id></element-citation></ref><ref id="R87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xia</surname><given-names>L</given-names></name><name><surname>Pont</surname><given-names>SC</given-names></name><name><surname>Heynderickx</surname><given-names>I</given-names></name></person-group><article-title>Light diffuseness metric part 1: Theory</article-title><source>Lighting Research &amp; Technology</source><year>2017</year><volume>49</volume><issue>4</issue><fpage>411</fpage><lpage>427</lpage></element-citation></ref><ref id="R88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiao</surname><given-names>B</given-names></name><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><article-title>Surface gloss and color perception of 3D objects</article-title><source>Visual Neuroscience</source><year>2008</year><volume>25</volume><fpage>371</fpage><lpage>385</lpage><pub-id pub-id-type="pmcid">PMC2538579</pub-id><pub-id pub-id-type="pmid">18598406</pub-id><pub-id pub-id-type="doi">10.1017/S0952523808080267</pub-id></element-citation></ref><ref id="R89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiao</surname><given-names>B</given-names></name><name><surname>Hurst</surname><given-names>B</given-names></name><name><surname>MacIntyre</surname><given-names>L</given-names></name><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><article-title>The color constancy of three-dimensional objects</article-title><source>Journal of Vision</source><year>2012</year><volume>12</volume><issue>4</issue><fpage>6</fpage><pub-id pub-id-type="pmcid">PMC3366466</pub-id><pub-id pub-id-type="pmid">22508953</pub-id><pub-id pub-id-type="doi">10.1167/12.4.6</pub-id></element-citation></ref><ref id="R90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>JN</given-names></name><name><surname>Maloney</surname><given-names>LT</given-names></name></person-group><article-title>Illuminant cues in surface color perception: Tests of three candidate cues</article-title><source>Vision Research</source><year>2001</year><volume>41</volume><fpage>2581</fpage><lpage>2600</lpage><pub-id pub-id-type="pmid">11520505</pub-id></element-citation></ref><ref id="R91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>JN</given-names></name><name><surname>Shevell</surname><given-names>SK</given-names></name></person-group><article-title>Surface color perception under two illuminants: The second illuminant reduces color constancy</article-title><source>Journal of Vision</source><year>2003</year><volume>3</volume><issue>5</issue><fpage>369</fpage><lpage>379</lpage><comment>4</comment><pub-id pub-id-type="pmid">12875633</pub-id></element-citation></ref><ref id="R92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>F</given-names></name><name><surname>de Ridder</surname><given-names>H</given-names></name><name><surname>Pont</surname><given-names>SC</given-names></name></person-group><article-title>Asymmetric perceptual confounds between canonical lightings and materials</article-title><source>Journal of Vision</source><year>2018</year><volume>18</volume><issue>11</issue><fpage>1</fpage><lpage>19</lpage><comment>11</comment><pub-id pub-id-type="pmid">30347097</pub-id></element-citation></ref><ref id="R93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>F</given-names></name><name><surname>de Ridder</surname><given-names>H</given-names></name><name><surname>Barla</surname><given-names>P</given-names></name><name><surname>Pont</surname><given-names>S</given-names></name></person-group><article-title>A systematic approach to testing and predicting light-material interactions</article-title><source>Journal of Vision</source><year>2019</year><volume>19</volume><issue>4</issue><fpage>1</fpage><lpage>22</lpage><comment>11</comment><pub-id pub-id-type="pmid">30952162</pub-id></element-citation></ref><ref id="R94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>F</given-names></name><name><surname>de Ridder</surname><given-names>H</given-names></name><name><surname>Barla</surname><given-names>P</given-names></name><name><surname>Pont</surname><given-names>S</given-names></name></person-group><article-title>Effects of light map orientation and shape on the visual perception of canonical materials</article-title><source>Journal of Vision</source><year>2020</year><volume>20</volume><issue>4</issue><fpage>1</fpage><lpage>18</lpage><comment>13</comment><pub-id pub-id-type="pmcid">PMC7405718</pub-id><pub-id pub-id-type="pmid">32324842</pub-id><pub-id pub-id-type="doi">10.1167/jov.20.4.13</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Graphical illustration of challenges to judging the color and gloss of a three-dimensional object placed under a complex lighting environment.</title><p>(a) As shown to the upper-right, we placed an object under an environmental illumination, applied a diffuse reflectance which has a fixed hue, chroma and lightness (denoted as <italic>h, C</italic><sub><italic>ab</italic></sub>* and <italic>L</italic>*, respectively) and a specular reflectance, set a view-point and rendered the object image using computer graphics techniques. The rendered image is shown on the left side. Hue, chroma and lightness vary largely across the three selected pixels at different regions on the object’s surface. (b) Effects of lighting environments (overcast environment on the left and indoor environment on the right) on the color variation at the same three locations. Note that the objects in panels (a) and (b) have identical material properties though their appearance may differ across lighting environments.</p></caption><graphic xlink:href="EMS158401-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Stimulus configuration for asymmetric matching in this study.</title><p>The task of participants was to adjust an underlying diffuse reflectance to change the color and an underlying specular reflectance to change the gloss of the reference object presented in the right reference image until it appears to be made of the same material as the test object presented in the left test image. Test images changed from one trial to another, but the reference image stayed the same throughout the experiment. White text and arrows were not presented during the actual experiment.</p></caption><graphic xlink:href="EMS158401-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><p>(a) Reference environmental illumination used for the reference image and its statistical characterization. The upper right plot shows the luminance histogram of all pixels, along with some image statistics (mean, standard deviation, skewness, kurtosis and Xia’s diffuseness metric). The lower left plot shows the <italic>a</italic>*<italic>b</italic>* chromatic distribution of 10% pixel colors that were randomly sampled. The lower right plot shows the power spectrum analyzed by spherical harmonic decomposition. (b) Six basis reflectance functions extracted from 1,269 matte Munsell color chips using non-negative matrix factorization. Subset of surface colors assigned to the object in the reference images at example lightness planes (<italic>L</italic>* = 30, 50 and 70). There were in total 21,600 colors, allowing participants to explore the stimulus space using method of adjustment. (c) Some example reference images, drawn from the four dimensional stimulus space of color and gloss adjustments.</p></caption><graphic xlink:href="EMS158401-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><p>(a) 6 outdoor lighting environments (left 4 environments are sunny days and the right 2 environments are overcast days) and (b) 6 indoor lighting environments, together with the luminance histogram of all pixels and the <italic>a</italic>*<italic>b</italic>* chromatic distribution of 10% of pixel colors sampled at random. The numbers in the histograms are mean, standard deviation (s.d.), skewness (skew.), kurtosis, Xia’s diffuseness metric (diffuse.) and slope of the power spectrum computed by spherical harmonic decomposition. Maximum luminance in each image was normalized to 1.0 (zero on a logarithmic scale) to allow the comparison across environments in this figure, but note that for actual test images each environment was scaled differently from this figure (detailed in main text). The intersection of horizontal and vertical thin gray lines denotes the white point (X=Y=Z=100) of the color space. The black cross symbol depicts mean <italic>a</italic>*<italic>b</italic>* chromaticity across the plotted 10% of pixels. The black solid line shows the CIE daylight locus.</p></caption><graphic xlink:href="EMS158401-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><p>36 test images (a, c, and e) and an example lighting environment along with its <italic>a</italic>*<italic>b</italic>* chromatic distribution (b, d, and f) in <xref ref-type="sec" rid="S1">Experiment 1</xref>. (a, b) natural lighting environment, (c, d) +90° gamut-rotated lighting environment and (e, f) phase-scrambled lighting environment. Each test image contains a single object whose shape (and orientation), color and gloss level were randomly assigned.</p></caption><graphic xlink:href="EMS158401-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><p>Results for the control condition (control image 1) where the test image and reference image had identical shapes and lighting environments to measure the precision of participants’ settings along each parameter. The settings were first averaged across 3 sessions for each observer and then averaged across 10 participants. The image on x-axis shows control image 1. The blue horizontal line depicts the ground-truth values assigned for the test object. The error bars show ± 1.0 S.D. across 10 participants.</p></caption><graphic xlink:href="EMS158401-f006"/></fig><fig id="F7" position="float"><label>Figure 7</label><caption><p>Results in Experiment 1 for (a) natural lighting environments, (b) gamut-rotated lighting environments, and (c) phase-scrambled lighting environments. Each setting was averaged across 10 participants. Error bars show ± 1.0 S.E.M. over 10 participants, which is smaller than the data point for most cases. Upper images in each panel show test images, with numbers showing the correspondence to data points. The number at the left-upper corner in each sub-panel shows the correlation coefficient (mean ± 1.0 S.E.) between ground-truth values and participants’ settings calculated separately for each participant and averaged across 10 participants. The blue line represents the precision computed from the mean absolute error in the symmetric matching data. Small colored arrows show test images whose gloss level was judged to be particularly high (red arrows), low (blue arrows) and the same level as the reference image (green arrows).</p></caption><graphic xlink:href="EMS158401-f007"/></fig><fig id="F8" position="float"><label>Figure 8</label><caption><p>Left bars and data points show correlation coefficients between each human participant and the ground-truth and right bars and data points show inter-participant correlation (correlation between each participant and the average across all participants) for (a) natural, (b) gamut-rotated, and (c) phase-scrambled lighting environments. Each dot shows an individual participant and each bar shows the average across 10 participants.</p></caption><graphic xlink:href="EMS158401-f008"/></fig><fig id="F9" position="float"><label>Figure 9</label><caption><p>Analysis of interaction between body color (diffuse reflectance) and perceived gloss level for (a) natural, (b) gamut-rotated, and (c) phase-scrambled lighting environments in <xref ref-type="sec" rid="S1">Experiment 1</xref>. Each data point corresponds to one test image and its color shows the body color (in sRGB format). The size of each data point represents the error of gloss setting (human setting minus ground-truth). Red and blue edge colors show positive and negative errors, respectively. Left plot shows representation in <italic>a</italic>*<italic>b</italic>* chromatic plane while the right plot shows lightness axis. There was no systematic trend found between the body color and the magnitude of gloss errors.</p></caption><graphic xlink:href="EMS158401-f009"/></fig><fig id="F10" position="float"><label>Figure 10</label><caption><p>Scatter plot to see whether interaction between chroma and lightness (saturation defined as <italic>C</italic>*<sub><italic>ab</italic></sub> / <italic>L</italic>*) can explain the observed chroma and lightness constancy failures. The x-axis shows ground-truth saturation while the y-axis shows saturation computed from matching results. Significant correlations between ground-truth and human settings suggest that participants might have used the perceived saturation to help determine whether the color was matched between reference object and test object.</p></caption><graphic xlink:href="EMS158401-f010"/></fig><fig id="F11" position="float"><label>Figure 11</label><caption><p>(a) Example images where humans perceived high gloss (left column), low gloss (center column) and where ground-truth gloss level and human settings matched well (right column). Upper 6 images show original images where diffuse and specular reflections are both included. Lower 6 images show objects without diffuse reflection and surrounding context to visualize the spatial structure of specular reflection on which participants might have based their gloss judgements. (b) Process to convert original test image to thresholded highlight image. (c) Calculation of three metrics to predict human gloss percepts: coverage, sharpness and sub-band contrast (see main text for details).</p></caption><graphic xlink:href="EMS158401-f011"/></fig><fig id="F12" position="float"><label>Figure 12</label><caption><title>Correlation coefficients between image statistics (calculated over the object regions in test images) and human settings.</title><p>(a) Natural environment, (b) gamut-rotated environment and (c) phase-scrambled environment. The magenta shaded areas show an inter-participant noise ceiling, equivalent to the correlation across participants, which correspond to magenta numbers in <xref ref-type="fig" rid="F8">Figure 8</xref>. Red diamonds show the correlation between image statistics calculated over the object region in test images that had only diffuse reflection (no specular reflection).</p></caption><graphic xlink:href="EMS158401-f012"/></fig><fig id="F13" position="float"><label>Figure 13</label><caption><title>216 test images used in <xref ref-type="sec" rid="S2">Experiment 2</xref>.</title><p>Images were generated by the factorial combination between 12 natural lighting environments in <xref ref-type="sec" rid="S1">Experiment 1</xref> and 18 shapes that were randomly sampled from the 36 shapes in <xref ref-type="sec" rid="S1">Experiment 1</xref>. Body color was fixed as a greenish color, which we found useful to increase the visibility of specular reflection on the objects’ surface.</p></caption><graphic xlink:href="EMS158401-f013"/></fig><fig id="F14" position="float"><label>Figure 14</label><caption><p>(a) Average setting across 10 participants in <xref ref-type="sec" rid="S2">Experiment 2</xref>, where settings were grouped by lighting environment. Each data point denotes one shape, and thus there are 18 data points in each scatter plot. Error bars show ± 1.0 S.E. across 10 participants. Regression lines fitted to the 18 data points are shown by a solid red line. Black numbers in the upper left corners of the plots show the correlation coefficient (± 1.0 S.E.) between human settings and ground-truth, computed for each participant first and then averaged across 10 participants. Red numbers show the slope of the regression line. Higher slope means that, on average, that lighting environment produced a high degree of perceived gloss. (b) Lefthand bar plot shows correlation coefficient between image statistics and human settings. The dark green bars show image statistics computed from specular reflection images, and the light green bars show simple luminance image statistics. Error bars show ± 1.0 S.D across 10 participants. The blue line shows the correlation coefficient between participants’ settings and ground-truth values, averaged over 12 lighting environments (average over 12 black numbers in panel (a)). The pink shaded areas show an inter-participant noise ceiling computed in the same way as for <xref ref-type="sec" rid="S1">Experiment 1</xref> (<xref ref-type="fig" rid="F12">Figure 12</xref>), which no model can exceed. The gray filled circles show the partial correlations between each image statistic and human settings, removing the influence of the correlation between image statistics and ground-truth. The red line shows the correlation coefficient between human settings and a weighted sum of 12 statistics where weights were optimized through multiple regression. Right-hand bar plot shows a standardized regression coefficient (<xref ref-type="bibr" rid="R6">Borgonovo &amp; Plischke, 2016</xref>) for each statistic averaged across 10 cross-validations, and error bars show ± 1.0 S.D.</p></caption><graphic xlink:href="EMS158401-f014"/></fig></floats-group></article>