<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS156219</article-id><article-id pub-id-type="doi">10.1101/2022.10.25.513770</article-id><article-id pub-id-type="archive">PPR562565</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A Bayesian model for human directional localization of broadband static sound sources</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Barumerli</surname><given-names>Roberto</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Majdak</surname><given-names>Piotr</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Geronazzo</surname><given-names>Michele</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Avanzini</surname><given-names>Federico</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Meijer</surname><given-names>David</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Baumgartner</surname><given-names>Robert</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib></contrib-group><aff id="A1"><label>1</label>Acoustics Research Institute, Austrian Academy of Sciences, 1040 Vienna, Austria</aff><aff id="A2"><label>2</label>Dept. of Management and Engineering, University of Padova, Italy</aff><aff id="A3"><label>3</label>Dyson School of Design Engineering, Imperial College London, London, United Kingdom</aff><aff id="A4"><label>4</label>Dept. of Computer Science, University of Milano, Italy</aff><pub-date pub-type="nihms-submitted"><day>27</day><month>10</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>25</day><month>10</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Humans estimate sound-source directions by combining prior beliefs with sensory evidence. Prior beliefs represent statistical knowledge about the environment while sensory evidence is acquired from auditory features such as interaural disparities and monaural spectral shapes. Models of directional sound localization often impose constraints on the contribution of these features to either the horizontal or vertical dimension. Instead, we propose a Bayesian model that more flexibly incorporates each feature according to its spatial precision and integrates prior beliefs in the inference process. We applied the model to directional localization of a single, broadband, stationary sound source presented to a static human listener in an anechoic environment. We simplified interaural features to be broadband and compared two model variants, each considering a different type of monaural spectral features: magnitude profiles and gradient profiles. Both model variants were fitted to the baseline performance of five listeners and evaluated on the effects of localizing with non-individual head-related transfer functions (HRTFs) and sounds with rippled spectrum. The model variant with spectral gradient profiles outperformed other localization models. This model variant appears particularly useful for the evaluation of HRTFs and may serve as a basis for future extensions towards modeling dynamic listening conditions.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><label>I</label><title>Introduction</title><p id="P2">When localizing a sound source, human listeners have to deal with numerous sources of uncertainty [<xref ref-type="bibr" rid="R1">1</xref>]. Uncertainties originate from ambiguities in the acoustic signal encoding the source position [<xref ref-type="bibr" rid="R2">2</xref>] as well as the limited precision of the auditory system in decoding the received acoustic information [<xref ref-type="bibr" rid="R3">3</xref>, <xref ref-type="bibr" rid="R4">4</xref>]. Bayesian inference describes a statistically optimal solution to deal with such uncertainties in the process of perceptual decision making [<xref ref-type="bibr" rid="R5">5</xref>] and has been applied to model sound localization in various ways [<xref ref-type="bibr" rid="R6">6</xref>–<xref ref-type="bibr" rid="R9">9</xref>].</p><p id="P3">Common approaches of sound localization models rely on the evaluation of several spatial auditory features. Head-related transfer functions (HRTFs) describe all the spatially dependent acoustic filtering produced by the listener’s ears, head, and body [<xref ref-type="bibr" rid="R10">10</xref>] and have been used to derive spatial auditory features. The way to quantify or extract those features is a matter of debate. In particular, a large variety of monaural spectral-shape features have been studied [<xref ref-type="bibr" rid="R11">11</xref>–<xref ref-type="bibr" rid="R17">17</xref>], with spectral magnitude profiles [<xref ref-type="bibr" rid="R14">14</xref>, <xref ref-type="bibr" rid="R17">17</xref>] and spectral gradient profiles [<xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R15">15</xref>] being the most established ones. Despite such details, there is consensus that the interaural time and level differences (ITDs and ILDs) [<xref ref-type="bibr" rid="R1">1</xref>] as well as some form of monaural spectral shapes are important features for the directional localization of broadband sound sources [<xref ref-type="bibr" rid="R18">18</xref>].</p><p id="P4">In order to decode the spatial direction from the auditory features, models rely on the assumption that listeners have learned to associate acoustic features with spatial directions [<xref ref-type="bibr" rid="R13">13</xref>, <xref ref-type="bibr" rid="R19">19</xref>]. In fact, the interaural features are particularly informative about the horizontal dimension [<xref ref-type="bibr" rid="R1">1</xref>] though rather ambiguous with respect to the vertical dimension, where evidence from the monaural spectral features is more important [<xref ref-type="bibr" rid="R12">12</xref>]. This anisotropic relevance of different features is the reason for why specific auditory features are often studied along a single dimension of the so-called modified interaural-polar coordinate system [<xref ref-type="bibr" rid="R20">20</xref>], with the lateral angle along the horizontal left/right dimension and the polar angle along the vertical and front/back dimension. However, this geometric separation is a simplification. Monaural spectral features, for instance, can also contribute to the inference process in the direction estimation along the lateral dimension [<xref ref-type="bibr" rid="R21">21</xref>–<xref ref-type="bibr" rid="R23">23</xref>]. Hence, directional sound localization models should rather exploit the joint information encoded by all auditory features.</p><p id="P5">Such joint information has already been considered in a model of directional sound localization based on Bayesian inference [<xref ref-type="bibr" rid="R6">6</xref>]. This model computes a spatial likelihood function from a precision-weighted integration of a set of noisy acoustic features. Then, the perceived source direction is assumed to be at the maximum of that likelihood function. While this model was built to assess which spatial information can be accessible to the auditory system, its predictions overestimate the actual human performance yielding unrealistically low front-back confusion rates and localization errors [<xref ref-type="bibr" rid="R24">24</xref>]. Still, in order to model human performance, this model can serve as a solid basis for improvements such as the consideration of monaural spectral features, the integration of response noise involved in typical localization tasks, and the incorporation of prior beliefs.</p><p id="P6">Prior beliefs are important in the process of Bayesian inference because they reflect the listener’s statistical knowledge about the environment, helping to compensate for uncertainties in the sensory evidence [<xref ref-type="bibr" rid="R25">25</xref>]. For example, listeners seem to effectively increase precision in a frontal localization task by assuming source directions to be more likely located at the eye-level rather than at extreme vertical positions [<xref ref-type="bibr" rid="R8">8</xref>]. However, such an increase in precision may come at the cost of decreasing accuracy. As it seems, the optimal accuracy-precision trade-off in directional localization depends on the statistical distribution of sound sources [<xref ref-type="bibr" rid="R26">26</xref>]. While listeners seem to adjust their prior beliefs to changes in the sound-source distribution [<xref ref-type="bibr" rid="R26">26</xref>, <xref ref-type="bibr" rid="R27">27</xref>], they may also establish long-term priors reflecting the distribution of sound sources in their everyday environment.</p><p id="P7">Here, we introduce a Bayesian inference model to predict the performance of a listener in estimating the direction of static broadband sounds. Similar to [<xref ref-type="bibr" rid="R6">6</xref>], our model implements a noisy feature extraction and probabilistically combines interaural and monaural spatial features. These features are then compared with templates of spatial features, obtained from listener-specific HRTFs, to generate the sensory evidence in the form of a likelihood function. Subsequently, the sensory evidence is combined with prior beliefs emphasizing directions at the eye level [<xref ref-type="bibr" rid="R8">8</xref>]. The estimated source direction is selected from the resulting (posterior) spatial representation according to a Bayesian decision function. In a final stage, the model incorporates response scattering [<xref ref-type="bibr" rid="R15">15</xref>] to account for the uncertainty introduced by pointing responses in localization experiments.</p><p id="P8">For evaluation, we considered a model variant based on spectral amplitudes and a model variant based on spectral gradients [<xref ref-type="bibr" rid="R15">15</xref>]. Each model’s free parameters were fitted to the sound-localization performance of individual listeners [<xref ref-type="bibr" rid="R28">28</xref>]. We then tested the simulated responses of both model variants against human responses from sound-localization experiments investigating the effects of non-individual HRTFs [<xref ref-type="bibr" rid="R29">29</xref>] and ripples in the source spectrum [<xref ref-type="bibr" rid="R30">30</xref>].</p><p id="P9">The paper is organized as follows: <xref ref-type="sec" rid="S2">Sec. II</xref> describes the auditory model (<xref ref-type="sec" rid="S3">Sec. IIA</xref>) and explains the parameter estimation (<xref ref-type="sec" rid="S7">Sec. IIB</xref>). Then, <xref ref-type="sec" rid="S8">Sec.III</xref> evaluates the model’s performance by comparing its estimations to the actual performance of human listeners. Finally, <xref ref-type="sec" rid="S14">Sec. IV</xref> discusses the model’s relevance as well as its limitations, and outlines its potential for future extensions.</p></sec><sec id="S2" sec-type="methods"><label>II</label><title>Methods</title><sec id="S3"><label>A</label><title>Model description</title><p id="P10">The proposed auditory model consists of three main stages, as shown in <xref ref-type="fig" rid="F1">Fig. 1</xref>: 1) The feature extraction stage determines the encoded acoustic spatial information represented as a set of spatial features altered by noise; 2) The Bayesian inference integrates the sensory evidence resulting from the decoding procedure based on feature templates with the prior belief and forms a perceptual decision, and 3) The response stage transforms the perceptual decision in a directional response by corrupting the estimation with uncertainty in the pointing action.</p><sec id="S4"><label>1</label><title>Feature extraction</title><p id="P11">The spatial auditory features are extracted from the sensory input which is provided by the directional transfer function transformed in the time domain (i.e. the HRTF processed to remove the direction independent component [<xref ref-type="bibr" rid="R31">31</xref>]). We follow [<xref ref-type="bibr" rid="R6">6</xref>] in that we decode the spatial information provided by a single sound source via the binaural stimulus from a vector of spatial features: <disp-formula id="FD1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant="bold-italic"><mml:mi>x</mml:mi></mml:mstyle><mml:mrow><mml:mi>L</mml:mi><mml:mo>,</mml:mo><mml:mtext>mon</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant="bold-italic"><mml:mi>x</mml:mi></mml:mstyle><mml:mrow><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>x<sub>itd</sub></italic> denotes a scalar ITD feature, <italic>x<sub>ild</sub></italic> a scalar ILD feature, and a vector that concatenates monaural spectral features for left ear, <italic><bold>x</bold><sub>L,mon</sub></italic>, and right ear, <italic><bold>x</bold><sub>R,mon</sub></italic>. Each feature is assumed to be extracted by different neural pathways responsible to deliver encoded spatial information to higher levels of the auditory system [<xref ref-type="bibr" rid="R1">1</xref>, <xref ref-type="bibr" rid="R4">4</xref>].</p><p id="P12">Assuming broadband and spatially stationary sources, interaural features can be heavily approximated by means of wideband estimators [<xref ref-type="bibr" rid="R18">18</xref>, <xref ref-type="bibr" rid="R32">32</xref>]. The ILD was approximated as the time-averaged broadband level difference between the left and right channels [<xref ref-type="bibr" rid="R18">18</xref>]. The ITD was estimated by first processing each channel of the binaural signal with a low-pass Butterworth filter (10<sup><italic>th</italic></sup> order and cutoff 3000 Hz) and an envelope extraction step based on the Hilbert transform. Then, the ITD value is computed with the interaural cross-correlation method which is a good estimator of perceived lateralization in static scenarios with noise bursts [<xref ref-type="bibr" rid="R32">32</xref>]. In addition, we applied the transformation proposed by Reijniers <italic>et al</italic>. [<xref ref-type="bibr" rid="R6">6</xref>] to compensate the increasing uncertainty levels for increasing ITDs [<xref ref-type="bibr" rid="R33">33</xref>] resulting in a dimensionless quantity with a more isotropic variance: <disp-formula id="FD2"><label>(2)</label><mml:math id="M2"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>sgn</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:mo>|</mml:mo><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>d</mml:mi><mml:mo>|</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> with <italic>itd</italic> denoting ITDs in <italic>μs</italic> and the parameters <italic>a<sub>itd</sub></italic> = 32.5<italic>μs</italic> and <italic>b<sub>itd</sub></italic> = 0.095 and ’sgn’ indicating the sign function (for details on the derivation based on signal detection theory, see Supplementary Information from [<xref ref-type="bibr" rid="R6">6</xref>]). An example of the interaural features as functions of the lateral angle is shown in <xref ref-type="fig" rid="F2">Fig. 2</xref>.</p><p id="P13">Monaural spectral features, <italic><bold>x</bold></italic><sub>{<italic>L,R</italic>}, <italic>mon</italic></sub>, were derived from approximate neural excitation patterns. To approximate the spectral resolution of the human cochlea, we processed the binaural signal by the gammatone filter-bank with non-overlapping equivalent rectangular bandwidths [<xref ref-type="bibr" rid="R34">34</xref>, <xref ref-type="bibr" rid="R35">35</xref>], resulting in <italic>N<sub>B</sub></italic> = 27 bands within the interval [0.7, 18] kHz [<xref ref-type="bibr" rid="R36">36</xref>, <xref ref-type="bibr" rid="R37">37</xref>]. Followed by half-wave rectification and square-root compression to model hair-cell transduction [e.g., 38, 39], it results in the unit-less excitation: <disp-formula id="FD3"><label>(3)</label><mml:math id="M3"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msubsup><mml:mover><mml:mtext>c</mml:mtext><mml:mo>_</mml:mo></mml:mover><mml:mrow><mml:mi>ζ</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>φ</mml:mi></mml:mstyle></mml:msubsup><mml:mo stretchy="false">[</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mi>ζ</mml:mi><mml:mstyle mathvariant="bold"><mml:mi>φ</mml:mi></mml:mstyle></mml:msubsup><mml:mo>*</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mtext>c</mml:mtext><mml:mrow><mml:mi>ζ</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>φ</mml:mi></mml:mstyle></mml:msubsup><mml:mo stretchy="false">[</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:msqrt><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mtext>c</mml:mtext><mml:mo>_</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>ζ</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>φ</mml:mi></mml:mstyle></mml:msubsup><mml:mo stretchy="false">[</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msqrt></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if</mml:mtext><mml:msubsup><mml:mrow><mml:mover><mml:mtext>c</mml:mtext><mml:mo>_</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>ζ</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>φ</mml:mi></mml:mstyle></mml:msubsup><mml:mo stretchy="false">[</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mtext>otherwise,</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula> where subscripts <italic>ζ</italic> ∈ {<italic>L, R</italic>} indicate the left and right ears, <italic>n</italic> = 1, …, <italic>N</italic> is the time index, <italic>b</italic> = 1, …, <italic>N<sub>B</sub></italic> is the band index, <italic>g<sub>b</sub></italic>[<italic>n</italic>] is the corresponding gammatone filter and <inline-formula><mml:math id="M4"><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>ζ</mml:mi><mml:mi>φ</mml:mi></mml:msubsup><mml:mo stretchy="false">[</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> is the binaural signal in a normalized scale with sound direction <italic><bold>φ</bold></italic> (i.e. a pair of head-related impulse responses or their convolution with a source signal).</p><p id="P14">We thus define the spectral feature for the magnitude profiles (MPs) with the vector <italic><bold>x</bold><sub>ζ,MP</sub></italic>. This vector is the collection of root mean square amplitudes across time in decibels for each of the spectral bands for each ear: <disp-formula id="FD4"><label>(4)</label><mml:math id="M5"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msubsup><mml:mtext>mp</mml:mtext><mml:mrow><mml:mi>ζ</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mi>φ</mml:mi></mml:mstyle></mml:msubsup><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:msub><mml:mi>log</mml:mi><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:msubsup><mml:mtext>c</mml:mtext><mml:mrow><mml:mi>ζ</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mi>φ</mml:mi></mml:mstyle></mml:msubsup><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mstyle mathvariant="bold-italic"><mml:mi>x</mml:mi></mml:mstyle><mml:mrow><mml:mi>ζ</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mtext>mp</mml:mtext><mml:mrow><mml:mi>ζ</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mi>φ</mml:mi></mml:mstyle></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mtext>mp</mml:mtext><mml:mrow><mml:mi>ζ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mi>φ</mml:mi></mml:mstyle></mml:msubsup><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula> where the function <inline-formula><mml:math id="M6"><mml:mrow><mml:msubsup><mml:mtext>c</mml:mtext><mml:mrow><mml:mi>ζ</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mi>φ</mml:mi></mml:mstyle></mml:msubsup><mml:mo stretchy="false">[</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> is defined in <xref ref-type="disp-formula" rid="FD3">Eq. 3</xref>.</p><p id="P15">An alternative spectral feature can be computed by positive gradient extraction over the frequency dimension. It has previously been shown that integrating such spectral features in an auditory model provides good agreement with human localization performance [<xref ref-type="bibr" rid="R15">15</xref>]. Therefore, we define a second possible spectral feature based on gradient profiles (GPs) with the vector <italic><bold>x</bold><sub>ζ,GP</sub></italic>. It includes the gradient extraction as an additional processing step: <disp-formula id="FD5"><label>(5)</label><mml:math id="M7"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msubsup><mml:mover accent="true"><mml:mrow><mml:mtext>gp</mml:mtext></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mrow><mml:mi>ζ</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mi>φ</mml:mi></mml:mstyle></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mtext>mp</mml:mtext><mml:mrow><mml:mi>ζ</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mi>φ</mml:mi></mml:mstyle></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mtext>mp</mml:mtext><mml:mrow><mml:mi>ζ</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mi>φ</mml:mi></mml:mstyle></mml:msubsup><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mtext>gp</mml:mtext><mml:mrow><mml:mi>ζ</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mi>φ</mml:mi></mml:mstyle></mml:msubsup><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mtable columnalign="left" equalrows="true" equalcolumns="true"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mtext>gp</mml:mtext></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>ζ</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mi>φ</mml:mi></mml:mstyle></mml:msubsup></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mspace width="1.8em"/><mml:mrow><mml:mtext>if</mml:mtext><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>g</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>ζ</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mi>φ</mml:mi></mml:mstyle></mml:msubsup><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1.9em"/><mml:mtext>otherwise</mml:mtext><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mstyle mathvariant="bold-italic"><mml:mi>x</mml:mi></mml:mstyle><mml:mrow><mml:mi>ζ</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>g</mml:mi><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>ζ</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mi>φ</mml:mi></mml:mstyle></mml:msubsup><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>ζ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mi>φ</mml:mi></mml:mstyle></mml:msubsup><mml:mo stretchy="false">]</mml:mo><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p id="P16">A visualization of these monaural features is shown in <xref ref-type="fig" rid="F3">Fig. 3</xref>.</p><p id="P17">To demonstrate the impact of monaural spectral feature type, we will analyze the results of both variants with the corresponding feature spaces defined as follows: <disp-formula id="FD6"><label>(6)</label><mml:math id="M8"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mstyle mathvariant="bold-italic"><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>MP</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant="bold-italic"><mml:mi>x</mml:mi></mml:mstyle><mml:mrow><mml:mi>L</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant="bold-italic"><mml:mi>x</mml:mi></mml:mstyle><mml:mrow><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mstyle mathvariant="bold-italic"><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>GP</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant="bold-italic"><mml:mi>x</mml:mi></mml:mstyle><mml:mrow><mml:mi>L</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant="bold-italic"><mml:mi>x</mml:mi></mml:mstyle><mml:mrow><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p id="P18">Limited precision in the feature extraction process leads to corruption of the features and can be modelled as additive internal noise [<xref ref-type="bibr" rid="R6">6</xref>]. Hence, we define the noisy internal representation of the target features as: <disp-formula id="FD7"><label>(7)</label><mml:math id="M9"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mstyle mathvariant="bold-italic"><mml:mi>t</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold-italic"><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mo>+</mml:mo><mml:mstyle mathvariant="bold-italic"><mml:mi>δ</mml:mi></mml:mstyle><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle mathvariant="bold-italic"><mml:mi>δ</mml:mi></mml:mstyle><mml:mo>∼</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold-italic"><mml:mn>0</mml:mn></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold-italic"><mml:mi>Σ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula> where <bold>Σ</bold> is the covariance matrix of the multivariate Gaussian noise. Furthermore, we assume each spatial feature to be processed independently and thus to be also corrupted by independent noise [<xref ref-type="bibr" rid="R1">1</xref>]. Hence, the covariance matrix <bold>Σ</bold> is defined as: <disp-formula id="FD8"><label>(8)</label><mml:math id="M10"><mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mi>Σ</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mtext>ild</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mtext>mon</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mstyle mathvariant="bold-italic"><mml:mi>I</mml:mi></mml:mstyle></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> with <inline-formula><mml:math id="M11"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M12"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> being the variances associated with the ITDs and ILDs and <inline-formula><mml:math id="M13"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mstyle mathvariant="bold-italic"><mml:mi>I</mml:mi></mml:mstyle></mml:mrow></mml:math></inline-formula> being the covariance matrix for the monaural features where <italic><bold>I</bold></italic> is the identity matrix and the scalar <italic>σ<sub>mon</sub></italic> represents a constant and identical uncertainty for all frequency bands.</p></sec><sec id="S5"><label>2</label><title>Bayesian inference</title><p id="P19">The observer infers the sound direction <italic><bold>φ</bold></italic> from the spatial features in <italic><bold>t</bold></italic> while taking into account potential prior beliefs about the sound direction. Within the Bayesian inference framework [<xref ref-type="bibr" rid="R5">5</xref>], this requires to weight the likelihood <italic>p</italic>(<italic><bold>t</bold></italic>|<italic><bold>φ</bold></italic>) with the prior <italic>p</italic>(<italic><bold>φ</bold></italic>) to obtain the posterior distribution by means of Bayes’ law as: <disp-formula id="FD9"><label>(9)</label><mml:math id="M14"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold-italic"><mml:mi>φ</mml:mi></mml:mstyle><mml:mo>∣</mml:mo><mml:mstyle mathvariant="bold-italic"><mml:mi>t</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold-italic"><mml:mi>t</mml:mi></mml:mstyle><mml:mo>∣</mml:mo><mml:mstyle mathvariant="bold-italic"><mml:mi>φ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold-italic"><mml:mi>φ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P20">The likelihood function is implemented by comparing <italic><bold>t</bold></italic> with the feature templates. Similarly to [<xref ref-type="bibr" rid="R6">6</xref>], the template <italic><bold>T</bold></italic>(<italic><bold>φ</bold></italic>) contains noiseless features of <xref ref-type="disp-formula" rid="FD1">Eq. 1</xref> for every sound direction <italic><bold>φ</bold></italic>. While the sound direction is defined on a continuous support, our implementation sampled it over a uniform spherical grid with a spacing of 4.5° between points (<italic>N<sub>φ</sub></italic> = 1500 over the full sphere). Template features were computed from the listener-specific HRTFs. To accommodate non-uniform HRTF acquisition grids, we performed spatial interpolation based on spherical harmonics with order <italic>N<sub>SH</sub></italic> = 15, followed by Tikhonov regularization [<xref ref-type="bibr" rid="R40">40</xref>].</p><p id="P21">Since the templates are constructed without noise there exists a one-to-one mapping between direction and template features. This allows us to write the likelihood function for each point of the direction grid as: <disp-formula id="FD10"><label>(10)</label><mml:math id="M15"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold-italic"><mml:mi>t</mml:mi></mml:mstyle><mml:mo>∣</mml:mo><mml:mstyle mathvariant="bold-italic"><mml:mi>φ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold-italic"><mml:mi>t</mml:mi></mml:mstyle><mml:mo>∣</mml:mo><mml:mstyle mathvariant="bold-italic"><mml:mi>T</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold-italic"><mml:mi>φ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold-italic"><mml:mi>t</mml:mi></mml:mstyle><mml:mo>∣</mml:mo><mml:mstyle mathvariant="bold-italic"><mml:mi>T</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold-italic"><mml:mi>φ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold-italic"><mml:mi>Σ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <bold>Σ</bold> represents the learned precision of the auditory system (i.e. the sensory uncertainty <italic><bold>δ</bold></italic> reported in <xref ref-type="disp-formula" rid="FD7">Eq. 7</xref>). Finally, we interpret the a-priori probability <italic>p</italic>(<italic><bold>φ</bold></italic>) to reflect long-term expectations of listeners where prior probabilities are modelled as uniformly distributed along the horizontal dimension but centered towards the horizon as [<xref ref-type="bibr" rid="R8">8</xref>]. In particular, we extend the results from Ege <italic>et al</italic>. for sources positioned in the front and as well as back positions with: <disp-formula id="FD11"><label>(11)</label><mml:math id="M16"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold-italic"><mml:mi>φ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> with <italic>ϵ</italic> denoting the elevation angle of <italic><bold>φ</bold></italic> and <inline-formula><mml:math id="M17"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> the variance of the prior distribution [<xref ref-type="bibr" rid="R8">8</xref>]. For simplicity, the prior definition is based for the spherical coordinate system. Importantly, the origin of that prior is currently unknown and its implications are discussed in <xref ref-type="sec" rid="S14">Sec. IV</xref>.</p><p id="P22">According to <xref ref-type="disp-formula" rid="FD9">Eq. 9</xref>, a posterior spatial probability distribution is computed for every sound by optimally combining sensory evidence with prior knowledge [<xref ref-type="bibr" rid="R25">25</xref>]. As shown in <xref ref-type="fig" rid="F4">Fig. 4</xref>, the most probable direction of the source <italic><bold>φ</bold></italic> is then selected as the maximum a-posteriori (MAP) estimate: <disp-formula id="FD12"><label>(12)</label><mml:math id="M18"><mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mover accent="true"><mml:mi>φ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mstyle><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>arg</mml:mi><mml:mi>max</mml:mi></mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mi>φ</mml:mi></mml:mstyle></mml:munder><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold-italic"><mml:mi>t</mml:mi></mml:mstyle><mml:mo>∣</mml:mo><mml:mstyle mathvariant="bold-italic"><mml:mi>T</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold-italic"><mml:mi>φ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold-italic"><mml:mi>φ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p></sec><sec id="S6"><label>3</label><title>Response stage</title><p id="P23">After a sound direction estimate has been inferred, experiments usually require the listener to provide a motor response (e.g. manual pointing). To account for the uncertainty introduced by such responses, we incorporate post-decision noise in the model’s response stage. Following the approach from previous work [<xref ref-type="bibr" rid="R15">15</xref>], we distort the location estimate by additive, direction-independent (i.e. isotropic noise) Gaussian noise: <disp-formula id="FD13"><label>(13)</label><mml:math id="M19"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold-italic"><mml:mover accent="true"><mml:mi>φ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mstyle><mml:mi>r</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold-italic"><mml:mover accent="true"><mml:mi>φ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mstyle><mml:mo>+</mml:mo><mml:mstyle mathvariant="bold-italic"><mml:mi>m</mml:mi></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic><bold>m</bold></italic> ~ vMF(0, <italic>ĸ<sub>m</sub></italic>) is a von-Mises-Fisher distribution with zero mean and concentration parameter <italic>ĸ<sub>m</sub></italic>. The concentration parameter <italic>ĸ<sub>m</sub></italic> can be interpreted as a standard deviation <inline-formula><mml:math id="M20"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>κ</mml:mi><mml:mi>m</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:mn>180</mml:mn><mml:msup><mml:mi>π</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">[</mml:mo><mml:mtext>deg</mml:mtext><mml:mo stretchy="false">]</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> The contribution of the response noise is also visible in <xref ref-type="fig" rid="F4">Fig. 4</xref>, where the final estimate is scattered independently of the spatial information provided by the a-posteriori distribution. With <xref ref-type="disp-formula" rid="FD13">Eq. 13</xref>, the model outputs the response of the estimated sound source direction.</p></sec></sec><sec id="S7"><label>B</label><title>Parameter estimation</title><p id="P24">The model includes the following free parameters: <italic>σ<sub>ild</sub></italic>, <italic>σ<sub>mon</sub></italic> (amount of noise per feature; <italic>σ<sub>itd</sub></italic> was fixed to 0.569 as in [<xref ref-type="bibr" rid="R6">6</xref>]), <italic>σ<sub>P,ϵ</sub></italic> (directional prior), and <italic>σ<sub>m</sub></italic> (amount of response noise). Because of the structure of the model, these parameters jointly contribute to the prediction of performance in both lateral and polar dimensions. To roughly account for listener-specific differences in localization performance [<xref ref-type="bibr" rid="R2">2</xref>], the parameters were fitted to match individual listener performance.</p><p id="P25">As for the objective fitting function, we selected a set of performance metrics widely used in the analysis of behavioral localization experiments [<xref ref-type="bibr" rid="R28">28</xref>, <xref ref-type="bibr" rid="R29">29</xref>, <xref ref-type="bibr" rid="R41">41</xref>], for a summary see [<xref ref-type="bibr" rid="R42">42</xref>]. A commonly used set of metrics contains the quadrant error rate (QE, i.e., frequency of polar errors larger than 90°), local polar errors (PE, i.e., root mean square error in the polar dimension that are smaller than 90°, limited to lateral angles in the range of ±30°), and lateral errors (LE, i.e., root mean square error in the lateral dimension) [<xref ref-type="bibr" rid="R29">29</xref>]. We accounted for the inherent stochasticity of the model estimations by averaging the simulated performance metrics over 300 repetitions of the <italic>N<sub>φ</sub></italic> = 1550 directions in the HRTF dataset (i.e. Monte Carlo approximation with 465000 model simulations). Model parameters were jointly adjusted in an iterative procedure (see below) until the relative residual between the actual performance metric <italic>E<sub>a</sub></italic> and the predicted performance metric <italic>E<sub>p</sub></italic> was minimized below a metric-specific threshold <italic>τ<sub>E</sub></italic>, i.e., <disp-formula id="FD14"><label>(14)</label><mml:math id="M21"><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P26">We set the thresholds to <italic>τ<sub>LE</sub></italic> = 0.1, <italic>τ<sub>PE</sub></italic> = 0.15, and <italic>τ<sub>QE</sub></italic> = 0.2 because those values were feasible for all subjects. In addition, the QE was transformed with the rationalized arcsine function to handle small and large values adequately [<xref ref-type="bibr" rid="R43">43</xref>].</p><p id="P27">We ran the estimation procedure separately for each feature space in <xref ref-type="disp-formula" rid="FD6">Eq. 6</xref> and each listener. First, initial values of the parameters were derived from previous literature: the variance of the prior distribution was set to <italic>σ<sub>P,ϵ</sub></italic> = 11.5° as in [<xref ref-type="bibr" rid="R8">8</xref>]. The interaural feature noise was set to <italic>σ<sub>ild</sub></italic> = 1 dB, reflecting the range of ILD thresholds for pure tones [<xref ref-type="bibr" rid="R44">44</xref>]. The starting value for the monaural feature noise was set to <italic>σ<sub>mon</sub></italic> = 3.5 dB similarly to in [<xref ref-type="bibr" rid="R6">6</xref>]. The response noise standard deviation was set to <italic>σ<sub>m</sub></italic> = 17° as the sensorimotor scatter found in [<xref ref-type="bibr" rid="R15">15</xref>]. Second, in an iterative procedure, <italic>σ<sub>m</sub></italic> was optimized to minimize the residual error relative to the PE metric and, similarly, <italic>σ<sub>mon</sub></italic> was adjusted to match the QE metric. Then, <italic>σ<sub>ild</sub></italic> was decreased to reach the LE metric. These steps were reiterated until the residual errors between actual and simulated metrics was less than the respective threshold. This procedure limited the <italic>σ<sub>m</sub></italic> to the interval [5°, 20°] and used a step-size of 0.1°, <italic>σ<sub>mon</sub></italic> was defined in the interval [0.5, 10] dB with a step-size of 0.05 dB; <italic>σ<sub>ild</sub></italic> was defined in the interval of [0.5, 2] dB with a step-size of 0.5 dB. If the procedure did not converge, we decreased <italic>σ<sub>P,ϵ</sub></italic> by 0.5° and reattempted the parameter optimization procedure.</p></sec></sec><sec id="S8" sec-type="results"><label>III</label><title>Results</title><p id="P28">We first report the quality of model fits to the calibration data itself [<xref ref-type="bibr" rid="R28">28</xref>] in <xref ref-type="sec" rid="S9">Sec. IIIA</xref>. Then, <xref ref-type="sec" rid="S11">Sec. IIIB</xref> quantitatively evaluates the simulated performances of our two model variants and of two previously proposed models against data from two additional sound localization experiments.</p><sec id="S9"><label>A</label><title>Parameter fits</title><p id="P29">The parameter estimation procedure was done with both model variants, based on either <inline-formula><mml:math id="M22"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold-italic"><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>MP</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="M23"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold-italic"><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>GP</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> and for five individuals tested in a previous study [<xref ref-type="bibr" rid="R28">28</xref>]. In that experiment, naive listeners were asked to localize broadband noise bursts of 500 ms duration presented from various directions on the sphere via binaural rendering through headphones based on listener-specific directional transfer functions. The subjects were wearing a head-mounted display and were asked to orient the pointer in their right hand to the perceived sound-source direction. The fitting procedure converged for both models and all sub jects. Notably, subject NH15 required to reduce the step size of <italic>σ<sub>m</sub></italic> to 0.1° to meet the convergence criteria. <xref ref-type="table" rid="T1">Tab. I</xref> reports the estimated parameters <italic>σ<sub>m</sub></italic>, <italic>σ<sub>P,ϵ</sub></italic>, <italic>σ<sub>mon</sub></italic> and <italic>σ<sub>ild</sub></italic> for every listener. The amount of response noise was similar for both model types. <xref ref-type="table" rid="T2">Tab. II</xref> contrasts the predicted performance metrics with the actual ones, averaged across listeners.</p><p id="P30">More in detail, <xref ref-type="fig" rid="F5">Fig. 5</xref> compares predicted localization performance to the actual performance of subjects estimating the direction of a noise burst for different spherical segments [<xref ref-type="bibr" rid="R28">28</xref>]. The predicted LEs and PEs, both as functions of the actual lateral and polar angles, respectively, were in good agreement with those from the actual experiment. Instead, the simulated QE metric failed to mimic the front back asymmetries present in four subjects. Finally, only small differences were observed between the two feature spaces <inline-formula><mml:math id="M24"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold-italic"><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>MP</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M25"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold-italic"><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>GP</mml:mtext></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula></p><sec id="S10"><title>Contribution of model stages</title><p id="P31"><xref ref-type="fig" rid="F6">Fig. 6</xref> illustrates the effects of different model stages on target-specific predictions. The example shows direction estimations from subject NH16 localizing broadband noise bursts [<xref ref-type="bibr" rid="R28">28</xref>] and the corresponding predictions of the model based on <inline-formula><mml:math id="M26"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold-italic"><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>GP</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> with different configurations of priors and response noise: without both (a), with priors only (b), and with both (c). While adding response noise scatters the estimated directions equally across spatial dimensions (compare c to b), including the spatial prior only affects the polar dimension (compare b to a). As observed in the actual responses, the prior causes more of the simulated estimations to be biased towards the horizon (0° and 180°).</p><p id="P32">In order to quantify the effect of introducing the spatial prior in the polar dimension, we computed the polar gain as a measure of accuracy [<xref ref-type="bibr" rid="R13">13</xref>] for both simulated and the actual responses. This metric relies on two regressions performed on the baseline condition, separating between targets in the front and back. The linear fits for the baseline condition are defined as: <disp-formula id="FD15"><label>(15)</label><mml:math id="M27"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula> with <italic>ϕ<sub>e</sub></italic> being the estimated polar angles and <italic>ϕ<sub>a</sub></italic> being the actual polar angles. The parameters are the localization bias <italic>b<sub>ϕ</sub></italic> in degrees, which is typically very small, and the dimensionless localization gain <italic>g<sub>ϕ</sub></italic>, which can be seen as a measure of accuracy [<xref ref-type="bibr" rid="R8">8</xref>, <xref ref-type="bibr" rid="R13">13</xref>]. The regression fits only incorporate <italic>ϕ<sub>e</sub></italic> that deviate from the regression line by less than 40°. Since that definition of outliers depends on the regression parameters, this procedure is initialized with <italic>b<sub>ϕ</sub></italic> = 0° and <italic>g<sub>ϕ</sub></italic> = 1 and re-iterated until convergence. In our analysis, only the frontal positions were considered. The polar gain of the actual responses, averaged over subjects, was 0.50, indicating that our subjects showed a localization error increasing with the angular distance to the horizontal plane. For the models without the prior, the predicted polar gain was 1.00 (<xref ref-type="fig" rid="F6">Fig. 6a</xref>). The polar gain obtained by the model including the prior was 0.62 (<xref ref-type="fig" rid="F6">Fig. 6b and c</xref>) showing a better correspondence to the actual polar gain. Hence, the introduction of the prior belief improved the agreement with the actual localization responses by biasing them towards the horizon.</p></sec></sec><sec id="S11"><label>B</label><title>Model evaluation</title><p id="P33">The performance evaluation was done at the group-level. For our model, we used the five calibrated parameter sets with templates <italic><bold>T</bold></italic>(<italic><bold>φ</bold></italic>) based on the individuals’ HRTFs as “digital observers”. Group-level results of these digital observers were then evaluated for two psychoacoustic experiments with acoustic stimuli as input that differed from the baseline condition with a flat source spectrum and individual HRTFs.</p><p id="P34">In addition, we compared our results with the ones of two previously published models. The first one, described by Reijniers et al. [<xref ref-type="bibr" rid="R6">6</xref>], is probabilistic and able to jointly estimate the lateral and polar dimensions similar to the model described in this work. Reijniers’ model deviates from the current model since it relies on a different feature extraction stage, uses a uniform spatial prior distribution, does not include response noise (<xref ref-type="disp-formula" rid="FD13">Eq. 13</xref>) and does not fit individualized parameters. The second model, described by Baumgartner et al. [<xref ref-type="bibr" rid="R15">15</xref>], estimates sound positions only in the polar dimension. Nevertheless, it shares a similar processing pipeline with the current model in that it considers both a perceptually relevant feature extraction stage, includes response noise, and considers individualized parameters. The main differences with our model resides on the incorporation of a directional prior and of the lateral dimension per se, and on how the distance between target and templates is computed. Par-ticularly, this previous work resorted on the <italic>l</italic><sup>2</sup>-norm to implement the template comparison procedure which is substantially different from our likelihood function. At the moment, this model is commonly used by the scientific community that is interested in elevation perception based on monaural spectral features for sound direction estimation [e.g., <xref ref-type="bibr" rid="R45">45</xref>, <xref ref-type="bibr" rid="R46">46</xref>]. We will refer to these two models as <monospace>reijniers2014</monospace> and <monospace>baumgartner2014</monospace>, respectively.</p><sec id="S12"><label>1</label><title>Effects of non-individual HRTFs</title><p id="P35">In first evaluation, sounds were spatialized using nonindividualized HRTFs [<xref ref-type="bibr" rid="R29">29</xref>]. Originally, eleven listeners localized Gaussian white noise bursts with a duration of 250 ms and sound directions were randomly sampled from the full sphere. Subjects were asked to estimate the direction of sounds that were spatialized using their own HRTFs in addition to sounds that were spatialized using up to 4 HRTFs from other subjects (21 cases in total). With the aim to reproduce these results, we had our pool of five digital listeners localize sounds from all available directions that were spatialized with their own individual HRTFs (<italic>Own</italic>) as well as sounds that were spatialized with HRTFs from the other 4 individuals (<italic>Other</italic>). We thus considered all inter-listener HRTF combinations for the non-individual condition.</p><p id="P36"><xref ref-type="fig" rid="F7">Fig. 7</xref> summarizes the results obtained for localization experiments with own and other HRTFs. In the <italic>Own</italic> condition, there is a small deviation between the actual results from [<xref ref-type="bibr" rid="R29">29</xref>] and our model predictions. This mismatch reflects the fact that the digital observers represent a different pool of subjects (taken from [<xref ref-type="bibr" rid="R28">28</xref>]) tested on a slightly different experimental protocol and setup. Differences in performance metrics are small between the two feature spaces, as already reported during parameter fitting. Predictions from the <monospace>baumgartner2014</monospace> model are only possible for the polar dimension. Instead, the model <monospace>reijniers2014</monospace> predicted too small errors, as also observed in previous simulations employing this model [<xref ref-type="bibr" rid="R24">24</xref>, <xref ref-type="bibr" rid="R47">47</xref>].</p><p id="P37">In the <italic>Other</italic> condition, both of our model variants predicted a smaller degradation for the lateral dimension as compared to the actual data. The lateral errors predicted by <monospace>reinjiers2014</monospace> increased moderately but remained too small in comparison to the actual data. In the polar dimension, both model variants resulted in increased PEs and QEs, but the amount of increase was larger and more similar to the actual data for the variant equipped with gradients profiles, especially with respect to QE. The predictions from <monospace>baumgartner2014</monospace> were very similar to the model based on spectral gradients, as expected given the similar method to extract monaural spectral feature. Instead, the simulations form <monospace>reijniers2014</monospace> demonstrates how this model reported super-human performances as already demonstrated in previous analysis [<xref ref-type="bibr" rid="R24">24</xref>].</p></sec><sec id="S13"><label>2</label><title>Effects of rippled-spectrum sources</title><p id="P38">The second evaluation tested the effect of spectral modulation of sound sources on directional localization in the polar dimension [<xref ref-type="bibr" rid="R30">30</xref>]. In that study, localization performance was probed by using noises in the frequency band [<xref ref-type="bibr" rid="R1">1</xref>, <xref ref-type="bibr" rid="R16">16</xref>] kHz which spectral shape were distorted with a sinusoidal modulation in the log-magnitude domain. The conditions considered different ripple depths, defined as the peak-to-peak difference of the log-spectral magnitude, and ripple densities, defined as the sinusoidal period along the logarithmic frequency scale. The actual experiment tested six trained subjects in a dark, anechoic chamber listening to the stimuli via loudspeakers. The sounds lasted 250 ms and were positioned between lateral angles of ±30° and polar angles of either 0 ± 60° for the front or 180 ± 60° for the back. A “baseline” condition included a broadband noise without any spectral modulation (ripple depth of 0dB). To quantify the localization performance, we used the polar error rate (PER) as they defined [<xref ref-type="bibr" rid="R30">30</xref>]. For every condition, two baseline regressions are computed as in <xref ref-type="sec" rid="S9">Sec. III A</xref> allowing to quantify the PER as the ratio of actual responses deviating by more than 45° from the predicted values of the baseline regression.</p><p id="P39"><xref ref-type="fig" rid="F8">Fig. 8</xref> shows the results of testing the fitted models with rippled spectra. In the baseline condition, our model exhibited similar performances to those obtained in the actual experiment, whereas <monospace>baumgartner2014</monospace> underestimates the baseline performance for this particular error metric. In the ripple conditions, actual listeners demonstrated poorest performance for ripple densities around one ripple per octave and a systematic increase in error rate with increasing ripple depth. These effects were well predicted by the model variant based on gradient profiles, similar to the predictions from <monospace>baumgartner2014</monospace>. In contrast, both <monospace>reijnier2014</monospace> and our model based on magnitude profiles were not able to reflect the effects of ripple density and depth as present in the actual data. Hence, the positive gradient extraction appears crucial processing step for predicting sagittal-plane localization of sources with a non-flat spectrum.</p></sec></sec></sec><sec id="S14" sec-type="discussion"><label>IV</label><title>Discussion</title><p id="P40">The proposed functional model aims at reproducing listeners’ performances when inferring the sound-source direction.The model formulation relies on Bayesian inference [<xref ref-type="bibr" rid="R25">25</xref>] as it integrates the sensory evidence for spatial directions obtained by combining binaural and monaural features [<xref ref-type="bibr" rid="R13">13</xref>] with a spatial prior [<xref ref-type="bibr" rid="R8">8</xref>]. Our approach considers uncertainties about the various sensory features, as in [<xref ref-type="bibr" rid="R6">6</xref>], in addition to the noise introduced by pointing responses [<xref ref-type="bibr" rid="R15">15</xref>]. These model components enabled us to successfully match overall performance metrics (LE, PE, and QE) for five subjects (see <xref ref-type="table" rid="T2">Tab. II</xref>) and within spatially restricted areas (<xref ref-type="fig" rid="F5">Fig. 5</xref>). Importantly, with the inclusion of a spatial prior the model was able to adequately explain listeners’ response biases towards the horizontal plane. Compared to previous models [<xref ref-type="bibr" rid="R6">6</xref>, <xref ref-type="bibr" rid="R15">15</xref>], our model better predicted the group-level effects of non-individualized HRTFs and rippled source spectra, yet only if positive spectral gradient profiles <inline-formula><mml:math id="M28"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold-italic"><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mrow><mml:mtext>GP</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> rather than magnitude profiles, served as monaural spectral features.</p><p id="P41">The validity of the model is limited to scenarios where both the source and subject are spatially static and situated in an acoustic free-field. Additionally, the current model evaluation only considered broadband and stationary sounds. For this reason, we have restricted the extraction of ITDs and ILDs to simple approximations that are sufficient for this set of conditions, as demonstrated both in our present work as well as in previous literature [<xref ref-type="bibr" rid="R18">18</xref>, <xref ref-type="bibr" rid="R32">32</xref>] by the high accuracy of estimated lateral angles, where those features are arguable most important. The model, as presented here, does not currently account for feature-specific non-linearities required to predict phenomena like the precedence effect [<xref ref-type="bibr" rid="R48">48</xref>, <xref ref-type="bibr" rid="R49">49</xref>] and cannot readily be applied to non-stationary sounds, such as speech, without extensions to the feature extraction procedure. Nevertheless, from an application point of view, the proposed model can be a useful tool to assess the perceptual validity of a non-individual HRTF dataset. As an example, one can use the model’s predictions to quantify differences in expected direction estimation performance for (input) sounds that are spatialized with generic vs. individual HRTFs [<xref ref-type="bibr" rid="R50">50</xref>].</p><p id="P42">In this work, the most probable source position is selected from the posterior distribution via the MAP decision rule. We preferred this widely-used estimator over the equally-common mean estimator to adequately deal with multiple modes of the posterior distribution generated by poor discrimination between front-back positions along sagittal planes. On the other hand, one could argue that the MAP estimator disproportionately biases direction estimates towards the mode where prior probability is large, at least under conditions of high sensory uncertainty. One of many possible alternative decision functions that may better describe stochastic human localization responses is posterior sampling [<xref ref-type="bibr" rid="R8">8</xref>]. With this decision function the model would probabilistically sample its best perceptual estimate from the full posterior distribution. Although often considered suboptimal, this strategy would allow an observer faster exploration and flexible adaptation to novel environmental statistics [<xref ref-type="bibr" rid="R51">51</xref>]. Importantly, a different decision rule might affect the here-estimated magnitudes of the sensory and motor noise. Therefore, comparative evaluations of different estimators would benefit from a more robust fitting procedure, which falls outside of the scope of the current study.</p><p id="P43">The model incorporates several non-acoustic components because they are deemed crucial to explain human performances [<xref ref-type="bibr" rid="R2">2</xref>, <xref ref-type="bibr" rid="R52">52</xref>]. Extending the <monospace>reijniers2014</monospace> model [<xref ref-type="bibr" rid="R6">6</xref>] by incorporating a spatial prior and response scatter appears vital to explain listeners’ estimation patterns. Without these components, fitting the model to the polar performance metrics was unfeasible [<xref ref-type="bibr" rid="R24">24</xref>]. First, response noise allowed us to control response precision at a local level (LE and PE) while leaving global errors (QE) largely unaffected. Instead, the occurrence rate of such global errors depends predominantly on the noise variance that is added to the monaural features. Second, the spatial prior shapes the response patterns by introducing a bias towards the horizon [<xref ref-type="bibr" rid="R41">41</xref>]. As shown in <xref ref-type="fig" rid="F6">Fig. 6</xref>, the prior contribution is visible in the polar component of the simulated responses which cluster around the eyelevel direction. Additional evidence is given by the polar gain measure as reported in <xref ref-type="sec" rid="S9">Sec. IIIA</xref> where the integration of prior beliefs leads to better matching performances in the vertical dimension. Our formulation of the spatial prior was extrapolated from previous work [<xref ref-type="bibr" rid="R8">8</xref>] in the sense that its spatial distribution was assumed to be front-back symmetric. Discrepancies observed between actual and predicted global errors (<xref ref-type="fig" rid="F5">Fig. 5</xref>) indicate that this assumption is likely incorrect and points towards an asymmetric prior instead. Nonetheless, at this point we can only speculate about the reasons behind the existence of such a long-term prior in spatial hearing. It potentially reflects the spatial distribution of sound sources during everyday exposure [<xref ref-type="bibr" rid="R53">53</xref>], or it may stem from an evolutionary emphasis on high relevance auditory signals [<xref ref-type="bibr" rid="R4">4</xref>], or could be related to the center of gaze as observed in barn owls [<xref ref-type="bibr" rid="R54">54</xref>] although the processing underlying the spatial inference mechanism might be different in mammals [<xref ref-type="bibr" rid="R3">3</xref>].</p><p id="P44">While the model currently only considers the static scenario, it sets the foundations for future work on predicting sound localization behavior in realistic environments. Evaluating the environment’s dynamics as a chain of consecutive events as in [<xref ref-type="bibr" rid="R7">7</xref>] may be a promising approach. Sequential updating of one’s beliefs, from prior to posterior to prior again, comes natural under the Bayesian inference scheme [<xref ref-type="bibr" rid="R25">25</xref>]. This makes the here-proposed model well suited as a basis for such investigations. Thus, a rich set of modulators might influence spatial hearing and the model’s prior belief is an entry point to account for many of those, c.f.: evidence accumulation to track source statistics [<xref ref-type="bibr" rid="R26">26</xref>, <xref ref-type="bibr" rid="R55">55</xref>], visual influences on auditory spatial perception [<xref ref-type="bibr" rid="R56">56</xref>], and auditory attention to segregate sources [<xref ref-type="bibr" rid="R57">57</xref>]. Selective temporal integration appears important to deal with the spatial information of many natural sources and their reflections competing in realistic scenarios. This aspect could be partially addressed by integrating recent findings related to interaural feature extraction [<xref ref-type="bibr" rid="R58">58</xref>]. To this end, the model will need to consider the dynamic interaction between the listener and the acoustic field. Consequently, these extensions will potentially enable the model to account for subject movements [<xref ref-type="bibr" rid="R9">9</xref>] and simultaneous tracking of source movements [<xref ref-type="bibr" rid="R59">59</xref>] while extracting spatial information from echoic scenarios [<xref ref-type="bibr" rid="R60">60</xref>].</p></sec><sec id="S15" sec-type="conclusions"><label>V</label><title>Conclusions</title><p id="P45">We proposed a computational auditory model for the perceptual inference of sound-source direction based on Bayesian inference. From a binaural input, interaural and monaural spatial features jointly provide the sensory evidence to estimate the sound direction. The model parameters are interpretable and related to sensory noise, prior uncertainty, and response noise. Having fitted the model parameters to match subject-specific performance in a baseline condition, the model accurately predicted the localization performance observed for test conditions with non-individualized HRTFs and spectrally-modulated source spectra. Regarding spectral monaural feature extraction, the model variant evaluating gradient profiles performed best.</p><p id="P46">The proposed model seems useful to assess the perceptual validity of HRTFs. The model’s domain is currently limited to static conditions, but it seems to provide a good basis for future extensions to spatially dynamic situations, spectro-temporally dynamic signals like speech and music, and reverberant environments.</p></sec></body><back><ack id="S16"><title>Acknowledgements</title><p>We thank Jonas Reijniers for providing the initial implementation of his model. We are also grateful to Günther Koliander and Clara Hollomey for helping with the mathematical formulation of some model aspects. This work was supported by the European Union (EU) within the project SONICOM (grant number: 101017743, RIA action of Horizon 2020, to P.M.) and the Austrian Science Fund (FWF) within the project Dynamates (grant number: ZK66, to R.Bau.).</p></ack><sec id="S17" sec-type="data-availability"><title>Data Availability Statement</title><p id="P47">The implementation of the model presented in this manuscript is available in the Auditory Modeling Toolbox (AMT 1.1) as <monospace>barumerli2022</monospace> [<xref ref-type="bibr" rid="R61">61</xref>]. Data from [<xref ref-type="bibr" rid="R28">28</xref>] are also available in the AMT 1.1 as <monospace>data_majdak2010</monospace>. Individual HRTF datasets of these subjects are publicly available within the ARI database at <ext-link ext-link-type="uri" xlink:href="http://sofacoustics.org/data/database/ari/">http://sofacoustics.org/data/database/ari/</ext-link>. Moreover, the implementations of the model <monospace>baumgartner2014</monospace> presented in [<xref ref-type="bibr" rid="R15">15</xref>], the model reijniers2014 from [<xref ref-type="bibr" rid="R6">6</xref>] and the data for the model evaluation procedure are available as <monospace>data_middlebrooks1999</monospace> and <monospace>data_macpherson2003</monospace> in the AMT 1.1.</p><p id="P48">Additional toolboxes were selected for the model implementation. To provide quasi-uniform sphere sampling the model relied on <ext-link ext-link-type="uri" xlink:href="https://github.com/AntonSemechko/S2-Sampling-Toolbox">https://github.com/AntonSemechko/S2-Sampling-Toolbox</ext-link>. For the implementation of the von Mises-Fisher distribution, we used <ext-link ext-link-type="uri" xlink:href="https://github.com/TerdikGyorgy/3D-Simulation-Visualization/">https://github.com/TerdikGyorgy/3D-Simulation-Visualization/</ext-link>.</p></sec><ref-list><ref id="R1"><label>[1]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Heijden</surname><given-names>K</given-names></name><name><surname>Rauschecker</surname><given-names>JP</given-names></name><name><surname>de Gelder</surname><given-names>B</given-names></name><name><surname>Formisano</surname><given-names>E</given-names></name></person-group><article-title>Cortical mechanisms of spatial hearing</article-title><source>Nature Reviews Neuroscience</source><year>2019</year><volume>20</volume><issue>10</issue><fpage>609</fpage><lpage>623</lpage><pub-id pub-id-type="pmcid">PMC7081609</pub-id><pub-id pub-id-type="pmid">31467450</pub-id><pub-id pub-id-type="doi">10.1038/s41583-019-0206-5</pub-id></element-citation></ref><ref id="R2"><label>[2]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Majdak</surname><given-names>P</given-names></name><name><surname>Baumgartner</surname><given-names>R</given-names></name><name><surname>Laback</surname><given-names>B</given-names></name></person-group><article-title>Acoustic and non-acoustic factors in modeling listener-specific performance of sagittal-plane sound localization</article-title><source>Frontiers in Psychology</source><year>2014</year><volume>5</volume><comment><ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00319/full">www.frontiersin.org/articles/10.3389/fpsyg.2014.00319/full</ext-link></comment><pub-id pub-id-type="pmcid">PMC4006033</pub-id><pub-id pub-id-type="pmid">24795672</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2014.00319</pub-id></element-citation></ref><ref id="R3"><label>[3]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grothe</surname><given-names>B</given-names></name><name><surname>Pecka</surname><given-names>M</given-names></name><name><surname>McAlpine</surname><given-names>D</given-names></name></person-group><article-title>Mechanisms of Sound Localization in Mammals</article-title><source>Physiological Reviews</source><year>2010</year><volume>90</volume><issue>3</issue><fpage>983</fpage><lpage>1012</lpage><pub-id pub-id-type="pmid">20664077</pub-id></element-citation></ref><ref id="R4"><label>[4]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pecka</surname><given-names>M</given-names></name><name><surname>Leibold</surname><given-names>C</given-names></name><name><surname>Grothe</surname><given-names>B</given-names></name></person-group><chapter-title>Biological aspects of perceptual space formation</chapter-title><source>The Technology of Binaural Understanding</source><publisher-name>Springer</publisher-name><year>2020</year><fpage>151</fpage><lpage>171</lpage></element-citation></ref><ref id="R5"><label>[5]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>WJ</given-names></name></person-group><article-title>Organizing probabilistic models of perception</article-title><source>Trends in Cognitive Sciences</source><year>2012</year><volume>16</volume><issue>10</issue><fpage>511</fpage><lpage>518</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(12)00201-X?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS136466131200201X%3Fshowall%3Dtrue">linkinghub.elsevier.com/retrieve/pii/S136466131200201X</ext-link></comment><pub-id pub-id-type="pmid">22981359</pub-id></element-citation></ref><ref id="R6"><label>[6]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reijniers</surname><given-names>J</given-names></name><name><surname>Vanderelst</surname><given-names>D</given-names></name><name><surname>Jin</surname><given-names>C</given-names></name><name><surname>Carlile</surname><given-names>S</given-names></name><name><surname>Peremans</surname><given-names>H</given-names></name></person-group><article-title>An ideal-observer model of human sound localization</article-title><source>Biological Cybernetics</source><year>2014</year><volume>108</volume><issue>2</issue><fpage>169</fpage><lpage>181</lpage><pub-id pub-id-type="pmid">24570350</pub-id></element-citation></ref><ref id="R7"><label>[7]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayser</surname><given-names>H</given-names></name><name><surname>Hohmann</surname><given-names>V</given-names></name><name><surname>Ewert</surname><given-names>SD</given-names></name><name><surname>Kollmeier</surname><given-names>B</given-names></name><name><surname>Anemüller</surname><given-names>J</given-names></name></person-group><article-title>Robust auditory localization using probabilistic inference and coherence-based weighting of interaural cues</article-title><source>The Journal of the Acoustical Society of America</source><year>2015</year><volume>138</volume><issue>5</issue><fpage>2635</fpage><lpage>2648</lpage><pub-id pub-id-type="pmid">26627742</pub-id></element-citation></ref><ref id="R8"><label>[8]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ege</surname><given-names>R</given-names></name><name><surname>van Opstal</surname><given-names>AJ</given-names></name><name><surname>van Wanrooij</surname><given-names>MM</given-names></name></person-group><article-title>Accuracy-precision trade-off in human sound localisation</article-title><source>Scientific Reports</source><year>2018</year><volume>8</volume><issue>1</issue><elocation-id>16399</elocation-id><comment><ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41598-018-34512-6">www.nature.com/articles/s41598-018-34512-6</ext-link></comment><pub-id pub-id-type="pmcid">PMC6219530</pub-id><pub-id pub-id-type="pmid">30401920</pub-id><pub-id pub-id-type="doi">10.1038/s41598-018-34512-6</pub-id></element-citation></ref><ref id="R9"><label>[9]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McLachlan</surname><given-names>Glen</given-names></name><name><surname>Majdak</surname><given-names>Piotr</given-names></name><name><surname>Reijniers</surname><given-names>Jonas</given-names></name><name><surname>Peremans</surname><given-names>Herbert</given-names></name></person-group><article-title>Towards modelling active sound localisation based on bayesian inference in a static environment</article-title><source>Acta Acust</source><year>2021</year><volume>5</volume><fpage>45</fpage><pub-id pub-id-type="doi">10.1051/aacus/2021039</pub-id></element-citation></ref><ref id="R10"><label>[10]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Møller</surname><given-names>H</given-names></name><name><surname>Sørensen</surname><given-names>MF</given-names></name><name><surname>Hammershøi</surname><given-names>D</given-names></name><name><surname>Jensen</surname><given-names>CB</given-names></name></person-group><article-title>Head-related transfer functions of human subjects</article-title><source>Journal of the Audio Engineering Society</source><year>1995</year><volume>43</volume><issue>5</issue><fpage>300</fpage><lpage>321</lpage></element-citation></ref><ref id="R11"><label>[11]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Middlebrooks</surname><given-names>JC</given-names></name></person-group><article-title>Narrow-band sound localization related to external ear acoustics</article-title><source>The Journal of the Acoustical Society of America</source><year>1992</year><volume>92</volume><issue>5</issue><fpage>2607</fpage><lpage>2624</lpage><pub-id pub-id-type="pmid">1479124</pub-id></element-citation></ref><ref id="R12"><label>[12]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zakarauskas</surname><given-names>P</given-names></name><name><surname>Cynader</surname><given-names>MS</given-names></name></person-group><article-title>A computational theory of spectral cue localization</article-title><source>The Journal of the Acoustical Society of America</source><year>1993</year><volume>94</volume><issue>3</issue><fpage>1323</fpage><lpage>1331</lpage></element-citation></ref><ref id="R13"><label>[13]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hofman</surname><given-names>PM</given-names></name><name><surname>van Opstal</surname><given-names>AJ</given-names></name></person-group><article-title>Spectro-temporal factors in two-dimensional human sound localization</article-title><source>The Journal of the Acoustical Society of America</source><year>1998</year><volume>103</volume><issue>5</issue><fpage>2634</fpage><lpage>2648</lpage><pub-id pub-id-type="pmid">9604358</pub-id></element-citation></ref><ref id="R14"><label>[14]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langendijk</surname><given-names>EHA</given-names></name><name><surname>Bronkhorst</surname><given-names>AW</given-names></name></person-group><article-title>Contribution of spectral cues to human sound localization</article-title><source>The Journal of the Acoustical Society of America</source><year>2002</year><volume>112</volume><issue>4</issue><fpage>1583</fpage><lpage>1596</lpage><pub-id pub-id-type="pmid">12398464</pub-id></element-citation></ref><ref id="R15"><label>[15]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baumgartner</surname><given-names>R</given-names></name><name><surname>Majdak</surname><given-names>P</given-names></name><name><surname>Laback</surname><given-names>B</given-names></name></person-group><article-title>Modeling sound-source localization in sagittal planes for human listeners</article-title><source>The Journal of the Acoustical Society of America</source><year>2014</year><volume>136</volume><issue>2</issue><fpage>791</fpage><lpage>802</lpage><pub-id pub-id-type="pmcid">PMC4582445</pub-id><pub-id pub-id-type="pmid">25096113</pub-id><pub-id pub-id-type="doi">10.1121/1.4887447</pub-id></element-citation></ref><ref id="R16"><label>[16]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baumgartner</surname><given-names>R</given-names></name><name><surname>Majdak</surname><given-names>P</given-names></name><name><surname>Laback</surname><given-names>B</given-names></name></person-group><article-title>Modeling the effects of sensorineural hearing loss on sound localization in the median plane</article-title><source>Trends in Hearing</source><year>2016</year><volume>20</volume><elocation-id>2331216516662003</elocation-id><pub-id pub-id-type="pmcid">PMC5055367</pub-id><pub-id pub-id-type="pmid">27659486</pub-id><pub-id pub-id-type="doi">10.1177/2331216516662003</pub-id></element-citation></ref><ref id="R17"><label>[17]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Opstal</surname><given-names>AJ</given-names></name><name><surname>Vliegen</surname><given-names>J</given-names></name><name><surname>van Esch</surname><given-names>T</given-names></name></person-group><article-title>Reconstructing spectral cues for sound localization from responses to rippled noise stimuli</article-title><source>PloS one</source><year>2017</year><volume>12</volume><issue>3</issue><elocation-id>e0174185</elocation-id><pub-id pub-id-type="pmcid">PMC5363849</pub-id><pub-id pub-id-type="pmid">28333967</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0174185</pub-id></element-citation></ref><ref id="R18"><label>[18]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Middlebrooks</surname><given-names>JC</given-names></name></person-group><chapter-title>Sound localization</chapter-title><source>Handbook of Clinical Neurology</source><publisher-name>Elsevier</publisher-name><year>2015</year><volume>129</volume><fpage>99</fpage><lpage>116</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/abs/pii/B9780444626301000068?via%3Dihub">linkinghub.elsevier.com/retrieve/pii/B9780444626301000068</ext-link></comment><pub-id pub-id-type="pmid">25726265</pub-id></element-citation></ref><ref id="R19"><label>[19]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Wanrooij</surname><given-names>MM</given-names></name><name><surname>van Opstal</surname><given-names>AJ</given-names></name></person-group><article-title>Relearning Sound Localization with a New Ear</article-title><source>Journal of Neuroscience</source><year>2005</year><volume>25</volume><issue>22</issue><fpage>5413</fpage><lpage>5424</lpage><pub-id pub-id-type="pmcid">PMC6724994</pub-id><pub-id pub-id-type="pmid">15930391</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0850-05.2005</pub-id></element-citation></ref><ref id="R20"><label>[20]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pollack</surname><given-names>K</given-names></name><name><surname>Kreuzer</surname><given-names>W</given-names></name><name><surname>Majdak</surname><given-names>P</given-names></name></person-group><chapter-title>Perspective Chapter: Modern Acquisition of Personalised Head-Related Transfer Functions – An Overview</chapter-title><person-group person-group-type="editor"><name><surname>Katz</surname><given-names>BFG</given-names></name><name><surname>Majdak</surname><given-names>P</given-names></name></person-group><source>Advances in Fundamental and Applied Research on Spatial Audio</source><publisher-name>IntechOpen</publisher-name><publisher-loc>Rijeka</publisher-loc><year>2022</year><comment>section: 2</comment><pub-id pub-id-type="doi">10.5772/intechopen.102908</pub-id></element-citation></ref><ref id="R21"><label>[21]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kumpik</surname><given-names>DP</given-names></name><name><surname>Kacelnik</surname><given-names>O</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name></person-group><chapter-title>Adaptive Reweighting of Auditory Localization Cues in Response to Chronic Unilateral Earplugging in Humans</chapter-title><source>Journal of Neuroscience</source><publisher-name>Society for Neuroscience</publisher-name><year>2010</year><volume>30</volume><issue>14</issue><fpage>4883</fpage><lpage>4894</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.jneurosci.org/content/30/14/4883">https://www.jneurosci.org/content/30/14/4883</ext-link>, eprint:<ext-link ext-link-type="uri" xlink:href="https://www.jneurosci.org/content/30/14/4883.full.pdf">https://www.jneurosci.org/content/30/14/4883.full.pdf</ext-link></comment><pub-id pub-id-type="pmcid">PMC4225134</pub-id><pub-id pub-id-type="pmid">20371808</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5488-09.2010</pub-id></element-citation></ref><ref id="R22"><label>[22]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wightman</surname><given-names>FL</given-names></name><name><surname>Kistler</surname><given-names>DJ</given-names></name></person-group><article-title>Monaural sound localization revisited</article-title><source>The Journal of the Acoustical Society of America</source><year>1997</year><volume>101</volume><issue>2</issue><fpage>1050</fpage><lpage>1063</lpage><pub-id pub-id-type="pmid">9035397</pub-id></element-citation></ref><ref id="R23"><label>[23]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stevenson-Hoare</surname><given-names>JO</given-names></name><name><surname>Freeman</surname><given-names>TCA</given-names></name><name><surname>Culling</surname><given-names>JF</given-names></name></person-group><article-title>The pinna enhances angular discrimination in the frontal hemifield</article-title><source>The Journal of the Acoustical Society of America</source><year>2022</year><volume>152</volume><issue>4</issue><fpage>2140</fpage><lpage>2149</lpage><pub-id pub-id-type="pmid">36319254</pub-id></element-citation></ref><ref id="R24"><label>[24]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barumerli</surname><given-names>R</given-names></name><name><surname>Majdak</surname><given-names>P</given-names></name><name><surname>Baumgartner</surname><given-names>R</given-names></name><name><surname>Geronazzo</surname><given-names>M</given-names></name><name><surname>Avanzini</surname><given-names>F</given-names></name></person-group><article-title>Evaluation of a human sound localization model based on bayesian inference</article-title><source>Forum Acusticum</source><year>2020</year></element-citation></ref><ref id="R25"><label>[25]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>WJ</given-names></name></person-group><article-title>Bayesian Decision Models: A Primer</article-title><source>Neuron</source><year>2019</year><volume>104</volume><issue>1</issue><fpage>164</fpage><lpage>175</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.cell.com/neuron/fulltext/S0896-6273(19)30840-2?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0896627319308402%3Fshowall%3Dtrue">linkinghub.elsevier.com/retrieve/pii/S0896627319308402</ext-link></comment><pub-id pub-id-type="pmid">31600512</pub-id></element-citation></ref><ref id="R26"><label>[26]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ege</surname><given-names>R</given-names></name><name><surname>Van Opstal</surname><given-names>AJ</given-names></name><name><surname>Van Wanrooij</surname><given-names>MM</given-names></name></person-group><article-title>Perceived Target Range Shapes Human SoundLocalization Behavior</article-title><source>eneuro</source><year>2019</year><volume>6</volume><issue>2</issue><elocation-id>ENEURO.0111-18.2019</elocation-id><pub-id pub-id-type="pmcid">PMC6451157</pub-id><pub-id pub-id-type="pmid">30963103</pub-id><pub-id pub-id-type="doi">10.1523/ENEURO.0111-18.2019</pub-id></element-citation></ref><ref id="R27"><label>[27]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krishnamurthy</surname><given-names>K</given-names></name><name><surname>Nassar</surname><given-names>MR</given-names></name><name><surname>Sarode</surname><given-names>S</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name></person-group><article-title>Arousal-related adjustments of perceptual biases optimize perception in dynamic environments</article-title><source>Nature human behaviour</source><year>2017</year><volume>1</volume><issue>6</issue><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="pmcid">PMC5638136</pub-id><pub-id pub-id-type="pmid">29034334</pub-id><pub-id pub-id-type="doi">10.1038/s41562-017-0107</pub-id></element-citation></ref><ref id="R28"><label>[28]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Majdak</surname><given-names>P</given-names></name><name><surname>Goupell</surname><given-names>MJ</given-names></name><name><surname>Laback</surname><given-names>B</given-names></name></person-group><article-title>3-d localization of virtual sound sources: Effects of visual environment, pointing method, and training</article-title><source>Attention, Perception, &amp; Psychophysics</source><year>2010</year><volume>72</volume><issue>2</issue><fpage>454</fpage><lpage>469</lpage><pub-id pub-id-type="pmcid">PMC2885955</pub-id><pub-id pub-id-type="pmid">20139459</pub-id><pub-id pub-id-type="doi">10.3758/APP.72.2.454</pub-id></element-citation></ref><ref id="R29"><label>[29]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Middlebrooks</surname><given-names>JC</given-names></name></person-group><article-title>Virtual localization improved by scaling nonindividualized external-ear transfer functions in frequency</article-title><source>The Journal of the Acoustical Society of America</source><year>1999</year><volume>106</volume><issue>3</issue><fpage>1493</fpage><lpage>1510</lpage><pub-id pub-id-type="pmid">10489706</pub-id></element-citation></ref><ref id="R30"><label>[30]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Macpherson</surname><given-names>EA</given-names></name><name><surname>Middlebrooks</surname><given-names>JC</given-names></name></person-group><article-title>Verticalplane sound localization probed with ripple-spectrum noise</article-title><source>The Journal of the Acoustical Society of America</source><year>2003</year><volume>114</volume><issue>1</issue><fpage>430</fpage><lpage>445</lpage><pub-id pub-id-type="pmid">12880054</pub-id></element-citation></ref><ref id="R31"><label>[31]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kistler</surname><given-names>DJ</given-names></name><name><surname>Wightman</surname><given-names>FL</given-names></name></person-group><article-title>A model of head-related transfer functions based on principal components analysis and minimum-phase reconstruction</article-title><source>The Journal of the Acoustical Society of America</source><year>1992</year><volume>91</volume><issue>3</issue><fpage>1637</fpage><lpage>1647</lpage><pub-id pub-id-type="pmid">1564200</pub-id></element-citation></ref><ref id="R32"><label>[32]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andreopoulou</surname><given-names>A</given-names></name><name><surname>Katz</surname><given-names>BF</given-names></name></person-group><article-title>Identification of perceptually relevant methods of inter-aural time difference estimation</article-title><source>The Journal of the Acoustical Society of America</source><year>2017</year><volume>142</volume><issue>2</issue><fpage>588</fpage><lpage>598</lpage><pub-id pub-id-type="pmid">28863557</pub-id></element-citation></ref><ref id="R33"><label>[33]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mossop</surname><given-names>JE</given-names></name><name><surname>Culling</surname><given-names>JF</given-names></name></person-group><article-title>Lateralization of large interaural delays</article-title><source>The Journal of the Acoustical Society of America</source><year>1998</year><volume>104</volume><issue>3</issue><fpage>1574</fpage><lpage>1579</lpage><pub-id pub-id-type="pmid">9745740</pub-id></element-citation></ref><ref id="R34"><label>[34]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glasberg</surname><given-names>BR</given-names></name><name><surname>Moore</surname><given-names>BC</given-names></name></person-group><article-title>Derivation of auditory filter shapes from notched-noise data</article-title><source>Hearing research</source><year>1990</year><volume>47</volume><issue>1-2</issue><fpage>103</fpage><lpage>138</lpage><pub-id pub-id-type="pmid">2228789</pub-id></element-citation></ref><ref id="R35"><label>[35]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saremi</surname><given-names>A</given-names></name><name><surname>Beutelmann</surname><given-names>R</given-names></name><name><surname>Dietz</surname><given-names>M</given-names></name><name><surname>Ashida</surname><given-names>G</given-names></name><name><surname>Kretzberg</surname><given-names>J</given-names></name><name><surname>Verhulst</surname><given-names>S</given-names></name></person-group><article-title>A comparative study of seven human cochlear filter models</article-title><source>The Journal of the Acoustical Society of America</source><year>2016</year><volume>140</volume><issue>3</issue><fpage>1618</fpage><lpage>1634</lpage><pub-id pub-id-type="pmid">27914400</pub-id></element-citation></ref><ref id="R36"><label>[36]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Algazi</surname><given-names>VR</given-names></name><name><surname>Avendano</surname><given-names>C</given-names></name><name><surname>Duda</surname><given-names>RO</given-names></name></person-group><article-title>Elevation localization and head-related transfer function analysis at low frequencies</article-title><source>The Journal of the Acoustical Society of America</source><year>2001</year><volume>109</volume><issue>3</issue><fpage>1110</fpage><lpage>1122</lpage><pub-id pub-id-type="pmid">11303925</pub-id></element-citation></ref><ref id="R37"><label>[37]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebrank</surname><given-names>J</given-names></name><name><surname>Wright</surname><given-names>D</given-names></name></person-group><article-title>Spectral cues used in the localization of sound sources on the median plane</article-title><source>The Journal of the Acoustical Society of America</source><year>1974</year><volume>56</volume><issue>6</issue><fpage>1829</fpage><lpage>1834</lpage><pub-id pub-id-type="pmid">4443482</pub-id></element-citation></ref><ref id="R38"><label>[38]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dietz</surname><given-names>M</given-names></name><name><surname>Ewert</surname><given-names>SD</given-names></name><name><surname>Hohmann</surname><given-names>V</given-names></name></person-group><article-title>Auditory model based direction estimation of concurrent speakers from binaural signals</article-title><source>Speech Communication</source><year>2011</year><volume>53</volume><issue>5</issue><fpage>592</fpage><lpage>605</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S016763931000097X">https://www.sciencedirect.com/science/article/pii/S016763931000097X</ext-link></comment><pub-id pub-id-type="doi">10.1016/j.specom.2010.05.006</pub-id></element-citation></ref><ref id="R39"><label>[39]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roman</surname><given-names>N</given-names></name><name><surname>Wang</surname><given-names>D</given-names></name><name><surname>Brown</surname><given-names>GJ</given-names></name></person-group><article-title>Speech segregation based on sound localization</article-title><source>J Acoust Soc Am</source><year>2003</year><volume>114</volume><issue>4</issue><fpage>18</fpage><pub-id pub-id-type="pmid">14587621</pub-id></element-citation></ref><ref id="R40"><label>[40]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zotkin</surname><given-names>DN</given-names></name><name><surname>Duraiswami</surname><given-names>R</given-names></name><name><surname>Gumerov</surname><given-names>NA</given-names></name></person-group><source>Regularized HRTF fitting using spherical harmonics</source><conf-name>2009 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics</conf-name><conf-loc>New Paltz, NY, USA</conf-loc><conf-sponsor>IEEE</conf-sponsor><year>2009</year><fpage>257</fpage><lpage>260</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://ieeexplore.ieee.org/document/5346521">ieeexplore.ieee.org/document/5346521/</ext-link></comment><pub-id pub-id-type="doi">10.1109/ASPAA.2009.5346521</pub-id></element-citation></ref><ref id="R41"><label>[41]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlile</surname><given-names>S</given-names></name><name><surname>Leong</surname><given-names>P</given-names></name><name><surname>Hyams</surname><given-names>S</given-names></name></person-group><article-title>The nature and distribution of errors in sound localization by human listeners</article-title><source>Hearing Research</source><year>1997</year><volume>114</volume><issue>1</issue><fpage>179</fpage><lpage>196</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/abs/pii/S0378595597001615?via%3Dihub">linkinghub.elsevier.com/retrieve/pii/S0378595597001615</ext-link></comment><pub-id pub-id-type="pmid">9447931</pub-id></element-citation></ref><ref id="R42"><label>[42]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Majdak</surname><given-names>P</given-names></name><name><surname>Goupell</surname><given-names>MJ</given-names></name><name><surname>Laback</surname><given-names>B</given-names></name></person-group><article-title>Twodimensional sound localization in cochlear implantees</article-title><source>Ear and hearing</source><year>2011</year><volume>32</volume><issue>2</issue><fpage>198</fpage><lpage>208</lpage><pub-id pub-id-type="pmcid">PMC5714263</pub-id><pub-id pub-id-type="pmid">21052005</pub-id><pub-id pub-id-type="doi">10.1097/AUD.0b013e3181f4dfe9</pub-id></element-citation></ref><ref id="R43"><label>[43]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Studebaker</surname><given-names>GA</given-names></name></person-group><article-title>A ”rationalized” arcsine transform</article-title><source>Journal of Speech, Language, and Hearing Research</source><year>1985</year><volume>28</volume><issue>3</issue><fpage>455</fpage><lpage>462</lpage><pub-id pub-id-type="pmid">4046587</pub-id></element-citation></ref><ref id="R44"><label>[44]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yost</surname><given-names>WA</given-names></name><name><surname>Dye</surname><given-names>RH</given-names></name></person-group><article-title>Discrimination of interaural differences of level as a function of frequency</article-title><source>The Journal of the Acoustical Society of America</source><year>1988</year><volume>83</volume><issue>5</issue><fpage>1846</fpage><lpage>1851</lpage><pub-id pub-id-type="pmid">3403800</pub-id></element-citation></ref><ref id="R45"><label>[45]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Barumerli</surname><given-names>R</given-names></name><name><surname>Geronazzo</surname><given-names>M</given-names></name><name><surname>Avanzini</surname><given-names>F</given-names></name></person-group><source>Localization in Elevation with Non-Individual Head-Related Transfer Functions: Comparing Predictions of Two Auditory Models</source><conf-name>2018 26th European Signal Processing Conference (EUSIPCO)</conf-name><year>2018</year><fpage>2539</fpage><lpage>2543</lpage><comment>iSSN: 2076-1465</comment><pub-id pub-id-type="doi">10.23919/EUSIPCO.2018.8553320</pub-id></element-citation></ref><ref id="R46"><label>[46]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marelli</surname><given-names>D</given-names></name><name><surname>Baumgartner</surname><given-names>R</given-names></name><name><surname>Majdak</surname><given-names>P</given-names></name></person-group><article-title>Efficient approximation of head-related transfer functions in subbands for accurate sound localization</article-title><source>IEEE/ACM Transactions on Audio, Speech, and Language Processing</source><year>2015</year><volume>23</volume><issue>7</issue><fpage>1130</fpage><lpage>1143</lpage><pub-id pub-id-type="pmcid">PMC4678625</pub-id><pub-id pub-id-type="pmid">26681930</pub-id></element-citation></ref><ref id="R47"><label>[47]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barumerli</surname><given-names>R</given-names></name><name><surname>Majdak</surname><given-names>P</given-names></name><name><surname>Reijniers</surname><given-names>J</given-names></name><name><surname>Baumgartner</surname><given-names>R</given-names></name><name><surname>Geronazzo</surname><given-names>M</given-names></name><name><surname>Avanzini</surname><given-names>F</given-names></name></person-group><article-title>Predicting Directional Sound-Localization of Human Listeners in both Horizontal and Vertical Dimensions</article-title><source>Audio Engineering Society Convention</source><year>2020</year><volume>148</volume><comment><ext-link ext-link-type="uri" xlink:href="https://www.aes.org/e-lib/browse.cfm?elib=20777">www.aes.org/e-lib/browse.cfm?elib=20777</ext-link></comment></element-citation></ref><ref id="R48"><label>[48]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Blauert</surname><given-names>J</given-names></name></person-group><chapter-title>Spatial hearing</chapter-title><source>The Psychophysics of Human Sound Localization, revised ed</source><publisher-name>The MIT Press</publisher-name><publisher-loc>Cambridge, MA</publisher-loc><year>1997</year></element-citation></ref><ref id="R49"><label>[49]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ege</surname><given-names>R</given-names></name><name><surname>Opstal</surname><given-names>AJv</given-names></name><name><surname>Bremen</surname><given-names>P</given-names></name><name><surname>Wanrooij</surname><given-names>MMv</given-names></name></person-group><article-title>Testing the Precedence Effect in the Median Plane Reveals Backward Spatial Masking of Sound</article-title><source>Scientific Reports</source><year>2018</year><volume>8</volume><issue>1</issue><elocation-id>8670</elocation-id><comment><ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41598-018-26834-2">https://www.nature.com/articles/s41598-018-26834-2</ext-link></comment><pub-id pub-id-type="pmcid">PMC5989261</pub-id><pub-id pub-id-type="pmid">29875363</pub-id><pub-id pub-id-type="doi">10.1038/s41598-018-26834-2</pub-id></element-citation></ref><ref id="R50"><label>[50]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geronazzo</surname><given-names>M</given-names></name><name><surname>Spagnol</surname><given-names>S</given-names></name><name><surname>Avanzini</surname><given-names>F</given-names></name></person-group><article-title>Do we need individual head-related transfer functions for vertical localization? The case study of a spectral notch distance metric</article-title><source>IEEE/ACM Transactions on Audio, Speech, and Language Processing</source><year>2018</year><volume>26</volume><issue>7</issue><fpage>1243</fpage><lpage>1256</lpage><pub-id pub-id-type="doi">10.1109/TASLP.2018.2821846</pub-id></element-citation></ref><ref id="R51"><label>[51]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gaissmaier</surname><given-names>W</given-names></name><name><surname>Schooler</surname><given-names>LJ</given-names></name></person-group><article-title>The smart potential behind probability matching</article-title><source>Cognition</source><year>2008</year><volume>109</volume><issue>3</issue><fpage>416</fpage><lpage>422</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0010027708002151">https://linkinghub.elsevier.com/retrieve/pii/S0010027708002151</ext-link></comment><pub-id pub-id-type="pmid">19019351</pub-id></element-citation></ref><ref id="R52"><label>[52]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>And’eol</surname><given-names>G</given-names></name><name><surname>Macpherson</surname><given-names>EA</given-names></name><name><surname>Sabin</surname><given-names>AT</given-names></name></person-group><article-title>Sound localization in noise and sensitivity to spectral shape</article-title><source>Hearing Research</source><year>2013</year><volume>304</volume><fpage>20</fpage><lpage>27</lpage><comment><ext-link ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/S0378595513001445">http://www.sciencedirect.com/science/article/pii/S0378595513001445</ext-link></comment><pub-id pub-id-type="pmid">23769958</pub-id></element-citation></ref><ref id="R53"><label>[53]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parise</surname><given-names>CV</given-names></name><name><surname>Knorre</surname><given-names>K</given-names></name><name><surname>Ernst</surname><given-names>MO</given-names></name></person-group><article-title>Natural auditory scene statistics shapes human spatial hearing</article-title><source>Proceedings of the National Academy of Sciences</source><year>2014</year><volume>111</volume><issue>16</issue><fpage>6104</fpage><lpage>6108</lpage><pub-id pub-id-type="pmcid">PMC4000839</pub-id><pub-id pub-id-type="pmid">24711409</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1322705111</pub-id></element-citation></ref><ref id="R54"><label>[54]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischer</surname><given-names>BJ</given-names></name><name><surname>Peña</surname><given-names>JL</given-names></name></person-group><article-title>Owl’s behavior and neural representation predicted by Bayesian inference</article-title><source>Nature Neuroscience</source><year>2011</year><volume>14</volume><issue>8</issue><fpage>1061</fpage><lpage>1066</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/nn.2872">www.nature.com/articles/nn.2872</ext-link></comment><pub-id pub-id-type="pmcid">PMC3145020</pub-id><pub-id pub-id-type="pmid">21725311</pub-id><pub-id pub-id-type="doi">10.1038/nn.2872</pub-id></element-citation></ref><ref id="R55"><label>[55]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skerritt-Davis</surname><given-names>B</given-names></name><name><surname>Elhilali</surname><given-names>M</given-names></name></person-group><article-title>Detecting change in stochastic sound sequences</article-title><source>PLoS computational biology</source><year>2018</year><volume>14</volume><issue>5</issue><elocation-id>e1006162</elocation-id><pub-id pub-id-type="pmcid">PMC5993325</pub-id><pub-id pub-id-type="pmid">29813049</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006162</pub-id></element-citation></ref><ref id="R56"><label>[56]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Odegaard</surname><given-names>B</given-names></name><name><surname>Beierholm</surname><given-names>UR</given-names></name><name><surname>Carpenter</surname><given-names>J</given-names></name><name><surname>Shams</surname><given-names>L</given-names></name></person-group><article-title>Prior expectation of objects in space is dependent on the direction of gaze</article-title><source>Cognition</source><year>2019</year><volume>182</volume><fpage>220</fpage><lpage>226</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/abs/pii/S0010027718302695?via%3Dihub">linkinghub.elsevier.com/retrieve/pii/S0010027718302695</ext-link></comment><pub-id pub-id-type="pmid">30359823</pub-id></element-citation></ref><ref id="R57"><label>[57]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaya</surname><given-names>EM</given-names></name><name><surname>Elhilali</surname><given-names>M</given-names></name></person-group><article-title>Modelling auditory attention</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><year>2017</year><volume>372</volume><issue>1714</issue><elocation-id>20160101</elocation-id><pub-id pub-id-type="pmcid">PMC5206269</pub-id><pub-id pub-id-type="pmid">28044012</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2016.0101</pub-id></element-citation></ref><ref id="R58"><label>[58]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dietz</surname><given-names>M</given-names></name><name><surname>Marquardt</surname><given-names>T</given-names></name><name><surname>Salminen</surname><given-names>NH</given-names></name><name><surname>McAlpine</surname><given-names>D</given-names></name></person-group><article-title>Emphasis of spatial cues in the temporal fine structure during the rising segments of amplitude-modulated sounds</article-title><source>Proceedings of the National Academy of Sciences</source><year>2013</year><volume>110</volume><issue>37</issue><fpage>15151</fpage><lpage>15156</lpage><pub-id pub-id-type="pmcid">PMC3773782</pub-id><pub-id pub-id-type="pmid">23980161</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1309712110</pub-id></element-citation></ref><ref id="R59"><label>[59]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hambrook</surname><given-names>DA</given-names></name><name><surname>Ilievski</surname><given-names>M</given-names></name><name><surname>Mosadeghzad</surname><given-names>M</given-names></name><name><surname>Tata</surname><given-names>M</given-names></name></person-group><article-title>A Bayesian computational basis for auditory selective attention using head rotation and the interaural time-difference cue</article-title><source>PLOS ONE</source><year>2017</year><volume>12</volume><issue>10</issue><elocation-id>e0186104</elocation-id><comment><ext-link ext-link-type="uri" xlink:href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0186104">journals.plos.org/plosone/article?id=10.1371/journal.pone.0186104</ext-link></comment><pub-id pub-id-type="pmcid">PMC5629026</pub-id><pub-id pub-id-type="pmid">28982139</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0186104</pub-id></element-citation></ref><ref id="R60"><label>[60]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ward</surname><given-names>D</given-names></name><name><surname>Lehmann</surname><given-names>E</given-names></name><name><surname>Williamson</surname><given-names>R</given-names></name></person-group><article-title>Particle filtering algorithms for tracking an acoustic source in a reverberant environment</article-title><source>IEEE Transactions on Speech and Audio Processing</source><year>2003</year><volume>11</volume><issue>6</issue><fpage>826</fpage><lpage>836</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://ieeexplore.ieee.org/document/1255469">ieeexplore.ieee.org/document/1255469/</ext-link></comment><pub-id pub-id-type="doi">10.1109/TSA.2003.818112</pub-id></element-citation></ref><ref id="R61"><label>[61]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Majdak</surname><given-names>Piotr</given-names></name><name><surname>Hollomey</surname><given-names>Clara</given-names></name><name><surname>Baumgartner</surname><given-names>Robert</given-names></name></person-group><article-title>Amt 1.x: A toolbox for reproducible research in auditory modeling</article-title><source>Acta Acust</source><year>2022</year><volume>6</volume><fpage>19</fpage><pub-id pub-id-type="doi">10.1051/aacus/2022011</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Fig. 1</label><caption><title>Model structure.</title><p>Gray blocks: Model’s processing pipeline consisting of 1) the <italic>feature extraction</italic> stage to compute spatial features from the binaural sound; 2) the <italic>Bayesian inference</italic> stage integrating the sensory evidence obtained by comparison with feature templates and with prior beliefs in order to estimate the most probable direction; and 3) the <italic>response stage</italic> transforming the internal estimate to the final localization response. White blocks: Elements required to fit the model to an individual subject consisting of listener performances in estimating sound direction and individual HRTF dataset.</p></caption><graphic xlink:href="EMS156219-f001"/></fig><fig id="F2" position="float"><label>Fig. 2</label><caption><p>Interaural features as functions of lateral angle in the horizontal frontal plane (<italic>ϕ</italic> = 0°). Left axis (blue solid line): Transformed ITD <italic>x<sub>itd</sub></italic> (dimensionless), see <xref ref-type="disp-formula" rid="FD2">Eq. 2</xref>. Right axis (green dashed line): ILD (in dB) obtained from the magnitude profiles. Example for subject NH12 [<xref ref-type="bibr" rid="R28">28</xref>].</p></caption><graphic xlink:href="EMS156219-f002"/></fig><fig id="F3" position="float"><label>Fig. 3</label><caption><p>Monaural spectral features as a function of polar angle in the median plane (<italic>θ</italic> = 0°). Top: Features obtained from the magnitude profiles. Bottom: Features obtained from the gradient profiles. Example for the left ear of subject NH12 [<xref ref-type="bibr" rid="R28">28</xref>].</p></caption><graphic xlink:href="EMS156219-f003"/></fig><fig id="F4" position="float"><label>Fig. 4</label><caption><title>Example of the model estimating the direction of a broadband sound source.</title><p>Red: Actual direction of the sound source. The grayscale represents the posterior probability distribution <italic>p</italic>(<italic><bold>φ</bold></italic>|<italic><bold>t</bold></italic>), shown, in order to increase the readability, on a logarithmic scale. Green: Direction inferred by the Bayesian-inference stage (without the response stage). Orange: Direction inferred by the model (with the response stage). Blue: Actual response of the subject.</p></caption><graphic xlink:href="EMS156219-f004"/></fig><fig id="F5" position="float"><label>Fig. 5</label><caption><title>Sound-localization performance as functions of sound-source direction.</title><p>Open symbols: Predictions obtained by the two model’s variants based on either spectral magnitude profiles (MPs) or gradient profiles (GPs). Filled grey symbol: Actual data from [<xref ref-type="bibr" rid="R28">28</xref>]. Top row: Lateral error, calculated for all targets with lateral angles of −65° ± 25°, −20° ± 20°, 20° ± 20°, and 65° ± 25°. Center and bottom rows: Polar error and quadrant error rates, respectively, calculated for all median-plane (±30°) targets with polar angles of 0° ± 30°, 90° ± 60°, and 180° ± 40°.</p></caption><graphic xlink:href="EMS156219-f005"/></fig><fig id="F6" position="float"><label>Fig. 6</label><caption><p>Effects of likelihood, prior, and response noise on predicted response patterns as a result of modeling the directional localization of broadband noise bursts. a) Likelihood obtained by sensory evidence (i.e, no spatial prior and no response noise). b) Bayesian inference (with the spatial prior but no response noise). c) Full model (with prior and response noise). Gray: actual data of NH16 from [<xref ref-type="bibr" rid="R28">28</xref>]. Black: estimation obtained by the model considering spectral gradient profiles (GPs). Red cross: frontal position. Blue dashed lines separate regions of front-back confusions.</p></caption><graphic xlink:href="EMS156219-f006"/></fig><fig id="F7" position="float"><label>Fig. 7</label><caption><p>Localization performance with individual (<italic>Own</italic>) and non-individual (<italic>Other</italic>) HRTFs. Actual data from [<xref ref-type="bibr" rid="R29">29</xref>] (<monospace>data_middlebrooks1999</monospace>). Model predictions for two model variants: spectral magnitude profiles (MPs) and spectral gradient profiles (GPs). As references, predictions by the models <monospace>reijniers2014</monospace> [<xref ref-type="bibr" rid="R6">6</xref>] and <monospace>baumgartner2014</monospace> [<xref ref-type="bibr" rid="R15">15</xref>] are shown. Note that <monospace>baumgartner2014</monospace> does not predict the lateral error.</p></caption><graphic xlink:href="EMS156219-f007"/></fig><fig id="F8" position="float"><label>Fig. 8</label><caption><title>Effect of spectral ripples in the source spectrum on sound localization performance in the median plane.</title><p>Right-most bottom panel: localization error rates obtained without spectral ripples serving as reference. Top and bottom left panels: Differences to the reference condition shown in the right-most bottom panel. In addition to predictions from the two model variants (MP and GP), predictions from <monospace>reijniers2014</monospace> [<xref ref-type="bibr" rid="R6">6</xref>] and <monospace>baumgartner2014</monospace> [<xref ref-type="bibr" rid="R15">15</xref>] as well as actual data from [<xref ref-type="bibr" rid="R30">30</xref>] (<monospace>data_macpherson2003</monospace>) as shown. Note that ripple densities were varied at a fixed ripple depth of 40 dB and ripple depths were varied at a fixed ripple density of one ripple per octave.</p></caption><graphic xlink:href="EMS156219-f008"/></fig><table-wrap id="T1" position="float" orientation="portrait"><label>Table I</label><caption><title>Parameters estimated to fit the models to actual subjects’ performance [<xref ref-type="bibr" rid="R28">28</xref>] for both model variants where either magnitude profiles (MPs) or gradient profiles (GPs) were the monaural spectral features.</title></caption><table frame="void" rules="groups"><thead><tr><th align="left" valign="middle">Variant</th><th align="left" valign="middle">Subject</th><th align="left" valign="middle"><italic>σ<sub>P,ϵ</sub></italic> [deg]</th><th align="left" valign="middle"><italic>σ<sub>ild</sub></italic> [dB]</th><th align="left" valign="middle"><italic>σ<sub>mon</sub></italic> [deg]</th><th align="left" valign="middle"><italic>σ<sub>m</sub></italic> [deg]</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="5">MP</td><td align="left" valign="middle">NH12</td><td align="left" valign="middle">11.50</td><td align="left" valign="middle">0.50</td><td align="left" valign="middle">3.40</td><td align="left" valign="middle">8.50</td></tr><tr><td align="left" valign="middle">NH15</td><td align="left" valign="middle">10.00</td><td align="left" valign="middle">0.50</td><td align="left" valign="middle">3.20</td><td align="left" valign="middle">14.27</td></tr><tr><td align="left" valign="middle">NH16</td><td align="left" valign="middle">11.50</td><td align="left" valign="middle">1.00</td><td align="left" valign="middle">3.60</td><td align="left" valign="middle">11.00</td></tr><tr><td align="left" valign="middle">NH17</td><td align="left" valign="middle">11.50</td><td align="left" valign="middle">0.50</td><td align="left" valign="middle">4.10</td><td align="left" valign="middle">14.30</td></tr><tr><td align="left" valign="middle">NH18</td><td align="left" valign="middle">11.50</td><td align="left" valign="middle">1.00</td><td align="left" valign="middle">6.50</td><td align="left" valign="middle">14.00</td></tr><tr><td colspan="6" align="left" valign="top" rowspan="1"><hr/></td></tr><tr><td align="left" valign="middle" rowspan="5">GP</td><td align="left" valign="middle">NH12</td><td align="left" valign="middle">11.50</td><td align="left" valign="middle">0.50</td><td align="left" valign="middle">1.10</td><td align="left" valign="middle">8.50</td></tr><tr><td align="left" valign="middle">NH15</td><td align="left" valign="middle">11.00</td><td align="left" valign="middle">0.50</td><td align="left" valign="middle">1.25</td><td align="left" valign="middle">14.30</td></tr><tr><td align="left" valign="middle">NH16</td><td align="left" valign="middle">11.50</td><td align="left" valign="middle">1.00</td><td align="left" valign="middle">1.25</td><td align="left" valign="middle">11.50</td></tr><tr><td align="left" valign="middle">NH17</td><td align="left" valign="middle">11.50</td><td align="left" valign="middle">1.00</td><td align="left" valign="middle">1.60</td><td align="left" valign="middle">14.00</td></tr><tr><td align="left" valign="middle">NH18</td><td align="left" valign="middle">11.50</td><td align="left" valign="middle">1.00</td><td align="left" valign="middle">2.10</td><td align="left" valign="middle">15.00</td></tr></tbody></table></table-wrap><table-wrap id="T2" position="float" orientation="portrait"><label>Table II</label><caption><title>Predicted performance metrics averaged across all subjects and directions (±1 standard deviation across subjects) for both model variants. Actual data from [<xref ref-type="bibr" rid="R28">28</xref>].</title></caption><table frame="void" rules="groups"><thead><tr><th align="center" valign="middle" rowspan="2">Metric</th><th align="center" valign="middle" rowspan="2">Actual</th><th align="center" valign="middle" colspan="2">Predicted</th></tr><tr><th align="center" valign="middle">MP</th><th align="center" valign="middle">GP</th></tr></thead><tbody><tr><td align="left" valign="middle">LE [deg]</td><td align="left" valign="middle">12.25 ± 2.43</td><td align="left" valign="middle">12.97 ± 2.50</td><td align="left" valign="middle">13.18 ± 2.66</td></tr><tr><td align="left" valign="middle">PE [deg]</td><td align="left" valign="middle">32.73 ± 3.44</td><td align="left" valign="middle">31.20 ± 4.04</td><td align="left" valign="middle">29.78 ± 4.01</td></tr><tr><td align="left" valign="middle">QE [%]</td><td align="left" valign="middle">7.83 ± 7.11</td><td align="left" valign="middle">8.32 ± 5.75</td><td align="left" valign="middle">9.80 ± 5.23</td></tr></tbody></table></table-wrap></floats-group></article>