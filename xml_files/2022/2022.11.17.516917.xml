<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS157281</article-id><article-id pub-id-type="doi">10.1101/2022.11.17.516917</article-id><article-id pub-id-type="archive">PPR572454</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">2</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Geometry of visual working memory information in human gaze patterns</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Linde-Domingo</surname><given-names>Juan</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref><xref ref-type="aff" rid="A5">5</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Spitzer</surname><given-names>Bernhard</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>Research Group Adaptive Memory and Decision Making, Max Planck Institute for Human Development, Berlin, Germany</aff><aff id="A2"><label>2</label>Center for Adaptive Rationality, Max Planck Institute for Human Development, Berlin, Germany</aff><aff id="A3"><label>3</label>Max Planck Dahlem Campus of Cognition, Max Planck Institute for Human Development, Berlin, Germany</aff><aff id="A4"><label>4</label>Mind, Brain and Behavior Research Center, University of Granada, Spain</aff><aff id="A5"><label>5</label>Department of Experimental Psychology, University of Granada, Spain</aff><author-notes><corresp id="CR1"><label>*</label>Correspondence to: Juan Linde-Domingo (<email>lindedomingo@mpib-berlin.mpg.de</email>) or Bernhard Spitzer (<email>spitzer@mpib-berlin-mpg.de</email>)</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>18</day><month>11</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>17</day><month>11</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Stimulus-dependent eye movements have been recognized as a potential confound in decoding visual working memory information from neural signals. Here, we combined eye-tracking with representational geometry analyses to uncover the very information in miniature gaze patterns while participants (n = 41) were cued to maintain visual object orientations. Although participants were discouraged from breaking fixation via real-time feedback, small gaze shifts (&lt; 1 degree) robustly encoded the to-be-maintained stimulus orientation, with evidence for encoding two sequentially presented orientations at the same time. While the orientation encoding upon stimulus presentation was object-specific, it changed to a more object-independent format during cued maintenance, particularly when attention had been temporarily withdrawn from the memorandum. Finally, categorical reporting biases increased after unattended storage, with indications of biased gaze geometries emerging already during the maintenance periods prior to behavioral reporting. These findings disclose a wealth of information in gaze patterns during visuospatial working memory, and suggest systematic changes in representational format when memory contents have been unattended.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Working Memory (WM) enables observers to actively keep stimulus information ‚Äúon the mind‚Äù for upcoming tasks. A key question in understanding WM function is which aspects of a task-relevant stimulus it retains, and in which format(s). Upon removal of a stimulus from sight, sensory systems briefly retain a detailed sensory memory of the just-removed (e.g., visual) input. Sans active maintenance, these rich ‚Äúphotographic‚Äù memories decay rapidly within a few hundreds of milliseconds (<xref ref-type="bibr" rid="R63">Sperling, 1960</xref>; but see e.g. <xref ref-type="bibr" rid="R10">Brady et al., 2008</xref>). It is widely assumed that only a limited amount of information can be accurately maintained in WM (<xref ref-type="bibr" rid="R7">Bays et al., 2009</xref>; <xref ref-type="bibr" rid="R17">Cowan, 2001</xref>; <xref ref-type="bibr" rid="R42">Luck &amp; Vogel, 1997</xref>; <xref ref-type="bibr" rid="R43">Ma et al., 2014</xref>). However, despite intense research, the very nature of the information that WM maintains remains poorly understood.</p><p id="P3">In neuroscientific experiments examining the representation of visual WM information in the brain, often only a single stimulus feature needs to be reported after a delay (e.g., the orientation of a visual grating; <xref ref-type="bibr" rid="R13">Christophel et al., 2018</xref>; <xref ref-type="bibr" rid="R30">Harrison &amp; Tong, 2009</xref>; <xref ref-type="bibr" rid="R53">Rademaker et al., 2019</xref>; <xref ref-type="bibr" rid="R61">Serences et al., 2009</xref>). At one extreme, such tasks could be solved by sustaining a concrete visual memory of the stimulus and its visual details. Various human neuroimaging studies have shown that WM contents can be decoded from early visual cortices (<xref ref-type="bibr" rid="R30">Harrison &amp; Tong, 2009</xref>; <xref ref-type="bibr" rid="R56">Riggall &amp; Postle, 2012</xref>; <xref ref-type="bibr" rid="R61">Serences et al., 2009</xref>), which seems consistent with storage in a sensory format (but see <xref ref-type="bibr" rid="R36">Kwak &amp; Curtis, 2022</xref>). At the other extreme, many WM tasks can also be solved with a high-level abstraction of the task-relevant stimulus parameter only, such as its orientation, speed, or color (e.g., <xref ref-type="bibr" rid="R36">Kwak &amp; Curtis, 2022</xref>; <xref ref-type="bibr" rid="R58">Romo et al., 1999</xref>; <xref ref-type="bibr" rid="R65">Spitzer &amp; Blankenburg, 2012</xref>; <xref ref-type="bibr" rid="R72">Vergara et al., 2016</xref>). Such abstractions may also be recoded into pre-existing categories (such as ‚Äúleft‚Äù, ‚Äúslow‚Äù, or ‚Äúgreen‚Äù; e,g. <xref ref-type="bibr" rid="R4">Bae et al., 2015</xref>; <xref ref-type="bibr" rid="R29">Hardman et al., 2017</xref>), which may result in memory reports that are biased (see also <xref ref-type="bibr" rid="R68">Taylor &amp; Bays, 2018</xref>) but still sufficient to achieve one‚Äôs behavioral goals. Abstraction may render working memories more robust, afford transfer across tasks, and may massively reduce the amount of information that needs to be maintained (see also <xref ref-type="bibr" rid="R29">Hardman et al., 2017</xref>; <xref ref-type="bibr" rid="R54">Ricker &amp; Cowan, 2010</xref>; <xref ref-type="bibr" rid="R73">Vergauwe et al., 2014</xref>).</p><p id="P4">However, progress in understanding the temporal dynamics of WM abstraction has thus far been limited. Few studies have examined the extent to which neural WM representations generalize (or not) across different stimulus inputs (<xref ref-type="bibr" rid="R13">Christophel, Allefeld, et al., 2018</xref>; <xref ref-type="bibr" rid="R36">Kwak &amp; Curtis, 2022</xref>; <xref ref-type="bibr" rid="R65">Spitzer &amp; Blankenburg, 2012</xref>; <xref ref-type="bibr" rid="R72">Vergara et al., 2016</xref>) and/or become categorically biased (e.g., <xref ref-type="bibr" rid="R2">Bae, 2021</xref>; <xref ref-type="bibr" rid="R76">Wolff et al., 2020</xref>; <xref ref-type="bibr" rid="R78">Yu et al., 2020</xref>). In humans, these studies used functional imaging (fMRI), which lacks the temporal resolution to disclose rapid format changes, or EEG/MEG, which often can decode the task-relevant stimulus information only during the first 1-2 seconds of unfilled WM delays (<xref ref-type="bibr" rid="R2">Bae, 2021</xref>; <xref ref-type="bibr" rid="R6">Barbosa et al., 2021</xref>; <xref ref-type="bibr" rid="R34">King et al., 2016</xref>; <xref ref-type="bibr" rid="R64">Spitzer &amp; Blankenburg, 2011</xref>; <xref ref-type="bibr" rid="R77">Wolff et al., 2017</xref>). Here, we used a different approach which leverages the finding that subtle ocular activity (e.g., microsaccades; <xref ref-type="bibr" rid="R22">Engbert &amp; Kliegl, 2003</xref>; <xref ref-type="bibr" rid="R40">Liu et al., 2022</xref>) can reflect attentional orienting during visuospatial WM tasks (e.g., <xref ref-type="bibr" rid="R71">van Ede et al., 2019</xref>). While traditionally considered a confound that experimenters seek to avoid, small gaze shifts can reflect certain types of visuospatial WM information with even greater fidelity than EEG/MEG recordings (<xref ref-type="bibr" rid="R46">Mostert et al., 2018</xref>; <xref ref-type="bibr" rid="R52">Quax et al., 2019</xref>), and even throughout prolonged WM delays, which opens new avenues for tracking dynamic format changes.</p><p id="P5">Based on previous behavioral and theoretical work, we hypothesized that the level of abstraction in WM may change when the to-be-maintained information has been temporarily unattended. While unattended, WM contents cannot easily be decoded with neuroimaging approaches (but see <xref ref-type="bibr" rid="R13">Christophel et al., 2018</xref>), and the neural substrates of unattended storage remain disputed (<xref ref-type="bibr" rid="R6">Barbosa et al., 2021</xref>; <xref ref-type="bibr" rid="R9">Beukers et al., 2021</xref>; <xref ref-type="bibr" rid="R67">Stokes, 2015</xref>). Behaviorally, however, temporary inattention renders working memories less precise (<xref ref-type="bibr" rid="R3">Bae &amp; Luck, 2018</xref>; <xref ref-type="bibr" rid="R21">Emrich et al., 2017</xref>) and more categorically biased (<xref ref-type="bibr" rid="R1">Bae &amp; Luck, 2019</xref>), which may suggest increased abstraction of the WM content (see also <xref ref-type="bibr" rid="R33">Kerr√©n et al., 2022</xref>). Physiological evidence for when and how such modifications may occur during WM maintenance is still lacking.</p><p id="P6">Here, we recorded eye movements while participants memorized the orientations of rotated objects in a dual retro-cue task (<xref ref-type="fig" rid="F1">Fig 1a</xref>). With such a task layout, it is commonly assumed that the initially uncued information is unattended (or deprioritized) in WM, while the cued information is in the focus of attention (<xref ref-type="bibr" rid="R38">Lewis-Peacock &amp; Postle, 2012</xref>; <xref ref-type="bibr" rid="R59">Rose et al., 2016</xref>; <xref ref-type="bibr" rid="R77">Wolff et al., 2017</xref>). Representational geometry analyses borrowed from neuroimaging (<xref ref-type="bibr" rid="R35">Kriegeskorte &amp; Kievit, 2013</xref>) allowed us to track with high temporal precision whether orientation encoding in gaze patterns was object-specific (indicating a concrete-visual memory), object-independent (indicating more generalized/abstract task coordinates), and/or categorically biased, throughout the different stages of the task. Participants were encouraged to keep fixation through online feedback (closed-loop) in order to restrict eye-movements to small and involuntary gaze shifts.</p><p id="P7">We found that despite this fixation monitoring, miniature gaze patterns clearly encoded the cued stimulus orientation throughout the WM delays. While the orientation encoding was object-specific at first (indicating attentional focusing on concrete visual details), its format rapidly became object-independent (generalized/abstract) when another stimulus was encoded or maintained in the focus of attention. We further found that temporary inattention increased repulsive cardinal bias in subsequent memory reports, with some evidence for such biases emerging already during the delay periods in the geometry of gaze patterns. Together, our findings indicate adaptive format changes during WM maintenance within and outside the focus of attention and highlight the utility of detailed gaze analysis for future work.</p></sec><sec id="S2" sec-type="results"><title>Results</title><p id="P8">Participants (n = 41) performed a cued visual working memory task (<xref ref-type="fig" rid="F1">Fig. 1a</xref>) while their gaze position was tracked. On each trial, two randomly oriented stimuli (pictures of real-world objects) were sequentially presented (each for 0.5 s followed by a 0.5 s blank screen), after which an auditory ‚Äúretro‚Äù-cue (Cue 1) indicated which of the two orientations was to be remembered after a delay period (Delay 1, 3.5 s) at Test 1. On half of the trials (randomly varied), Test 1 was followed by another retro-cue (Cue 2) and another delay period (Delay 2, 2.5 s), after which participants were required to also remember the orientation of the other, previously uncued stimulus (Test 2).</p><sec id="S3"><title>Behavioral accuracy</title><p id="P9">At each of the two memory tests, the probed stimulus was shown with a slightly altered orientation (+/-6.43¬∞), and participants were asked to re-rotate it to its previous orientation via button press (two-alternative forced choice, 2-AFC). As expected, the percentage of correct responses was descriptively higher on Test 1 (M = 73.41%; SD = 6.42%) than on Test 2 (M = 66.62%; SD = 5.78%). Further, the second presented orientation (Stimulus 2) was remembered better (M = 70.99%, SD = 7.07%) than the first presented orientation (Stimulus 1; M = 69.04%; SD = 6.79%). A 2 x 2 repeated measures ANOVA with the factors Test (1/2) and Stimulus (1/2) confirmed that both these effects were significant [F(1,40) = 95.396, p &lt; 0.001, Œ∑<sup>2</sup> = 0.521 and F(1,40) = 13.319, p &lt; 0.001, Œ∑<sup>2</sup> = 0.043], while there was no significant interaction between the two factors [F(1,40) = 3.681, p = 0.062, Œ∑<sup>2</sup> = 0.008].</p><p id="P10">The effects of presentation- and testing order may both be attributed to task periods since stimulus presentation during which the other stimulus was to be attended, either for perceptual processing (Stimulus 2) or for cued maintenance and reporting (Delay 1 and Test 1). We may combine these two factors into the ‚Äúmnemonic distance‚Äù of a stimulus, which in our experiment had 4 levels (from shortest to longest: Stimulus 2 at Test 1, Stimulus 1 at Test 1, Stimulus 2 at Test 2, and Stimulus 1 at Test 2). The behavioral accuracy results were compactly described as a monotonic decrease across these distance levels [t(40) = -9.404, d = -1.469, p &lt; 0.001; t-test of linear slope against zero], as would be expected if the processing of Stimulus 2 temporarily withdrew attention from the memory of Stimulus 1, similar (and additive) to the withdrawal of attention from the uncued item during Delay 1 and Test 1.</p></sec><sec id="S4"><title>Eye-tracking Results</title><p id="P11">We informed participants that their gaze would be monitored in order to ensure that they constantly fixated a centrally presented dot throughout the task. To enforce this, we provided real-time feedback (closed-loop) when fixation was lost (see <xref ref-type="sec" rid="S12">Methods</xref>). <xref ref-type="fig" rid="F1">Figure 1c</xref> shows the participants‚Äô gaze distribution in relation to stimulus size after rotating the trial data to the respective object‚Äôs real-world (upright) orientation (for similar approaches, see <xref ref-type="bibr" rid="R24">Ester et al., 2016</xref>; <xref ref-type="bibr" rid="R32">Kang &amp; Spitzer, 2021</xref>). Despite this rotation alignment, the gaze density was concentrated narrowly (mostly within &lt; 1¬∞ visual angle) at center, during both the stimulus- and the delay periods (<xref ref-type="fig" rid="F1">Fig. 1c</xref>). The instructions and online feedback thus proved effective in preventing participants from overtly gazing at the location of the objects‚Äô peripheral features (such as, e.g., the spire of the lighthouse in <xref ref-type="fig" rid="F1">Fig. 1a</xref>). However, inspecting the participants‚Äô average gaze positions for each stimulus orientation (without rotational alignment) disclosed miniature circle-like patterns (<xref ref-type="fig" rid="F1">Fig. 1d</xref>), indicating that miniscule gaze shifts near fixation did carry information about the objects‚Äô orientation (for related findings with other stimulus materials, see e.g., <xref ref-type="bibr" rid="R46">Mostert et al., 2018</xref>; <xref ref-type="bibr" rid="R52">Quax et al., 2019</xref>).</p><sec id="S5"><title>Object orientation was robustly reflected in miniature gaze patterns</title><p id="P12">For quantitative analysis of the orientation encoding in gaze, we used an approach based on Representational Similarity Analysis (RSA; <xref ref-type="bibr" rid="R35">Kriegeskorte &amp; Kievit, 2013</xref>). Specifically, we examined to what extent the gaze patterns showed the characteristic Euclidean distance structure of evenly spaced points on a circle (<xref ref-type="fig" rid="F2">Fig. 2a</xref>). We implemented RSA on the single-trial level (<xref ref-type="fig" rid="F2">Fig 2b</xref>) by correlating for each trial the model-predicted distances with the vector of gaze distances between the current trial and the trial average for each stimulus orientation. The procedure yields a cross-validated estimate of orientation encoding at each time point for every trial (see <xref ref-type="sec" rid="S12">Methods</xref>).</p><p id="P13">We first inspected the mean time courses of orientation encoding (averaged over trials) during stimulus presentation. We observed robust encoding of stimulus orientation from approximately 500 ms after stimulus onset, both for Stimulus 1 and for Stimulus 2 (both p<sub>cluster</sub> &lt; 0.001, cluster-based permutation tests, see <xref ref-type="sec" rid="S12">Methods</xref>). The encoding of either stimulus orientation peaked at approx. 650 ms (i.e., only after the stimuli‚Äôs offset; <xref ref-type="fig" rid="F2">Fig. 2c</xref>), after which it slowly decayed.</p></sec><sec id="S6"><title>Concurrent encoding of Stimulus 1 during encoding of Stimulus 2</title><p id="P14">Although gaze data are only two-dimensional, we found that while encoding the second presented orientation (Stimulus 2), the gaze pattern also continued to carry information about the first-presented orientation (Stimulus 1; see <xref ref-type="fig" rid="F2">Fig. 2c</xref>). Such a concurrency in the average time courses may have arisen if one of the orientations was encoded on some trials and the other orientation on others. Alternatively, however, the pattern may indicate that gaze encoded both orientations simultaneously (i.e., additively, on the same trials). To shed light on this, we capitalized on our single-trial approach (see <xref ref-type="sec" rid="S12">Methods</xref> and <xref ref-type="fig" rid="F2">Fig. 2b</xref>) and binned each participant‚Äôs trials according to how strongly the orientation of Stimulus 2 was encoded between 250 and 1000 ms after Stimulus 2 onset (<xref ref-type="fig" rid="F2">Fig. 2e</xref>). If encoding of the two orientations had alternated between different trials, we would expect a negative relationship with the encoding of the orientation of Stimulus 1 in the same time window. However, we found no significant relationship [t(40)= -1.368, d = -0.214, p = 0.18, linear trend analysis]. What is more, the encoding of Stimulus 1‚Äôs orientation was significantly above chance even on those trials on which the encoding of Stimulus 2‚Äôs orientation was maximally strong [t(40) = 3.656, d = 0.571, p = 0.01; t-test against 0]. Together, these results suggest that small shifts in 2D gaze space carried information about the two stimulus orientations simultaneously, on the same trials.</p></sec><sec id="S7"><title>Gaze patterns encoded the cued orientations throughout the delay periods</title><p id="P15">Our main interest was in how gaze patterns reflected information storage during the unfilled delay periods (Delay 1 and 2; see <xref ref-type="fig" rid="F1">Fig 1a</xref>). During Delay 1, approximately 500 ms after auditory cueing (Cue 1), the encoding of the cued orientation ramped up and continuously increased in strength until the time of Test 1, whereas the encoding of the uncued orientation slowly returned to baseline (<xref ref-type="fig" rid="F2">Fig 2c</xref>). During Delay 2 (which occurred in half of the trials), a similar ramping-up pattern was observed for the second-cued orientation (which was previously uncued; <xref ref-type="fig" rid="F2">Fig. 2d</xref>; both p<sub>cluster</sub> &lt; 0.001). Thus, miniature gaze deflections robustly encoded the currently cued (or ‚Äúattended‚Äù) memory information during the two delay periods, in a ramp-up fashion that resembled the encoding of WM information in neural recordings (e.g., in monkey prefrontal cortex; <xref ref-type="bibr" rid="R5">Barak et al., 2010</xref>; <xref ref-type="bibr" rid="R75">Watanabe &amp; Funahashi, 2007</xref>).</p></sec><sec id="S8"><title>Object-specific vs. object-independent orientation encoding</title><p id="P16">We next examined more closely in which format(s) the gaze patterns reflected the WM information. A priori, memory reports in our task could be based on a concrete visual memory of the presented stimulus, but they could also be based on a mental abstraction of orientation, e.g., in terms of directional spatial coordinates. To the extent that the small eye movements during WM maintenance reflected mental focusing on concrete visual features (e.g., the location of a specific point on the object‚Äôs contour), we expect the orientation encoding in gaze to be object-<italic>specific</italic>, that is, not fully transferable between different objects. In contrast, an abstraction of orientation (e.g., in terms of a direction in which any object may point with its real-world top) should be reflected in gaze patterns that are object-<italic>independent</italic> and transferable.</p><p id="P17">We examined object-specificity by comparing the orientation encoding in gaze distances <italic>within</italic> objects (<xref ref-type="fig" rid="F3">Fig. 3a</xref>, <italic>left</italic>) with that in gaze distances <italic>between</italic> objects (<xref ref-type="fig" rid="F3">Fig. 3a</xref>, <italic>right</italic>). Upon stimulus presentation, the orientation encoding in gaze patterns was object-specific, in that within-objects encoding clearly exceeded between-objects encoding (<xref ref-type="fig" rid="F3">Fig. 3c</xref>, all p<sub>cluster</sub> &lt; 0.012). For Stimulus 1, the object-specificity diminished abruptly after approx. 1300 ms (when the gaze patterns began to also encode Stimulus 2), and changed to a more object-independent format for the remainder of the trial epoch (<xref ref-type="fig" rid="F3">Fig. 3c</xref>, <italic>upper</italic>). For Stimulus 2, in contrast, when cued for Test 1, the object-specificity decayed less and was sustained throughout most of Delay 1 (<xref ref-type="fig" rid="F3">Fig. 3c</xref>, <italic>lower</italic>). Later, in Delay 2, no object-specificity was evident anymore for either stimulus (no p<sub>cluster</sub> &lt; 0.60). <xref ref-type="fig" rid="F3">Figure 3e-f</xref> summarize the temporal evolution of object-independence in terms of Bayes Factors, showing the swift change of Stimulus 1 encoding from object-specific (BF<sub>01</sub> &lt; ‚Öì) towards object-independent (BF<sub>01</sub> &gt; 3) at the time of Stimulus 2 encoding, whereas the encoding of Stimulus 2 retained object-specificity during Delay 1 (<xref ref-type="fig" rid="F3">Fig. 3e</xref>). In Delay 2, after unattended storage throughout Delay 1, the orientation encoding in gaze had become object-independent for both stimuli (<xref ref-type="fig" rid="F3">Fig. 3f</xref>).</p><p id="P18">Focusing on the delay periods, we examined whether the object-specificity of cued orientation encoding differed between the two delay periods (Delay 1 or 2) and/or between the first and second presented stimulus (Stimulus 1 or 2). A 2 x 2 repeated-measures ANOVA on the difference in encoding strength (within-minus between-objects, averaged across the respective delay periods) showed a main effect of delay period [Delay 1/2; F(1,40) = 6.204, p = 0.017, Œ∑<sup>2</sup> = 0.064], indicating greater object-specificity during Delay 1, but no effect of presentation order [Stimulus 1/2; F(1,40) = 0.985, p = 0.327]. There also was a moderate interaction between the two factors [F(1,40) = 4.466, p = 0.041, Œ∑<sup>2</sup> = 0.027], reflecting that the difference between the two delays was stronger for Stimulus 2. Again, we also inspected these results in terms of the mnemonic distance from stimulus presentation, that is, the time the orientation in question had been unattended while focusing on the other orientation (<xref ref-type="fig" rid="F3">Fig. 3b</xref>). Indeed, this analysis confirmed a decrease of object specificity with increasing mnemonic distance [t(40) = -2.473, d = -0.386, p = 0.018; t-test of linear slope against zero].</p><p id="P19">Together, these results showed that unlike during perceptual processing, gaze patterns during the delay periods reflected WM information in more generalized (or abstract) coordinates, and that the level of this abstraction increased after periods of temporary (or partial) inattention.</p></sec></sec><sec id="S9"><title>Cardinal repulsion bias in gaze patterns and behavior</title><p id="P20">In studies of WM for stimulus orientation (e.g., of Gabor gratings) it is commonly observed that behavioral reports are biased away from the cardinal (vertical and horizontal) axes (<xref ref-type="bibr" rid="R2">Bae, 2021</xref>; <xref ref-type="bibr" rid="R68">Taylor &amp; Bays, 2018</xref>). We asked (i) whether such a repulsive cardinal bias also occurred with our rotated object stimuli, (ii) whether the strength of bias was modulated by periods of inattention (<xref ref-type="bibr" rid="R1">Bae &amp; Luck, 2019</xref>), and (iii) to what extent such bias was already expressed in the geometry of the miniature gaze patterns observed during the delay periods.</p><p id="P21">To model bias in behavior, we used a geometrical approach that quantifies bias as a mixture of a perfect (unbiased) circle (<xref ref-type="fig" rid="F4">Fig. 4a</xref>, <italic>middle</italic>) with perfect (fully biased) square geometries (<xref ref-type="fig" rid="F4">Fig. 4a</xref>, <italic>leftmost</italic> and <italic>rightmost</italic>, see <xref ref-type="sec" rid="S12">Methods</xref>, Behavioral modeling for details). Intuitively, the mixture parameter ùêµ quantifies the extent to which the reported orientations were repulsed away from the cardinal axes, with ùêµ &gt; 0 indicating repulsion (i.e., cardinal bias), ùêµ = 0 no bias, and ùêµ &lt; 0 attraction.</p><p id="P22">Fitting the model to participants‚Äô behavioral responses (<xref ref-type="fig" rid="F4">Fig. 4b</xref>, <italic>left</italic> and <italic>middle</italic>), we observed values of ùêµ &gt; 0 (grand mean: 0.124, SD = 0.092) in both memory tests (Test 1 and 2) and for both orientations [Stimulus 1 and 2; all ùêµ &gt; 0.071; all t(40) &gt; 4, all p &lt; 0.001; t-tests against 0]. Thus, participants overall showed a repulsive cardinal bias, which replicates and extends previous work with simpler stimuli (such as gratings; <xref ref-type="bibr" rid="R2">Bae, 2021</xref>; <xref ref-type="bibr" rid="R68">Taylor &amp; Bays, 2018</xref>). A 2 x 2 repeated measures ANOVA showed a main effect of Test [1/2; F(1,40) = 19.743, p &lt; 0.001, Œ∑<sup>2</sup> = 0.144], indicating a stronger bias on Test 2, and a main effect of presentation order [Stimulus 1/2; F(1,40) = 4.669, p = 0.037, Œ∑<sup>2</sup> = 0.024] with no interaction between the two factors [F(1,40) = 1.083, p = 0.304]. The overall pattern could again be described compactly as an increase in cardinal bias with increasing mnemonic distance from stimulus presentation [t(40) = 5.315, d = 0.830, p &lt; 0.001; t-test of linear slope against zero, <xref ref-type="fig" rid="F4">Fig. 4b</xref>, <italic>left</italic>]. Thus, we found robust cardinal repulsion in participants‚Äô overt memory reports, and this bias increased with periods of unattended storage.</p><p id="P23">Finally, we addressed to what extent the cardinal bias was also reflected in the gaze patterns recorded throughout the two delay periods. To do so, our geometric model yields distinctive distance structures for extreme cardinal repulsion (ùêµ = 1; <xref ref-type="fig" rid="F4">Fig. 4a</xref>, <italic>rightmost)</italic> and attraction (ùêµ = -1; <xref ref-type="fig" rid="F4">Fig. 4a</xref>, <italic>leftmost</italic>), respectively. If the gaze patterns were unbiased, we would expect both these ‚Äòsquare‚Äô models to correlate less well with the data than the unbiased (‚Äòcircle‚Äô) model with ùêµ = 0 (<xref ref-type="fig" rid="F4">Fig. 4a</xref> <italic>middle</italic>). However, to the extent that the gaze patterns were repulsively biased, we would expect the repulsion model to outperform the attraction model, nearing (or in the case of extreme bias, even exceeding) the circle model (dashed black in <xref ref-type="fig" rid="F4">Fig. 4c</xref>). Contrasting repulsion and attraction models thus allowed us to quantify the extent of repulsive or attractive bias in the gaze patterns during the delay periods.</p><p id="P24">Descriptively, the three different models (repulsion, unbiased, attraction) showed only small differences in correlation with the data (<xref ref-type="fig" rid="F4">Fig. 4c</xref>), indicating that the statistical power to detect bias in the gaze data was relatively low (see <xref ref-type="sec" rid="S12">Methods</xref>, <italic>Model geometries</italic>). Nevertheless, contrasting the repulsion model with the attraction model showed mildly significant clusters (p<sub>cluster</sub> = 0.02 and p<sub>cluster</sub> = 0.035), indicating a repulsive bias near the end of the delay periods for Stimulus 1 (<xref ref-type="fig" rid="F4">Fig. 4c</xref>, upper). A similar tendency for Stimulus 2 failed to reach significance in Delay 2 (<xref ref-type="fig" rid="F4">Fig. 4c</xref>, lower right; p<sub>cluster</sub> = 0.085, below display threshold) and was absent in Delay 1 (<xref ref-type="fig" rid="F4">Fig. 4c</xref>, lower left; no cluster-forming time points). A 2 x 2 repeated measures ANOVA on the difference between repulsion and attraction models (averaged across the last second of the delay periods) showed a main effect of presentation order [Stimulus 1/2; F(1,40) = 4.561, p = 0.039 Œ∑<sup>2</sup> = 0.026; main effect of Delay 1/2: F(1,40) = 1.650, p = 0.206; interaction: F(1,40) &lt; 1], indicating a stronger repulsive bias for the first presented orientation (Stimulus 1). Complementary analysis in terms of mnemonic distance (<xref ref-type="fig" rid="F4">Fig. 4b</xref>, <italic>right</italic>) showed a positive trend similar to that in behavior, albeit only at the significance level of a one-tailed test [t(40) = 1.772, d = 0.278, p = 0.042; t-test of linear slope against zero; one-tailed, hypothesis derived from behavioral result]. Together, while the differentiation of models (repulsive, unbiased, attractive) in the gaze data was not as clear-cut as in behavior (cf. <xref ref-type="fig" rid="F4">Fig. 4b</xref>, <italic>right</italic> and <italic>left</italic>), we found indications that the gaze patterns may have carried a repulsive cardinal bias, most evidently during the later portions of the WM delays and after temporary and/or partial inattention to the WM information.</p></sec><sec id="S10"><title>Orientation-dependent microsaccades</title><p id="P25">While our RSA-based approach was designed to characterize the time-varying geometries of aggregate gaze position patterns (<xref ref-type="fig" rid="F2">Figs 2</xref>-<xref ref-type="fig" rid="F4">4</xref>), we performed additional analysis on request by reviewers to explore whether the findings may indeed be related to microsaccades (see <italic>Introduction</italic>), i.e., ‚Äújerk-like‚Äù (<xref ref-type="bibr" rid="R57">Rolfs, 2009</xref>) small eye movements. <xref ref-type="fig" rid="F5">Figure 5</xref> illustrates the directions of microsaccades detected after stimulus presentation (Stimulus 1, Stimulus 2) and during the two delay periods (Delay 1, Delay 2), respectively, for each of the 16 orientations of the currently relevant object. The saccade directions in the post-stimulus periods correlated positively with stimulus orientation (circular correlation: Stimulus 1, R = 0.089; Stimulus 2, R = 0.077; both p &lt; 0.001). Weakly positive correlations were also evident during the delay periods (Delay 1, R = 0.01; Delay 2, R = 0.03; both p &lt; 0.001). For further inspection, we again rotated the trial data (analogous to <xref ref-type="fig" rid="F1">Fig. 1b-c</xref>) to illustrate the saccade directions relative to the objects‚Äô real-world (upright) orientation. As expected if microsaccades reflected stimulus orientation, the aligned distributions were not uniform (all z‚Äôs &gt; 34.36, all p &lt; 0.001, Rayleigh tests for uniformity) but appeared egg-shaped, with a main peak near the object‚Äôs real-world top (at 90¬∞) and another, smaller peak near the opposite angle (270¬∞, which may reflect ‚Äòreturn‚Äô microsaccades to fixation). Together, these complementary results support the idea that the effects observed in our main analyses may have been related to microsaccadic activity during attempted fixation (<xref ref-type="bibr" rid="R22">Engbert &amp; Kliegl, 2003</xref>; <xref ref-type="bibr" rid="R28">Hafed &amp; Clark, 2002</xref>).</p></sec></sec><sec id="S11" sec-type="discussion"><title>Discussion</title><p id="P26">The processing of WM information during delay periods has been studied extensively using neural recordings (for reviews, see <xref ref-type="bibr" rid="R15">Christophel et al., 2017</xref>; <xref ref-type="bibr" rid="R20">D‚ÄôEsposito &amp; Postle, 2015</xref>; <xref ref-type="bibr" rid="R26">Goldman-Rakic, 1995</xref>; <xref ref-type="bibr" rid="R45">Miller et al., 2018</xref>; <xref ref-type="bibr" rid="R67">Stokes, 2015</xref>; <xref ref-type="bibr" rid="R74">Wang, 2021</xref>). Here, using novel stimulus materials and tailored geometry analyses, we showed that miniature gaze deflections can disclose an array of WM-associated phenomena that were previously only observed in neural signals, including (i) a sustained encoding of the task-relevant stimulus feature, which (ii) shows a different format than during perception, can (iii) persist while also encoding new perceptual information, (iv) ramps up throughout delay periods when relevant for an upcoming test, and (v) returns to baseline when uncued (or ‚Äúunattended‚Äù). Beyond this, the gaze geometries indicated that temporary inattention rendered the WM information more generalized (object-independent), and potentially more categorically biased. These format changes during maintenance were similarly observed when attention to the memorandum was withdrawn by explicit retro-cueing or by presenting additional WM information.</p><p id="P27">Behaviorally, our results replicate and extend previous findings that temporary inattention renders working memories less precise and more biased (<xref ref-type="bibr" rid="R1">Bae &amp; Luck, 2019</xref>; <xref ref-type="bibr" rid="R21">Emrich et al., 2017</xref>). The eye-tracking results shed light on the temporal unfolding of potentially underlying format changes during WM storage. The gaze patterns during perceptual processing were clearly object-specific, indicating a focus on concrete visual details. When the last seen stimulus (Stimulus 2) was immediately cued (with an auditory retro-cue that offered no visual distraction), some of this object-specificity was sustained throughout the ensuing WM delay. In contrast, for the first-presented stimulus (Stimulus 1), the object-specificity dropped abruptly as soon as Stimulus 2 processing commenced. Of note, Stimulus 1 encoding did continue throughout Stimulus 2 processing. However, its format changed to object-independent (or ‚Äúabstract‚Äù) during the object-specific (or ‚Äúconcrete‚Äù) encoding of Stimulus 2 ‚Äî as if the memory of Stimulus 1 was reformatted to ‚Äòevade‚Äô the format of the currently perceived stimulus. Subsequent cueing did not revert this effect, nor did we find any re-emergence of object-specificity after unattended storage, for either stimulus, in the second delay. Together, these results support the idea that temporary (or partial) inattention may render the task-relevant WM information (here, orientation) increasingly less ‚Äúconcrete‚Äù (visual-sensory) and more generalized or ‚Äúabstract‚Äù.</p><p id="P28">Further support for this idea comes from our analysis of the cardinal repulsion bias. In parallel with the object-independence of gaze patterns, the repulsive cardinal reporting bias increased with the time a given stimulus had been temporarily (or partially) unattended. Repulsive orientation bias in WM tasks has been explained, e.g., by efficient coding principles, in terms of relatively finer tuning to cardinal orientations, reflecting their relative prevalence in natural environments (<xref ref-type="bibr" rid="R68">Taylor &amp; Bays, 2018</xref>). An alternative framing of the cardinal repulsion biases in our experiment with real-life objects could be in terms of more explicit semantic categorization (e.g., ‚Äúleft‚Äù/‚Äúright‚Äù and ‚Äúup‚Äù/‚Äúupside down‚Äù; <xref ref-type="bibr" rid="R29">Hardman et al., 2017</xref>; <xref ref-type="bibr" rid="R55">Ricker et al., 2022</xref>). The results may thus also reflect increased reliance on semantics (<xref ref-type="bibr" rid="R33">Kerr√©n et al., 2022</xref>) and/or (pre-)verbal labels when restoring information from unattended storage (see also <xref ref-type="bibr" rid="R9">Beukers et al., 2021</xref>), which would be in line with a higher level of abstraction. While our geometrical analysis approach is agnostic to the mechanistic cause of cardinal biases, we found some indications that they were also evident in biased gaze geometries during the stimulus-free retention periods (for related findings in neuroimaging, see <xref ref-type="bibr" rid="R2">Bae, 2021</xref>; Ester et al., 2020; <xref ref-type="bibr" rid="R76">Wolff et al., 2020</xref>). The latter result was statistically weak and should be revisited in future work, possibly under conditions that induce even stronger biases in behavior (e.g., higher WM loads; <xref ref-type="bibr" rid="R68">Taylor &amp; Bays, 2018</xref>).</p><p id="P29">A remarkable aspect of our results is the small amplitude of the eye-movements that disclosed such rich information. The mass of the raw position samples in our analysis was within &lt; 1¬∞ visual angle around fixation (<xref ref-type="fig" rid="F1">Fig. 1c</xref>). A discernible ‚Äúcircular‚Äù structure in averaged data points (<xref ref-type="fig" rid="F1">Fig. 1d</xref>) measured only approx. 0.2-0.3¬∞ in diameter, which is near the eye-tracker‚Äôs accuracy limit, and was only a fraction of the memory items‚Äô physical size. Together with our online fixation control (see <xref ref-type="sec" rid="S12">Methods</xref>), these descriptives render it unlikely that our results were attributable to reflexive saccades to the location of peripheral stimulus features. Additional analysis (<xref ref-type="fig" rid="F5">Fig. 5</xref>) indicated that the findings more likely reflect microsaccadic activity during attempted fixation. Systematic microsaccade patterns have previously been linked to covert spatial attention (<xref ref-type="bibr" rid="R22">Engbert &amp; Kliegl, 2003</xref>; <xref ref-type="bibr" rid="R28">Hafed &amp; Clark, 2002</xref>), suggesting that in the present context, they might have reflected mental orienting towards a spatial coordinate or direction (<xref ref-type="bibr" rid="R40">Liu et al., 2022</xref>; <xref ref-type="bibr" rid="R71">van Ede et al., 2019</xref>). Together, our results indicate that participants generally oriented attention towards the objects‚Äô real-life ‚Äútop‚Äù, but with varying degrees of bias towards specific object features (resulting in object-specific orientation patterns), and/or away from cardinal axes (resulting in cardinal repulsion).</p><p id="P30">Under a view of the miniature gaze patterns reflecting covert spatial attention, our analysis tracked with high temporal resolution the time course of attention allocation to WM information in a dual retro-cue task. Before cueing, encoding a new stimulus (Stimulus 2) did not immediately eradicate or replace the attentional orienting to the previous stimulus (but did change its qualitative format, see above). At face value, the temporary simultaneity of both WM contents (<xref ref-type="fig" rid="F2">Fig. 2e</xref>), in a putative index of attention, might appear to be at odds with the idea of an exclusive single-item focus of attention in WM (<xref ref-type="bibr" rid="R48">Oberauer, 2002</xref>; <xref ref-type="bibr" rid="R49">Olivers et al., 2011</xref>; but see <xref ref-type="bibr" rid="R8">Beck et al., 2012</xref>; <xref ref-type="bibr" rid="R79">Zhang et al., 2018</xref>). However, another possible interpretation is that the (re)allocation of attention to different stimuli (or tasks) in WM may take time to complete. For instance, the encoding of the uncued stimulus fully returned to baseline only approx. 0.5-1.5 s after the cue, which is broadly consistent with previous behavioral and EEG work on the time course of WM-cueing effects (<xref ref-type="bibr" rid="R37">LaRocque et al., 2013</xref>; <xref ref-type="bibr" rid="R62">Souza &amp; Oberauer, 2016</xref>; <xref ref-type="bibr" rid="R66">Spitzer et al., 2014</xref>; <xref ref-type="bibr" rid="R64">Spitzer &amp; Blankenburg, 2011</xref>). Compared to this, the reformatting into a more generic, object-independent format was rapid, both for Stimulus 1 when encoding Stimulus 2 (see above), and for Stimulus 2 itself when it was uncued. Consistent with these results, a recent study found that low-level perceptual bias induced by concurrent WM information (cf. <xref ref-type="bibr" rid="R69">Teng &amp; Kravitz, 2019</xref>) dissipated quickly with new visual input (<xref ref-type="bibr" rid="R32">Kang &amp; Spitzer, 2021</xref>). These findings are in line with adaptive format changes in WM, potentially providing fast protection from interference beyond the overall reallocation of attention between different stimuli and/or tasks.</p><p id="P31">Previous WM studies using retro-cues yielded mixed results regarding potential costs for the first of two successively presented stimuli. Using visual retro-cues, one study found no differences between visual gratings presented first or second, neither in behavior nor in fMRI-decoding during the WM delay (Harrison &amp; Tong, 2007), which has been taken as evidence that intervening stimuli may cause little to no interference for visual WM representations (<xref ref-type="bibr" rid="R53">Rademaker et al., 2019</xref>). Another study, using visual retro-cues with tactile WM stimuli, did find lower performance for the first stimulus (<xref ref-type="bibr" rid="R64">Spitzer and Blankenburg, 2011</xref>), a finding we replicated here with auditory cueing of visual WM information. One possibility is that different-modality cues (e.g., auditory when the WM stimuli were visual) interfere less with the short-term memory of the last-presented stimulus than same-modality cues would (e.g., visual cues with visual WM stimuli; for related findings, see Bae and Luck; 2019). Different-modality cues may thus leave the memory trace of the last-presented stimulus more intact compared to the first stimulus (which is always followed by the same-modality input of the second stimulus). This aside, the format changes induced by the intervening stimulus were qualitatively similar to those after unattended storage, in line with a common explanation in terms of temporarily withdrawn attention.</p><p id="P32">Our findings of increasingly more object-independent gaze geometries do not rule out that the brain may maintain detailed visual memories in ways that would not register in eye-tracking. More generally, we can only speculate whether the minuscule eye-movements observed in our experiment played a functional role or whether they were merely epiphenomena of other processes. We consider it possible that our paradigm promoted aspects of WM-related processing to become visible at the surface of ocular activity, but that the ocular activity itself may have had little or no direct role in the WM processing proper (for related discussion, see <xref ref-type="bibr" rid="R40">Liu et al., 2022</xref>; <xref ref-type="bibr" rid="R41">Loaiza &amp; Souza, 2022</xref>; <xref ref-type="bibr" rid="R57">Rolfs, 2009</xref>; but see <xref ref-type="bibr" rid="R25">Ferreira et al., 2008</xref>; <xref ref-type="bibr" rid="R31">Johansson &amp; Johansson, 2014</xref>, for a role of eye-movements in episodic memory retrieval). This speculation also takes note of several recent failures to decode visuospatial WM information from eye-tracking, most notably in control analyses supplemental to neural decoding, where systematic eye-movements were ruled out as a potential confound (e.g., <xref ref-type="bibr" rid="R12">Brissenden et al., 2018</xref>; <xref ref-type="bibr" rid="R27">G√ºnseli et al., 2022</xref>; <xref ref-type="bibr" rid="R36">Kwak &amp; Curtis, 2022</xref>; <xref ref-type="bibr" rid="R47">Muhle-Karbe et al., 2021</xref>; but see <xref ref-type="bibr" rid="R46">Mostert et al., 2018</xref>; <xref ref-type="bibr" rid="R52">Quax et al., 2019</xref>; <xref ref-type="bibr" rid="R70">Thielen et al., 2019</xref>). At the same time, our findings sound a cautionary note that stimulus-dependent eye movements in visual WM tasks can be very small, hard to prevent, persistent, and above all, informative.</p><p id="P33">In summary, despite discouraging participants from eye motion through closed-loop fixation control, we found the orientation of visual objects robustly reflected in miniature gaze patterns during cued WM maintenance. The geometry of the gaze patterns underwent systematic changes, suggesting that temporary inattention increased the level of abstraction (and categorical bias) of the information in WM. Stimulus-dependent eye movements may not only pose a potential confound, but also a valuable source of information in studying visuospatial WM.</p></sec><sec id="S12" sec-type="methods"><title>Methods</title><sec id="S13" sec-type="subjects"><title>Participants</title><p id="P34">Fifty-five participants (31 female, 24 male, mean age 26.95 ¬± 3.98 years) took part in the experiment. Forty-four of the participants were recruited from a pool of external participants and 11 were recruited internally within the Max Planck Institute for Human Development. All participants were blind to our research questions and all of them received a compensation of ‚Ç¨10 per hour plus a bonus based on task performance (‚Ç¨5 bonus if four out of five randomly selected memory reports were correct). Written informed consent was obtained from all participants, and all experiments were approved by the ethics committee of the Max Planck Institute for Human Development. Two participants (both wearing glasses) were excluded due to difficulties in acquiring a stable eye-tracking signal, and one participant was excluded because she reported feeling unwell during the experimental session. Of the remaining participants, we excluded n = 9 for failing to perform above chance level in each of the two memory tests (p &lt; 0.05, Binomial test against 50% correct responses). Finally, after preprocessing the eye-tracking data, we excluded n = 2 participants for whom more than 15% of the data had to be rejected due to blinks and other recording artifacts. After this, n = 41 participants remained for analysis.</p></sec><sec id="S14"><title>Stimuli, Task, and Procedure</title><p id="P35">Nine color photographs of everyday objects from the BOSS database (candelabra, table, outdoor chair, crown, radio, lighthouse, lamppost, nightstand, gazebo) were used as stimuli. All objects were cropped (i.e., background removed), and one object (gazebo) was slightly modified using GNU image manipulation software (<ext-link ext-link-type="uri" xlink:href="http://www.gimp.org">http://www.gimp.org</ext-link>) to increase its mirror-symmetry. We grouped the pictures into 3 different sets of three, always combining objects with different aspect ratios (width/height; see example set in <xref ref-type="fig" rid="F3">Fig. 3a</xref>). Each participant was assigned one of these sets, with each set being used similarly often across the participant sample (two sets were used 18 times and one set 19 times). As auditory cue stimuli, we prepared recordings of the words ‚Äúone‚Äù, ‚Äútwo‚Äù, and ‚Äúthanks‚Äù spoken by a female lab member. The recordings were time-compressed to a common length of 350ms using a pitch-preserving algorithm provided in Audacity¬Æ (GNU software; <ext-link ext-link-type="uri" xlink:href="https://www.audacityteam.org/">https://www.audacityteam.org/</ext-link>).</p><p id="P36">Each trial started with a fixation dot (8√ó8 px, corresponding to 0.17 x 0.17¬∞ visual angle) displayed at the center of the screen for 500 to 1000 ms (randomly varied), followed by sequential presentation of two objects, each in a random orientation (see below). Each stimulus was displayed for 500 ms (display size approx. 6.5¬∞ visual angle, see <xref ref-type="fig" rid="F1">Fig. 1c</xref>) followed by a 500 ms blank screen. After this, an auditory retro-cue (‚Äúone‚Äù or ‚Äútwo‚Äù, 350 ms) indicated which of the two stimulus orientations was to be reported after a delay (Delay 1, 3500 ms) in the upcoming memory test (Test 1). Test 1 started with the cued object reappearing on display, but with its previous orientation changed by +/-6.43¬∞. Participants were asked to indicate via key press (2-AFC) whether the object would need to be rotated clockwise or anticlockwise (right and left arrow key) to match its memorized orientation. Upon key press, the object rotated accordingly (by 6.43¬∞), followed by a written feedback message (‚Äúcorrect‚Äù or ‚Äúincorrect‚Äù) displayed in the upper part of the screen (500 ms). After another 500ms, in half of trials, an auditory message (‚Äúthanks‚Äù, 350 ms) signaled the end of the trial. In the other half of trials (randomly varied), a second auditory retro-cue (Cue 2) was presented (e.g., ‚Äútwo‚Äù, if the first retro-cue was ‚Äúone‚Äù), indicating that the thus far untested stimulus orientation would still need to be reported. In these trials, another delay period ensued (Delay 2; 2500 ms), and participants‚Äô memory for the second-cued stimulus was tested (Test 2), using the same procedure as before for the first-cued stimulus in Test 1. Each participant performed 16 blocks of 32 trials, for a total of 512 trials (265 of which included a Test 2).</p><p id="P37">Stimulus presentation was pseudo-random across trials, with the following restrictions: (i) each pairing of objects from the participant‚Äôs object set occurred equally often, (ii) each object was equally often presented first (as Stimulus 1) and second (as Stimulus 2), and (iii) Stimulus 1 and Stimulus 2 were equally often cued for Test 1. The orientations of the two objects on each trial were drawn randomly and independently from 16 equidistant values (11.25¬∞ to 348.75¬∞ in steps of 22.5¬∞), which excluded the cardinal axes (0¬∞, 90¬∞, 180¬∞, and 270¬∞).</p><p id="P38">The experiment was run using Psychophysics Toolbox Version 3 (<xref ref-type="bibr" rid="R11">Brainard &amp; Vision, 1997</xref>) and the Eyelink Toolbox (<xref ref-type="bibr" rid="R16">Cornelissen et al., 2002</xref>) in MATLAB 2017a (MathWorks). The visual stimuli were presented on a 60√ó34 cm screen with a 2560√ó1440 px resolution and a frame rate of 60 Hz. The auditory cue words were presented via desktop loudspeakers (Harman Kardon KH206). To minimize head motion, participants performed the experiment with their head positioned on a chin rest with a viewing distance of approx. 62cm from the screen. Gaze position was monitored and recorded throughout the experiment at a sampling rate of 500 Hz using a desktop-mounted Eyelink 1000 eye-tracker (SR research), with file and link/analog filters set to ‚ÄòEXTRA‚Äô and ‚ÄòSTD‚Äô, respectively.</p><p id="P39">Participants were instructed to constantly keep their gaze on the fixation dot, which was displayed throughout the entire trial except for the test- and feedback periods. Whenever a participant‚Äôs gaze deviated more than 71 pixels (1.53¬∞ visual angle) from the center of the fixation dot either before object presentation or for longer than 500 ms during any of the two delay periods, a warning message (‚ÄúFixate‚Äù) was displayed at the center of the screen. This occurred during less than 15% (Mean: 13.03%) of the trial epoch on average.</p></sec><sec id="S15"><title>Behavioral modeling</title><p id="P40">To model participants‚Äô behavioral memory reports (2-AFC), we used a geometrical approach similar to that used in our eye-tracking analyses (see below). We first defined three prototypical geometries, (i) an unbiased ‚Äúcircle‚Äù model (<italic>ùëÄ<sub>circle</sub></italic>) corresponding to the memory items‚Äô 16 original orientations (see <xref ref-type="fig" rid="F4">Fig. 4a</xref>, <italic>middle</italic>), (ii) a cardinal model (ùëÄ<sub><italic>repulsion</italic></sub>) which shifts the 16 orientations to the nearest diagonal orientation (i.e., 45¬∞, 135¬∞ 225¬∞, or 315¬∞; see <xref ref-type="fig" rid="F4">Fig. 4a</xref>, <italic>rightmost</italic>) and (iii) a cardinal model (ùëÄ<sub><italic>attraction</italic></sub>) which shifts them to the nearest cardinal orientation (i.e., 0¬∞, 90¬∞, 180¬∞, or 270¬∞; see <xref ref-type="fig" rid="F4">Fig. 4a</xref>, <italic>leftmost</italic>). The continuum from attraction to repulsion was formalized with a mixture parameter ùêµ (ranging from -1 to 1) which blends the circle model with the repulsion model for ùêµ ‚â• 0 : <disp-formula id="FD1"><mml:math id="M1"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mo>¬∑</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>‚àí</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>¬∑</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula> and with the attraction model for ùêµ &lt; 0 : <disp-formula id="FD2"><mml:math id="M2"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mo>¬∑</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>¬∑</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula> where ¬∑ denotes scalar multiplication. <xref ref-type="fig" rid="F4">Figure 4a</xref> illustrates the resulting model continuum from ùêµ = -1 (maximal attraction) over ùêµ = 0 (unbiased) to ùêµ = 1 (maximal repulsion). To simulate memory reports (clockwise or counter-clockwise) for each trial, we computed the angular difference ùëë between the orientation modeled in ùëÄ<sub><italic>mix</italic></sub> and the probe orientation displayed at test, and transformed it into a probability of making a ‚Äúclockwise‚Äù response using a logistic choice function: <disp-formula id="FD3"><mml:math id="M3"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>‚àí</mml:mo><mml:mi>d</mml:mi><mml:mo>/</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula> where ùë† is a noise parameter that relates inversely to memory strength or -precision (see also <xref ref-type="bibr" rid="R60">Schurgin et al., 2020</xref>). For completeness, our model also allowed for greater memory precision near the cardinal axes (a so-called ‚Äúoblique effect‚Äù, e.g., <xref ref-type="bibr" rid="R51">Pratte et al., 2017</xref>; <xref ref-type="bibr" rid="R68">Taylor &amp; Bays, 2018</xref>). This was implemented by an additional parameter ùëê, which up- or downregulated noise ùë† for those 8 orientations in the stimulus set that were near to the cardinal axes (see <xref ref-type="fig" rid="F4">Fig. 4a</xref>, <italic>middle</italic>), relative to the remaining 8 orientations that were nearer to the diagonal axes: <disp-formula id="FD4"><mml:math id="M4"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mo>‚àí</mml:mo><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>c</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>¬∑</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mo>‚àí</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula> where values of ùëê &lt; 0 would indicate relatively greater precision (lower noise) near the cardinal axes (i.e., an oblique effect). The model was fitted to the memory reports of each participant individually, using exhaustive gridsearch (ùêµ, -1‚Ä¶1; ùë†, 0‚Ä¶1; ùëê, -0.5‚Ä¶0.5; with a step size of 0.01 for each parameter) and least squares to identify the best-fitting parameter values.</p><p id="P41">While our analysis focused on bias (ùêµ), we note for completeness that we also observed values of ùëê significantly smaller than 0 [mean across conditions: ùëê = -0.292, t(40) = -10.159, p &lt;.001, t-test against 0], i.e., an oblique effect, which replicates and extends previous work (e.g., <xref ref-type="bibr" rid="R68">Taylor &amp; Bays, 2018</xref>). The strength of this oblique effect tended to decrease with mnemonic distance [t(40) = 2.286, d = 0.357, p = 0.028; t-test of linear slope against zero] (see <xref ref-type="bibr" rid="R68">Taylor &amp; Bays, 2018</xref> for related findings).</p></sec><sec id="S16"><title>Eye-tracking analysis</title><p id="P42">The eye-tracking data was only minimally preprocessed. The data from each participant was zero-centered (using the overall mean over all trials), and data points with a Euclidean distance larger than 100 pixels (corresponding to 2.17¬∞ visual angle) from the zero-center were excluded from analysis (<xref ref-type="fig" rid="F1">Fig. 1c</xref> and <xref ref-type="fig" rid="F5">Fig. 5</xref> show data before this exclusion). We analyzed the data in two epochs of interest, one time-locked to Stimulus 1 onset (from -500 ms until the onset of Test 1 at 5850 ms), and the other time-locked to Cue 2 onset (from -500 ms until the onset of Test 2 at 2850 ms). After artifact exclusion, on average 97.87% (SD = 1.50%, first epoch) and 95.14% (SD = 3.67%, second epoch) of the data remained for analysis.</p><sec id="S17"><title>Representational Similarity Analysis (RSA)</title><p id="P43">RSA of the gaze position data was performed separately for each participant using a single-trial approach. For each trial, we first obtained the trial-average for each of the 16 orientations while leaving out the current trial. We then computed at each time point the 16 Euclidean distances between the gaze position in the current trial and the trial-averages formed from the remaining data. This yielded a representational dissimilarity vector (RDV) of the distances between the (single-trial) gaze associated with the orientation in the current trial and the (trial-averaged) gaze associated with each of the 16 orientations (<xref ref-type="fig" rid="F2">Fig. 2b</xref>). To examine orientation encoding, we computed at each time point and for each trial the Pearson correlation (rho) between the empirical RDV and the theoretical RDV predicted under a model of orientation encoding (see below) for the orientation on the current trial. When averaged over trials (and hence also across orientations), the procedure yields a leave-one-out cross-validated time-course of orientation encoding, similar to more conventional RSA approaches with trial-averages. However, the single-trial approach additionally retains the trial-by-trial variability in orientation encoding (<xref ref-type="fig" rid="F2">Fig. 2b</xref>, <italic>right</italic> and <xref ref-type="fig" rid="F2">Fig. 2e</xref>).</p><p id="P44">To examine orientation encoding within- and between objects (<xref ref-type="fig" rid="F3">Fig. 3a</xref>), we used the same approach, but obtained the 16 trial averages separately for each of the 3 different objects in the participant‚Äôs stimulus set. This yielded 3 empirical RDVs per trial (one within- and two between-objects) that were independently correlated (Pearson‚Äôs rho) with the model RDV. The two between-objects correlations (rho‚Äôs) were then averaged.</p></sec><sec id="S18"><title>Model geometries</title><p id="P45">Our basic orientation model was a perfect circle geometry (see <xref ref-type="fig" rid="F2">Fig. 2a</xref>, left), where the model RDVs reflected the pairwise Euclidean distances between 16 evenly spaced points on the unit circle (see <xref ref-type="fig" rid="F2">Fig. 2a</xref>, right; note that each line in the distance matrix corresponds to the model RDV for a given stimulus orientation; see <xref ref-type="fig" rid="F2">Fig. 2b</xref>). The geometry of this model corresponds to our behavioral analysis model with ùêµ = 0 (i.e., <italic>M<sub>circle</sub></italic>, unbiased). To examine bias in the gaze patterns (<xref ref-type="fig" rid="F4">Fig. 4</xref>), we used the Euclidean distance structures associated with our maximally biased models with ùêµ = -1 (<italic>M<sub>attraction</sub></italic>) and ùêµ = 1 (<italic>M<sub>repulsion</sub></italic>), respectively.</p><p id="P46">Comparing these two extreme models (which both have a square geometry) yields an estimate of the extent to which the gaze patterns were repulsively or attractively biased (see <italic>Results</italic>). Note that the distance structures expected under the three different models (ùêµ = 0, ùêµ = 1, and ùêµ = -1) correlate with each other (r = 0.77 and 0.34). We thus did not expect very large differences in their fit of the data and report the results with a more liberal statistical threshold (p<sub>cluster</sub> &lt; 0.05). In all cluster-based permutation tests (<xref ref-type="bibr" rid="R44">Maris &amp; Oostenveld, 2007</xref>), we first identified clusters of consecutive samples that showed an effect with p<sub>sample</sub> &lt; 0.05 (uncorrected) and used the sum of t-values within a cluster as its test statistic. We then estimated the probability p<sub>cluster</sub> that a cluster with a larger test statistic would emerge by chance, based on 20000 iterations where the individual subject effects were randomly sign-flipped. Unless otherwise specified, all reported statistical tests were two-sided.</p></sec><sec id="S19"><title>Microsaccade detection</title><p id="P47">For complementary analysis of microsaccades (<xref ref-type="fig" rid="F5">Fig. 5</xref>), we used a velocity-based detection algorithm established in previous work (<xref ref-type="bibr" rid="R18">De Vries et al., 2023</xref>; <xref ref-type="bibr" rid="R19">De Vries &amp; Van Ede, 2023</xref>; <xref ref-type="bibr" rid="R39">Liu et al., 2023</xref>). In brief, the gaze position data were transformed into a velocity time course by calculating the Euclidean distances between consecutive samples, and smoothing it with a 7 ms Gaussian kernel. Saccade onsets and endpoints were inferred from when the gaze velocity exceeded a trial-specific threshold (5 times the median velocity within the trial) and when it returned to below threshold, with a minimum interval of 100 ms between successively detected saccades.</p></sec></sec></sec></body><back><ack id="S20"><title>Acknowledgments</title><p>We thank Ivan Padezhki, Clara Wicharz, Josefine Hebisch, Anna Faschinger, Gabriele Inciuraite and Antonia Anouk Bielefeldt for their help with data collection, and Jann W√§scher for participant recruitment. We also thank Josefine Hebisch for recording the auditory stimuli, Martin Rolfs for helpful comments and discussion, Ralph Hertwig for general support, and Thomas Graham for editorial assistance. This research was supported by a European Research Council (ERC) Consolidator Grant ERC-2020-COG-101000972 (BS) and by DFG grant SP 1510/7-1 (BS).</p></ack><sec id="S21" sec-type="data-availability"><title>Data and code availability</title><p id="P48">All data and code supporting this study are available at <ext-link ext-link-type="uri" xlink:href="https://gin.g-node.org/lindedomingo/mpib_memoreye">https://gin.g-node.org/lindedomingo/mpib_memoreye</ext-link>
</p></sec><fn-group><fn id="FN1" fn-type="con"><p id="P49"><bold>Author contributions</bold></p><p id="P50">Juan Linde-Domingo: Conceptualization, Data Curation, Formal Analysis, Investigation, Project administration, Validation, Visualization, Writing ‚àí original draft, Writing ‚àí review &amp; editing Bernhard Spitzer: Conceptualization, Funding acquisition, Methodology, Project administration, Resources, Supervision, Writing ‚àí original draft, Writing ‚àí review &amp; editing</p></fn><fn id="FN2" fn-type="com"><p id="P51"><bold>Competing interests</bold></p><p id="P52">None</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bae</surname><given-names>G</given-names></name><name><surname>Luck</surname><given-names>SJ</given-names></name></person-group><article-title>What happens to an individual visual working memory representation when it is interrupted?</article-title><source>British Journal of Psychology</source><year>2019</year><volume>110</volume><issue>2</issue><fpage>268</fpage><lpage>287</lpage><pub-id pub-id-type="pmcid">PMC6358517</pub-id><pub-id pub-id-type="pmid">30069870</pub-id><pub-id pub-id-type="doi">10.1111/bjop.12339</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bae</surname><given-names>GY</given-names></name></person-group><article-title>Neural evidence for categorical biases in location and orientation representations in a working memory task: EEG decoding of categorical biases</article-title><source>NeuroImage</source><year>2021</year><volume>240</volume><month>June</month><elocation-id>118366</elocation-id><pub-id pub-id-type="pmid">34242785</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bae</surname><given-names>GY</given-names></name><name><surname>Luck</surname><given-names>SJ</given-names></name></person-group><article-title>Dissociable decoding of spatial attention and working memory from EEG oscillations and sustained potentials</article-title><source>Journal of Neuroscience</source><year>2018</year><volume>38</volume><issue>2</issue><fpage>409</fpage><lpage>422</lpage><pub-id pub-id-type="pmcid">PMC5761617</pub-id><pub-id pub-id-type="pmid">29167407</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2860-17.2017</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bae</surname><given-names>GY</given-names></name><name><surname>Olkkonen</surname><given-names>M</given-names></name><name><surname>Allred</surname><given-names>SR</given-names></name><name><surname>Flombaum</surname><given-names>JI</given-names></name></person-group><article-title>Why some colors appear more memorable than others: A model combining categories and particulars in color working memory</article-title><source>Journal of Experimental Psychology: General</source><year>2015</year><volume>144</volume><issue>4</issue><fpage>744</fpage><lpage>763</lpage><pub-id pub-id-type="pmid">25985259</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barak</surname><given-names>O</given-names></name><name><surname>Tsodyks</surname><given-names>M</given-names></name><name><surname>Romo</surname><given-names>R</given-names></name></person-group><article-title>Neuronal population coding of parametric working memory</article-title><source>Journal of Neuroscience</source><year>2010</year><volume>30</volume><issue>28</issue><fpage>9424</fpage><lpage>9430</lpage><pub-id pub-id-type="pmcid">PMC6632447</pub-id><pub-id pub-id-type="pmid">20631171</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1875-10.2010</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barbosa</surname><given-names>J</given-names></name><name><surname>Lozano-Soldevilla</surname><given-names>D</given-names></name><name><surname>Compte</surname><given-names>A</given-names></name></person-group><article-title>Pinging the brain with visual impulses reveals electrically active, not activity-silent, working memories</article-title><source>PLoS Biology</source><year>2021</year><volume>19</volume><issue>10</issue><elocation-id>e3001436</elocation-id><pub-id pub-id-type="pmcid">PMC8641864</pub-id><pub-id pub-id-type="pmid">34673775</pub-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3001436</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bays</surname><given-names>PM</given-names></name><name><surname>Catalao</surname><given-names>RFG</given-names></name><name><surname>Husain</surname><given-names>M</given-names></name></person-group><article-title>The precision of visual working memory is set by allocation of a shared resource</article-title><source>Journal of Vision</source><year>2009</year><volume>9</volume><issue>10</issue><fpage>7</fpage><lpage>7</lpage><pub-id pub-id-type="pmcid">PMC3118422</pub-id><pub-id pub-id-type="pmid">19810788</pub-id><pub-id pub-id-type="doi">10.1167/9.10.7</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beck</surname><given-names>VM</given-names></name><name><surname>Hollingworth</surname><given-names>A</given-names></name><name><surname>Luck</surname><given-names>SJ</given-names></name></person-group><article-title>Simultaneous control of attention by multiple working memory representations</article-title><source>Psychological Science</source><year>2012</year><volume>23</volume><issue>8</issue><fpage>887</fpage><lpage>898</lpage><pub-id pub-id-type="pmcid">PMC3419335</pub-id><pub-id pub-id-type="pmid">22760886</pub-id><pub-id pub-id-type="doi">10.1177/0956797612439068</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beukers</surname><given-names>AO</given-names></name><name><surname>Buschman</surname><given-names>TJ</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name></person-group><article-title>Is Activity Silent Working Memory Simply Episodic Memory?</article-title><source>Trends in Cognitive Sciences, xx</source><year>2021</year><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="pmid">33551266</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brady</surname><given-names>TF</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name><name><surname>Alvarez</surname><given-names>GA</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><article-title>Visual long-term memory has a massive storage capacity for object details</article-title><source>Proceedings of the National Academy of Sciences</source><year>2008</year><volume>105</volume><issue>38</issue><fpage>14325</fpage><lpage>14329</lpage><pub-id pub-id-type="pmcid">PMC2533687</pub-id><pub-id pub-id-type="pmid">18787113</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0803390105</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name><name><surname>Vision</surname><given-names>S</given-names></name></person-group><article-title>The psychophysics toolbox</article-title><source>Spatial Vision</source><year>1997</year><volume>10</volume><issue>4</issue><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brissenden</surname><given-names>JA</given-names></name><name><surname>Tobyne</surname><given-names>SM</given-names></name><name><surname>Osher</surname><given-names>DE</given-names></name><name><surname>Levin</surname><given-names>EJ</given-names></name><name><surname>Halko</surname><given-names>MA</given-names></name><name><surname>Somers</surname><given-names>DC</given-names></name></person-group><article-title>Topographic Cortico-cerebellar Networks Revealed by Visual Attention and Working Memory</article-title><source>Current Biology</source><year>2018</year><volume>28</volume><issue>21</issue><fpage>3364</fpage><lpage>3372</lpage><pub-id pub-id-type="pmcid">PMC6257946</pub-id><pub-id pub-id-type="pmid">30344119</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2018.08.059</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christophel</surname><given-names>TB</given-names></name><name><surname>Allefeld</surname><given-names>C</given-names></name><name><surname>Endisch</surname><given-names>C</given-names></name><name><surname>Haynes</surname><given-names>J-D</given-names></name></person-group><article-title>View-Independent Working Memory Representations of Artificial Shapes in Prefrontal and Posterior Regions of the Human Brain</article-title><source>Cerebral Cortex</source><year>2018</year><volume>28</volume><issue>6</issue><fpage>2146</fpage><lpage>2161</lpage><pub-id pub-id-type="pmid">28505235</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christophel</surname><given-names>TB</given-names></name><name><surname>Iamshchinina</surname><given-names>P</given-names></name><name><surname>Yan</surname><given-names>C</given-names></name><name><surname>Allefeld</surname><given-names>C</given-names></name><name><surname>Haynes</surname><given-names>J-D</given-names></name></person-group><article-title>Cortical specialization for attended versus unattended working memory</article-title><source>Nature Neuroscience</source><year>2018</year><volume>21</volume><issue>4</issue><fpage>494</fpage><lpage>496</lpage><pub-id pub-id-type="pmid">29507410</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christophel</surname><given-names>TB</given-names></name><name><surname>Klink</surname><given-names>PC</given-names></name><name><surname>Spitzer</surname><given-names>B</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name><name><surname>Haynes</surname><given-names>J-D</given-names></name></person-group><article-title>The Distributed Nature of Working Memory</article-title><source>Trends in Cognitive Sciences</source><year>2017</year><volume>21</volume><issue>2</issue><fpage>111</fpage><lpage>124</lpage><pub-id pub-id-type="pmid">28063661</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cornelissen</surname><given-names>FW</given-names></name><name><surname>Peters</surname><given-names>EM</given-names></name><name><surname>Palmer</surname><given-names>J</given-names></name></person-group><article-title>The Eyelink Toolbox: Eye tracking with MATLAB and the Psychophysics Toolbox</article-title><source>Behavior Research Methods, Instruments, &amp; Computers</source><year>2002</year><volume>34</volume><issue>4</issue><fpage>613</fpage><lpage>617</lpage><pub-id pub-id-type="pmid">12564564</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cowan</surname><given-names>N</given-names></name></person-group><article-title>The magical number 4 in short-term memory: A reconsideration of mental storage capacity</article-title><source>Behavioral and Brain Sciences</source><year>2001</year><volume>24</volume><issue>1</issue><fpage>87</fpage><lpage>114</lpage><pub-id pub-id-type="pmid">11515286</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Vries</surname><given-names>E</given-names></name><name><surname>Fejer</surname><given-names>G</given-names></name><name><surname>Van Ede</surname><given-names>F</given-names></name></person-group><article-title>No trade-off between the use of space and time for working memory [Preprint]</article-title><source>Neuroscience</source><year>2023</year><pub-id pub-id-type="doi">10.1101/2023.01.20.524861</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Vries</surname><given-names>E</given-names></name><name><surname>Van Ede</surname><given-names>F</given-names></name></person-group><article-title>Microsaccades track location-based object rehearsal in visual working memory [Preprint]</article-title><source>Neuroscience</source><year>2023</year><pub-id pub-id-type="doi">10.1101/2023.03.21.533618</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>D‚ÄôEsposito</surname><given-names>M</given-names></name><name><surname>Postle</surname><given-names>BR</given-names></name></person-group><article-title>The Cognitive Neuroscience of Working Memory</article-title><source>Annual Review of Psychology</source><year>2015</year><volume>66</volume><issue>1</issue><fpage>115</fpage><lpage>142</lpage><pub-id pub-id-type="pmcid">PMC4374359</pub-id><pub-id pub-id-type="pmid">25251486</pub-id><pub-id pub-id-type="doi">10.1146/annurev-psych-010814-015031</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Emrich</surname><given-names>SM</given-names></name><name><surname>Lockhart</surname><given-names>HA</given-names></name><name><surname>Al-Aidroos</surname><given-names>N</given-names></name></person-group><article-title>Attention mediates the flexible allocation of visual working memory resources</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>2017</year><volume>43</volume><issue>7</issue><fpage>1454</fpage><pub-id pub-id-type="pmid">28368161</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engbert</surname><given-names>R</given-names></name><name><surname>Kliegl</surname><given-names>R</given-names></name></person-group><article-title>Microsaccades uncover the orientation of covert attention</article-title><source>Vision Research</source><year>2003</year><volume>43</volume><issue>9</issue><fpage>1035</fpage><lpage>1045</lpage><pub-id pub-id-type="pmid">12676246</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ester</surname><given-names>EF</given-names></name><name><surname>Sprague</surname><given-names>TC</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><article-title>Categorical Biases in Human Occipitoparietal Cortex</article-title><source>The Journal of Neuroscience</source><year>2020</year><volume>40</volume><issue>4</issue><fpage>917</fpage><lpage>931</lpage><pub-id pub-id-type="pmcid">PMC6975303</pub-id><pub-id pub-id-type="pmid">31862856</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2700-19.2019</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ester</surname><given-names>EF</given-names></name><name><surname>Sutterer</surname><given-names>DW</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name><name><surname>Awh</surname><given-names>E</given-names></name></person-group><article-title>Feature-Selective Attentional Modulations in Human Frontoparietal Cortex</article-title><source>Journal of Neuroscience</source><year>2016</year><volume>36</volume><issue>31</issue><fpage>8188</fpage><lpage>8199</lpage><pub-id pub-id-type="pmcid">PMC4971365</pub-id><pub-id pub-id-type="pmid">27488638</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3935-15.2016</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferreira</surname><given-names>F</given-names></name><name><surname>Apel</surname><given-names>J</given-names></name><name><surname>Henderson</surname><given-names>JM</given-names></name></person-group><article-title>Taking a new look at looking at nothing</article-title><source>Trends in Cognitive Sciences</source><year>2008</year><volume>12</volume><issue>11</issue><fpage>405</fpage><lpage>410</lpage><pub-id pub-id-type="pmid">18805041</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldman-Rakic</surname><given-names>PS</given-names></name></person-group><article-title>Architecture of the Prefrontal Cortex and the Central Executive</article-title><source>Annals of the New York Academy of Sciences</source><year>1995</year><volume>769</volume><fpage>71</fpage><lpage>84</lpage><comment>(1 Structure and)</comment><pub-id pub-id-type="pmid">8595045</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>G√ºnseli</surname><given-names>E</given-names></name><name><surname>Foster</surname><given-names>J</given-names></name><name><surname>Sutterer</surname><given-names>D</given-names></name><name><surname>Todorova</surname><given-names>L</given-names></name><name><surname>Vogel</surname><given-names>EK</given-names></name><name><surname>Awh</surname><given-names>E</given-names></name></person-group><article-title>Overlapping Neural Representations for Dynamic Visual Imagery and Stationary Storage in Spatial Working Memory</article-title><source>BioRxiv</source><year>2022</year></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hafed</surname><given-names>ZM</given-names></name><name><surname>Clark</surname><given-names>JJ</given-names></name></person-group><article-title>Microsaccades as an overt measure of covert attention shifts</article-title><source>Vision Research</source><year>2002</year><volume>42</volume><issue>22</issue><fpage>2533</fpage><lpage>2545</lpage><pub-id pub-id-type="pmid">12445847</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hardman</surname><given-names>KO</given-names></name><name><surname>Vergauwe</surname><given-names>E</given-names></name><name><surname>Ricker</surname><given-names>TJ</given-names></name></person-group><article-title>Categorical working memory representations are used in delayed estimation of continuous colors</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>2017</year><volume>43</volume><issue>1</issue><fpage>30</fpage><lpage>54</lpage><pub-id pub-id-type="pmcid">PMC5179301</pub-id><pub-id pub-id-type="pmid">27797548</pub-id><pub-id pub-id-type="doi">10.1037/xhp0000290</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harrison</surname><given-names>SA</given-names></name><name><surname>Tong</surname><given-names>F</given-names></name></person-group><article-title>Decoding reveals the contents of visual working memory in early visual areas</article-title><source>Nature</source><year>2009</year><volume>458</volume><issue>7238</issue><fpage>632</fpage><lpage>635</lpage><pub-id pub-id-type="pmcid">PMC2709809</pub-id><pub-id pub-id-type="pmid">19225460</pub-id><pub-id pub-id-type="doi">10.1038/nature07832</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johansson</surname><given-names>R</given-names></name><name><surname>Johansson</surname><given-names>M</given-names></name></person-group><article-title>Look here, eye movements play a functional role in memory retrieval</article-title><source>Psychological Science</source><year>2014</year><volume>25</volume><issue>1</issue><fpage>236</fpage><lpage>242</lpage><pub-id pub-id-type="pmid">24166856</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kang</surname><given-names>Z</given-names></name><name><surname>Spitzer</surname><given-names>B</given-names></name></person-group><article-title>Concurrent visual working memory bias in sequential integration of approximate number</article-title><source>Scientific Reports</source><year>2021</year><volume>11</volume><issue>1</issue><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="pmcid">PMC7935854</pub-id><pub-id pub-id-type="pmid">33674642</pub-id><pub-id pub-id-type="doi">10.1038/s41598-021-84232-7</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kerr√©n</surname><given-names>C</given-names></name><name><surname>Linde-Domingo</surname><given-names>J</given-names></name><name><surname>Spitzer</surname><given-names>B</given-names></name></person-group><article-title>Prioritization of semantic over visuo-perceptual aspects in multi-item working memory [Preprint]</article-title><source>Neuroscience</source><year>2022</year><pub-id pub-id-type="doi">10.1101/2022.06.29.498168</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname><given-names>J-R</given-names></name><name><surname>Pescetelli</surname><given-names>N</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><article-title>Brain Mechanisms Underlying the Brief Maintenance of Seen and Unseen Sensory Information</article-title><source>Neuron</source><year>2016</year><volume>92</volume><issue>5</issue><fpage>1122</fpage><lpage>1134</lpage><pub-id pub-id-type="pmid">27930903</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Kievit</surname><given-names>RA</given-names></name></person-group><article-title>Representational geometry: Integrating cognition, computation, and the brain</article-title><source>Trends in Cognitive Sciences</source><year>2013</year><volume>17</volume><issue>8</issue><fpage>401</fpage><lpage>412</lpage><pub-id pub-id-type="pmcid">PMC3730178</pub-id><pub-id pub-id-type="pmid">23876494</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2013.06.007</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kwak</surname><given-names>Y</given-names></name><name><surname>Curtis</surname><given-names>CE</given-names></name></person-group><article-title>Unveiling the abstract format of mnemonic representations</article-title><source>Neuron</source><year>2022</year><volume>110</volume><issue>11</issue><fpage>1822</fpage><lpage>1828</lpage><pub-id pub-id-type="pmcid">PMC9167733</pub-id><pub-id pub-id-type="pmid">35395195</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2022.03.016</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LaRocque</surname><given-names>JJ</given-names></name><name><surname>Lewis-Peacock</surname><given-names>JA</given-names></name><name><surname>Drysdale</surname><given-names>AT</given-names></name><name><surname>Oberauer</surname><given-names>K</given-names></name><name><surname>Postle</surname><given-names>BR</given-names></name></person-group><article-title>Decoding attended information in short-term memory: An EEG study</article-title><source>Journal of Cognitive Neuroscience</source><year>2013</year><volume>25</volume><issue>1</issue><fpage>127</fpage><lpage>142</lpage><pub-id pub-id-type="pmcid">PMC3775605</pub-id><pub-id pub-id-type="pmid">23198894</pub-id><pub-id pub-id-type="doi">10.1162/jocn_a_00305</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewis-Peacock</surname><given-names>JA</given-names></name><name><surname>Postle</surname><given-names>BR</given-names></name></person-group><article-title>Decoding the internal focus of attention</article-title><source>Neuropsychologia</source><year>2012</year><volume>50</volume><issue>4</issue><fpage>470</fpage><lpage>478</lpage><pub-id pub-id-type="pmcid">PMC3288445</pub-id><pub-id pub-id-type="pmid">22108440</pub-id><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2011.11.006</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>B</given-names></name><name><surname>Alexopoulou</surname><given-names>Z-S</given-names></name><name><surname>Van Ede</surname><given-names>F</given-names></name></person-group><article-title>Jointly looking to the past and the future in visual working memory [Preprint]</article-title><source>Neuroscience</source><year>2023</year><pub-id pub-id-type="doi">10.1101/2023.01.30.526235</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>B</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name><name><surname>van Ede</surname><given-names>F</given-names></name></person-group><article-title>Functional but not obligatory link between microsaccades and neural modulation by covert spatial attention</article-title><source>Nature Communications</source><year>2022</year><volume>13</volume><issue>1</issue><elocation-id>3503</elocation-id><pub-id pub-id-type="pmcid">PMC9205986</pub-id><pub-id pub-id-type="pmid">35715471</pub-id><pub-id pub-id-type="doi">10.1038/s41467-022-31217-3</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loaiza</surname><given-names>VM</given-names></name><name><surname>Souza</surname><given-names>AS</given-names></name></person-group><article-title>The eyes don‚Äôt have it: Eye movements are unlikely to reflect refreshing in working memory</article-title><source>PloS One</source><year>2022</year><volume>17</volume><issue>7</issue><elocation-id>e0271116</elocation-id><pub-id pub-id-type="pmcid">PMC9282440</pub-id><pub-id pub-id-type="pmid">35834590</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0271116</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luck</surname><given-names>SJ</given-names></name><name><surname>Vogel</surname><given-names>EK</given-names></name></person-group><article-title>The capacity of visual working memory for features and conjunctions</article-title><source>Nature</source><year>1997</year><volume>390</volume><issue>6657</issue><fpage>279</fpage><lpage>281</lpage><pub-id pub-id-type="pmid">9384378</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Husain</surname><given-names>M</given-names></name><name><surname>Bays</surname><given-names>PM</given-names></name></person-group><article-title>Changing concepts of working memory</article-title><source>Nature Neuroscience</source><year>2014</year><volume>17</volume><issue>3</issue><fpage>347</fpage><lpage>356</lpage><pub-id pub-id-type="pmcid">PMC4159388</pub-id><pub-id pub-id-type="pmid">24569831</pub-id><pub-id pub-id-type="doi">10.1038/nn.3655</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title><source>Journal of Neuroscience Methods</source><year>2007</year><volume>164</volume><issue>1</issue><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="pmid">17517438</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Lundqvist</surname><given-names>M</given-names></name><name><surname>Bastos</surname><given-names>AM</given-names></name></person-group><article-title>Working Memory 2.0</article-title><source>Neuron</source><year>2018</year><volume>100</volume><issue>2</issue><fpage>463</fpage><lpage>475</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.09.023</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mostert</surname><given-names>P</given-names></name><name><surname>Albers</surname><given-names>AM</given-names></name><name><surname>Brinkman</surname><given-names>L</given-names></name><name><surname>Todorova</surname><given-names>L</given-names></name><name><surname>Kok</surname><given-names>P</given-names></name><name><surname>De Lange</surname><given-names>FP</given-names></name></person-group><article-title>Eye movement-related confounds in neural decoding of visual working memory representations</article-title><source>Eneuro</source><year>2018</year><volume>5</volume><fpage>4</fpage><pub-id pub-id-type="pmcid">PMC6179574</pub-id><pub-id pub-id-type="pmid">30310862</pub-id><pub-id pub-id-type="doi">10.1523/ENEURO.0401-17.2018</pub-id></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muhle-Karbe</surname><given-names>PS</given-names></name><name><surname>Myers</surname><given-names>NE</given-names></name><name><surname>Stokes</surname><given-names>MG</given-names></name></person-group><article-title>A hierarchy of functional states in working memory</article-title><source>Journal of Neuroscience</source><year>2021</year><volume>41</volume><issue>20</issue><fpage>4461</fpage><lpage>4475</lpage><pub-id pub-id-type="pmcid">PMC8152603</pub-id><pub-id pub-id-type="pmid">33888611</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3104-20.2021</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oberauer</surname><given-names>K</given-names></name></person-group><article-title>Access to information in working memory: Exploring the focus of attention. Journal of Experimental Psychology: Learning</article-title><source>Memory, and Cognition</source><year>2002</year><volume>28</volume><issue>3</issue><fpage>411</fpage><lpage>421</lpage><pub-id pub-id-type="pmid">12018494</pub-id></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olivers</surname><given-names>CNL</given-names></name><name><surname>Peters</surname><given-names>J</given-names></name><name><surname>Houtkamp</surname><given-names>R</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><article-title>Different states in visual working memory: When it guides attention and when it does not</article-title><source>Trends in Cognitive Sciences</source><year>2011</year><volume>15</volume><issue>7</issue><fpage>327</fpage><lpage>334</lpage><pub-id pub-id-type="pmid">21665518</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panichello</surname><given-names>MF</given-names></name><name><surname>DePasquale</surname><given-names>B</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name><name><surname>Buschman</surname><given-names>TJ</given-names></name></person-group><article-title>Error-correcting dynamics in visual working memory</article-title><source>Nature Communications</source><year>2019</year><volume>10</volume><issue>1</issue><elocation-id>3366</elocation-id><pub-id pub-id-type="pmcid">PMC6662698</pub-id><pub-id pub-id-type="pmid">31358740</pub-id><pub-id pub-id-type="doi">10.1038/s41467-019-11298-3</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pratte</surname><given-names>MS</given-names></name><name><surname>Park</surname><given-names>YE</given-names></name><name><surname>Rademaker</surname><given-names>RL</given-names></name><name><surname>Tong</surname><given-names>F</given-names></name></person-group><article-title>Accounting for stimulus-specific variation in precision reveals a discrete capacity limit in visual working memory</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>2017</year><volume>43</volume><issue>1</issue><fpage>6</fpage><pub-id pub-id-type="pmcid">PMC5189913</pub-id><pub-id pub-id-type="pmid">28004957</pub-id><pub-id pub-id-type="doi">10.1037/xhp0000302</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quax</surname><given-names>SC</given-names></name><name><surname>Dijkstra</surname><given-names>N</given-names></name><name><surname>van Staveren</surname><given-names>MJ</given-names></name><name><surname>Bosch</surname><given-names>SE</given-names></name><name><surname>van Gerven</surname><given-names>MA</given-names></name></person-group><article-title>Eye movements explain decodability during perception and cued attention in MEG</article-title><source>Neuroimage</source><year>2019</year><volume>195</volume><fpage>444</fpage><lpage>453</lpage><pub-id pub-id-type="pmid">30951848</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rademaker</surname><given-names>RL</given-names></name><name><surname>Chunharas</surname><given-names>C</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><article-title>Coexisting representations of sensory and mnemonic information in human visual cortex</article-title><source>Nature Neuroscience</source><year>2019</year><volume>22</volume><issue>8</issue><fpage>1336</fpage><lpage>1344</lpage><pub-id pub-id-type="pmcid">PMC6857532</pub-id><pub-id pub-id-type="pmid">31263205</pub-id><pub-id pub-id-type="doi">10.1038/s41593-019-0428-x</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ricker</surname><given-names>TJ</given-names></name><name><surname>Cowan</surname><given-names>N</given-names></name></person-group><article-title>Loss of visual working memory within seconds: The combined use of refreshable and non-refreshable features. Journal of Experimental Psychology: Learning</article-title><source>Memory, and Cognition</source><year>2010</year><volume>36</volume><issue>6</issue><fpage>1355</fpage><pub-id pub-id-type="pmcid">PMC2970679</pub-id><pub-id pub-id-type="pmid">20804281</pub-id><pub-id pub-id-type="doi">10.1037/a0020356</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ricker</surname><given-names>T</given-names></name><name><surname>Souza</surname><given-names>AS</given-names></name><name><surname>Vergauwe</surname><given-names>E</given-names></name></person-group><article-title>Feature identity determines representation structure in working memory [Preprint]</article-title><source>Open Science Framework</source><year>2022</year><pub-id pub-id-type="pmid">37166844</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riggall</surname><given-names>AC</given-names></name><name><surname>Postle</surname><given-names>BR</given-names></name></person-group><article-title>The Relationship between Working Memory Storage and Elevated Activity as Measured with Functional Magnetic Resonance Imaging</article-title><source>Journal of Neuroscience</source><year>2012</year><volume>32</volume><issue>38</issue><fpage>12990</fpage><lpage>12998</lpage><pub-id pub-id-type="pmcid">PMC3470886</pub-id><pub-id pub-id-type="pmid">22993416</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1892-12.2012</pub-id></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolfs</surname><given-names>M</given-names></name></person-group><article-title>Microsaccades: Small steps on a long way</article-title><source>Vision Research</source><year>2009</year><volume>49</volume><issue>20</issue><fpage>2415</fpage><lpage>2441</lpage><pub-id pub-id-type="pmid">19683016</pub-id></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Romo</surname><given-names>R</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name><name><surname>Hern√°ndez</surname><given-names>A</given-names></name><name><surname>Lemus</surname><given-names>L</given-names></name></person-group><article-title>Neuronal correlates of parametric working memory in the prefrontal cortex</article-title><source>Nature</source><year>1999</year><volume>399</volume><issue>6735</issue><fpage>470</fpage><lpage>473</lpage><pub-id pub-id-type="pmid">10365959</pub-id></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rose</surname><given-names>NS</given-names></name><name><surname>LaRocque</surname><given-names>JJ</given-names></name><name><surname>Riggall</surname><given-names>AC</given-names></name><name><surname>Gosseries</surname><given-names>O</given-names></name><name><surname>Starrett</surname><given-names>MJ</given-names></name><name><surname>Meyering</surname><given-names>EE</given-names></name><name><surname>Postle</surname><given-names>BR</given-names></name></person-group><article-title>Reactivation of latent working memories with transcranial magnetic stimulation</article-title><source>Science</source><year>2016</year><volume>354</volume><issue>6316</issue><fpage>1136</fpage><lpage>1139</lpage><pub-id pub-id-type="pmcid">PMC5221753</pub-id><pub-id pub-id-type="pmid">27934762</pub-id><pub-id pub-id-type="doi">10.1126/science.aah7011</pub-id></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schurgin</surname><given-names>MW</given-names></name><name><surname>Wixted</surname><given-names>JT</given-names></name><name><surname>Brady</surname><given-names>TF</given-names></name></person-group><article-title>Psychophysical scaling reveals a unified theory of visual memory strength</article-title><source>Nature Human Behaviour</source><year>2020</year><volume>4</volume><issue>11</issue><fpage>1156</fpage><lpage>1172</lpage><pub-id pub-id-type="pmid">32895546</pub-id></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serences</surname><given-names>JT</given-names></name><name><surname>Ester</surname><given-names>EF</given-names></name><name><surname>Vogel</surname><given-names>EK</given-names></name><name><surname>Awh</surname><given-names>E</given-names></name></person-group><article-title>Stimulus-Specific Delay Activity in Human Primary Visual Cortex</article-title><source>Psychological Science</source><year>2009</year><volume>20</volume><issue>2</issue><fpage>207</fpage><lpage>214</lpage><pub-id pub-id-type="pmcid">PMC2875116</pub-id><pub-id pub-id-type="pmid">19170936</pub-id><pub-id pub-id-type="doi">10.1111/j.1467-9280.2009.02276.x</pub-id></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Souza</surname><given-names>AS</given-names></name><name><surname>Oberauer</surname><given-names>K</given-names></name></person-group><article-title>In search of the focus of attention in working memory: 13 years of the retro-cue effect</article-title><source>Attention, Perception &amp; Psychophysics</source><year>2016</year><volume>78</volume><issue>7</issue><fpage>1839</fpage><lpage>1860</lpage><pub-id pub-id-type="pmid">27098647</pub-id></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sperling</surname><given-names>G</given-names></name></person-group><article-title>The information available in brief visual presentations</article-title><source>Psychological Monographs: General and Applied</source><year>1960</year><volume>74</volume><issue>11</issue><fpage>1</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1037/h0093759</pub-id></element-citation></ref><ref id="R64"><label>64</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spitzer</surname><given-names>B</given-names></name><name><surname>Blankenburg</surname><given-names>F</given-names></name></person-group><article-title>Stimulus-dependent EEG activity reflects internal updating of tactile working memory in humans</article-title><source>Proceedings of the National Academy of Sciences</source><year>2011</year><volume>108</volume><issue>20</issue><fpage>8444</fpage><lpage>8449</lpage><pub-id pub-id-type="pmcid">PMC3100957</pub-id><pub-id pub-id-type="pmid">21536865</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1104189108</pub-id></element-citation></ref><ref id="R65"><label>65</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spitzer</surname><given-names>B</given-names></name><name><surname>Blankenburg</surname><given-names>F</given-names></name></person-group><article-title>Supramodal Parametric Working Memory Processing in Humans</article-title><source>Journal of Neuroscience</source><year>2012</year><volume>32</volume><issue>10</issue><fpage>3287</fpage><lpage>3295</lpage><pub-id pub-id-type="pmcid">PMC6621033</pub-id><pub-id pub-id-type="pmid">22399750</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5280-11.2012</pub-id></element-citation></ref><ref id="R66"><label>66</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spitzer</surname><given-names>B</given-names></name><name><surname>Fleck</surname><given-names>S</given-names></name><name><surname>Blankenburg</surname><given-names>F</given-names></name></person-group><article-title>Parametric Alpha- and Beta-Band Signatures of Supramodal Numerosity Information in Human Working Memory</article-title><source>Journal of Neuroscience</source><year>2014</year><volume>34</volume><issue>12</issue><fpage>4293</fpage><lpage>4302</lpage><pub-id pub-id-type="pmcid">PMC6608098</pub-id><pub-id pub-id-type="pmid">24647949</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4580-13.2014</pub-id></element-citation></ref><ref id="R67"><label>67</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stokes</surname><given-names>MG</given-names></name></person-group><article-title>‚ÄúActivity-silent‚Äù working memory in prefrontal cortex: A dynamic coding framework</article-title><source>Trends in Cognitive Sciences</source><year>2015</year><volume>19</volume><issue>7</issue><fpage>394</fpage><lpage>405</lpage><pub-id pub-id-type="pmcid">PMC4509720</pub-id><pub-id pub-id-type="pmid">26051384</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2015.05.004</pub-id></element-citation></ref><ref id="R68"><label>68</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taylor</surname><given-names>R</given-names></name><name><surname>Bays</surname><given-names>PM</given-names></name></person-group><article-title>Efficient coding in visual working memory accounts for stimulus-specific variations in recall</article-title><source>Journal of Neuroscience</source><year>2018</year><volume>38</volume><issue>32</issue><fpage>7132</fpage><lpage>7142</lpage><pub-id pub-id-type="pmcid">PMC6083451</pub-id><pub-id pub-id-type="pmid">30006363</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1018-18.2018</pub-id></element-citation></ref><ref id="R69"><label>69</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teng</surname><given-names>C</given-names></name><name><surname>Kravitz</surname><given-names>DJ</given-names></name></person-group><article-title>Visual working memory directly alters perception</article-title><source>Nature Human Behaviour</source><year>2019</year><volume>3</volume><issue>8</issue><fpage>827</fpage><lpage>836</lpage><pub-id pub-id-type="pmid">31285620</pub-id></element-citation></ref><ref id="R70"><label>70</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thielen</surname><given-names>J</given-names></name><name><surname>Bosch</surname><given-names>SE</given-names></name><name><surname>van Leeuwen</surname><given-names>TM</given-names></name><name><surname>van Gerven</surname><given-names>MAJ</given-names></name><name><surname>van Lier</surname><given-names>R</given-names></name></person-group><article-title>Evidence for confounding eye movements under attempted fixation and active viewing in cognitive neuroscience</article-title><source>Scientific Reports</source><year>2019</year><volume>9</volume><issue>1</issue><elocation-id>17456</elocation-id><pub-id pub-id-type="pmcid">PMC6877555</pub-id><pub-id pub-id-type="pmid">31767911</pub-id><pub-id pub-id-type="doi">10.1038/s41598-019-54018-z</pub-id></element-citation></ref><ref id="R71"><label>71</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Ede</surname><given-names>F</given-names></name><name><surname>Chekroud</surname><given-names>SR</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name></person-group><article-title>Human gaze tracks attentional focusing in memorized visual space</article-title><source>Nature Human Behaviour</source><year>2019</year><volume>3</volume><issue>5</issue><fpage>462</fpage><lpage>470</lpage><pub-id pub-id-type="pmcid">PMC6546593</pub-id><pub-id pub-id-type="pmid">31089296</pub-id><pub-id pub-id-type="doi">10.1038/s41562-019-0549-y</pub-id></element-citation></ref><ref id="R72"><label>72</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vergara</surname><given-names>J</given-names></name><name><surname>Rivera</surname><given-names>N</given-names></name><name><surname>Rossi-Pool</surname><given-names>R</given-names></name><name><surname>Romo</surname><given-names>R</given-names></name></person-group><article-title>A Neural Parametric Code for Storing Information of More than One Sensory Modality in Working Memory</article-title><source>Neuron</source><year>2016</year><volume>89</volume><issue>1</issue><fpage>54</fpage><lpage>62</lpage><pub-id pub-id-type="pmid">26711117</pub-id></element-citation></ref><ref id="R73"><label>73</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vergauwe</surname><given-names>E</given-names></name><name><surname>Camos</surname><given-names>V</given-names></name><name><surname>Barrouillet</surname><given-names>P</given-names></name></person-group><article-title>The impact of storage on processing: How is information maintained in working memory? Journal of Experimental Psychology: Learning</article-title><source>Memory, and Cognition</source><year>2014</year><volume>40</volume><issue>4</issue><fpage>1072</fpage><lpage>1095</lpage><pub-id pub-id-type="pmid">24564542</pub-id></element-citation></ref><ref id="R74"><label>74</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>X-J</given-names></name></person-group><article-title>50 years of mnemonic persistent activity: Quo vadis?</article-title><source>Trends in Neurosciences</source><year>2021</year><volume>44</volume><issue>11</issue><fpage>888</fpage><lpage>902</lpage><pub-id pub-id-type="pmcid">PMC9087306</pub-id><pub-id pub-id-type="pmid">34654556</pub-id><pub-id pub-id-type="doi">10.1016/j.tins.2021.09.001</pub-id></element-citation></ref><ref id="R75"><label>75</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watanabe</surname><given-names>K</given-names></name><name><surname>Funahashi</surname><given-names>S</given-names></name></person-group><article-title>Prefrontal delay-period activity reflects the decision process of a saccade direction during a free-choice ODR task</article-title><source>Cerebral Cortex</source><year>2007</year><volume>17</volume><issue>suppl_1</issue><fpage>i88</fpage><lpage>i100</lpage><pub-id pub-id-type="pmid">17726006</pub-id></element-citation></ref><ref id="R76"><label>76</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolff</surname><given-names>MJ</given-names></name><name><surname>Jochim</surname><given-names>J</given-names></name><name><surname>Aky√ºrek</surname><given-names>EG</given-names></name><name><surname>Buschman</surname><given-names>TJ</given-names></name><name><surname>Stokes</surname><given-names>MG</given-names></name></person-group><article-title>Drifting codes within a stable coding scheme for working memory</article-title><source>PLoS Biology</source><year>2020</year><volume>18</volume><issue>3</issue><fpage>1</fpage><lpage>19</lpage><pub-id pub-id-type="pmcid">PMC7067474</pub-id><pub-id pub-id-type="pmid">32119658</pub-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3000625</pub-id></element-citation></ref><ref id="R77"><label>77</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolff</surname><given-names>MJ</given-names></name><name><surname>Jochim</surname><given-names>J</given-names></name><name><surname>Aky√ºrek</surname><given-names>EG</given-names></name><name><surname>Stokes</surname><given-names>MG</given-names></name></person-group><article-title>Dynamic hidden states underlying working-memory-guided behavior</article-title><source>Nature Neuroscience</source><year>2017</year><volume>20</volume><issue>6</issue><fpage>864</fpage><lpage>871</lpage><pub-id pub-id-type="pmcid">PMC5446784</pub-id><pub-id pub-id-type="pmid">28414333</pub-id><pub-id pub-id-type="doi">10.1038/nn.4546</pub-id></element-citation></ref><ref id="R78"><label>78</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>Q</given-names></name><name><surname>Panichello</surname><given-names>MF</given-names></name><name><surname>Cai</surname><given-names>Y</given-names></name><name><surname>Postle</surname><given-names>BR</given-names></name><name><surname>Buschman</surname><given-names>TJ</given-names></name></person-group><article-title>Delay-period activity in frontal, parietal, and occipital cortex tracks noise and biases in visual working memory</article-title><source>PLOS Biology</source><year>2020</year><volume>18</volume><issue>9</issue><elocation-id>e3000854</elocation-id><pub-id pub-id-type="pmcid">PMC7500688</pub-id><pub-id pub-id-type="pmid">32898172</pub-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3000854</pub-id></element-citation></ref><ref id="R79"><label>79</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>B</given-names></name><name><surname>Liu</surname><given-names>S</given-names></name><name><surname>Doro</surname><given-names>M</given-names></name><name><surname>Galfano</surname><given-names>G</given-names></name></person-group><article-title>Attentional Guidance from Multiple Working Memory Representations: Evidence from Eye Movements</article-title><source>Scientific Reports</source><year>2018</year><volume>8</volume><issue>1</issue><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="pmcid">PMC6141570</pub-id><pub-id pub-id-type="pmid">30224710</pub-id><pub-id pub-id-type="doi">10.1038/s41598-018-32144-4</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Eye-tracking during working memory for visual object orientation.</title><p><bold>a</bold>, Example trial. After presentation of two randomly oriented objects (Stimulus 1 and 2) an auditory cue indicated which of the two stimulus orientations was to be remembered after an unfilled retention interval (Delay 1). At test, participants were asked to re-rotate the probe stimulus to its memorized orientation (2-AFC). On half of the trials (randomized), participants were subsequently cued to also remember the orientation of previously uncued stimulus after another retention period (Delay 2 and Test 2). Participants were instructed to fixate a centered dot throughout the delay periods, and fixation breaks were penalized with closed-loop feedback from online eye-tracking (see <xref ref-type="sec" rid="S12">Methods</xref>). <bold>b,</bold> Across trials, the orientations of the memory items were randomly varied around the full circle (in 16 steps, excluding cardinal orientations). <bold>c,</bold> Illustration of the gaze data relative to visual stimulus size. Heatmaps show gaze densities (aggregated across subjects) after aligning (re-rotating) the data from each trial to the object‚Äôs upright (90¬∞) position. Panels show the densities aggregated over different trial periods (see <italic>a</italic>), with rotational alignment to the currently relevant stimulus orientation (see corresponding example objects in <italic>a</italic>). <bold>d</bold>, Mean gaze positions (without rotational alignment) during the trial periods in <italic>c</italic>, for the 16 different orientations of the currently relevant stimulus (see color legend in <italic>left</italic>). Plots show magnification (5x) of the central display area outlined in <italic>c, left</italic>. Saturated dots, mean; unsaturated dots, individual participants.</p></caption><graphic xlink:href="EMS157281-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Single-trial RSA and orientation encoding time courses.</title><p><bold>a</bold>, <italic>Left</italic>: Stimulus orientations (see <xref ref-type="fig" rid="F1">Fig. 1b</xref>) plotted as points on the unit circle. <italic>Right</italic>: Pairwise Euclidean distances between the circle points in <italic>a</italic>. <bold>b</bold>, RSA was performed at the single trial level. <italic>Left</italic>: Representational dissimilarity vector (RDV) of the Euclidean distances between the gaze position on the current trial (current orientation highlighted by red squares) and the mean gaze positions associated with the 16 different orientations (trial averages with the current trial data held out). Gaze RDVs were obtained at each time point and correlated trial-by-trial with the corresponding model RDV (i.e., the distances predicted by the circular orientation model in <italic>a</italic>), yielding a time course of orientation encoding for each trial (see <xref ref-type="sec" rid="S12">Methods</xref>). <italic>Right</italic>, the single-trial approach yields a mean time-course of orientation encoding as would be obtained with conventional RSA (black lines) while additionally retaining trial-by-trial variability (gray shadings). <bold>c</bold>, Mean encoding of the two stimulus orientations, shown separately for when Stimulus 1 or Stimulus 2 was cued for Test 1 (see <xref ref-type="fig" rid="F1">Fig. 1a</xref>). Vertical dotted lines indicate the times of stimulus onset. Gray vertical bars indicate duration of stimuli and auditory cues (‚Äúone‚Äù/‚Äùtwo‚Äù). Colored shadings show SEM. Colored marker lines at the bottom indicate significant orientation encoding (display threshold p<sub>cluster</sub> &lt; 0.0125 to account for testing in 4 conditions). <bold>d</bold>, Same as <italic>c</italic>, for the second delay period (Delay 2, see <xref ref-type="fig" rid="F1">Fig. 1a</xref>, <italic>right</italic>). Note that the strong pre-cue effect is explained by residual eye-movements related to Test 1. <bold>e</bold>, Single-trial analysis of Stimulus 1 orientation encoding concurrent to encoding the orientation of Stimulus 2 (see time window outlined by black marker in <italic>c</italic> at the bottom). Trials were binned according to orientation encoding strength for Stimulus 2 (x-axis; mean values of bins averaged over participants), with orientation encoding strength for Stimulus 1 plotted on the y-axis. Error bars show SEM. Asterisks indicate significant differences from 0 (p &lt; 0.05, Bonferroni-corrected). Significant encoding of the orientation of Stimulus 1 was evident at each bin, indicating that gaze position carried information about both stimulus orientations simultaneously.</p></caption><graphic xlink:href="EMS157281-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Object-specific vs. object-independent orientation encoding.</title><p><bold>a</bold>, Orientation model analogous to <xref ref-type="fig" rid="F2">Fig. 2a</xref>, but extended to separately examine orientation encoding within and between objects. <italic>Left</italic>, within-objects orientation encoding; <italic>Right</italic>, between-objects orientation encoding. Unsaturated colors delineate distances that are excluded from the respective sub-model. The degree of object-specificity is inferred from the extent to which within-objects encoding is stronger than between-objects encoding. <bold>b</bold>, Difference in orientation encoding within objects compared to between objects, for the cued stimulus during each delay period. Data are averaged from cue onset to the end of the delay. The four conditions are sorted by the time distance from stimulus presentation (‚Äúmnemonic distance‚Äù, from lowest to highest). Gray dots show individual participants results and trend lines show linear fits. Box plots show group means with SEM (boxes) and SD (vertical lines). <bold>c</bold>, Orientation encoding time courses within and between objects during Delay 1, shown separately for when the stimulus was cued (pink- and purple) or uncued (light and dark green). <italic>Top</italic>: Stimulus 1; <italic>bottom</italic>: Stimulus 2. Colored shadings show SEM. Colored marker lines on the bottom indicate significant object specificity in terms of stronger orientation encoding within than between objects (display threshold p<sub>cluster</sub> &lt; 0.0125). Otherwise same conventions as <xref ref-type="fig" rid="F2">Fig 2c</xref>. <bold>d</bold>, Same as <italic>c</italic>, but for the second delay period (Delay 2). <bold>e-f</bold>, Bayes Factor (BF<sub>01</sub>, one-tailed) analysis of the difference between within- and between-objects encoding. BF time courses are shown for the cued orientation in the respective task periods. Negative values on the log scale (y-axis) indicate stronger evidence for object-specific encoding (within &gt; between) than for object-independent encoding (within ‚â§ between), positive values indicate the opposite. Results are shown for periods of significant overall orientation encoding (cf. Fg. 2) where the comparison of within- and between objects encoding is meaningful. The data were smoothed with a 50 ms Gaussian kernel before this analysis. Saturated colors indicate stronger than anecdotal evidence (logBF &lt; -1.1 or &gt; 1.1, which corresponds to BFs &lt;1/3 or &gt; 3).</p></caption><graphic xlink:href="EMS157281-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Cardinal repulsion bias in gaze patterns and behavior.</title><p><bold>a</bold>, Model-predicted geometries (top) and RDMs (bottom) associated with different levels of cardinal bias. The mixture parameter ùêµ (black arrow axis on bottom) denotes the level of repulsion (ùêµ &gt; 1; ‚Äúcardinal bias‚Äù) or attraction (ùêµ &lt; 1) from/to cardinal axes, relative to the unbiased circle model (ùêµ = 0). <bold>b</bold>, <italic>Left and Middle:</italic> Results from the model fitted to the behavioral memory reports <italic>Left</italic>: Estimates of cardinal repulsion bias (ùêµ) for each stimulus (1/2) and test (1/2), sorted by the distance between stimulus presentation and test (same conventions as <xref ref-type="fig" rid="F3">Figure 3b</xref>). <italic>Middle</italic>: Polar plot shows the mean proportions of clockwise reports (green) and the predictions of the fitted model (magenta; ùêµ = 0.12) for each stimulus orientation. Results are averaged over both stimuli and tests. Dashed black line shows proportions (50%) expected under an unbiased circular model (ùêµ = 0) for visual reference. <italic>Right</italic>: Quantification of bias in the gaze patterns associated with the cued stimulus orientation. Shown are the differences in correlation of the gaze patterns with the repulsion model (ùêµ = 1) compared to the attraction model (ùêµ = -1), where a positive difference indicates repulsive cardinal bias. Results are shown for the last second of the respective delay period (see <italic>c</italic>). <bold>c,</bold> Time course of correlations with the repulsion (red) and attraction (blue) models during the delay periods (same layout as <xref ref-type="fig" rid="F3">Fig. 3c-d</xref>). Colored shadings show SEM. Dashed black line shows correlation with the unbiased circular model (ùêµ = 0) for visual reference. Red marker lines on the bottom indicate stronger correlation with the repulsion than the attraction model (display threshold p<sub>cluster</sub> &lt; 0.05).</p></caption><graphic xlink:href="EMS157281-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Complementary analysis of microsaccadic activity.</title><p>We performed microsaccade detection (see <xref ref-type="sec" rid="S12">Methods</xref>) in four time windows during which orientation encoding was evident in the gaze position data (see <xref ref-type="fig" rid="F2">Fig. 2c-d</xref>): from 300 to 1000 ms after the onset of each stimulus (Stimulus 1, Stimulus 2), from after Cue 1 until the end of the first delay (Delay 1), and from 300 ms after Cue 2 until the end of the second delay (Delay 2). <bold>a</bold>, Distribution of microsaccade directions (onset to endpoint, see <xref ref-type="sec" rid="S12">Methods</xref>) as a function of stimulus orientation (see color legend on the left), collapsed across all trials from all participants. The distributions during the delay periods (Delay 1 and 2) are color-coded according to the currently cued orientation, respectively. <bold>b</bold>, Distribution of microsaccade directions after alignment (analogous to <xref ref-type="fig" rid="F1">Fig. 1b-c</xref>, upper) relative to the objects‚Äô upright (90¬∞) position. <bold>c</bold>, Histograms of the sizes (amplitudes) of the saccades detected for analysis in <italic>a</italic> and <italic>b</italic>. The majority of saccades in each of the time windows was considerably smaller than 2¬∞, in line with an interpretation in terms of microsaccadic activity (<xref ref-type="bibr" rid="R57">Rolfs, 2009</xref>).</p></caption><graphic xlink:href="EMS157281-f005"/></fig></floats-group></article>