<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS157421</article-id><article-id pub-id-type="doi">10.1101/2022.11.19.517179</article-id><article-id pub-id-type="archive">PPR573809</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Optimising source identification from marmoset vocalisations with hierarchical machine learning classifiers</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Phaniraj</surname><given-names>Nikhil</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Wierucka</surname><given-names>Kaja</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Zürcher</surname><given-names>Yvonne</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Burkart</surname><given-names>Judith M.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A3">3</xref></contrib></contrib-group><aff id="A1"><label>1</label>Department of Evolutionary Anthropology, University of Zurich, Winterthurerstrasse 190, 8057 Zürich, Switzerland</aff><aff id="A2"><label>2</label>Division of Biology, Indian Institute of Science Education and Research Pune, Pune, Maharashtra 411008, India</aff><aff id="A3"><label>3</label>Center for the Interdisciplinary Study of Language Evolution (ISLE), University of Zurich, Affolternstrasse 56, 8050 Zürich, Switzerland</aff><pub-date pub-type="nihms-submitted"><day>21</day><month>11</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>20</day><month>11</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Marmosets, with their highly social nature and complex vocal communication system, are important models for comparative studies of vocal communication and, eventually, language evolution. However, our knowledge about marmoset vocalisations predominantly originates from playback studies or vocal interactions between dyads, and there is a need to move towards studying group-level communication dynamics. Efficient source identification from marmoset vocalisations is essential for this challenge, and machine learning algorithms (MLAs) can aid it. Here we built a pipeline capable of plentiful feature extraction, meaningful feature selection, and supervised classification of vocalisations of up to 18 marmosets. We optimised the classifier by building a hierarchical MLA that first learned to determine the sex of the source, narrowed down the possible source individuals based on their sex, and then determined the source identity. We were able to correctly identify the source individual with high precisions (87.21% – 94.42%, depending on call type, and up to 97.79% after the removal of twins from the dataset). We also examine the robustness of identification across varying sample sizes. Our pipeline is a promising tool not only for source identification from marmoset vocalisations but also for analysing vocalisations and tracking vocal learning trajectories of other species.</p></abstract><kwd-group><kwd>Machine learning</kwd><kwd>hierarchical classifier</kwd><kwd>marmoset calls</kwd><kwd>bioacoustics</kwd><kwd>time series analysis</kwd><kwd>source identification</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Comparative studies of primate communication are crucial for understanding the evolutionary origins of human speech and language as they allow for identifying the features of speech and language that we share with non-human primates [<xref ref-type="bibr" rid="R1">1</xref>–<xref ref-type="bibr" rid="R5">5</xref>]. A long tradition of comparative studies has found that some primates have the anatomical capabilities of producing human-like speech [<xref ref-type="bibr" rid="R6">6</xref>,<xref ref-type="bibr" rid="R7">7</xref>], can combine call types into syntactically ordered sequences [<xref ref-type="bibr" rid="R8">8</xref>], have basic phono-articulatory coupling potential [<xref ref-type="bibr" rid="R9">9</xref>–<xref ref-type="bibr" rid="R11">11</xref>] and show some vocal plasticity [<xref ref-type="bibr" rid="R12">12</xref>–<xref ref-type="bibr" rid="R14">14</xref>]. The vocal communication of callitrichid monkeys appears particularly rich among non-human primates. For instance, common marmosets (<italic>Callithrix jacchus</italic>) have been shown to possess superior control and flexibility in their calls. Among others, they can actively interrupt ongoing phee calls [<xref ref-type="bibr" rid="R15">15</xref>,<xref ref-type="bibr" rid="R16">16</xref>], make long-term changes to call frequency in response to noise [<xref ref-type="bibr" rid="R15">15</xref>], and converge in vocal space to a social partner [<xref ref-type="bibr" rid="R12">12</xref>]. As immatures, they go through a babbling phase [<xref ref-type="bibr" rid="R17">17</xref>], and social inputs and contingent vocal feedback from caregivers contribute to fully developing their repertoire [<xref ref-type="bibr" rid="R18">18</xref>–<xref ref-type="bibr" rid="R20">20</xref>]. Marmosets have a broad vocal repertoire and produce a large variety of compound calls formed by using one or more simple call types uttered in a sequence [<xref ref-type="bibr" rid="R21">21</xref>]. Trills are the most common call type and are used as within-group close contact signals; phees are louder long-distance contact calls, and food calls are elicited by individuals when they see food, during feeding, and to initiate food sharing [<xref ref-type="bibr" rid="R22">22</xref>].</p><p id="P3">Marmosets are not very closely related to humans. However, they are similar to us with regard to how they raise offspring: like humans, they are cooperative breeders, i.e., they have a reproductive system in which group members other than the parents significantly contribute to raising offspring. Such a system requires high levels of collective action and group coordination in turn requiring exceptional communicative capabilities. This convergence may thus have played a role in language evolution [<xref ref-type="bibr" rid="R23">23</xref>–<xref ref-type="bibr" rid="R26">26</xref>], making marmosets particularly relevant primate models. However, most studies on marmoset vocal communication are performed under highly controlled conditions, often focusing on a single individual only or physically separated dyads that can interact only vocally with each other. Only rarely are group-level communication dynamics taken into account [<xref ref-type="bibr" rid="R27">27</xref>]. Given the highly social nature of marmosets, the next frontier in studying marmoset vocal communication is to do so under more naturalistic conditions in social groups, where they display their communication skills in entirety.</p><p id="P4">A major bottleneck for studying group-level communication in marmosets and other animals lies in the ability of the researcher to accurately determine and label the identity of the caller (source identity) when recording the vocalisations from large groups. Such a task being laborious, has attracted the attention of signal processing experts and data scientists for its automation [<xref ref-type="bibr" rid="R28">28</xref>]. This has led to the development of a plethora of machine learning (ML) techniques for source identification, each competing for higher accuracy [<xref ref-type="bibr" rid="R29">29</xref>–<xref ref-type="bibr" rid="R32">32</xref>]. The automation might be trivial in some species, such as the songs of male zebra finches, where individual signatures are visible on the spectrograms of songs and can be told apart by a trained human eye [<xref ref-type="bibr" rid="R33">33</xref>]. The same task can be challenging when spectrograms of vocalisations from different individuals look similar to the human eye, such as in the case of many primates [<xref ref-type="bibr" rid="R34">34</xref>–<xref ref-type="bibr" rid="R37">37</xref>], including marmosets [<xref ref-type="bibr" rid="R21">21</xref>,<xref ref-type="bibr" rid="R38">38</xref>].</p><p id="P5">For automating acoustic data processing for marmosets, recent studies have trained machine learning algorithms (MLAs) to classify marmoset calls into different call types [<xref ref-type="bibr" rid="R39">39</xref>,<xref ref-type="bibr" rid="R40">40</xref>]. Although this is an essential step towards automation, the task itself is computationally not very demanding, as the call types look very different on the spectrogram and are easily distinguishable manually. On the other hand, calls of the same type produced by different individuals look very similar on a spectrogram. Therefore, the major challenge lies in automating the classification of calls based on source identity.</p><p id="P6">The goal of our study is to automatically determine source identity from marmoset calls with high accuracy. To do so, we address two issues that would potentially increase the accuracy. First, traditional approaches for animal vocalisation analysis may not capture enough variation of the calls, and second, hierarchical ML classifiers that take advantage of sex differences in calls may outperform non-hierarchical ones.</p><p id="P7">First, the traditional approach for the acoustic analysis of animal vocalisation analysis involves applying short-time Fourier transforms on the acoustic waveform to bring it to the frequency domain [<xref ref-type="bibr" rid="R41">41</xref>]. A handful of predetermined acoustic features are then extracted [<xref ref-type="bibr" rid="R42">42</xref>,<xref ref-type="bibr" rid="R43">43</xref>]. These features are typically chosen because we understand how modifications of the sound affect these feature values, making them easy to interpret [<xref ref-type="bibr" rid="R44">44</xref>–<xref ref-type="bibr" rid="R46">46</xref>]. A drawback of this approach is that not all animal vocalisations show maximum variability along these feature values. The small feature space and reduced class separability of calls due to limited features prevent ML approaches from achieving their maximum potential for obtaining high classification accuracies. In this paper, to overcome this shortcoming, we implement time series analysis directly on the acoustic waveform for plentiful feature extraction and tree-based classifiers for meaningful feature selection. This allows us to address the first issue by providing much more detailed representations of the calls than the traditional approach.</p><p id="P8">Second, we hypothesise that using a hierarchical ML classifier will improve classification accuracies by breaking up the classification problem into a hierarchy of smaller ones. Callitrichid vocalisations contain information about the sex of the caller [<xref ref-type="bibr" rid="R47">47</xref>–<xref ref-type="bibr" rid="R49">49</xref>], which can potentially assist MLAs with efficient source identification. We, therefore, develop a hierarchical ML classifier that first determines the sex of the source, thus narrowing down the possible source individuals, and then determines the source identity. We apply it to trills, phees, and food calls of 20 marmoset individuals and compare its performance to a non-hierarchical approach. We then analyse the acoustic features used by the hierarchical classifier for the various steps in the hierarchy and finally compare the precisions and recalls of the non-hierarchical and hierarchical classifiers at different sample sizes (i.e. number of calls per individual).</p></sec><sec id="S2" sec-type="methods"><title>Methods</title><sec id="S3"><title>Experimental Subjects</title><p id="P9">This study used marmoset vocalisations collected by Zürcher et al. [<xref ref-type="bibr" rid="R12">12</xref>]. The data contained vocalisations from 20 adult common marmosets, 10 males and 10 females. They were housed in 2.4m x 1.5m x 0.8m enclosures along with at least one other individual, with each group having access to a personal outdoor enclosure of the same dimension and a common experimental room. Lighting was regulated to maintain a 12/12 hour day/night cycle. Animals were fed a predetermined amount of vitamin-enriched mush in the morning, vegetables and fruits during noon, and one of either gum, boiled egg, cottage cheese, or insects after noon. Ad libitum access to water was always provided. All experiments were approved by Zürich’s cantonal veterinary office (licence ZH223/16).</p></sec><sec id="S4"><title>Vocalisation recordings and segmentation</title><p id="P10">Individuals were recorded in their home enclosures on multiple days spread over two to three weeks. Each recording session was about 30 minutes long. A condenser Microphone (CM16/CMPA, Avisoft Bioacoustics, Germany) connected to Avisoft UltraSoundGate 116H (Avisoft Bioacoustics, Germany) was used for recordings and calls were labelled in real time using Avisoft Recorder (Avisoft Bioacoustics, Germany). Calls were visualised and segmented using Avisoft Pro (Avisoft Bioacoustics, Germany). See [<xref ref-type="bibr" rid="R12">12</xref>,<xref ref-type="bibr" rid="R50">50</xref>] for detailed information about the recording procedure and processing.</p></sec><sec id="S5"><title>Datasets and feature extraction</title><p id="P11">As the original dataset was imbalanced (number of calls per call type per individual was highly variable), a combination of majority class random undersampling and Synthetic Minority Oversampling TEchnique (SMOTE) [<xref ref-type="bibr" rid="R51">51</xref>,<xref ref-type="bibr" rid="R52">52</xref>] was used to create the following 18 datasets (one original and five generated, for each of the three call types). SMOTE is a data augmentation technique that synthesises new data for minority classes to make them equal to the majority class. X = T for trills, P for phees, F for food calls in the name of the dataset. <list list-type="order" id="L1"><list-item><p id="P12">Original-X: The original dataset after HCTSA processing.</p></list-item><list-item><p id="P13">Imbalanced-X: Marmosets with &lt;25 calls per call type were removed from the Original-X datasets. No undersampling was done.</p></list-item><list-item><p id="P14">Balanced-X: SMOTE was applied on Imbalanced-X to obtain balanced datasets.</p></list-item><list-item><p id="P15">Balanced197-X: From the Imbalanced-X dataset, classes with &gt;197 calls were undersampled to 197 calls. SMOTE was applied to this.</p></list-item><list-item><p id="P16">Balanced99-X: From the Imbalanced-X dataset, classes with &gt;99 calls were undersampled to 99 calls. SMOTE was applied to this.</p></list-item><list-item><p id="P17">Balanced50-X: From the Imbalanced-X dataset, classes with &gt;50 calls were undersampled to 50 calls. SMOTE was applied to this.</p></list-item></list>
</p><p id="P18">The smaller Balanced197-X, Balanced99-X, and Balanced50-X datasets were used to test the capability of the ML approach to classify calls in limited sample size scenarios.</p><p id="P19">Recent advances in time series analyses allow for performing multiple operations that provide meaningful information about the nature of the time series [<xref ref-type="bibr" rid="R53">53</xref>]. This can be applied to acoustic data by viewing the acoustic waveform as a time series of pressure points. We used the Highly Comparative Time Series Analysis (HCTSA) [<xref ref-type="bibr" rid="R54">54</xref>] toolbox on MATLAB [<xref ref-type="bibr" rid="R55">55</xref>] for feature extraction from marmoset calls. Features common across calls of all individuals for a given call type were used for further analyses. We visualised the trill, phee, and food call datasets using t-distributed Stochastic Neighbour Embedding (t-SNE) [<xref ref-type="bibr" rid="R56">56</xref>].</p><p id="P20">We trained individual multiclass Adaptive Boosting algorithms with decision trees as weak learners and 10-fold cross-validation for determining the important features to use for classification and for performing the classification itself. This method uses multiple weak learners to create a strong learner [<xref ref-type="bibr" rid="R57">57</xref>] and is considered better than random forest classifiers due to its higher accuracy and lower susceptibility to overfitting in certain cases [<xref ref-type="bibr" rid="R58">58</xref>,<xref ref-type="bibr" rid="R59">59</xref>]. We first trained AdaBoost on Imbalanced-X, Balanced-X, Balanced197-X, and Balanced99-X datasets to classify calls based on source identity – as a direct or “non-hierarchical approach” (in contrast to the hierarchical approach that was used later) to determine source identity from calls (using MATLAB’s ‘fitcensemble’, ‘AdaboostM1’, and ‘AdaboostM2’ functions). We chose the number of trees and learning rate based on the observations of the classification loss function.</p><p id="P21">Although classification accuracy is the most widely used metric to assess ML models, it is not a good representation of the model’s performance on class-imbalanced datasets. This is because the classifier can get away with a high accuracy score by simply predicting most data points as belonging to the majority class. In such cases, the Receiver Operating Characteristic (ROC) curve can be used to evaluate the classifier’s performance. The ROC curve visualises how the true positive rate changes as a function of the false positive rate at various threshold values. The area under this ROC curve, simply ‘Area Under Curve’ (AUC), can be a useful tool for examining classifier performance along with accuracy. It is important to note here that ROC-AUCs represent how well the MLA can separate a positive class from the negative classes and do not represent how well the MLA would perform classification when actually handed the task. Therefore, for assessing the performance of AdaBoost on Imbalanced-X datasets, ROC-AUC was calculated in a one vs rest (one positive, rest negative classes) setting for each class in the dataset, along with the accuracies. Even while assessing the performance of MLAs on balanced datasets, accuracy does not represent how variable the predictions for each of the classes (marmoset individuals) are. Therefore, for the rest of the classifiers, precisions and recalls for each class were calculated, and the summary of these values was used to assess their performance.</p><p id="P22">For inspecting if individual variability of calls within marmoset groups can be explained by variation in sex and whether MLAs could exploit this to perform better, individual AdaBoosts were trained on Imbalanced-X and Balanced-X datasets to classify calls based on sex, and then the source individual. Sex was chosen as a cue because previous studies in Callitrichids have shown that they can discriminate calls based on the sex of the source [<xref ref-type="bibr" rid="R47">47</xref>–<xref ref-type="bibr" rid="R49">49</xref>], and in our case, the total number of classes could be split into half based on the sex of the individual (10 males and 10 females out of 20 individuals). Later, each dataset was divided into 2 sub-datasets based on the ‘true’ sex of the source individual, and separate AdaBoosts were trained on each of them. This was the hierarchical approach to determine first the sex, and then the source identity from calls (<xref ref-type="fig" rid="F1">figure 1</xref>). To assess the performance of the classifiers for the hierarchical classification approach, precisions and recalls for each individual were calculated for all classifiers as: <disp-formula id="FD1"><mml:math id="M1"><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo>|</mml:mo><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P23"><italic>Y</italic> is the precision or recall, and <italic>ind</italic> is the individual identity of the marmoset. For example, the final precision for a female individual would be the precision for determining the sex as a female, multiplied by the precision for determining the source individual among females.</p><p id="P24">Because of the difference in how the non-hierarchical and hierarchical approaches utilise the dataset for training, the usual pairwise t-test for mean precisions and recalls across the 10 folds could not be performed. Instead, we performed Wilcoxon signed-rank test to compare precision and recall scores for every class between the two approaches because the classes, i.e., the individuals for both the approaches were the same.</p></sec><sec id="S6"><title>Feature importance scoring</title><p id="P25">Along with classifying calls, each fold of the AdaBoost also provides predictor importance scores for each feature, which represents how important that feature was for AdaBoost in the classification task. First, we checked how (non-hierarchical) AdaBoost performed the feature selection task. For this, we ranked features by predictor importance scores given by the most accurate of the 10 models (run on 10 different folds) in the AdaBoost for each dataset. Then, for the 3 Balanced-X datasets, we used the top 20 features to visualise t-SNE clusters. t-SNE plots were generated using MATLAB’s ‘tsne’ function with the Barnes-Hut algorithm keeping the Barnes-Hut trade-off parameter at 0.5 to increase processing speed for large datasets. Exaggeration was set to 4, perplexity to n/100, and learning rate to n/12 (where n is the total number of calls for that call type, not to be confused with sample size per class) as these values are shown to provide robust results, which are comparable to UMAP [<xref ref-type="bibr" rid="R60">60</xref>], when datasets are large [<xref ref-type="bibr" rid="R61">61</xref>]. We compared these to t-SNE plots of the corresponding datasets, generated using 20 random features. For a more quantitative comparison, we selected 20 random features from each of the three datasets 100 times, plotted the histograms of the mean silhouette scores, fit Gaussians to these distributions, and calculated the probability of getting a mean silhouette score greater than that of the top 20 features by chance.</p><p id="P26">For the hierarchical classifiers, we analysed the features used by the various levels in the hierarchy when implemented on the 3 Balanced-X datasets. We checked what proportion of the total features utilised by the hierarchical classifier: (1) was used for determining the sex only, (2) was used for determining source identity among females only, (3) was used for determining source identity among males only, (4) were common between determining sex and individual identity among females, (5) were common between determining sex and individual identity among males, (6) were common between determining individual identities among females and males, and (7) were common between determining sex, individual identities among females, and individual identity among males.</p></sec></sec><sec id="S7" sec-type="results"><title>Results</title><sec id="S8"><title>Datasets and feature extraction</title><p id="P27">The sample sizes of each of the datasets obtained are listed in <xref ref-type="table" rid="T1">table 1</xref>. The Original-X and Imbalanced-X datasets have variable number of calls per call type and individual (<xref ref-type="supplementary-material" rid="SD1">supplementary table 1</xref>).</p><p id="P28">We could extract between 3776 to 4553 features from each marmoset call in the Original-X datasets, with 3255, 3395, 3477 features common across trills, phees, and food calls, respectively.</p></sec><sec id="S9"><title>ML classifiers and feature importance scoring</title><p id="P29">We monitored the classification loss function of AdaBoost with the addition of every new weak learner. We observed the loss function to plateau at around 500 trees while training AdaBoost to determine sex and around 2500 for other tasks.</p><sec id="S10"><title>Balanced vs unbalanced datasets</title><p id="P30">Classification accuracies ranged from 60.8% to 70.96% for imbalanced datasets versus 71.43% to 83.92% for balanced datasets. Mean ROC-AUCs were higher for all the Balanced-X datasets compared to Imbalanced-X datasets (<xref ref-type="table" rid="T2">table 2</xref>).</p></sec><sec id="S11"><title>Top 20 vs random 20 features</title><p id="P31">t-SNE plots obtained using top 20 features on the Balanced-X datasets showed visibly better clusters compared to t-SNE plots obtained using random 20 features on the corresponding datasets (<xref ref-type="fig" rid="F2">figure 2</xref>, <xref ref-type="supplementary-material" rid="SD1">supplementary tables 2, 3 and 4</xref>). The top 20 features provided by AdaBoost gave significantly greater silhouette scores than what would be obtained by chance for trills (Z = 7.0154, p = 1.146e-12), phees (Z = 4.8361, p = 6.621e-7), and food calls (Z = 6.8922, p = 2.747e-12).</p></sec><sec id="S12"><title>Hierarchical vs non-hierarchical classifiers at large sample sizes</title><p id="P32">The accuracies of AdaBoost to classify the following datasets based on sex were: Balanced-T = 99.4%, Balanced-P = 96.8%, Balanced-F = 96.7%. The hierarchical classification approach gave significantly better precision and recall scores compared to the non-hierarchical approach for the corresponding Balanced-X datasets (p &lt;0.05 across all call types, Wilcoxon signed-rank test; <xref ref-type="table" rid="T3">table 3</xref>). A thorough evaluation of the precisions and recalls for each individual by the hierarchical classifier revealed that the trills of 2 individuals - Washington and Wisconsine (twins), had significantly low scores (precisions: 74.23% and 67.46% respectively, recalls: 75.07% and 72.99%, <xref ref-type="fig" rid="F3">figure 3</xref>). The mean precision and recalls for trills for all except these two individuals were as high as 97.79% and 97.07%, respectively. The phee precision and recall scores for Washington and Wisconsine were also slightly lower than that of other females (<xref ref-type="supplementary-material" rid="SD1">supplementary figures 1A and 1B</xref>). For food calls, the precision and recall scores of these two individuals were similar or higher than the mean scores of all females (<xref ref-type="supplementary-material" rid="SD1">supplementary figures 1C and 1D</xref>).</p></sec><sec id="S13"><title>Hierarchical vs non-hierarchical classifiers at reduced sample sizes</title><p id="P33">With the decrease in sample size per class, the difference between the mean precisions and recalls of the hierarchical and non-hierarchical approaches reduced for all three call types (<xref ref-type="fig" rid="F4">figure 4</xref> for precision; see <xref ref-type="supplementary-material" rid="SD1">supplementary figure 3</xref> for recall).</p></sec><sec id="S14"><title>Feature selection at various levels of the hierarchical classifier</title><p id="P34">The features selected by each level of the hierarchy varied with the task. About 30%-60% of the features were utilised solely for determining sex across call types. Only a few features were common for determining individual identities among males and females (<xref ref-type="fig" rid="F5">figure 5</xref>).</p></sec></sec></sec><sec id="S15" sec-type="discussion"><title>Discussion</title><p id="P35">We show that the information about sex encoded in marmoset calls can be used to build hierarchical ML classifiers to determine the source identity with higher precisions and recalls than with their non-hierarchical counterparts. We incorporated such a hierarchical classifier in a pipeline that starts with plentiful feature extraction to provide a rich representation of the calls, followed by meaningful feature selection, and source identification from marmoset vocalisations. Our findings have implications for marmoset communication research and will be beneficial for tracking changes in vocalisations during vocal learning. The same methods can also be extended for efficient source identification in other species.</p><sec id="S16"><title>Optimising source identification</title><p id="P36">Of the large number of features extracted with HCTSA, we found that the AdaBoost algorithm reliably selected the most important features that would best cluster the data for its classification task (<xref ref-type="fig" rid="F2">figure 2</xref>, <xref ref-type="supplementary-material" rid="SD1">supplementary tables 1, 2, and 3</xref>). Intriguingly, the features that were selected varied with the task of the classifier (<xref ref-type="fig" rid="F4">figure 4</xref>). Because all subsequent quantitative analyses of vocalisations depend on feature selection, this step is important for studying animal vocalisations. In contrast to our approach, more traditional approaches typically use a standard set of acoustic features across all tasks e.g. [<xref ref-type="bibr" rid="R62">62</xref>,<xref ref-type="bibr" rid="R63">63</xref>] or [<xref ref-type="bibr" rid="R64">64</xref>,<xref ref-type="bibr" rid="R65">65</xref>]. This may lead to a suboptimal representation of the call for a specific task at hand, and thus lower classification accuracy. Hence, despite dealing with the same call type, a customised set of features seems beneficial for optimal performance in a specific classification task. Secker et al. [<xref ref-type="bibr" rid="R66">66</xref>] have previously used a hierarchical classifier with independent feature selection by the components to classify proteins successfully. They suggest that independent feature selection helps maintain high predictive performance while improving computational efficiency. In our case, the independent feature selection arguably enabled the hierarchical classifier to efficiently utilise broader-category cues (i.e., sex) for classification, hence boosting performance over its non-hierarchical counterpart. AdaBoost thus provides task specific and flexible feature extraction with a customised set of features for every dataset and can be applied to a wide range of animal vocalisation datasets. However, many of these features are complex, with some only defined by the mathematical operations performed on the audio waveform for obtaining the feature, hence less interpretable. Therefore, feature interpretability is a limitation. However, this is compensated by decision tree based AdaBoost, making the interpretability of our approach better than many “black-box” models such as deep neural networks [<xref ref-type="bibr" rid="R67">67</xref>].</p><p id="P37">Even with the substantial number of features provided by HCTSA for AdaBoost to work with, AdaBoost performed poorly on the unbalanced dataset with accuracies below 70% (<xref ref-type="table" rid="T2">table 2</xref>). A large body of ML literature points out to the problem of class-unbalanced datasets and its solutions [<xref ref-type="bibr" rid="R68">68</xref>,<xref ref-type="bibr" rid="R69">69</xref>]. Recent studies and reviews have emphasised the use of data augmentation and balancing techniques to improve ML accuracy when handling acoustic data [<xref ref-type="bibr" rid="R70">70</xref>–<xref ref-type="bibr" rid="R72">72</xref>]. Consistent with other studies [<xref ref-type="bibr" rid="R73">73</xref>–<xref ref-type="bibr" rid="R75">75</xref>], balancing the datasets significantly improved the performance of AdaBoost across call types. Therefore, data balancing using tools like SMOTE combined with random undersampling is an important step before running MLAs on any dataset.</p><p id="P38">Classifying data points from a noisy dataset to a large number of classes (10 – 18 different individuals in our case) is often a demanding task for an MLA. Here, we broke the problem of classifying calls to over ten sources into a first classification problem of assigning the sex of the source, and only then, given the sex, classifying source identity in a second step (<xref ref-type="fig" rid="F1">figure 1</xref>). We showed that at large sample sizes, mean precisions and recalls across datasets increased by more than eight percentage points with this hierarchical approach (<xref ref-type="table" rid="T3">table 3</xref>). We found the accuracies of the hierarchical classifier on the largest balanced datasets to remain satisfactory and higher than most recent studies classifying animal vocalisations using MLAs [<xref ref-type="bibr" rid="R62">62</xref>,<xref ref-type="bibr" rid="R76">76</xref>,<xref ref-type="bibr" rid="R77">77</xref>], even though the number of samples per class of data was lower than those studies. The same was reflected in the performance of the hierarchical classifier on the originally collected calls (<xref ref-type="supplementary-material" rid="SD1">supplementary figure 2</xref>).</p><p id="P39">The difference in precisions and recalls between the hierarchical and non-hierarchical approaches diminished as the sample sizes decreased suggesting that the hierarchical classifier requires exposure to enough data to perform significantly better than its non-hierarchical counterpart (<xref ref-type="fig" rid="F4">figure 4</xref>, <xref ref-type="supplementary-material" rid="SD1">supplementary figure 3</xref>). Lower sample sizes pose a higher risk of error cascading in the hierarchical classifier, and this has been identified in similar classifiers developed for text classification [<xref ref-type="bibr" rid="R78">78</xref>,<xref ref-type="bibr" rid="R79">79</xref>]. This is due to the error at the top of the hierarchy propagating down the levels of the hierarchical classifier. However, this limitation only arises at the level of the training dataset.</p><p id="P40">The requirement of a large training dataset and the presence of a small collected dataset can almost always be bridged. The median sample sizes per class in our Imbalanced-X datasets were 47 for trills, 83 for phees and 173 for food calls (<xref ref-type="supplementary-material" rid="SD1">supplementary table 1</xref>). Using this small, highly imbalanced dataset, we could generate larger balanced datasets and train and test our classifiers on them (see <xref ref-type="sec" rid="S2">methods</xref>). Therefore, to obtain optimal performance from the hierarchical classifier, one need not necessarily require a large sample size to begin with (see performance of the hierarchical classifier on the Imbalanced-X dataset in <xref ref-type="supplementary-material" rid="SD1">supplementary figure 2</xref>).</p></sec><sec id="S17"><title>Implications for understanding the marmoset communication system</title><p id="P41">The classification precisions and recalls for food calls were low compared to trills and phees despite over three times higher sample sizes (<xref ref-type="table" rid="T3">table 3</xref>). In particular, food calls required a higher sample size per class value to be classified as accurately as trills and phees (<xref ref-type="fig" rid="F4">figure 4</xref>). On the one hand, food calls are likely a highly heterogeneous group of call types [<xref ref-type="bibr" rid="R24">24</xref>]. Moreover, they are predominantly produced in bouts of multiple repeated call units [<xref ref-type="bibr" rid="R21">21</xref>]. Furthermore, each call unit is much shorter than a trill or a phee [<xref ref-type="bibr" rid="R21">21</xref>], therefore providing reduced information for time series analysis. The poor classification results could thus arise because some of the source identity information may well be encoded at the level of the bout. This is testable in the future by repeating the classification procedure on bouts of food calls.</p><p id="P42">Intriguingly, the mean precision and recall for determining source identity from trills when considering all except two individuals was as high as 98.38% and 97.65%, respectively (<xref ref-type="fig" rid="F3">figure 3</xref>). The two marmosets with low scores happened to be twins of the same sex. Thus, the low classification scores were likely due to the high vocal similarity between the calls of the twins. The same pattern was present to a lesser extent for phee calls but not for food calls (<xref ref-type="supplementary-material" rid="SD1">supplementary figure 1</xref>). Whereas the food calls may require further scrutiny at different levels of analyses (see above), the contrast between trills and phees is interpretable with regard to their biological function. Signalling identity is essential for the long-distance phee calls that individuals typically use to establish acoustic contact when visual contact is not possible [<xref ref-type="bibr" rid="R80">80</xref>]. In contrast, trill calls are given in close proximity to a social partner, and the caller's identity is thus redundant because the marmosets may also use visual or olfactory cues to recognise the partner [<xref ref-type="bibr" rid="R81">81</xref>]. It may thus well be that the twins actively diverged from each other in their phee calls but not in their trill calls. This is consistent with a recent study [<xref ref-type="bibr" rid="R50">50</xref>] on newly paired marmosets that found that partners would converge in the structure of their phee calls. However, newly formed pairs that had initially very similar phee calls before pairing diverged rather than converged in their call structure, supposedly to make themselves better distinguishable. In the future, our approach will be beneficial for precisely tracking changes in vocalisations in such situations, as well as during ontogeny because it would be possible to track changes in features that most differentiate two individuals. More precise tracking of vocal changes will enable us to identify why such a phenomenon might have gotten selected, both in adults and immatures [<xref ref-type="bibr" rid="R82">82</xref>,<xref ref-type="bibr" rid="R83">83</xref>]. Getting to the bottom of the factors facilitating vocal learning and other enhanced communicative capabilities of cooperatively breeding primates will better inform us about the evolutionary origins of language [<xref ref-type="bibr" rid="R23">23</xref>].</p><p id="P43">Finally, we know that callitrichids can differentiate between vocalisations originating from cage-mates versus foreign individuals and from males versus females [<xref ref-type="bibr" rid="R48">48</xref>,<xref ref-type="bibr" rid="R49">49</xref>]. An intriguing question is how they achieve that, and whether their decision-making process may likewise be hierarchically structured with broader-category cues used as a first distinction. Multiple studies on humans allude to the hierarchical nature of decision-making in various contexts [<xref ref-type="bibr" rid="R84">84</xref>–<xref ref-type="bibr" rid="R89">89</xref>]. Some frog species seem to employ hierarchical decision-making for prey capture [<xref ref-type="bibr" rid="R90">90</xref>], a hierarchical decision-making model appears to best explain the strategies used by two rhesus macaques while playing a slightly modified, semi-controlled adaptation of the video game ‘Pacman’ [<xref ref-type="bibr" rid="R91">91</xref>], a hierarchy of multimodal cues are used by sea lions during mother-offspring recognition [<xref ref-type="bibr" rid="R92">92</xref>,<xref ref-type="bibr" rid="R93">93</xref>], and evidence from fruit flies, locusts, and zebrafish suggest that they break down a complex problem of deciding between spatially distributed options into a series of smaller problems [<xref ref-type="bibr" rid="R94">94</xref>]. Hierarchical decision-making may thus be a widespread feature of cognitive processing. Given that sex could be attributed with extremely high precision by our classifier, we hypothesise that marmosets, too, use these sex-based cues for efficient source identification from calls, and they are doing so in a hierarchical manner, similar to the hierarchical classifier. Cognitive and psychological experiments will be required to test this hypothesis.</p><p id="P44">Overall, we demonstrated the capabilities of our pipeline for determining source identity from marmoset vocalisations by breaking down the larger classification problem into a hierarchy of smaller, easier problems. Although our hierarchical approach focused on sex-based cues in marmoset vocalisations, the idea is for it to be generalisable to all broad-category cues that animal vocalisations may provide. In the future, we hope to extend the idea to larger, more complex animal vocalisation datasets that would require multiple levels in the hierarchy and would utilise other cues, such as the age and social status of the animal, for efficient source identification. Finally, the ability to select customised features for every dataset and task, combined with the supervised nature of learning, makes our pipeline highly flexible and extendable for analysing vocalisations of diverse animal species.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary materials</label><media xlink:href="EMS157421-supplement-Supplementary_materials.pdf" mimetype="application" mime-subtype="pdf" id="d2aAdEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S18"><title>Acknowledgements and Funding</title><p>This work was supported by the Swiss National Science Foundation (grant number 31003A_149796, the NCCR Evolving Language, agreement number 51NF40_180888), and the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No 101001295). NP was a recipient of a grant by the A. H. Schultz foundation.</p></ack><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>C</given-names></name></person-group><article-title>Ontogeny and phylogeny of language</article-title><source>Proc Natl Acad Sci</source><year>2013</year><volume>110</volume><fpage>6324</fpage><lpage>6327</lpage><pub-id pub-id-type="pmcid">PMC3631656</pub-id><pub-id pub-id-type="pmid">23576720</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1216803110</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berwick</surname><given-names>RC</given-names></name><name><surname>Friederici</surname><given-names>AD</given-names></name><name><surname>Chomsky</surname><given-names>N</given-names></name><name><surname>Bolhuis</surname><given-names>JJ</given-names></name></person-group><article-title>Evolution, brain, and the nature of language</article-title><source>Trends Cogn Sci</source><year>2013</year><volume>17</volume><fpage>89</fpage><lpage>98</lpage><pub-id pub-id-type="pmid">23313359</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fedurek</surname><given-names>P</given-names></name><name><surname>Slocombe</surname><given-names>KE</given-names></name></person-group><article-title>Primate vocal communication: a useful tool for understanding human speech and language evolution?</article-title><source>Hum Biol</source><year>2011</year><volume>83</volume><fpage>153</fpage><lpage>173</lpage><pub-id pub-id-type="pmid">21615284</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zuberbühler</surname><given-names>K</given-names></name></person-group><chapter-title>The primate roots of human language</chapter-title><source>Primate Hearing and Communication</source><year>2017</year><fpage>175</fpage><lpage>200</lpage><publisher-loc>Springer</publisher-loc></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fitch</surname><given-names>WT</given-names></name></person-group><article-title>Empirical approaches to the study of language evolution</article-title><source>Psychon Bull Rev</source><year>2017</year><volume>24</volume><fpage>3</fpage><lpage>33</lpage><pub-id pub-id-type="pmid">28150125</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fitch</surname><given-names>WT</given-names></name><name><surname>De Boer</surname><given-names>B</given-names></name><name><surname>Mathur</surname><given-names>N</given-names></name><name><surname>Ghazanfar</surname><given-names>AA</given-names></name></person-group><article-title>Monkey vocal tracts are speech-ready</article-title><source>Sci Adv</source><year>2016</year><volume>2</volume><elocation-id>e1600723</elocation-id><pub-id pub-id-type="pmcid">PMC5148209</pub-id><pub-id pub-id-type="pmid">27957536</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.1600723</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boë</surname><given-names>L-J</given-names></name><name><surname>Berthommier</surname><given-names>F</given-names></name><name><surname>Legou</surname><given-names>T</given-names></name><name><surname>Captier</surname><given-names>G</given-names></name><name><surname>Kemp</surname><given-names>C</given-names></name><name><surname>Sawallis</surname><given-names>TR</given-names></name><name><surname>Becker</surname><given-names>Y</given-names></name><name><surname>Rey</surname><given-names>A</given-names></name><name><surname>Fagot</surname><given-names>J</given-names></name></person-group><article-title>Evidence of a Vocalic Proto-System in the Baboon (Papio papio) Suggests Pre-Hominin Speech Precursors</article-title><source>PLOS ONE</source><year>2017</year><volume>12</volume><elocation-id>e0169321</elocation-id><pub-id pub-id-type="pmcid">PMC5226677</pub-id><pub-id pub-id-type="pmid">28076426</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0169321</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zuberbühler</surname><given-names>K</given-names></name></person-group><article-title>Combinatorial capacities in primates</article-title><source>Curr Opin Behav Sci</source><year>2018</year><volume>21</volume><fpage>161</fpage><lpage>169</lpage></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bergman</surname><given-names>TJ</given-names></name></person-group><article-title>Speech-like vocalized lip-smacking in geladas</article-title><source>Curr Biol</source><year>2013</year><volume>23</volume><fpage>R268</fpage><lpage>R269</lpage><pub-id pub-id-type="pmid">23578870</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gustison</surname><given-names>ML</given-names></name><name><surname>Bergman</surname><given-names>TJ</given-names></name></person-group><article-title>Divergent acoustic properties of gelada and baboon vocalizations and their implications for the evolution of human speech</article-title><source>J Lang Evol</source><year>2017</year><volume>2</volume><fpage>20</fpage><lpage>36</lpage><pub-id pub-id-type="pmcid">PMC6681840</pub-id><pub-id pub-id-type="pmid">31402984</pub-id><pub-id pub-id-type="doi">10.1093/jole/lzx015</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Risueno-Segovia</surname><given-names>C</given-names></name><name><surname>Hage</surname><given-names>SR</given-names></name></person-group><article-title>Theta synchronization of phonatory and articulatory systems in marmoset monkey vocal production</article-title><source>Curr Biol</source><year>2020</year><volume>30</volume><fpage>4276</fpage><lpage>4283</lpage><pub-id pub-id-type="pmid">32888481</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zürcher</surname><given-names>Y</given-names></name><name><surname>Willems</surname><given-names>EP</given-names></name><name><surname>Burkart</surname><given-names>JM</given-names></name></person-group><article-title>Are dialects socially learned in marmoset monkeys? Evidence from translocation experiments</article-title><source>PLOS ONE</source><year>2019</year><volume>14</volume><elocation-id>e0222486</elocation-id><pub-id pub-id-type="pmcid">PMC6808547</pub-id><pub-id pub-id-type="pmid">31644527</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0222486</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lemasson</surname><given-names>A</given-names></name><name><surname>Ouattara</surname><given-names>K</given-names></name><name><surname>Petit</surname><given-names>EJ</given-names></name><name><surname>Zuberbühler</surname><given-names>K</given-names></name></person-group><article-title>Social learning of vocal structure in a nonhuman primate?</article-title><source>BMC Evol Biol</source><year>2011</year><volume>11</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="pmcid">PMC3260242</pub-id><pub-id pub-id-type="pmid">22177339</pub-id><pub-id pub-id-type="doi">10.1186/1471-2148-11-362</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snowdon</surname><given-names>CT</given-names></name><name><surname>Elowson</surname><given-names>AM</given-names></name></person-group><article-title>Pygmy marmosets modify call structure when paired</article-title><source>Ethology</source><year>1999</year><volume>105</volume><fpage>893</fpage><lpage>908</lpage></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>L</given-names></name><name><surname>Roy</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name></person-group><article-title>Rapid modulations of the vocal structure in marmoset monkeys</article-title><source>Hear Res</source><year>2019</year><volume>384</volume><elocation-id>107811</elocation-id><pub-id pub-id-type="pmid">31678893</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pomberger</surname><given-names>T</given-names></name><name><surname>Risueno-Segovia</surname><given-names>C</given-names></name><name><surname>Löschner</surname><given-names>J</given-names></name><name><surname>Hage</surname><given-names>SR</given-names></name></person-group><article-title>Precise motor control enables rapid flexibility in vocal behavior of marmoset monkeys</article-title><source>Curr Biol</source><year>2018</year><volume>28</volume><fpage>788</fpage><lpage>794</lpage><pub-id pub-id-type="pmid">29478857</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pistorio</surname><given-names>AL</given-names></name><name><surname>Vintch</surname><given-names>B</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name></person-group><article-title>Acoustic analysis of vocal development in a New World primate, the common marmoset (Callithrix jacchus</article-title><source>J Acoust Soc Am</source><year>2006</year><volume>120</volume><fpage>1655</fpage><lpage>1670</lpage><pub-id pub-id-type="pmid">17004487</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gultekin</surname><given-names>YB</given-names></name><name><surname>Hage</surname><given-names>SR</given-names></name></person-group><article-title>Limiting parental feedback disrupts vocal development in marmoset monkeys</article-title><source>Nat Commun</source><year>2017</year><volume>8</volume><elocation-id>14046</elocation-id><pub-id pub-id-type="pmcid">PMC5241798</pub-id><pub-id pub-id-type="pmid">28090084</pub-id><pub-id pub-id-type="doi">10.1038/ncomms14046</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gultekin</surname><given-names>YB</given-names></name><name><surname>Hage</surname><given-names>SR</given-names></name></person-group><article-title>Limiting parental interaction during vocal development affects acoustic call structure in marmoset monkeys</article-title><source>Sci Adv</source><year>2018</year><volume>4</volume><elocation-id>eaar4012</elocation-id><pub-id pub-id-type="pmcid">PMC5895450</pub-id><pub-id pub-id-type="pmid">29651461</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.aar4012</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takahashi</surname><given-names>DY</given-names></name><name><surname>Liao</surname><given-names>DA</given-names></name><name><surname>Ghazanfar</surname><given-names>AA</given-names></name></person-group><article-title>Vocal Learning via Social Reinforcement by Infant Marmoset Monkeys</article-title><source>Curr Biol</source><year>2017</year><volume>27</volume><fpage>1844</fpage><lpage>1852</lpage><elocation-id>e6</elocation-id><pub-id pub-id-type="pmid">28552359</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Agamaite</surname><given-names>JA</given-names></name><name><surname>Chang</surname><given-names>C-J</given-names></name><name><surname>Osmanski</surname><given-names>MS</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name></person-group><article-title>A quantitative acoustic analysis of the vocal repertoire of the common marmoset (Callithrix jacchus)</article-title><source>J Acoust Soc Am</source><year>2015</year><volume>138</volume><fpage>2906</fpage><lpage>2928</lpage><pub-id pub-id-type="pmcid">PMC4644241</pub-id><pub-id pub-id-type="pmid">26627765</pub-id><pub-id pub-id-type="doi">10.1121/1.4934268</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>GR</given-names></name><name><surname>Almond</surname><given-names>RE</given-names></name><name><surname>Bergen</surname><given-names>YV</given-names></name></person-group><article-title>Begging, stealing, and offering: food transfer in nonhuman primates</article-title><year>2004</year></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burkart</surname><given-names>JM</given-names></name><name><surname>Adriaense</surname><given-names>JEC</given-names></name><name><surname>Brügger</surname><given-names>RK</given-names></name><name><surname>Miss</surname><given-names>FM</given-names></name><name><surname>Wierucka</surname><given-names>K</given-names></name><name><surname>van Schaik</surname><given-names>CP</given-names></name></person-group><article-title>A convergent interaction engine: vocal communication among marmoset monkeys</article-title><source>Philos Trans R Soc B Biol Sci</source><year>2022</year><volume>377</volume><elocation-id>20210098</elocation-id><pub-id pub-id-type="pmcid">PMC9315454</pub-id><pub-id pub-id-type="pmid">35876206</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2021.0098</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burkart</surname><given-names>J</given-names></name><name><surname>Martins</surname><given-names>EG</given-names></name><name><surname>Miss</surname><given-names>F</given-names></name><name><surname>Zürcher</surname><given-names>Y</given-names></name></person-group><article-title>From sharing food to sharing information: Cooperative breeding and language evolution</article-title><source>Interact Stud</source><year>2018</year><volume>19</volume><fpage>136</fpage><lpage>150</lpage><pub-id pub-id-type="doi">10.1075/is.17026.bur</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Varella</surname><given-names>TT</given-names></name><name><surname>Ghazanfar</surname><given-names>AA</given-names></name></person-group><article-title>Cooperative care and the evolution of the prelinguistic vocal learning</article-title><source>Dev Psychobiol</source><year>2021</year><volume>63</volume><fpage>1583</fpage><lpage>1588</lpage><pub-id pub-id-type="pmcid">PMC8355020</pub-id><pub-id pub-id-type="pmid">33826142</pub-id><pub-id pub-id-type="doi">10.1002/dev.22108</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snowdon</surname><given-names>CT</given-names></name></person-group><article-title>Learning from monkey “talk”</article-title><source>Science</source><year>2017</year><volume>355</volume><fpage>1120</fpage><lpage>1122</lpage><pub-id pub-id-type="pmid">28302806</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jovanovic</surname><given-names>V</given-names></name><name><surname>Miller</surname><given-names>CT</given-names></name></person-group><source>Mechanisms for Communicating in a Marmoset ‘Cocktail Party’</source><year>2021</year><elocation-id>2020.12.08.416693</elocation-id><pub-id pub-id-type="doi">10.1101/2020.12.08.416693</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mcloughlin</surname><given-names>MP</given-names></name><name><surname>Stewart</surname><given-names>R</given-names></name><name><surname>McElligott</surname><given-names>AG</given-names></name></person-group><article-title>Automated bioacoustics: methods in ecology and conservation and their potential for animal welfare monitoring</article-title><source>J R Soc Interface</source><year>2019</year><volume>16</volume><elocation-id>20190225</elocation-id><pub-id pub-id-type="pmcid">PMC6597774</pub-id><pub-id pub-id-type="pmid">31213168</pub-id><pub-id pub-id-type="doi">10.1098/rsif.2019.0225</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burkett</surname><given-names>ZD</given-names></name><name><surname>Day</surname><given-names>NF</given-names></name><name><surname>Peñagarikano</surname><given-names>O</given-names></name><name><surname>Geschwind</surname><given-names>DH</given-names></name><name><surname>White</surname><given-names>SA</given-names></name></person-group><article-title>VoICE: A semi-automated pipeline for standardizing vocal analysis across models</article-title><source>Sci Rep</source><year>2015</year><volume>5</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="pmcid">PMC4446892</pub-id><pub-id pub-id-type="pmid">26018425</pub-id><pub-id pub-id-type="doi">10.1038/srep10237</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>J</given-names></name><name><surname>Sun</surname><given-names>Y</given-names></name><name><surname>Ji</surname><given-names>L</given-names></name></person-group><article-title>A call-independent and automatic acoustic system for the individual recognition of animals: A novel model using four passerines</article-title><source>Pattern Recognit</source><year>2010</year><volume>43</volume><fpage>3846</fpage><lpage>3852</lpage><pub-id pub-id-type="doi">10.1016/j.patcog.2010.04.026</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ovaskainen</surname><given-names>O</given-names></name><name><surname>Moliterno de Camargo</surname><given-names>U</given-names></name><name><surname>Somervuo</surname><given-names>P</given-names></name></person-group><article-title>Animal Sound Identifier (ASI): software for automated identification of vocal animals</article-title><source>Ecol Lett</source><year>2018</year><volume>21</volume><fpage>1244</fpage><lpage>1254</lpage><pub-id pub-id-type="pmid">29938881</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stowell</surname><given-names>D</given-names></name><name><surname>Petrusková</surname><given-names>T</given-names></name><name><surname>Šálek</surname><given-names>M</given-names></name><name><surname>Linhart</surname><given-names>P</given-names></name></person-group><article-title>Automatic acoustic identification of individuals in multiple species: improving identification across recording conditions</article-title><source>J R Soc Interface</source><year>2019</year><volume>16</volume><elocation-id>20180940</elocation-id><pub-id pub-id-type="pmcid">PMC6505557</pub-id><pub-id pub-id-type="pmid">30966953</pub-id><pub-id pub-id-type="doi">10.1098/rsif.2018.0940</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Price</surname><given-names>PH</given-names></name></person-group><article-title>Developmental determinants of structure in zebra finch song</article-title><source>J Comp Physiol Psychol</source><year>1979</year><volume>93</volume><fpage>260</fpage></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Owren</surname><given-names>MJ</given-names></name><name><surname>Rendall</surname><given-names>D</given-names></name></person-group><article-title>Sound on the rebound: bringing form and function back to the forefront in understanding nonhuman primate vocal signaling</article-title><source>Evol Anthropol Issues News Rev Issues News Rev</source><year>2001</year><volume>10</volume><fpage>58</fpage><lpage>71</lpage></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leliveld</surname><given-names>LM</given-names></name><name><surname>Scheumann</surname><given-names>M</given-names></name><name><surname>Zimmermann</surname><given-names>E</given-names></name></person-group><article-title>Acoustic correlates of individuality in the vocal repertoire of a nocturnal primate (Microcebus murinus)</article-title><source>J Acoust Soc Am</source><year>2011</year><volume>129</volume><fpage>2278</fpage><lpage>2288</lpage><pub-id pub-id-type="pmid">21476683</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rendall</surname><given-names>D</given-names></name></person-group><article-title>Acoustic correlates of caller identity and affect intensity in the vowel-like grunt vocalizations of baboons</article-title><source>J Acoust Soc Am</source><year>2003</year><volume>113</volume><fpage>3390</fpage><lpage>3402</lpage><pub-id pub-id-type="pmid">12822809</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rendall</surname><given-names>D</given-names></name><name><surname>Notman</surname><given-names>H</given-names></name><name><surname>Owren</surname><given-names>MJ</given-names></name></person-group><article-title>Asymmetries in the individual distinctiveness and maternal recognition of infant contact calls and distress screams in baboons</article-title><source>J Acoust Soc Am</source><year>2009</year><volume>125</volume><fpage>1792</fpage><lpage>1805</lpage><pub-id pub-id-type="pmcid">PMC2736728</pub-id><pub-id pub-id-type="pmid">19275336</pub-id><pub-id pub-id-type="doi">10.1121/1.3068453</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epple</surname><given-names>G</given-names></name></person-group><article-title>Comparative studies on vocalization in marmoset monkeys (Hapalidae)</article-title><source>Folia Primatol (Basel)</source><year>1968</year><volume>8</volume><fpage>1</fpage><lpage>40</lpage><pub-id pub-id-type="pmid">4966050</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turesson</surname><given-names>HK</given-names></name><name><surname>Ribeiro</surname><given-names>S</given-names></name><name><surname>Pereira</surname><given-names>DR</given-names></name><name><surname>Papa</surname><given-names>JP</given-names></name><name><surname>de Albuquerque</surname><given-names>VHC</given-names></name></person-group><article-title>Machine learning algorithms for automatic classification of marmoset vocalizations</article-title><source>PloS One</source><year>2016</year><volume>11</volume><elocation-id>e0163041</elocation-id><pub-id pub-id-type="pmcid">PMC5031457</pub-id><pub-id pub-id-type="pmid">27654941</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0163041</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y-J</given-names></name><name><surname>Huang</surname><given-names>J-F</given-names></name><name><surname>Gong</surname><given-names>N</given-names></name><name><surname>Ling</surname><given-names>Z-H</given-names></name><name><surname>Hu</surname><given-names>Y</given-names></name></person-group><article-title>Automatic detection and classification of marmoset vocalizations using deep and recurrent neural networks</article-title><source>J Acoust Soc Am</source><year>2018</year><volume>144</volume><fpage>478</fpage><lpage>487</lpage><pub-id pub-id-type="pmid">30075670</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>JB</given-names></name><name><surname>Rabiner</surname><given-names>LR</given-names></name></person-group><source>A unified approach to short-time Fourier analysis and synthesis</source><conf-name>Proc IEEE</conf-name><year>1977</year><volume>65</volume><fpage>1558</fpage><lpage>1564</lpage><pub-id pub-id-type="doi">10.1109/PROC.1977.10770</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gingras</surname><given-names>B</given-names></name><name><surname>Fitch</surname><given-names>WT</given-names></name></person-group><article-title>A three-parameter model for classifying anurans into four genera based on advertisement calls</article-title><source>J Acoust Soc Am</source><year>2013</year><volume>133</volume><fpage>547</fpage><lpage>559</lpage><pub-id pub-id-type="pmid">23297926</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Murugaiya</surname><given-names>R</given-names></name><name><surname>Gamage</surname><given-names>MMM</given-names></name><name><surname>Murugiah</surname><given-names>K</given-names></name><name><surname>Perumal</surname><given-names>M</given-names></name></person-group><source>Acoustic-Based Applications for Vertebrate Vocalization</source><year>2021</year><publisher-name>Springer Nature</publisher-name></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fichtel</surname><given-names>C</given-names></name><name><surname>Kappeler</surname><given-names>PM</given-names></name><name><surname>Perret</surname><given-names>M</given-names></name><name><surname>Huchard</surname><given-names>E</given-names></name><name><surname>Henry</surname><given-names>P-Y</given-names></name></person-group><article-title>Honest signaling in mouse lemur vocalizations?</article-title><source>Int J Primatol</source><year>2021</year><fpage>1</fpage><lpage>22</lpage></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfefferle</surname><given-names>D</given-names></name><name><surname>West</surname><given-names>PM</given-names></name><name><surname>Grinnell</surname><given-names>J</given-names></name><name><surname>Packer</surname><given-names>C</given-names></name><name><surname>Fischer</surname><given-names>J</given-names></name></person-group><article-title>Do acoustic features of lion, Panthera leo, roars reflect sex and male condition?</article-title><source>JAcoustSocAm</source><year>2007</year><volume>121</volume><fpage>3947</fpage><lpage>3953</lpage><pub-id pub-id-type="pmid">17552741</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Roe</surname><given-names>P</given-names></name></person-group><source>Acoustic features for multi-level classification of Australian frogs</source><conf-name>2015 10th International Conference on Information, Communications and Signal Processing (ICICS)</conf-name><conf-sponsor>IEEE</conf-sponsor><year>2015</year><fpage>1</fpage><lpage>5</lpage></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norcross</surname><given-names>JL</given-names></name><name><surname>Newman</surname><given-names>JD</given-names></name></person-group><article-title>Context and gender-specific differences in the acoustic structure of common marmoset (Callithrix jacchus) phee calls</article-title><source>Am J Primatol</source><year>1993</year><volume>30</volume><fpage>37</fpage><lpage>54</lpage><pub-id pub-id-type="pmid">31941179</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>CT</given-names></name><name><surname>Scarl</surname><given-names>J</given-names></name><name><surname>Hauser</surname><given-names>MD</given-names></name></person-group><article-title>Sensory biases underlie sex differences in tamarin long call structure</article-title><source>Anim Behav</source><year>2004</year><volume>68</volume><fpage>713</fpage><lpage>720</lpage></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>J</given-names></name><name><surname>Hauser</surname><given-names>M</given-names></name><name><surname>Miller</surname><given-names>C</given-names></name><name><surname>Gil-Da-Costa</surname><given-names>R</given-names></name></person-group><article-title>Selective phonotaxis by cotton-top tamarins (Saguinus oedipus)</article-title><source>Behaviour</source><year>2001</year><volume>138</volume><fpage>811</fpage><lpage>826</lpage></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zürcher</surname><given-names>Y</given-names></name><name><surname>Willems</surname><given-names>EP</given-names></name><name><surname>Burkart</surname><given-names>JM</given-names></name></person-group><article-title>Trade-offs between vocal accommodation and individual recognisability in common marmoset vocalizations</article-title><source>Sci Rep</source><year>2021</year><volume>11</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="pmcid">PMC8333328</pub-id><pub-id pub-id-type="pmid">34344939</pub-id><pub-id pub-id-type="doi">10.1038/s41598-021-95101-8</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chawla</surname><given-names>NV</given-names></name><name><surname>Bowyer</surname><given-names>KW</given-names></name><name><surname>Hall</surname><given-names>LO</given-names></name><name><surname>Kegelmeyer</surname><given-names>WP</given-names></name></person-group><article-title>SMOTE: synthetic minority over-sampling technique</article-title><source>J Artif Intell Res</source><year>2002</year><volume>16</volume><fpage>321</fpage><lpage>357</lpage></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Larsen</surname><given-names>B</given-names></name></person-group><article-title>Synthetic Minority Over-sampling Technique (SMOTE)</article-title><source>GitHub</source><year>2022</year><comment><ext-link ext-link-type="uri" xlink:href="https://github.com/dkbsl/matlab_smote/releases/tag/1.0">https://github.com/dkbsl/matlab_smote/releases/tag/1.0</ext-link></comment></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fulcher</surname><given-names>BD</given-names></name><name><surname>Jones</surname><given-names>NS</given-names></name></person-group><article-title>Highly Comparative Feature-Based Time-Series Classification</article-title><source>IEEE Trans Knowl Data Eng</source><year>2014</year><volume>26</volume><fpage>3026</fpage><lpage>3037</lpage><pub-id pub-id-type="doi">10.1109/TKDE.2014.2316504</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fulcher</surname><given-names>BD</given-names></name><name><surname>Jones</surname><given-names>NS</given-names></name></person-group><article-title>hctsa: A computational framework for automated time-series phenotyping using massive feature extraction</article-title><source>Cell Syst</source><year>2017</year><volume>5</volume><fpage>527</fpage><lpage>531</lpage><pub-id pub-id-type="pmid">29102608</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MatLab</surname><given-names>P</given-names></name></person-group><article-title>9.7. 0.1190202 (R2019b)</article-title><source>MathWorks Inc Natick MA USA</source><year>2018</year></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van der Maaten</surname><given-names>L</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><article-title>Visualizing data using t-SNE</article-title><source>J Mach Learn Res</source><year>2008</year><volume>9</volume></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schapire</surname><given-names>RE</given-names></name></person-group><article-title>The strength of weak learnability</article-title><source>Mach Learn</source><year>1990</year><volume>5</volume><fpage>197</fpage><lpage>227</lpage><pub-id pub-id-type="doi">10.1007/BF00116037</pub-id></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freund</surname><given-names>Y</given-names></name><name><surname>Schapire</surname><given-names>RE</given-names></name></person-group><article-title>A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting</article-title><source>J Comput Syst Sci</source><year>1997</year><volume>55</volume><fpage>119</fpage><lpage>139</lpage><pub-id pub-id-type="doi">10.1006/jcss.1997.1504</pub-id></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kégl</surname><given-names>B</given-names></name></person-group><article-title>The return of AdaBoost.MH: multi-class Hamming trees</article-title><source>ArXiv13126086 Cs</source><year>2013</year></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McInnes</surname><given-names>L</given-names></name><name><surname>Healy</surname><given-names>J</given-names></name><name><surname>Melville</surname><given-names>J</given-names></name></person-group><source>UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction</source><year>2020</year></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kobak</surname><given-names>D</given-names></name><name><surname>Berens</surname><given-names>P</given-names></name></person-group><article-title>The art of using t-SNE for single-cell transcriptomics</article-title><source>Nat Commun</source><year>2019</year><volume>10</volume><elocation-id>5416</elocation-id><pub-id pub-id-type="pmcid">PMC6882829</pub-id><pub-id pub-id-type="pmid">31780648</pub-id><pub-id pub-id-type="doi">10.1038/s41467-019-13056-x</pub-id></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barker</surname><given-names>AJ</given-names></name><name><surname>Veviurko</surname><given-names>G</given-names></name><name><surname>Bennett</surname><given-names>NC</given-names></name><name><surname>Hart</surname><given-names>DW</given-names></name><name><surname>Mograby</surname><given-names>L</given-names></name><name><surname>Lewin</surname><given-names>GR</given-names></name></person-group><article-title>Cultural transmission of vocal dialect in the naked mole-rat</article-title><source>Science</source><year>2021</year><volume>371</volume><fpage>503</fpage><lpage>507</lpage><pub-id pub-id-type="pmid">33510025</pub-id></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maciej</surname><given-names>P</given-names></name><name><surname>Fischer</surname><given-names>J</given-names></name><name><surname>Hammerschmidt</surname><given-names>K</given-names></name></person-group><article-title>Transmission Characteristics of Primate Vocalizations: Implications for Acoustic Analyses</article-title><source>PLOS ONE</source><year>2011</year><volume>6</volume><elocation-id>e23015</elocation-id><pub-id pub-id-type="pmcid">PMC3148239</pub-id><pub-id pub-id-type="pmid">21829682</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0023015</pub-id></element-citation></ref><ref id="R64"><label>64</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reby</surname><given-names>D</given-names></name><name><surname>McComb</surname><given-names>K</given-names></name></person-group><article-title>Anatomical constraints generate honesty: acoustic cues to age and weight in the roars of red deer stags</article-title><source>Anim Behav</source><year>2003</year><volume>65</volume><fpage>519</fpage><lpage>530</lpage><pub-id pub-id-type="doi">10.1006/anbe.2003.2078</pub-id></element-citation></ref><ref id="R65"><label>65</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zürcher</surname><given-names>Y</given-names></name><name><surname>Burkart</surname><given-names>JM</given-names></name></person-group><article-title>Evidence for dialects in three captive populations of common marmosets (Callithrix jacchus)</article-title><source>Int J Primatol</source><year>2017</year><volume>38</volume><fpage>780</fpage><lpage>793</lpage></element-citation></ref><ref id="R66"><label>66</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Secker</surname><given-names>A</given-names></name><name><surname>Davies</surname><given-names>MN</given-names></name><name><surname>Freitas</surname><given-names>AA</given-names></name><name><surname>Clark</surname><given-names>E</given-names></name><name><surname>Timmis</surname><given-names>J</given-names></name><name><surname>Flower</surname><given-names>DR</given-names></name></person-group><article-title>Hierarchical classification of G-protein-coupled receptors with data-driven selection of attributes and classifiers</article-title><source>Int J Data Min Bioinform</source><year>2010</year><volume>4</volume><fpage>191</fpage><lpage>210</lpage><pub-id pub-id-type="pmid">20423020</pub-id></element-citation></ref><ref id="R67"><label>67</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guidotti</surname><given-names>R</given-names></name><name><surname>Monreale</surname><given-names>A</given-names></name><name><surname>Ruggieri</surname><given-names>S</given-names></name><name><surname>Turini</surname><given-names>F</given-names></name><name><surname>Giannotti</surname><given-names>F</given-names></name><name><surname>Pedreschi</surname><given-names>D</given-names></name></person-group><article-title>A Survey of Methods for Explaining Black Box Models</article-title><source>ACM Comput Surv</source><year>2019</year><volume>51</volume><fpage>1</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1145/3236009</pub-id></element-citation></ref><ref id="R68"><label>68</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fernandez</surname><given-names>A</given-names></name><name><surname>Garcia</surname><given-names>S</given-names></name><name><surname>Herrera</surname><given-names>F</given-names></name><name><surname>Chawla</surname><given-names>NV</given-names></name></person-group><article-title>SMOTE for Learning from Imbalanced Data: Progress and Challenges, Marking the 15-year Anniversary</article-title><source>J Artif Intell Res</source><year>2018</year><volume>61</volume><fpage>863</fpage><lpage>905</lpage><pub-id pub-id-type="doi">10.1613/jair.1.11192</pub-id></element-citation></ref><ref id="R69"><label>69</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fernández</surname><given-names>A</given-names></name><name><surname>García</surname><given-names>S</given-names></name><name><surname>Galar</surname><given-names>M</given-names></name><name><surname>Prati</surname><given-names>RC</given-names></name><name><surname>Krawczyk</surname><given-names>B</given-names></name><name><surname>Herrera</surname><given-names>F</given-names></name></person-group><source>Learning from imbalanced data sets</source><year>2018</year><publisher-name>Springer</publisher-name></element-citation></ref><ref id="R70"><label>70</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>Y</given-names></name><name><surname>Midori Maeda</surname><given-names>T</given-names></name><name><surname>Solís-Lemus</surname><given-names>C</given-names></name><name><surname>Pimentel-Alarcón</surname><given-names>D</given-names></name><name><surname>Buřivalová</surname><given-names>Z</given-names></name></person-group><article-title>Classification of animal sounds in a hyperdiverse rainforest using convolutional neural networks with data augmentation</article-title><source>Ecol Indic</source><year>2022</year><volume>145</volume><elocation-id>109621</elocation-id><pub-id pub-id-type="doi">10.1016/j.ecolind.2022.109621</pub-id></element-citation></ref><ref id="R71"><label>71</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mao</surname><given-names>A</given-names></name><etal/></person-group><article-title>In press. Automated identification of chicken distress vocalizations using deep learning models</article-title><source>J R Soc Interface</source><volume>19</volume><elocation-id>20210921</elocation-id><pub-id pub-id-type="pmcid">PMC9240672</pub-id><pub-id pub-id-type="pmid">35765806</pub-id><pub-id pub-id-type="doi">10.1098/rsif.2021.0921</pub-id></element-citation></ref><ref id="R72"><label>72</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stowell</surname><given-names>D</given-names></name></person-group><article-title>Computational bioacoustics with deep learning: a review and roadmap</article-title><source>PeerJ</source><year>2022</year><volume>10</volume><elocation-id>e13152</elocation-id><pub-id pub-id-type="pmcid">PMC8944344</pub-id><pub-id pub-id-type="pmid">35341043</pub-id><pub-id pub-id-type="doi">10.7717/peerj.13152</pub-id></element-citation></ref><ref id="R73"><label>73</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Batista</surname><given-names>GE</given-names></name><name><surname>Prati</surname><given-names>RC</given-names></name><name><surname>Monard</surname><given-names>MC</given-names></name></person-group><article-title>A study of the behavior of several methods for balancing machine learning training data</article-title><source>ACM SIGKDD Explor Newsl</source><year>2004</year><volume>6</volume><fpage>20</fpage><lpage>29</lpage></element-citation></ref><ref id="R74"><label>74</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Laurikkala</surname><given-names>J</given-names></name></person-group><source>Improving identification of difficult small classes by balancing class distribution</source><conf-name>Conference on artificial intelligence in medicine in Europe</conf-name><conf-sponsor>Springer</conf-sponsor><year>2001</year><fpage>63</fpage><lpage>66</lpage></element-citation></ref><ref id="R75"><label>75</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Poolsawad</surname><given-names>N</given-names></name><name><surname>Kambhampati</surname><given-names>C</given-names></name><name><surname>Cleland</surname><given-names>JGF</given-names></name></person-group><source>Balancing class for performance of classification with a clinical dataset</source><conf-name>proceedings of the World Congress on Engineering</conf-name><year>2014</year><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="R76"><label>76</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ivanenko</surname><given-names>A</given-names></name><name><surname>Watkins</surname><given-names>P</given-names></name><name><surname>van Gerven</surname><given-names>MAJ</given-names></name><name><surname>Hammerschmidt</surname><given-names>K</given-names></name><name><surname>Englitz</surname><given-names>B</given-names></name></person-group><article-title>Classifying sex and strain from mouse ultrasonic vocalizations using deep learning</article-title><source>PLOS Comput Biol</source><year>2020</year><volume>16</volume><elocation-id>e1007918</elocation-id><pub-id pub-id-type="pmcid">PMC7347231</pub-id><pub-id pub-id-type="pmid">32569292</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007918</pub-id></element-citation></ref><ref id="R77"><label>77</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lehmann</surname><given-names>KDS</given-names></name><name><surname>Jensen</surname><given-names>FH</given-names></name><name><surname>Gersick</surname><given-names>AS</given-names></name><name><surname>Strandburg-Peshkin</surname><given-names>A</given-names></name><name><surname>Holekamp</surname><given-names>KE</given-names></name></person-group><article-title>Long-distance vocalizations of spotted hyenas contain individual, but not group, signatures</article-title><source>Proc R Soc B Biol Sci</source><year>2022</year><volume>289</volume><elocation-id>20220548</elocation-id><pub-id pub-id-type="pmcid">PMC9297016</pub-id><pub-id pub-id-type="pmid">35855604</pub-id><pub-id pub-id-type="doi">10.1098/rspb.2022.0548</pub-id></element-citation></ref><ref id="R78"><label>78</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Babbar</surname><given-names>R</given-names></name><name><surname>Partalas</surname><given-names>I</given-names></name><name><surname>Gaussier</surname><given-names>E</given-names></name><name><surname>Amini</surname><given-names>MR</given-names></name></person-group><article-title>On flat versus hierarchical classification in large-scale taxonomies</article-title><source>Adv Neural Inf Process Syst</source><year>2013</year><volume>26</volume></element-citation></ref><ref id="R79"><label>79</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Dumais</surname><given-names>S</given-names></name><name><surname>Chen</surname><given-names>H</given-names></name></person-group><source>Hierarchical classification of web content</source><conf-name>Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval</conf-name><year>2000</year><fpage>256</fpage><lpage>263</lpage></element-citation></ref><ref id="R80"><label>80</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>BS</given-names></name><name><surname>Harris</surname><given-names>DHR</given-names></name><name><surname>Catchpole</surname><given-names>CK</given-names></name></person-group><article-title>The stability of the vocal signature in phee calls of the common marmoset, Callithrix jacchus</article-title><source>Am J Primatol</source><year>1993</year><volume>31</volume><fpage>67</fpage><lpage>75</lpage><pub-id pub-id-type="pmid">32070085</pub-id></element-citation></ref><ref id="R81"><label>81</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>T</given-names></name></person-group><article-title>Individual olfactory signatures in common marmosets (Callithrix jacchus)</article-title><source>Am J Primatol</source><year>2006</year><volume>68</volume><fpage>585</fpage><lpage>604</lpage><pub-id pub-id-type="pmid">16715508</pub-id></element-citation></ref><ref id="R82"><label>82</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vernes</surname><given-names>SC</given-names></name><name><surname>Kriengwatana</surname><given-names>BP</given-names></name><name><surname>Beeck</surname><given-names>VC</given-names></name><name><surname>Fischer</surname><given-names>J</given-names></name><name><surname>Tyack</surname><given-names>PL</given-names></name><name><surname>ten Cate</surname><given-names>C</given-names></name><name><surname>Janik</surname><given-names>VM</given-names></name></person-group><article-title>The multi-dimensional nature of vocal learning</article-title><source>Philos Trans R Soc B Biol Sci</source><year>2021</year><volume>376</volume><elocation-id>20200236</elocation-id><pub-id pub-id-type="pmcid">PMC8419582</pub-id><pub-id pub-id-type="pmid">34482723</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2020.0236</pub-id></element-citation></ref><ref id="R83"><label>83</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruch</surname><given-names>H</given-names></name><name><surname>Zürcher</surname><given-names>Y</given-names></name><name><surname>Burkart</surname><given-names>JM</given-names></name></person-group><article-title>The function and mechanism of vocal accommodation in humans and other primates</article-title><source>Biol Rev</source><year>2018</year><volume>93</volume><fpage>996</fpage><lpage>1013</lpage><pub-id pub-id-type="pmid">29111610</pub-id></element-citation></ref><ref id="R84"><label>84</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Estes</surname><given-names>WK</given-names></name></person-group><article-title>An associative basis for coding and organization in memory</article-title><source>Coding Process Hum Mem</source><year>1972</year><fpage>161</fpage><lpage>190</lpage></element-citation></ref><ref id="R85"><label>85</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lashley</surname><given-names>KS</given-names></name></person-group><source>The problem of serial order in behavior</source><year>1951</year><publisher-loc>Bobbs-Merrill Oxford</publisher-loc></element-citation></ref><ref id="R86"><label>86</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>GA</given-names></name><name><surname>Eugene</surname><given-names>G</given-names></name><name><surname>Pribram</surname><given-names>KH</given-names></name></person-group><chapter-title>Plans and the Structure of Behaviour</chapter-title><source>Systems Research for Behavioral Sciencesystems Research</source><year>2017</year><fpage>369</fpage><lpage>382</lpage><publisher-name>Routledge</publisher-name></element-citation></ref><ref id="R87"><label>87</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Newell</surname><given-names>A</given-names></name><name><surname>Simon</surname><given-names>HA</given-names></name></person-group><article-title>GPS, a program that simulates human thought</article-title><year>1961</year></element-citation></ref><ref id="R88"><label>88</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneider</surname><given-names>DW</given-names></name><name><surname>Logan</surname><given-names>GD</given-names></name></person-group><article-title>Hierarchical control of cognitive processes: switching tasks in sequences</article-title><source>J Exp Psychol Gen</source><year>2006</year><volume>135</volume><elocation-id>623</elocation-id><pub-id pub-id-type="pmid">17087577</pub-id></element-citation></ref><ref id="R89"><label>89</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saaty</surname><given-names>TL</given-names></name></person-group><article-title>Decision making with the analytic hierarchy process</article-title><source>Int J Serv Sci</source><year>2008</year><volume>1</volume><fpage>83</fpage><lpage>98</lpage></element-citation></ref><ref id="R90"><label>90</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Monroy</surname><given-names>JA</given-names></name><name><surname>Nishikawa</surname><given-names>K</given-names></name></person-group><article-title>Prey capture in frogs: alternative strategies, biomechanical trade-offs, and hierarchical decision making</article-title><source>J Exp Zool Part Ecol Genet Physiol</source><year>2011</year><volume>315A</volume><fpage>61</fpage><lpage>71</lpage><pub-id pub-id-type="pmid">20309849</pub-id></element-citation></ref><ref id="R91"><label>91</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>Q</given-names></name><name><surname>Lin</surname><given-names>Z</given-names></name><name><surname>Zhang</surname><given-names>W</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Yang</surname><given-names>T</given-names></name></person-group><article-title>Monkey plays Pac-Man with compositional strategies and hierarchical decision-making</article-title><source>Elife</source><year>2022</year><volume>11</volume><elocation-id>e74500</elocation-id><pub-id pub-id-type="pmcid">PMC8963886</pub-id><pub-id pub-id-type="pmid">35286255</pub-id><pub-id pub-id-type="doi">10.7554/eLife.74500</pub-id></element-citation></ref><ref id="R92"><label>92</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wierucka</surname><given-names>K</given-names></name><name><surname>Pitcher</surname><given-names>BJ</given-names></name><name><surname>Harcourt</surname><given-names>R</given-names></name><name><surname>Charrier</surname><given-names>I</given-names></name></person-group><article-title>Multimodal mother–offspring recognition: the relative importance of sensory cues in a colonial mammal</article-title><source>Anim Behav</source><year>2018</year><volume>146</volume><fpage>135</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1016/j.anbehav.2018.10.019</pub-id></element-citation></ref><ref id="R93"><label>93</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wierucka</surname><given-names>K</given-names></name><name><surname>Pitcher</surname><given-names>BJ</given-names></name><name><surname>Harcourt</surname><given-names>R</given-names></name><name><surname>Charrier</surname><given-names>I</given-names></name></person-group><article-title>The role of visual cues in mother–pup reunions in a colonially breeding mammal</article-title><source>Biol Lett</source><year>2017</year><volume>13</volume><elocation-id>20170444</elocation-id><pub-id pub-id-type="pmcid">PMC5719376</pub-id><pub-id pub-id-type="pmid">29093175</pub-id><pub-id pub-id-type="doi">10.1098/rsbl.2017.0444</pub-id></element-citation></ref><ref id="R94"><label>94</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sridhar</surname><given-names>VH</given-names></name><name><surname>Li</surname><given-names>L</given-names></name><name><surname>Gorbonos</surname><given-names>D</given-names></name><name><surname>Nagy</surname><given-names>M</given-names></name><name><surname>Schell</surname><given-names>BR</given-names></name><name><surname>Sorochkin</surname><given-names>T</given-names></name><name><surname>Gov</surname><given-names>NS</given-names></name><name><surname>Couzin</surname><given-names>ID</given-names></name></person-group><article-title>The geometry of decision-making in individuals and collectives</article-title><source>Proc Natl Acad Sci</source><year>2021</year><volume>118</volume><elocation-id>e2102157118</elocation-id><pub-id pub-id-type="pmcid">PMC8685676</pub-id><pub-id pub-id-type="pmid">34880130</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2102157118</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>The hierarchical classification approach.</title><p>Features from trills, phees, and food calls were extracted using HCTSA. These features were used to train AdaBoost to first classify calls based on sex, and then individual identities. The oval denotes start, parallelograms inputs/outputs, rectangles processes, and the diamond a decision.</p></caption><graphic xlink:href="EMS157421-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Qualitative and quantitative comparisons of random 20 and top 20 features for clustering data.</title><p>(A) Qualitative comparison. Figures are t-SNE plots (squared Euclidean distance metric) of Balanced-X datasets using 20 randomly chosen features or the top 20 features for classification by AdaBoost for that call type. Each point is a call, coloured according to the source individual. (B) Quantitative comparison. Blue bars depict frequencies (histogram) of mean silhouette scores obtained after performing t-SNE using 20 random features selected 100 times for that call type (trills/phees/food calls) on Balanced-X datasets. The grey line depicts the Gaussian function fit to the histogram. Orange vertical bars denote the mean silhouette scores obtained after performing t-SNE using top 20 features for classification by AdaBoost for that call type. The p-value shown is the normalised area under the gaussian function to the right of the orange bar.</p></caption><graphic xlink:href="EMS157421-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Individual precisions and recalls for determining the source identity from trills by the hierarchical classifier for females (A) and males (B).</title><p>Confusion matrices are shown, with rows depicting the true source identity and columns representing the prediction made by the hierarchical classifier. The absolute number of calls is shown within the matrices with those correctly classified highlighted in blue and those wrongly classified highlighted in orange (intensity proportional to the number for both). The rows and columns are summarised with the row summary depicting individual precisions in blue and the columns summary showing individual recalls in blue.</p></caption><graphic xlink:href="EMS157421-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Classifier performance at different sample sizes.</title><p>Precisions of AdaBoost as a function of sample size per class for trills, phees, and food calls with s.d. represented as shaded regions around lines connecting means. Dashed black lines indicate the chance precision of classification for given call type. Note that the highest sample size per class was obtained by oversampling the minority classes so that they are equal to the majority class (SMOTE, see methods). See <xref ref-type="supplementary-material" rid="SD1">supplementary table 1</xref> for sample sizes in our original dataset.</p></caption><graphic xlink:href="EMS157421-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Mutual and distinct features used by hierarchical classifiers at various levels of classification.</title><p>Venn diagrams denote the set of features used for determining the sex, source identity among females (Female ID), and source identity among males (Male ID). The area of the circle is scaled to the number of features at that level. The percentage of total features used by the hierarchical classifier belonging to each area within the Venn diagram is denoted.</p></caption><graphic xlink:href="EMS157421-f005"/></fig><table-wrap id="T1" orientation="portrait" position="float"><label>Table 1</label><caption><title>Sample sizes of datasets.</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="middle" align="center">Dataset</th><th valign="middle" align="center">Trills</th><th valign="middle" align="center">Phees</th><th valign="middle" align="center">Food calls</th></tr></thead><tbody><tr><td valign="middle" align="center">Original-X</td><td valign="middle" align="center">1246</td><td valign="middle" align="center">1443</td><td valign="middle" align="center">4434</td></tr><tr><td valign="middle" align="center">Imbalanced-X</td><td valign="middle" align="center">1206</td><td valign="middle" align="center">1401</td><td valign="middle" align="center">4419</td></tr><tr><td valign="middle" align="center">Balanced-X</td><td valign="middle" align="center">4528</td><td valign="middle" align="center">3350</td><td valign="middle" align="center">15372</td></tr><tr><td valign="middle" align="center">Balanced197-X</td><td valign="middle" align="center">3152</td><td valign="middle" align="center">1970</td><td valign="middle" align="center">3546</td></tr><tr><td valign="middle" align="center">Balanced99-X</td><td valign="middle" align="center">1584</td><td valign="middle" align="center">990</td><td valign="middle" align="center">1782</td></tr><tr><td valign="middle" align="center">Balanced50-X</td><td valign="middle" align="center">800</td><td valign="middle" align="center">500</td><td valign="middle" align="center">900</td></tr></tbody></table></table-wrap><table-wrap id="T2" orientation="portrait" position="float"><label>Table 2</label><caption><title>AdaBoost performance for imbalanced and balanced datasets.</title><p>Imbalanced-X and Balanced-X datasets were used. Mean ± s.d. are provided for ROC-AUC scores.</p></caption><table frame="hsides" rules="rows"><thead><tr><th valign="middle" align="center" rowspan="2">Call</th><th valign="middle" align="center" rowspan="2">Classes (Individuals)</th><th valign="middle" align="center" colspan="3">Imbalanced</th><th valign="middle" align="center" colspan="3">Balanced</th></tr><tr><th valign="middle" align="center">Sample size</th><th valign="middle" align="center">ROC-AUC (%)</th><th valign="middle" align="center">Accuracy (%)</th><th valign="middle" align="center">Sample size</th><th valign="middle" align="center">ROC-AUC (%)</th><th valign="middle" align="center">Accuracy (%)</th></tr></thead><tbody><tr><td valign="middle" align="center">Trills</td><td valign="middle" align="center">16</td><td valign="middle" align="center">1206</td><td valign="middle" align="center">92.63 ± 4.30</td><td valign="middle" align="center">64.84</td><td valign="middle" align="center">4528</td><td valign="middle" align="center">98.95 ± 0.91</td><td valign="middle" align="center">82.91</td></tr><tr><td valign="middle" align="center">Phees</td><td valign="middle" align="center">10</td><td valign="middle" align="center">1498</td><td valign="middle" align="center">94.36 ± 4.54</td><td valign="middle" align="center">70.96</td><td valign="middle" align="center">3550</td><td valign="middle" align="center">98.55 ± 1.42</td><td valign="middle" align="center">83.92</td></tr><tr><td valign="middle" align="center">Food calls</td><td valign="middle" align="center">18</td><td valign="middle" align="center">4419</td><td valign="middle" align="center">92.72 ± 5.08</td><td valign="middle" align="center">60.8</td><td valign="middle" align="center">15372</td><td valign="middle" align="center">97.04 ± 2.31</td><td valign="middle" align="center">71.43</td></tr></tbody></table></table-wrap><table-wrap id="T3" orientation="portrait" position="float"><label>Table 3</label><caption><title>Comparing non-hierarchical and hierarchical approaches for classifying calls based on source identity.</title><p>Mean ± s.d. precisions and recalls with corresponding p-values for testing the hypotheses: mean precisions/recalls of non-hierarchical = hierarchical.</p></caption><table frame="hsides" rules="rows"><thead><tr><th valign="middle" align="center" rowspan="2">Call</th><th valign="middle" align="center" rowspan="2">Classes (individuals)</th><th valign="middle" align="center" colspan="2">Non-hierarchical</th><th valign="middle" align="center" colspan="2">Hierarchical</th><th valign="middle" align="center" colspan="2">p-value</th></tr><tr><th valign="middle" align="center">Precision (%)</th><th valign="middle" align="center">Recall (%)</th><th valign="middle" align="center">Precision (%)</th><th valign="middle" align="center">Recall (%)</th><th valign="middle" align="center">For precision</th><th valign="middle" align="center">For recall</th></tr></thead><tbody><tr><td valign="middle" align="center"><bold>Trills</bold></td><td valign="middle" align="center">16</td><td valign="middle" align="center">83.42 ± 11.48</td><td valign="middle" align="center">82.87 ± 8.63</td><td valign="middle" align="center">94.42 ± 9.61</td><td valign="middle" align="center">94.19 ± 8.24</td><td valign="middle" align="center"><bold>0.017</bold></td><td valign="middle" align="center"><bold>0.01</bold></td></tr><tr><td valign="middle" align="center"><bold>Phees</bold></td><td valign="middle" align="center">10</td><td valign="middle" align="center">84.15 ± 9.84</td><td valign="middle" align="center">83.93 ± 8.63</td><td valign="middle" align="center">92.57 ± 3.30</td><td valign="middle" align="center">92.55 ± 3.79</td><td valign="middle" align="center"><bold>0.027</bold></td><td valign="middle" align="center"><bold>0.037</bold></td></tr><tr><td valign="middle" align="center"><bold>Food calls</bold></td><td valign="middle" align="center">18</td><td valign="middle" align="center">72.23 ± 15.09</td><td valign="middle" align="center">71.43 ± 13.13</td><td valign="middle" align="center">87.21 ± 9.71</td><td valign="middle" align="center">86.65 ± 5.62</td><td valign="middle" align="center"><bold>0.01</bold></td><td valign="middle" align="center"><bold>0.002</bold></td></tr></tbody></table></table-wrap></floats-group></article>