<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS157131</article-id><article-id pub-id-type="doi">10.1101/2022.11.10.515968</article-id><article-id pub-id-type="archive">PPR571050</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">2</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Encoding of 2D self-centered plans and world-centered positions in the rat frontal orienting field</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Li</surname><given-names>Liujunli</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Flesch</surname><given-names>Timo</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Ma</surname><given-names>Ce</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Jingjie</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Chen</surname><given-names>Yizhou</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Chen</surname><given-names>Hung-Tu</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A7">7</xref></contrib><contrib contrib-type="author"><name><surname>Erlich</surname><given-names>Jeffrey C.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A5">5</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>NYU-ECNU Institute of Brain and Cognitive Science at NYU Shanghai, China</aff><aff id="A2"><label>2</label>NYU Shanghai, Shanghai, China</aff><aff id="A3"><label>3</label>Shanghai Key Laboratory of Brain Functional Genomics (Ministry of Education), East China Normal University, Shanghai, China</aff><aff id="A4"><label>4</label>Oxford University, Oxford, UK</aff><aff id="A5"><label>5</label>Sainsbury Wellcome Centre, University College London, London, UK</aff><aff id="A6"><label>6</label>Baylor College of Medicine, Houston, USA</aff><aff id="A7"><label>7</label>Dartmouth College, Hanover, USA</aff><author-notes><corresp id="CR1">
<label>*</label><bold>Correspondence to:</bold> <email>j.erlich@ucl.ac.uk</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>16</day><month>11</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>13</day><month>11</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">The neural mechanisms of motor planning have been extensively studied in rodents. Preparatory activity in the frontal cortex predicts upcoming choice, but limitations of typical tasks have made it challenging to determine whether the spatial information is in a self-centered direction reference frame or a world-centered position reference frame. Here, we trained rats to make delayed visually-guided orienting movements to six different directions, with four different target positions for each direction, which allowed us to disentangle direction versus position tuning in neural activity. We recorded single unit activity from the rat frontal orienting field (FOF) in the secondary motor cortex, a region involved in planning orienting movements. Population analyses revealed that the FOF encodes two separate 2D maps of space. First, a 2D map of the planned and ongoing movement in a self-centered direction reference frame. Second, a 2D map of the animal’s current position on the port wall in a world-centered reference frame. Thus, preparatory activity in the FOF represents self-centered upcoming movement directions, but FOF neurons multiplex both self- and world-reference frame variables at the level of single neurons. Neural network model comparison supports the view that despite the presence of world-centered representations, the FOF receives the target information as self-centered input and generates self-centered planning signals.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">We use multiple reference frames to represent the world. For example, as you plan a movement to reach for your morning coffee, the arm region of motor cortex may represent the goal in an arm-centered reference frame and your frontal eye field represents the goal in an eye-centered reference frame. Other areas of your brain may represent the cup relative to the room or table or the milk or sugar. During sensorimotor behaviors, sensory information is initially represented in sensor reference frames and motor commands are finally represented in muscle reference frames. Since the sensors and effectors are embodied, we can think of these representations as being in self-centered (or egocentric) reference frames, i.e. they are reference frames that move around with the subject as they move through the world. However, our lived experience is in a world-centered (or allocentric) reference frame: we feel as if we move around and make decisions in a stable world. Moreover, allocentric representations are found in a wide range of brain regions (<xref ref-type="bibr" rid="R27">Hafting et al., 2005</xref>, <xref ref-type="bibr" rid="R41">O’Keefe and Nadel, 1978</xref>, <xref ref-type="bibr" rid="R60">Taube et al., 1990</xref>, <xref ref-type="bibr" rid="R63">Wilber et al., 2014</xref>). Thus, a full understanding of the neurobiology of motor planning needs to address the question of where and how these reference frame transformations take place (<xref ref-type="bibr" rid="R2">Andersen et al., 1990</xref>, <xref ref-type="bibr" rid="R3">1985</xref>, <xref ref-type="bibr" rid="R4">Andersen and Mountcastle, 1983</xref>, <xref ref-type="bibr" rid="R15">Cohen and Andersen, 2002</xref>).</p><p id="P3">The neural mechanisms of motor planning in rodents have been extensively studied in two-alternative forced choice (2AFC) and go-nogo tasks (<xref ref-type="bibr" rid="R13">Chen et al., 2017</xref>, <xref ref-type="bibr" rid="R22">Erlich et al., 2011</xref>, <xref ref-type="bibr" rid="R25">Guo et al., 2014</xref>, <xref ref-type="bibr" rid="R59">Sul et al., 2011</xref>). Converging evidence has implicated the frontal orienting field (FOF), a subregion of the secondary motor cortex (M2), as a cortical substrate for planning orienting movements (<xref ref-type="bibr" rid="R22">Erlich et al., 2011</xref>, <xref ref-type="bibr" rid="R28">Hanks et al., 2015</xref>, <xref ref-type="bibr" rid="R42">Olson et al., 2019</xref>), especially when those plans require flexible sensorimotor processes (<xref ref-type="bibr" rid="R23">Erlich et al., 2015</xref>, <xref ref-type="bibr" rid="R57">Siniscalchi et al., 2016</xref>, <xref ref-type="bibr" rid="R68">Zhu et al., 2021</xref>). Quantitative models suggest that the FOF is a part of a bistable attractor network for short-term memory and decision making in 2AFC orienting movement planning (<xref ref-type="bibr" rid="R28">Hanks et al., 2015</xref>, <xref ref-type="bibr" rid="R32">Kopec et al., 2015</xref>, <xref ref-type="bibr" rid="R45">Piet et al., 2017</xref>). Similar work in the mouse anterior lateral motor cortex (ALM) during directional licking also identified discrete bistable attractor models as best accounting for observed neural activity and perturbation results (<xref ref-type="bibr" rid="R29">Inagaki et al., 2019</xref>, <xref ref-type="bibr" rid="R35">Li et al., 2016</xref>). Bistable attractor models are currently state-of-the-art for 2AFC motor planning, but they are ambiguous as to the spatial reference frame of the neural coding. Do they represent the planned movement direction in an self-centered reference frame, the target location in an world-centered reference frame, or a ‘decision’ in an abstract reference frame? Given the limitations of those behavioral paradigms, the answer is largely unknown.</p><p id="P4">The FOF is a potential site to integrate egocentric and allocentric spatial representations, as it receives input from the posterior parietal cortex (<xref ref-type="bibr" rid="R47">Reep et al., 1994</xref>) and the retrosplenial cortex (<xref ref-type="bibr" rid="R66">Yamawaki et al., 2016</xref>), both of which exhibit egocentric as well as allocentric spatial representations (<xref ref-type="bibr" rid="R61">Wang et al., 2020</xref>). To test the reference frame of action planning representation in the FOF, we designed a multi-directional, multi-positional orienting task that could distinguish between allocentric and egocentric reference frames. We found preparatory and movement-related activity in the FOF that were in the egocentric reference frame. Interestingly, allocentric target position information was also encoded in the FOF, but emerged later than direction encoding. The allocentric encoding represented the <italic>current</italic> rather than the upcoming position of the animals, indicating the allocentric activity did not play a primary role in action planning. These two reference frames were multiplexed at the single-neuron level: task-related activity was best described as an egocentric direction tuning multiplicatively modulated by the allocentric current position.</p><p id="P5">Similar gain fields have been previously reported in primate frontal and parietal eye fields during saccade planning (<xref ref-type="bibr" rid="R3">Andersen et al., 1985</xref>, <xref ref-type="bibr" rid="R11">Cassanello and Ferrera, 2010</xref>, <xref ref-type="bibr" rid="R12">2007</xref>) and were suggested to support reference frame transformation (<xref ref-type="bibr" rid="R46">Pouget and Sejnowski, 1997</xref>, <xref ref-type="bibr" rid="R53">Salinas and Abbott, 1995</xref>, <xref ref-type="bibr" rid="R69">Zipser and Andersen, 1988</xref>). However, a recurrent network model whose input and output were both in the egocentric reference frame had the most similar activity to the FOF neurons: including representing allocentric activity and gain fields. This finding calls into question the proposed role of gain fields in supporting reference frame transformations. Moreover, it hints that the allocentric representations in FOF are not necessary for the task. These results further support our conclusion that planning in the FOF takes place in an egocentric coordinate frame, although the multiplexed spatial encoding may support spatial information integration in downstream brain areas or online error correction which might be required for planning complex movements sequences.</p></sec><sec id="S2" sec-type="results"><title>Results</title><p id="P6">We trained rats to perform visually-guided orienting movements to multiple directions and multiple target positions. The training apparatus consisted of a vertical port wall with 7 operant ports and an additional port for reward delivery (<xref ref-type="fig" rid="F1">Figure 1A</xref>). For the majority of sessions, there were 24 trial types: 6 possible directions, with each direction starting from 4 different start positions (<xref ref-type="supplementary-material" rid="SD1">Figure S1A</xref>). This design allowed us to dissociate self-centered movement direction from the world-centered start and target position (<xref ref-type="fig" rid="F1">Figure 1B</xref>). Throughout the paper we indicate self-centered movement directions with a blue-red (left-right) and dark-light (down-up) color scheme (<xref ref-type="fig" rid="F1">Figure 1C</xref>). For world-centered positions we use a green-orange (left-right) and dark-light (down-up) color scheme (<xref ref-type="fig" rid="F1">Figure 1D</xref>).</p><p id="P7">Each trial began with illuminating the yellow and blue LEDs around a ‘start port’, which was randomly chosen from one of the 7 operant ports. Rats fixated in the start port until a go sound. The start port LEDs extinguished at the beginning of fixation, and the target port was illuminated with blue LEDs shortly after fixation onset with 0 to 0.29 s delay. For trials that started in the center, one of the six remaining operant ports was chosen at random as the target. For the other start positions, one of three adjacent ports was chosen as the target (<xref ref-type="supplementary-material" rid="SD1">Figure S1A</xref>). After the go sound, rats withdrew from the start port and moved to the target port. The target port LEDs extinguished once the animal arrived at the target port, or when the animal poked in another port in error. If the rat poked in the correct port, the water delivery port LED illuminated, a “correct” sound was played, and reward could be collected at the reward port (<xref ref-type="fig" rid="F1">Figure 1A&amp;E</xref>). If the rat poked into the wrong port, an “error” sound was played and there was no reward. Animals kept still during the fixation period (<xref ref-type="fig" rid="F1">Figure 1F&amp;G</xref>). A trial was considered incomplete if the animal did not poke into any port after the start port within 15 seconds. Unless otherwise specified, incomplete trials and fixation violations were not included in analyses. Rats performed 318.89 ± 7.76 (mean ± s.e.) trials in each 1.5 hour recording session. As expected from a visually-guided task, performance was good (% Correct = 94.05% ± 0.53%, mean ± s.e., n=104 sessions; <xref ref-type="supplementary-material" rid="SD1">Figure S2A</xref>).</p><sec id="S3"><title>FOF neurons were tuned to self-centered movement directions and world-centered head positions</title><p id="P8">We recorded 1224 single neurons in the FOF from 104 sessions in 4 rats. Consistent with previous findings, there were neurons tuned to the upcoming and ongoing movement (<xref ref-type="fig" rid="F2">Figure 2</xref>). To quantify the relative strength of tuning to the start position, direction or target position in single neurons, we fit spike counts in 500 ms time windows to three Poisson generalized linear models (GLMs), whose independent variables were the start position, movement direction or target position, respectively. The spatial variables were coded as factors to avoid assuming any specific functional form of tuning. As we expected, there were neurons more strongly tuned to the self-centered movement direction than the world-centered target position, during the planning phase (<xref ref-type="fig" rid="F2">Figure 2A-C</xref>) or the execution phase (<xref ref-type="fig" rid="F2">Figure 2D-F,G-I</xref>). Interestingly, there were also neurons more strongly tuned to the world-centered target position than self-centered movement directions upon target arrival (<xref ref-type="fig" rid="F2">Figure 2J-L,M-O,P-R</xref>). When firing rates were conditioned on both direction and target position, many neurons seemed to code the conjunction of position and direction (<xref ref-type="fig" rid="F2">Figure 2C,F,I,O,R</xref>). We will address conjunctive coding in later sections (<xref ref-type="fig" rid="F6">Figure 6</xref>); here we will examine another question: which spatial variable do FOF neurons most strongly encode at each phase of the trial?</p></sec><sec id="S4"><title>FOF neurons encoded self-centered movement plans prior to world-centered target positions</title><p id="P9">From visual examination of neural activity, there was considerable heterogeneity in task variable tuning as well as the temporal dynamics of tuning across the trial. To get a holistic view of the temporal dynamics of spatial tuning in single neurons, we fit the spike counts of each neuron in four 300 ms time windows to the three Poisson GLMs. The time windows were: ‘pre-cue’, -300 ms to 0 ms aligned to the visual cue onset; ‘post-cue’, 0 ms to 300 ms aligned to the visual cue onset; ‘go’, 0 ms to 300 ms aligned to the go sound; ‘arrival’, -150 ms to 150 ms aligned to the target arrival. Of the 1224 neurons, 541 (44%) were selective to one or more task variables (start position, direction or target position) in at least one time window. We estimated which of the three task variables <italic>best</italic> explained the neural activity in each time window. Most neurons were best tuned to the start position in the pre-cue time window. After the visual cue onset, direction tuning increased, whereas target position tuning emerged even later than movement direction and peaked at the arrival time window (<xref ref-type="fig" rid="F3">Figure 3A</xref>). Thus, the FOF encodes the egocentric movement direction before the allocentric target position, even though the appearance of a visual target cue provided information about the movement direction <italic>at the same time as</italic> the target position. We then extended the analysis by sliding time windows aligned to the visual cue, go sound or target arrival, and the same temporal trend was captured by the <italic>R</italic><sup>2</sup>s of the corresponding GLMs across time (<xref ref-type="supplementary-material" rid="SD1">Figure S7A</xref>).</p><p id="P10">In our task, the three task variables were correlated. For example, if the animal started from a port on the left, the movement direction and target position would be likely on the right. As such, a neuron tuned to movement direction could be spuriously found to be tuned for start or target position due to this correlation. To address this potential confound and validate the effectiveness of our method, we generated surrogate spike counts (matched to the tuning and firing rates of real neurons) and found that the errors in classification (e.g. incorrectly labeling a ‘start’ neuron as a ‘direction’ or ‘target’ neuron) to be less than 10% (<xref ref-type="supplementary-material" rid="SD1">Figure S7C</xref>).</p><p id="P11">We then used pseudopopulation decoding to examine the geometry of spatial representation on a continuous scale. We pooled all the neurons across sessions where there were at least 8 trials for each start position, direction or target position to construct the pseudopopulation (1197 neurons, 99 sessions, 3 rats). From visual examination, the first 4 principle components of the pseudopopulation activity represented the horizontal and vertical spatial coordinates of the task variables (<xref ref-type="supplementary-material" rid="SD1">Figure S8A-F</xref>). To prevent a few neurons in the population from dominating the decoding, we randomly resampled the neurons with replacement to construct 100 pseudopulations. We decoded the x and y coordinates of each task variable using multivariate linear decoders with two-fold cross validation, from the first 4 principle components of the pseudopopulation activity in 300 ms time windows (<xref ref-type="supplementary-material" rid="SD1">Figure S8G</xref>). The errors, defined as the Euclidean distance between the predicted and the actual spatial coordinates, were as small as the radius of the port (around 11 mm) at the best time window for each spatial variable (<xref ref-type="fig" rid="F3">Figure 3B&amp;C</xref>). Put in other words, the geometry of both the movement directions and the port positions were embedded in a linear subspace of the FOF activity.</p><p id="P12">Pseudopopulation decoding accuracy also demonstrated the sequential encoding of start position, movement direction and target position. The relative goodness of decoding between two spatial variables was quantified as the difference between the two mean decoding errors (<xref ref-type="fig" rid="F3">Figure 3D</xref>). Start position was decoded better than movement direction or target position in the early phase of a trial. At the time of the go cue, direction decoding was significantly better than target decoding. When decoding pseudopopulation activity at one time window with decoders trained at a different time window (<xref ref-type="fig" rid="F3">Figure 3E</xref>), start position tuning was stable across much of the trial. Decoders trained with start positions during fixation could accurately decode target positions around target poke, suggesting a consistent coding for the “current” head position throughout the trial (<xref ref-type="fig" rid="F3">Figure 3E</xref>).</p></sec><sec id="S5"><title>Single FOF neurons tracked the allocentric current head position</title><p id="P13">We reasoned that the current position coding in the pseudopopulation could be due to single neurons that had consistent tuning for the current head position (<xref ref-type="fig" rid="F4">Figure 4A-B</xref>). We quantified this consistency using the Pearson correlation between start position tuning in the “pre-cue” window and target position tuning in the “arrival” window, and denoted this as the “start-target tuning correlation” (<xref ref-type="fig" rid="F4">Figure 4C</xref>). Among neurons selective to both the start and the target position (<italic>p</italic> &lt; 0.05 for both the start and the target GLMs, n = 174), the mean start-target tuning correlation was significantly positive (0.66, [0.61,0.70], mean, [95% CI], <italic>p</italic> = 2 × 10<sup>−4</sup>, permutation test with 10000 replicates) (<xref ref-type="fig" rid="F4">Figure 4D</xref>).</p><p id="P14">To investigate the temporal dynamics of the correlation across the trial, we computed the start-target tuning correlation between two halves of trials of the same neuron between one time window and another. Among neurons selective to both the start and the target position (p &lt; 0.05 for both the start and the target GLMs, n = 174), the mean correlation between start position tuning early in the trial and target position tuning around target poke were positive (<xref ref-type="fig" rid="F4">Figure 4E</xref>). For comparison, in the same group of neurons, the peak of the mean start-target tuning correlation among these neurons was 0.555, on the same scale as the target-target tuning correlation, which was 0.579, suggesting highly consistent start and target position tuning (<xref ref-type="fig" rid="F4">Figure 4F&amp;G</xref>).</p><p id="P15">The consistency of start position and target position tuning were not limited to strongly tuned neurons. Among all the neurons with spatial selectivity (p &lt; 0.05 for any one of the GLMs, n = 808 ), the start tuning correlation, the target tuning correlation, and the start-target tuning correlation were all positively correlated (<xref ref-type="supplementary-material" rid="SD1">Figure S9</xref>). In other words, neurons tuned to the start position were more likely tuned to the target position, and the start and target tunings were more likely to be consistent. Collectively, these evidence suggested that single FOF neurons tracked the current head position.</p><p id="P16">One might notice the correlation between start position tuning late in the trial and target position tuning early in the trial (<xref ref-type="fig" rid="F4">Figure 4E</xref>). This was due to the correlation between start and target positions in the task, which led to the weak ‘mirroring’ of the strong early-start to late-target correlation.</p><p id="P17">Start position encoding transitioned to target position coding mainly during the movement period (<xref ref-type="fig" rid="F4">Figure 4H</xref>). We fit the neural spike counts across time to the start and the target GLMs, and defined the time of transition as the time when the <italic>R</italic><sup>2</sup> of the start position GLM first became smaller than the target position GLM (see Methods for details). For most of these neurons, the switch time was between the go sound and the target poke arrival time.</p><p id="P18">The numbers of neurons preferring each start position and target position spanned across all the ports (<xref ref-type="fig" rid="F4">Figure 4I-J</xref>). The preferred positions were not uniformly distributed. There were more neurons with preferred start positions (χ<sup>2</sup>(6, <italic>N</italic> = 202) = 17.35, p = 0.009) and target positions (χ<sup>2</sup>(6, <italic>N</italic> = 209) = 48.39, p = 9.86 × 10<sup>−9</sup>) at the most leftward and rightward ports. Consistent with the current head position coding, the distribution of the preferred start position among start position selective neurons were similar to the distribution of preferred target position among target position selective neurons (χ<sup>2</sup>(6, <italic>N</italic> = 411) = 9.67, <italic>p</italic> = 0.139, <xref ref-type="fig" rid="F4">Figure 4I&amp;J</xref>).</p></sec><sec id="S6"><title>FOF neurons represented movement directions asymmetrically</title><p id="P19">Despite our ability to decode both the vertical and horizontal direction during planning (<xref ref-type="fig" rid="F3">Figure 3B</xref>), single neurons in the FOF were not tuned equally to all movement directions (<xref ref-type="fig" rid="F5">Figure 5A&amp;B</xref>). First, there were significantly more neurons that preferred horizontal directions than those who preferred vertical directions (χ<sup>2</sup>(1, <italic>N</italic> = 274) = 76.91, <italic>p</italic> = 0 for the go window). Second, there were significantly more neurons who preferred downward directions than those who preferred upward directions (χ<sup>2</sup>(1, <italic>N</italic> = 274) = 5.37, <italic>p</italic> = 0.02 for the go window). Interestingly, rats mostly made errors into the same left/right side as instructed (<xref ref-type="supplementary-material" rid="SD1">Figure S2B</xref>) and to a lower direction (<xref ref-type="supplementary-material" rid="SD1">Figure S2C</xref>). For example, when instructed to top-left, errors were usually to the middle-left or bottom-left ports but rarely to the right. This type of behavioral asymmetry was consistently observed in other rats trained on similar tasks (<xref ref-type="supplementary-material" rid="SD1">Figure S3</xref>), and similar spatial asymmetries have been reported in human eye movements (<xref ref-type="bibr" rid="R16">Collewijn et al., 1988</xref>, <xref ref-type="bibr" rid="R17">Collewijn and Tamminga, 1984</xref>, <xref ref-type="bibr" rid="R31">Ke et al., 2013</xref>). Consistent with previous findings, there was no significant difference between the numbers of neurons preferring the ipsilateral and the contralateral side (χ<sup>2</sup>(1, <italic>N</italic> = 274) = 0.88, <italic>p</italic> = 0.34, <xref ref-type="bibr" rid="R22">Erlich et al., 2011</xref>).</p></sec><sec id="S7"><title>Mixed selectivity of movement direction and head position</title><p id="P20">In previous analyses, we examined the encoding and decoding of one spatial variable at a time, although single FOF neurons seemed to encode the conjunction of multiple spatial variables by visual inspection (<xref ref-type="fig" rid="F2">Figure 2</xref>). Nonlinear mixed selectivity supports flexible readout by allowing high-dimensional representation of information from multiple sources (<xref ref-type="bibr" rid="R51">Rigotti et al., 2013</xref>). For example, a nonlinear transformation is required to compute distance <inline-formula><mml:math id="M1"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msqrt><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> from neurons that encode the x and y position.</p><p id="P21">Gain field models are a common function form of nonlinear mixed coding of spatial information. They are so named since the information of one source multiplicatively modulates the information of another source (<xref ref-type="bibr" rid="R2">Andersen et al., 1990</xref>, <xref ref-type="bibr" rid="R3">1985</xref>, <xref ref-type="bibr" rid="R4">Andersen and Mountcastle, 1983</xref>, <xref ref-type="bibr" rid="R54">Salinas and Sejnowski, 2001</xref>). For example, the primate frontal eye field encodes a gain field mixture of the initial eye-in-orbit position and the saccade vector (<xref ref-type="bibr" rid="R11">Cassanello and Ferrera, 2010</xref>, <xref ref-type="bibr" rid="R12">2007</xref>). Following this, we hypothesized that FOF activity might be described as a gain-field mixture of the current position and the upcoming movement direction. We tested the hypothesis by comparing four encoding models: a pure direction tuning model (<xref ref-type="disp-formula" rid="FD5">Eq. 1</xref>, see Methods), a pure position tuning model (<xref ref-type="disp-formula" rid="FD6">Eq. 2</xref>), an additive mixed tuning model (<xref ref-type="disp-formula" rid="FD8">Eq. 4</xref>), and a gain-field mixed tuning model (<xref ref-type="disp-formula" rid="FD7">Eq. 3</xref>).</p><p id="P22">For each neuron, we fit spike counts in the 0 to 500 ms time window aligned to the visual cue onset to the four function forms (<xref ref-type="fig" rid="F6">Figure 6A-C</xref>), and quantified the goodness of fit with the cross-validated <italic>R</italic><sup>2</sup> (<italic>CV</italic> <italic>R</italic><sup>2</sup>). The vast majority of neurons were categorized as best fit by the gain field model, and the fraction of neurons best fit by a gain field model grew when the comparison was restricted to more spatially selective neurons (<xref ref-type="fig" rid="F6">Figure 6D</xref>). Among neurons whose average CV <italic>R</italic><sup>2</sup>s for the 4 models were larger than 0.05, the mean CV <italic>R</italic><sup>2</sup> for the gain field model was significantly larger than all the other models (<xref ref-type="fig" rid="F6">Figure 6E</xref>, <xref ref-type="table" rid="T1">Table 1</xref>). The result was qualitatively the same for spike counts in the -300 to 500 ms window aligned to the go sound, as well as in the 500 ms window with maximum cross-trial-type variance aligned to the visual cue onset, demonstrating consistency across time. Thus, the majority of FOF neurons had nonlinear mixed selectivity to the self-centered and the world-centered spatial variables, consistent with findings in the primate frontal and parietal cortices during motor planning (<xref ref-type="bibr" rid="R2">Andersen et al., 1990</xref>, <xref ref-type="bibr" rid="R3">1985</xref>, <xref ref-type="bibr" rid="R4">Andersen and Mountcastle, 1983</xref>, <xref ref-type="bibr" rid="R54">Salinas and Sejnowski, 2001</xref>).</p></sec><sec id="S8"><title>FOF population activity is most similar to a recurrent network with self-centered input and self-centered output</title><p id="P23">Gain fields have been suggested as a powerful computational mechanism for reference frame transformation, as it allows a downstream population to read out an arbitrary combination of the spatial information from different sources (<xref ref-type="bibr" rid="R46">Pouget and Sejnowski, 1997</xref>, <xref ref-type="bibr" rid="R53">Salinas and Abbott, 1995</xref>). That said, our current data doesn’t provide strong evidence whether the FOF is involved in the reference frame transformation or alternatively inherits representations from upstream regions. We turned to a computational modeling approach to gain insight into this question. We analyzed the activity of four types of networks (n=20 per class) trained on a similar task that as the visually-guided delayed-orienting task but with different input and output reference frame configurations (<xref ref-type="fig" rid="F7">Figure 7A</xref>,<xref ref-type="supplementary-material" rid="SD1">S11</xref>): self-centered input and self-centered output (ego→ego); self-centered input and world-centered output (ego→allo); world-centered input and self-centered output (allo→ego); world-centered input and world-centered output (allo→allo). Thus, two of the network types needed to perform a reference frame transformation (ego→allo, allo→ego) and two of them did not (ego→ego, allo→allo).</p><p id="P24">We decoded the start position, target position, and movement direction (the vector defined as target - start position) from the hidden unit activity during test trials in the same way as in real neurons, and found the ego-ego networks had the most similar temporal pattern of decoding accuracy as real neurons (<xref ref-type="fig" rid="F7">Figure 7A</xref>). Representational similarity analysis (RSA) also showed the highest resemblance between real neurons and the hidden units of the ego-ego model (<xref ref-type="fig" rid="F7">Figure 7B-D</xref>). RSA is a powerful technique to compare activity in networks (real or artificial) with different architecture, as long as the same stimuli can be presented to each network (<xref ref-type="bibr" rid="R33">Kriegeskorte, 2008</xref>). The idea behind RSA is to first estimate correlation between stimulus evoked activity patterns in each network and then to compare the patterns of correlations across the two networks. There was a significant effect of model on the representational similarity between the model and the real neural population (<italic>F</italic>(3, 236) = 41.6, <italic>p</italic> = 1.27 × 10<sup>−21</sup>, one-way ANOVA). The ego-ego model activity was significantly more similar to real neural population than all the other models in the post-cue window and the go window (<xref ref-type="table" rid="T2">Table 2</xref>, Welch’s t-test).</p><p id="P25">Interestingly, all four models showed gain-field-like activity in some of the hidden units (<xref ref-type="fig" rid="F7">Figure 7E</xref>), indicating that gain-field modulation is not uniquely associated with reference frame transformation within the network. These results suggested that the FOF does not transform preparatory activity from one reference frame to another, despite the observation of allocentric spatial encoding during the movement period.</p></sec></sec><sec id="S9" sec-type="discussion"><title>Discussion</title><p id="P26">Motor planning in rodents has previously been studied with tasks where the action space is either very high dimensional (such as navigating a maze or an open field) or very low dimension (such as 2AFC or go-nogo tasks). We took an intermediate approach with an orienting task that involved 6 movement directions and 7 head positions. We observed encoding of both egocentric and allocentric spatial parameters at the single-neuron and population level in the FOF. At the population level, two distinct 2D maps could be decoded from the neural activity in the FOF: a 2D map of current position on the poke wall and a 2D map of the future movement vector. The encoding of allocentric start position, egocentric movement direction and allocentric target position emerged sequentially over the trial. Despite the presence of allocentric information in the FOF, preparatory activity was in the egocentric reference frame.</p><p id="P27">This work presents a substantial advance in the conception of the function of the FOF. First, we established that preparatory activity in the FOF is encoded in a self-centered (egocentric) reference frame, consistent with the consensus view of preparatory activity in the primate frontal eye field (FEF; <xref ref-type="bibr" rid="R55">Schall, 2009</xref>). This preparatory activity is relatively abstract compared to the specific muscle sequence required for a movement: the upcoming direction of movement could be decoded ignoring the start position (<xref ref-type="fig" rid="F3">Figure 3B</xref>). This finding is again consistent with findings from the FEF: auditory and visually guided saccades produce similar activity in the FEF even though they result in different eye/head kinematics (<xref ref-type="bibr" rid="R52">Russo and Bruce, 1994</xref>). Thus, although the preparatory activity is self-centered, neither the activity in FEF nor FOF are likely to represent a motor command, but instead represents an abstract command to shift attention to a part of space. Alternatively, it might not represent a command at all, but represent a utility or priority map which is used to generate movement commands in a downstream region, like the superior colliculus.</p><p id="P28">Our result builds on a recent interesting finding of world-centered spatial context modulating self-centered movement planning in rat M2 during navigation (overlapping anatomically with the region we define as FOF; <xref ref-type="bibr" rid="R43">Olson et al., 2020</xref>). The authors deduced that the M2 integrates spatial information toward the updating of planned movements. However, their animals were not explicitly cued on each trial about the goal location or direction while navigating along a triple T-maze, which precluded them from demonstrating the temporal pattern of spatial encoding of different reference frames. With a good experimental control over planning versus execution phases in our task, we demonstrated that planning is done preferentially in egocentric coordinates. Moreover, our comparison between the dynamics of the FOF neurons and the artificial neural networks using RSA casts doubt on the idea that the FOF transforms spatial information from one reference frame to another (<xref ref-type="fig" rid="F3">Figure 3</xref>,<xref ref-type="fig" rid="F7">7</xref>). A second difference is that we use a vertically oriented port wall instead of maze, which allowed us to demonstrate that the FOF contains two 2-dimensional maps (azimuth and elevation): one for position and one for direction (<xref ref-type="fig" rid="F3">Figure 3B</xref>).</p><p id="P29">Although, it may appear that the FOF encodes the current position in a place-cell like fashion, there are ‘allocentric’ features of the box that could lead to this kind of tuning. For example, the right mystacial whiskers might touch the box edge when animals are in the right-most port or there could be visual cues that change (distance to the wall or box edges) depending on the position of the subject in the ports. That said, most of the position-tuned neurons had spatially smoothed tuning. For example, firing highest for the middle left port, but responding moderately to top left or bottom left (<xref ref-type="fig" rid="F2">Figure 2K</xref>). While it is not impossible for sensory responses to have this spatially smooth tuning (e.g. distance from an odour cue), it seems parsimonious to describe these as allocentric position tuned responses. Further experiments are required to better understand the nature of the allocentric tuning observed.</p><p id="P30">Subjects made few errors in the task, but when they did, they mostly made up/down errors, and three of the four rats made more errors downwards than upwards (<xref ref-type="supplementary-material" rid="SD1">Figure S2B&amp;C, S3</xref>). At the neural level, there was an over-representation of left versus right than up versus down direction coding in the FOF, and an over-representation of downwards versus upwards directions (<xref ref-type="fig" rid="F5">Figure 5</xref>). Although we do not have evidence for a causal relationship between the behavioral and neural observations, one might speculate that the left/right over-representation in the FOF might be behaviorally relevant (<xref ref-type="bibr" rid="R30">Jovalekic et al., 2011</xref>). In contrast to our findings, there were more pitch (up/down) tuned neurons than azimuth (left/right) tuned neurons reported in the M2 in rats foraging in a large arena (<xref ref-type="bibr" rid="R39">Mimica et al., 2018</xref>). There were two key differences between our experiment and theirs: first, they recorded from a larger anterior-posterior range of M2; second, our rats were under tight experimental control whereas their rats were foraging freely. Also, in our task, all visually-guided movements had a horizontal component which might have biased our results. It would be interesting to record the same neural populations in two tasks: one like ours and one like theirs to see if neurons in M2 can dynamically shift their tuning based on task demands.</p><p id="P31">The directional asymmetries in neural encoding and behavior suggest that a radially symmetric bump attractor model is insufficient as a computation model for the task (<xref ref-type="bibr" rid="R64">Wimmer et al., 2014</xref>). Plausible models for movement direction planning might be multiple discrete attractor models or ring attractor models that are asymmetric in connectivity weights: they might have stronger connectivity from upper direction preferring neurons to lower direction preferring neurons than the reverse, and have stronger connectivity between neurons that prefer the same left/right side than those preferring different left/right sides.</p><p id="P32">We found that allocentric and egocentric information is multiplexed as gain fields at the level of individual neurons. Gain modulation is a nonlinear process in which neurons combine information from two or more sources. Gain fields have been observed in a plethora of primate cortical and subcortical brain regions (for review, see <xref ref-type="bibr" rid="R54">Salinas and Sejnowski, 2001</xref>), including the primate FEF (<xref ref-type="bibr" rid="R11">Cassanello and Ferrera, 2010</xref>, <xref ref-type="bibr" rid="R12">2007</xref>). While conjuctive coding of allo- and ego-centric information has been found in rodent para- and post-subiculum and the medial entorhinal cortices (<xref ref-type="bibr" rid="R24">Gofman et al., 2019</xref>, <xref ref-type="bibr" rid="R44">Peyrache et al.,2017</xref>) they were not explicitly modeled as gain fields even though theoretical work suggests that they might take that form (<xref ref-type="bibr" rid="R7">Bicanski and Burgess, 2020</xref>). According to the theory of gain field modulation, a neural population downstream of a population with gain field encoding could easily compute an arbitrary combination of the two spatial variables (<xref ref-type="bibr" rid="R1">Alexander et al., 2023</xref>, <xref ref-type="bibr" rid="R46">Pouget and Sejnowski, 1997</xref>, <xref ref-type="bibr" rid="R53">Salinas and Abbott, 1995</xref>, <xref ref-type="bibr" rid="R69">Zipser and Andersen, 1988</xref>). However, our comparison of recurrent networks showed that networks that do not perform reference frame transformation could also have gain fields (<xref ref-type="fig" rid="F7">Figure 7</xref>).</p><p id="P33">Our task was inspired by paradigms widely used in non-human primates to study the neural mechanism of saccadic eye movements (<xref ref-type="bibr" rid="R8">Bruce and Goldberg, 1985</xref>) and rodent head orienting is, like a saccade a shift in overt attention, in that it redirects the sensory fields of vision, audition, olfaction and whiskers (<xref ref-type="bibr" rid="R9">Bush et al., 2016</xref>, <xref ref-type="bibr" rid="R38">McCluskey and Cullen, 2007</xref>, <xref ref-type="bibr" rid="R40">Monteon et al., 2010</xref>). Based on similarities in connectivity and function, the rodent FOF is a proposed functional analogue to the primate FEF (<xref ref-type="bibr" rid="R21">Ebbesen et al., 2018</xref>, <xref ref-type="bibr" rid="R22">Erlich et al., 2011</xref>, <xref ref-type="bibr" rid="R48">Reep et al., 1987</xref>, <xref ref-type="bibr" rid="R49">1990</xref>), although a strict correspondence between rodent and primate frontal regions may not exist (<xref ref-type="bibr" rid="R5">Barthas and Kwan, 2017</xref>, <xref ref-type="bibr" rid="R65">Wise, 2008</xref>). Our findings of gain field modulation of movement direction by initial head position support the analogy, as this is similar to the observation that eye-in-orbit position gain modulates the planned saccade vector in FEF (<xref ref-type="bibr" rid="R11">Cassanello and Ferrera, 2010</xref>, <xref ref-type="bibr" rid="R12">2007</xref>). One may argue that for the FEF both the initial gaze direction (or eye-in-orbit position) and the saccade vector are egocentric, and most of the reports about spatial encoding in the FEF was egocentric (but see <xref ref-type="bibr" rid="R6">Bharmauria et al., 2020</xref>). However, body positions in primate experiments are typically restricted by a primate chair. Recent work in freely moving monkeys found widespread world-centered coding across frontal and prefrontal cortex (<xref ref-type="bibr" rid="R36">Maisson et al., 2022</xref>), including supplementary motor area and dorsolateral prefrontal cortex. Although FEF was not one of the regions they recorded from, the finding suggests that allocentric representations are common across the brain, when animals’ movements are not restricted. Thus, we might speculate that FEF saccade planning activity in monkeys freely moving in an environment could be modulated by the current position of subject in the environment.</p><p id="P34">The observations of gain fields in the FOF, as well as the sequential encoding of movement direction and target position, were largely consistent with observations in primate motor planning more generally (<xref ref-type="bibr" rid="R3">Andersen et al., 1985</xref>, <xref ref-type="bibr" rid="R6">Bharmauria et al., 2020</xref>, <xref ref-type="bibr" rid="R10">Caruso et al., 2018</xref>, <xref ref-type="bibr" rid="R54">Salinas and Sejnowski, 2001</xref>,<xref ref-type="bibr" rid="R62">Wang et al., 2007</xref>, <xref ref-type="bibr" rid="R69">Zipser and Andersen, 1988</xref>). However, despite decades of research, a full understanding of phenomena like gain fields how they could causally contribute to reference frame shifts has been hampered by a lack of tools for precise perturbations for circuit dissection in primates (as discussed in <xref ref-type="bibr" rid="R56">Shenoy et al., 2013</xref>). In contrast, the neural circuits underlying basic phenomena of motor planning (i.e. spatial memory and movement initiation in 2AFC tasks), have been revealed in rodents models (<xref ref-type="bibr" rid="R6">Bharmauria et al., 2020</xref>, <xref ref-type="bibr" rid="R20">Duan et al., 2021</xref>, <xref ref-type="bibr" rid="R26">Guo et al., 2017</xref>, <xref ref-type="bibr" rid="R29">Inagaki et al., 2019</xref>, <xref ref-type="bibr" rid="R32">Kopec et al., 2015</xref>, <xref ref-type="bibr" rid="R35">Li et al., 2016</xref>, <xref ref-type="bibr" rid="R67">Yang, 2022</xref>). Our behavioral paradigm and neurophysiological observations provide a basis for employing the latest tools of rodent systems neuroscience to understand complex and ethological motor planning.</p></sec><sec id="S10" sec-type="materials | methods"><title>Materials and Methods</title><sec id="S11" sec-type="subjects"><title>Subjects</title><p id="P35">Three adult male Sprague-Dawley rats and one adult male Brown Norway rat (Vital River, Beijing, China) was used in this study. For a portion of the experiments presented here, rats were placed on a controlled-water schedule and had access to free water 20 minutes each day in addition to the water they earned in the task. For some experiments, rats were given free access to a 4% citric acid solution (<xref ref-type="bibr" rid="R50">Reinagel, 2018</xref>), in addition to the normal water they earned in the task. They were kept on a reversed 12 hour light–dark cycle and were trained during their dark cycle. Animal use procedures were approved by New York University Shanghai International Animal Care and Use Committee following both US and Chinese regulations.</p></sec><sec id="S12"><title>Behavior</title><p id="P36">Rats were trained in custom behavioral chambers, located inside sound- and light-attenuated boxes. Each chamber (23 × 23 × 23 cm) was fit with a vertical 2-D port wall that had 7 operant ports and 1 reward delivery port, with speakers located on the left and right side (<xref ref-type="fig" rid="F1">Figure 1</xref>). Each operant port contained a pair of blue and a pair of yellow light emitting diodes (LED) as visual cues, as well as an infrared LEDs and photo-transistors for detecting rats’ interactions with the ports. The reward delivery port contained a stainless steel tube for delivering water rewards.</p><p id="P37">The task timeline is described in detail in Results and <xref ref-type="fig" rid="F1">Figure 1</xref>. In one rat (2147), in addition to the main type of timeline, which we denote as “target during fixation”, there were two other types of trial timelines: “target before fixation” and “target after go”. These trial types are described in detail in <xref ref-type="supplementary-material" rid="SD1">Figure S5</xref>.</p><p id="P38">The duration of the fixation period was dynamically adjusted for each animal, ranging between 0 ~ 1.2 s (<xref ref-type="supplementary-material" rid="SD1">Figure S2</xref>). A trial was considered a fixation violation if the rat withdrew from the start port before the go sound. In fixation violation trials, an “error” sound was delivered and the trial was aborted.</p><p id="P39">In the final behavioral stage, three rats (subject ID 2068, 2095, 2134) moved in 6 directions and 30 movement trajectories, and one rat (subject ID 2147) moved in 4 directions and 16 movement trajectories (<xref ref-type="supplementary-material" rid="SD1">Figure S1</xref>).</p><p id="P40">In rat 2068, 2095 and 2134, there were two session types interleaved across days: the “reference” sessions and the “distance” sessions. In the “reference‘’ sessions, each of the 6 directions had 4 movement trajectories of the same distance. In the “distance” sessions, each of the 6 directions had 3 movement trajectories involving 3 ports, where the distance of one movement trajectory is twice of the movement of the other two(<xref ref-type="supplementary-material" rid="SD1">Figure S1</xref>).</p><p id="P41">In rat 2147, there were 4 movement directions, from and to 7 operant ports. Each direction had 4 trajectories of the same distance, summing up to a total of 16 trajectories (<xref ref-type="supplementary-material" rid="SD1">Figure S1</xref>).</p></sec><sec id="S13"><title>Behavior training pipeline</title><p id="P42">Rats went through a series of training stages, which had mainly two phases: (1) the operant conditioning phase, and then (2) the multi-directional orienting phase.</p><p id="P43">In the operant conditioning phase, rats became familiar with the training apparatus and learned to do a one step poke into the illuminated choice port. The first stage was to learn to collect reward from the reward delivery port. Each trial began with the illumination of the reward port, and water reward was immediately delivered upon port entry, followed by a short timeout period before the start of next trial. After the rats learned to poke into the reward port reliably (not missing any reward for 6 trials in a row), they proceeded to the next training stage. In the second stage, we turned on the LED for several random ports at the beginning of each trial. Rats had to first poke into any illuminated choice port before gaining water reward from the reward delivery port. The number of the illuminated port will gradually decrease to one after several trials when animals started to learn. After animals were able to poke the only illuminated choice port successfully for 6 trials in a row, we will upgrade them to the second training phase.</p><p id="P44">In the first stage of the multi-directional orienting phase, the start port was always the central port, and the target port was one of the 6 surrounding ports. Rats needed to poke into the start port to trigger the target port light, and then poke into the target port after a delay. Trials of the same movement trajectory was repeated until the animal could do several correct trials in a row. The training of “fixation” at the start port was introduced in this phase. Fixation means the rat had to keep its nose in the start port for a given time period (typically &gt; 0.5s). Fixation duration was initially 0.02 s early in the training, and was gradually increased based on an adaptive steps method: the fixation duration would increase on a successful fixation, and decrease when the fixation failed. We trained subjects to perform fixation for at least 0.6 s before the surgery, and fixation duration always jittered across trials in recording sessions. However, the speed to recover fixation after the surgery varied across subjects, thus we manually adjusted the fixation duration for each subject. In 2095, the mean fixation period in each session was shorter than 0.2 s for around 30% of sessions. In other subjects, the mean fixation period was typically longer than 0.4 s (<xref ref-type="supplementary-material" rid="SD1">Figure S2 G</xref>). In the second stage of the training phase, the start port could be any one of the 7 ports, and the target port was one of the 6 remaining ports. Rats were trained on the “target during fixation” trial class. Rat 2147 was then introduced to the “target before fixation” and “target after go” trial classes, described in <xref ref-type="supplementary-material" rid="SD1">Figure S5</xref>.</p></sec><sec id="S14"><title>Electrophysiology</title><p id="P45">Rats were unilaterally implanted with Cambridge Neurotech microdrives and 64 channel silicon probes. To target the frontal orienting field (FOF), silicon probes were placed at anteroposterior (AP) and mediolateral (ML) coordinates as following: rat 2068, AP +2.2, ML +1.5; rat 2095, AP +2.0, ML +1.5; rat 2134, AP +2.5, ML -1.5; rat 2147, AP +2.2, ML -1.4 (<xref ref-type="supplementary-material" rid="SD1">Figure S4</xref>). The probes were placed initially 0.5 mm below the brain surface and were advanced 50 to 100 um every 1 to 4 days, after each recording session. The same neurons could be sampled across sessions.</p></sec><sec id="S15"><title>Analysis of neural data</title><sec id="S16"><title>Spike sorting</title><p id="P46">Spike sorting was performed with automatic clustering and manual curation in JRClust. Single units were defined as those with signal-to-noise ratio larger than 5, fraction of inter-spike interval smaller than 1 ms less than 1% , and within-trial firing rate larger than 1 Hz. However, our main results were robust to different single neuron criteria (<xref ref-type="supplementary-material" rid="SD1">Figure S6</xref>).</p></sec><sec id="S17"><title>Data inclusion</title><p id="P47">For a unit to be included in the main figures, the session must have at least 8 trials for each of the 7 start positions, 4 or 6 movement directions and 7 target positions. The sessions could be “reference”, “distance”, or sessions from rat 2147. As described above, there were 3 types of trial timeline in rat 2147, but only “target during fixation” trials are included in the main figures. This resulted in 104 sessions and 1224 single cells in 4 animals.</p><p id="P48">In pseudopopulation decoding, we included cells from sessions that had at least 8 trials for each of the 6 movement directions, 7 start positions, and 7 target positions. The sessions could be “reference” or “distance” sessions. This resulted in 1197 cells from 99 sessions, 3 animals.</p><p id="P49">Video tracking was available in 58 sessions in 3 rats.</p><p id="P50">For neuron inclusion criteria for all the main figures, see <xref ref-type="supplementary-material" rid="SD1">Table S1</xref>.</p><p id="P51">Unless otherwise specified, only correctly performed trials were included in the analysis.</p></sec><sec id="S18"><title>Key time windows</title><p id="P52">In single-neuron level analysis for single spatial variables, we focused on 4 key time windows: <italic>pre-cue,</italic> -300 to 0 ms aligned to target cue onset; <italic>post-cue,</italic> 0 to 300 ms aligned to target cue onset; <italic>go,</italic> 0 to 300 ms aligned to the go sound; <italic>arrival,</italic> -150 to 150 ms aligned to arrival at the target.</p><p id="P53">When comparing the pure and mixed selective models (<xref ref-type="fig" rid="F6">Figure 6</xref>), we used the 0 to 500 ms time window aligned to the visual cue onset in.</p><p id="P54">For continuous time windows, neural response was quantified by counting the number of spikes in sliding windows of 300 ms width and 50 ms step, aligned causally to task events, unless otherwise specified. Causal alignment means that the value at time 0 refers to the neural activity in a time bin between -300 ms and 0 ms. The responses could be aligned to the time of visual target cue onset, the go sound onset, the time of “fixation out” (the time when the nose left the start port, detected by the IR sensors) or target poke (the time when the nose arrived at the target port, detected by the IR sensors).</p></sec><sec id="S19"><title>Cross-validated <italic>R</italic><sup>2</sup>s and log likelihood</title><p id="P55">The cross-validated <italic>R</italic><sup>2</sup>s (denoted as CV <italic>R</italic><sup>2</sup>s in figures) was defined as <disp-formula id="FD1"><mml:math id="M2"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi/></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p id="P56"><inline-formula><mml:math id="M3"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> was the predicted mean spike count in the ith trial from the test set, and <italic>y<sub>i</sub></italic> was the observed spike count in this trial. In the best case, <italic>y</italic> is equal to <inline-formula><mml:math id="M4"><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula>, and the cross-validated <italic>R</italic><sup>2</sup> 1. In the worst case, <inline-formula><mml:math id="M5"><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula> is uncorrelated with <italic>y</italic>, and <inline-formula><mml:math id="M6"><mml:mrow><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mi>∑</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> can be larger than 1 by chance, thus <italic>CV R</italic><sup>2</sup>s can be negative.</p><p id="P57">The cross-validated log likelihood is defined as <disp-formula id="FD2"><mml:math id="M7"><mml:mrow><mml:mi>L</mml:mi><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="P58"><inline-formula><mml:math id="M8"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denote the probability of observing the spike count on the <italic>i</italic>th trial being <italic>y<sub>i</sub></italic>, given the predicted mean spike count being <inline-formula><mml:math id="M9"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. <italic>N</italic> denote the number of trials. We assumed spike counts to follow the Poisson distribution.</p></sec><sec id="S20"><title>PETHs</title><p id="P59">For each neuron, we combined the spike trains in correctly performed trials by the movement directions or target positions, and generated smoothed PETHs with an half-Gaussian kernel of 200 ms standard deviation. The kernel was causal, such that selectivity at time <italic>t</italic> was contributed by neural activity at or before time <italic>t</italic>.</p></sec><sec id="S21"><title>Generalized linear models for single neuron selectivity</title><p id="P60">We fit the neural spike counts in specific time windows to 3 generalized linear models (GLMs) with Poisson distributions: <disp-formula id="FD3"><mml:math id="M10"><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.2em"/><mml:mo>~</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.2em"/><mml:mo>~</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.2em"/><mml:mo>~</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p id="P61">The spatial variables were all included as dummy variables. Start position had 7 levels, direction had 6 levels, and target position had 7 levels.</p><p id="P62">In the main text, we labeled neurons as “having significant selectivity to a spatial variable”. This significance is derived from the permutation test of the GLM, where the test statistic was the leave-one-out cross-validated log-likelihood, against the null hypothesis that the log-likelihood was not significantly different from when trial labels were shuffled. For each spatial variable, the trial labels were shuffled 1000 times to obtain a distribution of goodness-of-fits, then the p value of the GLM was the fraction of log-likelihoods from shuffling that is greater than the log-likelihood from actual data.</p><p id="P63">The best selectivity of a neural response (<xref ref-type="fig" rid="F3">Figure 3A</xref>) was assigned to the spatial variable with the smallest p value of the GLM. When there were ties in <italic>p</italic> values, we additionally compared the leave-one-out cross-validated log-likelihoods, and the best selectivity was assigned to the spatial variable with the largest sum of log-likelihoods. The reason for possible ties is that we are using permutation tests with 5000 permutations.</p><p id="P64">In <xref ref-type="fig" rid="F3">Figure 3 B</xref>, the <italic>R</italic><sup>2</sup>s were derived from GLMs fit similarly, but without cross-validation. Only the model with the largest <italic>R</italic><sup>2</sup> was plotted with the corresponding color at each time point. Both the best GLM measurement and the <italic>R</italic><sup>2</sup> measurement for the relative strength of spatial selectivity were robust to the task-induced correlation between spatial variables, which was verified with surrogate data (<xref ref-type="supplementary-material" rid="SD1">Figure S7</xref>, also see Surrogate data paragraph below).</p><p id="P65">Cross-validated <italic>R</italic><sup>2</sup>s of the GLMs in <xref ref-type="fig" rid="F2">Figure 2</xref> were derived from 10-fold cross validation.</p></sec><sec id="S22"><title>Spatial preference of single neurons</title><p id="P66">The preferred direction of a neuron in a specific time window was defined as the direction of the weighted vector sum of the coordinates of the 6 movement directions, where the weight for each direction was the mean firing rate in that direction. Horizontal directions were defined as directions between −45° and 45° around the horizontal directions, and so was for vertical directions. Downward directions were directions that were −90° to 90° around the downward direction, and so was for upward directions. The preferred position of a neuron at a specific time window was simply denoted as the port with the highest mean firing rate.</p></sec><sec id="S23"><title>Start-target tuning correlation</title><p id="P67">The start-target tuning correlation in <xref ref-type="fig" rid="F4">Figure 4D</xref> was defined as the Pearson correlation between the tuning curve for start position in the “pre-cue” window and target position in the “arrival” window: <disp-formula id="FD4"><mml:math id="M11"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>σ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi/></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="P68"><italic>FR<sub>start</sub></italic> is a vector where each element is the mean firing rate for a specific start position, and <italic>FR<sub>target</sub></italic> is a vector where each element is the mean firing rate for a specific target position. Similarly, we calculated the start-target tuning correlations for different pairs of time windows and the tuning correlation between time <italic>t<sub>i</sub></italic> and <italic>t<sub>j</sub></italic> for the same variable. The time windows were warped to align to the time of the visual cue, the go sound and the target poke. The 300 ms “pre-cue” and “arrival” time windows were preserved. For these time windows, we calculated the tuning correlations between to halves of trials that were randomly split, that is, we used start tuning in the first half versus target tuning in the second half, and vice versa, and then took the average of the two correlation coefficients.</p><p id="P69">The tuning correlation was subject to the Fisher-z transformation, and then tested against the null hypothesis that the mean value across a specific neural population was not significantly larger than zero. The p value indicated the fraction of mean lying below zero among 10<sup>4</sup> bootstraps, and was corrected with Bonforroni correction.</p></sec><sec id="S24"><title>Timing of start-target switching</title><p id="P70">In <xref ref-type="fig" rid="F4">Figure 4</xref> I, spike counts in 300 ms time windows of 50 ms steps were fit to the Poisson GLM of either start or target position. The <italic>R</italic><sup>2</sup>s of each GLM across time was smoothed with a moving average kernel with the size of 3 bins, and then the start GLM <italic>R</italic><sup>2</sup> was subtracted by the target GLM <italic>R</italic><sup>2</sup>. The time of switching from start encoding to target encoding was defined as the first time window where <inline-formula><mml:math id="M12"><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>, after the positive peak of <inline-formula><mml:math id="M13"><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>.</p></sec><sec id="S25"><title>Mixed selectivity</title><p id="P71">To detect mixed selectivity for each neuron, we compared between 4 models of the neural firing rate.</p><p id="P72">In the Gaussian direction tuning model, the firing rate was a Gaussian function centered by the preferred direction, defined as in the “Spatial preference of single neurons” section in Methods. <disp-formula id="FD5"><label>(1)</label><mml:math id="M14"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>σ</mml:mi><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="P73">In the start position plane model, the firing rate was modulated linearly by the horizontal and vertical coordinates of the start position. <disp-formula id="FD6"><label>(2)</label><mml:math id="M15"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>y</mml:mi></mml:mrow></mml:math></disp-formula></p><p id="P74">In the gain field model, the firing rate was a multiplicative combination of a Gaussian tuning centered by the preferred direction and a start position modulation. <disp-formula id="FD7"><label>(3)</label><mml:math id="M16"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>σ</mml:mi><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="P75">In the additive model, the firing rate was an additive combination of a Gaussian tuning centered by the preferred direction and a start position modulation. <disp-formula id="FD8"><label>(4)</label><mml:math id="M17"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="P76">In these functions, <italic>f</italic> was the firing rate, <italic>θ</italic> was the movement direction relative to the preferred direction, and <italic>x</italic> and <italic>y</italic> are the horizontal and vertical coordinates of the start positions in the port wall. <italic>b</italic><sub>0,1,2,3</sub> and <italic>σ</italic> were fit for each model with maximum likelihood estimation assuming Poisson spike counts, using the Matlab <italic>fmincon</italic> function. The models were fit with 20-fold cross validation.</p><p id="P77">For each pair of models, we compared the cross-validated <italic>R</italic><sup>2</sup>s among neurons whose average of cross-validated <italic>R</italic><sup>2</sup>s of the 4 compared models was larger than 0.05. Significance level was tested against the null hypothesis that the mean difference between the Fisher-z transformed cross-validated <italic>R</italic><sup>2</sup>s in the x-axis model and the y-axis model was not significantly larger than zero, quantified as the fraction of the mean value smaller than zero among 10<sup>5</sup> bootstraps.</p><p id="P78">For a given movement direction, the target position coordinates were simply an additive translation of the start position, thus the start position and target position modulations were the same in the gain field model and the additive model.</p></sec><sec id="S26"><title>Pseudopopulation decoding</title><p id="P79">Pseudopopulations were constructed separately when decoding each spatial variable. For example, when decoding the start position, we constructed the pseudopopulation by resampling trials for each start position. For each neuron, trials were randomly split into 2 folds for each condition of the spatial variable, and 32 trials were resampled for each fold in each condition, yielding 224 trials per fold for position decoding and 192 trials per fold for direction decoding. These resampled trials were termed “pseudo-trials”. Pseudo-trials were resampled randomly with a different seed for each neuron, so as to remove trial-by-trial correlations between neurons in the same session.</p><p id="P80">We included single neurons from sessions that had at least 8 trials for each of the 7 start positions, 6 directions and 7 target positions (1197 neurons, 99 sessions, 3 rats). In each pseudopopulation, neurons were also resampled with a different seed, so that only about 63.2% of the neurons were included in each pseudopopulation. We generated 100 pseudopopulations for the decoding each spatial variable.</p><p id="P81">The error of decoding was defined as the Euclidean distance between true and predicted coordinates. The goodness of prediction was measured as the mean error between the predicted and the actual coordinates across all the pseudo-trials in that pseudo session. For start and target position, we removed the trials with [0 0] coordinates from the quantification of decoding accuracy. This is because predictions from unsuccessful decoding tend to cluster at the [0 0] coordinates, so decoding accuracy for the [0 0] coordinates in positions (i.e. the central port) will always seem “good”.</p><p id="P82">Decoding was performed with multivariate regression models (<italic>mvregress</italic> function in Matlab Statistics and Machine Learning Toolbox). The spike counts of the training set and the test set were first combined, z-scored, and applied to principle component analysis, and then split again for training and decoding. We found that the cross-validated decoding accuracy was the best when including only the first 4 PCs when decoding start position from neural activity in the “pre-cue” time window (<xref ref-type="supplementary-material" rid="SD1">Figure S8G</xref>), so we used 4 PCs for all subsequent pseudopopulation decoding.</p><p id="P83">In <xref ref-type="fig" rid="F3">Figure 3B</xref>, multivariate regression models were trained and tested with spike counts in 3 key time window as indicated in the plot (see “Key time windows” section for details). The plots showed the result from one example pseudo-session, the same pseudo-session as the first row in <xref ref-type="fig" rid="F3">Figure 3C</xref>. In <xref ref-type="fig" rid="F3">Figure 3C</xref>, the time windows were 300 ms width with 50 ms step, causally aligned to the go sound. Each row was a different pseudo-session. Each row of the heat map was a pseudopopulation with a different resampling of neurons, and the color indicated the mean error with two-fold cross-validation. In <xref ref-type="fig" rid="F3">Figure 3D</xref>, the mean error of one spatial variable was subtracted by the mean error of another spatial variable in each pseudopopulation, termed the Delta error, which represented the relative goodness of decoding between the two spatial variables. The distribution of Delta errors of the 100 pseudopopulations was then illustrated with the mean and standard deviation.</p><p id="P84">In <xref ref-type="fig" rid="F3">Figure 3E</xref>, models were trained at one time window and tested at another also with two-fold cross validation. In the last panel, a multivariate regression model was trained to decode start position coordinates from the start position pseudopopulation data, and then tested its decoding of target position coordinates from the target position pseudopopulation data. The heat-map showed the average of mean errors across 100 pseudopopulations. P values for multiple comparisons were obtained with extreme pixel-based permutation test (<xref ref-type="bibr" rid="R14">Cohen, 2014</xref>). One dummy heat-map was generated for each pseudopopulation by shuffling trial labels, and the maximum of each heat map was gathered to construct a null distribution. The white contours enclosed the area in which the averaged mean error was smaller than the minimum value of this distribution (<italic>p</italic> &lt; 0.01).</p></sec></sec><sec id="S27"><title>Video tracking</title><p id="P85">Videos were acquired at 30 frames per second with one Raspberry Pi Camera at the top of the rig.</p><p id="P86">We estimated the coordinates of the head, the ears and the hip of the rat in the video frames using DeepLabCut (<xref ref-type="bibr" rid="R37">Mathis et al., 2018</xref>). Coordinates were in the unit of pixels. The head position was approximated as the Intan chip plugged on the animal’s head. For continuous sampling of body coordinates aligned to task events, we linearly interpolated to achieve the sampling rate of 100 frames per second.</p></sec><sec id="S28"><title>RNN</title><p id="P87">We trained recurrent neural networks with 100 hidden units using pytorch (nn.RNN). The input to the networks (at each time) was a 100-by-100 pixels image downsampled to 40-by-40 pixels. The hidden layer was connected to a two node linear output layer, representing coordinates (x,y).</p><p id="P88">In the training set, the start port and target port coordinates were randomly generated inside a ”port wall” bounding box, which was also the world frame. The image itself was the visual frame. The input image, which was the visual frame, had a dark background with an inner frame denoting the world frame. The start port, target port, and the distractors were denoted as intensities. Specifically, the start port had an intensity of 0.7, the target port had an intensity of 1, and the distractor had an intensity of 0.2. These elements were plotted as pseudo-colors in the figures. In the test set, the start port and target port corresponded to the 24 trial types in the ”reference” sessions.</p><p id="P89">The networks were trained to take flattened image inputs over 11 time frames and to report the 2-D spatial coordinates at the last time frame (<xref ref-type="fig" rid="F7">Figure 7A</xref>,<xref ref-type="supplementary-material" rid="SD1">S11</xref>). We presented the start port on the first frame, which stayed on for all 11 frames, and the target port was presented only on frame 4. The nonlinearity used in the model was tanh, and stochastic gradient descent was employed during training.</p><sec id="S29"><title>Reference frames of the models</title><p id="P90">There were 4 task configurations that differed in the reference frame of inputs and outputs, namely: self-centered input, self-centered output (ego-ego); self-centered input, world-centered output (ego-allo); world-centered input, self-centered output (allo-ego); world-centered input, world-centered output (allo-allo). For the ego-input networks, the start port was always centered to the image (i.e. the visual frame), thus the target coordinates were the same as the movement vector. For the allo-input networks, the world frame was always central to the image, thus the world-centered target port position is directly readable from its position in the visual frame.</p></sec><sec id="S30"><title>RSA on RNN data</title><p id="P91">To examine the representation similarity between the hidden units in the RNN and the real neurons, we focused on the hidden unit activity at frame 3, 7 and 11. These frames corresponded to real neural activity during the pre-cue, post-cue and go time windows in the task, respectively.</p><p id="P92">We first computed the first 4 principle components of the hidden unit activity and real neuron activity separately, under the 24 trial types. We then constructed the representation dissimilarity matrix (RDM) for the two datasets by calculating the correlation coefficients for each pair of trial configurations. Finally, we computed the Pearson correlation coefficient for the linearized lower-triangular matrix for the two RDMs, as the measurement for the similarity between neural representation of the RNN and the real neurons.</p></sec></sec><sec id="S31"><title>Surrogate data</title><p id="P93">To verify the reliability of our model comparison approaches, we performed the same analysis on surrogate data which were known to have specific spatial tuning as on real neurons. These surrogate spike counts were generated to have specific spatial tuning, to match the overall firing rate of a real neuron, and as if sampled from a real session.</p><p id="P94">To generate surrogate data matching one neuron but tuned to a specific model, we first fit the spike counts of the real neuron to that model, so as to obtain the predicted firing rate at each trial condition. We then randomly selected a real session and used a Poisson random process to generate spike counts that followed these predicted firing rates. In the surrogate data for GLMs, we only used neurons that have significant selectivity to that GLM to generate surrogate data. In surrogate data to compare between pure and mixed selectivity models, we used all neurons to generate surrogate data.</p><p id="P95">Following stages of analyses was identical for surrogate data and real data. If our methods were reliable, the surrogate data designed to have a specific functional form would be best fit by the same functional form.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplemental Figures</label><media xlink:href="EMS157131-supplement-Supplemental_Figures.pdf" mimetype="application" mime-subtype="pdf" id="d32aAdEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S32"><title>Acknowledgements</title><p>The authors thank Yingkun Li, Nengneng Gao and Anyu Fang for their assistance with behavioral training and animal care. We thank Xuan Wen, Cequn Wang, Yidi Chen, Josh Moller-Mara, and Sylvain Dubroqua for their help with developing hardware and software for the training facility. LLJL and JL were supported by the NYU Shanghai Doctoral Fellowship, Winston Foundation Fund, and BOCO Fund for Science and Research. The research was also supported by grants to JCE from: the Program of Shanghai Academic/Technology Research Leader (15XD1503000); the Science and Technology Commission of Shanghai Municipality (15JC1400104); the 111 Project, Base B16018; the National Natural Science Foundation of China (NSFC; 31970962); NYU-ECNU Institute of Brain and Cognitive Science at NYU Shanghai. JCE also acknowledges the funders of the Sainsbury Wellcome Centre (The Wellcome Trust and the Gatsby Charitable Foundation).</p></ack><sec id="S33" sec-type="code-availability"><title>Code and data sharing</title><p id="P96">Please visit <ext-link ext-link-type="uri" xlink:href="https://github.com/erlichlab/fof-visually-guided">https://github.com/erlichlab/fof-visually-guided</ext-link> to access the code used for analyses and to generate figures. Links to the data are available from the github repository.</p></sec><fn-group><fn id="FN1" fn-type="conflict"><p id="P97">The authors have no conflict of interest nor financial interests.</p></fn><fn id="FN2" fn-type="con"><p id="P98"><bold>Author Contributions</bold></p><p id="P99">JCE and LL conceptualized and designed the experiments. LL, CM, JCE and JL collected the experimental data. LL, CM and JL curated the spike sorting and contributed to analysis pipelines for electrophysiology. LL analysed the behavioral and neural data under supervision of JCE. CM extracted tracking information from videos. HC developed an earlier version of this task. YC contributed to an earlier version of the spike sorting pipeline. TF did the RNN modeling with input from JCE and LL. LL and JCE wrote the manuscript with comments from all authors.</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alexander</surname><given-names>AS</given-names></name><name><surname>Robinson</surname><given-names>JC</given-names></name><name><surname>Stern</surname><given-names>CE</given-names></name><name><surname>Hasselmo</surname><given-names>ME</given-names></name></person-group><article-title>Gated transformations from egocentric to allocentric reference frames involving retrosplenial cortex, entorhinal cortex, and hippocampus</article-title><source>Hippocampus</source><year>2023</year><volume>33</volume><issue>5</issue><fpage>465</fpage><lpage>487</lpage><comment>6 citations (Crossref) [2023-07-10]</comment><pub-id pub-id-type="pmcid">PMC10403145</pub-id><pub-id pub-id-type="pmid">36861201</pub-id><pub-id pub-id-type="doi">10.1002/hipo.23513</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andersen</surname><given-names>R</given-names></name><name><surname>Bracewell</surname><given-names>R</given-names></name><name><surname>Barash</surname><given-names>S</given-names></name><name><surname>Gnadt</surname><given-names>J</given-names></name><name><surname>Fogassi</surname><given-names>L</given-names></name></person-group><article-title>Eye position effects on visual, memory, and saccade-related activity in areas LIP and 7a of macaque</article-title><source>The Journal of Neuroscience</source><year>1990</year><volume>10</volume><issue>4</issue><fpage>1176</fpage><lpage>1196</lpage><pub-id pub-id-type="pmcid">PMC6570201</pub-id><pub-id pub-id-type="pmid">2329374</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.10-04-01176.1990</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andersen</surname><given-names>R</given-names></name><name><surname>Essick</surname><given-names>G</given-names></name><name><surname>Siegel</surname><given-names>R</given-names></name></person-group><article-title>Encoding of spatial location by posterior parietal neurons</article-title><source>Science</source><year>1985</year><volume>230</volume><issue>4724</issue><fpage>456</fpage><lpage>458</lpage><pub-id pub-id-type="pmid">4048942</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andersen</surname><given-names>R</given-names></name><name><surname>Mountcastle</surname><given-names>V</given-names></name></person-group><article-title>The influence of the angle of gaze upon the excitability of the light- sensitive neurons of the posterior parietal cortex</article-title><source>The Journal of Neuroscience</source><year>1983</year><volume>3</volume><issue>3</issue><fpage>532</fpage><lpage>548</lpage><pub-id pub-id-type="pmcid">PMC6564545</pub-id><pub-id pub-id-type="pmid">6827308</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.03-03-00532.1983</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barthas</surname><given-names>F</given-names></name><name><surname>Kwan</surname><given-names>AC</given-names></name></person-group><article-title>Secondary Motor Cortex: Where ‘Sensory’ Meets ‘Motor’ in the Rodent Frontal Cortex</article-title><source>Trends in Neurosciences</source><year>2017</year><volume>40</volume><issue>3</issue><fpage>181</fpage><lpage>193</lpage><pub-id pub-id-type="pmcid">PMC5339050</pub-id><pub-id pub-id-type="pmid">28012708</pub-id><pub-id pub-id-type="doi">10.1016/j.tins.2016.11.006</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bharmauria</surname><given-names>V</given-names></name><name><surname>Sajad</surname><given-names>A</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Yan</surname><given-names>X</given-names></name><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Crawford</surname><given-names>JD</given-names></name></person-group><article-title>Integration of Eye-Centered and Landmark-Centered Codes in Frontal Eye Field Gaze Responses</article-title><source>Cerebral Cortex</source><year>2020</year><elocation-id>bhaa090</elocation-id><pub-id pub-id-type="pmid">32390052</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bicanski</surname><given-names>A</given-names></name><name><surname>Burgess</surname><given-names>N</given-names></name></person-group><article-title>Neuronal vector coding in spatial cognition</article-title><source>Nature Reviews Neuroscience</source><year>2020</year><volume>21</volume><issue>9</issue><fpage>453</fpage><lpage>470</lpage><comment>67 citations (Crossref) [2023-07-11]</comment><pub-id pub-id-type="pmid">32764728</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruce</surname><given-names>CJ</given-names></name><name><surname>Goldberg</surname><given-names>ME</given-names></name></person-group><article-title>Primate frontal eye fields. I. Single neurons discharging before saccades</article-title><source>Journal of Neurophysiology</source><year>1985</year><volume>53</volume><issue>3</issue><fpage>603</fpage><lpage>635</lpage><comment>tex.ids: brucePrimateFrontalEye1985b</comment><pub-id pub-id-type="pmid">3981231</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bush</surname><given-names>NE</given-names></name><name><surname>Solla</surname><given-names>SA</given-names></name><name><surname>Hartmann</surname><given-names>MJ</given-names></name></person-group><article-title>Whisking mechanics and active sensing</article-title><source>Current Opinion in Neurobiology</source><year>2016</year><volume>40</volume><fpage>178</fpage><lpage>188</lpage><pub-id pub-id-type="pmcid">PMC5312677</pub-id><pub-id pub-id-type="pmid">27632212</pub-id><pub-id pub-id-type="doi">10.1016/j.conb.2016.08.001</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caruso</surname><given-names>VC</given-names></name><name><surname>Pages</surname><given-names>DS</given-names></name><name><surname>Sommer</surname><given-names>MA</given-names></name><name><surname>Groh</surname><given-names>JM</given-names></name></person-group><article-title>Beyond the labeled line: variation in visual reference frames from intraparietal cortex to frontal eye fields and the superior colliculus</article-title><source>Journal of Neurophysiology</source><year>2018</year><volume>119</volume><issue>4</issue><fpage>1411</fpage><lpage>1421</lpage><pub-id pub-id-type="pmcid">PMC5966730</pub-id><pub-id pub-id-type="pmid">29357464</pub-id><pub-id pub-id-type="doi">10.1152/jn.00584.2017</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cassanello</surname><given-names>C</given-names></name><name><surname>Ferrera</surname><given-names>VP</given-names></name></person-group><article-title>Vector subtraction and eye position gainfields in macaque frontal eye field</article-title><source>Journal of Vision</source><year>2010</year><volume>5</volume><issue>8</issue><fpage>580</fpage><comment>0 citations (Crossref) [2022-04-21]</comment></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cassanello</surname><given-names>CR</given-names></name><name><surname>Ferrera</surname><given-names>VP</given-names></name></person-group><article-title>Computing vector differences using a gain field-like mechanism in monkey frontal eye field: Eye position signals in FEF</article-title><source>The Journal of Physiology</source><year>2007</year><volume>582</volume><issue>2</issue><fpage>647</fpage><lpage>664</lpage><pub-id pub-id-type="pmcid">PMC2075335</pub-id><pub-id pub-id-type="pmid">17510192</pub-id><pub-id pub-id-type="doi">10.1113/jphysiol.2007.128801</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>T-W</given-names></name><name><surname>Li</surname><given-names>N</given-names></name><name><surname>Daie</surname><given-names>K</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><article-title>A Map of Anticipatory Activity in Mouse Motor Cortex</article-title><source>Neuron</source><year>2017</year><volume>94</volume><issue>4</issue><fpage>866</fpage><lpage>879</lpage><elocation-id>e4</elocation-id><comment>00000</comment><pub-id pub-id-type="pmid">28521137</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>MX</given-names></name></person-group><source>Analyzing Neural Time Series Data: Theory and Practice</source><publisher-name>The MIT Press</publisher-name><year>2014</year></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>YE</given-names></name><name><surname>Andersen</surname><given-names>RA</given-names></name></person-group><article-title>A common reference frame for movement plans in the posterior parietal cortex</article-title><source>Nature Reviews. Neuroscience; London</source><year>2002</year><volume>3</volume><issue>7</issue><fpage>553</fpage><lpage>62</lpage><comment>tex.copyright: Copyright Nature Publishing Group Jul 2002</comment><pub-id pub-id-type="pmid">12094211</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collewijn</surname><given-names>H</given-names></name><name><surname>Erkelens</surname><given-names>CJ</given-names></name><name><surname>Steinman</surname><given-names>RM</given-names></name></person-group><article-title>Binocular co-ordination of human vertical saccadic eye movements</article-title><source>The Journal of Physiology</source><year>1988</year><volume>404</volume><issue>1</issue><fpage>183</fpage><lpage>197</lpage><comment>113 citations (Crossref) [2022-12-21]</comment><pub-id pub-id-type="pmcid">PMC1190821</pub-id><pub-id pub-id-type="pmid">3253430</pub-id><pub-id pub-id-type="doi">10.1113/jphysiol.1988.sp017285</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collewijn</surname><given-names>H</given-names></name><name><surname>Tamminga</surname><given-names>EP</given-names></name></person-group><article-title>Human smooth and saccadic eye movements during voluntary pursuit of different target motions on different backgrounds</article-title><source>The Journal of Physiology</source><year>1984</year><volume>351</volume><issue>1</issue><fpage>217</fpage><lpage>250</lpage><comment>271 citations (Crossref) [2022-12-21]</comment><pub-id pub-id-type="pmcid">PMC1193114</pub-id><pub-id pub-id-type="pmid">6747865</pub-id><pub-id pub-id-type="doi">10.1113/jphysiol.1984.sp015242</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cowey</surname><given-names>A</given-names></name><name><surname>Ek</surname><given-names>TB</given-names></name></person-group><article-title>Contralateral ‘neglect’ after unilateral dorsomedial prefrontal lesions 1n rats</article-title><source>Brain Research</source><year>1973</year><volume>11</volume><pub-id pub-id-type="pmid">4830476</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crowne</surname><given-names>DP</given-names></name><name><surname>Pathria</surname><given-names>MN</given-names></name></person-group><article-title>Some attentional effects of unilateral frontal lesions in the rat</article-title><source>Behavioural Brain Research</source><year>1982</year><volume>6</volume><issue>1</issue><fpage>25</fpage><lpage>39</lpage><comment>73 citations (Crossref) [2022-11-03]</comment><pub-id pub-id-type="pmid">7126323</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Duan</surname><given-names>CA</given-names></name><name><surname>Pan</surname><given-names>Y</given-names></name><name><surname>Ma</surname><given-names>G</given-names></name><name><surname>Zhou</surname><given-names>T</given-names></name><name><surname>Zhang</surname><given-names>S</given-names></name><name><surname>Xu</surname><given-names>N-l</given-names></name></person-group><article-title>A cortico-collicular pathway for motor planning in a memory-dependent perceptual decision task</article-title><source>Nature Communications</source><publisher-name>Nature Publishing Group</publisher-name><year>2021</year><volume>12</volume><issue>1</issue><elocation-id>2727</elocation-id><comment>tex.ids= duan2021corticocolliculara number: 1</comment><pub-id pub-id-type="pmcid">PMC8113349</pub-id><pub-id pub-id-type="pmid">33976124</pub-id><pub-id pub-id-type="doi">10.1038/s41467-021-22547-9</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ebbesen</surname><given-names>CL</given-names></name><name><surname>Insanally</surname><given-names>MN</given-names></name><name><surname>Kopec</surname><given-names>CD</given-names></name><name><surname>Murakami</surname><given-names>M</given-names></name><name><surname>Saiki</surname><given-names>A</given-names></name><name><surname>Erlich</surname><given-names>JC</given-names></name></person-group><article-title>More than Just a “Motor”: Recent Surprises from the Frontal Cortex</article-title><source>The Journal of Neuroscience</source><year>2018</year><volume>38</volume><issue>44</issue><fpage>9402</fpage><lpage>9413</lpage><pub-id pub-id-type="pmcid">PMC6209835</pub-id><pub-id pub-id-type="pmid">30381432</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1671-18.2018</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erlich</surname><given-names>JC</given-names></name><name><surname>Bialek</surname><given-names>M</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><article-title>A cortical substrate for memory-guided orienting in the rat</article-title><source>Neuron</source><year>2011</year><volume>72</volume><issue>2</issue><fpage>330</fpage><lpage>343</lpage><comment>00000</comment><pub-id pub-id-type="pmcid">PMC3212026</pub-id><pub-id pub-id-type="pmid">22017991</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2011.07.010</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erlich</surname><given-names>JC</given-names></name><name><surname>Brunton</surname><given-names>BW</given-names></name><name><surname>Duan</surname><given-names>CA</given-names></name><name><surname>Hanks</surname><given-names>TD</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><article-title>Distinct effects of prefrontal and parietal cortex inactivations on an accumulation of evidence task in the rat</article-title><source>eLife</source><year>2015</year><volume>4</volume><comment>tex</comment><pub-id pub-id-type="pmcid">PMC4392479</pub-id><pub-id pub-id-type="pmid">25869470</pub-id><pub-id pub-id-type="doi">10.7554/eLife.05457</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gofman</surname><given-names>X</given-names></name><name><surname>Tocker</surname><given-names>G</given-names></name><name><surname>Weiss</surname><given-names>S</given-names></name><name><surname>Boccara</surname><given-names>CN</given-names></name><name><surname>Lu</surname><given-names>L</given-names></name><name><surname>Moser</surname><given-names>M-B</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name><name><surname>Morris</surname><given-names>G</given-names></name><name><surname>Derdikman</surname><given-names>D</given-names></name></person-group><article-title>Dissociation between Postrhinal Cortex and Downstream Parahippocampal Regions in the Representation of Egocentric Boundaries</article-title><source>Current Biology</source><year>2019</year><volume>29</volume><issue>16</issue><fpage>2751</fpage><lpage>2757</lpage><elocation-id>e4</elocation-id><comment>46 citations (Crossref) [2023-07-11]</comment><pub-id pub-id-type="pmid">31378610</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>Z</given-names></name><name><surname>Li</surname><given-names>N</given-names></name><name><surname>Huber</surname><given-names>D</given-names></name><name><surname>Ophir</surname><given-names>E</given-names></name><name><surname>Gutnisky</surname><given-names>D</given-names></name><name><surname>Ting</surname><given-names>J</given-names></name><name><surname>Feng</surname><given-names>G</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><article-title>Flow of Cortical Activity Underlying a Tactile Decision in Mice</article-title><source>Neuron</source><year>2014</year><volume>81</volume><issue>1</issue><fpage>179</fpage><lpage>194</lpage><comment>00107</comment><pub-id pub-id-type="pmcid">PMC3984938</pub-id><pub-id pub-id-type="pmid">24361077</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2013.10.020</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>ZV</given-names></name><name><surname>Inagaki</surname><given-names>HK</given-names></name><name><surname>Daie</surname><given-names>K</given-names></name><name><surname>Druckmann</surname><given-names>S</given-names></name><name><surname>Gerfen</surname><given-names>CR</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><article-title>Maintenance of persistent activity in a frontal thalamocortical loop</article-title><source>Nature</source><year>2017</year><volume>545</volume><issue>7653</issue><fpage>181</fpage><lpage>186</lpage><comment>00003</comment><pub-id pub-id-type="pmcid">PMC6431254</pub-id><pub-id pub-id-type="pmid">28467817</pub-id><pub-id pub-id-type="doi">10.1038/nature22324</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hafting</surname><given-names>T</given-names></name><name><surname>Fyhn</surname><given-names>M</given-names></name><name><surname>Molden</surname><given-names>S</given-names></name><name><surname>Moser</surname><given-names>M-B</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><article-title>Microstructure of a spatial map in the entorhinal cortex</article-title><source>Nature</source><year>2005</year><volume>436</volume><issue>7052</issue><fpage>801</fpage><lpage>806</lpage><pub-id pub-id-type="pmid">15965463</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanks</surname><given-names>TD</given-names></name><name><surname>Kopec</surname><given-names>CD</given-names></name><name><surname>Brunton</surname><given-names>BW</given-names></name><name><surname>Duan</surname><given-names>CA</given-names></name><name><surname>Erlich</surname><given-names>JC</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><article-title>Distinct relationships of parietal and prefrontal cortices to evidence accumulation</article-title><source>Nature</source><year>2015</year><volume>520</volume><issue>7546</issue><fpage>220</fpage><lpage>223</lpage><comment>00000</comment><pub-id pub-id-type="pmcid">PMC4835184</pub-id><pub-id pub-id-type="pmid">25600270</pub-id><pub-id pub-id-type="doi">10.1038/nature14066</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Inagaki</surname><given-names>HK</given-names></name><name><surname>Fontolan</surname><given-names>L</given-names></name><name><surname>Romani</surname><given-names>S</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><article-title>Discrete attractor dynamics underlies persistent activity in the frontal cortex</article-title><source>Nature</source><year>2019</year><volume>566</volume><issue>7743</issue><fpage>212</fpage><lpage>217</lpage><comment>tex.ids= inagakiDiscreteAttractorDynamics2019a</comment><pub-id pub-id-type="pmid">30728503</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jovalekic</surname><given-names>A</given-names></name><name><surname>Hayman</surname><given-names>R</given-names></name><name><surname>Becares</surname><given-names>N</given-names></name><name><surname>Reid</surname><given-names>H</given-names></name><name><surname>Thomas</surname><given-names>G</given-names></name><name><surname>Wilson</surname><given-names>J</given-names></name><name><surname>Jeffery</surname><given-names>K</given-names></name></person-group><article-title>Horizontal biases in rats’ use of three-dimensional space</article-title><source>Behavioural Brain Research</source><year>2011</year><volume>222</volume><issue>2</issue><fpage>279</fpage><lpage>288</lpage><pub-id pub-id-type="pmcid">PMC3157560</pub-id><pub-id pub-id-type="pmid">21419172</pub-id><pub-id pub-id-type="doi">10.1016/j.bbr.2011.02.035</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ke</surname><given-names>SR</given-names></name><name><surname>Lam</surname><given-names>J</given-names></name><name><surname>Pai</surname><given-names>DK</given-names></name><name><surname>Spering</surname><given-names>M</given-names></name></person-group><article-title>Directional Asymmetries in Human Smooth Pursuit Eye Movements</article-title><source>Investigative Opthalmology &amp; Visual Science</source><year>2013</year><volume>54</volume><issue>6</issue><elocation-id>4409</elocation-id><comment>35 citations (Crossref) [2022-12-01]</comment><pub-id pub-id-type="pmid">23716624</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kopec</surname><given-names>CD</given-names></name><name><surname>Erlich</surname><given-names>JC</given-names></name><name><surname>Brunton</surname><given-names>BW</given-names></name><name><surname>Deisseroth</surname><given-names>K</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><article-title>Cortical and Subcortical Contributions to Short-Term Memory for Orienting Movements</article-title><source>Neuron</source><year>2015</year><volume>88</volume><issue>2</issue><fpage>367</fpage><lpage>377</lpage><comment>00000</comment><pub-id pub-id-type="pmcid">PMC5521275</pub-id><pub-id pub-id-type="pmid">26439529</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2015.08.033</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Representational similarity analysis – connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><year>2008</year><pub-id pub-id-type="pmcid">PMC2605405</pub-id><pub-id pub-id-type="pmid">19104670</pub-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leonard</surname><given-names>CM</given-names></name></person-group><article-title>The prefrontal cortex of the rat. I. Cortical projection of the mediodorsal nucleus. II. Efferent connections</article-title><source>Brain Research</source><year>1969</year><volume>12</volume><issue>2</issue><fpage>321</fpage><lpage>343</lpage><comment>tex.eprint: 4184997 tex.eprinttype: pmid</comment><pub-id pub-id-type="pmid">4184997</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>N</given-names></name><name><surname>Daie</surname><given-names>K</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Druckmann</surname><given-names>S</given-names></name></person-group><article-title>Robust neuronal dynamics in premotor cortex during motor planning</article-title><source>Nature</source><year>2016</year><volume>532</volume><issue>7600</issue><fpage>459</fpage><lpage>464</lpage><comment>00035</comment><pub-id pub-id-type="pmcid">PMC5081260</pub-id><pub-id pub-id-type="pmid">27074502</pub-id><pub-id pub-id-type="doi">10.1038/nature17643</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maisson</surname><given-names>DJ-N</given-names></name><name><surname>Voloh</surname><given-names>B</given-names></name><name><surname>Cervera</surname><given-names>RL</given-names></name><name><surname>Conover</surname><given-names>I</given-names></name><name><surname>Zambre</surname><given-names>M</given-names></name><name><surname>Zimmermann</surname><given-names>J</given-names></name><name><surname>Hayden</surname><given-names>BY</given-names></name></person-group><article-title>Widespread coding of navigational variables in prefrontal cortex</article-title><source>Neuroscience</source><year>2022</year><comment>preprint</comment><pub-id pub-id-type="pmcid">PMC10984098</pub-id><pub-id pub-id-type="pmid">37541250</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2023.07.024</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname><given-names>A</given-names></name><name><surname>Mamidanna</surname><given-names>P</given-names></name><name><surname>Cury</surname><given-names>KM</given-names></name><name><surname>Abe</surname><given-names>T</given-names></name><name><surname>Murthy</surname><given-names>VN</given-names></name><name><surname>Mathis</surname><given-names>MW</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name></person-group><article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title><source>Nature Neuroscience</source><year>2018</year><volume>21</volume><issue>9</issue><fpage>1281</fpage><lpage>1289</lpage><pub-id pub-id-type="pmid">30127430</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCluskey</surname><given-names>MK</given-names></name><name><surname>Cullen</surname><given-names>KE</given-names></name></person-group><article-title>Eye, Head, and Body Coordination During Large Gaze Shifts in Rhesus Monkeys: Movement Kinematics and the Influence of Posture</article-title><source>Journal of Neurophysiology</source><year>2007</year><volume>97</volume><issue>4</issue><fpage>2976</fpage><lpage>2991</lpage><comment>40 citations (Crossref) [2022-11-03]</comment><pub-id pub-id-type="pmid">17229827</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mimica</surname><given-names>B</given-names></name><name><surname>Dunn</surname><given-names>BA</given-names></name><name><surname>Tombaz</surname><given-names>T</given-names></name><name><surname>Bojja</surname><given-names>VPTNCS</given-names></name><name><surname>Whitlock</surname><given-names>JR</given-names></name></person-group><article-title>Efficient cortical coding of 3D posture in freely behaving rats</article-title><source>Science</source><year>2018</year><volume>362</volume><issue>6414</issue><fpage>584</fpage><lpage>589</lpage><comment>tex.ids: mimicaEfficientCorticalCoding2018a</comment><pub-id pub-id-type="pmid">30385578</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Monteon</surname><given-names>JA</given-names></name><name><surname>Constantin</surname><given-names>AG</given-names></name><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Martinez-Trujillo</surname><given-names>J</given-names></name><name><surname>Crawford</surname><given-names>JD</given-names></name></person-group><article-title>Electrical Stimulation of the Frontal Eye Fields in the Head-Free Macaque Evokes Kinematically Normal 3D Gaze Shifts</article-title><source>Journal of Neurophysiology</source><year>2010</year><volume>104</volume><issue>6</issue><fpage>3462</fpage><lpage>3475</lpage><pub-id pub-id-type="pmid">20881198</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Nadel</surname><given-names>L</given-names></name></person-group><source>The hippocampus as a cognitive map</source><publisher-name>Clarendon Press; Oxford University Press</publisher-name><publisher-loc>Oxford : New York</publisher-loc><year>1978</year></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olson</surname><given-names>JM</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Montgomery</surname><given-names>S</given-names></name><name><surname>Nitz</surname><given-names>DA</given-names></name></person-group><article-title>Secondary Motor Cortex Transforms Spatial Information into Planned Action During Navigation</article-title><source>bioRxiv</source><year>2019</year><elocation-id>776765</elocation-id><pub-id pub-id-type="pmid">32302586</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olson</surname><given-names>JM</given-names></name><name><surname>Li</surname><given-names>JK</given-names></name><name><surname>Montgomery</surname><given-names>SE</given-names></name><name><surname>Nitz</surname><given-names>DA</given-names></name></person-group><article-title>Secondary Motor Cortex Transforms Spatial Information into Planned Action during Navigation</article-title><source>Current Biology</source><year>2020</year><elocation-id>S0960982220303547</elocation-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peyrache</surname><given-names>A</given-names></name><name><surname>Schieferstein</surname><given-names>N</given-names></name><name><surname>Buzsáki</surname><given-names>G</given-names></name></person-group><article-title>Transformation of the head-direction signal into a spatial code</article-title><source>Nature Communications</source><year>2017</year><volume>8</volume><issue>1</issue><elocation-id>1752</elocation-id><pub-id pub-id-type="pmcid">PMC5700966</pub-id><pub-id pub-id-type="pmid">29170377</pub-id><pub-id pub-id-type="doi">10.1038/s41467-017-01908-3</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piet</surname><given-names>AT</given-names></name><name><surname>Erlich</surname><given-names>JC</given-names></name><name><surname>Kopec</surname><given-names>CD</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><article-title>Rat Prefrontal Cortex Inactivations during Decision Making Are Explained by Bistable Attractor Dynamics</article-title><source>Neural Computation</source><year>2017</year><volume>29</volume><issue>11</issue><fpage>2861</fpage><lpage>2886</lpage><pub-id pub-id-type="pmcid">PMC6535097</pub-id><pub-id pub-id-type="pmid">28777728</pub-id><pub-id pub-id-type="doi">10.1162/neco_a_01005</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pouget</surname><given-names>A</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><article-title>Spatial Transformations in the Parietal Cortex Using Basis Functions</article-title><source>Journal of Cognitive Neuroscience</source><year>1997</year><volume>9</volume><issue>2</issue><fpage>222</fpage><lpage>237</lpage><comment>274 citations (Crossref) [2022-12-21]</comment><pub-id pub-id-type="pmid">23962013</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reep</surname><given-names>R</given-names></name><name><surname>Chandler</surname><given-names>H</given-names></name><name><surname>King</surname><given-names>V</given-names></name><name><surname>Corwin</surname><given-names>J</given-names></name></person-group><article-title>Rat posterior parietal cortex: Topography of corticocortical and thalamic connections</article-title><source>Experimental Brain Research</source><year>1994</year><volume>100</volume><issue>1</issue><comment>153 citations (Crossref) [2022-11-03]</comment><pub-id pub-id-type="pmid">7813654</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reep</surname><given-names>R</given-names></name><name><surname>Corwin</surname><given-names>J</given-names></name><name><surname>Hashimoto</surname><given-names>A</given-names></name><name><surname>Watson</surname><given-names>R</given-names></name></person-group><article-title>Efferent Connections of the Rostral Portion of Medial Agranular Cortex in Rats</article-title><source>Brain Research Bulletin</source><year>1987</year><volume>19</volume><issue>2</issue><fpage>203</fpage><lpage>221</lpage><comment>175 citations (Crossref) [2022-11-03]</comment><pub-id pub-id-type="pmid">2822206</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reep</surname><given-names>RL</given-names></name><name><surname>Goodwin</surname><given-names>GS</given-names></name><name><surname>Corwin</surname><given-names>JV</given-names></name></person-group><article-title>Topographic organization in the corticocortical connections of medial agranular cortex in rats</article-title><source>The Journal of Comparative Neurology</source><year>1990</year><volume>294</volume><issue>2</issue><fpage>262</fpage><lpage>280</lpage><pub-id pub-id-type="pmid">2332532</pub-id></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reinagel</surname><given-names>P</given-names></name></person-group><article-title>Training Rats Using Water Rewards Without Water Restriction</article-title><source>Frontiers in Behavioral Neuroscience</source><year>2018</year><volume>12</volume><fpage>84</fpage><comment>10 citations (Crossref) [2022-07-04]</comment><pub-id pub-id-type="pmcid">PMC5943498</pub-id><pub-id pub-id-type="pmid">29773982</pub-id><pub-id pub-id-type="doi">10.3389/fnbeh.2018.00084</pub-id></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rigotti</surname><given-names>M</given-names></name><name><surname>Barak</surname><given-names>O</given-names></name><name><surname>Warden</surname><given-names>MR</given-names></name><name><surname>Wang</surname><given-names>X-J</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Fusi</surname><given-names>S</given-names></name></person-group><article-title>The importance of mixed selectivity in complex cognitive tasks</article-title><source>Nature</source><year>2013</year><volume>497</volume><issue>7451</issue><fpage>585</fpage><lpage>590</lpage><comment>889 citations (Crossref) [2022-11-03]</comment><pub-id pub-id-type="pmcid">PMC4412347</pub-id><pub-id pub-id-type="pmid">23685452</pub-id><pub-id pub-id-type="doi">10.1038/nature12160</pub-id></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russo</surname><given-names>GS</given-names></name><name><surname>Bruce</surname><given-names>CJ</given-names></name></person-group><article-title>Frontal eye field activity preceding aurally guided saccades</article-title><source>Journal of Neurophysiology</source><year>1994</year><volume>71</volume><issue>3</issue><fpage>1250</fpage><lpage>1253</lpage><comment>91 citations (Crossref) [2023-09-29]</comment><pub-id pub-id-type="pmid">8201415</pub-id></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salinas</surname><given-names>E</given-names></name><name><surname>Abbott</surname><given-names>L</given-names></name></person-group><article-title>Transfer of coded information from sensory to motor networks</article-title><source>The Journal of Neuroscience</source><year>1995</year><volume>15</volume><issue>10</issue><fpage>6461</fpage><lpage>6474</lpage><comment>175 citations (Crossref) [2022-11-03]</comment><pub-id pub-id-type="pmcid">PMC6578023</pub-id><pub-id pub-id-type="pmid">7472409</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.15-10-06461.1995</pub-id></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salinas</surname><given-names>E</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><article-title>Gain Modulation in the Central Nervous System: Where Behavior, Neurophysiology, and Computation Meet</article-title><source>The Neuroscientist</source><year>2001</year><volume>7</volume><issue>5</issue><fpage>430</fpage><lpage>440</lpage><comment>169 citations (Crossref) [2022-11-03]</comment><pub-id pub-id-type="pmcid">PMC2887717</pub-id><pub-id pub-id-type="pmid">11597102</pub-id><pub-id pub-id-type="doi">10.1177/107385840100700512</pub-id></element-citation></ref><ref id="R55"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schall</surname><given-names>JD</given-names></name></person-group><chapter-title>Frontal Eye Fields</chapter-title><person-group person-group-type="editor"><name><surname>Squire</surname><given-names>L</given-names></name></person-group><source>Encyclopedia of Neuroscience</source><publisher-name>Academic Press</publisher-name><publisher-loc>Oxford</publisher-loc><year>2009</year><volume>4</volume><fpage>367</fpage><lpage>374</lpage></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name></person-group><article-title>Cortical Control of Arm Movements: A Dynamical Systems Perspective</article-title><source>Annual Review of Neuroscience</source><year>2013</year><volume>36</volume><issue>1</issue><fpage>337</fpage><lpage>359</lpage><comment>tex.ids: shenoyCorticalControlArm2013a</comment><pub-id pub-id-type="pmid">23725001</pub-id></element-citation></ref><ref id="R57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siniscalchi</surname><given-names>MJ</given-names></name><name><surname>Phoumthipphavong</surname><given-names>V</given-names></name><name><surname>Ali</surname><given-names>F</given-names></name><name><surname>Lozano</surname><given-names>M</given-names></name><name><surname>Kwan</surname><given-names>AC</given-names></name></person-group><article-title>Fast and slow transitions in frontal ensemble activity during flexible sensorimotor behavior</article-title><source>Nature Neuroscience</source><year>2016</year><volume>19</volume><issue>9</issue><fpage>1234</fpage><lpage>1242</lpage><comment>64 citations (Crossref) [2022-11-03]</comment><pub-id pub-id-type="pmcid">PMC5003707</pub-id><pub-id pub-id-type="pmid">27399844</pub-id><pub-id pub-id-type="doi">10.1038/nn.4342</pub-id></element-citation></ref><ref id="R58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sinnamon</surname><given-names>HM</given-names></name><name><surname>Galer</surname><given-names>BS</given-names></name></person-group><article-title>Head movements elicited by electrical stimulation of the anteromedial cortex of the rat</article-title><source>Physiology &amp; Behavior</source><year>1984</year><volume>33</volume><issue>2</issue><fpage>185</fpage><lpage>190</lpage><comment>tex.ids: sinnamon1984head</comment><pub-id pub-id-type="pmid">6505061</pub-id></element-citation></ref><ref id="R59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sul</surname><given-names>JH</given-names></name><name><surname>Jo</surname><given-names>S</given-names></name><name><surname>Lee</surname><given-names>D</given-names></name><name><surname>Jung</surname><given-names>MW</given-names></name></person-group><article-title>Role of rodent secondary motor cortex in value-based action selection</article-title><source>Nature Neuroscience</source><year>2011</year><volume>14</volume><issue>9</issue><fpage>1202</fpage><lpage>1208</lpage><pub-id pub-id-type="pmcid">PMC3164897</pub-id><pub-id pub-id-type="pmid">21841777</pub-id><pub-id pub-id-type="doi">10.1038/nn.2881</pub-id></element-citation></ref><ref id="R60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taube</surname><given-names>J</given-names></name><name><surname>Muller</surname><given-names>R</given-names></name><name><surname>Ranck</surname><given-names>J</given-names></name></person-group><article-title>Head-direction cells recorded from the postsubiculum in freely moving rats. I. Description and quantitative analysis</article-title><source>The Journal of Neuroscience</source><year>1990</year><volume>10</volume><issue>2</issue><fpage>420</fpage><lpage>435</lpage><pub-id pub-id-type="pmcid">PMC6570151</pub-id><pub-id pub-id-type="pmid">2303851</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.10-02-00420.1990</pub-id></element-citation></ref><ref id="R61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>C</given-names></name><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Knierim</surname><given-names>JJ</given-names></name></person-group><article-title>Egocentric and allocentric representations of space in the rodent brain</article-title><source>Current Opinion in Neurobiology</source><year>2020</year><volume>60</volume><fpage>12</fpage><lpage>20</lpage><pub-id pub-id-type="pmcid">PMC7080648</pub-id><pub-id pub-id-type="pmid">31794917</pub-id><pub-id pub-id-type="doi">10.1016/j.conb.2019.11.005</pub-id></element-citation></ref><ref id="R62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>W</given-names></name><name><surname>Chan</surname><given-names>SS</given-names></name><name><surname>Heldman</surname><given-names>DA</given-names></name><name><surname>Moran</surname><given-names>DW</given-names></name></person-group><article-title>Motor Cortical Representation of Position and Velocity During Reaching</article-title><source>Journal of Neurophysiology</source><year>2007</year><volume>97</volume><issue>6</issue><fpage>4258</fpage><lpage>4270</lpage><comment>81 citations (Crossref) [2022-11-21]</comment><pub-id pub-id-type="pmid">17392416</pub-id></element-citation></ref><ref id="R63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilber</surname><given-names>AA</given-names></name><name><surname>Clark</surname><given-names>BJ</given-names></name><name><surname>Forster</surname><given-names>TC</given-names></name><name><surname>Tatsuno</surname><given-names>M</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name></person-group><article-title>Interaction of Egocentric and World-Centered Reference Frames in the Rat Posterior Parietal Cortex</article-title><source>Journal of Neuroscience</source><year>2014</year><volume>34</volume><issue>16</issue><fpage>5431</fpage><lpage>5446</lpage><pub-id pub-id-type="pmcid">PMC3988403</pub-id><pub-id pub-id-type="pmid">24741034</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0511-14.2014</pub-id></element-citation></ref><ref id="R64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wimmer</surname><given-names>K</given-names></name><name><surname>Nykamp</surname><given-names>DQ</given-names></name><name><surname>Constantinidis</surname><given-names>C</given-names></name><name><surname>Compte</surname><given-names>A</given-names></name></person-group><article-title>Bump attractor dynamics in prefrontal cortex explains behavioral precision in spatial working memory</article-title><source>Nature Neuroscience</source><year>2014</year><volume>17</volume><issue>3</issue><fpage>431</fpage><lpage>439</lpage><pub-id pub-id-type="pmid">24487232</pub-id></element-citation></ref><ref id="R65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wise</surname><given-names>SP</given-names></name></person-group><article-title>Forward frontal fields: Phylogeny and fundamental function</article-title><source>Trends in Neurosciences</source><year>2008</year><volume>31</volume><issue>12</issue><fpage>599</fpage><lpage>608</lpage><comment>312 citations (Crossref) [2022-11-03]</comment><pub-id pub-id-type="pmcid">PMC2587508</pub-id><pub-id pub-id-type="pmid">18835649</pub-id><pub-id pub-id-type="doi">10.1016/j.tins.2008.08.008</pub-id></element-citation></ref><ref id="R66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamawaki</surname><given-names>N</given-names></name><name><surname>Radulovic</surname><given-names>J</given-names></name><name><surname>Shepherd</surname><given-names>GM</given-names></name></person-group><article-title>A Corticocortical Circuit Directly Links Retrosplenial Cortex to M2 in the Mouse</article-title><source>The Journal of Neuroscience</source><year>2016</year><volume>36</volume><issue>36</issue><fpage>9365</fpage><lpage>9374</lpage><comment>77 citations (Crossref) [2022-11-03] tex.eprint: 27605612 tex.eprinttype: pmid tex.ids: yamawaki2016Corticocortical tex.</comment><pub-id pub-id-type="pmcid">PMC5013186</pub-id><pub-id pub-id-type="pmid">27605612</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1099-16.2016</pub-id></element-citation></ref><ref id="R67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>W</given-names></name></person-group><article-title>Thalamus-driven functional populations in frontal cortex support decision-making</article-title><source>Nature Neuroscience</source><year>2022</year><volume>25</volume><fpage>39</fpage><pub-id pub-id-type="pmcid">PMC9534763</pub-id><pub-id pub-id-type="pmid">36171427</pub-id><pub-id pub-id-type="doi">10.1038/s41593-022-01171-w</pub-id></element-citation></ref><ref id="R68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>X</given-names></name><name><surname>Moller-Mara</surname><given-names>J</given-names></name><name><surname>Dubroqua</surname><given-names>S</given-names></name><name><surname>Bao</surname><given-names>C</given-names></name><name><surname>Erlich</surname><given-names>JC</given-names></name></person-group><article-title>Frontal but not parietal cortex is required for decisions under risk</article-title><source>Cold Spring Harbor Laboratory</source><year>2021</year><elocation-id>2021.11.19.469107</elocation-id><comment>0 citations (Crossref) [2022-11-03]</comment></element-citation></ref><ref id="R69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zipser</surname><given-names>D</given-names></name><name><surname>Andersen</surname><given-names>RA</given-names></name></person-group><article-title>A back-propagation programmed network that simulates response properties of a subset of posterior parietal neurons</article-title><source>Nature</source><year>1988</year><volume>331</volume><issue>6158</issue><fpage>679</fpage><pub-id pub-id-type="pmid">3344044</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>A visually-guided multi-directional orienting task in rats.</title><p><bold>A.</bold> Schematic of the task. Each trial began with the onset of a pair of blue and yellow LED, cuing the rat to nose poke into the start port. The start LED extinguished upon arrival in the start port. After a short delay, a blue LED illuminated, indicating the target port. After a go sound, the rat withdrew from the start port and poked into the target port. Water reward was delivered for correctly performed trials. <bold>B.</bold> The logic of the task. We consider three trials and how cells with different task tuning would respond. The orange and pink trials share start position. The blue and pink trials share direction. The blue and orange trials share target. Right, the postulated firing patterns of neurons that were tuned to start position, movement direction, or target position respectively. <bold>C.</bold> The color scheme of the 6 movement directions. <bold>D.</bold> The color scheme of the 7 port positions. <bold>E.</bold> Timeline of a trial in a typical session. <bold>F.</bold> An example of head position tracking data extracted from video using DeepLabCut. Each line is the horizontal head coordinate during a trial, in unit of pixels from the video frames. Only trials starting from the central port are shown. <bold>G.</bold> There is no correlation between the horizontal head position and the planned movement direction before the go cue. The line indicates significance level of the movement direction modulating the horizontal head position in unit of pixels across time aligned to the go sound, where the effect of start position was captured in the random effect. Line and error bars, mean ± s.d. of the <italic>p</italic> values over the 58 sessions.</p></caption><graphic xlink:href="EMS157131-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Example neurons with egocentric and allocentric spatial representation.</title><p><bold>A-C</bold> An example neuron more modulated by the egocentric movement directions than by the allocentric target positions. <bold>A</bold>. Raster plots and PETHs aligned to the go sound and sorted by movement directions. The top 6 panels show spike rasters grouped by the 6 movement direction, indicated by arrows on the left side of the panels. Circles in each raster panel indicate the time of the visual cue onset on each trial. The bottom panel shows the PETHs of the spikes generated by a causal half-Gaussian kernel with an SD of 200 ms. The shaded areas of the PETHs indicate ± s.e.. The grey bar at the bottom of the panel indicates the time window to compute the firing rate in each trial type in C. <bold>B</bold>. Raster plots and PETHs of the same cell and same alignment as in <bold>A</bold>, but sorted by target position. Target positions are color coded as in <xref ref-type="fig" rid="F1">Figure 1D</xref>. Circles in each raster panel indicate the time of visual cue onset on each trial. <bold>C.</bold> The maximum <italic>a posteriori</italic> estimated firing rate for each trial type, where the prior was a Poisson distribution whose mean was estimated from all trials. <bold>D-F</bold> and <bold>G-I</bold>, two more example cells as in A-C. <bold>J-L</bold>, <bold>M-N</bold> and <bold>P-R</bold>, example neurons more modulated by the target position than direction. Neural activity was aligned to the target poke. Circles in each raster panel indicated the time of the go sound.</p></caption><graphic xlink:href="EMS157131-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Temporal profile of start position, direction and target position encoding in the FOF population.</title><p><bold>A.</bold> The fraction of neurons best selective to the start position (red), the direction (green) or the target position (blue) in 4 time windows (see main text or Methods for definition). Shaded areas indicate the 95% binomial confidence intervals. “Pre-cue”, -300 to 0 ms to visual cue onset. “Post-cue”, 0 to 300 ms to visual cue onset. “Go”, 0 to 300 ms to go sound. “Arrival”, -150 to 150 ms to target poke. <bold>B.</bold> Predicted coordinates of start position, movement vector and target position in an example pseudopopulation decoding using all neurons with enough trials for each type (n = 1194). Each small circle indicates the predicted coordinates in a pseudo trial, where the color indicates the pseudo trial class. Each large circle indicates the coordinates and the diameter of a port on the port wall. <bold>C.</bold> Decoding errors for each pseudopopulation across time aligned to the go sound. Decoding error is defined as the Euclidean distance between the predicted and the actual coordinates, and is indicated by colors following the colorbar in E. Each row is a different pseudopopulation where 1197 neurons were resampled randomly with replacement. Red lines indicate the mean errors across the 100 pseudopopulations. <bold>D.</bold> Mean ± s.d. of the difference of decoding errors between two spatial variables across the 100 peudopopulations. Positive difference indicates the better decoding of the second variable, and vice versa. <bold>E.</bold> Decoding errors with cross-window decoding. Colors of the heat-maps indicate the mean Euclidean distance between the decoded and true spatial coordinates, averaged across 100 pseudopopulations. The decoders were trained at one time window and tested at another. In the last panel the multivariate linear model was trained for start position and decoded for target position. Contours, <italic>p</italic> &lt; 0.01 (extreme pixel-based test).</p></caption><graphic xlink:href="EMS157131-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>FOF neurons encoded the current head position.</title><p><bold>A.</bold> Raster plots and PETHs of an example neuron aligned to the cue, grouped by start position. The shaded grey area indicates the time window to calculate the x-axis firing rate in C. <bold>B.</bold> The same neuron aligned to target poke, grouped by target position. The shaded grey area indicates the time window to calculate the y-axis firing rate in C. <bold>C.</bold> The correlation between start position tuning and y position tuning in the example neuron, where <italic>r</italic> denotes start-target tuning correlation of this neuron as in D. Purple line denotes the total least square fit. <bold>D.</bold> The distribution of the start-target tuning correlation. Black bars are for neurons selective to both start position and target position, and white bars are for neurons selective to only one of the two variables (Mean, [95% CI], 0.66, [0.61, 0.70] for both selective and 0.29, [0.25, 0.34] for only 1 selective). Triangles indicate the means. <bold>E.</bold> The start-target tuning correlation in warped time windows aligned to the visual cue, the go sound and the arrival, averaged across neurons with both start and target selectivity (n = 174). The white contours indicate the areas where correlation is significantly larger than 0 (<italic>p</italic> &lt; 0.05 with Bonferonni correction). Different from C and D, these correlations were calculated between start tuning in half of the trials and target tuning in the other half of trials, and vice versa, then took the average. <bold>F.</bold> Similar to E, but for the mean Pearson correlation between pairs of time windows for start position tuning in one half of trials versus the other half. <bold>G.</bold> Similar to F, but for target position tuning. <bold>H.</bold> Time of transition from start position coding to target position coding in single neurons. The color of the heat-map indicate the difference between <italic>R</italic><sup>2</sup>s of the start position GLM and the target position GLM, calculated at each 300 ms time window aligned to the go sound. Each row was a neuron (n = 174). The red dots indicate the time of switching from <inline-formula><mml:math id="M18"><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> higher to <inline-formula><mml:math id="M19"><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> higher (see Methods for details). The white crosses indicate the averaged time of target poke for that session. <bold>I.</bold> The number of neurons preferring each start position in the “pre-cue” time window, among cells that had significant start position selectivity in the “pre-cue” window (p &lt; 0.01, permutation test for the start position GLM). <bold>J.</bold> The number of cells preferring each target position in the “arrival” time window, among cells that have significant target position selectivity in that window (p &lt; 0.01, permutation test for the target position GLM).</p></caption><graphic xlink:href="EMS157131-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Direction preference of FOF neurons.</title><p><bold>A.</bold> The preferred direction of 274 cells that had significant direction selectivity in either the “post-cue” or “go” time windows (p &lt; 0.01, permutation test for the GLM), in sliding windows aligned to the go sound (50 ms per bin, 300 ms bin size). The alignment was causal, i.e. time 0 indicated -300 to 0 ms. The color indicates the prefered angle (as in B.) and the saturation indicates the relative amplitude of the <italic>R</italic><sup>2</sup> of the direction GLM, and the neurons were sorted by the preferred direction in the “go” time window. <bold>B.</bold> The distribution of preferred direction in the “go” time window (0 to 300 ms after the go sound). <bold>C.</bold> Pearson correlation of direction tuning curves at one time versus another, among same neurons as in A and B. Color indicate the mean correlation across these neurons. White contour indicates the area with where correlation was significantly larger than zero with Bonferroni correction.</p></caption><graphic xlink:href="EMS157131-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><title>The spatial selectivity of FOF neurons were best explained by the gain field model.</title><p><bold>A.</bold> Raster plots and PETHs of an example neuron. Trials were grouped by the start position (left panel), the direction (middle panel) and the target position (right panel). Spikes were aligned to the visual cue onset. PETHs were generated by a causal half-Gaussian Kernel with an SD of 400 ms. The gray bar at the bottom indicates the 500 ms time window after the visual cue onset, the time window used for other panels in this figure. <bold>B.</bold> Estimated mean firing rate in each movement trajectory using a maximum a posterior estimator, same as in <xref ref-type="fig" rid="F2">Figure 2</xref>. <bold>C.</bold> Predicted firing rates of four fit models (lines) and the mean and s.e.(circles and error bars) of firing rates in each trial condition. <italic>CV R</italic><sup>2</sup>, cross-validated <italic>R</italic><sup>2</sup>. <bold>D.</bold> Left axis, fraction of neurons best fit by the model, among neurons whose mean cross-validated <italic>R</italic><sup>2</sup>s over the 4 models was larger than the x-axis indicated value. Best fit was defined as having the largest cross-validated <italic>R</italic><sup>2</sup> among the 4 models. Error bars were 95% confidence intervals of the binomial distribution. Right axis, neurons that crossed the mean <italic>CV R</italic><sup>2</sup> criteria for each x-axis value. **, <italic>p</italic> &lt; 0.001. *, <italic>p</italic> &lt; 0.05. <bold>E.</bold> Each panel plots the cross-validated <italic>R</italic><sup>2</sup>s of the x-axis model versus the y-axis model. Each circle indicates a neuron. The red line indicates the total least square fit to the data. The dashed black line marks the diagonal. All single units were included in the analysis (n = 1224), but each panel only showed neurons whose mean of cross-validated <italic>R</italic><sup>2</sup>s in the 4 models were larger than 0.05 (n = 199). Statistical test and total least square fitting were also based on these neurons. P values indicates the significance against the null hypothesis that the difference between the Fisher-z transformed cross-validated <italic>R</italic><sup>2</sup>s in the x-axis model and the y-axis model was not significantly different from zero (permutation test of the mean). The mean <italic>R</italic><sup>2</sup> in the x-axis model was larger than the y-axis model in all of these panels.</p></caption><graphic xlink:href="EMS157131-f006"/></fig><fig id="F7" position="float"><label>Figure 7</label><caption><title>FOF population activity is most similar to a recurrent network with self-centered input and self-centered output.</title><p><bold>A.</bold> The input and output of an example trial in different networks. The start port (cyan) and the target port (yellow) coordinates are relative to the light-blue bounding box (the world frame). The movement vector is the displacement between the start port and the target port. The start port is always at the center of the image for ego-** networks, and the world frame is always at the center of the image for allo-** netowrks. <bold>B.</bold> The <italic>R</italic><sup>2</sup>s between the decoded and the actual spatial variables. Shaded area denoted the s.e. over 20 training and testing epochs. The ego-ego network had the most similar temporal pattern of decoding accuracy as the FOF neurons. <bold>C.</bold> The representational similarity between the FOF neural population activity and the population activity in recurrent neural networks. The pre-cue, post-cue and go time windows in the networks were the 3rd, 7th and 11th time frames, respectively. The start position was visible throughout the 11 frames, and the target position (in the relevant reference frame) was visible briefly at the 4th frame. The representation similarity was computed with the the first 4 principle components of pseudopopulation activity in the FOF neural data and the network hidden unit data. <bold>D.</bold> The representation similarity between the FOF neural population activity and the network hidden unit activity during corresponding time windows. Circles and error bars were mean and s.d. from 20 training and testing epochs. The means were the same as the diagonal elements in figure C. Representation similarity of the ego-ego models were significantly higher than other models in post-cue and go windows(<xref ref-type="table" rid="T2">Table 2</xref>). <bold>E.</bold> Example units in the four networks that had gain-field-like selectivity to start position and direction. Each dot is a trial and each line is the average over trials with the same start position which is indicated by color.</p></caption><graphic xlink:href="EMS157131-f007"/></fig><table-wrap id="T1" position="float" orientation="portrait"><label>Table 1</label><caption><title>Difference of the Fisher-transformed <italic>CV R</italic><sup>2</sup>s between model pairs for neurons whose mean <italic>CV R</italic><sup>2</sup> &gt; 0.05 for all four models.</title><p>Permutation test for the null hypothesis that <italic>atanh</italic>(<italic>Model</italic>1) − <italic>atanh</italic>(<italic>Model</italic>2) = 0 with 10<sup>5</sup> permutations.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle"><italic>M</italic><sub>1</sub></th><th align="left" valign="middle"><italic>M</italic><sub>2</sub></th><th align="left" valign="middle"><italic>p</italic> value</th><th align="left" valign="middle"><italic>atanh</italic>(<italic>Model</italic>1) - <italic>atanh</italic>(<italic>Model</italic>2) Mean, [95% CI]</th></tr></thead><tbody><tr><td align="left" valign="middle">Gain field</td><td align="left" valign="middle">Additive</td><td align="left" valign="middle">2 × 10<sup>−5</sup></td><td align="left" valign="middle">0.0129, [0.0073, 0.0229]</td></tr><tr><td align="left" valign="middle">Gain field</td><td align="left" valign="middle">Direction</td><td align="left" valign="middle">2 × 10<sup>−5</sup></td><td align="left" valign="middle">0.0617, [0.0544, 0.0774]</td></tr><tr><td align="left" valign="middle">Gain field</td><td align="left" valign="middle">Start</td><td align="left" valign="middle">2 × 10<sup>−5</sup></td><td align="left" valign="middle">0.0753, [0.0650, 0.0960]</td></tr><tr><td align="left" valign="middle">Additive</td><td align="left" valign="middle">Direction</td><td align="left" valign="middle">2 × 10<sup>−5</sup></td><td align="left" valign="middle">0.0488, [0.0430, 0.0588]</td></tr><tr><td align="left" valign="middle">Additive</td><td align="left" valign="middle">Start</td><td align="left" valign="middle">2 × 10<sup>−5</sup></td><td align="left" valign="middle">0.0624, [0.0529, 0.0775]</td></tr><tr><td align="left" valign="middle">Direction</td><td align="left" valign="middle">Start</td><td align="left" valign="middle">0.02</td><td align="left" valign="middle">0.0136, [0.0025, 0.0259]</td></tr></tbody></table></table-wrap><table-wrap id="T2" position="float" orientation="portrait"><label>Table 2</label><caption><title>Comparing the representational similarity for the ego-ego model versus the other three models.</title><p>Welch’s t test for the alternative hypothesis that the mean of the representational similarity between the ego→ego model and real neurons were higher than those between another model and real neurons.</p></caption><table frame="hsides" rules="rows"><thead><tr><th align="left" valign="middle"/><th align="left" valign="middle">pre-cue</th><th align="left" valign="middle">post-cue</th><th align="left" valign="middle">go</th></tr></thead><tbody><tr><td align="left" valign="top"><bold>ego-ego vs. ego-allo</bold></td><td align="left" valign="top"><italic>t</italic>(38.00) = 1.05<break/><italic>p</italic> = 0.150</td><td align="left" valign="top"><italic>t</italic>(37.34) = 2.02<break/><italic>p</italic> = 0.025</td><td align="left" valign="top"><italic>t</italic>(32.95) = 4.69<break/><italic>p</italic> = 2.29 × 10<sup>−5</sup></td></tr><tr><td align="left" valign="top"><bold>ego-ego vs. allo-ego</bold></td><td align="left" valign="top"><italic>t</italic>(37.90) = 1.02<break/><italic>p</italic> = 0.156</td><td align="left" valign="top"><italic>t</italic>(35.03) = 7.96<break/><italic>p</italic> = 1.14 × 10<sup>−9</sup></td><td align="left" valign="top"><italic>t</italic>(36.79) = 2.11<break/><italic>p</italic> = 0.021</td></tr><tr><td align="left" valign="top"><bold>ego-ego vs. allo-allo</bold></td><td align="left" valign="top"><italic>t</italic>(38.00) = 2.86<break/><italic>p</italic> = 0.003</td><td align="left" valign="top"><italic>t</italic>(29.77) = 9.19<break/><italic>p</italic> = 1.68 × 10<sup>−10</sup></td><td align="left" valign="top"><italic>t</italic>(26.86) = 18.41<break/><italic>p</italic> = 4.65 × 10<sup>−17</sup></td></tr></tbody></table></table-wrap></floats-group></article>