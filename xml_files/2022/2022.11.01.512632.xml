<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS156585</article-id><article-id pub-id-type="doi">10.1101/2022.11.01.512632</article-id><article-id pub-id-type="archive">PPR565746</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Neural basis of melodic learning explains cross-cultural regularities in musical scales</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Pelofi</surname><given-names>Claire</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="fn" rid="FN1">†</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Rezaeizadeh</surname><given-names>Mohsen</given-names></name><xref ref-type="aff" rid="A3">3</xref><xref ref-type="fn" rid="FN1">†</xref></contrib><contrib contrib-type="author"><name><surname>Farbood</surname><given-names>Morwaread M.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Shamma</surname><given-names>Shihab</given-names></name><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref></contrib></contrib-group><aff id="A1"><label>1</label>Center for Language, Music and Emotion, New York University-Max Planck Institute</aff><aff id="A2"><label>2</label>Music and Audio Research Laboratory, New York University, New York, NY, USA</aff><aff id="A3"><label>3</label>Department of Electrical and Computer Engineering and Institute for Systems Research, University of Maryland, College Park</aff><aff id="A4"><label>4</label>Laboratoire des Systèmes Perceptifs, Département d’Études Cognitives, École Normale Supérieure, PSL University, Paris, France</aff><aff id="A5"><label>5</label>Department of Music and Performing Arts Professions, New York University, New York, NY, USA</aff><author-notes><fn id="FN1" fn-type="equal"><label>†</label><p id="P1">These authors have contributed equally to this work and share first authorship</p></fn><corresp id="CR1">
<label>*</label>Corresponding Authors: <email>claire.pelofi@nyu.edu</email>
</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>04</day><month>11</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>01</day><month>11</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><title>Summary</title><p id="P2">Seeking exposure to unfamiliar experiences constitutes an essential aspect of the human condition, and the brain must adapt to the constantly changing environment by learning the evolving statistical patterns emerging from it. Cultures are shaped by norms and conventions and therefore novel exposure to an unfamiliar culture induces a type of learning that is often described as implicit: when exposed to a set of stimuli constrained by unspoken rules, cognitive systems must rapidly build a mental representation of the underlying grammar. Music offers a unique opportunity to investigate this implicit statistical learning, as sequences of tones forming melodies exhibit structural properties learned by listeners during short-and long-term exposure. Understanding which specific structural properties of music enhance learning in naturalistic learning conditions reveals hard-wired properties of cognitive systems while elucidating the prevalence of these features across cultural variations. Here we provide behavioral and neural evidence that the prevalence of non-uniform musical scales may be explained by their facilitating effects on melodic learning. In this study, melodies were generated using an artificial grammar with either a uniform (rare) or non-uniform (prevalent) scale. After a short exposure phase, listeners had to detect ungrammatical new melodies while their EEG responses were recorded. Listeners’ performance on the task suggested that the extent of statistical learning during music listening depended on the musical scale context: non-uniform scales yielded better syntactic learning. This behavioral effect was mirrored by enhanced encoding of musical syntax in the context of non-uniform scales, which further suggests that their prevalence stems from fundamental properties of learning.</p></abstract><kwd-group><kwd>Statistical learning</kwd><kwd>Music universals</kwd><kwd>Cortical tracking</kwd><kwd>Musical scales</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="results | discussion"><title>Results and discussion</title><p id="P3">Music—like language—is produced in all known human cultures [<xref ref-type="bibr" rid="R1">1</xref>; <xref ref-type="bibr" rid="R2">2</xref>; <xref ref-type="bibr" rid="R3">3</xref>] and displays varying structural norms across cultures [<xref ref-type="bibr" rid="R4">4</xref>; <xref ref-type="bibr" rid="R5">5</xref>; <xref ref-type="bibr" rid="R6">6</xref>; <xref ref-type="bibr" rid="R7">7</xref>] as well as some structural stability [<xref ref-type="bibr" rid="R8">8</xref>; <xref ref-type="bibr" rid="R9">9</xref>; <xref ref-type="bibr" rid="R10">10</xref>]. The balance between diversity and stability in cross-cultural musical structure may be governed by general principles of cultural evolution [<xref ref-type="bibr" rid="R11">11</xref>; <xref ref-type="bibr" rid="R12">12</xref>; <xref ref-type="bibr" rid="R13">13</xref>], although there is still limited empirical evidence supporting this hypothesis [<xref ref-type="bibr" rid="R14">14</xref>; <xref ref-type="bibr" rid="R15">15</xref>]. Yet, gaining insight into the way universal structures are selected may shed considerable light on the underlying cognitive processes [<xref ref-type="bibr" rid="R16">16</xref>; <xref ref-type="bibr" rid="R15">15</xref>; <xref ref-type="bibr" rid="R17">17</xref>; <xref ref-type="bibr" rid="R14">14</xref>]. This study is dedicated to examining the cause behind the prevalence of a specific musical feature, the non-uniformity of musical scales, in light of its potential role in promoting statistical learning of melodies (given that scales directly affect the way the pitches of notes are organized to form melodies). Cross-cultural studies have demonstrated that listeners’ responses to music reflect their own long-term cultural exposure [<xref ref-type="bibr" rid="R18">18</xref>; <xref ref-type="bibr" rid="R19">19</xref>; <xref ref-type="bibr" rid="R20">20</xref>; <xref ref-type="bibr" rid="R21">21</xref>; <xref ref-type="bibr" rid="R22">22</xref>; <xref ref-type="bibr" rid="R7">7</xref>; <xref ref-type="bibr" rid="R23">23</xref>; <xref ref-type="bibr" rid="R24">24</xref>]. These are echoed by other studies reporting similar results but after only short-term exposure to an unfamiliar musical system [<xref ref-type="bibr" rid="R15">15</xref>; <xref ref-type="bibr" rid="R25">25</xref>; <xref ref-type="bibr" rid="R26">26</xref>; <xref ref-type="bibr" rid="R27">27</xref>; <xref ref-type="bibr" rid="R28">28</xref>]. In either case, this line of research has demonstrated that upon musical exposure, listeners implicitly acquire the structural properties of their exposure corpus [<xref ref-type="bibr" rid="R29">29</xref>; <xref ref-type="bibr" rid="R30">30</xref>; <xref ref-type="bibr" rid="R31">31</xref>]. A recent behavioral study provided further evidence that short-term grammar learning is enhanced in the context of non-uniform musical scales [<xref ref-type="bibr" rid="R15">15</xref>], suggesting that their striking prevalence across musical cultures [<xref ref-type="bibr" rid="R10">10</xref>] may stem from deeper cognitive principles of learning [<xref ref-type="bibr" rid="R32">32</xref>; <xref ref-type="bibr" rid="R33">33</xref>].</p><p id="P4">Building on the same design, the present study establishes that these facilitating effects originate from enhanced neural encoding of melodies derived from non-uniform scales. Non-uniform scales, such as the pentatonic scale and Western diatonic scales, are characterized by a specific positioning of the tones along the octave that create a unique set of relationships among them. This organization promotes the perception of a tonal hierarchy, a key aspect for melodic processing [<xref ref-type="bibr" rid="R32">32</xref>; <xref ref-type="bibr" rid="R33">33</xref>]. In contrast, uniform scales, which are very rarely observed [<xref ref-type="bibr" rid="R10">10</xref>; <xref ref-type="bibr" rid="R34">34</xref>], are characterized by an even distribution of tones around the octave, resulting in an entirely non-specified tonal space [<xref ref-type="bibr" rid="R15">15</xref>].</p><p id="P5">Here, statistical learning of melodies was investigated in the context of a uniform and non-uniform scale, schematically represented in <xref ref-type="fig" rid="F1">Figure 1.A</xref>. To generate statistical properties governing melodic syntax, an artificial first-order grammar (<xref ref-type="fig" rid="F1">Figure 1.B</xref>) was used to generate melodies. First, in an exposure phase, listeners were presented with a corpus of 100 <italic>reference</italic> melodies generated from the original grammar. In the following test phase, half of the presented melodies were from the original grammar (i.e. <italic>reference</italic> melodies), while the other half contained wrong (out-of-grammar) transitions that were inserted to induce syntactic violations in the melodies (i.e. <italic>alternative</italic> melodies). Listeners had to report whether each of the melodies sounded familiar with respect to what they heard in the exposure phase. This procedure was repeated two times, once for each scale condition. Details of the experimental setup, subjects, and stimuli are provided in <xref ref-type="sec" rid="S8">Methods</xref> section.</p><sec id="S2"><title>Behavioral results reveal learning benefits for non-uniform scales</title><p id="P6">Following the exposure phase, listeners were presented with either a reference or an alternative (i.e. containing incorrect transitions) melodies. As a way of probing syntactic learning, participants were asked to report whether or not the melody they heard in each trial sounded familiar with respect to what they heard in the exposure phase [<xref ref-type="bibr" rid="R15">15</xref>; <xref ref-type="bibr" rid="R35">35</xref>]. Mean <italic>d</italic>′ values for both scales were computed to assess performance and are shown in Figure <xref ref-type="fig" rid="F1">Figure 1.C</xref>. Performance was significantly higher when melodies were generated with the non-uniform scale (paired <italic>t</italic>-test, <italic>p</italic> = 0.023). Over the course of the test phase, listeners were presented with more and more incorrect transitions in the melodies, which could potentially alter the representation of the grammar they acquired throughout the exposure. To account for this expected drift in performance over time, mean <italic>d</italic>′ values were averaged across listeners for evenly divided sets of trials over time (first set: trials 1-27, middle set: trials 28-55 and last set: trials 56-80, <xref ref-type="fig" rid="F1">Figure 1.D</xref>). For both scales, a significant drift in performance over time was observed (two-way repeated-measure ANOVA: <italic>F</italic>(14, 2) = 4.75, <italic>p</italic> = 0 . 017 ).</p></sec><sec id="S3"><title>Neural correlates of melodic learning are modulated by scale conditions</title><p id="P7">To investigate the neural basis of this behavioral effect, we sought to probe the difference in grammar encoding for uniform and non-uniform scales using multivariate pattern analysis applied to neurophysiological data [<xref ref-type="bibr" rid="R36">36</xref>; <xref ref-type="bibr" rid="R37">37</xref>; <xref ref-type="bibr" rid="R38">38</xref>], a method that has advanced the understanding of spatio-temporal neural activations related to stimulus processing [<xref ref-type="bibr" rid="R39">39</xref>; <xref ref-type="bibr" rid="R40">40</xref>; <xref ref-type="bibr" rid="R41">41</xref>; <xref ref-type="bibr" rid="R42">42</xref>; <xref ref-type="bibr" rid="R43">43</xref>]. The idea is to train a set of linear classifiers to classify sets of neural data collected under different conditions. This provides insight into how the topographical maps collected with EEG sensors display a pattern of activity can discriminate between stimulus features [<xref ref-type="bibr" rid="R36">36</xref>; <xref ref-type="bibr" rid="R44">44</xref>; <xref ref-type="bibr" rid="R45">45</xref>; <xref ref-type="bibr" rid="R46">46</xref>], and also reveals how this discrimination evolves over time [<xref ref-type="bibr" rid="R36">36</xref>; <xref ref-type="bibr" rid="R47">47</xref>; <xref ref-type="bibr" rid="R48">48</xref>; <xref ref-type="bibr" rid="R46">46</xref>; <xref ref-type="bibr" rid="R41">41</xref>].</p><p id="P8">Here, we trained a set of linear classifiers to discriminate between the reference melodies (i.e. those generated with the same grammar as during the exposure phase) and the alternative melodies (i.e. those containing syntactic violations). We first used the EEG signal collected over the entire melody to probe the temporal dynamics of neural representation across the whole duration of the melodies. The method consisted in training a set of independent logistic regressors to discriminate signals using all 64 sensors as detailed in the <xref ref-type="sec" rid="S8">Methods</xref> section . The classifiers were trained to linearly separate the two melodic conditions (i.e. reference versus alternative) based on the EEG topographic maps at different time points. In addition, we probed the time generalization dynamics [<xref ref-type="bibr" rid="R36">36</xref>]: the classifiers were trained on the EEG response of 6 seconds of melodies at each time point <italic>t</italic> and tested at time <italic>t</italic>′, where <italic>t</italic> and <italic>t</italic>′ were different time points sampled over the time-window that spanned the entire duration of the melodies (0 to 6 seconds). In order to investigate neural representation of reference and alternative melodies from uniform and non-uniform scales, the analysis was conducted separately using the data-sets collected during each test phase of both scale conditions.</p><p id="P9"><xref ref-type="fig" rid="F2">Figure 2.A</xref> illustrates the time generalization dynamics of decoders’ performance for the melodies generated from the uniform (left) and non-uniform (right) scales. The classifiers’ scores were significantly above chance level for the non-uniform scale (<italic>p</italic><sub><italic>min</italic></sub> &lt; 0.05) between 3 and 5 seconds after onset of the first tone. This demonstrates that listeners could learn the unfamiliar and artificial musical grammar from melodies generated in this scale. The pattern of the significance region (right) suggests a temporally jittered activity due to the small variations in the emergence of the effects across subjects [<xref ref-type="bibr" rid="R36">36</xref>]. Conversely, the classifiers could not discriminate between the reference and alternative melodies when melodies were generated using the uniform scales (<italic>p</italic><sub><italic>min</italic></sub> &gt; 0.05). To simplify the visualization, we directly compared the diagonal scores in non-uniform and uniform scales (i.e., the scores for which testing and training data were synchronized); we observed a statistical difference between the classifier performances, in which the area under the receiver operating characteristic curve (AUC) was significantly larger for the non-uniform scale at around [3-5] seconds following the melody onsets (<xref ref-type="fig" rid="F2">Figure 2.B</xref>).</p><p id="P10">To directly test whether the enhanced decoding accuracy was related to behavioral performances, we computed the correlation between the classifier accuracy and the <italic>d</italic>′ values at the individual level. For the decoding scores, we averaged over the time window in which there were scores significantly above chance level (3-5 seconds) and found a significant correlation for the non-uniform scale (<italic>r</italic> = 0.75, <italic>p</italic> &lt; 0.01) as seen in <xref ref-type="fig" rid="F2">Figure 2.C</xref>. These results suggest a direct link between enhanced performance of the classifiers and behavioral performance on the task.</p></sec><sec id="S4"><title>The Neural Encoding of correct <italic>vs</italic> incorrect note transitions within the two scale contexts</title><p id="P11">In the previous analysis, the decoding accuracy rose above chance level at about 3 seconds after melody onsets, which is on average the time when the first non-syntactic notes from the alternative grammar are played, as detailed in the <xref ref-type="sec" rid="S8">Methods</xref> section . To investigate more directly the encoding of note transitions in the two scale contexts, we applied a set of linear classifiers to decode the neural data specifically collected during correct and incorrect transitions. Informed by the drift in behavioral performance for both scales (see <xref ref-type="fig" rid="F1">Figure 1.D</xref>), we divided the neural data into the corresponding time periods prior to training classifiers. As in the earlier findings, this analysis revealed that decoding scores were significantly above chance only for the non-uniform scale, starting at 350 ms after the onset of the tones (<italic>p</italic> &lt; 0.05) and only for the first set of trials, due to the drift in performance over time. By contrast, the classifier accuracy remained near chance levels for all the trial sets of data collected under the uniform scale condition.</p></sec><sec id="S5"><title>Scale-dependent syntactic processing revealed by the evoked responses of musical notes</title><p id="P12">In the previous analysis, scale effects were probed by comparing differences in decoding accuracy between reference versus alternative melodies, and by examining the topographic maps of EEG signal collected for melodies under both scale conditions. A more direct evaluation of the neural responses elicited by different scales is to compute the evoked responses time-locked to musical events in the melodies. “evoked responses” refer to the average from many repetitions of the neural response elicited by a specific stimulus event. This averaging removes noise and extracts specific negative or positive amplitude peaks directly related to stimulus processing. Here, we sought to investigate the evoked responses that are time-locked to correct and incorrect transitions, and hence gain insights into the type of processes at play under the two scale conditions. Because of the relatively small number of repetitions applied in each condition, we first denoised the data using <bold>D</bold>enoised <bold>S</bold>ource <bold>S</bold>eparation (DSS) [<xref ref-type="bibr" rid="R49">49</xref>] as detailed in the Methods section.</p><p id="P13">This algorithm consists of selecting components of the neural signals that are most repeatable across stimulus repetitions and therefore likely to reflect stimulus processing instead of noise. The components obtained are then projected back onto the sensor space, resulting in a clean EEG signal for each of the 64 channels. We used the DSS-denoised signal from Cz (the electrode situated on the mid-line sagittal plane center, as typically done for the analysis of auditory components) and time-locked between -100-400 ms to notes in the alternative melodies with incorrect transitions, i.e. potentially exhibiting a syntactic violation to the exposure grammar. As seen earlier, we took into account the drift over time in performance by dividing the neural data into the corresponding time periods. The resulting DSS-denoised evoked responses are plotted in <xref ref-type="fig" rid="F4">Figure 4.A</xref>. A bootstrap re-sampling conducted on evoked responses from correct and incorrect transitions revealed that the two signals were significantly different <italic>only</italic> for the non-uniform scale and in the first time-window (see <xref ref-type="sec" rid="S8">Methods</xref> section for details on the statistical analysis).</p><p id="P14">The evoked response of incorrect transitions for the non-uniform scale revealed a larger negative component between 200 and 400 ms after note onset. This late response elicited by an ungrammatical event is strongly evocative of the well-known ERAN component associated with syntactic processing in the context of music [<xref ref-type="bibr" rid="R50">50</xref>; <xref ref-type="bibr" rid="R51">51</xref>; <xref ref-type="bibr" rid="R52">52</xref>; <xref ref-type="bibr" rid="R53">53</xref>; <xref ref-type="bibr" rid="R54">54</xref>] and also referred as the “musical MMN component” [<xref ref-type="bibr" rid="R55">55</xref>].</p></sec><sec id="S6"><title>Global temporal and topographic characterization of the neural correlates of different scales</title><p id="P15">Finally, we examined the general sensor topography of grammar encoding under different scale conditions. For that, a Temporal Response Function (TRF) analysis [<xref ref-type="bibr" rid="R56">56</xref>] was conducted on the neural data collected for the uniform and non-uniform scales. The TRF is a decoding technique used to account for the neural encoding of continuous stimulus features such as envelope, semantics, or phoneme for speech [<xref ref-type="bibr" rid="R57">57</xref>; <xref ref-type="bibr" rid="R58">58</xref>; <xref ref-type="bibr" rid="R59">59</xref>; <xref ref-type="bibr" rid="R60">60</xref>] and envelope and syntax for music [<xref ref-type="bibr" rid="R61">61</xref>; <xref ref-type="bibr" rid="R62">62</xref>]. We used the -log probability of note transitions between the notes estimated for each melody (i.e., the syntactic structure) based on the reference grammar, which is very similar to the <italic>surprisal signal</italic> [<xref ref-type="bibr" rid="R63">63</xref>], a computation that provides good measures for interpretation of perceptual data [<xref ref-type="bibr" rid="R64">64</xref>; <xref ref-type="bibr" rid="R65">65</xref>]. Regressing surprisal from neural signal using TRF has been shown to yield significant cortical tracking of syntactic structure during melodic processing [<xref ref-type="bibr" rid="R62">62</xref>; <xref ref-type="bibr" rid="R66">66</xref>; <xref ref-type="bibr" rid="R67">67</xref>].</p><p id="P16">The TRF is essentially a kernel that describes the linear mapping of the stimulus into the neural response using ridge regression. The kernel is fit to minimize the mean-squared error between the actual neural response and the predicted neural response. The encoding index is then assessed using a cross-validated evaluation using <italic>r</italic> Pearson’s correlations between the predicted and actual neural responses for each individual channel. Details of the TRF analysis are provided in the <xref ref-type="sec" rid="S8">Methods</xref>. section . In brief, this analysis provides an index of how well the grammatical structure of the melodies is represented at each EEG sensor, thus revealing the general topography of the neural processing of syntax under both scale conditions.</p><p id="P17">A TRF was computed for each data set and each condition and trained on trials in the test phase. Then, Pearson’s correlations for only the first third of trials were averaged over participants and plotted in <xref ref-type="fig" rid="F4">Figure 4.B</xref>. The topographies reveal that for both scale conditions, the electrodes tracking the syntactic information are situated in the central region. They also reveal a significant effect of scale conditions (cluster-based permutation test, 2,000 iterations <italic>p</italic> &lt; 0.01) observed on the centro-lateral electrodes. This is consistent with earlier findings of syntactic processing in the context of music listening that report higher correlation with surprisal in these electrode regions [<xref ref-type="bibr" rid="R62">62</xref>; <xref ref-type="bibr" rid="R66">66</xref>; <xref ref-type="bibr" rid="R67">67</xref>], which further confirms that the main effect of scale condition is driven by differences in the grammatical learning of the unfamiliar melodic corpus [<xref ref-type="bibr" rid="R15">15</xref>].</p></sec></sec><sec id="S7" sec-type="conclusions"><title>Conclusion</title><p id="P18">Music is a universal feature of human societies but displays tremendous variability in its rhythmic, timbral, and pitch structure [<xref ref-type="bibr" rid="R4">4</xref>; <xref ref-type="bibr" rid="R5">5</xref>; <xref ref-type="bibr" rid="R10">10</xref>]. Yet converging findings from large datasets of musical production suggest that the musical world exhibits some quasi-universal structural traits [<xref ref-type="bibr" rid="R34">34</xref>; <xref ref-type="bibr" rid="R68">68</xref>; <xref ref-type="bibr" rid="R10">10</xref>] that could reveal common underlying cognitive properties in the processing of musical signals. In particular, musical scales with a non-uniform (asymmetric) tonal structure (i.e. note sequences separated by intervals of varying sizes) are far more prevalent than uniform ones across musical systems [<xref ref-type="bibr" rid="R34">34</xref>; <xref ref-type="bibr" rid="R10">10</xref>]. This motivates the hypothesis that these differences stem from the cognitive benefits of asymmetric scales in melodic learning [<xref ref-type="bibr" rid="R32">32</xref>; <xref ref-type="bibr" rid="R33">33</xref>; <xref ref-type="bibr" rid="R69">69</xref>].</p><p id="P19">Recent findings have further supported this hypothesis. For instance, a recent behavioral study highlighted the benefit conveyed by non-uniform scales for the learning of unfamiliar syntactic rules [<xref ref-type="bibr" rid="R15">15</xref>]. Non-uniform scales were found to enhance performance on a syntactic violation task within the context of familiar and unfamiliar musical systems. The benefit may originate from an enhanced internal representation of the tonal space in which relations among tones are better delineated by a non-uniform scale [<xref ref-type="bibr" rid="R32">32</xref>]. This study explored the neural correlates and origins of this preference by associating the behavioral effects of enhanced performance with simultaneous measurements of the neural responses while subjects learned melodies generated within uniform and non-uniform scales [<xref ref-type="bibr" rid="R15">15</xref>].</p><p id="P20">The behavioral and neural data were collected during a test phase in which, after only a short exposure to reference melodies, listeners’ ability to learn an artificial musical grammar was probed through their performance in detecting syntactic errors in alternative versus reference melodies. The results confirmed that the superior learning for melodies generated with the non-uniform scale—as indicated by higher <italic>d</italic>′ values—was paralleled by an enhanced neural encoding of that grammar. The latter findings were assessed by linear classifiers that better discriminated the topographical maps of EEG activation when the melodies were generated from the non-uniform scale. Finally, evoked responses for incorrect and correct transitions were also modulated by the scale condition: in the case of non-uniform scales, a late negative component evocative of the ERAN [<xref ref-type="bibr" rid="R55">55</xref>; <xref ref-type="bibr" rid="R53">53</xref>; <xref ref-type="bibr" rid="R54">54</xref>] was observed for incorrect transitions embedded in alternative melodies of the non-uniform scale.</p><p id="P21">In summary, we have demonstrated that the twos type of melodies, alternative and reference, were better represented in the neural data when they were derived from the non-uniform scale, and that this was correlated with behavioral performance at the individual level. Further analyses of temporal and spatial characteristics of the EEG responses confirmed that the effect of scale type was driven by a more efficient syntactic processing in the context of the non-uniform scale.</p><p id="P22">Humans spontaneously seek musical experiences for pleasure [<xref ref-type="bibr" rid="R70">70</xref>; <xref ref-type="bibr" rid="R71">71</xref>; <xref ref-type="bibr" rid="R72">72</xref>], arguably in search of social bonding that reinforces human survival and reproduction [<xref ref-type="bibr" rid="R73">73</xref>; <xref ref-type="bibr" rid="R74">74</xref>; <xref ref-type="bibr" rid="R75">75</xref>; <xref ref-type="bibr" rid="R76">76</xref>; <xref ref-type="bibr" rid="R77">77</xref>; <xref ref-type="bibr" rid="R78">78</xref>; <xref ref-type="bibr" rid="R79">79</xref>; <xref ref-type="bibr" rid="R80">80</xref>]. This highlights the relevance of music production and learning for cultural evolutionary theory [<xref ref-type="bibr" rid="R81">81</xref>]. Numerous and converging studies demonstrate that humans learn the musical regularities of their own cultural background [<xref ref-type="bibr" rid="R18">18</xref>; <xref ref-type="bibr" rid="R19">19</xref>; <xref ref-type="bibr" rid="R20">20</xref>; <xref ref-type="bibr" rid="R23">23</xref>] from a very early age [<xref ref-type="bibr" rid="R82">82</xref>; <xref ref-type="bibr" rid="R83">83</xref>], throughout life [<xref ref-type="bibr" rid="R26">26</xref>; <xref ref-type="bibr" rid="R27">27</xref>; <xref ref-type="bibr" rid="R28">28</xref>], and in the absence of explicit instructions, much like they learn their mother tongue [<xref ref-type="bibr" rid="R84">84</xref>]. In this perspective, musical features could be selected through evolutionary processes to enhance music learning and thus favor common pleasurable experiences and social bonding [<xref ref-type="bibr" rid="R74">74</xref>; <xref ref-type="bibr" rid="R85">85</xref>; <xref ref-type="bibr" rid="R81">81</xref>].</p><p id="P23">This study provides evidence of the facilitating effects in learning unfamiliar music in the context of a scale structure that is most prevalent across musical structures. This behavioral effect was paralleled by neural findings showing better neural encoding of melodies, most likely related to enhanced syntactic processing. Altogether, these results bring strong new evidence that cross-cultural universals in the music domain reveal cognitive principles of auditory processing.</p></sec><sec id="S8" sec-type="methods"><title>Method</title><sec id="S9" sec-type="subjects"><title>Participants</title><p id="P24">Sixteen adult participants with self-reported normal-hearing participated in this study, conducted at the University of Maryland. One participant was removed from the analysis for not keeping the earphones in place during the experimental procedure. Among the fifteen remaining participants (9 females, mean age = 25 years, <italic>SD</italic> = 8), two had five or more years of formal musical training and all were still engaged in daily musical practice. All participants were given course credits or monetary compensation for their participation. The experimental procedures were approved by the University of Maryland Institutional Review boards. Written informed consent was obtained from each subject before the experiment.</p></sec><sec id="S10"><title>Scale</title><p id="P25">Participants were presented with melodies generated from hexatonic scales in the two following structure conditions: uniform and non-uniform. Each scale was composed of six tones in 12-tone equal temperament (12-TET). The two scale conditions were obtained by positioning 12-TET tones in a manner that conformed to the different intervallic structural properties, as illustrated in <xref ref-type="fig" rid="F1">Figure 1.A</xref>. The uniform scale was composed of intervals (i.e., space in between the pitch of subsequent tones) of equal sizes. In contrast, the non-uniform scale was composed of intervals of different sizes so that each tone had a unique set of intervallic relations with all of the others tones when moving from one tone to another in the same direction (clockwise/counter-clockwise or up/down) across the octave span.</p></sec><sec id="S11"><title>Grammar</title><p id="P26">Melodies were composed of the tones within a given scale, and their construction was determined by a first-order Markov chain inspired by Rohrmeier et al. [<xref ref-type="bibr" rid="R86">86</xref>]. Since two scale types were used (hexatonic, 12-TET), two different grammars were used as well, each including all the tones in the scales. However, the complexity of both grammars was kept constant with respect to the transition probabilities that were used to generate the melodies. Schematic representations of the grammar are shown in <xref ref-type="fig" rid="F1">Figure 1.B</xref>. Each node corresponds to a tone in the scale. The correspondence between nodes and notes was randomized for each participant and for each structure condition. Arrows connecting nodes determine the permissible transitions between notes, along with the probability of transition. The “reference” version of the grammar was determined for each listener prior to the exposure phase. The “alternative” version of the grammar was obtained by switching nodes 3-4 and 5-6, which introduced 10 possible wrong transitions. Melodies generated with the alternative grammar contained a set of three transitions between tones that were never part of the melodies generated with the reference grammar.</p></sec><sec id="S12"><title>Melodies</title><p id="P27">All melodies were composed of 500 ms sine tones to which a tapered-cosine (Tukey) window was applied. Tones were not separated by a silence interval. During the exposure phase, 100 melodies were generated in real time using the grammar structure and the pitches of tones defined by each scale. During the exposure phase, melodies were produced using the current structure condition (uniform or non-uniform) and the reference version of the grammar. During the test phase, half of the melodies were produced the same way using the reference grammar and half of the melodies were produced the same way but using the alternative grammar. Forty reference and 40 alternative melodies were presented in a random order during the test phase. All melodies were constrained so that they did not exceed 15 tones and had to reach the final note, as defined by the grammar.</p></sec><sec id="S13" sec-type="methods"><title>Procedure</title><p id="P28">The experiment was divided into two parts, each part corresponding to a structure condition in which order of testing was randomized across participants. During each part, listeners had to first complete an exposure phase during which they listened to 100 melodies. During this phase, melodies were generated in real time with the designated scale and grammar. Only the correct version of the grammar was used to generate the exposure melodies. Throughout this phase, listeners had to simply click a mouse to play the next melody. Immediately following the exposure phase, participants completed a test phase during which 80 melodies were generated on the fly; half of them were generated using the reference version of the grammar and the other half with the alternative version. After each melody, participants had to report whether this melody sounded familiar or unfamiliar, with respect to what they just were exposed to in the previous phase. Participants were tested individually in an EEG testing booth. Audio files of the stimuli were encoded at 16-bit resolution and 44.1 kHz sampling rate and presented via Etymotics Research ER-2 earphones. The stimuli were presented at a comfortable loudness level above 60 dB SPL (A-weighted). Instructions were displayed on a computer screen and participants’ responses were collected with a keyboard and mouse. Informal debriefing with participants indicated that both scales were perceived as equally unfamiliar and no formal ratings of familiarity were collected after each session.</p></sec><sec id="S14"><title>Data Acquisition</title><p id="P29">Electroencephalogram (EEG) data were recorded using a 64-channel system (ActiCap, BrainProducts) at a sampling rate of 500 Hz with one ground electrode and re-referenced to the average. We used a default fabric head-cap that holds the electrodes (EasyCap, Equidistant layout).</p></sec><sec id="S15"><title>EEG Prepossessing</title><p id="P30">EEG data was first mean-centered to perform zero-order detrending. We detected bad channels as exhibiting amplitude above 3 standard deviation from the channel average. Selected bad channels were then interpolated using a weighed sum of neighboring channels’ signal. To avoid artifacts caused by low-pass filtering, we subtracted from each channel its slow varying trend by robust-fitting a <italic>30th</italic>-order polynomial [<xref ref-type="bibr" rid="R87">87</xref>]. We then applied an anti-aliasing low-pass Butterworth (IIR) 4-order filter with a 40 Hz cut-off and down-sampled the resulting data to 100 Hz. Using a time-shift PCA, eye-blink artifacts were isolated and projected out using data collected by the HEoG and VEoG channels [<xref ref-type="bibr" rid="R88">88</xref>]. Finally, the EEG data was re-referenced again by subtracting the robust mean (as defined in [<xref ref-type="bibr" rid="R87">87</xref>]) before it was epoched using the triggers sent at the beginning of each trial. Bad epochs were selected based on an amplitude above 3 standard deviation and discarded for the analysis.</p></sec><sec id="S16"><title>Decoding</title><p id="P31">To evaluate the separability of neural traces elicited by melodies from the two scales, we trained a set of logistic regression classifiers on the preprocessed EEG data (e.g. not DSS-denoised). At each time point <italic>t</italic> we used the matrix of observations <italic>X</italic><sub><italic>t</italic></sub> ϵ <italic>R</italic><sup><italic>N</italic></sup> <sup>x64</sup>, for <italic>N</italic> samples of all 64 electrodes to predict the labels <italic>y<sub>t′</sub></italic> ϵ {0,1}<sup><italic>N</italic></sup>. Here, the labels corresponded to the two grammar conditions (alternative vs. reference melodies) or the state of transitions (correct vs. incorrect), or the probability of transitions (low probability vs. high probability transitions). This was repeated for every time point <italic>t</italic>′ of each epoch. This analysis was conducted two times: on the epochs collected from the uniform scale conditions and the non-uniform scale condition. For each subject, we trained the decoders on EEG signals at each time points of the melodies (from onset to 6 s after onset). Therefore, the decoder at each time point learns to predict the grammar conditions (alternative vs. reference) using the topography of the EEG samples for this time point. Additionally, temporal generalization analysis was conducted to capture the dynamics of topographical patterns of EEG signal over time (for more details on that, see [<xref ref-type="bibr" rid="R36">36</xref>]). To achieve that, we systematically evaluated each classifier from each time point to all other time points. Concretely, this means that a classifier trained to separate labels at a given time point is then used to predict the labels at all other time points.</p><p id="P32">To validate the classifier’s performance, we used 5-fold cross-validation. This means that for each individual data set, over 5 iterations, the trained classifier was used to predict labels on a fifth portion of unseen data. The area under the receiver operating characteristic curve (AUC) was used to quantify the classifier’s performance. We implemented this decoding analysis using sci-kitlearn [<xref ref-type="bibr" rid="R89">89</xref>] and MNE [<xref ref-type="bibr" rid="R90">90</xref>] libraries in python 3.6.</p></sec><sec id="S17"><title>Temporal Response Function</title><p id="P33">To evaluate the different topographical mapping in melodic encoding between the two scale structures, we used a brain decoding method based on Temporal Response Functions (TRF) [<xref ref-type="bibr" rid="R56">56</xref>]. The TRF is based on a class of linear time-invariant models that describes the linear transformation of stimuli features to the neural signal (EEG) by its impulse response after ridge regression. Unlike an evoked response, the response function obtained reflects a modeled neural response to a <italic>specific</italic> set of features (when an ERP represents the grand average to the whole stimulus). More precisely, the TRF optimally describes the mapping between a given set of features of a sensory input <italic>s</italic>(<italic>t</italic>) and the neural response <italic>r</italic>(<italic>t</italic>) collected from each channel <italic>n</italic> of the neural signal such as defined in <xref ref-type="disp-formula" rid="FD1">Eq. 1</xref>: <disp-formula id="FD1"><label>(1)</label><mml:math id="M1"><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mi>∑</mml:mi><mml:mi>τ</mml:mi></mml:munder><mml:mi>w</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>ε</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></disp-formula></p><p id="P34">Where <italic>τ</italic> is the specific range of lags for which the response at time <italic>t</italic> is described (here, [-100 - 500] ms) and ε(t) is the residual error at each channel <italic>n</italic> not explained by the model. The TRF <italic>w</italic>(<italic>τ</italic>, <italic>n</italic>) is estimated by minimizing the mean-squared error between the actual neural response and the one predicted by the convolution <italic>w</italic>(<italic>τ</italic>, <italic>n</italic>) * <italic>s</italic>(<italic>t</italic> − <italic>τ</italic>). The model is optimized using ridge regression and assuming a certain degree of regularization to prevent over-fitting. This regularization parameter is optimized in the [10<sup>−3</sup>, 10<sup>3</sup>] interval, using logarithmic steps, and for each individual data set. To evaluate the performance of the model, a cross-validated via leave-one-out evaluation using Pearson’s correlations between the predicted and actual neural responses is conducted. The resulting topographical map indicates the strength of stimulus feature encoding at each EEG channel. Prior to conducting the TRF analysis, a visual inspection of trials was done to remove noisy portions of the data. Additionally, disparate external noise sources were removed by conducting an ICA and removing components which topography indicated signal from an external source.</p></sec><sec id="S18"><title>Denoised evoked responses</title><p id="P35">For the the evoked response analysis, a specific denoising algorithm called Denoised Source Separation (DSS) was applied (for detailed explanation see [<xref ref-type="bibr" rid="R49">49</xref>]). In a nutshell, DSS isolates components of signals that are mostly repeated across repetitions of trials, so as to keep the relevant signal (e.g. one that reflects stimuli properties) and to remove signal resulting from noise. In the present study, the Denoised Source Separation (DSS) filter’s output was the weighted sum of the signals from the 64 EEG electrodes, in which the weights were optimized to extract the repeated neural activities across trials. This transformation yielded to 64 uncorrelated brain source activities (e.g., DSS components) which were ordered by a repeatability score. Since the trials were not exactly identical between repetition, we selected only the first 5 most repeatable DSS components and projected them back in the sensor space to obtain cleaned signals. Finally, we used the obtained denoised Cz electrode (placed on the mid-line sagittal plane center) for the evoked response analysis.</p></sec><sec id="S19"><title>Statistical Analysis</title><sec id="S20"><title>Classifiers</title><p id="P36">Statistical analysis for the classifiers was performed with a one-sample <italic>t</italic>-test with random-effect Monte-Carlo cluster statistics for multiple comparison correction using the default parameters of the MNE spatio_temporal_cluster_1samp_test function [<xref ref-type="bibr" rid="R91">91</xref>]. Error bars in all figures represents ±SEM (standard error of the mean).</p></sec><sec id="S21"><title>Analysis of evoked responses</title><p id="P37">To compare the averaged evoked response between conditions, we performed bootstrap resampling in order to estimate the standard deviation (SD) of the difference between the type of transitions (correct vs. incorrect). Significance levels were set for difference between the two signals above 2× estimated SD. In <xref ref-type="fig" rid="F4">Figure 4.A</xref>, error bars represents ±SEM (standard error of the mean).</p></sec><sec id="S22"><title>Topography</title><p id="P38">In order to assess the different topographies for both scale conditions, we conducted a one-sample, cluster-based permutation test using <italic>r</italic>-values as input [<xref ref-type="bibr" rid="R92">92</xref>]. In this analysis, multiple <italic>t</italic>-tests are computed for each electrode. Then, best on clusters of electrodes in which the response significantly differs from zero are identified. These clusters are then formed over space by grouping electrodes that have significant initial <italic>t</italic>-test values. The sum of all <italic>t</italic>-scores within each cluster provides a cluster-level <italic>t</italic>-score (mass <italic>t</italic>-score). A permutation approach is then used to control for Type I errors (2,000 iterations) in order to build a data-driven null hypothesis distribution. The significance of a cluster is determined by whether it falls in the highest 5th percentile of the corresponding distribution (<italic>α</italic> = 0.05).</p></sec></sec></sec></body><back><ack id="S23"><title>Acknowledgments</title><p>We thank David Poeppel and Omri Raccah for their thorough comments on the manuscript.</p><sec id="S24"><title>Funding</title><p>This study was supported by an Advanced European Research Council grant (NEUME, 787836) and Air Force Office of Scientific Research and National Science Foundation grants to S.A.S.</p></sec></ack><fn-group><fn id="FN2" fn-type="conflict"><p id="P39"><bold>Competing interests</bold></p><p id="P40">Authors declare no competing interest.</p></fn><fn id="FN3" fn-type="con"><p id="P41"><bold>Author Contributions</bold></p><p id="P42">CP, MR, MF and SS: conceptualization. CP and MR: collection of data. CP and MR: analysis of data. CP, MR, MF and SS: writing and editing.</p></fn></fn-group><ref-list><ref id="R1"><label>[1]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Reck</surname><given-names>DB</given-names></name></person-group><source>Music of the whole earth</source><publisher-loc>Macmillan Reference USA</publisher-loc><year>1977</year></element-citation></ref><ref id="R2"><label>[2]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stevens</surname><given-names>C</given-names></name></person-group><article-title>Cross-cultural studies of musical pitch and time</article-title><source>Acoustical science and technology</source><year>2004</year><volume>25</volume><fpage>433</fpage><lpage>438</lpage></element-citation></ref><ref id="R3"><label>[3]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Nettl</surname><given-names>B</given-names></name></person-group><source>The study of ethnomusicology: Thirty-three discussions</source><publisher-name>University of Illinois Press</publisher-name><year>2015</year></element-citation></ref><ref id="R4"><label>[4]</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Fourer</surname><given-names>D</given-names></name><name><surname>Rouas</surname><given-names>JL</given-names></name><name><surname>Hanna</surname><given-names>P</given-names></name><name><surname>Robine</surname><given-names>M</given-names></name></person-group><source>Automatic timbre classification of ethnomu-sicological audio recordings</source><year>2014</year></element-citation></ref><ref id="R5"><label>[5]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polak</surname><given-names>R</given-names></name><etal/></person-group><article-title>Rhythmic prototypes across cultures: a comparative study of tapping synchronization</article-title><source>Music Perception: An Interdisciplinary Journal</source><year>2018</year><volume>36</volume><fpage>1</fpage><lpage>23</lpage></element-citation></ref><ref id="R6"><label>[6]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castellano</surname><given-names>MA</given-names></name><name><surname>Bharucha</surname><given-names>JJ</given-names></name><name><surname>Krumhansl</surname><given-names>CL</given-names></name></person-group><article-title>Tonal hierarchies in the music of north india</article-title><source>Journal of Experimental Psychology: General</source><year>1984</year><volume>113</volume><fpage>394</fpage></element-citation></ref><ref id="R7"><label>[7]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krumhansl</surname><given-names>CL</given-names></name><etal/></person-group><article-title>Cross-cultural music cognition: Cognitive methodology applied to north sami yoiks</article-title><source>Cognition</source><year>2000</year><volume>76</volume><fpage>13</fpage><lpage>58</lpage></element-citation></ref><ref id="R8"><label>[8]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>S</given-names></name><name><surname>Jordania</surname><given-names>J</given-names></name></person-group><article-title>Universals in the world’s musics</article-title><source>Psychology of Music</source><year>2013</year><volume>41</volume><fpage>229</fpage><lpage>248</lpage></element-citation></ref><ref id="R9"><label>[9]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehr</surname><given-names>SA</given-names></name><etal/></person-group><article-title>Universality and diversity in human song</article-title><source>Science</source><year>2019</year><volume>366</volume></element-citation></ref><ref id="R10"><label>[10]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Savage</surname><given-names>PE</given-names></name><name><surname>Brown</surname><given-names>S</given-names></name><name><surname>Sakai</surname><given-names>E</given-names></name><name><surname>Currie</surname><given-names>TE</given-names></name></person-group><article-title>Statistical universals reveal the structures and functions of human music</article-title><source>Proceedings of the National Academy of Sciences</source><year>2015</year><volume>112</volume><fpage>8987</fpage><lpage>8992</lpage></element-citation></ref><ref id="R11"><label>[11]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nakamura</surname><given-names>E</given-names></name><name><surname>Kaneko</surname><given-names>K</given-names></name></person-group><article-title>Statistical evolutionar y laws in music styles</article-title><source>Scientific reports</source><year>2019</year><volume>9</volume><fpage>1</fpage><lpage>11</lpage></element-citation></ref><ref id="R12"><label>[12]</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Youngblood</surname><given-names>M</given-names></name><name><surname>Ozaki</surname><given-names>Y</given-names></name><name><surname>Savage</surname><given-names>PE</given-names></name></person-group><source>Cultural evolution and music</source><year>2021</year></element-citation></ref><ref id="R13"><label>[13]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Savage</surname><given-names>PE</given-names></name></person-group><article-title>Cultural evolution of music</article-title><source>Palgrave Communications</source><year>2019</year><volume>5</volume><fpage>1</fpage><lpage>12</lpage></element-citation></ref><ref id="R14"><label>[14]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacoby</surname><given-names>N</given-names></name><etal/></person-group><article-title>Universal and non-universal features of musical pitch perception revealed by singing</article-title><source>Current Biology</source><year>2019</year><volume>29</volume><fpage>3229</fpage><lpage>3243</lpage></element-citation></ref><ref id="R15"><label>[15]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelofi</surname><given-names>C</given-names></name><name><surname>Farbood</surname><given-names>MM</given-names></name></person-group><article-title>Asymmetry in scales enhances learning of new musical structures</article-title><source>Proceedings of the National Academy of Sciences</source><year>2021</year><volume>118</volume></element-citation></ref><ref id="R16"><label>[16]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harasim</surname><given-names>D</given-names></name><name><surname>Moss</surname><given-names>FC</given-names></name><name><surname>Ramirez</surname><given-names>M</given-names></name><name><surname>Rohrmeier</surname><given-names>M</given-names></name></person-group><article-title>Exploring the foundations of tonality: Statistical cognitive modeling of modes in the history of western classical music</article-title><source>Humanities and Social Sciences Communications</source><year>2021</year><volume>8</volume><fpage>1</fpage><lpage>11</lpage></element-citation></ref><ref id="R17"><label>[17]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacoby</surname><given-names>N</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name></person-group><article-title>Integer ratio priors on musical rhythm revealed cross-culturally by iterated reproduction</article-title><source>Current Biology</source><year>2017</year><volume>27</volume><fpage>359</fpage><lpage>370</lpage></element-citation></ref><ref id="R18"><label>[18]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stevens</surname><given-names>CJ</given-names></name></person-group><article-title>Music perception and cognition: A review of recent cross-cultural research</article-title><source>Topics in Cognitive Science</source><year>2012</year><volume>4</volume><fpage>653</fpage><lpage>667</lpage></element-citation></ref><ref id="R19"><label>[19]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacoby</surname><given-names>N</given-names></name><etal/></person-group><article-title>Cross-cultural work in music cognition: Challenges, insights, and recommendations</article-title><source>Music Perception</source><year>2020</year><volume>37</volume><fpage>185</fpage><lpage>195</lpage></element-citation></ref><ref id="R20"><label>[20]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haumann</surname><given-names>NT</given-names></name><name><surname>Vuust</surname><given-names>P</given-names></name><name><surname>Bertelsen</surname><given-names>F</given-names></name><name><surname>Garza-Villarreal</surname><given-names>EA</given-names></name></person-group><article-title>Influence of musical enculturation on brain responses to metric deviants</article-title><source>Frontiers in neuroscience</source><year>2018</year><volume>12</volume><fpage>218</fpage></element-citation></ref><ref id="R21"><label>[21]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Unyk</surname><given-names>AM</given-names></name><name><surname>Carlsen</surname><given-names>JC</given-names></name></person-group><article-title>The influence of expectancy on melodic perception</article-title><source>Psychomusicology: A Journal ofResearch in Music Cognition</source><year>1987</year><volume>7</volume><fpage>3</fpage></element-citation></ref><ref id="R22"><label>[22]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krumhansl</surname><given-names>CL</given-names></name><name><surname>Louhivuori</surname><given-names>J</given-names></name><name><surname>Toiviainen</surname><given-names>P</given-names></name><name><surname>Järvinen</surname><given-names>T</given-names></name><name><surname>Eerola</surname><given-names>T</given-names></name></person-group><article-title>Melodic expectation in finnish spiritual folk hymns: Convergence of statistical, behavioral, and computational approaches</article-title><source>Music Perception</source><year>1999</year><volume>17</volume><fpage>151</fpage><lpage>195</lpage></element-citation></ref><ref id="R23"><label>[23]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eerola</surname><given-names>T</given-names></name><name><surname>Louhivuori</surname><given-names>J</given-names></name><name><surname>Lebaka</surname><given-names>E</given-names></name></person-group><article-title>Expectancy in sami yoiks revisited: The role of data-driven and schema-driven knowledge in the formation of melodic expectations</article-title><source>Musicae Scientiae</source><year>2009</year><volume>13</volume><fpage>231</fpage><lpage>272</lpage></element-citation></ref><ref id="R24"><label>[24]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krumhansl</surname><given-names>CL</given-names></name></person-group><article-title>Effects of musical context on similarity and expectancy</article-title><source>Systematische Musikwissenschaft</source><year>1995</year><volume>3</volume><fpage>211</fpage><lpage>250</lpage></element-citation></ref><ref id="R25"><label>[25]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guillemin</surname><given-names>C</given-names></name><name><surname>Tillmann</surname><given-names>B</given-names></name></person-group><article-title>Implicit learning of two artificial grammars</article-title><source>Cognitive Processing</source><year>2021</year><volume>22</volume><fpage>141</fpage><lpage>150</lpage></element-citation></ref><ref id="R26"><label>[26]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohrmeier</surname><given-names>M</given-names></name><name><surname>Widdess</surname><given-names>R</given-names></name></person-group><article-title>Incidental learning of melodic structure of north indian music</article-title><source>Cognitive Science</source><year>2017</year><volume>41</volume><fpage>1299</fpage><lpage>1327</lpage></element-citation></ref><ref id="R27"><label>[27]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daikoku</surname><given-names>T</given-names></name><name><surname>Yumoto</surname><given-names>M</given-names></name></person-group><article-title>Musical expertise facilitates statistical learning of rhythm and the perceptive uncertainty: A cross-cultural study</article-title><source>Neuropsychologia</source><year>2020</year><volume>146</volume><elocation-id>107553</elocation-id></element-citation></ref><ref id="R28"><label>[28]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>X</given-names></name><name><surname>Ding</surname><given-names>N</given-names></name><name><surname>Tao</surname><given-names>Y</given-names></name><name><surname>Yang</surname><given-names>YF</given-names></name></person-group><article-title>Syntactic complexity and musical proficiency modulate neural processing of non-native music</article-title><source>Neuropsychologia</source><year>2018</year><volume>121</volume><fpage>164</fpage><lpage>174</lpage></element-citation></ref><ref id="R29"><label>[29]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cecchetti</surname><given-names>G</given-names></name><name><surname>Herff</surname><given-names>SA</given-names></name><name><surname>Finkensiep</surname><given-names>C</given-names></name><name><surname>Rohrmeier</surname><given-names>MA</given-names></name></person-group><source>The experience of musical structure as computation: what can we learn?</source><year>2020</year><fpage>91</fpage><lpage>127</lpage><comment>The experience of musical structure as computation: what can we learn?</comment></element-citation></ref><ref id="R30"><label>[30]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearce</surname><given-names>MT</given-names></name><name><surname>Ruiz</surname><given-names>MH</given-names></name><name><surname>Kapasi</surname><given-names>S</given-names></name><name><surname>Wiggins</surname><given-names>GA</given-names></name><name><surname>Bhattacharya</surname><given-names>J</given-names></name></person-group><article-title>Unsupervised statistical learning underpins computational, behavioural, and neural manifestations of musical expectation</article-title><source>NeuroImage</source><year>2010</year><volume>50</volume><fpage>302</fpage><lpage>313</lpage></element-citation></ref><ref id="R31"><label>[31]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearce</surname><given-names>MT</given-names></name></person-group><article-title>Statistical learning and probabilistic prediction in music cognition: mechanisms of stylistic enculturation</article-title><source>Annals of the New York Academy of Sciences</source><year>2018</year><volume>1423</volume><fpage>378</fpage><lpage>395</lpage></element-citation></ref><ref id="R32"><label>[32]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Browne</surname><given-names>R</given-names></name></person-group><article-title>Tonal implications of the diatonic set</article-title><source>In Theory Only</source><year>1981</year><volume>5</volume><fpage>3</fpage><lpage>21</lpage></element-citation></ref><ref id="R33"><label>[33]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Butler</surname><given-names>D</given-names></name><name><surname>Brown</surname><given-names>H</given-names></name></person-group><article-title>Tonal structure versus function: Studies of the recognition of harmonic motion</article-title><source>Music Perception</source><year>1984</year><volume>2</volume><fpage>6</fpage><lpage>24</lpage></element-citation></ref><ref id="R34"><label>[34]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ellis</surname><given-names>AJ</given-names></name></person-group><article-title>On the musical scales of various nations</article-title><source>Journal of the Society of Arts</source><year>1885</year></element-citation></ref><ref id="R35"><label>[35]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loui</surname><given-names>P</given-names></name></person-group><article-title>Learning and liking of melody and harmony: Further studies in artificial grammar learning</article-title><source>Topics in Cognitive Science</source><year>2012</year><volume>4</volume><fpage>554</fpage><lpage>567</lpage></element-citation></ref><ref id="R36"><label>[36]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname><given-names>JR</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><article-title>Characterizing the dynamics of mental representations: the temporal generalization method</article-title><source>Trends in Cognitive Sciences</source><year>2014</year><volume>18</volume><fpage>203</fpage><lpage>210</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S1364661314000199">https://www.sciencedirect.com/science/article/pii/S1364661314000199</ext-link></comment></element-citation></ref><ref id="R37"><label>[37]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><article-title>Deconstructing multivariate decoding for the study of brain function</article-title><source>Neuroimage</source><year>2018</year><volume>180</volume><fpage>4</fpage><lpage>18</lpage></element-citation></ref><ref id="R38"><label>[38]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tong</surname><given-names>F</given-names></name><name><surname>Pratte</surname><given-names>MS</given-names></name></person-group><article-title>Decoding patterns of human brain activity</article-title><source>Annual review of psychology</source><year>2012</year><volume>63</volume><fpage>483</fpage></element-citation></ref><ref id="R39"><label>[39]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Teng</surname><given-names>S</given-names></name></person-group><article-title>Resolving the neural dynamics of visual and auditory scene processing in the human brain: a methodological approach</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><year>2017</year><volume>372</volume><elocation-id>20160108</elocation-id></element-citation></ref><ref id="R40"><label>[40]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aller</surname><given-names>M</given-names></name><name><surname>Noppeney</surname><given-names>U</given-names></name></person-group><article-title>To integrate or not to integrate: Temporal dynamics of hierarchical bayesian causal inference</article-title><source>PLoS biology</source><year>2019</year><volume>17</volume><elocation-id>e3000210</elocation-id></element-citation></ref><ref id="R41"><label>[41]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rezaeizadeh</surname><given-names>M</given-names></name><name><surname>Shamma</surname><given-names>S</given-names></name></person-group><article-title>Binding the acoustic features of an auditor y source through temporal coherence</article-title><source>Cerebral cortex communications</source><year>2021</year><volume>2</volume><elocation-id>tgab060</elocation-id></element-citation></ref><ref id="R42"><label>[42]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baroni</surname><given-names>F</given-names></name><etal/></person-group><article-title>Converging intracortical signatures of two separated processing timescales in human early auditory cortex</article-title><source>NeuroImage</source><year>2020</year><volume>218</volume><elocation-id>116882</elocation-id></element-citation></ref><ref id="R43"><label>[43]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonetti</surname><given-names>L</given-names></name><etal/></person-group><article-title>Spatiotemporal whole-brain dynamics of auditory patterns recognition</article-title><source>BioRxiv</source><year>2021</year><comment>2020-06</comment></element-citation></ref><ref id="R44"><label>[44]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Demarchi</surname><given-names>G</given-names></name><name><surname>Sanchez</surname><given-names>G</given-names></name><name><surname>Weisz</surname><given-names>N</given-names></name></person-group><article-title>Automatic and feature-specific prediction-related neural activity in the human auditory system</article-title><source>Nature communications</source><year>2019</year><volume>10</volume><fpage>1</fpage><lpage>11</lpage></element-citation></ref><ref id="R45"><label>[45]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takacs</surname><given-names>A</given-names></name><name><surname>Mückschel</surname><given-names>M</given-names></name><name><surname>Roessner</surname><given-names>V</given-names></name><name><surname>Beste</surname><given-names>C</given-names></name></person-group><article-title>Decoding stimulus-response representations and their stability using eeg-based multivariate pattern analysis</article-title><source>Cerebral Cortex Communications</source><year>2020</year><volume>1</volume><elocation-id>tgaa016</elocation-id></element-citation></ref><ref id="R46"><label>[46]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McMurray</surname><given-names>B</given-names></name><etal/></person-group><article-title>Decoding the temporal dynamics of spoken word and nonword processing from eeg</article-title><source>NeuroImage</source><year>2022</year><volume>260</volume><elocation-id>119457</elocation-id></element-citation></ref><ref id="R47"><label>[47]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heikel</surname><given-names>E</given-names></name><name><surname>Sassenhagen</surname><given-names>J</given-names></name><name><surname>Fiebach</surname><given-names>CJ</given-names></name></person-group><article-title>Time-generalized multivariate analysis of eeg responses reveals a cascading architecture of semantic mismatch processing</article-title><source>Brain and language</source><year>2018</year><volume>184</volume><fpage>43</fpage><lpage>53</lpage></element-citation></ref><ref id="R48"><label>[48]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gwilliams</surname><given-names>L</given-names></name><name><surname>King</surname><given-names>J</given-names></name><name><surname>Marantz</surname><given-names>A</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>Neural dynamics of phoneme sequences: Position-invariant code for content and order</article-title><year>2020</year></element-citation></ref><ref id="R49"><label>[49]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Cheveigné</surname><given-names>A</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><article-title>Denoising based on spatial filtering</article-title><source>Journal of Neuroscience Methods</source><year>2008</year><volume>171</volume><fpage>331</fpage><lpage>339</lpage><comment>NIHMS150003</comment></element-citation></ref><ref id="R50"><label>[50]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koelsch</surname><given-names>S</given-names></name><name><surname>Gunter</surname><given-names>T</given-names></name><name><surname>Friederici</surname><given-names>AD</given-names></name><name><surname>Schröger</surname><given-names>E</given-names></name></person-group><article-title>Brain indices of music processing:”nonmusicians” are musical</article-title><source>Journal of Cognitive Neuroscience</source><year>2000</year><volume>12</volume><fpage>520</fpage><lpage>541</lpage></element-citation></ref><ref id="R51"><label>[51]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leino</surname><given-names>S</given-names></name><name><surname>Brattico</surname><given-names>E</given-names></name><name><surname>Tervaniemi</surname><given-names>M</given-names></name><name><surname>Vuust</surname><given-names>P</given-names></name></person-group><article-title>Representation of harmony rules in the human brain: Further evidence from event-related potentials</article-title><source>Brain research</source><year>2007</year><volume>1142</volume><fpage>169</fpage><lpage>177</lpage></element-citation></ref><ref id="R52"><label>[52]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loui</surname><given-names>P</given-names></name><name><surname>Grent</surname><given-names>T</given-names></name><name><surname>Torpey</surname><given-names>D</given-names></name><name><surname>Woldorff</surname><given-names>M</given-names></name><etal/></person-group><article-title>Effects of attention on the neural processing of harmonic syntax in western music</article-title><source>Cognitive Brain Research</source><year>2005</year><volume>25</volume><fpage>678</fpage><lpage>687</lpage></element-citation></ref><ref id="R53"><label>[53]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koelsch</surname><given-names>S</given-names></name><name><surname>Mulder</surname><given-names>J</given-names></name></person-group><article-title>Electric brain responses to inappropriate harmonies during listening to expressive music</article-title><source>Clinical Neurophysiology</source><year>2002</year><volume>113</volume><fpage>862</fpage><lpage>869</lpage></element-citation></ref><ref id="R54"><label>[54]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinbeis</surname><given-names>N</given-names></name><name><surname>Koelsch</surname><given-names>S</given-names></name><name><surname>Sloboda</surname><given-names>JA</given-names></name></person-group><article-title>The role of harmonic expectancy violations in musical emotions: Evidence from subjective, physiological, and neural responses</article-title><source>Journal of cognitive neuroscience</source><year>2006</year><volume>18</volume><fpage>1380</fpage><lpage>1393</lpage></element-citation></ref><ref id="R55"><label>[55]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koelsch</surname><given-names>S</given-names></name></person-group><article-title>Music-syntactic processing and auditory memory: Similarities and differences between eran and mmn</article-title><source>Psychophysiology</source><year>2009</year><volume>46</volume><fpage>179</fpage><lpage>190</lpage></element-citation></ref><ref id="R56"><label>[56]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crosse</surname><given-names>MJ</given-names></name><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Bednar</surname><given-names>A</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><article-title>The multivariate temporal response function (mtrf) toolbox: a matlab toolbox for relating neural signals to continuous stimuli</article-title><source>Frontiers in human neuroscience</source><year>2016</year><volume>10</volume><fpage>604</fpage></element-citation></ref><ref id="R57"><label>[57]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akram</surname><given-names>S</given-names></name><name><surname>Presacco</surname><given-names>A</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name><name><surname>Babadi</surname><given-names>B</given-names></name></person-group><article-title>Robust decoding of selective auditory attention from meg in a competing-speaker environment via state-space modeling</article-title><source>NeuroImage</source><year>2016</year><volume>124</volume><fpage>906</fpage><lpage>917</lpage></element-citation></ref><ref id="R58"><label>[58]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wong</surname><given-names>DD</given-names></name><etal/></person-group><article-title>A comparison of temporal response function estimation methods for auditory attention decoding</article-title><source>Biorxiv</source><year>2018</year><fpage>1</fpage><lpage>22</lpage></element-citation></ref><ref id="R59"><label>[59]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Broderick</surname><given-names>MP</given-names></name><name><surname>Anderson</surname><given-names>AJ</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><article-title>Semantic context enhances the early auditory encoding of natural speech</article-title><source>Journal of Neuroscience</source><year>2019</year><volume>39</volume><fpage>7564</fpage><lpage>7575</lpage></element-citation></ref><ref id="R60"><label>[60]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>O’Sullivan</surname><given-names>JA</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><article-title>Low-frequency cortical entrainment to speech reflects phoneme-level processing</article-title><source>Current Biology</source><year>2015</year><volume>25</volume><fpage>2457</fpage><lpage>2465</lpage></element-citation></ref><ref id="R61"><label>[61]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Pelofi</surname><given-names>C</given-names></name><name><surname>Shamma</surname><given-names>S</given-names></name><name><surname>de Cheveigné</surname><given-names>A</given-names></name></person-group><article-title>Musical expertise enhances the cortical tracking of the acoustic envelope during naturalistic music listening</article-title><source>Acoustical Science and Technology</source><year>2020</year><volume>41</volume><fpage>361</fpage><lpage>364</lpage></element-citation></ref><ref id="R62"><label>[62]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Liberto</surname><given-names>GM</given-names></name><etal/></person-group><article-title>Cortical encoding of melodic expectations in human temporal cortex</article-title><source>eLife</source><year>2020</year><volume>9</volume><elocation-id>e51784</elocation-id></element-citation></ref><ref id="R63"><label>[63]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearce</surname><given-names>M</given-names></name><name><surname>Wiggins</surname><given-names>G</given-names></name></person-group><article-title>Auditory expectation: The information dynamics of music perception and cognition</article-title><source>topics in cognitive science</source><year>2012</year><volume>4</volume><issue>4</issue><fpage>625</fpage><lpage>652</lpage></element-citation></ref><ref id="R64"><label>[64]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chater</surname><given-names>N</given-names></name><name><surname>Vitányi</surname><given-names>P</given-names></name></person-group><article-title>Simplicity: a unifying principle in cognitive science?</article-title><source>Trends in Cognitive Sciences</source><year>2003</year><volume>7</volume><fpage>19</fpage><lpage>22</lpage></element-citation></ref><ref id="R65"><label>[65]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Attneave</surname><given-names>F</given-names></name></person-group><article-title>Some informational aspects of visual perception</article-title><source>Psychological review</source><year>1954</year><volume>61</volume><issue>3</issue><fpage>183</fpage><lpage>93</lpage></element-citation></ref><ref id="R66"><label>[66]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marion</surname><given-names>G</given-names></name><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><article-title>The music of silence. part i: Responses to musical imagery encode melodic expectations and acoustics</article-title><source>Journal of Neuroscience</source><year>2021</year><comment><ext-link ext-link-type="uri" xlink:href="https://www.jneurosci.org/content/early/2021/07/23/JNEUROSCI.0183-21.2021">https://www.jneurosci.org/content/early/2021/07/23/JNEUROSCI.0183-21.2021</ext-link><ext-link ext-link-type="uri" xlink:href="https://www.jneurosci.org/content/early/2021/07/23/JNEUROSCI.0183-21.2021.full.pdf">https://www.jneurosci.org/content/early/2021/07/23/JNEUROSCI.0183-21.2021.full.pdf</ext-link></comment></element-citation></ref><ref id="R67"><label>[67]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Marion</surname><given-names>G</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><article-title>The music of silence. part ii: Music listening induces imagery responses</article-title><source>Journal of Neuroscience</source><year>2021</year><comment><ext-link ext-link-type="uri" xlink:href="https://www.jneurosci.org/content/early/2021/07/22/JNEUROSCI.0184-21.2021">https://www.jneurosci.org/content/early/2021/07/22/JNEUROSCI.0184-21.2021</ext-link><ext-link ext-link-type="uri" xlink:href="https://www.jneurosci.org/content/early/2021/07/22/JNEUROSCI.0184-21.2021.full.pdf">https://www.jneurosci.org/content/early/2021/07/22/JNEUROSCI.0184-21.2021.full.pdf</ext-link></comment></element-citation></ref><ref id="R68"><label>[68]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nettl</surname><given-names>B</given-names></name></person-group><article-title>An ethnomusicologist contemplates universals in musical sound and musical culture</article-title><source>The Origins of Music</source><year>2000</year><volume>3</volume><fpage>463</fpage><lpage>472</lpage></element-citation></ref><ref id="R69"><label>[69]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Balzano</surname><given-names>GJ</given-names></name></person-group><chapter-title>The pitch set as a level of description for studying musical pitch perception</chapter-title><source>Music, Mind, and Brain</source><publisher-name>Springer</publisher-name><year>1982</year><fpage>321</fpage><lpage>351</lpage></element-citation></ref><ref id="R70"><label>[70]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferreri</surname><given-names>L</given-names></name><etal/></person-group><article-title>Dopamine modulates the reward experiences elicited by music</article-title><source>Proceedings of the National Academy of Sciences</source><year>2019</year><volume>116</volume><fpage>3793</fpage><lpage>3798</lpage></element-citation></ref><ref id="R71"><label>[71]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salimpoor</surname><given-names>VN</given-names></name><name><surname>Zald</surname><given-names>DH</given-names></name><name><surname>Zatorre</surname><given-names>RJ</given-names></name><name><surname>Dagher</surname><given-names>A</given-names></name><name><surname>McIntosh</surname><given-names>AR</given-names></name></person-group><article-title>Predictions and the brain: how musical sounds become rewarding</article-title><source>Trends in Cognitive Sciences</source><year>2015</year><volume>19</volume><fpage>86</fpage><lpage>91</lpage></element-citation></ref><ref id="R72"><label>[72]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mas-Herrero</surname><given-names>E</given-names></name><name><surname>Zatorre</surname><given-names>RJ</given-names></name><name><surname>Rodriguez-Fornells</surname><given-names>A</given-names></name><name><surname>Marco-Pallarés</surname><given-names>J</given-names></name></person-group><article-title>Dissociation between musical and monetary reward responses in specific musical anhedonia</article-title><source>Current Biology</source><year>2014</year><volume>24</volume><fpage>699</fpage><lpage>704</lpage></element-citation></ref><ref id="R73"><label>[73]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nummenmaa</surname><given-names>L</given-names></name><name><surname>Putkinen</surname><given-names>V</given-names></name><name><surname>Sams</surname><given-names>M</given-names></name></person-group><article-title>Social pleasures of music</article-title><source>Current Opinion in Behavioral Sciences</source><year>2021</year><volume>39</volume><fpage>196</fpage><lpage>202</lpage></element-citation></ref><ref id="R74"><label>[74]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Savage</surname><given-names>PE</given-names></name><etal/></person-group><article-title>Music as a coevolved system for social bonding</article-title><source>Behavioral and Brain Sciences</source><year>2021</year><volume>44</volume></element-citation></ref><ref id="R75"><label>[75]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>S</given-names></name></person-group><article-title>Evolutionary models of music: From sexual selection to group selection</article-title><source>Perspectives in ethology</source><publisher-name>Springer</publisher-name><year>2000</year><fpage>231</fpage><lpage>281</lpage></element-citation></ref><ref id="R76"><label>[76]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cross</surname><given-names>I</given-names></name><name><surname>Morley</surname><given-names>I</given-names></name></person-group><article-title>The evolution of music: Theories, definitions and the nature of the evidence</article-title><year>2010</year></element-citation></ref><ref id="R77"><label>[77]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dissanayake</surname><given-names>E</given-names></name></person-group><article-title>Root, leaf, blossom, or bole: Concerning the origin and adaptive function of music</article-title><source>Communicative musicality: Exploring the basis of human companionship</source><year>2009</year><fpage>17</fpage><lpage>30</lpage></element-citation></ref><ref id="R78"><label>[78]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oesch</surname><given-names>N</given-names></name></person-group><article-title>Music and language in social interaction: Synchrony, antiphony, and functional origins</article-title><source>Frontiers in Psychology</source><year>2019</year><elocation-id>1514</elocation-id></element-citation></ref><ref id="R79"><label>[79]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schulkin</surname><given-names>J</given-names></name><name><surname>Raglan</surname><given-names>GB</given-names></name></person-group><article-title>The evolution of music and human social capability</article-title><source>Frontiers in neuroscience</source><year>2014</year><volume>8</volume><fpage>292</fpage></element-citation></ref><ref id="R80"><label>[80]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trehub</surname><given-names>SE</given-names></name><name><surname>Becker</surname><given-names>J</given-names></name><name><surname>Morley</surname><given-names>I</given-names></name></person-group><article-title>Cross-cultural perspectives on music and musicality</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><year>2015</year><volume>370</volume><elocation-id>20140096</elocation-id></element-citation></ref><ref id="R81"><label>[81]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mesoudi</surname><given-names>A</given-names></name><name><surname>Whiten</surname><given-names>A</given-names></name><name><surname>Laland</surname><given-names>KN</given-names></name></person-group><article-title>Towards a unified science of cultural evolution</article-title><source>Behavioral and brain sciences</source><year>2006</year><volume>29</volume><fpage>329</fpage><lpage>347</lpage></element-citation></ref><ref id="R82"><label>[82]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arrasmith</surname><given-names>K</given-names></name></person-group><article-title>Infant music development and music experiences: A literature review</article-title><source>Update: Applications of Research in Music Education</source><year>2020</year><volume>38</volume><fpage>9</fpage><lpage>17</lpage></element-citation></ref><ref id="R83"><label>[83]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Politimou</surname><given-names>N</given-names></name><name><surname>Douglass-Kirk</surname><given-names>P</given-names></name><name><surname>Pearce</surname><given-names>M</given-names></name><name><surname>Stewart</surname><given-names>L</given-names></name><name><surname>Franco</surname><given-names>F</given-names></name></person-group><article-title>Melodic expectations in 5-and 6-year-old children</article-title><source>Journal of Experimental Child Psychology</source><year>2021</year><volume>203</volume><elocation-id>105020</elocation-id></element-citation></ref><ref id="R84"><label>[84]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trevarthen</surname><given-names>C</given-names></name><name><surname>Malloch</surname><given-names>S</given-names></name><name><surname>McPherson</surname><given-names>G</given-names></name><name><surname>Welch</surname><given-names>G</given-names></name></person-group><article-title>Musicality and musical culture: Sharing narratives of sound from early childhood</article-title><source>Music learning and teaching in infancy, childhood, and adolescence. An Oxford handbook of music education</source><year>2018</year><volume>2</volume><fpage>26</fpage><lpage>39</lpage></element-citation></ref><ref id="R85"><label>[85]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Creanza</surname><given-names>N</given-names></name><name><surname>Kolodny</surname><given-names>O</given-names></name><name><surname>Feldman</surname><given-names>MW</given-names></name></person-group><article-title>Cultural evolutionary theory: How culture evolves and why it matters</article-title><source>Proceedings of the National Academy of Sciences</source><year>2017</year><volume>114</volume><fpage>7782</fpage><lpage>7789</lpage></element-citation></ref><ref id="R86"><label>[86]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohrmeier</surname><given-names>M</given-names></name><name><surname>Rebuschat</surname><given-names>P</given-names></name><name><surname>Cross</surname><given-names>I</given-names></name></person-group><article-title>Incidental and online learning of melodic structure</article-title><source>Consciousness and Cognition</source><year>2011</year><volume>20</volume><fpage>214</fpage><lpage>222</lpage></element-citation></ref><ref id="R87"><label>[87]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Cheveigné</surname><given-names>A</given-names></name><name><surname>Arzounian</surname><given-names>D</given-names></name></person-group><article-title>Robust detrending, rereferencing, outlier detection, and inpainting for multichannel data</article-title><source>NeuroImage</source><year>2018</year><volume>172</volume><fpage>903</fpage><lpage>912</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S1053811918300351">https://www.sciencedirect.com/science/article/pii/S1053811918300351</ext-link></comment></element-citation></ref><ref id="R88"><label>[88]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Cheveigné</surname><given-names>A</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><article-title>Denoising based on time-shift PCA</article-title><source>Journal of Neuroscience Methods</source><year>2007</year><volume>165</volume><fpage>297</fpage><lpage>305</lpage></element-citation></ref><ref id="R89"><label>[89]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><etal/></person-group><article-title>Scikit-learn: Machine learning in python</article-title><source>Journal of Machine Learning Research</source><year>2011</year><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage><comment><ext-link ext-link-type="uri" xlink:href="http://jmlr.org/papers/v12/pedregosa11a.html">http://jmlr.org/papers/v12/pedregosa11a.html</ext-link></comment></element-citation></ref><ref id="R90"><label>[90]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gramfort</surname><given-names>A</given-names></name><etal/></person-group><article-title>Meg and eeg data analysis with mne-python</article-title><source>Frontiers in Neuroscience</source><year>2013</year><volume>7</volume><fpage>267</fpage><pub-id pub-id-type="doi">10.3389/fnins.2013.00267</pub-id></element-citation></ref><ref id="R91"><label>[91]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><article-title>Nonparametric statistical testing of eeg- and meg-data</article-title><source>Journal of Neuroscience Methods</source><year>2007</year><volume>164</volume><fpage>177</fpage><lpage>190</lpage><comment><ext-link ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/S0165027007001707">http://www.sciencedirect.com/science/article/pii/S0165027007001707</ext-link></comment></element-citation></ref><ref id="R92"><label>[92]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><article-title>Nonparametric statistical testing of eeg-and meg-data</article-title><source>Journal of neuroscience methods</source><year>2007</year><volume>164</volume><fpage>177</fpage><lpage>190</lpage></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Method and Behavioral data.</title><p><bold>(A)</bold> Schematic representation of the uniform (red) and non-uniform (blue) scales as circular diagrams. <bold>(B)</bold> The first-order Markov-chain grammar used to generate melodies from the uniform and non-uniform scales. Nodes represent scale notes; gray and green arrows represent the transitions between nodes used to generate exposure and reference melodies; and red arrows represent two possible examples of “incorrect” transitions used to generate half of the test melodies (i.e. the alternative melodies). <bold>(C)</bold> <italic>d</italic>′ values averaged across participants by symmetry condition: uniform (red) and non-uniform (blue). Error bars correspond to standard error. <bold>(D)</bold> To account for the drift in performance during the test session, <italic>d</italic>′ values are averaged across participants for three different trial groups: trials 1-27, trials 28-54 and trials 55-80.</p></caption><graphic xlink:href="EMS156585-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Decoding alternative versus reference melodies.</title><p><bold>(A)</bold> Decoding performance for <italic>uniform</italic> (left) and <italic>asymmetric</italic> (right) scales. Classifiers were trained and tested separately at each time in a 6-second time window following the onset of melodies. Cluster-corrected significance is contoured with a dashed line. The classifier scores were significantly above chance level only for the asymmetric scale (<italic>p</italic> &lt; 0.05). <bold>(B)</bold> Decoding performance for the same training and testing time points, which is equal to the diagonal scores in part <bold>A</bold>. The bold curve marks time points where predictions were significantly above chance level (<italic>p</italic> &lt; 0.05). The difference between the scores in uniform and non-uniform scales was significant for the gray bar. <bold>(C)</bold> The average decoding scores for the significant times (bold curves in part B) were correlated with the corresponding behavioral performance, across subjects. The decoding scores and <italic>d</italic>′ values are significantly correlated only for the non-uniform scale.</p></caption><graphic xlink:href="EMS156585-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Decoding the <italic>correct</italic> versus <italic>incorrect</italic> transitions.</title><p>Decoding performance for <italic>uniform</italic> (orange) and <italic>asymmetric</italic> (blue) scales in three separate time windows. Time windows 1, 2, and 3 referred to the first, second, and third portions of the trials. Cluster-corrected significance is marked with a bold line. The classifier scores were significantly above the chance level only for the asymmetric scale (<italic>p</italic> &lt; 0.05) in the the first time window. The difference between the scores in uniform and asymmetric scales was significant for the gray bar.</p></caption><graphic xlink:href="EMS156585-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Temporal and topographic markers of syntactic processing.</title><p><bold>(A)</bold> Evoked responses at channel Cz for <italic>correct</italic> and <italic>incorrect</italic> transitions. Comparison between the evoked responses due to the <italic>correct</italic> transitions with <italic>incorrect</italic> transitions for three separate sets of trials. Time windows 1, 2, and 3 refer to the first, second, and third portions of the trials. There were significant differences between the <italic>correct</italic> and <italic>incorrect</italic> transitions only in <italic>time window 1</italic> for the asymmetric scales. <bold>(B)</bold> Topography of difference in grammar encoding. Non-uniform - Uniform <italic>r</italic>-values obtained from the TRF of probability of notes for all melodies (reference and alternative) are first subtracted from a null model (100 permutations).</p></caption><graphic xlink:href="EMS156585-f004"/></fig></floats-group></article>