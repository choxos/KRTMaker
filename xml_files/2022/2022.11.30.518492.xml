<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS158023</article-id><article-id pub-id-type="doi">10.1101/2022.11.30.518492</article-id><article-id pub-id-type="archive">PPR578943</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A chromatic feature detector in the retina signals visual context changes</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Höfling</surname><given-names>Larissa</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Szatko</surname><given-names>Klaudia P.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="fn" rid="FN1">8</xref></contrib><contrib contrib-type="author"><name><surname>Behrens</surname><given-names>Christian</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Qiu</surname><given-names>Yongrong</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Klindt</surname><given-names>David A.</given-names></name><xref ref-type="aff" rid="A7">7</xref></contrib><contrib contrib-type="author"><name><surname>Jessen</surname><given-names>Zachary</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Schwartz</surname><given-names>Gregory W.</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Bethge</surname><given-names>Matthias</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Berens</surname><given-names>Philipp</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Franke</surname><given-names>Katrin</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="fn" rid="FN2">9</xref></contrib><contrib contrib-type="author"><name><surname>Ecker</surname><given-names>Alexander S.</given-names></name><xref ref-type="aff" rid="A5">5</xref><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Euler</surname><given-names>Thomas</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="corresp" rid="CR1">✉</xref></contrib></contrib-group><aff id="A1"><label>1</label>Institute for Ophthalmic Research, University of Tübingen, Tübingen, Germany</aff><aff id="A2"><label>2</label>Centre for Integrative Neuroscience, University of Tübingen, Tübingen, Germany</aff><aff id="A3"><label>3</label>Tübingen AI Center, University of Tübingen, Tübingen, Germany</aff><aff id="A4"><label>4</label>Feinberg School of Medicine, Department of Ophthalmology, Northwestern University, Chicago, IL, USA</aff><aff id="A5"><label>5</label>Institute of Computer Science and Campus Institute Data Science, University of Göttingen, Göttingen, Germany</aff><aff id="A6"><label>6</label>Max Planck Institute for Dynamics and Self-Organization, Göttingen, Germany</aff><aff id="A7"><label>7</label>Norwegian University of Science and Technology, Trondheim, Norway</aff><author-notes><corresp id="CR1"><bold>Correspondence:</bold> <email>thomas.euler@cin.uni-tuebingen.de</email></corresp><fn id="FN1"><label>8</label><p id="P1">Current affiliation: National Institute of Neurological Disorders and Stroke, National Institutes of Health, Bethesda, MD, USA</p></fn><fn id="FN2"><label>9</label><p id="P2">Current affiliation: Department of Neuroscience, Baylor College of Medicine, Houston, TX, USA</p></fn></author-notes><pub-date pub-type="nihms-submitted"><day>04</day><month>12</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>01</day><month>12</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P3">The retina transforms patterns of light into visual feature representations supporting behaviour. These representations are distributed across various types of retinal ganglion cells (RGCs), whose spatial and temporal tuning properties have been extensively studied in many model organisms, including the mouse. However, it has been difficult to link the potentially nonlinear retinal transformations of natural visual inputs to specific ethological purposes. Here, we discover a novel selectivity to chromatic contrast in an RGC type that allows the detection of transitions of the horizon across a retinal region. We trained a convolutional neural network (CNN) model on large-scale functional recordings of RGC responses to natural mouse movies, and then used this model to search <italic>in silico</italic> for stimuli that maximally excite distinct types of RGCs. This procedure predicted centre colour-opponency in transient Suppressed-by-Contrast RGCs (tSbC), a cell type whose function is being debated. We confirmed experimentally that these cells indeed responded very selectively to Green-OFF, UV-ON contrasts, which we found to be characteristic of transitions from ground to sky in the visual scene, as might be elicited by head- or eye-movements across the horizon. Because tSbCs reliably detected these transitions, we suggest a role for this RGC type in providing contextual information (i.e. sky or ground) necessary for the selection of appropriate behavioural responses to other stimuli, such as looming objects. Our work showcases how a combination of experiments with natural stimuli and computational modelling allows discovering novel types of stimulus selectivity and identifying their potential ethological relevance.</p></abstract><kwd-group><kwd>retina</kwd><kwd>colour vision</kwd><kwd>computational modelling</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P4">Sensory systems evolved to generate representations of an animal’s natural environment useful for survival and procreation (<xref ref-type="bibr" rid="R1">1</xref>). These environments are complex and high-dimensional, and different features are relevant for different species (reviewed in (<xref ref-type="bibr" rid="R2">2</xref>)). As a consequence, the representations are adapted to an animal’s needs: features of the world relevant for the animal are represented with enhanced precision, whereas less important features are discarded. Sensory processing is thus best understood within the context of the environment an animal evolved in and that it interacts with (reviewed in (<xref ref-type="bibr" rid="R3">3</xref>, <xref ref-type="bibr" rid="R4">4</xref>)).</p><p id="P5">The visual system is well-suited for studying sensory processing, as the first features are already extracted at its front-end, the retina (reviewed in (<xref ref-type="bibr" rid="R2">2</xref>, <xref ref-type="bibr" rid="R5">5</xref>)). In the mouse, this experimentally well-accessible tissue gives rise to more than 30 parallel feature channels (<xref ref-type="bibr" rid="R6">6</xref>–<xref ref-type="bibr" rid="R9">9</xref>), represented by different types of retinal ganglion cells (RGCs), whose axons send information to numerous visual centres in the brain (<xref ref-type="bibr" rid="R10">10</xref>). Some of these channels encode basic features, such as luminance changes and motion, that are only combined in downstream areas to support a range of behaviours such as cricket hunting in mice (<xref ref-type="bibr" rid="R11">11</xref>). Other channels directly extract specific features from natural scenes necessary for specific behaviours. For instance, transient OFF-<italic>α</italic> cells trigger freezing or escape behaviour in response to looming stimuli (<xref ref-type="bibr" rid="R12">12</xref>–<xref ref-type="bibr" rid="R14">14</xref>).</p><p id="P6">For many RGC types, however, we lack understanding of the features they encode and how these link to behaviour (<xref ref-type="bibr" rid="R15">15</xref>). One reason for this is that the synthetic stimuli commonly used to study retinal processing fail to drive retinal circuits “properly” and, hence, cannot uncover critical response properties triggered in natural environments. Colour, for example, is a salient feature in nature, and the mouse visual system dedicates intricate circuitry to the processing of chromatic information (<xref ref-type="bibr" rid="R16">16</xref>–<xref ref-type="bibr" rid="R21">21</xref>). Studies using synthetic stimuli have revealed nonlinear and centre-surround interactions between colour channels, but it is not clear how these are engaged in retinal processing of natural environments.</p><p id="P7">Indeed, stimuli capturing the statistics of natural environments have revealed a larger complexity in retinal spatial nonlinearities than had been previously described based on simpler synthetic stimuli (<xref ref-type="bibr" rid="R23">23</xref>). Such nonlinearities, crucial for the encoding of natural stimuli, cannot be captured by Linear-Nonlinear (LN) models of retinal processing, and several improvements over LN models have been proposed for the identification of receptive fields (RF) (reviewed in (<xref ref-type="bibr" rid="R24">24</xref>)). In recent years, however, convolutional neural network (CNN) models have become the state-of-the-art approach for predictive modelling of visual processing, both in the retina (<xref ref-type="bibr" rid="R25">25</xref>–<xref ref-type="bibr" rid="R27">27</xref>), as well as in higher visual areas (<xref ref-type="bibr" rid="R28">28</xref>–<xref ref-type="bibr" rid="R31">31</xref>). In the cortex, two recent studies took the CNN modelling approach further, beyond response prediction, by probing the networks for stimuli that would maximally excite the modelled neurons (<xref ref-type="bibr" rid="R32">32</xref>, <xref ref-type="bibr" rid="R33">33</xref>). The resulting <italic>maximally exciting inputs</italic> (MEIs) represent a nonlinear generalisation of linear RFs and were more complex and diverse than expected based on previous results obtained with synthetic stimuli and linear methods. Leveraging the power of this approach, another study high-lighted the ethological relevance of colour by uncovering a state-dependent shift in chromatic preference of V1 neurons, a shift that could facilitate the detection of aerial predators against a UV-bright sky (<xref ref-type="bibr" rid="R34">34</xref>).</p><p id="P8">Here, we combined the power of CNN-based modelling with large-scale recordings from RGCs to investigate colour processing in the mouse retina under natural stimulus conditions. Since mouse photoreceptors are sensitive to green and UV light (<xref ref-type="bibr" rid="R35">35</xref>), we recorded RGC responses to stimuli capturing the chromatic composition of natural mouse environments in these two chromatic channels. A model-guided search for MEIs in chromatic stimulus space predicted a novel type of chromatic tuning in transient Suppressed-by-Contrast (tSbC) RGCs, a type whose function is being debated vigorously (<xref ref-type="bibr" rid="R36">36</xref>–<xref ref-type="bibr" rid="R38">38</xref>).</p><p id="P9">A detailed <italic>in-silico</italic> characterisation followed up by experimental validation <italic>ex-vivo</italic> confirmed this cell type’s pronounced and unique selectivity for full-field changes from green-dominated to UV-dominated scenes, a type of visual input that matches the scene statistics of horizon transitions (<xref ref-type="bibr" rid="R22">22</xref>, <xref ref-type="bibr" rid="R39">39</xref>, <xref ref-type="bibr" rid="R40">40</xref>). We therefore suggest a role for tSbC RGCs in detecting horizon transitions, a feature that may support important functions, such as indicating context switches in the visual field (ground vs. sky). These findings showcase the utility of our approach for generating hypotheses about the ethological relevance of sensory representations.</p></sec><sec id="S2" sec-type="results"><title>Results</title><p id="P10">Here, we investigated colour processing in the mouse retina under natural stimulus conditions. To this end, we trained a CNN model on RGC responses to a movie covering both achromatic and chromatic contrasts occurring naturally in the mouse environment, and then performed a model-guided search for stimuli that maximise the responses of RGCs.</p><sec id="S3"><title>Mouse RGCs display diverse responses to a natural movie stimulus</title><p id="P11">Using two-photon population Ca<sup>2+</sup> imaging, we recorded responses from 8,388 cells (in 72 recording fields across 32 retinae) in the ganglion cell layer (GCL) of the isolated mouse retina (<xref ref-type="fig" rid="F1">Fig. 1a</xref>) to a range of visual stimuli. Since complex interactions between colour channels have been mostly reported in the ventral retina and opsin-transitional zone, we focused our recordings on these regions (<xref ref-type="bibr" rid="R19">19</xref>, <xref ref-type="bibr" rid="R20">20</xref>). The stimuli included two achromatic synthetic stimuli – a contrast and frequency modulation (“chirp” stimulus) and a bright-on-dark bar moving in eight directions (“moving bar”, MB) – to identify the functional cell type (see below), as well as a dichromatic natural movie (<xref ref-type="fig" rid="F1">Fig. 1b,c,d</xref>). The latter was composed of footage recorded outside in the field using a camera that captured the spectral bands (UV and green; (<xref ref-type="bibr" rid="R22">22</xref>)) to which mouse photoreceptors are sensitive <inline-formula><mml:math id="M1"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>S</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mn>360</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mn>510</mml:mn><mml:mspace width="0.2em"/><mml:mtext>nm</mml:mtext></mml:mrow></mml:math></inline-formula> for S- and M-cones, respectively (<xref ref-type="bibr" rid="R35">35</xref>)). We used 113 different movie clips, each lasting 5 s, that were displayed in pseudo-random order. Five of these constituted the test set and were repeated three times: at the beginning, in the middle and at the end of the movie presentation, thereby allowing to assess the reliability of neuronal responses across the recording (<xref ref-type="fig" rid="F1">Fig. 1b</xref>, top).</p><p id="P12">The responses elicited by the synthetic stimuli and the natural movie were diverse, displaying ON (<xref ref-type="fig" rid="F1">Fig. 1d</xref>, rows 4-9), ON-OFF (row 3) and OFF (rows 1 and 2), as well as sustained and transient characteristics (e.g. rows 8 and 4, respectively). Some responses were suppressed by temporal contrast (generally, rows 10, 11; at high contrast and frequency, row 9). A total of 6,984 GCL cells passed our response quality criteria (Methods); 3,527 cells could be assigned to one of 32 previously characterised functional RGC groups (<xref ref-type="bibr" rid="R7">7</xref>) based on their responses to the chirp and moving bar stimuli using our recently developed classifier (<xref ref-type="fig" rid="F1">Fig. 1e</xref>; <xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S1a</xref>) (<xref ref-type="bibr" rid="R41">41</xref>). Cells assigned to any of groups 33-46 were considered displaced amacrine cells and were not analysed in this study (for detailed filtering pipeline, see <xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S1c</xref>).</p></sec><sec id="S4"><title>CNN model captures diverse tuning of RGC groups and predicts MEIs</title><p id="P13">We trained a CNN model on the RGCs’ movie responses (<xref ref-type="fig" rid="F2">Fig. 2a</xref>) and evaluated model performance as the correlation between predicted and trial-averaged measured test responses, <inline-formula><mml:math id="M2"><mml:mrow><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="F2">Fig. 2b</xref>). This metric can be interpreted as an estimate of the achieved fraction of the maximally achievable correlation (see Methods). The mean correlation per RGC group ranged from 0.32 (G<sub>14</sub>) to 0.79 (G<sub>24</sub>) (<xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S1b</xref>) and achieved an average of 0.48 (for all N=3,527 cells passing filtering steps 1-3, <xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S1c</xref>). This established that the model generally predicted RGC responses to the natural movie stimulus very well. We also tested the performance of our nonlinear model against a linearised version (Methods) and found that the nonlinear CNN model achieved a higher test set correlation for all RGC groups (average correlation linear model: 0.38; G<sub>14</sub>: 0.2, G<sub>24</sub>: 0.65, <xref ref-type="fig" rid="F2">Fig. 2c</xref>).</p><p id="P14">To identify stimuli that optimally drive the potentially nonlinear RGCs, we next synthesised maximally exciting inputs (MEIs; (<xref ref-type="bibr" rid="R32">32</xref>)). We used gradient ascent on a randomly initialised, contrast- and range-constrained input to find the stimulus that maximised the mean activation of a given model neuron within a short time window (0.66 s; Methods; <xref ref-type="fig" rid="F2">Fig. 2d</xref>). The fixed contrast budget was shared across the two colour channels.</p></sec><sec id="S5"><title>CNN model predicts centre colour-opponency in RGC group G<sub>28</sub></title><p id="P15">The resulting MEIs were short, dichromatic movie clips; their spatial, temporal, and chromatic properties and interactions thereof are best appreciated in lower-dimensional visualisations (<xref ref-type="fig" rid="F3">Fig. 3a–c</xref>; more example MEIs in <xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S2</xref>).</p><p id="P16">To analyse the MEIs in terms of these properties, we decomposed them into their spatial and temporal components, separately for green and UV, and parameterised the spatial component as a Difference-of-Gaussians (DoG) (<xref ref-type="bibr" rid="R40">40</xref>) (N=1,613 out of 1,947, see Methods). We then located MEIs along the axes in stimulus space corresponding to three properties: centre size, mean temporal frequency, and centre contrast, separately for green and UV (<xref ref-type="fig" rid="F3">Fig. 3d-f</xref>). These MEI properties reflect RGC response properties classically probed with synthetic stimuli, such as spots of different sizes (<xref ref-type="bibr" rid="R6">6</xref>), temporal frequency modulations (<xref ref-type="bibr" rid="R7">7</xref>), and stimuli of varying chromatic contrast (<xref ref-type="bibr" rid="R19">19</xref>, <xref ref-type="bibr" rid="R20">20</xref>). The MEIs predicted by the model reproduce known properties of RGC groups (<xref ref-type="fig" rid="F3">Fig. 3g-i</xref>). For example, sustained ON <italic>α</italic> RGCs (G<sub>24</sub>), which are known to prefer large stimuli (<xref ref-type="bibr" rid="R7">7</xref>, <xref ref-type="bibr" rid="R36">36</xref>), had MEIs with large centres (G<sub>24</sub>, N=20 cells: green centre size, mean ± SD: 195 ± 82 <italic>μm;</italic> UV centre size 178 ± 45 <italic>μ</italic>m; average across all RGC groups: green 148 ± 42 <italic>μ</italic>m, UV 141 ± 42 <italic>μ</italic>m; see <xref ref-type="fig" rid="F3">Fig. 3g</xref>). The MEI’s temporal frequency relates to the temporal frequency preference of an RGC: MEIs of G<sub>20</sub> and G<sub>21</sub>, termed ON high frequency and ON low frequency (<xref ref-type="bibr" rid="R7">7</xref>), had high and low average temporal frequency, respectively (G<sub>20</sub>, N=40 cells, green, mean ± SD: 2.71 ± 0.16 Hz, UV 2.86 ± 0.22 Hz; G<sub>21</sub>, N=50 cells, green, mean ± SD: 2.32 ± 0.63 Hz, UV 1.98 ± 0.5 Hz; see <xref ref-type="fig" rid="F3">Fig. 3h</xref>). Finally, the contrast of an MEI reflects what is traditionally called a cell’s ON vs. OFF preference: MEIs of ON and OFF RGCs had positive and negative contrasts, respectively (<xref ref-type="fig" rid="F3">Fig. 3i</xref>). An ON-OFF preference can be interpreted as a tuning map with two local optima - one in the OFF- and one in the ON-contrast regime. Consequently, the MEI optimisation resulted in MEIs with ON or OFF contrast, depending on the relative strengths of the two optima and on the initial conditions (<xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S2</xref>, G<sub>10</sub>). Together with the consistency of MEIs within functional RGC groups (<xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S2</xref>), these results provided strong evidence that RGCs grouped based on responses to synthetic stimuli (chirp and MB) also form functional groups in natural movie response space.</p><p id="P17">Our goal was to explore chromatic tuning of RGCs, specifically focusing on regions in chromatic stimulus space where a given stimulus property differs for green and UV. Therefore, for centre size and temporal frequency, we asked, which RGC groups contributed to the 5% MEIs furthest away from the diagonal (i. e. achromatic tuning) (<xref ref-type="fig" rid="F3">Fig. 3g, h, j, k</xref>). These 5% MEIs furthest away from the diagonal were almost exclusively contributed by ON cells; and among these, more so by slow than by fast ON cells. MEI contrast needed to be analysed differently than size and temporal frequency for two reasons. First, due to the dominance of UV-sensitive S-opsin in the ventral retina (<xref ref-type="bibr" rid="R16">16</xref>) and the fixed contrast budget shared across channels (Methods), the contrast of most MEIs is strongly shifted towards UV (<xref ref-type="fig" rid="F3">Fig. 3i</xref>). Second, contrast in green and UV can not only vary along positive valued axes (as is the case for size and temporal frequency), but can also take on opposite signs, resulting in colour-opponent stimuli. Whereas most MEIs had the same contrast polarity in both colour channels (i.e. both ON or OFF, <xref ref-type="fig" rid="F3">Fig. 3c</xref>, blue and turquoise trajectories), some MEIs had opposing contrast polarities in UV and green (<xref ref-type="fig" rid="F3">Fig. 3c</xref>, orange trajectory, and <xref ref-type="fig" rid="F3">Fig. 3i</xref>, upper left quadrant). Thus, for contrast we asked which RGC groups contributed to colour-opponent MEIs (i.e. MEIs in the colour-opponent, upper left or lower right quadrant in <xref ref-type="fig" rid="F3">Fig. 3i</xref>). Again, slow ON RGCs made up most of the cells with colour-opponent MEIs. Here, G<sub>28</sub> stood out: 66% (24/36) of all cells of this group had colour-opponent MEIs (UV<sup>ON</sup>-green<sup>OFF</sup>), making up 39% of all colour-opponent MEIs, followed by G<sub>22</sub> that contributed another 15% of colour-opponent MEIs. This was not a form of centre-surround colour-opponency as described before (<xref ref-type="bibr" rid="R19">19</xref>), but rather a centre-opponency (“co-extensive” colour-opponent RF; reviewed in (<xref ref-type="bibr" rid="R42">42</xref>)), as can be seen in the lower-dimensional visualisations (<xref ref-type="fig" rid="F3">Fig. 3a,b</xref>, right column; <xref ref-type="fig" rid="F3">3c</xref>, orange trajectory).</p><p id="P18">In conclusion, our model-guided <italic>in-silico</italic> exploration of chromatic stimulus space revealed a variety of preferred stimuli that captured known properties of RGC groups, and high-lighted G<sub>28</sub> as exhibiting a previously unknown preference for centre colour-opponent, UV<sup>ON</sup>-green<sup>OFF</sup> stimuli.</p></sec><sec id="S6"><title>Experiments confirm selectivity for chromatic contrast</title><p id="P19">Next, we verified experimentally that the MEIs predicted for a given RGC group actually drive cells of that group optimally. To this end, we performed new experiments in which we added to our battery of stimuli a number of MEIs chosen according to the following criteria: We wanted the MEIs to (<italic>i</italic>) span the response space (ON, ON-OFF, OFF, transient, sustained, and contrast-suppressed) and (ii) to represent both well-described RGC types, such as <italic>α</italic> cells (i.e. G<sub>5,24</sub>), as well as poorly understood RGC types, such as suppressed-by-contrast cells (G<sub>28,31,32</sub>) (<xref ref-type="fig" rid="F4">Fig. 4a</xref>). We therefore chose MEIs of RGCs from groups G<sub>1</sub> (OFF local), G<sub>5</sub> (OFF <italic>α</italic> sustained), G10 (ON-OFF local-edge), G<sub>18</sub> (ON transient), G<sub>20</sub> (ON high frequency), G<sub>21</sub> (ON low frequency), G<sub>23</sub> (ON mini <italic>α</italic>), G<sub>24</sub> (sustained ON <italic>α</italic>), G<sub>28</sub> (ON contrast suppressed), G<sub>31</sub> (OFF suppressed 1), and G<sub>32</sub> (OFF suppressed 2). For simplicity, in the following we refer to the MEI of an RGC belonging to group <italic>g</italic> as group <italic>g</italic>’s MEI, or MEI <italic>g.</italic></p><p id="P20">We presented these MEIs on a regularly spaced 5 × 5 grid to achieve approximate centring of stimuli on RGC RFs in the recording field (<xref ref-type="fig" rid="F4">Fig. 4b,c</xref>). For these recordings, we fit models whose readout parameters allowed us to estimate the RGCs’ RF locations. We used these RF location estimates to calculate a spatially weighted average of the responses to the MEIs displayed at different locations, weighting the response at each location proportional to the RF strengths at those locations (<xref ref-type="fig" rid="F4">Fig. 4b</xref>, red highlight, and <xref ref-type="fig" rid="F4">Fig. 4d</xref>, top). We then performed the same experiment <italic>in-silico</italic>, confirming that the model accurately predicts responses to the MEIs (<xref ref-type="fig" rid="F4">Fig. 4d</xref>, bottom; <xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S3</xref>). These experiments allowed us to evaluate MEI responses at the RGC group level (<xref ref-type="fig" rid="F4">Fig. 4e–f</xref>; <xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S2</xref>).</p><p id="P21">We expected RGCs to show a strong response to their own group’s MEI, a weaker response to the MEIs of functionally related groups, and no response to MEIs of groups with different response profiles. Indeed, most RGC groups exhibited their strongest (G<sub>5</sub>,<sub>20</sub>,<sub>21</sub>,<sub>28</sub>,<sub>32</sub>) or second-strongest (G<sub>1</sub>,<sub>10</sub>,<sub>23</sub>) response to their own group’s MEI (<xref ref-type="fig" rid="F4">Fig. 4g</xref>, top). Conversely, RGC groups from opposing regions in response space showed no response to each others’ MEIs (e.g. G<sub>1</sub>,<sub>5</sub> (OFF cells) vs. G<sub>21-28</sub> (slow ON cells)). The model’s predictions showed a similar pattern (<xref ref-type="fig" rid="F4">Fig. 4g</xref>, bottom), thereby validating the model’s ability to generalise to the MEI stimulus regime.</p><p id="P22">Notably, G<sub>28</sub> RGCs responded very selectively to their own MEI 28, displaying only weak responses to most other MEIs (<xref ref-type="fig" rid="F4">Fig. 4f,g</xref>, selectivity index G<sub>28</sub> to MEI 28 <italic>SI</italic><sub><italic>G</italic></sub><sub>28</sub> (<xref ref-type="bibr" rid="R28">28</xref>) defined as the average difference in response between MEI 28 and all other MEIs in units of standard deviation of the response, mean ± SD: 2.58 ± 0.76; see Methods). This was in contrast to other RGC groups, such as G<sub>23</sub> and G24, that responded strongly to MEI 28, but also to other MEIs from the slow ON response regime (<xref ref-type="fig" rid="F4">Fig. 4g</xref>, top; <xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S3</xref>, SI<sub><italic>G</italic><sub>23</sub></sub> (<xref ref-type="bibr" rid="R28">28</xref>), mean ± SD: 1.04±0.69, <sub>SI<italic>G</italic></sub><sub>24</sub> (<xref ref-type="bibr" rid="R28">28</xref>), mean ± SD: 1.01 ± 0.46).</p></sec><sec id="S7"><title>Chromatic contrast selectivity derives from a warped representation of stimulus space</title><p id="P23">We hypothesised that the selectivity of G<sub>28</sub> to MEI 28 derives from a warping of stimulus space that amplifies the distance between MEI 28 and all other MEIs in neuronal response space, compared to their original distances in stimulus space. We tested this hypothesis in the framework of representational similarity analysis (RSA) which allows comparing representations in terms of their geometry (<xref ref-type="bibr" rid="R43">43</xref>).</p><p id="P24">We considered two different kinds of representations of MEIs. First, a representation <italic>ϕ<sub>s</sub></italic> in stimulus space, where an MEI’s location is defined by a vector holding its pixel values; and second, a representation <italic>ϕ<sub>ν</sub></italic>(<italic>g</italic>) for each RGC group <italic>g</italic>, where an MEI’s location is defined by a response vector of dimension <italic>n</italic>, with <italic>n</italic> equal to the number of neurons in RGC group <italic>g</italic> (<xref ref-type="fig" rid="F5">Fig. 5a,b</xref>, top panels). <italic>ϕ<sub>ν</sub>(<sub>g</sub></italic>) thus denotes the representation of MEIs in the neuronal response space of RGC group <italic>g</italic>. Based on these representations, we computed pairwise distances between MEIs and summarised them in representational dissimilarity matrices (RDMs), whose rows and columns are indexed by stimulus (<xref ref-type="fig" rid="F5">Fig 5a,b, bottom panels; and c</xref>; <xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S4a</xref>). RDMs capture the geometry of a representation (<xref ref-type="bibr" rid="R44">44</xref>). Therefore, we next correlated RDMs to yield a measure Γ(<italic>ϕ<sub>ν(g)</sub>, ϕ<sub>s</sub></italic>) that quantifies the similarity between the geometries of different representations (see Methods). This analysis showed that the representational geometry of G<sub>28</sub> is substantially warped relative to stimulus space, and more so than for most other RGC groups (<italic>N<sub>28</sub></italic> = 12, Γ(<italic>ϕ</italic><sub><italic>ν</italic>(28)</sub>,<italic>ϕ<sub>s</sub></italic>) = 0.14, bootstrapped 95% confidence interval CI<sub>95</sub> = [-0.05,0.20], mean ± SD across all groups: 0.61 ± 0.23; <xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S4b</xref>, c, see <xref ref-type="sec" rid="S15">Methods</xref>). This warping sacrifices discriminability in large regions of the stimulus space for increased discriminability of stimuli in the region of stimulus space that contains the centre colour-opponent MEIs 28 (<xref ref-type="bibr" rid="R44">44</xref>). We therefore hypothesised that the representation of visual input formed by G<sub>28</sub> might serve to detect an ethologically relevant, colour-opponent feature from the visual scene. What may be this feature?</p></sec><sec id="S8"><title>Warped representation allows for detection of ground-to-sky transitions</title><p id="P25">Studies analysing visual scenery from the mouse’s perspective have repeatedly found that chromatic contrast changes strongly at the horizon (<xref ref-type="bibr" rid="R20">20</xref>, <xref ref-type="bibr" rid="R22">22</xref>, <xref ref-type="bibr" rid="R39">39</xref>, <xref ref-type="bibr" rid="R40">40</xref>). Because the MEI of G<sub>28</sub> consists of a large, slow and sustained change in luminance from green to UV, we hypothesised that the ethologically relevant feature represented by G<sub>28</sub> might be a transition of the horizon across its RF, as elicited by head or eye movements. Being able to detect such transitions is useful for multiple reasons: For example, it could contribute to aligning the visual field with the horizon (<xref ref-type="bibr" rid="R45">45</xref>, <xref ref-type="bibr" rid="R46">46</xref>) or signal the brain a change in context (i.e. ground vs. sky), which may help interpreting signals in other RGC channels.</p><p id="P26">To test if G<sub>28</sub> responds to such a stimulus, we used the transitions between movie clips (<italic>inter-clip transitions;</italic> cf. <xref ref-type="fig" rid="F1">Fig. 1b</xref>) as a proxy for the type of visual input elicited by head or eye movements: ground-to-ground and sky-to-sky transitions for horizontal movements, and ground-to-sky and sky-to-ground transitions for vertical movements crossing the horizon. We then calculated the contrast of these transitions in the green and UV channel and mapped them to the chromatic contrast stimulus space (<xref ref-type="fig" rid="F6">Fig. 6a</xref>). We found that ground-to-ground and sky-to-sky transitions were distributed along the diagonal, whereas the two transitions resembling visual input elicited by vertical movements crossing the horizon fell into the two colour-opponent quadrants: sky-to-ground transitions in the lower right quadrant, and ground-to-sky transitions in the upper left quadrant (<xref ref-type="fig" rid="F6">Fig. 6a,b</xref>). Thus, UV<sup>ON</sup>-green<sup>OFF</sup> MEIs 28 share a location in stimulus space with ground-to-sky transitions in terms of chromatic contrast (cf. <xref ref-type="fig" rid="F3">Fig 3i</xref>).</p><p id="P27">Do G<sub>28</sub> RGCs indeed respond strongly to ground-to-sky transitions, i.e. to the “naturally occurring version” of their MEIs? To answer this question, we extracted the RGC responses to the inter-clip transitions, thereby mapping out their tuning across chromatic contrasts (<xref ref-type="fig" rid="F6">Fig. 6c-e</xref>; <xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S5a, b</xref>), and then averaged the resulting single-cell tuning maps for each RGC group (<xref ref-type="fig" rid="F6">Fig. 6f,g</xref>). G<sub>28</sub> is most strongly tuned to full-field transitions in the upper left quadrant containing mostly ground-to-sky inter-clip transitions (<xref ref-type="fig" rid="F6">Fig. 6f</xref>) – unlike, for example, non-colour-opponent reference RGC groups from the slow ON and OFF response regime (<xref ref-type="fig" rid="F6">Fig. 6g,h</xref>).</p><p id="P28">Could a downstream visual area detect ground-to-sky transitions based on input from G<sub>28</sub> RGCs? To answer this question, we performed a linear detection analysis for each RGC by sliding a threshold across its responses to the inter-clip transitions, classifying all transitions that elicited an above-threshold response as ground-to-sky, and evaluating false-positive and true-positive rates (FPR and TPR, respectively) for each threshold (<xref ref-type="fig" rid="F6">Fig. 6i</xref> for ventral cells; for dorsal cells, see <xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S6h</xref>). Plotting the resulting TPRs for all thresholds as a function of FPRs yields a receiver operating characteristic (ROC) curve (<xref ref-type="bibr" rid="R47">47</xref>) (<xref ref-type="fig" rid="F6">Fig. 6i</xref>, middle). The area under this curve (AUC) can be used as a measure of detection performance: it is equivalent to the probability that a given RGC will respond more strongly to a ground-to-sky transition than to any other type of transition. Indeed, G<sub>28</sub> RGCs achieved the highest AUC on average (<xref ref-type="fig" rid="F6">Fig. 6i</xref>, bottom, and <xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S5c</xref>; G<sub>28</sub>, mean ± SD AUC (N=78 cells): 0.68 ± 0.08; all other groups (N=3,449): 0.52 ± 0.09, △AUC = 0.16, bootstrapped 95% confidence interval CI95 = [0.14,0.18], Cohen’s <italic>d =</italic> 1.82, two-sample permutation test G<sub>28</sub> vs. all other groups (see Methods): <italic>p</italic> = 0 with 100,000 permutations; next-best performing G<sub>24</sub> (N=29): 0.61 ± 0.09, △AUC = 0.06, bootstrapped 95% confidence interval CI<sub>95</sub> = [0.03,0.1], Cohen’s <italic>d</italic> = 0.78, two-sample permutation test G<sub>28</sub> vs. G<sub>24</sub>: <italic>p</italic> = 0.00043 with 100,000 permutations; for statistics and AUC distributions for dorsal cells, see <xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S6h</xref> legend). So far, we focused only on cells in the ventral retina. However, the horizon may also appear in the lower visual field, that is, on the dorsal retina, where RGCs receive weaker UV input (<xref ref-type="bibr" rid="R19">19</xref>). Therefore, we recorded additional fields in the dorsal retina (<xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S6a</xref>). Also here, G<sub>28</sub> displayed the strongest tuning to ground-to-sky transitions among all dorsal RGCs (<xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S6b-g</xref>).</p><p id="P29">Hence, a downstream area, reading out from a single RGC group, would achieve the best performance in detecting ground-to-sky transitions if it based its decisions on inputs from G<sub>28</sub> RGCs.</p></sec><sec id="S9"><title>G<sub>28</sub> corresponds to the transient Suppressed-by-Contrast RGC type</title><p id="P30">Next, we sought to identify which RGC type G<sub>28</sub> corresponds to. In addition to its unique centre colour-opponency, the responses of G<sub>28</sub> displayed a pronounced transient suppression to temporal contrast modulations (cf. chirp response in <xref ref-type="fig" rid="F1">Fig. 1e</xref>). Therefore, we hypothesised that G<sub>28</sub> corresponds to the transient Suppressed-by-Contrast (tSbC) RGC type (<xref ref-type="bibr" rid="R37">37</xref>, <xref ref-type="bibr" rid="R38">38</xref>, <xref ref-type="bibr" rid="R48">48</xref>), which is one of three retinal SbC RGC types identified so far and is also referred to as ON delayed (OND) cell because of its delayed response onset (<xref ref-type="bibr" rid="R49">49</xref>).</p><p id="P31">To test this hypothesis, we performed cell-attached electrophysiology recordings (<xref ref-type="fig" rid="F7">Fig. 7</xref>) targeting tSbC/OND cells (N=4), identified by their responses to spots of multiple sizes (<xref ref-type="bibr" rid="R6">6</xref>), and later confirmed by their distinctive morphology ((<xref ref-type="bibr" rid="R49">49</xref>); type 73 in (<xref ref-type="bibr" rid="R8">8</xref>)) (<xref ref-type="fig" rid="F7">Fig. 7c,d</xref>). We recorded spikes in these RGCs while presenting the MEI stimuli (<xref ref-type="fig" rid="F7">Fig. 7a</xref>, top). Just like G<sub>28</sub> RGCs in the Ca<sup>2+</sup> imaging, tSbC/OND cells exhibited a pronounced selectivity for MEI 28, and were suppressed by most other MEIs (<xref ref-type="fig" rid="F7">Fig. 7a</xref>, middle and bottom). Notably, the characteristic delayed response onset was visible in both the Ca<sup>2+</sup> (<xref ref-type="fig" rid="F4">Fig. 4f</xref>, top) and electrical (<xref ref-type="fig" rid="F7">Fig. 7a</xref>) responses but was not predicted by the model (<xref ref-type="fig" rid="F4">Fig. 4f</xref>, bottom). As a control, we also recorded MEI responses of a different, well-characterised RGC type, sustained (s) ON <italic>α</italic> (G<sub>24</sub>; (<xref ref-type="bibr" rid="R50">50</xref>)) (<xref ref-type="fig" rid="F7">Fig. 7b</xref>, top; N=4). Again, the electrical recordings of the cells’ MEI responses yielded virtually the same results as the Ca<sup>2+</sup> imaging (<xref ref-type="fig" rid="F7">Fig. 7b, middle and bottom; cf</xref>. <xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S3</xref>). Crucially, sON <italic>α</italic> cells were not selective for MEI 28. These results suggest that the tSbC/OND RGC type signals horizon transitions to downstream visual centres in the brain.</p></sec></sec><sec id="S10" sec-type="discussion"><title>Discussion</title><p id="P32">We combined large-scale recordings of RGC responses to natural movie stimulation with CNN-based modelling to investigate colour processing in the mouse retina. By searching the stimulus space in <italic>silico</italic> to identify <italic>most exciting inputs</italic> (MEIs), we found a novel type of chromatic tuning in a distinct RGC type, the tSbC cells. We revealed this RGC type’s pronounced and unique selectivity for full-field changes from green-dominated to UV-dominated scenes, a stimulus that matches the statistics of ground-to-sky horizon transitions in natural scenes. Therefore, we suggest that tSbC cells may signal horizon transitions within their RF. Beyond our focus on tSbC cells, our study demonstrates the utility of an <italic>in silico</italic> approach for generating and testing hypotheses about the ethological relevance of sensory representations.</p><sec id="S11"><title>Circuit mechanisms for colour-opponency in tSbC RGCs</title><p id="P33">Most previous studies of colour-opponency in the mouse retina have identified sparse populations of colour-opponent RGCs that have not been systematically assigned to a particular functional type (<xref ref-type="bibr" rid="R19">19</xref>, <xref ref-type="bibr" rid="R20">20</xref>). The only studies that have examined the mechanisms of colour-opponency in identified mouse RGC types showed a centre-surround organisation, with RF centre and surround having different chromatic preferences (<xref ref-type="bibr" rid="R17">17</xref>, <xref ref-type="bibr" rid="R51">51</xref>); ((<xref ref-type="bibr" rid="R52">52</xref>), but see (<xref ref-type="bibr" rid="R53">53</xref>)), whereas we found that tSbC RGCs respond to spatially co-extensive colour-opponent stimuli. While centre-surround opponency has been attributed to the opsin gradient (<xref ref-type="bibr" rid="R51">51</xref>) and rod contributions in the outer retina (<xref ref-type="bibr" rid="R17">17</xref>, <xref ref-type="bibr" rid="R19">19</xref>), the circuitry for spatially co-extensive opponency in mice remains unknown. It seems unlikely, though, that the opsin gradient plays a major role in the cell’s colour opponency, because both ventral and dorsal tSbC cells preferentially responded to full-field green-to-UV transitions. In primates, spatially co-extensive colour-opponency in small bistratified RGCs is thought to arise from the selective wiring of S-ON and M/L-OFF bipolar cells onto the inner and outer dendritic strata, respectively ((<xref ref-type="bibr" rid="R54">54</xref>), but see (<xref ref-type="bibr" rid="R55">55</xref>)). A similar wiring pattern seems unlikely for tSbC RGCs, since their inner dendrites do not co-stratify with the S-ON (type 9) bipolar cells, nor do their outer dendrites costratify with the candidate M-OFF bipolar cell (type 1) (<xref ref-type="bibr" rid="R56">56</xref>). The large RF centres of the tSbC cells, extending well beyond their dendritic fields, come from a non-canonical circuit, in which tonic inhibition onto the RGC via GABA<sub>B</sub> receptors is relieved via serial inhibition from different amacrine cells using GABA<sub>C</sub> receptors (<xref ref-type="bibr" rid="R36">36</xref>). An intriguing possibility is that a colour-selective amacrine cell is part of this circuit, perhaps supporting chromatically tuned disinhibition in the absence of selective wiring from the aforementioned cone-selective bipolar cells onto the RGC.</p></sec><sec id="S12"><title>A new functional role for tSbC RGCs</title><p id="P34">Suppressed-by-contrast responses have been recorded along the early visual pathway in dorsal lateral geniculate nucleus (dLGN), superior colliculus (SC), and primary visual cortex (V1) (<xref ref-type="bibr" rid="R57">57</xref>–<xref ref-type="bibr" rid="R59">59</xref>), with their function still being debated (<xref ref-type="bibr" rid="R60">60</xref>). In the retina, three types of SbC RGCs have so far been identified (reviewed in (<xref ref-type="bibr" rid="R49">49</xref>)), among them the tSbC cell (<xref ref-type="bibr" rid="R36">36</xref>–<xref ref-type="bibr" rid="R38">38</xref>). Despite their relatively recent discovery, tSbC RGCs have been suggested to play a role in several different visual computations. The first report of their light responses in mice connected them to the SbC RGCs previously discovered in rabbit, cat, and macaque, and suggested a role in signalling self-generated stimuli, perhaps for saccade suppression (<xref ref-type="bibr" rid="R37">37</xref>). Aided by a new intersectional transgenic line to selectively label tSbC RGCs (<xref ref-type="bibr" rid="R38">38</xref>), their projections were traced to areas in SC, v- and dLGN, and nucleus of the optic tract (NOT). The latter stabilises horizontal eye movements; however, as the medial terminal nucleus (MTN), which serves stabilisation of vertical eye movements, lacks tSbC innervation, it is unclear whether and how these RGCs contribute to gaze stabilisation. A retinal study identified the circuit mechanisms responsible for some of the unique spatial and temporal response properties of tSbC cells and suggested a possible role in defocus detection to drive emmetropization in growing eyes and accommodation in adults (<xref ref-type="bibr" rid="R36">36</xref>, <xref ref-type="bibr" rid="R61">61</xref>). Here, we identified another potential role for these RGCs in vision based on the chromatic properties of their RFs: signalling visual context changes (see also next section). These different possible functional roles are not mutually exclusive, and might even be complementary in some cases, highlighting the difficulty in assigning single features to distinct RGC types (<xref ref-type="bibr" rid="R15">15</xref>). In particular, the centre colour-opponency that we discovered in tSbC RGCs could serve to enhance their role in defocus detection by adding a directional signal (myopic vs. hyperopic) based on the chromatic aberration of lens and cornea (<xref ref-type="bibr" rid="R62">62</xref>). Future studies may test these theories by manipulating these cells <italic>in vivo</italic> using the new transgenic tSbC mouse line (<xref ref-type="bibr" rid="R38">38</xref>).</p></sec><sec id="S13"><title>Behavioural relevance of horizon detection</title><p id="P35">The horizon is a prominent landmark in visual space: it bisects the visual field into two regions, ground and sky. Visual stimuli carry different meaning depending on where they occur relative to the horizon, and context-specific processing of visual inputs is necessary for selecting appropriate behavioural responses (reviewed in (<xref ref-type="bibr" rid="R63">63</xref>)). For example, it is sensible to assume that a looming stimulus above the horizon is a predator, the appropriate response to which would be avoidance (that is, escape or freezing). A similar stimulus below the horizon, however, is more likely to be harmless or even prey. To allow for time-critical perceptual decisions – predator or prey – and corresponding behavioural response selection – avoidance or approach – it might be useful to share circuitry for the joint extraction and processing of contextual and stimulus information. Notably, VGluT3-expressing amacrine cells (a “hub” for distributing information about motion) represent a shared element in upstream circuitry, providing opposite-sign input to tSbC and to RGCs implicated in triggering avoidance behaviour, such as tOFF <italic>α</italic> (<xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R50">50</xref>) and W3 cells (<xref ref-type="bibr" rid="R64">64</xref>). In downstream circuitry, SbC inputs have been found to converge with “conventional” RGC inputs onto targets in dLGN and NOT; whether tSbC axons specifically converge with tOFF <italic>α</italic> or W3 axons remains to be tested. Such convergence may allow “flagging” the activity of these RGCs with their local context (sky/threat or ground/no threat).</p></sec><sec id="S14"><title><italic>In-silico</italic> approaches to linking neural tuning and function</title><p id="P36">The modelling of retinal responses to natural stimuli has advanced our understanding of the complexity of retinal processing in recent years. As suggested in a recent review, it is helpful to consider the contributions of different studies in terms of one of three perspectives on the retinal encoding of natural scenes: The circuit perspective (“how?”), the normative perspective (“why?”), and the coding perspective (“what?”) (<xref ref-type="bibr" rid="R65">65</xref>). For example, an <italic>in-silico</italic> dissection of a CNN model of the retina offered explanations on how the surprisingly complex retinal computations, such as motion reversal, omitted stimulus response, and polarity reversal, emerge from simpler computations within retinal circuits (<xref ref-type="bibr" rid="R25">25</xref>, <xref ref-type="bibr" rid="R26">26</xref>). Taking on the normative perspective, anatomically constrained deep CNNs trained on image recognition suggested a dependency between the complexity of retinal representations and the computational power of downstream cortical networks: Whereas a computationally powerful cortex, as found in primates, can deal with faithful, linear representations of visual inputs, a simpler cortical circuitry, as found in mice, pushes feature extraction upstream, into the retina (<xref ref-type="bibr" rid="R66">66</xref>, <xref ref-type="bibr" rid="R67">67</xref>). However, the full potential of CNN models as tools for understanding sensory processing goes beyond reproducing effects that are already described in the literature. Reaping this potential for a take at the coding perspective (“what?”), Goldin and colleagues (<xref ref-type="bibr" rid="R68">68</xref>) performed a search in static stimulus space for locally optimal inputs for RGCs and found that the selectivity for positive or negative contrast in a subset of cells is context-dependent. These cells signal absolute contrast, a feature one level of abstraction and complexity above response polarity (“classical” ON vs. OFF).</p><p id="P37">Here, we developed an approach that allows investigating the complexity of retinal processing from all three perspectives: A global search for most exciting mouse RGC inputs in dynamic, chromatic stimulus space answers the question of <italic>what</italic> it is that retinal neurons encode. Classifying individual RGCs into types then allows to dissect the circuitry and to explain <italic>how</italic> specific retinal computations are implemented. And finally, interpreting the abstract features extracted by the retina against the backdrop of natural stimulus space points to <italic>why</italic> these features might be behaviourally relevant.</p></sec></sec><sec id="S15" sec-type="methods"><title>Methods</title><sec id="S16"><title>Animals and tissue preparation</title><p id="P38">All imaging experiments were conducted at the University of Tübingen; the corresponding animal procedures were approved by the governmental review board (Regierungspräsidium Tübingen, Baden-Württemberg, Konrad-Adenauer-Str. 20, 72072 Tübingen, Germany) and performed according to the laws governing animal experimentation issued by the German Government. All electrophysiological experiments were conducted at Northwestern University; the corresponding animal procedures were performed according to standards provided by Northwestern University Center for Comparative Medicine and approved by the Institutional Animal Care and Use Committee (IACUC).</p><p id="P39">For all imaging experiments, we used 4- to 15-week-old C57Bl/6 J mice (n=23; JAX 000664) of either sex (10 male, 13 female). These animals were housed under a standard 12 h day/night rhythm at 22° and 55% humidity. On the day of the recording experiment, animals were dark-adapted for at least 1 h, then anaesthetised with isoflurane (Baxter) and killed by cervical dislocation. All following procedures were carried out under very dim red (&gt; 650 nm) light. The eyes were enucleated and hemisected in car-boxygenated (95% O<sub>2</sub>, 5% CO<sub>2</sub>) artificial cerebrospinal fluid (ACSF) solution containing (in mM): 125 NaCl, 2.5 KCl, 2 CaCl<sub>2</sub>, 1 MgCl<sub>2</sub>, 1.25 NaH<sub>2</sub>PO<sub>4</sub>, 26 NaHCO<sub>3</sub>, 20 glucose, and 0.5 L-glutamine at pH 7.4. Next, the retinae were bulk-electroporated with the fluorescent Ca<sup>2+</sup> indicator Oregon—Green BAPTA-1 (OGB-1), as described earlier (<xref ref-type="bibr" rid="R69">69</xref>). In brief, the dissected retina was flat-mounted onto an Anodisc (#13, 0.2 μm pore size, GE Healthcare) with the RGCs facing up, and placed between a pair of 4-mm horizontal plate electrodes (CUY700P4E/L, Nepagene/Xceltis). A 10-<italic>μ</italic>l drop of 5mM OGB-1 (hexapotassium salt; Life Technologies) in ACSF was suspended from the upper electrode and lowered onto the retina. Next, nine pulses (≈ 9.2 V, 100 ms pulse width, at 1Hz) from a pulse generator/wide-band amplifier combination (TGP110 and WA301, Thurlby handar/Farnell) were applied. Finally, the tissue was placed into the microscope’s recording chamber, where it was perfused with carboxygenated ACSF (at ≈ 36° C) and left to recover for ≥ 30 min before recordings started. To visualise vessels and damaged cells in the red fluorescence channel, the ACSF contained ≈ 0.1 <italic>μ</italic>M Sulforhodamine-101 (SR101, Invitrogen) (<xref ref-type="bibr" rid="R70">70</xref>). All procedures were carried out under dim red (&gt; 650 nm) light.</p><p id="P40">For electrophysiology experiments, we used ChAT-Cre (JAX 006410) x Ai14 (JAX 007914) mice on a C57Bl/6J background (n=2, male, aged 27 and 30 weeks). Mice were housed with siblings in groups up to 4, fed normal mouse chow and maintained on a 12:12 h light/dark cycle. Before the experiment, mice were dark-adapted overnight and sacrificed by cervical dislocation. Retinal tissue was isolated under infrared illumination (900 nm) with the aid of night-vision goggles and IR dissection scope attachments (BE Meyers). Retinal orientation was identified using scleral landmarks (<xref ref-type="bibr" rid="R71">71</xref>), and preserved using relieving cuts in cardinal directions, with the largest cut at the dorsal retina. Retinas were mounted on 12mm poly-D-lysine coated glass affixed to a recording dish with grease, with the GCL up. Oxygenation was maintained by superfusing the dish with carboxygenated Ames medium (US Biological, A1372-25) warmed to 32 °C. For cell-attached single cell recordings, we used Symphony software (<ext-link ext-link-type="uri" xlink:href="https://symphony-das.github.io/">https://symphony-das.github.io/</ext-link>) with custom extensions (<ext-link ext-link-type="uri" xlink:href="https://github.com/Schwartz-AlaLaurila-Labs/sa-labs-extension">https://github.com/Schwartz-AlaLaurila-Labs/sa-labs-extension</ext-link>).</p><p id="P41">Owing to the exploratory nature of our study, we did not use randomisation and blinding. No statistical methods were used to predetermine sample size.</p></sec><sec id="S17"><title>Two-photon calcium imaging</title><p id="P42">We used a MOM-type two-photon microscope (designed by W. Denk; purchased from Sutter Instruments) (<xref ref-type="bibr" rid="R70">70</xref>, <xref ref-type="bibr" rid="R72">72</xref>), which was equipped with a mode-locked Ti:Sapphire laser (MaiTai-HP DeepSee, Newport Spectra-Physics) tuned to 927 nm, two fluorescence detection channels for OGB-1 (HQ 510/84, AHF/Chroma) and SR101 (HQ 630/60, AHF), and a water immersion objective (CF175 <italic>LWD</italic> × 16/0.8W, DIC N2, Nikon, Germany). Image acquisition was performed with custom-made software (ScanM by M. Müller and T.E.) running under IGOR Pro 6.3 for Windows (Wavemetrics), taking time-lapsed 64×64 pixel image scans (≈ (100 <italic>μm</italic>)<sup>2</sup> at 7.8125 Hz (<xref ref-type="fig" rid="F1">Fig. 1c</xref>). For simplicity, we refer to such a time-lapsed scan of a local population of GCL cells as a “recording”. For documenting the position of the recording fields, the retina under the microscope was oriented such that the most ventral edge pointed always towards the experimenter. In addition, higher resolution images (512×512 pixel) were acquired and recording field positions relative to the optic nerve were routinely logged.</p></sec><sec id="S18"><title>Data preprocessing</title><p id="P43">Ca<sup>2+</sup> traces were extracted for individual ROIs as described previously (<xref ref-type="bibr" rid="R7">7</xref>, <xref ref-type="bibr" rid="R19">19</xref>). Extracted traces <italic><bold>r</bold><sub>raw</sub></italic> were then detrended to remove slow drifts in the recorded signal that were unrelated to changes in the neural response. First, a smoothed version of the traces, <italic><bold>r</bold><sub>smooth</sub></italic>, was calculated by applying a Savitzky-Golay filter of 3<sup>rd</sup> polynomial order and a window length of 60 s using the SciPy implementation scipy.signal.savgol_-filter. This smoothed version was then subtracted from the raw traces to yield the detrended traces.</p><disp-formula id="FD1"><mml:math id="M3"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><p id="P44">To make traces non-negative (<italic><bold>r</bold><sub>nn</sub></italic>), we then clipped all values smaller than the 2.5<sup><italic>th</italic></sup> percentile, <italic>η<sub>2.5</sub></italic>, to that value, and then subtracted <italic>η<sub>2.5</sub></italic> from the detrended traces: <disp-formula id="FD2"><mml:math id="M4"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mn>2.5</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p id="P45">This procedure (i.e. clipping to, and subtracting <italic>η<sub>2.5</sub></italic>) was more robust than simply subtracting the minimum.</p><p id="P46">Finally, traces were then divided by the standard deviation within the time window before stimulus start at <italic>t<sub>0</sub></italic>: <disp-formula id="FD3"><mml:math id="M5"><mml:mrow><mml:mi>r</mml:mi><mml:mi/><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow/><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>:</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p id="P47">For training the model on movie response, we estimated firing rates from the detrended Ca<sup>2+</sup> traces using the package C2S (<ext-link ext-link-type="uri" xlink:href="https://github.com/lucastheis/c2s">https://github.com/lucastheis/c2s</ext-link>, Theis et al. (<xref ref-type="bibr" rid="R73">73</xref>)).</p></sec><sec id="S19"><title>Inclusion criteria</title><p id="P48">We applied a sequence of quality filtering steps to recorded cells before analysis illustrated in <xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S1c</xref>. As a first step, we applied a general response quality criterion, defined as a sufficiently reliable response to the Moving bar stimulus (as quantified by a quality index <italic>QI<sub>MB</sub> &gt;</italic> 0.6), <italic>or</italic> a sufficiently reliable response to the chirp stimulus (as quantified by a quality index <italic>QI<sub>chirp</sub> &gt;</italic> 0.35).</p><p id="P49">The quality index is defined as in ref.(<xref ref-type="bibr" rid="R7">7</xref>): <disp-formula id="FD4"><mml:math id="M6"><mml:mrow><mml:mtext>QI=</mml:mtext><mml:mfrac><mml:mrow><mml:mtext>Var</mml:mtext><mml:msub><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mi>r</mml:mi><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mtext>Var</mml:mtext><mml:msub><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>r</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
</p><p id="P50">where <italic><bold>r</bold></italic> is the T by I response matrix (time samples by stimulus repetitions) and 〈〉<sub><italic>x</italic></sub> and Var[]<sub><italic>x</italic></sub> denote the mean and variance across the indicated dimension <italic>x</italic>, respectively.</p><p id="P51">The second and third step made sure only cells were included that were assigned to a ganglion cell group (i.e., group index between 1 and 32) with sufficient confidence. Confidence is defined as the probability assigned to the predicted class by the random forest classifier (see (<xref ref-type="bibr" rid="R41">41</xref>)), and the threshold was set at ≥ 0.25.</p><p id="P52">The fourth step made sure only cells with a sufficient model prediction performance, defined as an average singletrial test set correlation of <inline-formula><mml:math id="M7"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mspace width="0.2em"/><mml:mo>&gt;</mml:mo><mml:mspace width="0.2em"/><mml:mn>.3</mml:mn></mml:mrow></mml:math></inline-formula>, were included.</p><p id="P53">All cells passing steps 1-3 were included in the horizon detection analysis (<xref ref-type="fig" rid="F6">Fig. 6</xref>); all cells passing steps 1-4 were included in the MEI analysis (<xref ref-type="fig" rid="F3">Fig. 3</xref>); the “red” cells passing steps 1-4 were included in the MEI validation analysis (<xref ref-type="fig" rid="F4">Fig. 4</xref>). In the process of analysing MEIs, we fitted DoGs to their green and UV spatial component (see <xref ref-type="sec" rid="S15">Methods</xref> section Concentric anisotropic 2D Difference-of-Gaussians fit). For the analysis of MEI properties (temporal frequency, centre size, chromatic contrast), we only included cells with a sufficient DoG goodness-of-fit, determined as a value of the cost function of &lt; .11 for both green and UV on the resulting DoG fit. This threshold was determined by visual inspection of the DoG fits and led to the inclusion of 1613 out of 1947 RGCs in the MEI property analysis.</p></sec><sec id="S20"><title>Visual stimulation</title><p id="P54">For light stimulation (imaging experiments), we projected the image generated by a digital light processing (DLP) projector (lightcrafter DPM-FE4500MKIIF, EKB Technologies Ltd) through the objective onto the tissue. The lightcrafter featured a light-guide port to couple in external, band-pass filtered UV and green LEDs (light-emitting diodes) (green: 576 BP 10, F37-576; UV: 387 BP 11, F39-387; both AHF/Chroma) (<xref ref-type="bibr" rid="R74">74</xref>). To optimise spectral separation of mouse M- and S-opsins, LEDs were band-pass filtered (390/576 dual-band, F59-003, AHF/Chroma). LEDs were synchronised with the microscope’s scan retrace. Stimulator intensity (as photoisomerization rate, 10<sup>3</sup> <italic>P*s<sup>–1</sup></italic> per cone) was calibrated to range from ≈ 0.5 (black image) to ≈ 20 for M- and S-opsins, respectively. Additionally, we estimated a steady illumination component of ≈ 10<sup>4</sup><italic>P</italic>* <italic>s</italic><sup>-1</sup> per cone to be present during the recordings because of two photon excitation of photopigments (<xref ref-type="bibr" rid="R70">70</xref>, <xref ref-type="bibr" rid="R72">72</xref>). Before data acquisition, the retina was adapted to the light stimulation by presenting a binary noise stimulus (20 × 15 matrix, (40 <italic>μ</italic>m)<sup>2</sup> pixels, balanced random sequence) at 5 Hz for 5 min to the tissue.</p><p id="P55">For electrophysiology experiments, stimuli were presented using a digital projector (DPM-FE4500MKII, EKB Technologies Ltd) at a frame rate of 60 Hz and a spatial resolution of 1140 × 912 pixels (1.3 <italic>μ</italic>m per pixel) focused on the photoreceptor layer. Neutral density filters (Thorlabs), a multi-band pass filter (69000x, Chroma), and a custom LED controller circuit were used to attenuate the light intensity of stimuli either to match that of the Ca<sup>2+</sup> imaging experiments (for MEI presentation) or to range from ≈ 0-200 <italic>P*s</italic><sup>–1</sup> per rod (for cell identification). Stimuli were presented using Symphony software (<ext-link ext-link-type="uri" xlink:href="https://symphony-das.github.io/">https://symphony-das.github.io/</ext-link>) with custom extensions (<ext-link ext-link-type="uri" xlink:href="https://github.com/Schwartz-AlaLaurila-Labs/sa-labs-extension">https://github.com/Schwartz-AlaLaurila-Labs/sa-labs-extension</ext-link>).</p></sec><sec id="S21"><title>Identifying retinal ganglion cell types</title><p id="P56">To functionally identify RGC groups in the Ca<sup>2+</sup> imaging experiments, we used our default “fingerprinting” stimuli, as described earlier (<xref ref-type="bibr" rid="R7">7</xref>). These stimuli included a full-field (700 <italic>μ</italic>m in diameter) chirp stimulus, and a 300 × 1,000 <italic>μ</italic>m bright bar moving at 1,000 <italic>μ</italic>m ·<italic>s</italic><sup>–1</sup> in eight directions across the recording field (with the shorter edge leading; <xref ref-type="fig" rid="F1">Fig. 1b</xref>).</p><p id="P57">The procedure and rationale for identifying cells in the electrophysiological recordings is presented in ref. (<xref ref-type="bibr" rid="R6">6</xref>). Cells with responses that qualitatively matched that of the OND and ON <italic>α</italic> types were included in the study. Following recording, cells were filled with AlexaFluor-488 by patch pipette and imaged under a two-photon microscope. Dendrites were traced in Fiji (NIH) using the SNT plugin (<xref ref-type="bibr" rid="R75">75</xref>). Dendritic arbours were computationally flattened using a custom MATLAB tool (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.6578530">https://doi.org/10.5281/zenodo.6578530</ext-link>) based on the method in ref. (<xref ref-type="bibr" rid="R76">76</xref>) to further confirm their identity as morphological type 73 from ref. (<xref ref-type="bibr" rid="R8">8</xref>).</p></sec><sec id="S22"><title>Mouse natural movies</title><p id="P58">The natural movie stimulus consisted of clips of natural scenes recording outside in the field with a specialised, calibrated camera (<xref ref-type="bibr" rid="R22">22</xref>). This camera featured a fish-eye lens, and two spectral channels, UV (band-pass filter F37-424, AHF, &gt; 90% transmission at 350–419nm) and green (F47-510, &gt; 90%, 470–550nm, AHF), approximating the spectral sensitivities of mouse opsins (<xref ref-type="bibr" rid="R35">35</xref>). In mice, eye movements often serve to stabilise the image on the retina during head movements (<xref ref-type="bibr" rid="R45">45</xref>). Therefore, the camera was also stabilised by mounting it on a gimbal. As a result, the horizon bisected the camera’s visual field.</p><p id="P59">A <italic>mouse cam movie</italic> frame contained a circular field of view (FOV) of 180° corresponding to 437 pixels along the diameter. To minimise the influence of potential chromatic and spatial aberrations introduced by the lenses, we focused on image cut-outs (crops; 30° × 26°, equivalent to 72 × 64 pixels in size) from upper and lower visual field, centred at [28°, 56°] and [-42°, -31°], respectively, relative to the horizon (for details, see (<xref ref-type="bibr" rid="R22">22</xref>)). Our <italic>stimulus movie</italic> consisted of 113 movie clips, each 150 frames (=5s) long. 108 clips were randomly reordered for each recording and split into two 54 clips-long <italic>training sequences.</italic> The remaining 5 clips formed a fixed <italic>test sequence</italic> that was presented before, in between, and after the training sequences (<xref ref-type="fig" rid="F1">Fig. 1b</xref>). To keep intensity changes at clip transitions small, we only used clips with mean intensities between 0.04 and 0.22 (for intensities in [0, 1]). For display during the experiments, intensities were then mapped to the range covered by the stimulator, i.e. [0, 255].</p></sec><sec id="S23"><title>Convolutional neural network model of the retina</title><p id="P60">We trained a convolutional neural network (CNN) model to predict responses of RGCs to a dichromatic natural movie. The CNN model consisted of two modules, a convolutional core that was shared between all neurons, and a readout that was specific for each neuron (<xref ref-type="bibr" rid="R77">77</xref>).</p><p id="P61">The core module was modelled as a two-layer convolutional neural network with 16 feature channels in each layer. Both layers consisted of space-time separable 3D convolutional kernels followed by a batch normalisation layer and an ELU nonlinearity. In the first layer, sixteen 2 × 11 × 11 × 21 (c=#channels × h=height × w=width × t=#frames) kernels were applied as valid convolution; in the second layer, sixteen 16 × 5 × 5 × 11 kernels were applied with zero padding along the spatial dimensions. We parameterised the temporal kernels as Fourier series and added one time stretching parameter per recording to account for inter-experimental variability affecting the speed of retinal processing (<xref ref-type="bibr" rid="R78">78</xref>).</p><p id="P62">In the readout, we modelled each cell’s spatial receptive field (RF) as a 2D isotropic Gaussian, parameterised as <inline-formula><mml:math id="M8"><mml:mrow><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>σ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. We then modelled the neural response as an affine function of the core feature maps weighted by the spatial RF, followed by a softplus nonlinearity.</p><p id="P63">For the linearised version of the model, the architecture was exactly the same except for the fact that there was no ELU nonlinearity after both convolutional layers.</p></sec><sec id="S24"><title>Model training and evaluation</title><p id="P64">We trained our network by minimising the Poisson loss <disp-formula id="FD5"><mml:math id="M9"><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mi>log</mml:mi><mml:msup><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p id="P65">where <italic>N</italic> is the number of neurons, <italic><bold>r</bold></italic>(<sup><italic>n</italic></sup>) is the measured and <inline-formula><mml:math id="M10"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> the predicted firing rate of neuron <italic>n</italic> for an input of duration t=50 frames. We followed the training schedule of Lurz et al. (2021)(<xref ref-type="bibr" rid="R79">79</xref>). Specifically, we used early stopping (<xref ref-type="bibr" rid="R80">80</xref>) on the correlation between predicted and measured neuronal responses on the validation set, which consisted of 15 out of the 108 movie clips. If the correlation failed to increase during any 5 consecutive passes through the entire training set (epochs), we stopped the training and restored the model to the best performing model over the course of training. We went through 4 cycles of early stopping, restoring the model to the best performing, and continuing training, each time reducing the initial learning rate of 0.01 by a learning rate decay factor of 0.3. Network parameters were iteratively optimised via stochastic gradient descent using the Adam optimiser (<xref ref-type="bibr" rid="R81">81</xref>) with a batch size of 32 and a chunk size (number of frames for each element in the batch) of 50. For all analyses and MEI generation, we used an ensemble of models as described in ref. (<xref ref-type="bibr" rid="R34">34</xref>). Briefly, we trained 5 instances of the same model initialised with different random seeds. Inputs to the ensemble model were passed to each member and the final ensemble model prediction was obtained by averaging the outputs of the 5 members. For ease of notation, we thus redefine <inline-formula><mml:math id="M11"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> to be the <italic>ensemble</italic> model prediction.</p><p id="P66">After training, we evaluated model performance for each modelled neuron <italic>n</italic> as the correlation to the mean, i.e. the correlation between predicted response <inline-formula><mml:math id="M12"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and measured response <italic>r</italic><sup>(<italic>n</italic></sup>) to the held-out test sequence, the latter averaged across 3 repetitions <inline-formula><mml:math id="M13"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:mspace width="0.2em"/><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Unlike the single-trial correlation <inline-formula><mml:math id="M14"><mml:mrow><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> which is always limited to values &lt; 1 by inherent neuronal noise, a perfect model can in theory achieve a value of 1 for the correlation to the mean, in the limit of infinitely many repetitions when the sample average <inline-formula><mml:math id="M15"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is a perfect estimate of the true underlying response <italic>ρ</italic><sup>(<italic>n</italic></sup>). The observed correlation to the mean can thus be interpreted as an estimate of the fraction of the maximally achievable correlation achieved by our model. For deciding which cells to exclude from analysis, we used average single-trial correlation <inline-formula><mml:math id="M16"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> since this measure reflects both model performance as well as reliability of the neuronal response to the movie stimulus for neuron <italic>n</italic> (see also <xref ref-type="sec" rid="S15">Methods</xref> section on Inclusion criteria).</p></sec><sec id="S25"><title>Synthesising MEIs</title><p id="P67">We synthesised maximally exciting inputs for RGCs as described previously (<xref ref-type="bibr" rid="R32">32</xref>). Formally, for each model neuron <italic>n</italic> we wanted to find <disp-formula id="FD6"><label>(1)</label><mml:math id="M17"><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>arg</mml:mi><mml:mi>max</mml:mi></mml:mrow><mml:mi>x</mml:mi></mml:munder><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>30</mml:mn><mml:mo>:</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula></p><p id="P68">i.e. the input <italic><bold>x</bold></italic>*<sup>(<italic>n</italic></sup>) where the model neuron’s response <inline-formula><mml:math id="M18"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>30</mml:mn><mml:mo>:</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, averaged across frames 30 to 50, attains a maximum, subject to norm and range constraints (see below). To this end, we randomly initialised an input <inline-formula><mml:math id="M19"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℛ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>×</mml:mo><mml:mi>w</mml:mi><mml:mo>×</mml:mo><mml:mi>h</mml:mi><mml:mo>×</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> of duration <italic>t</italic>=50 frames with Gaussian white noise, and then iteratively updated <inline-formula><mml:math id="M20"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> according to the gradient of the model neuron’s response: <disp-formula id="FD7"><label>(2)</label><mml:math id="M21"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mfrac><mml:mi>δ</mml:mi><mml:mrow><mml:mi>δ</mml:mi><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>30</mml:mn><mml:mo>:</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula></p><p id="P69">where <italic>λ</italic> = 10 was the learning rate. The optimisation was performed using Stochastic Gradient Descent (SGD), and was subject to a norm and a range constraint. The norm constraint was applied jointly across both channels and ensured that the L2 norm of each MEI did not exceed a fixed budget <italic>b</italic> of 30. The norm-constrained MEI <inline-formula><mml:math id="M22"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> was calculated at each iteration as <disp-formula id="FD8"><label>(3)</label><mml:math id="M23"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mi>b</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfrac><mml:mo>×</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:math></disp-formula></p><p id="P70">The range constraint was defined and applied for each colour channel separately and ensured that the range of the MEI values stayed within the range covered by the training movie. This was achieved by clipping values of the MEI exceeding the range covered by the training movie to the minimum or maximum value. Optimisation was run for at least 100 iterations, and then stopped when the number of iterations reached 1000, or when it had converged (whichever occurred first). Convergence was defined as 10 consecutive iterations with a change in model neuron activation of less than 0.001; model neuron activations ranged from ≈ 1 to ≈ 10. We denote the resulting MEI for neuron <italic>n</italic> as <italic><bold>x</bold></italic>*<sup>(<italic>n</italic>)</sup>.</p></sec><sec id="S26"><title>Analysing MEIs</title><p id="P71">We analysed MEIs to quantify their spatial, temporal, and chromatic properties.</p></sec><sec id="S27"><title>Spatial and temporal components of MEIs</title><p id="P72">For each colour channel <italic>c</italic>, we decomposed the spatiotemporal MEIs into a spatial component and a temporal component (henceforth spatial component and temporal component) by singular value decomposition: <disp-formula id="FD9"><mml:math id="M24"><mml:mrow><mml:mi>U</mml:mi><mml:mo>,</mml:mo><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:mtext>svd(</mml:mtext><mml:msubsup><mml:mi>x</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mtext>)</mml:mtext></mml:mrow></mml:math></disp-formula></p><p id="P73">with <inline-formula><mml:math id="M25"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℛ</mml:mi><mml:mrow><mml:mn>50</mml:mn><mml:mo>×</mml:mo><mml:mn>288</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> for <italic>c</italic> ∈ [green, UV] is the MEI of neuron <italic>n</italic> in a given colour channel with its spatial dimension (18x16=288) flattened out. As a result, any spatiotemporal dependencies are removed and we only analyse spatial and temporal properties separately. The following procedures were carried out in the same manner for the green and the UV component of the MEI, and we drop the color channel index <italic>c</italic> for ease of notation. The temporal component is then defined as the first left singular vector, <italic>U</italic><sub>:1</sub>, and the spatial component is defined as the first right singular vector, <inline-formula><mml:math id="M26"><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, reshaped to the original dimensions 18 × 16.</p></sec><sec id="S28"><title>Concentric anisotropic 2D Difference-of-Gaussians fit</title><p id="P74">We modelled the spatial component as concentric anisotropic Difference-of-Gaussians (DoG) using the nonlinear leastsquares solver scipy.optimize.least_squares with soft-L1 loss function (<xref ref-type="bibr" rid="R40">40</xref>). The DoGs were parameterized by a location (<italic>μ<sub>x</sub>,μ<sub>y</sub></italic>) shared between centre and surround, amplitudes A<sup><italic>c</italic></sup>, A<sup><italic>s</italic></sup>, variances <inline-formula><mml:math id="M27"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>x</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>y</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and rotation angles <italic>θ<sup>c</sup>, θ<sup>s</sup></italic> separately for centre and surround: <disp-formula id="FD10"><mml:math id="M28"><mml:mrow><mml:mtext>DoG</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>=</mml:mtext><mml:mspace width="0.2em"/><mml:msup><mml:mi>G</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>G</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow></mml:math></disp-formula> with <disp-formula id="FD11"><mml:math id="M29"><mml:mrow><mml:mtable columnalign="right"><mml:mtr columnalign="right"><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi>G</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mtext>A</mml:mtext><mml:mi>c</mml:mi></mml:msup><mml:mspace width="0.2em"/><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="right"><mml:mtd columnalign="right"><mml:mrow><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mi>g</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="right"><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mi>c</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula> and <disp-formula id="FD12"><mml:math id="M30"><mml:mrow><mml:mtable columnalign="right"><mml:mtr columnalign="right"><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:mrow></mml:mtd><mml:mtd columnalign="right"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>cos</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>θ</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:mtd><mml:mtd columnalign="right"><mml:mo>+</mml:mo></mml:mtd><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>sin</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>θ</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="right"><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi>g</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:mrow></mml:mtd><mml:mtd columnalign="right"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>sin</mml:mi><mml:mn>2</mml:mn><mml:msup><mml:mi>θ</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:mtd><mml:mtd columnalign="right"><mml:mo>−</mml:mo></mml:mtd><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>sin</mml:mi><mml:mn>2</mml:mn><mml:msup><mml:mi>θ</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="right"><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:mrow></mml:mtd><mml:mtd columnalign="right"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>sin</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>θ</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:mtd><mml:mtd columnalign="right"><mml:mo>+</mml:mo></mml:mtd><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>cos</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>θ</mml:mi><mml:mi>c</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p id="P75">and likewise for <italic>G<sup>s</sup>.</italic> We initialised (<italic>μ<sub>x</sub>,μ<sub>y</sub></italic>) in the following way: Since we set the model readout’s location parameters to (0, 0) for all model neurons when generating their MEIs, we also expected the MEIs to be centred at (0, 0), as well. Hence, we determined the location of the minimum and the maximum value of the MEI; whichever was closer to the centre (0,0) provided the initial values for the parameters (<italic>μ<sub>x</sub>,μ<sub>y</sub></italic>). Starting from there, we then first fit a single Gaussian to the MEI, and took the resulting parameters as initial parameters for the DoG fit. This was a constrained optimisation problem, with lower and upper bounds on all parameters; in particular, such that the location parameter would not exceed the canvas of the MEI, and such that the variance would be strictly positive.</p></sec><sec id="S29"><title>MEI properties</title><sec id="S30"><title>Centre size</title><p id="P76">We defined the diameter of the centre of the MEI in the horizontal and the vertical orientation, respectively, as <inline-formula><mml:math id="M31"><mml:mrow><mml:msubsup><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M32"><mml:mrow><mml:msubsup><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>. The centre size was calculated as <inline-formula><mml:math id="M33"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. We then estimated a contour outlining the MEI centre as the line that is defined by all points at which the 2D centre Gaussian <italic>G<sup>c</sup></italic> attains the value <italic>G<sup>c</sup></italic>(<italic>x,y</italic>) with <inline-formula><mml:math id="M34"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The centre mask <italic>m</italic> was then defined as a binary matrix with all pixels within the convex hull of this contour being 1 and all other pixels set to 0. This mask is used for calculating centre chromatic contrast (see below).</p></sec><sec id="S31"><title>Temporal frequency</title><p id="P77">To estimate temporal frequency of the MEIs, we estimated the power spectrum of the temporal components using a Fast Fourier Transform after attenuating high frequency noise by filtering with a 5th order low-pass Butterworth filter with cutoff frequency 10 Hz. We then estimated the mean frequency of the temporal component by calculating an average of the frequency components, each weighted with its relative power.</p></sec><sec id="S32"><title>Contrast</title><p id="P78">The contrast of the MEIs in the two channels, <inline-formula><mml:math id="M35"><mml:mrow><mml:mi>γ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for <italic>c</italic> ∈ [green, UV], was defined as the difference between the mean value within the centre mask <italic>m</italic> at the two last peaks of the temporal component of the MEI in the UV channel at time points <italic>t<sub>2</sub></italic> and <italic>t<sub>1</sub>:</italic> <disp-formula id="FD13"><mml:math id="M36"><mml:mrow><mml:mi>γ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>⊙</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>⊙</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P79">where ⨀ denotes the element-wise multiplication of the MEI and the binary mask. (see <xref ref-type="fig" rid="F3">Fig. 3</xref> f). The peaks were found with the function scipy.signal.find_peaks, and the peaks found for the UV channel were used to calculate contrast both in the green and the UV channel.</p></sec></sec><sec id="S33"><title>Validating MEIs experimentally</title><sec id="S34"><title>Generating MEI stimuli</title><p id="P80">In order to test experimentally whether the model correctly predicts which stimuli would maximally excite RGCs of different RGC groups, we performed a new set of experiments (numbers indicated in red in <xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S1c</xref>) where we complemented our stimulus set with MEI stimuli. For the MEI stimuli, we selected 11 RGCs, chosen to span the responses space and to represent both well-described and poorly understood RGC groups, for which we generated MEIs at different positions on a 5 × 5 grid (spanning 110<italic>μm</italic> in vertical and horizontal direction). We decomposed the MEIs as described above, and reconstructed MEIs as rank 1 tensors by taking the outer product of the spatial and temporal components: <disp-formula id="FD14"><mml:math id="M37"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⊗</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:math></disp-formula></p><p id="P81">We used this rank 1 reconstruction of the MEIs as stimuli, since the model’s convolutional kernels were space-time-separable, hence could not capture RGC selectivity to spatio-temporally varying stimulus properties, and so any MEI components of rank &gt; 1 were assumed to be spurious. The MEI stimuli, lasting 50 frames (1.66 s) were padded with 10 frames (.34 s) of inter-stimulus grey, and were randomly interleaved. With 11 stimuli, presented at 25 positions and lasting 2 s each, the total stimulus duration was 11 × 25 × 2 <italic>s</italic> = 550 <italic>s</italic>. Since the model operated on a z-scored (0 mean, 1 SD) version of the movie, MEIs as predicted by the model lived in the same space and had to be transformed back to the stimulator range ([0, 255]) before being used as stimuli in an experiment by scaling with the movie’s SD and adding the movie’s mean. The MEIs’ green channel was then displayed with the green LED, and the UV channel was displayed with the UV LED. For experiments at Northwestern University, an additional transform was necessary to achieve the same levels of photoreceptor activation (photoisomerization rates) for M- and S-cones with different LEDs. To ensure proper chromatic scaling between the different experimental apparatuses with different spectral profiles, we described the relative activation of M- and S-cones by the green and UV LEDs in the stimulation setup used in the two photon imaging experiments (setup A) by a matrix <disp-formula id="FD15"><mml:math id="M38"><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>A</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mn>0.19</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P82">and the relative activation of M- and S-cones by the stimulation setup used in the patch-clamp experiments (setup B) by a matrix <disp-formula id="FD16"><mml:math id="M39"><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>B</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mn>0.9</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>0.035</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P83">where diagonal entries describe the activation of M-cones by the green LED, and of S-cones by the UV LED, and entries in the off-diagonal describe the cross-activation (i.e., M-cones by UV-LED and S-cones by green LED). The activation of M-cones and S-cones <bold>e<sup>T</sup></bold> = (<italic>e<sub>m</sub>,e<sub>s</sub></italic>) by a stimulus <inline-formula><mml:math id="M40"><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℛ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> displayed on a given stimulation setup was approximated as <bold>e</bold> = <bold>A<italic>x</italic></bold> (<xref ref-type="bibr" rid="R82">82</xref>). Hence, a stimulus <inline-formula><mml:math id="M41"><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> displayed on setup B, defined as <inline-formula><mml:math id="M42"><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>B</mml:mi></mml:mstyle><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>A</mml:mi></mml:mstyle><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula>, will achieve the same photoreceptor activation as stimulus <italic><bold>x</bold></italic> displayed on setup A. Since the solution exceeded the valid range of the stimulator ([0, 255]), we added an offset and multiplied with a scalar factor to ensure all stimuli were within the valid range.</p></sec><sec id="S35"><title>Analysing RGC responses to MEI stimuli</title><p id="P84">We wanted to evaluate the responses of RGCs to the MEI stimuli in a spatially resolved fashion, i.e. weighting responses to MEIs displayed at different locations proportional to the strength of the RGCs RF at that location. In order to be able to meaningfully compare MEI responses between RGCs and across groups, for each RGC, we first centred and scaled the responses to zero mean and a standard deviation of 1. Then, for each RGC <italic>n</italic>, we computed a spatial average of its responses, weighting its responses at each spatial location (<italic>x, y</italic>) proportional to the Gaussian density <inline-formula><mml:math id="M43"><mml:mrow><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where the parameters of the Gaussian <italic><bold>μ<sub>n</sub></bold></italic> = (<italic>μ<sub>x</sub>,μ<sub>y</sub>),σ<sub>n</sub></italic> were the model’s estimated readout parameters for neuron <italic>n</italic> (<xref ref-type="fig" rid="F4">Fig. 4 b, c, d</xref> left): <disp-formula id="FD17"><mml:math id="M44"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>5</mml:mn></mml:munderover><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>5</mml:mn></mml:munderover><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p id="P85">where <inline-formula><mml:math id="M45"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mspace width="0.2em"/><mml:mo>∈</mml:mo><mml:mspace width="0.2em"/><mml:msup><mml:mi>ℛ</mml:mi><mml:mrow><mml:mn>11</mml:mn><mml:mo>×</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the 60 frames (2 s) long response of neuron <italic>n</italic> to an MEI at position <inline-formula><mml:math id="M46"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, resam-pled from the recording frame rate of 7.81 Hz to 30 Hz. We then averaged <inline-formula><mml:math id="M47"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> across time in the optimisation time window, i.e. frames 30-50, to get a scalar response <inline-formula><mml:math id="M48"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> for each MEI stimulus (<xref ref-type="fig" rid="F4">Fig. 4, d</xref>).</p></sec><sec id="S36"><title>Selectivity index</title><p id="P86">To quantify the selectivity of the response <inline-formula><mml:math id="M49"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of an RGC <italic>n</italic> to an MEI <inline-formula><mml:math id="M50"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>, we defined a selectivity index as follows. First, we standardised the responses <inline-formula><mml:math id="M51"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> across all MEIs by subtracting the mean and dividing by the standard deviation. The selectivity index of RGC group G<sub>g</sub> to MEI <inline-formula><mml:math id="M52"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> was then defined as <disp-formula id="FD18"><mml:math id="M53"><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>S</mml:mi></mml:mstyle><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>I</mml:mi></mml:mstyle><mml:mi>g</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>j</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P87">where <italic>δ<sub>ij</sub></italic> is the Kronecker delta. In words, the SI is the difference (in units of SD response) between the response to the MEI of interest <inline-formula><mml:math id="M54"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the mean response to all other (<xref ref-type="bibr" rid="R10">10</xref>) MEIs, <inline-formula><mml:math id="M55"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>j</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> averaged across all cells <italic>n</italic> belonging to the group of interest G<sub>g</sub>.</p></sec><sec id="S37"><title>Representational similarity analysis</title><p id="P88">To compare how RGC groups transform visual input into neural representations, and how these representations differ between groups, we performed representational similarity analysis (RSA) on the RGC group responses to the MEI stimuli (<xref ref-type="bibr" rid="R43">43</xref>). First, we considered representations <italic>ϕ<sub>k</sub></italic> of MEIs in different spaces. In stimulus space, MEIs are represented by the vectors holding their pixel values, <disp-formula id="FD19"><label>(4)</label><mml:math id="M56"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℛ</mml:mi><mml:mi>d</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P89">where <italic>d</italic> = <italic>c</italic> × <italic>h</italic> × <italic>w</italic> × <italic>t.</italic> Second, a group of <italic>n</italic> neurons spans an <italic>n</italic>-dimensional neural response space, where each MEI is represented as the <italic>n</italic>-dimensional response vector, <disp-formula id="FD20"><label>(5)</label><mml:math id="M57"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>ν</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P90">where <inline-formula><mml:math id="M58"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> as defined above.</p><p id="P91">We then calculated the pairwise Euclidean distances between MEIs in these different representations, and summarised them in representational dissimilarity matrices (RDMs): <disp-formula id="FD21"><mml:math id="M59"><mml:mrow><mml:msubsup><mml:mrow><mml:mtext>RDM</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></disp-formula></p><p id="P92">The entry <inline-formula><mml:math id="M60"><mml:mrow><mml:msubsup><mml:mrow><mml:mtext>RDM</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> thus tells us how (dis)similar MEIs <italic><bold>x</bold></italic><sup>|(<italic>i</italic>)</sup> and <italic><bold>x</bold></italic><sup>|(<italic>j</italic>)</sup> are under the representation <italic>ϕ<sub>k</sub></italic> (<xref ref-type="fig" rid="F5">Fig. 5 ac</xref> and <xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S4a</xref>). To quantify how (dis)similar MEI representations <italic>ϕ<sub>k</sub></italic> and <italic>ϕ<sub>l</sub></italic> are from each other, we then define ameasure Γ(<italic>ϕ<sub>k</sub>, ϕ<sub>l</sub></italic>) (<xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S4b, c</xref>) as <disp-formula id="FD22"><label>(6)</label><mml:math id="M61"><mml:mrow><mml:mo>Γ</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mtext>RDM</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mtext>RDM</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P93">where <italic>C</italic> is the correlation coefficient, and <italic>triu</italic>(RDM<italic>ϕ<sup>k</sup></italic>) is the upper triangle of RDM<italic><sup>ϕk</sup></italic>, since RDMs are symmetric about the diagonal. The correlation coefficient can in theory take values between -1 and 1, but in practice, we did not observe any anti-correlated RDMs.</p></sec><sec id="S38"><title>Detection performance analysis</title><p id="P94">To test the performance of individual RGCs of different groups in detecting the target class of inter-clip transitions (ground-to-sky) from all other classes of inter-clip transitions, we performed a receiver operating characteristic (ROC) analysis (<xref ref-type="bibr" rid="R47">47</xref>). For each RGC, we calculated its response to an inter-clip transition occurring at time <italic>t</italic><sub>0</sub> as the baseline-subtracted average response within 1 second following the transition, i.e. <inline-formula><mml:math id="M62"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula>, with T=30 frames at 30 Hz. For all n=40 equally spaced thresholds within the response range of a RGC, we then calculated the true positive rate (TPR) and false positive rate (FPR) of a hypothetical classifier classifying all transitions eliciting an above-threshold response as a positive, and all other transitions as negative. Plotting the TPR as a function of FPR yields an ROC curve, the area under which (AUC) is equivalent to the probability that the RGC will respond more strongly to a randomly chosen inter-clip transition of the target class than to a randomly chosen inter-clip transition of a different class. The AUC thus is a measure of performance for RGCs in this detection task.</p></sec></sec><sec id="S39"><title>Statistical analysis</title><sec id="S40"><title>Permutation test</title><p id="P95">We wanted to test how likely the difference in AUC observed for different RGC groups are to occur under the null hypothesis that the underlying distributions they are sampled from are equal. To this end, we performed a permutation test. We generated a null distribution for our test statistic, the absolute difference in AUC values △AUC, by shuffling the RGC group labels of the two groups of interest (e.g. G<sub>28</sub> and G<sub>24</sub>) and calculating the test statistic with shuffled labels 100,000 times. We then obtained a p-value for △AUC observed with true labels as the proportion of entries in the null distribution larger than △AUC.</p></sec><sec id="S41"><title>Bootstrapped confidence intervals</title><p id="P96">We bootstrapped confidence intervals for △AUC (<xref ref-type="fig" rid="F6">Fig. 6</xref> and <xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S6</xref>) and for Γ(<italic>ϕ<sub>s</sub>,ϕ<sub>Vg</sub></italic>) (<xref ref-type="fig" rid="F5">Fig. 5</xref> and <xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S4</xref>). For △AUC, we generated a bootstrapped distribution by sampling 100 times with replacement from the AUC values of the two groups that were being compared and calculated △AUC. We then estimated the 95 % confidence interval for △AUC as the interval defined by the 2.5<sup><italic>th</italic></sup> and 97.5<sup><italic>th</italic></sup> percentile of the bootstrapped distribution of △AUC.</p><p id="P97">For Γ(<italic>ϕ<sub>s</sub>,ϕ<sub>Vg</sub></italic>), we generated a bootstrapped distribution by sampling 100 times with replacement from the MEI responses of RGC group <italic>g</italic> and then calculating RDM<sup>ϕνg</sup> and Γ(<italic>ϕ<sub>s</sub>, ϕ<sub>Vg</sub></italic>) for each sample. We then estimated the 95 % confidence interval for Γ(<italic>ϕ<sub>s</sub>,ϕ<sub>Vg</sub></italic>) as the interval defined by the 2.5<sup><italic>th</italic></sup> and 97.5<sup><italic>th</italic></sup> percentile of the bootstrapped distribution of Γ(<italic>ϕ<sub>S</sub>, <sup>ϕ</sup><sub>Vg</sub></italic>).</p></sec><sec id="S42"><title>Estimating effect size</title><p id="P98">The effect size of difference in AUC observed for different RGC groups <italic>l</italic> and <italic>k</italic>, △AUC (<xref ref-type="fig" rid="F6">Fig. 6</xref> and <xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S6</xref>), was estimated as Cohen’s <italic>d</italic> (<xref ref-type="bibr" rid="R83">83</xref>, <xref ref-type="bibr" rid="R84">84</xref>): <disp-formula id="FD23"><mml:math id="M63"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mi>s</mml:mi></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> with <disp-formula id="FD24"><mml:math id="M64"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:msqrt></mml:mrow></mml:math></disp-formula></p><p id="P99">and <italic>m<sub>k</sub></italic> and <italic>s<sub>k</sub></italic> the sample mean and standard deviation, respectively, of the AUC observed for the <italic>N<sub>k</sub></italic> RGCs of group k.</p></sec><sec id="S43"><title>Estimating linear correlation</title><p id="P100">Wherever the linear correlation between two paired samples <italic>x</italic> and <italic>y</italic> of size <italic>N</italic> was calculated (for evaluating model performance, <xref ref-type="fig" rid="F2">Fig. 2</xref>, <xref ref-type="supplementary-material" rid="SD1">Suppl. Figs. S1, S3</xref>; and similarity between representations, <xref ref-type="fig" rid="F5">Fig. 5</xref>, <xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S4</xref>), we used Pearson’s correlation coefficient: <disp-formula id="FD25"><mml:math id="M65"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mi>i</mml:mi><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mi>i</mml:mi><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:msqrt><mml:msqrt><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mi>i</mml:mi><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p></sec></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary information</label><media xlink:href="EMS158023-supplement-Supplementary_information.pdf" mimetype="application" mime-subtype="pdf" id="d31aAdEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S44"><title>Acknowledgements</title><p>We thank Jonathan Oesterle and Dominic Gonschorek for feedback on the manuscript, Jan Lause for statistical consulting, and Merle Harrer for general assistance. We also thank all members of the Sinz lab for regular discussions on the project. This work was supported by the German Research Foundation (DFG; CRC 1233 “Robust Vision: Inference Principles and Neural Mechanisms”, project number 276693517 to P.B., M.B., T.E., K.F.; Heisenberg Professorship, BE5601/8-1 to P.B.; Excellence Cluster EXC 2064, project number 390727645 to P.B.), the Federal Ministry of Education and Research (FKZ 01IS18039A to P.B.), National Institutes of Health (NIH; NEI EY031029, NEI EY031329 to G.W.S.; NEI F30EY031565, NIGMS T32GM008152 to Z.J.), and the European Research Council (ERC) under the European Union’s Horizon Europe research and innovation programme (A.S.E, grant agreement No. 101041669).</p></ack><sec id="S45" sec-type="data-availability"><title>Data availability</title><p id="P101">The data and the movie stimulus will be made available at <ext-link ext-link-type="uri" xlink:href="https://retinal-functomics.net">https://retinal-functomics.net</ext-link> upon journal publication.</p><sec id="S46" sec-type="data-availability"><title>Code availability</title><p id="P102">Custom analysis and model training code will be made available at <ext-link ext-link-type="uri" xlink:href="https://github.com/eulerlab">https://github.com/eulerlab</ext-link> upon journal publication.</p></sec></sec><fn-group><fn id="FN3" fn-type="con"><p id="P103"><bold>Author contributions</bold></p><p id="P104"><bold>L. H.</bold>: Conceptualisation, methodology, software, validation, formal analysis, data curation, writing (original draft), visualisation <bold>K. P. S.</bold>: validation, investigation, data curation <bold>C. B.</bold>: methodology, software, formal analysis <bold>Y. Q.</bold>: resources, software <bold>D. A. K.</bold>: methodology, software, writing (review and editing) <bold>Z. J.</bold>: methodology, validation, investigation, visualisation, writing (original draft) <bold>G. W. S.</bold>: methodology, validation, investigation, writing (original draft), supervision, funding acquisition <bold>M. B.</bold>: conceptualisation, writing (review and editing), supervision, funding acquisition <bold>P. B.</bold>: conceptualisation, writing (review and editing), supervision, funding acquisition <bold>K. F.</bold>: conceptualisation, writing (review and editing), supervision, funding acquisition <bold>A. S. E.</bold>: conceptualisation, writing (review and editing), supervision, funding acquisition <bold>T. E.</bold>: conceptualisation, writing (original draft), visualisation, supervision, funding acquisition, project administration.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lettvin</surname><given-names>JY</given-names></name><name><surname>Maturana</surname><given-names>HR</given-names></name><name><surname>McCulloch</surname><given-names>WS</given-names></name><name><surname>Pitts</surname><given-names>WH</given-names></name></person-group><article-title>What the Frog’s Eye Tells the Frog’s Brain</article-title><source>Proceedings of the IRE</source><year>1959</year><volume>47</volume><issue>11</issue><fpage>1940</fpage><lpage>1959</lpage></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baden</surname><given-names>Tom</given-names></name><name><surname>Euler</surname><given-names>Thomas</given-names></name><name><surname>Berens</surname><given-names>Philipp</given-names></name></person-group><article-title>Understanding the retinal basis of vision across species</article-title><source>Nature Reviews Neuroscience</source><year>2020</year><volume>21</volume><issue>1</issue><fpage>5</fpage><lpage>20</lpage><comment>ISSN 14710048</comment><pub-id pub-id-type="pmid">31780820</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname><given-names>Maxwell H</given-names></name><name><surname>Gonzalo</surname><given-names>Luis</given-names></name><name><surname>Giraldo</surname><given-names>Sanchez</given-names></name><name><surname>Schwartz</surname><given-names>Odelia</given-names></name><name><surname>Rieke</surname><given-names>Fred</given-names></name></person-group><article-title>Stimulus-and goal-oriented frameworks for understanding natural vision</article-title><source>Nature Neuroscience</source><year>2019</year><volume>22</volume><issue>1</issue><fpage>15</fpage><lpage>24</lpage><comment>ISSN 15461726</comment><pub-id pub-id-type="pmcid">PMC8378293</pub-id><pub-id pub-id-type="pmid">30531846</pub-id><pub-id pub-id-type="doi">10.1038/s41593-018-0284-0</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simoncelli</surname><given-names>Eero P</given-names></name><name><surname>Olshausen</surname><given-names>Bruno A</given-names></name></person-group><article-title>Natural Image Statistics and Neural Representation</article-title><source>Annual Reviews Neurosciences</source><year>2001</year><volume>24</volume><fpage>1193</fpage><lpage>1216</lpage><pub-id pub-id-type="pmid">11520932</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><collab>Daniel Kerschensteiner</collab><article-title>Feature Detection by Retinal Ganglion Cells</article-title><source>Annual Review of Vision Science</source><year>2022</year><volume>8</volume><issue>1</issue><fpage>1</fpage><lpage>35</lpage><comment>ISSN 2374-4642, 9</comment><pub-id pub-id-type="pmid">35385673</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goetz</surname><given-names>Jillian</given-names></name><name><surname>Jessen</surname><given-names>Zachary F</given-names></name><name><surname>Jacobi</surname><given-names>Anne</given-names></name><name><surname>Mani</surname><given-names>Adam</given-names></name><name><surname>Cooler</surname><given-names>Sam</given-names></name><name><surname>Greer</surname><given-names>Devon</given-names></name><name><surname>Kadri</surname><given-names>Sabah</given-names></name><name><surname>Segal</surname><given-names>Jeremy</given-names></name><name><surname>Shekhar</surname><given-names>Karthik</given-names></name><name><surname>Sanes</surname><given-names>Joshua R</given-names></name><name><surname>Schwartz</surname><given-names>Gregory W</given-names></name></person-group><article-title>Unified classification of mouse retinal ganglion cells using function, morphology, and gene expression</article-title><source>Cell Reports</source><year>2022</year><volume>40</volume><issue>2</issue><elocation-id>111040</elocation-id><comment>ISSN 22111247, 7</comment><pub-id pub-id-type="pmcid">PMC9364428</pub-id><pub-id pub-id-type="pmid">35830791</pub-id><pub-id pub-id-type="doi">10.1016/j.celrep.2022.111040</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baden</surname><given-names>Tom</given-names></name><name><surname>Berens</surname><given-names>Philipp</given-names></name><name><surname>Franke</surname><given-names>Katrin</given-names></name><name><surname>Rosón</surname><given-names>Miroslav Román</given-names></name><name><surname>Bethge</surname><given-names>Matthias</given-names></name><name><surname>Euler</surname><given-names>Thomas</given-names></name></person-group><article-title>The functional diversity of retinal ganglion cells in the mouse</article-title><source>Nature</source><year>2016</year><volume>529</volume><issue>7586</issue><fpage>345</fpage><lpage>350</lpage><comment>ISSN 15206041</comment><pub-id pub-id-type="pmcid">PMC4724341</pub-id><pub-id pub-id-type="pmid">26735013</pub-id><pub-id pub-id-type="doi">10.1038/nature16468</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alexander Bae</surname><given-names>J</given-names></name><name><surname>Mu</surname><given-names>Shang</given-names></name><name><surname>Kim</surname><given-names>Jinseop S</given-names></name><name><surname>Turner</surname><given-names>Nicholas L</given-names></name><name><surname>Tartavull</surname><given-names>Ignacio</given-names></name><name><surname>Kemnitz</surname><given-names>Nico</given-names></name><name><surname>Jordan</surname><given-names>Chris S</given-names></name><name><surname>Norton</surname><given-names>Alex D</given-names></name><name><surname>Silversmith</surname><given-names>William M</given-names></name><name><surname>Prentki</surname><given-names>Rachel</given-names></name><name><surname>Sorek</surname><given-names>Marissa</given-names></name><etal/></person-group><article-title>Digital Museum of Retinal Ganglion Cells with Dense Anatomy and Physiology</article-title><source>Cell</source><year>2018</year><volume>173</volume><issue>5</issue><fpage>1293</fpage><lpage>1306</lpage><comment>ISSN 10974172</comment><pub-id pub-id-type="pmcid">PMC6556895</pub-id><pub-id pub-id-type="pmid">29775596</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2018.04.040</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rheaume</surname><given-names>Bruce A</given-names></name><name><surname>Jereen</surname><given-names>Amyeo</given-names></name><name><surname>Bolisetty</surname><given-names>Mohan</given-names></name><name><surname>Sajid</surname><given-names>Muhammad S</given-names></name><name><surname>Yang</surname><given-names>Yue</given-names></name><name><surname>Renna</surname><given-names>Kathleen</given-names></name><name><surname>Sun</surname><given-names>Lili</given-names></name><name><surname>Robson</surname><given-names>Paul</given-names></name><name><surname>Trakhtenberg</surname><given-names>Ephraim F</given-names></name></person-group><article-title>Single cell transcriptome profiling of retinal ganglion cells identifies cellular subtypes</article-title><source>Nature Communications</source><year>2018</year><volume>9</volume><issue>1</issue><comment>ISSN 20411723</comment><pub-id pub-id-type="pmcid">PMC6050223</pub-id><pub-id pub-id-type="pmid">30018341</pub-id><pub-id pub-id-type="doi">10.1038/s41467-018-05134-3</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martersteck</surname><given-names>Emily M</given-names></name><name><surname>Hirokawa</surname><given-names>Karla E</given-names></name><name><surname>Evarts</surname><given-names>Mariah</given-names></name><name><surname>Bernard</surname><given-names>Amy</given-names></name><name><surname>Duan</surname><given-names>Xin</given-names></name><name><surname>Li</surname><given-names>Yang</given-names></name><name><surname>Ng</surname><given-names>Lydia</given-names></name><name><surname>Oh</surname><given-names>Seung W</given-names></name><name><surname>Ouellette</surname><given-names>Benjamin</given-names></name><name><surname>Royall</surname><given-names>Joshua J</given-names></name><name><surname>Stoecklin</surname><given-names>Michelle</given-names></name><etal/></person-group><article-title>Diverse Central Projection Patterns of Retinal Ganglion Cells</article-title><source>CellReports</source><year>2017</year><volume>18</volume><issue>8</issue><fpage>2058</fpage><lpage>2072</lpage><comment>ISSN 22111247, 2</comment><pub-id pub-id-type="pmcid">PMC5357325</pub-id><pub-id pub-id-type="pmid">28228269</pub-id><pub-id pub-id-type="doi">10.1016/j.celrep.2017.01.075</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>Keith P</given-names></name><name><surname>Fitzpatrick</surname><given-names>Michael J</given-names></name><name><surname>Zhao</surname><given-names>Lei</given-names></name><name><surname>Wang</surname><given-names>Bing</given-names></name><name><surname>McCracken</surname><given-names>Sean</given-names></name><name><surname>Williams</surname><given-names>Philip R</given-names></name><name><surname>Kerschensteiner</surname><given-names>Daniel</given-names></name></person-group><article-title>Cell-type-specific binocular vision guides predation in mice</article-title><source>Neuron</source><year>2021</year><volume>3</volume><comment>ISSN 0896-6273</comment><pub-id pub-id-type="pmcid">PMC8112612</pub-id><pub-id pub-id-type="pmid">33784498</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2021.03.010</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Münch</surname><given-names>Thomas A</given-names></name><name><surname>Da Silveira</surname><given-names>Rava Azeredo</given-names></name><name><surname>Siegert</surname><given-names>Sandra</given-names></name><name><surname>Viney</surname><given-names>Tim James</given-names></name><name><surname>Awatramani</surname><given-names>Gautam B</given-names></name><name><surname>Roska</surname><given-names>Botond</given-names></name></person-group><article-title>Approach sensitivity in the retina processed by a multifunctional neural circuit</article-title><source>Nature Neuroscience</source><year>2009</year><volume>12</volume><issue>10</issue><fpage>1308</fpage><lpage>1316</lpage><comment>ISSN 10976256</comment><pub-id pub-id-type="pmid">19734895</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yilmaz</surname><given-names>Melis</given-names></name><name><surname>Meister</surname><given-names>Markus</given-names></name></person-group><article-title>Rapid innate defensive responses of mice to looming visual stimuli</article-title><source>Current Biology</source><year>2013</year><volume>23</volume><issue>20</issue><fpage>2011</fpage><lpage>2015</lpage><comment>ISSN 09609822</comment><pub-id pub-id-type="pmcid">PMC3809337</pub-id><pub-id pub-id-type="pmid">24120636</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2013.08.015</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>T</given-names></name><name><surname>Shen</surname><given-names>N</given-names></name><name><surname>Hsiang</surname><given-names>JC</given-names></name><name><surname>Johnson</surname><given-names>KP</given-names></name><name><surname>Kerschensteiner</surname><given-names>D</given-names></name></person-group><article-title>Dendritic and parallel processing of visual threats in the retina control defensive responses</article-title><source>Science Advances</source><year>2020</year><volume>6</volume><issue>47</issue><fpage>1</fpage><lpage>12</lpage><comment>ISSN 23752548</comment><pub-id pub-id-type="pmcid">PMC7673819</pub-id><pub-id pub-id-type="pmid">33208370</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.abc9920</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>Gregory W</given-names></name><name><surname>Swygart</surname><given-names>David</given-names></name></person-group><chapter-title>Circuits for Feature Selectivity in the Inner Retina</chapter-title><source>The Senses: A Comprehensive Reference</source><publisher-name>Elsevier</publisher-name><year>2020</year><day>1</day><fpage>275</fpage><lpage>292</lpage><pub-id pub-id-type="doi">10.1016/b978-0-12-809324-5.24186-2</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szél</surname><given-names>A</given-names></name><name><surname>Röhlich</surname><given-names>P</given-names></name><name><surname>Caffé</surname><given-names>AR</given-names></name><name><surname>Juliusson</surname><given-names>B</given-names></name><name><surname>Aguirre</surname><given-names>G</given-names></name><name><surname>Van Veen</surname><given-names>T</given-names></name></person-group><article-title>Unique topo-graphic separation of two spectral classes of cones in the mouse retina</article-title><source>Journal of Com-parative Neurology</source><year>1992</year><volume>325</volume><issue>3</issue><fpage>327</fpage><lpage>342</lpage><comment>ISSN 0021-9967</comment><pub-id pub-id-type="pmid">1447405</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joesch</surname><given-names>Maximilian</given-names></name><name><surname>Meister</surname><given-names>Markus</given-names></name></person-group><article-title>A neuronal circuit for colour vision based on rod-cone opponency</article-title><source>Nature</source><year>2016</year><volume>532</volume><issue>7598</issue><fpage>236</fpage><lpage>239</lpage><comment>ISSN 14764687</comment><pub-id pub-id-type="pmid">27049951</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baden</surname><given-names>Tom</given-names></name><name><surname>Schubert</surname><given-names>Timm</given-names></name><name><surname>Chang</surname><given-names>Le</given-names></name><name><surname>Wei</surname><given-names>Tao</given-names></name><name><surname>Zaichuk</surname><given-names>Mariana</given-names></name><name><surname>Wissinger</surname><given-names>Bernd</given-names></name><name><surname>Euler</surname><given-names>Thomas</given-names></name></person-group><article-title>A tale of two retinal domains: Near-Optimal sampling of achromatic contrasts in natural scenes through asymmetric photoreceptor distribution</article-title><source>Neuron</source><year>2013</year><volume>80</volume><issue>5</issue><fpage>1206</fpage><lpage>1217</lpage><comment>ISSN 08966273</comment><pub-id pub-id-type="pmid">24314730</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szatko</surname><given-names>Klaudia P</given-names></name><name><surname>Korympidou</surname><given-names>Maria M</given-names></name><name><surname>Ran</surname><given-names>Yanli</given-names></name><name><surname>Berens</surname><given-names>Philipp</given-names></name><name><surname>Dalkara</surname><given-names>Deniz</given-names></name><name><surname>Schubert</surname><given-names>Timm</given-names></name><name><surname>Euler</surname><given-names>Thomas</given-names></name><name><surname>Franke</surname><given-names>Katrin</given-names></name></person-group><article-title>Neural circuits in the mouse retina support color vision in the upper visual field</article-title><source>NatureCommunications</source><year>2020</year><volume>11</volume><issue>1</issue><comment>ISSN 20411723</comment><pub-id pub-id-type="pmcid">PMC7359335</pub-id><pub-id pub-id-type="pmid">32661226</pub-id><pub-id pub-id-type="doi">10.1038/s41467-020-17113-8</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khani</surname><given-names>Mohammad Hossein</given-names></name><name><surname>Gollisch</surname><given-names>Tim</given-names></name></person-group><article-title>Linear and nonlinear chromatic integration in1375 the mouse retina</article-title><source>Nature Communications</source><year>2021</year><volume>12</volume><issue>1</issue><elocation-id>1900</elocation-id><comment>ISSN 2041-1723, 12</comment><pub-id pub-id-type="pmcid">PMC7997992</pub-id><pub-id pub-id-type="pmid">33772000</pub-id><pub-id pub-id-type="doi">10.1038/s41467-021-22042-1</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mouland</surname><given-names>Josh W</given-names></name><name><surname>Pienaar</surname><given-names>Abigail</given-names></name><name><surname>Williams</surname><given-names>Christopher</given-names></name><name><surname>Watson</surname><given-names>Alex J</given-names></name><name><surname>Lucas</surname><given-names>Robert J</given-names></name><name><surname>Brown</surname><given-names>Timothy M</given-names></name></person-group><article-title>Extensive cone-dependent spectral opponency within a discrete1379 zone of the lateral geniculate nucleus supporting mouse color vision</article-title><source>Current Biology</source><year>2021</year><volume>31</volume><issue>15</issue><fpage>3391</fpage><lpage>3400</lpage><comment>ISSN 18790445</comment><pub-id pub-id-type="pmcid">PMC8360768</pub-id><pub-id pub-id-type="pmid">34111401</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2021.05.024</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qiu</surname><given-names>Yongrong</given-names></name><name><surname>Zhao</surname><given-names>Zhijian</given-names></name><name><surname>Klindt</surname><given-names>David</given-names></name><name><surname>Kautzky</surname><given-names>Magdalena</given-names></name><name><surname>Szatko</surname><given-names>Klaudia P</given-names></name><name><surname>Schaeffel</surname><given-names>Frank</given-names></name><name><surname>Rifai</surname><given-names>Katharina</given-names></name><name><surname>Franke</surname><given-names>Katrin</given-names></name><name><surname>Busse</surname><given-names>Laura</given-names></name><name><surname>Euler</surname><given-names>Thomas</given-names></name></person-group><article-title>Natural environment statistics in the upper and lower visual field are reflected in mouse reti-1384 nal specializations</article-title><source>Current Biology</source><year>2021</year><volume>31</volume><issue>15</issue><fpage>3233</fpage><lpage>3247</lpage><comment>ISSN 18790445</comment><pub-id pub-id-type="pmid">34107304</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karamanlis</surname><given-names>Dimokratis</given-names></name><name><surname>Gollisch</surname><given-names>Tim</given-names></name></person-group><article-title>Nonlinear spatial integration underlies the diversity1387 of retinal ganglion cell responses to natural images</article-title><source>Journal ofNeuroscience</source><year>2021</year><volume>41</volume><issue>15</issue><fpage>3479</fpage><lpage>3498</lpage><comment>ISSN 15292401, 4</comment><pub-id pub-id-type="pmcid">PMC8051676</pub-id><pub-id pub-id-type="pmid">33664129</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3075-20.2021</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharpee</surname><given-names>Tatyana O</given-names></name></person-group><article-title>Computational identification of receptive fields</article-title><source>Annual Review of Neuroscience</source><year>2013</year><volume>36</volume><fpage>103</fpage><lpage>120</lpage><comment>ISSN 0147006X</comment><pub-id pub-id-type="pmcid">PMC3760488</pub-id><pub-id pub-id-type="pmid">23841838</pub-id><pub-id pub-id-type="doi">10.1146/annurev-neuro-062012-170253</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maheswaranathan</surname><given-names>Niru</given-names></name><name><surname>McIntosh</surname><given-names>Lane T</given-names></name><name><surname>Tanaka</surname><given-names>Hidenori</given-names></name><name><surname>Grant</surname><given-names>Satchel</given-names></name><name><surname>Kastner</surname><given-names>David B</given-names></name><name><surname>Melander</surname><given-names>Josh B</given-names></name><name><surname>Nayebi</surname><given-names>Aran</given-names></name><name><surname>Brezovec</surname><given-names>Luke</given-names></name><name><surname>Wang</surname><given-names>Julia</given-names></name><name><surname>Ganguli</surname><given-names>Surya</given-names></name><name><surname>Baccus</surname><given-names>Stephen A</given-names></name></person-group><article-title>The dynamic neural code of the retina for natural scenes</article-title><source>bioRxiv</source><year>2018</year><elocation-id>340943</elocation-id><comment>ISSN 2692-8205</comment></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tanaka</surname><given-names>Hidenori</given-names></name><name><surname>Nayebi</surname><given-names>Aran</given-names></name><name><surname>Maheswaranathan</surname><given-names>Niru</given-names></name><name><surname>McIntosh</surname><given-names>Lane T</given-names></name><name><surname>Baccus</surname><given-names>Stephen A</given-names></name><name><surname>Ganguli</surname><given-names>Surya</given-names></name></person-group><source>From deep learning to mechanistic understanding in neuroscience:1398 the structure of retinal prediction</source><conf-name>Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS 2019)</conf-name><year>2019</year><issue>33</issue><fpage>1</fpage><lpage>11</lpage></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Batty</surname><given-names>Eleanor</given-names></name><name><surname>Merel</surname><given-names>Josh</given-names></name><name><surname>Brackbill</surname><given-names>Nora</given-names></name><name><surname>Heitman</surname><given-names>Alexander</given-names></name><name><surname>Sher</surname><given-names>Alexander</given-names></name><name><surname>Litke</surname><given-names>Alan</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name><name><surname>Paninski</surname><given-names>Liam</given-names></name></person-group><source>Multilayer Recurrent Network Models of Primate Retinal Ganglion Cell Responses</source><conf-name>Proceedings of the 5th International Conference on Learning Representations</conf-name><year>2017</year><issue>5</issue></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>Daniel LK</given-names></name><name><surname>Hong</surname><given-names>Ha</given-names></name><name><surname>Cadieu</surname><given-names>Charles F</given-names></name><name><surname>Solomon</surname><given-names>Ethan A</given-names></name><name><surname>Seibert</surname><given-names>Darren</given-names></name><name><surname>DiCarlo</surname><given-names>James J</given-names></name></person-group><source>Performance-optimized hierarchical models predict neural responses in higher visual cortex</source><conf-name>Proceedings of the National Academy of Sciences of the United States of America</conf-name><year>2014</year><volume>111</volume><issue>23</issue><fpage>8619</fpage><lpage>8624</lpage><comment>ISSN 10916490</comment><pub-id pub-id-type="pmcid">PMC4060707</pub-id><pub-id pub-id-type="pmid">24812127</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cadena</surname><given-names>Santiago A</given-names></name><name><surname>Denfield</surname><given-names>George H</given-names></name><name><surname>Walker</surname><given-names>Edgar Y</given-names></name><name><surname>Gatys</surname><given-names>Leon A</given-names></name><name><surname>Tolias</surname><given-names>Andreas S</given-names></name><name><surname>Bethge</surname><given-names>Matthias</given-names></name><name><surname>Ecker</surname><given-names>Alexander S</given-names></name></person-group><article-title>Deep convolutional models improve predictions of macaque V1 responses to natural images</article-title><source>PLoS Computational Biology</source><year>2019</year><volume>15</volume><issue>4</issue><fpage>1</fpage><lpage>28</lpage><comment>ISSN 15537358</comment><pub-id pub-id-type="pmcid">PMC6499433</pub-id><pub-id pub-id-type="pmid">31013278</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006897</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ecker</surname><given-names>Alexander S</given-names></name><name><surname>Sinz</surname><given-names>Fabian H</given-names></name><name><surname>Froudarakis</surname><given-names>Emmanouil</given-names></name><name><surname>Fahey</surname><given-names>Paul G</given-names></name><name><surname>Cadena</surname><given-names>Santiago A</given-names></name><name><surname>Walker</surname><given-names>Edgar Y</given-names></name><name><surname>Cobos</surname><given-names>Erick</given-names></name><name><surname>Reimer</surname><given-names>Jacob</given-names></name><name><surname>Tolias</surname><given-names>Andreas S</given-names></name><name><surname>Bethge</surname><given-names>Matthias</given-names></name></person-group><source>A rotation-equivariant convolutional neural network model of primary visual cortex</source><conf-name>Proceedings of the 7th International Conference on Learning Representations</conf-name><year>2019</year><issue>7</issue></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ustyuzhaninov</surname><given-names>Ivan</given-names></name><name><surname>Burg</surname><given-names>Max F</given-names></name><name><surname>Cadena</surname><given-names>Santiago A</given-names></name><name><surname>Fu</surname><given-names>Jiakun</given-names></name><name><surname>Muhammad</surname><given-names>Taliah</given-names></name><name><surname>Ponder</surname><given-names>Kayla</given-names></name><name><surname>Froudarakis</surname><given-names>Emmanouil</given-names></name><name><surname>Ding</surname><given-names>Zhiwei</given-names></name><name><surname>Bethge</surname><given-names>Matthias</given-names></name><name><surname>Tolias</surname><given-names>Andreas S</given-names></name><name><surname>Ecker</surname><given-names>Alexander S</given-names></name></person-group><article-title>Digital twin reveals combinatorial code of non-linear computations in the mouse primary visual cortex</article-title><source>bioRxiv</source><year>2022</year><volume>2</volume><elocation-id>2022.02.10.479884</elocation-id><pub-id pub-id-type="doi">10.1101/2022.02.10.479884</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walker</surname><given-names>Edgar Y</given-names></name><name><surname>Sinz</surname><given-names>Fabian H</given-names></name><name><surname>Cobos</surname><given-names>Erick</given-names></name><name><surname>Muhammad</surname><given-names>Taliah</given-names></name><name><surname>Froudarakis</surname><given-names>Emmanouil</given-names></name><name><surname>Fahey</surname><given-names>Paul G</given-names></name><name><surname>Ecker</surname><given-names>Alexander S</given-names></name><name><surname>Reimer</surname><given-names>Jacob</given-names></name><name><surname>Pitkow</surname><given-names>Xaq</given-names></name><name><surname>Tolias</surname><given-names>Andreas S</given-names></name></person-group><article-title>Inception loops discover what excites neurons most using deep predictive models</article-title><source>Nature Neuroscience</source><year>2019</year><month>December</month><volume>22</volume><comment>ISSN 15461726</comment><pub-id pub-id-type="pmid">31686023</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bashivan</surname><given-names>Pouya</given-names></name><name><surname>Kar</surname><given-names>Kohitij</given-names></name><name><surname>DiCarlo</surname><given-names>James J</given-names></name></person-group><article-title>Neural population control via deep image synthesis</article-title><source>Science</source><year>2019</year><volume>364</volume><issue>6439</issue><comment>ISSN 10959203</comment><pub-id pub-id-type="pmid">31048462</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franke</surname><given-names>Katrin</given-names></name><name><surname>Willeke</surname><given-names>Konstantin F</given-names></name><name><surname>Ponder</surname><given-names>Kayla</given-names></name><name><surname>Galdamez</surname><given-names>Mario</given-names></name><name><surname>Zhou</surname><given-names>Na</given-names></name><name><surname>Muhammad</surname><given-names>Taliah</given-names></name><name><surname>Patel</surname><given-names>Saumil</given-names></name><name><surname>Froudarakis</surname><given-names>Emmanouil</given-names></name><name><surname>Reimer</surname><given-names>Jacob</given-names></name><name><surname>Sinz</surname><given-names>Fabian H</given-names></name><name><surname>Tolias</surname><given-names>Andreas S</given-names></name></person-group><article-title>State-dependent pupil dilation rapidly shifts visual feature selectivity</article-title><source>Nature</source><year>2022</year><comment>ISSN 0028-0836, 9</comment><pub-id pub-id-type="pmid">36171291</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacobs</surname><given-names>Gerald H</given-names></name><name><surname>Williams</surname><given-names>Gary A</given-names></name><name><surname>Fenwick</surname><given-names>John A</given-names></name></person-group><article-title>Influence of cone pigment coexpression on spectral sensitivity and color vision in the mouse</article-title><source>Vision Research</source><year>2004</year><volume>44</volume><issue>14</issue><fpage>1615</fpage><lpage>1622</lpage><comment>ISSN 00426989</comment><pub-id pub-id-type="pmid">15135998</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mani</surname><given-names>Adam</given-names></name><name><surname>Schwartz</surname><given-names>Gregory W</given-names></name></person-group><article-title>Circuit Mechanisms of a Retinal Ganglion Cell with Stimulus-Dependent Response Latency and Activation Beyond Its Dendrites</article-title><source>Current Biology</source><year>2017</year><volume>27</volume><issue>4</issue><fpage>471</fpage><lpage>482</lpage><comment>ISSN 09609822</comment><pub-id pub-id-type="pmcid">PMC5319888</pub-id><pub-id pub-id-type="pmid">28132812</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2016.12.033</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tien</surname><given-names>Nai Wen</given-names></name><name><surname>Pearson</surname><given-names>James T</given-names></name><name><surname>Heller</surname><given-names>Charles R</given-names></name><name><surname>Demas</surname><given-names>Jay</given-names></name><name><surname>Kerschen-steiner</surname><given-names>Daniel</given-names></name></person-group><article-title>Genetically identified suppressed-by-contrast retinal ganglion cells reliably signal self-generated visual stimuli</article-title><source>Journal of Neuroscience</source><year>2015</year><volume>35</volume><issue>30</issue><fpage>10815</fpage><lpage>10820</lpage><comment>ISSN 15292401</comment><pub-id pub-id-type="pmcid">PMC4518055</pub-id><pub-id pub-id-type="pmid">26224863</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1521-15.2015</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tien</surname><given-names>Nai-Wen</given-names></name><name><surname>Vitale</surname><given-names>Carmela</given-names></name><name><surname>Badea</surname><given-names>Tudor C</given-names></name><name><surname>Kerschensteiner</surname><given-names>Daniel</given-names></name></person-group><article-title>Layer-Specific Developmentally Precise Axon Targeting of Transient Suppressed-by-Contrast Retinal Ganglion Cells (tSbC RGCs)</article-title><source>The Journal of Neuroscience</source><year>2022</year><day>9</day><volume>42</volume><issue>38</issue><fpage>2332</fpage><lpage>21</lpage><comment>ISSN 0270-6474</comment><pub-id pub-id-type="pmcid">PMC9512569</pub-id><pub-id pub-id-type="pmid">36002262</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2332-21.2022</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abballe</surname><given-names>Luca</given-names></name><name><surname>Asari</surname><given-names>Hiroki</given-names></name></person-group><article-title>Natural image statistics for mouse vision</article-title><source>Plos One</source><year>2022</year><volume>17</volume><issue>1</issue><elocation-id>e0262763</elocation-id><pub-id pub-id-type="pmcid">PMC8775586</pub-id><pub-id pub-id-type="pmid">35051230</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0262763</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gupta</surname><given-names>Divyansh</given-names></name><name><surname>Mlynarski</surname><given-names>Wiktor</given-names></name><name><surname>Symonova</surname><given-names>Olga</given-names></name><name><surname>Svaton</surname><given-names>Jan</given-names></name><name><surname>Joesch</surname><given-names>Maximilian</given-names></name></person-group><article-title>Panoramic visual statistics shape retina-wide organization of receptive fields</article-title><source>bioRxiv</source><year>2022</year><fpage>1</fpage><lpage>31</lpage></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qiu</surname><given-names>Yongrong</given-names></name><name><surname>Klindt</surname><given-names>David A</given-names></name><name><surname>Szatko</surname><given-names>Klaudia P</given-names></name><name><surname>Gonschorek</surname><given-names>Dominic</given-names></name><name><surname>Hoefling</surname><given-names>Larissa</given-names></name><name><surname>Schubert</surname><given-names>Timm</given-names></name><name><surname>Busse</surname><given-names>Laura</given-names></name><name><surname>Bethge</surname><given-names>Matthias</given-names></name><name><surname>Euler</surname><given-names>Thomas</given-names></name></person-group><article-title>Efficient coding of natural scenes improves neural system identification</article-title><source>bioRxiv</source><year>2022</year></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>Gregory William</given-names></name></person-group><chapter-title>Chapter 16 - Color processing</chapter-title><person-group person-group-type="editor"><name><surname>Schwartz</surname><given-names>Gregory William</given-names></name></person-group><source>Retinal Computation</source><publisher-name>Academic Press</publisher-name><year>2021</year><fpage>288</fpage><lpage>317</lpage><comment>ISBN 978-0-12-8198964</comment><pub-id pub-id-type="doi">10.1016/B978-0-12-819896-4.00017-2</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name><name><surname>Mur</surname><given-names>Marieke</given-names></name><name><surname>Bandettini</surname><given-names>Peter</given-names></name></person-group><article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><year>2008</year><month>NOV</month><volume>2</volume><fpage>1</fpage><lpage>28</lpage><comment>ISSN 16625137</comment><pub-id pub-id-type="pmcid">PMC2605405</pub-id><pub-id pub-id-type="pmid">19104670</pub-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name><name><surname>Wei</surname><given-names>Xue Xin</given-names></name></person-group><article-title>Neural tuning and representational geometry</article-title><source>Nature Reviews Neuroscience</source><year>2021</year><volume>22</volume><issue>11</issue><fpage>703</fpage><lpage>718</lpage><comment>ISSN 14710048</comment><pub-id pub-id-type="pmid">34522043</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname><given-names>Arne F</given-names></name><name><surname>O’Keefe</surname><given-names>John</given-names></name><name><surname>Poort</surname><given-names>Jasper</given-names></name></person-group><article-title>Two Distinct Types of Eye-Head Coupling in Freely Moving Mice</article-title><source>Current Biology</source><year>2020</year><volume>30</volume><issue>11</issue><fpage>2116</fpage><lpage>2130</lpage><comment>ISSN 18790445</comment><pub-id pub-id-type="pmcid">PMC7284311</pub-id><pub-id pub-id-type="pmid">32413309</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2020.04.042</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parker</surname><given-names>Philip RL</given-names></name><name><surname>Abe</surname><given-names>Elliott TT</given-names></name><name><surname>Leonard</surname><given-names>Emmalyn SP</given-names></name><name><surname>Martins</surname><given-names>Dylan M</given-names></name><name><surname>Niell</surname><given-names>Cristo-pher M</given-names></name></person-group><article-title>Joint coding of visual input and eye/head position in V1 of freely moving mice</article-title><source>Neuron</source><year>2022</year><volume>9</volume><comment>ISSN 08966273</comment><pub-id pub-id-type="pmcid">PMC9742335</pub-id><pub-id pub-id-type="pmid">36137549</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2022.08.029</pub-id></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fawcett</surname><given-names>Tom</given-names></name></person-group><article-title>An introduction to ROC analysis</article-title><source>Pattern Recognition Letters</source><year>2006</year><day>6</day><volume>27</volume><issue>8</issue><fpage>861</fpage><lpage>874</lpage><comment>ISSN 01678655</comment><pub-id pub-id-type="doi">10.1016/j.patrec.2005.10.010</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tien</surname><given-names>Nai Wen</given-names></name><name><surname>Kim</surname><given-names>Tahnbee</given-names></name><name><surname>Kerschensteiner</surname><given-names>Daniel</given-names></name></person-group><article-title>Target-Specific Glycinergic Transmission from VGluT3-Expressing Amacrine Cells Shapes Suppressive Contrast Responses in the Retina</article-title><source>Cell Reports</source><year>2016</year><volume>15</volume><issue>7</issue><fpage>1369</fpage><lpage>1375</lpage><comment>ISSN 22111247, 5</comment><pub-id pub-id-type="pmcid">PMC4871735</pub-id><pub-id pub-id-type="pmid">27160915</pub-id><pub-id pub-id-type="doi">10.1016/j.celrep.2016.04.025</pub-id></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacoby</surname><given-names>Jason</given-names></name><name><surname>Schwartz</surname><given-names>Gregory William</given-names></name></person-group><article-title>Typology and Circuitry of Suppressed-by-Contrast Retinal Ganglion Cells</article-title><source>Frontiers in Cellular Neuroscience</source><year>2018</year><month>August</month><volume>12</volume><fpage>1</fpage><lpage>7</lpage><comment>ISSN 16625102</comment><pub-id pub-id-type="pmcid">PMC6119723</pub-id><pub-id pub-id-type="pmid">30210298</pub-id><pub-id pub-id-type="doi">10.3389/fncel.2018.00269</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krieger</surname><given-names>Brenna</given-names></name><name><surname>Qiao</surname><given-names>Mu</given-names></name><name><surname>Rousso</surname><given-names>David L</given-names></name><name><surname>Sanes</surname><given-names>Joshua R</given-names></name><name><surname>Meister</surname><given-names>Markus</given-names></name></person-group><article-title>Four alpha ganglion cell types in mouse retina: Function, structure, and molecular signatures</article-title><source>PLoS ONE</source><year>2017</year><volume>12</volume><issue>7</issue><fpage>1</fpage><lpage>21</lpage><comment>ISSN 19326203</comment><pub-id pub-id-type="pmcid">PMC5533432</pub-id><pub-id pub-id-type="pmid">28753612</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0180091</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>Le</given-names></name><name><surname>Breuninger</surname><given-names>Tobias</given-names></name><name><surname>Euler</surname><given-names>Thomas</given-names></name></person-group><article-title>Chromatic Coding from Cone-type Uns-elective Circuits in the Mouse Retina</article-title><source>Neuron</source><year>2013</year><volume>77</volume><issue>3</issue><fpage>559</fpage><lpage>571</lpage><comment>ISSN 08966273</comment><pub-id pub-id-type="pmid">23395380</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stabio</surname><given-names>Maureen E</given-names></name><name><surname>Sabbah</surname><given-names>Shai</given-names></name><name><surname>Quattrochi</surname><given-names>Lauren E</given-names></name><name><surname>Ilardi</surname><given-names>Marissa C</given-names></name><name><surname>Fogerson</surname><given-names>Michelle P</given-names></name><name><surname>Leyrer</surname><given-names>Megan L</given-names></name><name><surname>Kim</surname><given-names>Min Tae</given-names></name><name><surname>Kim</surname><given-names>Inkyu</given-names></name><name><surname>Schiel</surname><given-names>Matthew</given-names></name><name><surname>Renna</surname><given-names>Jordan M</given-names></name><name><surname>Briggman</surname><given-names>Kevin L</given-names></name><etal/></person-group><article-title>The M5 Cell: A Color-Opponent Intrinsically Photosensitive Retinal Ganglion Cell</article-title><source>Neuron</source><year>2018</year><volume>97</volume><issue>1</issue><fpage>150</fpage><lpage>163</lpage><comment>ISSN 10974199</comment><pub-id pub-id-type="pmcid">PMC5757626</pub-id><pub-id pub-id-type="pmid">29249284</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2017.11.030</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sonoda</surname><given-names>Takuma</given-names></name><name><surname>Okabe</surname><given-names>Yudai</given-names></name><name><surname>Schmidt</surname><given-names>Tiffany M</given-names></name></person-group><article-title>Overlapping morphological and functional properties between M4 and M5 intrinsically photosensitive retinal ganglion cells</article-title><source>Journal of Comparative Neurology</source><year>2020</year><volume>528</volume><issue>6</issue><fpage>1028</fpage><lpage>1040</lpage><comment>ISSN 10969861, 4</comment><pub-id pub-id-type="pmcid">PMC7007370</pub-id><pub-id pub-id-type="pmid">31691279</pub-id><pub-id pub-id-type="doi">10.1002/cne.24806</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dacey</surname><given-names>Dennis M</given-names></name><name><surname>Lee</surname><given-names>Barry B</given-names></name></person-group><article-title>The ‘blue-on’ opponent pathway in primate retina originates from a distinct bistratified ganglion cell type</article-title><source>Nature</source><year>1994</year><volume>367</volume><issue>6465</issue><fpage>731</fpage><lpage>735</lpage><comment>ISSN 00280836</comment><pub-id pub-id-type="pmid">8107868</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Field</surname><given-names>Greg D</given-names></name><name><surname>Sher</surname><given-names>Alexander</given-names></name><name><surname>Gauthier</surname><given-names>Jeffrey L</given-names></name><name><surname>Greschner</surname><given-names>Martin</given-names></name><name><surname>Shlens</surname><given-names>Jonathon</given-names></name><name><surname>Litke</surname><given-names>Alan M</given-names></name><name><surname>Chichilnisky</surname><given-names>EJ</given-names></name></person-group><article-title>Spatial properties and functional organization of small bistratified ganglion cells in primate retina</article-title><source>Journal of Neuroscience</source><year>2007</year><volume>27</volume><issue>48</issue><fpage>13261</fpage><lpage>13272</lpage><comment>ISSN 02706474, 11</comment><pub-id pub-id-type="pmcid">PMC6673390</pub-id><pub-id pub-id-type="pmid">18045920</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3437-07.2007</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrens</surname><given-names>Christian</given-names></name><name><surname>Schubert</surname><given-names>Timm</given-names></name><name><surname>Haverkamp</surname><given-names>Silke</given-names></name><name><surname>Euler</surname><given-names>Thomas</given-names></name><name><surname>Berens</surname><given-names>Philipp</given-names></name></person-group><article-title>Connectivity map of bipolar cells and photoreceptors in the mouse retina</article-title><source>eLife</source><year>2016</year><month>NOVEMBER</month><volume>5</volume><fpage>1</fpage><lpage>20</lpage><comment>2016, ISSN 2050084X</comment><pub-id pub-id-type="pmcid">PMC5148610</pub-id><pub-id pub-id-type="pmid">27885985</pub-id><pub-id pub-id-type="doi">10.7554/eLife.20041</pub-id></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niell</surname><given-names>Cristopher M</given-names></name><name><surname>Stryker</surname><given-names>Michael P</given-names></name></person-group><article-title>Modulation of Visual Responses by Behavioral State in Mouse Visual Cortex</article-title><source>Neuron</source><year>2010</year><volume>65</volume><issue>4</issue><fpage>472</fpage><lpage>479</lpage><comment>ISSN 08966273</comment><pub-id pub-id-type="pmcid">PMC3184003</pub-id><pub-id pub-id-type="pmid">20188652</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2010.01.033</pub-id></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piscopo</surname><given-names>Denise M</given-names></name><name><surname>El-Danaf</surname><given-names>Rana N</given-names></name><name><surname>Huberman</surname><given-names>Andrew D</given-names></name><name><surname>Niell</surname><given-names>Cristopher M</given-names></name></person-group><article-title>Diverse visual features encoded in mouse lateral geniculate nucleus</article-title><source>Journal of Neuroscience</source><year>2013</year><volume>33</volume><issue>11</issue><fpage>4642</fpage><lpage>4656</lpage><comment>ISSN 02706474, 3</comment><pub-id pub-id-type="pmcid">PMC3665609</pub-id><pub-id pub-id-type="pmid">23486939</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5187-12.2013</pub-id></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ito</surname><given-names>Shinya</given-names></name><name><surname>Feldheim</surname><given-names>David A</given-names></name><name><surname>Litke</surname><given-names>Alan M</given-names></name></person-group><article-title>Segregation of visual response properties in the mouse superiorcolliculus and their modulation during locomotion</article-title><source>Journal of Neuroscience</source><year>2017</year><volume>37</volume><issue>35</issue><fpage>8428</fpage><lpage>8443</lpage><comment>ISSN 15292401, 8</comment><pub-id pub-id-type="pmcid">PMC5577856</pub-id><pub-id pub-id-type="pmid">28760858</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3689-16.2017</pub-id></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Masland</surname><given-names>Richard H</given-names></name><name><surname>Martin</surname><given-names>Paul R</given-names></name></person-group><article-title>The unsolved mystery of vision</article-title><source>Current Biology</source><year>2007</year><volume>17</volume><issue>15</issue><fpage>R577</fpage><lpage>R582</lpage><comment>ISSN 09609822, 8</comment><pub-id pub-id-type="pmid">17686423</pub-id></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baden</surname><given-names>Tom</given-names></name><name><surname>Schaeffel</surname><given-names>Frank</given-names></name><name><surname>Berens</surname><given-names>Philipp</given-names></name></person-group><article-title>Visual Neuroscience: A Retinal Ganglion Cell to Report Image Focus?</article-title><source>Current Biology</source><year>2017</year><volume>27</volume><issue>4</issue><fpage>R139</fpage><lpage>R141</lpage><comment>ISSN 09609822, 2</comment><pub-id pub-id-type="pmid">28222289</pub-id></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gawne</surname><given-names>Timothy J</given-names></name><name><surname>Norton</surname><given-names>Thomas T</given-names></name></person-group><article-title>An opponent dual-detector spectral drive model of emmetropization</article-title><source>Vision Research</source><year>2020</year><volume>173</volume><fpage>7</fpage><lpage>20</lpage><comment>ISSN 18785646, 8</comment><pub-id pub-id-type="pmcid">PMC7371364</pub-id><pub-id pub-id-type="pmid">32445984</pub-id><pub-id pub-id-type="doi">10.1016/j.visres.2020.03.011</pub-id></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Evans</surname><given-names>Dominic A</given-names></name><name><surname>Stempel</surname><given-names>A Vanessa</given-names></name><name><surname>Vale</surname><given-names>Ruben</given-names></name><name><surname>Branco</surname><given-names>Tiago</given-names></name></person-group><article-title>Cognitive Control of Escape Behaviour</article-title><source>Trends in Cognitive Sciences</source><year>2019</year><volume>23</volume><issue>4</issue><fpage>334</fpage><lpage>348</lpage><comment>ISSN 1879307X, 4</comment><pub-id pub-id-type="pmcid">PMC6438863</pub-id><pub-id pub-id-type="pmid">30852123</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2019.01.012</pub-id></element-citation></ref><ref id="R64"><label>64</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Kim</surname><given-names>I-J</given-names></name><name><surname>Sanes</surname><given-names>JR</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><article-title>The most numerous ganglion cell type of the mouse retina is a selective feature detector</article-title><source>Proceedings of the National Academy of Sciences</source><year>2012</year><volume>109</volume><issue>36</issue><fpage>E2391</fpage><lpage>E2398</lpage><comment>ISSN 0027-8424</comment><pub-id pub-id-type="pmcid">PMC3437843</pub-id><pub-id pub-id-type="pmid">22891316</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1211547109</pub-id></element-citation></ref><ref id="R65"><label>65</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karamanlis</surname><given-names>Dimokratis</given-names></name><name><surname>Schreyer</surname><given-names>Helene Marianne</given-names></name><name><surname>Gollisch</surname><given-names>Tim</given-names></name></person-group><article-title>Retinal Encoding of Natural Scenes</article-title><source>Annual Review of Vision Science</source><year>2022</year><volume>8</volume><issue>1</issue><comment>ISSN 2374-4642, 9</comment><pub-id pub-id-type="pmid">35676096</pub-id></element-citation></ref><ref id="R66"><label>66</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ocko</surname><given-names>Samuel A</given-names></name><name><surname>Lindsey</surname><given-names>Jack</given-names></name><name><surname>Ganguli</surname><given-names>Surya</given-names></name><name><surname>Deny</surname><given-names>Stephane</given-names></name></person-group><article-title>The emergence of multiple retinal cell types through efficient coding of natural movies</article-title><source>Advances in Neural Information Processing Systems</source><year>2018</year><fpage>9389</fpage><lpage>9400</lpage><comment>2018, ISSN 10495258, Decem(NeurIPS)</comment></element-citation></ref><ref id="R67"><label>67</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lindsey</surname><given-names>Jack</given-names></name><name><surname>Ocko</surname><given-names>Samuel A</given-names></name><name><surname>Ganguli</surname><given-names>Surya</given-names></name><name><surname>Deny</surname><given-names>Stephane</given-names></name></person-group><source>A Unified Theory of Early Visual Representations from Retina to Cortex through Anatomically Constrained Deep CNNs</source><conf-name>International Conference on Learning Representations</conf-name><year>2019</year><fpage>1</fpage><lpage>17</lpage></element-citation></ref><ref id="R68"><label>68</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldin</surname><given-names>Matías A</given-names></name><name><surname>Lefebvre</surname><given-names>Baptiste</given-names></name><name><surname>Virgili</surname><given-names>Samuele</given-names></name><name><surname>Van Cang</surname><given-names>Mathieu Kim Pham</given-names></name><name><surname>Ecker</surname><given-names>Alexander</given-names></name><name><surname>Mora</surname><given-names>Thierry</given-names></name><name><surname>Ferrari</surname><given-names>Ulisse</given-names></name><name><surname>Marre</surname><given-names>Olivier</given-names></name></person-group><article-title>Context-dependent selectivity to natural images in the retina</article-title><source>Nature Communications</source><year>2022</year><volume>13</volume><issue>1</issue><comment>ISSN 20411723</comment><pub-id pub-id-type="pmcid">PMC9499945</pub-id><pub-id pub-id-type="pmid">36138007</pub-id><pub-id pub-id-type="doi">10.1038/s41467-022-33242-8</pub-id></element-citation></ref><ref id="R69"><label>69</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Briggman</surname><given-names>Kevin L</given-names></name><name><surname>Euler</surname><given-names>Thomas</given-names></name></person-group><article-title>Bulk electroporation and population calcium imaging intheadultmammalian retina</article-title><source>Journal of Neurophysiology</source><year>2011</year><volume>105</volume><issue>5</issue><fpage>2601</fpage><lpage>2609</lpage><comment>ISSN 00223077</comment><pub-id pub-id-type="pmid">21346205</pub-id></element-citation></ref><ref id="R70"><label>70</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Euler</surname><given-names>Thomas</given-names></name><name><surname>Hausselt</surname><given-names>Susanne E</given-names></name><name><surname>Margolis</surname><given-names>David J</given-names></name><name><surname>Breuninger</surname><given-names>Tobias</given-names></name><name><surname>Castell</surname><given-names>Xavier</given-names></name><name><surname>Detwiler</surname><given-names>Peter B</given-names></name><name><surname>Denk</surname><given-names>Winfried</given-names></name></person-group><article-title>Eyecup scope-optical recordings of light stimulus-evoked fluorescence signals in the retina</article-title><source>Pflugers Archiv European Journal of Physiology</source><year>2009</year><volume>457</volume><issue>6</issue><fpage>1393</fpage><lpage>1414</lpage><comment>ISSN 00316768</comment><pub-id pub-id-type="pmcid">PMC3037819</pub-id><pub-id pub-id-type="pmid">19023590</pub-id><pub-id pub-id-type="doi">10.1007/s00424-008-0603-5</pub-id></element-citation></ref><ref id="R71"><label>71</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>Wei</given-names></name><name><surname>Elstrott</surname><given-names>Justin</given-names></name><name><surname>Feller</surname><given-names>Marla B</given-names></name></person-group><article-title>Two-photon targeted recording of GFP-expressing neurons for light responses and live-cell imaging in the mouse retina</article-title><source>Nature Protocols</source><year>2010</year><volume>5</volume><issue>7</issue><fpage>1347</fpage><lpage>1352</lpage><comment>ISSN 17502799, 7</comment><pub-id pub-id-type="pmcid">PMC4303237</pub-id><pub-id pub-id-type="pmid">20595962</pub-id><pub-id pub-id-type="doi">10.1038/nprot.2010.106</pub-id></element-citation></ref><ref id="R72"><label>72</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Euler</surname><given-names>Thomas</given-names></name><name><surname>Franke</surname><given-names>Katrin</given-names></name><name><surname>Baden</surname><given-names>Tom</given-names></name></person-group><chapter-title>Studying a Light Sensor with Light: Multiphoton Imaging in the Retina</chapter-title><person-group person-group-type="editor"><name><surname>Hartveit</surname><given-names>Espen</given-names></name></person-group><source>Multiphoton Microscopy</source><publisher-name>Humana Press</publisher-name><year>2019</year><fpage>225</fpage><lpage>250</lpage><comment>chapter 10, ISBN 9781493997015</comment><pub-id pub-id-type="doi">10.1002/9781118696736.ch22</pub-id></element-citation></ref><ref id="R73"><label>73</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Theis</surname><given-names>Lucas</given-names></name><name><surname>Berens</surname><given-names>Philipp</given-names></name><name><surname>Froudarakis</surname><given-names>Emmanouil</given-names></name><name><surname>Reimer</surname><given-names>Jacob</given-names></name><name><surname>Rosón</surname><given-names>Miroslav Román</given-names></name><name><surname>Baden</surname><given-names>Tom</given-names></name><name><surname>Euler</surname><given-names>Thomas</given-names></name><name><surname>Tolias</surname><given-names>Andreas S</given-names></name><name><surname>Bethge</surname><given-names>Matthias</given-names></name></person-group><article-title>Benchmarking Spike Rate Inference in Population Calcium Imaging</article-title><source>Neuron</source><year>2016</year><volume>90</volume><issue>3</issue><fpage>471</fpage><lpage>482</lpage><comment>ISSN 10974199</comment><pub-id pub-id-type="pmcid">PMC4888799</pub-id><pub-id pub-id-type="pmid">27151639</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2016.04.014</pub-id></element-citation></ref><ref id="R74"><label>74</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franke</surname><given-names>Katrin</given-names></name><name><surname>Chagas</surname><given-names>Maia André</given-names></name><name><surname>Zhao</surname><given-names>Zhijian</given-names></name><name><surname>Zimmermann</surname><given-names>Maxime JY</given-names></name><name><surname>Bartel</surname><given-names>Philipp</given-names></name><name><surname>Qiu</surname><given-names>Yongrong</given-names></name><name><surname>Szatko</surname><given-names>Klaudia P</given-names></name><name><surname>Baden</surname><given-names>Tom</given-names></name><name><surname>Euler</surname><given-names>Thomas</given-names></name></person-group><article-title>An arbitrary-spectrum spatial visual stimulatorforvision research</article-title><source>eLife</source><year>2019</year><volume>8</volume><comment>ISSN 2050084X, 9</comment><pub-id pub-id-type="pmcid">PMC6783264</pub-id><pub-id pub-id-type="pmid">31545172</pub-id><pub-id pub-id-type="doi">10.7554/eLife.48779</pub-id></element-citation></ref><ref id="R75"><label>75</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arshadi</surname><given-names>Cameron</given-names></name><name><surname>Günther</surname><given-names>Ulrik</given-names></name><name><surname>Eddison</surname><given-names>Mark</given-names></name><name><surname>Harrington</surname><given-names>Kyle IS</given-names></name><name><surname>Ferreira</surname><given-names>Tiago A</given-names></name></person-group><article-title>SNT: a unifying toolbox for quantification of neuronal anatomy</article-title><source>Nature Methods</source><year>2021</year><volume>18</volume><issue>4</issue><fpage>374</fpage><lpage>377</lpage><comment>ISSN 15487105, 4</comment><pub-id pub-id-type="pmid">33795878</pub-id></element-citation></ref><ref id="R76"><label>76</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sümbül</surname><given-names>Uygar</given-names></name><name><surname>Song</surname><given-names>Sen</given-names></name><name><surname>McCulloch</surname><given-names>Kyle</given-names></name><name><surname>Becker</surname><given-names>Michael</given-names></name><name><surname>Lin</surname><given-names>Bin</given-names></name><name><surname>Sanes</surname><given-names>Joshua R</given-names></name><name><surname>Masland</surname><given-names>Richard H</given-names></name><name><surname>Sebastian Seung</surname><given-names>H</given-names></name></person-group><article-title>A genetic and computational approach to structurally classify neuronal types</article-title><source>Nature Communications</source><year>2014</year><comment>ISSN 20411723, 5</comment><pub-id pub-id-type="pmcid">PMC4164236</pub-id><pub-id pub-id-type="pmid">24662602</pub-id><pub-id pub-id-type="doi">10.1038/ncomms4512</pub-id></element-citation></ref><ref id="R77"><label>77</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Klindt</surname><given-names>David A</given-names></name><name><surname>Ecker</surname><given-names>Alexander S</given-names></name><name><surname>Euler</surname><given-names>Thomas</given-names></name><name><surname>Bethge</surname><given-names>Matthias</given-names></name></person-group><source>Neural system identification for large populations separating “what” and “where”</source><conf-name>31st Conference on Neural Information Processing Systems, (Nips)</conf-name><year>2017</year><fpage>4</fpage><lpage>6</lpage><comment>ISSN 10495258</comment></element-citation></ref><ref id="R78"><label>78</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>Zhijian</given-names></name><name><surname>Klindt</surname><given-names>David A</given-names></name><name><surname>Chagas</surname><given-names>André Maia</given-names></name><name><surname>Szatko</surname><given-names>Klaudia P</given-names></name><name><surname>Rogerson</surname><given-names>Luke</given-names></name><name><surname>Protti</surname><given-names>Dario A</given-names></name><name><surname>Behrens</surname><given-names>Christian</given-names></name><name><surname>Dalkara</surname><given-names>Deniz</given-names></name><name><surname>Schubert</surname><given-names>Timm</given-names></name><name><surname>Bethge</surname><given-names>Matthias</given-names></name><name><surname>Franke</surname><given-names>Katrin</given-names></name><etal/></person-group><article-title>The temporal structure of the inner retina at a single glance</article-title><source>Scientific Reports</source><year>2020</year><volume>10</volume><issue>1</issue><elocation-id>4399</elocation-id><comment>ISSN 20452322, 12</comment><pub-id pub-id-type="pmcid">PMC7064538</pub-id><pub-id pub-id-type="pmid">32157103</pub-id><pub-id pub-id-type="doi">10.1038/s41598-020-60214-z</pub-id></element-citation></ref><ref id="R79"><label>79</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lurz</surname><given-names>Konstantin-Klemens</given-names></name><name><surname>Bashiri</surname><given-names>Mohammad</given-names></name><name><surname>Willeke</surname><given-names>Konstantin</given-names></name><name><surname>Jagadish</surname><given-names>Akshay K</given-names></name><name><surname>Wang</surname><given-names>Eric</given-names></name><name><surname>Walker</surname><given-names>Edgar Y</given-names></name><name><surname>Cadena</surname><given-names>Santiago A</given-names></name><name><surname>Muhammad</surname><given-names>Taliah</given-names></name><name><surname>Cobos</surname><given-names>Erick</given-names></name><name><surname>Tolias</surname><given-names>Andreas S</given-names></name><name><surname>Ecker</surname><given-names>Alexander S</given-names></name><etal/></person-group><source>Generalization in data-driven models of primary visual cortex</source><conf-name>Proceedings of the 9th International Conference on Learning Representations</conf-name><year>2021</year><issue>9</issue></element-citation></ref><ref id="R80"><label>80</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Prechelt</surname><given-names>Lutz</given-names></name></person-group><chapter-title>Early stopping - But when?</chapter-title><source>Neural Networks:Tricks of the Trade. Lecture Notes in Computer Science</source><year>1998</year><volume>1524</volume><fpage>55</fpage><lpage>69</lpage><comment>ISBN 9783642352881</comment><pub-id pub-id-type="doi">10.1007/978-3-642-35289-8_5</pub-id></element-citation></ref><ref id="R81"><label>81</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>Diederik P</given-names></name><name><surname>Ba</surname><given-names>Jimmy</given-names></name></person-group><source>Adam: A Method for Stochastic Optimization</source><conf-name>Proceedings of the 3rd International Conference on Learning Representations</conf-name><year>2015</year><issue>3</issue><fpage>1</fpage><lpage>15</lpage><comment>ISSN 09252312</comment><pub-id pub-id-type="doi">10.1145/1830483.1830503</pub-id></element-citation></ref><ref id="R82"><label>82</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christenson</surname><given-names>Matthias P</given-names></name><name><surname>Navid Mousavi</surname><given-names>S</given-names></name><name><surname>Oriol</surname><given-names>Elie</given-names></name><name><surname>Heath</surname><given-names>Sarah L</given-names></name><name><surname>Behnia</surname><given-names>Rudy</given-names></name></person-group><article-title>Exploiting colour space geometry for visual stimulus design across animals</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><year>2022</year><volume>377</volume><issue>1862</issue><comment>ISSN 14712970, 10</comment><pub-id pub-id-type="pmcid">PMC9441238</pub-id><pub-id pub-id-type="pmid">36058250</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2021.0280</pub-id></element-citation></ref><ref id="R83"><label>83</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>Jacob</given-names></name></person-group><source>Statistical Power Analysis for the Behavioral Sciences</source><publisher-name>Routledge</publisher-name><year>1988</year><comment>5</comment><pub-id pub-id-type="doi">10.4324/9780203771587</pub-id></element-citation></ref><ref id="R84"><label>84</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goulet-Pelletier</surname><given-names>Jean-Christophe</given-names></name><name><surname>Cousineau</surname><given-names>Denis</given-names></name></person-group><article-title>A review of effect sizes and their confidence intervals, Part I: The Cohen’s d family</article-title><source>The Quantitative Methods for Psychology</source><year>2018</year><volume>14</volume><issue>4</issue><fpage>242</fpage><lpage>265</lpage><comment>ISSN 2292-1354, 12</comment><pub-id pub-id-type="doi">10.20982/tqmp.14.4.p242</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Fig. 1</label><caption><title>Mouse RGCs display diverse responses to a natural movie stimulus</title><p><bold>(a)</bold> Illustration of a flat-mounted retina, with recording fields (white circles) and stimulus area centred on the red recording field indicated (cross marks optic disc; d, dorsal; v, ventral; t, temporal; n, nasal). <bold>(b)</bold> Natural movie stimulus structure (top) and example frames (bottom). The stimulus consisted of 5-s clips taken from UV-green footage recorded outside (<xref ref-type="bibr" rid="R22">22</xref>), with 3 repeats of a 5-clip test sequence (highlighted in grey) and a 108-clip training sequence (see Methods). <bold>(c)</bold> Representative recording field (bottom; marked by red square in (a)) showing somata of ganglion cell layer (GCL) cells loaded with Ca<sup>2+</sup> indicator OGB-1. <bold>(d)</bold> Ca<sup>2+</sup> responses of exemplary RGCs (indicated by circles in (c)) to chirp (left), moving bar (centre), and natural movie (right) stimulus. <bold>(e)</bold> Same recording field as in (c) but with cells colour-coded by functional RGC group (left; see Methods and (<xref ref-type="bibr" rid="R7">7</xref>)) and group responses (coloured, mean ± SD across cells; trace of example cells in (d) overlaid in black).</p></caption><graphic xlink:href="EMS158023-f001"/></fig><fig id="F2" position="float"><label>Fig. 2</label><caption><title>CNN model captures diverse tuning of RGC groups and predicts MEIs</title><p><bold>(a)</bold> Illustration of the CNN model and its output. The model takes natural movie clips as input (<xref ref-type="bibr" rid="R1">1</xref>), performs convolutions with 3D space-time separable filters (<xref ref-type="bibr" rid="R2">2</xref>) followed by a nonlinear activation function (<xref ref-type="bibr" rid="R3">3</xref>) in two consecutive layers (<xref ref-type="bibr" rid="R2">2</xref>–<xref ref-type="bibr" rid="R4">4</xref>) within its core, and feeds the output of its core into a per-neuron readout. For each RGC, the readout convolves the feature maps with a learned RF modelled as a 2D Gaussian (<xref ref-type="bibr" rid="R5">5</xref>), and finally feeds a weighted sum of the resulting vector through a softplus nonlinearity (<xref ref-type="bibr" rid="R6">6</xref>) to yield the firing rate prediction for that RGC (<xref ref-type="bibr" rid="R7">7</xref>). Numbers indicate averaged single-trial test set correlation between predicted (red) and recorded (black) responses. <bold>(b)</bold> Test set correlation between model prediction and neural response (averaged across three repetitions) as a function of response reliability (see Methods) for N=3,527 RGCs. Coloured dots correspond to example cells shown in <xref ref-type="fig" rid="F1">Fig. 1c-e</xref>. Dots in darker grey correspond to the N=1,947 RGCs that passed the model test correlation and movie response quality criterion (see Methods and <xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. S1c</xref>). <bold>(c)</bold> Test set correlation (as in (b)) of model vs. test set correlation of a linearised version of the model (for details, see Methods). Coloured dots correspond to RGC groups 1-32 (<xref ref-type="bibr" rid="R7">7</xref>). Dark and light grey dots as in (b). <bold>(d)</bold> Illustration of model-guided search for maximally exciting inputs (MEIs). The trained model captures neural tuning to stimulus features (far left). Starting from a randomly initialised input (2nd from left; a 3D tensor in space and time; only one colour channel illustrated here), the model follows the gradient along the tuning surface (far left) to iteratively update the input until it arrives at the stimulus (bottom right) that maximises the model neuron’s activation within an optimisation time window (0.66 s, grey box, top right).</p></caption><graphic xlink:href="EMS158023-f002"/></fig><fig id="F3" position="float"><label>Fig. 3</label><caption><title>Spatial, temporal and chromatic properties of MEIs differ between RGC groups</title><p><bold>(a)</bold> Spatial component of three example MEIs for green (top), UV (middle) and overlay (bottom). Solid and dashed circles indicate MEI centre and surround fit, respectively. For display, spatial components <italic>s</italic> in the two channels were re-scaled to a similar range and displayed on a common grey-scale map ranging from black for – <italic>max</italic>(|<italic>s</italic>|) to white for <italic>max</italic>(|<italic>s</italic>|), i.e. symmetric about 0 (grey). <bold>(b)</bold> Spatio-temporal (y-t) plot for the three example MEIs (from (a)) at a central vertical slice for green (top), UV (middle) and overlay (bottom). Grey-scale map analogous to (a). <bold>(c)</bold> Trajectories through colour space over time for the centre of the three MEIs. Trajectories start at the origin (grey level); direction of progress indicated by arrow heads. Bottom right: Bounding boxes of the respective trajectory plots. <bold>(d)</bold> Calculation of MEI centre size, defined as <italic>σ<sub>x</sub></italic> + <italic>σ<sub>y</sub></italic>, with <italic>σ<sub>x</sub></italic> and <italic>σ<sub>y</sub></italic> the s.d. in horizontal and vertical direction, respectively, of the DoG fit to the MEI. <bold>(e)</bold> Calculation of MEI temporal frequency: Temporal components are transformed using Fast Fourier Transform, and MEI frequency is defined as the amplitude-weighted average frequency of the Fourier-transformed temporal component. <bold>(f)</bold> Calculation of centre contrast, which is defined as the difference in intensity at the last two peaks (indicated by <italic>t</italic><sub>1</sub> and <italic>t</italic><sub>2</sub>, respectively, in (c)). For the example cell (orange markers and lines), green intensity decreases, resulting in OFF contrast, and UV intensity increases, resulting in ON contrast. <bold>(g)</bold> Distribution of green and UV MEI centre sizes across N=1,613 cells (example MEIs from (a-c) indicated by arrows; symbols as shown on top of (a)). 95% of MEIs were within an angle of ± 8° of the diagonal (solid and dashed lines); MEIs outside of this range are coloured by cell type. <bold>(h)</bold> As (g) but for distribution of green and UV MEI temporal frequency. 95% of MEIs were within an angle of ± 11.4° of the diagonal (solid and dashed lines). <bold>(i)</bold> As (g) but for distribution of green and UV MEI centre contrast. MEI contrast is shifted away from the diagonal (dashed line) towards UV by an angle of 33.2° due to the dominance of UV-sensitive S-opsin in the ventral retina. MEIs at an angle &gt; 45° occupy the upper left, colour-opponent (UV<sup>ON</sup>-green<sup>OFF</sup>) quadrant. <bold>(j, k)</bold> Fraction of MEIs per cell type that lie outside the angle about the diagonal containing 95% of MEIs for centre size and temporal frequency. Broad RGC response types indicated as in (<xref ref-type="bibr" rid="R7">7</xref>). <bold>(l)</bold> Fraction of MEIs per cell type in the upper-left, colour-opponent quadrant for contrast.</p></caption><graphic xlink:href="EMS158023-f003"/></fig><fig id="F4" position="float"><label>Fig. 4</label><caption><title>Experiments confirm MEIs predicted by model</title><p><bold>(a)</bold> MEIs shown during the experiment, with green and UV spatial components (top two rows), as well as green and UV temporal components (third row) and a spatio-temporal visualisation (fourth row). For display, spatial components <italic>s</italic> in the two channels were re-scaled to a similar range and displayed on a common grey-scale map ranging from black for – <italic>mαæ</italic>(|<italic>s</italic>|) to white for <italic>mαæ</italic>(|<italic>s</italic>|), i.e. symmetric about 0 (grey). Relative amplitudes of UV and green are shown in the temporal components. <bold>(b)</bold> Illustration of spatial layout of MEI experiment. White circles represent 5 × 5 grid of positions where MEIs were shown; red shading shows an example RF estimate of a recorded G<sub>32</sub> RGC, with black dot indicating the RF centre position (Methods). <bold>(c)</bold> Responses of example RGC from (b) to the 11 different MEI stimuli at 25 different positions. <bold>(d)</bold> Recorded (top, <italic>r<sup>(n</sup></italic>) and predicted (bottom, <inline-formula><mml:math id="M66"><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>) responses to the 11 different MEIs for example RGC <italic>n</italic> from (b, c). Left: responses are averaged across the indicated dimensions x, y (different MEI locations); black bar indicates MEI stimulus duration (from 0 to 1.66 s), grey rectangle marks optimisation time window (from 1 to 1.66s). Right: Response to different MEIs, additionally averaged across time (t; within optimisation time window). <bold>(e,f)</bold> Same as in (d), but additionally averaged across all RGCs (<italic>n</italic>) of G<sub>5</sub> (N=6) (e) and of G<sub>28</sub> (N=12) (f). Error bars show SD across cells. <bold>(g)</bold> Confusion matrix, each row showing the z-scored response magnitude of one RGC group (averaged across all RGCs of that group) to the MEIs in (a). Confusion matrix for recorded cells (top; “Data”) and for model neurons (bottom; “Model”). Black squares highlight broad RGC response types according to (<xref ref-type="bibr" rid="R7">7</xref>): OFF cells, (G<sub>1,5</sub>) ON-OFF cells (G<sub>10</sub>), fast ON cells (G<sub>18</sub>,<sub>20</sub>), slow ON (G<sub>21</sub>,<sub>23</sub>,<sub>24</sub>) and ON contrast suppressed (G<sub>28</sub>) cells, and OFF suppressed cells (G31,32).</p></caption><graphic xlink:href="EMS158023-f004"/></fig><fig id="F5" position="float"><label>Fig. 5</label><caption><title>Chromatic contrast selectivity derives from a warped representation of stimulus space</title><p><bold>(a)</bold> Schematic illustration of representations of MEIs in stimulus space. We consider the flattened spatio-temporal vectors defining each MEI as points in a D-dimensional space, where <italic>D = # channels × height × width × time</italic> = 28,800 (top). We then measure (dis)similarity between each pair (<italic>i</italic>, <italic>j</italic>) of MEIs in stimulus space representation, <italic>ϕ<sub>s</sub></italic>, as Euclidean distance between their vectors. We summarise the pairwise distances in a <italic>representational dissimilarity matrix</italic> RDM<italic><sup>ϕs</sup></italic> (bottom), whose rows and columns are indexed by stimulus. <bold>(b)</bold> Same as (a), but in neural response space of G<sub>20</sub> (real distances). Each MEI is represented as an <italic>N</italic>-dimensional vector holding the responses of <italic>N</italic> RGCs to this MEI, where <italic>N =</italic> # neurons for a given RGC group (here, N=8). <bold>(c)</bold> Same as RDM in (b) but for groups G<sub>23</sub> (top, N=40) and G<sub>28</sub> (bottom, N=12).</p></caption><graphic xlink:href="EMS158023-f005"/></fig><fig id="F6" position="float"><label>Fig. 6</label><caption><title>Chromatic contrast tuning allows detection of ground-to-sky transitions</title><p><bold>(a)</bold> Distribution of green and UV contrasts of all movie inter-clip transitions (centre), separately for the 4 transition types, for each of which an example is shown: ground-to-sky (N=525, top left, red triangle), ground-to-ground (N=494, top right, green disk), sky-to-ground (N=480, bottom left, black downward triangle), and sky-to-sky (N=499, bottom right, purple square). Images show last and first frame of pre- and post-transition clip, respectively. Traces show mean full-field luminance of green and UV channels in the last and first 1 s of the pre- and post-transition clip. Black trace shows luminance averaged across the two colour channels. <bold>(b)</bold> Distributions as in (a), but shown as contours indicating iso-density lines of inter-clip transitions in chromatic contrast space. Density of inter-clip transitions was estimated separately for each type of transition from histograms within 10 × 10 bins that were equally spaced within the coloured boxes. Four levels of iso-density for each transition type shown, with density levels at 20 % (outermost contour, strongest saturation), 40 %, 60 % and 80 % (innermost contour, weakest saturation) of the maximum density observed per transition: 28 sky-to-ground (black), 75 ground-to-ground (green), 42 sky-to-sky (purple) and 45 ground-to-sky (red) transitions per bin. Orange markers indicate locations of N=36 G<sub>28</sub> MEIs in chromatic contrast space (cf. <xref ref-type="fig" rid="F3">Fig. 3i</xref>). <bold>(c)</bold> Distribution of green and UV contrasts of N=122 inter-clip transitions seen by an example G<sub>28</sub> RGC, coloured by transition type. <bold>(d)</bold> Responses of example RGC in the 1 s following an inter-clip transition, averaged across transitions within the bins indicated by the grid. <bold>(e)</bold> Responses shown in (d), transformed into a tuning map by averaging within bins defined by grid in (c, d). Responses in (d,e) are z-scored (<italic>μ</italic> = 0, <italic>σ</italic> = 1). <bold>(f)</bold> Tuning map of G<sub>28</sub> RGCs (N=78), created by averaging the tuning maps of the individual RGCs (overlaid with outermost contour lines from (b)). <bold>(g,h)</bold> Same as (f) for G<sub>21</sub> RGCs ((g), N=97) and G<sup>5</sup> ((h), N=33). <bold>(i)</bold> <italic>Top:</italic> Illustration of ROC analysis for two RGCs, a G<sub>21</sub> (left) and a G<sub>28</sub> (right). For each RGC, responses to all inter-clip transitions were binned, separately for ground-to-sky (red) and all other transitions (grey). <italic>Middle:</italic> Sliding a threshold <italic>d</italic> across the response range, classifying all transitions with response <italic>&gt; d</italic> as ground-to-sky, and registering the false-positive-rate (FPR) and true-positive-rate (TPR) for each threshold yields an ROC curve. Numbers in brackets indicate (FPR, TPR) at the threshold indicated by vertical line in histograms. <italic>Bottom:</italic> We evaluated performance for each cell as area under the ROC curve (AUC), and plotted the distribution across AUC values for all cells (black), G<sub>21</sub> (grey), G<sub>5</sub> (blue), and G<sub>28</sub> (orange); AUC mean ± SD indicated as dots and horizontal lines above histograms.</p></caption><graphic xlink:href="EMS158023-f006"/></fig><fig id="F7" position="float"><label>Fig. 7</label><caption><title>Electrical single-cell recordings of responses to MEI stimuli confirm chromatic selectivity of tSbC RGCs.</title><p><bold>(a)</bold> Spiking activity (top, raster plot; middle, firing rate) of a OND RGC in response to different MEI stimuli (black bar indicates MEI stimulus duration; grey rectangle marks optimisation time window, from 1 to 1.66 s). Bottom: Activation relative to mean as a function of MEI stimulus, averaged across cells (solid line, from electrical recordings, N=4; dashed line, from Ca<sup>2+</sup> imaging, N=11 cells). Colours as in <xref ref-type="fig" rid="F4">Fig. 4</xref>. <bold>(b)</bold> Like (a) but for a sustained ON <italic>α</italic> cell (G<sub>24</sub>; N=4 cells, both for electrical and Ca<sup>2+</sup> recordings). <bold>(c)</bold> ON delayed (OND/tSbC, G<sub>28</sub>) RGC (green) from (a) dye-loaded by patch pipette after cell-attached electrophysiology recording (z-projection; x-y plane). <bold>(d)</bold> Cell from (c, green) as side-projection (x-z), showing dendritic stratification pattern relative to choline-acetyltransferase (ChAT) amacrine cells (tdTomato, red) within the inner plexiform layer (IPL).</p></caption><graphic xlink:href="EMS158023-f007"/></fig></floats-group></article>