<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS155622</article-id><article-id pub-id-type="doi">10.1101/2022.10.10.511571</article-id><article-id pub-id-type="archive">PPR557135</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group><subj-group subj-group-type="europepmc-category"><subject>Covid-19</subject></subj-group></article-categories><title-group><article-title>GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zvyagin</surname><given-names>Maxim</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="fn" rid="FN1">†</xref></contrib><contrib contrib-type="author"><name><surname>Brace</surname><given-names>Alexander</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="fn" rid="FN1">†</xref></contrib><contrib contrib-type="author"><name><surname>Hippe</surname><given-names>Kyle</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="fn" rid="FN1">†</xref></contrib><contrib contrib-type="author"><name><surname>Deng</surname><given-names>Yuntian</given-names></name><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref><xref ref-type="fn" rid="FN1">†</xref></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Bin</given-names></name><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Bohorquez</surname><given-names>Cindy Orozco</given-names></name><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Clyde</surname><given-names>Austin</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Kale</surname><given-names>Bharat</given-names></name><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Perez-Rivera</surname><given-names>Danilo</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A7">7</xref></contrib><contrib contrib-type="author"><name><surname>Ma</surname><given-names>Heng</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Mann</surname><given-names>Carla M.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Irvin</surname><given-names>Michael</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Pauloski</surname><given-names>J. Gregory</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Ward</surname><given-names>Logan</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Hayot-Sasson</surname><given-names>Valerie</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Emani</surname><given-names>Murali</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Foreman</surname><given-names>Sam</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Xie</surname><given-names>Zhen</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Lin</surname><given-names>Diangen</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Shukla</surname><given-names>Maulik</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Nie</surname><given-names>Weili</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Romero</surname><given-names>Josh</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Dallago</surname><given-names>Christian</given-names></name><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A9">9</xref></contrib><contrib contrib-type="author"><name><surname>Vahdat</surname><given-names>Arash</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Xiao</surname><given-names>Chaowei</given-names></name><xref ref-type="aff" rid="A8">8</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Gibbs</surname><given-names>Thomas</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Foster</surname><given-names>Ian</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Davis</surname><given-names>James J.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Papka</surname><given-names>Michael E.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A10">10</xref></contrib><contrib contrib-type="author"><name><surname>Brettin</surname><given-names>Thomas</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Stevens</surname><given-names>Rick</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Anandkumar</surname><given-names>Anima</given-names></name><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A11">11</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Vishwanath</surname><given-names>Venkatram</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Ramanathan</surname><given-names>Arvind</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>Argonne National Laboratory</aff><aff id="A2"><label>2</label>University of Chicago</aff><aff id="A3"><label>3</label>NVIDIA Inc</aff><aff id="A4"><label>4</label>Harvard University</aff><aff id="A5"><label>5</label>Cerebras Inc</aff><aff id="A6"><label>6</label>Northern Illinois University</aff><aff id="A7"><label>7</label>New York University</aff><aff id="A8"><label>8</label>Arizona State University</aff><aff id="A9"><label>9</label>Technical University of Munich</aff><aff id="A10"><label>10</label>University of Illinois Chicago</aff><aff id="A11"><label>11</label>California Institute of Technology</aff><author-notes><corresp id="CR1"><label>*</label>Contact authors: <email>venkat@anl.gov</email>, <email>anima@caltech.edu</email>, <email>ramanathana@anl.gov</email></corresp><fn id="FN1"><label>†</label><p id="P1">Joint first authors,</p></fn><fn id="FN2"><p id="P2"><bold>ACM Reference Format:</bold></p><p id="P3">Maxim Zvyagin<sup>1†</sup>, Alexander Brace<sup>1,2†</sup>, Kyle Hippe<sup>1†</sup>, Yuntian Deng<sup>3,4†</sup>, Bin Zhang<sup>5</sup>, Cindy Orozco Bohorquez<sup>5</sup>, Austin Clyde<sup>1,2</sup>, Bharat Kale<sup>6</sup>, Danilo Perez-Rivera<sup>1,7</sup>, Heng Ma<sup>1</sup>, Carla M. Mann<sup>1,2</sup>, Michael Irvin<sup>1</sup>, J. Gregory Pauloski<sup>2</sup>, Logan Ward<sup>1</sup>, Valerie Hayot-Sasson<sup>1,2</sup>, Murali Emani<sup>1</sup>, Sam Foreman<sup>1</sup>, Zhen Xie<sup>1</sup>, Diangen Lin<sup>1,2</sup>, Maulik Shukla<sup>1,2</sup>, Weili Nie<sup>3</sup>, Josh Romero<sup>3</sup>, Christian Dallago<sup>3,9</sup>, Arash Vahdat<sup>3</sup>, Chaowei Xiao<sup>8,3</sup>, Thomas Gibbs<sup>3</sup>, Ian Foster<sup>1,2</sup>, James J. Davis<sup>1,2</sup>, Michael E. Papka<sup>1,10</sup>, Thomas Brettin<sup>1</sup>, Rick Stevens<sup>1,2</sup>, Anima Anandkumar<sup>3,11*</sup>, Venkatram Vishwanath<sup>1*</sup>, Arvind Ramanathan<sup>1*</sup>. 2020. GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics. In <italic>Supercomputing ’22: International Conference for High Performance Computing, Networking, Storage, and Analysis</italic>. ACM, New York, NY, USA, 14 pages. https://doi.org/finalDOI</p></fn></author-notes><pub-date pub-type="nihms-submitted"><day>14</day><month>10</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>11</day><month>10</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P4">We seek to transform how new and emergent variants of pandemic-causing viruses, specifically SARS-CoV-2, are identified and classified. By adapting large language models (LLMs) for genomic data, we build genome-scale language models (GenSLMs) which can learn the evolutionary landscape of SARS-CoV-2 genomes. By pretraining on over 110 million prokaryotic gene sequences and finetuning a SARS-CoV-2-specific model on 1.5 million genomes, we show that GenSLMs can accurately and rapidly identify variants of concern. Thus, to our knowledge, GenSLMs represents one of the first whole genome scale foundation models which can generalize to other prediction tasks. We demonstrate scaling of GenSLMs on GPU-based supercomputers and AI-hardware accelerators utilizing 1.63 Zettaflops in training runs with a sustained performance of 121 PFLOPS in mixed precision and peak of 850 PFLOPS. We present initial scientific insights from examining GenSLMs in tracking evolutionary dynamics of SARS-CoV-2, paving the path to realizing this on large biological data.</p></abstract><kwd-group><kwd>SARS-CoV-2</kwd><kwd>COVID-19</kwd><kwd>HPC</kwd><kwd>AI</kwd><kwd>Large language models</kwd><kwd>whole genome analyses</kwd></kwd-group></article-meta></front><body><sec id="S1"><label>1</label><title>Justification</title><p id="P5">We demonstrate using &gt;1.63 Zettaflops, sustained performance of 121 PFLOPS (mixed precision) and 850 PFLOPS peak, in training one of the largest foundation models on whole genome sequences to characterize SARS-CoV-2 variants of concern. Our models will inform timely public health intervention strategies and downstream vaccine development for emerging variants.</p></sec><sec id="S2"><label>2</label><title>Performance attributes</title><table-wrap id="T7" position="anchor" orientation="portrait"><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Performance Attribute</th><th align="left" valign="top">Our Submission</th></tr></thead><tbody><tr><td align="left" valign="top">Category of achievement<break/>Type of method used<break/>Results reported on basis of Precision reported<break/>System scale<break/>Measurement mechanism</td><td align="left" valign="top">Scalability; time-to-solution<break/>Explicit; deep learning<break/>Whole application including I/O<break/>Mixed Precision<break/>Measured on full system<break/>Hardware performance<break/>counters; application timers;<break/>performance modeling</td></tr></tbody></table></table-wrap></sec><sec id="S3"><label>3</label><title>Overview of the problem</title><p id="P6">Tracking of novel and emergent variants for viruses such as severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has been enabled by rapid sequencing and sharing of whole genome sequence data (<xref ref-type="bibr" rid="R37">Otto et al., 2021</xref>). As of September 2022, &gt;13 million SARS-CoV-2 genomes have been deposited in the GISAID repository<sup><xref ref-type="fn" rid="FN3">1</xref></sup>. SARS-CoV-2 represents one of the most deeply sequenced viral genomes and is therefore a rich source of information for understanding various factors that drive its evolution. Despite its slow mutation rate, over the past three years SARS-CoV-2 has evolved several variant strains containing unique mutation patterns, many of which lead to novel viral phenotypes including higher antigenicity, transmissibility, and fitness (<xref ref-type="bibr" rid="R10">Cosar et al., 2022</xref>).</p><p id="P7">This has prompted the US Centers for Disease Control and Prevention (CDC) to identify four SARS-CoV-2 variant categories, including: variants being monitored (VBM), variants of interest (VOI), variants of concern (VOC), and variants of high consequence (VOHC). Classification stems from SARS-CoV-2 growth dynamics and threat to pre-existing immunity (<xref ref-type="bibr" rid="R10">Cosar et al., 2022</xref>, <xref ref-type="bibr" rid="R37">Otto et al., 2021</xref>). Today, SARS-CoV-2 VOCs include B.1.1.7 (Alpha), B.1.617.2 (Delta), and B.1.1.529/BA.1-BA.5 (Omicron). Although deep sequencing of viral genomes across patient populations has enabled substantial progress, identifying variants is still tedious and resource-intensive, requiring costly laboratory tests and diagnostics. Together, these factors contribute to significant time expenditure to recognize and subsequently make informed decisions for public health intervention strategies (<xref ref-type="bibr" rid="R4">Baker et al., 2021</xref>).</p><p id="P8">Artificial intelligence (AI) and machine learning (ML) approaches promise to transform real-time pandemic monitoring (<xref ref-type="bibr" rid="R52">Syrowatka et al., 2021</xref>). Instead of reacting after the emergence of variants to identify VOCs over potentially several weeks (see <xref ref-type="sec" rid="S5">Sec. 4.1</xref>), AI/ML techniques can leverage deep sequencing data to proactively identify mutations in viral proteins and characterize evolutionary patterns that can assist in predicting and describing future VOCs (<xref ref-type="bibr" rid="R7">Beguir et al., 2022</xref>, <xref ref-type="bibr" rid="R22">Hie et al., 2021</xref>). However, obtaining high-quality, global-scale genome datasets can be challenging, as diverse sequencing technologies can result in variable quality and coverage of sequenced genomes. Sequence-based feature extraction techniques followed by traditional ML approaches have demonstrated promise in the early identification of VOCs (<xref ref-type="bibr" rid="R7">Beguir et al., 2022</xref>, <xref ref-type="bibr" rid="R33">Maher et al., 2022</xref>, <xref ref-type="bibr" rid="R59">Wallace et al., 2022</xref>); however, they remain limited to sequence signatures of regions of interest in the genome. This unmet challenge presents an opportunity for effective whole genome-scale surveillance of global pandemics and early identification of VOCs, with the goal of enabling the development of robust public health intervention strategies prior to surges in case numbers and improving vaccine-design strategies on emerging variants.</p><p id="P9">We posit that by leveraging the recent success of large-language models (LLMs) in natural language processing (NLP) tasks (<xref ref-type="bibr" rid="R63">Wei et al., 2022</xref>), we can develop global-scale, whole genome surveillance tools. In this paper, we use LLMs to characterize SARS-CoV-2 evolutionary dynamics and reconstruct SARS-CoV-2 variant emergence. We adapt LLMs developed for understanding human languages to genomic sequences, called <italic>genome-scale language models (GenSLM)</italic>, and validate this approach in modeling VOC assignments for SARS-CoV-2 using historical data. Our contributions include:</p><list list-type="bullet" id="L1"><list-item><p id="P10">We develop the largest <italic>biological</italic> LLMs with codon tokenization (with 2.5 and 25 billion trainable parameters) to date, trained across a diverse set of 110 million prokaryotic gene sequences. These are the first foundation models trained on raw nucleotide sequences to demonstrate substantial improvement in predictive performance in identifying VOCs. We make these models and weights openly available to the scientific community <sup><xref ref-type="fn" rid="FN4">2</xref></sup>.</p></list-item><list-item><p id="P11">We design and validate a novel hierarchical transformerbased model that uses both Generative Pre-trained Transformers (GPT) (on individual gene sequences) and stable diffusion to capture the correct context and longer-range interactions in genome-scale datasets. This model enables us to prospectively model SARS-CoV-2 evolution by leveraging its generative capabilities.</p></list-item><list-item><p id="P12">We showcase training foundation models on both conventional (GPU-based) systems (Polaris at ALCF and Selene at NVIDIA) and on emerging AI-accelerator hardware (interconnected Cerebras CS-2 systems), and demonstrate high watermarks for time-to-solution (model performance described by its perplexity or accuracy). In addition, we present scaling benchmarks, which demonstrate that training GenSLMs can be intensive—performing over 1.63 × 10<sup>21</sup> floating point operations (a mix of FP16 and FP32; 1.63 Zettaflops) with a sustained performance of 121 PFLOPS (mixed precision) and 850 PFLOPS peak, over the course of training runs.</p></list-item></list><p id="P13">Together, these capabilities go beyond state-of-the-art techniques for global-scale whole genome surveillance of pandemic-causing viruses and address a critical infrastructure need for global public health organizations.</p></sec><sec id="S4"><label>4</label><title>Current state of the art</title><p id="P14">Current approaches for tracking viral evolution rely on infectious disease specialists who examine variations, identify epitopes of interest (i.e., portions of the virus that elicit immune response), classify variants, and eventually flag them for further laboratory testing and analysis (<xref ref-type="bibr" rid="R9">Brouwer et al., 2020</xref>, <xref ref-type="bibr" rid="R18">Greaney et al., 2021</xref>, <xref ref-type="bibr" rid="R27">Ju et al., 2020</xref>, <xref ref-type="bibr" rid="R67">Zost et al., 2020</xref>). This process is widely used for tracking viral infections, including seasonal influenza (<xref ref-type="bibr" rid="R14">Doud et al., 2018</xref>). Identifying strains of interest helps prioritize downstream vaccine development workflows. However, this process is time-consuming and laborious. While data sharing in the community has enabled unprecedented progress in developing vaccines for pandemics such as COVID-19, there still exists an unmet challenge in accelerating the detection and prediction of viral VOCs via computational and experimental toolkits.</p><sec id="S5"><label>4.1</label><title>Early warning systems for viral evolution</title><p id="P15">Several early warning systems for tracking COVID-19 have been developed; however, they utilize case counts, internet search parameters, and other allied data focused on monitoring case counts in a local geographic area (<xref ref-type="bibr" rid="R44">Ramchandani et al., 2020</xref>). The Bacterial and Viral Bioinformatics Resource Center (BV-BRC)<sup><xref ref-type="fn" rid="FN5">3</xref></sup> provides the SARS-CoV-2 Emerging Variant Tracking and Early Warning System, which enables users to browse current and past variant lineages and track their prevalence by isolation date, geographic location, and other metadata fields. A heuristic is used to compute month-over-month growth rates and highlight rapidly growing variants that may cause future infection surges. Mutations from each variant are mapped to known epitope sites and regions of the genome known to be involved in antibody escape to enable further assessment of mutation impact. Recently, Hie et al. (<xref ref-type="bibr" rid="R22">Hie et al., 2021</xref>) used protein language models (PLMs) and adapted concepts from NLP to model escape variants across three different viruses, including SARS-CoV-2. In each virus, they identified a certain protein (e.g., SARS-CoV-2 Spike/S protein) and modeled its evolutionary history using transformers to describe differences between ordinary variants and VOCs. Similarly, Beguir et al. (<xref ref-type="bibr" rid="R7">Beguir et al., 2022</xref>) leveraged a PLM to accurately classify VOCs; using experimental assays, they also validated these VOCs and demonstrated the ability to flag them in advance of World Health Organisation designation. However, viral evolution is not isolated to one protein but occurs at the genome scale. We propose a system that <italic>learns to model whole-genome evolution patterns using LLMs based on observed data</italic>.</p></sec><sec id="S6"><label>4.2</label><title>Large language models (LLMs)</title><p id="P16">The introduction of transformers (<xref ref-type="bibr" rid="R57">Vaswani et al., 2017</xref>) — and subsequent LLMs such as Bidirectional Encoder Representations from Transformers (BERT) (<xref ref-type="bibr" rid="R13">Devlin et al., 2018</xref>) and generative pre-trained transformers (GPT) (<xref ref-type="bibr" rid="R41">Radford et al., 2018</xref>) — has revolutionized natural language understanding. These models have been used to generate text, speech (<xref ref-type="bibr" rid="R19">Gulati et al., 2020</xref>), and images (<xref ref-type="bibr" rid="R21">Han et al., 2022</xref>). They have also been employed to understand the language of nucleic acids (DNA/RNA) and proteins. Protein language models (PLMs), using amino acid alphabets, are the most heavily investigated biological LLMs (<xref ref-type="bibr" rid="R16">Elnaggar et al., 2022</xref>, <xref ref-type="bibr" rid="R46">Rives et al., 2021</xref>), with demonstrated success in downstream tasks such as protein function prediction (<xref ref-type="bibr" rid="R56">Unsal et al., 2022</xref>) and engineering (<xref ref-type="bibr" rid="R17">Ferruz et al., 2022</xref>). Nucleotide LLMs, using DNA/RNA alphabets, are still understudied (<xref ref-type="bibr" rid="R2">Avsec et al., 2021</xref>). Compared to the rich alphabet and short length of information-dense protein sequences that traditional attention models from NLP can successfully learn, nucleotide LLMs rely on much simpler alphabets and extremely long-range signal (e.g., across open reading frames or co-evolutionary patterns) and require significant domain adaptation to yield good results. When applied on the scale of entire genomes, GenSLMs also operate on much larger sequence lengths than are traditionally seen in NLP applications—the max sequence length for SARS CoV2 tasks was 10,240 tokens in comparison to the standard 1,024 or 2,048. Further, viral genomes often undergo frameshift mutations leading to differential translation, introducing ambiguity not present at the protein scale. Our work addresses these challenges by leveraging a hierarchical LLM: a generative pre-trained transformer to capture shorter/local (codon-level) interactions, and a diffusionbased model to capture longer-range interactions to describe the biological complexity of viral evolution (<xref ref-type="sec" rid="S12">Sec. 5.2</xref>).</p></sec><sec id="S7"><label>4.3</label><title>Workflow infrastructure</title><p id="P17">Scientific applications for HPC are increasingly written as a composition of many interconnected components. Application components may have different hardware or software requirements, run durations and execution frequencies, or dependencies with other components. Workflow systems such as Swift, Parsl, Balsam, and RADICAL Cybertools support the design of applications as directed graphs of tasks and manage their execution on available resources.</p><p id="P18">There is significant diversity in workflow implementation; e.g., Swift/T expresses workflows in bespoke programming languages that are compiled into an MPI program (<xref ref-type="bibr" rid="R64">Wozniak et al., 2013</xref>). Parsl, in contrast, is built on Python’s native concurrency library and dynamically constructs a task graph as a Python program is interpreted (<xref ref-type="bibr" rid="R3">Babuji et al., 2019</xref>). Balsam (<xref ref-type="bibr" rid="R47">Salim et al., 2019</xref>) and RADICAL CyberTools (<xref ref-type="bibr" rid="R6">Balasubramanian et al., 2019</xref>) rely on a central task database from which the launcher, running on compute resources, pulls and executes tasks. A centralized database enables state persistence across runs, and task dependencies can be defined as a DAG. Most workflow systems support interfacing with HPC job schedulers or cloud providers to acquire resources and transmit files between remote resources–key features for our use case.</p><p id="P19">Dynamic workflows, where new tasks are continually added in response to new results, are emerging as an extension of workflow managers. Many dynamic workflow systems, such as DeepHyper (<xref ref-type="bibr" rid="R5">Balaprakash et al., 2018</xref>) and RocketSled (<xref ref-type="bibr" rid="R15">Dunn et al., 2019</xref>), are purpose-built to solve optimization problems. LibEnsemble (<xref ref-type="bibr" rid="R24">Hudson et al., 2022</xref>) provides a more general interface where users decouple a dynamic ensemble into a “generator” which spawns new tasks based on results from a “simulator.” Toolkits such as Ray (<xref ref-type="bibr" rid="R34">Moritz et al., 2018</xref>) and Colmena (<xref ref-type="bibr" rid="R62">Ward et al., 2021</xref>) provide more flexible approaches where a number of “agents” can cooperatively coordinate tasks. These libraries handle <italic>where</italic> and <italic>how</italic> tasks are executed and provide useful abstractions so users can focus on component/task logic (i.e., <italic>what</italic> and <italic>when)</italic>.</p></sec></sec><sec id="S8"><label>5</label><title>Innovations realized</title><p id="P20">Given the limitations of current approaches in identifying VOCs, there is a need to develop an integrated system that can automatically ‘learn’ features within the SARS-CoV-2 genome that distinguish VOCs, while also being able to generate new sequences that characterize emerging variants of the virus. We posit that by leveraging existing sequencing data on the virus, we can train LLMs that can model the SARS-CoV-2 evolutionary trajectory. Training LLMs on SARS-CoV-2 genome datasets is non-trivial due to the need to: (1) address the limitations of training LLMs with genomic sequences; and (2) overcome infrastructural challenges to enable LLM training on large sequence lengths in a reasonable time.</p><sec id="S9"><label>5.1</label><title>Data collection and description</title><sec id="S10"><label>5.1.1</label><title>SARS-CoV-2 genome dataset</title><p id="P21">The Bacterial and Viral Bioinformatics Resource Center (BV-BRC) web resource provides integrated data and analysis tools for bacterial and viral pathogens to support infectious disease research. It hosts &gt;600,000 bacterial genomes and 8.7 million viral genomes, including 6.4 million SARS-CoV-2 genomes. All SARS-CoV-2 genome sequences were acquired from NCBI’s GenBank and SRA databases and uniformly annotated using VIGOR4 (<xref ref-type="bibr" rid="R61">Wang et al., 2012</xref>) to provide accurate and consistent annotation of open reading frames (ORFs) and mature peptides across all SARS-CoV-2 genomes. Automated and manual curation provided accurate and uniform metadata across all genomes, including host name, geographic location, and collection date.</p><p id="P22">To build GenSLMs for detecting and predicting SARS-CoV-2 variants of interest, we used &gt;1.5 million high-quality BV-BRC SARS-CoV-2 complete genome sequences. We filtered out any genome sequences with &lt; 29,000 bp and &gt;1% ambiguous bases. However, we note here that the data collected might not have sufficient diversity—meaning that any model trained on the SARS-CoV-2 dataset may end up overfitting to the data, with little opportunity to generalize. Hence, we took a foundation model-based approach that allowed us to first build a more general model using a much larger collection of diverse genomic data, namely gene-level data from prokaryotes.</p><p id="P23">We also utilized a dataset collected by the Houston Methodist Hospital System - one of the largest single-institution collections of SARS-CoV-2 genome sequences in the United States. We started here with 70,000 SARS-CoV-2 patient samples collected from May 15, 2020 to January 14, 2022 and sequenced on Illumina instruments. To ensure high quality, we first masked the leading and trailing 100 nucleotides for each sequence, as well as 56 positions in the spike protein-encoding region that had low depth due to poor primer binding. Sequences with &gt;256 ambiguous characters were discarded, leaving 16,545 total sequences. This subset was used for building phylogenetic analyses at genome-scale (see <xref ref-type="sec" rid="S17">Sec. 5.3</xref>).</p></sec><sec id="S11"><label>5.1.2</label><title>BV-BRC dataset</title><p id="P24">To allow for better generalization and to avoid overfitting of models to the SARS-CoV-2 data, we used &gt;110 million unique prokaryotic gene sequences from BV-BRC. The BV-BRC database provides cross-genera protein families, PGfams, which allow for collection of homologous gene or protein sequences across taxa that perform the same biological function (<xref ref-type="bibr" rid="R11">Davis et al., 2016</xref>). We queried BV-BRC to find 10,206 unique PGfams, each with &gt;30,000 unique members. For each PGfam, we collected high-quality non-redundant gene and protein sequences after filtering out sequences that were more than one standard deviation from the PGfam’s mean length. We term the Genome-Scale Language Models (GenSLMs) models trained on this data <italic>foundation models</italic>.</p></sec></sec><sec id="S12"><label>5.2</label><title>Large language models</title><p id="P25">Large-language model (LLM) training required both algorithmic and performance-level innovations. For algorithmic innovations, we describe two key limitations of current LLMs. For performance innovations in achieving optimal time-to-solution (training time to achieve some accuracy or perplexity), we leverage an interconnected cluster of four Cerebras CS-2 AI accelerators and scale to large GPU-based supercomputers to train our LLMs.</p><sec id="S13"><label>5.2.1</label><title>Genome-scale Language Models (GenSLMs)</title><p id="P26">We introduce GenSLMs as a means to go beyond current PLMs to describe evolutionary dynamics of SARS-CoV-2. Instead of focusing on specific proteins, GenSLMs leverage genome-scale data to model individual mutations at the nucleotide scale, thus implicitly accounting for protein-level mutations at the codon level. <xref ref-type="fig" rid="F1">Fig. 1</xref> shows that GenSLMs take nucleotide sequences of SARS-CoV-2 genomes as input and learns a semantic embedding of individual codons, which can then be <italic>translated</italic> to the 29 individual protein sequences that are encoded by the virus.</p><p id="P27">However, there are two fundamental challenges when training GenSLMs directly from SARS-CoV-2 genome sequences: (1) The entire genome consists of ~30,000 nucleotides (which translates to ~10,000 codons/amino-acids). LLM training on long sequences can be challenging because attention mechanisms largely focus on shorter/local segments of the genome rather than global patterns. (2) The overall sequence similarity in SARS-CoV-2 genomes is high (&gt;~99%), with only a small (yet significant) number of changes that yield distinct phenotypes. Thus, there is a need to address diversity in the sequences such that the trained model can generalize. It is also necessary to account for frameshifts in viral genomes.</p><p id="P28">To overcome these challenges, GenSLM implicitly recognizes intrinsic hierarchy (based on the central dogma) of individual protein production via DNA transcription and mRNA translation. We trained on gene-level data from BV-BRC (see <xref ref-type="sec" rid="S9">Sec. 5.1</xref>) to mimic this process with GenSLMs. Although mapping between codons and amino acids is degenerate (multiple codons may encode the same amino acid) (<xref ref-type="bibr" rid="R48">Shin et al., 2015</xref>), we posited that with sufficient diversity in the dataset, GenSLMs could exploit intrinsic organization of gene-level data to learn biologically-meaningful latent representations. The training process follows a procedure similar to the one outlined in (<xref ref-type="bibr" rid="R66">Zhang et al., 2022</xref>). We refer to the models trained on the BV-BRC dataset as GenSLM foundation models.</p><p id="P29">While the benefits of pre-training LLMs on natural text are well known (<xref ref-type="bibr" rid="R55">Turc et al., 2019</xref>), obtaining the optimal number of transformer layers and training on such a diverse set of gene data were challenging. We, therefore, trained GenSLM foundation models on a wide set of parameter scales ranging from 25 million to 25 billion, with a maximum sequence length of 2,048 tokens on the BV-BRC dataset. Additionally, to evaluate performance on downstream tasks, we fine-tuned the foundation model GenSLMs using a maximum sequence length of 10,240 tokens for the 25M and 250M model sizes on the SARS-CoV-2 datasets (see <xref ref-type="table" rid="T1">Table 1</xref>). We note that for the larger model sizes (2.5B and 25B), training on the 10,240 length SARS-CoV-2 data was infeasible on GPU clusters due to out-of-memory errors during attention computation.</p><p id="P30">The entire repertoire of results from the GenSLM foundation models is beyond the scope of this paper. However, as an empirical demonstration of the power of GenSLMs trained on the SARS-CoV-2 genomes, the learned latent space projected onto a low-dimensional manifold as determined by t-distributed Stochastic Neighbor Embedding (t-SNE) meaningfully distinguishes the SARS-CoV-2 variants as shown in <xref ref-type="fig" rid="F2">Fig. 2</xref>. This observation is significant because this GenSLM-25M model was specifically trained only on the first year of the SARS-CoV-2 data (consisting of ~ 85,000 SARS-CoV-2 genome sequences) – meaning that the model did not have the opportunity to see any of the other strains. Thus, the ability of GenSLM to generalize and distinguish between SARS-CoV-2 variants implies that the learning process is robust and the underlying features can generalize to downstream tasks. We also note that as the model parameters increase, the perplexity of the model also improves, agreeing with previous observations (<xref ref-type="bibr" rid="R41">Radford et al., 2018</xref>).</p><p id="P31">We note however that these training runs frequently take &gt;1 week on dedicated GPU resources (such as Polaris@ALCF). To enable training of the larger models on the full sequence length (10,240 tokens), we leveraged AI-hardware accelerators such as Cerebras CS-2, both in a stand-alone mode and as an inter-connected cluster, and obtained GenSLMs that converge in less than a day (<xref ref-type="sec" rid="S16">Sec. 5.2.4</xref>).</p></sec><sec id="S14"><label>5.2.2</label><title>Reward-guided beam search for generative modeling</title><p id="P32">A subsequent use of the GenSLM models is in its ability to generate new SARS-CoV-2 sequences, with the eventual goal of predicting yet unseen VOCs. One challenge with such sequence-based generation strategies is sampling sequences with particular properties. Given a conditional sequence model <italic>p<sub>θ</sub></italic> with weights, <italic>θ</italic>, the most likely sequence is <inline-formula><mml:math id="M1"><mml:msub><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∏</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:msub><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> where <italic>c</italic> is the context from the previous inference. However, computing this directly is generally intractable as it is 𝓞(64<sup><italic>T</italic></sup>), where <italic>T</italic> is the maximum sequence length with a vocabulary of size 64. Heuristics like greedy sampling are commonly used, where a sequence is generated iteratively, with the next token <italic>x<sub>t</sub></italic>, maximizing <italic>pθ</italic>(<italic>x<sub>t</sub></italic>∣<italic>x<sub>0</sub></italic>,…,<italic>x</italic><sub><italic>t</italic>-1</sub>, <italic>c</italic>) with complexity 𝓞(<italic>T</italic>).</p><p id="P33">Beam search is standard practice, combining a search strategy with a heuristic where <italic>k</italic> is the number of beams explored with complexity 𝓞(<italic>kT</italic>). First, <italic>k</italic> samples are drawn with the highest probability (or sampled from a multinomial distribution) and added to the set of possible hits 𝓧<sub>beam</sub>. Let <inline-formula><mml:math id="M2"><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">X</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:math></inline-formula> be the set of beams of length <italic>i</italic>. Then, for time step <italic>t</italic>, select <italic>k</italic> tokens <italic>x<sub>t</sub></italic> from the set of all tokens which score highest (or sampled from multinominal distribution) via <inline-formula><mml:math id="M3"><mml:msub><mml:mi>p</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">X</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:math></inline-formula> The highest scoring beams from 𝓧<sub>beam</sub> are selected via <inline-formula><mml:math id="M4"><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>L</mml:mi><mml:mi>α</mml:mi></mml:msup></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:munderover><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> and output, where <italic>L</italic> is the length of a sequence and <italic>α</italic>, is a length penalty.</p><p id="P34">Given an episodic reward function <inline-formula><mml:math id="M5"><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:munderover><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:math></inline-formula> we modify the scoring function for beam search with <disp-formula id="FD1"><label>(1)</label><mml:math id="M6"><mml:mstyle displaystyle="true"><mml:mfrac><mml:mi>μ</mml:mi><mml:msup><mml:mi>L</mml:mi><mml:mi>α</mml:mi></mml:msup></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:munderover><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></disp-formula> where 0 ≤ <italic>μ</italic> ≤ 1 is a hyperparameter. Since the reward function is episodic, at each step of beam search, the highest scoring beams are chosen with <disp-formula id="FD2"><label>(2)</label><mml:math id="M7"><mml:mi>μ</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">X</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:math></disp-formula></p><p id="P35">This scoring modification effectively alters the likelihood of tokens to be sampled based on maximizing the reward function. In order to sample sequences that are similar to a fixed sequence <italic>y</italic>, we utilize <italic>r<sub>t</sub></italic>(<italic>x<sub>t</sub></italic>) equal to the global alignment score between <italic>y<sub>t</sub></italic> and <italic>x<sub>t</sub></italic> (<xref ref-type="bibr" rid="R35">Needleman and Wunsch, 1970</xref>). This scoring bias modification effectively implements a property scoring function into beam search without altering the complexity of beam search sampling. In the case of non-episodic reward functions, rewards can only be computed at the final time step in <xref ref-type="disp-formula" rid="FD1">eq. 1</xref>.</p></sec><sec id="S15"><label>5.2.3</label><title>Diffusion-based hierarchical modeling</title><p id="P36">Token-level autoregressive modeling has difficulty in generating coherent long sequences due to its underlying challenge in capturing long-range dependencies (<xref ref-type="bibr" rid="R39">Papalampidi et al., 2022</xref>, <xref ref-type="bibr" rid="R50">Sun et al., 2021</xref>, <xref ref-type="bibr" rid="R51">2022</xref>). We developed a new hierarchical-modeling method based on a latent-space diffusion model that operates on the ‘sentence’ level. For each genome sequence, we uniformly truncated every 512 codons by a special separator symbol; these 512 codons are considered a ‘sentence.’ (We set 512 codons to be a sentence such that the average number of sentences per sequence, around 20, matches the number of ORFs and non-coding regions: around 17.)</p><p id="P37">Our diffusion-based hierarchical modeling method consists of three parts: <bold>(1) Learning high-level representations:</bold> We trained a new encoder to embed sentences into a latent space with a contrastive loss such that learned features better capture high-level dynamics. Our contrastive loss is similar to the masked language modeling objective used in SpanBERT (<xref ref-type="bibr" rid="R26">Joshi et al., 2020</xref>), where we predict missing sentences in the middle by conditioning on the previous and the next sentences, and, at the same time, using randomly sampled sentences as negatives (distractors).</p><p id="P38"><bold>(2) Modeling high-level dynamics with a diffusion model:</bold> Given the encoder output of each genome, i.e., a sequence of sentence embeddings, we train a diffusion model to learn their distribution. The diffusion model parameterizes the distribution of high-level representations by applying a sequence of denoising operations on top of Gaussian noise. Similar to previous work (<xref ref-type="bibr" rid="R23">Ho et al., 2020</xref>, <xref ref-type="bibr" rid="R58">Vincent, 2011</xref>), we used denoising score matching as the training objective; we gradually apply noise to desired target representations and the diffusion model learns to denoise at each step.</p><p id="P39"><bold>(3) Fine-tuning LMs with high-level planning:</bold> Similar to Time Control LM (<xref ref-type="bibr" rid="R60">Wang et al., 2022</xref>), we fine-tuned GenSLMs as the decoder to generate the genome sequence conditioned on high-level representations learned in step (1), which we term the ‘high-level plan’. The decoder predicts the current codon token using previous codon tokens within the context window size and the corresponding sentence embeddings. The training objective is the same as in training the original GenSLMs.</p><p id="P40">The overall generation procedure is shown in <xref ref-type="fig" rid="F3">Fig. 3</xref>. Note that without the guidance of the high-level representations z<sub>0</sub>, the decoder can only take into account a limited amount of context, but with the guidance of z<sub>0</sub>, the decoder can take into account long-term context because z<sub>0</sub> is modeled globally.</p><p id="P41">We conducted experiments by training a baseline LM and a diffusion-based hierarchical LM on the 1.5M SARS-CoV-2 genomes (see <xref ref-type="sec" rid="S10">Sec. 5.1.1</xref>). The goal of this experiment was to primarily assess if the diffusion model can ‘stitch’ together the context of the genes together at the genome-scale (much like how words are ordered in a sentence). The baseline LM is the hierarchical LM without high-level guidance - essentially, a normal transformer language model. We initialized both the baseline LM and the hierarchical LM decoders from the 2.5B foundation model trained on individual genes from BV-BRC (see <xref ref-type="sec" rid="S11">Sec. 5.1.2</xref>). We used a context window size of 1,024. The sentence encoder is initialized from the 25M foundation model. The diffusion denoising model is a transformer with the same architecture as BERT (<xref ref-type="bibr" rid="R30">Kenton and Toutanova, 2019</xref>). We used 10 nodes from Polaris@ALCF for training, with a total of 40 A100 GPUs. We used an Adam optimizer with a learning rate of 1e-4, a batch size of 2, and trained for 13k updates. Training took approximately 6 hours. At generation time, we used a sliding window-based approach: we first generate 1,023 codons from a start-of-sequence symbol, then move the window 512 codons to the right, generate the next 512 codons, and repeat this process until either end-of-sequence is generated or a maximum of 15k codons have been generated.</p><p id="P42">To evaluate if the generated samples capture high-level dynamics, we compared the distribution of the number of 5’-3’ ORFs on real data and on 1,000 samples from the model. As shown in <xref ref-type="fig" rid="F4">Fig. 4A</xref>, the diffusion-based hierarchical model outperforms the baseline LM in generating realistic ORFs, possibly due to the high-level plan, whereas the baseline LM can only account for the previous 1,023 codons. We also display the phylogenetic tree (see <xref ref-type="sec" rid="S17">Sec. 5.3</xref>) of the generated sequences from the diffusion-based hierarchical model against real genomes in <xref ref-type="fig" rid="F4">Fig. 4B</xref>. The plot exhibits that the generated sequences cover the different lineages including all the variants. Note that sequences with &gt;120 mutations (1.4% of all generated sequences) were excluded; this further demonstrates that the diffusion-based hierarchical model can generate sequences that are of higher quality than the standard transformer-based model.</p></sec><sec id="S16"><label>5.2.4</label><title>Training with full viral genome sequences on Cerebras Wafer-Scale Cluster</title><p id="P43">Training LLMs on whole SARS-CoV-2 genomes with dense attention is challenging when using traditional approaches and hardware. With codon-based encoding, the model needs to handle sequences of 10,240 tokens. This results in high memory and computational demand, severe limitations to batch sizes to fit on a single device, and thus a need to develop and orchestrate complicated hybrid parallelism approaches to get reasonable performance with clusters of traditional devices. We overcome these challenges with the Cerebras Wafer-Scale Cluster (<xref ref-type="bibr" rid="R20">Hall et al., 2021</xref>), where it is possible to use only simple data parallelism, and achieve linear weak scaling, even when LLMs are trained on very long sequences. We pre-trained two GenSLMs to convergence on full viral genomes with dense attention (<xref ref-type="table" rid="T1">Table 1</xref>) using a sequence length of 10,240 codon tokens on a single CS-2, and on a cluster with four CS-2s, achieving desired accuracy and perplexity results in less than a day. Beyond compute performance, the Cerebras Wafer-Scale Cluster provides high usability through the appliance workflow, where users no longer need to handcraft different parallelism choices for their given hardware and only need to specify the number of CS-2s to start data-parallel training. This flexibility allows faster experiment iterations without compromising performance. Training GenSLMs with multiple CS-2s is pioneering work with the Cerebras Wafer-Scale Cluster, which demonstrates the potential of dedicated AI hardware to apply LLMs on long-range context and work with genome sequences at scale.</p></sec></sec><sec id="S17"><label>5.3</label><title>Phylogenetic analyses of whole genomes</title><p id="P44">As described in Sec. 5.1.1, we used a set of 16,545 sequences from the Houston Methodist Hospital System that were filtered for high-quality in order to analyze GenSLM outputs. We selected a diverse subset by embedding these sequences, tessellating the embedding space using a Gaussian mixture model (<italic>N</italic> = 40), and then sampling each tessellation using a uniform distribution, resulting in a set of 1,000 sequences maximizing coverage of the embedding space.</p><p id="P45">The 1,000 sequence subset was aligned to the NC_045512.1 severe acute respiratory syndrome coronavirus 2 isolate Wuhan-Hu-1 complete genome sequence using Mafft v7.310 (<xref ref-type="bibr" rid="R65">Yamada et al., 2016</xref>). We then generated a Newick-format phylogenetic tree from the alignment using RAxML Next Generation (<xref ref-type="bibr" rid="R31">Kozlov et al., 2019</xref>), which offers significant speed improvements over RAxML (<xref ref-type="bibr" rid="R49">Stamatakis, 2014</xref>). We then generated a phylogenetic tree using RAxML-NG’s “search” algorithm, which searches for a maximum-likelihood tree amongst 10 parsimonious trees and 10 randomly generated trees. This takes ~9 hours on 5 CPUs (the recommended RAxML-NG parallelization settings for our data.) We used the most likely tree generated as a seed tree for running further analyses with UShER.</p><p id="P46">UShER (Ultrafast Sample placement on Existing tRee, (<xref ref-type="bibr" rid="R54">Turakhia et al., 2021</xref>)) is a SARS-CoV-2-specific analysis tool that can quickly place new SARS-CoV-2 genomes onto an existing SARS-CoV-2 phylogenetic tree on the basis of mutation tracking. In addition to to a “seed” phylogenetic tree, UShER requires a variant call format (VCF) file to track mutation data, which we generated from our multiple sequence alignment by using snp-sites ((Page et al., [n.d.])). UShER stores the mutation information along with the tree in Google’s protobuff format created from the VCF and tree files.</p><p id="P47">We then used this tree as the basis for quickly examining and labeling generated sequences of interest. Generated sequences are (1) converted to fasta format, (2) aligned to the NC_045512 reference genome sequence, (3) mutation profiled using snp-sites, (4) placed on the seed phylogenetic tree using UShER, (5) proposed a variant label on the basis of the labels of its K nearest tree neighbors (where K=20 in our analyses), and (6) flagged for further examination if the sequence has the longest phylogenetic distance to the NC_045512 reference genome amongst its X nearest neighbors.</p><p id="P48">We chose this flagging scheme to select for sequences that were more distant to the original strain than the other sequences in their close lineage, as these sequences represent more heavily mutated novel genomes that may be more likely to produce variants of interest or concern.</p></sec><sec id="S18"><label>5.4</label><title>Integrated visualization</title><p id="P49">When visualizing long-distance genomic relationships, a linear layout created edges crossing over entities and affected readability. We therefore developed a circular arrangement to visualize relationships between entities and positions. The visualization is flexible and supports a rich set of layers to encode various data properties. The outermost layer shows ORFs along the SARS-CoV-2 genome, while the next layer displays protein-coding locations. These interactive layers allow users to select an ORF or protein for closer examination. All other layers (except the innermost) display different properties of codons in the gene sequence. The layers encode codon properties and are presented with custom visualizations based on the property type; e.g., <xref ref-type="fig" rid="F2">Fig. 2C and 2D</xref>, where probabilities of codons are encoded using a radial bar chart (intensity of the color represents the probability). The innermost layer visualizes the GenSLM attention relationships between codons. To reduce visual clutter, we employed hierarchical edge bundling techniques.</p></sec><sec id="S19"><label>5.5</label><title>Workflow infrastructure</title><p id="P50">As illustrated in <xref ref-type="fig" rid="F5">Fig. 5</xref>, we implemented and scaled the reward-guided beam search procedure (see <xref ref-type="sec" rid="S14">Sec. 5.2.2</xref>), leveraging a workflow that couples (1) a GenSLM sequence generator, and (2) a Bayesian optimizer to tune the reward mixing hyperparameter <italic>μ</italic> to bias the generator towards a target property. On startup, an ensemble of GenSLM generators are submitted to perform an initial grid search over the <italic>μ</italic> ∈ (0,1) parameter space, providing sequences to update a Gaussian process surrogate model. This, in turn, suggests new <italic>μ</italic> values throughout the duration of the run. Parameters are chosen by random sampling of <italic>n</italic> points, and are scored by their negative expected improvement (the optimization is a minimization). Each generation task uses a single A100 GPU on Polaris to run an instance of the a 25M parameter GenSLM model, whereas the optimizer task uses the CPUs on a single node.</p><p id="P51">We extend the Colmena workflow toolkit by implementing an Application abstraction for each workflow task (component). The Application provides for (1) inter-process communication when tasks are externally executable programs, and (2) warm-able functions to avoid duplicate initialization. The Application abstraction enables us to isolate the many generator instances from the Bayesian optimizer such that a single Thinker, executed on the login node, orchestrates communication and task submission to drive the property optimization. Leveraging Colmena allows us to implement concisely a multithreaded Thinker where one thread is responsible for handling outputs from the sequence generators and immediately submitting a new generation request to maximize utilization of the workers. This thread then handles any potential task failures by checking the return status and allows the workflow to be robust to application-level failures due to uncaught exceptions and hardware failures. The successful results are placed onto a queue where another thread reads and batches the results, (<italic>μ</italic>, sequence) pairs, for submission to the Bayesian optimizer application.</p><p id="P52">To further improve utilization of the workflow, we augment the Thinker with a inference-only copy of the surrogate model which is periodically transferred via pickling from the Bayesian optimizer application.</p><p id="P53">Workflows expressed with Colmena contain three components: a Thinker, a task server, and one or many workers. The Thinker defines the policies of the workflow, i.e., the dynamic dispatching of tasks and consumption of results. The Thinker is composed of agents; agents interact with each other and the task server via shared data structures. The task server pulls task definitions (task name and input pairs) from a task queue and executes tasks on workers via Parsl. The task server communicates task results from workers back to agents via a results queue.</p><p id="P54">For large task inputs or results, Colmena provides integration with <xref ref-type="bibr" rid="R1">ProxyStore (pro, 2021)</xref>, a library for decoupling data movement from control flow. Task inputs or results that exceed a user-defined threshold are automatically communicated to the worker executing the task via more optimal means (e.g., file system or Redis server). This reduces overheads in the task server and workflow manager and enables lower latency task execution and higher throughput.</p></sec></sec><sec id="S20"><label>6</label><title>How performance was measured</title><p id="P55">We evaluate the performance of GenSLM models on a diverse set of systems. We first explore the performance on two leadership class GPU-based supercomputing systems: 1) Polaris supercomputer at the Argonne Leadership Computing Facility (Polaris@ALCF), and 2) Selene supercomputer at NVIDIA (Selene@NVIDIA). Next, we evaluate the performance on the Cerebras CS-2 wafer-scale cluster.</p><p id="P56">In the June 2022 Top-500 list (<xref ref-type="bibr" rid="R53">Top500, 2022</xref>), Polaris is ranked at #14 with a peak of 44 PFLOPS and Selene is at #8 with a 63.4 PFLOPS peak. <xref ref-type="table" rid="T2">Table 2</xref> compares the two systems used for evaluation. The Polaris system is an HPE Apollo Gen10+ system with 560 nodes interconnected with HPE Slingshot 10 using a Dragonfly topology. Each node consists of an AMD “Milan” processor with 32 cores with 512GB of system memory. Each node has four NVIDIA A100 GPUs—each with 40GB memory. Each node has two Slingshot-10 endpoints at 12.5 GB/s for the interconnect network. Selene is based on the NVIDIA DGX SuperPOD platform and consists of 560 nodes interconnected with Mellanox HDR fabric. Each node consists of two AMD “Rome” processors, each with 64 cores and 2TB system memory. Each node has eight NVIDIA A100 GPUs, each with 80GB memory. Each node has eight Mellanox ConnectX-6 HDR endpoints at 20 GB/s each for the interconnect network. Each A100 NVIDIA GPU is capable of achieving a peak of 19.5 TFLOPS in FP32, 156 TFLOPS in TF32, and 312 TFLOPS in FP16 and BF16.</p><p id="P57">GenSLM was written with the PyTorch Lightning API (<xref ref-type="bibr" rid="R40">Pytorch, 2022</xref>), using transformer models from the Hugging Face repository (<xref ref-type="bibr" rid="R25">huggingface, 2022</xref>). PyTorch Lightning allows the use of several distributed training strategies to scale model training on clusters and supercomputers. This includes <italic>DistributedDataParallel</italic> and <italic>DeepSpeed</italic> (<xref ref-type="bibr" rid="R45">Rasley et al., 2020</xref>). We use mixed precision using FP16 and FP32 for our training runs. We focused our efforts on DeepSpeed, as its employment of various ZeRO strategies for optimization reduces the overall memory utilization in model training, particularly for large parameter models (<xref ref-type="bibr" rid="R42">Rajbhandari et al., 2020</xref>). Briefly, ZeRO strategies partition memory for training models— including the optimizer, gradient, and model states—to use aggregate memory across all GPUs. This enables training larger models on GPU-based systems and trades overall memory capacity for additional re-computation and communication. In particular, ZeRO-1 partitions optimizers across GPUs, ZeRO-2 partitions both the optimizers and gradients across all GPUs, and ZeRO-3 partitions the parameters, in addition to ZeRO-2 optimizations, across all GPUs. Additionally, ZeRO-3 can scale model sizes by leveraging CPU memory and any node-local storage to offload optimizer states, gradients, parameters, and optionally activations to CPU. We used PyTorch 1.12.0 and used NVIDIA NCCL 2.10.3 as the backend for DeepSpeed. We used an environment with Docker containers for the runs on Selene, and a bare-metal build using Conda on Polaris.</p><p id="P58">To measure compute performance of GenSLM model training, we use the DeepSpeed flops profiler (<xref ref-type="bibr" rid="R12">Deepspeed, 2022</xref>). The DeepSpeed flops profiler provides the flops and latency of the forward and backward passes and latency of the weight updates, and thus the compute performance of the GenSLM models. For scaling studies, we measure the entire end-to-end time including I/O as well as model training at scale. We measure achieved throughput in samples per second as the number of GPUs scales on the system. We use the NVIDIA Nsight tool (<xref ref-type="bibr" rid="R8">Bradley, 2012</xref>) to get an in-depth performance analysis.</p><sec id="S21"><title>Cerebras Wafer-Scale Cluster</title><p id="P59">We also evaluated training performance on full viral genomic sequences on a Cerebras Wafer-Scale Cluster with four CS-2s (<xref ref-type="bibr" rid="R20">Hall et al., 2021</xref>). The Cerebras Wafer-Scale Cluster uses a weight streaming execution mode where weights are stored off-chip on MemoryX, a memory extension. Weights are streamed onto each CS-2 node using a broadcast/reduce fabric called SwarmX. Each CS-2 node is powered by the Wafer-Scale Engine, with 850,000 compute cores, 40 GB of on-chip memory, and 20 petabytes/s of memory bandwidth. After the computations, gradients are streamed back to MemoryX where weights are updated.</p><p id="P60">We used data-parallelism in the Cerebras Wafer-Scale Cluster through the appliance workflow, where no code changes or additional libraries were required to use either one or multiple CS-2 systems. GenSLM 123M and 1.3B were trained using the Cerebras reference implementation for GPT-2 model. This implementation is based on the TensorFlow estimator and is instrumented to collect accuracy, perplexity and throughput measurements. We worked with a Python virtual environment that included Cerebras software version 1.6. All training was done using mixed precision.</p></sec></sec><sec id="S22"><label>7</label><title>Performance results</title><p id="P61">We evaluated the performance of scaling GenSLM training on the Selene and Polaris systems. We used two target sequence lengths (2,048 and 10,240) in our scaling studies. <xref ref-type="fig" rid="F6">Fig. 6</xref> depicts performance, in terms of overall throughput measured in samples/sec, as we scaled with the number of GPUs on both systems. In our runs, we used one rank per GPU with DeepSpeed ZeRO-3 optimizations. As we scaled the number of GPUs, we kept the batch size per GPU constant and scaled the global batch size appropriately. We modified the learning rate parameter to account for scaling the number of ranks. On Selene, for sequence length 2048, we employed twice the batch size used on Polaris for the 25M, 250M, and 2.5B models, as the A100 GPUs on Selene have twice the memory capacity compared to the Polaris GPUs. The performance obtained is the average of the throughput measured over multiple iterations.</p><p id="P62">We observed that as the model size increases from 25M to 25B, the total achievable throughput, in terms of samples/sec, decreases. This is expected as increasing the model size increases the computational, memory, and communication requirements. For the 25M, 250M, and 2.5B models, we observe a nearly 2× improvement in throughput on Selene in comparison to Polaris, as a double batch size is employed. In terms of efficiency, for smaller models, such as 25M, we observed a drop in scaling efficiency as we scaled beyond 256 GPUs. Two key attributes contributing to this include the fact that for smaller model sizes that run with ZeRO-3, the ratio of data movement to computational flops is too high to completely overlap these. We see better performance efficiency for larger models as they have higher utilization of computation and are able to better overlap communication with computation. Some inefficiencies here are also due to the performance of collectives and we investigate this further next. In the case of the 25B model, we are able to fit just a single batch on the GPU and observe a 50% improvement in the throughput achieved on Selene over Polaris. We attribute this to the increased interconnect performance on Selene together with the larger memory capacity. We observe a super-linear speedup for the 25B case on both systems as we scale to 1024 GPUs in comparison to the performance at 8 GPUs. This is attributed to the increased memory and data movement overheads at smaller GPU scales.</p><p id="P63">To gain detailed insights on the runs, we performed a profiling study on the 25M parameter model on both Polaris and Selene systems using the NVIDIA Nsight tool (<xref ref-type="bibr" rid="R36">NVIDIA, 2022</xref>). To account for the difference in the number of GPUs in a single node on both systems, we performed profiling runs with the 32 GPUs on 8 nodes on Polaris and 4 on Selene separately. We observed no significant delay between the steps/iterations—data loading and I/O were not bottlenecks. Given that the Selene DGX node has 80 GB memory compared with 40 GB on the Polaris node, it allowed doubling the batch size for the 25M parameter model, thereby achieving higher throughput than Polaris.</p><p id="P64">In addition, we performed a study comparing the scaling behavior of the distributed training framework implementations for PyTorch DistributedDataParallel (DDP) and with DeepSpeed with ZeRO Stage 2 and 3 on Selene. With DDP, we were constrained to smaller model sizes as it currently does not employ any memory optimization, unlike DeepSpeed. As seen from <xref ref-type="fig" rid="F6">Fig. 6B</xref>, DDP-based runs exhibit linear behavior, while the performance of DeepSpeed runs saturated beyond 256 GPUs for the ZeRO-2 optimizer and 512 GPUs for the ZeRO-3 optimizations. For the 25M case, at 512 GPUs, DDP achieves 99% scaling efficiency with a 10% improvement over ZeRO-3 and a 2× improvement over ZeRO-2. This could be attributed to the fact that DDP implements AllReduce collective communication while DeepSpeed implements Reduce-Scatter and AllGather collective communication operations. The performance of the NCCL backend is highly optimized for AllReduce in comparison to AllGather. This highlights an opportunity to explore further optimizations for the DeepSpeed implementation to scale on systems. We would also like to note that there are additional tuning knobs at the NCCL layer and in DeepSpeed, and this needs further investigation for optimal performance.</p><p id="P65">For sequence length 10,240, we used a batch size of 1 and ZeRO-3. As we increased the sequence length from 2,048 to 10,240, the memory requirements, including for activation and residuals, increased by a similar factor. The computation requirements also grew by 5×. We were able to fit only one batch for this sequence length on the GPU with the current stages employed. From <xref ref-type="fig" rid="F6">Fig. 6C</xref>, at 512 GPUs, for the 25M case, we observed a 50% improvement on Selene (64 nodes) over Polaris (128 nodes). For the 250M case, we observed only an 11% improvement for Selene over Polaris. As the model size increased for this sequence length, we were bottlenecked primarily by the memory subsystem performance and the overheads associated with staging residuals and parameters between the GPU and host. Additional staging optimization, model and activation partitioning will need to be explored.</p><sec id="S23"><title>Compute Performance</title><p id="P66">We discuss the overall compute performance of the GenSLM model as we scaled the model size from 25M to 25B on the Polaris system for our production science runs. <xref ref-type="table" rid="T3">Table 3</xref> illustrates the measured GPU performance obtained using the DeepSpeed profiler for smaller-scale runs. We next take the efficiency of the runs as we weak-scaled to larger nodes and GPU counts for the sustained PFLOPS. We would like to note that we account for the entire end-to-end application run, including data processing and checkpointing. The number of GPUs for our production science runs on Polaris was chosen based on system availability, and the number of steps run was chosen to achieve an appropriate loss scale. We observed that as we scaled the model size, the overall computational flops per step increased given the increase in model complexity. For the 25B model, we achieved a sustained performance of 44.79 PFLOPS in mixed precision (MP). We achieved a peak performance of 212.55 PFLOPS(MP) measured by accounting for the highest FLOPS consumed by a single layer in our network. For our production science runs, we used an aggregate of <bold>1.63 Zettaflops</bold>, and our 25B model used 1.48 Zettaflops to train on 1,024 GPUs for 2200 steps. For our scaling runs on Selene with the 25B model, we scale to 512 nodes with 4096 GPUs. We achieve a sustained performance of 121.26 PFLOPS(MP) and a peak performance of 850.21 PFLOPS(MP). The final model performance is described in <xref ref-type="table" rid="T4">Table 4</xref>.</p></sec><sec id="S24"><title>Cerebras Wafer-Scale Cluster Scalability</title><p id="P67">We measured the throughput and training time of the Wafer-Scale Cluster for GenSLM-123M and GenSLM-1.3B with a sequence length of 10,240 codon tokens (<xref ref-type="table" rid="T1">Table 1</xref>). The batch size per CS-2 for each model was chosen based on empirical experiences with models of similar sizes and was kept constant when scaled up to multiple CS-2s.</p><p id="P68"><xref ref-type="table" rid="T5">Table 5</xref> shows average samples per second training GenSLM-123M and GenSLM-1.3B for 200 steps using one, two, and four CS-2s. Regardless of the model configurations, we observed linear weak scaling when using up to four CS-2s.</p><p id="P69">We trained GenSLM-123M and GenSLM-1.3B from scratch using learned positional embeddings. <xref ref-type="table" rid="T6">Table 6</xref> shows training time and a total number of training samples used to achieve validation accuracy &gt;96% and perplexity &lt;1.03 using one CS-2 and with four CS-2s. Validation measurements were taken from checkpoints every 500 steps. For GenSLMs of the same size, fewer training steps were required to achieve comparable validation results when the global batch size was increased in a four-CS-2 Wafer-Scale Cluster with data parallelism. The reduced number of training steps plus linear weak scaling led to a reduction of at least a third of the training time when using four CS-2s versus one. All GenSLM training with full genomes on CS-2s converged within 12 hours. GenSLM-1.3B requires fewer training samples than the smaller GenSLM-123M to achieve comparable validation metrics, following the sample efficiency observation in neural language model scaling laws (<xref ref-type="bibr" rid="R29">Kaplan et al., 2020</xref>). We note that further hyperparameter tuning is required to 1) optimize the throughput on the Wafer-Scale Cluster, 2) draw firmer conclusions on the impact of model size on model quality.</p></sec><sec id="S25"><title>Workflow Performance</title><p id="P70">We measured the utilization of the sequence generation workflow on 224 nodes of Polaris by counting the number of workers actively serving a request as a function of runtime. As shown in <xref ref-type="fig" rid="F7">Fig. 7</xref>, we achieve 97.0% utilization over the 5.5-hour duration of the workflow. Persisting the GenSLMs in GPU memory between requests generated 3.85 sequences per second, whereas without model caching we estimate the workflow would have only generated 1.98 sequences per second by extrapolating the mean cold start time across the number of workers. This achieves 1.9× faster time to solution for generating synthetic sequences with notable properties, allowing for rapid analysis at time scales not previously feasible.</p></sec></sec><sec id="S26"><label>8</label><title>Implications</title><p id="P71">In this paper, we presented GenSLMs, one of the first LLMs trained on nucleotide sequences, particularly at the genome scale, and demonstrated its performance in modeling evolutionary dynamics of SARS-CoV-2. Our approach overcomes key challenges related to training LLMs for biological data, specifically with respect to longer sequence lengths and building biologically meaningful latent spaces which can then be used for a variety of downstream prediction tasks. GenSLM is a foundation model for biological sequence data and opens up avenues for building hierarchical AI models for several biological applications, including protein annotation workflows, metagenome reconstruction, protein engineering, and biological pathway design. We scaled the training of GenSLM for sequence length up to 10240 tokens and 25B parameters on GPU-based supercomputers. We scaled to 4,096 GPUs and utilized over 1.63 Zettaflops for science runs. We identified scaling avenues to be pursued in order to tackle larger models and sequence lengths needs for science. We demonstrated the efficacy of the Cerebras Wafer-Scale Cluster, an AI accelerator, to scale the training of GenSLM with high user-productivity and achieved linear scaling for 10,240 tokens and models up to 1.3B parameters.</p><p id="P72">We also note that the information contained within nucleotide sequences represents a much richer vocabulary compared to PLMs alone. Thus, the learned representation lets us capture a much larger repertoire of biological properties that are perhaps diminished while using PLMs, and enables a more faithful generation process that captures the intrinsic organization of the SARS-CoV-2 sequences. Further, the attention mechanism also reveals co-evolutionary patterns at the whole-genome scale that requires future investigation to fully understand how these long-range interactions may influence our ability to inform epitope modeling, immune escape, antibody design, and even vaccine design strategies. We however note that there is a need to rigorously compare PLMs with GenSLM-like approaches. It remains to be seen if the GenSLM model does possess richer representative power and if so how it can be further used. Note that we have also not been able to address the aspects of noise and bias in the data – similar to natural language models where the models demonstrated extreme bias, there needs to be rigorous analyses of GenSLMs generative capabilities. We welcome the community to drive the development of suitable test harnesses for rigorously evaluating GenSLM-like models.</p><p id="P73">A straightforward extension to our work would include the integration of GenSLMs with protein structure prediction workflows such as AlphaFold (<xref ref-type="bibr" rid="R28">Jumper et al., 2021</xref>)/OpenFold<sup><xref ref-type="fn" rid="FN6">4</xref></sup> and faster protein folding methods (<xref ref-type="bibr" rid="R32">Lin et al., 2022</xref>) to model both immune escape and fitness, which determine the ability of the virus to adapt to its host (human) (<xref ref-type="bibr" rid="R7">Beguir et al., 2022</xref>). Further, incorporating experimental and biophysical data into our workflow from antibody binding assays, molecular docking, and other quantitative metrics can also guide the training regimes for these models such that the generative process can be constrained to focus on potential future variants of concern.</p></sec></body><back><ack id="S27"><title>Acknowledgments</title><p>We thank the Argonne Leadership Computing Facility (ALCF) supported by the DOE under DE-AC02-06CH11357 and the National Energy Research Scientific Computing Center (NERSC) at Lawrence Berkeley National Laboratory supported by the DOE under Contract No. DE-AC02-05CH11231. We thank Bill Allcock, Silvio Rizzi and ALCF, Wahid Bhimji and NERSC for their timely help in enabling us to run these jobs at scale. We also thank Defne Gorgun, Lorenzo Casalino and Rommie Amaro for stimulating discussions. This research was supported by the Exascale Computing Project (17-SC-20-SC), a collaborative effort of the US DOE Office of Science and the National Nuclear Security Administration, the National Institute of Allergy and Infectious Diseases, National Institutes of Health Award Number P01AI165077 (AR), the National Science Foundation Award Number 2117896 and supported by the DOE through the National Virtual Biotechnology Laboratory, a consortium of DOE national laboratories focused on response to COVID-19, with funding from the Coronavirus CARES Act.</p></ack><fn-group><fn id="FN3"><label>1</label><p id="P74"><ext-link ext-link-type="uri" xlink:href="https://www.gisaid.org/">https://www.gisaid.org</ext-link></p></fn><fn id="FN4"><label>2</label><p id="P75"><ext-link ext-link-type="uri" xlink:href="https://github.com/ramanathanlab/genslm">https://github.com/ramanathanlab/genslm</ext-link></p></fn><fn id="FN5"><label>3</label><p id="P76"><ext-link ext-link-type="uri" xlink:href="https://www.bv-brc.org/">https://www.bv-brc.org</ext-link></p></fn><fn id="FN6"><label>4</label><p id="P77"><ext-link ext-link-type="uri" xlink:href="http://openfold.io/">http://openfold.io</ext-link></p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="web"><collab>ProxyStore</collab><year>2021</year><comment><ext-link ext-link-type="uri" xlink:href="https://github.com/proxystore/proxystore">https://github.com/proxystore/proxystore</ext-link></comment></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avsec</surname><given-names>Žiga</given-names></name><name><surname>Agarwal</surname><given-names>Vikram</given-names></name><name><surname>Visentin</surname><given-names>Daniel</given-names></name><name><surname>Ledsam</surname><given-names>Joseph R</given-names></name><name><surname>Grabska-Barwinska</surname><given-names>Agnieszka</given-names></name><name><surname>Taylor</surname><given-names>Kyle R</given-names></name><name><surname>Assael</surname><given-names>Yannis</given-names></name><name><surname>Jumper</surname><given-names>John</given-names></name><name><surname>Kohli</surname><given-names>Pushmeet</given-names></name><name><surname>Kelley</surname><given-names>David R</given-names></name></person-group><article-title>Effective gene expression prediction from sequence by integrating long-range interactions</article-title><source>Nature methods</source><year>2021</year><volume>18</volume><issue>10</issue><fpage>1196</fpage><lpage>1203</lpage><pub-id pub-id-type="pmcid">PMC8490152</pub-id><pub-id pub-id-type="pmid">34608324</pub-id><pub-id pub-id-type="doi">10.1038/s41592-021-01252-x</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Babuji</surname><given-names>Yadu</given-names></name><name><surname>Woodard</surname><given-names>Anna</given-names></name><name><surname>Li</surname><given-names>Zhuozhao</given-names></name><name><surname>Clifford</surname><given-names>Ben</given-names></name><name><surname>Kumar</surname><given-names>Rohan</given-names></name><name><surname>Lacinski</surname><given-names>Lukasz</given-names></name><name><surname>Chard</surname><given-names>Ryan</given-names></name><name><surname>Wozniak</surname><given-names>Justin</given-names></name><name><surname>Foster</surname><given-names>Ian</given-names></name><name><surname>Wilde</surname><given-names>Michael</given-names></name><name><surname>Katz</surname><given-names>Daniel</given-names></name><etal/></person-group><source>Parsl: Pervasive Parallel Programming in Python</source><conf-name>ACM International Symposium on High-Performance Parallel and Distributed Computing</conf-name><year>2019</year></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>Jordan J</given-names></name><name><surname>Mathy</surname><given-names>Christopher JP</given-names></name><name><surname>Schaletzky</surname><given-names>Julia</given-names></name></person-group><article-title>A proposed workflow for proactive virus surveillance and prediction of variants for vaccine design</article-title><source>PLOS Computational Biology</source><year>2021</year><month>12</month><volume>17</volume><issue>12</issue><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="pmcid">PMC8675697</pub-id><pub-id pub-id-type="pmid">34914686</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1009624</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Balaprakash</surname><given-names>Prasanna</given-names></name><name><surname>Salim</surname><given-names>Michael</given-names></name><name><surname>Uram</surname><given-names>Thomas D</given-names></name><name><surname>Vishwanath</surname><given-names>Venkat</given-names></name><name><surname>Wild</surname><given-names>Stefan M</given-names></name></person-group><source>DeepHyper: Asynchronous Hyperparameter Search for Deep Neural Networks</source><conf-name>25th International Conference on High Performance Computing</conf-name><conf-sponsor>IEEE</conf-sponsor><year>2018</year><pub-id pub-id-type="doi">10.1109/hipc.2018.00014</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balasubramanian</surname><given-names>Vivek</given-names></name><name><surname>Jha</surname><given-names>Shantenu</given-names></name><name><surname>Merzky</surname><given-names>Andre</given-names></name><name><surname>Turilli</surname><given-names>Matteo</given-names></name></person-group><article-title>RADICAL-Cybertools: Middleware Building Blocks for Scalable Science</article-title><source>arXiv</source><year>2019</year><elocation-id>arXiv:1904.03085</elocation-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beguir</surname><given-names>Karim</given-names></name><name><surname>Skwark</surname><given-names>Marcin J</given-names></name><name><surname>Fu</surname><given-names>Yunguan</given-names></name><name><surname>Pierrot</surname><given-names>Thomas</given-names></name><name><surname>Carranza</surname><given-names>Nicolas Lopez</given-names></name><name><surname>Laterre</surname><given-names>Alexandre</given-names></name><name><surname>Kadri</surname><given-names>Ibtissem</given-names></name><name><surname>Korched</surname><given-names>Abir</given-names></name><name><surname>Lowegard</surname><given-names>Anna U</given-names></name><name><surname>Lui</surname><given-names>Bonny Gaby</given-names></name><name><surname>Sänger</surname><given-names>Bianca</given-names></name><etal/></person-group><article-title>Early Computational Detection of Potential High Risk SARS-CoV-2 Variants</article-title><source>bioRxiv</source><year>2022</year><comment>arXiv:<ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/early/2022/09/20/2021.12.24.474095.full.pdf">https://www.biorxiv.org/content/early/2022/09/20/2021.12.24.474095.full.pdf</ext-link></comment><pub-id pub-id-type="doi">10.1101/2021.12.24.474095</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bradley</surname><given-names>Thomas</given-names></name></person-group><source>GPU performance analysis and optimisation</source><publisher-name>NVIDIA Corporation</publisher-name><year>2012</year></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brouwer</surname><given-names>Philip JM</given-names></name><name><surname>Caniels</surname><given-names>Tom G</given-names></name><name><surname>van der Straten</surname><given-names>Karlijn</given-names></name><name><surname>Snitselaar</surname><given-names>Jonne L</given-names></name><name><surname>Aldon</surname><given-names>Yoann</given-names></name><name><surname>Bangaru</surname><given-names>Sandhya</given-names></name><name><surname>Torres</surname><given-names>Jonathan L</given-names></name><name><surname>Okba</surname><given-names>Nisreen MA</given-names></name><name><surname>Claireaux</surname><given-names>Mathieu</given-names></name><name><surname>Kerster</surname><given-names>Gius</given-names></name><name><surname>Bentlage</surname><given-names>Arthur EH</given-names></name><etal/></person-group><article-title>Potent neutralizing antibodies from COVID-19 patients define multiple targets of vulnerability</article-title><source>Science</source><year>2020</year><volume>369</volume><issue>6504</issue><fpage>643</fpage><lpage>650</lpage><comment>arXiv:<ext-link ext-link-type="uri" xlink:href="https://www.science.org/doi/pdf/10.1126/science.abc5902">https://www.science.org/doi/pdf/10.1126/science.abc5902</ext-link></comment><pub-id pub-id-type="pmcid">PMC7299281</pub-id><pub-id pub-id-type="pmid">32540902</pub-id><pub-id pub-id-type="doi">10.1126/science.abc5902</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cosar</surname><given-names>Begum</given-names></name><name><surname>Karagulleoglu</surname><given-names>Zeynep Yagmur</given-names></name><name><surname>Unal</surname><given-names>Sinan</given-names></name><name><surname>Ince</surname><given-names>Ahmet Turan</given-names></name><name><surname>Uncuoglu</surname><given-names>Dilruba Beyza</given-names></name><name><surname>Tuncer</surname><given-names>Gizem</given-names></name><name><surname>Kilinc</surname><given-names>Bugrahan Regaip</given-names></name><name><surname>Ozkan</surname><given-names>Yunus Emre</given-names></name><name><surname>Ozkoc</surname><given-names>Hikmet Ceyda</given-names></name><name><surname>Demir</surname><given-names>Ibrahim Naki</given-names></name><name><surname>Eker</surname><given-names>Ali</given-names></name><etal/></person-group><article-title>SARS-CoV-2 Mutations and their Viral Variants</article-title><source>Cytokine Growth Factor Rev</source><year>2022</year><month>Feb</month><volume>63</volume><fpage>10</fpage><lpage>22</lpage><pub-id pub-id-type="pmcid">PMC8252702</pub-id><pub-id pub-id-type="pmid">34580015</pub-id><pub-id pub-id-type="doi">10.1016/j.cytogfr.2021.06.001</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davis</surname><given-names>James J</given-names></name><name><surname>Gerdes</surname><given-names>Svetlana</given-names></name><name><surname>Olsen</surname><given-names>Gary J</given-names></name><name><surname>Olson</surname><given-names>Robert</given-names></name><name><surname>Pusch</surname><given-names>Gordon D</given-names></name><name><surname>Shukla</surname><given-names>Maulik</given-names></name><name><surname>Vonstein</surname><given-names>Veronika</given-names></name><name><surname>Wattam</surname><given-names>Alice R</given-names></name><name><surname>Yoo</surname><given-names>Hyunseung</given-names></name></person-group><article-title>PATtyFams: Protein Families for the Microbial Genomes in the PATRIC Database</article-title><source>Front Microbiol</source><year>2016</year><volume>7</volume><fpage>118</fpage><pub-id pub-id-type="pmcid">PMC4744870</pub-id><pub-id pub-id-type="pmid">26903996</pub-id><pub-id pub-id-type="doi">10.3389/fmicb.2016.00118</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="web"><collab>Deepspeed</collab><source>Flops Profiler - Deepspeed</source><year>2022</year><comment><ext-link ext-link-type="uri" xlink:href="https://www.deepspeed.ai/tutorials/flops-profiler/">https://www.deepspeed.ai/tutorials/flops-profiler/</ext-link></comment></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devlin</surname><given-names>Jacob</given-names></name><name><surname>Chang</surname><given-names>Ming-Wei</given-names></name><name><surname>Lee</surname><given-names>Kenton</given-names></name><name><surname>Toutanova</surname><given-names>Kristina</given-names></name></person-group><article-title>Bert: Pretraining of deep bidirectional transformers for language understanding</article-title><source>arXiv preprint</source><year>2018</year><elocation-id>arXiv:1810.04805</elocation-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doud</surname><given-names>Michael B</given-names></name><name><surname>Lee</surname><given-names>Juhye M</given-names></name><name><surname>Bloom</surname><given-names>Jesse D</given-names></name></person-group><article-title>How single mutations affect viral escape from broad and narrow antibodies to H1 influenza hemagglutinin</article-title><source>Nature Communications</source><year>2018</year><volume>9</volume><issue>1</issue><fpage>1386</fpage><pub-id pub-id-type="pmcid">PMC5895760</pub-id><pub-id pub-id-type="pmid">29643370</pub-id><pub-id pub-id-type="doi">10.1038/s41467-018-03665-3</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dunn</surname><given-names>Alexander</given-names></name><name><surname>Brenneck</surname><given-names>Julien</given-names></name><name><surname>Jain</surname><given-names>Anubhav</given-names></name></person-group><article-title>Rocketsled: A software library for optimizing high-throughput computational searches</article-title><source>Journal of Physics: Materials</source><year>2019</year><month>April</month><volume>2</volume><issue>3</issue><elocation-id>034002</elocation-id><pub-id pub-id-type="doi">10.1088/2515-7639/ab0c3d</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elnaggar</surname><given-names>Ahmed</given-names></name><name><surname>Heinzinger</surname><given-names>Michael</given-names></name><name><surname>Dallago</surname><given-names>Christian</given-names></name><name><surname>Rehawi</surname><given-names>Ghalia</given-names></name><name><surname>Wang</surname><given-names>Yu</given-names></name><name><surname>Jones</surname><given-names>Llion</given-names></name><name><surname>Gibbs</surname><given-names>Tom</given-names></name><name><surname>Feher</surname><given-names>Tamas</given-names></name><name><surname>Angerer</surname><given-names>Christoph</given-names></name><name><surname>Steinegger</surname><given-names>Martin</given-names></name><name><surname>Bhowmik</surname><given-names>Debsindhu</given-names></name><etal/></person-group><article-title>ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><year>2022</year><volume>44</volume><issue>10</issue><fpage>7112</fpage><lpage>7127</lpage><pub-id pub-id-type="pmid">34232869</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferruz</surname><given-names>Noelia</given-names></name><name><surname>Heinzinger</surname><given-names>Michael</given-names></name><name><surname>Akdel</surname><given-names>Mehmet</given-names></name><name><surname>Goncearenco</surname><given-names>Alexander</given-names></name><name><surname>Naef</surname><given-names>Luca</given-names></name><name><surname>Dallago</surname><given-names>Christian</given-names></name></person-group><article-title>From sequence to function through structure: Deep learning for protein design</article-title><source>bioRxiv</source><year>2022</year><pub-id pub-id-type="doi">10.1101/2022.08.31.505981</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greaney</surname><given-names>Allison J</given-names></name><name><surname>Starr</surname><given-names>Tyler N</given-names></name><name><surname>Gilchuk</surname><given-names>Pavlo</given-names></name><name><surname>Zost</surname><given-names>Seth J</given-names></name><name><surname>Binshtein</surname><given-names>Elad</given-names></name><name><surname>Loes</surname><given-names>Andrea N</given-names></name><name><surname>Hilton</surname><given-names>Sarah K</given-names></name><name><surname>Huddleston</surname><given-names>John</given-names></name><name><surname>Eguia</surname><given-names>Rachel</given-names></name><name><surname>Crawford</surname><given-names>Katharine HD</given-names></name><name><surname>Dingens</surname><given-names>Adam S</given-names></name><etal/></person-group><article-title>Complete Mapping of Mutations to the SARS-CoV-2 Spike Receptor-Binding Domain that Escape Antibody Recognition</article-title><source>Cell Host &amp; Microbe</source><year>2021</year><volume>29</volume><issue>1</issue><fpage>44</fpage><lpage>57</lpage><elocation-id>e9</elocation-id><pub-id pub-id-type="pmcid">PMC7676316</pub-id><pub-id pub-id-type="pmid">33259788</pub-id><pub-id pub-id-type="doi">10.1016/j.chom.2020.11.007</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gulati</surname><given-names>Anmol</given-names></name><name><surname>Qin</surname><given-names>James</given-names></name><name><surname>Chiu</surname><given-names>Chung-Cheng</given-names></name><name><surname>Parmar</surname><given-names>Niki</given-names></name><name><surname>Zhang</surname><given-names>Yu</given-names></name><name><surname>Yu</surname><given-names>Jiahui</given-names></name><name><surname>Han</surname><given-names>Wei</given-names></name><name><surname>Wang</surname><given-names>Shibo</given-names></name><name><surname>Zhang</surname><given-names>Zhengdong</given-names></name><name><surname>Wu</surname><given-names>Yonghui</given-names></name><name><surname>Pang</surname><given-names>Ruoming</given-names></name><etal/></person-group><article-title>Conformer: Convolution-augmented Transformer for Speech Recognition</article-title><year>2020</year><pub-id pub-id-type="doi">10.48550/ARXIV.2005.08100</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Hall</surname><given-names>Stewart</given-names></name><name><surname>Schreiber</surname><given-names>Rob</given-names></name><name><surname>Lie</surname><given-names>Sean</given-names></name></person-group><source>Training Giant Neural Networks Using Weight Streaming on Cerebras Wafer-Scale Systems</source><year>2021</year><comment><ext-link ext-link-type="uri" xlink:href="https://f.hubspotusercontent30.net/hubfs/8968533/Virtual%20Booth%20Docs/CS%20Weight%20Streaming%20White%20Paper%20111521.pdf">https://f.hubspotusercontent30.net/hubfs/8968533/Virtual%20Booth%20Docs/CS%20Weight%20Streaming%20White%20Paper%20111521.pdf</ext-link></comment></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname><given-names>Kai</given-names></name><name><surname>Wang</surname><given-names>Yunhe</given-names></name><name><surname>Chen</surname><given-names>Hanting</given-names></name><name><surname>Chen</surname><given-names>Xinghao</given-names></name><name><surname>Guo</surname><given-names>Jianyuan</given-names></name><name><surname>Liu</surname><given-names>Zhenhua</given-names></name><name><surname>Tang</surname><given-names>Yehui</given-names></name><name><surname>Xiao</surname><given-names>An</given-names></name><name><surname>Xu</surname><given-names>Chunjing</given-names></name><name><surname>Xu</surname><given-names>Yixing</given-names></name><name><surname>Yang</surname><given-names>Zhaohui</given-names></name><etal/></person-group><article-title>A Survey on Vision Transformer</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><year>2022</year><fpage>1</fpage><pub-id pub-id-type="pmid">35180075</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hie</surname><given-names>Brian</given-names></name><name><surname>Zhong</surname><given-names>Ellen D</given-names></name><name><surname>Berger</surname><given-names>Bonnie</given-names></name><name><surname>Bryson</surname><given-names>Bryan</given-names></name></person-group><article-title>Learning the language of viral evolution and escape</article-title><source>Science</source><year>2021</year><volume>371</volume><issue>6526</issue><fpage>284</fpage><lpage>288</lpage><comment>arXiv:<ext-link ext-link-type="uri" xlink:href="https://www.science.org/doi/pdf/10.1126/science.abd7331">https://www.science.org/doi/pdf/10.1126/science.abd7331</ext-link></comment><pub-id pub-id-type="pmid">33446556</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ho</surname><given-names>Jonathan</given-names></name><name><surname>Jain</surname><given-names>Ajay</given-names></name><name><surname>Abbeel</surname><given-names>Pieter</given-names></name></person-group><article-title>Denoising diffusion probabilistic models</article-title><source>Advances in Neural Information Processing Systems</source><year>2020</year><volume>33</volume><fpage>6840</fpage><lpage>6851</lpage></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hudson</surname><given-names>Stephen</given-names></name><name><surname>Larson</surname><given-names>Jeffrey</given-names></name><name><surname>Navarro</surname><given-names>John-Luke</given-names></name><name><surname>Wild</surname><given-names>Stefan</given-names></name></person-group><article-title>libEnsemble: A Library to Coordinate the Concurrent Evaluation of Dynamic Ensembles of Calculations</article-title><source>IEEE Transactions on Parallel and Distributed Systems</source><year>2022</year><volume>33</volume><issue>4</issue><fpage>977</fpage><lpage>988</lpage><pub-id pub-id-type="doi">10.1109/tpds.2021.3082815</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="web"><collab>huggingface</collab><source>Transformers: State-of-the-art Machine Learning for Pytorch, Tensor-Flow, and JAX</source><year>2022</year><comment><ext-link ext-link-type="uri" xlink:href="https://github.com/huggingface/transformers">https://github.com/huggingface/transformers</ext-link></comment></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joshi</surname><given-names>Mandar</given-names></name><name><surname>Chen</surname><given-names>Danqi</given-names></name><name><surname>Liu</surname><given-names>Yinhan</given-names></name><name><surname>Weld</surname><given-names>Daniel S</given-names></name><name><surname>Zettlemoyer</surname><given-names>Luke</given-names></name><name><surname>Levy</surname><given-names>Omer</given-names></name></person-group><article-title>Spanbert: Improving pre-training by representing and predicting spans</article-title><source>Transactions of the Association for Computational Linguistics</source><year>2020</year><volume>8</volume><fpage>64</fpage><lpage>77</lpage></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ju</surname><given-names>Bin</given-names></name><name><surname>Zhang</surname><given-names>Qi</given-names></name><name><surname>Ge</surname><given-names>Jiwan</given-names></name><name><surname>Wang</surname><given-names>Ruoke</given-names></name><name><surname>Sun</surname><given-names>Jing</given-names></name><name><surname>Ge</surname><given-names>Xiangyang</given-names></name><name><surname>Yu</surname><given-names>Jiazhen</given-names></name><name><surname>Shan</surname><given-names>Sisi</given-names></name><name><surname>Zhou</surname><given-names>Bing</given-names></name><name><surname>Song</surname><given-names>Shuo</given-names></name><name><surname>Tang</surname><given-names>Xian</given-names></name><etal/></person-group><article-title>Human neutralizing antibodies elicited by SARS-CoV-2 infection</article-title><source>Nature</source><year>2020</year><volume>584</volume><issue>7819</issue><fpage>115</fpage><lpage>119</lpage><pub-id pub-id-type="pmid">32454513</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jumper</surname><given-names>John</given-names></name><name><surname>Evans</surname><given-names>Richard</given-names></name><name><surname>Pritzel</surname><given-names>Alexander</given-names></name><name><surname>Green</surname><given-names>Tim</given-names></name><name><surname>Figurnov</surname><given-names>Michael</given-names></name><name><surname>Ronneberger</surname><given-names>Olaf</given-names></name><name><surname>Tunyasuvunakool</surname><given-names>Kathryn</given-names></name><name><surname>Bates</surname><given-names>Russ</given-names></name><name><surname>Žídek</surname><given-names>Augustin</given-names></name><name><surname>Potapenko</surname><given-names>Anna</given-names></name><name><surname>Bridgland</surname><given-names>Alex</given-names></name><etal/></person-group><article-title>Highly accurate protein structure prediction with AlphaFold</article-title><source>Nature</source><year>2021</year><volume>596</volume><issue>7873</issue><fpage>583</fpage><lpage>589</lpage><pub-id pub-id-type="pmcid">PMC8371605</pub-id><pub-id pub-id-type="pmid">34265844</pub-id><pub-id pub-id-type="doi">10.1038/s41586-021-03819-2</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaplan</surname><given-names>Jared</given-names></name><name><surname>McCandlish</surname><given-names>Sam</given-names></name><name><surname>Henighan</surname><given-names>Tom</given-names></name><name><surname>Brown</surname><given-names>Tom B</given-names></name><name><surname>Chess</surname><given-names>Benjamin</given-names></name><name><surname>Child</surname><given-names>Rewon</given-names></name><name><surname>Gray</surname><given-names>Scott</given-names></name><name><surname>Radford</surname><given-names>Alec</given-names></name><name><surname>Wu</surname><given-names>Jeffrey</given-names></name><name><surname>Amodei</surname><given-names>Dario</given-names></name></person-group><article-title>Scaling laws for neural language models</article-title><year>2020</year><pub-id pub-id-type="doi">10.48550/ARXIV.2001.08361</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Devlin</surname><given-names>Jacob</given-names></name><name><surname>Chang</surname><given-names>Ming-Wei</given-names></name><name><surname>Kenton</surname><given-names>Lee</given-names></name><name><surname>Toutanova</surname><given-names>Kristina</given-names></name></person-group><source>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</source><conf-name>Proceedings of NAACL-HLT</conf-name><year>2019</year><fpage>4171</fpage><lpage>4186</lpage></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kozlov</surname><given-names>Alexey M</given-names></name><name><surname>Darriba</surname><given-names>Diego</given-names></name><name><surname>Flouri</surname><given-names>Tomáš</given-names></name><name><surname>Morel</surname><given-names>Benoit</given-names></name><name><surname>Stamatakis</surname><given-names>Alexandros</given-names></name></person-group><article-title>RAxML-NG: A fast, scalable and user-friendly tool for maximum likelihood phylogenetic inference</article-title><source>Bioinformatics</source><year>2019</year><month>Nov</month><volume>35</volume><issue>21</issue><fpage>4453</fpage><lpage>4455</lpage><pub-id pub-id-type="pmcid">PMC6821337</pub-id><pub-id pub-id-type="pmid">31070718</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btz305</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>Zeming</given-names></name><name><surname>Akin</surname><given-names>Halil</given-names></name><name><surname>Rao</surname><given-names>Roshan</given-names></name><name><surname>Hie</surname><given-names>Brian</given-names></name><name><surname>Zhu</surname><given-names>Zhongkai</given-names></name><name><surname>Lu</surname><given-names>Wenting</given-names></name><name><surname>dos Santos Costa</surname><given-names>Allan</given-names></name><name><surname>Fazel-Zarandi</surname><given-names>Maryam</given-names></name><name><surname>Sercu</surname><given-names>Tom</given-names></name><name><surname>Candido</surname><given-names>Sal</given-names></name><name><surname>Rives</surname><given-names>Alexander</given-names></name><etal/></person-group><article-title>Language models of protein sequences at the scale of evolution enable accurate structure prediction</article-title><source>bioRxiv</source><year>2022</year><comment>arXiv:<ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/early/2022/07/21/2022.07.20.500902.full.pdf">https://www.biorxiv.org/content/early/2022/07/21/2022.07.20.500902.full.pdf</ext-link></comment><pub-id pub-id-type="doi">10.1101/2022.07.20.500902</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maher</surname><given-names>M Cyrus</given-names></name><name><surname>Bartha</surname><given-names>Istvan</given-names></name><name><surname>Weaver</surname><given-names>Steven</given-names></name><name><surname>di Iulio</surname><given-names>Julia</given-names></name><name><surname>Ferri</surname><given-names>Elena</given-names></name><name><surname>Soriaga</surname><given-names>Leah</given-names></name><name><surname>Lempp</surname><given-names>Florian A</given-names></name><name><surname>Hie</surname><given-names>Brian L</given-names></name><name><surname>Bryson</surname><given-names>Bryan</given-names></name><name><surname>Berger</surname><given-names>Bonnie</given-names></name><name><surname>Robertson</surname><given-names>David L</given-names></name><etal/></person-group><article-title>Predicting the mutational drivers of future SARS-CoV-2 variants of concern</article-title><source>Science Translational Medicine</source><year>2022</year><volume>14</volume><issue>633</issue><elocation-id>eabk3445</elocation-id><comment>arXiv:<ext-link ext-link-type="uri" xlink:href="https://www.science.org/doi/pdf/10.1126/scitranslmed.abk3445">https://www.science.org/doi/pdf/10.1126/scitranslmed.abk3445</ext-link></comment><pub-id pub-id-type="pmcid">PMC8939770</pub-id><pub-id pub-id-type="pmid">35014856</pub-id><pub-id pub-id-type="doi">10.1126/scitranslmed.abk3445</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Moritz</surname><given-names>Philipp</given-names></name><name><surname>Nishihara</surname><given-names>Robert</given-names></name><name><surname>Wang</surname><given-names>Stephanie</given-names></name><name><surname>Tumanov</surname><given-names>Alexey</given-names></name><name><surname>Liaw</surname><given-names>Richard</given-names></name><name><surname>Liang</surname><given-names>Eric</given-names></name><name><surname>Elibol</surname><given-names>Melih</given-names></name><name><surname>Yang</surname><given-names>Zongheng</given-names></name><name><surname>Paul</surname><given-names>William</given-names></name><name><surname>Jordan</surname><given-names>Michael I</given-names></name><name><surname>Stoica</surname><given-names>Ion</given-names></name><etal/></person-group><source>Ray: A Distributed Framework for Emerging AI Applications</source><conf-name>13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)</conf-name><publisher-name>USENIX Association</publisher-name><publisher-loc>Carlsbad, CA</publisher-loc><year>2018</year><fpage>561</fpage><lpage>577</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.usenix.org/conference/osdi18/presentation/moritz">https://www.usenix.org/conference/osdi18/presentation/moritz</ext-link></comment></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Needleman</surname><given-names>Saul B</given-names></name><name><surname>Wunsch</surname><given-names>Christian D</given-names></name></person-group><article-title>A general method applicable to the search for similarities in the amino acid sequence of two proteins</article-title><source>Journal of Molecular Biology</source><year>1970</year><volume>48</volume><issue>3</issue><fpage>443</fpage><lpage>453</lpage><pub-id pub-id-type="pmid">5420325</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="web"><collab>NVIDIA</collab><source>NVIDIA Nsight Systems</source><year>2022</year><comment><ext-link ext-link-type="uri" xlink:href="https://developer.nvidia.com/nsight-systems">https://developer.nvidia.com/nsight-systems</ext-link></comment></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Otto</surname><given-names>Sarah P</given-names></name><name><surname>Day</surname><given-names>Troy</given-names></name><name><surname>Arino</surname><given-names>Julien</given-names></name><name><surname>Colijn</surname><given-names>Caroline</given-names></name><name><surname>Dushoff</surname><given-names>Jonathan</given-names></name><name><surname>Li</surname><given-names>Michael</given-names></name><name><surname>Mechai</surname><given-names>Samir</given-names></name><name><surname>Van Domselaar</surname><given-names>Gary</given-names></name><name><surname>Wu</surname><given-names>Jianhong</given-names></name><name><surname>Earn</surname><given-names>David JD</given-names></name><name><surname>Ogden</surname><given-names>Nicholas H</given-names></name><etal/></person-group><article-title>The origins and potential future of SARS-CoV-2 variants of concern in the evolving COVID-19 pandemic</article-title><source>Curr Biol</source><year>2021</year><month>Jul</month><volume>31</volume><issue>14</issue><fpage>R918</fpage><lpage>R929</lpage><pub-id pub-id-type="pmcid">PMC8220957</pub-id><pub-id pub-id-type="pmid">34314723</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2021.06.049</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Page</surname><given-names>Andrew J</given-names></name><name><surname>Taylor</surname><given-names>Ben</given-names></name><name><surname>Delaney</surname><given-names>Aidan J</given-names></name><name><surname>Soares</surname><given-names>Jorge</given-names></name><name><surname>Seemann</surname><given-names>Torsten</given-names></name><name><surname>Keane</surname><given-names>A</given-names></name><name><surname>Harris</surname><given-names>Simon R</given-names></name></person-group><article-title>SNP-sites: rapid efficient extraction of SNPs from multi-FASTA alignments</article-title><source>Microbial Genomics</source><fpage>5</fpage><comment>[n. d.]. ([n. d.])</comment><pub-id pub-id-type="pmcid">PMC5320690</pub-id><pub-id pub-id-type="pmid">28348851</pub-id><pub-id pub-id-type="doi">10.1099/mgen.0.000056</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Papalampidi</surname><given-names>Pinelopi</given-names></name><name><surname>Cao</surname><given-names>Kris</given-names></name><name><surname>Kocisky</surname><given-names>Tomas</given-names></name></person-group><article-title>Towards Coherent and Consistent Use of Entities in Narrative Generation</article-title><source>arXiv preprint</source><year>2022</year><elocation-id>arXiv:2202.01709</elocation-id></element-citation></ref><ref id="R40"><element-citation publication-type="web"><collab>Pytorch</collab><source>Pytorch Lightning</source><year>2022</year><comment><ext-link ext-link-type="uri" xlink:href="https://www.pytorchlightning.ai/">https://www.pytorchlightning.ai/</ext-link></comment></element-citation></ref><ref id="R41"><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Radford</surname><given-names>Alec</given-names></name><name><surname>Narasimhan</surname><given-names>Karthik</given-names></name><name><surname>Salimans</surname><given-names>Tim</given-names></name><name><surname>Sutskever</surname><given-names>Ilya</given-names></name><etal/></person-group><source>Improving language understanding by generative pre-training</source><year>2018</year></element-citation></ref><ref id="R42"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rajbhandari</surname><given-names>Samyam</given-names></name><name><surname>Rasley</surname><given-names>Jeff</given-names></name><name><surname>Ruwase</surname><given-names>Olatunji</given-names></name><name><surname>He</surname><given-names>Yuxiong</given-names></name></person-group><source>Zero: Memory optimizations toward training trillion parameter models</source><conf-name>SC20: International Conference for High Performance Computing, Networking, Storage and Analysis</conf-name><conf-sponsor>IEEE</conf-sponsor><year>2020</year><fpage>1</fpage><lpage>16</lpage></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rambaut</surname><given-names>Andrew</given-names></name><name><surname>Holmes</surname><given-names>Edward C</given-names></name><name><surname>O’Toole</surname><given-names>Áine</given-names></name><name><surname>Hill</surname><given-names>Verity</given-names></name><name><surname>McCrone</surname><given-names>John T</given-names></name><name><surname>Ruis</surname><given-names>Christopher</given-names></name><name><surname>du Plessis</surname><given-names>Louis</given-names></name><name><surname>Pybus</surname><given-names>Oliver G</given-names></name></person-group><article-title>A dynamic nomenclature proposal for SARS-CoV-2 lineages to assist genomic epidemiology</article-title><source>Nat Microbiol</source><year>2020</year><month>Nov</month><volume>5</volume><issue>11</issue><fpage>1403</fpage><lpage>1407</lpage><pub-id pub-id-type="pmcid">PMC7610519</pub-id><pub-id pub-id-type="pmid">32669681</pub-id><pub-id pub-id-type="doi">10.1038/s41564-020-0770-5</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramchandani</surname><given-names>Ankit</given-names></name><name><surname>Fan</surname><given-names>Chao</given-names></name><name><surname>Mostafavi</surname><given-names>Ali</given-names></name></person-group><article-title>DeepCOVIDNet: An Interpretable Deep Learning Model for Predictive Surveillance of COVID-19 Using Heterogeneous Features and Their Interactions</article-title><source>IEEE Access</source><year>2020</year><volume>8</volume><fpage>159915</fpage><lpage>159930</lpage><pub-id pub-id-type="pmcid">PMC8545302</pub-id><pub-id pub-id-type="pmid">34786287</pub-id><pub-id pub-id-type="doi">10.1109/ACCESS.2020.3019989</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rasley</surname><given-names>Jeff</given-names></name><name><surname>Rajbhandari</surname><given-names>Samyam</given-names></name><name><surname>Ruwase</surname><given-names>Olatunji</given-names></name><name><surname>He</surname><given-names>Yuxiong</given-names></name></person-group><source>Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters</source><conf-name>Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</conf-name><year>2020</year><fpage>3505</fpage><lpage>3506</lpage></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rives</surname><given-names>Alexander</given-names></name><name><surname>Meier</surname><given-names>Joshua</given-names></name><name><surname>Sercu</surname><given-names>Tom</given-names></name><name><surname>Goyal</surname><given-names>Siddharth</given-names></name><name><surname>Lin</surname><given-names>Zeming</given-names></name><name><surname>Liu</surname><given-names>Jason</given-names></name><name><surname>Guo</surname><given-names>Demi</given-names></name><name><surname>Ott</surname><given-names>Myle</given-names></name><name><surname>Zitnick</surname><given-names>C Lawrence</given-names></name><name><surname>Ma</surname><given-names>Jerry</given-names></name><name><surname>Fergus</surname><given-names>Rob</given-names></name><etal/></person-group><article-title>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</article-title><source>Proceedings of the National Academy of Sciences</source><year>2021</year><volume>118</volume><issue>15</issue><elocation-id>e2016239118</elocation-id><comment>arXiv:<ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/doi/pdf/10.1073/pnas.2016239118">https://www.pnas.org/doi/pdf/10.1073/pnas.2016239118</ext-link></comment><pub-id pub-id-type="pmcid">PMC8053943</pub-id><pub-id pub-id-type="pmid">33876751</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2016239118</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Salim</surname><given-names>Michael</given-names></name><name><surname>Uram</surname><given-names>Thomas</given-names></name><name><surname>Childers</surname><given-names>J Taylor</given-names></name><name><surname>Vishwanath</surname><given-names>Venkatram</given-names></name><name><surname>Papka</surname><given-names>Michael</given-names></name></person-group><source>Balsam: Near Real-Time Experimental Data Analysis on Supercomputers</source><conf-name>2019 IEEE/ACM 1st Annual Workshop on Large-scale Experiment-in-the-Loop Computing (XLOOP)</conf-name><conf-sponsor>IEEE</conf-sponsor><year>2019</year><pub-id pub-id-type="doi">10.1109/xloop49562.2019.00010</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shin</surname><given-names>Young C</given-names></name><name><surname>Bischof</surname><given-names>Georg F</given-names></name><name><surname>Lauer</surname><given-names>William A</given-names></name><name><surname>Desrosiers</surname><given-names>Ronald C</given-names></name></person-group><article-title>Importance of codon usage for the temporal regulation of viral gene expression</article-title><source>Proceedings of the National Academy of Sciences</source><year>2015</year><volume>112</volume><issue>45</issue><fpage>14030</fpage><lpage>14035</lpage><comment>arXiv:<ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/doi/pdf/10.1073/pnas.1515387112">https://www.pnas.org/doi/pdf/10.1073/pnas.1515387112</ext-link></comment><pub-id pub-id-type="pmcid">PMC4653223</pub-id><pub-id pub-id-type="pmid">26504241</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1515387112</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stamatakis</surname><given-names>Alexandros</given-names></name></person-group><article-title>RAxML version 8: a tool for phylogenetic analysis and post-analysis of large phylogenies</article-title><source>Bioinformatics</source><year>2014</year><month>May</month><volume>30</volume><issue>9</issue><fpage>1312</fpage><lpage>1313</lpage><pub-id pub-id-type="pmcid">PMC3998144</pub-id><pub-id pub-id-type="pmid">24451623</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btu033</pub-id></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>Simeng</given-names></name><name><surname>Krishna</surname><given-names>Kalpesh</given-names></name><name><surname>Mattarella-Micke</surname><given-names>Andrew</given-names></name><name><surname>Iyyer</surname><given-names>Mohit</given-names></name></person-group><article-title>Do Long-Range Language Models Actually Use Long-Range Context?</article-title><source>arXiv preprint</source><year>2021</year><elocation-id>arXiv:2109.09115</elocation-id></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>Simeng</given-names></name><name><surname>Thai</surname><given-names>Katherine</given-names></name><name><surname>Iyyer</surname><given-names>Mohit</given-names></name></person-group><article-title>ChapterBreak: A Challenge Dataset for Long-Range Language Models</article-title><source>arXiv preprint</source><year>2022</year><elocation-id>arXiv:2204.10878</elocation-id></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Syrowatka</surname><given-names>Ania</given-names></name><name><surname>Kuznetsova</surname><given-names>Masha</given-names></name><name><surname>Alsubai</surname><given-names>Ava</given-names></name><name><surname>Beckman</surname><given-names>Adam L</given-names></name><name><surname>Bain</surname><given-names>Paul A</given-names></name><name><surname>Craig</surname><given-names>Kelly Jean Thomas</given-names></name><name><surname>Hu</surname><given-names>Jianying</given-names></name><name><surname>Jackson</surname><given-names>Gretchen Purcell</given-names></name><name><surname>Rhee</surname><given-names>Kyu</given-names></name><name><surname>Bates</surname><given-names>David W</given-names></name></person-group><article-title>Leveraging artificial intelligence for pandemic preparedness and response: a scoping review to identify key use cases</article-title><source>npj Digital Medicine</source><year>2021</year><volume>4</volume><issue>1</issue><fpage>96</fpage><pub-id pub-id-type="pmcid">PMC8192906</pub-id><pub-id pub-id-type="pmid">34112939</pub-id><pub-id pub-id-type="doi">10.1038/s41746-021-00459-8</pub-id></element-citation></ref><ref id="R53"><element-citation publication-type="web"><collab>Top500</collab><source>June 2022 | TOP500</source><year>2022</year><comment><ext-link ext-link-type="uri" xlink:href="https://www.top500.org/lists/top500/2022/06/">https://www.top500.org/lists/top500/2022/06/</ext-link></comment></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turakhia</surname><given-names>Yatish</given-names></name><name><surname>Thornlow</surname><given-names>Bryan</given-names></name><name><surname>Hinrichs</surname><given-names>Angie S</given-names></name><name><surname>De Maio</surname><given-names>Nicola</given-names></name><name><surname>Gozashti</surname><given-names>Landen</given-names></name><name><surname>Lanfear</surname><given-names>Robert</given-names></name><name><surname>Haussler</surname><given-names>David</given-names></name><name><surname>Corbett-Detig</surname><given-names>Russell</given-names></name></person-group><article-title>Ultrafast Sample placement on Existing tRees (UShER) enables real-time phylogenetics for the SARS-CoV-2 pandemic</article-title><source>Nat Genet</source><year>2021</year><month>June</month><volume>53</volume><issue>6</issue><fpage>809</fpage><lpage>816</lpage><pub-id pub-id-type="pmcid">PMC9248294</pub-id><pub-id pub-id-type="pmid">33972780</pub-id><pub-id pub-id-type="doi">10.1038/s41588-021-00862-7</pub-id></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turc</surname><given-names>Iulia</given-names></name><name><surname>Chang</surname><given-names>Ming-Wei</given-names></name><name><surname>Lee</surname><given-names>Kenton</given-names></name><name><surname>Toutanova</surname><given-names>Kristina</given-names></name></person-group><article-title>Well-read students learn better: On the importance of pre-training compact models</article-title><source>arXiv preprint</source><year>2019</year><elocation-id>arXiv:1908.08962</elocation-id></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Unsal</surname><given-names>Serbulent</given-names></name><name><surname>Atas</surname><given-names>Heval</given-names></name><name><surname>Albayrak</surname><given-names>Muammer</given-names></name><name><surname>Turhan</surname><given-names>Kemal</given-names></name><name><surname>Acar</surname><given-names>Aybar C</given-names></name><name><surname>Doğan</surname><given-names>Tunca</given-names></name></person-group><article-title>Learning functional properties of proteins with language models</article-title><source>Nature Machine Intelligence</source><year>2022</year><volume>4</volume><issue>3</issue><fpage>227</fpage><lpage>245</lpage></element-citation></ref><ref id="R57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaswani</surname><given-names>Ashish</given-names></name><name><surname>Shazeer</surname><given-names>Noam</given-names></name><name><surname>Parmar</surname><given-names>Niki</given-names></name><name><surname>Uszkoreit</surname><given-names>Jakob</given-names></name><name><surname>Jones</surname><given-names>Llion</given-names></name><name><surname>Gomez</surname><given-names>Aidan N</given-names></name><name><surname>Kaiser</surname><given-names>Łukasz</given-names></name><name><surname>Polosukhin</surname><given-names>Illia</given-names></name></person-group><article-title>Attention is all you need</article-title><source>Advances in neural information processing systems</source><year>2017</year><volume>30</volume><pub-id pub-id-type="pmcid">PMC6105294</pub-id><pub-id pub-id-type="pmid">30147283</pub-id></element-citation></ref><ref id="R58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vincent</surname><given-names>Pascal</given-names></name></person-group><article-title>A connection between score matching and denoising autoencoders</article-title><source>Neural computation</source><year>2011</year><volume>23</volume><issue>7</issue><fpage>1661</fpage><lpage>1674</lpage><pub-id pub-id-type="pmid">21492012</pub-id></element-citation></ref><ref id="R59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallace</surname><given-names>Zachary S</given-names></name><name><surname>Davis</surname><given-names>James</given-names></name><name><surname>Niewiadomska</surname><given-names>Anna Maria</given-names></name><name><surname>Olson</surname><given-names>Robert D</given-names></name><name><surname>Shukla</surname><given-names>Maulik</given-names></name><name><surname>Stevens</surname><given-names>Rick</given-names></name><name><surname>Zhang</surname><given-names>Yun</given-names></name><name><surname>Zmasek</surname><given-names>Christian M</given-names></name><name><surname>Scheuermann</surname><given-names>Richard H</given-names></name></person-group><article-title>Early Detection of Emerging SARS-CoV-2 Variants of Interest for Experimental Evaluation</article-title><source>medRxiv</source><year>2022</year><pub-id pub-id-type="pmcid">PMC9638046</pub-id><pub-id pub-id-type="pmid">36353215</pub-id><pub-id pub-id-type="doi">10.3389/fbinf.2022.1020189</pub-id></element-citation></ref><ref id="R60"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Rose E</given-names></name><name><surname>Durmus</surname><given-names>Esin</given-names></name><name><surname>Goodman</surname><given-names>Noah</given-names></name><name><surname>Hashimoto</surname><given-names>Tatsunori</given-names></name></person-group><source>Language modeling via stochastic processes</source><conf-name>International Conference on Learning Representations</conf-name><year>2022</year></element-citation></ref><ref id="R61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Shiliang</given-names></name><name><surname>Sundaram</surname><given-names>Jaideep P</given-names></name><name><surname>Stockwell</surname><given-names>Timothy B</given-names></name></person-group><article-title>VIGOR extended to annotate genomes for additional 12 different viruses</article-title><source>Nucleic Acids Research</source><year>2012</year><month>06</month><volume>40</volume><issue>W1</issue><fpage>W186</fpage><lpage>W192</lpage><pub-id pub-id-type="pmcid">PMC3394299</pub-id><pub-id pub-id-type="pmid">22669909</pub-id><pub-id pub-id-type="doi">10.1093/nar/gks528</pub-id></element-citation></ref><ref id="R62"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ward</surname><given-names>L</given-names></name><name><surname>Sivaraman</surname><given-names>G</given-names></name><name><surname>Pauloski</surname><given-names>J</given-names></name><name><surname>Babuji</surname><given-names>Y</given-names></name><name><surname>Chard</surname><given-names>R</given-names></name><name><surname>Dandu</surname><given-names>N</given-names></name><name><surname>Redfern</surname><given-names>PC</given-names></name><name><surname>Assary</surname><given-names>RS</given-names></name><name><surname>Chard</surname><given-names>K</given-names></name><name><surname>Curtiss</surname><given-names>LA</given-names></name><name><surname>Thakur</surname><given-names>R</given-names></name><etal/></person-group><source>Colmena: Scalable Machine-Learning-Based Steering of Ensemble Simulations for High Performance Computing</source><conf-name>2021 IEEE/ACM Workshop on Machine Learning in High Performance ComputingEnvironments (MLHPC)</conf-name><conf-sponsor>IEEE Computer Society</conf-sponsor><conf-loc>Los Alamitos, CA, USA</conf-loc><year>2021</year><fpage>9</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1109/MLHPC54614.2021.00007</pub-id></element-citation></ref><ref id="R63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>Jason</given-names></name><name><surname>Tay</surname><given-names>Yi</given-names></name><name><surname>Bommasani</surname><given-names>Rishi</given-names></name><name><surname>Raffel</surname><given-names>Colin</given-names></name><name><surname>Zoph</surname><given-names>Barret</given-names></name><name><surname>Borgeaud</surname><given-names>Sebastian</given-names></name><name><surname>Yogatama</surname><given-names>Dani</given-names></name><name><surname>Bosma</surname><given-names>Maarten</given-names></name><name><surname>Zhou</surname><given-names>Denny</given-names></name><name><surname>Metzler</surname><given-names>Donald</given-names></name><etal/></person-group><article-title>Emergent abilities of large language models</article-title><source>arXiv preprint</source><year>2022</year><elocation-id>arXiv:2206.07682</elocation-id></element-citation></ref><ref id="R64"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wozniak</surname><given-names>JM</given-names></name><name><surname>Armstrong</surname><given-names>TG</given-names></name><name><surname>Wilde</surname><given-names>M</given-names></name><name><surname>Katz</surname><given-names>DS</given-names></name><name><surname>Lusk</surname><given-names>E</given-names></name><name><surname>Foster</surname><given-names>IT</given-names></name></person-group><source>Swift/T: Large-Scale Application Composition via Distributed-Memory Dataflow Processing</source><conf-name>13th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing</conf-name><year>2013</year><fpage>95</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.1109/CCGrid.2013.99</pub-id></element-citation></ref><ref id="R65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamada</surname><given-names>Kazunori D</given-names></name><name><surname>Tomii</surname><given-names>Kentaro</given-names></name><name><surname>Katoh</surname><given-names>Kazutaka</given-names></name></person-group><article-title>Application of the MAFFT sequence alignment program to large data—reexamination of the usefulness of chained guide trees</article-title><source>Bioinformatics</source><year>2016</year><month>Nov</month><volume>32</volume><issue>21</issue><fpage>3246</fpage><lpage>3251</lpage><pub-id pub-id-type="pmcid">PMC5079479</pub-id><pub-id pub-id-type="pmid">27378296</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btw412</pub-id></element-citation></ref><ref id="R66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Susan</given-names></name><name><surname>Roller</surname><given-names>Stephen</given-names></name><name><surname>Goyal</surname><given-names>Naman</given-names></name><name><surname>Artetxe</surname><given-names>Mikel</given-names></name><name><surname>Chen</surname><given-names>Moya</given-names></name><name><surname>Chen</surname><given-names>Shuohui</given-names></name><name><surname>Dewan</surname><given-names>Christopher</given-names></name><name><surname>Diab</surname><given-names>Mona</given-names></name><name><surname>Li</surname><given-names>Xian</given-names></name><name><surname>Lin</surname><given-names>Xi Victoria</given-names></name><etal/></person-group><article-title>Opt: Open pre-trained transformer language models</article-title><source>arXiv preprint</source><year>2022</year><elocation-id>arXiv:2205.01068</elocation-id></element-citation></ref><ref id="R67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zost</surname><given-names>Seth J</given-names></name><name><surname>Gilchuk</surname><given-names>Pavlo</given-names></name><name><surname>Chen</surname><given-names>Rita E</given-names></name><name><surname>Case</surname><given-names>James Brett</given-names></name><name><surname>Reidy</surname><given-names>Joseph X</given-names></name><name><surname>Trivette</surname><given-names>Andrew</given-names></name><name><surname>Nargi</surname><given-names>Rachel S</given-names></name><name><surname>Sutton</surname><given-names>Rachel E</given-names></name><name><surname>Suryadevara</surname><given-names>Naveenchandra</given-names></name><name><surname>Chen</surname><given-names>Elaine C</given-names></name><name><surname>Binshtein</surname><given-names>Elad</given-names></name><etal/></person-group><article-title>Rapid isolation and profiling of a diverse panel of human monoclonal antibodies targeting the SARS-CoV-2 spike protein</article-title><source>Nature Medicine</source><year>2020</year><volume>26</volume><issue>9</issue><fpage>1422</fpage><lpage>1427</lpage><pub-id pub-id-type="pmcid">PMC8194108</pub-id><pub-id pub-id-type="pmid">32651581</pub-id><pub-id pub-id-type="doi">10.1038/s41591-020-0998-x</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><p>Overview of GenSLM models for predictive modeling of SARS-CoV-2 evolution. The inputs to GenSLM are nucleotide sequences, encoded at the codon level (every three nucleotide represents a codon; hence the 20 natural amino acid language is described by 64 codons). These inputs are successively fed into transformer blocks (referred to as layers (<italic>L<sub>i</sub></italic>)), which ultimately results in learning a semantic embedding <inline-formula><mml:math id="M8"><mml:mover><mml:mi>z</mml:mi><mml:mo accent="false">→</mml:mo></mml:mover></mml:math></inline-formula> space from which one may obtain the probability of any given sequence token <inline-formula><mml:math id="M9"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover><mml:mi>X</mml:mi><mml:mo accent="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mover><mml:mi>X</mml:mi><mml:mo accent="false">→</mml:mo></mml:mover><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">[</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mi class="MJX-variant" mathvariant="normal">∖</mml:mi><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>i</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:math></inline-formula> where <italic>N</italic> represents the sequence length and <italic>i</italic> represents a particular position in the entire genome.</p></caption><graphic xlink:href="EMS155622-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>GenSLMs learned latent space describes biologically meaningful properties for SARS-CoV-2 genomes.</title><p>(A) The embeddings from GenSLMs are visualized with t-distributed stochastic neighbor embedding (t-SNE) and each gene sequence is represented as a dot in the 2D plot. We paint each sequence by its variant ID – although we have more than 515 PANGO (<xref ref-type="bibr" rid="R43">Rambaut et al., 2020</xref> lineages represented in the data, we only show those with WHO designated labels. (B) The latent space can also be painted with the MAFFT-determined alignment score (<xref ref-type="bibr" rid="R65">Yamada et al., 2016</xref>) with respect to an Omicron genome; clustering in the distance measures is clearly visible. Visualizing the sequence log-likelihood (blue bar) and the cross-protein attention (orange lines) from (C) Delta and (D) Omicron SARS-CoV-2 strains highlights how different the co-evolutionary patterns are in these lineages. It is interesting to note that while the Spike protein from Delta strain shows coupling to nsp3, nsp5, and other proteins, these couplings are not observed in the Omicron strain.</p></caption><graphic xlink:href="EMS155622-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><p>Illustration of diffusion-based hierarchical modeling. To predict a codon (such as TAA), we use both the previous codons within the context window (we use size 3 shown in green for illustration) and the high-level representations z.</p></caption><graphic xlink:href="EMS155622-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Diffusion-based hierarchical modeling of SARS-CoV-2 genomes results in generation of sequences that captures the correct context of various open reading frames (ORFs).</title><p>(A) Comparison of statistics measured on generated sequences and on real data for the ORFs. Diffusion-based hierarchical LM has a global high-level plan whereas the baseline can only take into account the previous 1023 codons. (B) Generated sequences (light blue) from the model overlaid on the phylogenetic tree demonstrate that these sequences are similar to observed strains.</p></caption><graphic xlink:href="EMS155622-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><p>Conceptual overview of our workflow. A “Thinker” orchestrates data flow between two applications, namely the sequence generator and the Bayesian optimization to drive the generated sequences towards a target property using reward-guided beam search, where <italic>μ</italic> represents the mixing constant used to balance the reward function against the log likelihood of generating the next token.</p></caption><graphic xlink:href="EMS155622-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><p>(A) Scaling results on Polaris and Selene systems for MSL=2048; (B) Scaling behavior of DDP vs. DeepSpeed runs on Selene (C) Scaling results on Polaris and Selene systems for MSL=10240;</p></caption><graphic xlink:href="EMS155622-f006"/></fig><fig id="F7" position="float"><label>Figure 7</label><caption><p>Workflow utilization measured by the number of active workers (applications actively serving requests) as a function of workflow runtime measured on 224 nodes of Polaris (896 A100 GPUs). The warm-able application design realizes 97% utilization, enabling 1.9X more sequences to be generated compared to a cold start baseline.</p></caption><graphic xlink:href="EMS155622-f007"/></fig><table-wrap id="T1" position="float" orientation="portrait"><label>Table 1</label><caption><p>Description of GenSLMs foundation model architectures. #H – number of attention heads; #L – number of layers; <italic>d</italic><sub>model</sub> – embedding size; LR – learning rate (if range is specified, decayed by factor of 10 each update); B – global batch size in number sequences per step; P – total number of trainable parameters; MSL – maximum sequence length. The <sup>†</sup> denotes models that we were also able to train on the 10,240 sequence length for the full genome.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle"/><th align="center" valign="top" style="border-right: solid thin">#H</th><th align="center" valign="top" style="border-right: solid thin">#L</th><th align="center" valign="top" style="border-right: solid thin"><italic>d</italic><sub>model</sub></th><th align="center" valign="top" style="border-right: solid thin">LR</th><th align="center" valign="top" style="border-right: solid thin">B</th><th align="center" valign="top" style="border-right: solid thin">P</th><th align="center" valign="top">MSL</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom: solid thin" rowspan="4">GPU</td><td align="center" valign="top" style="border-right: solid thin">8</td><td align="center" valign="top" style="border-right: solid thin">8</td><td align="center" valign="top" style="border-right: solid thin">512</td><td align="center" valign="top" style="border-right: solid thin">5e<sup>−05</sup></td><td align="center" valign="top" style="border-right: solid thin">4096</td><td align="center" valign="top" style="border-right: solid thin">25M</td><td align="center" valign="top">2048<sup>†</sup></td></tr><tr><td align="center" valign="top" style="border-right: solid thin">16</td><td align="center" valign="top" style="border-right: solid thin">12</td><td align="center" valign="top" style="border-right: solid thin">1,840</td><td align="center" valign="top" style="border-right: solid thin">5e<sup>−05</sup></td><td align="center" valign="top" style="border-right: solid thin">4096</td><td align="center" valign="top" style="border-right: solid thin">250M</td><td align="center" valign="top">2048<sup>†</sup></td></tr><tr><td align="center" valign="top" style="border-right: solid thin">64</td><td align="center" valign="top" style="border-right: solid thin">26</td><td align="center" valign="top" style="border-right: solid thin">3,968</td><td align="center" valign="top" style="border-right: solid thin">5e<sup>−05</sup></td><td align="center" valign="top" style="border-right: solid thin">512</td><td align="center" valign="top" style="border-right: solid thin">2.5B</td><td align="center" valign="top">2048</td></tr><tr><td align="center" valign="middle" style="border-bottom: solid thin; border-right: solid thin">64</td><td align="center" valign="middle" style="border-bottom: solid thin; border-right: solid thin">64</td><td align="center" valign="middle" style="border-bottom: solid thin; border-right: solid thin">8,192</td><td align="center" valign="middle" style="border-bottom: solid thin; border-right: solid thin">5e<sup>−05</sup> - 5e<sup>−09</sup></td><td align="center" valign="middle" style="border-bottom: solid thin; border-right: solid thin">1024</td><td align="center" valign="middle" style="border-bottom: solid thin; border-right: solid thin">25B</td><td align="center" valign="middle" style="border-bottom: solid thin">2048</td></tr><tr><td align="center" valign="middle" style="border-bottom: solid thin"/><td align="center" valign="middle" style="border-bottom: solid thin; border-right: solid thin">#H</td><td align="center" valign="middle" style="border-bottom: solid thin; border-right: solid thin">#L</td><td align="center" valign="middle" style="border-bottom: solid thin; border-right: solid thin"><italic>d</italic><sub>model</sub></td><td align="center" valign="middle" style="border-bottom: solid thin; border-right: solid thin">LR</td><td align="center" valign="middle" style="border-bottom: solid thin; border-right: solid thin">B</td><td align="center" valign="middle" style="border-bottom: solid thin; border-right: solid thin">P</td><td align="center" valign="middle" style="border-bottom: solid thin">MSL</td></tr><tr><td align="center" valign="middle" style="border-bottom: solid thin" rowspan="4">CS-2</td><td align="center" valign="top" style="border-right: solid thin">12</td><td align="center" valign="top" style="border-right: solid thin">12</td><td align="center" valign="top" style="border-right: solid thin">768</td><td align="center" valign="top" style="border-right: solid thin">2.8e<sup>−04</sup></td><td align="center" valign="top" style="border-right: solid thin">33</td><td align="center" valign="top" style="border-right: solid thin">123M</td><td align="center" valign="top">10240</td></tr><tr><td align="center" valign="top" style="border-right: solid thin">12</td><td align="center" valign="top" style="border-right: solid thin">12</td><td align="center" valign="top" style="border-right: solid thin">768</td><td align="center" valign="top" style="border-right: solid thin">2.8e<sup>−04</sup></td><td align="center" valign="top" style="border-right: solid thin">132</td><td align="center" valign="top" style="border-right: solid thin">123M</td><td align="center" valign="top">10240</td></tr><tr><td align="center" valign="top" style="border-right: solid thin">16</td><td align="center" valign="top" style="border-right: solid thin">24</td><td align="center" valign="top" style="border-right: solid thin">2048</td><td align="center" valign="top" style="border-right: solid thin">7.5e<sup>−05</sup></td><td align="center" valign="top" style="border-right: solid thin">11</td><td align="center" valign="top" style="border-right: solid thin">1.3B</td><td align="center" valign="top">10240</td></tr><tr><td align="center" valign="top" style="border-right: solid thin">16</td><td align="center" valign="top" style="border-right: solid thin">24</td><td align="center" valign="top" style="border-right: solid thin">2048</td><td align="center" valign="top" style="border-right: solid thin">1.5e<sup>−04</sup></td><td align="center" valign="top" style="border-right: solid thin">44</td><td align="center" valign="top" style="border-right: solid thin">1.3B</td><td align="center" valign="top">10240</td></tr></tbody></table></table-wrap><table-wrap id="T2" position="float" orientation="portrait"><label>Table 2</label><caption><title>GPU supercomputing systems used for evaluation.</title></caption><table frame="box" rules="groups"><thead><tr><th align="left" valign="middle" style="border-right: solid thin"/><th align="left" valign="top" style="border-right: solid thin">Polaris</th><th align="left" valign="top">Selene</th></tr></thead><tbody><tr><td align="left" valign="top" style="border-right: solid thin">June-2022 Top 500#</td><td align="left" valign="top" style="border-right: solid thin">14</td><td align="left" valign="top">8</td></tr><tr><td align="left" valign="top" style="border-right: solid thin">System size (nodes)</td><td align="left" valign="top" style="border-right: solid thin">560</td><td align="left" valign="top">560</td></tr><tr><td align="left" valign="top" style="border-right: solid thin">CPU</td><td align="left" valign="top" style="border-right: solid thin">AMD Milan</td><td align="left" valign="top">AMD Rome</td></tr><tr><td align="left" valign="top" style="border-right: solid thin">Sockets/Node (total cores)</td><td align="left" valign="top" style="border-right: solid thin">1(32)</td><td align="left" valign="top">2 (128)</td></tr><tr><td align="left" valign="middle" style="border-bottom: solid thin; border-right: solid thin">System Memory (TB)</td><td align="left" valign="middle" style="border-bottom: solid thin; border-right: solid thin">0.5</td><td align="left" valign="middle" style="border-bottom: solid thin">2</td></tr><tr><td align="left" valign="top" style="border-right: solid thin">Number of GPUs per node</td><td align="left" valign="top" style="border-right: solid thin">4</td><td align="left" valign="top">8</td></tr><tr><td align="left" valign="top" style="border-right: solid thin">At00 GPU Memory (GB)</td><td align="left" valign="top" style="border-right: solid thin">40</td><td align="left" valign="top">80</td></tr><tr><td align="left" valign="top" style="border-right: solid thin">GPU Memory Technology</td><td align="left" valign="top" style="border-right: solid thin">HBM2</td><td align="left" valign="top">HBM2e</td></tr><tr><td align="left" valign="top" style="border-right: solid thin">GPU Memory BW (TB/s)</td><td align="left" valign="top" style="border-right: solid thin">1.5</td><td align="left" valign="top">2.0</td></tr><tr><td align="left" valign="middle" style="border-bottom: solid thin; border-right: solid thin">Interconnect</td><td align="left" valign="middle" style="border-bottom: solid thin; border-right: solid thin">HPE Slingshot-10</td><td align="left" valign="middle" style="border-bottom: solid thin">Mellanox Infiniband HDR</td></tr><tr><td align="left" valign="top" style="border-right: solid thin">NICs per node</td><td align="left" valign="top" style="border-right: solid thin">2</td><td align="left" valign="top">8</td></tr><tr><td align="left" valign="top" style="border-right: solid thin">Network BW per direction (GB/s)</td><td align="left" valign="top" style="border-right: solid thin">12.5</td><td align="left" valign="top">25</td></tr><tr><td align="left" valign="top" style="border-right: solid thin">Number of nodes (GPUs) scaled</td><td align="left" valign="top" style="border-right: solid thin">512 (2048)</td><td align="left" valign="top">512 (4096)</td></tr></tbody></table></table-wrap><table-wrap id="T3" position="float" orientation="portrait"><label>Table 3</label><caption><p>Compute performance of the production runs in mixed precision (MP) for different model sizes with a sequence length of 2,048. This includes the I/O, computations needed for forward pass, backward pass and weight updates, and communication.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="bottom">Model Size</th><th align="center" valign="bottom">Tflops/step(MP)</th><th align="center" valign="bottom">sec/step</th><th align="center" valign="bottom">TFLOPS/GPU(MP)</th><th align="right" valign="bottom"># GPUs</th><th align="center" valign="bottom">Num steps</th><th align="center" valign="bottom">Sustained PFLOPS(MP)</th><th align="center" valign="bottom">Total Zflops(MP)</th></tr></thead><tbody><tr><td align="left" valign="top" style="border-right: solid thin">25M</td><td align="center" valign="top">13.2</td><td align="center" valign="top">0.9</td><td align="center" valign="top">14.7</td><td align="right" valign="top">512</td><td align="center" valign="top">3500</td><td align="center" valign="top">3.49</td><td align="center" valign="top">0.02</td></tr><tr><td align="left" valign="top" style="border-right: solid thin">250M</td><td align="center" valign="top">58.7</td><td align="center" valign="top">3.4</td><td align="center" valign="top">17.3</td><td align="right" valign="top">512</td><td align="center" valign="top">1800</td><td align="center" valign="top">7.59</td><td align="center" valign="top">0.05</td></tr><tr><td align="left" valign="top" style="border-right: solid thin">2.5B</td><td align="center" valign="top">135.3</td><td align="center" valign="top">4.5</td><td align="center" valign="top">30.3</td><td align="right" valign="top">256</td><td align="center" valign="top">2250</td><td align="center" valign="top">5.83</td><td align="center" valign="top">0.08</td></tr><tr><td align="left" valign="top" style="border-right: solid thin">25B</td><td align="center" valign="top">654.9</td><td align="center" valign="top">14.9</td><td align="center" valign="top">43.7</td><td align="right" valign="top">1024</td><td align="center" valign="top">2200</td><td align="center" valign="top">44.79</td><td align="center" valign="top">1.48</td></tr></tbody></table></table-wrap><table-wrap id="T4" position="float" orientation="portrait"><label>Table 4</label><caption><p>Final loss and perplexity values achieved by the GenSLM Foundation (F) (2,048 tokens) and SARS-CoV-2 (S) (10,240 tokens) models. Reported values for S models are trained on the first year of SARS-CoV-2 genomes. Perplexity is computed by taking the exponential of the loss and can be interpreted as the number of guesses needed for the model to correctly fill a masked token.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="top" style="border-right: solid thin">Metric</th><th align="center" valign="top">25MF</th><th align="center" valign="top">250M F</th><th align="center" valign="top">2.5BF</th><th align="center" valign="top" style="border-right: solid thin">25BF</th><th align="center" valign="top">25MS</th><th align="center" valign="top">250M S</th></tr></thead><tbody><tr><td align="center" valign="top" style="border-right: solid thin">Loss</td><td align="center" valign="top">0.57</td><td align="center" valign="top">0.46</td><td align="center" valign="top">0.30</td><td align="center" valign="top" style="border-right: solid thin">0.70</td><td align="center" valign="top">0.015</td><td align="center" valign="top">0.011</td></tr><tr><td align="center" valign="top" style="border-right: solid thin">Perplexity</td><td align="center" valign="top">1.78</td><td align="center" valign="top">1.59</td><td align="center" valign="top">1.34</td><td align="center" valign="top" style="border-right: solid thin">2.01</td><td align="center" valign="top">1.02</td><td align="center" valign="top">1.01</td></tr></tbody></table></table-wrap><table-wrap id="T5" position="float" orientation="portrait"><label>Table 5</label><caption><title>CerebrasWafer-Scale Cluster throughput training GenSLMs on a sequence length of 10,240 tokens.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" rowspan="2" style="border-right: solid thin">Model size</th><th align="center" valign="middle" style="border-bottom: solid thin" colspan="3">Samples/sec</th></tr><tr><th align="center" valign="top">1 CS-2</th><th align="center" valign="top">2 CS-2</th><th align="center" valign="top">4 CS-2</th></tr></thead><tbody><tr><td align="center" valign="top" style="border-right: solid thin">123M</td><td align="center" valign="top">11.1</td><td align="center" valign="top">23.1</td><td align="center" valign="top">46.2</td></tr><tr><td align="center" valign="top" style="border-right: solid thin">1.3B</td><td align="center" valign="top">0.88</td><td align="center" valign="top">1.76</td><td align="center" valign="top">3.52</td></tr></tbody></table></table-wrap><table-wrap id="T6" position="float" orientation="portrait"><label>Table 6</label><caption><title>Metrics of GenSLMs trained from scratch on a sequence length of 10,240 using CerebrasWafer-Scale Cluster.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" rowspan="2" style="border-right: solid thin"/><th align="center" valign="middle" style="border-bottom: solid thin; border-right: solid thin" colspan="2">GenSLM-123M</th><th align="center" valign="middle" style="border-bottom: solid thin" colspan="2">GenSLM-1.3B</th></tr><tr><th align="center" valign="top">1 CS-2</th><th align="center" valign="top" style="border-right: solid thin">4 CS-2</th><th align="center" valign="top">1 CS-2</th><th align="center" valign="top">4 CS-2</th></tr></thead><tbody><tr><td align="right" valign="top" style="border-right: solid thin">Training steps</td><td align="center" valign="top">5,000</td><td align="center" valign="top" style="border-right: solid thin">3,000</td><td align="center" valign="top">4,500</td><td align="center" valign="top">3,000</td></tr><tr><td align="right" valign="top" style="border-right: solid thin">Training samples</td><td align="center" valign="top">165,000</td><td align="center" valign="top" style="border-right: solid thin">396,000</td><td align="center" valign="top">49,500</td><td align="center" valign="top">132,000</td></tr><tr><td align="right" valign="middle" style="border-bottom: solid thin; border-right: solid thin">Time to train (h)</td><td align="center" valign="middle" style="border-bottom: solid thin">4.1</td><td align="center" valign="middle" style="border-bottom: solid thin; border-right: solid thin">2.4</td><td align="center" valign="middle" style="border-bottom: solid thin">15.6</td><td align="center" valign="middle" style="border-bottom: solid thin">10.4</td></tr><tr><td align="right" valign="top" style="border-right: solid thin">Validation accuracy</td><td align="center" valign="top">0.9615</td><td align="center" valign="top" style="border-right: solid thin">0.9625</td><td align="center" valign="top">0.9622</td><td align="center" valign="top">0.9947</td></tr><tr><td align="right" valign="top" style="border-right: solid thin">Validation perplexity</td><td align="center" valign="top">1.0310</td><td align="center" valign="top" style="border-right: solid thin">1.0290</td><td align="center" valign="top">1.0310</td><td align="center" valign="top">1.0255</td></tr></tbody></table></table-wrap></floats-group></article>