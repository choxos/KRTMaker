<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS156297</article-id><article-id pub-id-type="doi">10.1101/2022.10.26.513851</article-id><article-id pub-id-type="archive">PPR563010</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">2</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Semantic object processing is modulated by prior scene context</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Krugliak</surname><given-names>Alexandra</given-names></name><xref ref-type="aff" rid="A1">a</xref></contrib><contrib contrib-type="author"><name><surname>Draschkow</surname><given-names>Dejan</given-names></name><xref ref-type="aff" rid="A2">b</xref><xref ref-type="aff" rid="A3">c</xref></contrib><contrib contrib-type="author"><name><surname>Võ</surname><given-names>Melissa L.-H.</given-names></name><xref ref-type="aff" rid="A4">d</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Clarke</surname><given-names>Alex</given-names></name><xref ref-type="aff" rid="A1">a</xref></contrib></contrib-group><aff id="A1"><label>a</label>Department of Psychology, University of Cambridge, UK</aff><aff id="A2"><label>b</label>Department of Experimental Psychology, University of Oxford, UK</aff><aff id="A3"><label>c</label>Oxford Centre for Human Brain Activity, Wellcome Centre for Integrative Neuroimaging, Department of Psychiatry, University of Oxford, Oxford, UK</aff><aff id="A4"><label>d</label>Department of Psychology, Goethe University Frankfurt, Germany</aff><author-notes><corresp id="CR1">Corresponding author: Alex Clarke, <email>ac584@cam.ac.uk</email>
</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>28</day><month>10</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>27</day><month>10</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Objects that are congruent with a scene are recognised more efficiently than objects that are incongruent. Further, semantic integration of incongruent objects elicits a stronger N300/N400 EEG component. Yet, the time course and mechanisms of how contextual information supports access to semantic object information is unclear. We used computational modelling and EEG to test how context influences semantic object processing. Using representational similarity analysis, we established that EEG patterns dissociated between objects in congruent or incongruent scenes from around 300 ms. By modelling semantic processing of objects using independently normed properties, we confirm that the onset of semantic processing of both congruent and incongruent objects is similar (∼150 ms). Critically, after ∼275 ms, we discover a difference in the duration of semantic integration, lasting longer for incongruent compared to congruent objects. These results constrain our understanding of how contextual information supports access to semantic object information.</p></abstract><kwd-group><kwd>EEG</kwd><kwd>object recognition</kwd><kwd>semantics</kwd><kwd>congruency effect</kwd><kwd>RSA</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">In our daily lives, we easily recognise the objects around us. Yet, in certain situations we expect to see some objects more than others. For example, when walking down a street, we might expect to encounter a car but not an elephant. But if we visit a zoo, we would be much more likely to encounter an elephant in an enclosure than a car. In both scenarios, we recognise the object as a car and an elephant, however, the context in which we see these objects influences the way we perceive and respond to them. Objects that are congruent with their environment are recognised faster and more accurately than objects that are incongruent (<xref ref-type="bibr" rid="R3">Bar, 2004</xref>; <xref ref-type="bibr" rid="R4">Biederman et al., 1982</xref>; <xref ref-type="bibr" rid="R11">Davenport &amp; Potter, 2004</xref>; <xref ref-type="bibr" rid="R18">Greene et al., 2015</xref>; <xref ref-type="bibr" rid="R33">Oliva &amp; Torralba, 2007</xref>; <xref ref-type="bibr" rid="R34">Palmer, 1975</xref>). This is also reflected in neural processing, in that incongruent objects induce a stronger negativity of the N300/N400 EEG components than congruent objects (e.g. <xref ref-type="bibr" rid="R13">Draschkow et al., 2018</xref>; <xref ref-type="bibr" rid="R16">Ganis &amp; Kutas, 2003</xref>; <xref ref-type="bibr" rid="R30">Lauer et al., 2018</xref>; <xref ref-type="bibr" rid="R32">Lauer et al., 2020</xref>; <xref ref-type="bibr" rid="R28">Mudrik et al., 2010</xref>; <xref ref-type="bibr" rid="R29">Mudrik et al., 2014</xref>; <xref ref-type="bibr" rid="R42">Võ &amp; Wolfe, 2013</xref>). Such congruency effects for stimuli mismatching a context have been reported not only for objects but for a variety of stimuli, like words at the end of a sentence (e.g. <xref ref-type="bibr" rid="R24">Kutas &amp; Hillyard, 1980</xref>), images in a preceding sentence context (e.g. <xref ref-type="bibr" rid="R17">Ganis et al., 1996</xref>), or scene images preceded by a verbal cue of a scene category (<xref ref-type="bibr" rid="R22">Kumar et al., 2021</xref>), indicating that during the N300/N400 interval, semantic information becomes available and is integrated into the context (for a review see <xref ref-type="bibr" rid="R23">Kutas &amp; Federmeier, 2011</xref>).</p><p id="P3">Much of what we do know about the semantic processing of visual objects comes from research where objects are presented isolated from the background or in a stream of unconnected events. This line of research indicates that in the first ∼150 ms after the object appears, low- and middle-level object features are extracted, mostly in a feedforward fashion along the ventral visual stream (<xref ref-type="bibr" rid="R6">Cichy et al., 2016</xref>; <xref ref-type="bibr" rid="R12">DiCarlo et al., 2012</xref>; <xref ref-type="bibr" rid="R25">Lamme &amp; Roelfsema, 2000</xref>). More complex visual features and semantic features are processed at later latencies, beginning after 150-200 ms, supported by recurrent dynamics within the ventral temporal lobes (<xref ref-type="bibr" rid="R2">Bankson et al., 2018</xref>; <xref ref-type="bibr" rid="R5">Chan et al., 2011</xref>; <xref ref-type="bibr" rid="R7">Clarke, 2019</xref>; <xref ref-type="bibr" rid="R9">Clarke et al., 2011</xref>, <xref rid="R8" ref-type="bibr">2018</xref>; <xref ref-type="bibr" rid="R19">Kietzmann et al., 2019</xref>; <xref ref-type="bibr" rid="R35">Poch et al., 2015</xref>). In agreement with this object processing timeline, the effects of object-scene congruency on the N300/N400 EEG components occur at a similar time as semantic feature effects for single objects, which would allow for context to modulate object perception.</p><p id="P4">Previous EEG research suggests the scene context can directly modulate the timing of object processing. For instance, <xref ref-type="bibr" rid="R39">Truman and Mudrik (2018)</xref> reported that when intact and scrambled objects were shown embedded in congruent or incongruent scenes, EEG signals to intact objects in congruent contexts diverged from EEG signals to scrambled images within the N300 time window, while EEG signals to intact objects in incongruent contexts diverged in the N400 time window. They suggest that object identification is delayed when objects are in incongruent scenes and integration of the object and scene is enhanced. While this might suggest differences in the timing and duration of semantic access for objects in congruent and incongruent scenes, the research so far is limited in answering the question of how semantic object information is represented in these different situations, in terms of the timing of semantic activation and the nature of this semantic information. Contrasts between congruent and incongruent conditions are well suited to exploring differences in processing between these conditions, while understanding how we access semantic information for objects in different contexts is better aided through approaches that measure semantic processing individually for each of these conditions. By tracking the semantic processing of objects in congruent scenes, separately from the semantic processing of objects in incongruent scenes, we can more directly test for differences and similarities in how semantic information is accessed.</p><p id="P5">Three plausible scenarios for how semantic access is modulated by a prior scene context are (1) that semantic object information is accessed faster for objects in congruent compared to incongruent environments, meaning that later processing of objects in incongruent environments leads to a N300/N400 congruency effect, (2) semantic access is initiated at the same time in both conditions, but continues for longer in the incongruent case, with the additional semantic activation related to congruency effects, or (3) that semantic access is initiated at the same time and for the same duration for both congruent and incongruent conditions, and differences in the magnitude of semantic access relate to congruency effects.</p><p id="P6">Here, we re-analysed EEG data by <xref ref-type="bibr" rid="R13">Draschkow and colleagues (2018)</xref> to test the hypothesis that neural effects of congruency on the N300/N400 components are driven by differences in accessing semantic object information, by combining computational models with Representational Similarity Analysis (RSA; Kriegeskorte et al., 2008) - a methodology that allows testing specific hypotheses about what object features contribute to neural signals during object processing (<xref ref-type="bibr" rid="R2">Bankson et al., 2018</xref>; <xref ref-type="bibr" rid="R6">Cichy et al., 2016</xref>; <xref ref-type="bibr" rid="R8">Clarke et al., 2018</xref>). During RSA, the similarity of neural responses between individual objects is calculated and summarized in a Representational Dissimilarity Matrix (RDM). These RDMs of brain signals can be calculated at each point in time, and tested against a second set of RDMs that represent our predictions for why objects might be more or less similar to one another (e.g. due to congruency or semantic similarity). A significant relationship between the neural RDMs and RDMs of our predictions (or models), suggests that the predicted information is currently being represented in neural signals. For example, <xref ref-type="bibr" rid="R8">Clarke and colleagues (2018)</xref> demonstrated this approach using a model of semantics based on features from a property norming study (<xref ref-type="bibr" rid="R10">Devereux et al., 2014</xref>), which was related to MEG signals. The property norms were obtained by asking participants to name features associated with concept words, resulting in a collection of 3026 different features that capture the semantic representations of individual concepts (e.g. a car has the features ‘has wheels’, ‘has a driver’ and ‘made of metal’ but not the features ‘is edible’, ‘has wings’), which then allows an examination of the relationship between neural responses to single objects and the semantics defined by the norms. <xref ref-type="bibr" rid="R8">Clarke and colleagues (2018)</xref> reported the semantic model, based on such property norms, related to brain activity peaking around 250 ms after object onset - a latency similar to the onset of the N300/N400 component. Using a similar model here, based on the same independently normed semantic features as used by <xref ref-type="bibr" rid="R8">Clarke and colleagues (2018)</xref>, provides not only the intriguing opportunity to directly test if context indeed modulates semantics, but also how it effects the temporal processing of objects in congruent and incongruent settings independently, adjudicating between the three scenarios we set out above.</p><p id="P7">In the current EEG data set, participant viewed images of scenes where a cue indicated the location where either a congruent or incongruent object would appear. We extracted similarity of neural responses to objects and related them to different models. First, we used a simple congruency model that distinguishes between congruent and incongruent contexts, to establish when representational differences in the EEG signals emerge that suggests a dissociation of processing between the conditions. Then we modelled the EEG data with a semantic model based on property norms that describes the objects in terms of semantic features, to specifically test how congruency impacts the time course of processing semantic object information, and how this is different depending on the contextual congruency between the object and the scene.</p></sec><sec id="S2" sec-type="methods"><title>Methods</title><p id="P8">We re-analysed EEG data reported by <xref ref-type="bibr" rid="R13">Draschkow and colleagues (2018)</xref>. The data is freely available (<ext-link ext-link-type="uri" xlink:href="https://github.com/DejanDraschkow/n3n4">https://github.com/DejanDraschkow/n3n4</ext-link>). Here we provide a short summary of the main aspects of the study design covering participants, procedure, EEG recording and pre-processing, as well as the specifications of our RSA analyses.</p><sec id="S3"><title>Participants and procedure</title><p id="P9">Forty healthy participants viewed 152 scene images that were presented with either a semantically congruent or incongruent object (76 trials per condition). Each scene was paired with a congruent and an incongruent object, but participants saw each scene only once with either the congruent or the incongruent object (the conditions were counterbalanced across participants). At the beginning of each trial, a scene was presented for 500 ms, then a red dot appeared indicating the position where the object would appear. After 500 to 530 ms, the object was presented in the cued location of the scene and remained on the screen for 2000 ms (<xref ref-type="fig" rid="F1">Fig 1</xref>, <xref ref-type="fig" rid="F2">Fig 2A</xref>). The task was to report exact repetitions of scenes and objects (the repetition trials were excluded from subsequent analysis).</p></sec><sec id="S4"><title>EEG recording and pre-processing</title><p id="P10">EEG data was recorded with 64 active Ag/AgCl electrodes (Brain Products, GmhB), with a sampling rate of 1000 Hz. Data were down-sampled to 200 Hz, filtered between 0.1 Hz and 40 Hz, and eye and muscle artifacts were removed with independent component analysis. Epochs of 1100ms were created, from -200 ms to 900 ms centred around object onset, then baseline correction was applied from -200 ms to 0 ms. We started our analysis with the epoched data provided by <xref ref-type="bibr" rid="R13">Draschkow and colleagues (2018)</xref>, however, we identified noisy trials by visual inspection, specifically those trials that contained high frequency noise or large amplitude signals beyond the range of normal activity. On average 2.9% of trials were removed (range 0-24%). All electrodes were included in the subsequent Representation Similarity Analysis (RSA).</p></sec><sec id="S5"><title>Representational Similarity Analysis</title><p id="P11">RSA was used to relate model-based congruency and semantic similarity between objects to the neural similarity based on EEG data (<xref ref-type="fig" rid="F2">Figure 2</xref>). We computed the similarity between neural responses to objects at each moment in time as 1-Pearson correlation between the EEG signals for each object pair and summarized the similarity measures in symmetric Representational Dissimilarity Matrices (RDMs) per time-point. Then, using Spearman correlation, we related the EEG RDMs to RDMs that reflect a similarity structure between objects based on either the congruency or semantic properties. This method allows to reveal when and for how long the EEG signals distinguish between objects based on the contextual congruency or the semantic properties of those objects.</p><sec id="S6"><title>Congruency model analysis</title><p id="P12">The congruency model dissociates between objects presented in congruent and incongruent contexts. We created participant-specific models because the same scenes were presented to some participants with a congruent object and to other participants with an incongruent object (counterbalanced across participants). For each participant, we first assigned each of the 76 congruent scene-object trials a value of 1 and each of the 76 incongruent scene-object trials a value of 0, before calculating the Euclidean distance between each object pair and summarizing the results in a 152 × 152 RDM (<xref ref-type="fig" rid="F2">Fig 2B</xref>).</p><p id="P13">From the EEG data, we extracted brain responses to each object for the time points from -200 ms to 900 ms in intervals of 5 ms, resulting in 221 time-points. Next, we calculated the correlation distance between each object pair at each time-point. This resulted in participant-specific RDMs at each time-point that summarised the neural similarity between the objects.</p><p id="P14">In the following step, we related the congruency model RDM with the brain response RDMs using Spearman correlation resulting in an RSA time-series per participant (<xref ref-type="fig" rid="F2">Fig 2C</xref>). A random effects analysis assessed the model fit of the congruency model RDM and the brain RDMs at each time-point using a t-test against zero with an alpha of 0.01. To control for multiple comparisons across time we used a cluster-mass permutation test to assign p-values to clusters of significant tests (<xref ref-type="bibr" rid="R26">Maris &amp; Oostenveld, 2007</xref>). For each permutation, the sign of RSA correlation time-series between the model and brain RDM was randomly flipped for each participant, before t-tests were performed on the permuted data, and the size of the largest cluster added to the permutation distribution. Finally, the cluster p-value for clusters in the original data were defined as proportion of the 10000 permutations (plus the observed cluster mass) that was greater than or equal to the observed cluster mass.</p></sec><sec id="S7"><title>Semantic model analysis</title><p id="P15">The semantic model specified the semantic-feature similarity of object concepts based on a published set of property norms (Devereaux et al., 2014). The current version of the property norms is available from the Centre of Speech, Language, and the Brain (<ext-link ext-link-type="uri" xlink:href="https://cslb.psychol.cam.ac.uk/propnorms">https://cslb.psychol.cam.ac.uk/propnorms</ext-link>). The property norms we used summarised how 826 different concepts related to 3026 different features (e.g. a zebra ‘has stripes’, ‘eats grass’ etc), allowing us to represent each concept by a collection of features that together define the concept (e.g. a zebra ‘has legs, ‘has stripes’, but does not ‘live in trees’). We matched the objects used by <xref ref-type="bibr" rid="R13">Draschkow and colleagues (2018)</xref> with concepts in the property norms. A matching concept was found for 118 out of 152 objects that were presented in a congruent context, and for 116 out of 152 objects that were presented in an incongruent context. Trials containing objects for which no match could be found were excluded from further analysis. For the other trials, a semantic similarity space was defined by calculating the cosine distance between all possible pairs of objects, separately for objects that were presented in a congruent context and objects that were presented in an incongruent context, resulting in two semantic feature RDMs (<xref ref-type="fig" rid="F2">Fig 2B</xref>). The resulting RDM dimensions differed across participants because while all participants viewed the same scenes, the scenes were shown to one half of the participants with a congruent object and to the other half of participants with an incongruent object. For both groups of participants, the dimension of the RDM for incongruent trials was 58 × 58, and the RDM for congruent trials for half the participants was 57 × 57 and 61 × 61 for the remaining half.</p><p id="P16">The EEG data of each participant was separated for congruent and incongruent scene-object trials, before RDMs per time-point were calculated in the same way as for the congruency model, except that now two analyses were performed, one relating the semantic feature RDM to the brain RDMs for congruent trials, and one analysis relating the semantic feature RDM to the brain RDMs for the incongruent trials (<xref ref-type="fig" rid="F2">Fig 2C</xref>). Significant differences between the RSA model fit for congruent and incongruent context conditions was additionally assessed with a cluster-based permutation test using paired sample t-tests.</p></sec></sec></sec><sec id="S8" sec-type="results"><title>Results</title><p id="P17">We combined computational modelling with RSA to test if congruency effects in N300/N400 EEG components were driven by semantic object information. First, we constructed a model of consistency to uncover when the processing of congruent and incongruent objects diverged. Then, using a model based on semantic features, we investigated the time-course of semantic processing of objects that were presented in either congruent or incongruent contexts.</p><sec id="S9"><title>Congruency model analysis</title><p id="P18">We first assessed whether neural patterns distinguished between objects presented in congruent and incongruent environments. RSA analysis of the EEG signals revealed that the congruency model distinguished between congruent and incongruent scene-object context trials, where we saw a significant relationship between the congruency model and EEG patterns from approximately 290 to 450 ms (cluster p = 0.022; <xref ref-type="fig" rid="F4">Fig 4A</xref>, <xref ref-type="table" rid="T1">Table 1</xref>). The timing of this effect is in line with previous N300/N400 effects of congruency.</p></sec><sec id="S10"><title>Semantic model analysis</title><p id="P19">While the congruency effect shows that object processing and scene information interact, it does not tell us about how semantic knowledge may be accessed differentially depending on the scene context. To address this, we constructed a semantic model based on semantic features that collectively describe each of the concepts. Two separate RDMs were created, one for objects that were presented in a congruent context and one for objects that were presented in an incongruent context. The model RDMs were then correlated with the corresponding brain RDMs, i.e. the congruent model RDM was related to brain activation RDMs to congruent objects and the incongruent model RDM to brain activation RDMs to incongruent objects.</p><p id="P20">For congruent objects, although the semantic model showed no statistically significant relationship to brain activity, it numerically performed well from around 140 ms to 235 ms (cluster p = 0.06). For incongruent objects, the semantic model fitted the brain responses significantly during two clusters, the first cluster including time-points around 140 ms to 360 ms (cluster p = 0.005) and a second cluster with time-points between around 620 to 765 ms (cluster p = 0.021; <xref ref-type="fig" rid="F4">Fig 4B</xref>, <xref ref-type="table" rid="T1">Table 1</xref>). This might suggest that while semantic effects in both conditions seem to begin at similar times, approximately 150 ms after object onset, semantic effects for incongruent objects continue for a longer period of time. In order to test this, we compared the two conditions directly. The difference in model-fit between the two conditions was significant at two clusters, an early cluster including the time-points around 280 ms to 395 ms (cluster p = 0.049) and a second cluster including the time-points from approximately 600 ms to 870 ms (cluster p = 0.002; <xref ref-type="fig" rid="F4">Fig 4B</xref>, <xref ref-type="table" rid="T1">Table 1</xref>). While cluster-based permutation testing does not allow precise estimation of effect on- and off-sets (Sassanhagen &amp; Draschkow, 2019), qualitatively the time-windows of these clusters overlap with both our effects of the congruency model RDM, and known congruency effects in the N300/N400 (e.g. <xref ref-type="bibr" rid="R13">Draschkow et al., 2018</xref>; <xref ref-type="bibr" rid="R16">Ganis &amp; Kutas, 2003</xref>; <xref ref-type="bibr" rid="R30">Lauer et al., 2018</xref>; <xref ref-type="bibr" rid="R32">Lauer et al., 2020</xref>; <xref ref-type="bibr" rid="R28">Mudrik et al., 2010</xref>; <xref ref-type="bibr" rid="R29">Mudrik et al., 2014</xref>; <xref ref-type="bibr" rid="R42">Võ &amp; Wolfe, 2013</xref>), in addition to a regularly reported later effect coinciding with the P600 (e.g. <xref ref-type="bibr" rid="R16">Ganis &amp; Kutas, 2003</xref>; <xref ref-type="bibr" rid="R28">Mudrik et al., 2010</xref>; <xref ref-type="bibr" rid="R38">Sauvé et al., 2017</xref>; <xref ref-type="bibr" rid="R42">Võ &amp; Wolfe, 2013</xref>).</p></sec><sec id="S11"><title>Correlation analysis</title><p id="P21">The model-based analysis revealed overlap between the congruency and the semantic model RDMs in the time-window from approximately 290 to 395 ms. Within this time-window, the congruency model successfully distinguished if an object was presented in congruent or incongruent context, and the semantic model displayed significantly better model fit with objects that were presented in an incongruent context compared to objects that were presented in a congruent context. In order to test if these two effects were related, we correlated the time courses of the congruency model fit with the time-course of the difference between the semantic model fit for the congruent and incongruent conditions. The results confirm a significant correlation (r = 0.44, p &lt; 0.001; <xref ref-type="fig" rid="F4">Fig 4C</xref>), demonstrating that the two analyses could be capturing the same temporal effect of congruency, which might suggest that effects of congruency are explained by differences in the processing of semantic object features.</p></sec></sec><sec id="S12" sec-type="discussion"><title>Discussion</title><p id="P22">In the current study, we directly tested if scene context influenced object recognition through the modulation of processing semantic object information. We related the similarity based on EEG activity in response to visual objects with both a model of congruency and a semantic model that was based on semantic object property norms. Both the congruency model and semantic model captured an effect of scene context on object processing in the time-window for which N300/N400 effects were previously reported (e.g. <xref ref-type="bibr" rid="R13">Draschkow et al., 2018</xref>; <xref ref-type="bibr" rid="R16">Ganis &amp; Kutas, 2003</xref>; <xref ref-type="bibr" rid="R30">Lauer et al., 2018</xref>; <xref ref-type="bibr" rid="R32">Lauer et al., 2020</xref>; <xref ref-type="bibr" rid="R28">Mudrik et al., 2010</xref>; <xref ref-type="bibr" rid="R29">Mudrik et al., 2014</xref>; <xref ref-type="bibr" rid="R42">Võ &amp; Wolfe, 2013</xref>). Additionally, the semantic model revealed a difference in processing of congruent and incongruent objects in a later time-window beyond ∼ 600 ms, which has been reported in some previous studies (e.g. <xref ref-type="bibr" rid="R16">Ganis &amp; Kutas, 2003</xref>; <xref ref-type="bibr" rid="R28">Mudrik et al., 2010</xref>; <xref ref-type="bibr" rid="R38">Sauvé et al, 2017</xref>. <xref ref-type="bibr" rid="R42">Võ &amp; Wolfe, 2013</xref>). In these two time-windows, the semantic model displayed stronger fit for incongruent than for congruent objects, suggesting that the previously observed congruency effects were driven by the additional need for semantic processing of objects that were incongruent with their environment. This contrasts with alternative possibilities that semantic object information could have been accessed faster for objects in congruent compared to incongruent environments, or that semantic access was initiated at the same time and for the same duration for both congruent and incongruent conditions.</p><p id="P23">Our research is the first to employ a modelling-based approach to directly test the hypothesis that scene context influences object recognition by modulating the processing of semantic object information. The stronger model fit for incongruent objects beyond about 275 ms suggests that while both congruent and incongruent objects involve semantic processing beyond ∼150 ms, semantic processes are extended in the incongruent condition. It may well be that this extended semantic processing for incongruent objects is what underpins the congruency effect, which seems to begin at a similar time to the divergence of semantic model fits across the two conditions.</p><p id="P24">Overall, our findings are in agreement with a framework whereby context generates expectations about objects we might encounter, and thereby affects the way objects are processed (<xref ref-type="bibr" rid="R3">Bar, 2004</xref>; <xref ref-type="bibr" rid="R33">Oliva &amp; Torralba, 2007</xref>; <xref ref-type="bibr" rid="R14">Federmeier et al., 2016</xref>; <xref ref-type="bibr" rid="R7">Clarke, 2019</xref>; <xref ref-type="bibr" rid="R31">Lauer &amp; Võ, 2022</xref>). Our results, together with previous findings of congruency effects on N300/N400 EEG components, show stronger effects for objects that were unexpected compared to objects that were expected. This phenomenon is consistent with the predictive coding account (<xref ref-type="bibr" rid="R15">Friston, 2005</xref>) which states that the brain constructs prior expectations about upcoming sensory events based on experience, and generates an error response if the event does not match the expectation. In terms of scene-object congruency, exposure to a scene context could create a prediction about what objects are likely to appear in that scene. This is even more so here, as a fixation dot appeared prior to the object indicating the location the item would appear, thus limiting the range of likely object candidates. If the object is not congruent with the scene, and hence does not fit the prediction, it triggers a prediction error response causing delayed or enhancement of brain activity that is related to object processing, like is seen for N300/N400 EEG components (e.g. <xref ref-type="bibr" rid="R13">Draschkow et al., 2018</xref>; <xref ref-type="bibr" rid="R16">Ganis &amp; Kutas, 2003</xref>; <xref ref-type="bibr" rid="R30">Lauer et al., 2018</xref>; <xref ref-type="bibr" rid="R32">Lauer et al., 2020</xref>; <xref ref-type="bibr" rid="R28">Mudrik et al., 2010</xref>; <xref ref-type="bibr" rid="R29">Mudrik et al., 2014</xref>; <xref ref-type="bibr" rid="R42">Võ &amp; Wolfe, 2013</xref>; for a review see <xref ref-type="bibr" rid="R31">Lauer &amp; Võ, 2022</xref>). Similar effects of context on object recognition have been reported not only for scenes but also for other types of prior information like the presence of other objects (<xref ref-type="bibr" rid="R1">Auckland et al., 2007</xref>; <xref ref-type="bibr" rid="R20">Kovalenko et al., 2012</xref>; <xref ref-type="bibr" rid="R27">McPherson &amp; Holcomb, 1999</xref>), and has been demonstrated in semantic priming studies (<xref ref-type="bibr" rid="R36">Renoult et al., 2012</xref>). This indicates that the semantic effects we see here reflect a more general mechanism that is not restricted to scenes, whereby the context activates semantic or schema-consistent information within which the semantics of a new item are to be integrated, thus allowing us to use semantic information from the world around us to predict what to expect - be this what objects are likely to appear around the corner, or what words might be next in a sentence (for reviews see <xref ref-type="bibr" rid="R23">Kutas &amp; Federmeier, 2011</xref>; <xref ref-type="bibr" rid="R14">Federmeier et al., 2016</xref>; <xref ref-type="bibr" rid="R41">Võ et al., 2019</xref>; <xref ref-type="bibr" rid="R40">Võ, 2021</xref>). Taken together, having a prior expectation about likely objects might allow for more efficient semantic processing, with the consequence that we see rapid and short semantic effects for congruent objects, and extended semantic effects for incongruent objects.</p><p id="P25">In their original work, <xref ref-type="bibr" rid="R13">Draschkow and colleagues (2018)</xref> demonstrated how congruency effects influencing the N300 and N400 components constitute highly related processes which allow the decoding of congruency across the two time-windows, finding significant cross-decoding of congruency from about 200 ms after object onset. The consistency model analysis that we employed, likewise tested to distinguish between objects that were presented either in congruent or incongruent context. Our results highlight a similar time-window like the decoding analysis, thus confirming that a model-based approach is suitable to capture congruency effects in EEG data.</p><p id="P26">In addition to the congruency effects in the N300/N400 time-window, the semantic model analysis revealed a difference in processing of congruent and incongruent objects in a later time-window beyond ∼600 ms. Effects in this time window were previously reported in similar studies, in which objects were embedded into scenes (e.g. <xref ref-type="bibr" rid="R16">Ganis &amp; Kutas, 2003</xref>; <xref ref-type="bibr" rid="R28">Mudrik et al., 2010</xref>; <xref ref-type="bibr" rid="R38">Sauvé et al., 2017</xref>; <xref ref-type="bibr" rid="R42">Võ &amp; Wolfe, 2013</xref>). However, the exact nature of these later effects remains unclear as they vary depending on task-demands (<xref ref-type="bibr" rid="R16">Ganis &amp; Kutas, 2003</xref>; <xref ref-type="bibr" rid="R42">Võ &amp; Wolfe, 2013</xref>; <xref ref-type="bibr" rid="R38">Sauvé et al., 2017</xref>). <xref ref-type="bibr" rid="R16">Ganis and Kutas (2003)</xref>, for example, reported two different effects in this time window - a stronger positivity for incongruent objects when the task was to identify the object, and a topographically distinct effect that was stronger for congruent objects when participants were additionally instructed to provide confidence ratings. In our study, participants were required to report if a scene-object combination had been shown previously, and we find a stronger representation of semantic information when the object was incongruent, consistent with the first effect found by <xref ref-type="bibr" rid="R16">Ganis &amp; Kutas (2003)</xref>. As such, it is unlikely that our late effect reflects the confidence of a decision-making process. <xref ref-type="bibr" rid="R42">Võ and Wolfe (2013)</xref> reported a P600 component specifically in the context of mild syntactic violations of an object’s position in scenes (object misplaced), but not for extreme syntactic violations (object in impossible position, for example in the air). One possible explanation for finding these effects in the current data is that objects were embedded in scenes, and this has likely induced a combination of semantic and mild syntactic violations in incongruent scene-object trials. For example, if a ball is embedded in a photo of a kitchen and is placed inside a microwave, then in addition to the semantic incongruency, a mild syntactic violation is also created. This combination of semantic and syntactic violations might explain RSA effects of semantic object processing in incongruent scenes during a similar time window to the previously reported P600 congruency effects.</p><p id="P27">In conclusion, our results revealed that while semantic processing begins around 150 ms after the object appears, the modulatory effect of the prior scene context starts around the onset of the N300 components, resulting in longer processing of objects that are incongruent with a scene compared to objects that are congruent. Additionally, we replicated effects in previously reported time-windows of the N300/N400 and P600 EEG components using a computational modelling approach. Importantly, our study highlights how object recognition processes are flexibly adapted based on prior information, in this case showing the dynamics associated with accessing semantic knowledge are modulated by the prior context.</p></sec></body><back><ack id="S13"><title>Acknowledgements</title><p id="P28">This research was funded in whole, or in part, by the Wellcome Trust [Grant number 211200/Z/18/Z to AC]. The Wellcome Centre for Integrative Neuroimaging is supported by core funding from the Wellcome Trust (203139/Z/16/Z), and M L-H Võ supported by the Hessisches Ministerium für Wissenschaft und Kunst (HMWK; project “The Adaptive Mind”). For the purpose of open access, the author has applied a CC BY public copyright licence to any Author Accepted Manuscript version arising from this submission.</p></ack><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Auckland</surname><given-names>ME</given-names></name><name><surname>Cave</surname><given-names>KR</given-names></name><name><surname>Donnelly</surname><given-names>N</given-names></name></person-group><article-title>Nontarget objects can influence perceptual processes during object recognition</article-title><source>Psychonomic Bulletin and Review</source><year>2007</year><volume>14</volume><issue>2</issue><fpage>332</fpage><lpage>337</lpage><pub-id pub-id-type="pmid">17694922</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bankson</surname><given-names>BB</given-names></name><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Groen</surname><given-names>IIA</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><article-title>The temporal evolution of conceptual object representations revealed through models of behavior, semantics and deep neural networks</article-title><source>NeuroImage</source><year>2018</year><volume>178</volume><fpage>172</fpage><lpage>182</lpage><pub-id pub-id-type="pmid">29777825</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar</surname><given-names>M</given-names></name></person-group><article-title>Visual objects in context</article-title><source>Nature Reviews Neuroscience</source><year>2004</year><volume>5</volume><issue>8</issue><fpage>617</fpage><lpage>629</lpage><pub-id pub-id-type="pmid">15263892</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biederman</surname><given-names>I</given-names></name><name><surname>Mezzanotte</surname><given-names>RJ</given-names></name><name><surname>Rabinowitz</surname><given-names>JC</given-names></name></person-group><article-title>Scene perception: Detecting and judging objects undergoing relational violations</article-title><source>Cognitive Psychology</source><year>1982</year><volume>14</volume><issue>2</issue><fpage>143</fpage><lpage>177</lpage><pub-id pub-id-type="pmid">7083801</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chan</surname><given-names>AM</given-names></name><name><surname>Baker</surname><given-names>JM</given-names></name><name><surname>Eskandar</surname><given-names>E</given-names></name><name><surname>Schomer</surname><given-names>D</given-names></name><name><surname>Ulbert</surname><given-names>I</given-names></name><name><surname>Marinkovic</surname><given-names>K</given-names></name><name><surname>Cash</surname><given-names>SS</given-names></name><name><surname>Halgren</surname><given-names>E</given-names></name></person-group><article-title>First-pass selectivity for semantic categories in human anteroventral temporal lobe</article-title><source>Journal of Neuroscience</source><year>2011</year><volume>31</volume><issue>49</issue><fpage>18119</fpage><lpage>18129</lpage><pub-id pub-id-type="pmcid">PMC3286838</pub-id><pub-id pub-id-type="pmid">22159123</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3122-11.2011</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Khosla</surname><given-names>A</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Torralba</surname><given-names>A</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><article-title>Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence</article-title><source>Scientific Reports</source><year>2016</year><volume>6</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="pmcid">PMC4901271</pub-id><pub-id pub-id-type="pmid">27282108</pub-id><pub-id pub-id-type="doi">10.1038/srep27755</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Clarke</surname><given-names>A</given-names></name></person-group><chapter-title>Neural dynamics of visual and semantic object processing</chapter-title><source>Psychology of Learning and Motivation - Advances in Research and Theory</source><edition>1st ed</edition><year>2019</year><volume>70</volume><pub-id pub-id-type="doi">10.1016/bs.plm.2019.03.002</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clarke</surname><given-names>A</given-names></name><name><surname>Devereux</surname><given-names>BJ</given-names></name><name><surname>Tyler</surname><given-names>LK</given-names></name></person-group><article-title>Oscillatory Dynamics of Perceptual to Conceptual Transformations in the Ventral Visual Pathway</article-title><source>Journal of Cognitive Neuroscience</source><year>2018</year><volume>10</volume><pub-id pub-id-type="pmid">30125217</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clarke</surname><given-names>A</given-names></name><name><surname>Taylor</surname><given-names>KI</given-names></name><name><surname>Tyler</surname><given-names>LK</given-names></name></person-group><article-title>The evolution of meaning: Spatio-temporal dynamics of visual object recognition</article-title><source>Journal of Cognitive Neuroscience</source><year>2011</year><volume>23</volume><issue>8</issue><fpage>1887</fpage><lpage>1899</lpage><pub-id pub-id-type="pmid">20617883</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devereux</surname><given-names>BJ</given-names></name><name><surname>Tyler</surname><given-names>LK</given-names></name><name><surname>Geertzen</surname><given-names>J</given-names></name><name><surname>Randall</surname><given-names>B</given-names></name></person-group><article-title>The Centre for Speech, Language and the Brain (CSLB) concept property norms</article-title><source>Behavior Research Methods</source><year>2014</year><volume>46</volume><issue>4</issue><fpage>1119</fpage><lpage>1127</lpage><pub-id pub-id-type="pmcid">PMC4237904</pub-id><pub-id pub-id-type="pmid">24356992</pub-id><pub-id pub-id-type="doi">10.3758/s13428-013-0420-4</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davenport</surname><given-names>JL</given-names></name><name><surname>Potter</surname><given-names>MC</given-names></name></person-group><article-title>Scene consistency in object and background perception</article-title><source>Psychological Science</source><year>2004</year><volume>15</volume><issue>8</issue><fpage>559</fpage><lpage>564</lpage><pub-id pub-id-type="pmid">15271002</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name><name><surname>Rust</surname><given-names>NC</given-names></name></person-group><article-title>How does the brain solve visual object recognition?</article-title><source>Neuron</source><year>2012</year><volume>73</volume><issue>3</issue><fpage>415</fpage><lpage>434</lpage><pub-id pub-id-type="pmcid">PMC3306444</pub-id><pub-id pub-id-type="pmid">22325196</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2012.01.010</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Draschkow</surname><given-names>D</given-names></name><name><surname>Heikel</surname><given-names>E</given-names></name><name><surname>Võ</surname><given-names>MLH</given-names></name><name><surname>Fiebach</surname><given-names>CJ</given-names></name><name><surname>Sassenhagen</surname><given-names>J</given-names></name></person-group><article-title>No evidence from MVPA for different processes underlying the N300 and N400 incongruity effects in object-scene processing</article-title><source>Neuropsychologia</source><year>2018</year><volume>120</volume><fpage>9</fpage><lpage>17</lpage><pub-id pub-id-type="pmid">30261162</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Federmeier</surname><given-names>KD</given-names></name><name><surname>Kutas</surname><given-names>M</given-names></name><name><surname>Dickson</surname><given-names>DS</given-names></name></person-group><chapter-title>A Common Neural Progression to Meaning in About a Third of a Second</chapter-title><source>Neurobiology of Language</source><publisher-name>Elsevier Inc</publisher-name><year>2016</year><pub-id pub-id-type="doi">10.1016/B978-0-12-407794-2.00045-6</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K</given-names></name></person-group><article-title>A theory of cortical responses</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><year>2005</year><volume>360</volume><issue>1456</issue><fpage>815</fpage><lpage>836</lpage><pub-id pub-id-type="pmcid">PMC1569488</pub-id><pub-id pub-id-type="pmid">15937014</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2005.1622</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ganis</surname><given-names>G</given-names></name><name><surname>Kutas</surname><given-names>M</given-names></name></person-group><article-title>An electrophysiological study of scene effects on object identification</article-title><source>Cognitive Brain Research</source><year>2003</year><volume>16</volume><issue>2</issue><fpage>123</fpage><lpage>144</lpage><pub-id pub-id-type="pmid">12668221</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ganis</surname><given-names>G</given-names></name><name><surname>Kutas</surname><given-names>M</given-names></name><name><surname>Sereno</surname><given-names>MI</given-names></name></person-group><article-title>The Search for “Common Sense”: An Electrophysiological Study of the Comprehension of Words and Pictures in Reading</article-title><source>Journal of Cognitive Neuroscience</source><year>1996</year><volume>8</volume><issue>2</issue><fpage>89</fpage><lpage>106</lpage><pub-id pub-id-type="pmid">23971417</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greene</surname><given-names>MR</given-names></name><name><surname>Botros</surname><given-names>AP</given-names></name><name><surname>Beck</surname><given-names>DM</given-names></name><name><surname>Fei-Fei</surname><given-names>L</given-names></name></person-group><article-title>What you see is what you expect: rapid scene understanding benefits from prior experience</article-title><source>Attention, Perception, and Psychophysics</source><year>2015</year><volume>77</volume><issue>4</issue><fpage>1239</fpage><lpage>1251</lpage><pub-id pub-id-type="pmid">25776799</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kietzmann</surname><given-names>TC</given-names></name><name><surname>Spoerer</surname><given-names>CJ</given-names></name><name><surname>Sörensen</surname><given-names>LKA</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Hauk</surname><given-names>O</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Recurrence is required to capture the representational dynamics of the human visual system</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2019</year><volume>116</volume><issue>43</issue><fpage>21854</fpage><lpage>21863</lpage><pub-id pub-id-type="pmcid">PMC6815174</pub-id><pub-id pub-id-type="pmid">31591217</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1905544116</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kovalenko</surname><given-names>LY</given-names></name><name><surname>Chaumon</surname><given-names>M</given-names></name><name><surname>Busch</surname><given-names>NA</given-names></name></person-group><article-title>A pool of pairs of related objects (POPORO) for investigating visual semantic integration: Behavioral and electrophysiological validation</article-title><source>Brain Topography</source><year>2012</year><volume>25</volume><issue>3</issue><fpage>272</fpage><lpage>284</lpage><pub-id pub-id-type="pmid">22218845</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Representational similarity analysis – connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><year>2008</year><volume>2</volume><fpage>1</fpage><lpage>28</lpage><pub-id pub-id-type="pmcid">PMC2605405</pub-id><pub-id pub-id-type="pmid">19104670</pub-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumar</surname><given-names>M</given-names></name><name><surname>Federmeier</surname><given-names>KD</given-names></name><name><surname>Beck</surname><given-names>DM</given-names></name></person-group><article-title>The N300: An Index for Predictive Coding of Complex Visual Objects and Scenes</article-title><source>Cerebral Cortex Communications</source><year>2021</year><volume>2</volume><issue>2</issue><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="pmcid">PMC8171016</pub-id><pub-id pub-id-type="pmid">34296175</pub-id><pub-id pub-id-type="doi">10.1093/texcom/tgab030</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kutas</surname><given-names>M</given-names></name><name><surname>Federmeier</surname><given-names>KD</given-names></name></person-group><article-title>Thirty years and counting: Finding meaning in the N400 component of the event-related brain potential (ERP)</article-title><source>Annual Review of Psychology</source><year>2011</year><volume>62</volume><fpage>621</fpage><lpage>647</lpage><pub-id pub-id-type="pmcid">PMC4052444</pub-id><pub-id pub-id-type="pmid">20809790</pub-id><pub-id pub-id-type="doi">10.1146/annurev.psych.093008.131123</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kutas</surname><given-names>M</given-names></name><name><surname>Hillyard</surname><given-names>SA</given-names></name></person-group><article-title>Reading senseless sentences: Brain potentials reflect semantic incongruity</article-title><source>Science</source><year>1980</year><volume>207</volume><fpage>203</fpage><lpage>205</lpage><pub-id pub-id-type="pmid">7350657</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lamme</surname><given-names>VAF</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><article-title>The distinct modes of vision offered by feedforward and recurrent processing</article-title><source>Trends Neurosci</source><year>2000</year><volume>23</volume><fpage>571</fpage><lpage>579</lpage><pub-id pub-id-type="pmid">11074267</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><article-title>Nonparametric statistical testing of EEG- and MEG data</article-title><source>Journal of Neuroscience Methods</source><year>2007</year><volume>164</volume><issue>1</issue><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="pmid">17517438</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McPherson</surname><given-names>WB</given-names></name><name><surname>Holcomb</surname><given-names>PJ</given-names></name></person-group><article-title>An electrophysiological investigation of semantic priming with pictures of real objects</article-title><source>Psychophysiology</source><year>1999</year><volume>36</volume><issue>1</issue><fpage>53</fpage><lpage>65</lpage><pub-id pub-id-type="pmid">10098380</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mudrik</surname><given-names>L</given-names></name><name><surname>Lamy</surname><given-names>D</given-names></name><name><surname>Deouell</surname><given-names>LY</given-names></name></person-group><article-title>ERP evidence for context congruity effects during simultaneous object-scene processing</article-title><source>Neuropsychologia</source><year>2010</year><volume>48</volume><issue>2</issue><fpage>507</fpage><lpage>517</lpage><pub-id pub-id-type="pmid">19837103</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mudrik</surname><given-names>L</given-names></name><name><surname>Shalgi</surname><given-names>S</given-names></name><name><surname>Lamy</surname><given-names>D</given-names></name><name><surname>Deouell</surname><given-names>LY</given-names></name></person-group><article-title>Synchronous contextual irregularities affect early scene processing: Replication and extension</article-title><source>Neuropsychologia</source><year>2014</year><volume>56</volume><issue>1</issue><fpage>447</fpage><lpage>458</lpage><pub-id pub-id-type="pmid">24593900</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lauer</surname><given-names>T</given-names></name><name><surname>Cornelissen</surname><given-names>THW</given-names></name><name><surname>Draschkow</surname><given-names>D</given-names></name><name><surname>Willenbockel</surname><given-names>V</given-names></name><name><surname>Võ</surname><given-names>MLH</given-names></name></person-group><article-title>The role of scene summary statistics in object recognition</article-title><source>Scientific Reports</source><year>2018</year><volume>8</volume><issue>1</issue><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="pmcid">PMC6168578</pub-id><pub-id pub-id-type="pmid">30279431</pub-id><pub-id pub-id-type="doi">10.1038/s41598-018-32991-1</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lauer</surname><given-names>T</given-names></name><name><surname>Võ</surname><given-names>MLH</given-names></name></person-group><chapter-title>The Ingredients of Scenes that Affect Object Search and Perception</chapter-title><person-group person-group-type="editor"><name><surname>Ionescu</surname><given-names>B</given-names></name><name><surname>Bainbridge</surname><given-names>WA</given-names></name><name><surname>Murray</surname><given-names>N</given-names></name></person-group><source>Human Perception of Visual Information</source><publisher-name>Springer</publisher-name><year>2022</year><fpage>1</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-81465-6_1</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lauer</surname><given-names>T</given-names></name><name><surname>Willenbockel</surname><given-names>V</given-names></name><name><surname>Maffongelli</surname><given-names>L</given-names></name><name><surname>Võ</surname><given-names>MLH</given-names></name></person-group><article-title>The influence of scene and object orientation on the scene consistency effect</article-title><source>Behavioural Brain Research</source><year>2020</year><volume>394</volume><pub-id pub-id-type="pmid">32682913</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oliva</surname><given-names>A</given-names></name><name><surname>Torralba</surname><given-names>A</given-names></name></person-group><article-title>The role of context in object recognition</article-title><source>Trends in Cognitive Sciences</source><year>2007</year><volume>11</volume><issue>12</issue><fpage>520</fpage><lpage>527</lpage><pub-id pub-id-type="pmid">18024143</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palmer</surname><given-names>SE</given-names></name></person-group><article-title>The effects of contextual scenes on the identification of objects</article-title><source>Memory &amp; Cognition</source><year>1975</year><volume>3</volume><issue>5</issue><fpage>519</fpage><lpage>526</lpage><pub-id pub-id-type="pmid">24203874</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poch</surname><given-names>C</given-names></name><name><surname>Garrido</surname><given-names>MI</given-names></name><name><surname>Igoa</surname><given-names>JM</given-names></name><name><surname>Belinchón</surname><given-names>M</given-names></name><name><surname>García-Morales</surname><given-names>I</given-names></name><name><surname>Campo</surname><given-names>P</given-names></name></person-group><article-title>Time-varying effective connectivity during visual object naming as a function of semantic demands</article-title><source>Journal of Neuroscience</source><year>2015</year><volume>35</volume><issue>23</issue><fpage>8768</fpage><lpage>8776</lpage><pub-id pub-id-type="pmcid">PMC6605208</pub-id><pub-id pub-id-type="pmid">26063911</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4888-14.2015</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Renoult</surname><given-names>L</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Calcagno</surname><given-names>V</given-names></name><name><surname>Prévost</surname><given-names>M</given-names></name><name><surname>Debruille</surname><given-names>JB</given-names></name></person-group><article-title>From N400 to N300: Variations in the timing of semantic processing with repetition</article-title><source>NeuroImage</source><year>2012</year><volume>61</volume><issue>1</issue><fpage>206</fpage><lpage>215</lpage><pub-id pub-id-type="pmid">22406358</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sassenhagen</surname><given-names>J</given-names></name><name><surname>Draschkow</surname><given-names>D</given-names></name></person-group><article-title>Cluster-based permutation tests of MEG/EEG data do not establish significance of effect latency or location</article-title><source>Psychophysiology</source><year>2019</year><volume>56</volume><issue>6</issue><elocation-id>e13335</elocation-id><pub-id pub-id-type="pmid">30657176</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sauvé</surname><given-names>G</given-names></name><name><surname>Harmand</surname><given-names>M</given-names></name><name><surname>Vanni</surname><given-names>L</given-names></name><name><surname>Brodeur</surname><given-names>MB</given-names></name></person-group><article-title>The probability of object–scene co-occurrence influences object identification processes</article-title><source>Experimental Brain Research</source><year>2017</year><volume>235</volume><issue>7</issue><fpage>2167</fpage><lpage>2179</lpage><pub-id pub-id-type="pmid">28432384</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Truman</surname><given-names>A</given-names></name><name><surname>Mudrik</surname><given-names>L</given-names></name></person-group><article-title>Are incongruent objects harder to identify? The functional significance of the N300 component</article-title><source>Neuropsychologia</source><year>2018</year><volume>117</volume><fpage>222</fpage><lpage>232</lpage><pub-id pub-id-type="pmid">29885960</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Võ</surname><given-names>MLH</given-names></name></person-group><article-title>The meaning and structure of scenes</article-title><source>Vision Research, 181</source><year>2021</year><volume>181</volume><fpage>10</fpage><lpage>20</lpage><date-in-citation>August 2019</date-in-citation><pub-id pub-id-type="pmid">33429218</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Võ</surname><given-names>MLH</given-names></name><name><surname>Boettcher</surname><given-names>SE</given-names></name><name><surname>Draschkow</surname><given-names>D</given-names></name></person-group><article-title>Reading scenes: how scene grammar guides attention and aids perception in real-world environments</article-title><source>Current Opinion in Psychology</source><year>2019</year><volume>29</volume><fpage>205</fpage><lpage>210</lpage><pub-id pub-id-type="pmid">31051430</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Võ</surname><given-names>MLH</given-names></name><name><surname>Wolfe</surname><given-names>JM</given-names></name></person-group><article-title>Differential Electrophysiological Signatures of Semantic and Syntactic Scene Processing</article-title><source>Psychological Science</source><year>2013</year><volume>24</volume><issue>9</issue><fpage>1816</fpage><lpage>1823</lpage><pub-id pub-id-type="pmcid">PMC4838599</pub-id><pub-id pub-id-type="pmid">23842954</pub-id><pub-id pub-id-type="doi">10.1177/0956797613476955</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><p>An example trial showing a scene before a red dot appears to indicate the location the object will appear. The object that appeared could either be congruent with the scene, in this example a cushion, or incongruent with the scene, in this example a chopping board. Each scene is only shown once to a participant, with either a congruent or incongruent object.</p></caption><graphic xlink:href="EMS156297-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>An overview of Representation Similarity Analysis (RSA) that relates brain responses at each time point to the congruency and semantic information associated with the different trials.</title><p>(A) Example stimuli showing objects in congruent and incongruent scenes. (B) Model RDMs for an example participant. For the congruency model analysis all congruent object trials were each assigned a value of 1 and all incongruent object trials each a value of 0. Then the Euclidean distance between all object-pairs was calculated. The resulting RDM directly dissociates congruent and incongruent objects. For the semantic model analysis, the congruent and incongruent trials were analysed separately. Each object for which independently normed semantic features were available was assigned a corresponding semantic feature vector. Then the cosine distance was calculated between the feature vectors of all object-pairs, separately for consistent and inconsistent trials, resulting in two RDMs that describe the similarity of objects based on semantic properties. (C) The model RDMs are then statistically related to brain signals. For each object, the EEG response was extracted across all channels, and at each time-point the similarity between object pairs was calculated using correlation distance. Model RDMs were then related to these EEG RDMs using Spearman correlation.</p></caption><graphic xlink:href="EMS156297-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Model RDMs tested and EEG RDMs from an example participant at different time points.</title></caption><graphic xlink:href="EMS156297-f003"/></fig><fig id="F4" position="float"><label>Figure 3</label><caption><title>RSA results.</title><p>(A) The consistency model fit shows the similarity based on Spearman correlation between the model RDM and the EEG RDMs at each time-point. Shaded area shows +- 1 standard error of the mean. The horizontal bar shows a statistically significant cluster. (B) The semantic model fit is shown separately for congruent (grey line) and incongruent (black line) conditions. The horizontal bars show statistically significant clusters for incongruent objects (black solid line) and the difference of semantic model fit between congruent and incongruent objects (black dotted line). (C) Correlational analysis relating the congruency model to the difference of semantic model fit between congruent and incongruent objects.</p></caption><graphic xlink:href="EMS156297-f004"/></fig><table-wrap id="T1" position="float" orientation="portrait"><label>Table 1</label><caption><title>EEG RSA effects</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Model RDM</th><th align="center" valign="top">time-window</th><th align="center" valign="top">cluster-mass</th><th align="center" valign="top">cluster p-value</th></tr></thead><tbody><tr><td align="left" valign="top">Congruency model</td><td align="center" valign="top">286-450 ms</td><td align="center" valign="top">74.26</td><td align="center" valign="top">p = .022</td></tr><tr><td align="left" valign="top">Semantic model: consistent</td><td align="center" valign="top">141-235 ms</td><td align="center" valign="top">46.11</td><td align="center" valign="top">p = .064</td></tr><tr><td align="left" valign="top">Semantic model: inconsistent</td><td align="center" valign="top">141-360 ms</td><td align="center" valign="top">132.25</td><td align="center" valign="top">p = .005</td></tr><tr><td align="left" valign="top">Semantic model: inconsistent</td><td align="center" valign="top">616-765 ms</td><td align="center" valign="top">80.78</td><td align="center" valign="top">p = .021</td></tr><tr><td align="left" valign="top">Semantic model: incon &gt; con</td><td align="center" valign="top">276-395 ms</td><td align="center" valign="top">50.25</td><td align="center" valign="top">p = .049</td></tr><tr><td align="left" valign="top">Semantic model: incon &gt; con</td><td align="center" valign="top">596-870 ms</td><td align="center" valign="top">169.26</td><td align="center" valign="top">p = .002</td></tr></tbody></table></table-wrap></floats-group></article>