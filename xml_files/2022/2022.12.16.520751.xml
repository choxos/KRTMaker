<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS158947</article-id><article-id pub-id-type="doi">10.1101/2022.12.16.520751</article-id><article-id pub-id-type="archive">PPR586883</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Adaptive tuning of human learning and choice variability to unexpected uncertainty</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Lee</surname><given-names>Junseok K.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Rouault</surname><given-names>Marion</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Wyart</surname><given-names>Valentin</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>Laboratoire de Neurosciences Cognitives et Computationnelles, Institut National de la Santé et de la Recherche Médicale (Inserm), Paris, France</aff><aff id="A2"><label>2</label>Département d’Études Cognitives, Ecole Normale Supérieure, Université PSL, Paris, France</aff><author-notes><corresp id="CR1">Corresponding authors: <email>jun.seok.lee@ens.fr</email>, <email>valentin.wyart@inserm.fr</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>23</day><month>12</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>19</day><month>12</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nd/4.0/">CC BY-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Human value-based decisions are strikingly variable under uncertainty. This variability is known to arise from two distinct sources: variable choices aimed at exploring available options, and imprecise learning of option values due to limited cognitive resources. However, whether these two sources of decision variability are tuned to their specific costs and benefits remains unclear. To address this question, we compared the effects of expected and unexpected uncertainty on decision-making in the same reinforcement learning task. Across two large behavioral datasets, we found that humans choose more variably between options but simultaneously learn less imprecisely their values in response to unexpected uncertainty. Using simulations of learning agents, we demonstrate that these opposite adjustments reflect adaptive tuning of exploration and learning precision to the structure of uncertainty. Together, these findings indicate that humans regulate not only how much they explore uncertain options, but also how precisely they learn the values of these options.</p></abstract><kwd-group><kwd>reinforcement learning</kwd><kwd>exploration</kwd><kwd>cognitive control</kwd><kwd>computational modeling</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Human decisions exhibit a pervasive variability under uncertainty (<xref ref-type="bibr" rid="R1">1</xref>). In the context of value-based decisions, the source of this variability has classically been assigned to exploration (<xref ref-type="bibr" rid="R2">2</xref>–<xref ref-type="bibr" rid="R5">5</xref>) – i.e., purposeful bias and variance in choice policy aimed at reducing uncertainty about the values of choice options. Beside this well-described source of decision variability, recent work has shown that the computations used to learn option values from obtained rewards suffer from imprecisions due to limited cognitive resources (<xref ref-type="bibr" rid="R6">6</xref>–<xref ref-type="bibr" rid="R9">9</xref>). Learning imprecisions result in decision variability which can be mistaken for exploration, but these two sources of decision variability are dissimilar in nature. Exploration drives decision variability through the probabilistic selection of options which do not maximize expected value, whereas learning imprecisions reflect random noise in the reinforcement learning process that updates option values. In practice, these differences allow decomposing human decision variability into two separate sources through detailed computational modeling of human behavior (<xref ref-type="bibr" rid="R6">6</xref>).</p><p id="P3">Both exploration and imprecise computations entail significant reward costs. First, by selecting an option that does not maximize expected value, exploration temporarily foregoes the exploitation of the best available option (<xref ref-type="bibr" rid="R10">10</xref>). Second, relying on imprecise computations means that the option with the highest subjective value is less likely to be the objectively best option available. Despite these similar reward costs, the two sources of decision variability have very different cognitive benefits. Exploration reduces uncertainty about option values, whereas imprecise learning reduces demands in terms of cognitive and neural resources (<xref ref-type="bibr" rid="R11">11</xref>–<xref ref-type="bibr" rid="R13">13</xref>). It is well known that humans arbitrate the ‘explore-exploit’ trade-off under uncertainty in terms of its costs and benefits (<xref ref-type="bibr" rid="R3">3</xref>, <xref ref-type="bibr" rid="R4">4</xref>). However, these findings have been obtained using reinforcement learning models which assign all decision variability to exploration. Whether humans simultaneously regulate learning imprecisions in terms of their specific costs and benefits remains unknown.</p><p id="P4">Importantly, the costs and benefits of exploration and learning imprecisions depend on the dominant form of uncertainty in the environment: expected vs. unexpected uncertainty (<xref ref-type="bibr" rid="R14">14</xref>, <xref ref-type="bibr" rid="R15">15</xref>). Expected uncertainty refers to random stochasticity of rewards associated with a choice option around a constant mean value. By contrast, unexpected uncertainty refers to changes in the mean value of rewards associated with a choice option. Under expected uncertainty (i.e., reward stochasticity), individual rewards become less informative about their mean value as learning progresses, and agents can therefore tolerate low learning rates and little exploration. By contrast, under unexpected uncertainty (i.e., reward volatility), individual rewards are highly informative about changes in their mean value, and agents should therefore maintain high learning rates and frequent exploration of unobserved choice alternatives. There is ample experimental evidence that humans adjust their learning rates and exploration at short timescales depending on the dominant form of uncertainty in their environment (<xref ref-type="bibr" rid="R16">16</xref>–<xref ref-type="bibr" rid="R19">19</xref>). However, whether this regulation of learning rates and exploration is accompanied by a modulation of learning imprecisions remains unknown.</p><p id="P5">Here, we developed a novel experimental framework in which the effects of expected and unexpected uncertainty on decision variability can be compared in the context of the same reinforcement learning task (<xref ref-type="fig" rid="F1">Figure 1AB</xref>; see <xref ref-type="sec" rid="S10">Materials and Methods</xref>). We decomposed the decision variability of two large samples of human participants (a ‘discovery’ dataset and a ‘replication’ dataset; see <xref ref-type="sec" rid="S10">Materials and Methods</xref>) into exploration and learning imprecisions, by fitting a noisy reinforcement learning model to their behavior. We found that participants choose more variably between options but learn more precisely their values in response to unexpected uncertainty. By studying individual differences in these opposite adjustments, we show that humans regulate the variability of their decisions based on not one, but two cost-benefit trade-offs.</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>Two-armed bandit task design and performance</title><p id="P6">Adult participants (<italic>N =</italic> 200 per dataset; see <xref ref-type="sec" rid="S10">Materials and Methods</xref>) played a two-armed bandit task in three conditions (<xref ref-type="fig" rid="F1">Figure 1B</xref>). The ‘reference’ (Ref) condition consists of short rounds of trials using two choice options associated with reward distributions of fixed means and variances. The ‘stochastic’ (S+) condition differs from the reference condition in that reward distributions have larger variances – i.e., increased expected uncertainty. The ‘volatile’ (V+) condition differs from the reference condition in that it consists of long rounds of trials with reward distributions of changing means – i.e., unexpected uncertainty. Participants were instructed of the structure of uncertainty and relative difficulty of each condition (see <xref ref-type="sec" rid="S10">Materials and Methods</xref>). We describe below the analysis of the first, ‘discovery’ dataset of adult participants (<italic>N</italic> = 154 after exclusion; see <xref ref-type="sec" rid="S10">Materials and Methods</xref>) and point toward the replication of observed effects in the second, ‘replication’ dataset (<italic>N</italic> = 142 after exclusion; see <xref ref-type="sec" rid="S10">Materials and Methods</xref>).</p><p id="P7">As expected, participants were more accurate at choosing the option associated with the highest reward mean in the reference condition than in the other two conditions with increased uncertainty (<xref ref-type="fig" rid="F1">Figure 1C</xref>; signed-rank test, S+ minus Ref: <italic>z</italic> = -7.1, <italic>p</italic> &lt;; 0.001, V+ minus Ref: <italic>z</italic> = -10.2, <italic>p</italic> &lt; 0.001). Participants also switched more often between choice options in the stochastic and volatile conditions than in the reference condition (<xref ref-type="fig" rid="F1">Figure 1D</xref>; S+ minus Ref: <italic>z</italic> = 3.9, <italic>p</italic> &lt; 0.001, V+ minus Ref: <italic>z</italic> = 8.0, <italic>p</italic> &lt; 0.001). Comparing the two conditions with increased uncertainty showed that participants switched more often between choice options in the volatile condition (V+ minus S+: <italic>z</italic> = 5.3, <italic>p</italic> &lt; 0.001). These differences between conditions were replicated in the second dataset (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 1</xref>).</p></sec><sec id="S4"><title>Computational model specification</title><p id="P8">We first sought to compare human decisions to those of an optimal learning agent in the same task conditions. For this purpose, we derived a Kalman filter whose parameters were set to the generative values of each task condition (see <xref ref-type="sec" rid="S10">Materials and Methods</xref>). The optimal learning agent selected on each trial of each round the option associated with the highest estimated value (reward mean). Simulating the behavior of the optimal learning agent confirmed that human decisions were substantially less accurate and more variable than those of the optimal learning agent in all three conditions (<xref ref-type="fig" rid="F1">Figure 1CD</xref>). These differences between human and optimal decisions were replicated in the second dataset (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 1</xref>).</p><p id="P9">To capture the suboptimal variability of human decisions, we altered the optimal learning agent with four free parameters (<xref ref-type="fig" rid="F2">Figure 2AB</xref>; see <xref ref-type="sec" rid="S10">Materials and Methods</xref>). First, a learning rate <italic>α</italic> controls how much estimated option values are updated following each obtained reward. Unlike reinforcement learning (RL) models, the learning rate <italic>α</italic> of a Kalman filter reflects the dominant form of uncertainty assumed by the learning agent, and the effective magnitude of updates varies from trial to trial as a function of uncertainty regarding the current value of the chosen option (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Text</xref>). Second, a decay rate <italic>3</italic> controls the rate at which the value of the unchosen option regresses toward its prior value, reflecting a decay of unchosen option values in working memory. Third, a learning noise term triggers imprecise updates of estimated option values, controlled by a scaling factor ζ. As in recent work (<xref ref-type="bibr" rid="R6">6</xref>), we hypothesized that learning imprecisions follow Weber’s law, and scale with the magnitude of associated reward prediction errors. And fourth, a choice temperature τ generates exploration through a ‘softmax’ choice policy (<xref ref-type="bibr" rid="R3">3</xref>, <xref ref-type="bibr" rid="R4">4</xref>). Before analyzing fits of this suboptimal learning agent to human decisions, we performed model and parameter recovery analyses (<xref ref-type="bibr" rid="R20">20</xref>, <xref ref-type="bibr" rid="R21">21</xref>) to validate that our fitting procedure was capable of identifying the source(s) of decision variability from choice behavior (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 2</xref>; see <xref ref-type="sec" rid="S10">Materials and Methods</xref>). We also verified that the main findings are robust to the use of a reinforcement learning (RL) model instead of a Kalman filter to fit human decisions (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Text</xref>).</p></sec><sec id="S5"><title>Model parameter fits in stochastic and volatile conditions</title><p id="P10">We fitted the parameters of the suboptimal learning agent to each participant’s behavior in each condition using approximate Bayesian inference (see <xref ref-type="sec" rid="S10">Materials and Methods</xref>). We performed Bayesian model selection to compare the suboptimal learning agent including the two sources of decision variability (learning imprecisions and exploration) with variants including a single source (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 3</xref>). We found that both sources of decision variability are needed to fit participants’ behavior (exceedance <italic>p</italic> &gt; 0.999 in each condition). In line with recent work, we validated that learning imprecisions scale with the magnitude of updates following each outcome (comparison between flat and update-scaled imprecisions, exceedance <italic>p</italic> &gt; 0.999 in each condition). We found also evidence for a significant memory decay of unchosen option values in all three conditions (comparison between <italic>δ</italic> &gt; 0 and <italic>δ</italic> = 0, exceedance <italic>p</italic> &gt; 0.999 in each condition).</p><p id="P11">As expected, participants had higher learning rates in the volatile condition than in the other two conditions (<xref ref-type="fig" rid="F3">Figure 3A</xref>; V+ against Ref: <italic>z</italic> = 10.2, <italic>p</italic> &lt; 0.001, V+ against S+: <italic>z</italic> = 9.8, <italic>p</italic> &lt; 0.001). Regarding decision variability, participants had lower learning noise in the volatile condition (V+ against Ref: <italic>z</italic> = -2.8, <italic>p</italic> = 0.006, V+ against S+: <italic>z</italic> = -4.4, <italic>p</italic> &lt; 0.001), as well as higher choice temperature (V+ against Ref: <italic>z</italic> = 4.4, <italic>p</italic> &lt; 0.001, V+ against S+: <italic>z</italic> = 4.4, <italic>p</italic> &lt; 0.001). Importantly, we found no statistically significant difference between the reference and stochastic conditions in terms of any model parameter (learning rate: <italic>p</italic> = 0.437 decay rate: <italic>p</italic> = 0.479, learning noise: <italic>p</italic> = 0.100, choice temperature: <italic>p</italic> = 0.995). Unlike its optimal counterpart, this suboptimal learning agent was able to capture participants’ behavior in terms of accuracy and switch rate, as well as the trajectories of these quantities over the course of each round (<xref ref-type="fig" rid="F3">Figure 3BC</xref>). These effects were replicated in the second dataset (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 1</xref>).</p><p id="P12">Learning noise and choice temperature showed large individual differences in each condition. We took advantage of these differences to study how each parameter impacts decision variability. We correlated each parameter with decision accuracy and switch rate across participants (<xref ref-type="fig" rid="F4">Figure 4A</xref>). Participants’ accuracy showed significant negative correlations with learning imprecisions and exploration (rank correlation, learning noise: <italic>ρ</italic> = -0.18, <italic>p</italic> &lt; 0.001, choice temperature: <italic>ρ</italic> = -0.38, <italic>p</italic> &lt; 0.001). By contrast, participants’ switch rates showed a negative relation with learning noise (<italic>p</italic> = -0.14, <italic>p</italic> = 0.002), but a strong positive relation with choice temperature (<italic>ρ</italic> = 0.71, <italic>p</italic> &lt; 0.001). This means that unlike exploration, learning imprecision did not make participants switch more between the two choice options. Median splits of participants’ choice behavior as a function of learning noise (<xref ref-type="fig" rid="F4">Figure 4B</xref>) or choice temperature (<xref ref-type="fig" rid="F4">Figure 4C</xref>) confirmed these effects, which were replicated in the second dataset (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 3</xref>).</p></sec><sec id="S6"><title>Model parameter covariations across participants</title><p id="P13">We studied how the large individual differences in decision variability are shared between model parameters and across conditions. For this purpose, we computed the correlation matrix of model parameters across participants (12 parameters = 4 parameters per condition; see <xref ref-type="sec" rid="S10">Materials and Methods</xref>). This matrix revealed significant covariations between model parameters and conditions (<xref ref-type="fig" rid="F5">Figure 5A</xref>). First, learning rate, learning noise and choice temperature all showed significant within-parameter correlations between the reference and the stochastic or volatile conditions (<xref ref-type="fig" rid="F5">Figure 5C</xref>). Furthermore, learning rate correlated negatively with learning noise (<xref ref-type="fig" rid="F5">Figure 5D</xref>; rank correlation, <italic>ρ</italic> = -0.36, <italic>p</italic> &lt; 0.001), but positively with choice temperature (<italic>ρ</italic> = 0.43, <italic>p</italic> &lt; 0.001). We verified that these between-parameter correlations remained significant within each condition, and performed additional recovery analyses to validate that neither reflects spurious correlations arising from the fitting procedure (<xref ref-type="fig" rid="F5">Figure 5B</xref>; see <xref ref-type="sec" rid="S10">Materials and Methods</xref>). These correlations were also replicated in the second dataset (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figures 4 and 5</xref>).</p><p id="P14">The positive association of learning noise and learning rate as well as the negative association of choice temperature and learning rate across participants mirror their within-participant adjustments across task conditions. Indeed, the decrease in learning noise and the increase in choice temperature are associated with higher learning rate in the volatile condition. But are these simultaneous adjustments of decision variability distinct or tied to each other? To answer this question, we first measured the partial correlation between learning noise and choice temperature once the relation of these two quantities with learning rate are partialled out. This analysis indicated no direct relation between the two quantities (partial rank correlation, <italic>ρ</italic> = 0.03, <italic>p</italic> = 0.584). We then measured the relation between adjustments of each source of variability with adjustments of learning rate between conditions. We found that the decrease in learning noise in the volatile condition (V+ minus Ref) correlates with the increase in learning rate (<italic>ρ</italic> = -0.22, <italic>p</italic> = 0.007), but not with the increase in choice temperature (<italic>ρ</italic> = -0.03, <italic>p</italic> = 0.739). Together, these results indicate that the opposite adjustments of learning noise and choice temperature to volatility are independent of each other.</p></sec><sec id="S7"><title>Principal component analysis of model parameters</title><p id="P15">To provide additional support for independent adjustments of learning imprecision and exploration to volatility, we performed a principal component analysis (PCA) of model parameters (see <xref ref-type="sec" rid="S10">Materials and Methods</xref>). By construction, principal components (PCs) are orthogonal to each other and reflect uncorrelated sources of variability in model parameters. We therefore asked whether learning noise and choice temperature project onto distinct PCs. This analysis revealed two PCs (PC1 and PC2) that capture covariance in model parameters better than chance (<xref ref-type="fig" rid="F6">Figure 6A</xref>).</p><p id="P16">PC1 reflected the positive correlation between learning rate and choice temperature, and explained more variance in model parameters in the reference (Ref) and stochastic (S+) conditions (<xref ref-type="fig" rid="F6">Figure 6B</xref>, top). Importantly, PC1 explained significantly more variance in choice temperature than learning noise in all three conditions (Ref: bootstrapped <italic>p</italic> = 0.992, S+: bootstrapped <italic>p</italic> = 0.983, V+: bootstrapped <italic>p</italic> = 0.963). Behaviorally, high PC1 scores were associated with lower accuracy and higher switch rates (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Text</xref>). By contrast, PC2 (<xref ref-type="fig" rid="F6">Figure 6B</xref>, bottom) reflected the negative correlation between learning rate and learning noise, and explained more variance in model parameters in the volatile (V+) condition. Importantly, PC2 explained significantly more variance in learning noise than choice temperature in all conditions (Ref: bootstrapped <italic>p</italic> = 0.988, S+: bootstrapped <italic>p</italic> = 0.934, V+: bootstrapped <italic>p</italic> = 0.989). Behaviorally, PC2 showed a milder relation to accuracy and no significant relation to switch rate (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Text</xref>). These different effects were replicated in the second dataset (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 6</xref>). The covariance structure of model parameters extracted using PCA confirms that the simultaneous adjustments of learning noise and choice temperature to volatility are largely independent of each other.</p></sec><sec id="S8"><title>Adaptive regulation of learning and choice variability to uncertainty</title><p id="P17">Participants choose more variably but learn less imprecisely in the volatile condition as compared to the other two conditions. But do these opposite adjustments of decision variability reflect the changing costs of learning imprecisions and exploration across conditions? To address this question, we performed theoretical simulations of noisy reinforcement learning agents to measure the reward costs of learning imprecisions and exploration in each condition.</p><p id="P18">We computed the marginal reward costs associated with each source of decision variability through simulations of the suboptimal learning agent. We selectively varied the associated model parameter (either the learning noise ζ or the choice temperature τ) while holding all other model parameters constant and set to their best-fitting values (see <xref ref-type="sec" rid="S10">Materials and Methods</xref>). We measured marginal reward cost <italic>C<sub>x</sub></italic> as the fraction loss of reward excess (i.e., the difference between obtained and foregone rewards) compared to a learning agent without the corresponding source of variability <italic>x</italic> (x = ζ or τ) tested in the same condition (Ref, S+ or V+). This ‘relative’ definition of reward cost <italic>C</italic> was chosen to measure how much each source of variability impacts the reward that could have been obtained in each condition (from <italic>C<sub>x</sub> = 0</italic> for a cost-free source of variability to <italic>C<sub>x</sub> =</italic> 100% for a purely random agent).</p><p id="P19">Marginal reward costs <italic>C<sub>ζ</sub></italic> and <italic><sub>Cτ</sub></italic> increased monotonically with learning noise and choice temperature in all conditions (<xref ref-type="fig" rid="F7">Figure 7A</xref>). However, the same amount of learning noise ζ was associated with larger reward costs <italic>C<sub>ζ</sub></italic> in the volatile condition than in the other two conditions. By contrast, the same choice temperature τ was associated with smaller reward costs <italic>C<sub>τ</sub></italic> in the volatile condition. This means that learning noise is effectively more costly in the volatile condition, whereas choice temperature is effectively less costly in the volatile condition.</p><p id="P20">Based on these theoretical simulations, we estimated the marginal reward costs incurred by learning noise (<italic>C<sub>ζ</sub></italic>) and choice temperature (<italic>C<sub>τ</sub></italic>) for each participant and each condition (<xref ref-type="fig" rid="F7">Figure 7B</xref>; <italic>N</italic> = 296 across the two datasets). We found that <italic>Cζ</italic> did not differ between the S+ and V+ conditions (S+: 11.8% [10.5 13.9]; V+: 12.5% [10.9 14.3], median [bootstrapped 95% CI]; <italic>z</italic> = 1.0, <italic>p</italic> = 0.307). Like <italic>C<sub>ζ</sub></italic>, <italic>C<sub>τ</sub></italic> did not differ between these two conditions (S+: 4.1% [3.2 5.6]; V+: 4.4% [3.4 5.6]; z = 1.3, <italic>p</italic> = 0.190). In other words, the decrease in learning noise and increase in choice temperature in the volatile condition compensate for the larger reward costs of learning noise and the smaller reward costs of choice temperature in this condition. Importantly, expressing reward costs in terms of ‘joint’ reward costs of the two sources of variability <italic>C<sub>ζ,τ</sub></italic> (compared to a learning agent without any variability) also results in similar costs across conditions (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 7C</xref>).</p><p id="P21">Conversely, the amounts of learning noise ζ and choice temperature τ associated with fixed marginal reward costs across conditions show a decrease in ζ and an increase in τ in the volatile condition – as in participants (<xref ref-type="fig" rid="F7">Figure 7B</xref>). In additional theoretical analyses (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Text</xref>), we considered alternative definitions of reward costs and the possibility that participants optimize learning imprecisions and exploration in terms of a quantitative comparison between the costs (in terms of reward loss) and benefits (in terms of reduced cognitive resources for learning imprecisions, and of lower uncertainty for exploration). Together, these theoretical considerations suggest that the opposite adjustments of learning imprecisions and exploration to unexpected uncertainty follow the opposite changes in their reward costs between conditions.</p></sec></sec><sec id="S9" sec-type="discussion"><title>Discussion</title><p id="P22">Making variable decisions under uncertainty has clear benefits, but also significant costs (<xref ref-type="bibr" rid="R9">9</xref>, <xref ref-type="bibr" rid="R22">22</xref>). Previous research has provided compelling evidence that part of this variability reflects active and adaptive exploration aimed at reducing uncertainty about choice options (<xref ref-type="bibr" rid="R3">3</xref>–<xref ref-type="bibr" rid="R5">5</xref>). However, seeking information about uncertain options often means foregoing a rewarding option (<xref ref-type="bibr" rid="R10">10</xref>). Furthermore, recent work has shown that a substantial fraction of decision variability arises not only from exploration, but also from learning imprecisions (<xref ref-type="bibr" rid="R6">6</xref>). Here, we studied how humans adapt these two sources of decision variability to different forms of uncertainty. We obtained converging evidence for an independent tuning of learning imprecisions and exploration to their specific costs and benefits. We discuss below the implications of these findings for existing theories of the explore-exploit tradeoff that ignore learning imprecisions (<xref ref-type="bibr" rid="R23">23</xref>).</p><p id="P23">In agreement with existing theories (<xref ref-type="bibr" rid="R4">4</xref>, <xref ref-type="bibr" rid="R10">10</xref>, <xref ref-type="bibr" rid="R14">14</xref>, <xref ref-type="bibr" rid="R22">22</xref>), we found that humans adjust the explore-exploit trade-off depending on the dominant form of uncertainty in their environment. Participants made more variable choices in the volatile condition where option values change over the course of a single game. Such ‘random’ exploration is adaptive in this context to monitor possible changes in the values of recently unchosen options (<xref ref-type="bibr" rid="R3">3</xref>). Similar adaptations of the explore-exploit trade-off have been described across other task conditions. Humans make little to no exploratory choices when the outcomes of unchosen options are known (<xref ref-type="bibr" rid="R6">6</xref>), engage in ‘directed’ exploration in conditions with imbalanced uncertainty across options (<xref ref-type="bibr" rid="R5">5</xref>, <xref ref-type="bibr" rid="R22">22</xref>, <xref ref-type="bibr" rid="R24">24</xref>), and use structured exploration in environments with spatially correlated option values (<xref ref-type="bibr" rid="R25">25</xref>).</p><p id="P24">Here, by measuring imprecisions in the reinforcement learning process that updates option values, we found that humans increase learning precision in the volatile condition. This within-participant adjustment of learning precision is resource-efficient: while outcomes from stochastic options are weakly informative about their values, outcomes from volatile options are informative about changes in their values. Participants therefore require not only higher learning rates, but also more precise updates. In the noisy Kalman filter that we fitted to participants’ behavior, the learning rate α reflects the perceived volatility of choice options. We leveraged the large individual differences in learning rate in the volatile condition to validate a second prediction of our resource-efficient account: that individuals with high learning rates (i.e., who perceive options are more volatile) should (on average) learn option values more precisely than individuals with low learning rates. This pervasive relation between learning rate and learning precision reveals a second trade-off that shapes human learning and decision-making under uncertainty.</p><p id="P25">Adjustments of exploration and learning imprecisions are not only distinct in terms of their respective costs and benefits. They also correspond to fundamentally different types of adjustments in the decision process. Optimizing the explore-exploit trade-off consists in tuning the choice policy between available options (<xref ref-type="bibr" rid="R4">4</xref>), downstream from the reinforcement learning process which estimates option values based on obtained rewards. By contrast, optimizing the cost-benefit trade-off associated with imprecise computations consists in tempering with the estimation of option values themselves, upstream from the choice policy (<xref ref-type="bibr" rid="R6">6</xref>, <xref ref-type="bibr" rid="R9">9</xref>). The observation of opposite adjustments of exploration and learning imprecisions in response to unexpected uncertainty provides empirical evidence that humans can simultaneously and independently regulate how they choose between options, and how precisely they learn from choice outcomes.</p><p id="P26">Importantly, the tuning of learning imprecisions to uncertainty cannot be described in terms of ‘efficient coding’ theories (<xref ref-type="bibr" rid="R7">7</xref>, <xref ref-type="bibr" rid="R26">26</xref>–<xref ref-type="bibr" rid="R28">28</xref>). These influential theories explain how noise-free computations can be tuned at long timescales to minimize the impact of external noise on performance. Instead, we show that humans rely on imprecise computations to learn option values and that they regulate this internal noise (<xref ref-type="bibr" rid="R6">6</xref>,<xref ref-type="bibr" rid="R9">9</xref>) at short timescales as a function of task demands. This finding is in agreement with the idea that humans optimize the use of their limited cognitive resources in a flexible, context-dependent fashion as a function of task demands (<xref ref-type="bibr" rid="R29">29</xref>, <xref ref-type="bibr" rid="R30">30</xref>). The fact that humans regulate learning imprecisions confirm theoretical considerations that describe precise computations as extremely costly in terms of neural resources (<xref ref-type="bibr" rid="R13">13</xref>). This finding is also consistent with the observation that reinforcement learning in multidimensional environments relies on selective attention mechanisms that focus learning resources on a subset of learnable option features (<xref ref-type="bibr" rid="R31">31</xref>–<xref ref-type="bibr" rid="R33">33</xref>).</p><p id="P27">The fact that humans adapt their exploration and learning imprecisions across conditions does not imply that participants are aware and in control of these adaptations. In our computational model, learning imprecisions corrupt value representations in an implicit fashion – i.e., trial-to-trial learning errors are not explicitly accounted for by increasing the uncertainty about option values. It is well known that humans exhibit partial blindness to internal sources of error (<xref ref-type="bibr" rid="R34">34</xref>, <xref ref-type="bibr" rid="R35">35</xref>). In additional control analyses (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Text</xref>), we have considered variants of our learning agent where the uncertainty triggered by learning imprecisions alters learning rates. The fact that these variants provide a poorer fit to human decisions suggests that learning imprecisions may be regulated in an implicit, non-intentional fashion. By contrast, exploration is thought to reflect a source of decision variability that can be intentionally regulated – in particular in conditions where sources of uncertainty are known (as it is the case in our study). And in our model, exploration corresponds to explicit ‘non-greedy’ decisions that do not maximize expected value. Nevertheless, further work will be required to determine the extent to which exploration and learning imprecisions are regulated intentionally as a function of uncertainty.</p><p id="P28">We simulated the marginal reward costs of exploration and learning imprecisions in each condition, expressed in terms of relative loss compared to a learning agent without this source of variability. We found that unexpected uncertainty is associated with lower costs of exploration but higher costs of learning imprecisions than expected uncertainty. These opposite effects of unexpected uncertainty on the costs of exploration and learning imprecisions provide an adaptive account of their opposite adjustments. In agreement with this view, the marginal reward costs of participants’ exploration and learning imprecisions were found to be constant across conditions. It remains however unclear whether participants actively maintain fixed reward costs across conditions, or whether they optimize other cost-benefit trade-offs associated with these two sources of behavioral variability (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Text</xref>).</p><p id="P29">If humans regulate (intentionally or not) the variability of their decisions in terms of two distinct cost-benefit trade-offs, then their neurophysiological substrates should be dissociable. Trial-to-trial decision variability has been associated with brain activity in frontopolar (<xref ref-type="bibr" rid="R3">3</xref>, <xref ref-type="bibr" rid="R37">37</xref>) and anterior cingulate (<xref ref-type="bibr" rid="R38">38</xref>, <xref ref-type="bibr" rid="R39">39</xref>) cortices, and related to dopaminergic and noradrenergic pathways (<xref ref-type="bibr" rid="R40">40</xref>–<xref ref-type="bibr" rid="R43">43</xref>). These multiple effects have been linked to adjustments of the explore-exploit trade-off, which is the sole source of decision variability in state-of-the-art models relying on perfectly precise computations. Accounting for learning imprecisions has revealed that the variability of value updates correlates with BOLD activity in the anterior cingulate cortex and with pupil dilation (<xref ref-type="bibr" rid="R6">6</xref>). Pupil dilation correlates with the activity of the locus coeruleus (<xref ref-type="bibr" rid="R44">44</xref>, <xref ref-type="bibr" rid="R45">45</xref>), a brainstem nucleus with norepinephrine-containing neurons which has bidirectional projections with the anterior cingulate cortex in the primate brain (<xref ref-type="bibr" rid="R46">46</xref>). Adjustments of exploration and learning imprecisions could be achieved either by dissociable brain regions and neuromodulatory pathways, or by different signals in the same brain region (<xref ref-type="bibr" rid="R47">47</xref>). In particular, anterior cingulate activity has been related not only to exploration, but also to the cost-benefit trade-off associated with cognitive control (<xref ref-type="bibr" rid="R12">12</xref>). Future research should study how exploration and learning imprecisions are simultaneously regulated in the human brain (<xref ref-type="bibr" rid="R48">48</xref>).</p><p id="P30">The delineation of opposite adjustments of exploration and learning imprecisions in response to volatility suggests that models lacking either source may draw incorrect interpretations. Indeed, our findings show that humans adjust not only how much they explore uncertain choice options, by arbitrating the explore-exploit trade-off, but also how precisely they learn their values through reinforcement learning. These adjustments may generalize to different tasks and computational models of behavior. Indeed, decisions in perceptual categorization tasks suffer from a similar source of imprecisions in the underlying inference problem (<xref ref-type="bibr" rid="R49">49</xref>). These categorization tasks can be embedded in volatile conditions (<xref ref-type="bibr" rid="R50">50</xref>) where statistical inference models of behavior include a parameter that controls learning rate and another parameter that controls inferential imprecisions (<xref ref-type="bibr" rid="R51">51</xref>, <xref ref-type="bibr" rid="R52">52</xref>). In this context that differs widely from the two-armed bandit used in this study, the same negative relation between learning rate and inferential imprecisions may emerge. Taken together, these results indicate that the precision of cognitive computations acts as an important and flexible constraint on human cognition (<xref ref-type="bibr" rid="R9">9</xref>). Existing theories of cognitive control should therefore be revised to account for imprecise computations and their associated costs and benefits.</p></sec><sec id="S10" sec-type="materials | methods"><title>Materials and Methods</title><sec id="S11"><title>Experimental design</title><p id="P31">Three samples of <italic>N</italic> = 200 English-speaking adults were recruited on Prolific (prolific.co) for inclusion in the two datasets collected for this study (a ‘discovery’ dataset and a ‘replication’ dataset, total <italic>N</italic> = 400). We fixed this sample size of <italic>N</italic> = 200 per dataset a priori and calculated that it is sufficiently powered to detect a minimal between-subject correlation of 0.20 with 80% power at a significance threshold <italic>α</italic> = 0.05. We excluded from analyses any participant whose accuracy did not significantly exceed chance-level performance at a one-tailed threshold α = 0.05 in any of the three conditions (binomial test against 50% accuracy). This exclusion procedure left <italic>N</italic> = 154 participants (mean age = 29.6 ± 10.0 years, 53 females) in the first dataset, and <italic>N</italic> = 142 participants (mean age = 25.4 ± 6.4 years, 72 females) in the second dataset. All participants provided informed consent regarding their participation in the study, which followed guidelines approved by the ethical review committee of the Institut National de la Santé et de la Recherche Médicale (IRB #00003888).</p><p id="P32">Upon gaining access to the task, participants were prompted to set the task to full-screen before being able to continue. Next, participants were introduced to the mechanics of their responses (e.g. keys for choosing options) and the presentation of the task as a slot machine game. They were instructed on the dynamics of the reference (Ref), stochastic (S+) and volatile (V+) conditions and the difference between these conditions via written prompts on the screen. Participants always played the Ref condition first, and the order of the other two conditions was counterbalanced across participants. Before every condition, participants played a ‘practice’ block to familiarize them with the condition. At the end of each practice block, participants were presented with a debriefing of their performance and the condition with a visual illustration of the trajectory of their decisions and the rewards they obtained. The task was self-paced but had to be completed within 60 minutes to avoid disengagement. Participants were offered to take breaks between blocks.</p></sec><sec id="S12"><title>Task generation</title><p id="P33">The Ref and S+ (stochastic) conditions corresponded to two-armed bandits with fixed reward distributions, drawn between 1 and 99 points. The V+ (volatile) condition corresponded to a restless two-armed bandit task where the means of the reward distributions associated with each option follow a random walk over time. In both conditions, the mean rewards <inline-formula><mml:math id="M1"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>o</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> associated with the two options <italic>o</italic> ∈ {1,2} were symmetrical <inline-formula><mml:math id="M2"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>i.e</mml:mtext><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> Reversals were defined as time points where the mean rewards associated with the two options cross each other.</p><p id="P34">Task difficulty was matched between the S+ and V+ conditions based on the simulated performance of an optimal learning agent. For each participant, we first generated 2,000 random walks of mean rewards following a beta distribution of initial mean of 50 points and a drifting standard deviation of 5 points. All random walks that travelled below 10 points or above 90 points, or had steps larger than 10 points, were discarded. Excursions below or above 50 points of 8, 12, 16, 20, and 24 steps were chosen to populate the V+ (volatile) condition. The average reward of each excursion was used as the static mean reward of a round of trials for the Ref (reference) condition. We set the variance of sampled rewards such that 75% of sampled rewards from the better option in the Ref condition were higher than 50 points (and 25% lower than 50 points). We then computed the effective sampling variance <inline-formula><mml:math id="M3"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>S</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the effective drifting variance <inline-formula><mml:math id="M4"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>d</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of rewards, and simulated a standard Kalman filter with greedy choices in the V+ condition to obtain its accuracy (fraction of choices toward the option associated with the largest mean reward). To generate the last, S+ (stochastic) condition, we incrementally regressed the static mean reward from the Ref condition toward 50 until the choice accuracy of a Kalman filter matched that of the V+ condition (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 8</xref>). Participants played two instances of each condition (2 blocks of 80 trials for each condition).</p><p id="P35">Options were depicted as black shapes for the Ref and S+ conditions, and as colored discs in the V+ condition, to help participants distinguishing between stable (Ref and S+) and volatile (V+) conditions. To make sure that participants treated each new round of the Ref and V+ conditions as independent from the previous ones, we presented novel shapes for each new round. After each choice, the chosen option briefly appeared at the center of the screen, and was then followed by the number of points obtained from the chosen option (<xref ref-type="fig" rid="F1">Figure 1A</xref>). Participants earned £3.30 upon successful completion of the task. Participants gained an extra £1.00 as a bonus if they performed significantly better than chance across the entire experiment (α = 0.05, meaning a choice accuracy above 53.75%). Participants who answered a battery of mental health questionnaires performed after the task (not analyzed for this study) earned another £3.00 upon completion.</p><p id="P36">The visuals and dynamics of the task (i.e., the frontend code) were implemented using jsPsych (version 6.3.0) (<xref ref-type="bibr" rid="R53">53</xref>). The backend server was handled using Node.js. Participants’ responses to the task and questionnaires were stored in a MySQL database.</p></sec><sec id="S13"><title>Computational modeling</title><p id="P37">Following each outcome, the posterior value of the chosen option <inline-formula><mml:math id="M5"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is updated according to a standard Kalman filtered corrupted by additive random noise <italic>ε<sub>t</sub></italic> drawn from a normal distribution of zero mean and standard deviation <italic>η<sub>t</sub></italic>: <disp-formula id="FD1"><mml:math id="M6"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>c</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>c</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>ε</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula> where <italic>k<sub>t</sub></italic> is the Kalman gain, which depends on the posterior variance of the chosen option <inline-formula><mml:math id="M7"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>c</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and the sampling variance <italic>v<sub>s</sub></italic> of rewards, which was set to its true effective value (0.0163 for rewards rescaled between 0 and 1) and used as ‘scaling parameter’ in the model: <disp-formula id="FD2"><mml:math id="M8"><mml:mrow><mml:msubsup><mml:mi>k</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>c</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>c</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p id="P38">The posterior variances of the two options <italic>o</italic> ∈ {1,2} were updated using the standard equation of the Kalman filter: <disp-formula id="FD3"><mml:math id="M9"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>0</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula> where <italic>v<sub>d</sub></italic> is the drifting variance assumed by the Kalman filter. We fitted <italic>v<sub>d</sub></italic> in terms of its associated asymptotic Kalman gain <italic>α</italic> for an option that would be chosen on <inline-formula><mml:math id="M10"><mml:mrow><mml:mi>n</mml:mi><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:math></inline-formula> trials. Following recent findings (<xref ref-type="bibr" rid="R6">6</xref>), the standard deviation of the random noise corrupting the update of the posterior value of the chosen option scales as a constant fraction ζ of the update <inline-formula><mml:math id="M11"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>c</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> Posterior variances of the two options were initialized to the true variance of mean reward values across all conditions (0.0214 for rewards rescaled between 0 and 1).</p><p id="P39">The posterior value of the unchosen option <inline-formula><mml:math id="M12"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> decays toward the baseline value (50 points, i.e., 0.5 for values rescaled between 0 and 1). The rate of this decay is controlled by an exponential decay factor δ: <disp-formula id="FD4"><mml:math id="M13"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>u</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="P40">The probability of choosing option 1 <inline-formula><mml:math id="M14"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> based on the posterior values of the two options follows the standard softmax policy with choice temperature τ: <disp-formula id="FD5"><mml:math id="M15"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula> where <inline-formula><mml:math id="M16"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> corresponds to a purely greedy, argmax policy.</p><p id="P41">We used Sequential Monte Carlo (SMC) sampling methods to estimate the conditional likelihoods of the responses of each participant given the set of parameter values. Using Bayesian Adaptive Direct Search (BADS) with 10 random starting points for each parameter α, ζ, δ, τ (<xref ref-type="bibr" rid="R54">54</xref>), we first obtained point estimates of best-fitting parameter values. We then used these estimates as starting points for the estimation of their joint posterior distribution using Variational Bayes Monte Carlo (VBMC) (<xref ref-type="bibr" rid="R55">55</xref>, <xref ref-type="bibr" rid="R56">56</xref>). Both model fitting algorithms accounted for noise in log-likelihood estimates due to the presence of random learning noise in the Kalman filter updates.</p></sec><sec id="S14"><title>Computational model validation</title><p id="P42">To ensure that the inclusion of learning noise (controlled by its Weber fraction ζ) and soft choices (controlled by their temperature τ)were both necessary to fit participants’ choices, we performed random-effects Bayesian Model Selection (BMS) based on its standard Dirichlet parameterization described in the literature (<xref ref-type="bibr" rid="R57">57</xref>). In practice, as in recent work (<xref ref-type="bibr" rid="R6">6</xref>), we compared the model including both sources of decision variability with two model variants: one without learning noise where ζ = 0, and one with a purely greedy choice policy where τ = 0. The full model outperformed the other two reduced models (model prevalence of 70% for the full model, exceedance <italic>p</italic> &gt; 0.999).</p><p id="P43">Critically, we performed standard model recovery to validate our model simulation and fitting procedure (<xref ref-type="bibr" rid="R20">20</xref>, <xref ref-type="bibr" rid="R21">21</xref>). In particular, by simulating the different variances of our Kalman filter model using individual best-fitting parameters, we established that our three candidate models were indeed distinguishable between each other (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 2B</xref>). Our fitting procedure was not biased in that we were able to successfully recover all three model variances by simulating decisions from each model and then fitted all three models again to this simulated behavior for which the ground-truth model structure is known.</p></sec><sec id="S15"><title>Statistical analyses</title><p id="P44">We employed Wilcoxon signed-rank tests for the comparison of model-free behavioral variables across conditions (choice accuracy and switch rate). Wilcoxon signed-rank tests were also used for comparisons of best-fitting model parameters across conditions. Repeated-measures ANOVAs were performed for comparing choice accuracy curves and switch rate curves across conditions. Spearman (rank-based) correlations were used unless noted otherwise. Split-plot tests were used to calculate the effects of median splits with respect to each model parameter on choice accuracy and switch rate curves.</p><p id="P45">We performed a principal component analysis (PCA) on standardized (zero-mean and unit-variance) model parameter values such that, by construction: 1. the variance explained by a principal component (PC) in a given condition captures shared variance between model parameters, and 2. different PCs are orthogonal to each other and therefore reflect uncorrelated sources of variability in model parameters.</p><p id="P46">Standard errors on the Spearman’s rank correlation coefficients were generated through bootstrapping, by randomly sampled datasets from the original dataset (with identical sample size and replacement) 1,000 times and calculating the correlation on the new datasets. The same procedure was done to obtain the standard errors of PC coefficients and associated fractions of variance explained. Statistical significance of the fraction of variance explained was obtained by a one-sided comparison of the variance explained by bootstrapped PCs to the variance explained by bootstrapped PCs for shuffled data. A similar procedure was used to compare the fractions of variance of each PC explained by each model parameter.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Materials</label><media xlink:href="EMS158947-supplement-Supplementary_Materials.pdf" mimetype="application" mime-subtype="pdf" id="d68aAdEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S16"><title>Acknowledgments</title><p>This work was supported by a starting grant from the European Research Council (ERC-StG759341) awarded to V.W., and by a department-wide grant from the Agence Nationale de la Recherche (ANR-17-EURE-0017, EUR FrontCog). M.R. is the beneficiary of a postdoctoral fellowship from the AXA Research Fund. M.R. is supported by La Fondation des Treilles. The funders had no role in study design, data collection and analysis, decision to publish or preparation of the manuscript. J.K.L. designed the experiments, conducted the experiments, analyzed the data and wrote the paper. M.R. provided feedback on data analyses and wrote the paper. V.W. designed the experiments, analyzed the data, wrote the paper and supervised the study. The authors declare that they have no competing interests. The datasets generated and analyzed during the study will be made freely available on an online repository. The analysis code supporting the reported findings is currently available from the corresponding authors upon request and will be made freely available on an online repository upon publication of the manuscript.</p></ack><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wyart</surname><given-names>V</given-names></name><name><surname>Koechlin</surname><given-names>E</given-names></name></person-group><article-title>Choice variability and suboptimality in uncertain environments</article-title><source>Curr Opin Behav Sci</source><year>2016</year><volume>11</volume><fpage>109</fpage><lpage>115</lpage></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name></person-group><source>Reinforcement learning: an introduction</source><year>1998</year><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>O’Doherty</surname><given-names>JP</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Seymour</surname><given-names>B</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><article-title>Cortical substrates for exploratory decisions in humans</article-title><source>Nature</source><year>2006</year><volume>441</volume><fpage>876</fpage><lpage>879</lpage><pub-id pub-id-type="pmcid">PMC2635947</pub-id><pub-id pub-id-type="pmid">16778890</pub-id><pub-id pub-id-type="doi">10.1038/nature04766</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>JD</given-names></name><name><surname>McClure</surname><given-names>SM</given-names></name><name><surname>Yu</surname><given-names>AJ</given-names></name></person-group><article-title>Should I stay or should I go? How the human brain manages the trade-off between exploitation and exploration</article-title><source>Philos Trans R Soc Lond B Biol Sci</source><year>2007</year><volume>362</volume><fpage>933</fpage><lpage>942</lpage><pub-id pub-id-type="pmcid">PMC2430007</pub-id><pub-id pub-id-type="pmid">17395573</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2007.2098</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>RC</given-names></name><name><surname>Geana</surname><given-names>A</given-names></name><name><surname>White</surname><given-names>JM</given-names></name><name><surname>Ludvig</surname><given-names>EA</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><article-title>Humans use directed and random exploration to solve the explore-exploit dilemma</article-title><source>J Exp Psychol Gen</source><year>2014</year><volume>143</volume><fpage>2074</fpage><lpage>2081</lpage><pub-id pub-id-type="pmcid">PMC5635655</pub-id><pub-id pub-id-type="pmid">25347535</pub-id><pub-id pub-id-type="doi">10.1037/a0038199</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Findling</surname><given-names>C</given-names></name><name><surname>Skvortsova</surname><given-names>V</given-names></name><name><surname>Dromnelle</surname><given-names>R</given-names></name><name><surname>Palminteri</surname><given-names>S</given-names></name><name><surname>Wyart</surname><given-names>V</given-names></name></person-group><article-title>Computational noise in reward-guided learning drives behavioral variability in volatile environments</article-title><source>Nat Neurosci</source><year>2019</year><volume>22</volume><fpage>2066</fpage><lpage>2077</lpage><pub-id pub-id-type="pmid">31659343</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polania</surname><given-names>R</given-names></name><name><surname>Woodford</surname><given-names>M</given-names></name><name><surname>Ruff</surname><given-names>CC</given-names></name></person-group><article-title>Efficient coding of subjective value</article-title><source>Nat Neurosci</source><year>2019</year><volume>22</volume><fpage>134</fpage><lpage>142</lpage><pub-id pub-id-type="pmcid">PMC6314450</pub-id><pub-id pub-id-type="pmid">30559477</pub-id><pub-id pub-id-type="doi">10.1038/s41593-018-0292-0</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Juechems</surname><given-names>K</given-names></name><name><surname>Balaguer</surname><given-names>J</given-names></name><name><surname>Spitzer</surname><given-names>B</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><article-title>Optimal utility and probability functions for agents with finite computational precision</article-title><source>PNAS</source><year>2021</year><volume>118</volume><elocation-id>e2002232118</elocation-id><pub-id pub-id-type="pmcid">PMC7812798</pub-id><pub-id pub-id-type="pmid">33380453</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2002232118</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Findling</surname><given-names>C</given-names></name><name><surname>Wyart</surname><given-names>V</given-names></name></person-group><article-title>Computation noise in human learning and decision-making: origin, impact, function</article-title><source>Curr Opin Behav Sci</source><year>2021</year><volume>38</volume><fpage>124</fpage><lpage>132</lpage></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Averbeck</surname><given-names>BB</given-names></name></person-group><article-title>Theory of choice in bandit, information sampling and foraging tasks</article-title><source>PLOS Comput Biol</source><year>2015</year><volume>11</volume><elocation-id>e1004164</elocation-id><pub-id pub-id-type="pmcid">PMC4376795</pub-id><pub-id pub-id-type="pmid">25815510</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004164</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faisal</surname><given-names>AA</given-names></name><name><surname>Selen</surname><given-names>LPJ</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name></person-group><article-title>Noise in the nervous system</article-title><source>Nat Rev Neurosci</source><year>2008</year><volume>9</volume><fpage>292</fpage><lpage>303</lpage><pub-id pub-id-type="pmcid">PMC2631351</pub-id><pub-id pub-id-type="pmid">18319728</pub-id><pub-id pub-id-type="doi">10.1038/nrn2258</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shenhav</surname><given-names>A</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><article-title>The expected value of control: an integrative theory of anterior cingulate cortex function</article-title><source>Neuron</source><year>2013</year><volume>79</volume><fpage>217</fpage><lpage>240</lpage><pub-id pub-id-type="pmcid">PMC3767969</pub-id><pub-id pub-id-type="pmid">23889930</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2013.07.007</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deneve</surname><given-names>S</given-names></name><name><surname>Machens</surname><given-names>CK</given-names></name></person-group><article-title>Efficient codes and balanced networks</article-title><source>Nat Neurosci</source><year>2016</year><volume>19</volume><fpage>375</fpage><lpage>382</lpage><pub-id pub-id-type="pmid">26906504</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soltani</surname><given-names>A</given-names></name><name><surname>Izquierdo</surname><given-names>A</given-names></name></person-group><article-title>Adaptive learning under expected and unexpected uncertainty</article-title><source>Nat Rev Neurosci</source><year>2019</year><volume>20</volume><fpage>635</fpage><lpage>644</lpage><pub-id pub-id-type="pmcid">PMC6752962</pub-id><pub-id pub-id-type="pmid">31147631</pub-id><pub-id pub-id-type="doi">10.1038/s41583-019-0180-y</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piray</surname><given-names>P</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><article-title>A model for learning based on the joint estimation of stochasticity and volatility</article-title><source>Nat Commun</source><year>2021</year><volume>12</volume><elocation-id>6587</elocation-id><pub-id pub-id-type="pmcid">PMC8592992</pub-id><pub-id pub-id-type="pmid">34782597</pub-id><pub-id pub-id-type="doi">10.1038/s41467-021-26731-9</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Walton</surname><given-names>ME</given-names></name><name><surname>Rushworth</surname><given-names>MFS</given-names></name></person-group><article-title>Learning the value of information in an uncertain world</article-title><source>Nat Neurosci</source><year>2007</year><volume>10</volume><fpage>1214</fpage><lpage>1221</lpage><pub-id pub-id-type="pmid">17676057</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Browning</surname><given-names>M</given-names></name><name><surname>Behrens</surname><given-names>TE</given-names></name><name><surname>Jocham</surname><given-names>G</given-names></name><name><surname>O’Reilly</surname><given-names>JX</given-names></name><name><surname>Bishop</surname><given-names>SJ</given-names></name></person-group><article-title>Anxious individuals have difficulty learning the causal statistics of aversive environments</article-title><source>Nat Neurosci</source><year>2015</year><volume>18</volume><fpage>590</fpage><lpage>596</lpage><pub-id pub-id-type="pmcid">PMC4644067</pub-id><pub-id pub-id-type="pmid">25730669</pub-id><pub-id pub-id-type="doi">10.1038/nn.3961</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cook</surname><given-names>JL</given-names></name><name><surname>Swart</surname><given-names>JC</given-names></name><name><surname>Frobose</surname><given-names>MI</given-names></name><name><surname>Diaconescu</surname><given-names>AO</given-names></name><name><surname>Geurts</surname><given-names>DE</given-names></name><name><surname>den Ouden</surname><given-names>HE</given-names></name><name><surname>Cools</surname><given-names>R</given-names></name></person-group><article-title>Catecholaminergic modulation of meta-learning</article-title><source>eLife</source><year>2019</year><volume>8</volume><elocation-id>e51439</elocation-id><pub-id pub-id-type="pmcid">PMC6974360</pub-id><pub-id pub-id-type="pmid">31850844</pub-id><pub-id pub-id-type="doi">10.7554/eLife.51439</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piray</surname><given-names>P</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><article-title>A simple model for learning in volatile environments</article-title><source>PLOS Comput Biol</source><year>2020</year><volume>16</volume><elocation-id>e1007963</elocation-id><pub-id pub-id-type="pmcid">PMC7329063</pub-id><pub-id pub-id-type="pmid">32609755</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007963</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palminteri</surname><given-names>S</given-names></name><name><surname>Wyart</surname><given-names>V</given-names></name><name><surname>Koechlin</surname><given-names>E</given-names></name></person-group><article-title>The importance of falsification in computational cognitive modeling</article-title><source>Trends Cogn Sci</source><year>2017</year><volume>21</volume><fpage>425</fpage><lpage>433</lpage><pub-id pub-id-type="pmid">28476348</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>RC</given-names></name><name><surname>Collins</surname><given-names>AGE</given-names></name></person-group><article-title>Ten simple rules for the computational modeling of behavioral data</article-title><source>eLife</source><year>2019</year><volume>8</volume><elocation-id>e49547</elocation-id><pub-id pub-id-type="pmcid">PMC6879303</pub-id><pub-id pub-id-type="pmid">31769410</pub-id><pub-id pub-id-type="doi">10.7554/eLife.49547</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>RC</given-names></name><name><surname>Bonawitz</surname><given-names>E</given-names></name><name><surname>Costa</surname><given-names>VD</given-names></name><name><surname>Ebitz</surname><given-names>RB</given-names></name></person-group><article-title>Balancing exploration and exploitation with information and randomization</article-title><source>Curr Opin Behav Sci</source><year>2021</year><volume>38</volume><fpage>49</fpage><lpage>56</lpage><pub-id pub-id-type="pmcid">PMC7654823</pub-id><pub-id pub-id-type="pmid">33184605</pub-id><pub-id pub-id-type="doi">10.1016/j.cobeha.2020.10.001</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><article-title>Deconstructing the human algorithms for exploration</article-title><source>Cognition</source><year>2018</year><volume>173</volume><fpage>34</fpage><lpage>42</lpage><pub-id pub-id-type="pmcid">PMC5801139</pub-id><pub-id pub-id-type="pmid">29289795</pub-id><pub-id pub-id-type="doi">10.1016/j.cognition.2017.12.014</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schulz</surname><given-names>E</given-names></name><name><surname>Bhui</surname><given-names>R</given-names></name><name><surname>Love</surname><given-names>BC</given-names></name><name><surname>Brier</surname><given-names>B</given-names></name><name><surname>Todd</surname><given-names>MT</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><article-title>Structured, uncertainty-driven exploration in real-world consumer choice</article-title><source>Proc Natl Acad Sci USA</source><year>2019</year><volume>116</volume><fpage>13903</fpage><lpage>13908</lpage><pub-id pub-id-type="pmcid">PMC6628813</pub-id><pub-id pub-id-type="pmid">31235598</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1821028116</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>CM</given-names></name><name><surname>Schulz</surname><given-names>E</given-names></name><name><surname>Speekenbrink</surname><given-names>M</given-names></name><name><surname>Nelson</surname><given-names>JD</given-names></name><name><surname>Meder</surname><given-names>B</given-names></name></person-group><article-title>Generalization guides human exploration in vast decision spaces</article-title><source>Nat Hum Behav</source><year>2018</year><volume>2</volume><fpage>915</fpage><lpage>924</lpage><pub-id pub-id-type="pmid">30988442</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Barlow</surname><given-names>HB</given-names></name></person-group><source>Sensory Communication</source><person-group person-group-type="editor"><name><surname>Rosenblith</surname><given-names>WA</given-names></name></person-group><year>1961</year><fpage>217</fpage><lpage>234</lpage><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Averbeck</surname><given-names>BB</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><article-title>Neural correlations, population coding and computation</article-title><source>Nat Rev Neurosci</source><year>2006</year><volume>7</volume><fpage>358</fpage><lpage>366</lpage><pub-id pub-id-type="pmid">16760916</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>X-X</given-names></name><name><surname>Stocker</surname><given-names>AA</given-names></name></person-group><article-title>A Bayesian observer model constrained by efficient coding can explain “anti-Bayesian” percepts</article-title><source>Nat Neurosci</source><year>2015</year><volume>18</volume><fpage>1509</fpage><lpage>1517</lpage><pub-id pub-id-type="pmid">26343249</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lieder</surname><given-names>F</given-names></name><name><surname>Griffiths</surname><given-names>TL</given-names></name></person-group><article-title>Resource-rational analysis: Understanding human cognition as the optimal use of limited computational resources</article-title><source>Behav Brain Sci</source><year>2019</year><volume>43</volume><fpage>e1</fpage><pub-id pub-id-type="pmid">30714890</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Griffiths</surname><given-names>TL</given-names></name></person-group><article-title>Understanding human intelligence through human limitations</article-title><source>Trends Cogn Sci</source><year>2020</year><volume>24</volume><fpage>873</fpage><lpage>883</lpage><pub-id pub-id-type="pmid">33041198</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niv</surname><given-names>Y</given-names></name><name><surname>Daniel</surname><given-names>R</given-names></name><name><surname>Geana</surname><given-names>A</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Leong</surname><given-names>YC</given-names></name><name><surname>Radulescu</surname><given-names>A</given-names></name><name><surname>Wilson</surname><given-names>RC</given-names></name></person-group><article-title>Reinforcement learning in multidimensional environments relies on attention mechanisms</article-title><source>J Neurosci</source><year>2015</year><volume>35</volume><fpage>8145</fpage><lpage>8157</lpage><pub-id pub-id-type="pmcid">PMC4444538</pub-id><pub-id pub-id-type="pmid">26019331</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2978-14.2015</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leong</surname><given-names>YC</given-names></name><name><surname>Radulescu</surname><given-names>A</given-names></name><name><surname>Daniel</surname><given-names>R</given-names></name><name><surname>DeWoskin</surname><given-names>V</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><article-title>Dynamic interaction between reinforcement learning and attention in multidimensional environments</article-title><source>Neuron</source><year>2017</year><volume>93</volume><fpage>451</fpage><lpage>463</lpage><pub-id pub-id-type="pmcid">PMC5287409</pub-id><pub-id pub-id-type="pmid">28103483</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2016.12.040</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Radulescu</surname><given-names>A</given-names></name><name><surname>Shin</surname><given-names>YS</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><article-title>Human representation learning</article-title><source>Annu Rev Neurosci</source><year>2021</year><volume>44</volume><fpage>253</fpage><lpage>273</lpage><pub-id pub-id-type="pmcid">PMC9707489</pub-id><pub-id pub-id-type="pmid">33730510</pub-id><pub-id pub-id-type="doi">10.1146/annurev-neuro-092920-120559</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rahnev</surname><given-names>D</given-names></name><name><surname>Maniscalco</surname><given-names>B</given-names></name><name><surname>Graves</surname><given-names>T</given-names></name><name><surname>Huang</surname><given-names>E</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name><name><surname>Lau</surname><given-names>H</given-names></name></person-group><article-title>Attention induces conservative subjective biases in visual perception</article-title><source>Nat Neurosci</source><year>2011</year><volume>14</volume><fpage>1513</fpage><lpage>1515</lpage><pub-id pub-id-type="pmid">22019729</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herce Castañón</surname><given-names>S</given-names></name><name><surname>Moran</surname><given-names>R</given-names></name><name><surname>Ding</surname><given-names>J</given-names></name><name><surname>Egner</surname><given-names>T</given-names></name><name><surname>Bang</surname><given-names>D</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><article-title>Human noise blindness drives suboptimal cognitive inference</article-title><source>Nat Commun</source><year>2019</year><volume>10</volume><elocation-id>1719</elocation-id><pub-id pub-id-type="pmcid">PMC6461696</pub-id><pub-id pub-id-type="pmid">30979880</pub-id><pub-id pub-id-type="doi">10.1038/s41467-019-09330-7</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogel</surname><given-names>TA</given-names></name><name><surname>Savelson</surname><given-names>ZM</given-names></name><name><surname>Otto</surname><given-names>AR</given-names></name><name><surname>Roy</surname><given-names>M</given-names></name></person-group><article-title>Forced choices reveal a trade-off between cognitive effort and physical pain</article-title><source>eLife</source><year>2020</year><volume>9</volume><elocation-id>e59410</elocation-id><pub-id pub-id-type="pmcid">PMC7714391</pub-id><pub-id pub-id-type="pmid">33200988</pub-id><pub-id pub-id-type="doi">10.7554/eLife.59410</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raja Beharelle</surname><given-names>A</given-names></name><name><surname>Polania</surname><given-names>R</given-names></name><name><surname>Hare</surname><given-names>TA</given-names></name><name><surname>Ruff</surname><given-names>CC</given-names></name></person-group><article-title>Transcranial stimulation over frontopolar cortex elucidates the choice attributes and neural mechanisms used to resolve exploration-exploitation trade-offs</article-title><source>J Neurosci</source><year>2015</year><volume>35</volume><fpage>14544</fpage><lpage>14556</lpage><pub-id pub-id-type="pmcid">PMC6605460</pub-id><pub-id pub-id-type="pmid">26511245</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2322-15.2015</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donoso</surname><given-names>M</given-names></name><name><surname>Collins</surname><given-names>AGE</given-names></name><name><surname>Koechlin</surname><given-names>E</given-names></name></person-group><article-title>Foundations of human reasoning in the prefrontal cortex</article-title><source>Science</source><year>2014</year><volume>344</volume><fpage>1481</fpage><lpage>1486</lpage><pub-id pub-id-type="pmid">24876345</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blanchard</surname><given-names>TC</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><article-title>Pure correlates of exploration and exploitation in the human brain</article-title><source>Cogn Affect Behav Neurosci</source><year>2018</year><volume>18</volume><fpage>117</fpage><lpage>126</lpage><pub-id pub-id-type="pmcid">PMC5825268</pub-id><pub-id pub-id-type="pmid">29218570</pub-id><pub-id pub-id-type="doi">10.3758/s13415-017-0556-2</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frank</surname><given-names>MJ</given-names></name><name><surname>Doll</surname><given-names>BB</given-names></name><name><surname>Oas-Terpstra</surname><given-names>J</given-names></name><name><surname>Moreno</surname><given-names>F</given-names></name></person-group><article-title>The neurogenetics of exploration and exploitation: Prefrontal and striatal dopaminergic components</article-title><source>Nat Neurosci</source><year>2009</year><volume>12</volume><fpage>1062</fpage><lpage>1068</lpage><pub-id pub-id-type="pmcid">PMC3062477</pub-id><pub-id pub-id-type="pmid">19620978</pub-id><pub-id pub-id-type="doi">10.1038/nn.2342</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jepma</surname><given-names>M</given-names></name><name><surname>Nieuwenhuis</surname><given-names>S</given-names></name></person-group><article-title>Pupil diameter predicts changes in the exploration-exploitation trade-off: evidence for the adaptive gain theory</article-title><source>J Cogn Neurosci</source><year>2011</year><volume>23</volume><fpage>1587</fpage><lpage>1596</lpage><pub-id pub-id-type="pmid">20666595</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dubois</surname><given-names>M</given-names></name><name><surname>Habicht</surname><given-names>J</given-names></name><name><surname>Michely</surname><given-names>J</given-names></name><name><surname>Moran</surname><given-names>R</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Hauser</surname><given-names>TU</given-names></name></person-group><article-title>Human complex exploration strategies are enriched by noradrenaline-modulated heuristics</article-title><source>eLife</source><year>2021</year><volume>10</volume><elocation-id>e59907</elocation-id><pub-id pub-id-type="pmcid">PMC7815309</pub-id><pub-id pub-id-type="pmid">33393461</pub-id><pub-id pub-id-type="doi">10.7554/eLife.59907</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfeffer</surname><given-names>T</given-names></name><name><surname>Ponce-Alvarez</surname><given-names>A</given-names></name><name><surname>Tsetsos</surname><given-names>K</given-names></name><name><surname>Meindertsma</surname><given-names>T</given-names></name><name><surname>Gahnstrom</surname><given-names>CJ</given-names></name><name><surname>van den Brink</surname><given-names>RL</given-names></name><name><surname>Nolte</surname><given-names>G</given-names></name><name><surname>Engel</surname><given-names>AK</given-names></name><name><surname>Deco</surname><given-names>G</given-names></name><name><surname>Donner</surname><given-names>TH</given-names></name></person-group><article-title>Circuit mechanisms for the chemical modulation of cortex-wide network interactions and behavioral variability</article-title><source>Sci Adv</source><year>2021</year><volume>7</volume><elocation-id>eabf5620</elocation-id><pub-id pub-id-type="pmcid">PMC8284895</pub-id><pub-id pub-id-type="pmid">34272245</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.abf5620</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Usher</surname><given-names>M</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name><name><surname>Servan-Schreiber</surname><given-names>D</given-names></name><name><surname>Rajkowski</surname><given-names>J</given-names></name><name><surname>Aston-Jones</surname><given-names>G</given-names></name></person-group><article-title>The role of locus coeruleus in the regulation of cognitive performance</article-title><source>Science</source><year>1999</year><volume>283</volume><fpage>549</fpage><lpage>554</lpage><pub-id pub-id-type="pmid">9915705</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joshi</surname><given-names>S</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Kalwani</surname><given-names>RM</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name></person-group><article-title>Relationships between pupil diameter and neuronal activity in the locus coeruleus, colliculi, and cingulate cortex</article-title><source>Neuron</source><year>2016</year><volume>89</volume><fpage>221</fpage><lpage>234</lpage><pub-id pub-id-type="pmcid">PMC4707070</pub-id><pub-id pub-id-type="pmid">26711118</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2015.11.028</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joshi</surname><given-names>S</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name></person-group><article-title>Pupil size as a window on neural substrates of cognition</article-title><source>Trends Cogn Sci</source><year>2020</year><volume>24</volume><fpage>466</fpage><lpage>480</lpage><pub-id pub-id-type="pmcid">PMC7271902</pub-id><pub-id pub-id-type="pmid">32331857</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2020.03.005</pub-id></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joshi</surname><given-names>S</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name></person-group><article-title>Context-dependent relationships between locus coeruleus firing patterns and coordinated neural activity in the anterior cingulate cortex</article-title><source>eLife</source><year>2022</year><volume>11</volume><elocation-id>e63490</elocation-id><pub-id pub-id-type="pmcid">PMC8765756</pub-id><pub-id pub-id-type="pmid">34994344</pub-id><pub-id pub-id-type="doi">10.7554/eLife.63490</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname><given-names>AGE</given-names></name><name><surname>Cockburn</surname><given-names>J</given-names></name></person-group><article-title>Beyond dichotomies in reinforcement learning</article-title><source>Nat Rev Neurosci</source><year>2020</year><volume>21</volume><fpage>576</fpage><lpage>586</lpage><pub-id pub-id-type="pmcid">PMC7800310</pub-id><pub-id pub-id-type="pmid">32873936</pub-id><pub-id pub-id-type="doi">10.1038/s41583-020-0355-6</pub-id></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drugowitsch</surname><given-names>J</given-names></name><name><surname>Wyart</surname><given-names>V</given-names></name><name><surname>Devauchelle</surname><given-names>A-D</given-names></name><name><surname>Koechlin</surname><given-names>E</given-names></name></person-group><article-title>Computational precision of mental inference as critical source of human choice suboptimality</article-title><source>Neuron</source><year>2016</year><volume>92</volume><fpage>1398</fpage><lpage>411</lpage><pub-id pub-id-type="pmid">27916454</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glaze</surname><given-names>CM</given-names></name><name><surname>Kable</surname><given-names>JW</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name></person-group><article-title>Normative evidence accumulation in unpredictable environments</article-title><source>eLife</source><year>2015</year><volume>4</volume><elocation-id>08825</elocation-id><pub-id pub-id-type="pmcid">PMC4584511</pub-id><pub-id pub-id-type="pmid">26322383</pub-id><pub-id pub-id-type="doi">10.7554/eLife.08825</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weiss</surname><given-names>A</given-names></name><name><surname>Chambon</surname><given-names>V</given-names></name><name><surname>Lee</surname><given-names>JK</given-names></name><name><surname>Drugowitsch</surname><given-names>J</given-names></name><name><surname>Wyart</surname><given-names>V</given-names></name></person-group><article-title>Interacting with volatile environments stabilizes hidden-state inference and its brain signatures</article-title><source>Nat Commun</source><year>2021</year><volume>12</volume><elocation-id>2228</elocation-id><pub-id pub-id-type="pmcid">PMC8044147</pub-id><pub-id pub-id-type="pmid">33850124</pub-id><pub-id pub-id-type="doi">10.1038/s41467-021-22396-6</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rouault</surname><given-names>M</given-names></name><name><surname>Weiss</surname><given-names>A</given-names></name><name><surname>Lee</surname><given-names>JK</given-names></name><name><surname>Drugowitsch</surname><given-names>J</given-names></name><name><surname>Chambon</surname><given-names>V</given-names></name><name><surname>Wyart</surname><given-names>V</given-names></name></person-group><article-title>Controllability boosts neural and cognitive signatures of changes-of-mind in uncertain environments</article-title><source>eLife</source><year>2022</year><volume>11</volume><elocation-id>e75038</elocation-id><pub-id pub-id-type="pmcid">PMC9470160</pub-id><pub-id pub-id-type="pmid">36097814</pub-id><pub-id pub-id-type="doi">10.7554/eLife.75038</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Leeuw</surname><given-names>JR</given-names></name></person-group><article-title>jsPsych: a JavaScript library for creating behavioral experiments in a Web browser</article-title><source>Behav Res Methods</source><year>2015</year><volume>47</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="pmid">24683129</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Acerbi</surname><given-names>L</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name></person-group><source>arXiv</source><year>2017</year><elocation-id>1705.04405</elocation-id><comment><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1705.04405">http://arxiv.org/abs/1705.04405</ext-link></comment></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Acerbi</surname><given-names>L</given-names></name></person-group><conf-name>Advances in Neural Information Processing Systems (NeurIPS)</conf-name><person-group person-group-type="editor"><name><surname>Bengio</surname><given-names>S</given-names></name><name><surname>Wallach</surname><given-names>H</given-names></name><name><surname>Larochelle</surname><given-names>H</given-names></name><name><surname>Grauman</surname><given-names>K</given-names></name><name><surname>Cesa-Bianchi</surname><given-names>N</given-names></name><name><surname>Garnett</surname><given-names>R</given-names></name></person-group><year>2018</year><fpage>8213</fpage><lpage>8223</lpage><comment><ext-link ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/8043-variational-bayesian-monte-carlo.pdf">http://papers.nips.cc/paper/8043-variational-bayesian-monte-carlo.pdf</ext-link></comment></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Acerbi</surname><given-names>L</given-names></name></person-group><source>arXiv</source><year>2020</year><elocation-id>2006.08655</elocation-id><comment><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2006.08655">http://arxiv.org/abs/2006.08655</ext-link></comment></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stephan</surname><given-names>KE</given-names></name><name><surname>Penny</surname><given-names>WD</given-names></name><name><surname>Daunizeau</surname><given-names>J</given-names></name><name><surname>Moran</surname><given-names>RJ</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><article-title>Bayesian model selection for group studies</article-title><source>Neuroimage</source><year>2009</year><volume>46</volume><fpage>1004</fpage><lpage>1017</lpage><pub-id pub-id-type="pmcid">PMC2703732</pub-id><pub-id pub-id-type="pmid">19306932</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.03.025</pub-id></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rescorla</surname><given-names>RA</given-names></name><name><surname>Wagner</surname><given-names>AR</given-names></name></person-group><source>A theory of Pavlovian conditioning: variations in the effectiveness of reinforcement and nonreinforcement Classical Conditioning II</source><person-group person-group-type="editor"><name><surname>Black</surname><given-names>AH</given-names></name><name><surname>Prokasy</surname><given-names>WF</given-names></name></person-group><year>1972</year><fpage>64</fpage><lpage>99</lpage><publisher-name>Appleton-Century-Crofts</publisher-name><publisher-loc>New York, NY</publisher-loc></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name></person-group><source>Reinforcement learning: an introduction</source><year>1998</year><publisher-name>MIT Press</publisher-name><publisher-loc>Cambridge, MA</publisher-loc></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Speekenbrink</surname><given-names>M</given-names></name><name><surname>Konstantinidis</surname><given-names>E</given-names></name></person-group><article-title>Uncertainty and exploration in a restless bandit problem</article-title><source>Top Cogn Sci</source><year>2015</year><volume>7</volume><fpage>351</fpage><lpage>367</lpage><pub-id pub-id-type="pmid">25899069</pub-id></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Louie</surname><given-names>KL</given-names></name><name><surname>De Martino</surname><given-names>B</given-names></name></person-group><chapter-title>The neurobiology of context-dependent valuation and choice</chapter-title><source>Neuroeconomics</source><edition>second edition</edition><year>2014</year><fpage>455</fpage><lpage>476</lpage><publisher-name>Academic Press</publisher-name></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palminteri</surname><given-names>S</given-names></name><name><surname>Lebreton</surname><given-names>M</given-names></name></person-group><article-title>Context-dependent outcome encoding in human reinforcement learning</article-title><source>Current Opinion in Behavioral Sciences</source><year>2021</year><volume>41</volume><fpage>144</fpage><lpage>151</lpage></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Experimental task properties and behavioral results</title><p>(<bold>A</bold>) Trial modalities. Two options are shown to the participant. Once a choice is made, the reward associated with the option on that trial is displayed. (<bold>B</bold>) Reward schedule of an option during a block. Colored shapes indicate the reward given for an option. The set of shape options are shown for each block. Bold colored shapes indicate the option chosen on a trial. Vertical lines demarcate the beginning and the end of a block in the Reference (Ref) and the Stochastic (S+) conditions and a reversal in the Volatile (V+) condition. Upper: Bold red horizontal lines show the generative mean for the option corresponding to the correct shape in the Reference and Stochastic conditions. Lower: Bold lines indicate the generative reward mean for the best option drifting throughout the course of a block in the Volatile condition. Shaded areas around lines represent the values of the probability density function from which a reward (shape) was drawn. (<bold>C</bold>) Left: Average accuracy within each condition. Colored dots represent individual participants’ mean accuracy. White dots indicate the median accuracy. Error bars represent the 1st and 3rd quartiles. Shaded areas indicate 1st and 3rd quartiles of the optimal model’s accuracy. Right: Accuracy over time within each condition. The vertical line represents the start of a new block or a reversal. Solid lines indicate the mean accuracy across participants. Dotted lines indicate the mean accuracy of the optimal model. Shaded areas correspond to the SEM. (<bold>D</bold>) Left: Average switching rate within each condition. The vertical line represents the start of a new block or a reversal. Colored dots represent individual participants’ overall switch rate. White dots indicate the median switch rate. Error bars represent the 1st and 3rd quartiles. Shaded areas indicate 1st and 3rd quartiles of the optimal model’s switch rate. Right: Proportion of switches over time within each condition. Solid lines indicate the mean switch rate across participants. Dotted lines indicate the mean accuracy of the optimal model. Shaded areas correspond to the SEM.</p></caption><graphic xlink:href="EMS158947-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Details of the noisy Kalman Filter model</title><p>(<bold>A</bold>) System diagram of the noisy KF model. The model tracks the means <italic>x<sub>t</sub></italic> and variances <italic>v<sub>t</sub></italic> of option values based on the sampling variance <italic>v</italic><sub>sampling</sub>and drift variance <italic>v</italic><sub>drift</sub>of observed rewards <italic>r<sub>t</sub></italic>. The model includes four free parameters, which control the update of option values (α, green box), the corruption of option values by learning noise (ζ, blue box), the decay of unchosen option values to their overall mean (δ, orange box), and the softmax choice policy (τ purple box). Insets illustrate the effects of the learning noise and choice temperature parameters on the distributions of option value and decision probability, respectively. (<bold>B</bold>) Dynamics of the model within the task context (simulated on the V+ condition). Left: The trajectory of the estimated reward for each option over time. Chosen options are highlighted with a white border. Right: Detailed view of the model dynamics. Dots correspond to the tracked values in the left panel. Stars correspond to the reward feedback after choice. Smaller dots following the feedback correspond to potential learned values corrupted by learning noise. Distributions around dots correspond to the estimation uncertainty. Note: this estimation uncertainty increases for unchosen options as the tracked value itself decays over time.</p></caption><graphic xlink:href="EMS158947-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Summary of model fits to participant data</title><p>(<bold>A</bold>) Colored dots represent individual participants’ fits for each parameter. White dots indicate the median fitted parameter value. Error bars represent the 1st and 3rd quartiles. (<bold>B</bold>) Left: Scatter plots of the average accuracy of participants to their best fitting models in each condition. Right: Accuracy over time within each condition. Vertical line represents the start of a new block or a reversal. Solid lines indicate the mean accuracy across participants. Dashed lines indicate the mean accuracy of the best fitting model. (<bold>C</bold>) Left: Scatter plots of the switch rate of participants to their best fitting models in each condition. Right: Proportion of switches over time within each condition. Vertical line represents the start of a new block or a reversal. Solid lines indicate the mean switch rate across participants. Dotted lines indicate the mean accuracy of the best fitting model. Shaded areas correspond to the SEM.</p></caption><graphic xlink:href="EMS158947-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Behavioral expressions of learning and decision noise</title><p>(<bold>A</bold>) Correlations of behavioral measures with learning noise and choice temperature. Learning noise and choice temperature axes are spaced with log-scaling. Shaded areas are 95% CI. (<bold>B</bold>) Participant accuracy and switch rates with respect to learning noise (median split across participants’ best-fitting learning noise in each of the three conditions separately). Statistical significance determined from rank-sum tests on median split participants. (<bold>C</bold>) Participant accuracy and switch rates with respect to choice temperature (median split across participants’ best-fitting choice temperature in each of the three conditions separately). Error bars represent interquartile ranges. Upward triangles and solid lines signify the mean of the upper values of the measure on the median split of the parameter. Downward triangles and dashed lines signify the mean of the measure on the lower values of the median split of the parameter.</p></caption><graphic xlink:href="EMS158947-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Model parameter covariations across participants</title><p>(<bold>A</bold>) Correlation matrix of participants’ fitted parameters. (<bold>B</bold>) Null correlation matrix (correlation structure destroyed) using shuffled parameters for simulations, then fitted using the same procedure. All p-values in correlation matrices corrected for false discovery rates (a = 0.05). (<bold>C</bold>) Covariations of the learning rate, learning noise, and choice temperature between the reference (R) to the stochastic (S+) and the volatile (V+) conditions. Lines in scatter plots indicate the best-fitting regression line (blue: V+ to R; red: S+ to R). Shaded areas are 95% CI. Error bars on bar plots indicate 95% bootstrapped CI on the Spearman’s p value. (<bold>D</bold>) Correlations of learning noise and choice temperature with learning rate. Regression lines fitted on aggregate data pooling all conditions. Shaded areas are 95% CI. Error bars on bar plots indicate 95% bootstrapped CI on the Spearman’s ρ value.</p></caption><graphic xlink:href="EMS158947-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><title>Principal component analysis of model parameters</title><p>(<bold>A</bold>) Percent variance explained up to the first non-contributive PC. Colored dots correspond to the median value of the percent variance explained from the bootstrap procedure. Gray dots are median values of the percent variance explained from PCs of shuffled data. Statistical significance calculated from one-tailed bootstrap significance tests. (<bold>B</bold>) Left: Percent variance explained by the first two PCs within each condition. The first PC, dominated by the variation of choice temperature, explains best the parameter adaptations in the non-volatile conditions. The second PC, dominated by the variation of learning noise, explains best the parameter adaptations in the volatile condition. Upper right: Ingredients and coefficients of the first two principal components. Lower right: Coefficient of determination of each parameter for the PC. All error bars indicate 95% bootstrapped CI.</p></caption><graphic xlink:href="EMS158947-f006"/></fig><fig id="F7" position="float"><label>Figure 7</label><caption><title>Adaptive adjustment of model parameters to uncertainty</title><p>(<bold>A</bold>) Marginal reward costs (reward loss, expressed as % of maximum obtainable reward rate) associated with each source of decision variability through simulations of the suboptimal learning agent, by varying selectively the associated model parameter (either the learning noise ζ or the choice temperature τ while holding all other model parameters constant and set to their best-fitting values. Top: the same amount of learning noise is associated with larger marginal reward cost in the volatile (V+) condition. Bottom: the same choice temperature is associated with smaller marginal reward cost in the volatile (V+) condition. Inset: marginal reward costs associated with observed amounts of learning noise (top) and choice temperatures (bottom) in each of the three conditions. (<bold>B</bold>) Simulated vs. observed adjustments. Top: amounts of learning noise associated with the same marginal reward cost (here, 10% reward loss) in the three conditions. Obtained values (left) match observed learning noise estimates in participants’ data (right). Bottom: choice temperatures associated with the same marginal reward cost (here, 5% reward loss) in the three conditions. Obtained values (left) match observed choice temperature estimates in participants’ data (right).</p></caption><graphic xlink:href="EMS158947-f007"/></fig></floats-group></article>