<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS158908</article-id><article-id pub-id-type="doi">10.1101/2022.12.15.520591</article-id><article-id pub-id-type="archive">PPR585481</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">2</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Reconstructing Spatio-Temporal Trajectories of Visual Object Memories in the Human Brain</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Lifanov</surname><given-names>Julia</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Griffiths</surname><given-names>Benjamin J.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Linde-Domingo</surname><given-names>Juan</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref><xref ref-type="aff" rid="A5">5</xref><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Ferreira</surname><given-names>Catarina S.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Wilson</surname><given-names>Martin</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Mayhew</surname><given-names>Stephen D.</given-names></name><xref ref-type="aff" rid="A7">7</xref></contrib><contrib contrib-type="author"><name><surname>Charest</surname><given-names>Ian</given-names></name><xref ref-type="aff" rid="A8">8</xref></contrib><contrib contrib-type="author"><name><surname>Wimber</surname><given-names>Maria</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A9">9</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><aff id="A1"><label>1</label>School of Psychology and Centre for Human Brain Health (CHBH), University of Birmingham, Birmingham, United Kingdom</aff><aff id="A2"><label>2</label>Mind, Brain and Behavior Research Center, University of Granada, Granada, Spain</aff><aff id="A3"><label>3</label>Department of Experimental Psychology, University of Granada, Granada, Spain</aff><aff id="A4"><label>4</label>Research Group Adaptive Memory and Decision Making, Max Planck Institute for Human Development, Berlin, Germany</aff><aff id="A5"><label>5</label>Center for Adaptive Rationality, Max Planck Institute for Human Development, Berlin, Germany</aff><aff id="A6"><label>6</label>Max Planck Dahlem Campus of Cognition, Max Planck Institute for Human Development, Berlin, Germany</aff><aff id="A7"><label>7</label>Institute of Health and Neurodevelopment (IHN), School of Psychology, Aston University, Birmingham, United Kingdom</aff><aff id="A8"><label>8</label>Département de Psychologie, Université de Montréal, Montréal, Quebec, Canada</aff><aff id="A9"><label>9</label>School of Psychology &amp; Neuroscience and Centre for Cognitive Neuroimaging (CCNi), University of Glasgow, Glasgow, United Kingdom</aff></contrib-group><author-notes><corresp id="CR1"><label>*</label><bold>Contact Information / Corresponding Authors</bold> Maria Wimber, Julia Lifanov <email>maria.wimber@glasgow.ac.uk</email> <email>jxl1118@alumni.bham.ac.uk</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>23</day><month>12</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>15</day><month>12</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Our understanding of how information unfolds when we recall events from memory remains limited. In this study, we investigate whether the reconstruction of visual object memories follows a backward trajectory along the ventral visual stream with respect to perception, such that their neural feature representations are gradually reinstated from late areas close to the hippocampus backwards to lower-level sensory areas. We use multivariate analyses of fMRI activation patterns to map high- and low-level features of the object memories onto the brain during retrieval, and EEG-fMRI fusion to track the temporal evolution of the reactivated patterns. Participants studied new associations between verbs and randomly paired object images in an encoding phase, and subsequently recalled the objects when presented with the corresponding verb cue. Decoding reactivated memory features from fMRI activity revealed that retrieval patterns were dominated by conceptual features, represented in comparatively late visual and parietal areas. Representational-similarity-based fusion then allowed us to map the EEG patterns that emerged at each given time point of a trial onto the spatially resolved fMRI patterns. This fusion suggests that memory reconstruction proceeds from anterior fronto-temporal to posterior occipital and parietal regions, partly in line with a semantic-to-perceptual gradient. A linear regression on the peak time points of reactivated brain regions statistically confirms that the temporal progression is reversed with respect to encoding in ventral stream areas. Together, the results shed light onto the spatio-temporal trajectories along which memories and their constituent features are reconstructed during associative retrieval.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Relative to the substantial literature on visual perception, little is known about how the human brain reconstructs content that is retrieved from memory. We here investigated which features are preferentially reactivated where in the brain and when in time when visual objects are recalled from episodic memory.</p><p id="P3">Visual object perception triggers an information processing cascade along the ventral visual stream which, in the first few hundred milliseconds, follows a gradient of increasing abstraction: Early processing stages are dominated by low-level perceptual representations, while later stages increasingly code for conceptual features (<xref ref-type="bibr" rid="R12">Carlson et al., 2013</xref>; <xref ref-type="bibr" rid="R22">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="R25">Clarke &amp; Tyler, 2015</xref>; <xref ref-type="bibr" rid="R60">Kolb et al., 1995</xref>; <xref ref-type="bibr" rid="R83">Martin et al., 2018</xref>). After approximately 300ms, information reaches the hippocampus, where concept cells represent objects as highly abstract, perceptually invariant concepts (<xref ref-type="bibr" rid="R101">Quiroga, 2012</xref>). The hippocampus presumably binds together information from different sources (including dorsal stream) and serves the longer-term storage and retrieval of associative information (<xref ref-type="bibr" rid="R28">Danker et al., 2017</xref>; <xref ref-type="bibr" rid="R32">Eichenbaum, 2001</xref>; <xref ref-type="bibr" rid="R106">Rolls, 2010</xref>).</p><p id="P4">Computational models assume that the hippocampus can then use a partial reminder to recover the missing, associatively linked information, a process termed pattern completion (<xref ref-type="bibr" rid="R82">Marr, 1971</xref>; <xref ref-type="bibr" rid="R95">O’Reilly &amp; McClelland, 1994</xref>; <xref ref-type="bibr" rid="R106">Rolls, 2010</xref>, <xref ref-type="bibr" rid="R107">2013</xref>; <xref ref-type="bibr" rid="R116">Teyler &amp; DiScenna, 1986</xref>). Pattern completion in turn dictates that neocortical brain regions re-instantiate the various features of the relevant event (<xref ref-type="bibr" rid="R82">Marr, 1971</xref>; <xref ref-type="bibr" rid="R117">Teyler &amp; Rudy, 2007</xref>; <xref ref-type="bibr" rid="R116">Teyler &amp; DiScenna, 1986</xref>; <xref ref-type="bibr" rid="R107">Rolls, 2013</xref>; also see <xref ref-type="bibr" rid="R88">Moscovitch, 2008</xref>), presumably leading to the subjective re-experiencing of a past event (<xref ref-type="bibr" rid="R121">Tulving et al., 1983</xref>).</p><p id="P5">Central to these computational models, they describe memory reinstatement as an all-or-none process and assume that during memory recall, neocortical information is re-established the way it was represented during initial perception. Indices for copy-like encoding-retrieval-similarities (ERS; <xref ref-type="bibr" rid="R112">Staresina et al., 2012</xref>) have been found by various neuroimaging studies using similarity-or classification-based multivariate analyses (see <xref ref-type="bibr" rid="R27">Danker &amp; Anderson, 2010</xref>; <xref ref-type="bibr" rid="R103">Rissman &amp; Wagner, 2012</xref>; <xref ref-type="bibr" rid="R112">Staresina et al., 2012</xref>), including the reinstatement of individual episodes, semantic category, sensory modality, or task context (<xref ref-type="bibr" rid="R130">Wing et al., 2014</xref>; <xref ref-type="bibr" rid="R36">Ferreira et al., 2019</xref>; <xref ref-type="bibr" rid="R42">Griffiths et al., 2019</xref>; <xref ref-type="bibr" rid="R54">Jiang et al., 2020</xref>; <xref ref-type="bibr" rid="R99">Polyn et al., 2005</xref>). Evidence for pattern reinstatement has been found throughout the sensory processing pathways activated during initial event perception, including areas proximal to the hippocampus (<xref ref-type="bibr" rid="R112">Staresina et al., 2012</xref>) as well as more distant neocortical structures (<xref ref-type="bibr" rid="R99">Polyn et al., 2005</xref>; <xref ref-type="bibr" rid="R130">Wing et al., 2014</xref>). Additionally, successful retrieval is associated with the interplay between hippocampal activity and cortical reinstatement (<xref ref-type="bibr" rid="R7">Bosch et al., 2014</xref>; <xref ref-type="bibr" rid="R49">Horner et al., 2015</xref>; <xref ref-type="bibr" rid="R112">Staresina et al., 2012</xref>). Together, these findings support the central premise that hippocampal pattern completion is associated with a more or less truthful reinstatement of encoding patterns. In the present study, our first goal is to decompose visual object memories into perceptual and conceptual components, and to test if both types of features are equally reinstated, in the same areas representing these features during encoding. The latter question is particularly interesting in the light of recent evidence that information is not necessarily decodable from the same regions during encoding and retrieval (<xref ref-type="bibr" rid="R33">Favila et al., 2020</xref>; <xref ref-type="bibr" rid="R133">Xue, 2022</xref>).</p><p id="P6">Moving beyond spatial localization we asked next how the neocortical reinstatement process unfolds over time, and whether different features of a memory are reactivated with different temporal priority. Such prioritisation could naturally emerge from the hippocampus’ differential connectivity with neocortical regions during retrieval. Once pattern completion is initiated, most information is relayed via the subiculum to the entorhinal cortex which in turn projects back to other cortical areas (<xref ref-type="bibr" rid="R17">Chrobak et al., 2000</xref>; <xref ref-type="bibr" rid="R107">Rolls, 2013</xref>; <xref ref-type="bibr" rid="R113">Staresina &amp; Wimber, 2019</xref>). A majority of direct hippocampal and entorhinal efferents terminate in late areas of the visual processing hierarchy where representations cluster in abstract-semantic categories, while there are few connections to low-level sensory areas (<xref ref-type="bibr" rid="R26">Dalton et al., 2022</xref>; <xref ref-type="bibr" rid="R52">Insausti &amp; Muñoz, 2001</xref>). We therefore expect that memories are initially reinstated on a conceptual level, and information coded in lower-level visual areas can only be reached at later stages of memory reconstruction (<xref ref-type="bibr" rid="R52">Insausti &amp; Muñoz, 2001</xref>; <xref ref-type="bibr" rid="R74">Linde-Domingo et al., 2019</xref>). Indeed recent studies using EEG decoding and behavioural responses demonstrated a temporal prioritization of high-level conceptual over perceptual features during memory retrieval (<xref ref-type="bibr" rid="R74">Linde-Domingo et al., 2019</xref>; <xref ref-type="bibr" rid="R73">Lifanov et al., 2021</xref>), mirroring the hierarchy during perception of the same stimuli. Together with studies showing a similar reversed hierarchy and temporal prioritisation during mental imagery as opposed to perception (<xref ref-type="bibr" rid="R29">Dijkstra et al., 2020</xref>; <xref ref-type="bibr" rid="R48">Horikawa &amp; Kamitani, 2017</xref>), these findings suggest that reconstructing an object from memory induces a feed-backward propagation along the ventral visual stream. Where in the brain different memory features are reinstated, and whether the observed conceptual-to-perceptual gradient of feature recall truly maps onto a sensory backpropagation stream, remains an open question that is central to our study.</p><p id="P7">The present study combined fMRI with EEG pattern analysis to test if recalling an object from memory elicits a backwards processing cascade relative to perception. An associative recall paradigm with visual object stimuli similar to previous studies (<xref ref-type="bibr" rid="R74">Linde-Domingo et al., 2019</xref>, <xref ref-type="bibr" rid="R73">Lifanov et al., 2021</xref>) allowed us to differentiate between low-level perceptual and high-level conceptual object features. First, using multivariate analysis of the fMRI patterns only, we compared where in the brain these low- and high-level feature dimensions can be decoded during retrieval as opposed to encoding. Second, we asked whether the object information reconstructed during retrieval propagates backwards along the ventral visual stream, from high-level semantic to low-level sensory areas, using EEG-fMRI fusion to resolve spatial activation patterns in time.</p></sec><sec id="S2" sec-type="results"><title>Results</title><p id="P8">In our memory paradigm, based on <xref ref-type="bibr" rid="R74">Linde-Domingo et al. (2019)</xref>, participants first encoded novel associations between objects and action verbs. Importantly, object images varied along a perceptual (photographs versus line drawings) and a conceptual (animate versus inanimate) dimension that were used for later classification to distinguish between the two hierarchical processing levels. At the recall stage, following a distracter task, participants were asked to retrieve the object in as much detail as possible when presented with the verb, and to indicate the time point of subjective recall with a button press while holding the retrieved image in mind. Following this subjective button press, participants answered either a perceptual (Was the object a photograph/drawing?) or conceptual (Was the object animate/inanimate?) question on each recall trial. Using this paradigm in an fMRI environment, we aimed at pinpointing the retrieval-related reinstatement of different object features in the brain. A searchlight classification approach was used to spatially locate perceptual (photograph versus drawing) and conceptual (animate versus inanimate) representations during image encoding and retrieval. The next aim was to map spatial representations onto a precise timeline preceding the subjective recall button press on a trial-by-trial basis within subjects, using simultaneous EEG-fMRI data acquisition. Since the within-scanner data turned out to be too noisy to decode the relevant object features (see ‘<xref ref-type="sec" rid="S34">Analyses that did not work</xref>’), we instead used a previously acquired out-of-scanner EEG dataset (<xref ref-type="bibr" rid="R74">Linde-Domingo et al., 2019</xref>) to map the spatial representations onto the memory reconstruction timeline. This was done by means of an EEG-fMRI data fusion, using a second-order representational similarity analysis (RSA, <xref ref-type="bibr" rid="R64">Kriegeskorte, 2009</xref>; <xref ref-type="bibr" rid="R65">Kriegeskorte et al., 2006</xref>, <xref ref-type="bibr" rid="R67">2008</xref>; <xref ref-type="bibr" rid="R66">Kriegeskorte &amp; Kievit, 2013</xref>; <xref ref-type="bibr" rid="R91">Nili et al., 2014</xref>).</p><sec id="S3"><title>Behaviour</title><p id="P9">We first analysed the behaviour of participants who completed the fMRI study and descriptively compared their performance with the participants who took part in the out-of-scanner EEG study (<xref ref-type="bibr" rid="R74">Linde-Domingo et al., 2019</xref>). The in- and out-of-scanner encoding data showed comparable RTs and accuracy rates (<xref ref-type="table" rid="T1">Table 1</xref>). However, looking at subjective retrieval RTs (<xref ref-type="fig" rid="F1">Fig. 1d</xref>), participants in the fMRI study were 1.24 s faster on average to indicate subjective retrieval than participants in the EEG study. This RT difference was not only a result of the repeated retrievals in the fMRI group, since RTs of both the first (<italic>M1</italic> = 1.93 s; <italic>SD1</italic> = 0.67 s) and second (<italic>M2</italic> = 1.58 s, <italic>SD2</italic> = 0.66 s) retrieval repetition of the fMRI group were substantially shorter than those of the EEG group. Participants in the fMRI sample might thus have tended to indicate subjective retrieval prematurely, possibly due to discomfort in the scanner environment, and this may have had knock-on effect on the catch question RTs discussed in the next paragraph.</p><p id="P10">After an explorative inspection of the behavioural data, we compared accuracies and RTs between perceptual and conceptual catch questions using paired-sample t-tests (<xref ref-type="table" rid="T2">Table 2</xref>). Note that, different from our previous behavioural work (<xref ref-type="bibr" rid="R74">Linde-Domingo et al., 2019</xref>; <xref ref-type="bibr" rid="R73">Lifanov et al., 2021</xref>), the paradigm used here was optimised for capturing neural retrieval patterns rather than measuring feature-specific reaction times. After cue onset, participants were first instructed to mentally reinstate the object and indicate retrieval by button press. At this stage, we expected participants to have a fully reconstructed image of the recalled object in mind, such that answering the perceptual and conceptual catch question afterwards would take equally long. In the EEG study (also see <xref ref-type="bibr" rid="R74">Linde-Domingo et al., 2019</xref>), participants indeed answered conceptual and perceptual questions equally fast (<italic>t</italic>(23) =.5, <italic>p</italic> =.62 (uncorr.)) and accurately (<italic>t</italic>(23) = -1.80, <italic>p</italic> =.09 (uncorr.)). Participants in the fMRI study, however, performed less accurately (<italic>t</italic>(30) = -6.49, <italic>p</italic> &lt;.01 (uncorr.)) and slower (<italic>t</italic> (30) = 8.47, <italic>p</italic> &lt;.01 (uncorr.)) when answering perceptual compared to conceptual questions. The discrepancy between the EEG and fMRI samples, together with the difference in subjective retrieval time, suggests that participants in the fMRI study often pressed the subjective retrieval button before recall was complete, such that the feature reconstruction process carried over into the catch question period (<xref ref-type="bibr" rid="R74">Linde-Domingo et al., 2019</xref>; <xref ref-type="bibr" rid="R73">Lifanov et al., 2021</xref>).</p><p id="P11">Despite these RT differences, accuracies were comparably high in the two samples. Moreover, in the EEG-fMRI fusion analyses reported further below, the fMRI data is mainly used to derive the spatial representational patterns, while the precise time resolution is provided by the EEG data (for a review see <xref ref-type="bibr" rid="R56">Jorge et al., 2014</xref>). Considering the sluggishness of the hemodynamic response (<xref ref-type="bibr" rid="R39">Friston et al., 1994</xref>; <xref ref-type="bibr" rid="R68">Kruggel &amp; von Cramon, 1999</xref>; <xref ref-type="bibr" rid="R41">Glover, 2011</xref>), the faster button presses in the fMRI sample should thus only have minimal effects on the fusion results.</p></sec><sec id="S4"><title>fMRI multivariate results</title><sec id="S5"><title>fMRI searchlight LDA</title><p id="P12">Previous work suggests that during memory recall, conceptual information becomes accessible earlier than perceptual information, as evidenced in neural pattern decodability (<xref ref-type="bibr" rid="R74">Linde-Domingo et al., 2019</xref>) and behaviour (<xref ref-type="bibr" rid="R73">Lifanov et al., 2021</xref>; <xref ref-type="bibr" rid="R74">Linde-Domingo et al., 2019</xref>, and <xref ref-type="fig" rid="F1">Fig. 1d</xref>). The first aim of the present study was to determine the locus of these perceptual and conceptual representations both during encoding and retrieval. We thus performed a searchlight-based, linear discriminant analysis (LDA). Two separate classifiers were trained to distinguish between the different perceptual (photographs versus drawings) and the different conceptual (animate versus inanimate) classes of our visual objects (see <xref ref-type="sec" rid="S12">methods</xref> for details). During encoding (<xref ref-type="fig" rid="F2">Fig. 2a</xref>), when the object was present on the screen, perceptual class could be decoded in posterior regions along the ventral visual stream including V1, V2, lingual and fusiform gyrus (<italic>t</italic>(30) = 4.98, <italic>p</italic> &lt;.05 (FWE)). Conceptual class was decodable from more anterior lateral temporal lobe areas, including inferior, mid and superior temporal gyrus; and from precuneus, and inferior frontal gyrus, dorsolateral prefrontal cortex and anterior prefrontal cortex (<italic>t</italic>(30) = 4.82, <italic>p</italic> &lt;.05 (FWE)). Together, this mapping generally reflects a posterior-to-anterior perceptual-to-conceptual processing gradient. The results largely mirror the univariate analyses reported in the supplemental materials (<xref ref-type="fig" rid="F2">Fig. 2</xref>, <xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. 1</xref>) and complement them by showing additional content decoding in frontal and parietal regions, suggesting that these areas code information in fine-grained multi-voxel patterns that univariate analyses are unable to capture.</p><p id="P13">During retrieval (<xref ref-type="fig" rid="F2">Fig. 2b</xref>), perceptual features were most strongly decodable in the precentral gyrus (i.e. premotor cortex), but also in precuneus, angular gyrus, cingulate cortex, inferior frontal gyrus and in ventral areas including V2, fusiform gyrus and middle temporal lobe (<italic>t</italic>(30) = 4.80, <italic>p</italic> &lt;.05 (FWE)). Conceptual category membership was classified with the highest accuracy from fusiform gyrus, middle temporal gyrus, temporal pole, precuneus, angular gyrus, dorsolateral prefrontal cortex, and inferior and superior frontal gyrus (<italic>t</italic>(30) = 4.60, <italic>p</italic> &lt;.05 (FWE); for detailed LDA results see tables 3-6 in the Supplementary file). Hence, in addition to the ventral visual areas dominating encoding/perception, we also found extensive frontal and parietal areas in our retrieval searchlight, in line with previous work suggesting that mnemonic as opposed to sensory content can often be decoded from these areas (<xref ref-type="bibr" rid="R34">Favila et al., 2018</xref>, <xref ref-type="bibr" rid="R33">2020</xref>; <xref ref-type="bibr" rid="R36">Ferreira et al., 2019</xref>; <xref ref-type="bibr" rid="R69">Kuhl &amp; Chun, 2014</xref>). Together, the fMRI-based classification of the two dimensions that we explicitly manipulated demonstrates that brain activity patterns during the retrieval of object images carry information about perceptual format and semantic content, with the latter dominating the recall patterns. Reinstatement was present in some of the areas seen at encoding, but additionally comprised regions in fronto-parietal networks.</p><p id="P14">No voxels survived (at p &lt;.05, FWE-corrected) when cross-classifying perceptual or conceptual categories from encoding to retrieval, or from retrieval to encoding, which might be due to the overall noisier retrieval patterns or a transformation of these category representations between encoding and retrieval (see <xref ref-type="sec" rid="S10">Discussion</xref>).</p></sec></sec><sec id="S6"><title>EEG-fMRI data fusion</title><sec id="S7"><title>EEG multivariate analysis</title><p id="P15">The next goal of this study was to map the spatial patterns in a given brain area onto the EEG patterns present at each time point of a trial, to reveal how content representations evolve over the time course of encoding and retrieval. For this data fusion, we first needed to obtain a meaningful EEG matrix representing object-to-object similarities for each time point of encoding and retrieval, which we could then compare to the fMRI-based matrices. Correlation-based RDMs for the EEG data were too noisy to see a meaningful match between modalities (see section ‘Analyses that did not work’). We thus created a classification-based matrix representing the dissimilarities between each pair of objects, in each EEG time bin, based on a cross-participant classifier. Amplitudes over the 128 electrodes at a given time point served as classification features, and our 24 participants provided the repetitions of each object. The pairwise classification accuracies were entered in a single, time resolved RDM structure representing the dissimilarity between individual objects of our stimulus pool collapsed across participants (<xref ref-type="fig" rid="F3">Fig. 3a</xref>). For all fusion analyses, we investigated encoding timelines from image onset, and retrieval timelines leading up to the subjective button press.</p></sec><sec id="S8"><title>ROI fusion</title><p id="P16">Having obtained this EEG-based similarity time course, we performed a similarity-based fusion of the EEG and fMRI data, probing what brain regions (in the fMRI data) most closely matched the similarity structures contained in the EEG data at any given time point during encoding and retrieval. For a first fusion step, we used the representational geometries (i.e., the RDMs) from an anatomically pre-defined set of regions of interest (ROIs). We then calculated the correlation between these ROIs’ representational geometries and the geometries obtained from the EEG data for each time bin (<xref ref-type="fig" rid="F3">Fig. 3a</xref>), and statistically compared the correlations against zero using cluster-basted permutation testing.</p><p id="P17">To statistically evaluate whether the information flow is primarily forward or backward along ventral visual stream areas, we used a linear regression on the cumulative sums of the correlation time courses of the corresponding ROIs, a method previously established for MEG analyses (<xref ref-type="bibr" rid="R85">Michelmann et al., 2019</xref>). The intuition of this method is that if region A starts accumulating information before region B, and region B before region C, the cumulative sums will line up sequentially to reflect this systematic delay in information accumulation (see simulation in <xref ref-type="fig" rid="F3">Fig. 3b-d</xref>). In our case, we expected forward progressing latencies along the ventral visual stream during encoding and backward progressing latencies during retrieval (<xref ref-type="fig" rid="F3">Fig. 3b-d</xref>).</p><p id="P18">Our predefined set of ROIs included regions along the ventral visual stream (V1/V2, hIT, temporal pole, and MTL), and regions along the dorsal stream (superior parietal lobe, lateral inferior parietal cortex, and medial parietal cortex; see section ROIs). Since many computational models propose that the role of the hippocampus is related to association or indexing rather than feature representation (<xref ref-type="bibr" rid="R32">Eichenbaum, 2001</xref>; <xref ref-type="bibr" rid="R84">McClelland et al., 1995</xref>; <xref ref-type="bibr" rid="R96">O’Reilly &amp; Norman, 2002</xref>; <xref ref-type="bibr" rid="R106">Rolls, 2010</xref>, <xref ref-type="bibr" rid="R107">2013</xref>; <xref ref-type="bibr" rid="R116">Teyler &amp; DiScenna, 1986</xref>), we excluded the hippocampus from the MTL regions. In <xref ref-type="fig" rid="F4">Fig. 4</xref>, correlation time courses are plotted for each individual ROI as t-values resulting from t-tests against zero, with clusters resulting from a cluster-based permutation test (see section ‘Analyses’). The figure shows ventral visual regions in panel a, and dorsal regions in panel b.</p><p id="P19">Following the object onset at encoding (<xref ref-type="fig" rid="F4">Fig. 4a-b</xref>), posterior regions including V1 and V2 showed an increasing correlation with the EEG representations from approximately 120 ms (p &lt;.01 (uncorr.)). A significant cluster was seen from 240 ms onwards and another one at 1.31 s (p &lt;.05 (cluster)). The first cluster was followed by a correlation increase of later ventral visual areas (human inferior temporal cortex (hIT)) around 310 ms (p &lt;.01 (uncorr.)), reaching a significant peak at about 420 ms (p &lt;.05 (cluster)). Later lateral and medial temporal as well as parietal regions did not show a significant correlation with the EEG time series during encoding.</p><p id="P20">As a proof of principle, we then tested for a forward processing stream in the encoding data across ventral visual areas. Performing a linear regression on the cumulative sums of all ventral ROI time courses at each time point (see <xref ref-type="sec" rid="S12">Methods</xref> and <xref ref-type="fig" rid="F3">Fig. 3b-c</xref>), we found negative slopes reaching significance 500 ms after stimulus onset (p &lt;.05 (cluster)). This finding statistically corroborates the observation that earlier ventral visual regions code relevant information before later regions, supporting the well-established forward stream (<xref ref-type="fig" rid="F4">Fig. 4e</xref>). It should be noted, however, that evidence for such feed-forward processing was only present relatively late in the trial, likely since the strongest peaks of object identity decoding also fell in this late time window (see <xref ref-type="fig" rid="F4">Fig. 4</xref>, <xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. 1</xref>).</p><p id="P21">When performing the same analysis for ventral visual ROIs during retrieval (<xref ref-type="fig" rid="F4">Fig. 4c</xref>), several regions peaked below the threshold of cluster significance: MTL regions showed a peak correlation with the EEG representational geometries -2.96 s, -2.82 s and -1.97 s prior to the retrieval button press (p &lt;.01 (uncorr.)). Further, correlation peaks were found for the temporal pole at -2.13 s (p &lt;.01 (uncorr.)). HIT showed a peak correlation with the EEG geometry at -1.33 s and later at -1.22s (p &lt;.01 (uncorr.)), with a peak by V1/V2 at -1.33 s before button press (p &lt;.01 (uncorr.)).</p><p id="P22">Within the dorsal set of ROIs (<xref ref-type="fig" rid="F4">Fig. 4d</xref>), the inferior medial parietal lobe showed a correlation peak in a similar time window, at -1.33 s before button press (p &lt;.05 (cluster)). This peak in the inferior parietal, but no other parietal regions, coincided with the peak times of posterior ventral visual areas (V1/V2/hIT).</p><p id="P23">Our main aim of the data fusion was to test if memory reactivation followed a backward processing hierarchy along the ventral visual stream. We therefore also performed a linear regression on cumulative sums of ventral visual regions within retrieval. This analysis revealed a significant positive slope from -2.2 to -1.4 s before button press (p &lt;.05 (cluster)), speaking in favour of a backward stream and thus confirming our main hypothesis (<xref ref-type="fig" rid="F4">Fig. 4f</xref>).</p></sec><sec id="S9"><title>Searchlight fusion</title><p id="P24">Next, in a more explorative but also spatially better resolved approach, we conducted a whole-brain searchlight fusion to inspect where across the brain the fMRI-based representational geometries matched the EEG geometries over time. <xref ref-type="fig" rid="F5">Figure 5</xref> depicts spatial t-maps representing significant EEG-fMRI correlations in each searchlight radius for a given time bin, separately at encoding (<xref ref-type="fig" rid="F5">Fig. 5a</xref>) and retrieval (<xref ref-type="fig" rid="F5">Fig. 5b</xref>). The maps only show time points where significant spatial clusters emerged. During encoding, the searchlight fusion revealed a significant cluster in V1 and lingual gyrus at 60 ms following object onset, and again from 130 to 160 ms, and from 240 ms onwards, reactivating several more times (e.g. see timepoints 940 ms and 1.3 s) before fading around 1.4 s after stimulus onset. Importantly, over the time course of object perception, significant clusters of EEG-fMRI similarity gradually extended from early visual (V1, V2) to more ventral and lateral areas, including fusiform and inferior temporal gyrus. For the encoding data, the results of the searchlight approach thus overlap with the ROI fusion results, and mainly reveal ventral visual stream activation progressing in a forward (early-to-late visual cortex) manner.</p><p id="P25">During retrieval, the earliest cluster of significant EEG-fMRI correlations was found in the fusiform area, preceding the subjective retrieval button press by -2.82 s, accompanied by a small cluster in the pons. Contrary to our prediction, we also observed pattern overlap in hippocampus at -1.98 s before button press.</p><p id="P26">At time points closer to the button press, additionally clusters showing significant pattern correlations were found in lower-level regions. At -1.33 s, V1, V2, V4d and fusiform gyrus showed significant clusters. Further, at -1.25 s, clusters were found in premotor cortex, primary motor cortex, superior parietal lobule (SPL). Finally, clusters were found in premotor cortex, SPL, primary and associative visual cortex, fusiform area and Wernicke’s area at -1.22 s before subjective retrieval.</p><p id="P27">Together, the retrieval data showed convergence between the ROI- and searchlight fusion approach. It showed a progression of the peak correlations from fronto-temporal to posterior sensory regions, generally in line with a backward propagation hierarchy. This pattern of results is in line with previous work showing a conceptual-to-perceptual gradient in the same EEG dataset (<xref ref-type="bibr" rid="R74">Linde-Domingo et al., 2019</xref>). The present results additionally suggest that closest to the time point of subjective recollection (approximately -1.3 to -1.22 s preceding the button press), content information is most prominently represented in fronto-parietal and low-level visual areas.</p></sec></sec></sec><sec id="S10" sec-type="discussion"><title>Discussion</title><p id="P28">What mnemonic content is recovered where in the brain during memory retrieval, and how does the hippocampal-neocortical pattern completion process unfold in time? Recent memory research suggests that the information processing hierarchy is reversed during the recall of a visual object from episodic memory compared with its initial perception (<xref ref-type="bibr" rid="R74">Linde-Domingo et al., 2019</xref>; <xref ref-type="bibr" rid="R73">Lifanov et al., 2021</xref>; <xref ref-type="bibr" rid="R86">Mirjalili et al., 2021</xref>), with conceptual features becoming available earlier than perceptual features. Here, we investigated the locus of these feature representations during encoding and recall using fMRI-based decoding. Additionally, EEG-fMRI fusion allowed us to test whether this presumed reversed information processing cascade during memory reconstruction maps onto the same ventral visual stream areas that carry the information forward during perception, but now following a backward trajectory. Our study shows that recall is largely dominated by conceptual information in the posterior ventral regions, and that the reinstatement of visual object memories follows a primarily backward trajectory along ventral visual object processing pathways, involving an additional information relay to parietal and frontal regions.</p><p id="P29">We first used uni- and multivariate analyses on the fMRI data alone to map out the regions processing perceptual and conceptual object dimensions that were built into the stimulus set. As expected, during encoding when the object was visible on the screen, these features largely mapped onto the ventral visual pathway, where early visual areas coded the perceptual features (coloured photos versus black-and-white line drawings), whereas later visual areas coded the mid- to higher-level conceptual information (animate versus inanimate objects; <xref ref-type="bibr" rid="R76">Long et al., 2018</xref>). This general perceptual-to-conceptual gradient is in line with a multitude of findings in the basic object recognition and vision literature (<xref ref-type="bibr" rid="R12">Carlson et al., 2013</xref>; <xref ref-type="bibr" rid="R22">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="R63">Kravitz et al., 2013</xref>).</p><p id="P30">Univariate analyses were not powerful enough to detect differential activations between perceptual and conceptual categories when objects were reconstructed from memory. Activity patterns at the finer-grained multi-voxel level, however, carried information about the categorical features of the retrieved images. Memory-related reactivation of these features comprised some regions also found during object encoding, particularly in late visual areas. This overlap confirms a body of previous work showing that posterior visual areas are not only involved in visual perception but also in internally generated processes, such as mental imagery (<xref ref-type="bibr" rid="R30">Dijkstra et al., 2021</xref>; <xref ref-type="bibr" rid="R61">Kosslyn et al., 1993</xref>) or episodic memory. While most memory studies find reactivation in late areas along the ventral stream (e.g., <xref ref-type="bibr" rid="R42">Griffiths et al., 2019</xref>; <xref ref-type="bibr" rid="R99">Polyn et al., 2005</xref>; <xref ref-type="bibr" rid="R112">Staresina et al., 2012</xref>), others have reported areas as early as V1 (<xref ref-type="bibr" rid="R7">Bosch et al., 2014</xref>). It is also commonly observed that stronger reinstatement is associated with memory success and strength (e.g. <xref ref-type="bibr" rid="R50">Huijbers et al., 2011</xref>; <xref ref-type="bibr" rid="R105">Ritchey et al., 2013</xref>; <xref ref-type="bibr" rid="R112">Staresina et al., 2012</xref>; <xref ref-type="bibr" rid="R118">Thakral et al., 2015</xref>; <xref ref-type="bibr" rid="R130">Wing et al., 2014</xref>) and the vividness and detail of remembering (<xref ref-type="bibr" rid="R6">Bone et al., 2020</xref>; <xref ref-type="bibr" rid="R110">Simons et al., 2022</xref>; <xref ref-type="bibr" rid="R115">St-Laurent et al., 2015</xref>; <xref ref-type="bibr" rid="R128">Wheeler et al., 2000</xref>), suggesting an important functional role of sensory reactivation. Assuming a hierarchical reverse processing from higher to lower sensory regions during recall, incomplete retrieval would lead primarily to a loss of perceptual information. Premature retrieval button presses, as found in the present study (see behavioural results), could indicate such incomplete recall and explain the overall weaker decodability of perceptual features.</p><p id="P31">Although the spatial localizations of feature-specific patterns during recall partly overlapped with those found during encoding, they also encompassed regions outside the ventral visual object processing pathways. Most notably, conceptual object information during recall was decodable from frontal and parietal regions. The role of fronto-parietal cortex in representing mnemonic information is still debated (<xref ref-type="bibr" rid="R11">Buckner et al., 1999</xref>; <xref ref-type="bibr" rid="R34">Favila et al., 2018</xref>, <xref ref-type="bibr" rid="R33">2020</xref>; <xref ref-type="bibr" rid="R51">Humphreys et al., 2021</xref>; <xref ref-type="bibr" rid="R62">Kramer et al., 2005</xref>; <xref ref-type="bibr" rid="R72">Levy, 2012</xref>; <xref ref-type="bibr" rid="R113">Staresina &amp; Wimber, 2019</xref>; <xref ref-type="bibr" rid="R133">Xue, 2022</xref>). A shift of content decoding from sensory areas during perception to multimodal fronto-parietal areas during memory recall is a frequent observation, and has recently been discussed as indicating a representational transformation (<xref ref-type="bibr" rid="R34">Favila et al., 2018</xref>, <xref ref-type="bibr" rid="R33">2020</xref>; <xref ref-type="bibr" rid="R131">Xiao et al., 2017</xref>; <xref ref-type="bibr" rid="R133">Xue, 2022</xref>), potentially going along with a semanticization of the retrieved content (<xref ref-type="bibr" rid="R36">Ferreira et al., 2019</xref>; <xref ref-type="bibr" rid="R73">Lifanov et al., 2021</xref>). Some studies suggest that depending on the task and functional state (i.e. encoding or retrieval), content representations are preferentially expressed in sensory or parietal cortices, respectively, and mediated by differential connectivity with the hippocampus (<xref ref-type="bibr" rid="R34">Favila et al., 2018</xref>; <xref ref-type="bibr" rid="R77">Long &amp; Kuhl, 2021</xref>; for a related review about the posterior medial network see <xref ref-type="bibr" rid="R104">Ritchey &amp; Cooper, 2020</xref>). Others propose that parietal areas provide a contextualization of the retrieved memories (<xref ref-type="bibr" rid="R55">Jonker et al., 2018</xref>). Our findings support the previously described encoding-retrieval distinction and suggest that it is primarily conceptual information (<xref ref-type="fig" rid="F2">Fig. 2b</xref>) that is represented in parietal regions during retrieval, in line with previous work showing strong object identity - and even abstract concept coding in parietal cortex (<xref ref-type="bibr" rid="R36">Ferreira et al., 2019</xref>; <xref ref-type="bibr" rid="R53">Jeong &amp; Xu, 2016</xref>; <xref ref-type="bibr" rid="R57">Kaiser et al., 2022</xref>). The involvement of parietal networks is also discussed further below in relation to the timing of the reactivation.</p><p id="P32">Going beyond a purely spatial mapping of perceptual and conceptual representations, fusing the EEG and fMRI data allowed us to inject time information into these spatial maps, and to ask how the memory reconstruction stream evolves, building up to the time of subjective recollection. We used two complementary approaches for data fusion, both comparing the representational geometries found in the EEG patterns at each time point with the fMRI geometries found in a given brain region. Both the ROI-based and searchlight-based approach showed a mainly feedforward sweep of information processing during the first few hundred milliseconds of object perception, starting within early visual regions approximately 60 (searchlight fusion) to 120 ms (ROI fusion) after image onset, and spreading to more ventral visual regions within the next 200 ms. Importantly, a formal regression analysis allowed us to statistically corroborate the spatio-temporal direction of information flow, by modelling the information accumulation rate across the hierarchy of ventral visual stream regions (<xref ref-type="bibr" rid="R85">Michelmann et al., 2019</xref>). This regression analysis also showed a clear forward, posterior-visual to anterior-temporal progression of the correlation patterns during early phases of the encoding trials.</p><p id="P33">The EEG-fMRI fusion and sequence analysis applied to the retrieval data revealed a largely backward information processing trajectory, with information flowing from medial temporal lobe (both fusion approaches) and temporal pole (ROI fusion) to more posterior visual regions (both fusion approaches). The temporal pole has previously been associated with processing of high-level semantic information (e.g. <xref ref-type="bibr" rid="R24">Clarke, 2020</xref>; <xref ref-type="bibr" rid="R92">Noppeney &amp; Price, 2002</xref>; <xref ref-type="bibr" rid="R97">Patterson et al., 2007</xref>; <xref ref-type="bibr" rid="R102">Rice et al., 2018</xref>; <xref ref-type="bibr" rid="R125">Visser et al., 2010</xref>). Moreover, temporal pole reactivation around -2.1 s before button press was close to the time point when objects from different conceptual classes were more discriminable than objects from similar conceptual classes based on the EEG classification alone (see <xref ref-type="fig" rid="F4">Fig. 4</xref>, <xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. 1b</xref>). Note, that our decoding peaks were found considerably earlier than in previous work (<xref ref-type="bibr" rid="R74">Linde-Domingo et al., 2019</xref>), likely due to the cross-participant object-identity decoding approach (for comparison see <xref ref-type="bibr" rid="R74">Linde-Domingo et al., 2019</xref>; <xref ref-type="bibr" rid="R86">Mirjalili et al., 2021</xref>) which may rely on higher-level exemplar-level information presumably processed close to the hippocampus (e.g. <xref ref-type="bibr" rid="R25">Clarke &amp; Tyler, 2015</xref>). Together with our previous work using feature-specific reaction times (<xref ref-type="bibr" rid="R74">Linde-Domingo et al., 2019</xref>; <xref ref-type="bibr" rid="R73">Lifanov et al., 2021</xref>), these findings indicate that conceptual object features in semantic networks are among the first to be reactivated during memory retrieval.</p><p id="P34">More posterior areas including inferior temporal, inferior parietal and visual cortices, as well as frontal regions, reached their representational correlation peaks considerably later, from -1.33 s before button press onwards (<xref ref-type="fig" rid="F4">Fig. 4c-d</xref> &amp; <xref ref-type="fig" rid="F5">5b</xref>). While visual activations can be assumed to represent the perceptual content of the reactivated memories, the role of fronto-/parietal regions, as mentioned earlier, is still a matter of debate (e.g. <xref ref-type="bibr" rid="R5">Barry &amp; Maguire, 2019</xref>; <xref ref-type="bibr" rid="R37">Fischer et al., 2021</xref>; <xref ref-type="bibr" rid="R89">Naghavi &amp; Nyberg, 2005</xref>). In addition to the literature discussed above, parietal areas have been shown to play a role in contextual processing, imagery during recall and scene construction (<xref ref-type="bibr" rid="R78">Lundstrom et al., 2005</xref>; <xref ref-type="bibr" rid="R38">Fletcher et al., 1995</xref>; <xref ref-type="bibr" rid="R14">Chrastil, 2018</xref>). The latest reactivations might thus be indicative of a final stage of reinstatement leading up to subjective recollection, likely involving working memory and memory-related imagery, and hence, preparation for the upcoming categorisation task (<xref ref-type="bibr" rid="R15">Christophel et al., 2012</xref>, <xref ref-type="bibr" rid="R16">2017</xref>; <xref ref-type="bibr" rid="R40">Ganis et al., 2004</xref>). In summary, contrary to encoding, we find that retrieval follows a backward propagating stream indicative of a conceptual-to-perceptual reconstruction gradient. Moreover, the backward reconstruction flow is not limited to ventral visual brain areas that dominate the encoding patterns, but instead involves frontal and parietal regions that may serve as an episodic memory or imagery buffer for the retrieved representations (<xref ref-type="bibr" rid="R3">Baddeley, 1998</xref>, <xref ref-type="bibr" rid="R4">2000</xref>; <xref ref-type="bibr" rid="R72">Levy, 2012</xref>; <xref ref-type="bibr" rid="R126">Wagner et al., 2005</xref>).</p><p id="P35">The anterior-to-posterior retrieval stream shown here is reminiscent of a similar reversed stream for object imagery in absence of an episodic cue (<xref ref-type="bibr" rid="R29">Dijkstra et al., 2020</xref>), which has also been described as progressing in a hierarchical fashion along the ventral stream (<xref ref-type="bibr" rid="R48">Horikawa &amp; Kamitani, 2017</xref>). Generating mental images from general knowledge compared to cuing them from episodic memory likely involve different neural sources, in the latter case most notably the hippocampus. However, both processes rely on the internal generation of stored mental representations (i.e., memories), and it is thus perhaps not surprising that mental imagery and episodic retrieval overlap in many aspects, including their reverse reconstruction gradient. While there have been various studies comparing imagery with perception (<xref ref-type="bibr" rid="R29">Dijkstra et al., 2020</xref>; <xref ref-type="bibr" rid="R48">Horikawa &amp; Kamitani, 2017</xref>; <xref ref-type="bibr" rid="R132">Xie et al., 2020</xref>), it will be interesting for future studies to provide a more detailed picture of where in the processing hierarchy imagery and memory retrieval overlap.</p><p id="P36">The fMRI classification and the RSA-based EEG-fMRI fusion in this study offer complementary information. RSA is a highly useful tool to enable the comparison of neural representations measured by different neuroimaging modalities (<xref ref-type="bibr" rid="R66">Kriegeskorte &amp; Kievit, 2013</xref>). By creating similarity structures from EEG and fMRI activity patterns in the same format, it becomes possible to directly correlate the representational geometries emerging in neural space and time. However, RSA is in itself blind to the informational content that is driving the match in representations (see <xref ref-type="bibr" rid="R109">Schyns et al., 2020</xref>). Classification-based approaches on fMRI (or EEG) data, on the other hand, can pinpoint categorical representations that are explicitly built into the experimental design. However, they are restricted to very few categories (e.g., animacy, colour), in turn limiting the conclusions that can be drawn about the certainly much richer and more multidimensional content of mental (memory) representations. In sum, RSA and categorical decoding methods offer complementary insights into the content of reactivated memories, likely explaining why in our study, some correlation peaks in the searchlight fusion maps during retrieval did not overlap with the sources of either perceptual or conceptual features. Richer study designs with more stimulus variations on a trial-by-trial basis (<xref ref-type="bibr" rid="R109">Schyns et al., 2020</xref>) are a promising avenue for revealing what type of content representations are contained in, and dominate, reactivated memories. An exciting recent development is the use of deep neural networks (DNNs) trained on image categorization, and to compare the networks’ layer patterns to the representational patterns that emerge in the real brain (<xref ref-type="bibr" rid="R1">Allen et al., 2022</xref>; <xref ref-type="bibr" rid="R19">Cichy et al., 2019</xref>). Some recent work has employed this approach to shed light onto the nature of reactivated memory representations, for example to investigate what level of feature reactivation predicts the vividness and distinctiveness of a memory (<xref ref-type="bibr" rid="R6">Bone et al., 2020</xref>). Others have used DNNs in comparison with intracranial EEG to reveal memory transformations over time and across different processing stages (<xref ref-type="bibr" rid="R75">Liu et al., 2020</xref>). Finally, different types of models, including word embedding models, have been used to uncover the temporal and content structure of recall narratives, and how it compares to the original encoding (<xref ref-type="bibr" rid="R46">Heusser et al., 2021</xref>). In contrast to the relative black box of DNN layers, the latter models offer more transparency and interpretability of the dimensions contained in a memory, and a route to understanding how the recall of naturalistic, continuous events unfolds in time.</p><p id="P37">In summary, our study supports the idea of a conceptual-to-perceptual information processing gradient that is reversed with respect to encoding along ventral visual stream regions processing object identity. Additionally, we show that the backward trajectory of memory reconstruction is not limited to ventral visual stream regions involved during encoding, engaging multisensory fronto-parietal areas at distinct stages of retrieval processing. The present findings demonstrate how the fusion of temporally and spatially resolved methods can further our understanding of memory retrieval as a staged feature reconstruction process, tracking how the reconstruction of a memory trace unfolds in time and space.</p></sec><sec id="S11" sec-type="methods"><title>Methods</title><sec id="S12" sec-type="subjects"><title>Participants</title><p id="P38">We acquired fMRI data of 37 right-handed, fluent English-speaking participants at the University of Birmingham (26 females, 11 males, mean age (<italic>M<sub>age</sub></italic>) = 23.33, standard deviation (<italic>SD<sub>age</sub></italic>) = 3.89, one participant did not indicate their age). The a priori planned sample size for the full EEG-fMRI dataset was <italic>n</italic> = 24 subjects (as in <xref ref-type="bibr" rid="R74">Linde-Domingo et al., 2019</xref>). However, due to poor data quality and technical failures in 13 of the simultaneous EEG datasets, additional subjects were recorded leading to a larger sample size for the fMRI data alone. Three subjects were excluded from the fMRI analysis due to failed scanning sequences, and three additional subjects were excluded due to extensive motion within the scanner, exceeding the functional voxel size, such that 31 fMRI datasets remained for analysis. All participants were informed about the experimental procedure, underwent imaging safety screening and signed an informed consent. The research was approved by the STEM ethics committee of the University of Birmingham.</p><p id="P39">For the fusion analyses, we also included a previously published EEG dataset including the same stimulus set and a nearly identical associative learning and retrieval paradigm (<xref ref-type="bibr" rid="R74">Linde-Domingo et al., 2019</xref>). This additional dataset included 24 further participants with a clean, out-of-scanner EEG. For further information on the EEG data sample and the related ethical procedures, we refer to previous work (<xref ref-type="bibr" rid="R74">Linde-Domingo et al., 2019</xref>).</p></sec><sec id="S13" sec-type="materials"><title>Material</title><p id="P40">The paradigm was a visual verb-object association task (<xref ref-type="bibr" rid="R74">Linde-Domingo et al., 2019</xref>) adapted for fMRI in terms of timing. Stimuli included 128 action verbs and 128 pictures of everyday objects (<xref ref-type="bibr" rid="R74">Linde-Domingo et al., 2019</xref>). Action verbs were chosen because they do not elicit concrete object images in themselves but are still easy to associate with the object images. Importantly, all object images existed in two perceptual categories, a coloured photograph from the BOSS database (<xref ref-type="bibr" rid="R9">Brodeur et al., 2010</xref>, available on <ext-link ext-link-type="uri" xlink:href="https://sites.google.com/site/bosstimuli/home">https://sites.google.com/site/bosstimuli/home</ext-link>) or a black line drawing version of a respective photograph created by <xref ref-type="bibr" rid="R74">Linde-Domingo et al. (2019)</xref> by means of the free and open source GNU image manipulation software (<ext-link ext-link-type="uri" xlink:href="https://www.gimp.org/">www.gimp.org</ext-link>). Further each object belonged to one of two conceptual categories, i.e. animate vs inanimate. We selected 128 images per participant according to a fully balanced scheme, such that each combination of perceptual and conceptual categories included the same number of pictures (32 animate-photographs, 32 animate-drawings, 32 inanimate-photographs, 32 inanimate-drawings; <xref ref-type="fig" rid="F1">Fig. 1a</xref>). With respect to the later fusion with the out-of-scanner EEG data, it is important to note that while both experiments used the same set of 128 objects, the same object could appear in different perceptual versions between participants, due to the pseudo-randomised image selection. For example, an image of a camel could be shown as a photograph to one participant, and as a line drawing to another. Action verbs were randomly assigned to images in each participant and were presented together with pictures centrally overlaid on a white background.</p></sec><sec id="S14" sec-type="methods"><title>Procedure</title><p id="P41">Before the session, participants were informed about the experimental procedure and asked to perform a training task block in front of the computer. After completion of the training, the experimental session in the fMRI scanner included four runs with four task blocks each, summing up to a total of 16 task blocks. A typical block in the training and experimental task included the encoding of eight novel associations, a 20 s distractor task, and 16 retrieval trials (two repetitions per association). A 3 min break was included after each fMRI run, in which participants were asked to rest and close their eyes. In total, it took participants approximately 70 min to perform the entire task.</p></sec><sec id="S15"><title>Encoding</title><p id="P42">In the encoding phases (<xref ref-type="fig" rid="F1">Fig. 1b</xref>), participants were instructed to study eight novel verb-object pairings in random order. A trial started with a fixation cross presented for a jittered period between 500 and 1500 ms. The action verb was presented for 500 ms before an object was shown for a maximum duration of 5 s. To facilitate learning, participants were instructed to form a vivid visual mental image using the verb-object pairing, and once formed, to press a button with their thumb, which moved the presentation on to the next trial. Importantly, due to the artificial setting where there is no predictive structure to the events, we expected very limited top-down feedback and hence a dominating feedforward sweep during perception early on.</p></sec><sec id="S16"><title>Distractor</title><p id="P43">After each encoding phase, participants performed a self-paced distractor task for 20 s, indicating as fast as possible whether each of the consecutively presented numbers on the screen was odd or even, pressing a button with their index or middle finger, respectively. Feedback on the percentage of correct responses was provided at the end of each distractor phase.</p></sec><sec id="S17"><title>Retrieval</title><p id="P44">In the retrieval phases, participants were instructed to recall the previously associated objects upon presentation of the corresponding verb (<xref ref-type="fig" rid="F1">Fig. 1b</xref>). Trials started with the presentation of a fixation cross, jittered between 500 and 1500 ms and followed by a previously encoded action verb, presented for 500 ms. Cued with the verb, participants were instructed to recall the paired object within a maximum of 5 s, while a black fixation cross was presented on screen. Note that this maximum duration was shorter than in the original task (<xref ref-type="bibr" rid="R74">Linde-Domingo et al., 2019</xref>), which might have caused participants to react faster and possibly explains RT differences between the in-scanner and out-of-scanner datasets (<xref ref-type="fig" rid="F1">Fig. 1d</xref>). Participants were asked to indicate the time point of image recall by button press with their thumb, at which point the black fixation cross turned grey and was presented for an additional 3 s. This retrieval button press was meant to mark the time point of subjective recollection. During the remaining 3 s participants were asked to hold the mental image of the object in mind as vividly as possible. Last, they were asked about the perceptual (Was the object a photograph or a line drawing?) or conceptual (Was the object animate or inanimate?) features of the recalled object, answering with their index or middle finger within a maximum response period of 5 s. During the presentation of the catch question, participants also had the option to indicate with their ring finger that they forgot the features of the corresponding object. Importantly, each encoded stimulus was retrieved twice, once with a perceptual and once with a conceptual question. Trial order was pseudo-random within the first and second set of repetitions, with a minimum of two intervening trials before a specific object was recalled for the second time. The order of catch questions was counterbalanced across repetitions such that half of the associations were first probed with a perceptual question, and the other half was first probed with a conceptual question.</p><sec id="S18"><title>fMRI data acquisition</title><p id="P45">FMRI scanning was performed in a 3 Tesla Philips Achieva MRI scanner with a 32-channel SENSE head coil at the Birmingham University Imaging Centre (BUIC, now the Centre for Human Brain Health, CHBH). Slices covered the whole head. T1-weighted anatomical scans were collected with an MP-RAGE sequence (1 mm isotropic voxels, 256 × 256 matrix, 176 slices, no inter-slice gap, repetition time (TR) = 7.5 ms, field of view (FOV) = 256 × 256 × 176 mm, flip angle (FA) = 7°, echo time (TE) = 3.5 ms). T2*-weighted functional images were acquired by a dual-echo EPI pulse sequence with a low specific absorption rate (SAR) to optimize the image quality in regions highly susceptible for distortions by a long readout gradient (3 × 3 × 3.5 mm, 64 × 64 matrix, 34 slices, no inter-slice gap, TR = 2.2 s, full head coverage, FOV = 192 × 192 × 119 mm, FA = 80°, TE1 = 12 ms, TE2 = 34 ms (for dual-echo information see also <xref ref-type="bibr" rid="R44">Halai et al., 2014</xref>, <xref ref-type="bibr" rid="R43">2015</xref>; <xref ref-type="bibr" rid="R58">Kirilina et al., 2016</xref>). Slices were acquired in a continuous descending fashion. Furthermore, we collected 200 resting state volumes, 50 after each of the four task runs, used for the later combining of dual-echo images with the short and long TEs. During the acquisition of the functional scans, the helium pump was switched off to prevent contamination of the EEG by the compressor artifact at about 20-30 Hz. Stimulus timings were jittered in relation to the acquired scans. The task was presented to participants through a mirror system in the scanner and a JVC SX 21e projector with resolution 1280x1024 at 60 Hz (<ext-link ext-link-type="uri" xlink:href="https://de.jvc.com/">https://de.jvc.com/</ext-link>). Participants´ heads were extra padded to minimize movement artefacts. The stimulus presentation, and the collection of timing and accuracy information, was controlled by scripts written in Matlab 2016a (<ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/">www.mathworks.com</ext-link>) and Psychophysics Toolbox Version 3 (<xref ref-type="bibr" rid="R8">Brainard, 1997</xref>; <xref ref-type="bibr" rid="R59">Kleiner et al., 2007</xref>; <xref ref-type="bibr" rid="R98">Pelli, 1997</xref>). Responses were logged by NATA response boxes (<ext-link ext-link-type="uri" xlink:href="https://natatech.com/">https://natatech.com/</ext-link>).</p></sec><sec id="S19"><title>Out-of-scanner EEG data acquisition</title><p id="P46">As previously stated, a simultaneous EEG dataset was acquired during the fMRI session, but this EEG dataset was too noisy to decode retrieval-related information. We therefore decided to instead use an out-of-scanner EEG dataset for data fusion. This out-of-scanner data, used for all analyses reported here, originated from a previous publication (<xref ref-type="bibr" rid="R74">Linde-Domingo et al., 2019</xref>). We give a short description of the EEG data acquisition in the following and of the EEG preprocessing further below. More details can be found in the original publication.</p><p id="P47">For EEG data collection, 128 electrodes of silver metal and silver chloride were used. Further, an Active-Two amplifier system and the ActiView acquisition software aided data recording (BioSemi, Amsterdam, the Netherlands). Psychophysics Toolbox Version 3 and MATLAB 2014b (<ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/">www.mathworks.com</ext-link>) were used for task presentation and response collection (<xref ref-type="bibr" rid="R74">Linde-Domingo et al., 2019</xref>).</p></sec></sec></sec><sec id="S20"><title>Analyses</title><sec id="S21"><title>Behaviour</title><p id="P48">Behavioural data was analysed by MATLAB R2017b (<ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/">www.mathworks.com</ext-link>). For the inspection of RTs and accuracies, the behavioural data were preprocessed as follows. For the RT analysis, all trials with incorrect responses to the catch question were removed first. Additionally, catch question RTs of correct trials were removed if they were faster than 200 ms or exceeding the average RT of a participant in a given set of repetitions (first or second cycle) by more than three times the standard deviation (<xref ref-type="bibr" rid="R73">Lifanov et al., 2021</xref>; <xref ref-type="bibr" rid="R74">Linde-Domingo et al., 2019</xref>). For the analysis of the accuracy data, trials faster than 200 ms and objects with a missing response for either of the two questions were excluded in the corresponding cycle (<xref ref-type="bibr" rid="R73">Lifanov et al., 2021</xref>). The same procedures were applied to the data of the out-of-scanner EEG participants, who performed a nearly identical task. However, note that out-of-scanner EEG participants only went through one cycle of retrieval.</p></sec><sec id="S22"><title>FMRI data preprocessing</title><p id="P49">We used MATLAB R2017b (<ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/">www.mathworks.com</ext-link>) and SPM 12 (<italic>Statistical Parametric Mapping</italic>, 2007; <ext-link ext-link-type="uri" xlink:href="http://store.elsevier.com/product.jsp?isbn=9780123725608">http://store.elsevier.com/product.jsp?isbn=9780123725608</ext-link>) for the preprocessing and the analysis of fMRI data. All functional images were first realigned based on three motion and three rotation parameters, unwarped, and slice time corrected to the middle slice in time.</p><p id="P50">After these initial preprocessing steps, images obtained during the task at two echo times were combined as a weighted average (<xref ref-type="bibr" rid="R100">Poser et al., 2006</xref>). Importantly, the relative weights were obtained from the SNR of 200 resting state volumes per echo and its corresponding <italic>TE</italic> <disp-formula id="FD1"><label>(1)(equation from Poser et al., 2006)</label><mml:math id="M1"><mml:mrow><mml:mi>w</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:mi>R</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>S</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>∗</mml:mo><mml:mi>T</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>∑</mml:mi><mml:mi>n</mml:mi><mml:mi>S</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>∗</mml:mo><mml:mi>T</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula> where <italic>w</italic> is the weight for an individual voxel, <italic>SNR<sub>n</sub></italic> is the signal to noise ratio here calculated as ratio of mean to standard deviation of the given voxel calculated over time at the <italic>n</italic>th echo, and <italic>TE</italic> is the readout time of the <italic>n</italic>th echo.</p><p id="P51">Weights obtained from the resting state were then used to combine task volumes from both echoes. <disp-formula id="FD2"><label>(2)(as described by Poser et al., 2006)</label><mml:math id="M2"><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⋆</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⋆</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></disp-formula> where S is the final signal of an individual voxel over time, calculated by summing the weighted signals of both echoes. These methods and equations were applied as described in work on BOLD contrast optimization by multi-echo sequences (<xref ref-type="bibr" rid="R100">Poser et al., 2006</xref>). Combined image structures were written to NIFTI files by Tools for NIfTI and ANALYZE image (<ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/matlabcentral/fileexchange/8797-tools-for-nifti-and-analyze-image">https://www.mathworks.com/matlabcentral/fileexchange/8797-tools-for-nifti-and-analyze-image</ext-link> retrieved December 8, 2017, by Shen, 2021).</p><p id="P52">Anatomical images were segmented, co-registered with the functional images and normalized into a standard MNI template in SPM 12 (<italic>Statistical Parametric Mapping</italic>, 2007; <ext-link ext-link-type="uri" xlink:href="https://shop.elsevier.com/books/statistical-parametric-mapping-the-analysis-of-functional-brain-images/penny/978-0-12-372560-8">http://store.elsevier.com/product.jsp?isbn=9780123725608</ext-link>). Then, after the combination of functional images, the T2* images were also normalized into MNI space, using the T1-based normalization parameters. Finally, EPI images were smoothed for the univariate GLM analysis with a gaussian spatial filter of 8 mm full width at half maximum (FWHM). Note that multivariate analyses were performed on unsmoothed data in native space (see <xref ref-type="bibr" rid="R127">Weaverdyck et al., 2020</xref>).</p></sec><sec id="S23"><title>ROIs</title><p id="P53">ROIs were created from templates in MNI space as available in the WFU PickAtlas v3.0 (<xref ref-type="bibr" rid="R70">Lancaster et al., 1997</xref>, <xref ref-type="bibr" rid="R71">2000</xref>; <xref ref-type="bibr" rid="R80">Maldjian et al., 2003</xref>, <xref ref-type="bibr" rid="R79">2004</xref>; <xref ref-type="bibr" rid="R122">Tzourio-Mazoyer et al., 2002</xref>). These ROIs were then fitted to individual brains by applying the reversed T1-based normalization parameters (see previous section) to the ROI masks (as in <xref ref-type="bibr" rid="R13">Chang &amp; Glover, 2010</xref>). To define the anatomical masks, we used Brodmann areas (BAs, <xref ref-type="bibr" rid="R10">Brodmann, 1909</xref>) from the Talairach Daemon database atlases (<xref ref-type="bibr" rid="R70">Lancaster et al., 1997</xref>, <xref ref-type="bibr" rid="R71">2000</xref>) and automated anatomical labelling (AAL, <xref ref-type="bibr" rid="R122">Tzourio-Mazoyer et al., 2002</xref>). The anatomical masks used for our analyses included: an early visual mask (V1/V2), consisting of BAs 17 and 18; a human inferior temporal (hIT) mask, consisting of BAs 19 and 37; a temporal pole mask, consisting of superior and middle temporal pole regions as defined by AAL; a medial temporal lobe (MTL) mask, consisting of BAs 28, 34, 35, 36 and AAL rhinal sulcus and parahippocampal gyrus; a superior parietal lobe mask, consisting of BA 7; a lateral (inferior) parietal lobe mask, consisting of BA 39 and 40; and a medial parietal lobe mask, consisting of BA 29 and 30.</p></sec><sec id="S24"><title>EEG data preprocessing</title><p id="P54">EEG data (from <xref ref-type="bibr" rid="R74">Linde-Domingo et al., 2019</xref>, also available on <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/327EK">https://doi.org/10.17605/OSF.IO/327EK</ext-link>) were preprocessed in MATLAB (<ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/">www.mathworks.com</ext-link>) and the Fieldtrip toolbox version from 3 August, 2017 (<xref ref-type="bibr" rid="R94">Oostenveld et al., 2011</xref>); Donders Institute for Brain, Cognition and Behaviour, Radboud University Nijmegen, the Netherlands. See <ext-link ext-link-type="uri" xlink:href="https://www.fieldtriptoolbox.org/">http://www.ru.nl/neuroimaging/fieldtrip</ext-link>). During epoching, different temporal references were used for encoding and retrieval. Encoding epochs were stimulus locked to the onset of the object image, while retrieval timelines were locked to the subjective retrieval button press, in order to observe the reactivation stream leading up to the subjective experience of recollection. These epochs were created with a length of 2 s (-500 before to 1500 ms after object onset) for encoding and 4.5 s (-4 s before to 500 ms after retrieval button press) for retrieval. Line noise was removed by a FIR filter with a band-stop between 48 and 52 Hz. A high-pass filter with a cut-off frequency of 0.1 Hz was applied to remove slow temporal drifts, and a low-pass filter with a cut-off frequency of 100 Hz to remove high-frequency noise. Individual artifactual trials and bad electrodes were rejected manually. Remaining artifacts were removed by independent component analysis (ICA), after which any excluded electrodes were re-introduced by interpolation. The referencing of the data was set to the average across all scalp channels.</p><p id="P55">After this step, we implemented additional preprocessing steps using MATLAB R2017b (<ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/">www.mathworks.com</ext-link>) and the Fieldtrip toolbox version from 16 November, 2017 (<xref ref-type="bibr" rid="R93">Oostenveld et al., 2010</xref>; Donders Institute for Brain, Cognition and Behaviour, Radboud University Nijmegen, the Netherlands. See <ext-link ext-link-type="uri" xlink:href="https://www.fieldtriptoolbox.org/">http://www.ru.nl/neuroimaging/fieldtrip</ext-link>, version from 16th November, 2017) to prepare data for the specific fusion analyses of this paper. The encoding data was baseline corrected by subtracting the average signal in the pre-stimulus period (-0.2 to -0.1 s) for all post-stimulus time points, separately per electrode. The retrieval data was baseline corrected by whole trial demeaning, since retrieval trials had no obvious, uncontaminated baseline period. The EEG time series data was then downsampled to 128 Hz and temporally smoothed with a moving average with a time window of 40 ms.</p></sec><sec id="S25"><title>fMRI multivariate analyses</title><p id="P56">As preparation for the multivariate analyses, we performed a GLM, modelling individual object-specific regressors for encoding, and for each of the two retrieval repetitions separately, adding regressors of no interest for the presentation of verbs, button presses, and catch question onsets, as well as nuisance regressors for head motion, scanner drift and run means (one regressor per variable containing all onsets, see also (<xref ref-type="bibr" rid="R42">Griffiths et al., 2019</xref>). We used stick functions locked to the object onset to model the onset of encoding trials and boxcar functions with a duration of 2.5 s locked to the cue onset to model the onset of retrieval trials. The resulting beta weights were transformed into t-values for all subsequent multivariate analyses (<xref ref-type="bibr" rid="R87">Misaki et al., 2010</xref>).</p></sec><sec id="S26"><title>fMRI searchlight LDA</title><p id="P57">To investigate where in the brain activity patterns differentiated between the two perceptual and the two conceptual categories, we performed a volumetric LDA searchlight analysis on the non-normalized and unsmoothed fMRI data of each participant individually using the searchlight function of the RSA toolbox (<xref ref-type="bibr" rid="R64">Kriegeskorte, 2009</xref>; <xref ref-type="bibr" rid="R65">Kriegeskorte et al., 2006</xref>, <xref ref-type="bibr" rid="R67">2008</xref>; <xref ref-type="bibr" rid="R66">Kriegeskorte &amp; Kievit, 2013</xref>; <xref ref-type="bibr" rid="R91">Nili et al., 2014</xref>; <ext-link ext-link-type="uri" xlink:href="https://www.mrc-cbu.cam.ac.uk/methods-and-resources/toolboxes/">https://www.mrc-cbu.cam.ac.uk/methods-and-resources/toolboxes/</ext-link>). The LDA was calculated using the MVPA-Light toolbox (<xref ref-type="bibr" rid="R120">Treder, 2020</xref>; <ext-link ext-link-type="uri" xlink:href="https://github.com/treder/MVPA-Light">https://github.com/treder/MVPA-Light</ext-link>). This was done at each centre voxel of a sphere, where object-specific t-values of the voxels within a 3D searchlight radius of 12 mm were used as feature vectors. Using these feature vectors, we classified perceptual (photo vs drawing) and conceptual (animate vs inanimate) categories by a 5-fold LDA with 5 repetitions, preserving class proportions, separately for encoding and retrieval using all trials. Individual accuracy maps were normalized to MNI space and spatially smoothed with a 10 mm FWHM Gaussian kernel, before second-level t-tests were performed to statistically compare voxel-specific classification accuracies against 50% chance performance. Finally, the results were plotted on an MNI surface template brain.</p></sec><sec id="S27"><title>fMRI correlation-based RSA</title><p id="P58">As preparation for the fusion of EEG and fMRI data, we performed a representational similarity analysis (RSA, <xref ref-type="bibr" rid="R64">Kriegeskorte, 2009</xref>; <xref ref-type="bibr" rid="R65">Kriegeskorte et al., 2006</xref>, <xref ref-type="bibr" rid="R67">2008</xref>; <xref ref-type="bibr" rid="R66">Kriegeskorte &amp; Kievit, 2013</xref>; <xref ref-type="bibr" rid="R91">Nili et al., 2014</xref>; <ext-link ext-link-type="uri" xlink:href="https://www.mrc-cbu.cam.ac.uk/methods-and-resources/toolboxes/">https://www.mrc-cbu.cam.ac.uk/methods-and-resources/toolboxes/</ext-link>) on the non-normalized and unsmoothed fMRI data. FMRI-based t-maps corresponding to unique objects were arranged in the same order for all participants. As mentioned earlier, object correspondence across participants was only given on the level of object identity (and thus also conceptual category), but not perceptual format. In other words, all participants saw an image of a camel (i.e., an animate object) at some point in the experiment, but the camel could be presented as a photograph to some participants, and as a line drawing to others. For each voxel and its surrounding neighbours within a radius of 12 mm, we extracted object-specific t-value patterns resulting from the appropriate GLM and arranged these as one-dimensional feature vectors. Using these feature vectors, we calculated the Pearson correlation distance (1-<italic>r</italic>) between each pair of objects at each voxel location, separately for encoding and retrieval. The resulting RDM maps were saved and used at a later stage for the searchlight fusion with the EEG data.</p><p id="P59">In a similar fashion to the searchlight approach, RDMs were also calculated for the pre-defined set of anatomical regions of interest in the non-normalized and unsmoothed individual functional datasets (see section ROIs). The resulting ROI RDMs were used at a later stage for the ROI fusion.</p></sec><sec id="S28"><title>EEG multivariate analyses</title><sec id="S29"><title>EEG cross-subjects classifier-based RSA</title><p id="P60">Our next step was to construct a representational dissimilarity matrix (RDM) for each time point of the EEG recordings, resulting in one timeline representing the similarity structure of our object dataset across all subjects. Cells of the RDM represented the pair-wise discriminability of object pairs, based on a cross-subjects classification of object identity using the MVPA-Light toolbox (<xref ref-type="bibr" rid="R120">Treder, 2020</xref>; <ext-link ext-link-type="uri" xlink:href="https://github.com/treder/MVPA-Light">https://github.com/treder/MVPA-Light</ext-link>). To compute this matrix, we arranged object-specific trials in the same order between all participants, independent of their perceptual format, and up to24 repetitions of each individual object across participants were used for the pairwise classification (Note that after trial removal in the preprocessing, an average of M = 3.09 trials per object were missing in the encoding and M = 3.93 trials per object were missing in the retrieval data). Specifically, we performed a time resolved LDA using as feature vectors the EEG amplitude values from the 128 electrodes, at a given time bin, and participants as repetitions of the same object. To tackle problems of overfitting we used automatic shrinkage regularization as implemented in the MVPA-Light toolbox (<xref ref-type="bibr" rid="R120">Treder, 2020</xref>). We used the LDA to classify object identity among each pair of objects at each time bin, again with a 5-fold cross validation, preserving class proportions. The resulting discrimination accuracies were entered in a single time resolved RDM structure representing the dissimilarity between individual objects of our stimulus pool across participants over time. This classification procedure was performed independently for encoding and retrieval.</p><p id="P61">Before using the EEG-based RDMs for our data fusion, we also wanted to assess statistically how much information about object identity can be decoded from the EEG signals themselves (see Supplements). We therefore first calculated the average classification performance across all pairwise accuracies as a descriptive measure. We then tested if the pairwise accuracies resulting from the ‘real’ classification with correct object labels were significantly larger than the pairwise accuracies that resulted from a classification with permuted object labels. This test was performed in two steps. First, we created 25 classification-based RDMs (same classification procedure as for the ‘real’ RDM time course) but each one with randomly permuted object labels, keeping a given label permutation consistent across time in order to preserve the autocorrelation of the EEG time series. The 25 permutations were averaged to form a single ‘baseline’ RDM time course.</p><p id="P62">As a second step, we used a cluster-based permutation test to find clusters with temporally extended above-chance decoding accuracy. This cluster-based permutation test compared the t-statistic of each time point resulting from a ‘real matrix’ versus ‘baseline matrix’ t-test, with the t-statistics computed from a ‘real matrix’ versus ‘baseline matrix’ comparison, this time shuffling the ‘real’ and ‘baseline’ cells between the two matrices (again keeping a consistent shuffling across time, independent samples t-test to control for unequal variance, 1000 permutations, cluster-definition threshold of p &lt;.05, as used in previous publications (<xref ref-type="bibr" rid="R19">Cichy et al., 2019</xref>; <xref ref-type="bibr" rid="R21">Cichy &amp; Pantazis, 2017</xref>; <xref ref-type="bibr" rid="R31">Dobs et al., 2019</xref>). Note that the variance in the t-tests for each time bin comes from the pair-wise accuracies contained in the cells of the two (‘real’ and ‘baseline’) classification matrices. All remaining analyses were performed using the (‘real’) classification matrix with the correct object labels.</p><p id="P63">Next, we tested whether object discriminability systematically differs between objects coming from the same or different conceptual classes (i.e., animate and inanimate objects) within the classification with correct labels. We thus calculated the average accuracies of pairwise classes within and between conceptual categories as a descriptive measure. Using another cluster-based permutation test (paired samples t-test, 1000 permutations, cluster-definition threshold of p &lt;.05), we statistically tested for differences of within- against between-category accuracies over time. This analysis was conducted for conceptual classes only, as there was no correspondence of perceptual class between subjects. The encoding analyses focused on the time period from 0 s s to 1.5 s after object onset, while the retrieval analyses focused on the time period from -3 s to -1 s before retrieval button press. We based this latter time window of interest on previous findings (<xref ref-type="bibr" rid="R74">Linde-Domingo et al., 2019</xref>) which indicated perceptual and conceptual decoding peaks prior to -1 s before button press. All cluster permutation tests were implemented by means of the permutest toolbox (<ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/matlabcentral/fileexchange/71737-permutest">https://www.mathworks.com/matlabcentral/fileexchange/71737-permutest</ext-link> retrieved February 16, 2021, by Gerber, 2021; also see <xref ref-type="bibr" rid="R81">Maris &amp; Oostenveld, 2007</xref>).</p></sec></sec><sec id="S30"><title>EEG-fMRI data fusion</title><sec id="S31"><title>ROI fusion</title><p id="P64">Two distinct approaches were used for the fusion analyses, with either the fMRI data or the EEG data serving as starting point. The first approach used the fMRI patterns from a given ROI as a starting point, and we thus refer to it as ROI fusion. One RDM was created per participant per ROI, representing the similarity structure (i.e., RDM) in a given anatomical brain region. This ROI RDM was then correlated with the RDM from each time bin of the single, EEG-based RDM time course that represents the pooled similarity structure across subjects (see cross-subject classification of described above and in supplements). Correlations and classification accuracies were Fisher’s z-transformed before the data fusion (as in <xref ref-type="bibr" rid="R42">Griffiths et al., 2019</xref>). This analysis resulted in one correlation time course per individual ROI per subject who took part in the fMRI experiment (<italic>n</italic> = 31). The EEG-fMRI correlations were only computed for those cells of the matrix that an individual participant from the fMRI study remembered correctly. To test for statistical significance, we then contrasted the 31 correlation time courses against zero, using a cluster-based permutation test with 1000 permutations and a cluster-definition threshold of p &lt;.05, correcting for multiple comparisons in time as in previous studies (<xref ref-type="bibr" rid="R19">Cichy et al., 2019</xref>; <xref ref-type="bibr" rid="R21">Cichy &amp; Pantazis, 2017</xref>; <xref ref-type="bibr" rid="R31">Dobs et al., 2019</xref>).</p><p id="P65">To test for a sequential information progression over the ventral visual stream regions, we implemented a linear regression on the cumulative sums of the ROI time courses (based on <xref ref-type="bibr" rid="R85">Michelmann et al., 2019</xref>). To do so, we first calculated the cumulative sum of the ROI time courses over the time period from 0 to 1.5 s after image onset for encoding, and from -3 to -1 s before button press for retrieval. For an easier comparison, the cumulative sum of each ROI time course was normalized to an area under the curve that equals 1. For each time point, a linear regression was fitted across the cumulative correlation values of the ROIs within subjects. The resulting slopes of all subjects were then tested against zero in a one-sided cluster-based permutation test (with 1000 permutations and a cluster-definition threshold of p &lt;.05). This method enabled us to test for a forward stream during encoding and a backward stream during retrieval (<xref ref-type="fig" rid="F4">Fig. 4e-f</xref>). An example of the rationale is illustrated in <xref ref-type="fig" rid="F4">figure 4</xref>. In the case of a forward stream, the ROIs along the ventral visual stream activate sequentially from early towards late regions. Therefore, earlier regions (e.g. V1-hIT) show a higher cumulative sum than later regions (e.g. temporal pole - MTL) at 0.5 s after stimulus onset (and other time points). A linear fit across ROIs at 0.5 s will therefore show a significant negative slope. According to this rationale, a backward stream across the same regions would result in a significant positive slope. Since we expected a forward stream at encoding and a backward stream at retrieval, we used one-sided tests to confirm if the slope differs from zero (&lt; 0 at encoding, &gt; 0 at retrieval). The sequential ordering of our ROIs was based on the extensive literature describing the hierarchical organisation of the ventral visual stream in anatomical and functional neuroimaging studies (e.g. <xref ref-type="bibr" rid="R23">Cichy et al., 2016</xref>, <xref ref-type="bibr" rid="R21">2017</xref>; <xref ref-type="bibr" rid="R35">Felleman &amp; Essen, 1991</xref>).</p></sec><sec id="S32"><title>Searchlight fusion</title><p id="P66">The second, complementary fusion approach offers higher spatial resolution and a whole-brain perspective on how encoding and retrieval patterns emerge over time. Here, we used the EEG-based RDMs from each time point as starting point, and then searched for matching similarity structures across the entire brain, using a volumetric searchlight analysis on each individual’s fMRI data (<xref ref-type="bibr" rid="R64">Kriegeskorte, 2009</xref>; <xref ref-type="bibr" rid="R65">Kriegeskorte et al., 2006</xref>, <xref ref-type="bibr" rid="R67">2008</xref>; <xref ref-type="bibr" rid="R66">Kriegeskorte &amp; Kievit, 2013</xref>; <xref ref-type="bibr" rid="R91">Nili et al., 2014</xref>). We refer to this method as a (time-resolved) searchlight fusion. A second-order correlation was computed between the (classification-based) EEG RDM from each time bin and the (correlation-based) fMRI RDMs for each centre voxel and its neighbours within a searchlight radius of 3 voxels, separately for encoding and retrieval. This analysis results in a “fusion movie” (i.e., a time-resolved brain map) for each participant who took part in the fMRI experiment. The searchlight fusion for retrieval data was performed for correct trials within the fMRI data only (same as above). The fused data was Fisher’s z-transformed. The searchlight fusion was performed using the Python Representational Similarity Analysis (rsatoolbox) toolbox (<ext-link ext-link-type="uri" xlink:href="https://rsatoolbox.readthedocs.io/en/latest/">https://rsatoolbox.readthedocs.io/en/latest/</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://github.com/rsagroup/rsatoolbox">https://github.com/rsagroup/rsatoolbox</ext-link>) which works on Python (<xref ref-type="bibr" rid="R123">van Rossum, 1995</xref>), using the sys and os module, SciPy (<xref ref-type="bibr" rid="R124">Virtanen et al., 2020</xref>), NumPy (<xref ref-type="bibr" rid="R45">Harris et al., 2020</xref>), and nibabel (<ext-link ext-link-type="uri" xlink:href="https://nipy.org/nibabel/">https://nipy.org/nibabel/</ext-link>; <ext-link ext-link-type="uri" xlink:href="https://github.com/nipy/nibabel/releases">https://github.com/nipy/nibabel/releases</ext-link>).</p><p id="P67">To test for significant EEG-fMRI pattern similarity at individual voxels, we normalized the correlation maps to MNI space, smoothed them with a 10 mm FWHM Gaussian kernel and then tested them in a one-sample t-test against zero at each single time bin. The t-test included a spatial maximal permuted statistic correction combined with a threshold free cluster enhancement (<xref ref-type="bibr" rid="R90">Nichols &amp; Holmes, 2002</xref>; <xref ref-type="bibr" rid="R111">Smith &amp; Nichols, 2009</xref>). This cluster-based analysis was performed using the toolbox MatlabTFCE (<ext-link ext-link-type="uri" xlink:href="http://markallenthornton.com/blog/matlab-tfce/">http://markallenthornton.com/blog/matlab-tfce/</ext-link>) with 1000 permutations, a height exponent of 2, an extent exponent of 0.5, a connectivity parameter of 26 and a step number for cluster formation of.1 as suggested by <xref ref-type="bibr" rid="R111">Smith &amp; Nichols (2009)</xref>. The analysis resulted in time-resolved spatial t-maps, depicting clusters of significant EEG-fMRI correlations, created by Tools for NIfTI and ANALYZE image (<ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/matlabcentral/fileexchange/8797-tools-for-nifti-and-analyze-image">https://www.mathworks.com/matlabcentral/fileexchange/8797-tools-for-nifti-and-analyze-image</ext-link> retrieved December 8, 2017, by Shen, 2021).</p></sec></sec><sec id="S33"><title>Figures</title><p id="P68">Figures were created using SPM 12 (<italic>Statistical Parametric Mapping</italic>, 2007; <ext-link ext-link-type="uri" xlink:href="http://store.elsevier.com/product.jsp?isbn=9780123725608">http://store.elsevier.com/product.jsp?isbn=9780123725608</ext-link>), the RainCloud plots Version 1.1 (<ext-link ext-link-type="uri" xlink:href="https://github.com/RainCloudPlots/RainCloudPlots">https://github.com/RainCloudPlots/RainCloudPlots</ext-link>, <xref ref-type="bibr" rid="R2">Allen et al., 2019</xref>; <xref ref-type="bibr" rid="R129">Whitaker et al., 2019</xref>), Inkscape 1.0.1 (<ext-link ext-link-type="uri" xlink:href="https://inkscape.org/">https://inkscape.org/</ext-link>), WFU PickAtlas v3.0 (<xref ref-type="bibr" rid="R80">Maldjian et al., 2003</xref>, <xref ref-type="bibr" rid="R79">2004</xref>), MRIcron (<ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/mricron">https://www.nitrc.org/projects/mricron</ext-link>, <ext-link ext-link-type="uri" xlink:href="http://www.mricro.com">www.mricro.com</ext-link>, <xref ref-type="bibr" rid="R108">Rorden &amp; Brett, 2000</xref>) and a colin 27 average brain template (<ext-link ext-link-type="uri" xlink:href="http://www.bic.mni.mcgill.ca/ServicesAtlases/Colin27">http://www.bic.mni.mcgill.ca/ServicesAtlases/Colin27</ext-link>, <xref ref-type="bibr" rid="R47">Holmes et al., 1998</xref>).</p></sec></sec><sec id="S34"><title>Analyses that did not work</title><sec id="S35"><label>1)</label><title>Data fusion with the in-scanner EEG</title><p id="P69">The initial plan for this project was to fuse the fMRI data with the EEG data acquired during scanning. Using the simultaneous EEG data for fusion, the spatio-temporal mapping could then have been performed on a trial-by-trial basis within participants. However, neither a time-resolved RSA nor a perceptual or conceptual LDA classification on the EEG data showed interpretable results, likely due to extensive noise in the in-scanner EEG data.</p></sec><sec id="S36"><label>2)</label><title>Data fusion with correlation-based EEG RDMs</title><p id="P70">Having decided to use the out-of-scanner EEG dataset for the data fusion, we first attempted to perform the fMRI-EEG fusion using correlation-based RDMs calculated from the EEG data (see <xref ref-type="bibr" rid="R20">Cichy &amp; Oliva, 2020</xref>). To compute these RDMs, we arranged object-specific trials in the same order between all participants, independent of their perceptual format, identical to the arrangement in the fMRI data. We then performed a time-resolved RSA using EEG amplitude values from the 128 electrodes, at a given time bin, as feature vectors. Specifically, we calculated the pairwise distance (1-Pearson correlation (<italic>r</italic>)) between each pair of objects. However, inspecting the average overall similarity and contrasting the average similarity within versus between conceptual classes did not show interpretable results. This is most likely due to too unreliable and time-varying single-trial estimates from the EEG data. Following this initial analysis, we thus used the cross-subject, classification-based RDMs that produced more stable results on the EEG data itself.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplemental Figure 2-1</label><media xlink:href="EMS158908-supplement-Supplemental_Figure_2_1.jpg" mimetype="image" mime-subtype="jpeg" id="d2aAdGbB" position="anchor"/></supplementary-material><supplementary-material content-type="local-data" id="SD2"><label>Supplemental Figure 4-1</label><media xlink:href="EMS158908-supplement-Supplemental_Figure_4_1.jpg" mimetype="image" mime-subtype="jpeg" id="d2aAdGcB" position="anchor"/></supplementary-material></sec></body><back><ack id="S37"><title>Acknowledgements and Funding</title><p>We thank, Simrandeep Cheema, Dagmar Fraser, and Nina Salman for help with the data collection. Further we thank Karen Mullinger for useful analytical advice.</p><p>This work was supported by a European Research Council (ERC) Starting Grant StG-2016-715714 awarded to Maria Wimber, and a scholarship from the Midlands Integrative Biosciences Training Partnership (MIBTP) awarded to Juan Linde-Domingo. The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</p></ack><sec id="S38" sec-type="data-availability"><title>Data availability</title><p id="P71">The EEG data (<xref ref-type="bibr" rid="R74">Linde-Domingo et al., 2019</xref>) can be found under <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/327EK">https://doi.org/10.17605/OSF.IO/327EK</ext-link>. fMRI data can only partly be made publicly available to guarantee full anonymity in accordance with participants’ consent. Group-level data and non-identifiable individual-level data can be found on the Open Science Framework with the identifier <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/2T7SN">https://doi.org/10.17605/OSF.IO/2T7SN</ext-link>. Consent for the reuse of data does not include commercial research.</p><sec id="S39" sec-type="data-availability"><title>Code availability</title><p id="P72">The custom code used in this study is available on the Open Science Framework with the identifier <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/2T7SN">https://doi.org/10.17605/OSF.IO/2T7SN</ext-link>. Additional code associated with the EEG data (<xref ref-type="bibr" rid="R74">Linde-Domingo et al., 2019</xref>) can be found under <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/327EK">https://doi.org/10.17605/OSF.IO/327EK</ext-link>.</p></sec></sec><fn-group><fn fn-type="con" id="FN1"><p id="P73">Author contributions</p><p id="P74">J.L., J.L.D. and M. Wimber designed the experiments. J.L. and J.L.D. conducted the experiments. B.G. and C.F. helped with the EEG-fMRI acquisition. J.L. and J.L.D preprocessed the data. J.L. analysed the data. All authors contributed to the analyses. J.L. and M. Wimber wrote the manuscript. All authors contributed to reviewing and editing.</p></fn><fn fn-type="conflict" id="FN2"><p id="P75">Competing interests</p><p id="P76">The authors declare no competing interests.</p></fn><fn id="FN3"><p id="P77">Ethics</p><p id="P78">Human subjects: All participants gave informed written consent. The research (ethics code ERN_11-0429AP70) was approved by the STEM ethics committee of the University of Birmingham.</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>EJ</given-names></name><name><surname>St-Yves</surname><given-names>G</given-names></name><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>Breedlove</surname><given-names>JL</given-names></name><name><surname>Prince</surname><given-names>JS</given-names></name><name><surname>Dowdle</surname><given-names>LT</given-names></name><name><surname>Nau</surname><given-names>M</given-names></name><name><surname>Caron</surname><given-names>B</given-names></name><name><surname>Pestilli</surname><given-names>F</given-names></name><name><surname>Charest</surname><given-names>I</given-names></name><name><surname>Hutchinson</surname><given-names>JB</given-names></name><etal/></person-group><article-title>A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence</article-title><source>Nature Neuroscience</source><year>2022</year><volume>25</volume><issue>1</issue><pub-id pub-id-type="pmid">34916659</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>M</given-names></name><name><surname>Poggiali</surname><given-names>D</given-names></name><name><surname>Whitaker</surname><given-names>K</given-names></name><name><surname>Marshall</surname><given-names>TR</given-names></name><name><surname>Kievit</surname><given-names>RA</given-names></name></person-group><article-title>Raincloud plots: A multi-platform tool for robust data visualization</article-title><source>Wellcome Open Research</source><year>2019</year><volume>4</volume><fpage>63</fpage><pub-id pub-id-type="pmcid">PMC6480976</pub-id><pub-id pub-id-type="pmid">31069261</pub-id><pub-id pub-id-type="doi">10.12688/wellcomeopenres.15191.2</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baddeley</surname><given-names>A</given-names></name></person-group><article-title>Recent developments in working memory</article-title><source>Current Opinion in Neurobiology</source><year>1998</year><volume>8</volume><issue>2</issue><fpage>234</fpage><lpage>238</lpage><pub-id pub-id-type="pmid">9635207</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baddeley</surname><given-names>A</given-names></name></person-group><article-title>The episodic buffer: A new component of working memory?</article-title><source>Trends in Cognitive Sciences</source><year>2000</year><volume>4</volume><issue>11</issue><fpage>417</fpage><lpage>423</lpage><pub-id pub-id-type="pmid">11058819</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barry</surname><given-names>DN</given-names></name><name><surname>Maguire</surname><given-names>EA</given-names></name></person-group><article-title>Remote Memory and the Hippocampus: A Constructive Critique</article-title><source>Trends in Cognitive Sciences</source><year>2019</year><volume>23</volume><issue>2</issue><fpage>128</fpage><lpage>142</lpage><pub-id pub-id-type="pmid">30528612</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bone</surname><given-names>MB</given-names></name><name><surname>Ahmad</surname><given-names>F</given-names></name><name><surname>Buchsbaum</surname><given-names>BR</given-names></name></person-group><article-title>Feature-specific neural reactivation during episodic memory</article-title><source>Nature Communications</source><year>2020</year><volume>11</volume><issue>1</issue><pub-id pub-id-type="pmcid">PMC7181630</pub-id><pub-id pub-id-type="pmid">32327642</pub-id><pub-id pub-id-type="doi">10.1038/s41467-020-15763-2</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bosch</surname><given-names>SE</given-names></name><name><surname>Jehee</surname><given-names>JFM</given-names></name><name><surname>Fernández</surname><given-names>G</given-names></name><name><surname>Doeller</surname><given-names>CF</given-names></name></person-group><article-title>Reinstatement of Associative Memories in Early Visual Cortex Is Signaled by the Hippocampus</article-title><source>Journal of Neuroscience</source><year>2014</year><volume>34</volume><issue>22</issue><fpage>7493</fpage><lpage>7500</lpage><pub-id pub-id-type="pmcid">PMC6795246</pub-id><pub-id pub-id-type="pmid">24872554</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0805-14.2014</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><article-title>The Psychophysics Toolbox</article-title><source>Spatial Vision</source><year>1997</year><volume>10</volume><issue>4</issue><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brodeur</surname><given-names>MB</given-names></name><name><surname>Dionne-Dostie</surname><given-names>E</given-names></name><name><surname>Montreuil</surname><given-names>T</given-names></name><name><surname>Lepage</surname><given-names>M</given-names></name></person-group><article-title>The Bank of Standardized Stimuli (BOSS), a New Set of 480 Normative Photos of Objects to Be Used as Visual Stimuli in Cognitive Research</article-title><source>PLOS ONE</source><year>2010</year><volume>5</volume><issue>5</issue><elocation-id>e10773</elocation-id><pub-id pub-id-type="pmcid">PMC2879426</pub-id><pub-id pub-id-type="pmid">20532245</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0010773</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brodmann</surname><given-names>K</given-names></name></person-group><article-title>Vergleichende Lokalisationslehre der Grosshirnrinde in ihren Prinzipien dargestellt auf Grund des Zellenbaues von Dr K Brodmann … JA Barth</article-title><year>1909</year></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buckner</surname><given-names>RL</given-names></name><name><surname>Kelley</surname><given-names>WM</given-names></name><name><surname>Petersen</surname><given-names>SE</given-names></name></person-group><article-title>Frontal cortex contributes to human memory formation</article-title><source>Nature Neuroscience</source><year>1999</year><volume>2</volume><issue>4</issue><fpage>311</fpage><lpage>314</lpage><pub-id pub-id-type="pmid">10204536</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlson</surname><given-names>T</given-names></name><name><surname>Tovar</surname><given-names>DA</given-names></name><name><surname>Alink</surname><given-names>A</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Representational dynamics of object vision: The first 1000 ms</article-title><source>Journal of Vision</source><year>2013</year><volume>13</volume><issue>10</issue><fpage>1</fpage><lpage>1</lpage><pub-id pub-id-type="pmid">23908380</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>C</given-names></name><name><surname>Glover</surname><given-names>GH</given-names></name></person-group><article-title>Time–frequency dynamics of resting-state brain connectivity measured with fMRI</article-title><source>NeuroImage</source><year>2010</year><volume>50</volume><issue>1</issue><fpage>81</fpage><lpage>98</lpage><pub-id pub-id-type="pmcid">PMC2827259</pub-id><pub-id pub-id-type="pmid">20006716</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.12.011</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chrastil</surname><given-names>ER</given-names></name></person-group><article-title>Heterogeneity in human retrosplenial cortex: A review of function and connectivity</article-title><source>Behavioral Neuroscience</source><year>2018</year><volume>132</volume><issue>5</issue><fpage>317</fpage><lpage>338</lpage><pub-id pub-id-type="pmid">30160506</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christophel</surname><given-names>TB</given-names></name><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Haynes</surname><given-names>JD</given-names></name></person-group><article-title>Decoding the Contents of Visual Short-Term Memory from Human Visual and Parietal Cortex</article-title><source>Journal of Neuroscience</source><year>2012</year><volume>32</volume><issue>38</issue><fpage>12983</fpage><lpage>12989</lpage><pub-id pub-id-type="pmcid">PMC6621473</pub-id><pub-id pub-id-type="pmid">22993415</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0184-12.2012</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christophel</surname><given-names>TB</given-names></name><name><surname>Klink</surname><given-names>PC</given-names></name><name><surname>Spitzer</surname><given-names>B</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name><name><surname>Haynes</surname><given-names>JD</given-names></name></person-group><article-title>The Distributed Nature of Working Memory</article-title><source>Trends in Cognitive Sciences</source><year>2017</year><volume>21</volume><issue>2</issue><fpage>111</fpage><lpage>124</lpage><pub-id pub-id-type="pmid">28063661</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chrobak</surname><given-names>JJ</given-names></name><name><surname>Lörincz</surname><given-names>A</given-names></name><name><surname>Buzsáki</surname><given-names>G</given-names></name></person-group><article-title>Physiological patterns in the hippocampo-entorhinal cortex system</article-title><source>Hippocampus</source><year>2000</year><volume>10</volume><issue>4</issue><fpage>457</fpage><lpage>465</lpage><pub-id pub-id-type="pmid">10985285</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Jozwik</surname><given-names>KM</given-names></name><name><surname>van den Bosch</surname><given-names>JJF</given-names></name><name><surname>Charest</surname><given-names>I</given-names></name></person-group><article-title>Neural dynamics of real-world object vision that guide behaviour</article-title><source>bioRxiv</source><year>2017</year><elocation-id>147298</elocation-id><pub-id pub-id-type="doi">10.1101/147298</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Jozwik</surname><given-names>KM</given-names></name><name><surname>van den Bosch</surname><given-names>JJF</given-names></name><name><surname>Charest</surname><given-names>I</given-names></name></person-group><article-title>The spatiotemporal neural dynamics underlying perceived similarity for real-world objects</article-title><source>NeuroImage</source><year>2019</year><volume>194</volume><fpage>12</fpage><lpage>24</lpage><pub-id pub-id-type="pmcid">PMC6547050</pub-id><pub-id pub-id-type="pmid">30894333</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.03.031</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><article-title>A M/EEG-fMRI Fusion Primer: Resolving Human Brain Responses in Space and Time</article-title><source>Neuron</source><year>2020</year><volume>107</volume><issue>5</issue><fpage>772</fpage><lpage>781</lpage><pub-id pub-id-type="pmcid">PMC7612024</pub-id><pub-id pub-id-type="pmid">32721379</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2020.07.001</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name></person-group><article-title>Multivariate pattern analysis of MEG and EEG: A comparison of representational structure in time and space</article-title><source>NeuroImage</source><year>2017</year><volume>158</volume><fpage>441</fpage><lpage>454</lpage><pub-id pub-id-type="pmid">28716718</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><article-title>Resolving human object recognition in space and time</article-title><source>Nature Neuroscience</source><year>2014</year><volume>17</volume><issue>3</issue><fpage>455</fpage><lpage>462</lpage><pub-id pub-id-type="pmcid">PMC4261693</pub-id><pub-id pub-id-type="pmid">24464044</pub-id><pub-id pub-id-type="doi">10.1038/nn.3635</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><article-title>Similarity-Based Fusion of MEG and fMRI Reveals Spatio-Temporal Dynamics in Human Cortex During Visual Object Recognition</article-title><source>Cerebral Cortex</source><year>2016</year><volume>26</volume><issue>8</issue><fpage>3563</fpage><lpage>3579</lpage><pub-id pub-id-type="pmcid">PMC4961022</pub-id><pub-id pub-id-type="pmid">27235099</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhw135</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clarke</surname><given-names>A</given-names></name></person-group><article-title>Dynamic activity patterns in the anterior temporal lobe represents object semantics</article-title><source>Cognitive Neuroscience</source><year>2020</year><volume>11</volume><issue>3</issue><fpage>111</fpage><lpage>121</lpage><pub-id pub-id-type="pmcid">PMC7446031</pub-id><pub-id pub-id-type="pmid">32249714</pub-id><pub-id pub-id-type="doi">10.1080/17588928.2020.1742678</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clarke</surname><given-names>A</given-names></name><name><surname>Tyler</surname><given-names>LK</given-names></name></person-group><article-title>Understanding What We See: How We Derive Meaning From Vision</article-title><source>Trends in Cognitive Sciences</source><year>2015</year><volume>19</volume><issue>11</issue><fpage>677</fpage><lpage>687</lpage><pub-id pub-id-type="pmcid">PMC4636429</pub-id><pub-id pub-id-type="pmid">26440124</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2015.08.008</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dalton</surname><given-names>MA</given-names></name><name><surname>D’Souza</surname><given-names>A</given-names></name><name><surname>Lv</surname><given-names>J</given-names></name><name><surname>Calamante</surname><given-names>F</given-names></name></person-group><article-title>New insights into anatomical connectivity along the anterior–posterior axis of the human hippocampus using in vivo quantitative fibre tracking</article-title><source>eLife</source><year>2022</year><volume>11</volume><elocation-id>e76143</elocation-id><pub-id pub-id-type="pmcid">PMC9643002</pub-id><pub-id pub-id-type="pmid">36345716</pub-id><pub-id pub-id-type="doi">10.7554/eLife.76143</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Danker</surname><given-names>JF</given-names></name><name><surname>Anderson</surname><given-names>JR</given-names></name></person-group><article-title>The ghosts of brain states past: Remembering reactivates the brain regions engaged during encoding</article-title><source>Psychological Bulletin</source><year>2010</year><volume>136</volume><issue>1</issue><fpage>87</fpage><lpage>102</lpage><pub-id pub-id-type="pmcid">PMC2853176</pub-id><pub-id pub-id-type="pmid">20063927</pub-id><pub-id pub-id-type="doi">10.1037/a0017937</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Danker</surname><given-names>JF</given-names></name><name><surname>Tompary</surname><given-names>A</given-names></name><name><surname>Davachi</surname><given-names>L</given-names></name></person-group><article-title>Trial-by-Trial Hippocampal Encoding Activation Predicts the Fidelity of Cortical Reinstatement During Subsequent Retrieval</article-title><source>Cerebral Cortex</source><year>2017</year><volume>27</volume><issue>7</issue><fpage>3515</fpage><lpage>3524</lpage><pub-id pub-id-type="pmcid">PMC6059210</pub-id><pub-id pub-id-type="pmid">27288317</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhw146</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dijkstra</surname><given-names>N</given-names></name><name><surname>Ambrogioni</surname><given-names>L</given-names></name><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>van Gerven</surname><given-names>M</given-names></name></person-group><article-title>Neural dynamics of perceptual inference and its reversal during imagery</article-title><source>eLife</source><year>2020</year><volume>9</volume><elocation-id>e53588</elocation-id><pub-id pub-id-type="pmcid">PMC7371419</pub-id><pub-id pub-id-type="pmid">32686645</pub-id><pub-id pub-id-type="doi">10.7554/eLife.53588</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dijkstra</surname><given-names>N</given-names></name><name><surname>van Gaal</surname><given-names>S</given-names></name><name><surname>Geerligs</surname><given-names>L</given-names></name><name><surname>Bosch</surname><given-names>S</given-names></name><name><surname>van Gerven</surname><given-names>M</given-names></name></person-group><article-title>No overlap between unconscious and imagined representations</article-title><source>PsyArXiv</source><year>2021</year><pub-id pub-id-type="doi">10.31234/osf.io/ctdmk</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dobs</surname><given-names>K</given-names></name><name><surname>Isik</surname><given-names>L</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><article-title>How face perception unfolds over time</article-title><source>Nature Communications</source><year>2019</year><volume>10</volume><issue>1</issue><elocation-id>1258</elocation-id><pub-id pub-id-type="pmcid">PMC6425020</pub-id><pub-id pub-id-type="pmid">30890707</pub-id><pub-id pub-id-type="doi">10.1038/s41467-019-09239-1</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eichenbaum</surname><given-names>H</given-names></name></person-group><article-title>The hippocampus and declarative memory: Cognitive mechanisms and neural codes</article-title><source>Behavioural Brain Research</source><year>2001</year><volume>127</volume><issue>1</issue><fpage>199</fpage><lpage>207</lpage><pub-id pub-id-type="pmid">11718892</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Favila</surname><given-names>SE</given-names></name><name><surname>Lee</surname><given-names>H</given-names></name><name><surname>Kuhl</surname><given-names>BA</given-names></name></person-group><article-title>Transforming the Concept of Memory Reactivation</article-title><source>Trends in Neurosciences</source><year>2020</year><volume>43</volume><issue>12</issue><fpage>939</fpage><lpage>950</lpage><pub-id pub-id-type="pmcid">PMC7688497</pub-id><pub-id pub-id-type="pmid">33041061</pub-id><pub-id pub-id-type="doi">10.1016/j.tins.2020.09.006</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Favila</surname><given-names>SE</given-names></name><name><surname>Samide</surname><given-names>R</given-names></name><name><surname>Sweigart</surname><given-names>SC</given-names></name><name><surname>Kuhl</surname><given-names>BA</given-names></name></person-group><article-title>Parietal Representations of Stimulus Features Are Amplified during Memory Retrieval and Flexibly Aligned with Top-Down Goals</article-title><source>Journal of Neuroscience</source><year>2018</year><volume>38</volume><issue>36</issue><fpage>7809</fpage><lpage>7821</lpage><pub-id pub-id-type="pmcid">PMC6125807</pub-id><pub-id pub-id-type="pmid">30054390</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0564-18.2018</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Felleman</surname><given-names>DJ</given-names></name><name><surname>Essen</surname><given-names>DCV</given-names></name></person-group><article-title>Distributed hierarchical processing in the primate cerebral cortex</article-title><source>Cereb Cortex</source><year>1991</year><fpage>1</fpage><lpage>47</lpage><pub-id pub-id-type="pmid">1822724</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferreira</surname><given-names>CS</given-names></name><name><surname>Charest</surname><given-names>I</given-names></name><name><surname>Wimber</surname><given-names>M</given-names></name></person-group><article-title>Retrieval aids the creation of a generalised memory trace and strengthens episode-unique information</article-title><source>NeuroImage</source><year>2019</year><volume>201</volume><elocation-id>115996</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.07.009</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischer</surname><given-names>M</given-names></name><name><surname>Moscovitch</surname><given-names>M</given-names></name><name><surname>Alain</surname><given-names>C</given-names></name></person-group><article-title>A systematic review and meta-analysis of memory-guided attention: Frontal and parietal activation suggests involvement of fronto-parietal networks</article-title><source>WIREs Cognitive Science</source><year>2021</year><volume>12</volume><issue>1</issue><pub-id pub-id-type="pmid">33099860</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fletcher</surname><given-names>PC</given-names></name><name><surname>Frith</surname><given-names>CD</given-names></name><name><surname>Baker</surname><given-names>SC</given-names></name><name><surname>Shallice</surname><given-names>T</given-names></name><name><surname>Frackowiak</surname><given-names>RSJ</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><article-title>The Mind’s Eye—Precuneus Activation in Memory-Related Imagery</article-title><source>NeuroImage</source><year>1995</year><volume>2</volume><issue>3</issue><fpage>195</fpage><lpage>200</lpage><pub-id pub-id-type="pmid">9343602</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Jezzard</surname><given-names>P</given-names></name><name><surname>Turner</surname><given-names>R</given-names></name></person-group><article-title>Analysis of functional MRI time-series</article-title><source>Human Brain Mapping</source><year>1994</year><volume>1</volume><issue>2</issue><fpage>153</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1002/hbm.460010207</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ganis</surname><given-names>G</given-names></name><name><surname>Thompson</surname><given-names>WL</given-names></name><name><surname>Kosslyn</surname><given-names>SM</given-names></name></person-group><article-title>Brain areas underlying visual mental imagery and visual perception: An fMRI study</article-title><source>Cognitive Brain Research</source><year>2004</year><volume>20</volume><issue>2</issue><fpage>226</fpage><lpage>241</lpage><pub-id pub-id-type="pmid">15183394</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glover</surname><given-names>GH</given-names></name></person-group><article-title>Overview of Functional Magnetic Resonance Imaging</article-title><source>Neurosurgery Clinics of North America</source><year>2011</year><volume>22</volume><issue>2</issue><fpage>133</fpage><lpage>139</lpage><pub-id pub-id-type="pmcid">PMC3073717</pub-id><pub-id pub-id-type="pmid">21435566</pub-id><pub-id pub-id-type="doi">10.1016/j.nec.2010.11.001</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Griffiths</surname><given-names>BJ</given-names></name><name><surname>Mayhew</surname><given-names>SD</given-names></name><name><surname>Mullinger</surname><given-names>KJ</given-names></name><name><surname>Jorge</surname><given-names>J</given-names></name><name><surname>Charest</surname><given-names>I</given-names></name><name><surname>Wimber</surname><given-names>M</given-names></name><name><surname>Hanslmayr</surname><given-names>S</given-names></name></person-group><article-title>Alpha/beta power decreases track the fidelity of stimulus-specific information</article-title><source>ELife</source><year>2019</year><volume>8</volume><elocation-id>49562</elocation-id><pub-id pub-id-type="pmcid">PMC6904219</pub-id><pub-id pub-id-type="pmid">31782730</pub-id><pub-id pub-id-type="doi">10.7554/eLife.49562</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Halai</surname><given-names>AD</given-names></name><name><surname>Parkes</surname><given-names>LM</given-names></name><name><surname>Welbourne</surname><given-names>SR</given-names></name></person-group><article-title>Dual-echo fMRI can detect activations in inferior temporal lobe during intelligible speech comprehension</article-title><source>NeuroImage</source><year>2015</year><volume>122</volume><fpage>214</fpage><lpage>221</lpage><pub-id pub-id-type="pmcid">PMC4627358</pub-id><pub-id pub-id-type="pmid">26037055</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.05.067</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Halai</surname><given-names>AD</given-names></name><name><surname>Welbourne</surname><given-names>SR</given-names></name><name><surname>Embleton</surname><given-names>K</given-names></name><name><surname>Parkes</surname><given-names>LM</given-names></name></person-group><article-title>A comparison of dual gradient-echo and spin-echo fMRI of the inferior temporal lobe</article-title><source>Human Brain Mapping</source><year>2014</year><volume>35</volume><issue>8</issue><fpage>4118</fpage><lpage>4128</lpage><pub-id pub-id-type="pmcid">PMC6869502</pub-id><pub-id pub-id-type="pmid">24677506</pub-id><pub-id pub-id-type="doi">10.1002/hbm.22463</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Wieser</surname><given-names>E</given-names></name><name><surname>Taylor</surname><given-names>J</given-names></name><name><surname>Berg</surname><given-names>S</given-names></name><name><surname>Smith</surname><given-names>NJ</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><etal/></person-group><article-title>Array programming with NumPy</article-title><source>Nature</source><year>2020</year><volume>585</volume><issue>7825</issue><fpage>7825</fpage><pub-id pub-id-type="pmcid">PMC7759461</pub-id><pub-id pub-id-type="pmid">32939066</pub-id><pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heusser</surname><given-names>AC</given-names></name><name><surname>Fitzpatrick</surname><given-names>PC</given-names></name><name><surname>Manning</surname><given-names>JR</given-names></name></person-group><article-title>Geometric models reveal behavioural and neural signatures of transforming experiences into memories</article-title><source>Nature Human Behaviour</source><year>2021</year><volume>5</volume><issue>7</issue><fpage>7</fpage><pub-id pub-id-type="pmid">33574605</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holmes</surname><given-names>CJ</given-names></name><name><surname>Hoge</surname><given-names>R</given-names></name><name><surname>Collins</surname><given-names>L</given-names></name><name><surname>Woods</surname><given-names>R</given-names></name><name><surname>Toga</surname><given-names>AW</given-names></name><name><surname>Evans</surname><given-names>AC</given-names></name></person-group><article-title>Enhancement of MR Images Using Registration for Signal Averaging</article-title><source>Journal of Computer Assisted Tomography</source><year>1998</year><volume>22</volume><issue>2</issue><fpage>324</fpage><lpage>333</lpage><pub-id pub-id-type="pmid">9530404</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horikawa</surname><given-names>T</given-names></name><name><surname>Kamitani</surname><given-names>Y</given-names></name></person-group><article-title>Generic decoding of seen and imagined objects using hierarchical visual features</article-title><source>Nature Communications</source><year>2017</year><volume>8</volume><issue>1</issue><elocation-id>1</elocation-id><pub-id pub-id-type="pmcid">PMC5458127</pub-id><pub-id pub-id-type="pmid">28530228</pub-id><pub-id pub-id-type="doi">10.1038/ncomms15037</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horner</surname><given-names>AJ</given-names></name><name><surname>Bisby</surname><given-names>JA</given-names></name><name><surname>Bush</surname><given-names>D</given-names></name><name><surname>Lin</surname><given-names>WJ</given-names></name><name><surname>Burgess</surname><given-names>N</given-names></name></person-group><article-title>Evidence for holistic episodic recollection via hippocampal pattern completion</article-title><source>Nature Communications</source><year>2015</year><volume>6</volume><elocation-id>7462</elocation-id><pub-id pub-id-type="pmcid">PMC4506995</pub-id><pub-id pub-id-type="pmid">26136141</pub-id><pub-id pub-id-type="doi">10.1038/ncomms8462</pub-id></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huijbers</surname><given-names>W</given-names></name><name><surname>Pennartz</surname><given-names>CMA</given-names></name><name><surname>Rubin</surname><given-names>DC</given-names></name><name><surname>Daselaar</surname><given-names>SM</given-names></name></person-group><article-title>Imagery and retrieval of auditory and visual information: Neural correlates of successful and unsuccessful performance</article-title><source>Neuropsychologia</source><year>2011</year><volume>49</volume><issue>7</issue><fpage>1730</fpage><lpage>1740</lpage><pub-id pub-id-type="pmid">21396384</pub-id></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Humphreys</surname><given-names>GF</given-names></name><name><surname>Lambon Ralph</surname><given-names>MA</given-names></name><name><surname>Simons</surname><given-names>JS</given-names></name></person-group><article-title>A Unifying Account of Angular Gyrus Contributions to Episodic and Semantic Cognition</article-title><source>Trends in Neurosciences</source><year>2021</year><volume>44</volume><issue>6</issue><fpage>452</fpage><lpage>463</lpage><pub-id pub-id-type="pmid">33612312</pub-id></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Insausti</surname><given-names>R</given-names></name><name><surname>Muñoz</surname><given-names>M</given-names></name></person-group><article-title>Cortical projections of the non-entorhinal hippocampal formation in the cynomolgus monkey (Macaca fascicularis</article-title><source>European Journal of Neuroscience</source><year>2001</year><volume>14</volume><issue>3</issue><fpage>435</fpage><lpage>451</lpage><pub-id pub-id-type="pmid">11553294</pub-id></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jeong</surname><given-names>SK</given-names></name><name><surname>Xu</surname><given-names>Y</given-names></name></person-group><article-title>Behaviorally Relevant Abstract Object Identity Representation in the Human Parietal Cortex</article-title><source>Journal of Neuroscience</source><year>2016</year><volume>36</volume><issue>5</issue><fpage>1607</fpage><lpage>1619</lpage><pub-id pub-id-type="pmcid">PMC4737772</pub-id><pub-id pub-id-type="pmid">26843642</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1016-15.2016</pub-id></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>SF</given-names></name><name><surname>Guo</surname><given-names>W</given-names></name><name><surname>Fernandez</surname><given-names>C</given-names></name><name><surname>Wagner</surname><given-names>AD</given-names></name></person-group><article-title>Prefrontal reinstatement of contextual task demand is predicted by separable hippocampal patterns</article-title><source>Nature Communications</source><year>2020</year><volume>11</volume><issue>1</issue><elocation-id>1</elocation-id><pub-id pub-id-type="pmcid">PMC7188806</pub-id><pub-id pub-id-type="pmid">32345979</pub-id><pub-id pub-id-type="doi">10.1038/s41467-020-15928-z</pub-id></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jonker</surname><given-names>TR</given-names></name><name><surname>Dimsdale-Zucker</surname><given-names>H</given-names></name><name><surname>Ritchey</surname><given-names>M</given-names></name><name><surname>Clarke</surname><given-names>A</given-names></name><name><surname>Ranganath</surname><given-names>C</given-names></name></person-group><article-title>Neural reactivation in parietal cortex enhances memory for episodically linked information</article-title><source>Proceedings of the National Academy of Sciences</source><year>2018</year><volume>115</volume><issue>43</issue><fpage>11084</fpage><lpage>11089</lpage><pub-id pub-id-type="pmcid">PMC6205442</pub-id><pub-id pub-id-type="pmid">30297400</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1800006115</pub-id></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jorge</surname><given-names>J</given-names></name><name><surname>Van Der Zwaag</surname><given-names>W</given-names></name><name><surname>Figueiredo</surname><given-names>P</given-names></name></person-group><article-title>EEG–fMRI integration for the study of human brain function</article-title><source>NeuroImage</source><year>2014</year><volume>102</volume><fpage>24</fpage><lpage>34</lpage><pub-id pub-id-type="pmid">23732883</pub-id></element-citation></ref><ref id="R57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaiser</surname><given-names>D</given-names></name><name><surname>Jacobs</surname><given-names>AM</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><article-title>Modelling brain representations of abstract concepts</article-title><source>PLoS Computational Biology</source><year>2022</year><volume>18</volume><issue>2</issue><elocation-id>e1009837</elocation-id><pub-id pub-id-type="pmcid">PMC8849470</pub-id><pub-id pub-id-type="pmid">35120139</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1009837</pub-id></element-citation></ref><ref id="R58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kirilina</surname><given-names>E</given-names></name><name><surname>Lutti</surname><given-names>A</given-names></name><name><surname>Poser</surname><given-names>BA</given-names></name><name><surname>Blankenburg</surname><given-names>F</given-names></name><name><surname>Weiskopf</surname><given-names>N</given-names></name></person-group><article-title>The quest for the best: The impact of different EPI sequences on the sensitivity of random effect fMRI group analyses</article-title><source>NeuroImage</source><year>2016</year><volume>126</volume><fpage>49</fpage><lpage>59</lpage><pub-id pub-id-type="pmcid">PMC4739510</pub-id><pub-id pub-id-type="pmid">26515905</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.10.071</pub-id></element-citation></ref><ref id="R59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleiner</surname><given-names>M</given-names></name><name><surname>Brainard</surname><given-names>D</given-names></name><name><surname>Pelli</surname><given-names>D</given-names></name><name><surname>Ingling</surname><given-names>A</given-names></name><name><surname>Murray</surname><given-names>R</given-names></name><name><surname>Broussard</surname><given-names>C</given-names></name></person-group><article-title>What’s new in psychtoolbox-3</article-title><source>Perception</source><year>2007</year><volume>36</volume><issue>14</issue><fpage>1</fpage><lpage>16</lpage></element-citation></ref><ref id="R60"><element-citation publication-type="book"><person-group person-group-type="editor"><name><surname>Kolb</surname><given-names>H</given-names></name><name><surname>Fernandez</surname><given-names>E</given-names></name><name><surname>Nelson</surname><given-names>R</given-names></name></person-group><source>Webvision: The Organization of the Retina and Visual System</source><publisher-name>University of Utah Health Sciences Center</publisher-name><year>1995</year><comment><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/books/NBK11530/">http://www.ncbi.nlm.nih.gov/books/NBK11530/</ext-link></comment><pub-id pub-id-type="pmid">21413389</pub-id></element-citation></ref><ref id="R61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kosslyn</surname><given-names>SM</given-names></name><name><surname>Alpert</surname><given-names>NM</given-names></name><name><surname>Thompson</surname><given-names>WL</given-names></name><name><surname>Maljkovic</surname><given-names>V</given-names></name><name><surname>Weise</surname><given-names>SB</given-names></name><name><surname>Chabris</surname><given-names>CF</given-names></name><name><surname>Hamilton</surname><given-names>SE</given-names></name><name><surname>Rauch</surname><given-names>SL</given-names></name><name><surname>Buonanno</surname><given-names>FS</given-names></name></person-group><article-title>Visual Mental Imagery Activates Topographically Organized Visual Cortex: PET Investigations</article-title><source>Journal of Cognitive Neuroscience</source><year>1993</year><volume>5</volume><issue>3</issue><fpage>263</fpage><lpage>287</lpage><pub-id pub-id-type="pmid">23972217</pub-id></element-citation></ref><ref id="R62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kramer</surname><given-names>JH</given-names></name><name><surname>Rosen</surname><given-names>HJ</given-names></name><name><surname>Du</surname><given-names>AT</given-names></name><name><surname>Schuff</surname><given-names>N</given-names></name><name><surname>Hollnagel</surname><given-names>C</given-names></name><name><surname>Weiner</surname><given-names>MW</given-names></name><name><surname>Miller</surname><given-names>BL</given-names></name><name><surname>Delis</surname><given-names>DC</given-names></name></person-group><article-title>Dissociations in Hippocampal and Frontal Contributions to Episodic Memory Performance</article-title><source>Neuropsychology</source><year>2005</year><volume>19</volume><issue>6</issue><fpage>799</fpage><lpage>805</lpage><pub-id pub-id-type="pmcid">PMC1851935</pub-id><pub-id pub-id-type="pmid">16351355</pub-id><pub-id pub-id-type="doi">10.1037/0894-4105.19.6.799</pub-id></element-citation></ref><ref id="R63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kravitz</surname><given-names>DJ</given-names></name><name><surname>Saleem</surname><given-names>KS</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name><name><surname>Mishkin</surname><given-names>M</given-names></name></person-group><article-title>The ventral visual pathway: An expanded neural framework for the processing of object quality</article-title><source>Trends in Cognitive Sciences</source><year>2013</year><volume>17</volume><issue>1</issue><fpage>26</fpage><lpage>49</lpage><pub-id pub-id-type="pmcid">PMC3532569</pub-id><pub-id pub-id-type="pmid">23265839</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2012.10.011</pub-id></element-citation></ref><ref id="R64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Relating population-code representations between man, monkey, and computational models</article-title><source>Frontiers in Neuroscience</source><year>2009</year><volume>3</volume><pub-id pub-id-type="pmcid">PMC2796920</pub-id><pub-id pub-id-type="pmid">20198153</pub-id><pub-id pub-id-type="doi">10.3389/neuro.01.035.2009</pub-id></element-citation></ref><ref id="R65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>Bandettini</surname><given-names>P</given-names></name></person-group><article-title>Information-based functional brain mapping</article-title><source>Proceedings of the National Academy of Sciences</source><year>2006</year><volume>103</volume><issue>10</issue><fpage>3863</fpage><lpage>3868</lpage><pub-id pub-id-type="pmcid">PMC1383651</pub-id><pub-id pub-id-type="pmid">16537458</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0600244103</pub-id></element-citation></ref><ref id="R66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Kievit</surname><given-names>RA</given-names></name></person-group><article-title>Representational geometry: Integrating cognition, computation, and the brain</article-title><source>Trends in Cognitive Sciences</source><year>2013</year><volume>17</volume><issue>8</issue><fpage>401</fpage><lpage>412</lpage><pub-id pub-id-type="pmcid">PMC3730178</pub-id><pub-id pub-id-type="pmid">23876494</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2013.06.007</pub-id></element-citation></ref><ref id="R67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name></person-group><article-title>Representational similarity analysis—Connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><year>2008</year><volume>2</volume><pub-id pub-id-type="pmcid">PMC2605405</pub-id><pub-id pub-id-type="pmid">19104670</pub-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id></element-citation></ref><ref id="R68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kruggel</surname><given-names>F</given-names></name><name><surname>von Cramon</surname><given-names>DY</given-names></name></person-group><article-title>Temporal properties of the hemodynamic response in functional MRI</article-title><source>Human Brain Mapping</source><year>1999</year><volume>8</volume><issue>4</issue><fpage>259</fpage><lpage>271</lpage><pub-id pub-id-type="pmcid">PMC6873324</pub-id><pub-id pub-id-type="pmid">10619419</pub-id><pub-id pub-id-type="doi">10.1002/(SICI)1097-0193(1999)8:4&amp;#x0003c;259::AID-HBM9&amp;#x0003e;3.0.CO;2-K</pub-id></element-citation></ref><ref id="R69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuhl</surname><given-names>BA</given-names></name><name><surname>Chun</surname><given-names>MM</given-names></name></person-group><article-title>Successful Remembering Elicits Event-Specific Activity Patterns in Lateral Parietal Cortex</article-title><source>Journal of Neuroscience</source><year>2014</year><volume>34</volume><issue>23</issue><fpage>8051</fpage><lpage>8060</lpage><pub-id pub-id-type="pmcid">PMC4044259</pub-id><pub-id pub-id-type="pmid">24899726</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4328-13.2014</pub-id></element-citation></ref><ref id="R70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lancaster</surname><given-names>JL</given-names></name><name><surname>Summerlin</surname><given-names>JL</given-names></name><name><surname>Rainey</surname><given-names>L</given-names></name><name><surname>Freitas</surname><given-names>CS</given-names></name><name><surname>Fox</surname><given-names>PT</given-names></name></person-group><article-title>The Talairach Daemon a database server for talairach atlas labels</article-title><source>NeuroImage</source><year>1997</year><volume>5</volume><issue>4 PART II</issue><fpage>S633</fpage></element-citation></ref><ref id="R71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lancaster</surname><given-names>JL</given-names></name><name><surname>Woldorff</surname><given-names>MG</given-names></name><name><surname>Parsons</surname><given-names>LM</given-names></name><name><surname>Liotti</surname><given-names>M</given-names></name><name><surname>Freitas</surname><given-names>CS</given-names></name><name><surname>Rainey</surname><given-names>L</given-names></name><name><surname>Kochunov</surname><given-names>PV</given-names></name><name><surname>Nickerson</surname><given-names>D</given-names></name><name><surname>Mikiten</surname><given-names>SA</given-names></name><name><surname>Fox</surname><given-names>PT</given-names></name></person-group><article-title>Automated Talairach Atlas labels for functional brain mapping</article-title><source>Human Brain Mapping</source><year>2000</year><volume>10</volume><issue>3</issue><fpage>120</fpage><lpage>131</lpage><pub-id pub-id-type="pmcid">PMC6871915</pub-id><pub-id pub-id-type="pmid">10912591</pub-id><pub-id pub-id-type="doi">10.1002/1097-0193(200007)10:3&amp;#x0003c;120::AID-HBM30&amp;#x0003e;3.0.CO;2-8</pub-id></element-citation></ref><ref id="R72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levy</surname><given-names>DA</given-names></name></person-group><article-title>Towards an understanding of parietal mnemonic processes: Some conceptual guideposts</article-title><source>Frontiers in Integrative Neuroscience</source><year>2012</year><volume>0</volume><pub-id pub-id-type="pmcid">PMC3389369</pub-id><pub-id pub-id-type="pmid">22783175</pub-id><pub-id pub-id-type="doi">10.3389/fnint.2012.00041</pub-id></element-citation></ref><ref id="R73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lifanov</surname><given-names>J</given-names></name><name><surname>Linde-Domingo</surname><given-names>J</given-names></name><name><surname>Wimber</surname><given-names>M</given-names></name></person-group><article-title>Feature-specific reaction times reveal a semanticisation of memories over time and with repeated remembering</article-title><source>Nature Communications</source><year>2021</year><volume>12</volume><issue>1</issue><elocation-id>1</elocation-id><pub-id pub-id-type="pmcid">PMC8155072</pub-id><pub-id pub-id-type="pmid">34039970</pub-id><pub-id pub-id-type="doi">10.1038/s41467-021-23288-5</pub-id></element-citation></ref><ref id="R74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Linde-Domingo</surname><given-names>J</given-names></name><name><surname>Treder</surname><given-names>MS</given-names></name><name><surname>Kerrén</surname><given-names>C</given-names></name><name><surname>Wimber</surname><given-names>M</given-names></name></person-group><article-title>Evidence that neural information flow is reversed between object perception and object reconstruction from memory</article-title><source>Nature Communications</source><year>2019</year><volume>10</volume><issue>1</issue><elocation-id>1</elocation-id><pub-id pub-id-type="pmcid">PMC6331625</pub-id><pub-id pub-id-type="pmid">30643124</pub-id><pub-id pub-id-type="doi">10.1038/s41467-018-08080-2</pub-id></element-citation></ref><ref id="R75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Yu</surname><given-names>T</given-names></name><name><surname>Ni</surname><given-names>D</given-names></name><name><surname>Ren</surname><given-names>L</given-names></name><name><surname>Yang</surname><given-names>Q</given-names></name><name><surname>Lu</surname><given-names>B</given-names></name><name><surname>Wang</surname><given-names>D</given-names></name><name><surname>Heinen</surname><given-names>R</given-names></name><name><surname>Axmacher</surname><given-names>N</given-names></name><name><surname>Xue</surname><given-names>G</given-names></name></person-group><article-title>Stable maintenance of multiple representational formats in human visual short-term memory</article-title><source>Proceedings of the National Academy of Sciences</source><year>2020</year><volume>117</volume><issue>51</issue><fpage>32329</fpage><lpage>32339</lpage><pub-id pub-id-type="pmcid">PMC7768765</pub-id><pub-id pub-id-type="pmid">33288707</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2006752117</pub-id></element-citation></ref><ref id="R76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Long</surname><given-names>B</given-names></name><name><surname>Yu</surname><given-names>CP</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name></person-group><article-title>Mid-level visual features underlie the high-level categorical organization of the ventral stream</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2018</year><volume>115</volume><issue>38</issue><fpage>E9015</fpage><lpage>E9024</lpage><pub-id pub-id-type="pmcid">PMC6156638</pub-id><pub-id pub-id-type="pmid">30171168</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1719616115</pub-id></element-citation></ref><ref id="R77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Long</surname><given-names>NM</given-names></name><name><surname>Kuhl</surname><given-names>BA</given-names></name></person-group><article-title>Cortical Representations of Visual Stimuli Shift Locations with Changes in Memory States</article-title><source>Current Biology</source><year>2021</year><volume>31</volume><issue>5</issue><fpage>1119</fpage><lpage>1126</lpage><elocation-id>e5</elocation-id><pub-id pub-id-type="pmcid">PMC7946763</pub-id><pub-id pub-id-type="pmid">33577747</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2021.01.004</pub-id></element-citation></ref><ref id="R78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lundstrom</surname><given-names>BN</given-names></name><name><surname>Ingvar</surname><given-names>M</given-names></name><name><surname>Petersson</surname><given-names>KM</given-names></name></person-group><article-title>The role of precuneus and left inferior frontal cortex during source memory episodic retrieval</article-title><source>NeuroImage</source><year>2005</year><volume>27</volume><issue>4</issue><fpage>824</fpage><lpage>834</lpage><pub-id pub-id-type="pmid">15982902</pub-id></element-citation></ref><ref id="R79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maldjian</surname><given-names>JA</given-names></name><name><surname>Laurienti</surname><given-names>PJ</given-names></name><name><surname>Burdette</surname><given-names>JH</given-names></name></person-group><article-title>Precentral gyrus discrepancy in electronic versions of the Talairach atlas</article-title><source>NeuroImage</source><year>2004</year><volume>21</volume><issue>1</issue><fpage>450</fpage><lpage>455</lpage><pub-id pub-id-type="pmid">14741682</pub-id></element-citation></ref><ref id="R80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maldjian</surname><given-names>JA</given-names></name><name><surname>Laurienti</surname><given-names>PJ</given-names></name><name><surname>Kraft</surname><given-names>RA</given-names></name><name><surname>Burdette</surname><given-names>JH</given-names></name></person-group><article-title>An automated method for neuroanatomic and cytoarchitectonic atlas-based interrogation of fMRI data sets</article-title><source>NeuroImage</source><year>2003</year><volume>19</volume><issue>3</issue><fpage>1233</fpage><lpage>1239</lpage><pub-id pub-id-type="pmid">12880848</pub-id></element-citation></ref><ref id="R81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><article-title>Nonparametric statistical testing of EEG-and MEG-data</article-title><source>Journal of Neuroscience Methods</source><year>2007</year><volume>164</volume><issue>1</issue><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="pmid">17517438</pub-id></element-citation></ref><ref id="R82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marr</surname><given-names>D</given-names></name></person-group><article-title>Simple Memory: A Theory for Archicortex</article-title><source>Philosophical Transactions of the Royal Society of London Series B, Biological Sciences</source><year>1971</year><volume>262</volume><issue>841</issue><fpage>23</fpage><lpage>81</lpage><pub-id pub-id-type="pmid">4399412</pub-id></element-citation></ref><ref id="R83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname><given-names>CB</given-names></name><name><surname>Douglas</surname><given-names>D</given-names></name><name><surname>Newsome</surname><given-names>RN</given-names></name><name><surname>Man</surname><given-names>LL</given-names></name><name><surname>Barense</surname><given-names>MD</given-names></name></person-group><article-title>Integrative and distinctive coding of visual and conceptual object features in the ventral visual stream</article-title><source>eLife</source><year>2018</year><volume>7</volume><elocation-id>e31873</elocation-id><pub-id pub-id-type="pmcid">PMC5832413</pub-id><pub-id pub-id-type="pmid">29393853</pub-id><pub-id pub-id-type="doi">10.7554/eLife.31873</pub-id></element-citation></ref><ref id="R84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McClelland</surname><given-names>JL</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name><name><surname>O’Reilly</surname><given-names>RC</given-names></name></person-group><article-title>Why there are complementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory</article-title><source>Psychological Review</source><year>1995</year><volume>102</volume><issue>3</issue><fpage>419</fpage><lpage>457</lpage><pub-id pub-id-type="pmid">7624455</pub-id></element-citation></ref><ref id="R85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Michelmann</surname><given-names>S</given-names></name><name><surname>Staresina</surname><given-names>BP</given-names></name><name><surname>Bowman</surname><given-names>H</given-names></name><name><surname>Hanslmayr</surname><given-names>S</given-names></name></person-group><article-title>Speed of time-compressed forward replay flexibly changes in human episodic memory</article-title><source>Nature Human Behaviour</source><year>2019</year><volume>3</volume><issue>2</issue><fpage>143</fpage><lpage>154</lpage><pub-id pub-id-type="pmid">30944439</pub-id></element-citation></ref><ref id="R86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mirjalili</surname><given-names>S</given-names></name><name><surname>Powell</surname><given-names>P</given-names></name><name><surname>Strunk</surname><given-names>J</given-names></name><name><surname>James</surname><given-names>T</given-names></name><name><surname>Duarte</surname><given-names>A</given-names></name></person-group><article-title>Context Memory Encoding and Retrieval Temporal Dynamics are Modulated by Attention across the Adult Lifespan</article-title><source>eNeuro</source><year>2021</year><volume>8</volume><issue>1</issue><comment>ENEURO.0387-20.2020</comment><pub-id pub-id-type="pmcid">PMC7877465</pub-id><pub-id pub-id-type="pmid">33436445</pub-id><pub-id pub-id-type="doi">10.1523/ENEURO.0387-20.2020</pub-id></element-citation></ref><ref id="R87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Misaki</surname><given-names>M</given-names></name><name><surname>Kim</surname><given-names>Y</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Comparison of multivariate classifiers and response normalizations for pattern-information fMRI</article-title><source>NeuroImage</source><year>2010</year><volume>53</volume><issue>1</issue><fpage>103</fpage><lpage>118</lpage><pub-id pub-id-type="pmcid">PMC2914143</pub-id><pub-id pub-id-type="pmid">20580933</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.05.051</pub-id></element-citation></ref><ref id="R88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moscovitch</surname><given-names>M</given-names></name></person-group><article-title>The hippocampus as a ‘stupid,’ domain-specific module: Implications for theories of recent and remote memory, and of imagination</article-title><source>Canadian Journal of Experimental Psychology/Revue Canadienne de Psychologie Expérimentale</source><year>2008</year><volume>62</volume><issue>1</issue><fpage>62</fpage><lpage>79</lpage><pub-id pub-id-type="pmid">18473631</pub-id></element-citation></ref><ref id="R89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naghavi</surname><given-names>HR</given-names></name><name><surname>Nyberg</surname><given-names>L</given-names></name></person-group><article-title>Common fronto-parietal activity in attention, memory, and consciousness: Shared demands on integration?</article-title><source>Consciousness and Cognition</source><year>2005</year><volume>14</volume><issue>2</issue><fpage>390</fpage><lpage>425</lpage><pub-id pub-id-type="pmid">15950889</pub-id></element-citation></ref><ref id="R90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nichols</surname><given-names>TE</given-names></name><name><surname>Holmes</surname><given-names>AP</given-names></name></person-group><article-title>Nonparametric permutation tests for functional neuroimaging: A primer with examples</article-title><source>Human Brain Mapping</source><year>2002</year><volume>15</volume><issue>1</issue><fpage>1</fpage><lpage>25</lpage><pub-id pub-id-type="pmcid">PMC6871862</pub-id><pub-id pub-id-type="pmid">11747097</pub-id><pub-id pub-id-type="doi">10.1002/hbm.1058</pub-id></element-citation></ref><ref id="R91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nili</surname><given-names>H</given-names></name><name><surname>Wingfield</surname><given-names>C</given-names></name><name><surname>Walther</surname><given-names>A</given-names></name><name><surname>Su</surname><given-names>L</given-names></name><name><surname>Marslen-Wilson</surname><given-names>W</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>A Toolbox for Representational Similarity Analysis</article-title><source>PLOS Computational Biology</source><year>2014</year><volume>10</volume><issue>4</issue><elocation-id>e1003553</elocation-id><pub-id pub-id-type="pmcid">PMC3990488</pub-id><pub-id pub-id-type="pmid">24743308</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003553</pub-id></element-citation></ref><ref id="R92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noppeney</surname><given-names>U</given-names></name><name><surname>Price</surname><given-names>CJ</given-names></name></person-group><article-title>A PET study of stimulus-and task-induced semantic processing</article-title><source>NeuroImage</source><year>2002</year><volume>15</volume><issue>4</issue><fpage>927</fpage><lpage>935</lpage><pub-id pub-id-type="pmid">11906233</pub-id></element-citation></ref><ref id="R93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name></person-group><article-title>FieldTrip: Open Source Software for Advanced Analysis of MEG, EEG, and Invasive Electrophysiological Data</article-title><source>Computational Intelligence and Neuroscience</source><year>2010</year><volume>2011</volume><elocation-id>e156869</elocation-id><pub-id pub-id-type="pmcid">PMC3021840</pub-id><pub-id pub-id-type="pmid">21253357</pub-id><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id></element-citation></ref><ref id="R94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name></person-group><article-title>FieldTrip: Open Source Software for Advanced Analysis of MEG, EEG, and Invasive Electrophysiological Data</article-title><source>Computational Intelligence and Neuroscience</source><year>2011</year><volume>2011</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id></element-citation></ref><ref id="R95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Reilly</surname><given-names>RC</given-names></name><name><surname>McClelland</surname><given-names>JL</given-names></name></person-group><article-title>Hippocampal conjunctive encoding, storage, and recall: Avoiding a trade-off</article-title><source>Hippocampus</source><year>1994</year><volume>4</volume><issue>6</issue><fpage>661</fpage><lpage>682</lpage><pub-id pub-id-type="pmid">7704110</pub-id></element-citation></ref><ref id="R96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Reilly</surname><given-names>RC</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name></person-group><article-title>Hippocampal and neocortical contributions to memory: Advances in the complementary learning systems framework</article-title><source>Trends in Cognitive Sciences</source><year>2002</year><volume>6</volume><issue>12</issue><fpage>505</fpage><lpage>510</lpage><pub-id pub-id-type="pmid">12475710</pub-id></element-citation></ref><ref id="R97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patterson</surname><given-names>K</given-names></name><name><surname>Nestor</surname><given-names>PJ</given-names></name><name><surname>Rogers</surname><given-names>TT</given-names></name></person-group><article-title>Where do you know what you know? The representation of semantic knowledge in the human brain</article-title><source>Nature Reviews Neuroscience</source><year>2007</year><volume>8</volume><issue>12</issue><fpage>976</fpage><lpage>987</lpage><pub-id pub-id-type="pmid">18026167</pub-id></element-citation></ref><ref id="R98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelli</surname><given-names>DG</given-names></name></person-group><article-title>The VideoToolbox software for visual psychophysics: Transforming numbers into movies</article-title><source>Spatial Vision</source><year>1997</year><volume>10</volume><issue>4</issue><fpage>437</fpage><lpage>442</lpage><pub-id pub-id-type="pmid">9176953</pub-id></element-citation></ref><ref id="R99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polyn</surname><given-names>SM</given-names></name><name><surname>Natu</surname><given-names>VS</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name></person-group><article-title>Category-Specific Cortical Activity Precedes Retrieval During Memory Search</article-title><source>Science</source><year>2005</year><volume>310</volume><issue>5756</issue><fpage>1963</fpage><lpage>1966</lpage><pub-id pub-id-type="pmid">16373577</pub-id></element-citation></ref><ref id="R100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poser</surname><given-names>BA</given-names></name><name><surname>Versluis</surname><given-names>MJ</given-names></name><name><surname>Hoogduin</surname><given-names>JM</given-names></name><name><surname>Norris</surname><given-names>DG</given-names></name></person-group><article-title>BOLD contrast sensitivity enhancement and artifact reduction with multiecho EPI: Parallel-acquired inhomogeneity-desensitized fMRI</article-title><source>Magnetic Resonance in Medicine</source><year>2006</year><volume>55</volume><issue>6</issue><fpage>1227</fpage><lpage>1235</lpage><pub-id pub-id-type="pmid">16680688</pub-id></element-citation></ref><ref id="R101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quiroga</surname><given-names>RQ</given-names></name></person-group><article-title>Concept cells: The building blocks of declarative memory functions</article-title><source>Nature Reviews Neuroscience</source><year>2012</year><volume>13</volume><issue>8</issue><fpage>587</fpage><lpage>597</lpage><pub-id pub-id-type="pmid">22760181</pub-id></element-citation></ref><ref id="R102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rice</surname><given-names>GE</given-names></name><name><surname>Caswell</surname><given-names>H</given-names></name><name><surname>Moore</surname><given-names>P</given-names></name><name><surname>Hoffman</surname><given-names>P</given-names></name><name><surname>Lambon Ralph</surname><given-names>MA</given-names></name></person-group><article-title>The Roles of Left Versus Right Anterior Temporal Lobes in Semantic Memory: A Neuropsychological Comparison of Postsurgical Temporal Lobe Epilepsy Patients</article-title><source>Cerebral Cortex</source><year>2018</year><volume>28</volume><issue>4</issue><fpage>1487</fpage><lpage>1501</lpage><pub-id pub-id-type="pmcid">PMC6093325</pub-id><pub-id pub-id-type="pmid">29351584</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhx362</pub-id></element-citation></ref><ref id="R103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rissman</surname><given-names>J</given-names></name><name><surname>Wagner</surname><given-names>AD</given-names></name></person-group><article-title>Distributed Representations in Memory: Insights from Functional Brain Imaging</article-title><source>Annual Review of Psychology</source><year>2012</year><volume>63</volume><issue>1</issue><fpage>101</fpage><lpage>128</lpage><pub-id pub-id-type="pmcid">PMC4533899</pub-id><pub-id pub-id-type="pmid">21943171</pub-id><pub-id pub-id-type="doi">10.1146/annurev-psych-120710-100344</pub-id></element-citation></ref><ref id="R104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ritchey</surname><given-names>M</given-names></name><name><surname>Cooper</surname><given-names>RA</given-names></name></person-group><article-title>Deconstructing the Posterior Medial Episodic Network</article-title><source>Trends in Cognitive Sciences</source><year>2020</year><volume>24</volume><issue>6</issue><fpage>451</fpage><lpage>465</lpage><pub-id pub-id-type="pmid">32340798</pub-id></element-citation></ref><ref id="R105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ritchey</surname><given-names>M</given-names></name><name><surname>Wing</surname><given-names>EA</given-names></name><name><surname>LaBar</surname><given-names>KS</given-names></name><name><surname>Cabeza</surname><given-names>R</given-names></name></person-group><article-title>Neural Similarity Between Encoding and Retrieval is Related to Memory Via Hippocampal Interactions</article-title><source>Cerebral Cortex</source><year>2013</year><volume>23</volume><issue>12</issue><fpage>2818</fpage><lpage>2828</lpage><pub-id pub-id-type="pmcid">PMC3827709</pub-id><pub-id pub-id-type="pmid">22967731</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhs258</pub-id></element-citation></ref><ref id="R106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolls</surname><given-names>ET</given-names></name></person-group><article-title>A computational theory of episodic memory formation in the hippocampus</article-title><source>Behavioural Brain Research</source><year>2010</year><volume>215</volume><issue>2</issue><fpage>180</fpage><lpage>196</lpage><pub-id pub-id-type="pmid">20307583</pub-id></element-citation></ref><ref id="R107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolls</surname><given-names>ET</given-names></name></person-group><article-title>The mechanisms for pattern completion and pattern separation in the hippocampus</article-title><source>Frontiers in Systems Neuroscience</source><year>2013</year><volume>7</volume><pub-id pub-id-type="pmcid">PMC3812781</pub-id><pub-id pub-id-type="pmid">24198767</pub-id><pub-id pub-id-type="doi">10.3389/fnsys.2013.00074</pub-id></element-citation></ref><ref id="R108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rorden</surname><given-names>C</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name></person-group><article-title>Stereotaxic display of brain lesions</article-title><source>Behavioural Neurology</source><year>2000</year><volume>12</volume><issue>4</issue><fpage>191</fpage><lpage>200</lpage><pub-id pub-id-type="pmid">11568431</pub-id></element-citation></ref><ref id="R109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schyns</surname><given-names>PG</given-names></name><name><surname>Zhan</surname><given-names>J</given-names></name><name><surname>Jack</surname><given-names>RE</given-names></name><name><surname>Ince</surname><given-names>RAA</given-names></name></person-group><article-title>Revealing the information contents of memory within the stimulus information representation framework</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><year>2020</year><volume>375</volume><issue>1799</issue><elocation-id>20190705</elocation-id><pub-id pub-id-type="pmcid">PMC7209912</pub-id><pub-id pub-id-type="pmid">32248774</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2019.0705</pub-id></element-citation></ref><ref id="R110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simons</surname><given-names>JS</given-names></name><name><surname>Ritchey</surname><given-names>M</given-names></name><name><surname>Fernyhough</surname><given-names>C</given-names></name></person-group><article-title>Brain Mechanisms Underlying the Subjective Experience of Remembering</article-title><source>Annual Review of Psychology</source><year>2022</year><volume>73</volume><fpage>159</fpage><lpage>186</lpage><pub-id pub-id-type="pmid">34587777</pub-id></element-citation></ref><ref id="R111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name></person-group><article-title>Threshold-free cluster enhancement: Addressing problems of smoothing, threshold dependence and localisation in cluster inference</article-title><source>NeuroImage</source><year>2009</year><volume>44</volume><issue>1</issue><fpage>83</fpage><lpage>98</lpage><pub-id pub-id-type="pmid">18501637</pub-id></element-citation></ref><ref id="R112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Staresina</surname><given-names>BP</given-names></name><name><surname>Henson</surname><given-names>RNA</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Alink</surname><given-names>A</given-names></name></person-group><article-title>Episodic Reinstatement in the Medial Temporal Lobe</article-title><source>Journal of Neuroscience</source><year>2012</year><volume>32</volume><issue>50</issue><fpage>18150</fpage><lpage>18156</lpage><pub-id pub-id-type="pmcid">PMC3605913</pub-id><pub-id pub-id-type="pmid">23238729</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4156-12.2012</pub-id></element-citation></ref><ref id="R113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Staresina</surname><given-names>BP</given-names></name><name><surname>Wimber</surname><given-names>M</given-names></name></person-group><article-title>A Neural Chronometry of Memory Recall</article-title><source>Trends in Cognitive Sciences</source><year>2019</year><volume>23</volume><issue>12</issue><fpage>1071</fpage><lpage>1085</lpage><pub-id pub-id-type="pmid">31672429</pub-id></element-citation></ref><ref id="R114"><element-citation publication-type="book"><source>Statistical Parametric Mapping</source><publisher-name>Elsevier</publisher-name><year>2007</year><pub-id pub-id-type="doi">10.1016/B978-0-12-372560-8.X5000-1</pub-id></element-citation></ref><ref id="R115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>St-Laurent</surname><given-names>M</given-names></name><name><surname>Abdi</surname><given-names>H</given-names></name><name><surname>Buchsbaum</surname><given-names>BR</given-names></name></person-group><article-title>Distributed Patterns of Reactivation Predict Vividness of Recollection</article-title><source>Journal of Cognitive Neuroscience</source><year>2015</year><volume>27</volume><issue>10</issue><fpage>2000</fpage><lpage>2018</lpage><pub-id pub-id-type="pmid">26102224</pub-id></element-citation></ref><ref id="R116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teyler</surname><given-names>TJ</given-names></name><name><surname>DiScenna</surname><given-names>P</given-names></name></person-group><article-title>The hippocampal memory indexing theory</article-title><source>Behavioral Neuroscience</source><year>1986</year><volume>100</volume><issue>2</issue><fpage>147</fpage><lpage>154</lpage><pub-id pub-id-type="pmid">3008780</pub-id></element-citation></ref><ref id="R117"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teyler</surname><given-names>TJ</given-names></name><name><surname>Rudy</surname><given-names>JW</given-names></name></person-group><article-title>The hippocampal indexing theory and episodic memory: Updating the index</article-title><source>Hippocampus</source><year>2007</year><volume>17</volume><issue>12</issue><fpage>1158</fpage><lpage>1169</lpage><pub-id pub-id-type="pmid">17696170</pub-id></element-citation></ref><ref id="R118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thakral</surname><given-names>PP</given-names></name><name><surname>Wang</surname><given-names>TH</given-names></name><name><surname>Rugg</surname><given-names>MD</given-names></name></person-group><article-title>Cortical reinstatement and the confidence and accuracy of source memory</article-title><source>NeuroImage</source><year>2015</year><volume>109</volume><fpage>118</fpage><lpage>129</lpage><pub-id pub-id-type="pmcid">PMC5505560</pub-id><pub-id pub-id-type="pmid">25583615</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.01.003</pub-id></element-citation></ref><ref id="R119"><element-citation publication-type="web"><collab>Tools for NIfTI and ANALYZE image</collab><comment>Retrieved 28 May 2021, from <ext-link ext-link-type="uri" xlink:href="https://uk.mathworks.com/matlabcentral/fileexchange/8797-tools-for-nifti-and-analyze-image">https://uk.mathworks.com/matlabcentral/fileexchange/8797-tools-for-nifti-and-analyze-image</ext-link></comment></element-citation></ref><ref id="R120"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treder</surname><given-names>MS</given-names></name></person-group><article-title>MVPA-Light: A Classification and Regression Toolbox for Multi-Dimensional Data</article-title><source>Frontiers in Neuroscience</source><year>2020</year><volume>14</volume><pub-id pub-id-type="pmcid">PMC7287158</pub-id><pub-id pub-id-type="pmid">32581662</pub-id><pub-id pub-id-type="doi">10.3389/fnins.2020.00289</pub-id></element-citation></ref><ref id="R121"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tulving</surname><given-names>E</given-names></name><name><surname>Le Voi</surname><given-names>ME</given-names></name><name><surname>Routh</surname><given-names>DA</given-names></name><name><surname>Loftus</surname><given-names>E</given-names></name><name><surname>Broadbent</surname><given-names>DE</given-names></name></person-group><article-title>Ecphoric processes in episodic memory</article-title><source>Philosophical Transactions of the Royal Society of London B, Biological Sciences</source><year>1983</year><volume>302</volume><issue>1110</issue><fpage>361</fpage><lpage>371</lpage><pub-id pub-id-type="doi">10.1098/rstb.1983.0060</pub-id></element-citation></ref><ref id="R122"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tzourio-Mazoyer</surname><given-names>N</given-names></name><name><surname>Landeau</surname><given-names>B</given-names></name><name><surname>Papathanassiou</surname><given-names>D</given-names></name><name><surname>Crivello</surname><given-names>F</given-names></name><name><surname>Etard</surname><given-names>O</given-names></name><name><surname>Delcroix</surname><given-names>N</given-names></name><name><surname>Mazoyer</surname><given-names>B</given-names></name><name><surname>Joliot</surname><given-names>M</given-names></name></person-group><article-title>Automated Anatomical Labeling of Activations in SPM Using a Macroscopic Anatomical Parcellation of the MNI MRI Single-Subject Brain</article-title><source>NeuroImage</source><year>2002</year><volume>15</volume><issue>1</issue><fpage>273</fpage><lpage>289</lpage><pub-id pub-id-type="pmid">11771995</pub-id></element-citation></ref><ref id="R123"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>van Rossum</surname><given-names>G</given-names></name></person-group><source>Python tutorial (R 9526)</source><year>1995</year><fpage>9526</fpage><comment>Article R <ext-link ext-link-type="uri" xlink:href="https://ir.cwi.nl/pub/5007">https://ir.cwi.nl/pub/5007</ext-link></comment></element-citation></ref><ref id="R124"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name><name><surname>Haberland</surname><given-names>M</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Burovski</surname><given-names>E</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Bright</surname><given-names>J</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><etal/></person-group><article-title>Author Correction: SciPy 1.0: fundamental algorithms for scientific computing in Python</article-title><source>Nature Methods</source><year>2020</year><volume>17</volume><issue>3</issue><fpage>352</fpage><pub-id pub-id-type="pmcid">PMC7056644</pub-id><pub-id pub-id-type="pmid">32015543</pub-id><pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id></element-citation></ref><ref id="R125"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Visser</surname><given-names>M</given-names></name><name><surname>Jefferies</surname><given-names>E</given-names></name><name><surname>Lambon Ralph</surname><given-names>MA</given-names></name></person-group><article-title>Semantic Processing in the Anterior Temporal Lobes: A Meta-analysis of the Functional Neuroimaging Literature</article-title><source>Journal of Cognitive Neuroscience</source><year>2010</year><volume>22</volume><issue>6</issue><fpage>1083</fpage><lpage>1094</lpage><pub-id pub-id-type="pmid">19583477</pub-id></element-citation></ref><ref id="R126"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wagner</surname><given-names>AD</given-names></name><name><surname>Shannon</surname><given-names>BJ</given-names></name><name><surname>Kahn</surname><given-names>I</given-names></name><name><surname>Buckner</surname><given-names>RL</given-names></name></person-group><article-title>Parietal lobe contributions to episodic memory retrieval</article-title><source>Trends in Cognitive Sciences</source><year>2005</year><volume>9</volume><issue>9</issue><fpage>445</fpage><lpage>453</lpage><pub-id pub-id-type="pmid">16054861</pub-id></element-citation></ref><ref id="R127"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weaverdyck</surname><given-names>ME</given-names></name><name><surname>Lieberman</surname><given-names>MD</given-names></name><name><surname>Parkinson</surname><given-names>C</given-names></name></person-group><article-title>Tools of the Trade Multivoxel pattern analysis in fMRI: A practical introduction for social and affective neuroscientists</article-title><source>Social Cognitive and Affective Neuroscience</source><year>2020</year><volume>15</volume><issue>4</issue><fpage>487</fpage><lpage>509</lpage><pub-id pub-id-type="pmcid">PMC7308652</pub-id><pub-id pub-id-type="pmid">32364607</pub-id><pub-id pub-id-type="doi">10.1093/scan/nsaa057</pub-id></element-citation></ref><ref id="R128"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wheeler</surname><given-names>ME</given-names></name><name><surname>Petersen</surname><given-names>SE</given-names></name><name><surname>Buckner</surname><given-names>RL</given-names></name></person-group><article-title>Memory’s echo: Vivid remembering reactivates sensory-specific cortex</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2000</year><volume>97</volume><issue>20</issue><fpage>11125</fpage><lpage>11129</lpage><pub-id pub-id-type="pmcid">PMC27159</pub-id><pub-id pub-id-type="pmid">11005879</pub-id><pub-id pub-id-type="doi">10.1073/pnas.97.20.11125</pub-id></element-citation></ref><ref id="R129"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Whitaker</surname><given-names>K</given-names></name><name><surname>TomRhysMarshall</surname></name><name><surname>van Mourik</surname><given-names>T</given-names></name><name><surname>Martinez</surname><given-names>PA</given-names></name><name><surname>poggiali</surname><given-names>davide</given-names></name><name><surname>Ye</surname><given-names>H</given-names></name><name><surname>MariusKlug</surname></name></person-group><source>RainCloudPlots/RainCloudPlots: WellcomeOpenResearch</source><publisher-name>Zenodo</publisher-name><year>2019</year><pub-id pub-id-type="doi">10.5281/zenodo.3368186</pub-id></element-citation></ref><ref id="R130"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wing</surname><given-names>EA</given-names></name><name><surname>Ritchey</surname><given-names>M</given-names></name><name><surname>Cabeza</surname><given-names>R</given-names></name></person-group><article-title>Reinstatement of Individual Past Events Revealed by the Similarity of Distributed Activation Patterns during Encoding and Retrieval</article-title><source>Journal of Cognitive Neuroscience</source><year>2014</year><volume>27</volume><issue>4</issue><fpage>679</fpage><lpage>691</lpage><pub-id pub-id-type="pmcid">PMC4376373</pub-id><pub-id pub-id-type="pmid">25313659</pub-id><pub-id pub-id-type="doi">10.1162/jocn_a_00740</pub-id></element-citation></ref><ref id="R131"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiao</surname><given-names>X</given-names></name><name><surname>Dong</surname><given-names>Q</given-names></name><name><surname>Gao</surname><given-names>J</given-names></name><name><surname>Men</surname><given-names>W</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Xue</surname><given-names>G</given-names></name></person-group><article-title>Transformed Neural Pattern Reinstatement during Episodic Memory Retrieval</article-title><source>Journal of Neuroscience</source><year>2017</year><volume>37</volume><issue>11</issue><fpage>2986</fpage><lpage>2998</lpage><pub-id pub-id-type="pmcid">PMC6596730</pub-id><pub-id pub-id-type="pmid">28202612</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2324-16.2017</pub-id></element-citation></ref><ref id="R132"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>S</given-names></name><name><surname>Kaiser</surname><given-names>D</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><article-title>Visual Imagery and Perception Share Neural Representations in the Alpha Frequency Band</article-title><source>Current Biology</source><year>2020</year><volume>30</volume><issue>13</issue><fpage>2621</fpage><lpage>2627</lpage><elocation-id>e5</elocation-id><pub-id pub-id-type="pmcid">PMC7342016</pub-id><pub-id pub-id-type="pmid">32531274</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2020.04.074</pub-id></element-citation></ref><ref id="R133"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xue</surname><given-names>G</given-names></name></person-group><article-title>From remembering to reconstruction: The transformative neural representation of episodic memory</article-title><source>Progress in Neurobiology</source><year>2022</year><volume>219</volume><elocation-id>102351</elocation-id><pub-id pub-id-type="pmid">36089107</pub-id></element-citation></ref></ref-list></back><floats-group><boxed-text id="BX1" position="float" orientation="portrait"><caption><title>Significance statement</title></caption><p>In our work, we combined EEG and fMRI to investigate which features of a visual object are reactivated when recalled from episodic memory, and how the memory reconstruction stream unfolds over time and across the brain. Our findings suggest that with respect to perception, memory retrieval follows a backwards information trajectory along a conceptual-to-perceptual gradient, and additionally relays retrieved information to multimodal fronto-parietal brain regions. These findings address the fundamental question of whether memories are more or less complete and truthful reconstructions of past events, or instead are subject to systematic biases that prioritise some types of features over others. Our data suggests that episodic memory retrieval is a dynamic and highly reconstructive process with clear prioritisation of abstract-conceptual over detailed-perceptual information.</p></boxed-text><fig id="F1" position="float"><label>Figure 1</label><caption><title>Overview of stimuli and task.</title><p>a) Design of the stimuli. The 128 pictures used in any given participant were orthogonally split into 64 drawings and 64 photographs, out of which 32 were animate and 32 inanimate objects, respectively. Each object could thus be classified along a perceptual (photo/drawing, cyan) or conceptual (animate-inanimate, magenta) dimension. b) One prototypical task block of the paradigm. At encoding, participants were asked to associate verb-object pairings, and indicate the successful formation of an association by button press. After a 20 s distractor task, participants did a memory test where on each trial, they recalled the previously associated object upon presentation of a verb cue and indicated successful subjective recollection by button press (referred to as retrieval button press). Then participants were asked to hold the mental image of the object in mind for a further 3 seconds, before answering a perceptual or conceptual catch question. Each association was tested twice during retrieval, once with a perceptual, once with a conceptual question, with several intervening test trials in between repetitions of the same association. Participants performed 16 task blocks overall, with eight novel associations per block. Stimuli illustrated are chosen from the BOSS database (<ext-link ext-link-type="uri" xlink:href="https://sites.google.com/site/bosstimuli/home">https://sites.google.com/site/bosstimuli/home</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-sa/3.0/">https://creativecommons.org/licenses/by-sa/3.0/</ext-link>) and customized with free and open source GNU image manipulation software (<ext-link ext-link-type="uri" xlink:href="http://www.gimp.org">www.gimp.org</ext-link>; see <xref ref-type="bibr" rid="R74">Linde-Domingo et al., 2019</xref>). Figure adapted from <xref ref-type="bibr" rid="R73">Lifanov et al. (2021)</xref> and <xref ref-type="bibr" rid="R74">Linde-Domingo et al. (2019)</xref>. c-e) Behavioural data acquired within (EEG-fMRI group) and out of scanner (EEG only group). c) Subjective retrieval reaction times (RTs), d) RTs for each type of catch question, and e) catch question accuracies for the two question types. Filled circles represent the overall mean, boxplots represent median and 25th and 75th percentiles; whiskers represent 2nd and 98th percentile; dots represent the means of individual subjects. Grey represents all trials, cyan represents responses to perceptual, magenta to conceptual questions. In-scanner data comes from n = 24 independent subjects, out-of-scanner-data from n = 31 independent subjects. Figures made using RainCloud plots Version 1.1 (<ext-link ext-link-type="uri" xlink:href="https://github.com/RainCloudPlots/RainCloudPlots">https://github.com/RainCloudPlots/RainCloudPlots</ext-link>, <xref ref-type="bibr" rid="R2">Allen et al., 2019</xref>; <xref ref-type="bibr" rid="R129">Whitaker et al., 2019</xref>).</p></caption><graphic xlink:href="EMS158908-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Searchlight LDA results.</title><p>Second-level t-contrasts show a) Encoding and b) retrieval accuracies significantly higher than the 50% chance level when classifying perceptual (cyan) and conceptual (magenta) object features. All contrasts are thresholded at p &lt;.05 (FWE-corrected). N = 31 independent subjects. Figure made using MRIcron (<ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/mricron">https://www.nitrc.org/projects/mricron</ext-link>, <ext-link ext-link-type="uri" xlink:href="http://www.mricro.com">www.mricro.com</ext-link>, <xref ref-type="bibr" rid="R108">Rorden &amp; Brett, 2000</xref>) and a colin 27 average brain template (<ext-link ext-link-type="uri" xlink:href="http://www.bic.mni.mcgill.ca/ServicesAtlases/Colin27">http://www.bic.mni.mcgill.ca/ServicesAtlases/Colin27</ext-link>, <xref ref-type="bibr" rid="R47">Holmes et al., 1998</xref>; Copyright (C) 1993–2009 Louis Collins, McConnell Brain Imaging Centre, Montreal Neurological Institute, McGill University).</p></caption><graphic xlink:href="EMS158908-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><p>a) Overview of ROI and searchlight fusion. First, brain activity patterns are extracted from fMRI and EEG to create representational dissimilarity matrices (RDMs) in the spatial and temporal domain, respectively. Then, second-order correlations of the RDMs from the two imaging modalities are computed for the data fusion. The EEG RDMs are classification-based, derived from binary classifiers trained and tested to distinguish each pair of objects across participants. FMRI RDMs are computed within participants and are correlation-based. FMRI RDMs can be computed at each voxel centre (v) including voxels within a specific radius, or for larger regions of interest (ROI). Depending on this choice, the data fusion will be searchlight- or ROI-based, respectively. b-c) Linear regression approach on ROI time courses in a forward stream simulation. b) Normalized cumulative sum of each ROI time course of the ventral visual stream. At time point 0.5 s, earlier visual regions show a higher cumulative sum than later regions along the ventral visual stream. c) A linear regression is fitted across ROI values at 0.5 s and results in a negative slope. d) Average slopes and standard errors are illustrated for each time point across the window 0-1.5 s and tested against zero. Significant time points are indicated by points (p &lt;.01 uncorrected). Method and panels b-d adapted from <xref ref-type="bibr" rid="R85">Michelmann et al. (2019)</xref>. Brain figures made using MRIcron (<ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/mricron">https://www.nitrc.org/projects/mricron</ext-link>, <ext-link ext-link-type="uri" xlink:href="http://www.mricro.com">www.mricro.com</ext-link>, <xref ref-type="bibr" rid="R108">Rorden &amp; Brett, 2000</xref>), a colin 27 average brain template (<ext-link ext-link-type="uri" xlink:href="http://www.bic.mni.mcgill.ca/ServicesAtlases/Colin27">http://www.bic.mni.mcgill.ca/ServicesAtlases/Colin27</ext-link>, <xref ref-type="bibr" rid="R47">Holmes et al., 1998</xref>; Copyright (C) 1993–2009 Louis Collins, McConnell Brain Imaging Centre, Montreal Neurological Institute, McGill University) and WFU PickAtlas v3.0 (<xref ref-type="bibr" rid="R70">Lancaster et al., 1997</xref>, <xref ref-type="bibr" rid="R71">2000</xref>; <xref ref-type="bibr" rid="R80">Maldjian et al., 2003</xref>, <xref ref-type="bibr" rid="R79">2004</xref>; <xref ref-type="bibr" rid="R122">Tzourio-Mazoyer et al., 2002</xref>).</p></caption><graphic xlink:href="EMS158908-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><p>a-d) T-values from one-sample t-test of correlation time courses between the EEG RDMs and ventral visual (a &amp; c) and dorsal (b &amp; d) regions of interest (ROIs) during encoding (a &amp; b) and retrieval (c &amp; d). ROIs are colour-coded as indicated in the legends. Significant time points are indicated by points (p &lt;.01 uncorrected) and asterisk (p &lt;.05 cluster). e-f) Results of the test for sequential processing, using a linear fit of the cumulative sums of correlation time courses across ROIs within the ventral visual stream at e) encoding and f) retrieval. Average slopes and standard errors are shown for each time point and statistically tested against zero. Slopes are colour-coded as explained in the legends. A negative slope suggests that earlier ROIs along the ventral visual stream have a higher cumulative sum than later ROIs, indicative of a forward stream. According to the same logic, a positive slope indicates a backward stream. Significant time points are indicated by grey points above the curve (p &lt;.05 cluster). Method adapted from <xref ref-type="bibr" rid="R85">Michelmann et al., 2019</xref>. In all subpanels, time point 0 s marks the object onset during encoding, and the subjective recall button press during retrieval. The latter is not included on time axis as it does not lie within the time window of interest (see <xref ref-type="sec" rid="S12">Methods</xref>, also for details on cluster correction). Variance in all subpanels comes from n = 31 independent subjects. Brain figures made using MRIcron (<ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/mricron">https://www.nitrc.org/projects/mricron</ext-link>, <ext-link ext-link-type="uri" xlink:href="http://www.mricro.com">www.mricro.com</ext-link>, <xref ref-type="bibr" rid="R108">Rorden &amp; Brett, 2000</xref>), a colin 27 average brain template (<ext-link ext-link-type="uri" xlink:href="http://www.bic.mni.mcgill.ca/ServicesAtlases/Colin27">http://www.bic.mni.mcgill.ca/ServicesAtlases/Colin27</ext-link>, <xref ref-type="bibr" rid="R47">Holmes et al., 1998</xref>; Copyright (C) 1993–2009 Louis Collins, McConnell Brain Imaging Centre, Montreal Neurological Institute, McGill University) and WFU PickAtlas v3.0 (<xref ref-type="bibr" rid="R70">Lancaster et al., 1997</xref>, <xref ref-type="bibr" rid="R71">2000</xref>; <xref ref-type="bibr" rid="R80">Maldjian et al., 2003</xref>, <xref ref-type="bibr" rid="R79">2004</xref>; <xref ref-type="bibr" rid="R122">Tzourio-Mazoyer et al., 2002</xref>).</p></caption><graphic xlink:href="EMS158908-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><p>T-maps from one-sample t-test of EEG-fMRI correlations at a) encoding and b) retrieval showing time points in the trial time course where significant spatial clusters of correlations emerged (p<sub>cluster</sub> &lt;.05, see <xref ref-type="sec" rid="S12">Methods</xref> for further details). T-test involved a spatial maximal permuted statistic correction combined with a threshold free cluster enhancement (<xref ref-type="bibr" rid="R90">Nichols &amp; Holmes, 2002</xref>; <xref ref-type="bibr" rid="R111">Smith &amp; Nichols, 2009</xref>). At encoding, time point 0 s would mark the object onset. At retrieval, time point 0 s marks the button press, however, both zero time points are not included in figure as no clusters emerged. N = 31 independent subjects. Brain figures made using MRIcron (<ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/mricron">https://www.nitrc.org/projects/mricron</ext-link>, <ext-link ext-link-type="uri" xlink:href="http://www.mricro.com">www.mricro.com</ext-link>, <xref ref-type="bibr" rid="R108">Rorden &amp; Brett, 2000</xref>) and a colin 27 average brain template (<ext-link ext-link-type="uri" xlink:href="http://www.bic.mni.mcgill.ca/ServicesAtlases/Colin27">http://www.bic.mni.mcgill.ca/ServicesAtlases/Colin27</ext-link>, <xref ref-type="bibr" rid="R47">Holmes et al., 1998</xref>; Copyright (C) 1993–2009 Louis Collins, McConnell Brain Imaging Centre, Montreal Neurological Institute, McGill University).</p></caption><graphic xlink:href="EMS158908-f005"/></fig><table-wrap id="T1" orientation="portrait" position="float"><label>Table 1</label><caption><title>Means (M) and Standard deviations (SD) of encoding and subjective retrieval reaction times (RTs) and accuracies of in- and out-of-scanner data.</title></caption><table frame="void" rules="cols"><thead><tr><th valign="top" align="left"/><th valign="top" align="left" style="border-bottom:solid thin">In-scanner</th><th valign="top" align="left" style="border-bottom:solid thin">Out-of-scanner</th></tr></thead><tbody><tr><td valign="top" align="left">Encoding RT</td><td valign="top" align="left"><italic>M</italic> = 3.03 s; <italic>SD</italic> = 0.90 s</td><td valign="top" align="left"><italic>M</italic> = 2.82 s, <italic>SD</italic> = 1.56 s</td></tr><tr><td valign="top" align="left">Retrieval RT</td><td valign="top" align="left"><italic>M</italic> = 1.75 s; <italic>SD</italic> = 0.66 s</td><td valign="top" align="left"><italic>M</italic> = 2.99 s; <italic>SD</italic> =.81s</td></tr><tr><td valign="top" align="left">Retrieval accuracy</td><td valign="top" align="left"><italic>M</italic> = 84.58 %, <italic>SD</italic> =.07</td><td valign="top" align="left"><italic>M</italic> = 86.75 %; <italic>SD</italic> =.06</td></tr></tbody></table></table-wrap><table-wrap id="T2" orientation="portrait" position="float"><label>Table 2</label><caption><title>Means (M) and Standard deviations (SD) of reaction times (RTs) and accuracies for perceptual and conceptual questions separately of in- and out-of-scanner data and paired-sample t-test between perceptual and conceptual performances.</title></caption><table frame="void" rules="groups"><thead><tr><th valign="top" align="center">a) In-scanner</th><th valign="top" align="left" style="border-bottom:solid thin">RTs</th><th valign="top" align="left" style="border-bottom:solid thin">Accuracy</th></tr></thead><tbody><tr><td valign="top" align="left">Perceptual</td><td valign="top" align="left"><italic>M</italic> = 1.30 s, <italic>SD</italic> = 0.35 s;</td><td valign="top" align="left"><italic>M</italic> = 0.82, <italic>SD=</italic> 0.08</td></tr><tr><td valign="top" align="left">Conceptual</td><td valign="top" align="left"><italic>M</italic> = 1.06 s, <italic>SD</italic> = 0.25 s;</td><td valign="top" align="left"><italic>M</italic> = 0.88, <italic>SD</italic> = 0.07</td></tr><tr><td valign="top" align="left">t-test</td><td valign="top" align="left"><italic>t</italic> (30) = 8.47,</td><td valign="top" align="left"><italic>t</italic> (30) = -6.49,</td></tr><tr><td valign="top" align="left"/><td valign="top" align="left"><italic>p</italic> &lt;.01 (uncorr.)</td><td valign="top" align="left"><italic>p</italic> &lt;.01 (uncorr.)</td></tr></tbody></table><table frame="void" rules="cols"><thead><tr><th valign="top" align="center">b) Out-of-scanner</th><th valign="top" align="left" style="border-bottom: solid thin">RTs</th><th valign="top" align="left" style="border-bottom: solid thin">Accuracy</th></tr></thead><tbody><tr><td valign="top" align="left">Perceptual</td><td valign="top" align="left"><italic>M</italic> = 1.38 s, <italic>SD</italic> = 0.35 s;</td><td valign="top" align="left"><italic>M</italic> = 0.86, <italic>SD</italic> = 0.07</td></tr><tr><td valign="top" align="left">Conceptual</td><td valign="top" align="left"><italic>M</italic> = 1.36 s, <italic>SD</italic> = 0.30 s;</td><td valign="top" align="left"><italic>M</italic> = 0.88, <italic>SD</italic> = 0.07</td></tr><tr><td valign="top" align="left">t-test</td><td valign="top" align="left"><italic>t</italic>(23) =.5,</td><td valign="top" align="left"><italic>t</italic>(23) = -1.80,</td></tr><tr><td valign="top" align="left"/><td valign="top" align="left"><italic>p</italic> =.62 (uncorr.)</td><td valign="top" align="left"><italic>p</italic> =.09 (uncorr.)</td></tr></tbody></table></table-wrap></floats-group></article>