<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS158957</article-id><article-id pub-id-type="doi">10.1101/2022.12.19.519427</article-id><article-id pub-id-type="archive">PPR587527</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Omada: Robust clustering of transcriptomes through multiple testing</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Kariotis</surname><given-names>Sokratis</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Fang</surname><given-names>Tan Pei</given-names></name><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Lu</surname><given-names>Haiping</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Rhodes</surname><given-names>Chris</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Wilkins</surname><given-names>Martin</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Lawrie</surname><given-names>Allan</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Dennis</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A4">4</xref><xref ref-type="aff" rid="A5">5</xref><xref ref-type="aff" rid="A6">6</xref><xref ref-type="corresp" rid="CR1">#</xref></contrib></contrib-group><aff id="A1"><label>1</label>Department of Neuroscience, University of Sheffield, Sheffield UK</aff><aff id="A2"><label>2</label>Department of Infection, Immunity &amp; Cardiovascular Disease, University of Sheffield, Sheffield, UK</aff><aff id="A3"><label>3</label>Department of Computer Science, University of Sheffield, Sheffield UK</aff><aff id="A4"><label>4</label>National Heart and Lung Institute, Imperial College London, London, UK</aff><aff id="A5"><label>5</label>Singapore Institute for Clinical Sciences, A*STAR Research Entities, Singapore</aff><aff id="A6"><label>6</label>Bioinformatics Institute, A*STAR Research Entities, Singapore</aff><author-notes><corresp id="CR1">Corresponding authors: Sokratis Kariotis, Dennis Wang (<email>dennis.wang@imperial.ac.uk</email>)</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>23</day><month>12</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>22</day><month>12</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Cohort studies increasingly collect biosamples for molecular profiling and are observing molecular heterogeneity. High throughput RNA sequencing is providing large datasets capable of reflecting disease mechanisms. Clustering approaches have produced a number of tools to help dissect complex heterogeneous datasets, however, selecting the appropriate method and parameters to perform exploratory clustering analysis of transcriptomic data requires deep understanding of machine learning and extensive computational experimentation. Tools that assist with such decisions without prior field knowledge are nonexistent. To address this we have developed Omada, a suite of tools aiming to automate these processes and make robust unsupervised clustering of transcriptomic data more accessible through automated machine learning based functions. The efficiency of each tool was tested with five datasets characterised by different expression signal strengths to capture a wide spectrum of RNA expression datasets. Our toolkit’s decisions reflected the real number of stable partitions in datasets where the subgroups are discernible. Within datasets with less clear biological distinctions, our tools either formed stable subgroups with different expression profiles and robust clinical associations or revealed signs of problematic data such as biased measurements.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">The rapid development of next-generation sequencing boosted the quantitative analysis of gene expression in a variety of human tissues and organs<sup><xref ref-type="bibr" rid="R1">1</xref>,<xref ref-type="bibr" rid="R2">2</xref></sup> generating valuable resources<sup><xref ref-type="bibr" rid="R3">3</xref></sup> for downstream investigative analysis. In recent years, such analyses aim to elucidate disease mechanisms<sup><xref ref-type="bibr" rid="R4">4</xref></sup> and construct genomic profiles<sup><xref ref-type="bibr" rid="R5">5</xref></sup> to explain diagnosis<sup><xref ref-type="bibr" rid="R6">6</xref></sup>, prognosis and treatment patterns. However, transcriptomic profiles can be heterogeneous due to several causes pertaining to technical biases that produce batch effects<sup><xref ref-type="bibr" rid="R7">7</xref></sup>, cellular diversity<sup><xref ref-type="bibr" rid="R8">8</xref></sup>, disease heterogeneity<sup><xref ref-type="bibr" rid="R9">9</xref></sup> as well as differences between individuals and populations<sup><xref ref-type="bibr" rid="R10">10</xref>,<xref ref-type="bibr" rid="R11">11</xref></sup>. In turn, this heterogeneity hinders traditional research efforts<sup><xref ref-type="bibr" rid="R12">12</xref></sup> aiming to define structures especially under complex diseases which led to the utilisation of the field of machine learning towards this data demanding goal. More specifically, unsupervised machine learning i.e. clustering, in the form of transcriptomic profiling based on sequencing data<sup><xref ref-type="bibr" rid="R13">13</xref>–<xref ref-type="bibr" rid="R15">15</xref></sup>, has been explored in terms of symptomatic heterogeneity in complex diseases revealing differences in molecular states<sup><xref ref-type="bibr" rid="R16">16</xref>–<xref ref-type="bibr" rid="R18">18</xref></sup> and phenotypes described by the gene expression of diseased tissue. However, deep medical and molecular knowledge is required to identify solvable problems and interpret results within the context of various diseases and conditions. Simultaneously, specialised knowledge and experience is needed to create functional, efficient and insightful models which generate reproducible solutions. Despite the inherent power of these models, most times a default model is not sufficiently tuned to the specific dataset thus unable to extract essential information. Many models have been compared, tested and found to work on different data and research questions<sup><xref ref-type="bibr" rid="R19">19</xref>,<xref ref-type="bibr" rid="R20">20</xref></sup> highlighting that no single model is always optimal without tuning (or optimising) on the specific dataset at hand, especially with state-of-the-art methodologies<sup><xref ref-type="bibr" rid="R21">21</xref>,<xref ref-type="bibr" rid="R22">22</xref></sup>.</p><p id="P3">Machine learning (ML) is currently being used in many forms and combinations<sup><xref ref-type="bibr" rid="R23">23</xref></sup>, for different types of projects within diverse fields of biomedical research<sup><xref ref-type="bibr" rid="R24">24</xref>–<xref ref-type="bibr" rid="R26">26</xref></sup>. Supervised and unsupervised methods are being developed to address specific questions and/or data problems as the pace of new data generation increases rapidly. Big data has made the importance of tailored methodologies essential for specialised datasets<sup><xref ref-type="bibr" rid="R27">27</xref>,<xref ref-type="bibr" rid="R28">28</xref></sup>, as speed and accuracy pose an even greater obstacle, especially when handling sizable medical data. The impact of machine learning in biomedical sciences has risen considerably with the multitude of methodologies leading to previously unfeasible computations<sup><xref ref-type="bibr" rid="R29">29</xref>,<xref ref-type="bibr" rid="R30">30</xref></sup>. Unsupervised learning proved to be an invaluable tool towards exploring heterogeneity in complex diseases since its functioning without any prior knowledge or assumptions of sample labels. Due to this diversity, there is a need for methods that support non-expert users to utilise the characteristics of various methodologies in their unsupervised work. One of the most important aspects of sample partitioning is the stability of the generated groups as unstable clusters, usually imply the lack of signal which should be present and drive the clusters. Signals can take many forms, for example the level of gene activity in RNA sequencing datasets. Cluster instability can be caused inherently by the data points or by the type and application quality of a clustering technique.</p><p id="P4">With the above obstacles in mind, we introduce Omada, a toolkit with multiple functions based on cluster stability and machine learning formulas to provide assistance to both experienced and inexperienced users during the steps from dataset assessment to the formation of the subgroups. Each function’s results are based on machine learning theory and multiple metrics to ensure that a wealth of methods will be considered in the decision and clustering process.</p></sec><sec id="S2" sec-type="methods"><title>Methods</title><p id="P5">This toolkit consists of a pipeline that takes in a gene expression matrix to identify transcriptomic subgroups of samples (<xref ref-type="fig" rid="F1">Figure 1</xref> and <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 1</xref>). Starting from a matrix of gene expression values (e.g. transcripts per million from RNAsequencing), the most suitable clustering method is chosen, followed by selecting the transcript features for clustering and determining the number of clusters and memberships.</p><sec id="S3"><title>Sample and gene expression preprocessing</title><p id="P6">A preprocessing step is recommended by the user before the application of these tools on any dataset to heighten the chances of any underlying important signal to be discovered. Data biases and format can often drive clustering attempts to focus on discriminating data points based solely, or mostly, on known information producing no new insights irrespectively of the method used<sup><xref ref-type="bibr" rid="R21">21</xref>,<xref ref-type="bibr" rid="R31">31</xref></sup>. To address this, it is recommended to attempt to remove/normalise any data points that might be introducing strong biases to allow the novel signal to be detected. Furthermore, numerical data may need to be normalised in order to account for potential misdirecting quantities (i.e. outliers) or specifically transformed to satisfy an algorithm's input criteria. Data points or samples have to be filtered based on field knowledge to allow the data to answer specific scientific questions. Expression data should go through proper quality control depending on the manner of collection to identify outliers and remove unreliable datapoints. For microarrays it’s important to assess sample, hybridization and overall signal qualities along with signal comparability and potential biases. Array correlations through PCA and correlation plots should also be considered<sup><xref ref-type="bibr" rid="R32">32</xref></sup>. RNA sequencing experiments also produce data that need to be controlled for potential trimming of adapter sequences, low quality reads, uncalled based and contaminants by using a plethora of available tools<sup><xref ref-type="bibr" rid="R33">33</xref>,<xref ref-type="bibr" rid="R34">34</xref></sup>. Additionally, qPCR generated data should be checked for abnormal amplification, positive and negative control samples, control on PCR replicate variation and determine reference gene expression stability and deviating sample normalisation factors<sup><xref ref-type="bibr" rid="R35">35</xref></sup>. As for the number of genes, it is advised for larger genesets (&gt;1000 genes) to filter down to the most variable ones before the application of any function as genes that do not vary across samples do not contribute towards identifying heterogeneity. Moreover, large genesets require increased computational power and extended runtime without adding any real value due to the large number of non useful genes. Lastly, it is important to note that technical artefacts, such as sampling location or machine specifications, may drive clustering causing the formation of very distinct clusters which can solely be attributed to relevant biases. It is very important for those cases to be identified and extracted insights should be disregarded as they do not reflect real signals or data trends.</p></sec><sec id="S4"><title>Determining clustering potential</title><p id="P7">At the start of each study, we assess the suitability of the input dataset for clustering to ensure general dataset attributes do not influence the process (<xref ref-type="fig" rid="F1">Figure 1</xref>). The number of samples and features -i.e. genes-, as well as the balance of the two dimensions, directly affects the capabilities of clustering methods to handle the dataset. An inadequate number of samples does not provide enough training power<sup><xref ref-type="bibr" rid="R36">36</xref></sup>, while an overabundance of samples might clutter the provided information and confuse most methodologies<sup><xref ref-type="bibr" rid="R37">37</xref></sup>. Similarly, too few features can lead to weak clustering criteria and too many features might lead a methodology away from the features that can really differentiate between clusters of samples. Therefore, to estimate the feasibility of a clustering procedure on a specifically sized dataset we rely on measurable metrics of cluster quality, such as stability. Clusters of high stability denote both a partitionable dataset as well as a dataset-suitable methodology<sup><xref ref-type="bibr" rid="R38">38</xref></sup>. The feasibility score of any dataset is a function of both dimensions as well as the number of classes requested. As such, if too many or a single class is requested of a relatively small dataset the calculation will reflect low feasibility due to insufficient samples and/or features to form the desired classes.</p></sec><sec id="S5"><title>Simulating datasets</title><p id="P8">To assess the quality of the dataset to be used, our toolkit includes two functions for simulating datasets of different dimensionalities for stability assessment. We use those to understand the relation between the number of samples, genes and cluster sizes. The first function simulates datasets allowing for selecting the number of samples (<italic>n</italic>), genes (<italic>m</italic>) and clusters (<italic>c</italic>). Each cluster contains <inline-formula><mml:math id="M1"><mml:mrow><mml:mfrac><mml:mi>n</mml:mi><mml:mi>c</mml:mi></mml:mfrac></mml:mrow></mml:math></inline-formula> samples drawn from a normal distribution with a different mean and standard deviation. Each mean is drawn from a sequence of <italic>c</italic> evenly spaced integers that belong to the range [5, <italic>c</italic> * 10]. Each standard deviation is similarly drawn from a range of [1, <italic>c</italic> * 2]. To estimate the difference between distributions, we calculate the two sided Kolmogorov's D statistic between each pair of distributions representing the generated classes and plot the empirical cumulative distribution function (EDCF).</p><p id="P9">Subsequently, we calculate the stability of each <italic>k</italic> (number of clusters for a particular clustering run) using the clusterboot function in R package fpc v2.2-3. The number of clusters <italic>k</italic> to be considered belong to <italic>k</italic> ∈ [<italic>number of classes</italic> – 2, <italic>number of classes</italic> + 2], with a minimum of <italic>k</italic> = 2.The maximum and average stabilities over all <italic>k</italic> are reported, providing a stability-based quality score that provides an insight in deciding whether a prospect dataset is suitable for a clustering study.</p><p id="P10">To assess the clustering feasibility of an existing dataset this tool kit also provides a similar function which generates a simulated dataset based on an input dataset and the user’s estimation of the number of classes. The number of samples and genes equal those of the input dataset and its mean (<italic>m<sub>input</sub></italic>) and standard deviation (<italic>sd<sub>input</sub></italic>) affect those of each generated class within the dataset. Specifically, if <italic>n</italic> ∈ (1, 2, 3, …) is the number of classes, each class mean (<italic>m<sub>class</sub></italic>) equals <italic>m<sub>input</sub></italic> * 10 * <italic>n</italic> and each class standard deviation (<italic>sd<sub>class</sub></italic>) equals <italic>sd<sub>input</sub></italic> * 2 * <italic>n</italic>.</p></sec><sec id="S6"><title>Intra-method Clustering Agreement</title><p id="P11">Unsupervised learning offers a multitude of methods to be applied on specific types of data due to their nature (e.g. numeric, binary) or underlying signal to be detected. Most studies employ widely-used methods (e.g. hierarchical clustering) without exercising any kind of selection method that would point towards the most effective methodology. Selecting an appropriate approach requires extensive machine learning and data analysis knowledge coupled with tuning and testing of multiple different algorithms. To enable non-machine learning expert users to utilise the vast capabilities of this field and avoid default limited efficiency methodologies we present a clustering selection tool that offers an intelligent selection method with unbiased results through parameter randomization. The nature of this selection method allows any number of well established unsupervised methods to be considered.</p><p id="P12">To address the lack of class labels and thus a performance measure in unsupervised models, we compare how consistently different approaches partition our data when one or more parameters change. As high consistency we define the high agreement score calculated between different variations of a clustering algorithm. When two different clustering runs agree on the partitioning of the samples they also show robustness since they do not randomly assign samples to subgroups but rather are driven by the underlying structure of the data.</p><p id="P13">We implemented a tool (<xref ref-type="fig" rid="F1">Figure 1</xref>) to calculate an average agreement score per clustering approach by comparing a number of runs within each of the three clustering approaches (hierarchical<sup><xref ref-type="bibr" rid="R39">39</xref></sup>, k-means<sup><xref ref-type="bibr" rid="R40">40</xref></sup>, spectral clustering<sup><xref ref-type="bibr" rid="R41">41</xref></sup>) using multiple parameters (kernels, measures, algorithms) specifically based on the data set provided. The number of comparisons (<italic>c</italic>), between runs of the same approach, is an additional overarching parameter of this tool and contributes to the agreement score. For each comparison, the parameters of the two runs are drawn randomly from a predefined set (<xref ref-type="table" rid="T1">Table 1</xref>) selected randomly with replacement while not allowing the same parameters to be used within one comparison. In the interest of performance and computational time we suggest three comparisons to be used. Depending on <italic>c</italic>, we generate variations of the base clustering algorithms (package kernlab v0.9-29), along with the various distance measures and clustering categories they belong to. Within each pair of clustering runs the agreement is calculated using the adjusted Rand Index (package fossil v0.3.7), the corrected-for-chance version of the original Rand index<sup><xref ref-type="bibr" rid="R42">42</xref></sup>, which is based on the number of times any pair of points is partitioned in the same subgroup throughout different clusterings runs. To calculate the agreement within each clustering algorithm (spectral, k-means, hierarchical) we are considering pairs of runs using the same algorithm but different parameters. For those pairs the agreement is averaged across clustering runs and <italic>k</italic> number of clusters tested. The algorithm that presents the highest intra-method agreement over a logical range of clusters (<italic>k</italic> ∈ [2,%]) is noted as the most appropriate clustering of the samples based on a detected signal. A logical range of <italic>k</italic> is considered a set of successive <italic>k’s</italic> (where <italic>k</italic>⩾2) that is most probable to exist within our data, often determined by prior knowledge of the data, previous studies or domain expertise. This selection procedure is mainly affected by the type and size of the data leading similar datasets to opt for the same method due to the specific mathematical formulas within each algorithm. <boxed-text id="BX1" position="anchor" content-type="below"><caption><title>Spectral clustering algorithm<sup><xref ref-type="bibr" rid="R41">41</xref></sup></title></caption><p id="P14">Given a set of points S = {s<sub>1</sub> …, s<sub>n</sub>} in R<sup>1</sup> that we want to cluster into <bold>k</bold> subsets: <list list-type="order" id="L1"><list-item><p id="P15">Form the affinity matrix A ∈ R<sup>n<bold>x</bold>n</sup> defined by A<sub>iJ</sub> = exp(-||s<sub>i</sub>, - s<sub>j</sub>||<sup>2</sup>/2σ<sup>2</sup>) if i ≠ j, and A<sub>ii</sub> = 0</p></list-item><list-item><p id="P16">Define D to be the diagonal matrix whose (i, i)-element is the sum of A’s i-th row, and construct the matrix L = D<sup>-l/ 2</sup> AD<sup>-l/ 2</sup></p></list-item><list-item><p id="P17">Find x<sub>1</sub>, x<sub>2</sub>, …, x<sub>k</sub>, the k largest eigenvectors of L (chosen to be orthogonal to each other in the case of repeated eigenvalues), and form the matrix X = [x<sub>1</sub>x<sub>2</sub> … x<sub>k</sub>] ∈ <sup>n<bold>x</bold>k</sup> by stacking the eigenvectors in columns</p></list-item><list-item><p id="P18"><italic>Form the matrix Y from X by renormalizing each of X's rows to have unit length (i.e. Y<sub>ij</sub> = X<sub>ij</sub>/ (∑<sub>j</sub>X<sup>2</sup><sub>ij</sub>)<sup>1/2</sup></italic>)</p></list-item><list-item><p id="P19"><italic>Treating each row of Y as a point in R<sup>k</sup>, cluster them into k clusters via K-means or any other algorithm (that attempts to minimize distortion</italic>)</p></list-item><list-item><p id="P20">Finally, assign the original point s<sub>i</sub> to cluster j if and only if row i of the matrix Y was assigned to cluster j</p></list-item></list>
</p></boxed-text>
<boxed-text id="BX2" position="anchor" content-type="below"><caption><title>Hierarchical clustering algorithm (average linkage)</title></caption><p id="P21">Given a set of points S = {s<sub>1</sub> …, s<sub>n</sub>} that we want to cluster into <bold>k</bold> subsets: <list list-type="order" id="L2"><list-item><p id="P22"><italic>Initialize with n clusters, each containing one data point (s<sub>i</sub></italic>)</p></list-item><list-item><p id="P23">Compute the between-cluster distance D(r, s) as the between-object distance of the two data points in clusters r and s respectively, r, s =1, 2, …, n. Let the square matrix D = (D(r, s)). Various distances can be used (euclidean, manhattan, canberra, minkowski, maximum).</p></list-item><list-item><p id="P24">Find the most similar pair of clusters r and s, such that D(r, s) is minimum among all pairwise distances</p></list-item><list-item><p id="P25">Merge r and s to a new cluster t and compute the between-cluster distance D(t, k) for any existing cluster k ≠ r, s. Once the distances are obtained, delete the rows and columns corresponding to the old cluster r and s in the D matrix, as r and s do not exist anymore. Then add a new row and column in D corresponding to cluster t.</p></list-item><list-item><p id="P26">Repeat Step 3 a total of n – 1 times until there is only one cluster left.</p></list-item><list-item><p id="P27"><italic>Decide on a point to cut the cluster tree created above so as to obtain the desirable number of clusters (k</italic>)</p></list-item></list>
</p></boxed-text>
<boxed-text id="BX3" position="anchor" content-type="below"><caption><title>K-means<sup><xref ref-type="bibr" rid="R40">40</xref></sup></title></caption><p id="P28">K: kernel matrix, k: number of clusters, w: weights for each point, tmax: optional maximum number of iterations, {π<sub>c</sub><sup>(0)</sup>}<sup>k</sup><sub>c=1</sub>: optional initial clusters <list list-type="order" id="L3"><list-item><p id="P29">If no initial clustering is given, initialize the k clusters π<sub>1</sub><sup>(0)</sup>, …, π<sub>k</sub><sup>(0)</sup> (i.e. randomly). Set t = 0</p></list-item><list-item><p id="P30"><italic>For each a<sub>i</sub> and every cluster c, compute</italic> <disp-formula id="FD1"><mml:math id="M2"><mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>Σ</mml:mi><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msubsup><mml:mi>π</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:msub><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mi>∑</mml:mi><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msubsup><mml:mi>π</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mi>∑</mml:mi><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mi>α</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msubsup><mml:mi>π</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mi>w</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Σ</mml:mi><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msubsup><mml:mi>π</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:msub><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p id="P31"><italic>Find c*(a<sub>i</sub>) = argmin<sub>c</sub>d(a<sub>i</sub>, m<sub>c</sub>) resolving ties arbitrarily. Compute the updated clusters as</italic> <disp-formula id="FD2"><mml:math id="M3"><mml:mrow><mml:msubsup><mml:mi>π</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>:</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p id="P32"><italic>If not converged or t<sub>max</sub> &gt; t, set t = t + 1 and go to Step 2; Otherwise, stop and output final clusters</italic><disp-formula id="FD3"><mml:math id="M4"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mi>π</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:msubsup></mml:mrow></mml:math></disp-formula></p></list-item></list>
</p></boxed-text>
</p></sec><sec id="S7"><title>Feature set subsampling</title><p id="P33">While gene expression data provide measures on the thousands of transcripts in the transcriptome, not all of them may provide discriminative information on the samples and may not be useful for clustering. Moreover, most clustering algorithms are heavily affected by a large number of features both computationally due to input size and in performance due to misdirecting data noise<sup><xref ref-type="bibr" rid="R43">43</xref></sup>. A common strategy to select interesting and potentially useful RNA features is to measure their variance across samples and select the ones with the highest scores instead of those that are either housekeeping or do not differentiate in our context. In this tool, we exclude RNA features that remain stable across samples and are therefore unable to offer any discriminatory power to our unsupervised machine learning models. Furthermore, the exhaustive feature selection procedure incrementally considers all the genes in the feature set and takes into account the stability of all generated test clusters and number of cluster ranges. This step does not require any deep knowledge or filtering decisions by the user.</p><p id="P34">Based on this observation our sample selection step, which is a part of the tool for bootstrap resampling of features presented in <xref ref-type="fig" rid="F1">Figures 1C</xref> and <xref ref-type="fig" rid="F2">2A</xref>, first ranks features in a descending order of variance (var() function from the Stats R package) across samples, generating a list of the most variable features. Subsequently, multiple datasets of all samples and subsets of features are generated. All subsets draw a different number of features from the top of the variance list with replacement. The first dataset uses a relatively small number of features (<italic>n</italic>), depending on the total number of features (<italic>N</italic>) and the granularity of the result desired. The following datasets re-draw from the initial list increasing the number of features by <italic>n</italic>, ending up with <inline-formula><mml:math id="M5"><mml:mrow><mml:mfrac><mml:mi>N</mml:mi><mml:mi>n</mml:mi></mml:mfrac></mml:mrow></mml:math></inline-formula> datasets.</p></sec><sec id="S8"><title>Stability-based assessment of feature sets</title><p id="P35">To assess the suitability of each resampled feature set for our clustering, we measure the average stability of the clusters they generate per run when a clustering method is applied over a range of <italic>k</italic>’s (<xref ref-type="fig" rid="F2">Figure 2B</xref>). First, the clustering range, where the stability of each dataset will be calculated, is selected. For each dataset we generate the bootstrap stability for every <italic>k</italic> within range. To calculate each bootstrap stability score the data is randomly sampled with replacement and clustered internally using a spectral approach. We then compute the Jaccard similarities between the original clusters and the most similar clusters in the resampled data. The above procedure results in a stability score for each <italic>k</italic> and each dataset. We then calculate the final stability of each dataset by averaging the stability over <italic>k</italic>. The genes that comprise the dataset with the highest stability are the ones that compose the most appropriate set for the downstream analysis.</p></sec><sec id="S9"><title>Choosing <italic>k</italic> number of clusters</title><p id="P36">Most clustering methods require the number of <italic>k</italic> clusters to be defined as a parameter before the application of the algorithm on the data. The lack of a concrete way to determine the real number of clusters in a dataset led many studies to base their estimation on field/prior knowledge or various estimation methods such as the Silhouette score<sup><xref ref-type="bibr" rid="R44">44</xref></sup>. However, each method favours different aspects of the generated clusters (i.e. how compact clusters are and how far apart cluster centres are) and therefore suits specific datasets and may introduce bias towards the selection of <italic>k</italic>. To encompass these different angles in one methodology, avoid the risk of selecting an ineffective index and present a more general solution, this tool uses an ensemble learning approach (<xref ref-type="fig" rid="F1">Figure 1</xref>) where multiple internal cluster indexes contribute to the decision making<sup><xref ref-type="bibr" rid="R45">45</xref></sup>. This approach prevents any bias from specific metrics and frees the user from making decisions on any specific metric and assumptions on the optimal number of clusters.</p><p id="P37">Initially, the value of the 15 indexes is calculated for each <italic>k</italic> within a cluster range of [2, <italic>x</italic>], where <italic>x</italic> is a logical upper limit of the number of clusters realistic for our dataset. The means over <italic>k</italic> are calculated per index and the optimal <italic>k</italic> is estimated by majority voting of the 14 means that evaluate the compactness and/or the distance between different subgroups. The selection of indexes can be found in <xref ref-type="table" rid="T2">Table 2</xref>. It is important to note that the most important aspect of determining <italic>k</italic> is minimum loss of information which directs us to overestimate and not underestimate <italic>k<sup><xref ref-type="bibr" rid="R43">43</xref></sup></italic> while interpreting the voting results. Furthermore, cases that present only a single <italic>k</italic> as the optimal number of clusters should be treated with caution in case they are a result of a biased dataset.</p></sec><sec id="S10"><title>Optimal parameter tuning</title><p id="P38">Previous steps have selected the optimal method, number of features and clusters. To perform the optimal clustering we automate the selection of parameters for each method so that manual tuning is not required. Towards that goal we utilise cluster stabilities to decide on the parameters (which depend on the specific algorithm i.e. kernels in k-means and spectral clustering, linkage method in hierarchical clustering) selected by this toolkit. All available parameters (<xref ref-type="table" rid="T1">Table 1</xref>) participate in the selection procedure where we measure the average bootstrap stability of the clusters (clusterboot function in R package fpc v2.2-3) using the previously determined optimal <italic>k</italic> and feature set for each parameter. The parameter that produces the highest stability is used for the optimal clustering run.</p></sec><sec id="S11"><title>Test datasets</title><p id="P39">Five datasets were used to validate different capabilities of the Omada package. First, two datasets were simulated by Omada’s functions. Function feasibilityAnalysisDataBased() was used to generate a multi-class dataset with 359 samples and 300 genes based on the contents and dimensions of the original RNA-seq data<sup><xref ref-type="bibr" rid="R18">18</xref></sup> and composed of five groups of samples drawn from five different distributions with means (5,16,27,38,50) and sd (1,3,5,7,10), representing the five classes. Function feasibilityAnalysis() simulated a single-class dataset of 100 samples and 100 genes drawn from a single distribution. For the multissue Pan-cancer dataset we downloaded RNAseq expression data for 2244 samples and 253 genes representing three types of cancers: breast (n=1084), lung (n=566) and colon/rectal (n=594) downloaded through cbioportal<sup><xref ref-type="bibr" rid="R59">59</xref></sup> from TCGA PanCancer Atlas<sup><xref ref-type="bibr" rid="R60">60</xref></sup>. The mRNA expression was in the form of z-scores relative to normal samples where we applied an extra step of arcsine normalisation. After filtering for tissue-specific genes<sup><xref ref-type="bibr" rid="R61">61</xref></sup> for the three cancer types we retained 243 genes. Next, we utilised a PAH dataset (25,955 genes) generated from 359 patient samples with idiopathic and heritable pulmonary arterial hypertension (IPAH/HPAH). The transcriptomic data can be found in the EGA (the European Genome-phenome Archive) database under accession code EGAS0000100553265<sup><xref ref-type="bibr" rid="R62">62</xref></sup> (restricted access) and all pre-processing details and parameters used can be found in <sup><xref ref-type="bibr" rid="R18">18</xref></sup>. Finally, we used an RNA dataset from the whole blood of 238 mothers during midgestation (26-28 weeks of pregnancy). Read counts were extracted from GEO (accession number GSE182409<sup><xref ref-type="bibr" rid="R63">63</xref></sup>) and were then read into R and converted into TPM using the <italic>convertCounts</italic> function available in the <italic>DGEobj.utils package</italic>. For the purpose of clustering, we mapped the TPM dataset to the list of 24,070 genes used in the PAH dataset described in a previous section.</p></sec></sec><sec id="S12" sec-type="results"><title>Results</title><p id="P40">Omada was applied to five diverse gene expression datasets to demonstrate its utility in guiding cluster analysis and identifying plausible subgroups of samples. Two datasets were simulated by our tools. The simulated dataset with multiple distinct classes was used to determine Omada’s ability to accurately estimate <italic>k</italic> with reasonable stability when we know the existence of sample classes. In contrast, samples in the single-class simulated dataset were drawn from a single class and used to demonstrate the toolkit’s ability to point towards the lack of sample subgroups by indicating inconclusive low scores throughout the analysis. A multi-tissue pancan dataset was introduced to assess Omada’s capability to generate signal-based clusters that closely follow the tissue-specific patient sample distributions. In addition, to determine whether Omada can identify distinct heterogeneous subgroups from data without any prior classification information but potential present heterogeneity, we used a whole blood RNA-seq dataset from patients with pulmonary arterial hypertension (PAH)<sup><xref ref-type="bibr" rid="R18">18</xref></sup>. Lastly, implementation of the toolkit on a whole blood expression dataset (GUSTO) was included to demonstrate a case with potential technical biases and no known subgroups since it is composed of healthy participants.</p><p id="P41">For the above, we measured its consistency on algorithm, feature and number of clusters (k) selection and the stability of the generated clusters for a particular <italic>k</italic> (<italic>stability<sub>k</sub></italic>) and the average across <italic>k’s</italic> (<italic>stability<sub>avg</sub></italic>). It's important to note that the value of this validation is derived from the fact that unstable clusters should not be interpreted as this instability comes from problematic data or an incorrect approach. However, cluster stability only provides a mechanistic way to assess the underlying data structure and further information is required to fully biologically validate the clusters<sup><xref ref-type="bibr" rid="R38">38</xref></sup>.</p><sec id="S13"><title>Identifying known clusters</title><sec id="S14"><title>Multi-class dataset: Five distinct simulated expression classes representing heterogeneity</title><p id="P42">Omada’s basic function is to help identify samples that come from different sources and group together samples that come from the same source. Towards that end, we simulated a dataset with five sets of expression profiles with 50 samples each and 120 genes sourced from five unique distributions of expression data that represent heterogeneity within our samples. The means and standard deviations of each class are presented in <xref ref-type="fig" rid="F3">Figure 3A</xref>, depicting the expression differences. Additionally, the empirical Cumulative Distribution Functions (ECDFs) of the five simulated classes (<xref ref-type="fig" rid="F3">Figure 3B</xref>) as well as the high average Kolmogorov-Smirnov distances (D<sub>avg</sub>=88.3%, <xref ref-type="supplementary-material" rid="SD1">supplementary Table 1</xref>) show distinct differences between the distributions in respect to the expression in the simulated RNA-seq dataset. To demonstrate the effect of different sample and gene numbers, multiple datasets were simulated with an increasing number of samples and genes (<xref ref-type="fig" rid="F3">Figure 3C</xref>). The calculated cluster stabilities, where each value represents the stability over a range of <italic>k</italic> and a specific number of samples and features, show five or less samples per class provide highly unstable and unreliable clusters. The minimum acceptable stability threshold of 60% was achieved with at least 20 samples and a reliable stability of 75% was achieved using 1000 samples.</p><p id="P43">To test the ability of the clustering tools to produce stable clusters in various contexts we first apply them in sequence on strategically simulated data. The data are composed of distinct classes (based on class mean <italic>m<sub>class</sub></italic> and standard deviation <italic>sd<sub>class</sub></italic>) and due to that strong signal our tools are expected to determine an accurate <italic>k</italic> with reasonable stability, scoring above 60%. To allow for a more direct comparison, we used a multi-class simulated dataset (see <xref ref-type="sec" rid="S11">Test datasets</xref> in <italic>Methods</italic>) based on the original RNA-seq data<sup><xref ref-type="bibr" rid="R18">18</xref></sup>. When considering ranges of <italic>k</italic> we are using [<xref ref-type="bibr" rid="R2">2</xref>, <xref ref-type="bibr" rid="R6">6</xref>] clusters to observe a broader range of results for comparison reasons. First, the clustering feasibility tool showed that the highest stability was 78% (<xref ref-type="supplementary-material" rid="SD1">Supplementary Table 4</xref>) providing a strong indication of stability across our clusters. Since we selected a limited range of <italic>k</italic> ∈ [<xref ref-type="bibr" rid="R2">2</xref>, <xref ref-type="bibr" rid="R6">6</xref>] where the stability should remain high, the averaged -over every tested <italic>k-</italic> stability (stability<sub>avg</sub>) of 72% indicates a dataset of adequate size and class definition to proceed to clustering analysis. It should be noted that when large ranges of <italic>k</italic> are selected the average stability will naturally decrease as the calculations will take into account <italic>k</italic>’s much larger or smaller than the actual number of classes in the data. In such cases the user can review the individual <italic>k</italic> stabilities generated as part of this tool to conclude whether those values are satisfying i.e a minimum of 60%. Next, we calculated the partitioning agreement of three clustering algorithms and spectral clustering showed the highest average score of 56% (<xref ref-type="fig" rid="F5">Figure 5A</xref> and <xref ref-type="supplementary-material" rid="SD1">Supplementary Table 4</xref>). Partitioning agreement scores should be interpreted across algorithms applied on the same dataset rather than as absolute values keeping in mind that a score below 50% represents a random partitioning and subsequently a non-robust clustering. In the subsequent feature selection step, the highest average stability was registered when using all 300 features (stability<sub>avg</sub> = 78%, <xref ref-type="supplementary-material" rid="SD1">Supplementary Table 4</xref>), not discarding any feature as they all demonstrated very similar variance due to the nature of the simulated data. Finally, 8 out of 15 internal metrics voted five clusters as the optimal number during the k estimation step (<xref ref-type="fig" rid="F5">Figure 5B</xref>) providing a confident estimation above 50%.</p></sec><sec id="S15"><title>Single-class dataset: Homogeneously simulated dataset</title><p id="P44">To demonstrate Omada’s ability to identify datasets without any present clusters where all patients belong to one class, we used the single-class simulated dataset (see <xref ref-type="sec" rid="S11">Test datasets</xref> in <italic>Methods</italic>). All potential <italic>k</italic> of two or higher achieved low scores with average and maximum stabilities of 45% and 55%, respectively (<xref ref-type="supplementary-material" rid="SD1">supplementary Table 2</xref>). It is recommended to avoid clustering analysis on such low score datasets and instead opt for scores of at least 60%. Ideally, stabilities of 80-90% are considered very strong<sup><xref ref-type="bibr" rid="R64">64</xref></sup>, however the potential of several signals in transcriptomic data and the exploration across multiple <italic>k</italic> generally decreases the output stability to an acceptable threshold of 60-70%. Next, <xref ref-type="fig" rid="F4">Figure 4A</xref> shows the overall low partitional consistencies (averaged over all tested <italic>k</italic>) for all algorithms with spectral average partition agreement of 52%, kmeans average partition agreement of 3% and hierarchical average partition agreement of 26%. With the best performing algorithm showing an agreement of around 50% we can assume that the tested algorithms are randomly assigning memberships, therefore we cannot achieve a robust model with the current data. When using spectral clustering to select the most appropriate set of genes, the cluster stability rapidly dropped below 50% when using more than 20 genes (<xref ref-type="fig" rid="F4">Figure 4B</xref>) indicating that the algorithm got worse in assigning memberships as we considered more simulated genes. Finally, the ensemble voting step showed the majority of the votes supporting five clusters (<xref ref-type="fig" rid="F4">Figure 4C</xref>), a significant variation from the single simulated class of this dataset. In such unexpected outputs, one should examine the generated metric scores. In this case, the vast majority of metric scores are worse when we are testing single-class instead of multi-class simulations (<xref ref-type="supplementary-material" rid="SD1">supplementary Table 3</xref>) inferring lower cluster quality, i.e lower compactness and smaller distance between clusters. Additionally, worse scores (decided according to <xref ref-type="table" rid="T2">Table 2</xref>) infer higher uncertainty during the voting process.</p></sec></sec><sec id="S16"><title>Discriminating cancer types from pan cancer tissue expression data</title><p id="P45">An integral capability of Omada is to accurately stratify patients according to any biologically relevant signal present in expression data and detect differences stemming from genes, pathways, tissues etc. Real multi-tissue samples are often the focus of exploratory studies as they present cell-type differences but still unknown factors that may discriminate them. Using expression data from multiple cancer types (Pan-cancer dataset as described in <italic>Test datasets</italic> in <italic>Methods),</italic> we expect our tools to identify clusters that are consistent with the samples’ tissues of origin. Due to the different types of tumours we explored the potential cluster range of [<xref ref-type="bibr" rid="R2">2</xref>, <xref ref-type="bibr" rid="R5">5</xref>] for each pipeline step. The clustering feasibility of the dataset (2244 samples, 243 genes) presented an average stability of 88% and maximum stability of 100% (<xref ref-type="supplementary-material" rid="SD1">Supplementary Table 5</xref>) providing confidence for the downstream analysis. Spectral clustering showed the highest consistency (partition agreement<sub>avg</sub> = 63% closely resembling the simulated multiclass dataset, <xref ref-type="fig" rid="F5">Figure 5A</xref>) and was therefore deemed as the most robust. In this example hierarchical clustering showed high instability, as shown in <xref ref-type="fig" rid="F5">Figure 5A</xref>, demonstrating the importance of selecting the appropriate algorithm to create a robust model. According to our selection tool, all 243 genes produced the most stable set of clusters with a stability of 96% (<xref ref-type="supplementary-material" rid="SD1">Supplementary Table 5</xref>) which coupled with the high algorithm robustness indicated a model that most likely detects a signal in the data. Additionally, a very important observation is that all genes were deemed important to produce nearly perfectly stable clusters agreeing with the filtering of genes based on the cancer type annotations we performed prior to this clustering analysis. The ensemble voting tool estimated our dataset to contain three clusters of samples with the support of 57% of the metrics (<xref ref-type="fig" rid="F5">Figure 5B</xref>). When comparing these results with the simulated five-class dataset, both achieved higher certainty on the five clusters (&gt;50%, <xref ref-type="fig" rid="F5">Figure 5B</xref>) reflecting the rigid differences between the clusters when dealing with cancer tissues and simulated classes. In the case of the pancan partitioning, the breast, lung and colon/rectal samples almost perfectly grouped in their respective clusters (<xref ref-type="fig" rid="F5">Figure 5C</xref>).</p></sec><sec id="S17"><title>RNAseq data from diseased tissue with unknown heterogeneity</title><p id="P46">It is important for Omada to be able to robustly identify patient subgroups when heterogeneity for the cohort has not been previously characterised. We applied our tools on such a dataset (PAH dataset as described in <italic>Test datasets</italic> in <italic>Methods</italic>) to assess whether they can still produce stable clusters that differ in terms of expression profiles and other phenotypic measures. The feasibility for this dataset’s simulation showed an average stability of 61% and a maximum stability of 74% both acceptable to proceed with the clustering analysis (<xref ref-type="supplementary-material" rid="SD1">Supplementary Table 7</xref>). A notable 13% difference between average and maximum stability provides a positive indication that a specific <italic>k</italic> might prove significantly more stable downstream. The spectral clustering technique recorded the highest partitional consistency (partition agreement<sub>avg</sub> = 86% and partition agreement<sub>max</sub> = 96%, <xref ref-type="supplementary-material" rid="SD1">Supplementary Table 7</xref>) when we examined each algorithm's partition agreement for up to ten clusters. The bootstrapping subset selection tool estimated the 300 most variable genes as the most stable clustering parameter with a maximum stability of 73% (<xref ref-type="fig" rid="F6">Figure 6A</xref>) showing an impressive reduction from the initial gene set (25,955) and ensuring the removal of a lot of data noise. According to the ensemble voting tool two clusters were voted by 71% of the internal metrics followed by <italic>k</italic> = 3 (14%) and <italic>k</italic> = 5 (7%). Despite the strong indication of two clusters, <italic>k</italic> = 5 was selected to prevent loss of information occuring when smaller embedded clusters are disregarded. As shown by the downstream analysis, fully presented in <sup><xref ref-type="bibr" rid="R18">18</xref></sup>, selecting the higher <italic>k</italic>, even as a second estimate, allowed us to detect strong expression profiles. After considering cluster sizes the three predominant subgroups showed significant differences in expression, immunity and survival profiles as well as risk category distributions (<xref ref-type="fig" rid="F6">Figure 6B, C</xref>).</p></sec><sec id="S18"><title>RNA-seq data from healthy whole blood tissue</title><p id="P47">Next we tested how Omada would discriminate samples from healthy individuals from a single tissue type. Generally, in studies based on a dataset with no discernible heterogeneity to be explored - i.e a dataset without patients of dissimilar outcomes or controls - clustering algorithms may not be robust and may generate variable results. Useful partitionings might still be formed, such as unforeseen disease subgroups, but these observations must be validated. Towards that end we used the GUSTO RNA dataset of 238 mothers, as seen in <xref ref-type="sec" rid="S11">Test datasets</xref> in <italic>Methods.</italic> During determining clustering potential our simulated dataset showed stability<sub>avg</sub> = 56% and stability<sub>max</sub> = 59% (<xref ref-type="supplementary-material" rid="SD1">Supplementary Table 8</xref>), a similar low-stability score as in the simulated single-class (45% and 55%). We examined a k-range of [<xref ref-type="bibr" rid="R2">2</xref>, <xref ref-type="bibr" rid="R5">5</xref>] where spectral and k-means clustering showed very similar internal average partitional agreements of 61% and 60% and very high maximum agreements of 93% and 88% (<xref ref-type="supplementary-material" rid="SD1">Supplementary Table 8</xref>), respectively. The extremely high agreement scores should be interpreted with caution as they might not reflect a very strong signal but an underlying bias that partitions samples in similar groups repeatedly, over-powering the parameter changes. The 50 most variable genes were estimated to produce the most stable clustering with maximum stability = 71% (<xref ref-type="fig" rid="F6">Figure 6A</xref>). Similarly to the agreement scores, a small number of genes driving the most stable clusters (starting from 24,070 genes) might indicate either a strong expression signal or a pre-existing bias. When estimating the number of clusters, two (46%) and three (40%) clusters were voted by the majority showing a general consensus. Considering all the above strong indications, we need to assess the dataset and the resulting subgroups for potential biases before relying on the cluster memberships. Towards that end and utilising clinical data, the association results show the dataset might be biased based on technical batches with sequencer machine and flowID presenting significant differences between clusters (1.39e-03 and 2.55e-06, respectively) with hospital location coming close to significance with p-value = 0.072 (<xref ref-type="supplementary-material" rid="SD1">Supplementary Table 9</xref>). Additional statistical tests and regression analysis with maternal and foetal physiological and clinical phenotypes did not show any association with the clusters. The expression profiles of the two clusters show visible differences as do the t-SNE and PCA analyses (<xref ref-type="fig" rid="F6">Figure 6E</xref>) with the first principal component of the latter explaining 79% of the variance in the GUSTO dataset.</p></sec></sec><sec id="S19" sec-type="discussion"><title>Discussion</title><p id="P48">Our toolkit is designed to answer multiple questions arising during transcriptomic exploratory studies that target to uncover heterogeneity and subtypes within any condition that might be driven by expression changes. With the plurality of unsupervised methods available and their specialised nature, the selection of the most appropriate approach is a multi-factor problem. A lot of technical decisions are required in the procedure starting with a dataset and completed with a meaningful set of subgroups. To assist with this problem, our toolkit initially assesses the potential of a target dataset and provides estimates of the most appropriate method, gene set and number of subgroups finally outputting a partition based on optimised parameters unburdening the user of specialised decision making (<xref ref-type="fig" rid="F1">Figure 1</xref>). All individual tools are computed internally and therefore do not require prior deep knowledge of machine learning by the user. All results, intermediate and final, are observable and each step is justified by multiple measures and indices representing widely used machine learning techniques.</p><p id="P49">Applying unsupervised learning on expression datasets is often not a straightforward task as it contains the element of uncertainty mainly introduced by the lack of knowledge on the data points. No methods or metrics can give a definitive answer to the main clustering questions, as presented in previous sections, therefore each tool has to be used with caution, i.e. determining the dataset clustering potential is an indication rather than a clear sign that partitioning the dataset will yield informational subgroups. Additionally, clustering can often contain non-deterministic steps allowing for each function to behave slightly differently between similar runs. To reduce the uncertainty and provide a reliable set of tools, this toolkit has been applied on various gene expression datasets where its efficiency has been demonstrated. However, it is important to note that despite the use of multiple methodological approaches within this toolkit the inherent exploratory characteristics of clustering do not allow for clusters of definite value, instead they are meant to be dealt with scientific caution and biological validation. Aside from the actual memberships, the functions in this package can also reveal useful information about the input data. The GUSTO RNA-seq dataset showed that biases can be discovered by applying simple tests, such as PCA or tSNE, in conjunction with the cluster members. It is also possible for Omada to hint towards the existence of a single class, and therefore no heterogeneity, by consistently revealing low partition agreement and stability scores across multiple functions, as demonstrated in our single-class dataset example. Furthermore, Omada can help in selecting a small group of genes with potential partitioning capabilities as the feature selection step is expected to greatly reduce the number of genes which in most cases count to thousands. This toolkit is currently based on specific clustering techniques and metrics but its modular nature allows its extension to accommodate different data types that come in the form of continuous numeric values such as microRNA, metabolite or single cell RNA datasets. Furthermore, the structure of this toolkit allows for additional approaches to be added in the future to the pool of clustering algorithms to be tested keeping up with the current state of the art techniques.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary file</label><media xlink:href="EMS158957-supplement-Supplementary_file.pdf" mimetype="application" mime-subtype="pdf" id="d97aAdEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S20"><title>Acknowledgements</title><p>The UK National Cohort of Idiopathic and Heritable PAH is supported by grants from the British Heart Foundation (SP/12/12/29836 &amp; SP/18/10/33975) and the UK Medical Research Council (MR/K020919/1). Additional samples from the Sheffield Teaching Hospitals Observational Study of Pulmonary Hypertension, Cardiovascular and other Respiratory Diseases were supported by British Heart Foundation (PG/11/116/29288). S.K. is supported by a Donald Heath Ph.D. Studentship award and A*STAR Research Attachment Programme (ARAP) award.</p><p>The GUSTO study group includes Allan Sheppard, Amutha Chinnadurai, Anne Eng Neo Goh, Anne Rifkin-Graboi, Anqi Qiu, Arijit Biswas, Bee Wah Lee, Birit Froukje Philipp Broekman, Boon Long Quah, Chai Kiat Chng, Cheryl Shufen Ngo, Choon Looi Bong, Christiani Jeyakumar Henry, Daniel Yam Thiam Goh, Doris Ngiuk Lan Loh, Fabian Kok Peng Yap, George Seow Heong Yeo, Helen Yu Chen, Hugo P. S. van Bever, Iliana Magiati, Inez Bik Yun Wong, Ivy Yee-Man Lau, Jeevesh Kapur, Jenny L. Richmond, Jerry Kok Yen Chan, Joanna Dawn Holbrook, Johan G. Eriksson, Joshua J. Gooley, Keith M. Godfrey, Kenneth Yung Chiang Kwek, Kok Hian Tan, Krishnamoorthy Naiduvaje, Leher Singh, Lin Lin Su, Lourdes Mary Daniel, Lynette Pei-Chi Shek, Marielle V. Fortier, Mark Hanson, Mary Foong-Fong Chong, Mary Rauff, Mei Chien Chua, Michael J. Meaney, Mya Thway Tint, Neerja Karnani, Ngee Lek, Oon Hoe Teoh, P. C. Wong, Peter David Gluckman, Pratibha Keshav Agarwal, Rob Martinus van Dam, Salome A. Rebello, Seang Mei Saw, Shang Chee Chong, Shirong Cai, Shu-E Soh, Sok Bee Lim, Stephen Chin-Ying Hsu, Victor Samuel Rajadurai, Walter Stunkel, Wee Meng Han, Wei Wei Pang, Yap Seng Chong, Yin Bun Cheung, Yiong Huak Chan and Yung Seng Lee.</p></ack><sec id="S21" sec-type="data-availability"><title>Data Availability</title><p id="P50">The expression datasets used in this work can be accessed through the following sources: The two simulated, by Omada, datasets (single and multi-class) can be accessed and downloaded at <ext-link ext-link-type="uri" xlink:href="https://github.com/BioSok/OmadaSimulatedDatasets">https://github.com/BioSok/OmadaSimulatedDatasets</ext-link>. The Pan cancer tissue expression data can be accessed through (<ext-link ext-link-type="uri" xlink:href="https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq">https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq</ext-link>). The transcriptomic data used in this study can be accessed through the EGA (the European Genome-phenome Archive) database under accession code EGAS00001005532. In compliance with the Ethics under which these data and samples have been collected, the transcriptomic data are available through restricted access for approved researchers who agree to the conditions of use, i.e. keeping it secure and only using it for approved purposes. To apply for access please contact <email>cohortcoordination@medschl.cam.ac.uk</email>. You will receive an application form within 30 days. The ‘UK National PAH Cohort Study Data Access Committee’ will review requests within 3 months of receipt of the completed application form and if approved, provide details for access to the RNAseq data stored at the EGA. All requesters must agree to the data access conditions found in EGA. The data used to generate statistics, plots and figures are accessible through our interactive portal found in <ext-link ext-link-type="uri" xlink:href="https://sheffield-university.shinyapps.io/ipah-rnaseq-app/">https://sheffield-university.shinyapps.io/ipah-rnaseq-app/</ext-link>. The GUSTO expression dataset is available in NCBI Gene Expression Omnibus (GEO; <ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/">https://www.ncbi.nlm.nih.gov/geo/</ext-link>) under the accession numbers GSE182409 (Corresponding Reviewer token number: qjolmmeudnofnsv).</p><sec id="S22" sec-type="data-availability"><title>Code Availability</title><p id="P51">Code will be available on github and as a bioconductor software package (Omada) at 10.18129/B9.bioc.omada.</p></sec></sec><fn-group><fn id="FN1" fn-type="con"><p id="P52"><bold>Author Contributions</bold></p><p id="P53">SK and DW conceived the tools. SK undertook computational work and drafted the work with DW. AL, CR, TPF and MW participated in the data acquisition of the work. All authors revised it critically for important intellectual content; and gave final approval of the version submitted for publication.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>NY-L</given-names></name><etal/></person-group><article-title>Complementing tissue characterization by integrating transcriptome profiling from the Human Protein Atlas and from the FANTOM5 consortium</article-title><source>Nucleic Acids Res</source><year>2015</year><volume>43</volume><fpage>6787</fpage><lpage>6798</lpage><pub-id pub-id-type="pmcid">PMC4538815</pub-id><pub-id pub-id-type="pmid">26117540</pub-id><pub-id pub-id-type="doi">10.1093/nar/gkv608</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keen</surname><given-names>JC</given-names></name><name><surname>Moore</surname><given-names>HM</given-names></name></person-group><article-title>The Genotype-Tissue Expression (GTEx) Project: Linking Clinical Data with Molecular Analysis to Advance Personalized Medicine</article-title><source>J Pers Med</source><year>2015</year><volume>5</volume><fpage>22</fpage><lpage>29</lpage><pub-id pub-id-type="pmcid">PMC4384056</pub-id><pub-id pub-id-type="pmid">25809799</pub-id><pub-id pub-id-type="doi">10.3390/jpm5010022</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uhlén</surname><given-names>M</given-names></name><etal/></person-group><article-title>Proteomics. Tissue-based map of the human proteome</article-title><source>Science</source><year>2015</year><volume>347</volume><elocation-id>1260419</elocation-id><pub-id pub-id-type="pmid">25613900</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L</given-names></name><etal/></person-group><article-title>RNA sequencing-based longitudinal transcriptomic profiling gives novel insights into the disease mechanism of generalized pustular psoriasis</article-title><source>BMC Med Genomics</source><year>2018</year><volume>11</volume><elocation-id>52</elocation-id><pub-id pub-id-type="pmcid">PMC5989375</pub-id><pub-id pub-id-type="pmid">29871627</pub-id><pub-id pub-id-type="doi">10.1186/s12920-018-0369-3</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neff</surname><given-names>RA</given-names></name><etal/></person-group><article-title>Molecular subtyping of Alzheimer’s disease using RNA sequencing data reveals novel mechanisms and targets</article-title><source>Sci Adv</source><year>2021</year><volume>7</volume><pub-id pub-id-type="pmcid">PMC7787497</pub-id><pub-id pub-id-type="pmid">33523961</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.abb5398</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saeidian</surname><given-names>AH</given-names></name><name><surname>Youssefian</surname><given-names>L</given-names></name><name><surname>Vahidnezhad</surname><given-names>H</given-names></name><name><surname>Uitto</surname><given-names>J</given-names></name></person-group><article-title>Research Techniques Made Simple: Whole-Transcriptome Sequencing by RNA-Seq for Diagnosis of Monogenic Disorders</article-title><source>J Invest Dermatol</source><year>2020</year><volume>140</volume><fpage>1117</fpage><lpage>1126</lpage><elocation-id>e1</elocation-id><pub-id pub-id-type="pmcid">PMC8722382</pub-id><pub-id pub-id-type="pmid">32446329</pub-id><pub-id pub-id-type="doi">10.1016/j.jid.2020.02.032</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tran</surname><given-names>HTN</given-names></name><etal/></person-group><article-title>A benchmark of batch-effect correction methods for single-cell RNA sequencing data</article-title><source>Genome Biol</source><year>2020</year><volume>21</volume><elocation-id>12</elocation-id><pub-id pub-id-type="pmcid">PMC6964114</pub-id><pub-id pub-id-type="pmid">31948481</pub-id><pub-id pub-id-type="doi">10.1186/s13059-019-1850-9</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xing</surname><given-names>QR</given-names></name><etal/></person-group><article-title>Unraveling Heterogeneity in Transcriptome and Its Regulation Through Single-Cell Multi-Omics Technologies</article-title><source>Front Genet</source><year>2020</year><volume>11</volume><elocation-id>662</elocation-id><pub-id pub-id-type="pmcid">PMC7380244</pub-id><pub-id pub-id-type="pmid">32765578</pub-id><pub-id pub-id-type="doi">10.3389/fgene.2020.00662</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Firth</surname><given-names>AL</given-names></name><name><surname>Mandel</surname><given-names>J</given-names></name><name><surname>Yuan</surname><given-names>JX-J</given-names></name></person-group><article-title>Idiopathic pulmonary arterial hypertension</article-title><source>Dis Model Mech</source><year>2010</year><volume>3</volume><fpage>268</fpage><lpage>273</lpage><pub-id pub-id-type="pmcid">PMC2860847</pub-id><pub-id pub-id-type="pmid">20427556</pub-id><pub-id pub-id-type="doi">10.1242/dmm.003616</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koirala</surname><given-names>B</given-names></name><etal/></person-group><article-title>Heterogeneity of Cardiovascular Disease Risk Factors Among Asian Immigrants: Insights From the 2010 to 2018 National Health Interview Survey</article-title><source>J Am Heart Assoc</source><year>2021</year><volume>10</volume><elocation-id>e020408</elocation-id><pub-id pub-id-type="pmcid">PMC8403310</pub-id><pub-id pub-id-type="pmid">34182790</pub-id><pub-id pub-id-type="doi">10.1161/JAHA.120.020408</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rivera-Andrade</surname><given-names>A</given-names></name><name><surname>Luna</surname><given-names>MA</given-names></name></person-group><article-title>Trends and heterogeneity of cardiovascular disease and risk factors across Latin American and Caribbean countries</article-title><source>Prog Cardiovasc Dis</source><year>2014</year><volume>57</volume><fpage>276</fpage><lpage>285</lpage><pub-id pub-id-type="pmid">25218566</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manchia</surname><given-names>M</given-names></name><name><surname>Cullis</surname><given-names>J</given-names></name><name><surname>Turecki</surname><given-names>G</given-names></name><name><surname>Rouleau</surname><given-names>GA</given-names></name><name><surname>Uher</surname><given-names>R</given-names></name></person-group><article-title>The impact of phenotypic and genetic heterogeneity on results of genome wide association studies of complex diseases</article-title><source>PLoS One</source><year>2013</year><pub-id pub-id-type="pmcid">PMC3795757</pub-id><pub-id pub-id-type="pmid">24146854</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0076295</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vidman</surname><given-names>L</given-names></name><name><surname>Kallberg</surname><given-names>D</given-names></name><name><surname>Ryden</surname><given-names>P</given-names></name></person-group><article-title>Cluster analysis on high dimensional RNA-seq data with applications to cancer research-An evaluation study</article-title><source>PLoS One</source><year>2019</year><volume>14</volume><elocation-id>e0219102</elocation-id><pub-id pub-id-type="pmcid">PMC6894875</pub-id><pub-id pub-id-type="pmid">31805048</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0219102</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ren</surname><given-names>Z</given-names></name><name><surname>Wang</surname><given-names>W</given-names></name><name><surname>Li</surname><given-names>J</given-names></name></person-group><article-title>Identifying molecular subtypes in human colon cancer using gene expression and DNA methylation microarray data</article-title><source>Int J Oncol</source><year>2016</year><volume>48</volume><fpage>690</fpage><lpage>702</lpage><pub-id pub-id-type="pmcid">PMC4725456</pub-id><pub-id pub-id-type="pmid">26647925</pub-id><pub-id pub-id-type="doi">10.3892/ijo.2015.3263</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sotiriou</surname><given-names>C</given-names></name><name><surname>Neo</surname><given-names>SY</given-names></name><name><surname>McShane</surname><given-names>LM</given-names></name></person-group><article-title>Breast cancer classification and prognosis based on gene expression profiles from a population-based study</article-title><source>Proceedings of the Royal 626 Society B: Biological Sciences</source><year>2003</year></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lapointe</surname><given-names>J</given-names></name><etal/></person-group><article-title>Gene expression profiling identifies clinically relevant subtypes of prostate cancer</article-title><source>Proc Natl Acad Sci U S A</source><year>2004</year><volume>101</volume><fpage>811</fpage><lpage>816</lpage><pub-id pub-id-type="pmcid">PMC321763</pub-id><pub-id pub-id-type="pmid">14711987</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0304146101</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>F</given-names></name><etal/></person-group><article-title>Single-cell profiling of tumor heterogeneity and the microenvironment in advanced non-small cell lung cancer</article-title><source>Nat Commun</source><year>2021</year><volume>12</volume><elocation-id>2540</elocation-id><pub-id pub-id-type="pmcid">PMC8100173</pub-id><pub-id pub-id-type="pmid">33953163</pub-id><pub-id pub-id-type="doi">10.1038/s41467-021-22801-0</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kariotis</surname><given-names>S</given-names></name><etal/></person-group><article-title>Biological heterogeneity in idiopathic pulmonary arterial hypertension identified through unsupervised transcriptomic profiling of whole blood</article-title><source>Nat Commun</source><year>2021</year><volume>12</volume><elocation-id>7104</elocation-id><pub-id pub-id-type="pmcid">PMC8651638</pub-id><pub-id pub-id-type="pmid">34876579</pub-id><pub-id pub-id-type="doi">10.1038/s41467-021-27326-0</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>D</given-names></name><name><surname>Tian</surname><given-names>Y</given-names></name></person-group><article-title>A Comprehensive Survey of Clustering Algorithms</article-title><source>Annals of Data Science</source><year>2015</year><volume>2</volume><fpage>165</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.1007/s40745-015-0040-1</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reddy</surname><given-names>CK</given-names></name><name><surname>Vinzamuri</surname><given-names>B</given-names></name></person-group><article-title>A Survey of Partitional and Hierarchical Clustering Algorithms</article-title><source>Data Clustering</source><year>2018</year><fpage>87</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1201/9781315373515-4</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jamail</surname><given-names>I</given-names></name><name><surname>Moussa</surname><given-names>A</given-names></name></person-group><chapter-title>Current State-of-the-Art of Clustering Methods for Gene Expression Data with RNA-Seq</chapter-title><source>Applications of Pattern Recognition</source><publisher-name>IntechOpen</publisher-name><year>2020</year></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ezugwu</surname><given-names>AE</given-names></name><etal/></person-group><article-title>A comprehensive survey of clustering algorithms: State-of-the-art machine learning applications, taxonomy, challenges, and future research prospects</article-title><source>Eng Appl Artif Intell</source><year>2022</year><volume>110</volume><elocation-id>104743</elocation-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cao</surname><given-names>Y</given-names></name><name><surname>Geddes</surname><given-names>TA</given-names></name><name><surname>Yang</surname><given-names>JYH</given-names></name><name><surname>Yang</surname><given-names>P</given-names></name></person-group><article-title>Ensemble deep learning in bioinformatics</article-title><source>Nature Machine Intelligence</source><year>2020</year><volume>2</volume><fpage>500</fpage><lpage>508</lpage></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>C</given-names></name><name><surname>Took</surname><given-names>CC</given-names></name><name><surname>Seong</surname><given-names>J-K</given-names></name></person-group><article-title>Machine learning in biomedical engineering</article-title><source>Biomed Eng Lett</source><year>2018</year><volume>8</volume><fpage>1</fpage><lpage>3</lpage><pub-id pub-id-type="pmcid">PMC6208556</pub-id><pub-id pub-id-type="pmid">30603186</pub-id><pub-id pub-id-type="doi">10.1007/s13534-018-0058-3</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Choy</surname><given-names>G</given-names></name><etal/></person-group><article-title>Current Applications and Future Impact of Machine Learning in Radiology</article-title><source>Radiology</source><year>2018</year><volume>288</volume><fpage>318</fpage><lpage>328</lpage><pub-id pub-id-type="pmcid">PMC6542626</pub-id><pub-id pub-id-type="pmid">29944078</pub-id><pub-id pub-id-type="doi">10.1148/radiol.2018171820</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stafford</surname><given-names>IS</given-names></name><etal/></person-group><article-title>A systematic review of the applications of artificial intelligence and machine learning in autoimmune diseases</article-title><source>NPJ Digit Med</source><year>2020</year><volume>3</volume><elocation-id>30</elocation-id><pub-id pub-id-type="pmcid">PMC7062883</pub-id><pub-id pub-id-type="pmid">32195365</pub-id><pub-id pub-id-type="doi">10.1038/s41746-020-0229-3</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hulsen</surname><given-names>T</given-names></name><etal/></person-group><article-title>From Big Data to Precision Medicine</article-title><source>Front Med</source><year>2019</year><volume>6</volume><elocation-id>34</elocation-id><pub-id pub-id-type="pmcid">PMC6405506</pub-id><pub-id pub-id-type="pmid">30881956</pub-id><pub-id pub-id-type="doi">10.3389/fmed.2019.00034</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Williams</surname><given-names>C</given-names></name><name><surname>Liu</surname><given-names>ZH</given-names></name><name><surname>Croghan</surname><given-names>J</given-names></name></person-group><article-title>Big data management challenges in health research—a literature review</article-title><source>Brief Bioinform</source><year>2019</year><volume>20</volume><fpage>156</fpage><lpage>167</lpage><pub-id pub-id-type="pmcid">PMC6488939</pub-id><pub-id pub-id-type="pmid">28968677</pub-id><pub-id pub-id-type="doi">10.1093/bib/bbx086</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nayyar</surname><given-names>A</given-names></name><name><surname>Gadhavi</surname><given-names>L</given-names></name><name><surname>Zaman</surname><given-names>N</given-names></name></person-group><article-title>Machine learning in healthcare: review, opportunities and challenges</article-title><source>Machine Learning and the Internet of Medical Things in Healthcare</source><year>2021</year><fpage>23</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.1016/b978-0-12-821229-5.00011-2</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gaba</surname><given-names>D</given-names></name><name><surname>Mittal</surname><given-names>N</given-names></name></person-group><article-title>2. Implementation and classification of machine learning algorithms in healthcare informatics: approaches, challenges, and future scope</article-title><source>Computational Intelligence for Machine Learning and Healthcare Informatics</source><year>2020</year><fpage>21</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1515/9783110648195-002</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>C</given-names></name><name><surname>Gao</surname><given-names>X</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name></person-group><article-title>Impact of data preprocessing on cell-type clustering based on single-cell RNA-seq data</article-title><source>BMC Bioinformatics</source><year>2020</year><volume>21</volume><elocation-id>440</elocation-id><pub-id pub-id-type="pmcid">PMC7541255</pub-id><pub-id pub-id-type="pmid">33028196</pub-id><pub-id pub-id-type="doi">10.1186/s12859-020-03797-8</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eijssen</surname><given-names>LMT</given-names></name><etal/></person-group><article-title>User-friendly solutions for microarray quality control and pre-processing on ArrayAnalysis.org</article-title><source>Nucleic Acids Res</source><year>2013</year><volume>41</volume><fpage>W71</fpage><lpage>6</lpage><pub-id pub-id-type="pmcid">PMC3692049</pub-id><pub-id pub-id-type="pmid">23620278</pub-id><pub-id pub-id-type="doi">10.1093/nar/gkt293</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Andrews</surname><given-names>S</given-names></name></person-group><source>FastQC: a quality control tool for high throughput sequence data</source><year>2010</year><comment>Preprint at</comment></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bolger</surname><given-names>AM</given-names></name><name><surname>Lohse</surname><given-names>M</given-names></name><name><surname>Usadel</surname><given-names>B</given-names></name></person-group><article-title>Trimmomatic: a flexible trimmer for Illumina sequence data</article-title><source>Bioinformatics</source><year>2014</year><volume>30</volume><fpage>2114</fpage><lpage>2120</lpage><pub-id pub-id-type="pmcid">PMC4103590</pub-id><pub-id pub-id-type="pmid">24695404</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btu170</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>D’haene</surname></name><name><surname>Hellemans</surname></name></person-group><article-title>The importance of quality control during qPCR data analysis</article-title><source>Int Drug Discov</source></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baccarella</surname><given-names>A</given-names></name><name><surname>Williams</surname><given-names>CR</given-names></name><name><surname>Parrish</surname><given-names>JZ</given-names></name><name><surname>Kim</surname><given-names>CC</given-names></name></person-group><article-title>Empirical assessment of the impact of sample number and read depth on RNA-Seq analysis workflow performance</article-title><source>BMC Bioinformatics</source><year>2018</year><volume>19</volume><elocation-id>423</elocation-id><pub-id pub-id-type="pmcid">PMC6234607</pub-id><pub-id pub-id-type="pmid">30428853</pub-id><pub-id pub-id-type="doi">10.1186/s12859-018-2445-2</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Yue</surname><given-names>S</given-names></name><name><surname>Yu</surname><given-names>X</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name></person-group><article-title>An efficient data reduction method and its application to cluster analysis</article-title><source>Neurocomputing</source><year>2017</year><volume>238</volume><fpage>234</fpage><lpage>244</lpage></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hennig</surname><given-names>C</given-names></name></person-group><article-title>Cluster-wise assessment of cluster stability</article-title><source>Comput Stat Data Anal</source><year>2007</year><volume>52</volume><fpage>258</fpage><lpage>271</lpage></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hartigan</surname><given-names>JA</given-names></name></person-group><source>Clustering Algorithms</source><publisher-name>John Wiley &amp; Sons, Inc</publisher-name><year>1975</year></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dhillon</surname><given-names>IS</given-names></name></person-group><source>A Unified View of Kernel K-means, Spectral Clustering and Graph Cuts</source><publisher-name>Computer Science Department, University of Texas at Austin</publisher-name><year>2004</year></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ng</surname><given-names>AY</given-names></name><name><surname>Jordan</surname><given-names>MI</given-names></name><name><surname>Weiss</surname><given-names>Y</given-names></name></person-group><chapter-title>On Spectral Clustering: Analysis and an algorithm</chapter-title><source>Advances in Neural Information Processing Systems</source><person-group person-group-type="editor"><name><surname>Dietterich</surname><given-names>TG</given-names></name><name><surname>Becker</surname><given-names>S</given-names></name><name><surname>Ghahramani</surname><given-names>Z</given-names></name></person-group><publisher-name>MIT Press</publisher-name><year>2002</year><volume>14</volume><fpage>849</fpage><lpage>856</lpage></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rand</surname><given-names>WM</given-names></name></person-group><article-title>Objective Criteria for the Evaluation of Clustering Methods</article-title><source>J Am Stat Assoc</source><year>1971</year><volume>66</volume><fpage>846</fpage><lpage>850</lpage></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rodriguez</surname><given-names>MZ</given-names></name><etal/></person-group><article-title>Clustering algorithms: A comparative approach</article-title><source>PLoS One</source><year>2019</year><volume>14</volume><elocation-id>e0210236</elocation-id><pub-id pub-id-type="pmcid">PMC6333366</pub-id><pub-id pub-id-type="pmid">30645617</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0210236</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rousseeuw</surname><given-names>PJ</given-names></name></person-group><article-title>Silhouettes: A graphical aid to the interpretation and validation of cluster analysis</article-title><source>Journal of Computational and Applied Mathematics</source><year>1987</year><volume>20</volume><fpage>53</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1016/0377-0427(87)90125-7</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polikar</surname><given-names>R</given-names></name></person-group><article-title>Ensemble based systems in decision making</article-title><source>IEEE Circuits and Systems Magazine</source><year>2006</year><volume>6</volume><fpage>21</fpage><lpage>45</lpage></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calinski</surname><given-names>T</given-names></name><name><surname>Harabasz</surname><given-names>J</given-names></name></person-group><article-title>A dendrite method for cluster analysis</article-title><source>Communications in Statistics-Theory and Methods</source><year>1974</year><volume>3</volume><fpage>1</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1080/03610927408827101</pub-id></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dunn</surname><given-names>JC</given-names><suffix>†</suffix></name></person-group><article-title>Well-Separated Clusters and Optimal Fuzzy Partitions</article-title><source>Journal of Cybernetics</source><year>1974</year><volume>4</volume><fpage>95</fpage><lpage>104</lpage><pub-id pub-id-type="doi">10.1080/01969727408546059</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pakhira</surname><given-names>MK</given-names></name><name><surname>Bandyopadhyay</surname><given-names>S</given-names></name><name><surname>Maulik</surname><given-names>U</given-names></name></person-group><article-title>Validity index for crisp and fuzzy clusters</article-title><source>Pattern Recognition</source><year>2004</year><volume>37</volume><fpage>487</fpage><lpage>501</lpage><pub-id pub-id-type="doi">10.1016/j.patcog.2003.06.005</pub-id></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kendall</surname><given-names>MG</given-names></name></person-group><article-title>A New Measure of Rank Correlation</article-title><source>Biometrika</source><year>1938</year><volume>30</volume><fpage>81</fpage><pub-id pub-id-type="doi">10.2307/2332226</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>FB</given-names></name><name><surname>Hubert</surname><given-names>LJ</given-names></name></person-group><article-title>Measuring the Power of Hierarchical Cluster Analysis</article-title><source>Journal of the American Statistical Association</source><year>1975</year><volume>70</volume><fpage>31</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.1080/01621459.1975.10480256</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubert</surname><given-names>LJ</given-names></name><name><surname>Levin</surname><given-names>JR</given-names></name></person-group><article-title>A general statistical framework for assessing categorical clustering in free recall</article-title><source>Psychological Bulletin</source><year>1976</year><volume>83</volume><fpage>1072</fpage><lpage>1080</lpage><pub-id pub-id-type="doi">10.1037//0033-2909.83.6.1072</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davies</surname><given-names>DL</given-names></name><name><surname>Bouldin</surname><given-names>DW</given-names></name></person-group><article-title>A Cluster Separation Measure</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><year>1979</year><volume>PAMI-1</volume><fpage>224</fpage><lpage>227</lpage><pub-id pub-id-type="pmid">21868852</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McClain</surname><given-names>JO</given-names></name><name><surname>Rao</surname><given-names>VR</given-names></name></person-group><article-title>CLUSTISZ: A program to test for the quality of clustering of a set of objects</article-title><source>J Mark Res</source><year>1975</year><volume>12</volume><fpage>456</fpage><lpage>460</lpage></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Halkidi</surname><given-names>M</given-names></name><name><surname>Batistakis</surname><given-names>Y</given-names></name><name><surname>Vazirgiannis</surname><given-names>M</given-names></name></person-group><article-title>On Clustering Validation Techniques</article-title><source>J Intell Inf Syst</source><year>2001</year><volume>17</volume><fpage>107</fpage><lpage>145</lpage></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ray</surname><given-names>S</given-names></name><name><surname>Turi</surname><given-names>RH</given-names></name></person-group><source>Determination of number of clusters in k-means clustering and application in <italic>colour image segmentation</italic></source><conf-name>Proceedings of the 4th international conference on advances in pattern recognition and digital techniques</conf-name><conf-sponsor>Citeseer</conf-sponsor><year>1999</year><fpage>137</fpage><lpage>143</lpage></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohlf</surname><given-names>FJ</given-names></name></person-group><article-title>Methods of Comparing Classifications</article-title><source>Annual Review of Ecology and Systematics</source><year>1974</year><volume>5</volume><fpage>101</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1146/annurev.es.05.110174.000533</pub-id></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Halkidi</surname><given-names>M</given-names></name><name><surname>Vazirgiannis</surname><given-names>M</given-names></name></person-group><source>Clustering validity assessment: finding the optimal partitioning of a data set</source><conf-name>Proceedings 2001 IEEE International Conference on Data Mining</conf-name><pub-id pub-id-type="doi">10.1109/icdm.2001.989517</pub-id></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Song</surname><given-names>Y</given-names></name></person-group><source>Class compactness for data clustering</source><conf-name>2010 IEEE International Conference on Information Reuse &amp; Integration</conf-name><conf-sponsor>IEEE</conf-sponsor><year>2010</year><fpage>86</fpage><lpage>91</lpage></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="web"><source>cBioPortal for Cancer Genomics</source><comment><ext-link ext-link-type="uri" xlink:href="https://www.cbioportal.org/datasets">https://www.cbioportal.org/datasets</ext-link></comment></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weinstein</surname><given-names>JN</given-names></name><etal/></person-group><article-title>The Cancer Genome Atlas Pan-Cancer analysis project</article-title><source>Nature Genetics</source><year>2013</year><volume>45</volume><fpage>1113</fpage><lpage>1120</lpage><pub-id pub-id-type="pmcid">PMC3919969</pub-id><pub-id pub-id-type="pmid">24071849</pub-id><pub-id pub-id-type="doi">10.1038/ng.2764</pub-id></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>P</given-names></name><etal/></person-group><article-title>TissGDB: tissue-specific gene database in cancer</article-title><source>Nucleic Acids Res</source><year>2018</year><volume>46</volume><fpage>D1031</fpage><lpage>D1038</lpage><pub-id pub-id-type="pmcid">PMC5753286</pub-id><pub-id pub-id-type="pmid">29036590</pub-id><pub-id pub-id-type="doi">10.1093/nar/gkx850</pub-id></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kariotis</surname><given-names>S</given-names></name><name><surname>Jammeh</surname><given-names>E</given-names></name></person-group><article-title>BioSok/spectralclusteringofIPAH: v1.0.1</article-title><year>2021</year><pub-id pub-id-type="doi">10.5281/zenodo.5549872</pub-id></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>H</given-names></name><etal/></person-group><article-title>Integrative Multi-Omics database (iMOMdb) of Asian Pregnant Women</article-title><source>Hum Mol Genet</source><year>2022</year><pub-id pub-id-type="pmcid">PMC9476622</pub-id><pub-id pub-id-type="pmid">35445712</pub-id><pub-id pub-id-type="doi">10.1093/hmg/ddac079</pub-id></element-citation></ref><ref id="R64"><label>64</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tibshirani</surname><given-names>R</given-names></name><name><surname>Walther</surname><given-names>G</given-names></name></person-group><article-title>Cluster Validation by Prediction Strength</article-title><source>J Comput Graph Stat</source><year>2005</year><volume>14</volume><fpage>511</fpage><lpage>528</lpage></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>An overview of steps for discovering gene expression subgroups using Omada’s clustering tools.</title><p>First, processing, quality control and feasibility analysis are ensuring the input data are suitable for clustering. Then, choose the most robust clustering methodology that provides the most consistent partitions. Next, determine the genes that are useful for discriminating samples and provide the most stable clusters. Finally, determine the number of subgroups that potentially exist in the cohort by selecting the number of clusters (k) supported by the majority of internal machine learning indexes. The end result, after the final optimised clustering, consists of the assignment of a cluster to each sample driven solely by its expression profile.</p></caption><graphic xlink:href="EMS158957-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Sample selection overview.</title><p>(A) Ranking of samples based on their variance across features and the subsequent generation of datasets of increasing size. (B) Calculation of the stability score of each generated dataset. Initially, we select a cluster range to run our clustering method for each dataset. After the clustering procedure, we calculate and average the stability over the generated clusters. Finally, we average the stabilities over k per dataset and determine a final stability score for each dataset. The features of the dataset with the highest stability are the ones that compose the most appropriate set for the downstream pipeline</p></caption><graphic xlink:href="EMS158957-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><p>A) Expression boxplots for the five clusters showing the means and standard deviations B) The cumulative probability (as calculated from the empirical cumulative distribution function) for the five clusters calculated by a two sided Kolmogorov-Smirnov Test C) Average over-k stabilities for simulated datasets of increasing sample and gene numbers. A small number of samples consistently provides extremely unstable clusters (orange) while increasing both numbers consistently produces datasets that pass the stability threshold of 0.6 (blue).</p></caption><graphic xlink:href="EMS158957-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Performance criteria for single-class simulated dataset.</title><p>The results demonstrate low scores for the majority of steps. A) shows the average partition agreement of all three algorithms below the 52% mark indicating very unstable clustering runs overall. In B) the stability of every possible subsets of genes does not surpass 51.3% underlying overall unstable clusters. C) shows five clusters as the first estimate (voted by 8 metrics), significantly different from the one class this dataset contains.</p></caption><graphic xlink:href="EMS158957-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Performance criteria for two heterogeneous datasets, simulated multi-class and Pancan dataset.</title><p>The multi-class dataset contains artificial samples from five distinct clusters and the Pancan dataset is composed of three different cancer types presenting biological heterogeneity. A) The agreement between the predicted and true clusters (Adjusted Rand Index) from three different clustering algorithms (HC: hierarchical, KM: K-means, SC: spectral clustering) applied to the two datasets. B) Shows the real number of clusters for the dataset (black text) and the three most likely number of clusters k, with estimates of their percent probability. C) The contingency tables of the combinations between generated clusters (1st estimates) and real classes in the datasets. Darker red colour intensity denotes higher frequency.</p></caption><graphic xlink:href="EMS158957-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><title>Performance criteria for PAH and GUSTO datasets which have no known subgroups.</title><p>Panel A) depicts the average and max sample set stabilities (percentage) for both datasets. The red dashed line represents the threshold of a stable clustering (60%). The PAH RNA-seq dataset contains expression of IPAH patients with panel B) showing the gene expression heatmap and C) survival profiles for discovered subgroups. The GUSTO dataset contains expression from healthy maternal whole blood with panel D) showing the gene expression heatmap. The following panels show the distribution of cluster members across E) sequencer machines(chi-square p-value 1.39e-03), F) flow IDs (chi-square p-value 2.55e-06) and G) hospitals where the data were collected (chi-square p-value 0.072). H) t-SNE and PCA plots of the expression profiles with labelling of the two discovered subgroups.</p></caption><graphic xlink:href="EMS158957-f006"/></fig><table-wrap id="T1" position="float" orientation="portrait"><label>Table 1</label><caption><title>The clustering algorithms, their approach category and the various distance measures tested</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="top">Clustering algorithms</th><th align="center" valign="top">Category</th><th align="center" valign="top">Distance measures/kernels</th><th align="center" valign="top">Additional parameters</th></tr></thead><tbody><tr><td align="center" valign="top">K-means</td><td align="center" valign="top">Partitioning</td><td align="center" valign="top">Hartigan-Wong, Lloyd, Forgy, MacQueen</td><td align="center" valign="top">-</td></tr><tr><td align="center" valign="top">Hierarchical</td><td align="center" valign="top">Hierarchical</td><td align="center" valign="top">Euclidean, Manhattan, Minkowski, Canberra</td><td align="center" valign="top">Average, complete, median (linkage)</td></tr><tr><td align="center" valign="top">Spectral</td><td align="center" valign="top">Graph Theory</td><td align="center" valign="top">Rbfdot, Polydot, Tanhdot, Laplacedot, Vanilladot, Anovadot, Splinedot</td><td align="center" valign="top">-</td></tr></tbody></table></table-wrap><table-wrap id="T2" position="float" orientation="portrait"><label>Table 2</label><caption><title>The list of 15 internal indexes used to estimate the optimal number of clusters (k).</title></caption><table frame="box" rules="all"><thead><tr><th align="center" valign="top">Internal index</th><th align="center" valign="top">Ideal</th><th align="center" valign="top">Formula</th><th align="center" valign="top">Source</th></tr></thead><tbody><tr><td align="center" valign="middle">Calinski-Harabasz</td><td align="center" valign="middle">max</td><td align="center" valign="middle"><inline-formula><mml:math id="M6"><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mtext> </mml:mtext><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mtext> </mml:mtext><mml:mi>var</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mtext> </mml:mtext><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mtext> </mml:mtext><mml:mi>var</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle">46</td></tr><tr><td align="center" valign="middle">Dunn</td><td align="center" valign="middle">max</td><td align="center" valign="middle"><inline-formula><mml:math id="M7"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mtext> </mml:mtext><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle">47</td></tr><tr><td align="center" valign="middle">Pbm</td><td align="center" valign="middle">max</td><td align="center" valign="middle"><inline-formula><mml:math id="M8"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>k</mml:mi></mml:mfrac><mml:mo>*</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mi>W</mml:mi></mml:msup></mml:mrow></mml:mfrac><mml:mo>*</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle">48</td></tr><tr><td align="center" valign="middle">Tau</td><td align="center" valign="middle">max</td><td align="center" valign="middle"><inline-formula><mml:math id="M9"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>*</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>W</mml:mi></mml:msub><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle">49</td></tr><tr><td align="center" valign="middle">Gamma</td><td align="center" valign="middle">max</td><td align="center" valign="middle"><inline-formula><mml:math id="M10"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle">50</td></tr><tr><td align="center" valign="middle">C index</td><td align="center" valign="middle">min</td><td align="center" valign="middle"><inline-formula><mml:math id="M11"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>W</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle">51</td></tr><tr><td align="center" valign="middle">Davies–Bouldin</td><td align="center" valign="middle">min</td><td align="center" valign="middle"><inline-formula><mml:math id="M12"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mo>*</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:mtext> </mml:mtext><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mtext> </mml:mtext><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mtext> </mml:mtext><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext></mml:mrow><mml:mrow><mml:mtext> </mml:mtext><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mtext> </mml:mtext><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle">52</td></tr><tr><td align="center" valign="middle">Mcclain rao</td><td align="center" valign="middle">min</td><td align="center" valign="middle"><inline-formula><mml:math id="M13"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>W</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>*</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>W</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle">53</td></tr><tr><td align="center" valign="middle">sd_dis</td><td align="center" valign="middle">min</td><td align="center" valign="middle"><italic>a * (avg scattering for clusters</italic>) + <italic>total separation between clusters</italic></td><td align="center" valign="middle">54</td></tr><tr><td align="center" valign="middle">Ray-Turi</td><td align="center" valign="middle">min</td><td align="center" valign="middle"><inline-formula><mml:math id="M14"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mo>*</mml:mo><mml:mfrac><mml:mrow><mml:mtext> </mml:mtext><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mtext> </mml:mtext><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mspace width="0.2em"/><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mtext> </mml:mtext><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext><mml:mi>s</mml:mi><mml:mi>q</mml:mi><mml:mo>.</mml:mo><mml:mtext> </mml:mtext><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mtext> </mml:mtext><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mtext> </mml:mtext><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mtext> </mml:mtext><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>y</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mtext> </mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle">55</td></tr><tr><td align="center" valign="middle">g_plus</td><td align="center" valign="middle">min</td><td align="center" valign="middle"><inline-formula><mml:math id="M15"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>*</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>−</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula></td><td align="center" valign="middle">56</td></tr><tr><td align="center" valign="middle">Silhouette</td><td align="center" valign="middle">max</td><td align="center" valign="middle"><italic>average distance between clusters</italic></td><td align="center" valign="middle">44</td></tr><tr><td align="center" valign="middle">s_dbw</td><td align="center" valign="middle">min</td><td align="center" valign="middle"><italic>mean dispersion of clusters</italic> + <italic>between cluster density</italic></td><td align="center" valign="middle">57</td></tr><tr><td align="center" valign="middle">Compact ness</td><td align="center" valign="middle">max</td><td align="center" valign="middle"><italic>Intra – Cluster distance</italic></td><td align="center" valign="middle">58</td></tr><tr><td align="center" valign="middle">Connecti vity</td><td align="center" valign="middle">max</td><td align="center" valign="middle"><italic>The extent by which the items are placed in the same cluster as their nearest neighbours in the data space</italic></td><td align="center" valign="middle">-</td></tr></tbody></table><table-wrap-foot><fn id="TFN1"><p id="P54">All indexes are using different formulas to score a partitioning, measuring one or both of the following concepts: a) how compact each cluster is and b) how well the clusters separate. For each index we present which value is preferred (min or max) and its source. For the formulas: k = number of clusters, n = number of data points, E<sup>T</sup> = sum of the distances of all the points to the barycenter G of the entire dataset, E<sup>W</sup> = sum of the distances of the points of each cluster to their barycenter, NB = pairs constituted of points which do not belong to the same cluster, NB = pairs constituted of points which belong to the same cluster, N<sub>t</sub> = N<sub>W</sub> + NB, S<sub>W</sub> = sum of the N<sub>W</sub> distances between all the pairs of points inside each cluster, S<sub>MIN</sub> = sum of the N<sub>W</sub> smallest distances between all the pairs of points in the entire data set, S<sub>MAX</sub> = sum of the N<sub>W</sub> largest distances between all the pairs of points in the entire data set, SB = sum of the between-cluster distances, a = weight equal to the value of average scattering of clusters obtained for the partition with the greatest number of clusters.</p></fn></table-wrap-foot></table-wrap></floats-group></article>