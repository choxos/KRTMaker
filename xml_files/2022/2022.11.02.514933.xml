<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS156703</article-id><article-id pub-id-type="doi">10.1101/2022.11.02.514933</article-id><article-id pub-id-type="archive">PPR567255</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>CineFinch: An animated female zebra finch for studying courtship interactions</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Phaniraj</surname><given-names>Nikhil</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Joshi</surname><given-names>Sanjana</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Trimbake</surname><given-names>Pradeepkumar</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Pujari</surname><given-names>Aditya</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Ramadurai</surname><given-names>Samyuktha</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Kalra</surname><given-names>Shikha</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Ratnaparkhi</surname><given-names>Nikhil</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Rajan</surname><given-names>Raghav</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>Division of Biology, Indian Institute of Science Education and Research Pune, Pune, Maharashtra, 411008 INDIA</aff><aff id="A2"><label>2</label>Birla Institute of Technology and Science, Pilani, Rajasthan, 333031, INDIA</aff><author-notes><corresp id="CR1">
<label>*</label>Correspondence should be addressed to Raghav Rajan. <email>raghav@iiserpune.ac.in</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>06</day><month>11</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>03</day><month>11</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Dummies, videos and computer animations have been used extensively in animal behaviour to study simple social interactions. These methods allow complete control of one interacting animal, making it possible to test hypotheses about the significance and relevance of different elements of animal displays. Recent studies have demonstrated the potential of videos and interactive displays for studying more complex courtship interactions in the zebra finch, a well-studied songbird. Here, we extended these techniques by developing an animated female zebra finch and showed that ~40% of male zebra finches (n=5/12) sing to this animation. To study real-time social interactions, we developed two possible methods for closed loop control of animations; (1) an arduino based system to initiate videos/animations based on perch hops and (2) a video game engine based system to change animations. Overall, our results provide an important tool for understanding the dynamics of complex social interactions during courtship.</p></abstract><kwd-group><kwd>Bird song</kwd><kwd>courtship displays</kwd><kwd>video interactions</kwd><kwd>animations</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Complex behavioural displays are used by animals to communicate with each other (<xref ref-type="bibr" rid="R3">Bradbury and Vehrencamp, 2011</xref>). These behavioural displays carry information necessary for successful communication and often, have multiple different components that are produced together. For example, the courtship dance of the blue-capped cordon bleu, a songbird, involves multiple rapid foot and head movements that occur just before song, a vocal signal (<xref ref-type="bibr" rid="R18">Ota et al., 2015</xref>). Are all of these different components (foot, head movements and vocal signal) important for successful courtship? Do individual components of the display carry information about courtship potential or is courtship potential signalled only by presence of the entire display? Answering these questions requires complete control over one of the interacting animals to ensure that different components can be produced independent of each other. Such control is provided by robotic dummies, videos and computer animations and animal behaviour studies have a rich tradition of using such stimuli (<xref ref-type="bibr" rid="R31">Tinbergen, 1948</xref>; <xref ref-type="bibr" rid="R38">Woo and Rieucau, 2011</xref>). For example, Patricelli and colleagues used robotic female satin bowerbirds in artificially staged courtship interactions to demonstrate that male satin bowerbirds regulate the intensity of their courtship displays based on female responses (<xref ref-type="bibr" rid="R19">Patricelli et al., 2006</xref>). Van Dyk and Evans used computer animations of Jacky dragon lizards to understand the dynamics of aggressive encounters and showed that lizards use multiple signals to assess the level of aggression (<xref ref-type="bibr" rid="R34">Van Dyk and Evans, 2008</xref>).</p><p id="P3">Recent studies have extended the use of dummies and videos to understand more complex interactions like courtship in songbirds. In the zebra finch, a well-studied songbird native to Australia, courtship involves a song and a dance by the male (<xref ref-type="bibr" rid="R25">Sossinka and Böhner, 1980</xref>; <xref ref-type="bibr" rid="R33">Ullrich et al., 2016</xref>; <xref ref-type="bibr" rid="R39">Zann, 1996</xref>). Females also respond with vocalizations and tail-quivering displays (<xref ref-type="bibr" rid="R39">Zann, 1996</xref>). Male zebra finches sing to taxidermically stuffed female zebra finches and to videos of female zebra finches and the characteristics of these songs are highly similar to courtship song directed at a live female bird (<xref ref-type="bibr" rid="R2">Bischof et al., 1981</xref>; <xref ref-type="bibr" rid="R5">Galoch and Bischof, 2007</xref>; <xref ref-type="bibr" rid="R9">James et al., 2019</xref>). In addition to courtship interactions, male zebra finches also interact with juvenile zebra finches during tutoring sessions. Juveniles learn songs more accurately from a live tutor than from song playbacks from a speaker (<xref ref-type="bibr" rid="R4">Derégnaucourt et al., 2013</xref>). This suggests that the visual stimulus of the tutor and possibly social interactions with a tutor are also important for accurate learning. While the mere presence of a visual tutor, in the form of a video, is not sufficient to enhance learning (<xref ref-type="bibr" rid="R36">Varkevisser et al., 2022a</xref>), a recent study showed that a robotic zebra finch that vocally interacts with juvenile birds does enhance learning (<xref ref-type="bibr" rid="R1">Araguas et al., 2022</xref>). Importantly, the robotic zebra finch provided closed-loop vocal interactions, i.e. vocal interactions were provided only when juveniles interacted with the robot and the timing of these vocal interactions were comparable to the timing of interactions with live tutors. The robotic zebra finch highlighted the potential of appropriately manipulatable, artificial stimuli for probing complex social interactions, but such stimuli have not been tested in the context of courtship.</p><p id="P4">In addition to robots, animations provide an attractive method to provide closed-loop social interactions. The advent of fast computer hardware and user-friendly open-source graphics software has made it much easier to generate and control animations (<xref ref-type="bibr" rid="R27">Stowers et al., 2017</xref>). They provide a complementary approach to robots with considerable flexibility for studying social interactions. Here, we developed an animation of a female zebra finch and showed that ~40% of male zebra finches tested, sang to this animation. We also demonstrated two possible ways to interactively control these animations.</p></sec><sec id="S2" sec-type="materials | methods"><title>Materials and Methods</title><p id="P5">All experimental procedures conducted were approved by IISER Pune’s Institute Animal Ethical Committee (IAEC) and were in accordance with the guidelines of the Committee for the Purpose of Control and Supervision of Experiments on Animals (CPSCEA), New Delhi. Zebra finches (n=25 males and n=5 females) were procured from a local vendor (n=7) or bred in our colony at IISER Pune (n=23). Birds bought from an outside vendor were used for experiments only after they had been in our colony at IISER Pune for more than 30 days. Birds were raised in individual cages (120 cm x 50 cm x 50 cm cage, up to 6 birds per cage) along with other birds of the same gender. Light conditions were regulated to maintain a 14/10 hour day/night cycle. <italic>Ad libitum</italic> access to food and water were provided at all times, unless otherwise mentioned. All birds were &gt;100 dph at the time of the experiment (males &gt; 100dph and females &gt; 300dph).</p><sec id="S3"><title>Experimental Apparatus</title><p id="P6">The apparatus consisted of a metal double-cage (46 cm x 23 cm x 23 cm) with each half separated by a glass slab (23 cm x 23 cm x 0.3 cm). One half housed the subject male bird. During live female trials, the other half housed a stimulus female bird. During video playback or animation trials, the other half had a Samsung galaxy tab S4 placed upright lengthwise with its screen touching the glass slab. To record song produced by the male, we placed a microphone (AKG Acoustics C417PP omnidirectional condenser microphone) on top of the cage above the male. A camera (GoPro Hero7) was placed outside the apparatus to record videos of the sessions.</p></sec><sec id="S4"><title>Experimental subjects</title><p id="P7">For the experiments where males were presented with videos of females (video trials), we used 10 male zebra finches for the 30s trials and 10 males for the 4-minute trials. For the animation experiments, we used 12 males. 4 males were common across the 30s and 4-minute trials, 1 male was common across both 4 minute trials and animation trials and 1 bird was common for 30s and 4-minute video trials and animation trials. For the video experiments, we used 3 female zebra finches for the 30s trials and 2 female zebra finches for the 4-minute trials. For the animation experiments, we used 2 females. One female was common across 4-minute and 30s video trials and one female was common across 30s video and animation trials.</p></sec><sec id="S5"><title>Stimulus videos</title><p id="P8">4 minute long videos (2560 x 1440 pixels @60 fps) of a female bird were recorded while the female interacted with a male bird (see <xref ref-type="supplementary-material" rid="SD1">Movie 1</xref> for part of a video). The same experimental setup that was used for video trials with the male was used for this and the male was positioned just beside the video camera, on the other side of the glass slab (<xref ref-type="fig" rid="F1">Fig. 1A</xref>). The aspect ratio of the video was fixed at 15.5 cm x 8.7 cm to ensure that the female bird in the video appeared life-sized.</p></sec><sec id="S6"><title>Construction of Animations</title><p id="P9">Initially, a 13.67s long animation of a female zebra finch was made using Blender 2.90.1 (<ext-link ext-link-type="uri" xlink:href="https://www.blender.org/">https://www.blender.org/</ext-link>). This initial animation consisted of 820 frames which were played at 60fps. To make this, first, a 3D model of a female zebra finch was sculpted (<xref ref-type="fig" rid="F2">Fig. 2A</xref>). Next, colours from a photograph of a female zebra finch were extracted and used to paint the skin of the 3D model (<xref ref-type="fig" rid="F2">Fig. 2B</xref>). A feathery texture was added. 2 core bones (grey in <xref ref-type="fig" rid="F2">Fig. 2C</xref>) and 9 structural bones (yellow in <xref ref-type="fig" rid="F2">Fig. 2C</xref>) were rigged into the body and 5 control bones (blue in <xref ref-type="fig" rid="F2">Fig. 2C</xref>) were placed outside that connected to a single bone or multiple distant bones for synchronized body movements and provided for puppeteerl-ike control of the zebra finch (<xref ref-type="fig" rid="F2">Fig. 2A-2D</xref>). One of the control bones was placed in front of the beak and the beak bone programmed to always point towards the beak control bone. The beak-head joint was kept rigid whereas the head bone was free to rotate around the neck bone giving the head-neck joint 3 degrees of freedom. The second control bone was placed behind the tail and the tail bone programmed to always point to the tail control bone. The tail bone could rotate around the joint in both horizontal and vertical plane but not around its long axis, giving it 2 degrees of freedom. 2 of the control bones were placed in front of the claw and the claw bones were programmed to always point to the claw control bone. 2 additional bones were placed behind the knee, outside the body, to mark endpoints for inverse kinematics of the knee (red in <xref ref-type="fig" rid="F2">Fig. 2C</xref>). These two external bones prevented the knee joints from displacing beyond the marked endpoints. The ankle joint was kept rigid whereas the knee and the pelvis were allowed restricted rotation in only the vertical plane giving them 1 degree of freedom. The 5th control bone was the master control bone (deep blue in <xref ref-type="fig" rid="F2">Fig. 2C</xref>) placed directly below the centre of mass of the bird and this bone connected the other 4 control bones and allowed for coordinated movements of the two legs, tail, and head.</p><p id="P10">The initial animation started with the female zebra finch hopping in from the left, performing 2 head turns (towards its right and its left), hopping to the centre, rotating towards the viewer, bending, and tail quivering (see <xref ref-type="fig" rid="F2">Fig. 2E-2H</xref> for frames from the animation). After the tail quivering, the female was made to go back to the initial posture, hop to the right, perform 2 head turns (towards its right and its left), and hop to the right again – out of view. The velocity of hops, head turns and tail quivering were roughly similar to those of the female in one of the 30s responsive videos. We only matched the velocity qualitatively and did not reproduce exact statistics of movements. This initial animation was duplicated, laterally inverted and stitched to the end of the original animation to make the female enter from the left, exit from the right, then enter from the right, and exit from the left. This entire animation sequence, now 1640 frames (27.33s) long was again duplicated thrice and the 4 parts stitched together. A 10s clip showing an image of an empty cage was added to the beginning and the entire animation video was ~ 2mins (119.33s) long. A 5s long audio clip with 3 female calls was added to the first 5s of the animation (see <xref ref-type="supplementary-material" rid="SD2">Movie 2</xref>). The final animation video was played with the audio on (Call animation) or muted (No call animation). A color control animation was made where the bird in the animation was entirely painted with deep blue and the feathery texture removed (see <xref ref-type="supplementary-material" rid="SD2">Movie 2</xref>). Everything else was kept the same.</p></sec><sec id="S7"><title>General experimental procedures</title><p id="P11">All birds used for experiments were housed singly in small cages (23 cm x 23 cm x 23 cm) starting a few days before the start of the experiment and for the entire duration of the experiment, unless otherwise mentioned. For 30s trials, pairs of male subject birds were acclimatized to the setup by housing them in the apparatus – one bird on each half of the setup for 48 hours – 2 weeks before the start of the experimental trials, and again for 48 hours, before the time of the experimental trial. All experimental trials were conducted in sound-attenuation enclosures (Newtech Engineering Systems, Bengaluru or custom-made enclosures). For all experiments, songs were recorded with a microphone placed at the top of the cage. Signals from the microphone were amplified using a mixer (Behringer XENYX 802) and then digitized on a computer system at a sampling rate of 44,100 Hz using a custom-written Python based software. Undirected songs were recorded over a period of 24 hours, with birds housed singly in a cage (23 cm x 23 cm x 23 cm) in the same sound isolation box. 1 of the 7 birds never sang undirected songs.</p></sec><sec id="S8"><title>Video presentation procedures</title><p id="P12">Across all trial types, videos were played manually by the experimenter, without the audio, as a window on the screen. The video window overlapped an image of an empty cage in the background. The tablet was placed in its position as soon as the video was started. Once the video ended, the video window disappeared showing only the image of the empty cage on the screen. Animations were also converted into videos as described above and played using the same procedure.</p></sec><sec id="S9"><title>Short duration experimental trials (30s Video trials)</title><p id="P13">For the 30s video trials, experimental trials were conducted over a period of 10 days with one bird going through the experiment each day. Two, 30s long, silent, videos of each female were chosen as stimuli; (1) a “responsive” video where the female showed tail quivering and (2) an “unresponsive” video where the female did not show tail quivering. These videos were cut out of the 4 minute long videos of female birds that were recorded as described above. Along with the “responsive” and “unresponsive” videos, we also presented a live female as a third stimulus. Each of the three stimuli was repeated thrice and the 9 stimuli were presented to the male in random order separated by 5 minutes (<xref ref-type="fig" rid="F1">Fig. 1B</xref>).</p></sec><sec id="S10"><title>Long duration experimental trials (4-minute video trials)</title><p id="P14">For the 4-minute video trials, we used the 4 minute long videos recorded as described above. Experimental trials were conducted over multiple days. On a given day, male birds were briefly checked with ~10s exposure to the tablet to see if they sang to the video. Birds went through the trial only if they sang to the video during the brief exposure (n=3/10 birds did not sing to the video on their first exposure and were not used further). During the actual trial, the subject bird was exposed to either the live female or the video for 4 minutes. For the live female condition, one of the two females was randomly chosen as the stimulus. For the video playback condition, one of the two videos (one of each female stimulus) was chosen to be played. For each male, only one type of trial was conducted on a given day. This procedure was repeated for each male on separate days until we obtained atleast 6 song bouts for each trial type (median of 7 days per male; range: 6-18).</p></sec><sec id="S11"><title>Animation trials</title><p id="P15">For the animation trials, birds were tested with a single exposure to the 2 minute animation of the female zebra finch without audio. Only 5/12 birds tested sang to this animation and these 5 birds were chosen for further experiments. Experimental trials consisted of 3 days of experiments with each day consisting of 5 types of trials (live female, video of female, animation with call, animation without call and colour control animation) in a random sequence. Each trial lasted 2 minutes and trials were separated by 5 minutes.</p></sec><sec id="S12"><title>Song Analysis</title><p id="P16">Songs were analysed as described earlier (<xref ref-type="bibr" rid="R10">Kalra et al., 2021</xref>; <xref ref-type="bibr" rid="R22">Rajan and Doupe, 2013</xref>; <xref ref-type="bibr" rid="R23">Rao et al., 2019</xref>), using custom-written scripts on MATLAB (MathWorks). Briefly, songs were first segmented into syllables based on an amplitude threshold. Syllables with inter-syllable intervals &lt; 5 ms were merged into one syllable and syllables shorter than 10s were discarded. Then, syllables were manually labelled based on their appearance on a spectrogram; similar looking syllables were given the same label. The sequence of syllables that repeated in a bird’s song was identified as a motif (<xref ref-type="bibr" rid="R25">Sossinka and Böhner, 1980</xref>; <xref ref-type="bibr" rid="R39">Zann, 1996</xref>). Short syllables that were repeated a variable number of times at the beginning of a bout were considered as introductory notes (<xref ref-type="bibr" rid="R10">Kalra et al., 2021</xref>; <xref ref-type="bibr" rid="R21">Rajan, 2018</xref>; <xref ref-type="bibr" rid="R22">Rajan and Doupe, 2013</xref>; <xref ref-type="bibr" rid="R23">Rao et al., 2019</xref>). The number of introductory notes at the beginning of a bout was quantified by starting with the introductory note immediately preceding the first syllable of the bout and counting backwards until either (1) the previous note was not an introductory note or (2) there was a silence &gt; 500 ms (<xref ref-type="bibr" rid="R10">Kalra et al., 2021</xref>; <xref ref-type="bibr" rid="R11">Kao et al., 2005</xref>; <xref ref-type="bibr" rid="R22">Rajan and Doupe, 2013</xref>; <xref ref-type="bibr" rid="R23">Rao et al., 2019</xref>; <xref ref-type="bibr" rid="R25">Sossinka and Böhner, 1980</xref>). Song bouts were defined as groups of vocalizations that contained at least one motif syllable and were separated from other such groups by more than 2 s of silence. All song bouts sung by birds towards live females and video/animation were considered for analysis as independent song bouts and were combined with all other song bouts across sessions/days. Of all the undirected bouts sung by a bird, 15 were randomly chosen for analysis for the 4 minute trials (median: 15; range: 14-16) and 4 were chosen randomly for analysis for the 30s trials (median: 4; range: 2-6). The number of undirected bouts to analyse was chosen to be comparable to the number of song bouts produced by the male in response to the live female and video trials.</p></sec><sec id="S13"><title>Tutoring of birds with videos</title><p id="P17">Juveniles used for tutoring were isolated as described earlier (<xref ref-type="bibr" rid="R10">Kalra et al., 2021</xref>). Briefly, fathers were removed from the nest around 12 days post hatching (range: 9-13) and juveniles were housed along with their mother until they could feed on their own (range: ~35-40 days post hatch). Following this, individual juveniles were housed singly and tutoring began on ~37dph (range: ~ 36-46 dph). Tutoring was done in a cage that contained two perches; one was active and associated with playback of the tutor song as described below and one perch was a dummy. The active perch had an IR beam running across it, which was monitored by an Arduino board (<ext-link ext-link-type="uri" xlink:href="https://www.arduino.cc/">https://www.arduino.cc/</ext-link>). When the bird jumped onto the perch a break in the IR beam was detected by the Arduino-MATLAB interface. This triggered the tablet screen to play the video of the male bird along with audio playback of the tutor song. Specifically, an IR-beam break initiated a command to virtually “tap” on a specific coordinate on the tablet screen. This “tap” opened the video in a VLC window of whose size was chosen to ensure that the bird in the video was of a realistic size. The latency of the perch-hop to display loop was generally &lt;1 sec but we noticed that it was longer for the first few hops in a session. After the video ended, the display was reset to an image of the far end of the cage as it would have appeared if the tablet were not present. This was also used as the initial image before tutoring began. Tutoring was done for 1-2 sessions per day (median number of tutoring sessions across birds = 45; range = 24-51; some days, we did not conduct tutoring). In each session, we limited the number of possible song playbacks to 20 as too much exposure to song has been shown to result in poor imitation (<xref ref-type="bibr" rid="R29">Tchernichovski et al., 1999</xref>). Each session lasted until the bird had exhausted the quota of 20 playbacks. If the bird did not trigger 20 playbacks, then the session was terminated after 1 hour. Passive playbacks were used occassionally to prompt the bird to hop on the perch. Playback songs consisted of introductory notes (INs) followed by 2 motifs (n=3 were tutored with 2 INs and n=3 were tutored with 7 INs). At ~70 dph (range: 63-79 dph), we switched the number of INs for the tutor song and tutoring with the changed song continued till 90 dph. These were done to determine if the number of INs can be changed during the course of learning and are part of another ongoing study. However, for the purpose of this study, we only considered the accuracy of imitation of the motif and this motif was not changed during the entire duration of tutoring.</p><p id="P18">Song similarity was calculated using 5 example motifs recorded from these birds at ~90 dph (range: 90-100 dph). Similarity calculations were done using Sound Analysis Pro (SAP) (<xref ref-type="bibr" rid="R30">Tchernichovski et al., 2000</xref>) as described earlier (<xref ref-type="bibr" rid="R10">Kalra et al., 2021</xref>). Briefly, asymmetric time-course similarity measurements were made between tutor song and 5 example motifs and the average of these similarity values was taken as a measure of similarity to the tutor song. For comparison with birds tutored without videos, we used birds tutored at IISER Pune for a different study (n=10) (<xref ref-type="bibr" rid="R10">Kalra et al., 2021</xref>). These birds were tutored with 2 motifs without INs using active playback methods.</p></sec><sec id="S14"><title>Closed-loop animations</title><p id="P19">The animated female zebra finch described earlier was used in conjunction with a video game engine to create interactive animations. First, animations were added to the female model using keyframing, a technique used in blender. Animations included movements like hops, neck movements, body rotation, tail quivering, and panting. The animation also included female calls (3 calls per stimulus). The animated female was then exported in FBX (filmbox) format. FBX format was chosen for compatibility with the game development software, Unreal Game engine 4 (<ext-link ext-link-type="uri" xlink:href="https://www.unrealengine.com/en-US">https://www.unrealengine.com/en-US</ext-link>). This model was imported into Unreal engine 4 (version 4.27.2). Unreal engine 4 was used to create a simple game for generating potential stimuli. First, we created a 1D blend space, i.e. an axis that allowed us to gradually blend a stationary animated female zebra finch into a hopping, tail-quivering zebra finch. Then, key-presses were associated with different positions along this axis, thereby allowing us to control the speed of hopping and tail-quivering. The key-press associations were made within the code of the game engine (<xref ref-type="supplementary-material" rid="SD1">Fig. S3</xref>) and such stimuli generated from the game would allow interactive-control of the animation in real-time.</p></sec><sec id="S15"><title>Statistical analysis</title><p id="P20">We did not perform any apriori sample size calculations. All statistical analyses was done with R. Sample sizes used were comparable to other studies (<xref ref-type="bibr" rid="R8">Ikebuchi and Okanoya, 1999</xref>; <xref ref-type="bibr" rid="R9">James et al., 2019</xref>; <xref ref-type="bibr" rid="R28">Takahasi et al., 2005</xref>) Generalized Linear Mixed Models (GLMMs) were used to statistically analyse the data. We considered each song bout as a single unit and considered all song bouts across trial types for statistical analyses. Response variables were fit using the appropriate distributions; bouts per presentation and number of INs were count variables and were fit using a Poisson distribution, bout durations were skewed towards lower duration values, so we log transformed these values and a log-linked Gaussian distribution was used. Bird identity was a random effect and stimulus type was a fixed effect for all analyses. Day Number, Presentation number and bout number (all ordinals) were included as fixed terms while analyzing the number of INs and bout durations in the animation trials. Only day number and presentation number were included as fixed terms (along with stimulus type) for analyzing the number of bouts per stimulus for animation trials. Only presentation number and stimulus type were fixed terms for analyzing the number of bouts per stimulus for the 30 s video trials. For the rest, stimulus type was the only fixed term (day, presentation, and bout numbers could not be included when comparisons with undirected songs were made). Type II Wald Chi Sq. tests were used to determine statistical significance followed by pairwise comparisons between stimulus type using estimated marginal means with Holm correction. To compare the log ratio of time spent singing across different conditions, and to analyze the number of bouts per stimulus in the 4-minute video trials, we used a Kruskal-Wallis test. An alpha level of 0.05 was considered for all statistical analyses.</p></sec></sec><sec id="S16" sec-type="results"><title>Results</title><sec id="S17"><title>Male zebra finches sang short song bouts to 30s videos of female zebra finches</title><p id="P21">Our goal was to make an animated female zebra finch that could be used for understanding social interactions. We first tested our display systems by examining responses of male zebra finches to videos of female zebra finches. We used two different videos (see <xref ref-type="fig" rid="F1">Fig. 1A</xref> and Methods for video recording procedures) of the same female: (1) a “responsive” video where the female made tail-quivering movements and (2) an “unresponsive” video without tail-quivering movements (<xref ref-type="supplementary-material" rid="SD1">Movie 1</xref>). Individual males were presented with 3 presentations each of the two types of videos and 3 presentations of a live female in random order (<xref ref-type="fig" rid="F1">Fig. 1B</xref>; see <xref ref-type="sec" rid="S2">Methods for details</xref>). Male birds sang comparable number of songs to videos of female birds and to live females (<xref ref-type="fig" rid="F1">Fig. 1C, 1D</xref>, <xref ref-type="supplementary-material" rid="SD1">Movie 1</xref>, <xref ref-type="supplementary-material" rid="SD4">Fig. S1A, Fig. S2A</xref>). As reported earlier (<xref ref-type="bibr" rid="R9">James et al., 2019</xref>), song bouts directed to either of the videos were considerably shorter than song bouts directed to a live female (<xref ref-type="fig" rid="F1">Fig. 1E</xref>). Songs directed towards live females and songs directed towards videos of females were preceded by greater number of introductory notes as compared to “undirected” songs produced when the bird was alone (<xref ref-type="fig" rid="F1">Fig. 1F</xref>), indicating the “directed” nature of songs sung to the videos (<xref ref-type="bibr" rid="R25">Sossinka and Böhner, 1980</xref>). Overall, these results showed that males sing “directed” songs to videos of females and singing behaviour was not influenced by responsiveness of the female.</p></sec><sec id="S18"><title>Longer duration videos elicited longer song bouts</title><p id="P22">To test whether longer stimulus durations elicit longer song bouts, we next presented males with 4 minute videos consisting of both “responsive” and “unresponsive” segments. Each day, individual males were presented with either a video or a live female each day (<xref ref-type="fig" rid="F1">Fig. 1G</xref>) and trials were conducted for multiple days in a random order until we obtained atleast 6 song bouts in both conditions (see <xref ref-type="sec" rid="S2">Methods for details</xref>). Birds sang comparable number of song bouts to the live female and to the videos (<xref ref-type="fig" rid="F1">Fig. 1H, 1I</xref>, <xref ref-type="supplementary-material" rid="SD4">S1C</xref>). Song bouts to the live female were still longer (<xref ref-type="fig" rid="F1">Fig. 1J</xref>), but the difference in length of song bouts to videos and live females was smaller with longer video durations (<xref ref-type="supplementary-material" rid="SD4">Fig. S1C, S2C</xref>). This was largely a result of more singing after the first 30s (<xref ref-type="supplementary-material" rid="SD4">Fig. S1C</xref>). Songs directed to the 4 minute videos were also preceded by higher number of introductory notes compared to undirected songs (<xref ref-type="fig" rid="F1">Fig. 1K</xref>). Overall, these results suggest that video duration plays a role in determining song bout duration possibly by giving the bird more time to detect and respond to the presence of a female on the screen.</p></sec><sec id="S19"><title>Building an animation of a female zebra finch</title><p id="P23">To obtain greater control of the behaviour, we next constructed an animation of a female zebra finch. The animation was constructed in Blender by sculpting a 3D model of a female zebra finch, painting the skin with realistic colours and adding a feather texture (<xref ref-type="fig" rid="F2">Fig. 2A, 2B</xref>). Bones were rigged into the body to allow for puppeteer-like control of movement of specific body parts (<xref ref-type="fig" rid="F2">Fig. 2C</xref>; see <xref ref-type="sec" rid="S2">Methods for details</xref>). Finally, the bird was placed in a background of a cage (<xref ref-type="fig" rid="F2">Fig. 2D</xref>). We used this to construct a simple animation that involved the female hopping in, performing a few head turns and quivering its tail. This sequence was repeated multiple times to construct a 2 minute video that began with the image of an empty cage (<xref ref-type="fig" rid="F2">Fig. 2E – 2H</xref>; first 2 minutes of <xref ref-type="supplementary-material" rid="SD2">Movie 2</xref>; see <xref ref-type="sec" rid="S2">Methods for full details</xref>). This video was played with the 5s of female calls on appearance of the female (Call animation) or without audio (No call animation). As a control, we used a colour-modified version where the entire bird was painted with deep blue and the feathery texture removed (last 2 minutes of <xref ref-type="supplementary-material" rid="SD2">Movie 2</xref>). Everything else including all the movements were kept the same.</p></sec><sec id="S20"><title>Male zebra finches produced courtship songs directed at the animations</title><p id="P24">To test the ability of this animation to elicit song, we tested male birds across 3 sessions (1 session/day) using 5 stimuli presented in random order in each session. The five stimuli used were (1) a live female, (2) a video of a female, (3) video of the animated female zebra finch with and without audio (3 and 4) and the (5) colour-modified control animation without audio (<xref ref-type="fig" rid="F2">Fig. 2I</xref>; see <xref ref-type="sec" rid="S2">Methods for details</xref>). Stimulus duration was kept at 2 minutes. We selected birds based on response to a single 2 minute exposure to the animation without calls. 5/12 males sang to this single exposure and were chosen for the 3 sessions. Male birds sang to all stimuli with comparable numbers of song bouts (<xref ref-type="fig" rid="F2">Fig. 2J, 2K</xref>, <xref ref-type="supplementary-material" rid="SD4">S1B, S2B</xref>). Song bouts directed at the animation (<xref ref-type="supplementary-material" rid="SD3">Movie 3</xref>) were comparable in duration to song bouts directed at videos (<xref ref-type="supplementary-material" rid="SD4">Fig. S2D</xref>), but significantly shorter than song bouts directed at live females (<xref ref-type="fig" rid="F2">Fig. 2L</xref>). Songs directed at animations began with a large number of introductory notes confirming the “directed” nature of these songs (<xref ref-type="fig" rid="F2">Fig. 2M</xref>). Interestingly, 3/5 birds also sang to the colour-modified control animations (<xref ref-type="fig" rid="F2">Fig. 2J, 2K, 2L</xref>). These bouts were also preceded by a large number of introductory notes and were comparable in length to bouts directed towards the animation (<xref ref-type="fig" rid="F2">Fig. 2M</xref>). These results demonstrate the potential of using animations to elicit song and suggest that such animations could be used to further study courtship interactions in the zebra finch.</p></sec><sec id="S21"><title>Methods for closed loop control of the animation</title><p id="P25">To use these animations to study courtship interactions, we needed methods to change these animations in real-time, i.e. we needed a closed-loop system. Here, we used two different approaches to produce interactive animations; (1) using an Arduino to control the appearance of a video and (2) using the animation in conjunction with a gaming engine.</p><p id="P26">In the first approach, we used an Arduino (a single-board microcontroller) based approach to control the appearance of a video for tutoring young birds. Since young birds learn better from a live tutor than from song playback from a speaker (<xref ref-type="bibr" rid="R4">Derégnaucourt et al., 2013</xref>), we used active tutoring methods to test whether including a video of a male bird along with song playback increased copying accuracy. Juveniles could hop on a perch to elicit song playback and video playback of a male bird (<xref ref-type="fig" rid="F3">Fig. 3A</xref>; see <xref ref-type="sec" rid="S2">Methods for details of procedure</xref>). Juveniles tutored by this method (n=6) copied tutor songs with accuracies similar to juveniles tutored by song playback alone (<xref ref-type="fig" rid="F3">Fig. 3B</xref> shows spectrograms of tutor song and songs of two juveniles; <xref ref-type="fig" rid="F3">Fig. 3C</xref>; n=6 birds with video and n=10 birds with only audio playbacks from Kalra et al. (<xref ref-type="bibr" rid="R10">Kalra et al., 2021</xref>)). These results suggested that the visual presence of an adult bird, albeit on a screen, does not improve copying accuracy and is supported by similar results from a recent study (<xref ref-type="bibr" rid="R36">Varkevisser et al., 2022a</xref>).</p><p id="P27">While, the Arduino based approach was simple and easy to use, real-time control of the animation was not possible, as latencies from perch hop to video playback were long (mean +/- std. was 0.74s +/-0.16s). In our second approach, we used a video game engine, Unreal Engine 4 to construct interactive animations. We imported the female zebra finch from Blender into Unreal Engine 4 and created a 1D blend space, i.e. an axis that allowed us to gradually blend a stationary animated female zebra finch into a tail-quivering zebra finch (<xref ref-type="fig" rid="F3">Fig. 3D</xref>, <xref ref-type="supplementary-material" rid="SD4">S3</xref>; see <xref ref-type="sec" rid="S2">Methods</xref> for details). Different speeds of movement along the axis were connected to different durations of a “key press” on a regular keyboard. While, we connected “key-presses” to different states of the animation, it should be possible to use input from a microphone to control movement of the animation allowing for song-contingent control. Such methods have been used to make birds change the fundamental frequency of their vocalizations (<xref ref-type="bibr" rid="R32">Tumer and Brainard, 2007</xref>). Overall, these two methods provide the potential for generating closed-loop, interactive, animations for studying courtship dynamics.</p></sec></sec><sec id="S22" sec-type="discussion"><title>Discussion</title><p id="P28">Here, we demonstrated potential use of animations to study courtship interactions in a songbird, the zebra finch. Specifically, we first showed that male zebra finches sang to videos of female zebra finches and song bouts were longer in response to longer videos. Second, we built an animation of a female zebra finch and showed that 42% of males sang to the animation. Finally, we showed the potential for real-time control with tools for interactive control of the animation; a feature that is essential for analysing courtship interactions in real-time. Overall, our results highlight the power of animations for studying courtship interactions in songbirds and provide a new tool for the same.</p><sec id="S23"><title>Duration of the video is a factor that influences the amount of song produced</title><p id="P29">Artificial stimuli like animations, need to capture important details of the natural stimulus to be effective. What aspects of a video/animation stimulus are important for eliciting song from a male zebra finch? Our results show that stimulus duration is an important factor to consider. Male birds sang longer song bouts when presented with 4 minute videos as compared to 30s videos of females (<xref ref-type="fig" rid="F1">Fig. 1</xref>). Both in earlier studies (<xref ref-type="bibr" rid="R9">James et al., 2019</xref>) and our experiments, videos were played without sounds. Live females call and so, when presented with a live female, male birds would experience both visual and auditory stimuli. Such multimodality is known to be essential for learning from a tutor (<xref ref-type="bibr" rid="R37">Varkevisser et al., 2022b</xref>) and multimodal signals are common in the courtship displays of other songbird species (<xref ref-type="bibr" rid="R17">Mitoyen et al., 2019</xref>; <xref ref-type="bibr" rid="R18">Ota et al., 2015</xref>). Given the absence of auditory cues in the videos, longer duration videos may give birds more time to notice the video and decide to sing. Integration of auditory cues may increase responses to videos. In line with this, a recent study also used audio and visual cues to successfully allow two birds to interact with each other virtually (<xref ref-type="bibr" rid="R12">Larsen et al., 2022</xref>).</p></sec><sec id="S24"><title>Improving responses to animations</title><p id="P30">Our animation successfully elicited songs from only 42% of the birds tested. What can be done to make more birds respond to animations? One possibility is to make animations more realistic by incorporating realistic movement parameters by using pose-estimation software like DeepLabCut to track, quantify movements of live females (<xref ref-type="bibr" rid="R13">Lauer et al., 2022</xref>; <xref ref-type="bibr" rid="R16">Mathis et al., 2018</xref>). Another possibility is to use colour corrections based on the bird's visual system. Such colour-corrected videos have been shown to increase the attention of juvenile birds to videos of tutors (<xref ref-type="bibr" rid="R37">Varkevisser et al., 2022b</xref>). In our own experiments, the response of some birds to colour control animations suggests the possibility that the colour scale was not realistic enough.</p></sec><sec id="S25"><title>Outlook for using animations for closed-loop studies of social interactions</title><p id="P31">Our animations and the use of these animations in conjunction with video game engines (like Unreal Engine) provide a novel way to study social interactions in birds. Recent studies have used a robotic zebra finch for studying social interactions between juvenile birds and a tutor (the robotic bird) (<xref ref-type="bibr" rid="R1">Araguas et al., 2022</xref>). While a robot provides physical interactions that animations cannot provide, animations can be built and manipulated more easily. Animations, like the one we have developed would provide a complementary approach to robotic zebra finches to study social interactions using virtual-reality setups. A recent study used a virtual reality arrangement to show that two zebra finches can successfully interact with each other virtually (<xref ref-type="bibr" rid="R12">Larsen et al., 2022</xref>). Our animations and video game engines, combined with this setup could facilitate testing and analyses of social interactions in much greater detail.</p></sec><sec id="S26"><title>Broader applications of animated animals</title><p id="P32">Here, we focused on developing and controlling an animation for studying courtship interactions in zebra finches. Similar animations with other animal species can be useful for a diverse range of socio-cognitive and behavioural experiments. For instance, such animations could be used in social learning experiments to study the process of social learning. Such experiments typically require one animal in a group to learn a novel task asocially, following which social transmission of this learning can be studied (<xref ref-type="bibr" rid="R7">Heyes, 1994</xref>; <xref ref-type="bibr" rid="R24">Rendell et al., 2010</xref>; <xref ref-type="bibr" rid="R35">Van Schaik and Burkart, 2011</xref>). However, such asocial learning can take time. This can be speeded up using videos to teach the first animal a novel task. Animations could also be used to study interspecies encounters and animals’ responses towards heterospecifics (<xref ref-type="bibr" rid="R6">Gröning and Hochkirch, 2008</xref>; <xref ref-type="bibr" rid="R20">Peiman and Robinson, 2010</xref>; <xref ref-type="bibr" rid="R26">Sridhar and Guttal, 2018</xref>). In some species, response to heterospecifics is plastic and is observed to vary with heterospecific individuals (<xref ref-type="bibr" rid="R14">Lehtonen et al., 2010</xref>; <xref ref-type="bibr" rid="R15">Magellan, 2020</xref>). However whether physical features (size, body shape, etc.) or behavioural features (aggression, submission, etc.) are used by animals to identify heterospecifics remains unclear. Animations, such as ours, offer independent control over physical and behavioural features and would understand interspecies interactions. Overall, our methods will not only aid the study of animal courtship interactions but will also be extendable to other fields from animal cognition to ecology.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Movie 1</label><media xlink:href="EMS156703-supplement-Movie_1.mp4" mimetype="video" mime-subtype="mp4" id="d25aAdEbB" position="anchor"/></supplementary-material><supplementary-material content-type="local-data" id="SD2"><label>Movie 2</label><media xlink:href="EMS156703-supplement-Movie_2.mp4" mimetype="video" mime-subtype="mp4" id="d25aAdEcB" position="anchor"/></supplementary-material><supplementary-material content-type="local-data" id="SD3"><label>Movie 3</label><media xlink:href="EMS156703-supplement-Movie_3.mp4" mimetype="video" mime-subtype="mp4" id="d25aAdEdB" position="anchor"/></supplementary-material><supplementary-material content-type="local-data" id="SD4"><label>Supplementary Material</label><media xlink:href="EMS156703-supplement-Supplementary_Material.pdf" mimetype="application" mime-subtype="pdf" id="d25aAdEeB" position="anchor"/></supplementary-material></sec></body><back><ack id="S27"><title>Acknowledgments</title><p>We would like to thank Prakash Raut for bird colony maintenance and Vrushali Rao Gumnur and Aboli Ektare for preliminary tests with different monitors and video stimuli. We would like to thank Hamish Mehaffey, Mimi Kao and Girish Deshpande for helpful comments on the manuscript and the Rajan lab for useful discussion related to the project.</p><sec id="S28"><title>Funding</title><p>This work was supported by grants from the Department of Science and Technology, India (DST/CSRI/2017/163), the Department of Biotechnology (DBT), India, Ramalingaswami Fellowship (BT/HRD/35/02/2006), the Science and Engineering Research Board, India (SERB EMR/2015/000829), the DBT/Wellcome Trust India Alliance Senior Fellowship (IA/S/21/1/505621) and IISER Pune to RR. SK is a graduate student supported by funding from IISER Pune and a senior research fellowship from CSIR, India (9/936(0159)/2016 EMR-1). NP was a recipient of the INSPIRE Scholarship for Higher Education.</p></sec></ack><fn-group><fn id="FN1" fn-type="conflict"><p id="P33"><bold>Competing Interests</bold></p><p id="P34">We declare we have no competing interests.</p></fn><fn id="FN2" fn-type="con"><p id="P35"><bold>Author Contributions</bold></p><p id="P36">NP and RR designed the study. NP did the experiments with videos and created the animated female zebra finch in Blender. SJ carried out the animation related experiments. NR did some initial work on interactive animations using Unity and PT used Unreal engine 4 to construct interactive animations. SK, AP and SR constructed the tutoring setup and tutored juveniles with video and audio playbacks. NP and RR wrote the manuscript in consultation with SJ, PT, SK, AP, SR and NR.</p></fn><fn id="FN3"><p id="P37"><bold>Data Accessibility</bold></p><p id="P38">All data, videos, animations and files related to animation are available on reasonable request from the corresponding author (RR – <email>raghav@iiserpune.ac.in</email>).</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Araguas</surname><given-names>A</given-names></name><name><surname>Guellaï</surname><given-names>B</given-names></name><name><surname>Gauthier</surname><given-names>P</given-names></name><name><surname>Richer</surname><given-names>F</given-names></name><name><surname>Montone</surname><given-names>G</given-names></name><name><surname>Chopin</surname><given-names>A</given-names></name><name><surname>Derégnaucourt</surname><given-names>S</given-names></name></person-group><article-title>Design of a robotic zebra finch for experimental studies on developmental song learning</article-title><source>Journal of Experimental Biology</source><year>2022</year><volume>225</volume><elocation-id>jeb242949</elocation-id><pub-id pub-id-type="pmid">35048975</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bischof</surname><given-names>H-J</given-names></name><name><surname>Böhner</surname><given-names>J</given-names></name><name><surname>Sossinka</surname><given-names>R</given-names></name></person-group><article-title>Influence of external stimuli on the quality of the song of the zebra finch (Taeniopygia guttata castanotis Gould)</article-title><source>Ethology</source><year>1981</year><volume>57</volume><fpage>261</fpage><lpage>267</lpage></element-citation></ref><ref id="R3"><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Bradbury</surname><given-names>JW</given-names></name><name><surname>Vehrencamp</surname><given-names>SL</given-names></name></person-group><source>Principles of animal communication</source><year>2011</year></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Derégnaucourt</surname><given-names>S</given-names></name><name><surname>Poirier</surname><given-names>C</given-names></name><name><surname>Van der Kant</surname><given-names>A</given-names></name><name><surname>Van der Linden</surname><given-names>A</given-names></name><name><surname>Gahr</surname><given-names>M</given-names></name></person-group><article-title>Comparisons of different methods to train a young zebra finch (Taeniopygia guttata) to learn a song</article-title><source>Journal of Physiology-Paris</source><year>2013</year><volume>107</volume><fpage>210</fpage><lpage>218</lpage><pub-id pub-id-type="pmid">22982543</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Galoch</surname><given-names>Z</given-names></name><name><surname>Bischof</surname><given-names>H-J</given-names></name></person-group><article-title>Behavioural responses to video playbacks by zebra finch males</article-title><source>Behavioural Processes</source><year>2007</year><volume>74</volume><fpage>21</fpage><lpage>26</lpage><pub-id pub-id-type="pmid">17045422</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gröning</surname><given-names>J</given-names></name><name><surname>Hochkirch</surname><given-names>A</given-names></name></person-group><article-title>Reproductive interference between animal species</article-title><source>The Quarterly review of biology</source><year>2008</year><volume>83</volume><fpage>257</fpage><lpage>282</lpage><pub-id pub-id-type="pmid">18792662</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heyes</surname><given-names>CM</given-names></name></person-group><article-title>Social learning in animals: categories and mechanisms</article-title><source>Biological Reviews</source><year>1994</year><volume>69</volume><fpage>207</fpage><lpage>231</lpage><pub-id pub-id-type="pmid">8054445</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ikebuchi</surname><given-names>M</given-names></name><name><surname>Okanoya</surname><given-names>K</given-names></name></person-group><article-title>Male zebra finches and Bengalese finches emit directed songs to the video images of conspecific females projected onto a TFT display</article-title><source>Zoological Science</source><year>1999</year><volume>16</volume><fpage>63</fpage><lpage>70</lpage></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>James</surname><given-names>LS</given-names></name><name><surname>Fan</surname><given-names>R</given-names></name><name><surname>Sakata</surname><given-names>JT</given-names></name></person-group><article-title>Behavioural responses to video and live presentations of females reveal a dissociation between performance and motivational aspects of birdsong</article-title><source>Journal of Experimental Biology</source><year>2019</year><volume>222</volume><elocation-id>jeb206318</elocation-id><pub-id pub-id-type="pmid">31331939</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalra</surname><given-names>S</given-names></name><name><surname>Yawatkar</surname><given-names>V</given-names></name><name><surname>James</surname><given-names>LS</given-names></name><name><surname>Sakata</surname><given-names>JT</given-names></name><name><surname>Rajan</surname><given-names>R</given-names></name></person-group><article-title>Introductory gestures before songbird vocal displays are shaped by learning and biological predispositions</article-title><source>Proc Biol Sci</source><year>2021</year><volume>288</volume><elocation-id>20202796</elocation-id><pub-id pub-id-type="pmcid">PMC7893256</pub-id><pub-id pub-id-type="pmid">33468007</pub-id><pub-id pub-id-type="doi">10.1098/rspb.2020.2796</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kao</surname><given-names>MH</given-names></name><name><surname>Doupe</surname><given-names>AJ</given-names></name><name><surname>Brainard</surname><given-names>MS</given-names></name></person-group><article-title>Contributions of an avian basal ganglia-forebrain circuit to real-time modulation of song</article-title><source>Nature</source><year>2005</year><volume>433</volume><fpage>638</fpage><lpage>643</lpage><pub-id pub-id-type="pmid">15703748</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Larsen</surname><given-names>LB</given-names></name><name><surname>Adam</surname><given-names>I</given-names></name><name><surname>Berman</surname><given-names>GJ</given-names></name><name><surname>Hallam</surname><given-names>J</given-names></name><name><surname>Elemans</surname><given-names>CPH</given-names></name></person-group><article-title>Driving singing behaviour in songbirds using a multi-modal, multi-agent virtual environment</article-title><source>Sci Rep</source><year>2022</year><volume>12</volume><elocation-id>13414</elocation-id><pub-id pub-id-type="pmcid">PMC9352672</pub-id><pub-id pub-id-type="pmid">35927295</pub-id><pub-id pub-id-type="doi">10.1038/s41598-022-16456-0</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lauer</surname><given-names>J</given-names></name><name><surname>Zhou</surname><given-names>M</given-names></name><name><surname>Ye</surname><given-names>S</given-names></name><name><surname>Menegas</surname><given-names>W</given-names></name><name><surname>Schneider</surname><given-names>S</given-names></name><name><surname>Nath</surname><given-names>T</given-names></name><name><surname>Rahman</surname><given-names>MM</given-names></name><name><surname>Di Santo</surname><given-names>V</given-names></name><name><surname>Soberanes</surname><given-names>D</given-names></name><name><surname>Feng</surname><given-names>G</given-names></name><etal/></person-group><article-title>Multi-animal pose estimation, identification and tracking with DeepLabCut</article-title><source>Nat Methods</source><year>2022</year><volume>19</volume><fpage>496</fpage><lpage>504</lpage><pub-id pub-id-type="pmcid">PMC9007739</pub-id><pub-id pub-id-type="pmid">35414125</pub-id><pub-id pub-id-type="doi">10.1038/s41592-022-01443-0</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lehtonen</surname><given-names>TK</given-names></name><name><surname>McCrary</surname><given-names>JK</given-names></name><name><surname>Meyer</surname><given-names>A</given-names></name></person-group><article-title>Territorial aggression can be sensitive to the status of heterospecific intruders</article-title><source>Behavioural Processes</source><year>2010</year><volume>84</volume><fpage>598</fpage><lpage>601</lpage><pub-id pub-id-type="pmid">20206674</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Magellan</surname><given-names>K</given-names></name></person-group><article-title>Behavioural crypsis in a South African galaxiid fish induced by predatory and non-predatory heterospecifics</article-title><source>Journal of Fish Biology</source><year>2020</year><volume>96</volume><fpage>1278</fpage><lpage>1283</lpage><pub-id pub-id-type="pmid">31429067</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname><given-names>A</given-names></name><name><surname>Mamidanna</surname><given-names>P</given-names></name><name><surname>Cury</surname><given-names>KM</given-names></name><name><surname>Abe</surname><given-names>T</given-names></name><name><surname>Murthy</surname><given-names>VN</given-names></name><name><surname>Mathis</surname><given-names>MW</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name></person-group><article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title><source>Nat Neurosci</source><year>2018</year><volume>21</volume><fpage>1281</fpage><lpage>1289</lpage><pub-id pub-id-type="pmid">30127430</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mitoyen</surname><given-names>C</given-names></name><name><surname>Quigley</surname><given-names>C</given-names></name><name><surname>Fusani</surname><given-names>L</given-names></name></person-group><article-title>Evolution and function of multimodal courtship displays</article-title><source>Ethology</source><year>2019</year><volume>125</volume><fpage>503</fpage><lpage>515</lpage><pub-id pub-id-type="pmcid">PMC6618153</pub-id><pub-id pub-id-type="pmid">31341343</pub-id><pub-id pub-id-type="doi">10.1111/eth.12882</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ota</surname><given-names>N</given-names></name><name><surname>Gahr</surname><given-names>M</given-names></name><name><surname>Soma</surname><given-names>M</given-names></name></person-group><article-title>Tap dancing birds: the multimodal mutual courtship display of males and females in a socially monogamous songbird</article-title><source>Sci Rep</source><year>2015</year><volume>5</volume><elocation-id>16614</elocation-id><pub-id pub-id-type="pmcid">PMC4994120</pub-id><pub-id pub-id-type="pmid">26583485</pub-id><pub-id pub-id-type="doi">10.1038/srep16614</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patricelli</surname><given-names>GL</given-names></name><name><surname>Coleman</surname><given-names>SW</given-names></name><name><surname>Borgia</surname><given-names>G</given-names></name></person-group><article-title>Male satin bowerbirds, Ptilonorhynchus violaceus, adjust their display intensity in response to female startling: an experiment with robotic females</article-title><source>Animal Behaviour</source><year>2006</year><volume>71</volume><fpage>49</fpage><lpage>59</lpage></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peiman</surname><given-names>K</given-names></name><name><surname>Robinson</surname><given-names>B</given-names></name></person-group><article-title>Ecology and evolution of resource-related heterospecific aggression</article-title><source>The Quarterly Review of Biology</source><year>2010</year><volume>85</volume><fpage>133</fpage><lpage>158</lpage><pub-id pub-id-type="pmid">20565037</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajan</surname><given-names>R</given-names></name></person-group><article-title>Pre-bout neural activity changes in premotor nucleus HVC correlate with successful initiation of learned song sequence</article-title><source>Journal of Neuroscience</source><year>2018</year><volume>38</volume><fpage>5925</fpage><lpage>5938</lpage><pub-id pub-id-type="pmcid">PMC6021991</pub-id><pub-id pub-id-type="pmid">29853628</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3003-17.2018</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajan</surname><given-names>R</given-names></name><name><surname>Doupe</surname><given-names>AJ</given-names></name></person-group><article-title>Behavioral and neural signatures of readiness to initiate a learned motor sequence</article-title><source>Curr Biol</source><year>2013</year><volume>23</volume><fpage>87</fpage><lpage>93</lpage><pub-id pub-id-type="pmcid">PMC3683078</pub-id><pub-id pub-id-type="pmid">23246408</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2012.11.040</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>D</given-names></name><name><surname>Kojima</surname><given-names>S</given-names></name><name><surname>Rajan</surname><given-names>R</given-names></name></person-group><article-title>Sensory feedback independent pre-song vocalizations correlate with time to song initiation</article-title><source>J Exp Biol</source><year>2019</year><volume>222</volume><pub-id pub-id-type="pmcid">PMC6467462</pub-id><pub-id pub-id-type="pmid">30877225</pub-id><pub-id pub-id-type="doi">10.1242/jeb.199042</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rendell</surname><given-names>L</given-names></name><name><surname>Boyd</surname><given-names>R</given-names></name><name><surname>Cownden</surname><given-names>D</given-names></name><name><surname>Enquist</surname><given-names>M</given-names></name><name><surname>Eriksson</surname><given-names>K</given-names></name><name><surname>Feldman</surname><given-names>MW</given-names></name><name><surname>Fogarty</surname><given-names>L</given-names></name><name><surname>Ghirlanda</surname><given-names>S</given-names></name><name><surname>Lillicrap</surname><given-names>T</given-names></name><name><surname>Laland</surname><given-names>KN</given-names></name></person-group><article-title>Why copy others? Insights from the social learning strategies tournament</article-title><source>Science</source><year>2010</year><volume>328</volume><fpage>208</fpage><lpage>213</lpage><pub-id pub-id-type="pmcid">PMC2989663</pub-id><pub-id pub-id-type="pmid">20378813</pub-id><pub-id pub-id-type="doi">10.1126/science.1184719</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sossinka</surname><given-names>R</given-names></name><name><surname>Böhner</surname><given-names>J</given-names></name></person-group><article-title>Song Types in the Zebra Finch Poephila guttata castanotis1</article-title><source>Zeitschrift für Tierpsychologie</source><year>1980</year><volume>53</volume><fpage>123</fpage><lpage>132</lpage></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sridhar</surname><given-names>H</given-names></name><name><surname>Guttal</surname><given-names>V</given-names></name></person-group><article-title>Friendship across species borders: factors that facilitate and constrain heterospecific sociality</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><year>2018</year><volume>373</volume><elocation-id>20170014</elocation-id><pub-id pub-id-type="pmcid">PMC5882984</pub-id><pub-id pub-id-type="pmid">29581399</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2017.0014</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stowers</surname><given-names>JR</given-names></name><name><surname>Hofbauer</surname><given-names>M</given-names></name><name><surname>Bastien</surname><given-names>R</given-names></name><name><surname>Griessner</surname><given-names>J</given-names></name><name><surname>Higgins</surname><given-names>P</given-names></name><name><surname>Farooqui</surname><given-names>S</given-names></name><name><surname>Fischer</surname><given-names>RM</given-names></name><name><surname>Nowikovsky</surname><given-names>K</given-names></name><name><surname>Haubensak</surname><given-names>W</given-names></name><name><surname>Couzin</surname><given-names>ID</given-names></name></person-group><article-title>Virtual reality for freely moving animals</article-title><source>Nature methods</source><year>2017</year><volume>14</volume><fpage>995</fpage><lpage>1002</lpage><pub-id pub-id-type="pmcid">PMC6485657</pub-id><pub-id pub-id-type="pmid">28825703</pub-id><pub-id pub-id-type="doi">10.1038/nmeth.4399</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takahasi</surname><given-names>M</given-names></name><name><surname>Ikebuchi</surname><given-names>M</given-names></name><name><surname>Okanoya</surname><given-names>K</given-names></name></person-group><article-title>Spatiotemporal properties of visual stimuli for song induction in Bengalese finches</article-title><source>Neuroreport</source><year>2005</year><volume>16</volume><fpage>1339</fpage><lpage>1343</lpage><pub-id pub-id-type="pmid">16056136</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tchernichovski</surname><given-names>O</given-names></name><name><surname>Lints</surname><given-names>T</given-names></name><name><surname>Mitra</surname><given-names>PP</given-names></name><name><surname>Nottebohm</surname><given-names>F</given-names></name></person-group><article-title>Vocal imitation in zebra finches is inversely related to model abundance</article-title><source>Proceedings of the National Academy of Sciences</source><year>1999</year><volume>96</volume><fpage>12901</fpage><lpage>12904</lpage><pub-id pub-id-type="pmcid">PMC23154</pub-id><pub-id pub-id-type="pmid">10536020</pub-id><pub-id pub-id-type="doi">10.1073/pnas.96.22.12901</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tchernichovski</surname></name><name><surname>Nottebohm</surname></name><name><surname>Ho</surname></name><name><surname>Pesaran</surname></name><name><surname>Mitra</surname></name></person-group><article-title>A procedure for an automated measurement of song similarity</article-title><source>Anim Behav</source><year>2000</year><volume>59</volume><fpage>1167</fpage><lpage>1176</lpage><pub-id pub-id-type="pmid">10877896</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tinbergen</surname><given-names>N</given-names></name></person-group><article-title>Social releasers and the experimental method required for their study</article-title><source>The Wilson Bulletin</source><year>1948</year><fpage>6</fpage><lpage>51</lpage></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tumer</surname><given-names>EC</given-names></name><name><surname>Brainard</surname><given-names>MS</given-names></name></person-group><article-title>Performance variability enables adaptive plasticity of “crystallized” adult birdsong</article-title><source>Nature</source><year>2007</year><volume>450</volume><fpage>1240</fpage><lpage>1244</lpage><pub-id pub-id-type="pmid">18097411</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ullrich</surname><given-names>R</given-names></name><name><surname>Norton</surname><given-names>P</given-names></name><name><surname>Scharff</surname><given-names>C</given-names></name></person-group><article-title>Waltzing Taeniopygia: integration of courtship song and dance in the domesticated Australian zebra finch</article-title><source>Animal Behaviour</source><year>2016</year><volume>112</volume><fpage>285</fpage><lpage>300</lpage></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Dyk</surname><given-names>DA</given-names></name><name><surname>Evans</surname><given-names>CS</given-names></name></person-group><article-title>Opponent assessment in lizards: examining the effect of aggressive and submissive signals</article-title><source>Behavioral Ecology</source><year>2008</year><volume>19</volume><fpage>895</fpage><lpage>901</lpage></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Schaik</surname><given-names>CP</given-names></name><name><surname>Burkart</surname><given-names>JM</given-names></name></person-group><article-title>Social learning and evolution: the cultural intelligence hypothesis</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><year>2011</year><volume>366</volume><fpage>1008</fpage><lpage>1016</lpage><pub-id pub-id-type="pmcid">PMC3049085</pub-id><pub-id pub-id-type="pmid">21357223</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2010.0304</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Varkevisser</surname><given-names>JM</given-names></name><name><surname>Simon</surname><given-names>R</given-names></name><name><surname>Mendoza</surname><given-names>E</given-names></name><name><surname>How</surname><given-names>M</given-names></name><name><surname>van Hijlkema</surname><given-names>I</given-names></name><name><surname>Jin</surname><given-names>R</given-names></name><name><surname>Liang</surname><given-names>Q</given-names></name><name><surname>Scharff</surname><given-names>C</given-names></name><name><surname>Halfwerk</surname><given-names>WH</given-names></name><name><surname>Riebel</surname><given-names>K</given-names></name></person-group><article-title>Adding colour-realistic video images to audio playbacks increases stimulus engagement but does not enhance vocal learning in zebra finches</article-title><source>Animal Cognition</source><year>2022a</year><volume>25</volume><fpage>249</fpage><lpage>274</lpage><pub-id pub-id-type="pmcid">PMC8940817</pub-id><pub-id pub-id-type="pmid">34405288</pub-id><pub-id pub-id-type="doi">10.1007/s10071-021-01547-8</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Varkevisser</surname><given-names>JM</given-names></name><name><surname>Mendoza</surname><given-names>E</given-names></name><name><surname>Simon</surname><given-names>R</given-names></name><name><surname>Manet</surname><given-names>M</given-names></name><name><surname>Halfwerk</surname><given-names>W</given-names></name><name><surname>Scharff</surname><given-names>C</given-names></name><name><surname>Riebel</surname><given-names>K</given-names></name></person-group><article-title>Multimodality during live tutoring is relevant for vocal learning in zebra finches</article-title><source>Animal Behaviour</source><year>2022b</year><volume>187</volume><fpage>263</fpage><lpage>280</lpage></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woo</surname><given-names>KL</given-names></name><name><surname>Rieucau</surname><given-names>G</given-names></name></person-group><article-title>From dummies to animations: a review of computer-animated stimuli used in animal behavior studies</article-title><source>Behav Ecol Sociobiol</source><year>2011</year><volume>65</volume><fpage>1671</fpage><lpage>1685</lpage></element-citation></ref><ref id="R39"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zann</surname><given-names>RA</given-names></name></person-group><source>Zebra Finch</source><publisher-loc>Oxford</publisher-loc><year>1996</year></element-citation></ref></ref-list></back><floats-group><boxed-text id="BX1" position="float" orientation="portrait"><caption><title>Summary Statement</title></caption><p>We develop and test an animation of a female zebra finch to study song and courtship interactions in the male zebra finch.</p></boxed-text><fig id="F1" position="float"><label>Figure 1</label><caption><title>Male zebra finches sing longer song bouts to 4 minute videos as compared to 30s videos</title><p>A) Schematic depicting the way in which videos of females were recorded. The male is on the left side and the female is on the right side, separated by a glass partition. A camera was placed near the male to record videos of the female.</p><p>(B) Order of 30s trials in a session for one representative bird. LF represents “live female”, RV represents “responsive” video and UV represents “unresponsive” video. Inter-trial-time of 5 minutes was used.</p><p>(C) Example spectrograms showing songs produced by one male for the 3 different kinds of stimuli and undirected singing (UD) when the bird was alone.</p><p>(D), (E), (F) mean number of song bouts per session (D), mean song bout duration (E) and mean number of introductory notes (INs) (F) for all birds for the different trial types. Grey circles represent data for individual birds and grey lines connect data from the same bird. Black circles and whiskers represent mean and s.e.m across all birds (n=9 birds).</p><p>(G) Schematic depicting the experimental protocol for long duration experimental trials (4 minute trials). LF represents live female and VP represents video presentation.</p><p>(H) Example spectrograms showing the songs produced by one male for the 2 different kinds of stimuli and undirected singing (UD) when the bird was alone.</p><p>(I), (J), (K) mean number of song bouts per session (I), mean song bout duration (J) and mean number of introductory notes (INs) (K) for all birds for the different trial types. Grey circles represent data for single birds and grey lines connect data from the same bird. Black circles and whiskers represent mean and s.e.m across all birds (n=7 birds).</p><p>* represents p &lt; 0.05, ** - p &lt; 0.01, *** - p &lt; 0.001 - Type II Wald Chi Square test followed by pairwise comparisons using estimated marginal means with Holm correction.</p></caption><graphic xlink:href="EMS156703-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Construction and validation of an animated female zebra finch</title><p>(A), (B), (C), (D) Different stages in the construction of an animated female zebra finch in Blender; 3D model (A), with colour and feather texture (B). Positions of control bones with yellow representing structural bones, grey representing core bones, blue representing control bones, deep blue representing the master control bone and red representing the knee stop bones (C) (see <xref ref-type="sec" rid="S2">Methods for details</xref>) and final image with empty cage (D).</p><p>(E), (F), (G), (H) represent different animation frames with the female hopping into view, looking to either side and quivering its tail.</p><p>I) Order of 2 minute experimental trials across 3 sessions for one representative bird. LF represents live female, CA represents call with animation, NCA represents no call animation, VP represents video presentation and CC represents colour control animation. Inter-trial-time of 5 minutes was used.</p><p>(J) Example spectrograms showing the songs produced by one male for the 5 different kinds of stimuli.</p><p>(K), (L), (M) mean number of song bouts per session (K), mean song bout duration (L) and mean number of introductory notes (INs) (M) are shown for all birds for the different trial types. Grey circles represent data for individual birds and grey lines connect data from the same bird. Black circles and whiskers represent mean and s.e.m across all birds (n=5 birds).</p><p>* represents p &lt; 0.05, ** - p &lt; 0.01, *** - p &lt; 0.001 - Type II Wald Chi Square test followed by pairwise comparisons using estimated marginal means with Holm correction.</p></caption><graphic xlink:href="EMS156703-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Potential methods for interactive control of the animation</title><p>(A) Schematic of experimental setup and Arduino based approach to control song audio and video playback based on perch hopping behaviour of a juvenile zebra finch. Briefly, hopping on a perch triggers playback of song audio and video through an Arduino-MATLAB connection.</p><p>(B) Example spectrograms of tutor song and the songs of two juveniles tutored using this paradigm.</p><p>(C) Boxplots showing the similarity to tutor song for juveniles tutored using only audio playbacks (subset of birds from an earlier study (<xref ref-type="bibr" rid="R10">Kalra et al., 2021</xref>)) and audio-visual playbacks. Circles represent data from individual birds.</p><p>(D) A view of the 1D blendspace in Unreal Engine 4 with the axis extremes representing “An idle animation” and “Tail quivering animation”. Different positions along the axis represent different speeds of the tail quivering.</p></caption><graphic xlink:href="EMS156703-f003"/></fig></floats-group></article>