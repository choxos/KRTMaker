<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS158985</article-id><article-id pub-id-type="doi">10.1101/2022.12.22.521541</article-id><article-id pub-id-type="archive">PPR588444</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">3</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>An unsupervised map of excitatory neurons’ dendritic morphology in the mouse visual cortex</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Weis</surname><given-names>Marissa A.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Papadopoulos</surname><given-names>Stelios</given-names></name><xref ref-type="aff" rid="A4">4</xref><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Hansel</surname><given-names>Laura</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Lüddecke</surname><given-names>Timo</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Celii</surname><given-names>Brendan</given-names></name><xref ref-type="aff" rid="A4">4</xref><xref ref-type="aff" rid="A5">5</xref><xref ref-type="aff" rid="A10">10</xref></contrib><contrib contrib-type="author"><name><surname>Fahey</surname><given-names>Paul G.</given-names></name><xref ref-type="aff" rid="A4">4</xref><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Eric Y.</given-names></name><xref ref-type="aff" rid="A4">4</xref><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Bae</surname><given-names>J. Alexander</given-names></name><xref ref-type="aff" rid="A6">6</xref><xref ref-type="aff" rid="A8">8</xref></contrib><contrib contrib-type="author"><name><surname>Bodor</surname><given-names>Agnes L.</given-names></name><xref ref-type="aff" rid="A9">9</xref></contrib><contrib contrib-type="author"><name><surname>Brittain</surname><given-names>Derrick</given-names></name><xref ref-type="aff" rid="A9">9</xref></contrib><contrib contrib-type="author"><name><surname>Buchanan</surname><given-names>JoAnn</given-names></name><xref ref-type="aff" rid="A9">9</xref></contrib><contrib contrib-type="author"><name><surname>Bumbarger</surname><given-names>Daniel J.</given-names></name><xref ref-type="aff" rid="A9">9</xref></contrib><contrib contrib-type="author"><name><surname>Castro</surname><given-names>Manuel A.</given-names></name><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Collman</surname><given-names>Forrest</given-names></name><xref ref-type="aff" rid="A9">9</xref></contrib><contrib contrib-type="author"><name><surname>da Costa</surname><given-names>Nuno Maҫarico</given-names></name><xref ref-type="aff" rid="A9">9</xref></contrib><contrib contrib-type="author"><name><surname>Dorkenwald</surname><given-names>Sven</given-names></name><xref ref-type="aff" rid="A6">6</xref><xref ref-type="aff" rid="A7">7</xref></contrib><contrib contrib-type="author"><name><surname>Elabbady</surname><given-names>Leila</given-names></name><xref ref-type="aff" rid="A9">9</xref></contrib><contrib contrib-type="author"><name><surname>Halageri</surname><given-names>Akhilesh</given-names></name><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Jia</surname><given-names>Zhen</given-names></name><xref ref-type="aff" rid="A6">6</xref><xref ref-type="aff" rid="A7">7</xref></contrib><contrib contrib-type="author"><name><surname>Jordan</surname><given-names>Chris</given-names></name><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Kapner</surname><given-names>Dan</given-names></name><xref ref-type="aff" rid="A9">9</xref></contrib><contrib contrib-type="author"><name><surname>Kemnitz</surname><given-names>Nico</given-names></name><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Kinn</surname><given-names>Sam</given-names></name><xref ref-type="aff" rid="A9">9</xref></contrib><contrib contrib-type="author"><name><surname>Lee</surname><given-names>Kisuk</given-names></name><xref ref-type="aff" rid="A6">6</xref><xref ref-type="aff" rid="A11">11</xref></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Kai</given-names></name><xref ref-type="aff" rid="A6">6</xref><xref ref-type="aff" rid="A7">7</xref></contrib><contrib contrib-type="author"><name><surname>Lu</surname><given-names>Ran</given-names></name><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Macrina</surname><given-names>Thomas</given-names></name><xref ref-type="aff" rid="A6">6</xref><xref ref-type="aff" rid="A7">7</xref></contrib><contrib contrib-type="author"><name><surname>Mahalingam</surname><given-names>Gayathri</given-names></name><xref ref-type="aff" rid="A9">9</xref></contrib><contrib contrib-type="author"><name><surname>Mitchell</surname><given-names>Eric</given-names></name><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Mondal</surname><given-names>Shanka Subhra</given-names></name><xref ref-type="aff" rid="A6">6</xref><xref ref-type="aff" rid="A8">8</xref></contrib><contrib contrib-type="author"><name><surname>Mu</surname><given-names>Shang</given-names></name><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Nehoran</surname><given-names>Barak</given-names></name><xref ref-type="aff" rid="A6">6</xref><xref ref-type="aff" rid="A7">7</xref></contrib><contrib contrib-type="author"><name><surname>Popovych</surname><given-names>Sergiy</given-names></name><xref ref-type="aff" rid="A6">6</xref><xref ref-type="aff" rid="A7">7</xref></contrib><contrib contrib-type="author"><name><surname>Reid</surname><given-names>R. Clay</given-names></name><xref ref-type="aff" rid="A9">9</xref></contrib><contrib contrib-type="author"><name><surname>Schneider-Mizell</surname><given-names>Casey M.</given-names></name><xref ref-type="aff" rid="A9">9</xref></contrib><contrib contrib-type="author"><name><surname>Seung</surname><given-names>H. Sebastian</given-names></name><xref ref-type="aff" rid="A6">6</xref><xref ref-type="aff" rid="A7">7</xref></contrib><contrib contrib-type="author"><name><surname>Silversmith</surname><given-names>William</given-names></name><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Takeno</surname><given-names>Marc</given-names></name><xref ref-type="aff" rid="A9">9</xref></contrib><contrib contrib-type="author"><name><surname>Torres</surname><given-names>Russel</given-names></name><xref ref-type="aff" rid="A9">9</xref></contrib><contrib contrib-type="author"><name><surname>Turner</surname><given-names>Nicholas L.</given-names></name><xref ref-type="aff" rid="A6">6</xref><xref ref-type="aff" rid="A7">7</xref></contrib><contrib contrib-type="author"><name><surname>Wong</surname><given-names>William</given-names></name><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Wu</surname><given-names>Jingpeng</given-names></name><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Yin</surname><given-names>Wenjing</given-names></name><xref ref-type="aff" rid="A9">9</xref></contrib><contrib contrib-type="author"><name><surname>Yu</surname><given-names>Szi-chieh</given-names></name><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Reimer</surname><given-names>Jacob</given-names></name><xref ref-type="aff" rid="A4">4</xref><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Berens</surname><given-names>Philipp</given-names></name><xref ref-type="aff" rid="A12">12</xref><xref ref-type="aff" rid="A13">13</xref></contrib><contrib contrib-type="author"><name><surname>Tolias</surname><given-names>Andreas S.</given-names></name><xref ref-type="aff" rid="A4">4</xref><xref ref-type="aff" rid="A5">5</xref><xref ref-type="aff" rid="A14">14</xref><xref ref-type="aff" rid="A15">15</xref><xref ref-type="aff" rid="A16">16</xref><xref ref-type="aff" rid="A17">17</xref></contrib><contrib contrib-type="author"><name><surname>Ecker</surname><given-names>Alexander S.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>1nstitute of Computer Science and Campus Institute Data Science, University of Göttingen, Germany</aff><aff id="A2"><label>2</label>Institute for Theoretical Physics, University of Tübingen, Germany</aff><aff id="A3"><label>3</label>Max Planck Institute for Dynamics and Self-Organization, Göttingen, Germany</aff><aff id="A4"><label>4</label>Department of Neuroscience, Baylor College of Medicine, Houston, USA</aff><aff id="A5"><label>5</label>Center for Neuroscience and AI, Baylor College of Medicine, Houston, USA</aff><aff id="A6"><label>6</label>Princeton Neuroscience Institute, Princeton University, USA</aff><aff id="A7"><label>7</label>Department of Computer Science, Princeton University, USA</aff><aff id="A8"><label>8</label>Department of Electrical Engineering, Princeton University, USA</aff><aff id="A9"><label>9</label>Allen Institute for Brain Science, Seattle, WA, USA</aff><aff id="A10"><label>10</label>Rice University, Houston, TX, USA</aff><aff id="A11"><label>11</label> Massachusetts Institute of Technology, Cambridge, MA, USA</aff><aff id="A12"><label>12</label>Hertie Institute for AI in Brain Health, University of Tübingen, Germany</aff><aff id="A13"><label>13</label>Tübingen AI Center, Tübingen, Germany</aff><aff id="A14"><label>14</label>Department of Ophthalmology, Stanford University, Stanford, CA, USA</aff><aff id="A15"><label>15</label>Department of Electrical Engineering, Stanford University, Stanford, CA, USA</aff><aff id="A16"><label>16</label>Stanford BioX, Stanford University, Stanford, CA, USA</aff><aff id="A17"><label>17</label>Wu Tsai Neurosciences Institute, Stanford University, Stanford, CA, USA</aff><author-notes><corresp id="CR1">
<label>*</label>Correspondence: <email>ecker@cs.uni-goettingen.de</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>24</day><month>12</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>22</day><month>12</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Neurons in the neocortex exhibit astonishing morphological diversity which is critical for properly wiring neural circuits and giving neurons their functional properties. However, the organizational principles underlying this morphological diversity remain an open question. Here, we took a data-driven approach using graph-based machine learning methods to obtain a low-dimensional morphological “bar code” describing more than 30,000 excitatory neurons in mouse visual areas V1, AL and RL that were reconstructed from the millimeter scale MICrONS serial-section electron microscopy volume. Contrary to previous classifications into discrete morphological types (m-types), our data-driven approach suggests that the morphological landscape of cortical excitatory neurons is better described as a continuum, with a few notable exceptions in layers 5 and 6. Dendritic morphologies in layers 2–3 exhibited a trend towards a decreasing width of the dendritic arbor and a smaller tuft with increasing cortical depth. Inter-area differences were most evident in layer 4, where V1 contained more atufted neurons than higher visual areas. Moreover, we discovered neurons in V1 on the border to layer 5 which avoided deeper layers with their dendrites. In summary, we suggest that excitatory neurons’ morphological diversity is better understood by considering axes of variation than using distinct m-types.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><label>1</label><title>Introduction</title><p id="P2">Neurons have incredibly complex and diverse shapes. Since Ramón y Cajal, neuroanatomists have studied their morphology [<xref ref-type="bibr" rid="R29">29</xref>] and have classified them into different types. From a computational point of view, a neuron’s dendritic morphology constrains which inputs it receives, how these inputs are integrated and, thus, which computations the neuron and the circuit it is part of can learn to perform.</p><p id="P3">Less than 15% of neocortical neurons are inhibitory, yet they are morphologically the most diverse and can be classified reliably into well-defined subtypes [<xref ref-type="bibr" rid="R7">7</xref>, <xref ref-type="bibr" rid="R13">13</xref>]. The vast majority of cortical neurons are excitatory. Excitatory cells can be divided into spiny stellate and pyramidal cells [<xref ref-type="bibr" rid="R26">26</xref>]. Although pyramidal cells have a very stereotypical dendritic morphology, they exhibit a large degree of morphological diversity. Recent studies subdivide them into 10–20 cell types using manual classification [<xref ref-type="bibr" rid="R21">21</xref>] or clustering algorithms applied to dendritic morphological features [<xref ref-type="bibr" rid="R10">10</xref>, <xref ref-type="bibr" rid="R16">16</xref>, <xref ref-type="bibr" rid="R25">25</xref>].</p><p id="P4">Existing studies of excitatory morphologies have revealed a number of consistent patterns, such as the well-known thick-tufted pyramidal cells of layer 5 [<xref ref-type="bibr" rid="R14">14</xref>, <xref ref-type="bibr" rid="R10">10</xref>, <xref ref-type="bibr" rid="R16">16</xref>, <xref ref-type="bibr" rid="R21">21</xref>, <xref ref-type="bibr" rid="R25">25</xref>]. However, a commonly agreed-upon morphological taxonomy of excitatory neuron types is yet to be established. For instance, Markram et al. [<xref ref-type="bibr" rid="R21">21</xref>] describe two types of thick-tufted pyramidal cells based on the location of the bifurcation point of the apical dendrite (early vs. late). Later studies suggest that these form two ends of a continuous spectrum [<xref ref-type="bibr" rid="R16">16</xref>, <xref ref-type="bibr" rid="R10">10</xref>]. Other authors even observe that morphological features overall do not form isolated clusters and suggest an organization into families with more continuous variation within families [<xref ref-type="bibr" rid="R31">31</xref>]. There are two main limitations of previous morphological characterizations: First, many rely on relatively small numbers of reconstructed neurons used to asses the morphological landscape. Second, they represent the dendritic morphology using summary statistics such as point counts, segment lengths, volumes, density profiles (so-called morphometrics; [<xref ref-type="bibr" rid="R25">25</xref>, <xref ref-type="bibr" rid="R33">33</xref>, <xref ref-type="bibr" rid="R20">20</xref>]) or graph-based topological measures [<xref ref-type="bibr" rid="R15">15</xref>]. These features were handcrafted by humans and may not capture all crucial axes of variation.</p><p id="P5">We here take a data-driven approach using a recently developed unsupervised representation learning approach [<xref ref-type="bibr" rid="R42">42</xref>] to extract a morphological feature representation directly from the dendritic skeleton. We apply this approach to a large-scale anatomical dataset [<xref ref-type="bibr" rid="R6">6</xref>] to obtain low-dimensional vector embeddings (“bar codes”) of more than 30,000 neurons in mouse visual areas V1, AL and RL. Our analysis suggests that excitatory neurons’ morphologies form a continuum, with notable exceptions such as layer 5 thick-tufted cells, and vary with respect to three major axes: soma depth, total apical and total basal skeletal length. Moreover, we found a number of novel morphological features in the upper layers: Neurons in layers 2/3 showed a trend of a decreasing width of their dendritic arbor and a smaller tuft with increasing cortical depth. In layer 4, morphologies showed area-specific variation: atufted neurons were primarily located in the primary visual cortex, while tufted neurons were more abundant in higher visual areas. Finally, layer 4 neurons in V1 on the border to layer 5 showed a tendency towards avoiding layer 5 with their dendrites.</p></sec><sec id="S2" sec-type="results"><label>2</label><title>Results</title><sec id="S3"><label>2.1</label><title>Self-supervised learning of embeddings for 30,000 excitatory neurons from visual cortex</title><p id="P6">Our goal was to perform a large-scale census of the dendritic morphologies of excitatory neurons without prescribing a-priori which morphological features to use. Therefore, we used machine learning techniques [<xref ref-type="bibr" rid="R42">42</xref>] to learn the features directly from the neuronal morphology.</p><p id="P7">Our starting point was a 1.3 × 0.87 × 0.82 mm<sup>3</sup> volume of tissue from the visual cortex of an adult P75–87 mouse, which has been densely reconstructed using serial section electron microscopy [<xref ref-type="bibr" rid="R6">6</xref>]. This volume has been segmented into individual cells, including non-neuronal types and more than 54,000 neurons whose soma was located within the volume. From these detailed reconstructions we extracted each neuron’s dendritic tree and represented it as a skeleton (<xref ref-type="fig" rid="F1">Fig. 1A</xref>) [<xref ref-type="bibr" rid="R4">4</xref>]: each neuron’s dendritic morphology was represented as a graph, where each node had a location in 3d space. This means we focused on the location and branching patterns of the dendritic tree, not fine-grained details of spines or synapses (see companion paper [<xref ref-type="bibr" rid="R32">32</xref>]), or any subcellular structures (see companion paper [<xref ref-type="bibr" rid="R8">8</xref>]).</p><p id="P8">Our next step was to embed these graphs into a vector space that defined a measure of similarity, such that similar morphologies were mapped onto nearby points in embedding space (<xref ref-type="fig" rid="F1">Fig. 1B</xref>). To do so, we employed a recently developed self-supervised learning method called GRAPHDINO [<xref ref-type="bibr" rid="R42">42</xref>] that learns semantic representations of graphs without relying on manual annotations. The idea of this method is to generate two “views” of the same input by applying random identitypreserving transformations such as rotations around the vertical axis, slightly perturbing node locations or dropping subbranches (<xref ref-type="fig" rid="F1">Fig. 1B</xref>, top and bottom). Then both views are encoded using a neural network. The neural network is trained to map both views onto similar vector embeddings. For model training, the data was split into training, validation and test data to ensure that the model did not overfit (<xref ref-type="sec" rid="S19">Sec. 4.5</xref>). The model outputs a 32-dimensional vector for each neuron that captures the morphological features of the neuron’s dendritic tree. Thus, each neuron is represented as a point in this 32-dimensional vector space (<xref ref-type="fig" rid="F1">Fig. 1C</xref>).</p><p id="P9">At this stage, we performed another quality control step: Using the learned embeddings as a similarity metric between neurons, we clustered the neurons into 100 clusters and manually inspected the resulting clusters. We found a non-negligible fraction of neurons whose apical dendrite left the volume or was lost during tracing (see <xref ref-type="sec" rid="S14">Methods</xref> for details). We removed neurons whose somata are in close proximity to the imaged volume boundary (<xref ref-type="fig" rid="F2">Fig. 2A</xref>). Additionally, we used the clusters containing fragmented neurons as examples for broken neurons and trained a classifier to predict whether a neuron has reconstruction errors using the learned morphological embeddings as input features (<xref ref-type="fig" rid="F2">Fig. 2B</xref>, <xref ref-type="supplementary-material" rid="SD1">Fig. A.2A, B</xref>). We then removed all neurons from the dataset that were classified as erroneous. Also, at this point we removed all interneurons from the dataset since we focused on excitatory neurons in this paper (<xref ref-type="fig" rid="F2">Fig. 2C</xref>, <xref ref-type="supplementary-material" rid="SD1">Fig. A.2B, C</xref>).</p><p id="P10">The vector embeddings of the remaining 33,997 excitatory neurons in the dataset were organized by cortical depth (<xref ref-type="fig" rid="F2">Fig. 2E</xref>) and, as a consequence, could distinguish well between different cortical layers (<xref ref-type="fig" rid="F2">Fig. 2F, G</xref>; note that there is no 1:1 correspondence between cortical depth and layer as the layer boundaries varied across the volume.). The learned embeddings could also distinguish between broad cell types (<xref ref-type="fig" rid="F2">Fig. 2H, I</xref>) that were assigned by expert neuroanatomists [<xref ref-type="bibr" rid="R32">32</xref>] using cortical origin of the somata and their long-range projection type (IT: intratelencephalic or intracortical; ET: extratelencephalic or subcortical projecting, NP: near projecting, and CT: cortico-thalamic). Note that neither the location of the soma nor the projection type were provided to the model, showing that the dendritic morphology by itself provides information on these broad cell types. One exception are the 6P-CT and 6P-IT cells, which were partly intermingled in embedding space. 6P-IT cells show a high variance in their dendritic morphology which in some cases are indistinguishable from 6P-CT cells when no information about the projection type is used (<xref ref-type="fig" rid="F2">Fig. 2H, I</xref>).</p><p id="P11">To test that the learned embedding is generally applicable beyond EM datasets and the MICRONS dataset specifically, we used the GRAPHDINO model trained on MICRONS to embed 61 neurons from mouse visual cortex [<xref ref-type="bibr" rid="R1">1</xref>] that have been recorded using PatchSeq [<xref ref-type="bibr" rid="R2">2</xref>] and show that the model generalizes to other datasets and recording techniques (<xref ref-type="supplementary-material" rid="SD1">Fig. A.10</xref>).</p></sec><sec id="S4"><label>2.2</label><title>Dendritic morphologies mostly form a continuum with distinct clusters only in deeper layers</title><p id="P12">We noticed that the embedding space appeared to form largely a continuum, with only a few fairly distinct clusters, such as the layer 5 ET cells (<xref ref-type="fig" rid="F2">Fig. 2H</xref>, purple). Previous papers have characterized excitatory morphologies by categorizing neurons into morphological types (m-types), with the number of types varying between nine and nineteen [<xref ref-type="bibr" rid="R25">25</xref>, <xref ref-type="bibr" rid="R21">21</xref>, <xref ref-type="bibr" rid="R24">24</xref>, <xref ref-type="bibr" rid="R15">15</xref>, <xref ref-type="bibr" rid="R10">10</xref>, <xref ref-type="bibr" rid="R32">32</xref>]. But is categorization into discrete types the best way of describing the landscape of morphologies or is it rather characterized by continuous variation? The answer depends on the structure of the data. Consider the following toy example where the data is generated by a mixture of two normal distributions (<xref ref-type="fig" rid="F3">Fig. 3A</xref>): If the two components are well separated, it makes sense to define each one as a distinct type (<xref ref-type="fig" rid="F3">Fig. 3A</xref>, left). However, if they are strongly overlapping such that the resulting data distribution is not even bimodal (<xref ref-type="fig" rid="F3">Fig. 3A</xref>, right), describing the distribution by two types is not useful and identifying the two types by clustering will not work reliably, either. But there are also scenarios in between, where the distinction is not as straightforward (<xref ref-type="fig" rid="F3">Fig. 3A</xref>, middle). Thus, the question of whether a distribution is discrete or forms a continuum does not have a binary answer – it is rather a matter of degree.</p><p id="P13">To understand to what degree our dataset forms a continuum, we devised a simple procedure based on synthetic data that emulates the real data to some extent but allows us to manipulate the degree of separation. The synthetic data was generated from a Gaussian mixture model (GMM) fit to our morphological embeddings, from which we kept the cluster means and weights, but replaced the covariance matrix to be spherical with varying variances (<italic>σ</italic><sup>2</sup>). Following previous estimations of number of excitatory cell types in the rodent sensory cortex [<xref ref-type="bibr" rid="R25">25</xref>, <xref ref-type="bibr" rid="R21">21</xref>, <xref ref-type="bibr" rid="R24">24</xref>, <xref ref-type="bibr" rid="R15">15</xref>, <xref ref-type="bibr" rid="R10">10</xref>, <xref ref-type="bibr" rid="R32">32</xref>], we generated synthetic data distributions with 20 clusters (<xref ref-type="fig" rid="F3">Fig. 3B</xref>), as well as with 10 and 40 clusters as controls (<xref ref-type="supplementary-material" rid="SD1">Fig. A.5</xref>). When the variance was small (<italic>σ</italic><sup>2</sup> = 0.05), all clusters were clearly distinct (<xref ref-type="fig" rid="F3">Fig. 3B</xref>, left). As we increased the variance to 1, the distribution became more and more continuous (<xref ref-type="fig" rid="F3">Fig. 3B</xref>, right). At intermediate values of 0.3 or 0.5 the synthetic data distribution resembled qualitatively the real data (<xref ref-type="fig" rid="F3">Fig. 3D</xref>).</p><p id="P14">To make the comparison more quantitative, we asked two questions, which can be answered using the synthetic data for which the ground truth generating process is known. First, we asked under which conditions we could reliably identify the underlying clusters that generated the data (<xref ref-type="fig" rid="F3">Fig. 3C</xref>). To do so, we assumed we did not know the generative process and clustered the synthetic data repeatedly by fitting Gaussian mixture models (GMMs) with varying number of components and random initial conditions. We found that in the extreme scenario when all clusters were clearly separated, the result of the clustering was highly consistent across runs when the number of clusters matched the ground truth (<xref ref-type="fig" rid="F3">Fig. 3C</xref>; ARI ≥ 0.85 for <italic>σ</italic><sup>2</sup> ≤ 0.1 and number of ground truth components equal to number of GMM components). As the degree of overlap between the clusters increased, the consistency of the clustering result decreased and the optimal number of clusters was increasingly less clearly defined. For a larger degree of overlap (<italic>σ</italic><sup>2</sup> &gt; 0.5) the consistency of clusterings decreased monotonically with the number of clusters and no optimal number of clusters could be determined. The same was true for the real data (<xref ref-type="fig" rid="F3">Fig. 3E</xref>): There was no noticeable peak in the ARI across different number of clusters, suggesting that the scenario with <italic>σ</italic><sup>2</sup> ≥ 0.5 is realistic in this regard (Neuronal data: ARI = 0.63 for number of clusters = 20; compared to ARI = 0.62 for number of clusters = 20 and <italic>σ</italic><sup>2</sup> = 0.5 for the synthetic data).</p><p id="P15">Next, we investigated the degree to which individual clusters were distinct from their neighboring clusters. Even though certain parts of the distribution appeared continuous, there could be clusters that are separable. To address this question, we built a <italic>k</italic>-nearest-neighbor graph from the clustering output, connecting each cluster to its <italic>k</italic> = 3 nearest neighbors. We then quantified for each pair of neighboring clusters how separated they are. To do so, we projected all data points assigned to the pair onto the direction connecting the two cluster means (<xref ref-type="fig" rid="F3">Fig. 3B</xref>, insets left) and computed the dip statistic [<xref ref-type="bibr" rid="R12">12</xref>]. The dip statistic measures how bimodal a distribution is by computing how much its empirical cumulative distribution deviates from that of the closest uniform distribution (<xref ref-type="fig" rid="F3">Fig. 3B</xref>, insets right). It is close to zero for unimodal distributions and increases with increasing separation of the two modes of a bimodal distribution. This analysis confirmed the qualitative impression from the t-distributed stochastic neighbor embedding (t-SNE; [<xref ref-type="bibr" rid="R37">37</xref>]) that the layer 5 ET cluster (purple cluster 12 in <xref ref-type="fig" rid="F3">Fig. 3B, D</xref>) was separated more from its nearest neighbor (cluster 0, green) than two representative example clusters from layer 2/3 (clusters 1 and 17, red and teal), which were not separated and appeared to divide a continuum more or less arbitrarily. These two patterns of results in the neuronal data were reproduced well by the synthetic data with a standard deviation of 0.5 (<xref ref-type="fig" rid="F3">Fig. 3B</xref>, insets 5&amp;6). Examination of the entire nearest-neighbor graph showed that layers 2–4, including the upper part of layer 5, form a continuum with no neighboring clusters being well-separated, clusters in layer 5 were more distinct, and two clusters in layer 6 (inverted and subplate neurons) stood out from a larger clique of layer 6 clusters (<xref ref-type="fig" rid="F3">Fig. 3F</xref>). Over the entire dataset, the maximum dip statistic (maximally separated clusters) of the neuronal data was in between the maximal dip statistic for the synthetic data with <italic>σ</italic><sup>2</sup> = 0.3 and <italic>σ</italic><sup>2</sup> = 0.5 (<xref ref-type="fig" rid="F3">Fig. 3G</xref>), again suggesting that the qualitative visualization by t-SNE captures the underlying structure of the data well.</p><p id="P16">The analyses presented so far established that our learned morphological embeddings form mostly a continuum. Could this result be caused by our learning methods? This was not the case, as using a different contrastive learning objective (<xref ref-type="supplementary-material" rid="SD1">Fig. A.7</xref>) to train GRAPHDINO and varying model hyperparameters (<xref ref-type="supplementary-material" rid="SD1">Fig. A.6</xref>) produced the same result. Similarly, using handcrafted morphometrics from earlier studies [<xref ref-type="bibr" rid="R10">10</xref>, <xref ref-type="bibr" rid="R32">32</xref>] on our data did not change our conclusions (<xref ref-type="supplementary-material" rid="SD1">Fig. A.9</xref>, <xref ref-type="supplementary-material" rid="SD1">Sec. A.1</xref>). Additionally, we employed alternative dimensionality reduction techniques with varying settings (<xref ref-type="supplementary-material" rid="SD1">Fig. A.8</xref>) to ensure that our interpretation is not dependent on t-SNE for visualization.</p></sec><sec id="S5"><label>2.3</label><title>The landscape of morphological variation across layers</title><p id="P17">Given the results from the previous section, we conclude that excitatory morphologies were mostly organized along a continuum, with only a few distinct clusters in the deeper layers. Therefore we did not base our subsequent analyses on a set of m-types as previous studies did, but instead investigated the major axes of variation within the morphological embedding space. The cortical organization into layers is well established, so we separated cells by cortical layer. We determined the layer boundaries by training a classifier using our 32-dimensional morphological embeddings and a set of 922 neurons manually assigned to layers by experts (<xref ref-type="fig" rid="F2">Fig. 2D, F, G</xref>). As expected, the inferred layer boundaries indicated that layer 4 was approximately 20% thicker in V1 than in higher visual areas RL and AL (<xref ref-type="fig" rid="F2">Fig. 2G</xref>; mean±SD: 118±6 μm in V1 vs. 97±6 μm in HVA), the difference being compensated for by layers 2/3 and 6 each being approximately 10 μm thinner. In the following we proceed by assigning neurons to layers based on their soma location relative to these inferred boundaries.</p><p id="P18">To visualize the main axes of morphological variation within each layer, we performed nonlinear dimensionality reduction using t-SNE and identified morphological features that formed major axes of variation within the two-dimensional space (<xref ref-type="fig" rid="F5">Fig. 5</xref>).</p><p id="P19">What do these axes of variation in the two-dimensional t-SNE embeddings mean in human-interpretable terms? To answer this question, we looked for morphological metrics that formed gradients within the t-SNE embedding space. Based on visual inspection, we found the following six morphological metrics to account well for a large fraction of the dendritic morphological diversity in our dataset (see <xref ref-type="fig" rid="F4">Fig. 4</xref> for an illustration): (1) depth of the soma relative to the pia, (2) height of the cell, (3) total length of the apical dendrites, (4) width of the apical dendritic tree, (5) total length of the basal dendrites, and (6) location of the basal dendritic tree relative to the soma (“basal bias”).</p></sec><sec id="S6"><label>2.4</label><title>Layer 2/3: Width and length of apical dendrites decrease with depth</title><p id="P20">In layer 2/3 (L2/3), we found a continuum of dendritic morphologies that formed a gradient from superficial to deep, with deeper neurons (in terms of soma depth) becoming thinner and less tufted (<xref ref-type="fig" rid="F5">Fig. 5A</xref> L2/3 a,b,c). The strongest predictors of the embeddings were the depth of the soma relative to the pia and the total height of the cell (coefficient of determination <italic>R</italic><sup>2</sup> &gt; 0.9; <xref ref-type="fig" rid="F5">Fig. 5B</xref> L2/3; <xref ref-type="supplementary-material" rid="SD1">Tab. A.3</xref>). These two metrics were also strongly correlated (Spearman’s rank correlation coefficient, <italic>ρ</italic> = 0.93; <xref ref-type="fig" rid="F5">Fig. 5C</xref> L2/3; <xref ref-type="supplementary-material" rid="SD1">Tab. A.4</xref>), since nearly all L2/3 cells had an apical dendritic tree that reached to the pial surface (see example morphologies in <xref ref-type="fig" rid="F5">Fig. 5A</xref> L2/3, top). L2/3 cells varied in terms of their degree of tuftedness: both total length and width of their apical tuft decreased with the depth of the soma relative to the pia (<xref ref-type="fig" rid="F5">Fig. 5A</xref> L2/3 b,c). L2/3 cells also varied along a third axis: the skeletal length of their basal dendrites (<xref ref-type="fig" rid="F5">Fig. 5A</xref> L2/3 d), but this property was not strongly correlated with either soma depth or shape of the apical dendrites (<xref ref-type="fig" rid="F5">Fig. 5C</xref> L2/3).</p></sec><sec id="S7"><label>2.5</label><title>Layer 4: Small or no tufts and some cells’ basal dendrites avoid layer 5</title><p id="P21">The dendritic morphology of layer 4 (L4) was again mostly a continuum and appeared to be a continuation of the trends from L2/3: The skeletal length of the apical dendrites was shorter, on average, than that of most L2/3 cells (<xref ref-type="fig" rid="F5">Fig. 5A</xref> L4 b) and approximately 20% of the cells were atufted. Within L4 the total apical skeletal length was not correlated with the depth of the soma (<italic>ρ</italic> = 0.0; <xref ref-type="fig" rid="F5">Fig. 5C</xref> L4; <xref ref-type="supplementary-material" rid="SD1">Tab. A.4</xref>), suggesting that it forms an independent axis of variation. Considerable variability was observed in terms of the total length of the basal dendritic tree, but – as in L2/3 – it was not correlated with any of the other properties.</p><p id="P22">Our data-driven embeddings revealed another axis of variation that had previously not been considered important: the location of the basal dendritic tree relative to the soma (“basal bias”; <xref ref-type="fig" rid="F4">Fig. 4</xref>). We found that many L4 cells avoided reaching into L5 with their dendrites (<xref ref-type="fig" rid="F5">Fig. 5A</xref> L4 c). As a result, the depth of the basal dendrites was anticorrelated with the depth of the soma (<italic>ρ</italic> = −0.29; <xref ref-type="fig" rid="F5">Fig. 5A</xref> L4 c and <xref ref-type="fig" rid="F5">Fig. 5C</xref> L4; <xref ref-type="supplementary-material" rid="SD1">Tab. A.4</xref>). We will come back to this observation later (see <xref ref-type="sec" rid="S11">Sec. 2.9</xref>).</p></sec><sec id="S8"><label>2.6</label><title>Layer 5: Thick-tufted cells stand out</title><p id="P23">Layer 5 (L5) showed a less uniformly distributed latent space than L2/3 or L4 (<xref ref-type="fig" rid="F5">Fig. 5A</xref> L5, <xref ref-type="fig" rid="F3">Fig. 3F</xref>). Most distinct was the cluster of well-known thick-tufted pyramidal tract (PT) cells [<xref ref-type="bibr" rid="R14">14</xref>, <xref ref-type="bibr" rid="R10">10</xref>, <xref ref-type="bibr" rid="R16">16</xref>, <xref ref-type="bibr" rid="R21">21</xref>, <xref ref-type="bibr" rid="R25">25</xref>] on the bottom right (<xref ref-type="fig" rid="F5">Fig. 5A</xref> L5 d, light green points), also known as extratelencephalic (ET) projection neurons. These cells accounted for approximately 17% of the cells within L5 (based on a classifier trained on a smaller, manually annotated subset of the data; see <xref ref-type="sec" rid="S14">Methods</xref>). They were restricted almost exclusively to the deeper half of L5 (<xref ref-type="fig" rid="F5">Fig. 5A</xref> L5 a and d, inset 2; C inset top right) and compared to other L5 cells they have the longest skeleton for all three dendritic compartments: apical, basal and oblique.</p><p id="P24">Another morphologically distinct type of cell was apparent at the end of the layer 5 spectrum: the near-projecting (NP) cells [<xref ref-type="bibr" rid="R17">17</xref>, <xref ref-type="bibr" rid="R10">10</xref>] with their long and sparse basal dendrites (<xref ref-type="fig" rid="F5">Fig. 5A</xref> L5 d, inset 3). These cells accounted for approximately 4% of the cells within L5. They tended to send their dendrites deeper (relative to the soma), had little or no obliques and tended to have small or no apical tufts.</p><p id="P25">The remaining roughly 80% of the cells within L5 varied continuously in terms of the skeletal length of the different dendritic compartments. While there was a correlation between apical and basal skeletal length (apical vs. basal: <italic>ρ</italic> = 0.43; <xref ref-type="fig" rid="F5">Fig. 5</xref> L5 C; <xref ref-type="supplementary-material" rid="SD1">Tab. A.4</xref>), there was also a significant degree of diversity. Within this group there was no strong correlation of morphological features with the location of the soma within L5 (depth vs. apical length <italic>ρ</italic> = 0.2, depth vs. basal <italic>ρ</italic> = 0.06; <xref ref-type="fig" rid="F5">Fig. 5</xref> L5 C; <xref ref-type="supplementary-material" rid="SD1">Tab. A.4</xref>).</p><p id="P26">In upper L5 we found a group of cells that resembled the L4 cells whose dendrites avoid L5 (<xref ref-type="fig" rid="F5">Fig. 5A</xref> L5 d, inset 1). These cells were restricted to the uppermost portion of L5 and morphologically resembled L4 cells by being mostly atufted and exhibiting upwards curved basal dendrites. We refer to these cells as displaced L4 cells. Their presence could be caused by our piece-wise linear estimation of the layer boundaries being not precise enough. Alternatively, it could suggest that there are no precise laminar boundaries based on morphological features of neurons, but instead different layers blend into one another as observed by previous studies [<xref ref-type="bibr" rid="R25">25</xref>, <xref ref-type="bibr" rid="R8">8</xref>].</p></sec><sec id="S9"><label>2.7</label><title>Layer 6: Long and narrow, oblique and inverted pyramidal neurons</title><p id="P27">Dendritic morphology in layer 6 (L6) also formed a continuum with a large degree of morphological diversity. The dominant feature of L6 was the large variety of cell heights (<italic>R</italic><sup>2</sup> &gt; 0.9; <xref ref-type="fig" rid="F5">Fig. 5</xref> L6 B; <xref ref-type="supplementary-material" rid="SD1">Tab. A.3</xref>). Overall, the height of a cell was not strongly correlated with its soma’s location within L6 (<italic>ρ</italic> = −0.13; <xref ref-type="fig" rid="F5">Fig. 5</xref> L6 C; <xref ref-type="supplementary-material" rid="SD1">Tab. A.4</xref>). Unlike other layers, where the apical dendrites usually reach all the way up to layer 1, many cells in L6 have shorter apical dendrites. However, due to tracing errors, our analysis overestimated the number of such short cells. We therefore manually inspected 183 putative atufted early-terminating neurons within L6 and found that, among those, 45% were incompletely traced, whereas 55% were true atufted cells whose apical dendrite terminated clearly below L1 (<xref ref-type="sec" rid="S27">Sec. 4.9</xref>).</p><p id="P28">As described previously [<xref ref-type="bibr" rid="R10">10</xref>, <xref ref-type="bibr" rid="R25">25</xref>], the dendritic tree of L6 cells is narrower than in the layers above. Also consistent with previous work, we found a substantial number of horizontal and inverted pyramidal neurons, where the apical dendrite points sideways or downwards, respectively (<xref ref-type="fig" rid="F5">Fig. 5A</xref> L6 d, inset 1 &amp; 6). However, apicals of inverted and horizontal cells are currently not detected by the automatic compartment identification (see companion paper [<xref ref-type="bibr" rid="R4">4</xref>]), rendering an automatic analysis of the apical dendrites in layer 6 currently unreliable. This does not affect the learned embeddings as GRAPHDINO is trained without knowledge about the differentiation of dendritic compartments.</p></sec><sec id="S10"><label>2.8</label><title>Pyramidal neurons are less tufted in V1 than in higher visual areas</title><p id="P29">After our layer-wise survey of excitatory neurons’ morphological features, we next asked whether there are inter-areal differences between primary visual cortex (V1) and higher visual areas (HVAs). The total length of the apical dendrites of neurons in V1 was significantly shorter than for neurons in HVA (<xref ref-type="fig" rid="F6">Fig. 6A</xref>): For L2/3, neurons in V1 had on average 16% shorter apical branches than in HVA (mean±SD: 1,423±440 μm in V1 vs. 1,688±554 μm in HVA; <italic>t-test</italic>: <italic>p</italic> &lt; 10<sup>−10</sup>, Cohen’s <italic>d</italic> = 0.53). Similarly, L4 neurons in V1 had on average 16% shorter apical branches than in HVA (851±264 μm vs. 1,019±313 μm; <italic>p</italic> &lt; 10<sup>−10</sup>, <italic>d</italic> = 0.58). In L5, neurons in V1 had on average 14% shorter apical branches than L5 neurons in HVA (1,326±661 μm vs. 1,549±745 μm; <italic>p</italic> &lt; 10<sup>−10</sup>, <italic>d</italic> = 0.32). While the trend continued in L6, the difference in apical length between V1 and HVA neurons was smaller. There was only a 4% increase in apical length in HVA compared to V1 (1,112±383 μm vs. 1,159±397 μm; <italic>p</italic> = 1.810<sup>−6</sup>, <italic>d</italic> = 0.12). For this analysis, only neurons with identified apical dendrites were taken into account (see companion paper [<xref ref-type="bibr" rid="R4">4</xref>]).</p><p id="P30">Upon closer inspection, we observed that L4 contained substantially more atufted neurons in V1 than in higher visual areas RL and AL (<xref ref-type="fig" rid="F6">Fig. 6A</xref>). We clustered each layer’s morphological embeddings into 15 clusters using a Gaussian Mixture Model and looked for clusters that were restricted to particular brain areas. Clusters that were clearly confined to V1 or HVAs were primarily found in L4. When classifying (manually, at the cluster-level) L4 neurons into atufted, small tufted and tufted, we observed that atufted neurons were almost exclusively located in V1, while tufted neurons were more frequent in HVAs (<xref ref-type="fig" rid="F6">Fig. 6B</xref>).</p></sec><sec id="S11"><label>2.9</label><title>Layer 4 cells avoiding layer 5 are located primarily in primary visual cortex</title><p id="P31">We observed a second area difference which was related to the novel morphological cell type in L4. Recall that these cells’ dendrites avoid reaching into L5. Interestingly, these cells were located in a very narrow strip of approximately 50 μm above the border between L4 and L5 (<xref ref-type="fig" rid="F7">Fig. 7A</xref>). Moreover, they were atufted and almost exclusively located in V1 (<xref ref-type="fig" rid="F7">Fig. 7B</xref>).</p></sec><sec id="S12"><label>2.10</label><title>Morphological property of avoiding layer 5 has functional correlate</title><p id="P32">Lastly, we asked whether morphological variation can be linked to the neurons’ functional properties. While an extensive investigation of the structure–function relationship is beyond the scope of this study, we took one novel morphological aspect revealed by our study as a proof of principle: We investigated whether L4 neurons that avoided reaching into layer 5 with their dendrites differ in their tuning to visual stimuli from other neurons in layer 4. To address this question, we made use of the fact that for many of the neurons in our dataset, we have measurements of how they respond to natural stimuli [<xref ref-type="bibr" rid="R6">6</xref>]. We leveraged a functional digital twin – a model that accurately predicted the response of a neuron to arbitrary visual stimuli [<xref ref-type="bibr" rid="R38">38</xref>] – to extract a functional bar code – a vector embedding <italic>f<sub>i</sub></italic> that describes the input-output function of a neuron analogous to how our morphological bar codes describe their morphology (<xref ref-type="fig" rid="F7">Fig. 7C</xref>). From this functional bar code of each neuron, we predicted one of its morphological properties: the basal bias metric. We found that the basal bias of L4 neurons could be predicted reasonably well from the neurons’ response functions to visual stimuli (<xref ref-type="fig" rid="F7">Fig. 7D</xref>; Pearson correlation <italic>ρ</italic> = 0.41, <italic>p</italic> &lt; 10<sup>−10</sup>). This analysis could be confounded by cortical depth being predictive of the basal bias. However, a model predicting the basal bias from cortical depth and functional bar code explained significantly more variance in the basal bias metric than one using only cortical depth as predictor (<italic>R</italic><sup>2</sup> = 0.28 for both predictors vs. 0.21 for depth only; <italic>ρ</italic> = 0.53 and <italic>ρ</italic> = 0.46, respectively; Fisher’s z-test of difference between the correlation coefficients: <italic>p</italic> = 0.0015).</p></sec></sec><sec id="S13" sec-type="discussion"><label>3</label><title>Discussion</title><p id="P33">In summary, our data-driven unsupervised learning approach identified the known morphological features of excitatory cortical neurons’ dendrites and enabled us to make four novel observations: (1) Superficial L2/3 neurons are wider than deep ones; (2) L4 neurons in V1 are less tufted than those in HVAs; (3) a novel atufted L4 cell type that is specific to V1 whose basal dendrites avoid reaching into L5; (4) excitatory cortical neurons form mostly a continuum with respect to dendritic morphology, with some notable exceptions.</p><p id="P34">First, our finding that superficial L2/3 neurons are wider than deeper ones is clearly visible in the data both qualitatively and quantitatively. A similar observation has been made recently in concurrent work [<xref ref-type="bibr" rid="R41">41</xref>].</p><p id="P35">Second, in L4 a substantial number of cells are completely atufted. Here we see a differentiation with respect to brain areas: completely atufted cells are mostly restricted to V1 while HVA neurons in L4 tend to be more tufted. Why would V1 neurons be less tufted than those in higher visual areas? V1 – as the first cortical area for visual information processing – and L4 – as the input layer, in particular – might be less modulated by feedback connections than other layers and higher visual areas. Therefore, these neurons might sample the feedback input in L1 less than other neurons.</p><p id="P36">Third, we found that some neurons at the bottom of L4 of V1 avoid reaching into L5 with their dendrites. To our knowledge, this morphological pattern has not been described before in the visual cortex. Retrospectively, it can be observed in Gouwens and colleagues’ data: their spiny m-types 4 and 5, which are small- or atufted L4 neurons, show a positive basal bias (assuming their “basal bias y“ describes the same property; Gouwens et al. [<xref ref-type="bibr" rid="R10">10</xref>]; <xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. 15</xref>). Whether such cells are restricted to the bottom of layer 4 or are simply morphologically insdistinguishable from other cells when located more superficially cannot be answered from our data. However, interestingly, this morphological pattern correlated with the functional properties of the neurons. While this is by no means an exhaustive characterization of <italic>how</italic> morphology and function are related, this result shows that they are, and that such relationships can be identified by data-driven methods. What function could avoiding L5 have? Similarly to the non-existing tuft of these neurons, avoiding L5 could support these neurons in focusing on the thalamic input (which targets primarily L4) and, thus, represent and distribute the feedforward drive within the local circuit. It is therefore tempting to speculate that these atufted, L5-avoiding L4 neurons might be precursors of spiny stellate cells, which are nearly absent in the mouse visual cortex [<xref ref-type="bibr" rid="R30">30</xref>], but exist only in somewhat more developed sensory areas like barrel cortex or in cat and primate V1.</p><p id="P37">Fourth, except from the well-known L5 extratelencephalic (ET) projection neurons and some characteristic morphologies in L6 (subplate and inverted cells), our data and methods suggest that excitatory neurons in the mouse visual cortex form mostly a continuum with respect to dendritic morphology. Previous studies, in contrast, work on the premise that discrete cell types exist and categorize neurons into up to 20 m-types [<xref ref-type="bibr" rid="R25">25</xref>, <xref ref-type="bibr" rid="R21">21</xref>, <xref ref-type="bibr" rid="R24">24</xref>, <xref ref-type="bibr" rid="R10">10</xref>, <xref ref-type="bibr" rid="R16">16</xref>, <xref ref-type="bibr" rid="R8">8</xref>, <xref ref-type="bibr" rid="R32">32</xref>], most of them using clustering methods on morphological features [<xref ref-type="bibr" rid="R25">25</xref>, <xref ref-type="bibr" rid="R22">22</xref>, <xref ref-type="bibr" rid="R24">24</xref>, <xref ref-type="bibr" rid="R10">10</xref>]. While they assume that each cluster corresponds to a distinct m-type, they report the presence of variability within their proposed m-types. Furthermore, their visualizations of morphometrics per m-type depict further intra-class variability [<xref ref-type="bibr" rid="R10">10</xref>, <xref ref-type="bibr" rid="R16">16</xref>, <xref ref-type="bibr" rid="R32">32</xref>]. Thus, we believe that our data is consistent with previous work, but our data-driven, quantitative approach suggests that the morphological landscape of cortical excitatory neurons is better described as a continuum, with a few notable exceptions in deeper layers. This notion has also been brought up recently by transcriptomics studies, which observe continuous variation among cell types in cortex [<xref ref-type="bibr" rid="R35">35</xref>, <xref ref-type="bibr" rid="R36">36</xref>, <xref ref-type="bibr" rid="R11">11</xref>, <xref ref-type="bibr" rid="R31">31</xref>, <xref ref-type="bibr" rid="R43">43</xref>] as well as subcortical areas [<xref ref-type="bibr" rid="R34">34</xref>, <xref ref-type="bibr" rid="R39">39</xref>]. Furthermore, variation within transcriptomic types found in several of the studies aligns with variation observed in other modalities [<xref ref-type="bibr" rid="R11">11</xref>, <xref ref-type="bibr" rid="R31">31</xref>]. Scala et al. [<xref ref-type="bibr" rid="R31">31</xref>] suggest that neurons are organized into a small number of distinct and broad “families”, each of which exhibits substantial continuous variation among its family members. In their case, a substantial degree of morphological variation was evident among excitatory neurons of the IT type, and this variation correlated with transcriptomic variation within the type as well as the cortical depth of the neuron – resembling the gradual decrease in the width of the apical tuft with increasing cortical depth we observed. Our analysis supports the notion of broad “families” with intrinsic variation: excitatory cells can be mostly separated by layers into roughly a handful of families, each of which contains a substantial degree of variation in terms of morphology, which might also co-vary with other modalities.</p><p id="P38">This result does not rule out the possibility that there are in fact distinct types; it simply suggests that features beyond dendritic morphology need to be taken into account to clearly identify these types. For instance, the results of [<xref ref-type="bibr" rid="R32">32</xref>] suggest that the 5P-NP cells can be separated from other layer 5 pyramidal neurons by considering the class of interneurons that target them. It is also not guaranteed that our data-driven method identifies all relevant morphological features. Every method has (implicit or explicit) inductive biases. We tried to avoid explicit human-defined features, but by choosing a graph-based input representation we provided different inductive biases than, for instance, a voxel-based representation or one based on point clouds. However, the fact that we could reconcile known morphological features, discover novel ones and achieve good classification accuracy on an annotated subset of the data suggests that our learned embeddings indeed contain a rich and expressive representation of a neuron’s dendritic morphology.</p><p id="P39">In summary, recent studies of morphological as well as transcriptomic characteristics of cortical excitatory neurons suggest the presence of a few broad families of cell types, each exhibiting considerable intrinsic variation [<xref ref-type="bibr" rid="R11">11</xref>, <xref ref-type="bibr" rid="R31">31</xref>, <xref ref-type="bibr" rid="R43">43</xref>]. Due to this continuous variation a separation into finer cell types within these families is ambiguous. This raises the question whether it is feasible to establish a comprehensive atlas of cortical excitatory cell types. We suggest that we should rather think of the variability across cells as axes of variation, understand how these axes of variation correlate between modalities and whether they are just insignificant biological heterogeneity or indeed functionally relevant.</p></sec><sec id="S14" sec-type="methods"><label>4</label><title>Methods</title><sec id="S15"><label>4.1</label><title>Dataset</title><p id="P40">The dataset consists of a 1.3 × 0.87 × 0.82 <italic>mm</italic><sup>3</sup> volume of tissue from the visual cortex of an adult P75-87 mouse, which has been densely reconstructed using serial section electron microscopy (EM) [<xref ref-type="bibr" rid="R6">6</xref>]. We used the subvolume 65, which covers approximately 1.3 × 0.56 × 0.82 <italic>mm</italic><sup>3</sup>. It includes all layers of cortex and spans primary visual cortex (V1) and two higher visual areas, anterolateral area (AL) and rostrolateral area (RL). We refer to the original paper on the dataset [<xref ref-type="bibr" rid="R6">6</xref>] for details on the identification and morphological reconstruction of individual neurons.</p></sec><sec id="S16"><label>4.2</label><title>Skeletonization and cell compartment label assignment</title><p id="P41">The EM reconstructions yielded neuronal meshes. These meshes might be incomplete or exhibit different kinds of errors including merges of other neuronal or non-neuronal compartments onto the neurons. Therefore an automatic proof-reading pipeline that resulted in neuronal skeletons was executed (companion paper; Celii et al. [<xref ref-type="bibr" rid="R4">4</xref>]).</p><p id="P42">For the skeletal detection from the reconstructed meshes, the meshes were first downsampled to 25% of their resolution and made watertight. Then glia and nuclei meshes were identified and removed. For the remaining meshes the locations of the somata were identified using a soma detection algorithm [<xref ref-type="bibr" rid="R44">44</xref>]. Each neurite submesh was then skeletonized using a custom skeletonization algorithm which transformed axonal and dendritic processes into a series of line segments to obtain the skeleton (companion paper; Celii et al. [<xref ref-type="bibr" rid="R4">4</xref>]). For each skeleton, the highest probability axon subgraph was determined and all other non-soma nodes were labeled as dendrites. A final heuristic algorithm classifies subgraphs of dendritic nodes into compartments, such as apical trunks generally projecting from the top half of somas and with a general upward trajectory and obliques as projections off the apical trunks at an approximate 90 degree angle. For further details on the compartment label assignment, please see companion paper [<xref ref-type="bibr" rid="R4">4</xref>].</p></sec><sec id="S17"><label>4.3</label><title>Coordinate transformations</title><p id="P43">The EM volume is not perfectly aligned. First, the pial surface is not a horizontal plane parallel to the (<italic>x</italic>, <italic>z</italic>)-plane, but is instead slightly tilted. Second, the thickness of the cortex varies across the volume such that the distance from pia to white matter is not constant. Without any pre-processing, an unsupervised learning algorithm would pick up these differences and, for instance, find differences of layer 6 neurons across the volume simply because in some parts of the volume they tend to be located deeper than in others and their apical dendrites that reach to layer 1 tend to be larger. Using <italic>relative</italic> coordinates solves such issues if pia and white matter correspond to planes (approximately) parallel to the (<italic>x</italic>, <italic>z</italic>)-plane. To transform our coordinate system in such standardized coordinates, we first applied a rotation about the <italic>z</italic>-axis of 3.5 degrees. This transformation removed the systematic rotation with respect to the native axes (<xref ref-type="supplementary-material" rid="SD1">Fig. A.1B</xref>). To standardize measurements across depth (<italic>y</italic>-axis) and to account for differential thickness of the cortex, we estimated the best linear fit for both pial surface and white matter boundary by using a set of manually placed points, which are located on a regular grid along (<italic>x</italic>, <italic>z</italic>) with a spacing of 25 μm. For each (<italic>x</italic>, <italic>z</italic>)-coordinate, the <italic>y</italic>-coordinate was normalized such that the pia’s <italic>z</italic> coordinate corresponded to the average depth of the pia and the same for the white matter. This transformation resulted in an approximation of the volume where pia and white matter boundaries are horizontal planes orthogonal to the <italic>y</italic> axis and parallel to the (<italic>x</italic>, <italic>z</italic>)-plane. <xref ref-type="supplementary-material" rid="SD1">Fig. A.1C</xref> shows example neurons before and after normalization. All training and subsequent analysis were performed on this pre-processed data.</p></sec><sec id="S18"><label>4.4</label><title>Expert cell type labels</title><p id="P44">For a subset of the neurons in the volume experts labeled neurons according the following cell types: layer 2/3 and 4 pyramidal neurons, layer 5 near-projecting (NP), extratelencenphalic (ET) and intratelencenphalic (IT) neurons, layer 6 intratelencenphalic (IT) and cortico-thalamic (CT) neurons, Martinotti cells (MC), basket cells (BC), bipolar cells (BPC) and neurogliaform cells (NGC). Cell types were assigned based on visual inspection of individual cells taking into account morphology, synapses and connectivity, nucleus features and their (<italic>x</italic>, <italic>y</italic>, <italic>z</italic>)-location. All neurons were taken from one 100 μm column in the primary visual cortex (see companion paper, Schneider-Mizell et al. [<xref ref-type="bibr" rid="R32">32</xref>]). We did not use neurons with expert labels to train GRAPHDINO, but used them only for evaluation.</p></sec><sec id="S19"><label>4.5</label><title>Morphological feature learning using GRAPHDINO</title><p id="P45">For learning morphological features in an unsupervised, purely data-driven way, we used a recently developed machine learning method called GRAPHDINO [<xref ref-type="bibr" rid="R42">42</xref>]. GRAPHDINO maps the skeleton graph of a neuron onto a 32-dimensional feature vector, which we colloquially refer to as the neuron’s “bar code”. For training GRAPHDINO, each neuron’s skeleton was represented as an undirected graph <italic>G</italic> = (<italic>V, E</italic>). <italic>V</italic> is the set of nodes <inline-formula><mml:math id="M1"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <italic>E</italic> the set of undirected edges <italic>E</italic> = {<italic>e<sub>ij</sub></italic> = (<italic>v<sub>i</sub></italic>, <italic>v<sub>j</sub></italic>)} that connect two nodes <italic>v<sub>i</sub></italic>, <italic>v<sub>j</sub></italic>. Each node has a feature vector attached to it that holds the 3d Cartesian coordinate of the node, relative to the soma of the neuron. The soma has the coordinate (0, 0, 0), i.e. is at the origin of the coordinate system. Because axons have not been reconstructed well in the data yet, we focused on the dendritic skeleton only and removed segments labeled as axon. We trained GRAPHDINO on a subset of the dataset, retaining 5,113 neurons for validation and 2,941 neurons for testing. The test set was chosen to contain the 1,011 neurons that were labeled by expert anatomists into morphological cell types (<xref ref-type="sec" rid="S18">Sec. 4.4</xref>; [<xref ref-type="bibr" rid="R32">32</xref>]), while the other 1,930 neurons were i.i.d. sampled. The training and validation sets were i.i.d. sampled from the remaining neurons with a 90%-10% split (<xref ref-type="supplementary-material" rid="SD1">Fig. A.4</xref>).</p><p id="P46">GRAPHDINO is trained by generating two “views” of the same input graph by applying random identity-preserving transformations (described below). These two views are both encoded by the same neural network. The training objective is to maximize the similarity between the embeddings of these two views. To obtain the two views of one input graph, we subsampled the graph, randomly rotated it around the <italic>y</italic>-axis (orthogonal to pia), dropped subbranches and perturbed node locations. When subsampling the graph, we randomly dropped all but 200 nodes, always retaining the branching points. Rotations around the y-axis were uniformly distributed around the circle. During subbranch deletion we removed <italic>n</italic> = 5 subbranches. For node location jittering, we used <italic>σ</italic> = 1. In addition, the entire graph was randomly translated with <italic>σ</italic> =1. For further details on the augmentation strategies, see Weis et al. [<xref ref-type="bibr" rid="R42">42</xref>].</p><p id="P47">The ADJACENCY-CONDITIONED ATTENTION network architecture had seven AC-ATTENTION layers with four attention heads each. The dimensionality of the latent representation <inline-formula><mml:math id="M2"><mml:mrow><mml:mi>z</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> was set to <italic>d</italic><sub>1</sub> = 32 and the dimensionality of the projection <inline-formula><mml:math id="M3"><mml:mrow><mml:mi>p</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> was <italic>d</italic><sub>2</sub> = 5, 000. All other architecture details are as described in the original paper [<xref ref-type="bibr" rid="R42">42</xref>]. For training, we used the Adam optimizer [<xref ref-type="bibr" rid="R18">18</xref>] with a batch size of 128 for 50, 000 iterations. The learning rate was linearly increased to 10<sup>−3</sup> during the first 1,000 iterations and then decayed using an exponential schedule with a decay rate of 0.5.</p><p id="P48">We ran ablation experiments using different dimensionalities for the latent space <italic>d</italic><sub>1</sub> ∈ {16, 32, 64, 128} and varied the number of training iterations <italic>i</italic> ∈ {25 000, 50 000, 100 000, 200 000} (<xref ref-type="supplementary-material" rid="SD1">Fig. A.6</xref>). Additionally, we replaced the cross-entropy loss with the contrastive SIMCLR loss [<xref ref-type="bibr" rid="R5">5</xref>] and trained variants with different mini-batch size <italic>b</italic> ∈ {128, 1024, 2048} (<xref ref-type="supplementary-material" rid="SD1">Fig. A.7</xref>), as contrastive losses have been shown to be sensitive to the number of negative samples used in the loss [<xref ref-type="bibr" rid="R5">5</xref>]. Training with <italic>b</italic> = 2, 048 diverged.</p></sec><sec id="S20"><label>4.6</label><title>Morphological clustering</title><p id="P49">For qualitative inspection of the data and the analyses in <xref ref-type="fig" rid="F6">Fig. 6B</xref> and <xref ref-type="fig" rid="F7">Fig. 7B</xref>, we clustered the neurons using the learned vector embedding of each neuron’s morphological features. We fit a Gaussian Mixture model (GMM) with diagonal covariance matrix using scipy [<xref ref-type="bibr" rid="R27">27</xref>] on the whole dataset as well as per cortical layer using 60 clusters and 15 clusters, respectively. As we found no evidence that these clusters (or any other clustering with fewer or more clusters) represent distinct cell types, we did not use this clustering to define cell types, but rather think of them as modes or representing groups of neurons with similar morphological features.</p></sec><sec id="S21"><label>4.7</label><title>Data quality control steps</title><p id="P50">The dataset was generated by automatic segmentation of EM images and subsequent automatic processing into skeletons. As a consequence, not all cells are reconstructed perfectly. There is a significant fraction of wrongly merged or incompletely segmented cells. We used a combination of our learned GRAPHDINO embeddings and supervised classifiers trained on a subset of the neurons (<italic>n</italic> = 1, 011) which were manually proofread and annotated by experts (see <xref ref-type="sec" rid="S18">Sec. 4.4</xref> and companion paper, Schneider-Mizell et al. [<xref ref-type="bibr" rid="R32">32</xref>]). Our quality control pipeline was as follows: First, we computed GRAPHDINO embeddings on the full dataset of 54,192 neurons (including both excitatory and inhibitory neurons). Next, we removed neurons which are close to the boundaries of the volume, as these neurons are only partly reconstructed. After this step, we were left with 43,666 neurons. Within this dataset we identified neurons which are incorrectly reconstructed using a supervised classifier described in the next section, reducing the dataset to 37,362 neurons. Subsequently, we identified interneurons using a supervised classifier described in the next section, reducing the dataset to 33,997 excitatory neurons. Finally, on this dataset we manually proofread around 480 atufted neurons. As a result, we identify and remove another set of 2,684 neurons whose reconstructions were incomplete, leaving us with a final sample size of 31,313 putative excitatory and correctly reconstructed neurons for our main analyses.</p></sec><sec id="S22"><label>4.8</label><title>Supervised classifiers</title><p id="P51">To identify reconstruction errors and interneurons, we used a subset of the dataset (<italic>n</italic> = 1, 011) that was manually proofread and annotated with cell type labels by experts (see <xref ref-type="sec" rid="S18">Sec. 4.4</xref> and companion paper, Schneider-Mizell et al. [<xref ref-type="bibr" rid="R32">32</xref>]). Based on these and additional neurons we identified as segmentation errors, we trained classifiers to detect segmentation errors, inhibitory cells and cortical layer membership using our learned 32-dimensional vector embeddings of the neurons’ skeletons (see <xref ref-type="sec" rid="S19">Sec. 4.5</xref>). In our subsequent analysis, we focused on neurons that were identified as complete and excitatory by our classifier. We used the inferred cortical layer labels to perform layer-specific analyses.</p><p id="P52">For all classifiers, we used ten-fold cross-validation on a grid search to find the best hyperparameters. We tested logistic regression with the following hyperparameters: type of regularization (none, L1, L2 or elastic net), regularization weight <italic>C</italic> ∈ 0.5, 1, 3, 5, 10, 20, 30 and whether to use class weights that are inversely proportional to class frequencies or no class weights. In addition, we tested support vector machines (SVMs) with the following hyperparameters: type of kernel (Linear, RBF or polynomial), L2 regularization weight <italic>C</italic> ∈ 0.5, 1, 3, 5, 10, 20, 30 and degree of polynomial <italic>d</italic> ∈ 2, 3, 5, 7, 10, 20 for the polynomial kernel and whether to use class weights or no weights. After having determined the optimal hyperparameters using cross-validation, we retrained the classifier using the optimal hyperparameters on its entire labeled set.</p><sec id="S23"><title>Removal of fragmented neurons</title><p id="P53">To remove fragmented neurons prior to analysis, we trained a classifier to differentiate between the manually proofread neurons from all layers (<italic>n</italic> = 1, 011) and fragmented cells (<italic>n</italic> = 240). We identified fragmented cells using our clustering of the vector embeddings of the whole dataset without boundary neurons (<italic>n</italic> = 43, 666) into 25 clusters per layer and manually identify clusters that contained fragmented cells (2–3 clusters per layer). We then sampled 60 fragmented cells per layer as training data for our classifier.</p><p id="P54">We trained a support vector machine (SVM) using cross-validation as described above. Its cross-validated accuracy was 95% (<xref ref-type="supplementary-material" rid="SD1">Fig. A.3A</xref>). The best hyperparameters were: polynomial kernel of degree 4 and <italic>C</italic> = 3. We used those hyperparameters to retrain the classifier on the full training set of 1,251 neurons. Using this classifier, we inferred whether a neuron is fragmented for the entire dataset (<italic>n</italic> = 43, 666). We then removed cells predicted to be fragmented (<italic>n</italic> = 6, 304) from subsequent analyses.</p><p id="P55">To validate the classification into fragmented and whole cells, we manually inspected ten neurons that were not in “fragmented” clusters before classification, but were flagged as fragmented by the classifier. Nine out of ten had missing segments due to segmentation errors or due to apical dendrites leaving the volume.</p></sec><sec id="S24"><title>Removal of inhibitory neurons</title><p id="P56">Analogously, we trained a classifier to predict whether a neuron is excitatory or inhibitory by using the manually proofread and annotated neurons (<italic>n</italic> = 1, 011) (<xref ref-type="sec" rid="S18">Sec. 4.4</xref>). As input features to the classifier we used our learned embeddings and additionally two morphometric features: synaptic density on apical shafts (number of synapses per micrometer of skeletal length except those located on spines) and spine density (number of spines per micrometer of skeletal length). These two features have been shown to separate excitatory from inhibitory neurons well in previous work (see companion paper, Celii et al. [<xref ref-type="bibr" rid="R4">4</xref>]). The annotated dataset contains 922 excitatory and 89 inhibitory neurons.</p><p id="P57">We trained a logistic regression. Its cross-validated accuracy was 99% (<xref ref-type="supplementary-material" rid="SD1">Fig. A.3B</xref>). The best hyperparameters were: L2 regularization (<italic>C</italic> = 5) and using class weights. We used those hyperparameters to retrain on the full training set of 1,011 neurons. Using this classifier, we inferred whether a neuron is excitatory or inhibitory for the entire dataset after removing fragmented cells and after the removal of 227 neurons that do not have spine and synapse densities available (<italic>n</italic> = 37, 135). We then removed all inhibitory cells from subsequent analyses (<italic>n</italic> = 3, 138).</p></sec><sec id="S25"><title>Inference of cortical layers</title><p id="P58">To determine cortical layer labels for the entire dataset, we followed a two-stage procedure. First, we inferred the layer of each neuron using a trained classifier. Then we determined anatomical layer boundaries based on the optimal cortical depth that separates adjacent layers.</p><p id="P59">We first trained a SVM classifier for excitatory cells on the 922 manually annotated excitatory neurons by pooling the cell type labels per layer. Its cross-validated balanced accuracy was 89% (<xref ref-type="supplementary-material" rid="SD1">Fig. A.3C</xref>). The best hyperparameters were: polynomial kernel of degree 5, <italic>C</italic> = 3. Using this classifier, we inferred the cortical layer of all excitatory neurons (<italic>n</italic> = 33, 997; <xref ref-type="fig" rid="F2">Fig. 2</xref>).</p><p id="P60">The spatial distribution of inferred layer assignments was overall well confined to their respective layers. As expected, there was some spatial overlap of labels at the boundaries, since layer boundaries are not sharp. We nevertheless opted for assigned neurons to layers based on their anatomical location rather than their inferred label. To do so, we determined the optimal piece-wise linear function that separated two consecutive layers. Thus, the layer assignments used for subsequent analyses were purely based on the soma depth of each neuron relative to the inferred layer boundaries – not on the classifier output.</p></sec><sec id="S26"><title>Inference of coarse cell type labels</title><p id="P61">In <xref ref-type="fig" rid="F5">Fig. 5</xref> we show cell type labels for layer 5. These were determined by training a SVM to classify the excitatory neurons into cell types using the 922 manually annotated neurons. The cross-validated balanced accuracy of this classifier was 85% (<xref ref-type="supplementary-material" rid="SD1">Fig. A.3D</xref>). The best hyperparameters were: polynomial kernel of degree 2, <italic>C</italic> = 20, using class weights. Using this classifier, we inferred cell type labels for all excitatory neurons (<italic>n</italic> = 33, 997).</p></sec></sec><sec id="S27"><label>4.9</label><title>Manual validation of apical skeletons</title><p id="P62">We found a significant fraction of atufted neurons across layers 4–6. To determine the extent to which these cells are actually atufted or an artifact of incomplete reconstructions, we manually inspected 479 neurons in Neuroglancer [<xref ref-type="bibr" rid="R9">9</xref>] with respect to the validity of their apical termination. During manual inspection, we annotated neurons’ reconstruction as “naturally terminating,” “out-of-bounds,” “reconstruction issue” or “unsegmented region.” Reconstruction issues were the case where the EM slice was segmented correctly, but the tracing missed to connect two parts of the same neuron. Unsegmented regions were the case where one or multiple EM images or parts thereof were not segmented correctly and therefore the neuron could not be traced correctly. In addition, we classified the neurons as either “atufted,” “small tufted” or “tufted,” both before validation and after correcting reconstruction errors.</p><p id="P63">For layer 4, we inspected 120 atufted neurons. Of those, 64% have missing segments on their apical dendrites and 36% have a natural termination. Note, however, that 74% of the neurons had a consistent tuft before and after validation. Even though parts of the apical dendrite were missing, qualitatively the degree of tuftedness did not change. For atufted neurons this means that their apical dendrite merely terminated early, but this reconstruction error did not change their classification as atufted. In layer 4, neurons with a natural termination end more superficially than neurons with missing segments. We therefore excluded L4 neurons from the analysis whose apicals end more than 154 micrometers below the pia to exclude neurons with reconstruction errors from our analysis. This threshold was selected such that the F1-score is maximized, i.e. retaining as many atufted neurons with natural termination, while removing as many neurons with missing segments as possible. The threshold was computed on the 120 validated neurons. This process excludesd 557 neurons from layer 4.</p><p id="P64">For layer 5, we inspected 176 neurons with early-terminating apical dendrites. Of those, 59 showed a natural apical termination, while 117 had reconstruction issues or left the volume. We found no clear quantitative metric like the depth of the apical to exclude neurons with unnatural terminations. Therefore, we excluded neurons based on their cluster membership from further analysis if the cluster contained more than 50% of neurons with unnatural terminations. Of the 15 clusters, we excluded four, corresponding to 1,258 out of 5,858 L5 neurons.</p><p id="P65">For layer 6, we inspected 183 neurons with early terminating apicals. Of those, 100 showed a natural apical termination, while 83 had reconstruction issues or left the volume. Due to the slant of the volume, long, narrow L6 cells near the volume boundary have a high likelihood of leaving the boundary with their apical dendrite. Therefore, we excluded all L6 neurons whose apical dendrite left the volume (n = 867) prior to our analysis. We considered a neuron as leaving the volume if the most superficial point of its apical tree is within a few micrometers of the volume boundary.</p><p id="P66">Overall, we excluded 2,684 neurons as a result of this manual validation step, resulting in a final sample size of 31,313 neurons used in our analysis (<xref ref-type="fig" rid="F5">Figs. 5</xref>+<xref ref-type="fig" rid="F6">6</xref>+<xref ref-type="fig" rid="F7">7</xref>).</p></sec><sec id="S28"><label>4.10</label><title>Cortical area boundaries</title><p id="P67">Cortical area boundaries were manually drawn from retinotopic maps of visual cortex taken before EM imaging. For further details see companion paper [<xref ref-type="bibr" rid="R6">6</xref>].</p></sec><sec id="S29"><label>4.11</label><title>Dimensionality reduction</title><p id="P68">For visualization of the learned embeddings, we reduced the dimensionality of the 32d embedding vector to 2d using t-distributed stochastic neighbor embedding (t-SNE; [<xref ref-type="bibr" rid="R37">37</xref>]) using the openTSNE package [<xref ref-type="bibr" rid="R28">28</xref>] with cosine distance and a perplexity of 30 for t-SNE plots of individual cortical layers and a perplexity of 300 for the whole dataset. The perplexity of t-SNE needs to be set dependent on the dataset size. We followed the recommendation of Kobak and Berens [<xref ref-type="bibr" rid="R19">19</xref>] of setting it to <italic>perplexity</italic> = <italic>n</italic>/100, which led to the approximate perplexity of 300 for our dataset of <italic>n</italic> = 33, 997 excitatory cells. However, to show that our interpretation is not restricted to this specific perplexity we visualized additional runs with <italic>p</italic> ∈ {30, 100, 1 000} (<xref ref-type="supplementary-material" rid="SD1">Fig. A.8</xref>). Additionally, we used UMAP [<xref ref-type="bibr" rid="R23">23</xref>] and PaCMAP [<xref ref-type="bibr" rid="R40">40</xref>] with different number of neighbors <italic>p</italic> ∈ {30, 100, 300, 1 000} to show that our interpretation is not dependent on the use of t-SNE (<xref ref-type="supplementary-material" rid="SD1">Fig. A.8</xref>).</p></sec><sec id="S30"><label>4.12</label><title>Morphometric descriptors</title><p id="P69">We computed morphometrics based on the neuronal skeletons for the analysis of the learned latent space. Morphometrics were not used for learning the morphological vector embeddings. We computed morphometrics based on compartment labels: soma, apical dendrites, basal dendrites and oblique dendrites (<xref ref-type="sec" rid="S16">Sec. 4.2</xref>). They are visualized in <xref ref-type="fig" rid="F4">Fig. 4</xref>. TOTAL APICAL LENGTH is defined as the total length of all segments of the skeletons that are classified as apical dendrites. TOTAL BASAL LENGTH is computed analogously. DEPTH refers to the depth of the soma centroid relative to the pia after volume normalization (<xref ref-type="sec" rid="S17">Sec. 4.3</xref>), where pia depth is equal to zero. HEIGHT is the absolute difference between the highest and the lowest skeleton node of a neuron in <italic>y</italic>-direction. APICAL WIDTH refers to the widest extent of apical dendrites in the (<italic>x</italic>, <italic>z</italic>)-plane. BASAL BIAS describes the difference between the soma depth and the center of mass of the basal dendrites along the <italic>y</italic>-axis. Due to the dataset size, compartment labeling was done automatically (see companion paper [<xref ref-type="bibr" rid="R4">4</xref>]). However, identifying apical dendrites rule-based does not work well for all neurons. For instance, it fails for the inverted L6 neurons [<xref ref-type="bibr" rid="R4">4</xref>]. For <xref ref-type="fig" rid="F5">Fig. 5</xref>, we removed neurons for which the automatic morphometric pipeline failed. For layer 2/3 10,196 of 10,564 neurons are included in the analysis, for layer 4 7,751 of 7,775, for layer 5 4,443 of 4,600 and for layer 6 8,274 of 8,374. The GRAPHDINO feature space has the advantage of being independent of knowing which branches are apical and which are basal dendrites. However, our downstream analysis relies on it in certain parts (<xref ref-type="fig" rid="F5">Fig. 5</xref>+<xref ref-type="fig" rid="F6">Fig. 6</xref>+<xref ref-type="fig" rid="F7">Fig. 7</xref>).</p></sec><sec id="S31"><label>4.13</label><title>Statistics</title><p id="P70">Apical lengths in <xref ref-type="sec" rid="S10">Sec. 2.8</xref> were compared between V1 and HVA per laminar layer with four independent two-tailed Student’s t-tests. The single-test significance level of 0.01 was corrected to 0.0025 for multiple tests using Bonferroni correction. Only neurons that have any nodes labeled as apical were taken into account for this analysis. In L2/3, <italic>n</italic> = 6, 760 neurons were taken into account from V1 and <italic>n</italic> = 3, 436 from HVA; for L4 <italic>n</italic> = 5, 217 (V1) and <italic>n</italic> = 2, 534 (HVA); for L5 <italic>n</italic> = 3, 708 (V1) and <italic>n</italic> = 1, 924 (HVA); and for L6 <italic>n</italic> = 3, 959 (V1) and <italic>n</italic> = 2, 618 (HVA).</p></sec><sec id="S32"><label>4.14</label><title>Cluster analysis</title><sec id="S33"><title>Generation of synthetic data</title><p id="P71">To obtain synthetic data distributions that are close to the neuronal data, we first fit Gaussian mixture models (GMMs) with the number of components <italic>n</italic> ∈ {10, 20, 40} and diagonal covariance matrices to the neuronal embeddings, extracting cluster means and weights of the fit mixture components. Using these, we subsequently generated synthetic data from Gaussian mixtures with isotropic covariance matrices with increasing variances spanning the space from distinctly separated clusters to continuous distributions (<xref ref-type="fig" rid="F3">Fig. 3B</xref> &amp; <xref ref-type="supplementary-material" rid="SD1">Fig. A.5</xref>).We used variances <italic>σ</italic><sup>2</sup> ∈ {0.005, 0.01, 0.03, 0.05, 0.07, 0.1, 0.3, 0.5, 0.7, 1.0} for each number of components <italic>n</italic> ∈ {10, 20, 40}, resulting in 27 synthetic datasets. For each Gaussian mixture, we drew 33,997 samples equivalent to the number of excitatory neurons. Samples were 32-dimensional like the morphological embeddings.</p></sec><sec id="S34"><title>ARI analysis</title><p id="P72">To judge whether the correct number of clusters can be recovered, we split the data (both synthetic datasets and the neuronal data) into training and validation data (90%:10% split). For each synthetic dataset and the neuronal data, we fit 100 GMMs with number of components ∈ {7, 10, 15, 20, 40, 60, 80} and isotropic covariance matrix. We then computed the pairwise adjusted rank index (ARI) between the different clustering runs for the same number of components and report the average ARI on the validation set (<xref ref-type="fig" rid="F3">Fig. 3B</xref> &amp; <xref ref-type="supplementary-material" rid="SD1">Fig. A.5</xref>). All visualizations show clustering runs with the best log-likelihood score on the validation set (<xref ref-type="fig" rid="F3">Fig. 3</xref>).</p></sec><sec id="S35"><title>Unimodality versus bimodality of neighboring clusters</title><p id="P73">To examine if two neighboring clusters (neighboring in terms of least Euclidean distance between cluster means) form rather a uni- or bimodal distribution, we first projected the samples of the two clusters onto the line connecting the two cluster means. We then visualized the 1d histogram as well as the cumulative distribution function (CDF) of the samples from both clusters. Additionally, we computed the dip statistic [<xref ref-type="bibr" rid="R12">12</xref>] to quantify how close two neighboring clusters are to forming a unimodal distribution.<sup><xref ref-type="fn" rid="FN3">1</xref></sup> We scaled the dip statistic with a factor of 4 such that the extreme case of two delta distributions at <italic>x</italic><sub>i</sub> and <italic>x</italic><sub>j</sub> with <italic>i</italic> ≠ <italic>j</italic> result in dip = 1. As exemplified by the synthetic data, when neighboring clusters evolve from discrete clusters to forming a continuum, the dip statistic decreases and the CDF forms a smooth curve (<xref ref-type="fig" rid="F3">Fig. 3B</xref>, grey insets 1–6).</p></sec><sec id="S36"><title>Connectivity graph</title><p id="P74">For each cluster of the Gaussian mixture model with 20 components of the neuronal data, we computed the dip statistic to its three nearest neighbors based on Euclidean distance in the 32-dimensional embedding space. We thresholded the neighbor selection by the average distance of all clusters to their third-nearest neighbor to avoid including spurious connections between clusters that do not have any close neighbors (threshold = 2.38 Euclidean distance in latent space). The line width of the graph (<xref ref-type="fig" rid="F3">Fig. 3F</xref>) was determined as the inverse dip statistic between the nearest neighbors. Additionally, we computed the maximum dip statistic between all clusters and their nearest neighbor for the neuronal data and the synthetic datasets (<xref ref-type="fig" rid="F3">Fig. 3G</xref>).</p></sec></sec><sec id="S37"><label>4.15</label><title>Prediction of morphological features from functional bar codes</title><p id="P75">The MICRONS dataset encompasses EM images as well as Calcium imaging of the same portion of the visual cortex of one mouse [<xref ref-type="bibr" rid="R6">6</xref>]. The companion paper by Wang et al. [<xref ref-type="bibr" rid="R38">38</xref>] created a digital twin of the functional properties of the neurons from the Calcium imaging data (<xref ref-type="fig" rid="F7">Fig. 7C</xref>). We used the resulting functional embeddings of the neurons as input features to a linear regression model to predict the basal bias metric of the layer 4 neurons, thereby predicting a morphological feature from the functional properties of the neurons. There are 2,347 L4 neurons in V1 with both functional and morphological data available. We performed nested cross-validation to select hyperparameters and report test set performance using 10-fold cross validation for the inner and the outer loop. To select hyperparameters a grid search over regularization strength <italic>α</italic> ∈ {0.01, 0.1, 0.5, 1, 5, 10} as well as L1 to L2 ratio ∈ {0, 0.25, 0.5, 0.75, 1.0} was performed. The best model had a <italic>R</italic><sup>2</sup>-score of 0.17 and ground truth and predicted basal bias had a Pearson correlation of 0.41 (<xref ref-type="fig" rid="F7">Fig. 7D</xref>, <italic>p</italic> &lt; 10<sup>−10</sup>). To control for soma depth as a confounder, we repeated the analysis predicting the basal bias from the soma depth as well as from the functional embeddings in addition to the soma depth, resulting in <italic>R</italic><sup>2</sup> = 0.28 for both predictors vs. 0.21 for depth only (<italic>ρ</italic> = 0.53, <italic>p</italic> &lt; 10<sup>−10</sup> and <italic>ρ</italic> = 0.46, <italic>p</italic> &lt; 10<sup>−10</sup>, respectively). We tested the difference in the correlation coefficients using a two tailed Fisher’s z-test resulting in a significant difference between the two (<italic>p</italic> = 0.0015).</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Appendix</label><media xlink:href="EMS158985-supplement-Appendix.pdf" mimetype="application" mime-subtype="pdf" id="d15aAdEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S38"><title>Acknowledgements</title><p>M.A.W. was supported by the International Max Planck Research School for Intelligent Systems (IMPRS-IS), Tübingen. This work was supported by the European Research Council (ERC) under the European Union’s Horizon Europe research and innovation programme (Grant agreement No. 101041669). The work was supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) contract number D16PC00003, D16PC00004, and D16PC0005. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/IBC, or the U.S. Government. The authors thank David Markowitz, the IARPA MICrONS Program Manager, who coordinated this work during all three phases of the MICrONS program. We thank IARPA program managers Jacob Vogelstein and David Markowitz for co-developing the MICrONS program. We thank Jennifer Wang, IARPA SETA for her assistance. We also thank the Allen Institute for Brain Science founder, Paul G. Allen, for his vision, encouragement and support. This work was also supported by the National Institute of Mental Health under Award Numbers R01 MH109556, P30EY002520, the NSF NeuroNex program through grant NSF-1707400, the National Institute of Mental Health and National Institute of Neurological Disorders And Stroke under Award Number U19MH114830, and the National Eye Institute award number R01 EY026927 as well as Core Grant for Vision Research T32-EY-002520-37.</p></ack><sec id="S39" sec-type="data-availability"><title>Data availability</title><p id="P76">Data for this paper was analyzed at materialization version 374. Data is publicly available via <ext-link ext-link-type="uri" xlink:href="https://www.microns-explorer.org/cortical-mm3">https://www.microns-explorer.org/cortical-mm3</ext-link> and will be updated closer to publication.</p></sec><sec id="S40" sec-type="code-availability"><title>Code availability</title><p id="P77">The code for GRAPHDINO is available at <ext-link ext-link-type="uri" xlink:href="https://eckerlab.org/code/weis2021b/">https://eckerlab.org/code/weis2021b/</ext-link>. Analysis code will be made available on the Eckerlab Github repository (forthcoming). Analyses were performed in Python 3.10 using custom code and the libraries Matplotlib362, Numpy124, openTSNE062, Pandas152, Pytorch113, Scikit-learn120, Scipy110, and Seaborn012 for general computation, machine learning and data visualization.</p></sec><fn-group><fn id="FN1" fn-type="con"><p id="P78"><bold>Author Contributions</bold></p><p id="P79">We use the CRediT system for author roles. Conceptualization: ASE, MAW, AST, PB. Methodology: MAW, PB, ASE, EYW. Software: MAW, SP, TL. Validation: MAW, SP. Formal analysis: MAW, SP, LH. Investigation: MAW, SP, BC. Resources: BC, PGF, JAB, ALB, DB, JB, DJB, MAC, FC, NMdC, SD, LE, AH, ZJ, CJ, DK, NK, SK, KiL, KaL, RL, TM, GM, EM, SSM, SM, BN, SP, RCR, CMSM, HSS, WS, MT, RT, NLT, WW, JW, WY, SY. Data curation: MAW, SP, BC. Writing - Original draft: MAW, SP, ASE. Writing - Review &amp; editing: MAW, SP, LH, TL, AST, ASE. Visualization: MAW, SP, TL. Supervision: ASE, AST, JR. Project administration: ASE. Funding acquisition: ASE, AST, JR.</p></fn><fn id="FN2" fn-type="conflict"><p id="P80"><bold>Conflict of Interest</bold></p><p id="P81">A.S.T is cofounder of Vathes Inc., and UploadAI LLC companies in which he has financial interests. J.R. is co founder of Vathes Inc., and UploadAI LLC companies in which he has financial interests. A.S.E. is cofounder of Maddox AI GmbH, in which he has financial interests. TM and HSS disclose financial interests in Zetta AI LLC.</p></fn><fn id="FN3"><label>1</label><p id="P82">Dip statistic was computed using the python package <italic>diptest</italic> (<ext-link ext-link-type="uri" xlink:href="https://pypi.org/project/diptest/">https://pypi.org/project/diptest/</ext-link>).</p></fn></fn-group><ref-list><ref id="R1"><label>[1]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berg</surname><given-names>Jim</given-names></name><name><surname>Sorensen</surname><given-names>Staci A</given-names></name><name><surname>Ting</surname><given-names>Jonathan T</given-names></name><name><surname>Miller</surname><given-names>Jeremy A</given-names></name><name><surname>Chartrand</surname><given-names>Thomas</given-names></name><name><surname>Buchin</surname><given-names>Anatoly</given-names></name><name><surname>Bakken</surname><given-names>Trygve E</given-names></name><name><surname>Budzillo</surname><given-names>Agata</given-names></name><name><surname>Dee</surname><given-names>Nick</given-names></name><name><surname>Ding</surname><given-names>Song-Lin</given-names></name><name><surname>Gouwens</surname><given-names>Nathan W</given-names></name><etal/></person-group><article-title>Human neocortical expansion involves glutamatergic neuron diversification</article-title><source>Nature</source><year>2021</year><volume>598</volume><issue>7879</issue><fpage>151</fpage><lpage>158</lpage><pub-id pub-id-type="pmcid">PMC8494638</pub-id><pub-id pub-id-type="pmid">34616067</pub-id><pub-id pub-id-type="doi">10.1038/s41586-021-03813-8</pub-id></element-citation></ref><ref id="R2"><label>[2]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cadwell</surname><given-names>Cathryn</given-names></name><name><surname>Palasantza</surname><given-names>Athanasia</given-names></name><name><surname>Jiang</surname><given-names>Xiaolong</given-names></name><name><surname>Berens</surname><given-names>Philipp</given-names></name><name><surname>Deng</surname><given-names>Qiaolin</given-names></name><name><surname>Yilmaz</surname><given-names>Marlene</given-names></name><name><surname>Reimer</surname><given-names>Jacob</given-names></name><name><surname>Shen</surname><given-names>Shan</given-names></name><name><surname>Bethge</surname><given-names>Matthias</given-names></name><name><surname>Tolias</surname><given-names>Kimberley</given-names></name><name><surname>Sandberg</surname><given-names>Rickard</given-names></name><etal/></person-group><article-title>Electrophysiological, transcriptomic and morphologic profiling of single neurons using patch-seq</article-title><source>Nature Biotechnology</source><year>2015</year><volume>34</volume><pub-id pub-id-type="pmcid">PMC4840019</pub-id><pub-id pub-id-type="pmid">26689543</pub-id><pub-id pub-id-type="doi">10.1038/nbt.3445</pub-id></element-citation></ref><ref id="R3"><label>[3]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Caron</surname><given-names>Mathilde</given-names></name><name><surname>Touvron</surname><given-names>Hugo</given-names></name><name><surname>Misra</surname><given-names>Ishan</given-names></name><name><surname>Jégou</surname><given-names>Hervé</given-names></name><name><surname>Mairal</surname><given-names>Julien</given-names></name><name><surname>Bojanowski</surname><given-names>Piotr</given-names></name><name><surname>Joulin</surname><given-names>Armand</given-names></name></person-group><source>Emerging properties in self-supervised vision transformers</source><conf-name>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</conf-name><year>2021</year><fpage>9650</fpage><lpage>9660</lpage></element-citation></ref><ref id="R4"><label>[4]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Celii</surname><given-names>Brendan</given-names></name><name><surname>Papadopoulos</surname><given-names>Stelios</given-names></name><name><surname>Ding</surname><given-names>Zhuokun</given-names></name><name><surname>Fahey</surname><given-names>Paul G</given-names></name><name><surname>Wang</surname><given-names>Eric</given-names></name><name><surname>Papadopoulos</surname><given-names>Christos</given-names></name><name><surname>Kunin</surname><given-names>Alexander</given-names></name><name><surname>Patel</surname><given-names>Saumil</given-names></name><name><surname>Bae</surname><given-names>J Alexander</given-names></name><name><surname>Bodor</surname><given-names>Agnes L</given-names></name><name><surname>Brittain</surname><given-names>Derrick</given-names></name><etal/></person-group><article-title>Neurd: A mesh decomposition framework for automated proofreading and morphological analysis of neuronal em reconstructions</article-title><source>bioRxiv</source><year>2023</year><pub-id pub-id-type="doi">10.1101/2023.03.14.532674</pub-id></element-citation></ref><ref id="R5"><label>[5]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Ting</given-names></name><name><surname>Kornblith</surname><given-names>Simon</given-names></name><name><surname>Norouzi</surname><given-names>Mohammad</given-names></name><name><surname>Hinton</surname><given-names>Geoffrey</given-names></name></person-group><source>A simple framework for contrastive learning of visual representations</source><conf-name>Proc of the International Conf on Machine learning (ICML)</conf-name><year>2020</year></element-citation></ref><ref id="R6"><label>[6]</label><element-citation publication-type="journal"><collab>MICrONS Consortium</collab><person-group person-group-type="author"><name><surname>Bae</surname><given-names>J Alexander</given-names></name><name><surname>Baptiste</surname><given-names>Mahaly</given-names></name><name><surname>Bodor</surname><given-names>Agnes L</given-names></name><name><surname>Brittain</surname><given-names>Derrick</given-names></name><name><surname>Buchanan</surname><given-names>JoAnn</given-names></name><name><surname>Bumbarger</surname><given-names>Daniel J</given-names></name><name><surname>Castro</surname><given-names>Manuel A</given-names></name><name><surname>Celii</surname><given-names>Brendan</given-names></name><name><surname>Cobos</surname><given-names>Erick</given-names></name><name><surname>Collman</surname><given-names>Forrest</given-names></name><name><surname>da Costa</surname><given-names>Nuno Macarico</given-names></name><etal/></person-group><article-title>Functional connectomics spanning multiple areas of mouse visual cortex</article-title><source>bioRxiv</source><year>2021</year><pub-id pub-id-type="doi">10.1101/2021.07.28.454025</pub-id></element-citation></ref><ref id="R7"><label>[7]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DeFelipe</surname><given-names>Javier</given-names></name><name><surname>López-Cruz</surname><given-names>Pedro L</given-names></name><name><surname>Benavides-Piccione</surname><given-names>Ruth</given-names></name><name><surname>Bielza</surname><given-names>Concha</given-names></name><name><surname>Larrañaga</surname><given-names>Pedro</given-names></name><name><surname>Anderson</surname><given-names>Stewart</given-names></name><name><surname>Burkhalter</surname><given-names>Andreas</given-names></name><name><surname>Cauli</surname><given-names>Bruno</given-names></name><name><surname>Fairén</surname><given-names>Alfonso</given-names></name><name><surname>Feldmeyer</surname><given-names>Dirk</given-names></name><etal/></person-group><article-title>New insights into the classification and nomenclature of cortical GABAergic interneurons</article-title><source>Nature Reviews Neuroscience</source><year>2013</year><volume>14</volume><issue>3</issue><fpage>202</fpage><lpage>216</lpage><pub-id pub-id-type="pmcid">PMC3619199</pub-id><pub-id pub-id-type="pmid">23385869</pub-id><pub-id pub-id-type="doi">10.1038/nrn3444</pub-id></element-citation></ref><ref id="R8"><label>[8]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elabbady</surname><given-names>Leila</given-names></name><name><surname>Seshamani</surname><given-names>Sharmishtaa</given-names></name><name><surname>Mu</surname><given-names>Shang</given-names></name><name><surname>Mahalingam</surname><given-names>Gayathri</given-names></name><name><surname>Schneider-Mizell</surname><given-names>Casey</given-names></name><name><surname>Bodor</surname><given-names>Agnes</given-names></name><name><surname>Bae</surname><given-names>J Alexander</given-names></name><name><surname>Brittain</surname><given-names>Derrick</given-names></name><name><surname>Buchanan</surname><given-names>JoAnn</given-names></name><name><surname>Bumbarger</surname><given-names>Daniel J</given-names></name><name><surname>Castro</surname><given-names>Manuel A</given-names></name><etal/></person-group><article-title>Quantitative census of local somatic features in mouse visual cortex</article-title><source>bioRxiv</source><year>2022</year><pub-id pub-id-type="doi">10.1101/2022.07.20.499976</pub-id></element-citation></ref><ref id="R9"><label>[9]</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Maitin-Shepard</surname><given-names>Jeremy</given-names></name><etal/></person-group><source>google/neuroglancer</source><year>2021</year><comment>URL <ext-link ext-link-type="uri" xlink:href="https://github.com/google/neuroglancer">https://github.com/google/neuroglancer</ext-link></comment></element-citation></ref><ref id="R10"><label>[10]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gouwens</surname><given-names>Nathan</given-names></name><name><surname>Sorensen</surname><given-names>Staci</given-names></name><name><surname>Berg</surname><given-names>Jim</given-names></name><name><surname>Lee</surname><given-names>Changkyu</given-names></name><name><surname>Jarsky</surname><given-names>Tim</given-names></name><name><surname>Ting</surname><given-names>Jonathan</given-names></name><name><surname>Sunkin</surname><given-names>Susan</given-names></name><name><surname>Feng</surname><given-names>David</given-names></name><name><surname>Anastassiou</surname><given-names>Costas</given-names></name><name><surname>Barkan</surname><given-names>Eliza</given-names></name><name><surname>Bickley</surname><given-names>Kris</given-names></name><etal/></person-group><article-title>Classification of electrophysiological and morphological neuron types in the mouse visual cortex</article-title><source>Nature Neuroscience</source><year>2019</year><volume>22</volume><pub-id pub-id-type="pmcid">PMC8078853</pub-id><pub-id pub-id-type="pmid">31209381</pub-id><pub-id pub-id-type="doi">10.1038/s41593-019-0417-0</pub-id></element-citation></ref><ref id="R11"><label>[11]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gouwens</surname><given-names>Nathan W</given-names></name><name><surname>Sorensen</surname><given-names>Staci A</given-names></name><name><surname>Baftizadeh</surname><given-names>Fahimeh</given-names></name><name><surname>Budzillo</surname><given-names>Agata</given-names></name><name><surname>Lee</surname><given-names>Brian R</given-names></name><name><surname>Jarsky</surname><given-names>Tim</given-names></name><name><surname>Alfiler</surname><given-names>Lauren</given-names></name><name><surname>Baker</surname><given-names>Katherine</given-names></name><name><surname>Barkan</surname><given-names>Eliza</given-names></name><name><surname>Berry</surname><given-names>Kyla</given-names></name><name><surname>Bertagnolli</surname><given-names>Darren</given-names></name><etal/></person-group><article-title>Integrated morphoelectric and transcriptomic classification of cortical gabaergic cells</article-title><source>Cell</source><year>2020</year><volume>183</volume><issue>4</issue><fpage>935</fpage><lpage>953</lpage><elocation-id>e19</elocation-id><pub-id pub-id-type="pmcid">PMC7781065</pub-id><pub-id pub-id-type="pmid">33186530</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2020.09.057</pub-id></element-citation></ref><ref id="R12"><label>[12]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hartigan</surname><given-names>JA</given-names></name><name><surname>Hartigan</surname><given-names>PM</given-names></name></person-group><article-title>The dip test of unimodality</article-title><source>The Annals of Statistics</source><year>1985</year><volume>13</volume><issue>1</issue><fpage>70</fpage><lpage>84</lpage><comment>ISSN 00905364</comment></element-citation></ref><ref id="R13"><label>[13]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>Xiaolong</given-names></name><name><surname>Shen</surname><given-names>Shan</given-names></name><name><surname>Cadwell</surname><given-names>Cathryn R</given-names></name><name><surname>Berens</surname><given-names>Philipp</given-names></name><name><surname>Sinz</surname><given-names>Fabian</given-names></name><name><surname>Ecker</surname><given-names>Alexander S</given-names></name><name><surname>Patel</surname><given-names>Saumil</given-names></name><name><surname>Tolias</surname><given-names>Andreas S</given-names></name></person-group><article-title>Principles of connectivity among morphologically defined cell types in adult neocortex</article-title><source>Science</source><year>2015</year><month>November</month><volume>350</volume><issue>6264</issue><fpage>1095</fpage><lpage>9203</lpage><elocation-id>aac9462</elocation-id><comment>ISSN 0036-8075</comment><pub-id pub-id-type="pmcid">PMC4809866</pub-id><pub-id pub-id-type="pmid">26612957</pub-id><pub-id pub-id-type="doi">10.1126/science.aac9462</pub-id></element-citation></ref><ref id="R14"><label>[14]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalmbach</surname><given-names>Brian E</given-names></name><name><surname>Hodge</surname><given-names>Rebecca D</given-names></name><name><surname>Jorstad</surname><given-names>Nikolas L</given-names></name><name><surname>Owen</surname><given-names>Scott</given-names></name><name><surname>Bakken</surname><given-names>Trygve E</given-names></name><name><surname>de Frates</surname><given-names>Rebecca</given-names></name><name><surname>Yanny</surname><given-names>Anna Marie</given-names></name><name><surname>Dalley</surname><given-names>Rachel</given-names></name><name><surname>Graybuck</surname><given-names>Lucas T</given-names></name><name><surname>Daigle</surname><given-names>Tanya L</given-names></name><name><surname>Radaelli</surname><given-names>Cristina</given-names></name><etal/></person-group><article-title>Signature morpho-electric, transcriptomic, and dendritic properties of extratelencephalic-projecting human layer 5 neocortical pyramidal neurons</article-title><source>bioRxiv</source><year>2020</year><pub-id pub-id-type="pmcid">PMC8570452</pub-id><pub-id pub-id-type="pmid">34534454</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2021.08.030</pub-id></element-citation></ref><ref id="R15"><label>[15]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanari</surname><given-names>Lida</given-names></name><name><surname>Dlotko</surname><given-names>Pawel</given-names></name><name><surname>Scolamiero</surname><given-names>Martina</given-names></name><name><surname>Levi</surname><given-names>Ran</given-names></name><name><surname>Shillcock</surname><given-names>Julian C</given-names></name><name><surname>Hess</surname><given-names>Kathryn</given-names></name><name><surname>Markram</surname><given-names>Henry</given-names></name></person-group><article-title>A topological representation of branching neuronal morphologies</article-title><source>Neuroinformatics</source><year>2017</year><volume>16</volume><fpage>3</fpage><lpage>13</lpage><pub-id pub-id-type="pmcid">PMC5797226</pub-id><pub-id pub-id-type="pmid">28975511</pub-id><pub-id pub-id-type="doi">10.1007/s12021-017-9341-1</pub-id></element-citation></ref><ref id="R16"><label>[16]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanari</surname><given-names>Lida</given-names></name><name><surname>Ramaswamy</surname><given-names>Srikanth</given-names></name><name><surname>Shi</surname><given-names>Ying</given-names></name><name><surname>Morand</surname><given-names>Sebastien</given-names></name><name><surname>Meystre</surname><given-names>Julie</given-names></name><name><surname>Perin</surname><given-names>Rodrigo</given-names></name><name><surname>Abdellah</surname><given-names>Marwan</given-names></name><name><surname>Wang</surname><given-names>Yun</given-names></name><name><surname>Hess</surname><given-names>Kathryn</given-names></name><name><surname>Markram</surname><given-names>Henry</given-names></name></person-group><article-title>Objective morphological classification of neocortical pyramidal cells</article-title><source>Cerebral Cortex</source><year>2019</year><volume>29</volume><issue>4</issue><fpage>1719</fpage><lpage>1735</lpage><pub-id pub-id-type="pmcid">PMC6418396</pub-id><pub-id pub-id-type="pmid">30715238</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhy339</pub-id></element-citation></ref><ref id="R17"><label>[17]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>Euiseok J</given-names></name><name><surname>Juavinett</surname><given-names>Ashley L</given-names></name><name><surname>Kyubwa</surname><given-names>Espoir M</given-names></name><name><surname>Jacobs</surname><given-names>Matthew W</given-names></name><name><surname>Callaway</surname><given-names>Edward M</given-names></name></person-group><article-title>Three types of cortical layer 5 neurons that differ in brain-wide connectivity and function</article-title><source>Neuron</source><year>2015</year><volume>88</volume><issue>6</issue><fpage>1253</fpage><lpage>1267</lpage><comment>ISSN 0896-6273</comment><pub-id pub-id-type="pmcid">PMC4688126</pub-id><pub-id pub-id-type="pmid">26671462</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2015.11.002</pub-id></element-citation></ref><ref id="R18"><label>[18]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>Diederik P</given-names></name><name><surname>Ba</surname><given-names>Jimmy</given-names></name></person-group><source>Adam: A method for stochastic optimization</source><conf-name>Proc of the International Conf on Learning Representations (ICLR)</conf-name><year>2015</year></element-citation></ref><ref id="R19"><label>[19]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kobak</surname><given-names>Dmitry</given-names></name><name><surname>Berens</surname><given-names>Philipp</given-names></name></person-group><article-title>The art of using t-sne for single-cell transcriptomics</article-title><source>Nature Communications</source><year>2019</year><volume>10</volume><elocation-id>5416</elocation-id><pub-id pub-id-type="pmcid">PMC6882829</pub-id><pub-id pub-id-type="pmid">31780648</pub-id><pub-id pub-id-type="doi">10.1038/s41467-019-13056-x</pub-id></element-citation></ref><ref id="R20"><label>[20]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laturnus</surname><given-names>Sophie</given-names></name><name><surname>Kobak</surname><given-names>Dmitry</given-names></name><name><surname>Berens</surname><given-names>Philipp</given-names></name></person-group><article-title>A Systematic Evaluation of Interneuron Morphology Representations for Cell Type Discrimination</article-title><source>Neuroinform</source><year>2020</year><month>October</month><volume>18</volume><issue>4</issue><fpage>591</fpage><lpage>609</lpage><comment>ISSN 1559-0089</comment><pub-id pub-id-type="pmcid">PMC7498503</pub-id><pub-id pub-id-type="pmid">32367332</pub-id><pub-id pub-id-type="doi">10.1007/s12021-020-09461-z</pub-id></element-citation></ref><ref id="R21"><label>[21]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markram</surname><given-names>Henry</given-names></name><name><surname>Muller</surname><given-names>Eilif</given-names></name><name><surname>Ramaswamy</surname><given-names>Srikanth</given-names></name><name><surname>Reimann</surname><given-names>Michael</given-names></name><name><surname>Abdellah</surname><given-names>Marwan</given-names></name><name><surname>Aguado</surname><given-names>Carlos</given-names></name><name><surname>Ailamaki</surname><given-names>Anastasia</given-names></name><name><surname>Alonso-Nanclares</surname><given-names>Lidia</given-names></name><name><surname>Antille</surname><given-names>Nicolas</given-names></name><name><surname>Arsever</surname><given-names>Selim</given-names></name><name><surname>Antoine</surname><given-names>Atenekeng Kahou Guy</given-names></name><etal/></person-group><article-title>Reconstruction and simulation of neocortical microcircuitry</article-title><source>Cell</source><year>2015</year><volume>163</volume><fpage>456</fpage><lpage>492</lpage><pub-id pub-id-type="pmid">26451489</pub-id></element-citation></ref><ref id="R22"><label>[22]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marx</surname><given-names>Manuel</given-names></name><name><surname>Feldmeyer</surname><given-names>Dirk</given-names></name></person-group><article-title>Morphology and physiology of excitatory neurons in layer 6b of the somatosensory rat barrel cortex</article-title><source>Cerebral cortex</source><year>2012</year><volume>23</volume><issue>12</issue><fpage>2803</fpage><lpage>2817</lpage><pub-id pub-id-type="pmcid">PMC3827708</pub-id><pub-id pub-id-type="pmid">22944531</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhs254</pub-id></element-citation></ref><ref id="R23"><label>[23]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McInnes</surname><given-names>Leland</given-names></name><name><surname>Healy</surname><given-names>John</given-names></name><name><surname>Saul</surname><given-names>Nathaniel</given-names></name><name><surname>Großberger</surname><given-names>Lukas</given-names></name></person-group><article-title>Umap: Uniform manifold approximation and projection</article-title><source>Journal of Open Source Software</source><year>2018</year><volume>3</volume><issue>29</issue><fpage>861</fpage><pub-id pub-id-type="doi">10.21105/joss.00861</pub-id></element-citation></ref><ref id="R24"><label>[24]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Narayanan</surname><given-names>Annamalai</given-names></name><name><surname>Chandramohan</surname><given-names>Mahinthan</given-names></name><name><surname>Venkatesan</surname><given-names>Rajasekar</given-names></name><name><surname>Chen</surname><given-names>Lihui</given-names></name><name><surname>Liu</surname><given-names>Yang</given-names></name><name><surname>Jaiswal</surname><given-names>Shantanu</given-names></name></person-group><article-title>graph2vec: Learning distributed representations of graphs</article-title><source>arXivorg</source><year>2017</year><elocation-id>1707.05005</elocation-id></element-citation></ref><ref id="R25"><label>[25]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oberlaender</surname><given-names>Marcel</given-names></name><name><surname>de Kock</surname><given-names>Christiaan PJ</given-names></name><name><surname>Bruno</surname><given-names>Randy M</given-names></name><name><surname>Ramirez</surname><given-names>Alejandro</given-names></name><name><surname>Meyer</surname><given-names>Hanno S</given-names></name><name><surname>Dercksen</surname><given-names>Vincent J</given-names></name><name><surname>Helmstaedter</surname><given-names>Moritz</given-names></name><name><surname>Sakmann</surname><given-names>Bert</given-names></name></person-group><article-title>Cell Type–Specific Three-Dimensional Structure of Thalamocortical Circuits in a Column of Rat Vibrissal Cortex</article-title><source>Cerebral Cortex</source><year>2012</year><volume>22</volume><issue>10</issue><fpage>2375</fpage><lpage>2391</lpage><pub-id pub-id-type="pmcid">PMC3432239</pub-id><pub-id pub-id-type="pmid">22089425</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhr317</pub-id></element-citation></ref><ref id="R26"><label>[26]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Leary</surname><given-names>James L</given-names></name></person-group><article-title>Structure of the area striata of the cat</article-title><source>Journal of Comparative Neurology</source><year>1941</year><volume>75</volume><issue>1</issue><fpage>131</fpage><lpage>164</lpage><comment>ISSN 1096-9861</comment><pub-id pub-id-type="doi">10.1002/cne.900750107</pub-id></element-citation></ref><ref id="R27"><label>[27]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Michel</surname><given-names>V</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Grisel</surname><given-names>O</given-names></name><name><surname>Blondel</surname><given-names>M</given-names></name><name><surname>Prettenhofer</surname><given-names>P</given-names></name><name><surname>Weiss</surname><given-names>R</given-names></name><name><surname>Dubourg</surname><given-names>V</given-names></name><name><surname>Vanderplas</surname><given-names>J</given-names></name><etal/></person-group><article-title>Scikit-learn: Machine learning in Python</article-title><source>Journal of Machine Learning Research (JMLR)</source><year>2011</year><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></element-citation></ref><ref id="R28"><label>[28]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poličar</surname><given-names>Pavlin G</given-names></name><name><surname>Stražar</surname><given-names>Martin</given-names></name><name><surname>Zupan</surname><given-names>Blaž</given-names></name></person-group><article-title>opentsne: a modular python library for t-sne dimensionality reduction and embedding</article-title><source>bioRxiv</source><year>2019</year><pub-id pub-id-type="doi">10.1101/731877</pub-id></element-citation></ref><ref id="R29"><label>[29]</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Cajal</surname><given-names>Santiago Ramón y</given-names></name></person-group><source>Histologie du système nerveux de l’homme et des vertébrés</source><year>1911</year></element-citation></ref><ref id="R30"><label>[30]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scala</surname><given-names>Federico</given-names></name><name><surname>Kobak</surname><given-names>Dmitry</given-names></name><name><surname>Shan</surname><given-names>Shen</given-names></name><name><surname>Bernaerts</surname><given-names>Yves</given-names></name><name><surname>Laturnus</surname><given-names>Sophie</given-names></name><name><surname>Cadwell</surname><given-names>Cathryn Rene</given-names></name><name><surname>Hartmanis</surname><given-names>Leonard</given-names></name><name><surname>Froudarakis</surname><given-names>Emmanouil</given-names></name><name><surname>Castro</surname><given-names>Jesus Ramon</given-names></name><name><surname>Tan</surname><given-names>Zheng Huan</given-names></name><etal/></person-group><article-title>Layer 4 of mouse neocortex differs in cell types and circuit organization between sensory areas</article-title><source>Nature communications</source><year>2019</year><volume>10</volume><issue>1</issue><elocation-id>4174</elocation-id><pub-id pub-id-type="pmcid">PMC6744474</pub-id><pub-id pub-id-type="pmid">31519874</pub-id><pub-id pub-id-type="doi">10.1038/s41467-019-12058-z</pub-id></element-citation></ref><ref id="R31"><label>[31]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scala</surname><given-names>Federico</given-names></name><name><surname>Kobak</surname><given-names>Dmitry</given-names></name><name><surname>Bernabucci</surname><given-names>Matteo</given-names></name><name><surname>Bernaerts</surname><given-names>Yves</given-names></name><name><surname>Cadwell</surname><given-names>Cathryn</given-names></name><name><surname>Castro</surname><given-names>Jesus</given-names></name><name><surname>Hartmanis</surname><given-names>Leonard</given-names></name><name><surname>Jiang</surname><given-names>Xiaolong</given-names></name><name><surname>Laturnus</surname><given-names>Sophie</given-names></name><name><surname>Miranda</surname><given-names>Elanine</given-names></name><name><surname>Mulherkar</surname><given-names>Shalaka</given-names></name><etal/></person-group><article-title>Phenotypic variation of transcriptomic cell types in mouse motor cortex</article-title><source>Nature</source><year>2021</year><volume>598</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="pmcid">PMC8113357</pub-id><pub-id pub-id-type="pmid">33184512</pub-id><pub-id pub-id-type="doi">10.1038/s41586-020-2907-3</pub-id></element-citation></ref><ref id="R32"><label>[32]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneider-Mizell</surname><given-names>Casey M</given-names></name><name><surname>Bodor</surname><given-names>Agnes</given-names></name><name><surname>Brittain</surname><given-names>Derrick</given-names></name><name><surname>Buchanan</surname><given-names>JoAnn</given-names></name><name><surname>Bumbarger</surname><given-names>Daniel J</given-names></name><name><surname>Elabbady</surname><given-names>Leila</given-names></name><name><surname>Kapner</surname><given-names>Daniel</given-names></name><name><surname>Kinn</surname><given-names>Sam</given-names></name><name><surname>Mahalingam</surname><given-names>Gayathri</given-names></name><name><surname>Seshamani</surname><given-names>Sharmishtaa</given-names></name><name><surname>Suckow</surname><given-names>Shelby</given-names></name><etal/></person-group><article-title>Cell-type-specific inhibitory circuitry from a connectomic census of mouse visual cortex</article-title><source>bioRxiv</source><year>2023</year><pub-id pub-id-type="doi">10.1101/2023.01.23.525290</pub-id></element-citation></ref><ref id="R33"><label>[33]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scorcioni</surname><given-names>Ruggero</given-names></name><name><surname>Polavaram</surname><given-names>Sridevi</given-names></name><name><surname>Ascoli</surname><given-names>Giorgio A</given-names></name></person-group><article-title>L-measure: a web-accessible tool for the analysis comparison and search of digital reconstructions of neuronal morphologies</article-title><source>Nature protocols</source><year>2008</year><volume>3</volume><issue>5</issue><fpage>866</fpage><lpage>876</lpage><pub-id pub-id-type="pmcid">PMC4340709</pub-id><pub-id pub-id-type="pmid">18451794</pub-id><pub-id pub-id-type="doi">10.1038/nprot.2008.51</pub-id></element-citation></ref><ref id="R34"><label>[34]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stanley</surname><given-names>Geoffrey</given-names></name><name><surname>Gokce</surname><given-names>Ozgun</given-names></name><name><surname>Malenka</surname><given-names>Robert C</given-names></name><name><surname>Südhof</surname><given-names>Thomas</given-names></name><name><surname>Quake</surname><given-names>Stephen R</given-names></name></person-group><article-title>Continuous and discrete neuron types of the adult murine striatum</article-title><source>Neuron</source><year>2019</year><volume>105</volume><issue>4</issue><fpage>688</fpage><lpage>699</lpage><pub-id pub-id-type="pmid">31813651</pub-id></element-citation></ref><ref id="R35"><label>[35]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tasic</surname><given-names>Bosiljka</given-names></name><name><surname>Menon</surname><given-names>Vilas</given-names></name><name><surname>Nguyen</surname><given-names>Thuc Nghi</given-names></name><name><surname>Kim</surname><given-names>Tae Kyung</given-names></name><name><surname>Jarsky</surname><given-names>Tim</given-names></name><name><surname>Yao</surname><given-names>Zizhen</given-names></name><name><surname>Levi</surname><given-names>Boaz</given-names></name><name><surname>Gray</surname><given-names>Lucas T</given-names></name><name><surname>Sorensen</surname><given-names>Staci A</given-names></name><name><surname>Dolbeare</surname><given-names>Tim</given-names></name><name><surname>Bertagnolli</surname><given-names>Darren</given-names></name><etal/></person-group><article-title>Adult mouse cortical cell taxonomy revealed by single cell transcriptomics</article-title><source>Nature Neuroscience</source><year>2016</year><volume>19</volume><fpage>335</fpage><lpage>346</lpage><pub-id pub-id-type="pmcid">PMC4985242</pub-id><pub-id pub-id-type="pmid">26727548</pub-id><pub-id pub-id-type="doi">10.1038/nn.4216</pub-id></element-citation></ref><ref id="R36"><label>[36]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tasic</surname><given-names>Bosiljka</given-names></name><name><surname>Yao</surname><given-names>Zizhen</given-names></name><name><surname>Graybuck</surname><given-names>Lucas T</given-names></name><name><surname>Smith</surname><given-names>Kimberly A</given-names></name><name><surname>Nguyen</surname><given-names>Thuc Nghi</given-names></name><name><surname>Bertagnolli</surname><given-names>Darren</given-names></name><name><surname>Goldy</surname><given-names>Jeff</given-names></name><name><surname>Garren</surname><given-names>Emma</given-names></name><name><surname>Economo</surname><given-names>Michael N</given-names></name><name><surname>Viswanathan</surname><given-names>Sarada</given-names></name><name><surname>Penn</surname><given-names>Osnat</given-names></name><etal/></person-group><article-title>Shared and distinct transcriptomic cell types across neocortical areas</article-title><source>Nature</source><year>2018</year><volume>563</volume><issue>7729</issue><fpage>72</fpage><lpage>78</lpage><pub-id pub-id-type="pmcid">PMC6456269</pub-id><pub-id pub-id-type="pmid">30382198</pub-id><pub-id pub-id-type="doi">10.1038/s41586-018-0654-5</pub-id></element-citation></ref><ref id="R37"><label>[37]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Maaten</surname><given-names>Laurens</given-names></name><name><surname>Hinton</surname><given-names>Geoffrey</given-names></name></person-group><article-title>Visualizing data using t-SNE</article-title><source>Journal of Machine Learning Research (JMLR)</source><year>2008</year><volume>9</volume><issue>86</issue><fpage>2579</fpage><lpage>2605</lpage></element-citation></ref><ref id="R38"><label>[38]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Eric Y</given-names></name><name><surname>Fahey</surname><given-names>Paul G</given-names></name><name><surname>Ponder</surname><given-names>Kayla</given-names></name><name><surname>Ding</surname><given-names>Zhuokun</given-names></name><name><surname>Chang</surname><given-names>Andersen</given-names></name><name><surname>Muhammad</surname><given-names>Taliah</given-names></name><name><surname>Patel</surname><given-names>Saumil</given-names></name><name><surname>Ding</surname><given-names>Zhiwei</given-names></name><name><surname>Tran</surname><given-names>Dat</given-names></name><name><surname>Fu</surname><given-names>Jiakun</given-names></name><name><surname>Papadopoulos</surname><given-names>Stelios</given-names></name><etal/></person-group><article-title>Towards a foundation model of the mouse visual cortex</article-title><source>bioRxiv</source><year>2023</year><pub-id pub-id-type="doi">10.1101/2023.03.21.533548</pub-id></element-citation></ref><ref id="R39"><label>[39]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Wendy X</given-names></name><name><surname>Lefebvre</surname><given-names>Julie L</given-names></name></person-group><article-title>Morphological pseudotime ordering and fate mapping reveal diversification of cerebellar inhibitory interneurons</article-title><source>Nature Commununication</source><year>2022</year><volume>13</volume><pub-id pub-id-type="pmcid">PMC9197879</pub-id><pub-id pub-id-type="pmid">35701402</pub-id><pub-id pub-id-type="doi">10.1038/s41467-022-30977-2</pub-id></element-citation></ref><ref id="R40"><label>[40]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Yingfan</given-names></name><name><surname>Huang</surname><given-names>Haiyang</given-names></name><name><surname>Rudin</surname><given-names>Cynthia</given-names></name><name><surname>Shaposhnik</surname><given-names>Yaron</given-names></name></person-group><article-title>Understanding how dimension reduction tools work: An empirical approach to deciphering t-sne, umap, trimap, and pacmap for data visualization</article-title><source>Journal of Machine Learning Research (JMLR)</source><year>2021</year><volume>22</volume><issue>201</issue><fpage>1</fpage><lpage>73</lpage></element-citation></ref><ref id="R41"><label>[41]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weiler</surname><given-names>Simon</given-names></name><name><surname>Nilo</surname><given-names>Drago Guggiana</given-names></name><name><surname>Bonhoeffer</surname><given-names>Tobias</given-names></name><name><surname>Hübener</surname><given-names>Mark</given-names></name><name><surname>Rose</surname><given-names>Tobias</given-names></name><name><surname>Scheuss</surname><given-names>Volker</given-names></name></person-group><article-title>Orientation and direction tuning align with dendritic morphology and spatial connectivity in mouse visual cortex</article-title><source>Current Biology</source><year>2022</year><month>April</month><volume>32</volume><issue>8</issue><fpage>1743</fpage><lpage>1753</lpage><elocation-id>e7</elocation-id><comment>ISSN 09609822</comment><pub-id pub-id-type="pmid">35276098</pub-id></element-citation></ref><ref id="R42"><label>[42]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weis</surname><given-names>Marissa A</given-names></name><name><surname>Pede</surname><given-names>Laura</given-names></name><name><surname>Lüddecke</surname><given-names>Timo</given-names></name><name><surname>Ecker</surname><given-names>Alexander S</given-names></name></person-group><article-title>Self-supervised representation learning of neuronal morphologies</article-title><year>2021</year></element-citation></ref><ref id="R43"><label>[43]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yao</surname><given-names>Zizhen</given-names></name><name><surname>van Velthoven</surname><given-names>Cindy TJ</given-names></name><name><surname>Nguyen</surname><given-names>Thuc Nghi</given-names></name><name><surname>Goldy</surname><given-names>Jeff</given-names></name><name><surname>Sedeno-Cortes</surname><given-names>Adriana E</given-names></name><name><surname>Baftizadeh</surname><given-names>Fahimeh</given-names></name><name><surname>Bertagnolli</surname><given-names>Darren</given-names></name><name><surname>Casper</surname><given-names>Tamara</given-names></name><name><surname>Chiang</surname><given-names>Megan</given-names></name><name><surname>Crichton</surname><given-names>Kirsten</given-names></name><name><surname>Ding</surname><given-names>Song-Lin</given-names></name><etal/></person-group><article-title>A taxonomy of transcriptomic cell types across the isocortex and hippocampal formation</article-title><source>Cell</source><year>2021</year><volume>184</volume><issue>12</issue><fpage>3222</fpage><lpage>3241</lpage><pub-id pub-id-type="pmcid">PMC8195859</pub-id><pub-id pub-id-type="pmid">34004146</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2021.04.021</pub-id></element-citation></ref><ref id="R44"><label>[44]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Yaz</surname><given-names>Ilker O</given-names></name><name><surname>Loriot</surname><given-names>Sébastien</given-names></name></person-group><chapter-title>Triangulated surface mesh segmentation</chapter-title><source>CGAL User and Reference Manual</source><edition>5.5.1 edition</edition><publisher-name>CGAL Editorial Board</publisher-name><year>2022</year><comment>URL <ext-link ext-link-type="uri" xlink:href="https://doc.cgal.org/5.5.1/Manual/packages.html#PkgSurfaceMeshSegmentation">https://doc.cgal.org/5.5.1/Manual/packages.html#PkgSurfaceMeshSegmentation</ext-link></comment></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Pipeline to generate vector embeddings for large scale datasets that capture the morphological features of the neurons’ dendritic trees.</title><p><bold>A.</bold> Imaging of brain volume via electron microscopy and subsequent segmentation and tracing to render 3D meshes of individual neurons that are used for skeletonization. <bold>B.</bold> Self-supervised learning of low dimensional vector embeddings <italic>z</italic><sub>1</sub>, <italic>z</italic><sub>2</sub> that capture the essence of the 3D morphology of individual neurons using GraphDINO. Two augmented “views” of the neuron are input into the network, where the weights of one encoder (bottom) are an exponential moving average (EMA) of the other encoder (top). The objective is to maximize the similarity between the vector embeddings of both views. Vector embeddings of similar neurons are close to each other in latent space. <bold>C.</bold> An individual neuron is represented by its vector embedding as a point in the 32-dimensional vector space. <bold>D.</bold> Quality control to remove neurons with tracing errors.</p></caption><graphic xlink:href="EMS158985-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Visualization of soma depths and cortical layer assignments of excitatory neuronal morphologies showing mostly a continuum with distinct clusters only in deeper layers.</title><p><bold>A.</bold> Top view of the EM volume with approximate visual areas indicated. All neurons with their soma origin within the red boundary were used for analysis. <bold>B.</bold> Distribution of complete neurons and fragments along cortical depth as determined by our classifier based on the morphological embeddings. <bold>C.</bold> Distribution of excitatory neurons and interneurons along cortical depth. <bold>D.</bold> Classifier prediction for cortical layer origin based on the learned morphological embeddings. <bold>E.</bold> t-SNE embedding (perplexity = 300) of the vector embeddings of excitatory neuronal morphologies colored by the respective soma depth of the neurons relative to the pia (<italic>n</italic> = 33,997). <bold>F.</bold> t-SNE embedding colored by cortical layer assignments as predicted by a cross-validated classifier trained on the morphological embeddings as features and a subset of manually labeled excitatory neurons (<italic>n</italic> = 922). <bold>G.</bold> Cross-section of the brain volume depicting soma positions of neurons colored by their assigned cortical layer. Cortical layer thicknesses for primary visual cortex (V1) (left) and higher visual areas (HVA) (right) given as mean ± standard deviation. <bold>H.</bold> t-SNE embedding of excitatory neuronal morphologies colored by expert-defined cell types. <bold>I.</bold> Example morphologies of the expert-defined cell types.</p></caption><graphic xlink:href="EMS158985-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Cluster versus continuum analysis</title><p><bold>A.</bold> Histograms of samples from a 1d Gaussian mixture (<italic>n</italic> = 30000, number of components = 2) in green and the underlying mixture components with means <italic>μ</italic><sub>1</sub> = −1 and <italic>μ</italic><sub>2</sub> = 1 in yellow. Data distributions evolve from discrete to continuous by increasing the standard deviation (SD) from left to right. <bold>B.</bold> t-SNE representation of synthetic data (<italic>n</italic> = 33997, perplexity= 300). Synthetic data is sampled from Gaussian mixtures with 20 components. Cluster means and weights are estimated from neuronal data. Isotropic variance is set to obtain data evolving from discrete clusters to uniform distributions. Grey insets (1–6) show histograms of two sample clusters (12 and 1) and their nearest neighbors (0 and 17, respectively) projected onto the direction connecting their cluster means (left), as well as the cumulative distribution of the samples assigned to these two clusters’ along this direction (right). <bold>C.</bold> Mean adjusted rand index (ARI) of 100 GMMs with increasing number of components fit to the synthetic datasets. The correct number of underlying components can be identified as long as the variance in the data is not too high (&lt; 0.5 for 20 components). <bold>D.</bold> t-SNE representation (<italic>n</italic> = 33997, perplexity= 300) of neuronal data colored by cluster membership (GMM with 20 components). Grey insets (7 &amp; 8) show 1d projections of the clusters 12 and 1 onto the line connecting their means with their nearest neighbors (0 and 17, respectively). Cumulative distributions show that while there is a gap between cluster 12 and its neighbors, there is none between cluster 1 and its neighbors. <bold>E.</bold> Cluster analysis as in <bold>C.</bold> for neuronal data. No specific number of components can be recovered. <bold>F.</bold> t-SNE representation of neuronal data overlaid with nearest neighbor graph between clusters. Line width indicates dip statistic (thicker = more connected). <bold>G.</bold> Maximum dip statistic between all clusters and their nearest neighbor for the synthetic data with 20 components and varying variance (yellow curve) and for the neuronal data clustered with 20 components (red dashed line).</p></caption><graphic xlink:href="EMS158985-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Schematic of morphometric descriptors computed from neuronal skeletons and their labeled compartments.</title><p>SOMA depth. Depth of the centroid of the soma relative to the pia. HEIGHT. Extent of the cell in y-axis. TOTAL APICAL LENGTH. Total length of the skeletal branches of the apical dendrites. APICAL WIDTH. Maximum extent of the apical dendritic tree in the xz-plane. TOTAL BASAL LENGTH. Total length of the skeletal branches of the basal dendrites. BASAL BIAS. Depth in y-axis of center of mass of basal dendrites relative to the soma.</p></caption><graphic xlink:href="EMS158985-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>t-SNE visualization of vector embeddings per cortical layer reveal axes of variation in neuronal morphologies.</title><p><bold>A.</bold> t-SNE embeddings per layer colored by percentiles of various morphometric descriptors with example neuronal morphologies along the axis of variation displayed above the embedding. <bold>B.</bold> <italic>R</italic><sup>2</sup> scores of the six morphometric descriptors (see <xref ref-type="fig" rid="F4">Fig. 4</xref>) per layer showing the strength as predictors of the 32d embeddings. <bold>C.</bold> Spearman’s rank correlation coefficient between morphometric descriptors per layer. <bold>Layer 2/3</bold> (blue) Continuum of dendritic morphologies with thinner and less tufted neurons in increasing distance to the pia. <bold>Layer 4</bold> (turquoise) Continuation of L2/3 trends with shorter apical dendrites and more atufted cells. Many cells avoid reaching dendrites into L5 (basal bias). <bold>Layer 5</bold> (green) Clustering of thick-tufted ET and NP cells. Upper L5 cells resemble L4 cells that avoid reaching into L5, indicating too strict laminar borders. <bold>Layer 6</bold> (orange) Continuum with a large morphological diversity e.g. in cell heights, and existence of horizontal and inverted pyramidal neurons.</p></caption><graphic xlink:href="EMS158985-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><title>Inter-areal differences between primary visual cortex (V1) and higher visual areas (HVAs).</title><p><bold>A.</bold> Side view of the cortical volume. Each point represents the soma location of one neuron and is colored by apical skeletal length of the respective neuron (dark = no apical, bright = maximal apical skeleton length). projection from the side orthogonal to the V1/HVA border after a 14 degree rotation around y-axis (vertical dashed line); top: pia; bottom: white matter. <bold>B.</bold> Top view of the volume showing the density of atufted (left), small tufted (middle) and tufted (right) L4 cells. Atufted neurons are mostly confined to V1, while tufted neurons are more abundant in HVA. Dashed lines: area borders between primary visual cortex (V1), anterolateral area (AL) and rostrolateral area (RL), estimated from reversal of the retinotopic map measured using functional imaging [<xref ref-type="bibr" rid="R6">6</xref>].</p></caption><graphic xlink:href="EMS158985-f006"/></fig><fig id="F7" position="float"><label>Figure 7</label><caption><title>Basal bias neurons in primary visual cortex (V1).</title><p><bold>A.</bold> Side view of the cortical volume. Each point represents the soma location of one neuron and is colored by its respective basal bias (dark = negative basal bias: center of mass of basal dendrites is above the soma; bright = positive basal bias: center of mass of basal dendrites is below soma). <bold>B.</bold> Example neuronal morphologies of basal bias neurons (top) and top view of the volume (as in <xref ref-type="fig" rid="F6">Fig. 6B</xref>) showing horizontal density distribution of L4 cells whose dendrites avoid reaching into L5 and who are mostly located in V1 (bottom). <bold>C.</bold> Functional digital twin can predict the functional response of the neurons to input stimuli such as natural movies. The input-output function of each neurons is described by a functional bar code <italic>f<sub>i</sub></italic> [<xref ref-type="bibr" rid="R38">38</xref>]. Schematic adapted from Consortium et al. [<xref ref-type="bibr" rid="R6">6</xref>] (<xref ref-type="fig" rid="F1">Fig. 1</xref>). <bold>D.</bold> Predictions of basal bias metric from functional bar code fi using linear regression.</p></caption><graphic xlink:href="EMS158985-f007"/></fig></floats-group></article>