<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS158022</article-id><article-id pub-id-type="doi">10.1101/2022.12.01.518647</article-id><article-id pub-id-type="archive">PPR578962</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">3</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Prior Knowledge Biases the Visual Memory of Body Postures</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Qiu</surname><given-names>Han</given-names></name><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Gandolfo</surname><given-names>Marco</given-names></name></contrib><contrib contrib-type="author"><name><surname>Peelen</surname><given-names>Marius V.</given-names></name></contrib><aff id="A1">Donders Institute for Brain, Cognition and Behaviour, Radboud University, 6525 HR, Nijmegen, The Netherlands</aff></contrib-group><author-notes><corresp id="CR1"><label>*</label>Correspondence: <email>qiu.han@donders.ru.nl</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>04</day><month>12</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>01</day><month>12</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nd/4.0/">CC BY-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Body postures provide information about others’ actions, intentions, and emotional states. Little is known about how postures are represented in the brain’s visual system. Considering our extensive visual and motor experience with body postures, we hypothesized that priors derived from this experience may systematically bias visual body posture representations. We examined two priors: gravity and biomechanical constraints. Gravity pushes lifted body parts downwards, while biomechanical constraints limit the range of possible postures (e.g., an arm raised far behind the head cannot go down further). Across three experiments (N=246) we probed participants’ visual memory of briefly presented postures using change discrimination and adjustment tasks. Results showed that lifted arms were misremembered as lower and as more similar to the nearest biomechanically plausible postures. Inverting the body stimuli eliminated both biases, ruling out visual confounds. These findings show that visual memory representations of body postures are modulated by a combination of category-general and category-specific priors.</p></abstract><kwd-group><kwd>Body</kwd><kwd>Postures</kwd><kwd>Prior</kwd><kwd>Bias</kwd><kwd>Bayesian theory</kwd><kwd>Expectation</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Body posture is an important social cue that provides information about others’ emotions, intentions, and mental states. The pressure to quickly and accurately recognize bodies and their movements has resulted in humans’ typically excellent performance in detecting and discriminating body posture and body motion <sup><xref ref-type="bibr" rid="R1">1</xref>–<xref ref-type="bibr" rid="R4">4</xref></sup>, a skill that is supported by dedicated brain regions in visual cortex including the extrastriate body area <sup><xref ref-type="bibr" rid="R5">5</xref></sup>, fusiform body area <sup><xref ref-type="bibr" rid="R6">6</xref></sup>, and superior temporal sulcus <sup><xref ref-type="bibr" rid="R7">7</xref></sup>. When bodies are presented inverted, which is inconsistent with our daily experience, the ability to detect and discriminate postures is impaired <sup><xref ref-type="bibr" rid="R2">2</xref>,<xref ref-type="bibr" rid="R3">3</xref>,<xref ref-type="bibr" rid="R8">8</xref>–<xref ref-type="bibr" rid="R10">10</xref></sup>. This inversion effect is more pronounced for faces and bodies than for other objects, indicating more configural processing for these visually highly familiar stimuli <sup><xref ref-type="bibr" rid="R2">2</xref>,<xref ref-type="bibr" rid="R9">9</xref></sup>.</p><p id="P3">Owning a body ourselves, we also have extensive motor, tactile, and proprioceptive experience of a body and its dynamics <sup><xref ref-type="bibr" rid="R11">11</xref></sup>. Neuropsychological evidence suggests that we have an internal model of the physical relationships between body parts that helps us execute our own actions and understand those of others <sup><xref ref-type="bibr" rid="R12">12</xref></sup>. Together with our extensive visual experience, these sensory modalities provide us with additional knowledge of hierarchical limb structure, the possible range of movements of joints, and the effort required for executing specific body actions. Here, we asked whether our experience observing and executing a biased range of body postures modulates the perceptual representation of these postures.</p><p id="P4">Previous research has shown that perception is influenced by knowledge and expectations <sup><xref ref-type="bibr" rid="R13">13</xref>,<xref ref-type="bibr" rid="R14">14</xref></sup>. Specifically, Bayesian accounts of perception propose that priors are integrated with sensory input, weighted by their uncertainty to support perceptual inference <sup><xref ref-type="bibr" rid="R15">15</xref>–<xref ref-type="bibr" rid="R17">17</xref></sup>. An example of this integration is the hollow-face illusion: a mask viewed from the concave side still gives a vivid impression of a convex face, due to the strong prior of faces being convex. Priors are shaped by environmental statistics, including the distribution of visual properties like orientation <sup><xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R19">19</xref></sup>, basic physical principles of motion <sup><xref ref-type="bibr" rid="R20">20</xref>,<xref ref-type="bibr" rid="R21">21</xref></sup>, gravity <sup><xref ref-type="bibr" rid="R22">22</xref>,<xref ref-type="bibr" rid="R23">23</xref></sup>, and physical state <sup><xref ref-type="bibr" rid="R24">24</xref></sup>.</p><p id="P5">Effects of prior knowledge on perception have also been observed for the perception of body movements. For example, observers tend to perceive or imagine a biomechanically plausible movement compared to an awkward one <sup><xref ref-type="bibr" rid="R25">25</xref></sup>. Furthermore, when observing apparent body movements, the perceived movement tends to follow a biomechanically plausible path, even if that path is longer <sup><xref ref-type="bibr" rid="R26">26</xref></sup>. Other examples include the finding that the extrapolation of biomechanically plausible movements is larger than implausible ones <sup><xref ref-type="bibr" rid="R27">27</xref></sup>, and that unstable postures leaning backward are judged to be more likely to fall than postures leaning forward <sup><xref ref-type="bibr" rid="R28">28</xref></sup>. These findings indicate that the perceptual interpretation of real or apparent body movements is influenced by knowledge of biomechanical constraints. However, body movements involve sequences of postures unfolding over time, requiring the viewer to predict and construct the upcoming posture. A single static posture may not automatically evoke such predictive processes. It is therefore unclear whether perceptual representations of static postures are influenced by priors in the way that body movements are.</p><p id="P6">To address this question, we considered two priors that are relevant for static body postures. The first is the general prior of gravity: Gravity is an omnipresent force that pushes everything down, resulting in a strong prior for perception and action <sup><xref ref-type="bibr" rid="R29">29</xref></sup>. Previous studies have found that the position of an unsupported object will be remembered as lower, in line with the influence of a gravity prior <sup><xref ref-type="bibr" rid="R30">30</xref>,<xref ref-type="bibr" rid="R31">31</xref></sup>. Accordingly, because arms will fall if not supported by muscles, we hypothesized that a lifted arm will be remembered as slightly lower than its actual position.</p><p id="P7">The second prior follows from biomechanical constraints. Because of the biomechanical structure of the body, particularly the range of motion of joints, postures are confined to a limited range. For example, the shoulder joints can flex from the resting position to the front by 180 degrees, but to the back, they can only extend to around 60 degrees <sup><xref ref-type="bibr" rid="R32">32</xref>,<xref ref-type="bibr" rid="R33">33</xref></sup>. If prior knowledge of these constraints informs perception, the representation of a nearly impossible posture may be biased towards the nearest possible posture. Crucially, biomechanical constraints can counteract the effect of gravity: an arm raised in front of the head will fall but an arm raised behind the head can hardly fall lower (<xref ref-type="fig" rid="F1">Figure 1</xref>).</p><p id="P8">To test these hypotheses, we used four arm postures subject to one or both of the two biases (<xref ref-type="fig" rid="F1">Figure 1</xref>). We predicted that lifted arms will generally be remembered as lower, towards the ground, reflecting a gravity-related bias. Furthermore, we predicted that lifted arms will be biased towards biomechanically possible postures. Specifically, biomechanical constraints limit further movement of the arm when the arm is raised behind the head, counteracting the gravity bias (<xref ref-type="fig" rid="F1">Figure 1b</xref>), while adding to the gravity bias when the arm is behind the hip (<xref ref-type="fig" rid="F1">Figure 1d</xref>).</p><p id="P9">We designed two tasks to probe the existence of biases in body posture representation due to gravity and biomechanical constraints. In the change discrimination task (Experiment 1, <xref ref-type="fig" rid="F2">Figure 2a</xref>), participants compared two sequential postures whose arm positions slightly differed, with the second arm posture being slightly higher or lower. We first tested upper postures (<xref ref-type="fig" rid="F1">Fig. 1a &amp; 1b</xref>) in Experiment 1a, then followed with lower postures (<xref ref-type="fig" rid="F1">Fig. 1c &amp; 1d</xref>) in Experiment 1b to generalize the findings to visually different postures. We then replicated the results using a within-subject design with an adjustment task (Experiment 2, <xref ref-type="fig" rid="F2">Figure 2b</xref>) where the participants needed to reproduce the remembered posture by adjusting the arm of a figure. The error of their adjustment reflects memory biases. Finally, we replicated these results again in Experiment 3, and used inverted body postures as control stimuli to test whether the effects rely on configural body processing.</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>Experiment 1a &amp; 1b</title><p id="P10">In this change discrimination task, participants needed to decide whether the arm in the second posture was higher or lower than in the first one (<xref ref-type="fig" rid="F2">Figure 2a</xref>). In the absence of biases, participants should detect upward and downward changes equally well. Instead, if priors bias the representation of the first posture during the brief interval, we may observe that detecting a change in one direction is easier than a change in the other. The perceptual biases of interest were thus quantified by the criterion (c) from signal detection theory. Taking upward movement as the signal, a negative criterion means that participants responded more up than down. Data of Experiment 1a (upper postures, N = 60) and 1b (lower postures, N = 60) were pooled in the analysis.</p><p id="P11">Participants’ responses were in line with a gravity bias (<xref ref-type="fig" rid="F3">Figure 3</xref>): The arm in the target posture was remembered as lower than its actual position, as indexed by a criterion significantly below zero for all postures (Upper-front: M = -0.25, 95% CI = [-0.32, -0.18], <italic>t</italic>(59) = -7.08, <italic>p</italic> &lt; .001, <italic>d</italic> = -0.91, BF<sub>10</sub> = 5.75E6; Upper-back: M = -0.144, 95% CI = [-0.21, -0.07], <italic>t</italic>(59) = -4.38, <italic>p</italic> &lt; .001, <italic>d</italic> = -0.57, BF<sub>10</sub>= 413; Lower-back: M = -0.11, 95% CI = [-0.19, -0.04], <italic>t</italic>(59) = -2.96, <italic>p</italic> = .004, <italic>d</italic> = -0.38, BF<sub>10</sub> = 7.14) but not the Lower-front: M = -0.06, 95% CI = [-0.14, 0.02], <italic>t</italic>(59) = -1.60, <italic>p</italic> = .114, <italic>d</italic> = -0.21, BF<sub>10</sub> = 0.47.</p><p id="P12">Next, we combined Experiments 1a and 1b using a mixed ANOVA with arm height (upper, Experiment 1a; versus lower, Experiment 1b) as a between-subject factor and arm direction as a within-subject factor to test the presence of a biomechanical bias. As illustrated in <xref ref-type="fig" rid="F1">Figure 1</xref>, compared to the front, the downward bias in the back should be diminished by biomechanical constraints when in the upper quadrant (<xref ref-type="fig" rid="F1">Figure 1a vs. 1b</xref>), but strengthened when in the lower quadrant (<xref ref-type="fig" rid="F1">Figure 1c vs. 1d</xref>). We thus predicted an interaction between arm height and arm direction. We indeed found this interaction (<xref ref-type="fig" rid="F3">Figure 3</xref>): <italic>F</italic>(1, 118) = 8.09, <italic>p</italic> = .005, η<sup>2</sup><sub>p</sub> = 0.064, BF<sub>10</sub> = 7.63. Specifically, for the upper postures, the gravity bias was stronger in the front than in the back, M = -0.108, 95% CI = [-0.03, -0.19], <italic>t</italic>(59) = -2.74, <italic>p</italic> = .008, <italic>d</italic> = -0.35, BF<sub>10</sub> = 4.17, indicating that the upward biomechanical constraint counteracted the gravity bias. The downward bias for the Lower-back was numerically stronger than that for the Lower-front, in line with an additive effect of biomechanical constraint bias and gravity bias, but this difference did not reach significance: M = 0.05, 95% CI = [-0.03, 0.13], <italic>t</italic>(59) = 1.3, <italic>p</italic> = .197, <italic>d</italic> = 0.17, BF<sub>10</sub> = 0.32. All statistics are provided in <xref ref-type="supplementary-material" rid="SD1">Table S1, S2, and S3</xref>.</p></sec><sec id="S4"><title>Experiment 2</title><p id="P13">The change discrimination task provided evidence for both gravity and biomechanical biases. We wondered whether these results would be specific to the change detection task, in which the two consecutive body postures may be perceived as part of an action. If so, the results could reflect biases in human action perception rather than biases in the static representation of the target posture. To address this, in Experiment 2 (N=60) we tested whether the identified biases replicate in an adjustment task (<xref ref-type="fig" rid="F2">Figure 2b</xref>), in which participants were asked to remember a target posture and then to reproduce this target posture by adjusting the arm of a human figure on the screen.</p><p id="P14">In the adjustment task, the direction and magnitude of biases are directly reflected in the direction and magnitude of the adjustment error. A negative error indicates that the target was remembered as lower than its actual position, reflecting a gravity bias. This was the case for all of the postures tested (<xref ref-type="fig" rid="F4">Figure 4a</xref>, Upper-front: M = -2.54, 95% CI = [-2.97, -2.11], <italic>t</italic>(59) = -11.8, <italic>p</italic> &lt; .001, <italic>d</italic> = -1.52, BF<sub>10</sub> = 1.68E14; Upper-back: M = -1.89, 95% CI = [-2.34, -1.44], <italic>t</italic>(59) = -8.46, <italic>p</italic> &lt; .001, <italic>d</italic> = -1.09, BF<sub>10</sub> = 9.86E8; Lower-back: M = -1.07, 95% CI = [-1.48, -0.66], <italic>t</italic>(59) = -5.22, <italic>p</italic> &lt; .001, <italic>d</italic> = -0.67, BF<sub>10</sub> = 6.52E3) except for the lower-front (M = 0.09, 95% CI = [-0.35, 0.52], <italic>t</italic>(59) = -0.39, <italic>p</italic> = .694, <italic>d</italic> = 0.05, BF<sub>10</sub> = 0.15).</p><p id="P15">Also consistent with Experiment 1, a two-way repeated-measures ANOVA showed an interaction between arm height and arm direction, revealing the effect of biomechanical constraints, <italic>F</italic>(1, 59) = 48.1, <italic>p</italic> &lt; .001, η<sup>2</sup><sub>p</sub> = 0.45, BF<sub>10</sub> = 2.02E8. As in Experiment 1, for the upper postures, the gravity bias was stronger in the front than the back: M = -0.65, 95% CI = [-0.25, -1.05], <italic>t</italic>(59) = -3.28, <italic>p</italic> = .002, <italic>d</italic> = -0.42, BF<sub>10</sub> = 16.5. By contrast, as predicted, for the lower postures, the gravity bias was stronger in the back than the front: M = -1.15, 95% CI = [-0.79, -1.52], <italic>t</italic>(59) = -6.32, <italic>p</italic> &lt; .001, <italic>d</italic> = -0.82, BF<sub>10</sub> = 3.44E5, showing that biomechanical constraints also influence visual memory of lower arm postures.</p><p id="P16">For visualization purposes, we computed two indexes that reflect the two hypothesized effects. The gravity bias was given by the overall adjustment error, averaged across the four conditions (Upper-front, Upper-Back, Lower-front, Lower-back). The biomechanical constraint bias was indexed by the difference between postures with vs without biomechanical constraint, averaged across upper and lower postures (the mean of (Upper-front – Upper-back) and (Lower-back - Lower-front); <xref ref-type="fig" rid="F4">Figure 4b</xref>). <xref ref-type="fig" rid="F4">Figure 4b</xref> shows the bias indexes for individual participants. Taking the four postures together, the error caused by gravity was significantly different from zero, M = -1.35, 95% CI = [-1.58, -1.12], <italic>t</italic>(59) = -11.61, <italic>p</italic> &lt; .001, <italic>d</italic> = -1.50, BF<sub>10</sub> = 8.50E13. The overall biomechanical constraint (M = -0.90, 95% CI = [-1.16, -0.64], <italic>t</italic>(59) = -6.94, <italic>p</italic> &lt; .001, <italic>d</italic> = -0.90, BF<sub>10</sub> = 3.37E6, also reflected in the interaction in the ANOVA) was also highly consistent across individuals. These results confirm and extend the results of Experiment 1 using a different task, generalizing the effects to a scenario where no action or motion is implied.</p></sec><sec id="S5"><title>Experiment 3</title><p id="P17">Experiment 3 aimed to test whether the effects were caused by local visual features, including the overlap between the arm and the head, the curvature of the arm, and so on. A prominent feature of body perception is its susceptibility to inversion. Inversion has been shown to disrupt body and face perception more than other objects, which is believed to reflect the disrupted configural processing of bodies and faces <sup><xref ref-type="bibr" rid="R2">2</xref>,<xref ref-type="bibr" rid="R3">3</xref>,<xref ref-type="bibr" rid="R8">8</xref>,<xref ref-type="bibr" rid="R10">10</xref>,<xref ref-type="bibr" rid="R34">34</xref></sup>. Inverted bodies thus serve as an ideal control for typical upright bodies since they are identical in terms of local features but are processed less as integral postures. In Experiment 3, we tested whether the effects of gravity and biomechanical constraints were contingent on the configural processing instead of local features using the inversion effect (N = 66).</p><p id="P18">We found a significant three-way interaction among arm direction, arm height and body orientation, <italic>F</italic>(1,65) = 7.15, <italic>p</italic> = .009, η<sup>2</sup><sub>p</sub> = .10, BF<sub>10</sub> = 10.3, indicating that body orientation modulated the interaction between arm height and arm direction. Inspecting upright and inverted conditions separately (<xref ref-type="fig" rid="F5">Figure 5</xref>), the interaction of arm height and arm direction was significant for the upright body, replicating results from Experiment 2, <italic>F</italic>(1,65) = 24.1, <italic>p</italic> &lt; .001, η<sup>2</sup><sub>p</sub> = .27, BF<sub>10</sub> = 8.50E4. In contrast, the inverted body did not show the interaction between arm height and arm direction, <italic>F</italic>(1,65) = 1.91, <italic>p</italic> = .171, η<sup>2</sup><sub>p</sub> = .029, BF<sub>10</sub> = 0.55. A main effect of body orientation (<italic>F</italic>(1,65) = 46.9, <italic>p</italic> &lt; .001, η<sup>2</sup><sub>p</sub> = .42, BF<sub>10</sub> = 2.92E6) showed that inversion diminished the overall negative adjustment error, indicating a reduced gravity bias (<xref ref-type="fig" rid="F5">Figure 5</xref>).</p><p id="P19">As in Experiment 2, bias indexes were also calculated for visualization purposes and for a more direct description of the inversion effect. As shown in <xref ref-type="fig" rid="F5">Figure 5b</xref>, inversion significantly reduced both gravity bias: M = 1.25, 95% CI = [0.88, 1.61], <italic>t</italic>(65) = 2.67, <italic>p</italic> &lt; .001, <italic>d</italic> = 0.84, BF<sub>10</sub> = 3.52E6 and biomechanical constraints: M = 0.61, 95% CI = [0.15, 1.06], <italic>t</italic>(65) = 2.67, <italic>p</italic> = .009, <italic>d</italic> = -0.33, BF<sub>10</sub> = 3.54. This experiment excluded the possibility that these biases emerge from part-based processing of bodies or by the stimuli’s low-level visual features.</p></sec></sec><sec id="S6" sec-type="discussion"><title>Discussion</title><p id="P20">The current results demonstrate that priors resulting from gravity and biomechanical constraints jointly shape visual memory representations of human body postures. In three experiments, these effects were replicated both by directly repeating the same task and by using a different task. Importantly, the biases were absent when bodies were inverted, ruling out low-level visual confounds and indicating that the biases emerge from whole-body representations. Together, the two tasks we used excluded potential confounds of motion perception, response biases and local visual feature processing, demonstrating a top-down influence on static body representations.</p><p id="P21">Our results are well explained by Bayesian theories in which perception is the result of an interplay between sensory input and priors <sup><xref ref-type="bibr" rid="R15">15</xref>,<xref ref-type="bibr" rid="R35">35</xref></sup>. Priors are shaped by environmental statistics <sup><xref ref-type="bibr" rid="R18">18</xref></sup> and serve to achieve optimal inference <sup><xref ref-type="bibr" rid="R17">17</xref></sup>. In the case of body postures, multiple regularities jointly shape the prior distribution of postures, including biomechanical constraints that confine postures to a certain range and gravity that pulls limbs downward. When input is more ambiguous, priors will have a stronger influence, such that we tend to perceive what is most likely according to our prior. In our task, a body posture had to be maintained in visual memory for a brief interval. The fidelity of the sensory information will be reduced during this interval, making the posture representation susceptible to the influence of priors. Following this account, the biases should become larger when the uncertainty about the stimuli is increased, which can be tested in future studies, for example by degrading the stimulus or by increasing the memory interval.</p><p id="P22">Besides gravity and biomechanical constraints, the prior distribution of postures is also influenced by other factors. For example, common postures and movements like standing, walking, and using tools will lead to a relatively high probability of arms being in the lower front position. This might also explain the current finding that the lower front arm postures were judged and adjusted more accurately than the other postures (see <xref ref-type="fig" rid="F3">Figure 3</xref>, <xref ref-type="fig" rid="F4">4a</xref>, <xref ref-type="fig" rid="F5">5a</xref>). Analyzing posture probabilities in large video databases and measuring biases at a higher resolution of the posture space would help establishing the link between posture probability and the biases observed here.</p><p id="P23">An alternative interpretation of the current findings is that they reflect visuomotor simulation, where observed postures are simulated in the viewer’s motor system as a mechanism to understand postures <sup><xref ref-type="bibr" rid="R36">36</xref></sup>. This interpretation has been used to account for the finding of smaller representational momentum for biomechanically awkward arm movements <sup><xref ref-type="bibr" rid="R37">37</xref></sup>. However, although the motor system has been shown to be activated during action observation <sup><xref ref-type="bibr" rid="R38">38</xref>,<xref ref-type="bibr" rid="R39">39</xref></sup>, motor simulation may not be essential for understanding actions or representing body postures, as individuals born without upper limbs exhibit similar performance in action observation, action prediction, and mental imagery of postures <sup><xref ref-type="bibr" rid="R40">40</xref>,<xref ref-type="bibr" rid="R41">41</xref></sup>. Knowing whether the effects observed here are also present in individuals born without limbs will give us more insight into the contributions of visual and motor experience. Furthermore, if motor experience plays a role in the visual memory of body postures, we might also expect an influence of commonly self-experienced postures <sup><xref ref-type="bibr" rid="R42">42</xref>,<xref ref-type="bibr" rid="R43">43</xref></sup>. Accordingly, factors that influence an individual’s motor experience, including age <sup><xref ref-type="bibr" rid="R44">44</xref></sup>, obesity <sup><xref ref-type="bibr" rid="R45">45</xref></sup>, pain <sup><xref ref-type="bibr" rid="R46">46</xref>,<xref ref-type="bibr" rid="R47">47</xref></sup>, and other clinical conditions, could potentially influence the biases reported here.</p><p id="P24">The current findings also raise new questions about the neural representation of body postures. Neuroimaging research on body perception has provided evidence for multiple cortical areas that are specifically engaged in body perception, including the extrastriate body area (EBA, Downing et al., 2001) and the fusiform body area (FBA, Peelen &amp; Downing, 2005; Schwarzlose et al., 2005). Compared to the more extensively studied fusiform face area (FFA, Kanwisher et al., 1997), little is known about the representational structure of these areas <sup><xref ref-type="bibr" rid="R51">51</xref></sup>. It has been shown that the FFA represents faces in a face space centered around the average face, with distances from the center representing the deviation from the mean face <sup><xref ref-type="bibr" rid="R52">52</xref>,<xref ref-type="bibr" rid="R53">53</xref></sup>. Based on the current results that knowledge of body structure informs posture perception, the body-selective areas may store an internal model of the body, including its constraints. Given the many combinations of body part postures, it would be advantageous for neurons to be tuned primarily to biomechanically possible postures. Indeed, previous work has shown that body representations in the EBA more strongly represent postures in commonly experienced visual field locations <sup><xref ref-type="bibr" rid="R54">54</xref></sup>. Our results suggest that the representational space of body postures in body-selective regions might be biased, reflecting perceived rather than physical distances between postures.</p><p id="P25">In sum, we show that body posture representation is biased towards the ground and towards biomechanically plausible postures, indicating an influence of both general knowledge of the world and specific knowledge of the body. These findings may reflect the influence of an internal model of the body based on environmental statistics. By employing such an encoding scheme, the visual system can efficiently predict upcoming postures, a critical component for humans’ ability to read others’ actions, intentions, and social interactions <sup><xref ref-type="bibr" rid="R55">55</xref></sup>.</p><sec id="S7"><title>Limitations of the study</title><p id="P26">This study revealed that the visual memory of a lifted-arm posture is modulated by knowledge about the world and the biomechanics of the body. A limitation of our study is that we only tested arm postures; future studies need to generalize our findings to other postures, for example leg postures. Another limitation is that our study could not address the origin of the prior. Future research is needed to determine whether visual experience, motor experience, or both contribute to the biases found here.</p></sec></sec><sec id="S8" sec-type="methods"><title>STAR★Methods</title><sec id="S9"><title>Resource Availability</title><sec id="S10"><title>Lead contact</title><p id="P27">Further information and requests for resources and reagents should be directed to and will be fulfilled by the lead contact, Qiu Han (<email>qiu.han@donders.ru.nl</email>).</p></sec><sec id="S11"><title>Materials availability</title><p id="P28">All the stimuli generated for this study are publicly available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/qmtkw/">https://osf.io/qmtkw/</ext-link>.</p></sec></sec><sec id="S12"><title>Experimental Model and Study Participant Details</title><p id="P29">All the studies were conducted on online platforms. Participants of Experiment 1 were recruited using the SONA system in return for course credits. Participants of Experiments 2 and 3 were recruited through Prolific in return for monetary reward. We required the participants to be above 18 years old with normal or corrected-to-normal vision. Digital informed consent was obtained from all participants. The procedures were approved by the University’s Ethical committee (Ethics no.: ECSW-2022-079).</p><p id="P30">The desired sample size was set to 60 for all experiments before testing. This was determined by power analysis using Jamovi suggesting that a sample size of 52 was needed to detect a minimum effect size of <italic>d</italic> = 0.4 with 80% power, as suggested recently as a first estimate for a reproducible effect size <sup><xref ref-type="bibr" rid="R56">56</xref></sup>. We rounded this recommendation up to 60, resulting in 80% power to detect a minimum effect size of <italic>d</italic> = 0.368. Recruitment stopped when the sample size reached 60 after the exclusion of low-quality data (see <xref ref-type="sec" rid="S14">METHOD DETAILS</xref>). For Experiment 1a, we recruited 75 participants. Of these, one participant did not finish the task and 14 were excluded. For Experiment 1b, we recruited 67 participants. Of these, two did not finish the task and five were excluded. We thus acquired an effective sample size of 60 for both Experiment 1a (49 females, 11 males; age: M = 20.1, range = [18, 36]) and Experiment 1b (48 females, 12 males, 2 other; age: M = 20.6, range = [18, 47]).</p><p id="P31">Experiment 2 adopted a within-subject design. The sample size was kept consistent with Experiment 1. 60 subjects (30 females, 30 males; age: M = 33.42, range = [21, 45]) were recruited and no one was excluded.</p><p id="P32">In Experiment 3, 66 participants (21 females, 45 males; age: M = 30.04, range = [18, 45]) were recruited. First, we recruited the intended sample size of 60, however, the number of participants starting with the upright vs inverted condition was not yet balanced when the sample size reached 60, therefore six additional participants were recruited. Data from only the first 60 participants yielded highly similar results.</p></sec><sec id="S13"><title>Method Details</title><sec id="S14"><title>Experiment 1</title><sec id="S15"><title>Stimuli</title><p id="P33">Body images were generated by rendering digital human models in DAZ studio 4.15 (Daz Productions, Inc). A female character and a male character were used. The characters were standing in profile, one arm lifted, the other leaning naturally on the hip. The lifted arm positions were categorized into four quadrants (<xref ref-type="fig" rid="F1">Figure 1</xref>): two directions (front and back) x two arm heights (upper and lower). In each quadrant, we designated the upper bound of that quadrant as the zero point, with larger angles meaning that the arm is lower, i.e., closer to the feet. The arm could be presented at an angle of 36, 39, 42, 45, 48, 51, or 54 degrees in each quadrant. Both left-facing and right-facing figures were generated, so that an arm in front of the body was equally often presented in the left and right visual field to avoid possible confounds of visual field differences between the conditions (<xref ref-type="fig" rid="F1">Figure 1e</xref>). The lifted arm was always on the viewer’s side (right arm lifted when facing right, left arm lifted when facing left) to avoid the arm being occluded by other body parts.</p><p id="P34">Mask images were grey-scale checkerboard images. Body images were 300 pixels wide, 480 pixels high. Size in degree depended on the online participant’s screen resolution and the eye-to-screen distance. Mask images were 350 pixels wide, 525 pixels high. Masks were presented slightly larger than the body to achieve a better masking effect.</p></sec><sec id="S16" sec-type="methods"><title>Procedures</title><p id="P35">Experimental procedures were programmed with jsPsych library <sup><xref ref-type="bibr" rid="R57">57</xref></sup> and the psychophysics plugin <sup><xref ref-type="bibr" rid="R58">58</xref></sup>. Experiments 1a and 1b tested the front and the back arm directions for upper postures (Experiment 1a) and lower postures (Experiment 1b). Data were aggregated for analysis. Experiment 1a also included a machine condition which was not relevant to the purpose of the current study (see <xref ref-type="supplementary-material" rid="SD1">Figure S2</xref>).</p><p id="P36">In each trial, a fixation cross was first presented at the center for 800, 900, or 1000 ms, then a target body posture of either 36, 39, 42, 45, 48, 51, or 54 degrees in either quadrant was shown for 200 ms (<xref ref-type="fig" rid="F2">Figure 2a</xref>). Participants were instructed to remember the posture of the target and hold it in memory. Immediately after the target, a 1000-ms dynamic mask consisting of four consecutive checkerboard images (250 ms each) was shown to minimize aftereffects and/or apparent motion of the arm, after which the probe image appeared for 200 ms. Compared to the target, the arm in the probe would move upwards or downwards by an angle of 3, 6, or 9 degrees (equiprobable). The task was to judge whether the arm had moved up or down relative to the target. Participants indicated their choice by pressing F or J on the keyboard. The key-response mapping was counterbalanced across participants. Participants were asked to respond as accurately and as quickly as possible. The trial ended upon response or 4000 ms after the probe had disappeared.</p><p id="P37">Each combination of arm direction and angle difference included 24 trials, resulting in 288 trials in total, separated into four blocks. Angle difference, arm direction, and figure gender were completely interleaved while facing direction was kept identical within blocks to avoid extra effort for switching viewpoint between trials. The two left-facing and two right-facing blocks were in ABBA order, with about half of the subjects starting with a left-facing block and the others starting with a right-facing block. A practice session of 12 trials was delivered before the formal experiment. Feedback on the accuracy and mean response time across conditions were shown to the participant at the end of each block.</p></sec></sec><sec id="S17"><title>Experiment 2</title><p id="P38">Experiment 2 included the same four conditions as in Experiment 1. In each quadrant, six target angles (36, 39, 42, 48, 51, and 54) were used. Consistent with Experiment 1, a fixation and then the target posture was shown, followed by the mask. After the mask disappeared, the subjects were instructed to press one of the left-arrow or right-arrow keys to show the test image for adjustment. The initial posture of the test was randomized between 30 and 60 degrees, but always in the same quadrant as the target. Participants then pressed up and down arrow keys to manipulate the arm of the test image to move upwards or downwards. After adjusting the arm to the remembered target position, participants pressed space to confirm their answer. If no response was made, the trial ended after 10 s. If the test image was not initiated within 3 s after the mask, the trial skipped and participants were warned to start the adjustment more quickly in the following trials.</p><p id="P39">All the other factors, were kept consistent with Experiment 1. Facing direction was blocked, and other factors were interleaved. Each angle in each quadrant was presented eight times, resulting in 48 trials for each quadrant, 192 trials in total. The trials were divided into four blocks, with the order manipulated as in Experiment 1. Two mini practice blocks were completed before the start of the experiment. Feedback on average absolute error was given at the end of each block.</p></sec><sec id="S18"><title>Experiment 3</title><p id="P40">The task was identical to Experiment 2 except that both upright and inverted conditions were tested. The inverted body images were generated by vertically flipping the upright images. Half of the participants started with the upright condition and half with the inverted condition. Both upright and inverted conditions contained a left-facing block and a right-facing block, with the order randomized across participants but kept the same for upright and inverted conditions. Within each block, trials of different combinations of arm direction, arm height, within-quadrant angle, and figure identities were interleaved. Because of the inclusion of the inverted condition, the trial number for each angle was halved compared to Experiment 2. In total, Upper-front, Upper-back, Lower-front, and Lower-back all included 24 trials for the upright and 24 for the inverted condition.</p></sec></sec><sec id="S19"><title>Quantification and Statistical Analysis</title><sec id="S20"><title>Experiment 1</title><p id="P41">Responses with RTs &lt; 250 ms (relative to probe onset) were excluded (0.10% of the total number of trials across Exp 1a and 1b), as these most likely reflect anticipatory responses. For each participant, data quality was inspected by plotting the percentage of up responses for each angle difference level in each quadrant (<xref ref-type="supplementary-material" rid="SD1">Figure S1a</xref>). Participants following task instructions should show an increase in the percentage of up responses as the angle difference decreases. That is, the more obvious that the arm moves up, the more likely people choose up, yielding a sigmoid curve. Some participants exhibited a flat or reversed curve, suggesting that they misunderstood the task or pressed randomly. These participants were detected using a slope index (<xref ref-type="supplementary-material" rid="SD1">Figure S1b</xref>) of the difference between the mean up response percentage of the two most obvious moving-up levels (-9 and -6) and the mean of the two most obvious moving-down levels (9 and 6). Participants with a slope index below 0.2 in either the front or the back condition were excluded, resulting in 14 exclusions in Experiment 1a and 5 exclusions in Experiment 1b.</p><p id="P42">We calculated individual criterion in each condition from the hit rate (up response percentage when the arm actually moved up) and false alarm rate (up response percentage when the arm actually moved down) using the Psycho package <sup><xref ref-type="bibr" rid="R59">59</xref></sup> in R <sup><xref ref-type="bibr" rid="R60">60</xref></sup>:</p><disp-formula id="FD1"><mml:math id="M1"><mml:mrow><mml:mtext>c</mml:mtext><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>z</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>hit</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mtext>z</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>false  alarm</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></disp-formula><p id="P43">Statistics were done using bruceR package in R and JASP (JASP Team, 2023). Both frequentist results and Bayesian factor (BF) were provided. By convention, a BF (the ratio of the probability of acquiring the data given one model against another) &gt; 3 indicates some evidence supporting the first model <sup><xref ref-type="bibr" rid="R61">61</xref></sup>. For BF ANOVA, each effect was tested by comparing models that contain that effect against their equivalent models stripped of the effect. Results of d prime were also analyzed and are presented in <xref ref-type="supplementary-material" rid="SD1">Figure S1</xref>. For simplicity, statistics of effects of interest are reported in the Results section and all the other statistics are provided in <xref ref-type="supplementary-material" rid="SD1">Table S1, S2, and S3</xref>.</p></sec><sec id="S21"><title>Experiment 2</title><p id="P44">Trials in which participants pressed space before adjusting the test image and trials in which the test image was not initiated within 3 s after the mask were discarded. Trials with an absolute error larger than 15 degrees were also discarded. Altogether, this led to the rejection of 5.41% of the total number of trials. No participants were excluded.</p><p id="P45">The error of the response from the target was used as the index of biases, calculated as the target angle minus the response angle. For example, a target of 48 degrees adjusted as 52 degrees would yield an error of -4 degrees. Negative values indicate that the posture was adjusted to be lower than it was actually shown. Errors of all trials in one quadrant were averaged to get the mean error for each quadrant of each participant. Group mean error data were tested in the same way as the criterion in Experiment 1, except that a repeated-measures ANOVA was used instead of a mixed ANOVA.</p></sec><sec id="S22"><title>Experiment 3</title><p id="P46">The coding of angles for the inverted condition followed a body-centered reference frame rather than a spatial reference frame. For example, negative errors for an upright body indicate that the arm was adjusted as closer to the feet and thus closer to the lower part of the screen. Similarly, negative errors for an inverted body indicate that the arm was adjusted as closer to the feet; however, because of the inversion and the reference to the body, this is now closer to the upper part of the screen.</p><p id="P47">Data cleaning procedures were identical to Experiment 2, with an exclusion rate of 6.90% of the total number of trials. Adjustment errors for each quadrant were averaged for upright and inverted conditions separately. A three-way repeated-measures ANOVA with arm height, arm direction, and body orientation was conducted. Two bias indexes for upright and inverted conditions were calculated and compared with two-tailed t-tests to test whether inversion diminished the biases.</p></sec></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Figures and Tables</label><media xlink:href="EMS158022-supplement-Supplementary_Figures_and_Tables.pdf" mimetype="application" mime-subtype="pdf" id="d25aAdEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S23"><title>Acknowledgments</title><p>We thank Paul E. Downing and Eelke Spaak for feedback on earlier versions of the manuscript and the Peelen Lab members for the helpful suggestions on experimental design during lab meetings.</p><p>This project has received funding from the China Scholarship Council (CSC), European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie (grant agreement No. 101033489), and European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No 725970).</p></ack><sec id="S24" sec-type="data-availability"><title>Data and code availability</title><p id="P48">All the codes, and raw data related to the experiments reported here are publicly available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/qmtkw/">https://osf.io/qmtkw/</ext-link>.</p><p id="P49">Additional information required to reanalyze the data reported in this paper is available from the lead contact upon request.</p></sec><fn-group><fn id="FN1" fn-type="con"><p id="P50"><bold>Author Contributions</bold></p><p id="P51">Qiu Han: Conceptualization, Methodology, Software, Investigation, Formal Analysis, Validation, Visualization, Writing – Original Draft, Writing – Review &amp; Editing, Funding Acquisition</p><p id="P52">Marco Gandolfo: Conceptualization, Methodology, Software, Validation, Writing – Review &amp; Editing, Funding Acquisition</p><p id="P53">Marius Peelen: Conceptualization, Methodology, Validation, Writing – Review &amp; Editing, Funding Acquisition</p></fn><fn id="FN2" fn-type="conflict"><p id="P54"><bold>Declaration of interests</bold></p><p id="P55">The authors declare no competing interests.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neri</surname><given-names>P</given-names></name><name><surname>Morrone</surname><given-names>MC</given-names></name><name><surname>Burr</surname><given-names>DC</given-names></name></person-group><article-title>Seeing biological motion</article-title><source>Nature</source><year>1998</year><volume>395</volume><fpage>894</fpage><lpage>896</lpage><pub-id pub-id-type="pmid">9804421</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reed</surname><given-names>CL</given-names></name><name><surname>Stone</surname><given-names>VE</given-names></name><name><surname>Bozova</surname><given-names>S</given-names></name><name><surname>Tanaka</surname><given-names>J</given-names></name></person-group><article-title>The Body-Inversion Effect</article-title><source>Psychol Sci</source><year>2003</year><volume>14</volume><fpage>302</fpage><lpage>308</lpage><pub-id pub-id-type="pmid">12807401</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stein</surname><given-names>T</given-names></name><name><surname>Sterzer</surname><given-names>P</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><article-title>Privileged detection of conspecifics: Evidence from inversion effects during continuous flash suppression</article-title><source>Cognition</source><year>2012</year><volume>125</volume><fpage>64</fpage><lpage>79</lpage><pub-id pub-id-type="pmid">22776239</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thorat</surname><given-names>S</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><article-title>Body shape as a visual feature: Evidence from spatially-global attentional modulation in human visual cortex</article-title><source>NeuroImage</source><year>2022</year><volume>255</volume><elocation-id>119207</elocation-id><pub-id pub-id-type="pmid">35427768</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Downing</surname><given-names>PE</given-names></name><name><surname>Jiang</surname><given-names>Y</given-names></name><name><surname>Shuman</surname><given-names>M</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><article-title>A Cortical Area Selective for Visual Processing of the Human Body</article-title><source>Science</source><year>2001</year><volume>293</volume><fpage>2470</fpage><lpage>2473</lpage><pub-id pub-id-type="pmid">11577239</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelen</surname><given-names>MV</given-names></name><name><surname>Downing</surname><given-names>PE</given-names></name></person-group><article-title>The neural basis of visual body perception</article-title><source>Nat Rev Neurosci</source><year>2007</year><volume>8</volume><fpage>636</fpage><lpage>648</lpage><pub-id pub-id-type="pmid">17643089</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grossman</surname><given-names>ED</given-names></name><name><surname>Blake</surname><given-names>R</given-names></name></person-group><article-title>Brain activity evoked by inverted and imagined biological motion</article-title><source>Vision Research</source><year>2001</year><volume>41</volume><fpage>1475</fpage><lpage>1482</lpage><pub-id pub-id-type="pmid">11322987</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gandolfo</surname><given-names>M</given-names></name><name><surname>Downing</surname><given-names>PE</given-names></name></person-group><article-title>Asymmetric visual representation of sex from human body shape</article-title><source>Cognition</source><year>2020</year><volume>205</volume><elocation-id>104436</elocation-id><pub-id pub-id-type="pmid">32919115</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reed</surname><given-names>CL</given-names></name><name><surname>Stone</surname><given-names>VE</given-names></name><name><surname>Grubb</surname><given-names>JD</given-names></name><name><surname>Mcgoldrick</surname><given-names>JE</given-names></name></person-group><article-title>Turning configural processing upside down: Part and whole body postures</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>2006</year><fpage>73</fpage><lpage>87</lpage><pub-id pub-id-type="pmid">16478327</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yin</surname><given-names>RK</given-names></name></person-group><article-title>Looking at upside-down faces</article-title><source>Journal of Experimental Psychology</source><year>1969</year><volume>81</volume><fpage>141</fpage><lpage>145</lpage><pub-id pub-id-type="doi">10.1037/h0027474</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berlucchi</surname><given-names>G</given-names></name><name><surname>Aglioti</surname><given-names>SM</given-names></name></person-group><article-title>The body in the brain revisited</article-title><source>Exp Brain Res</source><year>2010</year><volume>200</volume><fpage>25</fpage><lpage>35</lpage><pub-id pub-id-type="pmid">19690846</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Vignemont</surname><given-names>F</given-names></name></person-group><article-title>Body schema and body image—Pros and cons</article-title><source>Neuropsychologia</source><year>2010</year><volume>48</volume><fpage>669</fpage><lpage>680</lpage><pub-id pub-id-type="pmid">19786038</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruner</surname><given-names>JS</given-names></name><name><surname>Postman</surname><given-names>L</given-names></name></person-group><article-title>On the Perception of Incongruity: A Paradigm</article-title><source>Journal of Personality</source><year>1949</year><volume>18</volume><fpage>206</fpage><lpage>223</lpage><pub-id pub-id-type="pmid">15409076</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Lange</surname><given-names>FP</given-names></name><name><surname>Heilbron</surname><given-names>M</given-names></name><name><surname>Kok</surname><given-names>P</given-names></name></person-group><article-title>How Do Expectations Shape Perception?</article-title><source>Trends in Cognitive Sciences</source><year>2018</year><volume>22</volume><fpage>764</fpage><lpage>779</lpage><pub-id pub-id-type="pmid">30122170</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>W</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name><name><surname>Goldreich</surname><given-names>D</given-names></name></person-group><source>Bayesian Models of Perception and Action</source><publisher-name>MIT Press</publisher-name><year>2022</year><comment><ext-link ext-link-type="uri" xlink:href="https://mitpress.mit.edu/9780262047593/bayesian-models-of-perception-and-action/">https://mitpress.mit.edu/9780262047593/bayesian-models-of-perception-and-action/</ext-link>.</comment></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Reilly</surname><given-names>JX</given-names></name><name><surname>Jbabdi</surname><given-names>S</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name></person-group><article-title>How can a Bayesian approach inform neuroscience?</article-title><source>Eur J of Neuroscience</source><year>2012</year><volume>35</volume><fpage>1169</fpage><lpage>1179</lpage><pub-id pub-id-type="pmid">22487045</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pouget</surname><given-names>A</given-names></name><name><surname>Beck</surname><given-names>JM</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name></person-group><article-title>Probabilistic brains: knowns and unknowns</article-title><source>Nat Neurosci</source><year>2013</year><volume>16</volume><fpage>1170</fpage><lpage>1178</lpage><pub-id pub-id-type="pmcid">PMC4487650</pub-id><pub-id pub-id-type="pmid">23955561</pub-id><pub-id pub-id-type="doi">10.1038/nn.3495</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Girshick</surname><given-names>AR</given-names></name><name><surname>Landy</surname><given-names>MS</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><article-title>Cardinal rules: visual orientation perception reflects knowledge of environmental statistics</article-title><source>Nat Neurosci</source><year>2011</year><volume>14</volume><fpage>926</fpage><lpage>932</lpage><pub-id pub-id-type="pmcid">PMC3125404</pub-id><pub-id pub-id-type="pmid">21642976</pub-id><pub-id pub-id-type="doi">10.1038/nn.2831</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weiss</surname><given-names>Y</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name><name><surname>Adelson</surname><given-names>EH</given-names></name></person-group><article-title>Motion illusions as optimal percepts</article-title><source>Nat Neurosci</source><year>2002</year><volume>5</volume><fpage>598</fpage><lpage>604</lpage><pub-id pub-id-type="pmid">12021763</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freyd</surname><given-names>JJ</given-names></name><name><surname>Finke</surname><given-names>RA</given-names></name></person-group><article-title>Representational momentum</article-title><source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source><year>1984</year><volume>10</volume><fpage>126</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.1037/0278-7393.10.1.126</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McBeath</surname><given-names>MK</given-names></name><name><surname>Morikawa</surname><given-names>K</given-names></name><name><surname>Kaiser</surname><given-names>MK</given-names></name></person-group><article-title>Perceptual Bias for Forward-Facing Motion</article-title><source>Psychol Sci</source><year>1992</year><volume>3</volume><fpage>362</fpage><lpage>367</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.1992.tb00048.x</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubbard</surname><given-names>TL</given-names></name></person-group><article-title>Cognitive representation of linear motion: Possible direction and gravity effects in judged displacement</article-title><source>Memory &amp; Cognition</source><year>1990</year><volume>18</volume><fpage>299</fpage><lpage>309</lpage><pub-id pub-id-type="pmid">2355859</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubbard</surname><given-names>TL</given-names></name></person-group><article-title>Representational gravity: Empirical findings and theoretical implications</article-title><source>Psychon Bull Rev</source><year>2020</year><volume>27</volume><fpage>36</fpage><lpage>55</lpage><pub-id pub-id-type="pmid">31515734</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hafri</surname><given-names>A</given-names></name><name><surname>Boger</surname><given-names>T</given-names></name><name><surname>Firestone</surname><given-names>C</given-names></name></person-group><article-title>Melting Ice With Your Mind: Representational Momentum for Physical States</article-title><source>Psychol Sci</source><year>2022</year><volume>33</volume><fpage>725</fpage><lpage>735</lpage><pub-id pub-id-type="pmid">35471852</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parsons</surname><given-names>LM</given-names></name></person-group><article-title>Imagined spatial transformations of one’s hands and feet</article-title><source>Cognitive Psychology</source><year>1987</year><volume>19</volume><fpage>178</fpage><lpage>241</lpage><pub-id pub-id-type="pmid">3581757</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shiffrar</surname><given-names>M</given-names></name><name><surname>Freyd</surname><given-names>JJ</given-names></name></person-group><article-title>Apparent Motion of the Human Body</article-title><source>Psychol Sci</source><year>1990</year><volume>1</volume><fpage>257</fpage><lpage>264</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.1990.tb00210.x</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vandenberghe</surname><given-names>A</given-names></name><name><surname>Vannuscorps</surname><given-names>G</given-names></name></person-group><article-title>Predictive extrapolation of observed body movements is tuned by knowledge of the body biomechanics</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>2023</year><volume>49</volume><fpage>188</fpage><lpage>196</lpage><pub-id pub-id-type="pmid">36442047</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonnet</surname><given-names>C</given-names></name><name><surname>Paulos</surname><given-names>C</given-names></name><name><surname>Nithart</surname><given-names>C</given-names></name></person-group><article-title>Visual Representations of Dynamic Actions from Static Pictures</article-title><source>Perception</source><year>2005</year><volume>34</volume><fpage>835</fpage><lpage>846</lpage><pub-id pub-id-type="pmid">16124269</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jörges</surname><given-names>B</given-names></name><name><surname>López-Moliner</surname><given-names>J</given-names></name></person-group><article-title>Gravity as a Strong Prior: Implications for Perception and Action</article-title><source>Frontiers in Human Neuroscience</source><year>2017</year><volume>11</volume><pub-id pub-id-type="pmcid">PMC5408029</pub-id><pub-id pub-id-type="pmid">28503140</pub-id><pub-id pub-id-type="doi">10.3389/fnhum.2017.00203</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bertamini</surname><given-names>M</given-names></name></person-group><article-title>Memory for position and dynamic representations</article-title><source>Memory &amp; Cognition</source><year>1993</year><volume>21</volume><fpage>449</fpage><lpage>457</lpage><pub-id pub-id-type="pmid">8350736</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freyd</surname><given-names>JJ</given-names></name></person-group><article-title>Representing the dynamics of a static form</article-title><source>Mem Cogn</source><year>1983</year><volume>11</volume><fpage>342</fpage><lpage>346</lpage><pub-id pub-id-type="pmid">6633251</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="book"><collab>American Academy of Orthopaedic Surgeons</collab><source>Joint motion: method of measuring and recording</source><publisher-name>American Academy of Orthopaedic Surgeons</publisher-name><year>1965</year></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Norkin</surname><given-names>CC</given-names></name><name><surname>White</surname><given-names>DJ</given-names></name></person-group><source>Measurement Of Joint Motion: A Guide To Goniometry</source><publisher-name>FA Davis</publisher-name><year>2016</year></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brandman</surname><given-names>T</given-names></name><name><surname>Yovel</surname><given-names>G</given-names></name></person-group><article-title>Bodies are Represented as Wholes Rather Than Their Sum of Parts in the Occipital-Temporal Cortex</article-title><source>Cerebral Cortex</source><year>2016</year><volume>26</volume><fpage>530</fpage><lpage>543</lpage><pub-id pub-id-type="pmid">25217470</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knill</surname><given-names>DC</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><article-title>The Bayesian brain: the role of uncertainty in neural coding and computation</article-title><source>Trends in Neurosciences</source><year>2004</year><volume>27</volume><fpage>712</fpage><lpage>719</lpage><pub-id pub-id-type="pmid">15541511</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>M</given-names></name><name><surname>Knoblich</surname><given-names>G</given-names></name></person-group><article-title>The Case for Motor Involvement in Perceiving Conspecifics</article-title><source>Psychological Bulletin</source><year>2005</year><volume>131</volume><fpage>460</fpage><lpage>473</lpage><pub-id pub-id-type="pmid">15869341</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>M</given-names></name><name><surname>Lancaster</surname><given-names>J</given-names></name><name><surname>Emmorey</surname><given-names>K</given-names></name></person-group><article-title>Representational momentum for the human body: Awkwardness matters, experience does not</article-title><source>Cognition</source><year>2010</year><volume>116</volume><fpage>242</fpage><lpage>250</lpage><pub-id pub-id-type="pmcid">PMC2900450</pub-id><pub-id pub-id-type="pmid">20510405</pub-id><pub-id pub-id-type="doi">10.1016/j.cognition.2010.05.006</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buccino</surname><given-names>G</given-names></name><name><surname>Binkofski</surname><given-names>F</given-names></name><name><surname>Riggio</surname><given-names>L</given-names></name></person-group><article-title>The mirror neuron system and action recognition</article-title><source>Brain Lang</source><year>2004</year><volume>89</volume><fpage>370</fpage><lpage>376</lpage><pub-id pub-id-type="pmid">15068920</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rizzolatti</surname><given-names>G</given-names></name><name><surname>Craighero</surname><given-names>L</given-names></name></person-group><article-title>THE MIRROR-NEURON SYSTEM</article-title><source>Annu Rev Neurosci</source><year>2004</year><volume>27</volume><fpage>169</fpage><lpage>192</lpage><pub-id pub-id-type="pmid">15217330</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vannuscorps</surname><given-names>G</given-names></name><name><surname>Pillon</surname><given-names>A</given-names></name><name><surname>Andres</surname><given-names>M</given-names></name></person-group><article-title>Effect of biomechanical constraints in the hand laterality judgment task: where does it come from?</article-title><source>Front Hum Neurosci</source><year>2012</year><volume>6</volume><pub-id pub-id-type="pmcid">PMC3485652</pub-id><pub-id pub-id-type="pmid">23125830</pub-id><pub-id pub-id-type="doi">10.3389/fnhum.2012.00299</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vannuscorps</surname><given-names>G</given-names></name><name><surname>Caramazza</surname><given-names>A</given-names></name></person-group><article-title>Typical action perception and interpretation without motor simulation</article-title><source>Proc Natl Acad Sci USA</source><year>2016</year><volume>113</volume><fpage>186</fpage><lpage>91</lpage><pub-id pub-id-type="pmcid">PMC4711885</pub-id><pub-id pub-id-type="pmid">26699468</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1516978112</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Romano</surname><given-names>D</given-names></name><name><surname>Marini</surname><given-names>F</given-names></name><name><surname>Maravita</surname><given-names>A</given-names></name></person-group><article-title>Standard body-space relationships: Fingers hold spatial information</article-title><source>Cognition</source><year>2017</year><volume>165</volume><fpage>105</fpage><lpage>112</lpage><pub-id pub-id-type="pmid">28531806</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Romano</surname><given-names>D</given-names></name><name><surname>Mioli</surname><given-names>A</given-names></name><name><surname>D’Alonzo</surname><given-names>M</given-names></name><name><surname>Maravita</surname><given-names>A</given-names></name><name><surname>Di Lazzaro</surname><given-names>V</given-names></name><name><surname>Di Pino</surname><given-names>G</given-names></name></person-group><article-title>Behavioral and Physiological Evidence of a favored Hand Posture in the Body Representation for Action</article-title><source>Cerebral Cortex</source><year>2021</year><volume>31</volume><fpage>3299</fpage><lpage>3310</lpage><pub-id pub-id-type="pmcid">PMC8196246</pub-id><pub-id pub-id-type="pmid">33611384</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhab011</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zapparoli</surname><given-names>L</given-names></name><name><surname>Saetta</surname><given-names>G</given-names></name><name><surname>De Santis</surname><given-names>C</given-names></name><name><surname>Gandola</surname><given-names>M</given-names></name><name><surname>Zerbi</surname><given-names>A</given-names></name><name><surname>Banfi</surname><given-names>G</given-names></name><name><surname>Paulesu</surname><given-names>E</given-names></name></person-group><article-title>When I am (almost) 64: The effect of normal ageing on implicit motor imagery in young elderlies</article-title><source>Behavioural Brain Research</source><year>2016</year><volume>303</volume><fpage>137</fpage><lpage>151</lpage><pub-id pub-id-type="pmid">26851363</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scarpina</surname><given-names>F</given-names></name><name><surname>Paschino</surname><given-names>C</given-names></name><name><surname>Scacchi</surname><given-names>M</given-names></name><name><surname>Mauro</surname><given-names>A</given-names></name><name><surname>Sedda</surname><given-names>A</given-names></name></person-group><article-title>Does physical weight alter the mental representation of the body? Evidence from motor imagery in obesity</article-title><source>Q J Exp Psychol (Hove)</source><year>2022</year><volume>75</volume><fpage>2349</fpage><lpage>2365</lpage><pub-id pub-id-type="pmid">35001709</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Filbrich</surname><given-names>L</given-names></name><name><surname>Verfaille</surname><given-names>C</given-names></name><name><surname>Vannuscorps</surname><given-names>G</given-names></name><name><surname>Berquin</surname><given-names>A</given-names></name><name><surname>Barbier</surname><given-names>O</given-names></name><name><surname>Libouton</surname><given-names>X</given-names></name><name><surname>Fraselle</surname><given-names>V</given-names></name><name><surname>Mouraux</surname><given-names>D</given-names></name><name><surname>Legrain</surname><given-names>V</given-names></name></person-group><article-title>Atypical influence of biomechanical knowledge in Complex Regional Pain Syndrome-towards a different perspective on body representation</article-title><source>Sci Rep</source><year>2023</year><volume>13</volume><fpage>520</fpage><pub-id pub-id-type="pmcid">PMC9832000</pub-id><pub-id pub-id-type="pmid">36627332</pub-id><pub-id pub-id-type="doi">10.1038/s41598-023-27733-x</pub-id></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moseley</surname><given-names>GL</given-names></name><name><surname>Zalucki</surname><given-names>N</given-names></name><name><surname>Birklein</surname><given-names>F</given-names></name><name><surname>Marinus</surname><given-names>J</given-names></name><name><surname>van Hilten</surname><given-names>JJ</given-names></name><name><surname>Luomajoki</surname><given-names>H</given-names></name></person-group><article-title>Thinking about movement hurts: The effect of motor imagery on pain and swelling in people with chronic arm pain</article-title><source>Arthritis Care &amp; Research</source><year>2008</year><volume>59</volume><fpage>623</fpage><lpage>631</lpage><pub-id pub-id-type="pmid">18438892</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelen</surname><given-names>MV</given-names></name><name><surname>Downing</surname><given-names>PE</given-names></name></person-group><article-title>Selectivity for the Human Body in the Fusiform Gyrus</article-title><source>Journal of Neurophysiology</source><year>2005</year><volume>93</volume><fpage>603</fpage><lpage>608</lpage><pub-id pub-id-type="pmid">15295012</pub-id></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwarzlose</surname><given-names>RF</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><article-title>Separate Face and Body Selectivity on the Fusiform Gyrus</article-title><source>J Neurosci</source><year>2005</year><volume>25</volume><fpage>11055</fpage><lpage>11059</lpage><pub-id pub-id-type="pmcid">PMC6725864</pub-id><pub-id pub-id-type="pmid">16306418</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2621-05.2005</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>McDermott</surname><given-names>J</given-names></name><name><surname>Chun</surname><given-names>MM</given-names></name></person-group><article-title>The Fusiform Face Area: A Module in Human Extrastriate Cortex Specialized for Face Perception</article-title><source>J Neurosci</source><year>1997</year><volume>17</volume><fpage>4302</fpage><lpage>4311</lpage><pub-id pub-id-type="pmcid">PMC6573547</pub-id><pub-id pub-id-type="pmid">9151747</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.17-11-04302.1997</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Downing</surname><given-names>PE</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><article-title>The role of occipitotemporal body-selective regions in person perception</article-title><source>Cognitive Neuroscience</source><year>2011</year><volume>2</volume><fpage>186</fpage><lpage>203</lpage><pub-id pub-id-type="pmid">24168534</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leopold</surname><given-names>DA</given-names></name><name><surname>O’Toole</surname><given-names>AJ</given-names></name><name><surname>Vetter</surname><given-names>T</given-names></name><name><surname>Blanz</surname><given-names>V</given-names></name></person-group><article-title>Prototype-referenced shape encoding revealed by high-level aftereffects</article-title><source>Nat Neurosci</source><year>2001</year><volume>4</volume><fpage>89</fpage><lpage>94</lpage><pub-id pub-id-type="pmid">11135650</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loffler</surname><given-names>G</given-names></name><name><surname>Yourganov</surname><given-names>G</given-names></name><name><surname>Wilkinson</surname><given-names>F</given-names></name><name><surname>Wilson</surname><given-names>HR</given-names></name></person-group><article-title>fMRI evidence for the neural representation of faces</article-title><source>Nat Neurosci</source><year>2005</year><volume>8</volume><fpage>1386</fpage><lpage>1391</lpage><pub-id pub-id-type="pmid">16136037</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chan</surname><given-names>AW-Y</given-names></name><name><surname>Kravitz</surname><given-names>DJ</given-names></name><name><surname>Truong</surname><given-names>S</given-names></name><name><surname>Arizpe</surname><given-names>J</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><article-title>Cortical representations of bodies and faces are strongest in commonly experienced configurations</article-title><source>Nat Neurosci</source><year>2010</year><volume>13</volume><fpage>417</fpage><lpage>418</lpage><pub-id pub-id-type="pmcid">PMC2846985</pub-id><pub-id pub-id-type="pmid">20208528</pub-id><pub-id pub-id-type="doi">10.1038/nn.2502</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quadflieg</surname><given-names>S</given-names></name><name><surname>Koldewyn</surname><given-names>K</given-names></name></person-group><article-title>The neuroscience of people watching: how the human brain makes sense of other people’s encounters</article-title><source>Annals of the New York Academy of Sciences</source><year>2017</year><volume>1396</volume><fpage>166</fpage><lpage>182</lpage><pub-id pub-id-type="pmid">28405964</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brysbaert</surname><given-names>M</given-names></name></person-group><article-title>How many participants do we have to include in properly powered experiments? A tutorial of power analysis with reference tables</article-title><source>Journal of Cognition</source><year>2019</year><volume>2</volume><fpage>16</fpage><pub-id pub-id-type="pmcid">PMC6640316</pub-id><pub-id pub-id-type="pmid">31517234</pub-id><pub-id pub-id-type="doi">10.5334/joc.72</pub-id></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Leeuw</surname><given-names>JR</given-names></name></person-group><article-title>jsPsych: A JavaScript library for creating behavioral experiments in a Web browser</article-title><source>Behav Res</source><year>2015</year><volume>47</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="pmid">24683129</pub-id></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuroki</surname><given-names>D</given-names></name></person-group><article-title>A new jsPsych plugin for psychophysics, providing accurate display duration and stimulus onset asynchrony</article-title><source>Behav Res</source><year>2021</year><volume>53</volume><fpage>301</fpage><lpage>310</lpage><pub-id pub-id-type="doi">10.3758/s13428-020-01445-w</pub-id></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Makowski</surname><given-names>D</given-names></name></person-group><article-title>The psycho Package: an Efficient and Publishing-Oriented Workflow for Psychological Science</article-title><source>Journal of Open Source Software</source><year>2018</year><volume>3</volume><fpage>470</fpage><pub-id pub-id-type="doi">10.21105/joss.00470</pub-id></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="book"><collab>R Core Team</collab><source>R A language and environment for statistical computing</source><publisher-name>R Foundation for Statistical Computing</publisher-name><publisher-loc>Vienna, Austria</publisher-loc><year>2021</year><comment>URL <ext-link ext-link-type="uri" xlink:href="https://www.R-project.org/">https://www.R-project.org/</ext-link></comment></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmalz</surname><given-names>X</given-names></name><name><surname>Biurrun Manresa</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name></person-group><article-title>What is a Bayes factor?</article-title><source>Psychological Methods</source><year>2021</year><volume>28</volume><pub-id pub-id-type="pmid">34780246</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><p>a)~d), Illustration of the hypothesis. The orange and blue arrows indicate the direction of the gravity and the biomechanical constraints, respectively; transparent arms indicate the predicted perceived arm positions according the two hypotheses. Here the arrows and the predicted arms only suggest the direction but not the extent of the effects. For the Upper-back posture (b), gravity and the biomechanical constraints point in opposite direction, potentially eliminating each other’s influence. For the lower-back posture (d), gravity and the biomechanical constraints go in the same direction, their effects potentially adding up. e), Stimulus examples. For each quadrant, we show the lowest posture (54°) and the highest posture (36°) used in the experiments. In the experiment, both figures had left-facing and right-facing versions, though only one of them is shown here.</p></caption><graphic xlink:href="EMS158022-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Trial Procedures.</title><p>a) Change discrimination task used in Experiments 1a and 1b. In the trial shown here, the target is 45 degrees in the Upper-front, and the probe moves -9 degrees (i.e., upwards). Participants indicated whether the arm had moved up or down. The up-down text screen is shown for illustration purposes. b) Adjustment task used in Experiments 2 and 3. The target posture was either 36, 39, 42, 48, 51, or 54 degrees within each quadrant. The starting posture in the test image was chosen randomly from 30 to 60 from the same quadrant as the target. Participants adjusted the arm, indicated by transparent arms, using the up-arrow and down-arrow keys to match the target angle.</p></caption><graphic xlink:href="EMS158022-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><p>Criterion results for the four conditions in Experiment 1. A negative criterion reflects a bias to respond “up”, indicating that the first posture was remembered as lower than the second posture. We interpret this overall bias as reflecting knowledge of gravity. The difference between Front and Back indicates that the criterion was influenced by whether the arm is at an extreme posture. Results showed an interaction between Front/Back and Upper/Lower, in line with biomechanical constraints (see <xref ref-type="fig" rid="F1">Figure 1</xref>).</p><p>***: p &lt; .001, **: p &lt; .01, *: p &lt; .05, n.s.: not significant. Error bars denote 95% CI.</p></caption><graphic xlink:href="EMS158022-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Results of Experiment 2.</title><p>a) Mean error of the four conditions. b) Bias Indexes for individual participants. On the top are the calculation methods for the two indexes. ***: p &lt; .001, **: p &lt; .01, *: p &lt; .05, n.s.: not significant. Error bars denote 95% CI.</p></caption><graphic xlink:href="EMS158022-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Results of Experiment 3.</title><p>a) Mean error of the four conditions. Left: upright, Right: inverted. b) Bias Indexes for individual participants. On top are the calculation methods for the two indexes. ***: p &lt; .001, **: p &lt; .01, *: p &lt; .05, n.s.: not significant. Error bars denote 95% CI.</p></caption><graphic xlink:href="EMS158022-f005"/></fig></floats-group></article>