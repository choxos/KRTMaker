<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS147055</article-id><article-id pub-id-type="doi">10.1101/2020.09.26.314815</article-id><article-id pub-id-type="archive">PPR219197</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Predictive Maps in Rats and Humans for Spatial Navigation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>de Cothi</surname><given-names>William</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">†</xref></contrib><contrib contrib-type="author"><name><surname>Nyberg</surname><given-names>Nils</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Griesbauer</surname><given-names>Eva-Maria</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Ghanamé</surname><given-names>Carole</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Zisch</surname><given-names>Fiona</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Lefort</surname><given-names>Julie M.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Fletcher</surname><given-names>Lydia</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Newton</surname><given-names>Coco</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Renaudineau</surname><given-names>Sophie</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Bendor</surname><given-names>Daniel</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Grieves</surname><given-names>Roddy</given-names></name><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Duvelle</surname><given-names>Éléonore</given-names></name><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Barry</surname><given-names>Caswell</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Spiers</surname><given-names>Hugo J.</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="corresp" rid="CR1">†</xref></contrib></contrib-group><aff id="A1"><label>1</label>Department of Cell and Developmental Biology, University College London, UK</aff><aff id="A2"><label>2</label>Institute of Behavioural Neuroscience, Department of Experimental Psychology, Division of Psychology and Language Sciences, University College London, UK</aff><aff id="A3"><label>3</label>The Bartlett School of Architecture, University College London, UK</aff><aff id="A4"><label>4</label>Department of Clinical Neurosciences, University of Cambridge, UK</aff><aff id="A5"><label>5</label>Department of Psychological and Brain Sciences, Dartmouth College, Hanover, NH, USA</aff><author-notes><corresp id="CR1"><label>†</label>Corresponding authors <email>h.spiers@ucl.ac.uk</email>, <email>w.decothi@ucl.ac.uk</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>04</day><month>07</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>28</day><month>06</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><title>Summary</title><p id="P1">Much of our understanding of navigation comes from the study of individual species, often with specific tasks tailored to those species. Here, we provide a novel experimental and analytic framework, integrating across humans, rats and simulated reinforcement learning (RL) agents to interrogate the dynamics of behaviour during spatial navigation. We developed a novel open-field navigation task (‘Tartarus Maze’) requiring dynamic adaptation (shortcuts and detours) to frequently changing obstructions in the path to a hidden goal. Humans and rats were remarkably similar in their trajectories. Both species showed the greatest similarity to RL agents utilising a ‘successor representation’, which creates a predictive map. Humans also displayed trajectory features similar to model-based RL agents, which implemented an optimal tree-search planning procedure. Our results help refine models seeking to explain mammalian navigation in dynamic environments, and highlight the utility of modelling the behaviour of different species to uncover the shared mechanisms that support behaviour.</p></abstract><kwd-group><kwd>Artificial Intelligence</kwd><kwd>Reinforcement Learning</kwd><kwd>Spatial Memory</kwd><kwd>Decision-making</kwd><kwd>Planning</kwd><kwd>Hippocampus</kwd><kwd>Cognitive Map</kwd><kwd>Successor Representation</kwd><kwd>Detour</kwd><kwd>Shortcut</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Adapting to change is fundamental for survival. Adapting to changes in the structure of the environment has been studied in a huge diversity of psychological experiments in humans<sup><xref ref-type="bibr" rid="R1">1</xref></sup>, but also more ethologically in a remarkable range of different species<sup><xref ref-type="bibr" rid="R2">2</xref></sup>. One challenge that unites all motile animals on our planet is spatial navigation. In particular, prime examples are finding a new path when a familiar route is blocked and exploiting a novel shortcut. Efficient detours and shortcuts are considered the hallmarks of a cognitive map - an internal representation of the environment that enables novel inferences to guide behaviour<sup><xref ref-type="bibr" rid="R3">3</xref>–<xref ref-type="bibr" rid="R6">6</xref></sup>.</p><p id="P3">Both rodents and humans can show an impressive capacity to identify shortcuts and take optimal detours<sup><xref ref-type="bibr" rid="R5">5</xref>,<xref ref-type="bibr" rid="R7">7</xref>–<xref ref-type="bibr" rid="R16">16</xref></sup>. However, not all studies report successful adaptive behaviour<sup><xref ref-type="bibr" rid="R17">17</xref></sup>. Rats often require multiple exposures to a set of paths before they are able to shift towards an optimal detour <sup><xref ref-type="bibr" rid="R9">9</xref></sup>, and may fail to select an optimal shortcut from a set of novel paths <sup><xref ref-type="bibr" rid="R18">18</xref></sup>. Humans too can be poor at judging the directions between locations in walled mazes, hindering the capacity to identify shortcuts<sup><xref ref-type="bibr" rid="R19">19</xref>,<xref ref-type="bibr" rid="R20">20</xref></sup>.</p><p id="P4">Much of the research into navigation implicitly assumes that rodents and humans navigate in a fundamentally similar way<sup><xref ref-type="bibr" rid="R4">4</xref>,<xref ref-type="bibr" rid="R21">21</xref></sup> and this has been used to support the integration of insights across both species<sup><xref ref-type="bibr" rid="R1">1</xref>,<xref ref-type="bibr" rid="R5">5</xref>,<xref ref-type="bibr" rid="R22">22</xref>–<xref ref-type="bibr" rid="R25">25</xref></sup>. In mammals, the hippocampus is thought to form a cognitive map<sup><xref ref-type="bibr" rid="R24">24</xref></sup>, evidenced by spatial-tuned cells (such as ‘place cells’) in the hippocampal formation of rodents and humans<sup><xref ref-type="bibr" rid="R26">26</xref>–<xref ref-type="bibr" rid="R28">28</xref></sup>. However, despite the wide array of human and rodent research, few experiments have sought to compare rodents and humans on a directly homologous task. Understanding the similarities and differences of these two species on the same task would be useful for allowing the better integration of findings from different methods, such as combining data from neuroimaging in humans with neural recordings and disruption methods in rodents<sup><xref ref-type="bibr" rid="R29">29</xref>–<xref ref-type="bibr" rid="R31">31</xref></sup>. Moreover, such integration could potentially benefit the translation of assessments in rodents to assessments for clinical trials in humans, for example where tests of spatial navigation may be important for the early detection of Alzheimer’s disease<sup><xref ref-type="bibr" rid="R32">32</xref>–<xref ref-type="bibr" rid="R34">34</xref></sup>.</p><p id="P5">When considering how humans and rodents might differ during navigation, differences in sensory perception are important. Whilst humans have binocular vision, they may differ in olfaction<sup><xref ref-type="bibr" rid="R35">35</xref></sup> and lack the tactility of whiskers. Meanwhile, rodents have a larger visual field of view, lower visual acuity and can move their eyes independently<sup><xref ref-type="bibr" rid="R36">36</xref></sup>. In terms of neuroanatomy, the prefrontal cortical regions associated with spatial planning differ greatly between rodents and primates<sup><xref ref-type="bibr" rid="R37">37</xref>,<xref ref-type="bibr" rid="R38">38</xref></sup>; while the hippocampus and surrounding structures associated with spatial representations are relatively similar<sup><xref ref-type="bibr" rid="R39">39</xref></sup>. Given these similarities and differences, it is possible that rodents and humans navigate in a similar fashion or show pronounced differences in certain situations. Understanding such patterns in behaviour is important not only for understanding navigation, but how the behaviour of different species is inter-related and may have emerged through evolutionary pressure.</p><p id="P6">One approach for identifying potential cross-species mechanisms underlying goal-directed behaviour is through comparison with reinforcement learning models<sup><xref ref-type="bibr" rid="R40">40</xref>–<xref ref-type="bibr" rid="R45">45</xref></sup>. Reinforcement learning (RL) is an area of machine learning that addresses the theoretical problem of how a learner and decision maker, called an agent, should act in an environment in order to achieve a certain goal - for which it earns rewards. Specifically, the agent is not told which actions it should take, but instead must learn the actions that maximise its expected future rewards - known as value. Such RL models can be used to examine how rapid learning and control can be developed in artificial systems, outcompeting human performance<sup><xref ref-type="bibr" rid="R40">40</xref>,<xref ref-type="bibr" rid="R46">46</xref>–<xref ref-type="bibr" rid="R48">48</xref></sup>, or used for comparison to patterns seen in animals or human behaviour<sup><xref ref-type="bibr" rid="R45">45</xref>,<xref ref-type="bibr" rid="R49">49</xref>–<xref ref-type="bibr" rid="R52">52</xref></sup>.</p><p id="P7">Solutions to reinforcement learning problems have traditionally been divided into two categories: model-based methods that afford the agent a model of the environment, used to decide actions via a planning procedure<sup><xref ref-type="bibr" rid="R53">53</xref></sup>, and model-free methods that learn from experience which actions lead to the most rewarding future<sup><xref ref-type="bibr" rid="R54">54</xref>,<xref ref-type="bibr" rid="R55">55</xref></sup>. Provided that the model implemented in a model-based algorithm contains an accurate depiction of the environment, model-based methods are typically able to respond quickly and optimally to environmental perturbations. However, the planning procedure - for example a tree search<sup><xref ref-type="bibr" rid="R47">47</xref></sup> - required to successfully exploit the model brings with it computational complexity and overhead, particularly in large state spaces with deep transition structures such as navigating a city.</p><p id="P8">In contrast to model-based methods, model-free methods are generally more simple and computationally inexpensive through a reliance on temporal-difference learning rules<sup><xref ref-type="bibr" rid="R54">54</xref></sup>, however this comes with a reduced flexibility to environmental changes. As such, model-free mechanisms are often associated with the formation of habits<sup><xref ref-type="bibr" rid="R56">56</xref>,<xref ref-type="bibr" rid="R57">57</xref></sup>. To achieve their simplicity, model-free methods typically learn by directly estimating the value of taking a particular action in a particular state. This makes it easy to then compare the values of different actions available to the agent, without the need to know how the states are interconnected.</p><p id="P9">Whilst model-free and model-based methods appear to function at opposite ends of an algorithmic spectrum, intermediary methods do exist. One such algorithm that has increased in application recently is the successor representation<sup><xref ref-type="bibr" rid="R58">58</xref></sup> (SR). The SR somewhat combines parts of model-free and model-based learning<sup><xref ref-type="bibr" rid="R59">59</xref>,<xref ref-type="bibr" rid="R60">60</xref></sup> by using experience to learn a predictive map between the states in an environment. This predictive map can be readily combined with a separately learned reward associated with each state, in order to explicitly compute value. Thus the SR negates the need for a complicated planning procedure in order to use the predictive map to guide action selection.</p><p id="P10">The SR has been able to provide a good account of behaviour and hippocampal representations in humans<sup><xref ref-type="bibr" rid="R61">61</xref>–<xref ref-type="bibr" rid="R65">65</xref></sup> and rodents<sup><xref ref-type="bibr" rid="R50">50</xref>,<xref ref-type="bibr" rid="R66">66</xref>,<xref ref-type="bibr" rid="R67">67</xref></sup>. The tasks often used to draw these comparisons with RL agents typically focus on small state-spaces, with 2-step transition structures – as such the extent of planning often requires one or two actions. Furthermore, due to the conceptual nature of the underlying task space, translational research usually requires differing sensory implementations for humans<sup><xref ref-type="bibr" rid="R68">68</xref></sup> and rodents<sup><xref ref-type="bibr" rid="R69">69</xref></sup>.</p><p id="P11">Here, we created a configurable open-field maze with a layout of barriers that reconfigured after a set of trials (Tartarus Maze). We tested the navigation of rats in a physical instantiation of the maze, humans via an immersive head-mounted display virtual environment and RL agents in a simulation. Using a range of analytic methods we probed how rat and human spatial behaviours compare to each other and to model-free, model-based and SR reinforcement learners. We found a strong similarity in the occupancy patterns of rats and humans. Both rats and humans showed the greatest likelihood and trajectory similarity to SR based RL agents, with humans also displaying trajectory features similar to model-based RL agents implementing an optimal planning procedure in early trials on a new maze configuration.</p></sec><sec id="S2" sec-type="results"><title>Results</title><p id="P12">Navigation was tested in a large square environment with a fixed hidden goal location and a prominent directional black wall cue in one direction (<xref ref-type="fig" rid="F1">Fig. 1</xref>, <xref ref-type="supplementary-material" rid="SD1">Video S1</xref> - rats, and <xref ref-type="supplementary-material" rid="SD1">Video S2</xref> - humans). The maze was divided in a 10x10 grid of moveable sections that could either be removed, leaving impassable gaps to force detour taking, or added, creating shortcuts. The speed and size of the humans in the virtual environment was set to match a rat travelling at 20cm/s. During training, all 10x10 maze modules were present and the rats and humans were trained to reach the goal within a 45s time limit, (<xref ref-type="fig" rid="F1">Fig 1A</xref>), while RL agents were initialised with the optimal value function. During the testing phase of the experiment, maze modules were removed to make specific maze configurations that block the direct route to the goal (<xref ref-type="fig" rid="F1">Fig 1B</xref>). Humans (n=18), rats (n=9) and agents were tested on the same sequence of 25 maze configurations each with 10 trials in which a set of defined starting locations were selected to optimally probe navigation (<xref ref-type="fig" rid="F1">Fig 1C</xref>). See Methods for details.</p><sec id="S3"><title>Behavioural performance is relatively similar between rats and humans</title><p id="P13">We first asked how well humans (<xref ref-type="fig" rid="F2">Fig 2A</xref>) and rodents (<xref ref-type="fig" rid="F2">Fig 2B</xref>) were able to complete the task during the test sessions. As expected, repeated exposure over trials to a new maze configuration corresponded to a general increase in the ability of both the humans and rats to navigate to the goal within the 45s time limit (<xref ref-type="fig" rid="F2">Fig 2C</xref>; first 5 trials vs last 5 trials: humans t(17)=6.3, <italic>p</italic>&lt;.001; rats t(8)=4.0, <italic>p</italic>= .004). Humans were also generally better than the rats at finding the goal across the 25 maze configurations (<xref ref-type="fig" rid="F2">Fig 2D</xref>; humans vs rats: t(25)=3.0, <italic>p</italic>=.006). There were 3 maze configurations in which rats outperformed humans (2, 10 and 19). We saw a strong correlation between the occupancy of the rats and human participants (occupancy correlation, humans vs rats: ρ=.67), in particular towards the later trials when both were better at navigating to the goal (<xref ref-type="fig" rid="F2">Fig 2E</xref>; occupancy correlations for first 5 trials vs last 5 trials: t(8)=3.2, <italic>p</italic>=.013). The routes used were also more efficient (<xref ref-type="fig" rid="F2">Fig 2E</xref>; deviation from optimal path, first 5 trials vs last 5 trials: human t(17)=-5.0, <italic>p</italic>&lt;.001; rats t(8)=-4.0, <italic>p</italic>=.004) with humans generally choosing more optimal routes than the rats (deviation from optimal path humans vs rats: t(25)=-8.2, <italic>p</italic>&lt;.001). Inspection of trajectories showed that in some cases near optimal paths could be observed in both species even on the first trial of a maze configuration (see <xref ref-type="supplementary-material" rid="SD1">Video S1&amp;2</xref>).</p></sec><sec id="S4"><title>Observations of the behaviour of the RL agents</title><p id="P14">Examining the trajectories of the RL agents we observed a number of consistent patterns. As expected, model-free agents were relatively unable to adapt to changes in the maze layout; whenever there was a barrier obstructing the learnt route to the goal the model-free agent would remain in a similar region of space and often fail when the new path required travelling away from the goal or around obstacles (<xref ref-type="supplementary-material" rid="SD1">Video S3</xref>). This behaviour logically follows from the fact that it has no representation of the transition structure and relies on previously cached optimal actions to select which transitions to make. The model-based RL agents generally chose more optimal routes, especially as the trials progressed, although can initially be seen to occasionally make poor choices in paths (<xref ref-type="supplementary-material" rid="SD1">Video S4</xref>). This is consistent with them requiring an accurate model of the environment in order to conduct a useful tree-search over routes to the goal. However when a change in the transition structure occurred and that model was no longer accurate, they do not have cached values to rely upon and must extensively explore to acquire a model of the new environment they can exploit. SR RL agents initially appear to make similar errors to model-free agents, but adapt more efficiently to the change in the transition structure, for example avoiding deadends after a few trials (<xref ref-type="supplementary-material" rid="SD1">Video S5</xref>). This is consistent with them updating a stored transition structure using past experience. Thus, unlike the model-based agents, SR agents have a learned set of biases they will fallback to which can aid choice-making after changes in the environment.</p></sec><sec id="S5"><title>Likelihood analysis of actions reveals rats and humans are both most similar to an SR agent</title><p id="P15">We next investigated how the human and rat trajectories compared to the RL agents’ representation of optimal actions. To do this, we computed the likelihood of the human and rat behaviour matching each model by restricting the RL agents to follow the biological trajectories. We then used the internal value estimates of the agents to compute a softmax probability distribution over the available actions at each timestep. Using these probabilities to compute the likelihood of the biological data for each agent, we calculated the maximum likelihood parameter estimates for each model’s learning rate and discount factor across individual humans (<xref ref-type="supplementary-material" rid="SD1">Table S1</xref>) and rats (<xref ref-type="supplementary-material" rid="SD1">Table S2</xref>).</p><p id="P16">Comparing the model-free, model-based and SR algorithms, the value representation of the SR agent consistently provided the most likely fit to the biological behaviour (<xref ref-type="fig" rid="F3">Fig 3A</xref>; Likelihood-Ratio (LR) test: SR vs MF for human data ln(LR) = 1911.1; SR vs MB for human data ln(LR) = 538.2; SR vs MF for rat data ln(LR) = 842.0; SR vs MB for rat data ln(LR) = 225.2), with the model-free agent consistently providing the worst fit (MF vs MB for human data ln(LR) = -1372.9; MF vs MB for rat data ln(LR) = -616.9). Consequently, the SR agent was the maximum likelihood model for 70% of the human trials and 60% of the rat trials (<xref ref-type="fig" rid="F3">Fig 3B</xref>). Normalising these likelihoods by trial length and using a uniform random walk as a baseline, we observed this trend was robust throughout the time spent on a maze configuration (<xref ref-type="fig" rid="F3">Fig 3C</xref>) and across individuals (SR vs MF for human data: t(17) = 29.2 <italic>p</italic>&lt;.001; SR vs MB for human data: t(17) = 11.9, <italic>p</italic>&lt;.001; SR vs MF for rat data: t(8) = 13.0, <italic>p</italic>&lt;.001; SR vs MB for rat data: t(8) = 9.6, <italic>p</italic>&lt;.001). We also observed that the agent likelihoods for humans and rats varied across maze configurations (<xref ref-type="fig" rid="F3">Fig 3D</xref>), with a strong correlation between the fits to the biological data (r=.57, <italic>p</italic>&lt;.001).</p></sec><sec id="S6"><title>Simulating agents using parameters derived from the human and rat data reveals closest match to SR agent</title><p id="P17">To investigate whether these differences in agent likelihoods transfer into measurable differences in the resulting behaviour, we simulated agent trajectories according to each rat and human participant using their individual maximum likelihood parameters (<xref ref-type="supplementary-material" rid="SD1">Table S1 &amp; S2</xref>). Importantly, these agents were trained on the trajectories taken by that individual on all maze configurations prior to the one being simulated. The agents then carried over all model and value representations learnt across the 10 trials on the simulated maze configuration. To generate the behaviour, the agents followed an ε-greedy policy that linearly decayed from ε=0.1 to ε=0.01 across the 10 trials on a maze configuration. This means for the first trial on a new maze configuration, the agents exploit (i.e. choose the action with maximum expected value) 90% of the time and explore (i.e. choose a random action) on the remaining 10%. Then for each subsequent trial, the agents increase their proportion of time spent exploiting by 1%. To accurately depict the distribution of trajectories generated by an agent under such a policy, we simulated each RL algorithm 100 times per rat and human participant, with the maximum number of state transitions each agent could make set to match the maximum possible for a rat travelling along the grid axes at 20cm/s (i.e. max 45 transitions per trial, 1 transition per second). In the subsequent analyses, individual rats and human participants are compared to the RL agent simulations trained on their individual behaviour, using the maximum likelihood parameters fit to their individual behaviour (<xref ref-type="supplementary-material" rid="SD1">Table S1 &amp; S2</xref>)</p><p id="P18">The model-based algorithm generally outperformed the biological behaviour (<xref ref-type="fig" rid="F4">Fig 4A-B</xref>), particularly on the first few trials of a new maze configuration (paired t-test, proportion goal reached on first 5 trials: MB vs humans, t(17)=2.74, <italic>p</italic>=.014; MB vs rats, t(8)=3.20, <italic>p</italic>=.013; last 5 trials: MB vs humans, t(17)=0.45, <italic>p</italic>=.656; MB vs rats, t(8)=2.56, <italic>p</italic>=.034). The model-based algorithm also consistently outperformed the other RL agents (<xref ref-type="fig" rid="F4">Fig 4A-B</xref>; paired t-test, proportion goal reached, human simulations: MB vs SR, t(17)=22.8, <italic>p</italic>&lt;.001; MB vs MF, t(17)=167, <italic>p</italic>&lt;.001; rat simulations: MB vs SR t(8)=29.0, <italic>p</italic>&lt;.001, MB vs MF t(8)=119, <italic>p</italic>&lt;.001), with the model-free agent performing worst (human parameters: MF vs SR, t(17)=-83.8, <italic>p</italic>&lt;.001; rat parameters: MF vs SR, t(8)=-47.0, <italic>p</italic>&lt;.001). As with the humans and rats, the model-based and SR agents progressively improved throughout the trials on a given maze configuration (<xref ref-type="fig" rid="F4">Fig 4A-B</xref>; first 5 vs last 5 trials, human simulations: MB, t(17)=-40.6, <italic>p</italic>&lt;.001; SR, t(17)=-23.6, <italic>p</italic>&lt;.001; rat simulations: MB t(8)=-18.6, <italic>p</italic>&lt;.001; SR: t(8)=-18.2, <italic>p</italic>&lt;.001). Meanwhile, the model-free agents became progressively worse at reaching the goal (first 5 vs last 5 trials: human parameters, t(17)=35.8, <italic>p</italic>&lt;0.001; rat parameters, t(8)=16.5, <italic>p</italic>&lt;.001) – indicative of the increasingly complex trajectories required from successive starting positions on a maze configuration. Goal-reaching performance for the RL algorithms varied across maze configurations (Fig C-D), with configurations that had a contradictory optimal policy to the one preceding it seeming particularly difficult (e.g. configurations 4, 8, 13, 21 – see <xref ref-type="fig" rid="F1">Fig 1C</xref> for specific layouts). Conversely, maze configurations that possess a high degree of coherence in optimal policy with the previous configuration (e.g. 2, 7, 25) were consistent with higher levels of agent goal-reaching - due to the improved accuracy of the initial value representations. Ranking maze configuration difficulty by order of goal-reaching performance revealed a significantly more positive correlation between the human and SR agent difficulty rankings, than either the model-based or model-free agents (<xref ref-type="fig" rid="F4">Fig 4E</xref>; paired t-test following Fisher transformation: SR vs MF, t(17)=3.27, p=.004; SR vs MB, t(17)=4.57, p&lt;.001). Similarly, the rat difficulty rankings were significantly more correlated with that of the SR agent than model-free (<xref ref-type="fig" rid="F4">Fig 4F</xref>; (paired t-test following Fisher transformation: SR vs MF, t(8)=2.87, p=.021), with no significant difference to the model-based agent (SR vs. MB, t(8)=1.00, p=.345).</p><p id="P19">In order to establish whether the routes taken by the rats, humans and RL agents within a maze configuration tended to follow consistent patterns of behaviour, we next quantified each trajectory using diffusivity measures that were inspired by statistical mechanics and the modelling of particles moving in boxes. Specifically, for each trajectory we calculated the linear diffusivity and the sine and cosine of the angular diffusivity (<xref ref-type="fig" rid="F5">Fig 5A</xref>). The linear and angular diffusivities respectively describe the overall directness and direction of the route which vary from trial to trial (<xref ref-type="fig" rid="F5">Fig 5B-C</xref>). Taken together across the entire experiment we see that when the trajectories are quantified this way, unsupervised clustering reveals clear patterns of behaviour for each of the three RL agents (<xref ref-type="fig" rid="F5">Fig 5D</xref>). Given these distinct clusters, we then used the Mahalanobis distance to measure the level of dissimilarity between the biological and agent trajectories per maze configuration. The Mahalanobis distance was used as it accounts for covariance between the diffusivity measures when calculating the dissimilarity. Using these diffusivities to quantify the general shape of the routes taken within a configuration, we found that the trajectories of the SR agents were consistently more similar to the corresponding rat and human behaviour than the other agents (<xref ref-type="fig" rid="F5">Fig 5E-F</xref>; rat simulations: SR vs MF, t(8)=-11.1, <italic>p</italic>&lt;.001; SR vs MB, t(8)=-12.7, <italic>p</italic>&lt;.001; human simulations: SR vs MF, t(17)=-55.7, <italic>p</italic>&lt;.001; SR vs MB, t(17)=-4.21, <italic>p</italic>&lt;.001). Further, the model-free agent was generally the least similar to the biological behaviour across the maze configurations (rat simulations: MB vs MF, t(8)=-2.76, <italic>p</italic>=.025; human simulations: MB vs MF, t(17)=-28.6, <italic>p</italic>&lt;.001; SR vs MB, t(17)=-4.21, <italic>p</italic>&lt;. 001).</p><p id="P20">Finally, to test whether these differences in diffusivity measures across maze configurations directly translated to a physical closeness between individual trajectories, we calculated the minimum path distance between each human/rat trajectory and the simulated trajectories of the agents trained on each individual’s behaviour. Calculating this at every state along a human/rat trajectory and averaging across the length of the trajectory gives a measure of similarity between the biological and agent routes taken. We see that the SR agent trajectories are generally closer to both the human (<xref ref-type="fig" rid="F6">Fig. 6A</xref>; SR vs MB: t(17)=8.32, <italic>p</italic>&lt;.001 SR vs MF: t(17)=28.8, <italic>p</italic>&lt;.001) and rat paths (<xref ref-type="fig" rid="F6">Fig. 6B</xref>; SR vs MB: t(8)=6.44, <italic>p</italic>&lt;.001; SR vs MF: t(17)=26.4, <italic>p</italic>&lt;.001). Interestingly, humans displayed evidence of model-based planning on the early trials of a new maze configuration (<xref ref-type="fig" rid="F6">Fig. 6C</xref>; first 5 trials MB vs SR: t(17)=3.30, <italic>p</italic>=.004; first 5 trials MB vs MF: t(17)=17.0, <italic>p</italic>&lt;.001), with the latter half of trials – when the routes to the goal were longer and more complex – being significantly more SR-like in both humans (<xref ref-type="fig" rid="F6">Fig. 6C</xref>; last 5 trials SR vs MB: t(17)=8.95, <italic>p</italic>&lt;.001; last 5 trials SR vs MF: t(17)=33.6, <italic>p</italic>&lt;.001) and rats (<xref ref-type="fig" rid="F6">Fig6D</xref>; last 5 trials SR vs MB: t(8)=13.4, <italic>p</italic>&lt;.001; last 5 trials SR vs MF: t(8)=24.8, <italic>p</italic>&lt;.001). Viewing how this measure of similarity changes across maze configurations again reveals noticeable variation (<xref ref-type="fig" rid="F6">Fig 6EF</xref>), with a strong correlation in the level agent similarity between the rats and humans (Pearson correlation: ρ = .93, <italic>p</italic>&lt;.001).</p><p id="P21">In summary, we used three approaches to compare RL agents to rats and humans: a likelihood analysis of rat and human actions under different agents; the similarity in performance between rats, humans and RL agents trained on the biological behaviour; and the similarity of the resulting trajectories generated by these agents to the individual rats and humans on which they were fit and trained. Our results show that both species match more closely the SR RL agents’ than model-free or model-based agents, with some features of behaviour during early exposure to a new maze configuration being consistent with model-based planning.</p></sec></sec><sec id="S7" sec-type="discussion"><title>Discussion</title><p id="P22">To understand the underlying processes that support flexible navigation in rats and humans we compared their navigation performance with three classic instantiations of RL agents in a maze environment using a dynamic layout of barriers. Using a combination of likelihood, performance and trajectory similarity analyses, we find that both rats and humans rapidly adapted to the dynamic environments, producing similar navigation choices and trajectory patterns that most resembled successor representation RL agents. This was most evident for rats, while humans were also found to show some trajectory patterns similar to model-based RL agents in early trials. Our findings provide novel convergent cross-species insights into spatial navigation behaviour and mechanistic understanding of the different choices made when adapting to a changing environment. In doing so we identified a set of metrics that could allow the prediction of future behavioural and neural dynamics across a wide range of methods in humans and animals. We discuss: i) how these results inform our understanding of mammalian navigation, ii) insights into RL models, iii) similarities and differences between rodent and human behaviour, and iv) directions for future research.</p><sec id="S8"><title>A predictive map for navigation?</title><p id="P23">Naively, one might view rats as ‘creatures of habit’, while humans could be considered deep thinkers, mulling over future possibilities. These two stereotypes map, to some degree, onto model-free RL (habit-like) agents and model-based RL (flexible planning) agents. Rather than finding such a dichotomy between rats and humans, we found the behaviour of both species is best captured by an RL agent that creates a predictive map of the environment to guide navigation: the successor representation. The SR has been proposed as an efficient alternative to the relatively inflexible model-free RL and the computationally expensive model-based RL. SR stores a matrix of the possible transitions within the environment and integrates this with information about reward<sup><xref ref-type="bibr" rid="R58">58</xref></sup>. Recently, it has been proposed that the hippocampus may implement a system similar to a successor representation to create a predictive map to guide navigation<sup><xref ref-type="bibr" rid="R66">66</xref>,<xref ref-type="bibr" rid="R67">67</xref>,<xref ref-type="bibr" rid="R70">70</xref>,<xref ref-type="bibr" rid="R71">71</xref></sup>. Here we find behavioural evidence to support the proposal that both rats and humans use such a predictive map to guide flexible navigation behaviour. This match of the rodent behaviour to SR agents is consistent with evidence that rats can carefully evaluate different options for navigation<sup><xref ref-type="bibr" rid="R72">72</xref></sup>.</p><p id="P24">A range of previous experiments comparing human behaviour to RL agents have generally focused on a competition of model-free vs model-based agents to capture behaviours in small conceptual state spaces with 2-step transitions. These have found evidence for model-based planning in humans<sup><xref ref-type="bibr" rid="R68">68</xref>,<xref ref-type="bibr" rid="R73">73</xref>,<xref ref-type="bibr" rid="R74">74</xref></sup>. Using a much larger state space with potential for recursive transitions (i.e. loops leading back to the same state), we extend this approach into a more complex and naturalistic framework. Our findings add to recent evidence that the choices of humans are best explained by a combination of SR and model-based behaviours<sup><xref ref-type="bibr" rid="R63">63</xref>–<xref ref-type="bibr" rid="R65">65</xref></sup>. Since the SR encodes the environment’s transition structure, it is itself a transition model that can be leveraged for intuitive planning<sup><xref ref-type="bibr" rid="R75">75</xref></sup> or more explicit planning procedures such as a tree search - which may partially explain trajectories observed during hippocampal replay<sup><xref ref-type="bibr" rid="R23">23</xref>,<xref ref-type="bibr" rid="R44">44</xref>,<xref ref-type="bibr" rid="R76">76</xref>–<xref ref-type="bibr" rid="R79">79</xref></sup>. Given that the model-based learner will generally improve the accuracy of its learnt model as it has more experience of a maze layout, it might be surprising that we observed a greater match to SR agents during the second half of trials on a new maze configuration. However, the model-based planning mechanism of simulating possible future paths is considerably more resource intensive than drawing upon a cached knowledge of past behaviours gleaned from experience. Thus, for a metabolically constrained learning system it would be more efficient to fallback on simpler processing mechanisms when they reach a certain threshold in terms of performance (e.g. maximising reward and/or minimising uncertainty in expected reward) – which is supported by evidence of in both rodents and humans<sup><xref ref-type="bibr" rid="R50">50</xref>,<xref ref-type="bibr" rid="R80">80</xref></sup>.</p><p id="P25">A few studies have explored human navigation in VR environments and compared navigational choices with RL agents, reporting that behaviour matches a mix of model-based and model-free choices<sup><xref ref-type="bibr" rid="R45">45</xref>,<xref ref-type="bibr" rid="R49">49</xref></sup>. For example, when paths are short but decision times are longer, model-based RL agents were found to better match human behaviour<sup><xref ref-type="bibr" rid="R49">49</xref></sup>. However, these past studies did not compare performance with a successor representation, nor were the trajectories examined in relation metrics such as the diffusivity and physical closeness to understand the match to different RL agents. Here we show that navigation in humans is most similar to SR and that using trajectory information is useful in providing convergent evidence to understand this.</p><p id="P26">In our experiment the model-free RL agents showed poor adaptation to the changes in maze layout. Rather than improving over trials, performance declined. This could be accounted for by our task structure; the minimum trajectory length increased as trials progressed, requiring longer and more complex routes to the goal. Our maze configurations were designed to be simple but to include dead-end zones and/or regions where the barriers extended around the goal zone requiring extended trajectories away from the goal to then reach it. It is possible with different layouts model-free learners would succeed more efficiently. Understanding how the structure of the task and state space leads to the emergence of different policies is an important question for future research.</p><p id="P27">A key observation in our data is that it is not sufficient to conclude on the basis of overall performance which simulated agents will best fit the biological agent’s data. While the model-based RL agent performed best and was closest overall to the human performance, the SR RL model produced the greatest match in terms of the proportion of trials as the maximum likelihood model. This is because the patterns of choices made by the model-based RL fail to capture aspects of choice and trajectory patterns in the rats and humans to the same degree as the successor representation agents. This highlights the utility of an environment in which a wide diversity of trajectories can be achieved by the rats and humans to allow models to be discriminated.</p></sec><sec id="S9"><title>Similarities and differences in the flexible behaviour of rats and humans</title><p id="P28">Past research has suggested that rats do not always optimally adapt to selecting appropriate alternative routes when navigating<sup><xref ref-type="bibr" rid="R9">9</xref></sup> and can take time to adjust to such changes to select the optimal route<sup><xref ref-type="bibr" rid="R14">14</xref>,<xref ref-type="bibr" rid="R81">81</xref></sup>. Similarly, humans can struggle to take optimal shortcuts when presented with options<sup><xref ref-type="bibr" rid="R8">8</xref>,<xref ref-type="bibr" rid="R19">19</xref>,<xref ref-type="bibr" rid="R20">20</xref></sup>. Here rats and humans had to reach the correct goal from a set of 100 possible locations within a 45 second time limit. Since the maze transition structure was re-configured every 10 trials, achieving this was non-trivial. Despite this, both species were able to reach the goal on the first trial of a new layout on many trials. This parallels recent evidence from mice learning new paths remarkably fast in a large labyrinth<sup><xref ref-type="bibr" rid="R82">82</xref></sup>. In several cases we saw examples of routes near the optimal path on the first attempt for both species (e.g. see <xref ref-type="supplementary-material" rid="SD1">Videos S1&amp;2</xref>). Moreover, the occupancy correlation between rats and humans was relatively high even from the first trial, and improved as performance increased across trials in a configuration. These results show that our Tartarus maze, with its visual access to landmarks, boundary geometry and canyon-styled barriers, provides a useful assay for goal-directed navigation across two species, revealing a remarkable similarity in the patterns of navigation across species.</p><p id="P29">Despite similarities in behaviour, there were noticeable differences between species. Humans were more successful at adapting to the changes in maze layout and learning, while rats spent more time on the perimeter of the maze’s edge. The overall difference in learning likely relates to the physical differences in our maze used between rats and humans (real vs VR83) and the biological differences between species (e.g. differences in vision, movement, whisking, olfaction, grooming and predator/prey status). The current models assume optimal routes minimise distance. However, rats will also need to avoid predators, thus selecting certain routes that are safer may also drive route choice<sup><xref ref-type="bibr" rid="R14">14</xref></sup>. Rats also need to find suitable and safe places to groom their fur. These factors may underlie the generally poorer fit of the models to rats than humans. Additionally, while fog was used in the human VR to better match the visual acuity and depth perception between rodents and humans<sup><xref ref-type="bibr" rid="R119">119</xref></sup>, the rats had visual access to more of the maze during the experiment - with recent evidence in humans suggesting this can bias strategies towards a SR<sup><xref ref-type="bibr" rid="R52">52</xref></sup>. Further research would be needed to disentangle the various contributions that give rise to the differences we observed.</p></sec><sec id="S10"><title>Benefits of a dynamic open field environment with barriers</title><p id="P30">Prior studies examining navigation in mazes have generally either used track-based or open field environments<sup><xref ref-type="bibr" rid="R28">28</xref>,<xref ref-type="bibr" rid="R84">84</xref>,<xref ref-type="bibr" rid="R85">85</xref></sup>. While open field environments place more demands on self-localisation and vector-based navigation<sup><xref ref-type="bibr" rid="R78">78</xref>,<xref ref-type="bibr" rid="R86">86</xref>,<xref ref-type="bibr" rid="R87">87</xref></sup>, mazes with tracks enable testing the effect of blocked paths and shortcut behaviours<sup><xref ref-type="bibr" rid="R6">6</xref>,<xref ref-type="bibr" rid="R9">9</xref>,<xref ref-type="bibr" rid="R81">81</xref>,<xref ref-type="bibr" rid="R84">84</xref></sup>. By contrast the Tartarus maze places demands on both vector based navigation and the capacity to take detours and shortcuts, as occurs with much of the terrain in the real world. The recent development of the honeycomb maze for rats<sup><xref ref-type="bibr" rid="R88">88</xref></sup> provides a parallel approach to self-localisation and obstructed paths to goals, where rats sequentially navigate to a goal over a number of hexagonal platforms that are made available in pairs until the goal platform is reached. Such an approach allows for a precise assessment of choice options at different time points, whilst placing demands on self-localisation in relation to distal cues. While the tartarus maze also demands choices and navigation to distal cues it allows continual, often ballistic trajectories to be taken to the goal, mimicking naturalistic behaviours that enable integration with more ethological approaches to navigation<sup><xref ref-type="bibr" rid="R89">89</xref></sup>.</p><p id="P31">A number of rodent studies have examined how maze layout and changes in layout relates to exploration behaviour<sup><xref ref-type="bibr" rid="R82">82</xref>,<xref ref-type="bibr" rid="R90">90</xref>–<xref ref-type="bibr" rid="R94">94</xref></sup> or escape behaviour<sup><xref ref-type="bibr" rid="R14">14</xref>,<xref ref-type="bibr" rid="R95">95</xref></sup>. Here we found rats and humans rapidly adapted to changes in the maze structure, by exploring the new layout. By matching to RL models it is possible to provide a more mechanistic account of how goal-directed behaviour is organised during the navigation of a dynamic environment. In the case of Rosenberg et al. (2021)<sup><xref ref-type="bibr" rid="R82">82</xref></sup> mice had to learn the paths in a maze with a large number of options. Akin to our task, learning was rapid. This differs from many non-spatial learning tasks where learning is typically slow (see Rosenberg et al., 2021). Other recent rodent studies exploring navigation behaviour have shown the capacity to model behaviour in goal learning and homing vectors for safety<sup><xref ref-type="bibr" rid="R14">14</xref>,<xref ref-type="bibr" rid="R87">87</xref>,<xref ref-type="bibr" rid="R94">94</xref>,<xref ref-type="bibr" rid="R96">96</xref></sup>. Such studies highlight the value in modelling to understand the mechanisms guiding behaviour. Here, we demonstrate the added benefit of modelling behaviour with simulated agents, examining the trajectory properties (e.g. diffusivity) and comparing across two different species. Across many studies distal cues are kept constant allowing for rapid learning of the new layout. Manipulating these distal cues would be an interesting direction for future research.</p><p id="P32">Prior human virtual navigation studies exploring flexible navigation behaviour have tended to involve complicated VR environments that would likely be too demanding for rodents to learn<sup><xref ref-type="bibr" rid="R10">10</xref>,<xref ref-type="bibr" rid="R12">12</xref>,<xref ref-type="bibr" rid="R13">13</xref>,<xref ref-type="bibr" rid="R20">20</xref>,<xref ref-type="bibr" rid="R45">45</xref>,<xref ref-type="bibr" rid="R52">52</xref>,<xref ref-type="bibr" rid="R97">97</xref>,<xref ref-type="bibr" rid="R98">98</xref></sup>. Here, we sought to recreate an environment that would challenge human participants generating sufficient variation in performance and to allow comparison to rodents within the same maze structure. Being able to integrate behavioural data from humans, rodents and RL agents, opens the possibilities for incorporating data from a wide array of neuroscience methods in humans and rodents. Recent students have shown the utility of this approach<sup><xref ref-type="bibr" rid="R29">29</xref>–<xref ref-type="bibr" rid="R31">31</xref></sup>. A recent study by Zhu et al. (2021)<sup><xref ref-type="bibr" rid="R52">52</xref></sup> highlights the benefit of examining eye-movement dynamics during the navigation of virtual environments, using a similar head-mounted display to our human VR, but where the whole transition structure was visible to look at. Their results show patterns of eye-movements that sweep across key points in mazes, maximally important for planning, showing forward sweeps to the goal, as well as backward sweeps from the goal. Furthermore, they show evidence that patterns in eye-movements that scan relevant available transitions relate to SR agent performance. Future work with eye-tracking integrated into our human task would be useful to study the selection of sub-goals and eye-movements after changing the maze layout; could eye-movements predict future choices of route and the match to different RL agents in subsequent behaviour? Eye-tracking in rodents is a bigger challenge but may also hold some promise<sup><xref ref-type="bibr" rid="R99">99</xref></sup>. Finally, it may also be useful to explore the search behaviour of RL agents, humans and rats in relation to models of utility and biased search<sup><xref ref-type="bibr" rid="R100">100</xref></sup>. Such explorations would be interesting to examine in mazes ranging in complexity, visibility (fog-levels) and frequency of re-configuration. Based on our results we would predict that human behaviour would match model-based agents more in rapidly changing environments and environments where they can see more of a complex layout that would benefit from deliberating over the options.</p></sec><sec id="S11"><title>How might the RL agents be improved?</title><p id="P33">The learning efficiency of RL agents could be improved using offline replay of randomly sampled past experiences<sup><xref ref-type="bibr" rid="R44">44</xref>,<xref ref-type="bibr" rid="R60">60</xref>,<xref ref-type="bibr" rid="R64">64</xref>,<xref ref-type="bibr" rid="R101">101</xref></sup>. These replays are typically implemented between agent timesteps, and the manner in which they are sampled can further accelerate learning by prioritising the most useful learning experiences to replay<sup><xref ref-type="bibr" rid="R76">76</xref></sup>. Prioritised replay also has strong parallels to the phenomenon of hippocampal replay of place cell activity during sleep or quiescence<sup><xref ref-type="bibr" rid="R78">78</xref>,<xref ref-type="bibr" rid="R102">102</xref>,<xref ref-type="bibr" rid="R103">103</xref></sup>. However, in this study we did not implement agent replay in order to keep the value representations, and consequently the likelihoods of agents, deterministic. An alternative way to improve the goal-reaching of agents could be through improving their exploration policy. The agents simulated here relied on an ε-greedy policy through which exploration is driven purely by chance. However, methods that include curiosity<sup><xref ref-type="bibr" rid="R104">104</xref></sup> or uncertainty in the value function<sup><xref ref-type="bibr" rid="R105">105</xref>,<xref ref-type="bibr" rid="R106">106</xref></sup> could be used to guide more efficient exploration of a new maze configuration and consequently lead to faster learning. Finally, navigation using an options-framework might allow for more efficient navigation<sup><xref ref-type="bibr" rid="R107">107</xref></sup>; rather than planning step by step, efficient navigation in our maze configurations can be achieved by selecting a clockwise vs a counter-clockwise path to the goal. Being able to exploit a hierarchical segmentation of the environment might allow RL agents to better approximate human and rat behaviour (see e.g. Balaguer et al., 2016). Further, the points where agents switch between different options may be able to predict where rats and humans would pause in the maze<sup><xref ref-type="bibr" rid="R72">72</xref></sup>. More broadly, new approaches to RL<sup><xref ref-type="bibr" rid="R109">109</xref></sup> and deep learning methods may provide new ways to examine navigation<sup><xref ref-type="bibr" rid="R40">40</xref>,<xref ref-type="bibr" rid="R110">110</xref></sup>, as well as integrating our approach with biologically-inspired network models that seek to explain neural dynamics during navigation<sup><xref ref-type="bibr" rid="R111">111</xref>,<xref ref-type="bibr" rid="R112">112</xref></sup>.</p></sec><sec id="S12"><title>Exploring the neural substrates of a predictive map</title><p id="P34">Recent neuroimaging in humans has shown that activity in hippocampal and connected regions tracks the modelled parameters from a successor representation<sup><xref ref-type="bibr" rid="R62">62</xref>,<xref ref-type="bibr" rid="R63">63</xref>,<xref ref-type="bibr" rid="R65">65</xref>,<xref ref-type="bibr" rid="R67">67</xref>,<xref ref-type="bibr" rid="R77">77</xref></sup>. Convergent evidence in rodents suggests that the place cell activity in the dorsal CA1 of rodents may operate as a successor representation<sup><xref ref-type="bibr" rid="R66">66</xref>,<xref ref-type="bibr" rid="R67">67</xref>,<xref ref-type="bibr" rid="R70">70</xref>,<xref ref-type="bibr" rid="R113">113</xref>,<xref ref-type="bibr" rid="R114">114</xref></sup>. Our protocol would allow for evidence from both rodent and human data to be integrated within a single framework to consider how patterns in the data may interrelate across species and in relation the parameters from RL modelled agents. Evidence from other recent approaches shows the utility of such an approach<sup><xref ref-type="bibr" rid="R29">29</xref>–<xref ref-type="bibr" rid="R31">31</xref></sup>. Our recent analysis of CA1 place cell activity found little evidence for changes in the place field maps when the state space changed due to blocked doorways in a 4-room maze<sup><xref ref-type="bibr" rid="R109">109</xref></sup>. However, it appears the changes in layout are evident during hippocampal replay events, where paths activated follow the new layout, perhaps consistent with MB-RL search patterns<sup><xref ref-type="bibr" rid="R79">79</xref></sup>. One consideration for future research will be to explore changes in neural activity linked to particular strategies that might occur over trials or even within a route. For example, one might predict a shift to a more striatally mediated strategy, linked to model-free RL, if the number of trials for a configuration was increased<sup><xref ref-type="bibr" rid="R50">50</xref>,<xref ref-type="bibr" rid="R56">56</xref></sup>. Shifts between the engagement of different structures to guide control may also occur alongside shifts within structures<sup><xref ref-type="bibr" rid="R50">50</xref>,<xref ref-type="bibr" rid="R80">80</xref></sup>. For example, the hippocampus might be involved in simulating paths via replay to guide behaviour in highly dynamic environments, but shift to a more cached expression of the stored hippocampal map once the possible paths have been repeatedly experienced.</p><p id="P35">Recent work modelling multi-scale SR agents<sup><xref ref-type="bibr" rid="R116">116</xref></sup> has shown patterns similar to the goal distance tuned activity of CA1 cells in navigating bats<sup><xref ref-type="bibr" rid="R117">117</xref></sup>. Might such patterns emerge in our task? An important step in better understanding the neural systems for navigation would be to examine the impact of temporally targeted inactivation of the hippocampal regions, as well as the prefrontal cortex which is thought to support route planning<sup><xref ref-type="bibr" rid="R37">37</xref></sup>. Such an approach would provide more causal evidence for the role of brain structures for supporting our task. More broadly, the task we have developed could be adapted for study with a range of other species which have been examined in isolation: ants, bees, drosophila, bats, birds and other primates. Integrating across invertebrates and vertebrate species may further our understanding of the common mechanisms for goal-directed behaviour and adaptations that occurred through evolution.</p></sec></sec><sec id="S13" sec-type="conclusions"><title>Conclusion</title><p id="P36">In summary, we found that rats and humans both display behaviour most similar to a successor representation RL agent, with humans also showing some behaviour matching model-based planning. Future work exploring single-unit recording or disruption to neural activity may be useful in revealing how distance to the goal may be coded, as past studies have failed to dissociate path and Euclidean distance. Moreover it will be useful to examine how neural activity in humans and rodents relates to the parameters from RL agents with behaviour adjusted to match the humans and rats. More broadly, the approach provided here could be adapted to compare behaviour across a range of species and different RL models to help understand the broad spectrum of navigation behaviours shown by the diverse species on our planet.</p></sec><sec id="S14" sec-type="methods" specific-use="web-only"><title>Star Methods</title><sec id="S15"><title>Resource Availability</title><sec id="S16"><title>Lead contact</title><p id="P37">Further information and requests for resources should be directed to and will be fulfilled by the Lead Contact William de Cothi (<email>w.decothi@ucl.ac.uk</email>)</p></sec><sec id="S17"><title>Materials availability</title><p id="P38">This study did not generate any unique reagents.</p></sec></sec><sec id="S18"><title>Experimental Model and Subject Details</title><p id="P39">Nine adult male Lister Hooded rats were handled daily (at start of training: 10-20 weeks old, 350-400 g) and housed communally in groups of three. All rats were subjected to a reverse light-dark cycle (11:11 light:dark, with 1 hour x2 simulated dawn/dusk) and were on food-restriction sufficient to maintain 90% of free-feeding weight, with ad libitum access to water. The free-feeding weight was continuously adjusted according to a calculated growth curve for Lister Hooded Rats<sup><xref ref-type="bibr" rid="R118">118</xref></sup>. Six rats were naive, while three rats had previously been trained for 2-3 weeks in a shortcut navigation task for a different maze setup. The procedures were conducted according to UCL ethical guidelines and licensed by the UK Home Office subject to the restrictions and provisions contained in the Animals Scientific Procedures Act of 1986.</p><p id="P40">For the human version of the task, 18 healthy participants (9 female; aged = 24.6 ± 5.9, mean ± sd) were recruited from the UCL Psychology Subject Pool and trained to navigate to an unmarked goal in a virtual arena of approximately the same relative proportion as for the rats. All participants gave written consent to participate in the study in accordance with the UCL Research Ethics Committee.</p></sec><sec id="S19"><title>Method Details</title><sec id="S20"><title>General methods</title><p id="P41">Navigation was tested in a large square environment with a fixed hidden goal location and a prominent directional black wall cue in one direction (<xref ref-type="fig" rid="F1">Fig. 1</xref>, <xref ref-type="supplementary-material" rid="SD1">Video S1 and S2</xref>). The maze was divided in a 10x10 grid of moveable sections that could either be removed, leaving impassable gaps to force detour taking, or added, creating shortcuts. During training, all maze modules were present. Rats, humans and RL agents were trained to reach the goal within a 45s time limit, (<xref ref-type="fig" rid="F1">Fig 1A</xref>). During the testing phase of the experiment, maze modules were removed to block the direct route to the goal (<xref ref-type="fig" rid="F1">Fig 1B</xref>). Humans (n=18), rats (n=9) and agents were tested on the same sequence of 25 maze configurations each with 10 trials in which a set of defined starting locations were selected to optimally probe navigation (<xref ref-type="fig" rid="F1">Fig 1C</xref>). These maze configurations were generated from a pilot testing with 9 rats and the configuration sequence chosen maximised the differences in the layouts between trials. The starting positions on each maze configuration gradually increased in the required tortuosity (path distance/Euclidean distance) of the shortest path to the goal to test complex trajectories whilst keeping the rodents motivated.</p><p id="P42">Upon reaching the goal module, rats and humans had to wait 5s to receive their reward. Human participants were rewarded with a financial bonus and rats received chocolate milk delivered in a well (<xref ref-type="supplementary-material" rid="SD1">supp. fig 1</xref>). In order to better match the visual acuity and depth perception between rodents and humans<sup><xref ref-type="bibr" rid="R119">119</xref></sup>, a thick virtual fog lined the floor of the maze enabling them to only see adjacent maze modules and the distal black wall cue (<xref ref-type="supplementary-material" rid="SD1">supp. fig 2; Video S2</xref>). Modules were made visually indistinct to avoid humans counting them when traversing the space. Human participants were informed that reward was hidden in the environment and that their task was to maximise their financial return as quickly and efficiently as possible. The human and rat trajectories were discretised into the underlying 10x10 modular grid (<xref ref-type="fig" rid="F2">Fig 2A-B</xref>) in order to facilitate comparison between each other and the RL agents.</p><p id="P43">In all versions of the experiment, the environment (raised off the floor) consisted of a 10x10 grid of maze modules. These modules could be removed from the grid in order to form impassable barriers in the environment. One of the modules was rewarded and thus was the location of the goal in the maze. Navigation was facilitated by a single distal cue consisting of a black curtain that spanned the majority of one side of the maze. The goal was kept in the same position with respect to this distal cue throughout all versions of the task. All participants, rats and learning agents were initially trained to navigate to the goal module on the open maze, without any maze modules removed. Once trained, they were all put through the same sequence of 25 maze configurations, with the same sequence of starting locations on each configuration.</p></sec><sec id="S21"><title>Rodent methods</title><p id="P44">All procedures were conducted during the animals’ dark period. The experiment was carried out in a custom-made modular 2x2m square maze composed of 100 identical square platform tiles elevated 50cm above the ground (<xref ref-type="supplementary-material" rid="SD1">supp. fig 1</xref>). The maze was constructed from Medium Density Fibrewood, with the platforms painted in grey. Each platform contained a plastic well (32mm diameter) at its centre, which could be attached to a polymeric tubing system installed beneath the maze. This tubing allowed the experimenter to reward the rat at the goal module filling the well with chocolate milk (0.1 ml). Importantly, all modules in the rodent maze were identical in appearance and construction with chocolate milk rubbed into the well of non-goal modules to lower reliance on olfactory navigational cues. The maze was surrounded on all sides by a white curtain, with a black sheet overlaid on one side to provide a single extra-maze cue. To ensure that no other cue could be used by the animal (uncontrolled room cues, olfactory traces on the maze) the black sheet was rotated 90° clockwise between sessions. The goal module was always in the same position with respect to this cue. Moreover, the experimenter stayed next to the maze inside the curtained area throughout all sessions, his positions relative to the goal were randomised.</p><sec id="S22"><title>Familiarisation</title><p id="P45">During the first day, the rats received a small amount (0.1ml per rat) of chocolate milk in the home cage to decrease neophobia in the maze. For the subsequent two days, each rat underwent two 15 minute maze familiarisation sessions, in which the rat was placed at the centre of the maze and would forage for pieces of chocolate cereal (Weetos) scattered throughout the maze. More cereal was concentrated in the centre to encourage the animal to be comfortable in the middle of the maze.</p></sec><sec id="S23"><title>Training</title><p id="P46">Training consisted of two stages, rats were given 2 training sessions per day. In each training trial the rat had 45s to find the goal module.</p><p id="P47">For stage 1 of training the goal well was filled with 0.1ml of chocolate milk and the rats were initially placed on the modules adjacent to the goal, facing the goal. If the rat made two consecutive direct runs to the goal (without exploration of other parts of the maze), the next trial began one module further away from the goal. Conversely, if the rat failed two consecutive training trials, the next trial began one module closer to the goal until the rat was back at the goal-adjacent modules. On day 1, this procedure was continued until 15 min had elapsed.On the following days, the number of trials was fixed to 16. This procedure was followed every day until the rat was able to make direct runs from the far edges of the maze.</p><p id="P48">Stage 2 was similar to stage1 but a delay in the release of chocolate milk was introduced. This delay started at 1s and was gradually increased until the rat could wait at the goal location for 5s before the chocolate milk was released. Furthermore, the rat’s starting position and orientation were randomised. The number of daily trials could be increased up to 25. This procedure was followed until the rats were able to successfully navigate directly to the goal and on at least 90% of trials. The training phase took on average 24 sessions.</p></sec><sec id="S24"><title>Tartarus Maze testing</title><p id="P49">Rats were run on the 25 maze configurations. For each maze configuration, rats were given 10 trials where they were placed by hand at the starting positions indicated in <xref ref-type="fig" rid="F1">figure 1</xref>. Trials were 45s long and rats were required to navigate to the goal within this time and wait for 5s in order to receive the reward (0.1ml of chocolate milk). If the rat failed to reach the goal, they received no reward and were placed by hand at the next starting location. The rats would usually complete 3 configurations per day. At the beginning of each day, rats were given a brief reminder session that consisted of 5 trials from phase 2 of the training phase.</p></sec></sec><sec id="S25"><title>Human methods</title><p id="P50">Participants were reimbursed for their time as well as a bonus of up to £25 for good performance in the testing phase. Participants experienced the virtual environment via a HTC Vive virtual reality headset whilst sat on a swivel chair. They were able to adjust movement speed using the HTC Vive controller and movement direction was controlled by the participant’s orientation on the chair. Upon successful navigation to the goal module, participants were informed of their financial reward along with the presence of a revolving gold star (<xref ref-type="supplementary-material" rid="SD1">supp. fig 2</xref>) at the goal location. In accordance with the rodent experiment, navigation was aided by the presence of a black distal cue that took up the majority of one of the walls. Goal location, maze configurations and starting positions were all defined with respect to this distal cue and were identical to the rodent experiment. Importantly, a fog lined the floor (<xref ref-type="supplementary-material" rid="SD1">supp. fig 2, Video S2</xref>) of the maze to prevent the participants from understanding what maze modules were missing until they were at adjacent locations. This also provided a better match to visual information available to the rats - which are known to have less visual acuity and binocular depth perception 119. Seamless textures were applied to the floor and walls of the virtual environment, and these were rotated every 10 trials to prevent them from being used as extraneous cues for navigation.</p><p id="P51">The experiment took place over four sessions on four consecutive days. The majority of the first session was usually spent training the participants to navigate to the goal module. To accelerate this learning process, the participants were initially able to see a revolving gold star in the goal location. As they progressed through the training session the star became increasingly transparent until invisible, with the star only appearing again upon successful navigation to the goal module. Along with the decreasing visibility of the goal, the participants’ starting positions were moved progressively further from the goal in a similar manner to the rat training phase. All training and testing trials were 45s in length. Training was terminated when the participants were able to navigate to the hidden goal on at least 80% of trials after being randomly placed at the far edges of the environment. Mean time to complete this training was 41 ± 21 minutes. In order to make the participants’ experience similar to that of the rodents, they were not given any explicit information about the nature of the task - only that financial reward was hidden in the environment in the form of a gold star and their task was to maximise their financial return as quickly and efficiently as possible.</p><p id="P52">The testing took place over the remaining sessions and on average lasted 125 ± 25 minutes, with participants encouraged to take short breaks every 10-20 trials to reduce virtual reality sickness. At the beginning of each testing session, participants completed a short reminder task, which consisted of 5 trials from the end of the training phase.</p></sec><sec id="S26"><title>Reinforcement learner simulations</title><p id="P53">Reinforcement learning seeks to address how an agent should choose actions in order to maximise its expected accumulated reward <italic>R</italic> yielded from future states <italic>s<sub>t</sub></italic>, which is known as the value function <italic>V</italic>: <disp-formula id="FD1"><mml:math id="M1"><mml:mrow><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>𝔼</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="P54">The parameter <italic>γ</italic> is a discount factor that determines the timescale of how motivating future rewards are, such that for <italic>γ</italic> &lt; 1 the agent exponentially discounts future rewards<sup><xref ref-type="bibr" rid="R53">53</xref></sup>.</p><p id="P55">The reinforcement learning agents were implemented in a 10x10 grid world environment, with each state in the grid world corresponding to a maze module in the human/rat versions of the task (see <xref ref-type="supplementary-material" rid="SD1">Fig. S1&amp;2</xref>). Thus, unlike the humans and rats, the agents were not explicitly required to self-localise with respect to distal cues, rather they were given absolute knowledge of their current location (state) on the maze in the form of a one-hot vector (a vector with a ‘1’ in the element corresponding to the current state, with all other elements in the vector being ‘0’). Upon receiving this information pertaining to its current location, the agent was able to choose actions (i.e. up, down, left, right) which transition it to adjacent states, with the ultimate aim being to choose a sequence of states leading to the goal. Crucially, the way in which an agent chooses this sequence of states is different for the model-free, model-based and successor representation algorithms - which are explained in more detail below. At the beginning of the experiment, all agents were endowed with the optimal policy on the open maze to simulate the training phase undertaken by rats and humans. They were then run consecutively on the 25 maze configurations, using the maximum likelihood parameters fit to each individual rat or human participant’s data. For a given individual rat or human, agent behaviour was simulated on each maze configuration by first training the agent on all of that individual’s trajectories (in the same sequential order) prior to the configuration being simulated. Agents then carried over all models/value representations learnt during their 10 trials on the maze configuration being simulated. Hence, the simulated behaviour of agents was never trained using the human/rat trajectories on the configuration being simulated, only the trajectories on all configurations prior. Each type of agent (model-free, model-based and successor representation) was simulated N=100 times per rat/human, using an ε-greedy policy with ε linearly decaying from ε = 0.1 to ε = 0.01 across the 10 trials on a maze configuration. This means that on a new configuration the agents initially chose the greedy action 90% of the time and a random action the remaining 10% of the time (in order to manage the exploration-exploitation tradeoff), with the agents increasing the proportion of greedy actions they take by 1% on each subsequent trial. Due to the behavioural variance introduced by this policy, each algorithm was implemented 100 times for each rat/human to produce the distribution of behaviour used for the comparison with biology. In the subsequent analyses, each individual rat or human was compared to the simulated agents trained on their behaviour, using the maximum likelihood parameters fit to their behaviour.</p><sec id="S27"><title>Model-free agent</title><p id="P56">The model-free method uses the state-action value function <italic>Q</italic> instead of the state value function <italic>V.</italic> <disp-formula id="FD2"><mml:math id="M2"><mml:mrow><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>𝔼</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:msub><mml:mi>a</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mstyle></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="P57">State-action values were learned using the Q-learning algorithm <sup><xref ref-type="bibr" rid="R55">55</xref></sup> combined with an eligibility trace<sup><xref ref-type="bibr" rid="R53">53</xref></sup>. The eligibility trace is a decaying trace of recently taken state-action pairs. Specifically, after taking action <italic>a<sub>t</sub></italic> in state <italic>s<sub>t</sub></italic> and transitioning to state <italic>s</italic><sub><italic>t</italic>+1</sub> where it receives reward <italic>r<sub>t</sub></italic>, the agent will first decay its eligibility trace <italic>e</italic> - a matrix with the same dimensions as <italic>Q</italic>: <disp-formula id="FD3"><mml:math id="M3"><mml:mrow><mml:mi>e</mml:mi><mml:mo>←</mml:mo><mml:mi>λ</mml:mi><mml:mi>γ</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:math></disp-formula> where <italic>λ</italic> = 0.5 is the eligibility trace decay parameter and <italic>γ</italic> is the discount factor of the value function in <xref ref-type="supplementary-material" rid="SD1">tables S1 &amp; S2</xref>. Next, the model-free agent will update its eligibility trace: <disp-formula id="FD4"><mml:math id="M4"><mml:mrow><mml:mi>e</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>←</mml:mo><mml:mi>e</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></disp-formula> before finally updating the state-action values according to: <disp-formula id="FD5"><mml:math id="M5"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>←</mml:mo><mml:mi>Q</mml:mi><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mspace width="0.2em"/><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mi>a</mml:mi></mml:munder><mml:mspace width="0.2em"/><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:math></disp-formula> where <italic>α</italic> is the learning rate in <xref ref-type="supplementary-material" rid="SD1">tables S1 &amp; S2</xref>. Under a greedy policy, the model-free agent at decision time will choose the action <italic>a</italic> in state <italic>s</italic> with the highest state-action value <italic>Q</italic>(<italic>s</italic>, <italic>a</italic>). If multiple actions with the same maximal value exist, then the agent samples from these with equal probability. The eligibility trace <italic>e</italic> is set to zero at the beginning of each trial</p></sec><sec id="S28"><title>Model-based agent</title><p id="P58">The model-based agent is provided with an internal 10x10 binary grid representation of which maze modules are present or not in the environment. Every state <italic>s</italic> in the agent’s model <italic>x</italic> corresponds a module in the maze (see <xref ref-type="fig" rid="F1">Fig 1A-B</xref>); as it transitions through the environment, it updates the internal model at every timestep according to the adjacent states <italic>s</italic>′. <disp-formula id="FD6"><mml:math id="M6"><mml:mrow><mml:mi>χ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>←</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>module</mml:mtext><mml:mspace width="0.2em"/><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mspace width="0.2em"/><mml:mtext>is</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>present</mml:mtext></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>module</mml:mtext><mml:mspace width="0.2em"/><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mspace width="0.2em"/><mml:mtext>is</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>missing</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="P59">At decision time, the model-based agent uses its model <italic>χ</italic> and to plan the shortest route to the goal from each possible next state. Shortest routes were calculated using an A* tree search algorithm <sup><xref ref-type="bibr" rid="R120">120</xref></sup>. In the event of multiple equally short routes to the goal, their respective actions were sampled with equal probability.</p></sec><sec id="S29"><title>Successor representation agent</title><p id="P60">The SR somewhat combines parts of model-free and model-based learning<sup><xref ref-type="bibr" rid="R59">59</xref>,<xref ref-type="bibr" rid="R60">60</xref></sup> by using experience to learn a predictive map <italic>M</italic> between the states in an environment. For a one-step state transition matrix <italic>T,</italic> the predictive map is equivalent to the discounted sum of future state transitions: <disp-formula id="FD7"><mml:math id="M7"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mi>I</mml:mi><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mn>...</mml:mn><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:msup><mml:mi>T</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p id="P61">This discounting of transitions means <italic>M</italic> can be readily combined with a separately learned reward <italic>R</italic> associated with each state <italic>s</italic> in order to explicitly compute value. <disp-formula id="FD8"><mml:math id="M8"><mml:mrow><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:munder><mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p id="P62">The SR agent uses temporal-difference learning and eligibility traces to update the successor matrix <italic>M</italic> <sup><xref ref-type="bibr" rid="R121">121</xref></sup>. After transitioning from state <italic>s<sub>t</sub></italic> and to state <italic>s</italic><sub><italic>t</italic>+1</sub>, the agent will first decay its eligibility trace <italic>e</italic> - a vector with length equal to the number of states in the environment: <disp-formula id="FD9"><mml:math id="M9"><mml:mrow><mml:mi>e</mml:mi><mml:mo>←</mml:mo><mml:mi>λ</mml:mi><mml:mi>γ</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:math></disp-formula> where <italic>λ</italic> = 0.5 is the eligibility trace decay parameter and <italic>γ</italic> is the discount factor of the value function in <xref ref-type="supplementary-material" rid="SD1">tables S1 &amp; S2</xref>. Next, the successor representation agent will update its eligibility trace: <disp-formula id="FD10"><mml:math id="M10"><mml:mrow><mml:mi>e</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>←</mml:mo><mml:mi>e</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></disp-formula> before finally updating the successor representation <sup><xref ref-type="bibr" rid="R121">121</xref></sup>: <disp-formula id="FD11"><mml:math id="M11"><mml:mrow><mml:mi>M</mml:mi><mml:mo>←</mml:mo><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mo>𝟙</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="0.2em"/><mml:mo>⊗</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:math></disp-formula> where ⊗ indicates an outer product. This can then be combined with the state-rewards <italic>R</italic> at decision time to compute the value of prospective future states. <disp-formula id="FD12"><mml:math id="M12"><mml:mrow><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext><mml:mspace width="0.2em"/><mml:mi>s</mml:mi><mml:mspace width="0.2em"/><mml:mtext>is</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>the</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>goal</mml:mtext></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>otherwise</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="P63">Under a greedy policy, the successor representation agent at decision time will choose the next available state with the highest value. If multiple available states exist with equatlly high values, then the agent samples from these with equal probability. The eligibility trace <italic>e</italic> is set to zero at the beginning of each trial.</p></sec></sec></sec><sec id="S30"><title>Quantification and Statistical Analyses</title><p id="P64">Optimal paths were calculated using the A* tree search algorithm<sup><xref ref-type="bibr" rid="R120">120</xref></sup> in the 10x10 grid state space, with path length measured in terms of state visitations. Occupancy correlations were calculated using the Pearson correlation between the proportion of time spent in each state of the 10x10 grid state space. One- and two-sample t-tests were implemented using MATLAB’s ttest and ttest2 functions.</p><p id="P65">Likelihoods were calculated by inputting individual human/rat state trajectories to the RL agents and calculating the internal value estimates of the available state transitions conditional on the human/rat’s past trajectories. These value estimates were used in a softmax function to calculate at each time point, the probability that the agent would take each of the available actions conditioned on the human/rat’s past. Maximum likelihood parameters were estimated using MATLAB’s fmincon function to minimise the negative log-likelihood.</p><p id="P66">Mahalanobis distances were calculated using MATLAB’s pdist2 function on the diffusivity metrics for the humans, rats, model-free, model-based and successor representation agents.</p><p id="P67">The minimum path distance analysis used an individual human/rat trajectory as a reference trajectory. At each time point along that trajectory, the A* tree search algorithm<sup><xref ref-type="bibr" rid="R120">120</xref></sup> was used to find the shortest path distance on the maze configuration to the agent trajectories trained from that individual human/rat’s behaviour. Averaging along the length of the trajectory then gives a measure of similarity between that reference trajectory and the simulated agents.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Material</label><media xlink:href="EMS147055-supplement-Supplementary_Material.pdf" mimetype="application" mime-subtype="pdf" id="d315aAdFbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S31"><title>Acknowledgments</title><p>We would like to thank Célia Lacaux and Charles Middleton for their help building the maze. WdC was funded by an EPSRC CASE studentship with Deepmind Ltd. NN and HJS received support from the European Union’s Horizon 2020 Framework Programme for Research under Marie Sklodowska-Curie ITN (EU-M-GATE 765549). CB is funded by Wellcome SRF (212281/Z/18/Z). HJS received funding from Deepmind Ltd.</p></ack><sec id="S32" sec-type="data-availability"><title>Data and code availability</title><p id="P68">All data and code have been deposited at zenodo and are publicly available as of the date of publication. DOIs are listed in the key resources table.</p></sec><fn-group><fn id="FN1" fn-type="con"><p id="P69"><bold>Author contributions</bold></p><p id="P70">Conceptualisation, WdC &amp; HJS; Methodology, WdC, FZ, SR, DB, RG, ED, CB &amp; HJS; Formal Analysis, WdC; Investigation, WdC, NN, EMG, CG, JL, LF and CN; Writing - Original Draft, WdC &amp; HJS; Writing - Review &amp; Editing, WdC, NN, CN, DB, RG, ED, CB &amp; HJS; Visualisation, WdC, FZ, HJS; Supervision, CB &amp; HJS.</p></fn><fn id="FN2" fn-type="conflict"><p id="P71"><bold>Declaration of interests</bold></p><p id="P72">The authors declare no competing interests.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ekstrom</surname><given-names>AD</given-names></name><name><surname>Spiers</surname><given-names>HJ</given-names></name><name><surname>Bohbot</surname><given-names>VD</given-names></name><name><surname>Rosenbaum</surname><given-names>RS</given-names></name></person-group><source>Human spatial navigation</source><publisher-name>Princeton University Press</publisher-name><year>2018</year></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gallistel</surname><given-names>CR</given-names></name></person-group><source>The organization of learning</source><publisher-name>The MIT Press</publisher-name><year>1990</year></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Muller</surname><given-names>TH</given-names></name><name><surname>Whittington</surname><given-names>JCR</given-names></name><name><surname>Mark</surname><given-names>S</given-names></name><name><surname>Baram</surname><given-names>AB</given-names></name><name><surname>Stachenfeld</surname><given-names>KL</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Z</given-names></name></person-group><article-title>What Is a Cognitive Map? Organizing Knowledge for Flexible Behavior</article-title><source>Neuron</source><year>2018</year><volume>100</volume><fpage>490</fpage><lpage>509</lpage><pub-id pub-id-type="pmid">30359611</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epstein</surname><given-names>RA</given-names></name><name><surname>Patai</surname><given-names>EZ</given-names></name><name><surname>Julian</surname><given-names>JB</given-names></name><name><surname>Spiers</surname><given-names>HJ</given-names></name></person-group><article-title>The cognitive map in humans: spatial navigation and beyond</article-title><source>Nat Neurosci</source><year>2017</year><volume>20</volume><fpage>1504</fpage><lpage>1513</lpage><pub-id pub-id-type="pmcid">PMC6028313</pub-id><pub-id pub-id-type="pmid">29073650</pub-id><pub-id pub-id-type="doi">10.1038/nn.4656</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tolman</surname><given-names>EC</given-names></name></person-group><article-title>Cognitive maps in rats and men</article-title><source>Psychol Rev</source><year>1948</year><volume>55</volume><fpage>189</fpage><pub-id pub-id-type="pmid">18870876</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tolman</surname><given-names>EC</given-names></name><name><surname>Honzik</surname><given-names>CH</given-names></name></person-group><article-title>Introduction and removal of reward, and maze performance in rats</article-title><source>Univ Calif Publ Psychol</source><year>1930</year></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alvernhe</surname><given-names>A</given-names></name><name><surname>Cauter</surname><given-names>TV</given-names></name><name><surname>Save</surname><given-names>E</given-names></name><name><surname>Poucet</surname><given-names>B</given-names></name></person-group><article-title>Different CA1 and CA3 Representations of Novel Routes in a Shortcut Situation</article-title><source>J Neurosci</source><year>2008</year><volume>28</volume><fpage>7324</fpage><lpage>7333</lpage><pub-id pub-id-type="pmcid">PMC6670401</pub-id><pub-id pub-id-type="pmid">18632936</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1909-08.2008</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>TI</given-names></name><name><surname>Gagnon</surname><given-names>SA</given-names></name><name><surname>Wagner</surname><given-names>AD</given-names></name></person-group><article-title>Stress Disrupts Human Hippocampal-Prefrontal Function during Prospective Spatial Navigation and Hinders Flexible Behavior</article-title><source>Curr Biol</source><year>2020</year><volume>30</volume><fpage>1821</fpage><lpage>1833</lpage><elocation-id>e8</elocation-id><pub-id pub-id-type="pmcid">PMC7331937</pub-id><pub-id pub-id-type="pmid">32243859</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2020.03.006</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grieves</surname><given-names>RM</given-names></name><name><surname>Dudchenko</surname><given-names>PA</given-names></name></person-group><article-title>Cognitive maps and spatial inference in animals: Rats fail to take a novel shortcut, but can take a previously experienced one</article-title><source>Learn Motiv</source><year>2013</year><volume>44</volume><fpage>81</fpage><lpage>92</lpage></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howard</surname><given-names>LR</given-names></name><name><surname>Javadi</surname><given-names>AH</given-names></name><name><surname>Yu</surname><given-names>Y</given-names></name><name><surname>Mill</surname><given-names>RD</given-names></name><name><surname>Morrison</surname><given-names>LC</given-names></name><name><surname>Knight</surname><given-names>R</given-names></name><name><surname>Loftus</surname><given-names>MM</given-names></name><name><surname>Staskute</surname><given-names>L</given-names></name><name><surname>Spiers</surname><given-names>HJ</given-names></name></person-group><article-title>The hippocampus and entorhinal cortex encode the path and euclidean distances to goals during navigation</article-title><source>Curr Biol</source><year>2014</year><volume>24</volume><fpage>1331</fpage><lpage>1340</lpage><pub-id pub-id-type="pmcid">PMC4062938</pub-id><pub-id pub-id-type="pmid">24909328</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2014.05.001</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Javadi</surname><given-names>AH</given-names></name><name><surname>Emo</surname><given-names>B</given-names></name><name><surname>Howard</surname><given-names>LR</given-names></name><name><surname>Zisch</surname><given-names>FE</given-names></name><name><surname>Yu</surname><given-names>Y</given-names></name><name><surname>Knight</surname><given-names>R</given-names></name><name><surname>Pinelo Silva</surname><given-names>J</given-names></name><name><surname>Spiers</surname><given-names>HJ</given-names></name></person-group><article-title>Hippocampal and prefrontal processing of network topology to simulate the future</article-title><source>Nat Commun</source><year>2017</year><volume>8</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="pmcid">PMC5364395</pub-id><pub-id pub-id-type="pmid">28323817</pub-id><pub-id pub-id-type="doi">10.1038/ncomms14652</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Javadi</surname><given-names>AH</given-names></name><name><surname>Patai</surname><given-names>EZ</given-names></name><name><surname>Marin-Garcia</surname><given-names>E</given-names></name><name><surname>Margois</surname><given-names>A</given-names></name><name><surname>Tan</surname><given-names>H-RM</given-names></name><name><surname>Kumaran</surname><given-names>D</given-names></name><name><surname>Nardini</surname><given-names>M</given-names></name><name><surname>Penny</surname><given-names>W</given-names></name><name><surname>Duzel</surname><given-names>E</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><etal/></person-group><article-title>Backtracking during navigation is correlated with enhanced anterior cingulate activity and suppression of alpha oscillations and the ‘default-mode’ network</article-title><source>Proc R Soc B Biol Sci</source><year>2019</year><volume>286</volume><elocation-id>20191016</elocation-id><pub-id pub-id-type="pmcid">PMC6710605</pub-id><pub-id pub-id-type="pmid">31362634</pub-id><pub-id pub-id-type="doi">10.1098/rspb.2019.1016</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patai</surname><given-names>EZ</given-names></name><name><surname>Javadi</surname><given-names>AH</given-names></name><name><surname>Ozubko</surname><given-names>JD</given-names></name><name><surname>O’Callaghan</surname><given-names>A</given-names></name><name><surname>Ji</surname><given-names>S</given-names></name><name><surname>Robin</surname><given-names>J</given-names></name><name><surname>Grady</surname><given-names>C</given-names></name><name><surname>Winocur</surname><given-names>G</given-names></name><name><surname>Rosenbaum</surname><given-names>RS</given-names></name><name><surname>Moscovitch</surname><given-names>M</given-names></name><etal/></person-group><article-title>Hippocampal and Retrosplenial Goal Distance Coding after Long-term Consolidation of a Real-World Environment</article-title><source>Cereb Cortex</source><year>2019</year><volume>29</volume><fpage>2748</fpage><lpage>2758</lpage><pub-id pub-id-type="pmcid">PMC6519689</pub-id><pub-id pub-id-type="pmid">30916744</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhz044</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shamash</surname><given-names>P</given-names></name><name><surname>Olesen</surname><given-names>SF</given-names></name><name><surname>Iordanidou</surname><given-names>P</given-names></name><name><surname>Campagner</surname><given-names>D</given-names></name><name><surname>Banerjee</surname><given-names>N</given-names></name><name><surname>Branco</surname><given-names>T</given-names></name></person-group><article-title>Mice learn multi-step routes by memorizing subgoal locations</article-title><source>Nat Neurosci</source><year>2021</year><volume>24</volume><fpage>1270</fpage><lpage>1279</lpage><pub-id pub-id-type="pmid">34326540</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tolman</surname><given-names>EC</given-names></name><name><surname>Ritchie</surname><given-names>BF</given-names></name><name><surname>Kalish</surname><given-names>D</given-names></name></person-group><article-title>Studies in spatial learning. I. Orientation and the short-cut</article-title><source>J Exp Psychol</source><year>1946</year><volume>36</volume><fpage>13</fpage><pub-id pub-id-type="pmid">21015338</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Evensmoen</surname><given-names>HR</given-names></name><name><surname>Lehn</surname><given-names>H</given-names></name><name><surname>Pintzka</surname><given-names>CWS</given-names></name><name><surname>Haaberg</surname><given-names>AK</given-names></name></person-group><article-title>Persistent posterior and transient anterior medial temporal lobe activity during navigation</article-title><source>Neuroimage</source><year>2010</year><volume>52</volume><fpage>1654</fpage><lpage>1666</lpage><pub-id pub-id-type="pmid">20677377</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spiers</surname><given-names>HJ</given-names></name><name><surname>Gilbert</surname><given-names>SJ</given-names></name></person-group><article-title>Solving the detour problem in navigation: a model of prefrontal and hippocampal interactions</article-title><source>Front Hum Neurosci</source><year>2015</year><volume>9</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="pmcid">PMC4366647</pub-id><pub-id pub-id-type="pmid">25852515</pub-id><pub-id pub-id-type="doi">10.3389/fnhum.2015.00125</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gentry</surname><given-names>G</given-names></name><name><surname>Brown</surname><given-names>WL</given-names></name><name><surname>Kaplan</surname><given-names>SJ</given-names></name></person-group><article-title>An experimental analysis of the spatial location hypothesis in learning</article-title><source>J Comp Physiol Psychol</source><year>1947</year><volume>40</volume><fpage>309</fpage><pub-id pub-id-type="pmid">20267823</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foo</surname><given-names>P</given-names></name><name><surname>Warren</surname><given-names>WH</given-names></name><name><surname>Duchon</surname><given-names>A</given-names></name><name><surname>Tarr</surname><given-names>MJ</given-names></name></person-group><article-title>Do humans integrate routes into a cognitive map? Map-Versus landmark-based navigation of novel shortcuts</article-title><source>J Exp Psychol Learn Mem Cogn</source><year>2005</year><volume>31</volume><fpage>195</fpage><lpage>215</lpage><pub-id pub-id-type="pmid">15755239</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marchette</surname><given-names>SA</given-names></name><name><surname>Bakker</surname><given-names>A</given-names></name><name><surname>Shelton</surname><given-names>AL</given-names></name></person-group><article-title>Cognitive Mappers to Creatures of Habit: Differential Engagement of Place and Response Learning Mechanisms Predicts Human Navigational Behavior</article-title><source>J Neurosci</source><year>2011</year><volume>31</volume><fpage>15264</fpage><lpage>15268</lpage><pub-id pub-id-type="pmcid">PMC4826051</pub-id><pub-id pub-id-type="pmid">22031872</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3634-11.2011</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ekstrom</surname><given-names>AD</given-names></name><name><surname>Ranganath</surname><given-names>C</given-names></name></person-group><article-title>Space, time, and episodic memory: The hippocampus is all over the cognitive map</article-title><source>Hippocampus</source><year>2018</year><volume>28</volume><fpage>680</fpage><lpage>687</lpage><pub-id pub-id-type="pmid">28609014</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gahnstrom</surname><given-names>CJ</given-names></name><name><surname>Spiers</surname><given-names>HJ</given-names></name></person-group><article-title>Striatal and hippocampal contributions to flexible navigation in rats and humans</article-title><source>Brain Neurosci Adv</source><year>2020</year><volume>4</volume><elocation-id>2398212820979772</elocation-id><pub-id pub-id-type="pmcid">PMC7755934</pub-id><pub-id pub-id-type="pmid">33426302</pub-id><pub-id pub-id-type="doi">10.1177/2398212820979772</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nyberg</surname><given-names>N</given-names></name><name><surname>Duvelle</surname><given-names>É</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name><name><surname>Spiers</surname><given-names>HJ</given-names></name></person-group><article-title>Spatial goal coding in the hippocampal formation</article-title><source>Neuron</source><year>2022</year><volume>110</volume><fpage>394</fpage><lpage>422</lpage><pub-id pub-id-type="pmid">35032426</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Nadel</surname><given-names>L</given-names></name></person-group><source>The hippocampus as a cognitive map</source><publisher-name>Oxford: Clarendon Press</publisher-name><year>1978</year></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spiers</surname><given-names>HJ</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name></person-group><article-title>Neural systems supporting navigation</article-title><source>Curr Opin Behav Sci</source><year>2015</year><volume>1</volume><fpage>47</fpage><lpage>55</lpage></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ekstrom</surname><given-names>AD</given-names></name><name><surname>Kahana</surname><given-names>MJ</given-names></name><name><surname>Caplan</surname><given-names>JB</given-names></name><name><surname>Fields</surname><given-names>TA</given-names></name><name><surname>Isham</surname><given-names>EA</given-names></name><name><surname>Newman</surname><given-names>EL</given-names></name><name><surname>Fried</surname><given-names>I</given-names></name></person-group><article-title>Cellular networks underlying human spatial navigation</article-title><source>Nature</source><year>2003</year><volume>425</volume><fpage>184</fpage><lpage>188</lpage><pub-id pub-id-type="pmid">12968182</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grieves</surname><given-names>RM</given-names></name><name><surname>Jeffery</surname><given-names>KJ</given-names></name></person-group><article-title>The representation of space in the brain</article-title><source>Behav Processes</source><year>2017</year><pub-id pub-id-type="pmid">28034697</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poulter</surname><given-names>S</given-names></name><name><surname>Hartley</surname><given-names>T</given-names></name><name><surname>Lever</surname><given-names>C</given-names></name></person-group><article-title>The Neurobiology of Mammalian Navigation</article-title><source>Curr Biol</source><year>2018</year><volume>28</volume><fpage>R1023</fpage><lpage>R1042</lpage><pub-id pub-id-type="pmid">30205053</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barron</surname><given-names>HC</given-names></name><name><surname>Reeve</surname><given-names>HM</given-names></name><name><surname>Koolschijn</surname><given-names>RS</given-names></name><name><surname>Perestenko</surname><given-names>PV</given-names></name><name><surname>Shpektor</surname><given-names>A</given-names></name><name><surname>Nili</surname><given-names>H</given-names></name><name><surname>Rothaermel</surname><given-names>R</given-names></name><name><surname>Campo-Urriza</surname><given-names>N</given-names></name><name><surname>O’Reilly</surname><given-names>JX</given-names></name><name><surname>Bannerman</surname><given-names>DM</given-names></name><etal/></person-group><article-title>Neuronal Computation Underlying Inferential Reasoning in Humans and Mice</article-title><source>Cell</source><year>2020</year><volume>183</volume><fpage>228</fpage><lpage>243</lpage><elocation-id>e21</elocation-id><pub-id pub-id-type="pmcid">PMC7116148</pub-id><pub-id pub-id-type="pmid">32946810</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2020.08.035</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barron</surname><given-names>HC</given-names></name><name><surname>Mars</surname><given-names>RB</given-names></name><name><surname>Dupret</surname><given-names>D</given-names></name><name><surname>Lerch</surname><given-names>JP</given-names></name><name><surname>Sampaio-Baptista</surname><given-names>C</given-names></name></person-group><article-title>Cross-species neuroscience: closing the explanatory gap</article-title><source>Philos Trans R Soc B Biol Sci</source><year>2021</year><volume>376</volume><elocation-id>20190633</elocation-id><pub-id pub-id-type="pmcid">PMC7116399</pub-id><pub-id pub-id-type="pmid">33190601</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2019.0633</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Samanta</surname><given-names>A</given-names></name><name><surname>van Rongen</surname><given-names>LS</given-names></name><name><surname>Rossato</surname><given-names>JI</given-names></name><name><surname>Jacobse</surname><given-names>J</given-names></name><name><surname>Schoenfeld</surname><given-names>R</given-names></name><name><surname>Genzel</surname><given-names>L</given-names></name></person-group><article-title>Sleep Leads to Brain-Wide Neural Changes Independent of Allocentric and Egocentric Spatial Training in Humans and Rats</article-title><source>Cereb Cortex</source><year>2021</year><pub-id pub-id-type="pmcid">PMC8491695</pub-id><pub-id pub-id-type="pmid">34037203</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhab135</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coughlan</surname><given-names>G</given-names></name><name><surname>Laczó</surname><given-names>J</given-names></name><name><surname>Hort</surname><given-names>J</given-names></name><name><surname>Minihane</surname><given-names>A-M</given-names></name><name><surname>Hornberger</surname><given-names>M</given-names></name></person-group><article-title>Spatial navigation deficits – overlooked cognitive marker for preclinical Alzheimer disease?</article-title><source>Nat Rev Neurol</source><year>2018</year><volume>14</volume><fpage>496</fpage><lpage>506</lpage><pub-id pub-id-type="pmid">29980763</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coughlan</surname><given-names>G</given-names></name><name><surname>Coutrot</surname><given-names>A</given-names></name><name><surname>Khondoker</surname><given-names>M</given-names></name><name><surname>Minihane</surname><given-names>A-M</given-names></name><name><surname>Spiers</surname><given-names>H</given-names></name><name><surname>Hornberger</surname><given-names>M</given-names></name></person-group><article-title>Toward personalized cognitive diagnostics of at-genetic-risk Alzheimer’s disease</article-title><source>Proc Natl Acad Sci</source><year>2019</year><volume>116</volume><fpage>9285</fpage><lpage>9292</lpage><pub-id pub-id-type="pmcid">PMC6511014</pub-id><pub-id pub-id-type="pmid">31015296</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1901600116</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laczó</surname><given-names>J</given-names></name><name><surname>Andel</surname><given-names>R</given-names></name><name><surname>Vyhnalek</surname><given-names>M</given-names></name><name><surname>Vlcek</surname><given-names>K</given-names></name><name><surname>Magerova</surname><given-names>H</given-names></name><name><surname>Varjassyova</surname><given-names>A</given-names></name><name><surname>Nedelska</surname><given-names>Z</given-names></name><name><surname>Gazova</surname><given-names>I</given-names></name><name><surname>Bojar</surname><given-names>M</given-names></name><name><surname>Sheardova</surname><given-names>K</given-names></name><etal/></person-group><article-title>From Morris Water Maze to Computer Tests in the Prediction of Alzheimer’s Disease</article-title><source>Neurodegener Dis</source><year>2012</year><volume>10</volume><fpage>153</fpage><lpage>157</lpage><pub-id pub-id-type="pmid">22205134</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McGann</surname><given-names>JP</given-names></name></person-group><article-title>Poor human olfaction is a 19th-century myth</article-title><source>Science</source><year>2017</year><volume>356</volume><elocation-id>eaam7263</elocation-id><pub-id pub-id-type="pmcid">PMC5512720</pub-id><pub-id pub-id-type="pmid">28495701</pub-id><pub-id pub-id-type="doi">10.1126/science.aam7263</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallace</surname><given-names>DJ</given-names></name><name><surname>Greenberg</surname><given-names>DS</given-names></name><name><surname>Sawinski</surname><given-names>J</given-names></name><name><surname>Rulla</surname><given-names>S</given-names></name><name><surname>Notaro</surname><given-names>G</given-names></name><name><surname>Kerr</surname><given-names>JND</given-names></name></person-group><article-title>Rats maintain an overhead binocular field at the expense of constant fusion</article-title><source>Nature</source><year>2013</year><volume>498</volume><fpage>65</fpage><lpage>69</lpage><pub-id pub-id-type="pmid">23708965</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patai</surname><given-names>EZ</given-names></name><name><surname>Spiers</surname><given-names>HJ</given-names></name></person-group><article-title>The Versatile Wayfinder: Prefrontal Contributions to Spatial Navigation</article-title><source>Trends Cogn Sci</source><year>2021</year><volume>25</volume><fpage>520</fpage><lpage>533</lpage><pub-id pub-id-type="pmid">33752958</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Uylings</surname><given-names>HBM</given-names></name><name><surname>Groenewegen</surname><given-names>HJ</given-names></name><name><surname>Kolb</surname><given-names>B</given-names></name></person-group><source>Do rats have a prefrontal cortex?</source><publisher-name>Elsevier</publisher-name><year>2003</year><pub-id pub-id-type="pmid">14643455</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Andersen</surname><given-names>P</given-names></name><name><surname>Morris</surname><given-names>R</given-names></name><name><surname>Amaral</surname><given-names>D</given-names></name><name><surname>Bliss</surname><given-names>T</given-names></name><name><surname>O’Keefe</surname><given-names>J</given-names></name></person-group><source>The hippocampus book</source><publisher-name>Oxford university press</publisher-name><year>2006</year></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Banino</surname><given-names>A</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name><name><surname>Uria</surname><given-names>B</given-names></name><name><surname>Blundell</surname><given-names>C</given-names></name><name><surname>Lillicrap</surname><given-names>T</given-names></name><name><surname>Mirowski</surname><given-names>P</given-names></name><name><surname>Pritzel</surname><given-names>A</given-names></name><name><surname>Chadwick</surname><given-names>MJ</given-names></name><name><surname>Degris</surname><given-names>T</given-names></name><name><surname>Modayil</surname><given-names>J</given-names></name><etal/></person-group><article-title>Vector-based navigation using grid-like representations in artificial agents</article-title><source>Nature</source><year>2018</year><volume>557</volume><fpage>429</fpage><lpage>433</lpage><pub-id pub-id-type="pmid">29743670</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bermudez-Contreras</surname><given-names>E</given-names></name><name><surname>Clark</surname><given-names>BJ</given-names></name><name><surname>Wilber</surname><given-names>A</given-names></name></person-group><article-title>The Neuroscience of Spatial Navigation and the Relationship to Artificial Intelligence</article-title><source>Front Comput Neurosci</source><year>2020</year><volume>14</volume><elocation-id>63</elocation-id><pub-id pub-id-type="pmcid">PMC7399088</pub-id><pub-id pub-id-type="pmid">32848684</pub-id><pub-id pub-id-type="doi">10.3389/fncom.2020.00063</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Botvinick</surname><given-names>M</given-names></name><name><surname>Wang</surname><given-names>JX</given-names></name><name><surname>Dabney</surname><given-names>W</given-names></name><name><surname>Miller</surname><given-names>KJ</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Z</given-names></name></person-group><source>Deep Reinforcement Learning and Its Neuroscientific Implications</source><publisher-name>Cell Press</publisher-name><year>2020</year><pub-id pub-id-type="pmid">32663439</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><source>Decision theory, reinforcement learning, and the brain</source><publisher-name>Springer</publisher-name><year>2008</year><pub-id pub-id-type="pmid">19033240</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Momennejad</surname><given-names>I</given-names></name></person-group><source>Learning Structures: Predictive Representations, Replay, and Generalization</source><publisher-name>Elsevier Ltd</publisher-name><year>2020</year><pub-id pub-id-type="pmcid">PMC9004662</pub-id><pub-id pub-id-type="pmid">35419465</pub-id><pub-id pub-id-type="doi">10.1016/j.cobeha.2020.02.017</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simon</surname><given-names>DA</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><article-title>Neural Correlates of Forward Planning in a Spatial Decision Task in Humans</article-title><source>J Neurosci</source><year>2011</year><volume>31</volume><fpage>5526</fpage><lpage>5539</lpage><pub-id pub-id-type="pmcid">PMC3108440</pub-id><pub-id pub-id-type="pmid">21471389</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4647-10.2011</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mnih</surname><given-names>V</given-names></name><name><surname>Kavukcuoglu</surname><given-names>K</given-names></name><name><surname>Silver</surname><given-names>D</given-names></name><name><surname>Rusu</surname><given-names>AA</given-names></name><name><surname>Veness</surname><given-names>J</given-names></name><name><surname>Bellemare</surname><given-names>MG</given-names></name><name><surname>Graves</surname><given-names>A</given-names></name><name><surname>Riedmiller</surname><given-names>M</given-names></name><name><surname>Fidjeland</surname><given-names>AK</given-names></name><name><surname>Ostrovski</surname><given-names>G</given-names></name><etal/></person-group><article-title>Human-level control through deep reinforcement learning</article-title><source>Nature</source><year>2015</year><volume>518</volume><fpage>529</fpage><lpage>533</lpage><pub-id pub-id-type="pmid">25719670</pub-id></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silver</surname><given-names>D</given-names></name><name><surname>Huang</surname><given-names>A</given-names></name><name><surname>Maddison</surname><given-names>CJ</given-names></name><name><surname>Guez</surname><given-names>A</given-names></name><name><surname>Sifre</surname><given-names>L</given-names></name><name><surname>van den Driessche</surname><given-names>G</given-names></name><name><surname>Schrittwieser</surname><given-names>J</given-names></name><name><surname>Antonoglou</surname><given-names>I</given-names></name><name><surname>Panneershelvam</surname><given-names>V</given-names></name><name><surname>Lanctot</surname><given-names>M</given-names></name><etal/></person-group><article-title>Mastering the game of Go with deep neural networks and tree search</article-title><source>Nature</source><year>2016</year><volume>529</volume><fpage>484</fpage><lpage>489</lpage><pub-id pub-id-type="pmid">26819042</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silver</surname><given-names>D</given-names></name><name><surname>Schrittwieser</surname><given-names>J</given-names></name><name><surname>Simonyan</surname><given-names>K</given-names></name><name><surname>Antonoglou</surname><given-names>I</given-names></name><name><surname>Huang</surname><given-names>A</given-names></name><name><surname>Guez</surname><given-names>A</given-names></name><name><surname>Hubert</surname><given-names>T</given-names></name><name><surname>Baker</surname><given-names>L</given-names></name><name><surname>Lai</surname><given-names>M</given-names></name><name><surname>Bolton</surname><given-names>A</given-names></name><etal/></person-group><article-title>Mastering the game of Go without human knowledge</article-title><source>Nature</source><year>2017</year><volume>550</volume><fpage>354</fpage><lpage>359</lpage><pub-id pub-id-type="pmid">29052630</pub-id></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anggraini</surname><given-names>D</given-names></name><name><surname>Glasauer</surname><given-names>S</given-names></name><name><surname>Wunderlich</surname><given-names>K</given-names></name></person-group><article-title>Neural signatures of reinforcement learning correlate with strategy adoption during spatial navigation</article-title><source>Sci Rep</source><year>2018</year><volume>8</volume><elocation-id>10110</elocation-id><pub-id pub-id-type="pmcid">PMC6031619</pub-id><pub-id pub-id-type="pmid">29973606</pub-id><pub-id pub-id-type="doi">10.1038/s41598-018-28241-z</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geerts</surname><given-names>JP</given-names></name><name><surname>Chersi</surname><given-names>F</given-names></name><name><surname>Stachenfeld</surname><given-names>KL</given-names></name><name><surname>Burgess</surname><given-names>N</given-names></name></person-group><article-title>A general model of hippocampal and dorsal striatal learning and decision making</article-title><source>Proc Natl Acad Sci</source><year>2020</year><pub-id pub-id-type="pmcid">PMC7733794</pub-id><pub-id pub-id-type="pmid">33229541</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2007981117</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Voudouris</surname><given-names>K</given-names></name><name><surname>Crosby</surname><given-names>M</given-names></name><name><surname>Beyret</surname><given-names>B</given-names></name><name><surname>Hernández-Orallo</surname><given-names>J</given-names></name><name><surname>Shanahan</surname><given-names>M</given-names></name><name><surname>Halina</surname><given-names>M</given-names></name><name><surname>Cheke</surname><given-names>L</given-names></name></person-group><article-title>Direct Human-AI Comparison in the Animal-AI Environment</article-title><year>2021</year><pub-id pub-id-type="pmcid">PMC9172850</pub-id><pub-id pub-id-type="pmid">35686061</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2022.711821</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>SL</given-names></name><name><surname>Lakshminarasimhan</surname><given-names>KJ</given-names></name><name><surname>Arfaei</surname><given-names>N</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><article-title>Eye Movements Reveal Spatiotemporal Dynamics of Active Sensing and Planning in Navigation</article-title><year>2021</year></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name></person-group><source>Reinforcement Learning: An Introduction</source><publisher-name>MIT Press</publisher-name><year>2018</year><edition>2nd</edition></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name></person-group><article-title>Learning to predict by the methods of temporal differences</article-title><source>Mach Learn</source><year>1988</year><volume>3</volume><fpage>9</fpage><lpage>44</lpage></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watkins</surname><given-names>CJCH</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><article-title>Q-Learning</article-title><source>Mach Learn</source><year>1992</year><volume>8</volume><fpage>279</fpage><lpage>292</lpage></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><article-title>Goals and Habits in the Brain</article-title><source>Neuron</source><year>2013</year><volume>80</volume><fpage>312</fpage><lpage>325</lpage><pub-id pub-id-type="pmcid">PMC3807793</pub-id><pub-id pub-id-type="pmid">24139036</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2013.09.007</pub-id></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>JJ</given-names></name><name><surname>Keramati</surname><given-names>M</given-names></name></person-group><article-title>Flexibility to contingency changes distinguishes habitual and goal-directed strategies in humans</article-title><source>PLoS Comput Biol</source><year>2017</year><volume>13</volume><pub-id pub-id-type="pmcid">PMC5634647</pub-id><pub-id pub-id-type="pmid">28957319</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005753</pub-id></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><article-title>Improving generalization for temporal difference learning: The successor representation</article-title><source>Neural Comput</source><year>1993</year><volume>5</volume><fpage>613</fpage><lpage>624</lpage></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><article-title>The Successor Representation: Its Computational Logic and Neural Substrates</article-title><source>J Neurosci</source><year>2018</year><volume>38</volume><fpage>7193</fpage><lpage>7200</lpage><pub-id pub-id-type="pmcid">PMC6096039</pub-id><pub-id pub-id-type="pmid">30006364</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0151-18.2018</pub-id></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russek</surname><given-names>EM</given-names></name><name><surname>Momennejad</surname><given-names>I</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><article-title>Predictive representations can link model-based reinforcement learning to model-free mechanisms</article-title><source>PLoS Comput Biol</source><year>2017</year><volume>13</volume><pub-id pub-id-type="pmcid">PMC5628940</pub-id><pub-id pub-id-type="pmid">28945743</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005768</pub-id></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bellmund</surname><given-names>JLS</given-names></name><name><surname>de Cothi</surname><given-names>W</given-names></name><name><surname>Ruiter</surname><given-names>TA</given-names></name><name><surname>Nau</surname><given-names>M</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name><name><surname>Doeller</surname><given-names>CF</given-names></name></person-group><article-title>Deforming the metric of cognitive maps distorts memory</article-title><source>Nat Hum Behav</source><year>2019</year><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="pmid">31740749</pub-id></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunec</surname><given-names>IK</given-names></name><name><surname>Momennejad</surname><given-names>I</given-names></name></person-group><article-title>Predictive Representations in Hippocampal and Prefrontal Hierarchies</article-title><source>bioRxiv</source><year>2019</year><elocation-id>786434</elocation-id></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garvert</surname><given-names>MM</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Behrens</surname><given-names>TE</given-names></name></person-group><article-title>A map of abstract relational knowledge in the human hippocampal-entorhinal cortex</article-title><source>eLife</source><year>2017</year><volume>6</volume><fpage>1</fpage><lpage>20</lpage><pub-id pub-id-type="pmcid">PMC5407855</pub-id><pub-id pub-id-type="pmid">28448253</pub-id><pub-id pub-id-type="doi">10.7554/eLife.17086</pub-id></element-citation></ref><ref id="R64"><label>64</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Momennejad</surname><given-names>I</given-names></name><name><surname>Russek</surname><given-names>EM</given-names></name><name><surname>Cheong</surname><given-names>JH</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><article-title>The successor representation in human reinforcement learning</article-title><source>Nat Hum Behav</source><year>2017</year><volume>1</volume><fpage>680</fpage><lpage>692</lpage><pub-id pub-id-type="pmcid">PMC6941356</pub-id><pub-id pub-id-type="pmid">31024137</pub-id><pub-id pub-id-type="doi">10.1038/s41562-017-0180-8</pub-id></element-citation></ref><ref id="R65"><label>65</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russek</surname><given-names>EM</given-names></name><name><surname>Momennejad</surname><given-names>I</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><article-title>Neural evidence for the successor representation in choice evaluation</article-title><year>2021</year></element-citation></ref><ref id="R66"><label>66</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Cothi</surname><given-names>W</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name></person-group><article-title>Neurobiological successor features for spatial navigation</article-title><source>Hippocampus</source><year>2020</year><volume>30</volume><fpage>1347</fpage><lpage>1355</lpage><pub-id pub-id-type="pmcid">PMC8432165</pub-id><pub-id pub-id-type="pmid">32584491</pub-id><pub-id pub-id-type="doi">10.1002/hipo.23246</pub-id></element-citation></ref><ref id="R67"><label>67</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stachenfeld</surname><given-names>KL</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><article-title>The hippocampus as a predictive map</article-title><source>Nat Neurosci</source><year>2017</year><pub-id pub-id-type="pmid">28967910</pub-id></element-citation></ref><ref id="R68"><label>68</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Seymour</surname><given-names>B</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><article-title>Model-based influences on humans' choices and striatal prediction errors</article-title><source>Neuron</source><year>2011</year><volume>69</volume><fpage>1204</fpage><lpage>1215</lpage><pub-id pub-id-type="pmcid">PMC3077926</pub-id><pub-id pub-id-type="pmid">21435563</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2011.02.027</pub-id></element-citation></ref><ref id="R69"><label>69</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>KJ</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><article-title>Dorsal hippocampus contributes to model-based planning</article-title><source>Nat Neurosci</source><year>2017</year><volume>20</volume><fpage>1269</fpage><lpage>1276</lpage><pub-id pub-id-type="pmcid">PMC5575950</pub-id><pub-id pub-id-type="pmid">28758995</pub-id><pub-id pub-id-type="doi">10.1038/nn.4613</pub-id></element-citation></ref><ref id="R70"><label>70</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sosa</surname><given-names>M</given-names></name><name><surname>Giocomo</surname><given-names>LM</given-names></name></person-group><article-title>Navigating for reward</article-title><source>Nat Rev Neurosci</source><year>2021</year><volume>22</volume><fpage>472</fpage><lpage>487</lpage><pub-id pub-id-type="pmid">34230644</pub-id></element-citation></ref><ref id="R71"><label>71</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>George</surname><given-names>TM</given-names></name><name><surname>de Cothi</surname><given-names>W</given-names></name><name><surname>Stachenfeld</surname><given-names>K</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name></person-group><article-title>Rapid learning of predictive maps with STDP and theta phase precession</article-title><year>2022</year><elocation-id>2022.04.20.488882</elocation-id></element-citation></ref><ref id="R72"><label>72</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Redish</surname><given-names>AD</given-names></name></person-group><article-title>Vicarious trial and error</article-title><source>Nat Rev Neurosci</source><year>2016</year><volume>17</volume><fpage>147</fpage><lpage>159</lpage><pub-id pub-id-type="pmcid">PMC5029271</pub-id><pub-id pub-id-type="pmid">26891625</pub-id><pub-id pub-id-type="doi">10.1038/nrn.2015.30</pub-id></element-citation></ref><ref id="R73"><label>73</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vikbladh</surname><given-names>OM</given-names></name><name><surname>Meager</surname><given-names>MR</given-names></name><name><surname>King</surname><given-names>J</given-names></name><name><surname>Blackmon</surname><given-names>K</given-names></name><name><surname>Devinsky</surname><given-names>O</given-names></name><name><surname>Shohamy</surname><given-names>D</given-names></name><name><surname>Burgess</surname><given-names>N</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><article-title>Hippocampal Contributions to Model-Based Planning and Spatial Memory</article-title><source>Neuron</source><year>2019</year><volume>102</volume><fpage>683</fpage><lpage>693</lpage><elocation-id>e4</elocation-id><pub-id pub-id-type="pmcid">PMC6508991</pub-id><pub-id pub-id-type="pmid">30871859</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2019.02.014</pub-id></element-citation></ref><ref id="R74"><label>74</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wunderlich</surname><given-names>K</given-names></name><name><surname>Smittenaar</surname><given-names>P</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><article-title>Dopamine Enhances Model-Based over Model-Free Choice Behavior</article-title><source>Neuron</source><year>2012</year><volume>75</volume><fpage>418</fpage><lpage>424</lpage><pub-id pub-id-type="pmcid">PMC3417237</pub-id><pub-id pub-id-type="pmid">22884326</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2012.03.042</pub-id></element-citation></ref><ref id="R75"><label>75</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baram</surname><given-names>AB</given-names></name><name><surname>Muller</surname><given-names>TH</given-names></name><name><surname>Whittington</surname><given-names>JCR</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name></person-group><article-title>Intuitive planning: global navigation through cognitive maps based on grid-like codes</article-title><source>bioRxiv</source><year>2018</year><elocation-id>421461</elocation-id></element-citation></ref><ref id="R76"><label>76</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mattar</surname><given-names>MG</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><article-title>Prioritized memory access explains planning and hippocampal replay</article-title><source>Nat Neurosci</source><year>2018</year><volume>21</volume><fpage>1609</fpage><lpage>1617</lpage><pub-id pub-id-type="pmcid">PMC6203620</pub-id><pub-id pub-id-type="pmid">30349103</pub-id><pub-id pub-id-type="doi">10.1038/s41593-018-0232-z</pub-id></element-citation></ref><ref id="R77"><label>77</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Momennejad</surname><given-names>I</given-names></name><name><surname>Otto</surname><given-names>AR</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name></person-group><article-title>Offline replay supports planning in human reinforcement learning</article-title><source>eLife</source><year>2018</year><volume>7</volume><fpage>1</fpage><lpage>25</lpage><pub-id pub-id-type="pmcid">PMC6303108</pub-id><pub-id pub-id-type="pmid">30547886</pub-id><pub-id pub-id-type="doi">10.7554/eLife.32548</pub-id></element-citation></ref><ref id="R78"><label>78</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfeiffer</surname><given-names>BE</given-names></name><name><surname>Foster</surname><given-names>DJ</given-names></name></person-group><article-title>Hippocampal place-cell sequences depict future paths to remembered goals</article-title><source>Nature</source><year>2013</year><volume>497</volume><pub-id pub-id-type="pmcid">PMC3990408</pub-id><pub-id pub-id-type="pmid">23594744</pub-id><pub-id pub-id-type="doi">10.1038/nature12112</pub-id></element-citation></ref><ref id="R79"><label>79</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Widloski</surname><given-names>J</given-names></name><name><surname>Foster</surname><given-names>DJ</given-names></name></person-group><article-title>Flexible rerouting of hippocampal replay sequences around changing barriers in the absence of global place field remapping</article-title><source>Neuron</source><year>2022</year><pub-id pub-id-type="pmcid">PMC9473153</pub-id><pub-id pub-id-type="pmid">35180390</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2022.02.002</pub-id></element-citation></ref><ref id="R80"><label>80</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><article-title>Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control</article-title><source>Nat Neurosci</source><year>2005</year><volume>8</volume><fpage>1704</fpage><lpage>1711</lpage><pub-id pub-id-type="pmid">16286932</pub-id></element-citation></ref><ref id="R81"><label>81</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alvernhe</surname><given-names>A</given-names></name><name><surname>Save</surname><given-names>E</given-names></name><name><surname>Poucet</surname><given-names>B</given-names></name></person-group><article-title>Local remapping of place cell firing in the Tolman detour task</article-title><source>Eur J Neurosci</source><year>2011</year><volume>33</volume><fpage>1696</fpage><lpage>1705</lpage><pub-id pub-id-type="pmid">21395871</pub-id></element-citation></ref><ref id="R82"><label>82</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenberg</surname><given-names>M</given-names></name><name><surname>Zhang</surname><given-names>T</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><article-title>Mice in a labyrinth show rapid learning, sudden insight, and efficient exploration</article-title><source>eLife</source><year>2021</year><volume>10</volume><elocation-id>e66175</elocation-id><pub-id pub-id-type="pmcid">PMC8294850</pub-id><pub-id pub-id-type="pmid">34196271</pub-id><pub-id pub-id-type="doi">10.7554/eLife.66175</pub-id></element-citation></ref><ref id="R83"><label>83</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zisch</surname><given-names>FE</given-names></name><name><surname>Newton</surname><given-names>C</given-names></name><name><surname>Coutrot</surname><given-names>A</given-names></name><name><surname>Murcia</surname><given-names>M</given-names></name><name><surname>Motala</surname><given-names>A</given-names></name><name><surname>Greaves</surname><given-names>J</given-names></name><name><surname>de Cothi</surname><given-names>W</given-names></name><name><surname>de Steed</surname><given-names>A</given-names></name><name><surname>Tyler</surname><given-names>N</given-names></name><name><surname>Gage</surname><given-names>SA</given-names></name><etal/></person-group><article-title>Comparable human spatial memory distortions in physical, desktop virtual and immersive virtual environments</article-title><year>2022</year><elocation-id>2022.01.11.475791</elocation-id></element-citation></ref><ref id="R84"><label>84</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Small</surname><given-names>WS</given-names></name></person-group><article-title>Experimental Study of the Mental Processes of the Rat II</article-title><source>Am J Psychol</source><volume>12</volume><fpage>206</fpage><lpage>239</lpage></element-citation></ref><ref id="R85"><label>85</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiener</surname><given-names>JM</given-names></name><name><surname>Büchner</surname><given-names>SJ</given-names></name><name><surname>Hölscher</surname><given-names>C</given-names></name></person-group><article-title>Taxonomy of Human Wayfinding Tasks: A Knowledge-Based Approach</article-title><source>Spat Cogn Comput</source><year>2009</year><volume>9</volume><fpage>152</fpage><lpage>165</lpage></element-citation></ref><ref id="R86"><label>86</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morris</surname><given-names>R</given-names></name></person-group><article-title>Developments of a water-maze procedure for studying spatial learning in the rat</article-title><source>J Neurosci Methods</source><year>1984</year><volume>11</volume><fpage>47</fpage><lpage>60</lpage><pub-id pub-id-type="pmid">6471907</pub-id></element-citation></ref><ref id="R87"><label>87</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tessereau</surname><given-names>C</given-names></name><name><surname>O’Dea</surname><given-names>R</given-names></name><name><surname>Coombes</surname><given-names>S</given-names></name><name><surname>Bast</surname><given-names>T</given-names></name></person-group><article-title>Reinforcement learning approaches to hippocampus-dependent flexible spatial navigation</article-title><source>Brain Neurosci Adv</source><year>2021</year><volume>5</volume><elocation-id>2398212820975634</elocation-id><pub-id pub-id-type="pmcid">PMC8042550</pub-id><pub-id pub-id-type="pmid">33954259</pub-id><pub-id pub-id-type="doi">10.1177/2398212820975634</pub-id></element-citation></ref><ref id="R88"><label>88</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wood</surname><given-names>RA</given-names></name><name><surname>Bauza</surname><given-names>M</given-names></name><name><surname>Krupic</surname><given-names>J</given-names></name><name><surname>Burton</surname><given-names>S</given-names></name><name><surname>Delekate</surname><given-names>A</given-names></name><name><surname>Chan</surname><given-names>D</given-names></name><name><surname>O’Keefe</surname><given-names>J</given-names></name></person-group><article-title>The honeycomb maze provides a novel test to study hippocampal-dependent spatial navigation</article-title><source>Nature</source><year>2018</year><volume>554</volume><fpage>102</fpage><lpage>105</lpage><pub-id pub-id-type="pmcid">PMC6342259</pub-id><pub-id pub-id-type="pmid">29364869</pub-id><pub-id pub-id-type="doi">10.1038/nature25433</pub-id></element-citation></ref><ref id="R89"><label>89</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mobbs</surname><given-names>D</given-names></name><name><surname>Wise</surname><given-names>T</given-names></name><name><surname>Suthana</surname><given-names>N</given-names></name><name><surname>Guzmán</surname><given-names>N</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Leibo</surname><given-names>JZ</given-names></name></person-group><article-title>Promises and challenges of human computational ethology</article-title><source>Neuron</source><year>2021</year><volume>109</volume><fpage>2224</fpage><lpage>2238</lpage><pub-id pub-id-type="pmcid">PMC8769712</pub-id><pub-id pub-id-type="pmid">34143951</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2021.05.021</pub-id></element-citation></ref><ref id="R90"><label>90</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alonso</surname><given-names>A</given-names></name><name><surname>Bokeria</surname><given-names>L</given-names></name><name><surname>van der Meij</surname><given-names>J</given-names></name><name><surname>Samanta</surname><given-names>A</given-names></name><name><surname>Eichler</surname><given-names>R</given-names></name><name><surname>Spooner</surname><given-names>P</given-names></name><name><surname>Lobato</surname><given-names>IN</given-names></name><name><surname>Genzel</surname><given-names>L</given-names></name></person-group><article-title>The HexMaze: A previous knowledge and schema task for mice</article-title><year>2020</year></element-citation></ref><ref id="R91"><label>91</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alvernhe</surname><given-names>A</given-names></name><name><surname>Sargolini</surname><given-names>F</given-names></name><name><surname>Poucet</surname><given-names>B</given-names></name></person-group><article-title>Rats build and update topological representations through exploration</article-title><source>Anim Cogn</source><year>2012</year><volume>15</volume><fpage>359</fpage><lpage>368</lpage><pub-id pub-id-type="pmid">21915695</pub-id></element-citation></ref><ref id="R92"><label>92</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poucet</surname><given-names>B</given-names></name><name><surname>Herrmann</surname><given-names>T</given-names></name></person-group><article-title>Exploratory patterns of rats on a complex maze provide evidence for topological coding</article-title><source>Behav Processes</source><year>2001</year><volume>53</volume><fpage>155</fpage><lpage>162</lpage><pub-id pub-id-type="pmid">11334703</pub-id></element-citation></ref><ref id="R93"><label>93</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uster</surname><given-names>HJ</given-names></name><name><surname>Bättig</surname><given-names>K</given-names></name><name><surname>Nägeli</surname><given-names>HH</given-names></name></person-group><article-title>Effects of maze geometry and experience on exploratory behavior in the rat</article-title><source>Anim Learn Behav</source><year>1976</year><volume>4</volume><fpage>84</fpage><lpage>88</lpage></element-citation></ref><ref id="R94"><label>94</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vallianatou</surname><given-names>C-A</given-names></name><name><surname>Alonso</surname><given-names>A</given-names></name><name><surname>Aleman</surname><given-names>A</given-names></name><name><surname>Genzel</surname><given-names>L</given-names></name><name><surname>Stella</surname><given-names>F</given-names></name></person-group><article-title>Learning-induced shifts in mice navigational strategies are unveiled by a minimal behavioral model of spatial exploration</article-title><source>eNeuro</source><year>2021</year><pub-id pub-id-type="pmcid">PMC8489025</pub-id><pub-id pub-id-type="pmid">34330819</pub-id><pub-id pub-id-type="doi">10.1523/ENEURO.0553-20.2021</pub-id></element-citation></ref><ref id="R95"><label>95</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ellard</surname><given-names>CG</given-names></name><name><surname>Eller</surname><given-names>MC</given-names></name></person-group><article-title>Spatial cognition in the gerbil: computing optimal escape routes from visual threats</article-title><source>Anim Cogn</source><year>2009</year><volume>12</volume><fpage>333</fpage><lpage>345</lpage><pub-id pub-id-type="pmid">18956215</pub-id></element-citation></ref><ref id="R96"><label>96</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dollé</surname><given-names>L</given-names></name><name><surname>Chavarriaga</surname><given-names>R</given-names></name><name><surname>Guillot</surname><given-names>A</given-names></name><name><surname>Khamassi</surname><given-names>M</given-names></name></person-group><article-title>Interactions of spatial strategies producing generalization gradient and blocking: A computational approach</article-title><source>PLOS Comput Biol</source><year>2018</year><volume>14</volume><elocation-id>e1006092</elocation-id><pub-id pub-id-type="pmcid">PMC5908205</pub-id><pub-id pub-id-type="pmid">29630600</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006092</pub-id></element-citation></ref><ref id="R97"><label>97</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Javadi</surname><given-names>AH</given-names></name><name><surname>Patai</surname><given-names>EZ</given-names></name><name><surname>Marin-Garcia</surname><given-names>E</given-names></name><name><surname>Margolis</surname><given-names>A</given-names></name><name><surname>Tan</surname><given-names>H-RM</given-names></name><name><surname>Kumaran</surname><given-names>D</given-names></name><name><surname>Nardini</surname><given-names>M</given-names></name><name><surname>Penny</surname><given-names>W</given-names></name><name><surname>Duzel</surname><given-names>E</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><etal/></person-group><article-title>Prefrontal Dynamics Associated with Efficient Detours and Shortcuts: A Combined Functional Magnetic Resonance Imaging and Magnetoencenphalography Study</article-title><source>J Cogn Neurosci</source><year>2019</year><volume>31</volume><fpage>1227</fpage><lpage>1247</lpage><pub-id pub-id-type="pmid">30990386</pub-id></element-citation></ref><ref id="R98"><label>98</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spiers</surname><given-names>HJ</given-names></name><name><surname>Maguire</surname><given-names>EA</given-names></name></person-group><article-title>Thoughts, behaviour, and brain dynamics during navigation in the real world</article-title><source>NeuroImage</source><year>2006</year><volume>31</volume><fpage>1826</fpage><lpage>1840</lpage><pub-id pub-id-type="pmid">16584892</pub-id></element-citation></ref><ref id="R99"><label>99</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname><given-names>AF</given-names></name><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Poort</surname><given-names>J</given-names></name></person-group><article-title>Two Distinct Types of Eye-Head Coupling in Freely Moving Mice</article-title><source>Curr Biol</source><year>2020</year><volume>30</volume><fpage>2116</fpage><lpage>2130</lpage><elocation-id>e6</elocation-id><pub-id pub-id-type="pmcid">PMC7284311</pub-id><pub-id pub-id-type="pmid">32413309</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2020.04.042</pub-id></element-citation></ref><ref id="R100"><label>100</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kryven</surname><given-names>M</given-names></name><name><surname>Yu</surname><given-names>S</given-names></name><name><surname>Kleiman-Weiner</surname><given-names>M</given-names></name><name><surname>Tenenbaum</surname><given-names>J</given-names></name></person-group><article-title>Adventures of human planners in Maze Search Task</article-title><year>2021</year></element-citation></ref><ref id="R101"><label>101</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name></person-group><source>Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming</source><conf-name>Machine Learning Proceedings 1990</conf-name><conf-sponsor>Elsevier</conf-sponsor><year>1990</year><fpage>216</fpage><lpage>224</lpage></element-citation></ref><ref id="R102"><label>102</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Z</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name></person-group><article-title>Human Replay Spontaneously Reorganizes Experience</article-title><source>Cell</source><year>2019</year><volume>178</volume><fpage>640</fpage><lpage>652</lpage><elocation-id>e14</elocation-id><pub-id pub-id-type="pmcid">PMC6657653</pub-id><pub-id pub-id-type="pmid">31280961</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2019.06.012</pub-id></element-citation></ref><ref id="R103"><label>103</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Mattar</surname><given-names>MG</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><article-title>Experience replay is associated with efficient nonlocal learning</article-title><source>Science</source><year>2021</year><volume>372</volume><elocation-id>eabf1357</elocation-id><pub-id pub-id-type="pmcid">PMC7610948</pub-id><pub-id pub-id-type="pmid">34016753</pub-id><pub-id pub-id-type="doi">10.1126/science.abf1357</pub-id></element-citation></ref><ref id="R104"><label>104</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Still</surname><given-names>S</given-names></name><name><surname>Precup</surname><given-names>D</given-names></name></person-group><article-title>An information-theoretic approach to curiosity-driven reinforcement learning</article-title><source>Theory Biosci</source><year>2012</year><volume>131</volume><fpage>139</fpage><lpage>148</lpage><pub-id pub-id-type="pmid">22791268</pub-id></element-citation></ref><ref id="R105"><label>105</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Geerts</surname><given-names>J</given-names></name><name><surname>Stachenfeld</surname><given-names>K</given-names></name><name><surname>Burgess</surname><given-names>N</given-names></name></person-group><source>Probabilistic Successor Representations with Kalman Temporal Differences</source><conf-name>Conference on Cognitive Computational Neuroscience</conf-name><year>2019</year></element-citation></ref><ref id="R106"><label>106</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><article-title>A Unifying Probabilistic View of Associative Learning</article-title><source>PLoS Comput Biol</source><year>2015</year><volume>11</volume><fpage>1</fpage><lpage>20</lpage><pub-id pub-id-type="pmcid">PMC4633133</pub-id><pub-id pub-id-type="pmid">26535896</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004567</pub-id></element-citation></ref><ref id="R107"><label>107</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Stolle</surname><given-names>M</given-names></name><name><surname>Precup</surname><given-names>D</given-names></name></person-group><chapter-title>Learning Options in Reinforcement Learning</chapter-title><source>Abstraction, Reformulation, and Approximation Lecture Notes in Computer Science</source><person-group person-group-type="editor"><name><surname>Koenig</surname><given-names>S</given-names></name><name><surname>Holte</surname><given-names>RC</given-names></name></person-group><publisher-name>Springer</publisher-name><year>2002</year><fpage>212</fpage><lpage>223</lpage></element-citation></ref><ref id="R108"><label>108</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balaguer</surname><given-names>J</given-names></name><name><surname>Spiers</surname><given-names>H</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><article-title>Neural Mechanisms of Hierarchical Planning in a Virtual Subway Network</article-title><source>Neuron</source><year>2016</year><volume>90</volume><fpage>893</fpage><lpage>903</lpage><pub-id pub-id-type="pmcid">PMC4882377</pub-id><pub-id pub-id-type="pmid">27196978</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2016.03.037</pub-id></element-citation></ref><ref id="R109"><label>109</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piray</surname><given-names>P</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><article-title>Linear reinforcement learning in planning, grid fields, and cognitive control</article-title><source>Nat Commun</source><year>2021</year><volume>12</volume><elocation-id>4942</elocation-id><pub-id pub-id-type="pmcid">PMC8368103</pub-id><pub-id pub-id-type="pmid">34400622</pub-id><pub-id pub-id-type="doi">10.1038/s41467-021-25123-3</pub-id></element-citation></ref><ref id="R110"><label>110</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frey</surname><given-names>M</given-names></name><name><surname>Tanni</surname><given-names>S</given-names></name><name><surname>Perrodin</surname><given-names>C</given-names></name><name><surname>O’Leary</surname><given-names>A</given-names></name><name><surname>Nau</surname><given-names>M</given-names></name><name><surname>Kelly</surname><given-names>J</given-names></name><name><surname>Banino</surname><given-names>A</given-names></name><name><surname>Bendor</surname><given-names>D</given-names></name><name><surname>Lefort</surname><given-names>J</given-names></name><name><surname>Doeller</surname><given-names>CF</given-names></name><etal/></person-group><article-title>Interpreting wide-band neural activity using convolutional neural networks</article-title><source>eLife</source><year>2021</year><volume>10</volume><elocation-id>e66551</elocation-id><pub-id pub-id-type="pmcid">PMC8328518</pub-id><pub-id pub-id-type="pmid">34338632</pub-id><pub-id pub-id-type="doi">10.7554/eLife.66551</pub-id></element-citation></ref><ref id="R111"><label>111</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erdem</surname><given-names>UM</given-names></name><name><surname>Hasselmo</surname><given-names>M</given-names></name></person-group><article-title>A goal-directed spatial navigation model using forward trajectory planning based on grid cells</article-title><source>Eur J Neurosci</source><year>2012</year><volume>35</volume><fpage>916</fpage><lpage>931</lpage><pub-id pub-id-type="pmcid">PMC3564559</pub-id><pub-id pub-id-type="pmid">22393918</pub-id><pub-id pub-id-type="doi">10.1111/j.1460-9568.2012.08015.x</pub-id></element-citation></ref><ref id="R112"><label>112</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kubie</surname><given-names>JL</given-names></name><name><surname>Fenton</surname><given-names>AA</given-names></name></person-group><article-title>Heading-vector navigation based on head-direction cells and path integration</article-title><source>Hippocampus</source><year>2009</year><volume>19</volume><fpage>456</fpage><lpage>479</lpage><pub-id pub-id-type="pmid">19072761</pub-id></element-citation></ref><ref id="R113"><label>113</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehta</surname><given-names>MR</given-names></name><name><surname>Quirk</surname><given-names>MC</given-names></name><name><surname>Wilson</surname><given-names>MA</given-names></name></person-group><article-title>Experience-Dependent Asymmetric Shape of Hippocampal Receptive Fields</article-title><source>Neuron</source><year>2000</year><volume>25</volume><fpage>707</fpage><lpage>715</lpage><pub-id pub-id-type="pmid">10774737</pub-id></element-citation></ref><ref id="R114"><label>114</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehta</surname><given-names>MR</given-names></name><name><surname>Barnes</surname><given-names>CA</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name></person-group><article-title>Experience-dependent, asymmetric expansion of hippocampal place fields</article-title><source>Proc Natl Acad Sci</source><year>1997</year><volume>94</volume><fpage>8918</fpage><lpage>8921</lpage><pub-id pub-id-type="pmcid">PMC23195</pub-id><pub-id pub-id-type="pmid">9238078</pub-id><pub-id pub-id-type="doi">10.1073/pnas.94.16.8918</pub-id></element-citation></ref><ref id="R115"><label>115</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duvelle</surname><given-names>É</given-names></name><name><surname>Grieves</surname><given-names>RM</given-names></name><name><surname>Liu</surname><given-names>A</given-names></name><name><surname>Jedidi-Ayoub</surname><given-names>S</given-names></name><name><surname>Holeniewska</surname><given-names>J</given-names></name><name><surname>Harris</surname><given-names>A</given-names></name><name><surname>Nyberg</surname><given-names>N</given-names></name><name><surname>Donnarumma</surname><given-names>F</given-names></name><name><surname>Lefort</surname><given-names>JM</given-names></name><name><surname>Jeffery</surname><given-names>KJ</given-names></name><etal/></person-group><article-title>Hippocampal place cells encode global location but not connectivity in a complex space</article-title><source>Curr Biol</source><year>2021</year><volume>31</volume><fpage>1221</fpage><lpage>1233</lpage><elocation-id>e9</elocation-id><pub-id pub-id-type="pmcid">PMC7988036</pub-id><pub-id pub-id-type="pmid">33581073</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2021.01.005</pub-id></element-citation></ref><ref id="R116"><label>116</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Momennejad</surname><given-names>I</given-names></name><name><surname>Howard</surname><given-names>MW</given-names></name></person-group><article-title>Predicting the Future with Multi-scale Successor Representations</article-title><year>2018</year></element-citation></ref><ref id="R117"><label>117</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sarel</surname><given-names>A</given-names></name><name><surname>Finkelstein</surname><given-names>A</given-names></name><name><surname>Las</surname><given-names>L</given-names></name><name><surname>Ulanovsky</surname><given-names>N</given-names></name></person-group><article-title>Vectorial representation of spatial goals in the hippocampus of bats</article-title><source>Science</source><year>2017</year><volume>355</volume><fpage>176</fpage><lpage>180</lpage><pub-id pub-id-type="pmid">28082589</pub-id></element-citation></ref><ref id="R118"><label>118</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clemens</surname><given-names>LE</given-names></name><name><surname>Jansson</surname><given-names>EKH</given-names></name><name><surname>Portal</surname><given-names>E</given-names></name><name><surname>Riess</surname><given-names>O</given-names></name><name><surname>Nguyen</surname><given-names>HP</given-names></name></person-group><article-title>A behavioral comparison of the common laboratory rat strains Lister Hooded, Lewis, Fischer 344 and Wistar in an automated homecage system</article-title><source>Genes Brain Behav</source><year>2014</year><volume>13</volume><fpage>305</fpage><lpage>321</lpage><pub-id pub-id-type="pmid">24119005</pub-id></element-citation></ref><ref id="R119"><label>119</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heffner</surname><given-names>RS</given-names></name><name><surname>Heffner</surname><given-names>HE</given-names></name></person-group><article-title>Visual factors in sound localization in mammals</article-title><source>J Comp Neurol</source><year>1992</year><volume>317</volume><fpage>219</fpage><lpage>232</lpage><pub-id pub-id-type="pmid">1577997</pub-id></element-citation></ref><ref id="R120"><label>120</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hart</surname><given-names>PE</given-names></name><name><surname>Nilsson</surname><given-names>NJ</given-names></name><name><surname>Raphael</surname><given-names>B</given-names></name></person-group><article-title>A Formal Basis for the Heuristic Determination of Minimum Cost Paths</article-title><source>IEEE Trans Syst Sci Cybern</source><year>1968</year><volume>4</volume><fpage>100</fpage><lpage>107</lpage></element-citation></ref><ref id="R121"><label>121</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Moore</surname><given-names>CD</given-names></name><name><surname>Todd</surname><given-names>MT</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name><name><surname>Sederberg</surname><given-names>PB</given-names></name></person-group><article-title>The Successor Representation and Temporal Context</article-title><source>Neural Comput</source><year>2012</year><volume>24</volume><fpage>1553</fpage><lpage>1568</lpage><pub-id pub-id-type="pmid">22364500</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>The Tartarus Maze.</title><p><bold>(A)</bold> Schematic of the maze composed of 10x10 units for humans, rats and RL agents. For rats each unit (20x20cm) had a port to potentially deliver a chocolate milk reward after the rat waited 5 sec at the goal (See <xref ref-type="supplementary-material" rid="SD1">Video S1</xref>). For humans each unit could be associated with a hidden gold star linked to financial reward, which appeared after waiting 5 sec at the goal location. Gaps between units were not visible to humans to avoid counting distance to the hidden goal (see <xref ref-type="supplementary-material" rid="SD1">Video S2</xref>). Example trial shows one of the possible pseudo-random starting locations on the edge of the maze. <bold>(B)</bold> After training, flexible navigation was tested by removing units from the maze to create maze configurations with gaps between traversable surfaces. Example from maze configuration 21 is shown with one of the 10 starting locations tested. Each configuration was tested for 10 trials, with each trial having a different starting location. <bold>(C)</bold> Sequence of 25 maze configurations used. <bold>(D)</bold> Illustration of the trial sequence, highlighting the transition in layout every 10 trials across the 250 trials tested in the 25 maze configurations.</p></caption><graphic xlink:href="EMS147055-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Humans and rats were able to successfully navigate to the hidden goal and generally did so using similar routes.</title><p>Examples of the human <bold>(A)</bold> and rat <bold>(B)</bold> trajectories overlaying occupancy maps for a given trial. The white-black colour gradient shows the beginning-end of each trajectory. <bold>(C)</bold> Proportion of trials where the goal was reached averaged over all configurations as a function of trial number, for rats (red) or humans (green). Grey areas indicate standard error from the mean. <bold>(D)</bold> Proportion of goals reached by humans and rats during the time limit. The rats outperformed humans on a total of three maze configurations (2,10,19), which was most pronounced for configuration 2. Configuration 1 was the configuration both species performed most inaccurately on. <bold>(E)</bold> Correlation between the human and rat occupancy maps. Note the increase across exposure to a maze configuration implying they take increasingly similar routes. <bold>(F)</bold> Average deviation from optimal path (measured in extra maze module visitations) for rats and humans as a function of trial number; humans navigate to the goal with more efficient routes.</p></caption><graphic xlink:href="EMS147055-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Maximum likelihood analyses of the human and rat trajectories.</title><p>Likelihood analysis reveals that the behaviour of humans (left) and rats (right) is better predicted by a successor representation agent (SR) than model-based (MB) or model-free (MF). <bold>(A)</bold> The value estimates generated by the SR agent provide a more likely explanation of the biological behaviour than either the MF or MB agents. <bold>(B)</bold> The SR agent was the maximum likelihood model to explain the biological behaviour for the majority of trials. <bold>(C)</bold> This trend is true across all individuals in both species (humans n=18; rats n=9) and robust throughout exposure to a maze configuration. <bold>(D)</bold> Likelihood estimates vary across the maze configurations used, with a strong correlation between the model likelihoods for the human and rat behaviour (r = .57, <italic>p</italic>&lt;.001).</p></caption><graphic xlink:href="EMS147055-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Human and rat performance compared with maximum likelihood reinforcement learning agents.</title><p>Goal reaching for agents (n=100 per participant/animal) using the maximum likelihood parameters fit to individual human <bold>(A)</bold> and rat <bold>(B)</bold> trajectories. Grey areas indicate standard error from the mean. <bold>(C-D)</bold> Goal reaching varied across maze configurations. Using this to rank maze configurations by difficulty reveals a significantly more positive correlation between the human and successor representation (SR) agent’s difficulty rankings <bold>(E)</bold> than the model-based (MB) or model-free (MF) (paired t-test following Fisher transformation: SR vs MF, t(17)=3.27, p=.004; SR vs MB, t(17)=4.57, p&lt;.001; MB vs MF, t(17)=0.35, p=.728). The rat difficulty rankings <bold>(F)</bold> correlated significantly lower with the model-free agent’s than either of the other agent’s (paired t-test following Fisher transformation: MF vs MB, t(8)=-4.33, p=.002; MF vs SR, t(8)=-2.87, p=.021; SR vs. MB, t(8)=1.00, p=.345). Error bars indicate standard error from the mean.</p></caption><graphic xlink:href="EMS147055-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Diffusivity analysis reveals rat and human trajectories are most similar to a successor representation agent.</title><p><bold>(A)</bold> Each trajectory was quantified using the average linear diffusivity <bold>(B)</bold> and the sine <bold>(C)</bold> and cosine of the average angular diffusivity. <bold>(D)</bold> Using these metrics to quantify the trajectories on each maze configuration, agent behaviour can be seen to form clusters (shown here via t-sne), where each point represents the average of an individual human, rat or agent over the whole experiment. Note, that for different embeddings a similar pattern emerges. Calculating the Mahalanobis distance between clusters reveals the human <bold>(E)</bold> and rat <bold>(F)</bold> behaviour is more similar to a successor representation (SR) agent than the model-based (MB) or model-free (MF) agents (Humans: SR vs MF, t(17)=-55.7, <italic>p</italic>&lt;.001; SR vs MB, t(17)=-4.21, <italic>p</italic>&lt;.001; Rats: SR vs MF, t(8)=-11.1, <italic>p</italic>&lt;.001; SR vs MB, t(8)=-12.7, <italic>p</italic>&lt;.001).</p></caption><graphic xlink:href="EMS147055-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><title>Trajectory similarity analysis identifies SR agent trajectories as closest to human and rat behaviour.</title><p>Trajectory similarity to the model-free (MF), model-based (MB) and successor representation (SR) agents was measured using the average minimum path distance along each human (left column) and rat (right column) trajectory. <bold>(A)</bold> SR agent trajectories were in general closest to both the human (SR vs MB: t(17)=8.32, <italic>p</italic>&lt;.001 SR vs MF: t(17)=28.8, <italic>p</italic>&lt;.001) and <bold>(B)</bold> rat behaviour (SR vs MB: t(8)=6.44, <italic>p</italic>&lt;.001; SR vs MF: t(17)=26.4, <italic>p</italic>&lt;.001), although <bold>(C)</bold> humans displayed evidence of model-based planning in early trials on a maze new configuration (first 5 trials on a configuration, MB vs SR: t(17)=3.30, <italic>p</italic>=.004; MB vs MF: t(17)=17.0, <italic>p</italic>&lt;.001). Later trials on a maze configuration, which required longer and more complex routes to the goal, were again closest to the SR agent behaviour for both the human (last 5 trials on a configuration, SR vs MB: t(17)=8.95, <italic>p</italic>&lt;.001; SR vs MF: t(17)=33.6, <italic>p</italic>&lt;.001) and <bold>(D)</bold> rat data (last 5 trials on a configuration SR vs MB: t(8)=13.4, <italic>p</italic>&lt;.001; SR vs MF: t(8)=24.8, <italic>p</italic>&lt;.001). The level of similarity to agent trajectories varied across maze configurations <bold>(E-F)</bold> with a strong correlation between the humans and rats (Pearson correlation between both matrices: ρ = .93, <italic>p</italic>&lt;.001).</p></caption><graphic xlink:href="EMS147055-f006"/></fig></floats-group></article>