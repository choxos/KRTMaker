<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS144881</article-id><article-id pub-id-type="doi">10.1101/2020.12.17.423244</article-id><article-id pub-id-type="archive">PPR256176</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Consensus clustering for Bayesian mixture models</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Coleman</surname><given-names>Stephen</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Kirk</surname><given-names>Paul D.W.</given-names></name><email>paul.kirk@mrc-bsu.cam.ac.uk</email><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Wallace</surname><given-names>Chris</given-names></name><email>cew54@cam.ac.uk</email><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib></contrib-group><aff id="A1"><label>1</label>MRC Biostatistics Unit</aff><aff id="A2"><label>2</label>Cambridge Institute of Therapeutic Immunology &amp; Infectious Disease University of Cambridge, U.K.</aff><author-notes><corresp id="CR1"><label>*</label>Corresponding author: <email>stephen.coleman@mrc-bsu.cam.ac.uk</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>10</day><month>05</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>09</day><month>05</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Cluster analysis is an integral part of precision medicine and systems biology, used to define groups of patients or biomolecules. Consensus clustering is an ensemble approach that is widely used in these areas, which combines the output from multiple runs of a non-deterministic clustering algorithm. Here we consider the application of consensus clustering to a broad class of heuristic clustering algorithms that can be derived from Bayesian mixture models (and extensions thereof) by adopting an early stopping criterion when performing sampling-based inference for these models. While the resulting approach is non-Bayesian, it inherits the usual benefits of consensus clustering, particularly in terms of computational scalability and providing assessments of clustering stability/robustness.</p><p id="P2">In simulation studies, we show that our approach can successfully uncover the target clustering structure, while also exploring different plausible clusterings of the data. We show that, when a parallel computation environment is available, our approach offers significant reductions in runtime compared to performing sampling-based Bayesian inference for the underlying model, while retaining many of the practical benefits of the Bayesian approach, such as exploring different numbers of clusters. We propose a heuristic to decide upon ensemble size and the early stopping criterion, and then apply consensus clustering to a clustering algorithm derived from a Bayesian integrative clustering method. We use the resulting approach to perform an integrative analysis of three ‘omics datasets for budding yeast and find clusters of co-expressed genes with shared regulatory proteins. We validate these clusters using data external to the analysis. These clusters can help assign likely function to understudied genes, for example <italic>GAS3</italic> clusters with histones active in S-phase, suggesting a role in DNA replication.</p><p id="P3">Our approach can be used as a wrapper for essentially any existing sampling-based Bayesian clustering implementation, and enables meaningful clustering analyses to be performed using such implementations, even when computational Bayesian inference is not feasible, e.g. due to poor exploration of the target density (often as a result of increasing numbers of features) or a limited computational budget that does not along sufficient samples to drawn from a single chain. This enables researchers to straightforwardly extend the applicability of existing software to much larger datasets, including implementations of sophisticated models such as those that jointly model multiple datasets.</p></abstract><kwd-group><kwd>Cluster analysis</kwd><kwd>Multiomics</kwd><kwd>Ensemble learning</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Background</title><p id="P4">From defining a taxonomy of disease to creating molecular sets, grouping items can help us to understand and make decisions using complex biological data. For example, grouping patients based upon disease characteristics and personal omics data may allow the identification of more homogeneous subgroups, enabling stratified medicine approaches. Defining and studying molecular sets can improve our understanding of biological systems as these sets are more interpretable than their constituent members (<xref ref-type="bibr" rid="R1">1</xref>), and study of their interactions and perturbations may have ramifications for diagnosis and drug targets (<xref ref-type="bibr" rid="R2">2</xref>, <xref ref-type="bibr" rid="R3">3</xref>). The act of identifying such groups is referred to as <italic>cluster analysis.</italic> Many traditional methods such as <italic>K</italic>-means clustering (<xref ref-type="bibr" rid="R4">4</xref>, <xref ref-type="bibr" rid="R5">5</xref>) condition upon a fixed choice of <italic>K</italic>, the number of clusters. These methods are often heuristic in nature, relying on rules of thumb to decide upon a final value for <italic>K.</italic> For example, different choices of <italic>K</italic> are compared under some metric such as silhouette (<xref ref-type="bibr" rid="R6">6</xref>) or the within-cluster sum of squared errors (<bold>SSE</bold>) as a function of <italic>K.</italic> Moreover, <italic>K</italic>-means clustering can exhibit sensitivity to initialisation, necessitating multiple runs in practice (<xref ref-type="bibr" rid="R7">7</xref>).</p><p id="P5">Another common problem is that traditional methods offer no measure of the stability or robustness of the final clustering. Returning to the stratified medicine example of clustering patients, there might be individuals that do not clearly belong to any one particular cluster; however if only a point estimate is obtained, this information is not available. Ensemble methods address this problem, as well as reducing sensitivity to initialisation. These approaches have had great success in supervised learning, most famously in the form of Random Forest (<xref ref-type="bibr" rid="R8">8</xref>) and boosting (<xref ref-type="bibr" rid="R9">9</xref>). In clustering, consensus clustering (<xref ref-type="bibr" rid="R10">10</xref>) is a popular method which has been implemented in R (<xref ref-type="bibr" rid="R11">11</xref>) and to a variety of methods (<xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R13">13</xref>) and been applied to problems such as cancer subtyping (<xref ref-type="bibr" rid="R14">14</xref>, <xref ref-type="bibr" rid="R15">15</xref>) and identifying subclones in single cell analysis (<xref ref-type="bibr" rid="R16">16</xref>). Consensus clustering uses <italic>W</italic> runs of some base clustering algorithm (such as <italic>K</italic>-means). These <italic>W</italic> proposed partitions are commonly compiled into a <italic>consensus matrix</italic>, the (<italic>i</italic>, <italic>j</italic>)<sup><italic>th</italic></sup> entries of which contain the proportion of model runs for which the <italic>i<sup>th</sup></italic> and <italic>j<sup>th</sup></italic> individuals co-cluster (for this and other definitions see <xref ref-type="supplementary-material" rid="SD1">section 1 of the Supplementary Material</xref>), although this step is not fundamental to consensus clustering and there is a large body of literature aimed at interpreting a collection of partitions (see, e.g., <xref ref-type="bibr" rid="R17">17</xref>, <xref ref-type="bibr" rid="R18">18</xref>, <xref ref-type="bibr" rid="R19">19</xref>). This consensus matrix provides an assessment of the stability of the clustering. Furthermore, ensembles can offer reductions in computational runtime because the individual members of the ensemble are often computationally inexpensive to fit (e.g, because they are fitted using only a subset of the available data) and because the learners in most ensemble methods are independent of each other and thus enable use of a parallel environment for each of the quicker model runs (<xref ref-type="bibr" rid="R20">20</xref>).</p><p id="P6">Traditional clustering methods usually condition upon a fixed choice of <italic>K</italic>, the number of clusters with the choice of <italic>K</italic> being a difficult problem in itself. In consensus clustering, Monti <italic>et al.</italic> (<xref ref-type="bibr" rid="R10">10</xref>) proposed methods for choosing <italic>K</italic> using the consensus matrix and Ünlü <italic>et al.</italic> (<xref ref-type="bibr" rid="R21">21</xref>) offer an approach to estimating <italic>K</italic> given the collection of partitions, but each clustering run uses the same, fixed, number of clusters. An alternative clustering approach, mixture modelling, embeds the cluster analysis within a formal, statistical framework (<xref ref-type="bibr" rid="R22">22</xref>). This means that models can be compared formally, and problems such as the choice of <italic>K</italic> can be addressed as a model selection problem (<xref ref-type="bibr" rid="R23">23</xref>). Moreover, <italic>Bayesian mixture models</italic> can be used to try to directly infer <italic>K</italic> from the data. Such inference can be performed through use of a Dirichlet Process mixture model (<xref ref-type="bibr" rid="R24">24</xref>, <xref ref-type="bibr" rid="R25">25</xref>, <xref ref-type="bibr" rid="R26">26</xref>), a mixture of finite mixture models (<xref ref-type="bibr" rid="R27">27</xref>, <xref ref-type="bibr" rid="R28">28</xref>) or an over-fitted mixture model (<xref ref-type="bibr" rid="R29">29</xref>). The Bayesian model also assesses the uncertainty in the cluster allocations, and if <italic>K</italic> is treated as a random variable uncertainty about the value of <italic>K</italic> propagates through to the clustering. Furthermore, the Bayesian hierarchical modelling framework enables extrapolating the mixture model to capture more complex dependencies, for example, integrative clustering methods tailored for multi-omics analysis (<xref ref-type="bibr" rid="R30">30</xref>, <xref ref-type="bibr" rid="R31">31</xref>, <xref ref-type="bibr" rid="R32">32</xref>). Bayesian clustering methods have a history of successful application to a diverse range of biological problems such as finding clusters of gene expression profiles (<xref ref-type="bibr" rid="R33">33</xref>), cell types in flow cytometry (<xref ref-type="bibr" rid="R34">34</xref>, <xref ref-type="bibr" rid="R35">35</xref>) or scRNAseq experiments (<xref ref-type="bibr" rid="R36">36</xref>), and estimating protein localisation (<xref ref-type="bibr" rid="R37">37</xref>).</p><p id="P7">Markov chain Monte Carlo (MCMC) methods are the most common tool for performing computational Bayesian inference. They guarantee an exact description of the posterior distribution in the limit of infinite iterations in contrast to Variational Inference (<xref ref-type="bibr" rid="R38">38</xref>). In Bayesian clustering, they are used to draw a collection of clustering partitions from the posterior distribution. In practice MCMC methods may explore the parameter space very inefficiently despite their ergodic properties. As the number of features/measurements increases this inefficiency can become pathological with chains prone to becoming stuck in local posterior modes preventing convergence in any feasible period of runtime (see, e.g., the <xref ref-type="supplementary-material" rid="SD1">Supplementary Materials of 39</xref>); this problem is frequently referred to as poor mixing within the chain. There is a rich zoo of MCMC methods designed to overcome the different limitations of the most basic samplers. For example, there are MCMC methods that use parallel chains to improve the scalability with an increasing number of observations, such as Consensus Monte Carlo (<xref ref-type="bibr" rid="R40">40</xref>, <xref ref-type="bibr" rid="R41">41</xref>, <xref ref-type="bibr" rid="R42">42</xref>). Consensus Monte Carlo methods subsample the original dataset and run separate chains on each smaller dataset. In this way they can use a far smaller quantity of data for each Monte Carlo algorithm and treat each chain as embarrassingly parallel enabling simultaneous model runs across machines, with the samples then averaged across chains. This parallelisation and reduced dataset size offers a significant reduction in runtime for large <italic>N</italic> datasets. Another method designed to improve scaling to large datasets, is stochastic gradient MCMC (SGMCMC 43). This uses a subset of the data in each sampling iteration and has provable guarantees (<xref ref-type="bibr" rid="R44">44</xref>). However, SGMCMC converges at a slower rate than traditional MCMC algorithms and remains computationally costly (<xref ref-type="bibr" rid="R45">45</xref>, <xref ref-type="bibr" rid="R46">46</xref>). While these methods help in scaling to large N data, they are less helpful in situations where we have high-dimension but only moderate sample size, such as frequently arises in analysis of ‘omics data. Other methods such as coupling (<xref ref-type="bibr" rid="R47">47</xref>) use multiple chains to reduce the bias of the Monte Carlo estimate.</p><p id="P8">Other MCMC methods make efforts to overcome the problem of poor mixing at the cost of increased computational cost per iteration (<xref ref-type="bibr" rid="R48">48</xref>). In clustering models introducing split-merge moves into the sampler are the most common examples of such bold exploration moves (see, e.g., <xref ref-type="bibr" rid="R49">49</xref>, <xref ref-type="bibr" rid="R50">50</xref>,<xref ref-type="bibr" rid="R51">51</xref>, <xref ref-type="bibr" rid="R52">52</xref>). However, these methods are difficult to implement and frequently propose many rejected moves, thereby increasing computational cost without necessarily guaranteeing full exploration of the target density in any finite amount of time. Furthermore, most available Bayesian clustering methods are implemented using a basic Gibbs sampler and would require reimplementation to exploit more scalable samplers, a costly investment of time and effort. Ideally these existing implementations could be used despite their simple sampler.</p><p id="P9">There also exists a range of alternative clustering methods that are designed or have been extended to scale well with increasing sample size, e.g., (<italic>k</italic>-means clustering (<xref ref-type="bibr" rid="R53">53</xref>, <xref ref-type="bibr" rid="R54">54</xref>), spectral clustering (<xref ref-type="bibr" rid="R55">55</xref>, <xref ref-type="bibr" rid="R56">56</xref>), density-based clustering (<xref ref-type="bibr" rid="R57">57</xref>, <xref ref-type="bibr" rid="R58">58</xref>), etc., any of which could be used within a consensus clustering wrapper. However, while these methods have better scalability than sampling based clustering methods, they suffer from a lack of flexibility. They do not, in general, have the ability to explore multiple values of <italic>K</italic> in a single model run, it is not easy to extend these methods to the multiple dataset problem and they are often restricted to a specific data type.</p><p id="P10">As described above, sampling-based Bayesian clustering methods are flexible, capable of modelling complex dependencies and the number of clusters present. However, they suffer from prohibitive runtimes and poor exploration in high-dimensional data (i.e., large <italic>P</italic> data). This limits the consistency of their inference in biomedical applications where data is often very high dimensional with only moderate sample size (for some discussion around stability in clustering, please see <xref ref-type="bibr" rid="R59">59</xref>, <xref ref-type="bibr" rid="R60">60</xref>, <xref ref-type="bibr" rid="R61">61</xref>). Motivated by this, we aim to develop a general and straightforward procedure that exploits the flexibility of Bayesian model based clustering methods but improves their performance under a constrained computational budget without requiring reimplementation. Specifically, we make use of existing sampling-based Bayesian clustering implementations, but only run them for a fixed (and relatively small) number of iterations, stopping before they have converged to their target stationary distribution. We initialise each chain with a random draw from an uninformative prior distribution on the space of partitions and then collect the final sampled partition. Doing this repeatedly, we obtain an ensemble of clustering partitions which has large variety in its initialisation. We use this set to perform consensus clustering, constructing a consensus matrix (thereby avoiding the label-switching problem) which describes uncertainty about the latent structure in the data. This can be used to infer a point estimate clustering. We propose a heuristic for deciding upon the ensemble size (the number of learners used, <italic>W</italic>) and the ensemble depth (the number of iterations, <italic>D</italic>), inspired by the use of scree plots in Principal Component Analysis (<bold>PCA;</bold> <xref ref-type="bibr" rid="R62">62</xref>). We hope to show a way of scaling Bayesian mixture models and their extensions with increasing numbers of features that can explore a range of <italic>K</italic> in a single model run and can be tailored to specific properties of a given dataset. We note that, despite the similarity in names, our consensus clustering approach for Bayesian mixture models is very different to the consensus Monte Carlo approach of Ni <italic>et al.</italic> (<xref ref-type="bibr" rid="R41">41</xref>), which was designed to enable Bayesian mixture models to scale to large <italic>N</italic> datasets. Our approach leans into the ensemble framework of Monti <italic>et al.</italic> (<xref ref-type="bibr" rid="R10">10</xref>); we consider the case that our individual chains are too short to have converged and in which case the inference is non-Bayesian, in contrast to Consensus Monte Carlo. Our primary aim is to mitigate the problem of poor mixing which tends to emerge when the data has a high numbers of features relative to the sample size and individual chains struggle to converge in a reasonable runtime, and to enable the use of complex models such as arise in multi-view or integrative analyses for which each iteration of the MCMC algorithm is slow even for small sample data and running a long chain might not be feasible.</p><p id="P11">We show via simulation that our approach can successfully identify meaningful clustering structures even when chains are very short. We then illustrate the use of our approach to extend the applicability of existing Bayesian clustering implementations, using as a case study the Multiple Dataset Integration (<bold>MDI</bold> <xref ref-type="bibr" rid="R30">30</xref>, <xref ref-type="supplementary-material" rid="SD1">section 2 of the Supplementary Material</xref>) model for Bayesian integrative clustering applied to real data. While the simulation results serve to validate our method, it is important to also evaluate methods on real data which may represent more challenging problems. For our real data, we use three ‘omics datasets relating to the cell cycle of <italic>Saccharomyces cerevisiae</italic> with the aim of inferring clusters of genes across datasets. As there is no ground truth available, we then validate these clusters using knowledge external to the analysis.</p></sec><sec id="S2" sec-type="materials | methods"><title>Material and methods</title><sec id="S3"><title>Consensus clustering for Bayesian mixture models</title><p id="P12">We apply consensus clustering to MCMC based Bayesian clustering models using the method described in <xref ref-type="other" rid="BX1">algorithm 1</xref>. Our application of consensus clustering has two main parameters at the ensemble level, the chain depth, <italic>D</italic>, and ensemble width, <italic>W</italic>. We infer a point clustering from the consensus matrix using the <monospace>maxpear</monospace> function (<xref ref-type="bibr" rid="R63">63</xref>) from the R package <monospace>mcclust</monospace> (<xref ref-type="bibr" rid="R64">64</xref>) which maximises the posterior expected adjusted <boxed-text id="BX1" position="anchor" content-type="above"><caption><title>Algorithm 1: Consensus clustering for Bayesian mixture models</title><p id="P13"><bold>Data:</bold><italic>X =</italic> (<italic>x</italic><sub>1</sub>,… ,<italic>x<sub>N</sub></italic>)</p><p id="P14"><bold>Input:</bold> </p><p id="P15">The number of chains to run, <italic>W</italic></p><p id="P16">The number of iterations within each chain, <italic>D</italic></p><p id="P17">A clustering method that uses MCMC methods to generate samples of clusterings of the data <italic>Cluster</italic>(<italic>X, d</italic>)</p><p id="P18"><bold>Output:</bold> </p><p id="P19">A predicted clustering, <italic>Ŷ</italic></p><p id="P20">The consensus matrix M</p><p id="P21"><bold>begin</bold> </p><p id="P22">      /* initialise an empty consensus matrix*/</p><p id="P23">      M←0<sub><italic>N</italic>×<italic>N</italic></sub>;</p><p id="P24">      <bold>for</bold> <italic>w</italic> = 1 <bold>to</bold> <italic>W</italic> <bold>do</bold></p><p id="P25">           /* set the random seed controlling initialisation and MCMC moves*/</p><p id="P26">           <italic>set.seed</italic>(<italic>w</italic>)<italic>;</italic></p><p id="P27">           /* initialise a random partition on <italic>X</italic> drawn from the prior distribution</p><p id="P28">      */</p><p id="P29">           <italic>Y</italic><sub>(0.<italic>w</italic>)</sub> ← <italic>Initialise</italic> (<italic>X</italic>)<italic>;</italic></p><p id="P30">           <bold>for</bold> <italic>d</italic> = 1 <bold>to</bold> <italic>D</italic> <bold>do</bold></p><p id="P31">               /* generate a markov chain for the membership vector*/</p><p id="P32">               <italic>Y</italic><sub>(0.<italic>w</italic>) ←</sub> <italic>Cluster</italic>(<italic>X,d</italic>)<italic>;</italic></p><p id="P33">           <bold>end</bold></p><p id="P34">           /* create a coclustering matrix from the <italic>D<sup>th</sup></italic> sample*/</p><p id="P35">           B<sup>(<italic>w</italic>)</sup><italic>Y</italic><sub>(<italic>D</italic>,<italic>w</italic>)</sub>;</p><p id="P36">           M ←M + B<sup>(<italic>W</italic>)</sup>;</p><p id="P37">   <bold>end</bold></p><p id="P38">   M ← <inline-formula><mml:math id="M1"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>W</mml:mi></mml:mfrac></mml:math></inline-formula>M;</p><p id="P39">   <italic>Ŷ</italic> ← partition <italic>X</italic> based upon M;</p><p id="P40"><bold>end</bold> </p></caption></boxed-text>
</p><p id="P41">Rand index between the true clustering and point estimate if the matrix is composed of samples drawn from the posterior distribution (<xref ref-type="supplementary-material" rid="SD1">section 3 of the Supplementary Material</xref> for details). There are alternative choices of methods to infer a point estimate which minimise different loss functions (see, e.g., <xref ref-type="bibr" rid="R65">65</xref>, <xref ref-type="bibr" rid="R66">66</xref>, <xref ref-type="bibr" rid="R67">67</xref>).</p></sec><sec id="S4"><title>Determining the ensemble depth and width</title><p id="P42">As our ensemble sidesteps the problem of convergence within each chain, we need an alternative stopping rule for growing the ensemble in chain depth, <italic>D</italic>, and number of chains, <italic>W.</italic> We propose a heuristic based upon the consensus matrix to decide if a given value of <italic>D</italic> and <italic>W</italic> are sufficient. We suspect that increasing <italic>W</italic> and <italic>D</italic> might continuously improve the performance of the ensemble, but we observe in our simulations that these changes will become smaller and smaller for greater values, eventually converging for each of <italic>W</italic> and <italic>D.</italic> We notice that this behaviour is analogous to PCA in that where for consensus clustering some improvement might always be expected for increasing chain depth or ensemble width, more variance will be captured by increasing the number of components used in PCA. However, increasing this number beyond some threshold has diminishing returns, diagnosed in PCA by a scree plot. Following from this, we recommend, for some set of ensemble parameters, <italic>D’ =</italic> {<italic>d</italic><sub>1</sub><italic>,..., d<sub>I</sub></italic>} and <italic>W’ =</italic> {<italic>w</italic><sub>1</sub>,..<italic>.,w<sub>J</sub>}</italic>, find the mean absolute difference of the consensus matrix for the <inline-formula><mml:math id="M2"><mml:msubsup><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> iteration from <italic>w<sub>j</sub></italic> chains to that for the <inline-formula><mml:math id="M3"><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> from <italic>w<sub>j</sub></italic> chains and plot these values as a function of chain depth, and the analogue for sequential consensus matrices for increasing ensemble width and constant depth.</p><p id="P43">If this heuristic is used, we believe that the consensus matrix and the resulting inference should be stable (see, e.g., <xref ref-type="bibr" rid="R59">59</xref>, <xref ref-type="bibr" rid="R60">60</xref>), providing a robust estimate of the clustering. In contrast, if there is still strong variation in the consensus matrix for varying chain length or number, then we believe that the inferred clustering is influenced significantly by the random initialisation and that the inferred partition is unlikely to be stable for similar datasets or reproducible for a random choice of seeds.</p></sec></sec><sec id="S5"><title>Simulation study</title><p id="P44">We use a finite mixture with independent features as the data generating model within the simulation study. Within this model there exist “irrelevant features” (<xref ref-type="bibr" rid="R68">68</xref>) that have global parameters rather than cluster specific parameters. The generating model is <disp-formula id="FD1"><label>(1)</label><mml:math id="M4"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtext>      </mml:mtext><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi>π</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:munderover><mml:mrow><mml:mi>p</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mi>p</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p id="P45">for data <italic>X</italic> = (<italic>x</italic><sub>1</sub>,..., <italic>x<sub>N</sub></italic>), cluster label or allocation variable <italic>c</italic> = (<italic>c</italic><sub>1</sub>,..., <italic>c<sub>N</sub></italic>), cluster weight <italic>π</italic> = (<italic>π</italic><sub>1</sub>,..., <italic>π<sub>K</sub></italic>), <italic>K</italic> clusters and the relevance variable, <italic>ø</italic> ϵ {0,1} with <italic>ø<sub>p</sub></italic> = 1 indicating that the <italic>p<sup>th</sup></italic> feature is relevant to the clustering. We used a <italic>Gaussian</italic> density, so <italic>θ<sub>kp</sub></italic> = (<italic>μ<sub>kp</sub></italic>, <inline-formula><mml:math id="M5"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:math></inline-formula> We defined three scenarios and simulated 100 datasets in each (<xref ref-type="fig" rid="F1">figure 1</xref> and <xref ref-type="table" rid="T1">table 1</xref>). Additional details of the simulation process and additional scenarios are included in <xref ref-type="supplementary-material" rid="SD1">section 4.1 of the Supplementary Materials</xref>.</p><p id="P46">In each of these scenarios we apply a variety of methods (listed below) and compare the inferred point clusterings to the generating labels using the Adjusted Rand Index <bold>(ARI,</bold> <xref ref-type="bibr" rid="R69">69</xref>). <list list-type="bullet" id="L1"><list-item><p id="P47"><monospace>Mclust</monospace>, a maximum likelihood implementation of a finite mixture of Gaussian densities (for a range of modelled clusters, <italic>K</italic>),</p></list-item><list-item><p id="P48">10 chains of 1 million iterations, thinning to every thousandth sample for the overfitted Bayesian mixture of Gaussian densities, and</p></list-item><list-item><p id="P49">A variety of consensus clustering ensembles defined by inputs of <italic>W</italic> chains and <italic>D</italic> iterations within each chain (see <xref ref-type="other" rid="BX1">algorithm 1</xref>) with <italic>W</italic> ϵ {1,10, 30, 50,100} and <italic>D</italic> ϵ {1,10,100,1000,10000} where the base learner is an overfitted Bayesian mixture of Gaussian densities.</p></list-item></list>
</p><p id="P50">Note that none of the applied methods include a model selection step and as such there is no modelling of the relevant variables. This and the unknown value of <italic>K</italic> is what separates the models used and the generating model described in <xref ref-type="disp-formula" rid="FD1">equation 1</xref>. More specifically, the likelihood of a point <italic>X<sub>n</sub></italic> for each method is <disp-formula id="FD2"><label>(2)</label><mml:math id="M6"><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mtext>Σ</mml:mtext><mml:mo>,</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mrow><mml:msup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mtext>​</mml:mtext></mml:msup></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:msub><mml:mi>π</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>Σ</mml:mtext><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula></p><p id="P51">where <italic>p</italic>(<italic>X<sub>n</sub></italic>|<italic>μ<sub>k</sub></italic>, <italic>Σ<sub>k</sub></italic>) is the probability density function of the multivariate Gaussian distribution parameterised by a mean vector, <italic>μ<sub>k</sub></italic>, and a covariance matrix, Σ<italic><sub>k</sub></italic>, and <italic>π<sub>k</sub></italic> is the component weight such that <inline-formula><mml:math id="M7"><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> The implementation of the Bayesian mixture model restricts Σ<italic><sub>k</sub></italic> to be a diagonal matrix while <monospace>Mclust</monospace> models a number of different covariance structures. Note that while we use the overfitted Bayesian mixture model, this is purely from convenience and we expect that a true Dirichlet Process mixture or a mixture of mixture models would display similar behaviour in an ensemble.</p><p id="P52">The ARI is a measure of similarity between two partitions, <italic>c</italic><sub>1</sub>, <italic>c</italic><sub>2</sub>, corrected for chance, with 0 indicating <italic>c</italic><sub>1</sub> is no more similar to <italic>c</italic><sub>2</sub> than a random partition would be expected to be and a value of 1 showing that <italic>c</italic><sub>1</sub> and <italic>c</italic><sub>2</sub> perfectly align. Details of the methods in the simulation study can be found in <xref ref-type="supplementary-material" rid="SD1">sections 4.2, 4.3 and 4.4 of the Supplementary Material</xref>.</p><sec id="S6"><title><monospace>Mclust</monospace></title><p id="P53"><monospace>Mclust</monospace> (<xref ref-type="bibr" rid="R70">70</xref>) is a function from the R package <monospace>mclust</monospace>. It estimates Gaussian mixture models for <italic>K</italic> clusters based upon the maximum likelihood estimator of the parameters. It initialises upon a hierarchical clustering of the data cut to <italic>K</italic> clusters. A range of choices of <italic>K</italic> and different covariance structures are compared and the “best” selected using the Bayesian information criterion, (<xref ref-type="bibr" rid="R71">71</xref>) (details in <xref ref-type="supplementary-material" rid="SD1">section 4.2 of the Supplementary Material</xref>).</p></sec><sec id="S7"><title>Bayesian inference</title><p id="P54">To assess within-chain convergence of our Bayesian inference we use the Geweke <italic>Z</italic>-score statistic (<xref ref-type="bibr" rid="R72">72</xref>). Of the chains that appear to behave properly we then asses across-chain convergence using <italic>Ȓ</italic>(<xref ref-type="bibr" rid="R73">73</xref>) and the recent extension provided by (<xref ref-type="bibr" rid="R74">74</xref>). If a chain has reached its stationary distribution the Geweke <italic>Z</italic>-score statistic is expected to be normally distributed. Normality is tested for using a Shapiro-Wilks test (<xref ref-type="bibr" rid="R75">75</xref>). If a chain fails this test (i.e., the associated <italic>p</italic>-value is less than 0.05), we assume that it has not achieved stationarity and it is excluded from the remainder of the analysis. The samples from the remaining chains are then pooled and a posterior similarity matrix (<bold>PSM</bold>) constructed. We use the <monospace>maxpear</monospace> function to infer a point clustering. For more details see <xref ref-type="supplementary-material" rid="SD1">section 4.3 of the Supplementary Material</xref>.</p></sec></sec><sec id="S8"><title>Analysis of the cell cycle in budding yeast</title><sec id="S9"><title>Datasets</title><p id="P55">The cell cycle is crucial to biological growth, repair, reproduction, and development (<xref ref-type="bibr" rid="R76">76</xref>, <xref ref-type="bibr" rid="R77">77</xref>, <xref ref-type="bibr" rid="R78">78</xref>) and is highly conserved among eukaryotes (<xref ref-type="bibr" rid="R78">78</xref>). This means that understanding of the cell cycle of <italic>S. cerevisiae</italic> can provide insight into a variety of cell cycle perturbations including those that occur in human cancer (<xref ref-type="bibr" rid="R79">79</xref>, <xref ref-type="bibr" rid="R77">77</xref>) and ageing (<xref ref-type="bibr" rid="R80">80</xref>). We aim to create clusters of genes that are co-expressed, have common regulatory proteins and share a biological function. To achieve this, we use three datasets that were generated using different ‘omics technologies and target different aspects of the molecular biology underpinning the cell cycle process. <list list-type="bullet" id="L3"><list-item><p id="P56">Microarray profiles of RNA expression from (<xref ref-type="bibr" rid="R81">81</xref>), comprising measurements of cell-cycle-regulated gene expression at 5-minute intervals for 200 minutes (up to three cell division cycles) and is referred to as the <bold>time course</bold> dataset. The cells are synchronised at the START checkpoint in late G1-phase using alpha factor arrest (<xref ref-type="bibr" rid="R81">81</xref>). We include only the genes identified by (<xref ref-type="bibr" rid="R81">81</xref>) as having periodic expression profiles.</p></list-item><list-item><p id="P57">Chromatin immunoprecipitation followed by microarray hybridization <bold>(ChlP-chip)</bold> data from (<xref ref-type="bibr" rid="R82">82</xref>). This dataset discretizes <italic>p</italic>-values from tests of association between 117 DNA-binding transcriptional regulators and a set of yeast genes. Based upon a significance threshold these <italic>p-</italic> values are represented as either a 0 (no interaction) or a 1 (an interaction).</p></list-item><list-item><p id="P58">Protein-protein interaction <bold>(PPI)</bold> data from BioGrid (<xref ref-type="bibr" rid="R83">83</xref>). This database consists of of physical and genetic interactions between gene and gene products, with interactions either observed in high throughput experiments or computationally inferred. The dataset we used contained 603 proteins as columns. An entry of 1 in the (<italic>i</italic>, <italic>j</italic>)<sup><italic>th</italic></sup> cell indicates that the <italic>i<sup>th</sup></italic> gene has a protein product that is believed to interact with the <italic>j<sup>th</sup></italic> protein.</p></list-item></list>
</p><p id="P59">The datasets were reduced to the 551 genes with no missing data in the PPI and ChlP-chip data, as in (<xref ref-type="bibr" rid="R30">30</xref>).</p></sec><sec id="S10"><title>Multiple dataset integration</title><p id="P60">We applied consensus clustering to MDI for our integrative analysis. Details of MDI are in <xref ref-type="supplementary-material" rid="SD1">section 2.2 of the Supplementary Material</xref>, but in short MDI jointly models the clustering in each dataset, inferring individual clusterings for each dataset. These partitions are informed by similar structure in the other datasets, with MDI learning this similarity as it models the partitions. The model does not assume global structure. This means that the similarity between datasets is not strongly assumed in our model; individual clusters or genes that align across datasets are based solely upon the evidence present in the data and not due to strong modelling assumptions. Thus, datasets that share less common information can be included without fearing that this will warp the final clusterings in some way.</p><p id="P61">The datasets were modelled using a mixture of Gaussian processes in the time course dataset and Multinomial distributions in the ChlP-chip and PPI datasets.</p></sec></sec><sec id="S11" sec-type="results"><title>Results</title><sec id="S12"><title>Simulated data</title><p id="P62">We use the ARI between the generating labels and the inferred clustering of each method to be our metric of predictive performance. In <xref ref-type="fig" rid="F2">figure 2</xref>, we see <monospace>Mclust</monospace> performs very well in the 2D and Small <italic>N</italic>, large <italic>P</italic> scenarios, correctly identifying the true structure. However, the irrelevant features scenario sees a collapse in performance, <monospace>Mclust</monospace> is blinded by the irrelevant features and identifies a clustering of <italic>K</italic> = 1.</p><p id="P63">The pooled samples from multiple long chains performs very well across all scenarios and appears to act as an upper bound on the more practical implementations of consensus clustering.</p><p id="P64">Consensus clustering does uncover some of the generating structure in the data, even using a small number of short chains. With sufficiently large ensembles and chain depth, consensus clustering is close to the pooled Bayesian samples in predictive performance. It appears that for a constant chain depth increasing the ensemble width used follows a pattern of diminishing returns. There are strong initial gains for a greater ensemble width, but the improvement decreases for each successive chain. A similar pattern emerges in increasing chain length for a constant number of chains (<xref ref-type="fig" rid="F2">figure 2</xref>).</p><p id="P65">For the PSMs from the individual chains, all entries are 0 or 1 (<xref ref-type="fig" rid="F3">figure 3</xref>). This means only a single clustering is sampled within each chain, implying very little uncertainty in the partition. However, three different clustering solutions emerge across the chains, indicating that each individual chain is failing to explore the full support of the posterior distribution of the clustering. In general, while MCMC convergence theorems hold as the number of iterations tend to infinity, any finite chain might suffer in representing the full support of the posterior distribution, as we observe here. Moreover, the mixing of each chain can be poor as well (i.e. it may take a long time to reach the stationary distribution from an arbitrary initialisation). In our empirical study, we find that using many short runs provide similar point and interval estimates to running a small number of long chains (<xref ref-type="fig" rid="F3">figure 3</xref>), while being computationally less expensive (<xref ref-type="fig" rid="F4">figure 4</xref>), and hence more convenient for our applications.</p><p id="P66"><xref ref-type="fig" rid="F4">Figure 4</xref> shows that chain length is directly proportional to the time taken for the chain to run. This means that using an ensemble of shorter chains, as in consensus clustering, can offer large reductions in the time cost of analysis when a parallel environment is available compared to standard Bayesian inference. Even on a laptop of 8 cores running an ensemble of 1,000 chains of length 1,000 will require approximately half as much time as running 10 chains of length 100,000 due to parallelisation, and the potential benefits are far greater when using a large computing cluster.</p><p id="P67">Additional results for these and other simulations are in <xref ref-type="supplementary-material" rid="SD1">section 4.4 of the Supplementary Material</xref>.</p></sec><sec id="S13"><title>Multi-omics analysis of the cell cycle in budding yeast</title><p id="P68">We use the stopping rule proposed in to determine our ensemble depth and width. In <xref ref-type="fig" rid="F5">figure 5</xref>, we see that the change in the consensus matrices from increasing the ensemble depth and width is diminishing in keeping with results in the simulations. We see no strong improvement after <italic>D</italic> = 6, 000 and increasing the number of learners from 500 to 1,000 has small effect. We therefore use the largest ensemble available, a depth <italic>D</italic> = 10001 and width <italic>W</italic> = 1000, believing this ensemble is stable (additional evidence in <xref ref-type="supplementary-material" rid="SD1">section 5.1 of the Supplementary Material</xref>).</p><p id="P69">We focus upon the genes that tend to have the same cluster label across multiple datasets. More formally, we analyse the clustering structure among genes for which <inline-formula><mml:math id="M8"><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn></mml:math></inline-formula>, where <italic>c<sub>nl</sub></italic> denotes the cluster label of gene <italic>n</italic> in dataset <italic>l</italic>. In our analysis it is the signal shared across the time course and ChlP-chip datasets that is strongest, with 261 genes (nearly half of the genes present) in this pairing tending to have a common label, whereas only 56 genes have a common label across all three datasets. Thus, we focus upon this pairing of datasets in the results of the analysis performed using all three datasets. We show the gene expression and regulatory proteins of these genes separated by their cluster in <xref ref-type="fig" rid="F6">figure 6</xref>. In <xref ref-type="fig" rid="F6">figure 6</xref>, the clusters in the time series data have tight, unique signatures (having different periods, amplitudes, or both) and in the ChlP-chip data clusters are defined by a small number of well-studied transcription factors <bold>(TFs)</bold>(see <xref ref-type="supplementary-material" rid="SD1">table 2 of the Supplementary Material</xref> for details of these TFs, many of which are well known to regulate cell cycle expression, 84).</p><p id="P70">As an example, we briefly analyse clusters 9 and 16 in greater depth. Cluster 9 has strong association with MBP1 and some interactions with SWI6, as can be seen in <xref ref-type="fig" rid="F6">figure 6</xref>. The Mbpl-Swi6p complex, MBF, is associated with DNA replication (<xref ref-type="bibr" rid="R85">85</xref>). The first time point, 0 minutes, in the time course data is at the START checkpoint, or the G1/S transition. The members of cluster 9 begin highly expressed at this point before quickly dropping in expression (in the first of the 3 cell cycles). This suggests that many transcripts are produced immediately in advance of S-phase, and thus are required for the first stages of DNA synthesis. These genes’ descriptions (found using <monospace>org.Sc.sgd.db</monospace>, <xref ref-type="bibr" rid="R86">86</xref>, and shown in <xref ref-type="supplementary-material" rid="SD1">table 3 of the Supplementary Material</xref>) support this hypothesis, as many of the members are associated with DNA replication, repair and/or recombination. Additionally, <italic>TOF1, MRC1</italic> and <italic>RAD53</italic>, members of the replication checkpoint (<xref ref-type="bibr" rid="R87">87</xref>, <xref ref-type="bibr" rid="R88">88</xref>) emerge in the cluster as do members of the cohesin complex. Cohesin is associated with sister chromatid cohesion which is established during the S-phase of the cell cycle (<xref ref-type="bibr" rid="R89">89</xref>) and also contributes to transcription regulation, DNA repair, chromosome condensation, homolog pairing (<xref ref-type="bibr" rid="R90">90</xref>), fitting the theme of cluster 9.</p><p id="P71">Cluster 16 appears to be a cluster of S-phase genes, consisting of <italic>GAS3, NRM1</italic> and <italic>PDS1</italic> and the genes encoding the histones H1, H2A, H2B, H3 and H4. Histones are the chief protein components of chromatin (<xref ref-type="bibr" rid="R91">91</xref>) and are important contributors to gene regulation (<xref ref-type="bibr" rid="R92">92</xref>). They are known to peak in expression in S-phase (<xref ref-type="bibr" rid="R81">81</xref>), which matches the first peak of this cluster early in the time series. Of the other members, <italic>NRM1</italic> is a transcriptional co-repressor of MBF-regulated gene expression acting at the transition from G1 to S-phase (<xref ref-type="bibr" rid="R93">93</xref>, <xref ref-type="bibr" rid="R94">94</xref>). Pds1p binds to and inhibits the Esp1 class of sister separating proteins, preventing sister chromatids separation before M-phase (<xref ref-type="bibr" rid="R95">95</xref>, <xref ref-type="bibr" rid="R89">89</xref>). <italic>GAS3</italic>, is not well studied. It interacts with <italic>SMT3</italic> which regulates chromatid cohesion, chromosome segregation and DNA replication (among other things). Chromatid cohesion ensures the faithful segregation of chromosomes in mitosis and in both meiotic divisions (<xref ref-type="bibr" rid="R96">96</xref>) and is instantiated in S-phase (<xref ref-type="bibr" rid="R89">89</xref>). These results, along with the very similar expression profile to the histone genes in the time course data, suggest that <italic>GAS3</italic> may be more directly involved in DNA replication or chromatid cohesion than is currently believed.</p><p id="P72">We attempt to perform a similar analysis using traditional Bayesian inference of MDI, but after 36 hours of runtime there is no consistency or convergence across chains. We use the Geweke statistic and <italic>Ȓ</italic> to reduce to the five best behaved chains (none of which appear to be converged, see <xref ref-type="supplementary-material" rid="SD1">section 5.2 of the Supplementary Material</xref> for details). If we then compare the distribution of sampled values for the <italic>ø</italic> parameters for these long chains, the final ensemble used (D = 10001, W = 1000) and the pooled samples from the 5 long chains, then we see that the distribution of the pooled samples from the long chains (which might be believed to sampling different parts of the posterior distribution) is closer in appearance to the distributions sampled by the consensus clustering than to any single chain (<xref ref-type="fig" rid="F7">figure 7</xref>). Further disagreement between chains is shown in the Gene Ontology term over-representation analysis in <xref ref-type="supplementary-material" rid="SD1">section 5.3 of the Supplementary Material</xref>.</p></sec></sec><sec id="S14" sec-type="discussion"><title>Discussion</title><p id="P73">Our proposed method has demonstrated good performance on simulation studies, uncovering the generating structure in many cases and performing comparably to <monospace>Mclust</monospace> and long chains in many scenarios. We saw that when the chains are sufficiently deep that the ensemble approximates Bayesian inference, as shown by the similarity between the PSMs and the CM in the 2D scenario where the individual chains do not become trapped in a single mode. We have shown cases where many short runs are computationally less expensive than one long chain and give meaningful point and interval estimates; estimates that are very similar to those from the limiting case of a Markov chain. Thus if individual chains are suffering from mixing problems or are too computationally expensive to run, consensus clustering may provide a viable option. We also showed that the ensemble of short chains is more robust to irrelevant features than <monospace>Mclust</monospace>.</p><p id="P74">We proposed a method of assessing ensemble stability and deciding upon ensemble size which we used when performing an integrative analysis of yeast cell cycle data using MDI, an extension of Bayesian mixture models that jointly models multiple datasets. We uncovered many genes with shared signal across several datasets and explored the meaning of some of the inferred clusters using data external to the analysis. We found biologically meaningful results as well as signal for possibly novel biology. We also showed that individual chains for the existing implementation of MDI do not converge in a practical length of time, having run 10 chains for 36 hours with no consistent behaviour across chains. This means that Bayesian inference of the MDI model is not practical on this dataset with the software currently available.</p><p id="P75">However, consensus clustering does lose the theoretical framework of true Bayesian inference. We attempt to mitigate this with our assessment of stability in the ensemble, but this diagnosis is heuristic and subjective, and while there is empirical evidence for its success, it lacks the formal results for the tests of model convergence for Bayesian inference.</p><p id="P76">More generally, we have benchmarked the use of an ensemble of Bayesian mixture models, showing that this approach can infer meaningful clusterings and overcomes the problem of multi-modality in the likelihood surface even in high dimensions, thereby providing more stable clusterings than individual long chains that are prone to becoming trapped in individual modes. We also show that the ensemble can be significantly quicker to run. In our multi-omics study we have demonstrated that the method can be applied as a wrapper to more complex Bayesian clustering methods using existing implementations and that this provides meaningful results even when individual chains fail to converge. This enables greater application of complex Bayesian clustering methods without requiring re-implementation using more clever MCMC methods, a process that would involve a significant investment of human time.</p><p id="P77">We expect that researchers interested in applying some of the Bayesian integrative clustering models such as MDI and Clusternomics (<xref ref-type="bibr" rid="R32">32</xref>) will be enabled to do so, as consensus clustering overcomes some of the unwieldiness of existing implementations of these complex models. More generally, we expect that our method will be useful to researchers performing cluster analysis of high-dimensional data where the runtime of MCMC methods becomes too onerous and multi-modality is more likely to be present.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Material</label><media xlink:href="EMS144881-supplement-Supplementary_Material.pdf" mimetype="application" mime-subtype="pdf" id="d2aAeGbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S15"><title>Funding</title><p>This work was funded by the MRC (MC UU 00002/4, MC UU 00002/13) and supported by the NIHR Cambridge Biomedical Research Centre (BRC-1215-20014). The views expressed are those of the author(s) and not necessarily those of the NHS, the NIHR or the Department of Health and Social Care. This research was funded in whole, or in part, by the Wellcome Trust [WT107881], For the purpose of Open Access, the author has applied a CC BY public copyright licence to any Author Accepted Manuscript version arising from this submission.</p></ack><sec id="S16" sec-type="data-availability"><title>Availability of data and materials</title><p id="P78">The code and datasets supporting the conclusions of this article are available in the github repository, <ext-link ext-link-type="uri" xlink:href="https://github.com/stcolema/ConsensusClusteringForBayesianMixtureModels">https://github.com/stcolema/ConsensusClusteringForBayesianMixtureModels</ext-link></p></sec><glossary><title>Abbreviations</title><def-list><def-item><term>ARI</term><def><p>Adjusted Rand Index</p></def></def-item><def-item><term>ChlP-chip</term><def><p>Chromatin immunoprecipitation followed by microarray hybridization</p></def></def-item><def-item><term>CM</term><def><p>Consensus Matrix</p></def></def-item><def-item><term>MCMC</term><def><p>Markov chain Monte Carlo</p></def></def-item><def-item><term>MDI</term><def><p>Multiple Dataset Integration</p></def></def-item><def-item><term>PCA</term><def><p>Principal Component Analysis</p></def></def-item><def-item><term>PPI</term><def><p>Protein-Protein Interaction</p></def></def-item><def-item><term>PSM</term><def><p>Posterior Similarity Matrix</p></def></def-item><def-item><term>SSE</term><def><p>Sum of Squared Errors</p></def></def-item><def-item><term>TF</term><def><p>Transcription Factor</p></def></def-item></def-list></glossary><fn-group><fn id="FN2" fn-type="conflict"><p id="P79"><bold>Competing interests</bold></p><p id="P80">The authors declare that they have no competing interests.</p></fn><fn fn-type="con" id="FN3"><p id="P81"><bold>Authors’ contributions</bold></p><p id="P82">SC designed the simulation study with contributions from PK and CW, performed the analyses and wrote the manuscript. PK and CW provided an equal contribution of joint supervision, directing the research and provided suggestions such as the stopping rule. All contributed to interpreting the results of the analyses. All authors revised and approved the final manuscript.</p></fn></fn-group><ref-list><ref id="R1"><label>[1]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hejblum</surname><given-names>BP</given-names></name><name><surname>Skinner</surname><given-names>J</given-names></name><name><surname>Thiébaut</surname><given-names>R</given-names></name></person-group><article-title>Time-course gene set analysis for longitudinal gene expression data</article-title><source>PLoS computational biology</source><year>2015</year><volume>1l</volume><issue>6</issue><elocation-id>el004310</elocation-id></element-citation></ref><ref id="R2"><label>[2]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bai</surname><given-names>JP</given-names></name><name><surname>Alekseyenko</surname><given-names>AV</given-names></name><name><surname>Statnikov</surname><given-names>A</given-names></name><name><surname>Wang</surname><given-names>IM</given-names></name><name><surname>Wong</surname><given-names>PH</given-names></name></person-group><article-title>Strategic applications of gene expression: from drug discovery/development to bedside</article-title><source>The AAPS journal</source><year>2013</year><volume>15</volume><issue>2</issue><fpage>427</fpage><lpage>437</lpage></element-citation></ref><ref id="R3"><label>[3]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Emmert-Streib</surname><given-names>F</given-names></name><name><surname>Dehmer</surname><given-names>M</given-names></name><name><surname>Haibe-Kains</surname><given-names>B</given-names></name></person-group><article-title>Gene regulatory networks and their applications: understanding biological and medical problems in terms of networks</article-title><source>Frontiers in cell and developmental ι biology</source><year>2014</year><volume>38</volume></element-citation></ref><ref id="R4"><label>[4]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Floyd</surname><given-names>S</given-names></name></person-group><article-title>Feast squares quantization in PCM</article-title><source>IEEE transactions on information theory</source><year>1982</year><volume>8</volume><issue>2</issue><fpage>129</fpage><lpage>137</lpage></element-citation></ref><ref id="R5"><label>[5]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Forgy</surname><given-names>EW</given-names></name></person-group><article-title>Cluster analysis of multivariate data: efficiency versus interpretability of classifications</article-title><source>biometrics</source><year>1965</year><volume>21</volume><fpage>768</fpage><lpage>769</lpage></element-citation></ref><ref id="R6"><label>[6]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rousseeuw</surname><given-names>PJ</given-names></name></person-group><article-title>Silhouettes: A Graphical Aid to the Interpretation and Validation of Cluster Analysis</article-title><source>Journal of Computational and Applied Mathematics</source><year>1987</year><month>Nov</month><volume>20</volume><fpage>53</fpage><lpage>65</lpage></element-citation></ref><ref id="R7"><label>[7]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Arthur</surname><given-names>D</given-names></name><name><surname>Vassilvitskii</surname><given-names>S</given-names></name></person-group><source>k-means++: The advantages of careful seeding</source><publisher-name>Stanford</publisher-name><year>2006</year></element-citation></ref><ref id="R8"><label>[8]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Breiman</surname><given-names>E</given-names></name></person-group><article-title>Random forests</article-title><source>Machine learning</source><year>2001</year><volume>45</volume><issue>1</issue><fpage>5</fpage><lpage>32</lpage></element-citation></ref><ref id="R9"><label>[9]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friedman</surname><given-names>JH</given-names></name></person-group><article-title>Stochastic gradient boosting</article-title><source>Computational statistics &amp; data analysis</source><year>2002</year><volume>8</volume><issue>4</issue><fpage>367</fpage><lpage>378</lpage></element-citation></ref><ref id="R10"><label>[10]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Monti</surname><given-names>S</given-names></name><name><surname>Tamayo</surname><given-names>P</given-names></name><name><surname>Mesirov</surname><given-names>J</given-names></name><name><surname>Golub</surname><given-names>T</given-names></name></person-group><article-title>Consensus clustering: a resampling-based method for class : discovery and visualization of gene expression microarray data</article-title><source>Machine learning</source><year>2003</year><volume>2</volume><issue>1-2</issue><fpage>91</fpage><lpage>118</lpage></element-citation></ref><ref id="R11"><label>[11]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilkerson</surname><given-names>DM</given-names></name><name><surname>Hayes</surname><given-names>Neil D</given-names></name></person-group><article-title>Consensus ClusterPlus: a class discovery tool with confidence assessments and item tracking</article-title><source>Bioinformatics</source><year>2010</year><volume>6</volume><issue>12</issue><fpage>1572</fpage><lpage>1573</lpage></element-citation></ref><ref id="R12"><label>[12]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>John</surname><given-names>CR</given-names></name><name><surname>Watson</surname><given-names>D</given-names></name><name><surname>Russ</surname><given-names>D</given-names></name><name><surname>Goldmann</surname><given-names>K</given-names></name><name><surname>Ehrenstein</surname><given-names>M</given-names></name><name><surname>Pitzalis</surname><given-names>C</given-names></name><etal/></person-group><article-title>M3C: Monte Carlo reference-based consensus clustering</article-title><source>Scientific reports</source><year>2020</year><volume>10</volume><issue>1</issue><fpage>1</fpage><lpage>14</lpage></element-citation></ref><ref id="R13"><label>[13]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gu</surname><given-names>Z</given-names></name><name><surname>Schlesner</surname><given-names>M</given-names></name><name><surname>Hubschmann</surname><given-names>D</given-names></name></person-group><article-title>cola: an R/Bioconductor package for consensus partitioning through a general framework</article-title><source>Nucleic Acids Research</source><year>2020</year><volume>2</volume><elocation-id>Gkaal 146</elocation-id><pub-id pub-id-type="doi">10.1093/nar/gkaall46</pub-id></element-citation></ref><ref id="R14"><label>[14]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lehmann</surname><given-names>BD</given-names></name><name><surname>Bauer</surname><given-names>JA</given-names></name><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Sanders</surname><given-names>ME</given-names></name><name><surname>Chakravarthy</surname><given-names>AB</given-names></name><name><surname>Shyr</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Identification of human triple-negative breast cancer subtypes and preclinical models for selection of targeted therapies</article-title><source>The Journal of clinical investigation</source><year>2011</year><volume>121</volume><issue>7</issue><fpage>2750</fpage><lpage>2767</lpage></element-citation></ref><ref id="R15"><label>[15]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verhaak</surname><given-names>RG</given-names></name><name><surname>Hoadley</surname><given-names>KA</given-names></name><name><surname>Purdom</surname><given-names>E</given-names></name><name><surname>Wang</surname><given-names>V</given-names></name><name><surname>Qi</surname><given-names>Y</given-names></name><name><surname>Wilkerson</surname><given-names>MD</given-names></name><etal/></person-group><article-title>Integrated genomic analysis identifies clinically relevant subtypes of glioblastoma characterized by abnormalities in PDGFRA, IDH1, EGFR, and NFL</article-title><source>Cancer cell</source><year>2010</year><volume>17</volume><issue>1</issue><fpage>98</fpage><lpage>110</lpage></element-citation></ref><ref id="R16"><label>[16]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiselev</surname><given-names>VY</given-names></name><name><surname>Kirschner</surname><given-names>K</given-names></name><name><surname>Schaub</surname><given-names>MT</given-names></name><name><surname>Andrews</surname><given-names>T</given-names></name><name><surname>Yiu</surname><given-names>A</given-names></name><name><surname>Chandra</surname><given-names>T</given-names></name><etal/></person-group><article-title>SC3: consensus clustering of single-cell RNA-seq data</article-title><source>Nature methods</source><year>2017</year><volume>14</volume><issue>5</issue><fpage>483</fpage><lpage>486</lpage></element-citation></ref><ref id="R17"><label>[17]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Li</surname><given-names>T</given-names></name><name><surname>Ding</surname><given-names>C</given-names></name></person-group><source>Weighted Consensus Clustering</source><conf-name>Proceedings of the 2008 SIAM International Conference on Data Mining</conf-name><publisher-name>Society for Industrial and Applied Mathematics</publisher-name><year>2008</year><fpage>798</fpage><lpage>809</lpage></element-citation></ref><ref id="R18"><label>[18]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carpineto</surname><given-names>C</given-names></name><name><surname>Romano</surname><given-names>G</given-names></name></person-group><article-title>Consensus Clustering Based on a New Probabilistic Rand Index with Application to Subtopic Retrieval</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><year>2012</year><volume>34</volume><issue>12</issue><fpage>2315</fpage><lpage>2326</lpage></element-citation></ref><ref id="R19"><label>[19]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strehl</surname><given-names>A</given-names></name><name><surname>Ghosh</surname><given-names>J</given-names></name></person-group><article-title>Cluster Ensembles - A Knowledge Reuse Framework for Combining Multiple Partitions</article-title><source>Journal of Machine Learning Research</source><year>2002</year><fpage>583</fpage><lpage>617</lpage></element-citation></ref><ref id="R20"><label>[20]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghaemi</surname><given-names>R</given-names></name><name><surname>Sulaiman</surname><given-names>MN</given-names></name><name><surname>Ibrahim</surname><given-names>H</given-names></name><name><surname>Mustapha</surname><given-names>N</given-names></name><etal/></person-group><article-title>A survey: clustering ensembles techniques</article-title><source>World Academy of Science, Engineering and Technology</source><year>2009</year><volume>50</volume><fpage>636</fpage><lpage>645</lpage></element-citation></ref><ref id="R21"><label>[21]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Unlii</surname><given-names>R</given-names></name><name><surname>Xanthopoulos</surname><given-names>P</given-names></name></person-group><article-title>Estimating the Number of Clusters in a Dataset via Consensus Clustering</article-title><source>Expert Systems with Applications</source><year>2019</year><month>Jul</month><volume>125</volume><fpage>33</fpage><lpage>39</lpage></element-citation></ref><ref id="R22"><label>[22]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fraley</surname><given-names>C</given-names></name><name><surname>Raftery</surname><given-names>AE</given-names></name></person-group><article-title>Model-based clustering, discriminant analysis, and density estimation</article-title><source>Journal of the American statistical Association</source><year>2002</year><volume>7</volume><issue>45-8</issue><fpage>611</fpage><lpage>631</lpage></element-citation></ref><ref id="R23"><label>[23]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fraley</surname><given-names>C</given-names></name></person-group><article-title>How Many Clusters? Which Clustering Method? Answers Via Model-Based Cluster Analysis</article-title><source>The Computer Journal</source><year>1998</year><month>Aug</month><volume>41</volume><issue>8</issue><fpage>578</fpage><lpage>588</lpage></element-citation></ref><ref id="R24"><label>[24]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Antoniak</surname><given-names>CE</given-names></name></person-group><article-title>Mixtures of Dirichlet Processes with Applications to Bayesian Nonparametric Problems</article-title><source>The Annals of Statistics</source><year>1974</year><month>Nov</month><volume>2</volume><issue>6</issue><fpage>1152</fpage><lpage>1174</lpage></element-citation></ref><ref id="R25"><label>[25]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ferguson</surname><given-names>TS</given-names></name></person-group><chapter-title>Bayesian Density Estimation by Mixtures of Normal Distributions</chapter-title><person-group person-group-type="editor"><name><surname>Rizvi</surname><given-names>MH</given-names></name><name><surname>Rustagi</surname><given-names>JS</given-names></name><name><surname>Siegmund</surname><given-names>D</given-names></name></person-group><source>Recent Advances in Statistics</source><publisher-name>Academic Press</publisher-name><year>1983</year><fpage>287</fpage><lpage>302</lpage></element-citation></ref><ref id="R26"><label>[26]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lo</surname><given-names>AY</given-names></name></person-group><article-title>On a Class of Bayesian Nonparametric Estimates: I. Density Estimates</article-title><source>The Annals of Statistics</source><year>1984</year><volume>12</volume><issue>1</issue><fpage>351</fpage><lpage>357</lpage></element-citation></ref><ref id="R27"><label>[27]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richardson</surname><given-names>S</given-names></name><name><surname>Green</surname><given-names>PJ</given-names></name></person-group><article-title>On Bayesian analysis of mixtures with an unknown number of components</article-title><source>Journal of the Royal Statistical Society: series B</source><year>1997</year><volume>59</volume><issue>4</issue><fpage>731</fpage><lpage>792</lpage></element-citation></ref><ref id="R28"><label>[28]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>JW</given-names></name><name><surname>Harrison</surname><given-names>MT</given-names></name></person-group><article-title>Mixture models with a prior on the number of components</article-title><source>Journal of the American Statistical Association</source><year>2018</year><volume>113</volume><issue>521</issue><fpage>340</fpage><lpage>356</lpage></element-citation></ref><ref id="R29"><label>[29]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rousseau</surname><given-names>J</given-names></name><name><surname>Mengersen</surname><given-names>K</given-names></name></person-group><article-title>Asymptotic behaviour of the posterior distribution in overfitted mixture models</article-title><source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source><year>2011</year><volume>73</volume><issue>5</issue><fpage>689</fpage><lpage>710</lpage></element-citation></ref><ref id="R30"><label>[30]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kirk</surname><given-names>P</given-names></name><name><surname>Griffin</surname><given-names>JE</given-names></name><name><surname>Savage</surname><given-names>RS</given-names></name><name><surname>Ghahramani</surname><given-names>Z</given-names></name><name><surname>Wild</surname><given-names>DL</given-names></name></person-group><article-title>Bayesian correlated clustering to integrate multiple datasets</article-title><source>Bioinformatics</source><year>2012</year><volume>28</volume><issue>24</issue><fpage>3290</fpage><lpage>3297</lpage></element-citation></ref><ref id="R31"><label>[31]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lock</surname><given-names>EF</given-names></name><name><surname>Dunson</surname><given-names>DB</given-names></name></person-group><article-title>Bayesian consensus clustering</article-title><source>Bioinformatics</source><year>2013</year><month>8</month><volume>29</volume><issue>20</issue><fpage>2610</fpage><lpage>2616</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btt425</pub-id></element-citation></ref><ref id="R32"><label>[32]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gabasova</surname><given-names>E</given-names></name><name><surname>Reid</surname><given-names>J</given-names></name><name><surname>Wemisch</surname><given-names>L</given-names></name></person-group><article-title>Clustemomics: Integrative context-dependent clustering for heteroge¬neous datasets</article-title><source>PLoS computational biology</source><year>2017</year><volume>13</volume><issue>10</issue><elocation-id>el005781</elocation-id></element-citation></ref><ref id="R33"><label>[33]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Medvedovic</surname><given-names>M</given-names></name><name><surname>Sivaganesan</surname><given-names>S</given-names></name></person-group><article-title>Bayesian infinite mixture model based clustering of gene expression profiles</article-title><source>Bioinformatics</source><year>2002</year><volume>18</volume><issue>9</issue><fpage>1194</fpage><lpage>1206</lpage></element-citation></ref><ref id="R34"><label>[34]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chan</surname><given-names>C</given-names></name><name><surname>Feng</surname><given-names>F</given-names></name><name><surname>Ottinger</surname><given-names>J</given-names></name><name><surname>Foster</surname><given-names>D</given-names></name><name><surname>West</surname><given-names>M</given-names></name><name><surname>Kepler</surname><given-names>TB</given-names></name></person-group><article-title>Statistical mixture modeling for cell subtype identification in flow cytometry</article-title><source>Cytometry Part A: The Journal of the International Society for Analytical Cytology</source><year>2008</year><volume>73</volume><issue>8</issue><fpage>693</fpage><lpage>701</lpage></element-citation></ref><ref id="R35"><label>[35]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hejblum</surname><given-names>BP</given-names></name><name><surname>Alkhassim</surname><given-names>C</given-names></name><name><surname>Gottardo</surname><given-names>R</given-names></name><name><surname>Caron</surname><given-names>F</given-names></name><name><surname>Thiebaut</surname><given-names>R</given-names></name><etal/></person-group><article-title>Sequential Dirichlet process mix lures of multivariate skew (-distributions for model-based clustering of flow cytometry data</article-title><source>The Annals of Applied Statistics</source><year>2019</year><volume>13</volume><issue>1</issue><fpage>638</fpage><lpage>660</lpage></element-citation></ref><ref id="R36"><label>[36]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prabhakaran</surname><given-names>S</given-names></name><name><surname>Azizi</surname><given-names>E</given-names></name><name><surname>Carr</surname><given-names>A</given-names></name><name><surname>Pe’er</surname><given-names>D</given-names></name></person-group><article-title>Dirichlet process mixture model for correcting technical variation in single-cell gene expression data</article-title><source>International Conference on Machine Learning</source><year>2016</year><fpage>110</fpage><lpage>1079</lpage></element-citation></ref><ref id="R37"><label>[37]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crook</surname><given-names>OM</given-names></name><name><surname>Mulvey</surname><given-names>CM</given-names></name><name><surname>Kirk</surname><given-names>PD</given-names></name><name><surname>Lilley</surname><given-names>KS</given-names></name><name><surname>Gatto</surname><given-names>L</given-names></name></person-group><article-title>A Bayesian mixture modelling approach for spatial proteomics</article-title><source>PLoS computational biology</source><year>2018</year><volume>14</volume><issue>1l</issue><elocation-id>el006516</elocation-id></element-citation></ref><ref id="R38"><label>[38]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname><given-names>GM</given-names></name><name><surname>Frazier</surname><given-names>DT</given-names></name><name><surname>Robert</surname><given-names>CP</given-names></name></person-group><article-title>Computing Bayes: Bayesian Computation from 1763 to the 21st Century</article-title><source>arXiv preprint</source><year>2020</year><elocation-id>arXiv:200406425</elocation-id></element-citation></ref><ref id="R39"><label>[39]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strauss</surname><given-names>ME</given-names></name><name><surname>Kirk</surname><given-names>PD</given-names></name><name><surname>Reid</surname><given-names>IE</given-names></name><name><surname>Wemisch</surname><given-names>L</given-names></name></person-group><article-title>GPseudoClust: deconvolution of shared pseudo-profiles at single-cell resolution</article-title><source>Bioinformatics</source><year>2020</year><volume>36</volume><issue>5</issue><fpage>1484</fpage><lpage>1491</lpage></element-citation></ref><ref id="R40"><label>[40]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scott</surname><given-names>SL</given-names></name><name><surname>Blocker</surname><given-names>AW</given-names></name><name><surname>Bonassi</surname><given-names>FV</given-names></name><name><surname>Chipman</surname><given-names>HA</given-names></name><name><surname>George</surname><given-names>El</given-names></name><name><surname>McCulloch</surname><given-names>RE</given-names></name></person-group><article-title>Bayes and big data: the consensus Monte Carlo algorithm</article-title><source>International Journal of Management Science and Engineering Management</source><year>2016</year><volume>11</volume><issue>2</issue><fpage>78</fpage><lpage>88</lpage><pub-id pub-id-type="doi">10.1080/17509653.2016.1142191</pub-id></element-citation></ref><ref id="R41"><label>[41]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ni</surname><given-names>Y</given-names></name><name><surname>Muller</surname><given-names>P</given-names></name><name><surname>Diesendruck</surname><given-names>M</given-names></name><name><surname>Williamson</surname><given-names>S</given-names></name><name><surname>Zhu</surname><given-names>Y</given-names></name><name><surname>Ji</surname><given-names>Y</given-names></name></person-group><article-title>Scalable Bayesian Nonparametric Clustering and Classification</article-title><source>Journal of Computational and Graphical Statistics</source><year>2020</year><volume>9</volume><issue>l</issue><fpage>53</fpage><lpage>65</lpage><pub-id pub-id-type="pmid">32982129</pub-id><pub-id pub-id-type="doi">10.1080/10618600.2019.1624366</pub-id></element-citation></ref><ref id="R42"><label>[42]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ni</surname><given-names>Y</given-names></name><name><surname>Ji</surname><given-names>Y</given-names></name><name><surname>Muller</surname><given-names>P</given-names></name></person-group><article-title>Consensus Monte Carlo for Random Subsets Using Shared Anchors</article-title><source>Journal of Computational and Graphical Statistics</source><year>2020</year><volume>9</volume><issue>4</issue><fpage>703</fpage><lpage>714</lpage><pub-id pub-id-type="doi">10.1080/10618600.2020.1737085</pub-id></element-citation></ref><ref id="R43"><label>[43]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Welling</surname><given-names>M</given-names></name><name><surname>Teh</surname><given-names>YW</given-names></name></person-group><source>Bayesian Learning via Stochastic Gradient Langevin Dynamics</source><conf-name>Proceedings of the 28th International Conference on International Conference on Machine Learning. ICML’11</conf-name><publisher-loc>Madison, WI, USA</publisher-loc><publisher-name>Omnipress</publisher-name><year>2011</year><fpage>681</fpage><lpage>688</lpage></element-citation></ref><ref id="R44"><label>[44]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teh</surname><given-names>YW</given-names></name><name><surname>Thiery</surname><given-names>AH</given-names></name><name><surname>Vollmer</surname><given-names>SJ</given-names></name></person-group><article-title>Consistency and Fluctuations for Stochastic Gradient Langevin Dynamics</article-title><source>J Mach Learn Res</source><year>2016</year><month>Jan</month><volume>17</volume><issue>1</issue><fpage>193</fpage><lpage>225</lpage></element-citation></ref><ref id="R45"><label>[45]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johndrow</surname><given-names>JE</given-names></name><name><surname>Pillai</surname><given-names>NS</given-names></name><name><surname>Smith</surname><given-names>A</given-names></name></person-group><article-title>No Free Lunch for Approximate MCMC</article-title><source>arXiv</source><year>2020</year><comment><ext-link ext-link-type="uri" xlink:href="arxiv.org/abs/2010.12514">arxiv.org/abs/2010.12514</ext-link></comment></element-citation></ref><ref id="R46"><label>[46]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nemeth</surname><given-names>C</given-names></name><name><surname>Fearnhead</surname><given-names>P</given-names></name></person-group><article-title>Stochastic Gradient Markov Chain Monte Carlo</article-title><source>Journal of the American Statistical Association</source><year>2021</year><volume>116</volume><issue>533</issue><fpage>433</fpage><lpage>450</lpage><pub-id pub-id-type="doi">10.1080/01621459.2020.1847120</pub-id></element-citation></ref><ref id="R47"><label>[47]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacob</surname><given-names>PE</given-names></name><name><surname>O’Leary</surname><given-names>J</given-names></name><name><surname>Atchade</surname><given-names>YF</given-names></name></person-group><article-title>Unbiased Markov Chain Monte Carlo Methods with Couplings</article-title><source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source><year>2020</year><volume>82</volume><issue>3</issue><fpage>543</fpage><lpage>600</lpage></element-citation></ref><ref id="R48"><label>[48]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robert</surname><given-names>CP</given-names></name><name><surname>Elvira</surname><given-names>V</given-names></name><name><surname>Tawn</surname><given-names>N</given-names></name><name><surname>Wu</surname><given-names>C</given-names></name></person-group><article-title>Accelerating MCMC algorithms</article-title><source>Wiley Interdisciplinary Reviews: Computational Statistics</source><year>2018</year><volume>10</volume><issue>5</issue><elocation-id>el435</elocation-id></element-citation></ref><ref id="R49"><label>[49]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jain</surname><given-names>S</given-names></name><name><surname>Neal</surname><given-names>RM</given-names></name></person-group><article-title>A Split-Merge Markov chain Monte Carlo Procedure for the Dirichlet Process Mixture Model</article-title><source>Journal of Computational and Graphical Statistics</source><year>2004</year><volume>13</volume><issue>1</issue><fpage>158</fpage><lpage>182</lpage><pub-id pub-id-type="doi">10.1198/1061860043001</pub-id></element-citation></ref><ref id="R50"><label>[50]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jain</surname><given-names>S</given-names></name><name><surname>Neal</surname><given-names>RM</given-names></name></person-group><article-title>Splitting and merging components of a nonconjugate Dirichlet process mixlure model</article-title><source>Bayesian Analysis</source><year>2007</year><issue>3</issue><fpage>445</fpage><lpage>472</lpage><pub-id pub-id-type="doi">10.1214/07-BA219</pub-id></element-citation></ref><ref id="R51"><label>[51]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bouchard-Côté</surname><given-names>A</given-names></name><name><surname>Doucet</surname><given-names>A</given-names></name><name><surname>Roth</surname><given-names>A</given-names></name></person-group><article-title>Particle Gibbs split-merge sampling for Bayesian inference in mixture models</article-title><source>The Journal of Machine Learning Research</source><year>2017</year><volume>18</volume><issue>1</issue><fpage>868</fpage><lpage>906</lpage></element-citation></ref><ref id="R52"><label>[52]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dahl</surname><given-names>DB</given-names></name><name><surname>Newcomb</surname><given-names>S</given-names></name></person-group><article-title>Sequentially allocated merge-split samplers for conjugate Bayesian nonpara¬metric models</article-title><source>Journal of Statistical Computation and Simulation</source><year>2022</year><volume>92</volume><issue>7</issue><fpage>1487</fpage><lpage>1511</lpage><pub-id pub-id-type="doi">10.1080/00949655.2021.1998502</pub-id></element-citation></ref><ref id="R53"><label>[53]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Broder</surname><given-names>A</given-names></name><name><surname>Garcia-Pueyo</surname><given-names>L</given-names></name><name><surname>Josifovski</surname><given-names>V</given-names></name><name><surname>Vassilvitskii</surname><given-names>S</given-names></name><name><surname>Venkatesan</surname><given-names>S</given-names></name></person-group><source>Scalable K-Means by Ranked Retrieval</source><conf-name>Proceedings of the 7th ACM International Conference on Web Search and Data Mining. WSDM ‘14</conf-name><publisher-loc>New York, NY, USA</publisher-loc><publisher-name>Association for Computing Machinery</publisher-name><year>2014</year><fpage>233</fpage><lpage>242</lpage><pub-id pub-id-type="doi">10.1145/2556195.2556260</pub-id></element-citation></ref><ref id="R54"><label>[54]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bachem</surname><given-names>O</given-names></name><name><surname>Lucic</surname><given-names>M</given-names></name><name><surname>Krause</surname><given-names>A</given-names></name></person-group><source>Scalable k -Means Clustering via Lightweight Coresets</source><conf-name>Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. KDD’18</conf-name><publisher-loc>New York, NY, USA</publisher-loc><publisher-name>Association for Computing Machinery</publisher-name><year>2018</year><fpage>1119</fpage><lpage>1127</lpage><pub-id pub-id-type="doi">10.1145/3219819.3219973</pub-id></element-citation></ref><ref id="R55"><label>[55]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname><given-names>D</given-names></name><name><surname>Chen</surname><given-names>X</given-names></name></person-group><article-title>Large Scale Spectral Clustering Via Landmark-Based Sparse Representation</article-title><source>IEEE Transactions on Cybernetics</source><year>2015</year><volume>45</volume><issue>8</issue><fpage>1669</fpage><lpage>1680</lpage></element-citation></ref><ref id="R56"><label>[56]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>L</given-names></name><name><surname>Ray</surname><given-names>N</given-names></name><name><surname>Guan</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name></person-group><article-title>Fast Large-Scale Spectral Clustering via Explicit Feature Mapping</article-title><source>IEEE Transactions on Cybernetics</source><year>2019</year><volume>49</volume><issue>3</issue><fpage>1058</fpage><lpage>1071</lpage></element-citation></ref><ref id="R57"><label>[57]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rinaldo</surname><given-names>A</given-names></name><name><surname>Singh</surname><given-names>A</given-names></name><name><surname>Nugent</surname><given-names>R</given-names></name><name><surname>Wasserman</surname><given-names>L</given-names></name></person-group><article-title>Stability of density-based clustering</article-title><source>Journal of Machine Learning Research</source><year>2012</year><volume>13</volume><fpage>905</fpage></element-citation></ref><ref id="R58"><label>[58]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kent</surname><given-names>BP</given-names></name><name><surname>Rinaldo</surname><given-names>A</given-names></name><name><surname>Verstynen</surname><given-names>T</given-names></name></person-group><source>DeBaCl: A Python Package for Interactive DEnsity-BAsed CLustering</source><year>2013</year></element-citation></ref><ref id="R59"><label>[59]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Von Luxburg</surname><given-names>U</given-names></name><name><surname>Ben-David</surname><given-names>S</given-names></name></person-group><source>Towards a statistical theory of clustering</source><conf-name>Pascal workshop on statistics and optimization of clustering</conf-name><conf-sponsor>Citeseer</conf-sponsor><year>2005</year><fpage>20</fpage><lpage>26</lpage></element-citation></ref><ref id="R60"><label>[60]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meinshausen</surname><given-names>N</given-names></name><name><surname>Buhlmann</surname><given-names>P</given-names></name></person-group><article-title>Stability selection</article-title><source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source><year>2010</year><volume>72</volume><issue>4</issue><fpage>417</fpage><lpage>473</lpage></element-citation></ref><ref id="R61"><label>[61]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Von Luxburg</surname><given-names>U</given-names></name></person-group><article-title>Clustering stability: anoverview</article-title><source>Foundations and Trends in Machine Learning</source><year>2010</year><volume>2</volume><issue>3</issue><fpage>235</fpage><lpage>274</lpage></element-citation></ref><ref id="R62"><label>[62]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wold</surname><given-names>S</given-names></name><name><surname>Esbensen</surname><given-names>K</given-names></name><name><surname>Geladi</surname><given-names>P</given-names></name></person-group><article-title>Principal component analysis</article-title><source>Chemometrics and intelligent laboratory systems</source><year>1987</year><issue>l-3</issue><fpage>37</fpage><lpage>52</lpage></element-citation></ref><ref id="R63"><label>[63]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fritsch</surname><given-names>A</given-names></name><name><surname>Ickstadt</surname><given-names>K</given-names></name></person-group><article-title>Improved criteria for clustering based on the posterior similarity matrix</article-title><source>Bayesian analysis</source><year>2009</year><volume>4</volume><issue>2</issue><fpage>367</fpage><lpage>391</lpage></element-citation></ref><ref id="R64"><label>[64]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fritsch</surname><given-names>A</given-names></name></person-group><article-title>mcclust: process an MCMC sample of clusterings</article-title><source>R package version 1.0</source><year>2012</year><comment>Available from: <ext-link ext-link-type="uri" xlink:href="https://CRAN.R-project.org/package=mcclust">https://CRAN.R-project.org/package=mcclust</ext-link></comment></element-citation></ref><ref id="R65"><label>[65]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wade</surname><given-names>S</given-names></name><name><surname>Ghahramani</surname><given-names>Z</given-names></name></person-group><article-title>Bayesian Cluster Analysis: Point Estimation and Credible Balls (with Discussion)</article-title><source>Bayesian Analysis</source><year>2018</year><month>Jun</month><volume>13</volume><issue>2</issue><fpage>559</fpage><lpage>626</lpage></element-citation></ref><ref id="R66"><label>[66]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lourenço</surname><given-names>A</given-names></name><name><surname>Rota Bulò</surname><given-names>S</given-names></name><name><surname>Rebagliati</surname><given-names>N</given-names></name><name><surname>Fred</surname><given-names>ALN</given-names></name><name><surname>Figueiredo</surname><given-names>MAT</given-names></name><name><surname>Pelillo</surname><given-names>M</given-names></name></person-group><article-title>Probabilistic Consensus Clustering Using Evidence Accumulation</article-title><source>Machine Learning</source><year>2015</year><month>Jan</month><volume>98</volume><issue>l</issue><fpage>331</fpage><lpage>357</lpage></element-citation></ref><ref id="R67"><label>[67]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dahl</surname><given-names>DB</given-names></name><name><surname>Johnson</surname><given-names>DJ</given-names></name><name><surname>Mueller</surname><given-names>P</given-names></name></person-group><article-title>Search Algorithms and Loss Functions for Bayesian Clustering</article-title><source>arXiv:210504451 [stat]</source><year>2021</year><month>May</month></element-citation></ref><ref id="R68"><label>[68]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Law</surname><given-names>MH</given-names></name><name><surname>Jain</surname><given-names>AK</given-names></name><name><surname>Figueiredo</surname><given-names>M</given-names></name></person-group><article-title>Feature selection in mixture-based clustering</article-title><source>Advances in neural information processing systems</source><year>2003</year><fpage>641</fpage><lpage>648</lpage></element-citation></ref><ref id="R69"><label>[69]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubert</surname><given-names>L</given-names></name><name><surname>Arabie</surname><given-names>P</given-names></name></person-group><article-title>Comparing partitions</article-title><source>Journal of classification</source><year>1985</year><volume>2</volume><issue>1</issue><fpage>193</fpage><lpage>218</lpage></element-citation></ref><ref id="R70"><label>[70]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scrucca</surname><given-names>L</given-names></name><name><surname>Fop</surname><given-names>M</given-names></name><name><surname>Murphy</surname><given-names>BT</given-names></name><name><surname>Raftery</surname><given-names>AE</given-names></name></person-group><article-title>mclust 5: clustering, classification and density estimation using Gaussian finite mixture models</article-title><source>The R Journal</source><year>2016</year><volume>8</volume><issue>1</issue><fpage>289</fpage><lpage>317</lpage><pub-id pub-id-type="doi">10.32614/RJ-2016-021</pub-id></element-citation></ref><ref id="R71"><label>[71]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwarz</surname><given-names>G</given-names></name><etal/></person-group><article-title>Estimating the dimension of a model</article-title><source>The annals of statistics</source><year>1978</year><volume>6</volume><issue>2</issue><fpage>461</fpage><lpage>464</lpage></element-citation></ref><ref id="R72"><label>[72]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Geweke</surname><given-names>J</given-names></name><etal/></person-group><source>Evaluating the accuracy of sampling-based approaches to the calculation of posterior moments</source><volume>196</volume><publisher-name>Federal Reserve Bank of Minneapolis, Research Department</publisher-name><publisher-loc>Minneapolis, MN</publisher-loc><year>1991</year></element-citation></ref><ref id="R73"><label>[73]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Rubin</surname><given-names>DB</given-names></name><etal/></person-group><article-title>Inference from iterative simulation using multiple sequences</article-title><source>Statistical science</source><year>1992</year><volume>7</volume><issue>4</issue><fpage>457</fpage><lpage>472</lpage></element-citation></ref><ref id="R74"><label>[74]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vats</surname><given-names>D</given-names></name><name><surname>Knudson</surname><given-names>C</given-names></name></person-group><article-title>Revisiting the Gelman-Rubin diagnostic</article-title><source>arXiv preprint</source><elocation-id>arXiv:181209384</elocation-id><year>2018</year></element-citation></ref><ref id="R75"><label>[75]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shapiro</surname><given-names>SS</given-names></name><name><surname>Wilk</surname><given-names>MB</given-names></name></person-group><article-title>An analysis of variance test for normality (complete samples)</article-title><source>Biometrika</source><year>1965</year><volume>52</volume><issue>3/4</issue><fpage>591</fpage><lpage>611</lpage></element-citation></ref><ref id="R76"><label>[76]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tyson</surname><given-names>JJ</given-names></name><name><surname>Chen</surname><given-names>KC</given-names></name><name><surname>Novak</surname><given-names>B</given-names></name></person-group><chapter-title>Cell Cycle, Budding Yeast</chapter-title><person-group person-group-type="editor"><name><surname>Dubitzky</surname><given-names>W</given-names></name><name><surname>Wolkenhauer</surname><given-names>O</given-names></name><name><surname>Cho</surname><given-names>KH</given-names></name><name><surname>Yokota</surname><given-names>H</given-names></name></person-group><source>Encyclopedia of Systems Biology</source><publisher-loc>New York, NY</publisher-loc><publisher-name>Springer New York</publisher-name><year>2013</year><fpage>337</fpage><lpage>341</lpage></element-citation></ref><ref id="R77"><label>[77]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>KC</given-names></name><name><surname>Calzone</surname><given-names>L</given-names></name><name><surname>Csikasz-Nagy</surname><given-names>A</given-names></name><name><surname>Cross</surname><given-names>FR</given-names></name><name><surname>Novak</surname><given-names>B</given-names></name><name><surname>Tyson</surname><given-names>JJ</given-names></name></person-group><article-title>Integrative analysis of cell cycle control in budding yeast</article-title><source>Molecular biology of the cell</source><year>2004</year><volume>15</volume><issue>8</issue><fpage>3841</fpage><lpage>3862</lpage></element-citation></ref><ref id="R78"><label>[78]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alberts</surname><given-names>B</given-names></name><name><surname>Johnson</surname><given-names>A</given-names></name><name><surname>Eewis</surname><given-names>J</given-names></name><name><surname>Raff</surname><given-names>M</given-names></name><name><surname>Roberts</surname><given-names>K</given-names></name><name><surname>Walter</surname><given-names>P</given-names></name></person-group><article-title>The cell cycle and programmed cell death</article-title><source>Molecular biology of the cell</source><year>2002</year><fpage>983</fpage><lpage>1027</lpage></element-citation></ref><ref id="R79"><label>[79]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ingalls</surname><given-names>B</given-names></name><name><surname>Duncker</surname><given-names>B</given-names></name><name><surname>Kim</surname><given-names>D</given-names></name><name><surname>McConkey</surname><given-names>B</given-names></name></person-group><article-title>Systems level modeling of the cell cycle using budding yeast</article-title><source>Cancer informatics</source><year>2007</year><elocation-id>117693510700300020</elocation-id></element-citation></ref><ref id="R80"><label>[80]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jimenez</surname><given-names>J</given-names></name><name><surname>Bru</surname><given-names>S</given-names></name><name><surname>Ribeiro</surname><given-names>M</given-names></name><name><surname>Clotet</surname><given-names>J</given-names></name></person-group><article-title>Five fast, die soon: cell cycle progression and lifespan in yeast cells</article-title><source>Microbial Cell</source><year>2015</year><volume>2</volume><issue>3</issue><fpage>62</fpage></element-citation></ref><ref id="R81"><label>[81]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Granovskaia</surname><given-names>MV</given-names></name><name><surname>Jensen</surname><given-names>EJ</given-names></name><name><surname>Ritchie</surname><given-names>ME</given-names></name><name><surname>Toedling</surname><given-names>J</given-names></name><name><surname>Ning</surname><given-names>Y</given-names></name><name><surname>Bork</surname><given-names>P</given-names></name><etal/></person-group><article-title>High-resolution transcription atlas of the mitotic cell cycle in budding yeast</article-title><source>Genome biology</source><year>2010</year><volume>11</volume><issue>3</issue><fpage>1</fpage><lpage>11</lpage></element-citation></ref><ref id="R82"><label>[82]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harbison</surname><given-names>CT</given-names></name><name><surname>Gordon</surname><given-names>DB</given-names></name><name><surname>Eee</surname><given-names>TI</given-names></name><name><surname>Rinaldi</surname><given-names>NJ</given-names></name><name><surname>Macisaac</surname><given-names>KD</given-names></name><name><surname>Danford</surname><given-names>TW</given-names></name><etal/></person-group><article-title>Transcriptional regulatory code of a eukaryotic genome</article-title><source>Nature</source><year>2004</year><volume>431</volume><issue>7004</issue><fpage>99</fpage><lpage>104</lpage></element-citation></ref><ref id="R83"><label>[83]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stark</surname><given-names>C</given-names></name><name><surname>Breitkreutz</surname><given-names>BJ</given-names></name><name><surname>Reguly</surname><given-names>T</given-names></name><name><surname>Boucher</surname><given-names>E</given-names></name><name><surname>Breitkreutz</surname><given-names>A</given-names></name><name><surname>Tyers</surname><given-names>M</given-names></name></person-group><article-title>BioGRID: a general repository for interaction datasets</article-title><source>Nucleic acids research</source><year>2006</year><volume>34</volume><issue>suppl_l</issue><fpage>D535</fpage><lpage>D539</lpage></element-citation></ref><ref id="R84"><label>[84]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simon</surname><given-names>I</given-names></name><name><surname>Barnett</surname><given-names>J</given-names></name><name><surname>Hannett</surname><given-names>N</given-names></name><name><surname>Harbison</surname><given-names>CT</given-names></name><name><surname>Rinaldi</surname><given-names>NJ</given-names></name><name><surname>Volkert</surname><given-names>TL</given-names></name><etal/></person-group><article-title>Serial regulation of transcriptional regulators in the yeast cell cycle</article-title><source>Cell</source><year>2001</year><volume>106</volume><issue>6</issue><fpage>697</fpage><lpage>708</lpage></element-citation></ref><ref id="R85"><label>[85]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iyer</surname><given-names>VR</given-names></name><name><surname>Horak</surname><given-names>CE</given-names></name><name><surname>Scafe</surname><given-names>CS</given-names></name><name><surname>Botstein</surname><given-names>D</given-names></name><name><surname>Snyder</surname><given-names>M</given-names></name><name><surname>Brown</surname><given-names>PO</given-names></name></person-group><article-title>Genomic binding sites of the yeast cell-cycle transcription factors SBF and MBF</article-title><source>Nature</source><year>2001</year><volume>409</volume><issue>6819</issue><fpage>533</fpage><lpage>538</lpage></element-citation></ref><ref id="R86"><label>[86]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlson</surname><given-names>M</given-names></name><name><surname>Falcon</surname><given-names>S</given-names></name><name><surname>Pages</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>N</given-names></name><collab>Org. sc. sgd. db</collab></person-group><article-title>Genome wide annotation for yeast</article-title><source>R package version</source><year>2014</year><volume>2</volume><issue>l</issue></element-citation></ref><ref id="R87"><label>[87]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bando</surname><given-names>M</given-names></name><name><surname>Katou</surname><given-names>Y</given-names></name><name><surname>Komata</surname><given-names>M</given-names></name><name><surname>Tanaka</surname><given-names>H</given-names></name><name><surname>Itoh</surname><given-names>T</given-names></name><name><surname>Sutani</surname><given-names>T</given-names></name><etal/></person-group><article-title>Csm3, Tofl, and Mrcl form a heterotrimeric mediator complex that associates with DNA replication forks</article-title><source>Journal of Biological Chemistry</source><year>2009</year><volume>284</volume><issue>49</issue><fpage>34355</fpage><lpage>34365</lpage></element-citation></ref><ref id="R88"><label>[88]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lao</surname><given-names>JP</given-names></name><name><surname>Ulrich</surname><given-names>KM</given-names></name><name><surname>Johnson</surname><given-names>JR</given-names></name><name><surname>Newton</surname><given-names>BW</given-names></name><name><surname>Vashisht</surname><given-names>AA</given-names></name><name><surname>Wohlschlegel</surname><given-names>JA</given-names></name><etal/></person-group><article-title>The yeast DNA damage checkpoint kinase Rad53 targets the exoribonuclease, Xrn1</article-title><source>G3: Genes, Genomes, Genetics</source><year>2018</year><volume>8</volume><issue>12</issue><fpage>3931</fpage><lpage>3944</lpage></element-citation></ref><ref id="R89"><label>[89]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tόth</surname><given-names>A</given-names></name><name><surname>Ciosk</surname><given-names>R</given-names></name><name><surname>Uhlmann</surname><given-names>F</given-names></name><name><surname>Galova</surname><given-names>M</given-names></name><name><surname>Schleiffer</surname><given-names>A</given-names></name><name><surname>Nasmyth</surname><given-names>K</given-names></name></person-group><article-title>Yeast cohesin complex requires a conserved protein, Ecolp (Ctf7), to establish cohesion between sister chromatids during DNA replication</article-title><source>Genes &amp; development</source><year>1999</year><volume>13</volume><issue>3</issue><fpage>320</fpage><lpage>333</lpage></element-citation></ref><ref id="R90"><label>[90]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehta</surname><given-names>GD</given-names></name><name><surname>Kumar</surname><given-names>R</given-names></name><name><surname>Srivastava</surname><given-names>S</given-names></name><name><surname>Ghosh</surname><given-names>SK</given-names></name></person-group><article-title>Cohesin: functions beyond sister chromatid cohesion</article-title><source>FEBS letters</source><year>2013</year><volume>587</volume><issue>15</issue><fpage>2299</fpage><lpage>2312</lpage></element-citation></ref><ref id="R91"><label>[91]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischle</surname><given-names>W</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Allis</surname><given-names>CD</given-names></name></person-group><article-title>Histone and chromatin cross-talk</article-title><source>Current opinion in cell biology</source><year>2003</year><volume>15</volume><issue>2</issue><fpage>172</fpage><lpage>183</lpage></element-citation></ref><ref id="R92"><label>[92]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bannister</surname><given-names>AJ</given-names></name><name><surname>Kouzarides</surname><given-names>T</given-names></name></person-group><article-title>Regulation of chromatin by histone modifications</article-title><source>Cell research</source><year>2011</year><volume>21</volume><issue>3</issue><fpage>381</fpage><lpage>395</lpage></element-citation></ref><ref id="R93"><label>[93]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>deBruin</surname><given-names>RA</given-names></name><name><surname>Kalashnikova</surname><given-names>TI</given-names></name><name><surname>Chahwan</surname><given-names>C</given-names></name><name><surname>McDonald</surname><given-names>WH</given-names></name><name><surname>Wohlschlegel</surname><given-names>J</given-names></name><name><surname>Yates</surname><given-names>J</given-names><prefix>III</prefix></name><etal/></person-group><article-title>Con¬straining G1-specific transcription to late G1 phase: the MBF-associated corepressor Nrml acts via negative feedback</article-title><source>Molecular cell</source><year>2006</year><volume>23</volume><issue>4</issue><fpage>483</fpage><lpage>496</lpage></element-citation></ref><ref id="R94"><label>[94]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aligianni</surname><given-names>S</given-names></name><name><surname>Lackner</surname><given-names>DH</given-names></name><name><surname>Klier</surname><given-names>S</given-names></name><name><surname>Rustici</surname><given-names>G</given-names></name><name><surname>Wilhelm</surname><given-names>BT</given-names></name><name><surname>Marguerat</surname><given-names>S</given-names></name><etal/></person-group><article-title>The fission yeast homeodomain protein Yoxlp binds to MBF and confines MBF-dependent cell-cycle transcription to Gl-S via negative feedback</article-title><source>PLoS Genet</source><year>2009</year><volume>5</volume><issue>8</issue><elocation-id>e1000626</elocation-id></element-citation></ref><ref id="R95"><label>[95]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ciosk</surname><given-names>R</given-names></name><name><surname>Zachariae</surname><given-names>W</given-names></name><name><surname>Michaelis</surname><given-names>C</given-names></name><name><surname>Shevchenko</surname><given-names>A</given-names></name><name><surname>Mann</surname><given-names>M</given-names></name><name><surname>Nasmyth</surname><given-names>K</given-names></name></person-group><article-title>An ESP1/PDS1 complex regulates loss of sister chromatid cohesion at the metaphase to anaphase transition in yeast</article-title><source>Cell</source><year>1998</year><volume>93</volume><issue>6</issue><fpage>1067</fpage><lpage>1076</lpage></element-citation></ref><ref id="R96"><label>[96]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cooper</surname><given-names>KF</given-names></name><name><surname>Mallory</surname><given-names>MJ</given-names></name><name><surname>Guacci</surname><given-names>V</given-names></name><name><surname>Lowe</surname><given-names>K</given-names></name><name><surname>Strich</surname><given-names>R</given-names></name></person-group><article-title>Pdslp is required for meiotic recombination and prophase I progression in Saccharomyces cerevisiae</article-title><source>Genetics</source><year>2009</year><volume>181</volume><issue>1</issue><fpage>65</fpage><lpage>79</lpage></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><p>Example of generated datasets. Each row is an item being clustered and each column a feature of generated data. The 2D dataset (which is ordered by hierarchical clustering here) should enable proper mixing of chains in the MCMC. The small <italic>N</italic>, large <italic>P</italic> case has clear structure (observable by eye). This is intended to highlight the problems of poor mixing due to high dimensions even when the generating labels are quite identifiable. In the irrelevant features case, the structure is clear in the relevant features (on the left-hand side of this heatmap). This setting is intended to test how sensitive each approach is to noise.</p></caption><graphic xlink:href="EMS144881-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><p>Model performance in the 100 simulated datasets for each scenario, defined as the ARI between the generating labels and the inferred clustering. <italic>CC</italic>(<italic>d, w</italic>) denotes consensus clustering using the clustering from the <italic>d<sup>th</sup></italic> iteration from <italic>w</italic> different chains.</p></caption><graphic xlink:href="EMS144881-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><p>Comparison of similarity matrices from a dataset for the Small <italic>N</italic>, large <italic>P</italic> scenario. In each matrix, the (<italic>i</italic>, <italic>j</italic>)<sup><italic>th</italic></sup> entry is the proportion of clusterings for which the <italic>i<sup>th</sup></italic> and <italic>j<sup>th</sup></italic> items co-clustered for the method in question. In the first row the PSM of the pooled Bayesian samples is compared to the CM for CC(100, 50), with a common ordering of rows and columns in both heatmaps. In the following rows, 6 of the long chains that passed the tests of convergence are shown.</p></caption><graphic xlink:href="EMS144881-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><p>The time taken for different numbers of iterations of MCMC moves in log<sub>10</sub>(<italic>seconds</italic>). The relationship between chain length, <italic>D</italic>, and the time taken is linear (the slope is approximately 1 on the log<sub>10</sub> scale), with a change of intercept for different dimensions. The runtime of each Markov chain was recorded using the terminal command <monospace>time</monospace>, measured in milliseconds.</p></caption><graphic xlink:href="EMS144881-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><p>The mean absolute difference between the sequential Consensus matrices. For a set of chain lengths, <italic>D’</italic> = {<italic>d</italic><sub>1</sub>,… ,<italic>d<sub>I</sub></italic>} and number of chains, <italic>W<sup>r</sup></italic> = {<italic>w</italic><sub>1</sub>,..., <italic>w<sub>J</sub></italic>}, we take the mean of the absolute difference between the consensus matrix for (<italic>d<sub>i</sub></italic>, <italic>w<sub>j</sub></italic>) and (<italic>d<sub>i</sub></italic><sub>–1</sub>, <italic>w<sub>j</sub></italic>) (here <italic>D</italic>’ = {101, 501,1001, 2001,..., 10001} and <italic>W</italic> = {100, 500,1000}).</p></caption><graphic xlink:href="EMS144881-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><p>The gene clusters which tend to have a common label across the time course and ChIP-chip datasets, shown in these datasets. We include only the clusters with more than one member and more than half the members having some interactions in the ChIP-chip data. Red lines for the most common transcription factors are included.</p></caption><graphic xlink:href="EMS144881-f006"/></fig><fig id="F7" position="float"><label>Figure 7</label><caption><p>The sampled values for the <italic>ø</italic> parameters from the long chains, their pooled samples and the consensus using 1000 chains of depth 10,001. The long chains display a variety of behaviours. Across chains there is no clear consensus on the nature of the posterior distribution. The samples from any single chain are not particularly close to the behaviour of the pooled samples across all three parameters. It is the consensus clustering that most approaches this pooled behaviour.</p></caption><graphic xlink:href="EMS144881-f007"/></fig><table-wrap id="T1" orientation="portrait" position="float"><label>Table 1</label><caption><title>Parameters defining the simulation scenarios as used in generating data and labels.</title><p>Δ<italic>μ</italic> is the distance between neighbouring cluster means within a single feature. The number of relevant features (<italic>P</italic><sub><italic>s</italic></sub>) is Σ<sub><italic>p</italic></sub>ø<sub><italic>p</italic></sub>, and <italic>P<sub>n</sub></italic> = <italic>P</italic> — <italic>P<sub>s</sub></italic>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="middle" align="left">Scenario</th><th valign="middle" align="center"><italic>N</italic></th><th valign="middle" align="center"><italic>P<sub>s</sub></italic></th><th valign="middle" align="center"><italic>P<sub>n</sub></italic></th><th valign="middle" align="center"><italic>K</italic></th><th valign="middle" align="center">Δ<sub><italic>μ</italic></sub></th><th valign="middle" align="center">σ<sup>2</sup></th><th valign="middle" align="center">π</th></tr></thead><tbody><tr><td valign="middle" align="left">2D</td><td valign="middle" align="center">100</td><td valign="middle" align="center">2</td><td valign="middle" align="center">0</td><td valign="middle" align="center">5</td><td valign="middle" align="center">3.0</td><td valign="middle" align="center">1</td><td valign="middle" align="center"><inline-formula><mml:math id="M9"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>5</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>5</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>5</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>5</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>5</mml:mn></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></td></tr><tr><td valign="middle" align="left">Small N, large P</td><td valign="middle" align="center">50</td><td valign="middle" align="center">500</td><td valign="middle" align="center">0</td><td valign="middle" align="center">5</td><td valign="middle" align="center">1.0</td><td valign="middle" align="center">1</td><td valign="middle" align="center"><inline-formula><mml:math id="M10"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>5</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>5</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>5</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>5</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>5</mml:mn></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></td></tr><tr><td valign="middle" align="left">Irrelevant features</td><td valign="middle" align="center">200</td><td valign="middle" align="center">20</td><td valign="middle" align="center">100</td><td valign="middle" align="center">5</td><td valign="middle" align="center">1.0</td><td valign="middle" align="center">1</td><td valign="middle" align="center"><inline-formula><mml:math id="M11"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>5</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>5</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>5</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>5</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>5</mml:mn></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></td></tr></tbody></table></table-wrap></floats-group></article>