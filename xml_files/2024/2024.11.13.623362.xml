<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS200145</article-id><article-id pub-id-type="doi">10.1101/2024.11.13.623362</article-id><article-id pub-id-type="archive">PPR940512</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Predictability of next elements in chimpanzee gesture sequences</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Mielke</surname><given-names>Alexander</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Badihi</surname><given-names>Gal</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Donnellan</surname><given-names>Ed</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Graham</surname><given-names>Kirsty E.</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Hashimoto</surname><given-names>Chie</given-names></name><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Mine</surname><given-names>Joseph G.</given-names></name><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Piel</surname><given-names>Alex K.</given-names></name><xref ref-type="aff" rid="A7">7</xref><xref ref-type="aff" rid="A8">8</xref></contrib><contrib contrib-type="author"><name><surname>Safryghin</surname><given-names>Alexandra</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Slocombe</surname><given-names>Katie E.</given-names></name><xref ref-type="aff" rid="A9">9</xref></contrib><contrib contrib-type="author"><name><surname>Soldati</surname><given-names>Adrian</given-names></name><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Stewart</surname><given-names>Fiona A.</given-names></name><xref ref-type="aff" rid="A7">7</xref><xref ref-type="aff" rid="A8">8</xref></contrib><contrib contrib-type="author"><name><surname>Townsend</surname><given-names>Simon W.</given-names></name><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A6">6</xref><xref ref-type="aff" rid="A12">12</xref></contrib><contrib contrib-type="author"><name><surname>Wilke</surname><given-names>Claudia</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A9">9</xref></contrib><contrib contrib-type="author"><name><surname>Zuberbühler</surname><given-names>Klaus</given-names></name><xref ref-type="aff" rid="A10">10</xref></contrib><contrib contrib-type="author"><name><surname>Zulberti</surname><given-names>Chiara</given-names></name><xref ref-type="aff" rid="A11">11</xref></contrib><contrib contrib-type="author"><name><surname>Hobaiter</surname><given-names>Catherine</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><aff id="A1"><label>1</label>School of Biological and Behavioural Sciences, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/026zzn846</institution-id><institution>Queen Mary University of London</institution></institution-wrap>, <city>London</city>, <country country="GB">UK</country></aff><aff id="A2"><label>2</label>Wild Minds Lab, School of Psychology and Neuroscience, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02wn5qz54</institution-id><institution>University of St Andrews</institution></institution-wrap>, <city>St Andrews</city>, <country country="GB">UK</country></aff><aff id="A3"><label>3</label>Department of Psychology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01a77tt86</institution-id><institution>University of Warwick</institution></institution-wrap>, <country country="GB">UK</country></aff><aff id="A4"><label>4</label>Department of Psychology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/00g2xk477</institution-id><institution>Hunter College</institution></institution-wrap>, <city>CUNY</city>, <country country="US">USA</country></aff><aff id="A5"><label>5</label>Primate Research Institute, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02kpeqv85</institution-id><institution>Kyoto University</institution></institution-wrap>, <country country="JP">Japan</country></aff><aff id="A6"><label>6</label>Department of Evolutionary Anthropology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02crff812</institution-id><institution>University of Zürich</institution></institution-wrap>, <country country="CH">Switzerland</country></aff><aff id="A7"><label>7</label>Department of Anthropology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap>, <city>London</city>, <country country="GB">UK</country></aff><aff id="A8"><label>8</label>Department of Human Origins, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02a33b393</institution-id><institution>Max Planck Institute of Evolutionary Anthropology</institution></institution-wrap>, <city>Leipzig</city>, <country country="DE">Germany</country></aff><aff id="A9"><label>9</label>Department of Psychology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/04m01e293</institution-id><institution>University of York</institution></institution-wrap>, <city>York</city>, <country country="GB">UK</country></aff><aff id="A10"><label>10</label>Institute of Biology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/00vasag41</institution-id><institution>University of Neuchâtel</institution></institution-wrap>, <city>Neuchâtel</city>, <country country="CH">Switzerland</country></aff><aff id="A11"><label>11</label>Institute of Biology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/03s7gtk40</institution-id><institution>University of Leipzig</institution></institution-wrap>, <city>Leipzig</city>, <country country="DE">Germany</country></aff><aff id="A12"><label>12</label>Centre for the Interdisciplinary Study of Language Evolution, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02crff812</institution-id><institution>University of Zurich</institution></institution-wrap>, <country country="CH">Switzerland</country></aff></contrib-group><pub-date pub-type="nihms-submitted"><day>17</day><month>11</month><year>2024</year></pub-date><pub-date pub-type="preprint"><day>14</day><month>11</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Recent research has produced evidence for basic combinatorial abilities in the vocal systems of different animal species. Here, we investigate the structure of gesture sequences in Eastern chimpanzees (<italic>Pan troglodytes schweinfurthii</italic>) to detect whether gestural communication shows non-random combinations and how combinatorial rules influence predictability. Gesture, as compared to vocalization, offers greater flexibility in how signals are combined—for example overlapping in time — and as the parsing of signals into sequences is dependent on researcher decisions, we employ a multiverse approach, considering four different definitions of what constitutes a ‘sequence’ based on varying time thresholds. Our results indicate that sequences tend to be short (even with the most liberal time-window) and that transitions between some gesture types occur more frequently than expected by chance, with some transitions showing significant association across all time-windows. These transitions often involve repetition, suggesting persistence as a key aspect of chimpanzee gestural sequences. Information about previous gestures reduced uncertainty in predicting subsequent gestures. The order of gestures within sequences appears to be less critical than their cooccurrence, challenging assumptions based on the linear patterning derived from vocal communication. Our findings highlight the importance of methodological choices in sequence definition and suggest that chimpanzee gestural communication is characterised by a mix of predictability and flexibility, with implications for understanding the evolution of complex communication systems.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Despite substantial differences in the expression of human language and other animals’ communication systems, research has increasingly uncovered similarities in structure (e.g., <xref ref-type="bibr" rid="R6">Berthet et al., 2023</xref>), providing evidence that there are fewer qualitative differences than previously envisioned. Animal communication systems are characterised by a rich array of structural properties, including evidence of hierarchical organisation (<xref ref-type="bibr" rid="R61">ten Cate &amp; Okanoya, 2012</xref>; <xref ref-type="bibr" rid="R64">Weiss et al., 2014</xref>), non-Markovian dependencies (<xref ref-type="bibr" rid="R33">Kershenbaum et al., 2014</xref>), and combinatoriality (<xref ref-type="bibr" rid="R13">Coye et al., 2018</xref>; <xref ref-type="bibr" rid="R19">Engesser et al., 2016</xref>; <xref ref-type="bibr" rid="R48">Oña et al., 2019</xref>; <xref ref-type="bibr" rid="R63">Walsh et al., 2023</xref>). Combinatoriality, the ability to predictably produce more than one element either in sequence or simultaneously, has the potential to dramatically increase the amount of information that can be transmitted with a finite repertoire, especially when combinations can take on meaning that is distinct from the constituting parts (compositionality). Many studies of combinatoriality in animal communication have been conducted on systems that are not considered to have meaning-bearing signal elements (in the linguistic sense), for example the song-like sequences of whales, birds, and some primates (<xref ref-type="bibr" rid="R1">Allen et al., 2019</xref>; <xref ref-type="bibr" rid="R7">Berwick et al., 2011</xref>; <xref ref-type="bibr" rid="R12">Clarke et al., 2006</xref>). Research on meaning-bearing signals has often focussed on restricted sets of highly specific meaning-bearing systems, such as alarm calls (<xref ref-type="bibr" rid="R13">Coye et al., 2018</xref>; <xref ref-type="bibr" rid="R19">Engesser et al., 2016</xref>; <xref ref-type="bibr" rid="R59">Suzuki et al., 2019</xref>), usually favouring vocal communication. Using specific contexts to test animals’ ability to combine signals non-randomly has the advantage of reducing noise, but it gives us little indication of how these skills (if they exist) influence the communication system as a whole. Here, we use the largest database of coded chimpanzee gestures to test whether there are non-random transitions between gesture actions (n-grams), based on one or two previous elements in a sequence, and how any combinatorial rules underlying these transitions influence the predictability of sequences.</p><p id="P3">There is ample evidence in singing species that individuals combine signals into predictable structures, the acquisition of which often involves evidence of individual learning and culture (<xref ref-type="bibr" rid="R1">Allen et al., 2019</xref>; <xref ref-type="bibr" rid="R7">Berwick et al., 2011</xref>; <xref ref-type="bibr" rid="R12">Clarke et al., 2006</xref>; <xref ref-type="bibr" rid="R20">Garland et al., 2011</xref>; <xref ref-type="bibr" rid="R34">Kershenbaum et al., 2012</xref>). Datasets for this approach can be very large, as song sequences are typically longer than those in other forms of non-human communication, so probabilistic transition patterns can be established using methods imported from natural language processing (<xref ref-type="bibr" rid="R32">Kershenbaum et al., 2016</xref>). For meaning-bearing communication, we have evidence from birds’ and primates’ vocalisations using observational and experimental approaches that identify predictable transitions between elements. These studies often focus on identifying specific combinations that can then be tested experimentally (e.g., <xref ref-type="bibr" rid="R17">Dutour et al., 2019</xref>; <xref ref-type="bibr" rid="R53">Schlenker et al., 2016</xref>; <xref ref-type="bibr" rid="R56">Spiess et al., 2022</xref>; <xref ref-type="bibr" rid="R60">Suzuki et al., 2016</xref>). In non-vocal communication, datasets have been more limited—there is at least preliminary evidence for predictable sequences or co-occurrences in gestural communication (<xref ref-type="bibr" rid="R21">Genty &amp; Byrne, 2010</xref>), facial signals (<xref ref-type="bibr" rid="R48">Oña et al., 2019</xref>), and multimodal signals (<xref ref-type="bibr" rid="R3">Aychet et al., 2021</xref>; <xref ref-type="bibr" rid="R47">Muschinski et al., 2023</xref>; <xref ref-type="bibr" rid="R66">Wilke et al., 2017</xref>), but a particular challenge in these systems is that the repertoires of units are often far larger than found in vocal communication (<xref ref-type="bibr" rid="R27">Hobaiter &amp; Byrne, 2011b</xref>) and the ratio of distinct communication units to sample size has made it difficult to understand the system as a whole, rather than for small subsamples of common units.</p><p id="P4">Here, we analyse the structure of chimpanzee gesture sequences as a system, rather than focusing on a single context, using an uncommonly large dataset (<xref ref-type="bibr" rid="R25">Grund et al., 2023</xref>; <xref ref-type="bibr" rid="R43">Mielke et al., 2024</xref>). Given their evolutionary proximity to our own species, their large repertoire of gestural units, the diversity of contexts in which they communicate, and existing research on the meaning, flexibility, and intentionality of this system, chimpanzees are a prime target for this type of study. Chimpanzee gestural communication is frequently sequential by nature, partly due to chimpanzees often persisting and elaborating in order to meet their goals (<xref ref-type="bibr" rid="R27">Hobaiter &amp; Byrne, 2011b</xref>; <xref ref-type="bibr" rid="R36">Leavens et al., 2005</xref>; <xref ref-type="bibr" rid="R50">Rodrigues &amp; Fröhlich, 2021</xref>). However, sequences are often short, with a previous study finding that almost 80% of ‘rapid-fire sequences’ (those made with less than a second between units) were composed of only 2-elements (<xref ref-type="bibr" rid="R26">Hobaiter &amp; Byrne, 2011a</xref>). In rapid-fire sequences, repetition of the same gesture is relatively rare, but it becomes increasingly common when longer pauses between gestures, including periods of response waiting, are considered (<xref ref-type="bibr" rid="R26">Hobaiter &amp; Byrne, 2011a</xref>). Response waiting is a characteristic of intentional imperative communication in which the signaller seeks to achieve a particular goal with respect to a specific recipient (<xref ref-type="bibr" rid="R4">Bates et al., 1975</xref>) — as a result, as opposed to continuing to broadcast information, the signaller emits a signal and then waits to see whether the recipient will respond, in order to determine whether they should produce additional signals. Younger individuals use more and longer sequences, possibly because these are more common in play, but also potentially to explore their large gestural repertoires (<xref ref-type="bibr" rid="R27">Hobaiter &amp; Byrne, 2011b</xref>). There is divergent evidence about the impact of gesture sequences on communicative efficiency, with some studies failing to identify a clear impact (<xref ref-type="bibr" rid="R21">Genty &amp; Byrne, 2010</xref>) while others suggesting that stringing gestures together increases success, but only because it increased the likelihood of incorporating an inherently more successful gesture unit (<xref ref-type="bibr" rid="R26">Hobaiter &amp; Byrne, 2011a</xref>). Transitions between gesture units have been reported in lowland gorillas, showing a number of regular transitions, many of which were repetitions (<xref ref-type="bibr" rid="R21">Genty &amp; Byrne, 2010</xref>). Gesture actions in lowland gorillas form clusters, with physical contact gestures within play often occurring in an unordered cluster and play initiation/regulation gestures occurring in an ordered cluster (<xref ref-type="bibr" rid="R21">Genty &amp; Byrne, 2010</xref>), a pattern also seen in chimpanzee play (<xref ref-type="bibr" rid="R44">Mielke &amp; Carvalho, 2022</xref>). Combinations of facial signals with gestures can alter their meaning in predictable ways, with bared-teeth faces impacting whether a gesture elicits an affiliative response (<xref ref-type="bibr" rid="R48">Oña et al., 2019</xref>).</p><p id="P5">More evidence of predictable rules in chimpanzee sequential signalling derives from vocal communication. Sequential utterances are present from birth (<xref ref-type="bibr" rid="R55">Soldati et al., 2022</xref>), and sequence production increases in ‘complexity’ (in terms of their length and diversity) throughout infancy, coinciding with a diversification of social interactions (<xref ref-type="bibr" rid="R8">Bortolato et al., 2023</xref>). Evidence from different populations now shows that call combinations remain common in adulthood (<xref ref-type="bibr" rid="R14">Crockford &amp; Boesch, 2005</xref>; <xref ref-type="bibr" rid="R38">Leroux et al., 2022</xref>). Here analyses have focused on how sequences change the information encoded in both specific contexts (e.g., food detection, <xref ref-type="bibr" rid="R37">Leroux et al., 2021</xref>) and the repertoire more widely (<xref ref-type="bibr" rid="R22">Girard-Buttoz et al., 2022</xref>; <xref ref-type="bibr" rid="R38">Leroux et al., 2022</xref>). These datasets show that chimpanzees can combine units flexibly (i.e., each unit occurs in sequence with multiple others), that there are production biases for unit order (AB occurring at different rates than BA), and evidence of the recombination of two-unit combinations (bigrams) in three-unit combinations and larger structures (<xref ref-type="bibr" rid="R22">Girard-Buttoz et al., 2022</xref>; <xref ref-type="bibr" rid="R37">Leroux et al., 2021</xref>). However, in many vocal studies, vocalisations are lumped into small repertoire sets of higher-order call types. Recent work provides ample evidence that calls are graded and— importantly—encode specific context at a much finer-grained scale (<xref ref-type="bibr" rid="R15">Crockford et al., 2018</xref>; <xref ref-type="bibr" rid="R54">Slocombe &amp; Zuberbühler, 2005</xref>). As a result, it remains challenging to assess the true flexibility of the system: it might be that ‘hoos’ in general (or ‘screams’ or ‘barks’) are combined with a number of other units, but more specific units such as ‘travel hoos’ only occur in fixed combinations, belying the appearance of recombination.</p><p id="P6">Adding to the challenge of studying combinations, in the broader study of non-human communication, there is no natural definition of what constitutes a cohesive sequence of units. This ambiguity has led to a proliferation of different cut-off values to differentiate between one sequence and another—in the case of analyses of bird or whale songs, for example (<xref ref-type="bibr" rid="R1">Allen et al., 2019</xref>), sequences are anything that happens between the start and end of the song. In contrast, gesture researchers have historically applied different thresholds; for example, considering units to be in the same rapid-fire sequence when the following gesture occurs within one second of the end of the previous one (<xref ref-type="bibr" rid="R26">Hobaiter &amp; Byrne, 2011a</xref>), a rule that has also been adopted in some vocal research (<xref ref-type="bibr" rid="R22">Girard-Buttoz et al., 2022</xref>), while other researchers have advocated a data-driven threshold detection approach (<xref ref-type="bibr" rid="R3">Aychet et al., 2021</xref>). Rapid-fire sequences in previous studies were differentiated from more extended sequences (sometimes called “bouts”) that represent the addition of further units for the purpose of persistence or elaboration following the failure of an initial communication (<xref ref-type="bibr" rid="R36">Leavens et al., 2005</xref>). The distinction between sequences and bouts was inspired by the idea of response waiting. If response waiting does define a different <italic>type</italic> of sequence, the different time-windows might contain different structural properties; for example, rapid-fire sequences (those without response waiting between gestures) might have distinct combinatorial rules as compared to sequences that include response waiting between signals (<xref ref-type="bibr" rid="R26">Hobaiter &amp; Byrne, 2011a</xref>).</p><p id="P7">Importantly, while strictly vocal signals must almost always occur as separate signals produced in a linear sequence, gestures and multimodal sequences can be produced simultaneously using multiple limbs. This feature offers intriguing possibilities for encoding structure in dimensions such as space, as well as in time, as seen in signed languages (<xref ref-type="bibr" rid="R2">Armstrong et al., 1994</xref>; <xref ref-type="bibr" rid="R52">Sandler &amp; Lillo-Martin, 2006</xref>; <xref ref-type="bibr" rid="R58">Stokoe, 1980</xref>) and human facial signals (<xref ref-type="bibr" rid="R62">Trujillo &amp; Holler, 2024</xref>). Thus, we can define ‘sequences’ in a gestural context following different rules that might have both biological implications (because individuals communicate different pieces of information at different speeds, including units that overlap in time) and statistical implications (because more liberal sequence definitions will create longer sequences and therefore increase sample size). To address this, we take a multiverse approach (<xref ref-type="bibr" rid="R57">Steegen et al., 2016</xref>), conducting the same analytical steps using datasets following different preprocessing decisions to reduce researcher biases. For all analyses addressing first order transitions (A leading to B), we will produce results based on 4 different definitions of ‘sequence’ and use differences in outcomes to understand potential reasons for the differences in time thresholds and/or production speed. For analyses addressing higher-order transitions (AB leading to C), we will rely on the most liberal definition of sequence (here any gesture that occurs within 5 seconds of the end of the minimal action unit of the previous gesture), which adds noise but at the same time substantially increases the sample size for sequences above the length of 2 elements.</p><p id="P8">We further the study of sequence use in non-human species by analysing the largest dataset of chimpanzee gestures. We ask three questions: 1) <bold>Are there non-random transitions between individual units and higher-order combinations?</bold> We test this question by comparing observed transition probabilities between antecedent and consequent gestures with expected probabilities based on a null model of random distribution of elements (<xref ref-type="bibr" rid="R3">Aychet et al., 2021</xref>; <xref ref-type="bibr" rid="R10">Bosshard et al., 2022</xref>; <xref ref-type="bibr" rid="R44">Mielke &amp; Carvalho, 2022</xref>). Thus, we establish which gesture actions are connected to each other, whether individual gesture actions have deterministic connections to specific subsequent gestures, and whether we find highly structured networks of gestures that occur together. We hypothesise that, as in vocalisations, there are bigrams and trigrams of gesture actions that occur next to each other in sequences more than would be expected if gesture actions were randomly distributed within or between sequences [P(B|A)] (<xref ref-type="bibr" rid="R10">Bosshard et al., 2022</xref>; <xref ref-type="bibr" rid="R22">Girard-Buttoz et al., 2022</xref>; <xref ref-type="bibr" rid="R38">Leroux et al., 2022</xref>). We hypothesise that with an increasing number of previous units, the number of significant n-grams decreases, but that we still find higher-order significant collocations [P(C|AB)] (<xref ref-type="bibr" rid="R22">Girard-Buttoz et al., 2022</xref>). We predict that, when analysing the transition network, we can identify clusters of gesture action that occur together frequently, even if their exact temporal order might not be relevant (<xref ref-type="bibr" rid="R44">Mielke &amp; Carvalho, 2022</xref>). We also specifically compare the solitary gesture network and rapid-fire sequence network, as these potentially represent two distinct systems of combinatoriality with separate use: solitary gestures here are defined by individual elements separated by clear response waiting, so the sequence is clearly influenced by partner responses. Rapid-fire sequences are so fast that signaller planning should precede the partner response. <bold>2) Does sequence order matter?</bold> Order effects are important for establishing combinatoriality because they rule out a simpler alternative - that sequences are strings of a number of elements with the same meaning that are randomly put together until the partner reacts appropriately. We predict that we observe two-element pairs where the conditional probability of A following B exceeds what would be expected if sequences were randomly strung together. <bold>3) How do these rules influence predictability?</bold> We test this using entropy and classifier accuracy to establish the level of predictability and order in the system. Both provide us with estimates of the predictability of the system as a whole. For all models of Markov transitions (A leading to B), we employ four time-windows (below). We predict that knowing the previous unit(s) in a sequence improves the predictability and order in the system (quantified using entropy and classifier accuracy). We predict that order matters—randomising the order of A and B in the latter example should reduce predictability. When specifically focusing on cases where we predict a unit based on two preceding units, we predict that knowing both units [P(C|AB)] will improve prediction accuracy over just knowing either of them [P(C|A) or P(C|B)].</p></sec><sec id="S2" sec-type="methods"><title>Methods</title><sec id="S3"><title>Data</title><p id="P9">The dataset contains 7,749 gestures from 5 communities of East African chimpanzees (<italic>Pan troglodytes schweinfurthii)</italic> (<xref ref-type="bibr" rid="R43">Mielke et al., 2024</xref>), structured in varying numbers of sequences of between one and 15 units, depending on the definition (<xref ref-type="table" rid="T1">Table 1</xref>). The coding scheme has been described in detail (<xref ref-type="bibr" rid="R25">Grund et al., 2023</xref>). The 92 gesture actions (distinct and delineated types of gestures) used in the original coding scheme were lumped based on predefined rules (see <xref ref-type="bibr" rid="R43">Mielke et al., 2024</xref>) to reduce the occurrence of rare cases, with gesture actions that occurred fewer than 10 times excluded (<xref ref-type="bibr" rid="R43">Mielke et al., 2024</xref>). We retained 42 gesture actions in the current analyses. Each gesture action has a clearly defined Minimum Action Unit (MAU) based on the minimum information necessary to distinguish between gesture actions, starting from the moment the individual moves the body part and finishing when the gesture action is fully in place (<xref ref-type="bibr" rid="R25">Grund et al., 2023</xref>). Note that gesture actions can continue to be held in place or repeated (through the use of an optional hold or repetition phase, <italic>sensu</italic> Kendon, 2004) beyond the end of the MAU, followed by recovery of the articulator to rest. Sequences were defined based on 4 distinct time-windows (<xref ref-type="fig" rid="F1">Fig. 1</xref>), going from the broadest to the most restrictive: a) gestures that occurred within 5 seconds after the end of the MAU of the previous unit (<italic>‘5 seconds</italic>’); b) gestures that occurred within 1 second after the end of the MAU of the previous unit (previously termed ‘<italic>rapid-fire sequences</italic>’); c) gestures that occurred with overlap of their MAU (‘<italic>overlap</italic>’); d) gestures that were separated by at least one second, but occurred within the same 5 second windows (<italic>solitary-gestures-plus-waiting</italic>’). Category d) represents sequences where the choice to add a gesture to the sequence can be most clearly related to the recipient’s actions, while a) through c) at least partially contain sequences where individuals choose gestures in advance. Repetitions of the same gesture were treated depending on structural features (<xref ref-type="bibr" rid="R25">Grund et al., 2023</xref>): rhythmic repetition of the same gesture action (i.e. the same action repeated at the same pace in a continuous movement) was treated as a single occurrence with modifying information, while non-rhythmic repetition of the same gesture action (i.e. the action is repeated but the pace of motion differs and/or is not continuous) was treated as two distinct occurrences. This distinction differs from preprocessing applied to vocal sequences, where any repetition was treated as the same unit (<xref ref-type="bibr" rid="R22">Girard-Buttoz et al., 2022</xref>).</p><p id="P10">For next unit prediction, we broke longer sequences into n-grams of consecutive units. As this constitutes a very small dataset for the higher-order n-grams (e.g., 2,586 bigrams; 1,041 trigrams; and 502 4-grams for the 5-seconds sequences), we used bootstraps of the conditional probabilities to check whether analyses of trigrams and 4-grams could provide any information at all. We sampled sequences with replacement and plotted the range of conditional probabilities for each n-gram (<xref ref-type="fig" rid="F2">Fig. 2</xref>). Because the 4-grams were unstable (most probabilities could take any value between 0 and 1), we dropped them from further analyses, focusing on bigrams and trigrams and acknowledging that the latter show considerable sample error. For the other time-windows, higher-order n-grams were too rare, so we only reported bigrams for these.</p><p id="P11">All analyses were conducted in R and R studio (<xref ref-type="bibr" rid="R49">R Development Core Team &amp; R Core Team, 2020</xref>) using the ‘tidyverse’ (<xref ref-type="bibr" rid="R65">Wickham et al., 2019</xref>) and ‘tidymodels’ (<xref ref-type="bibr" rid="R35">Kuhn &amp; Wickham, 2020</xref>) environments.</p></sec><sec id="S4"><title>Significant transitions</title><p id="P12">To test which transitions occur at higher probabilities than expected, we calculated the conditional probabilities of a consequent unit given one (P(B|A)) and two (P(C|AB)) antecedents, corresponding to bigrams and trigrams. To calculate which transition probabilities occurred at significantly higher probabilities than expected, we compared the observed values to a null model generated by shuffling gesture actions across sequences while controlling for the number of units per sequence. ‘Significant’ transitions in this case are those that have a higher conditional probability than would be expected if units were randomly distributed (observed transition probability &gt; 950 out of 1000 permutations) while accounting for the base probability of each gesture type. We only report transitions that occurred at least five times in the dataset, as conditional probabilities were nearly random for less frequent transitions (<xref ref-type="fig" rid="F2">Fig. 2</xref>). Because we calculate these transitions across four different time-windows, we report and plot transition patterns that are significant and occur at least 5 times in at least three out of the four datasets, with the rationale that these will be the transition patterns that are most robust to researcher decisions. We also specifically compared the solitary-gesture-with waiting network and rapid-fire network, as these potentially represent two distinct systems of combinatoriality with separate use: solitary gestures here are defined by individual elements separated by clear response waiting, so the sequence is clearly influenced by partner responses. Rapid-fire sequences are so fast that signaller planning should precede the partner response. To find higher-order structure of transition patterns, we created the bigram transition network, representing each gesture action as a node and each significant bigram transition that occurred at least 5 times as an edge using igraph in R (<xref ref-type="bibr" rid="R16">Csardi &amp; Nepusz, 2006</xref>). Clusters of elements were determined using the ‘cluster_optimal’ community detection algorithm (Brandes et al., 2008). Loops (conditional probabilities between gesture actions and themselves) are meaningful and relevant here, so we present them graphically.</p><p id="P13">To test whether there are dyads of units with a prescribed order (AB rather than BA), we randomised gesture action within sequences while keeping the identity of actions within sequences the same. ‘Significant’ transitions in this case are those that are observed more often than would be expected if order within sequences did not matter (transition probability &gt; 950 out of 1000 permutations).</p></sec><sec id="S5"><title>Entropy and prediction accuracy</title><p id="P14">We use two measures to quantify how ordered next-element prediction is in this system. These provide similar information about how easy it would be for an individual to predict the next gesture in a sequence without additional information other than the previous gestures, but we promote diversity of methods to rule out that results are based on researcher choices.</p><list list-type="simple" id="L1"><list-item><label>a)</label><p id="P15">Conditional entropies of transition probabilities: across time-windows, we calculated the observed conditional entropy for the bigrams. For the 5-second sequence, we also calculated the conditional entropy for the trigrams, plotting the entropy development (<xref ref-type="bibr" rid="R40">McCowan et al., 1999</xref>). Entropy was calculated using the ‘infotheo’ package in R (<xref ref-type="bibr" rid="R41">Meyer, 2022</xref>) and reported in bits. The entropy was calculated as: <disp-formula id="FD1"><mml:math id="M1"><mml:mrow><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mstyle displaystyle="true"><mml:mi>∑</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mtext>log</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula> Lower entropy indicates less uncertainty and more order. To account for the different sample sizes at different levels and time-windows, we compared the observed entropies with those expected if gesture actions were randomly distributed across sequences by calculating the entropy ratio (observed entropy/random entropy), with values closer to 0 indicating that, knowing previous actions leads to higher predictability and values closer to 1 indicating random distribution. Because some time-windows have much smaller datasets, we show the development of the entropy ratio for randomly selected subsamples of each time-window (<xref ref-type="bibr" rid="R45">Mielke et al., 2021</xref>). To account for the possibility that previous units might reduce uncertainty but that the unit order might not matter in a trigram, we also added ‘alphabetised order’ for AB-C: treating AB and BA the same, we reduce the overall number of possible transitions; if we find lower entropy for this, by level, than for the ordered transitions, we would assume that order does not matter and adds noise. We calculated entropies for 100 bootstraps (sampling with replacement) to give a sense of uncertainty around the entropy values.</p></list-item><list-item><label>b)</label><p id="P16">Naive Bayes classifier: A Naive Bayes classifier is a probabilistic machine learning model based on the Bayes’ theorem (<xref ref-type="bibr" rid="R18">Eisenstein, 2019</xref>). The Naive Bayes classifier assumes that the presence of each gesture action in the antecedent is conditionally independent given the outcome variable: it assumes that A and B are independent, given C. While this independence assumption is often violated in practice, the classifier has been shown to perform well due to its simplicity and efficiency and it is often the first classifier used for natural language processing, before exploring more complex models (<xref ref-type="bibr" rid="R31">Hvitfeldt &amp; Silge, 2022</xref>). For each n-gram, the classifier calculates the posterior probability of the sequence leading to a particular next unit by multiplying the prior probability of the next unit with the conditional likelihood of the next unit given the sequence’s conditional probabilities. The outcome with the highest posterior probability is then assigned to the sequence. The Naive Bayes classifier does not encode sequence information; it learns about the probabilities at location A or B and how they affect C, but not the AB combination. Naive Bayes was implemented using the ‘tidymodels’ package ‘discrim’ in R (<xref ref-type="bibr" rid="R30">Hvitfeldt et al., 2023</xref>).</p></list-item></list><p id="P17">We also attempted to use a Long Short-Term Memory (LSTM) classifier as a type of Recurrent Neural Network architecture designed to address sequential data (<xref ref-type="bibr" rid="R11">Chollet et al., 2022</xref>), but faced considerable overfitting for the training data (a common issue for this type of model; Hvitfeld &amp; Silge, 2022), so we focus on the simpler Naive Bayes classifier instead. For the Markov transitions (A-&gt;B), we used the classifier to compare the predictability of the different time-windows. For the 5-second time-window, we use the classifier to calculate how well the two antecedents independently or in combination predicted the next unit in a trigram. To reduce overfitting, we used a k-fold cross validation approach (<xref ref-type="bibr" rid="R31">Hvitfeldt &amp; Silge, 2022</xref>): the dataset was cut into 50 equally-sized portions, and 98% of data were repeatedly used to predict next units in the remaining 2%. We upsampled the data to account for uneven distributions of outcomes, by randomly adding duplicates of existing cases for each outcome until all levels were equally probable (<xref ref-type="bibr" rid="R31">Hvitfeldt &amp; Silge, 2022</xref>). We report prediction accuracy (correct predictions/all predictions). Within trigrams, we classified the next unit based on the correctly ordered antecedent (AB-C), the previous unit only (B-C), the first unit only (A-C), and the randomly shuffled full antecedent (BA-C). If A and B predict C equally well, we would see that units are clustered within sequences but order does not matter. If B outperforms A, we assume that there is a basic order effect based on Markov transitions (only previous unit matters). If AB outperforms B, we assume that there is some effect of combinations: adding two units together is more informative than just the previous unit. If AB outperforms B and the randomised BA, we have evidence of compositionality: knowing more than one unit and the order of both units adds information about the next unit. If AB and randomised BA perform similarly, we assume that there is an elaboration effect, where seeing both A and B together reduces uncertainty about C but that their exact order does not matter.</p><p id="P18">This comparison of a full model (AB - C) as compared with the two single-antecedent models for the same outcome data as a way to measure predictability differences of different conditions is similar to a full-null model comparison in traditional statistics and is sometimes referred to as an ‘ablation test’ in machine learning (<xref ref-type="bibr" rid="R46">Molnar, 2022</xref>). We use pairwise paired samples t-tests with False Discovery Rate correction (<xref ref-type="bibr" rid="R5">Benjamini &amp; Hochberg, 1995</xref>) between all four conditions to determine whether some antecedents outperform others. Using model error or prediction accuracy as a measure of predictability is not without problems, as it is dependent on the model structure and complexity. Interpreting blackbox models is always difficult (<xref ref-type="bibr" rid="R46">Molnar, 2022</xref>), but we believe that the staggered approach chosen here allows us to gain valuable insights into a system that would otherwise be too complex for analysis, and we interpret all results in combination with the entropy measures and transition probabilities.</p></sec></sec><sec id="S6" sec-type="results"><title>Results</title><sec id="S7"><title>Bootstrap stability</title><p id="P19"><xref ref-type="fig" rid="F2">Figure 2</xref> shows the stability of conditional probabilities at the different n-gram levels for the 5-second sequences (as the one with the largest number of transitions) by plotting the range of probabilities for each transition against the occurrence count of the antecedent unit. <xref ref-type="fig" rid="F2">Figure 2</xref>.<xref ref-type="fig" rid="F4">4</xref> shows that 4-grams are essentially random, while trigrams also contain considerable uncertainty, especially for rarer antecedents. Bigrams for rare units can also take any value, meaning we have to interpret any effects with care and remove any rare combinations.</p><sec id="S8"><title>First Order Transitions Across Time-Windows</title></sec></sec><sec id="S9"><title>Significant N-grams</title><p id="P20">For bigrams, the four time-windows differed considerably in the number of transitions that occurred significantly more than expected at an alpha 0.05 level and that occurred at least 5 times. Longer time-windows produced more significant transitions than shorter time-windows. For the 5-second sequences, 101 out of 476 transitions that were observed in total (21.2%) were significantly more likely than expected under the null model. For the rapid-fire sequences, 57 out of 315 observed transitions (18.1%) were significant. For the overlap sequences, 35 out of 187 observed transitions (18.7%) were significant; for the solitary-gestures-with-waiting sequences, 30 out of 242 transitions (12.3%). There were no transitions that were significant in any of the shorter time-windows that were not also significant in the longest time-window (5-seconds). We found very few indications of deterministic transitions. The only deterministic transition (conditional probability equals 1) occurred in the overlap network, where all Embrace gestures were followed by Bite gestures (but the same bigram was not deterministic in the longer time-windows). The time-windows differed in the number of observed repetitions; in the solitary-gesture-with-waiting sequences, 45% of gestures in sequences were followed by the same gesture, compared to 18% in overlap sequences (because this would necessitate doing the same gesture with different limbs), with the other time-windows (rapid-fire sequences: 27%; 5-second sequences: 34%) in between.</p></sec><sec id="S10"><title>Transition Network Structure</title><p id="P21">We present the transition network based on the concurrence of the different time-windows - edges are included if they are significant in at least 3 out of 4 time-windows, to ensure that the presented network represents one that is largely independent from researcher choices of sequence definition (<xref ref-type="fig" rid="F3">Fig. 3</xref>). In the <xref ref-type="supplementary-material" rid="SD1">Supplementary</xref>, we present a table of all significant transitions for all time-windows. The transition network revealed five clusters containing more than one repeated unit. These were clustered around modality (contact vs non-contact vs object-contact) and context; e.g., one larger play initiation cluster, one grooming initiation cluster, and several for redirecting partner attention. One cluster consisting of <italic>GrabHold, Grab, Bite</italic>, and <italic>Embrace</italic> were contact gestures that allow individuals to hold the partner in close proximity. <italic>Big Loud Scratch, Present</italic>, and <italic>Raise</italic> are all closely associated with grooming requests. This grooming cluster is connected to a cluster associated with repositioning or reorienting the partner (<italic>Push</italic> and <italic>Touch</italic>, with the latter also being a contact extension of <italic>Reach</italic> in a begging context). <italic>Dangle, Swing</italic> and <italic>Object Shake</italic> are all gestures performed while holding a static object. The largest cluster (<italic>HitObject, StompObject, ObjectMove, ObjectShake, Jump</italic>) contains object manipulations that can act as play invitations.</p><p id="P22">When comparing the similarity of which transitions were significant in the different time-windows, the two sequence definitions with response waiting of below 1 second between individual units (rapid-fire sequences, overlap) produced highly similar results. <xref ref-type="fig" rid="F4">Figure 4</xref> shows transitions that were the same in the solitary-gestures-with-waiting and rapid-fire sequence networks (left), those that were significant only for the rapid-fire sequences (middle), and those that were significant only after solitary gestures with response waiting (right).</p><p id="P23">From <xref ref-type="fig" rid="F4">Figure 4</xref>, we see that those transitions that were specific to rapid-fire sequences were primarily focused on object-related gestures and those that can be achieved using different body parts (e.g., <italic>Bite</italic> and <italic>Embrace/Grab</italic>). Solitary-gestures-with-waiting sequences more commonly included gestures that would not be marked as distinct if the response waiting was not observed (e.g., <italic>Grab</italic> and <italic>GrabHold</italic>, or <italic>Touch</italic> and <italic>Push</italic> would probably be marked as one of the options without the behavioural indication of waiting separating them).</p></sec><sec id="S11"><title>Bigram order</title><p id="P24">There were 21 cases where AB was significantly more common than predicted for BA given random distribution within the 5-seconds sequences, see <xref ref-type="table" rid="T2">Table 3</xref>. Those were usually related to contact gesture actions, which tended to follow non-contact gesture actions. Aside from the 7 most directional combinations, in which the conditional probability was more than 5% higher than expected, effect sizes (measured as the difference between the observed and expected conditional probability) here were small. The other time-windows follow the same pattern (with reduced numbers of cases because of smaller sample sizes) with the exception of solitary gesture sequences, in which the effect sizes (how much more likely than expected was a transition) were even smaller, indicating that instances of gesturing clearly differentiated by a period of response waiting did not show order effects.</p></sec><sec id="S12"><title>Entropy</title><p id="P25">In this analysis, we compared the information content (measured in bits) of conditional probabilities at different levels with what would be expected given random distribution of units, represented by the ratio of the two. Because of the sample size differences for the four time-windows, we present a graph that shows the entropy of the first-order transition for different subsamples of each time-window (<xref ref-type="fig" rid="F5">Figure 5</xref>). The change in entropy with increasing sample size was similar for the different time-windows, with all of them showing similar ratios of observed and expected entropy (around 0.5-0.6) already with relatively small samples. An entropy ratio of 0.5 indicates that the observed entropy is half the expected entropy under the null model of random distribution of units, a sign of a mix of predictability and unpredictability at this level. The overlap sequences and sequences containing solitary-gestures-with-waiting, as the most restrictive definitions of what constituted a sequence, showed lower entropy than the other time-windows, but the effect was small. <xref ref-type="table" rid="T3">Table 4</xref> contains the observed and expected entropies for the full data for all time-windows.</p></sec><sec id="S13"><title>Next element prediction</title><p id="P26">In this analysis, we first compared the prediction accuracy for the four time-windows, assuming first-order transitions. <xref ref-type="fig" rid="F6">Figure 6</xref> shows the prediction accuracy of k-fold cross validation with k = 50 using the Naive Bayes classifier (i.e., 98% of the data is used for training data), applied to subsets of each time-window of different sample sizes to account for the differences between datasets. We observe that increased sample sizes improved predictions, but that this trend was not uniform. It was very pronounced for the solitary gesture sequences, which also had the highest prediction accuracy despite the smallest overall sample size (accuracy = 0.31), probably because of a high level of repetitions. The other time-windows had accuracies that were comparable to each other for similar sample sizes (5-seconds: 0.29; overlap: 0.24; rapid-fire: 0.21). That means that even without further information about the context or individuals or other modalities, a basic classifier with a small training set could achieve almost 30% correct classifications of the next unit in a sequence based on basic occurrence and transition probabilities.</p><sec id="S14"><title>Second Order Transitions For 5-Seconds Sequences</title></sec></sec><sec id="S15"><title>Significant N-Grams</title><p id="P27">For the 5-seconds sequences and focusing on trigrams, 31 out of 593 transitions that were ever observed were significant and occurred over 5 times (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Material</xref>). All significant trigrams contained at least one occurrence of the consequent in the antecedent (ABA or BAA), and often both antecedent units were the same as the consequent (AAA). All significant antecedents in the trigrams were also significant bigrams.</p></sec><sec id="S16"><title>Entropy</title><p id="P28">For the 5-seconds sequences, we could also analyse trigrams, including the entropy under the assumption that order does not matter (by alphabetising antecedents before calculating probabilities, so that AB and BA were considered together). <xref ref-type="fig" rid="F7">Figure 7</xref> shows that at the bigram level (one antecedent known), the observed entropy was lower than would be expected, indicating that the observed transition probabilities indeed reduced uncertainty about which unit was seen next. For trigrams, this effect was less pronounced, with a higher entropy ratio when the order of antecedent units was considered; probably due to the small sample size for most trigrams. However, when removing the order information from the antecedent (thereby also increasing the sample for each distinct antecedent), we saw that knowing the antecedent reduces the entropy as compared to the random distribution. This outcome indicates that, at least at the sample sizes observed here, order adds noise rather than reducing it.</p></sec><sec id="S17"><title>Next Element Prediction</title><p id="P29">We compare the prediction accuracies of models containing only the unit two steps removed from the consequent (A), the immediately preceding unit (B), both units together (AB), and both units together with order information removed through alphabetising them (rAB). As a reminder, if A outperforms B, then we assume Markov effects. If AB outperforms B alone, we count this as evidence for possible composition effects. If AB and rAB perform similarly, we consider this evidence for an elaboration effect, where order does not matter but having two gestures reduces uncertainty about the subsequent order. If AB outperforms rAB, we consider that order matters for the purposes of prediction.</p><p id="P30">In <xref ref-type="fig" rid="F8">Figure 8</xref> and <xref ref-type="table" rid="T4">Table 4</xref>, we see that B (accuracy: 0.185) does not significantly outperform A (accuracy: 0.17). AB (0.27) and rAB (0.25) performed better than A or B alone, and AB slightly outperforms rAB.</p><p id="P31">When testing which gesture actions were more accurately classified based on the two preceding units, rather than on one preceding unit alone, <xref ref-type="fig" rid="F9">Figure 9</xref> shows that there are a small number of units where the single predictor performed more accurately (e.g., <italic>Head Stand, Throw Object, Stomp Other</italic>, and <italic>Present</italic>); some where neither antecedent had any predictive power; some showed the same performance (these were all very rare); some where the single unit had no predictive power but the combination performed quite well; and some where both showed some accuracy but accounting for more antecedent units improved performance.</p></sec></sec><sec id="S18" sec-type="discussion"><title>Discussion</title><p id="P32">In this study, we analysed transition patterns in Eastern chimpanzee gesture sequences. Combinatorial capacities in naturally occurring communication sequences have recently been reported in a variety of species, but with a strong focus on vocalisations and song-like communication</p><p id="P33">(<xref ref-type="bibr" rid="R1">Allen et al., 2019</xref>). For chimpanzees, reliable associations between units have been described in vocal (<xref ref-type="bibr" rid="R22">Girard-Buttoz et al., 2022</xref>; <xref ref-type="bibr" rid="R37">Leroux et al., 2021</xref>) and multimodal (<xref ref-type="bibr" rid="R48">Oña et al., 2019</xref>) sequences. Here, we showed that sequences in the chimpanzee gestural system, characterised by a larger number of meaning-bearing and intentional signals, were generally short and there were many rare units and rare transitions between units that did not pass our thresholds, mirroring earlier analyses with a subset of the current dataset (<xref ref-type="bibr" rid="R26">Hobaiter &amp; Byrne, 2011a</xref>).</p><p id="P34">We first asked whether there were non-random transitions between elements. We identified a number of transitions that occurred at higher-than-expected frequencies, with sample sizes limiting us to statements about combinations of one or two antecedent units (and considerable variation for the latter). Around one third of the 101 significant first-order transitions in a 5-second time-window were repetitions of the same unit, indicating that persistence or redundancy constitute an important part of chimpanzee gesture sequences. This outcome is expected, given that intentionality is based on persistence as a core property (<xref ref-type="bibr" rid="R4">Bates et al., 1975</xref>; <xref ref-type="bibr" rid="R36">Leavens et al., 2005</xref>), so what is considered a ‘gesture’ here (as compared to other actions) is biased towards repetitions. In both earlier studies (<xref ref-type="bibr" rid="R27">Hobaiter &amp; Byrne, 2011b</xref>) and in our own findings the likelihood of a gestural unit being repeated increased strongly after a period of response waiting.</p><p id="P35">When combining all time-windows, we identified a core network of transitions between units that could be detected independently of the time-window in question. For some element pairs, only one direction was significantly more likely than expected, potentially because of physical constraints of actions: for example, individuals <italic>Grab, GrabHold</italic>, or <italic>Embrace</italic> a partner (establishing sustained physical contact) and then <italic>Bite</italic> them; they would also first <italic>Reach</italic> and then <italic>Touch</italic>, and first <italic>Dangle</italic> from a branch, then <italic>HitOther</italic>, then <italic>StompOther</italic>. In other cases, there was a higher probability that one action precedes, but both directions had significantly higher probabilities than chance, for example: <italic>Present, BigLoudScratch</italic>, and <italic>Raise</italic> (all common in grooming initiations) were observed in any bigram order, even though <italic>BigLoudScratch</italic> usually followed the <italic>Raise</italic>. Observed clusters of significant transitions were a mix of repetitions (loops in the network), physical contingencies (e.g., object-related gestures co-occurring at high rates), potentially synonymous actions (e.g., <italic>Grab, GrabHold</italic>, and <italic>Embrace</italic> preceding the same gesture), and shared meaning (<italic>Present, BigLoudScratch</italic>, and <italic>Raise</italic> as grooming gestures).</p><p id="P36">Did unit order matter? We identified a small number of two-element dyads where transition direction seemed to significantly matter, but they either had very small effect sizes (probability increase for one direction over that expected of 5% or below) or the order effect could be explained with relative ease through physical affordances (individuals first grab a partner to pull them closer, then bite them). In the entropy measures, the correct order was noise that increased entropy as compared to the alphabetised order (indicating that it was the co-occurrence that mattered, not the production order). For the prediction models, the correctly ordered combination performed better than the alphabetised order, low level evidence that order information improved predictability. Thus, it appears as if combinations of gestures serve to disambiguate the subsequent elements in a system in which there are very large numbers of units, each associated with different sets of goals (<xref ref-type="bibr" rid="R24">Graham et al., 2018</xref>, <xref ref-type="bibr" rid="R23">2020</xref>). For example, <italic>Big Loud Scratch</italic>, a very common gesture for which we can be fairly certain about the transition probabilities, was followed by 15 different gestures when considered by itself, but only by three when it was associated with <italic>Present</italic>; mainly by repetitions of <italic>Present</italic> or <italic>Big Loud Scratch</italic>. However, <italic>Big Loud Scratch</italic>/<italic>Present</italic> and <italic>Present</italic>/<italic>Big Loud Scratch</italic> did not lead to different consequent units, showing either that order did not matter for next-unit prediction or that applying strict linear temporal structure to the definition of gesture sequences was insufficient. Unlike in most vocalisations, multiple gesture actions can be performed simultaneously and gestures may not be restricted to the linear ordered structure that we assume for animal vocal communication. Other non-vocal communication systems, such as signed languages (<xref ref-type="bibr" rid="R2">Armstrong et al., 1994</xref>; <xref ref-type="bibr" rid="R52">Sandler &amp; Lillo-Martin, 2006</xref>; <xref ref-type="bibr" rid="R58">Stokoe, 1980</xref>), incorporate spatial dimensions into their syntactic structure, but it remains speculative whether the same is true for chimpanzee gestures.</p><p id="P37">Regarding the predictability of the system as a whole, information about previous gestures reduced uncertainty about consequent gestures, with two antecedent gestures improving prediction accuracy and entropy ratios as compared to a single one, similarly to the pattern found in a recent study of marmoset call combinations (<xref ref-type="bibr" rid="R9">Bosshard et al., 2024</xref>). We found conflicting evidence on whether the order of the antecedent units mattered: alphabetising the order of AB in a bigram antecedent led to worse predictions, but reduced the entropy ratio. This result could be explained by methodological differences; the Naive Bayes classifier uses the joint conditional probabilities of the two antecedents in their position predicting the consequent, while the entropy uses the conditional probability of the combination itself while disregarding the probability of each individual antecedent. At the same time, we found a remarkable level of predictability: even a basic classifier like Naive Bayes, using the relatively small datasets available (as compared to typical machine learning datasets), achieved prediction accuracy of around 30% for bigrams, well above chance. Encouragingly, bigram entropies could be established accurately with relatively small sample sizes for all time windows and were remarkably consistent across the different datasets. This level of robustness, indicates that the choice of time window had only minor impacts on overall predictability of the resulting sequence system, and that system-level parameters are less dependent on researcher choices than transition patterns. It also suggests that this approach would be reliable in the smaller datasets typically available for most studies of ape gesture. In terms of predictability, the two sequence definitions with the most restrictive rules (i.e., gestures have to overlap or gestures have to be separated by 1 second) were the most predictable using both entropy and classification accuracy, because of their much smaller transition networks.</p><p id="P38">Comparing different time-windows for the bigram transitions revealed an impact of methodological decisions, with potential biological implications. Given different traditions within the field for defining what constitutes a sequence based on temporal and/or behavioural criteria (with a mix of different a priori or data-driven time thresholds and the inclusion or exclusion of sender and receiver behaviour in delineating sequences), this is potentially worrisome. In many cases the investigation of group- or species-level comparisons in ape signalling is reliant on comparison across samples and studies where methodological decisions are not necessarily similar or transparent (<xref ref-type="bibr" rid="R51">Rodrigues et al., 2021</xref>). Our findings suggest that direct comparisons between studies that involve different approaches could be problematically biased. As in any signal detection task, we face a sensitivity-specificity trade-off: reducing restrictions (by considering larger time-windows) increases the sample size and reduces false negative results, but potentially leads to more false positives and the inclusion of noise, for example: different gesture sequences with different goals being grouped together for longer thresholds. Using a time-based approach based purely on gesture start and end times (using 5 seconds from the end of the previous MAU) created more and longer sequences than an approach based on response waiting or overlap, with considerably more significant transitions between units. Given sample size and time constraints in many studies, creating sequences based on temporal thresholds without regard for sender or recipient behaviour will remain an important approach for many researchers. Reassuringly, all transitions that were identified as significant in either of the more restrictive time-windows were also represented in this largest time-window. Using a time-based approach with 1 second intervals was nearly indistinguishable from the rapid-fire sequences and was therefore dropped, indicating that researchers who do not code behavioural markers to distinguish sequences can nevertheless rely on time-based approaches and get similar results. Approaches that use strong a priori reasoning for restricting the time-window (e.g., based on recipient behavioural markers) benefit from higher resolution and potentially allow researchers to differentiate structural dimensions of sequences. In our case, the difference between the two most distinct time-windows (rapid-fire and solitary gestures) could partially be explained by the inability for coders to distinguish certain gesture types if they were to occur in rapid sequence. In the absence of strong data- or theory-driven thresholds or markers, we advocate the use of multiverse analysis. Doing so allows studies using different thresholds that are not directly comparable to include multiple thresholds and report outcomes, increasing replicability and reuse (<xref ref-type="bibr" rid="R29">Hoffmann et al., 2021</xref>; <xref ref-type="bibr" rid="R42">Mielke, 2023</xref>; <xref ref-type="bibr" rid="R57">Steegen et al., 2016</xref>).</p><p id="P39">In comparison to vocal sequences in Western chimpanzees (<xref ref-type="bibr" rid="R22">Girard-Buttoz et al., 2022</xref>), gesture sequences were made up of considerably more independent units with much lower occurrence frequencies, partially as a result of methodological decisions to lump vocalisations, making direct comparisons difficult. For example, the least common unit in the vocal study occurred 78 times, which is more common than half of our gesture actions, impacting the reliability of the calculated conditional probabilities. However, following the definitions provided by <xref ref-type="bibr" rid="R22">Girard-Buttoz et al. (2022)</xref>, the gesture system showed high levels of flexibility (each unit is followed by multiple others); limited ordering effects (each unit is associated with a small subset of other units, with a few cases of non-random order within bigrams); and recombination (two antecedent units improve prediction accuracy for subsequent units over single units, indicating that they are repeatedly used with additional units in sequences). In the latter case, the strongest rule for trigrams was repetition: all significant two-unit antecedent combinations contained the consequent as either one or both antecedent units; sequences often contain redundant information or persistence, perhaps in response to recipients who do not react in the expected or desired way. Overall, we remain cautious when comparing sequences of different signal types as these may stem from repertoires determined using different criteria and thresholds, which in turn may affect sequence number and diversity. We encourage researchers working on different species and signals to apply transparent methods and find common ground that allows more comparable approaches.</p><p id="P40">More and more animal species and communication systems show rudimentary combinatorial capacities, with some indications of basic syntactic structures (<xref ref-type="bibr" rid="R7">Berwick et al., 2011</xref>). Largely confirming previous results on great ape gestures (<xref ref-type="bibr" rid="R21">Genty &amp; Byrne, 2010</xref>; <xref ref-type="bibr" rid="R23">Graham et al., 2020</xref>; <xref ref-type="bibr" rid="R26">Hobaiter &amp; Byrne, 2011a</xref>; <xref ref-type="bibr" rid="R39">Liebal et al., 2004</xref>) and mirroring vocal research (<xref ref-type="bibr" rid="R37">Leroux et al., 2021</xref>) using a larger dataset, we showed that chimpanzee use short sequences of gestures, and that transitions in the sequences are not randomly distributed. Significant transition patterns seemed to be driven by a mix of affordances (e.g., performing different actions with an object) and shared meaning (e.g., grooming or play initiations). We found that these rules make the system more predictable. At the same time, while we found biologically-relevant order-effects, these were most likely shaped by the ease of signal production, indicating that for the majority of sequences linear-ordering was unlikely to shape information. This result does not preclude compositional or syntax-like structure, but indicates that a more targeted approach than repertoire-wide transition probabilities is necessary to move from combination to composition. The combination of elements in sequence to expand the information-bearing capacity of a system is far more likely to be relevant in smaller sets of signal units. The chimpanzee gesture system is currently estimated to contain somewhere between 70 and 140 units (<xref ref-type="bibr" rid="R43">Mielke et al., 2024</xref>), potentially an order of magnitude greater than their vocal repertoire. The information content of meaning-bearing ape gestures is flexible — with most units employed to express 2-3 meanings (<xref ref-type="bibr" rid="R24">Graham et al., 2018</xref>; <xref ref-type="bibr" rid="R28">Hobaiter &amp; Byrne, 2014</xref>). As a result, the structured combination of signals in gestural sequences may serve to disambiguate meaning in a specific <italic>occasion</italic> of use. The ability for the same signal to encode different meanings on different occasions has been suggested as a critical component of language use. As experimental studies of gestural communication are near impossible, incorporating meaning into observational analyses of sequence production will provide a valuable next step to the approach taken here. Formalising the syntactic structures present in signed languages required linguists to reconsider the nature of the structural dimensions they described (<xref ref-type="bibr" rid="R58">Stokoe, 1980</xref>), fully describing the combinatorial properties of ape gesture may require a similar reimagining of the ways in which non-human gestural systems encode information.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary</label><media xlink:href="EMS200145-supplement-Supplementary.docx" mimetype="application" mime-subtype="vnd.openxmlformats-officedocument.wordprocessingml.document" id="d17aAcEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S19"><title>Acknowledgements</title><p>AM was funded by a Leverhulme Early Career Fellowship. CH, GB, KEG, and AS were supported by funding from the European Research Council under Gestural Origins Grant No: 802719. KS and CW were supported by funding from the European Research Council under Grant No: ERC_CoG 2016_724608. We thank all the staff of the Budongo Conservation Field Station, its founder Vernon Reynolds, and the Royal Zoological Society of Scotland who provide core funding. We thank the directors of the Kibale Chimpanzee Project for permission to use video data archives. We thank the Uganda Wildlife Authority, the National Forestry Authority, the President’s Office, and the Uganda National Council for Science and Technology for providing research permits and permissions to conduct research in Budongo, Kalinzu, and Kanyawara. The Issa project (GMERC) is grateful for long-term support provided from the UCSD/Salk Center for Academic Research and Training in Anthropogeny (CARTA). We thank the Tanzanian Wildlife Research Institute (TAWIRI), Commission for Science and Technology (COSTECH), and Tanganyika District for permission to conduct research in the Issa Valley. We thank all the field assistants and local staff across field sites for the decades of work that make this kind of research possible.</p></ack><sec id="S20" sec-type="data-availability"><title>Data availability</title><p id="P41">All scripts and data can be found here: <ext-link ext-link-type="uri" xlink:href="https://github.com/AlexMielke1988/Mielke-et-al-Ngrams">https://github.com/AlexMielke1988/Mielke-et-al-Ngrams</ext-link>. Data are provided as prepared sequences only to allow for replication; for the full data, please contact the first or last author.</p></sec><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>JA</given-names></name><name><surname>Garland</surname><given-names>EC</given-names></name><name><surname>Dunlop</surname><given-names>RA</given-names></name><name><surname>Noad</surname><given-names>MJ</given-names></name></person-group><article-title>Network analysis reveals underlying syntactic features in a vocally learnt mammalian display, humpback whale song</article-title><source>Proceedings of the Royal Society B: Biological Sciences</source><year>2019</year><volume>286</volume><issue>1917</issue><pub-id pub-id-type="pmcid">PMC6939930</pub-id><pub-id pub-id-type="pmid">31847766</pub-id><pub-id pub-id-type="doi">10.1098/rspb.2019.2014</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Armstrong</surname><given-names>DF</given-names></name><name><surname>Stokoe</surname><given-names>WC</given-names></name><name><surname>Wilcox</surname><given-names>SE</given-names></name></person-group><article-title>Signs of the Origin of Syntax</article-title><source>Current Anthropology</source><year>1994</year><volume>35</volume><issue>4</issue><fpage>349</fpage><lpage>368</lpage><pub-id pub-id-type="doi">10.1086/204290</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aychet</surname><given-names>J</given-names></name><name><surname>Blois-Heulin</surname><given-names>C</given-names></name><name><surname>Lemasson</surname><given-names>A</given-names></name></person-group><article-title>Sequential and network analyses to describe multiple signal use in captive mangabeys</article-title><source>Animal Behaviour</source><year>2021</year><volume>182</volume><fpage>203</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1016/J.ANBEHAV.2021.09.005</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bates</surname><given-names>E</given-names></name><name><surname>Camaioni</surname><given-names>L</given-names></name><name><surname>Volterra</surname><given-names>V</given-names></name></person-group><article-title>The Acquisition of Performatives Prior to Speech</article-title><source>Merrill-Palmer Quarterly of Behavior and Development</source><year>1975</year><volume>21</volume><issue>3</issue><fpage>205</fpage><lpage>226</lpage></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname><given-names>Y</given-names></name><name><surname>Hochberg</surname><given-names>Y</given-names></name></person-group><article-title>Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing</article-title><source>Journal of the Royal Statistical Society: Series B (Methodological)</source><year>1995</year><volume>57</volume><issue>1</issue><fpage>289</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1111/j.2517-6161.1995.tb02031.x</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berthet</surname><given-names>M</given-names></name><name><surname>Coye</surname><given-names>C</given-names></name><name><surname>Dezecache</surname><given-names>G</given-names></name><name><surname>Kuhn</surname><given-names>J</given-names></name></person-group><article-title>Animal linguistics: A primer</article-title><source>Biological Reviews</source><year>2023</year><volume>98</volume><issue>1</issue><fpage>81</fpage><lpage>98</lpage><pub-id pub-id-type="pmcid">PMC10091714</pub-id><pub-id pub-id-type="pmid">36189714</pub-id><pub-id pub-id-type="doi">10.1111/brv.12897</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berwick</surname><given-names>RC</given-names></name><name><surname>Okanoya</surname><given-names>K</given-names></name><name><surname>Beckers</surname><given-names>GJL</given-names></name><name><surname>Bolhuis</surname><given-names>JJ</given-names></name></person-group><article-title>Songs to syntax: The linguistics of birdsong</article-title><source>Trends in Cognitive Sciences</source><year>2011</year><volume>15</volume><issue>3</issue><fpage>113</fpage><lpage>121</lpage><pub-id pub-id-type="pmid">21296608</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bortolato</surname><given-names>T</given-names></name><name><surname>Mundry</surname><given-names>R</given-names></name><name><surname>Wittig</surname><given-names>RM</given-names></name><name><surname>Girard-Buttoz</surname><given-names>C</given-names></name><name><surname>Crockford</surname><given-names>C</given-names></name></person-group><article-title>Slow development of vocal sequences through ontogeny in wild chimpanzees (Pan troglodytes verus)</article-title><source>Developmental Science</source><year>2023</year><volume>26</volume><issue>4</issue><elocation-id>e13350</elocation-id><pub-id pub-id-type="pmid">36440660</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bosshard</surname><given-names>AB</given-names></name><name><surname>Burkart</surname><given-names>JM</given-names></name><name><surname>Merlo</surname><given-names>P</given-names></name><name><surname>Cathcart</surname><given-names>C</given-names></name><name><surname>Townsend</surname><given-names>SW</given-names></name><name><surname>Bickel</surname><given-names>B</given-names></name></person-group><article-title>Beyond bigrams: Call sequencing in the common marmoset (Callithrix jacchus) vocal system</article-title><source>Royal Society Open Science</source><year>2024</year><volume>11</volume><issue>11</issue><elocation-id>240218</elocation-id><pub-id pub-id-type="pmcid">PMC11537759</pub-id><pub-id pub-id-type="pmid">39507993</pub-id><pub-id pub-id-type="doi">10.1098/rsos.240218</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bosshard</surname><given-names>AB</given-names></name><name><surname>Leroux</surname><given-names>M</given-names></name><name><surname>Lester</surname><given-names>NA</given-names></name><name><surname>Bickel</surname><given-names>B</given-names></name><name><surname>Stoll</surname><given-names>S</given-names></name><name><surname>Townsend</surname><given-names>SW</given-names></name></person-group><article-title>From collocations to call-ocations: Using linguistic methods to quantify animal call combinations</article-title><source>Behavioral Ecology and Sociobiology</source><year>2022</year><volume>76</volume><issue>9</issue><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="pmcid">PMC9395491</pub-id><pub-id pub-id-type="pmid">36034316</pub-id><pub-id pub-id-type="doi">10.1007/s00265-022-03224-3</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chollet</surname><given-names>F</given-names></name><name><surname>Kalinowski</surname><given-names>T</given-names></name><name><surname>Allaire</surname><given-names>JJ</given-names></name></person-group><chapter-title>Deep Learning in R</chapter-title><source>R-bloggers</source><edition>2nd ed</edition><year>2022</year><issue>7080</issue><publisher-name>Manning Publications</publisher-name><comment><ext-link ext-link-type="uri" xlink:href="https://www.manning.com/books/deep-learning-with-r">https://www.manning.com/books/deep-learning-with-r</ext-link></comment></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clarke</surname><given-names>E</given-names></name><name><surname>Reichard</surname><given-names>UH</given-names></name><name><surname>Zuberbühler</surname><given-names>K</given-names></name></person-group><article-title>The Syntax and Meaning of Wild Gibbon Songs</article-title><source>PLOS ONE</source><year>2006</year><volume>1</volume><issue>1</issue><fpage>e73</fpage><pub-id pub-id-type="pmcid">PMC1762393</pub-id><pub-id pub-id-type="pmid">17183705</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0000073</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Coye</surname><given-names>C</given-names></name><name><surname>Townsend</surname><given-names>SW</given-names></name><name><surname>Lemasson</surname><given-names>A</given-names></name></person-group><chapter-title>From animal communication to linguistics and back: Insight from combinatorial abilities in monkeys and birds</chapter-title><source>Origins of Human Language: Continuities and Discontinuities with Nonhuman Primates</source><year>2018</year><publisher-name>Peter Lang International Academic Publishers</publisher-name></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crockford</surname><given-names>C</given-names></name><name><surname>Boesch</surname><given-names>C</given-names></name></person-group><article-title>Call Combinations in Wild Chimpanzees</article-title><source>Behaviour</source><year>2005</year><volume>142</volume><issue>4</issue><fpage>397</fpage><lpage>421</lpage></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crockford</surname><given-names>C</given-names></name><name><surname>Gruber</surname><given-names>T</given-names></name><name><surname>Zuberbühler</surname><given-names>K</given-names></name></person-group><article-title>Chimpanzee quiet hoo variants differ according to context</article-title><source>Royal Society Open Science</source><year>2018</year><volume>5</volume><issue>5</issue><pub-id pub-id-type="pmcid">PMC5990785</pub-id><pub-id pub-id-type="pmid">29892396</pub-id><pub-id pub-id-type="doi">10.1098/rsos.172066</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Csardi</surname><given-names>G</given-names></name><name><surname>Nepusz</surname><given-names>T</given-names></name></person-group><article-title>The igraph software package for complex network research</article-title><source>InterJournal: Vol Complex Sy</source><year>2006</year><fpage>1695</fpage></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dutour</surname><given-names>M</given-names></name><name><surname>Lengagne</surname><given-names>T</given-names></name><name><surname>Léna</surname><given-names>J-P</given-names></name></person-group><article-title>Syntax manipulation changes perception of mobbing call sequences across passerine species</article-title><source>Ethology</source><year>2019</year><volume>125</volume><issue>9</issue><fpage>635</fpage><lpage>644</lpage><pub-id pub-id-type="doi">10.1111/eth.12915</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Eisenstein</surname><given-names>J</given-names></name></person-group><source>Introduction to Natural Language Processing</source><year>2019</year><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engesser</surname><given-names>S</given-names></name><name><surname>Ridley</surname><given-names>AR</given-names></name><name><surname>Townsend</surname><given-names>SW</given-names></name></person-group><article-title>Meaningful call combinations and compositional processing in the southern pied babbler</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2016</year><volume>113</volume><issue>21</issue><fpage>5976</fpage><lpage>5981</lpage><pub-id pub-id-type="pmcid">PMC4889383</pub-id><pub-id pub-id-type="pmid">27155011</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1600970113</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garland</surname><given-names>EC</given-names></name><name><surname>Goldizen</surname><given-names>AW</given-names></name><name><surname>Rekdahl</surname><given-names>ML</given-names></name><name><surname>Constantine</surname><given-names>R</given-names></name><name><surname>Garrigue</surname><given-names>C</given-names></name><name><surname>Hauser</surname><given-names>ND</given-names></name><name><surname>Poole</surname><given-names>MM</given-names></name><name><surname>Robbins</surname><given-names>J</given-names></name><name><surname>Noad</surname><given-names>MJ</given-names></name></person-group><article-title>Dynamic Horizontal Cultural Transmission of Humpback Whale Song at the Ocean Basin Scale</article-title><source>Current Biology</source><year>2011</year><volume>21</volume><issue>8</issue><fpage>687</fpage><lpage>691</lpage><pub-id pub-id-type="pmid">21497089</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Genty</surname><given-names>E</given-names></name><name><surname>Byrne</surname><given-names>RW</given-names></name></person-group><article-title>Why do gorillas make sequences of gestures?</article-title><source>Animal Cognition</source><year>2010</year><volume>13</volume><issue>2</issue><fpage>287</fpage><lpage>301</lpage><pub-id pub-id-type="pmid">19649664</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Girard-Buttoz</surname><given-names>C</given-names></name><name><surname>Zaccarella</surname><given-names>E</given-names></name><name><surname>Bortolato</surname><given-names>T</given-names></name><name><surname>Friederici</surname><given-names>AD</given-names></name><name><surname>Wittig</surname><given-names>RM</given-names></name><name><surname>Crockford</surname><given-names>C</given-names></name></person-group><article-title>Chimpanzees produce diverse vocal sequences with ordered and recombinatorial properties</article-title><source>Communications Biology</source><year>2022</year><volume>5</volume><issue>1</issue><pub-id pub-id-type="pmcid">PMC9110424</pub-id><pub-id pub-id-type="pmid">35577891</pub-id><pub-id pub-id-type="doi">10.1038/s42003-022-03350-8</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graham</surname><given-names>KE</given-names></name><name><surname>Furuichi</surname><given-names>T</given-names></name><name><surname>Byrne</surname><given-names>RW</given-names></name></person-group><article-title>Context, not sequence order, affects the meaning of bonobo (Pan paniscus) gestures</article-title><source>Gesture</source><year>2020</year><volume>19</volume><issue>2–3</issue><fpage>335</fpage><lpage>364</lpage><pub-id pub-id-type="doi">10.1075/GEST.19028.GRA/CITE/REFWORKS</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graham</surname><given-names>KE</given-names></name><name><surname>Hobaiter</surname><given-names>C</given-names></name><name><surname>Ounsley</surname><given-names>J</given-names></name><name><surname>Furuichi</surname><given-names>T</given-names></name><name><surname>Byrne</surname><given-names>RW</given-names></name></person-group><article-title>Bonobo and chimpanzee gestures overlap extensively in meaning</article-title><source>PLoS Biology</source><year>2018</year><volume>16</volume><issue>2</issue><pub-id pub-id-type="pmcid">PMC5828348</pub-id><pub-id pub-id-type="pmid">29485994</pub-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2004825</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grund</surname><given-names>C</given-names></name><name><surname>Badihi</surname><given-names>G</given-names></name><name><surname>Graham</surname><given-names>KE</given-names></name><name><surname>Safryghin</surname><given-names>A</given-names></name><name><surname>Hobaiter</surname><given-names>C</given-names></name></person-group><article-title>GesturalOrigins: A bottom-up framework for establishing systematic gesture data across ape species</article-title><source>Behavior Research Methods</source><year>2023</year><pub-id pub-id-type="pmcid">PMC10830607</pub-id><pub-id pub-id-type="pmid">36922450</pub-id><pub-id pub-id-type="doi">10.3758/s13428-023-02082-9</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hobaiter</surname><given-names>C</given-names></name><name><surname>Byrne</surname><given-names>RW</given-names></name></person-group><article-title>Serial gesturing by wild chimpanzees: Its nature and function for communication</article-title><source>Animal Cognition</source><year>2011a</year><volume>14</volume><issue>6</issue><fpage>827</fpage><lpage>838</lpage><pub-id pub-id-type="pmid">21562816</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hobaiter</surname><given-names>C</given-names></name><name><surname>Byrne</surname><given-names>RW</given-names></name></person-group><article-title>The gestural repertoire of the wild chimpanzee</article-title><source>Animal Cognition</source><year>2011b</year><volume>14</volume><issue>5</issue><fpage>745</fpage><lpage>767</lpage><pub-id pub-id-type="pmid">21533821</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hobaiter</surname><given-names>C</given-names></name><name><surname>Byrne</surname><given-names>RW</given-names></name></person-group><article-title>The meanings of chimpanzee gestures</article-title><source>Current Biology</source><year>2014</year><volume>24</volume><issue>14</issue><fpage>1596</fpage><lpage>1600</lpage><pub-id pub-id-type="pmid">24998524</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoffmann</surname><given-names>S</given-names></name><name><surname>Schönbrodt</surname><given-names>F</given-names></name><name><surname>Elsas</surname><given-names>R</given-names></name><name><surname>Wilson</surname><given-names>R</given-names></name><name><surname>Strasser</surname><given-names>U</given-names></name><name><surname>Boulesteix</surname><given-names>A-L</given-names></name></person-group><article-title>The multiplicity of analysis strategies jeopardizes replicability: Lessons learned across disciplines</article-title><source>Royal Society Open Science</source><year>2021</year><volume>8</volume><issue>4</issue><elocation-id>rsos.201925</elocation-id><pub-id pub-id-type="pmcid">PMC8059606</pub-id><pub-id pub-id-type="pmid">33996122</pub-id><pub-id pub-id-type="doi">10.1098/rsos.201925</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Hvitfeldt</surname><given-names>E</given-names></name><name><surname>Kuhn</surname><given-names>M</given-names></name><name><surname>Software</surname><given-names>P</given-names></name></person-group><collab>PBC</collab><source>discrim: Model Wrappers for Discriminant Analysis (Version 1.0.1)</source><year>2023</year><comment>[Computer software] <ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/web/packages/discrim/index.html">https://cran.r-project.org/web/packages/discrim/index.html</ext-link></comment></element-citation></ref><ref id="R31"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hvitfeldt</surname><given-names>E</given-names></name><name><surname>Silge</surname><given-names>J</given-names></name></person-group><source>Supervised Machine Learning for Text Analysis in R</source><edition>1st ed</edition><year>2022</year><publisher-name>CRC Press</publisher-name></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kershenbaum</surname><given-names>A</given-names></name><name><surname>Blumstein</surname><given-names>DT</given-names></name><name><surname>Roch</surname><given-names>MA</given-names></name><name><surname>Akçay</surname><given-names>Ç</given-names></name><name><surname>Backus</surname><given-names>G</given-names></name><name><surname>Bee</surname><given-names>MA</given-names></name><name><surname>Bohn</surname><given-names>K</given-names></name><name><surname>Cao</surname><given-names>Y</given-names></name><name><surname>Carter</surname><given-names>G</given-names></name><name><surname>Cäsar</surname><given-names>C</given-names></name><name><surname>Coen</surname><given-names>M</given-names></name><etal/></person-group><article-title>Acoustic sequences in non-human animals: A tutorial review and prospectus</article-title><source>Biological Reviews</source><year>2016</year><volume>91</volume><issue>1</issue><fpage>13</fpage><lpage>52</lpage><pub-id pub-id-type="pmcid">PMC4444413</pub-id><pub-id pub-id-type="pmid">25428267</pub-id><pub-id pub-id-type="doi">10.1111/brv.12160</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kershenbaum</surname><given-names>A</given-names></name><name><surname>Bowles</surname><given-names>AE</given-names></name><name><surname>Freeberg</surname><given-names>TM</given-names></name><name><surname>Jin</surname><given-names>DZ</given-names></name><name><surname>Lameira</surname><given-names>AR</given-names></name><name><surname>Bohn</surname><given-names>K</given-names></name></person-group><article-title>Animal vocal sequences: Not the Markov chains we thought they were</article-title><source>Proceedings of the Royal Society B: Biological Sciences</source><year>2014</year><volume>281</volume><issue>1792</issue><pub-id pub-id-type="pmcid">PMC4150325</pub-id><pub-id pub-id-type="pmid">25143037</pub-id><pub-id pub-id-type="doi">10.1098/rspb.2014.1370</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kershenbaum</surname><given-names>A</given-names></name><name><surname>Ilany</surname><given-names>A</given-names></name><name><surname>Blaustein</surname><given-names>L</given-names></name><name><surname>Geffen</surname><given-names>E</given-names></name></person-group><article-title>Syntactic structure and geographical dialects in the songs of male rock hyraxes</article-title><source>Proceedings of the Royal Society B: Biological Sciences</source><year>2012</year><volume>279</volume><issue>1740</issue><fpage>2974</fpage><lpage>2981</lpage><pub-id pub-id-type="pmcid">PMC3385477</pub-id><pub-id pub-id-type="pmid">22513862</pub-id><pub-id pub-id-type="doi">10.1098/rspb.2012.0322</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuhn</surname><given-names>M</given-names></name><name><surname>Wickham</surname><given-names>H</given-names></name></person-group><source>Tidymodels: A collection of packages for modeling and machine learning using tidyverse principles [Computer software]</source><year>2020</year></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leavens</surname><given-names>DA</given-names></name><name><surname>Russell</surname><given-names>JL</given-names></name><name><surname>Hopkins</surname><given-names>WD</given-names></name></person-group><article-title>Intentionality as Measured in the Persistence and Elaboration of Communication by Chimpanzees (Pan troglodytes)</article-title><source>Child Development</source><year>2005</year><volume>76</volume><issue>1</issue><fpage>291</fpage><lpage>306</lpage><pub-id pub-id-type="pmcid">PMC2043155</pub-id><pub-id pub-id-type="pmid">15693773</pub-id><pub-id pub-id-type="doi">10.1111/j.1467-8624.2005.00845.x</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leroux</surname><given-names>M</given-names></name><name><surname>Bosshard</surname><given-names>AB</given-names></name><name><surname>Chandia</surname><given-names>B</given-names></name><name><surname>Manser</surname><given-names>A</given-names></name><name><surname>Zuberbühler</surname><given-names>K</given-names></name><name><surname>Townsend</surname><given-names>SW</given-names></name></person-group><article-title>Chimpanzees combine pant hoots with food calls into larger structures</article-title><source>Animal Behaviour</source><year>2021</year><volume>179</volume><fpage>41</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1016/J.ANBEHAV.2021.06.026</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leroux</surname><given-names>M</given-names></name><name><surname>Chandia</surname><given-names>B</given-names></name><name><surname>Bosshard</surname><given-names>AB</given-names></name><name><surname>Zuberbühler</surname><given-names>K</given-names></name><name><surname>Townsend</surname><given-names>SW</given-names></name></person-group><article-title>Call combinations in chimpanzees: A social tool?</article-title><source>Behavioral Ecology</source><year>2022</year><volume>33</volume><issue>5</issue><fpage>1036</fpage><lpage>1043</lpage><pub-id pub-id-type="doi">10.1093/beheco/arac074</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liebal</surname><given-names>K</given-names></name><name><surname>Call</surname><given-names>J</given-names></name><name><surname>Tomasello</surname><given-names>M</given-names></name></person-group><article-title>Use of gesture sequences in chimpanzees</article-title><source>American Journal of Primatology</source><year>2004</year><volume>64</volume><issue>4</issue><fpage>377</fpage><lpage>396</lpage><pub-id pub-id-type="pmid">15580580</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCowan</surname><given-names>B</given-names></name><name><surname>Hanser</surname><given-names>SF</given-names></name><name><surname>Doyle</surname><given-names>LR</given-names></name></person-group><article-title>Quantitative tools for comparing animal communication systems: Information theory applied to bottlenose dolphin whistle repertoires</article-title><source>Animal Behaviour</source><year>1999</year><volume>57</volume><issue>2</issue><fpage>409</fpage><lpage>419</lpage><pub-id pub-id-type="pmid">10049481</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Meyer</surname><given-names>PE</given-names></name></person-group><source>infotheo: Information-Theoretic Measures (Version 1.2.0.1) [Computer software]</source><year>2022</year><comment><ext-link ext-link-type="uri" xlink:href="https://CRAN.R-project.org/package=infotheo">https://CRAN.R-project.org/package=infotheo</ext-link></comment></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mielke</surname><given-names>A</given-names></name></person-group><article-title>Impact of dominance rank specification in dyadic interaction models</article-title><source>PLOS ONE</source><year>2023</year><volume>18</volume><issue>7</issue><elocation-id>e0277130</elocation-id><pub-id pub-id-type="pmcid">PMC10358901</pub-id><pub-id pub-id-type="pmid">37471413</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0277130</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mielke</surname><given-names>A</given-names></name><name><surname>Badihi</surname><given-names>G</given-names></name><name><surname>Graham</surname><given-names>KE</given-names></name><name><surname>Grund</surname><given-names>C</given-names></name><name><surname>Hashimoto</surname><given-names>C</given-names></name><name><surname>Piel</surname><given-names>AK</given-names></name><name><surname>Safryghin</surname><given-names>A</given-names></name><name><surname>Slocombe</surname><given-names>KE</given-names></name><name><surname>Stewart</surname><given-names>F</given-names></name><name><surname>Wilke</surname><given-names>C</given-names></name><name><surname>Zuberbühler</surname><given-names>K</given-names></name><etal/></person-group><article-title>Many morphs: Parsing gesture signals from the noise</article-title><source>Behavior Research Methods</source><year>2024</year><volume>56</volume><issue>7</issue><fpage>6520</fpage><lpage>6537</lpage><pub-id pub-id-type="pmcid">PMC11362259</pub-id><pub-id pub-id-type="pmid">38438657</pub-id><pub-id pub-id-type="doi">10.3758/s13428-024-02368-6</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mielke</surname><given-names>A</given-names></name><name><surname>Carvalho</surname><given-names>S</given-names></name></person-group><article-title>Chimpanzee play sequences are structured hierarchically as games</article-title><source>PeerJ</source><year>2022</year><volume>10</volume><elocation-id>e14294</elocation-id><pub-id pub-id-type="pmcid">PMC9675342</pub-id><pub-id pub-id-type="pmid">36411837</pub-id><pub-id pub-id-type="doi">10.7717/peerj.14294</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mielke</surname><given-names>A</given-names></name><name><surname>Preis</surname><given-names>A</given-names></name><name><surname>Samuni</surname><given-names>L</given-names></name><name><surname>Gogarten</surname><given-names>JF</given-names></name><name><surname>Lester</surname><given-names>JD</given-names></name><name><surname>Crockford</surname><given-names>C</given-names></name><name><surname>Wittig</surname><given-names>RM</given-names></name></person-group><article-title>Consistency of Social Interactions in Sooty Mangabeys and Chimpanzees</article-title><source>Frontiers in Ecology and Evolution</source><year>2021</year><volume>8</volume><pub-id pub-id-type="doi">10.3389/fevo.2020.603677</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Molnar</surname><given-names>C</given-names></name></person-group><source>Interpretable Machine Learning</source><year>2022</year><comment><ext-link ext-link-type="uri" xlink:href="http://Lulu.com">Lulu.com</ext-link></comment></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muschinski</surname><given-names>J</given-names></name><name><surname>Mielke</surname><given-names>A</given-names></name><name><surname>Carvalho</surname><given-names>S</given-names></name></person-group><article-title>A network-based analysis of signal use during approach interactions across sexes in chacma baboons (Papio ursinus griseipes)</article-title><source>bioRxiv</source><year>2023</year><pub-id pub-id-type="doi">10.1101/2023.02.04.527103</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oña</surname><given-names>L</given-names></name><name><surname>Sandler</surname><given-names>W</given-names></name><name><surname>Liebal</surname><given-names>K</given-names></name></person-group><article-title>A stepping stone to compositionality in chimpanzee communication</article-title><source>PeerJ</source><year>2019</year><pub-id pub-id-type="pmcid">PMC6745191</pub-id><pub-id pub-id-type="pmid">31565566</pub-id><pub-id pub-id-type="doi">10.7717/peerj.7623</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><collab>R Development Core Team &amp; R Core Team</collab><article-title>R: A language and environment for statistical computing</article-title><source>R Foundation for Statistical Computing Vienna Austria</source><year>2020</year><comment>0, {ISBN} 3-900051-07-0</comment><pub-id pub-id-type="doi">10.1038/sj.hdy.6800737</pub-id></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rodrigues</surname><given-names>ED</given-names></name><name><surname>Fröhlich</surname><given-names>M</given-names></name></person-group><article-title>Operationalizing Intentionality in Primate Communication: Social and Ecological Considerations</article-title><source>International Journal of Primatology</source><year>2021</year><pub-id pub-id-type="doi">10.1007/s10764-021-00248-w</pub-id></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rodrigues</surname><given-names>ED</given-names></name><name><surname>Santos</surname><given-names>AJ</given-names></name><name><surname>Veppo</surname><given-names>F</given-names></name><name><surname>Pereira</surname><given-names>J</given-names></name><name><surname>Hobaiter</surname><given-names>C</given-names></name></person-group><article-title>Connecting primate gesture to the evolutionary roots of language: A systematic review</article-title><source>American Journal of Primatology</source><year>2021</year><volume>83</volume><issue>9</issue><elocation-id>e23313</elocation-id><pub-id pub-id-type="pmid">34358359</pub-id></element-citation></ref><ref id="R52"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sandler</surname><given-names>W</given-names></name><name><surname>Lillo-Martin</surname><given-names>DC</given-names></name></person-group><source>Sign Language and Linguistic Universals</source><year>2006</year><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schlenker</surname><given-names>P</given-names></name><name><surname>Chemla</surname><given-names>E</given-names></name><name><surname>Schel</surname><given-names>AM</given-names></name><name><surname>Fuller</surname><given-names>J</given-names></name><name><surname>Gautier</surname><given-names>JP</given-names></name><name><surname>Kuhn</surname><given-names>J</given-names></name><name><surname>Veselinović</surname><given-names>D</given-names></name><name><surname>Arnold</surname><given-names>K</given-names></name><name><surname>Cäsar</surname><given-names>C</given-names></name><name><surname>Keenan</surname><given-names>S</given-names></name><name><surname>Lemasson</surname><given-names>A</given-names></name><etal/></person-group><article-title>Formal monkey linguistics</article-title><source>Theoretical Linguistics</source><year>2016</year><volume>42</volume><issue>1–2</issue><fpage>1</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1515/tl-2016-0001</pub-id></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Slocombe</surname><given-names>KE</given-names></name><name><surname>Zuberbühler</surname><given-names>K</given-names></name></person-group><article-title>Agonistic screams in wild chimpanzees (Pan troglodytes schweinfurthii) vary as a function of social role</article-title><source>Journal of Comparative Psychology</source><year>2005</year><volume>119</volume><issue>1</issue><fpage>67</fpage><lpage>77</lpage><pub-id pub-id-type="pmid">15740431</pub-id></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soldati</surname><given-names>A</given-names></name><name><surname>Muhumuza</surname><given-names>G</given-names></name><name><surname>Dezecache</surname><given-names>G</given-names></name><name><surname>Fedurek</surname><given-names>P</given-names></name><name><surname>Taylor</surname><given-names>D</given-names></name><name><surname>Call</surname><given-names>J</given-names></name><name><surname>Zuberbühler</surname><given-names>K</given-names></name></person-group><article-title>The Ontogeny of Vocal Sequences: Insights from a Newborn Wild Chimpanzee (Pan troglodytes schweinfurthii)</article-title><source>International Journal of Primatology</source><year>2022</year><pub-id pub-id-type="doi">10.1007/s10764-022-00321-y</pub-id></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spiess</surname><given-names>S</given-names></name><name><surname>Mylne</surname><given-names>HK</given-names></name><name><surname>Engesser</surname><given-names>S</given-names></name><name><surname>Mine</surname><given-names>JG</given-names></name><name><surname>O’Neill</surname><given-names>LG</given-names></name><name><surname>Russell</surname><given-names>AF</given-names></name><name><surname>Townsend</surname><given-names>SW</given-names></name></person-group><article-title>Syntax-like Structures in Maternal Contact Calls of Chestnut-Crowned Babblers (Pomatostomus ruficeps)</article-title><source>International Journal of Primatology</source><year>2022</year><pub-id pub-id-type="pmcid">PMC11211148</pub-id><pub-id pub-id-type="pmid">38948101</pub-id><pub-id pub-id-type="doi">10.1007/s10764-022-00332-9</pub-id></element-citation></ref><ref id="R57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steegen</surname><given-names>S</given-names></name><name><surname>Tuerlinckx</surname><given-names>F</given-names></name><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Vanpaemel</surname><given-names>W</given-names></name></person-group><article-title>Increasing Transparency Through a Multiverse Analysis</article-title><source>Perspectives on Psychological Science</source><year>2016</year><volume>11</volume><issue>5</issue><fpage>702</fpage><lpage>712</lpage><pub-id pub-id-type="pmid">27694465</pub-id></element-citation></ref><ref id="R58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stokoe</surname><given-names>WC</given-names></name></person-group><article-title>Sign Language Structure</article-title><source>Annual Review of Anthropology</source><year>1980</year><volume>9</volume><fpage>365</fpage><lpage>390</lpage><comment>Volume 9, 1980</comment><pub-id pub-id-type="doi">10.1s146/annurev.an.09.100180.002053</pub-id></element-citation></ref><ref id="R59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suzuki</surname><given-names>TN</given-names></name><name><surname>Griesser</surname><given-names>M</given-names></name><name><surname>Wheatcroft</surname><given-names>D</given-names></name></person-group><article-title>Syntactic rules in avian vocal sequences as a window into the evolution of compositionality</article-title><source>Animal Behaviour</source><year>2019</year><volume>151</volume><fpage>267</fpage><lpage>274</lpage><pub-id pub-id-type="doi">10.1016/j.anbehav.2019.01.009</pub-id></element-citation></ref><ref id="R60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suzuki</surname><given-names>TN</given-names></name><name><surname>Wheatcroft</surname><given-names>D</given-names></name><name><surname>Griesser</surname><given-names>M</given-names></name></person-group><article-title>Experimental evidence for compositional syntax in bird calls</article-title><source>Nature Communications</source><year>2016</year><volume>7</volume><issue>1</issue><comment>Article 1</comment><pub-id pub-id-type="pmcid">PMC4786783</pub-id><pub-id pub-id-type="pmid">26954097</pub-id><pub-id pub-id-type="doi">10.1038/ncomms10986</pub-id></element-citation></ref><ref id="R61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>ten Cate</surname><given-names>C</given-names></name><name><surname>Okanoya</surname><given-names>K</given-names></name></person-group><article-title>Revisiting the syntactic abilities of nonhuman animals: Natural vocalizations and artificial grammar learning</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><year>2012</year><volume>367</volume><issue>1598</issue><fpage>1984</fpage><lpage>1994</lpage><pub-id pub-id-type="pmcid">PMC3367684</pub-id><pub-id pub-id-type="pmid">22688634</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2012.0055</pub-id></element-citation></ref><ref id="R62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trujillo</surname><given-names>JP</given-names></name><name><surname>Holler</surname><given-names>J</given-names></name></person-group><article-title>Conversational facial signals combine into compositional meanings that change the interpretation of speaker intentions</article-title><source>Scientific Reports</source><year>2024</year><volume>14</volume><issue>1</issue><elocation-id>2286</elocation-id><pub-id pub-id-type="pmcid">PMC10821935</pub-id><pub-id pub-id-type="pmid">38280963</pub-id><pub-id pub-id-type="doi">10.1038/s41598-024-52589-0</pub-id></element-citation></ref><ref id="R63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walsh</surname><given-names>SL</given-names></name><name><surname>Engesser</surname><given-names>S</given-names></name><name><surname>Townsend</surname><given-names>SW</given-names></name><name><surname>Ridley</surname><given-names>AR</given-names></name></person-group><article-title>Multi-level combinatoriality in magpie non-song vocalizations</article-title><source>Journal of The Royal Society Interface</source><year>2023</year><volume>20</volume><issue>199</issue><elocation-id>20220679</elocation-id><pub-id pub-id-type="pmcid">PMC9890321</pub-id><pub-id pub-id-type="pmid">36722171</pub-id><pub-id pub-id-type="doi">10.1098/rsif.2022.0679</pub-id></element-citation></ref><ref id="R64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weiss</surname><given-names>M</given-names></name><name><surname>Hultsch</surname><given-names>H</given-names></name><name><surname>Adam</surname><given-names>I</given-names></name><name><surname>Scharff</surname><given-names>C</given-names></name><name><surname>Kipper</surname><given-names>S</given-names></name></person-group><article-title>The use of network analysis to study complex animal communication systems: A study on nightingale song</article-title><source>Proceedings of the Royal Society B: Biological Sciences</source><year>2014</year><volume>281</volume><issue>1785</issue><pub-id pub-id-type="pmcid">PMC4024303</pub-id><pub-id pub-id-type="pmid">24807258</pub-id><pub-id pub-id-type="doi">10.1098/rspb.2014.0460</pub-id></element-citation></ref><ref id="R65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wickham</surname><given-names>H</given-names></name><name><surname>Averick</surname><given-names>M</given-names></name><name><surname>Bryan</surname><given-names>J</given-names></name><name><surname>Chang</surname><given-names>W</given-names></name><name><surname>McGowan</surname><given-names>LD</given-names></name><name><surname>François</surname><given-names>R</given-names></name><name><surname>Grolemund</surname><given-names>G</given-names></name><name><surname>Hayes</surname><given-names>A</given-names></name><name><surname>Henry</surname><given-names>L</given-names></name><name><surname>Hester</surname><given-names>J</given-names></name><name><surname>Kuhn</surname><given-names>M</given-names></name><etal/></person-group><article-title>Welcome to the Tidyverse</article-title><source>Journal of Open Source Software</source><year>2019</year><volume>4</volume><issue>43</issue><elocation-id>1686</elocation-id><pub-id pub-id-type="doi">10.21105/joss.01686</pub-id></element-citation></ref><ref id="R66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilke</surname><given-names>C</given-names></name><name><surname>Kavanagh</surname><given-names>E</given-names></name><name><surname>Donnellan</surname><given-names>E</given-names></name><name><surname>Waller</surname><given-names>BM</given-names></name><name><surname>Machanda</surname><given-names>ZP</given-names></name><name><surname>Slocombe</surname><given-names>KE</given-names></name></person-group><article-title>Production of and responses to unimodal and multimodal signals in wild chimpanzees, <italic>Pan troglodytes schweinfurthii</italic></article-title><source>Animal Behaviour</source><year>2017</year><volume>123</volume><fpage>305</fpage><lpage>316</lpage><pub-id pub-id-type="doi">10.1016/j.anbehav.2016.10.024</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><p>Difference between time-windows in classifying sequences: the 5 seconds time-window classes all five gestures as part of the same sequence based on the time between elements. The rapid-fire time-window classes gestures starting within 1 second of the previous MOU end as part of the same sequence. The overlap time-window only classes those with temporal overlap as part of the same sequence. The solitary-gesture-plus-waiting time-window classifies those that are the only gesture to occur in a 1 second window but are less than 5 seconds apart as belonging to the same sequence.</p></caption><graphic xlink:href="EMS200145-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><p>Variability of conditional transition probabilities based on the occurrence frequency of the antecedent unit for bigrams (one antecedent), trigrams (two antecedents), and 4-grams (three antecedents). The y-axis depicts the difference between the maximum and minimum conditional probability for that transition across 1,000 bootstraps.</p></caption><graphic xlink:href="EMS200145-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><p>Transition network based on the concurrence of the different time-windows (i.e., transitions that are significant in at least three out of four time-windows) including cluster assignment based on ‘optimal clustering’ algorithm and loops.</p></caption><graphic xlink:href="EMS200145-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><p>Network of significant transitions that are shared between the rapid-fire sequences and solitary gesture sequences (left, red), occur only in the rapid-fire sequences (centre, green) or only in the solitary-gestures-with-waiting sequences (right, blue). Significant transitions are marked by edges between gesture actions. Loops indicate that a gesture action significantly follows itself.</p></caption><graphic xlink:href="EMS200145-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><p>Entropy ratio values of conditional probabilities with antecedents of one unit. Colours indicate the time-window. Entropy ratios (observed/expected entropy) closer to 0 are considered more predictable than random assignment, 1 or above are random distributions.</p></caption><graphic xlink:href="EMS200145-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><p>Accuracy values of Naive Bayes classifier (correct predictions/all predictions), predicting the consequent units based on antecedents of one unit. Colours indicate the time-window.</p></caption><graphic xlink:href="EMS200145-f006"/></fig><fig id="F7" position="float"><label>Figure 7</label><caption><p>Entropy values of conditional probabilities with antecedents of 1 or 2 units. True order in red. Each line represents one bootstrap (resampling with replacement) to indicate uncertainty of estimates. Values closer to 0 mean that observed values are lower than expected values and the system is more ordered and more predictable than random assignment. ‘Alphabetical’ ordering for 2-unit antecedents (portrayed in orange) means that the constituting units were ordered alphabetically before calculating probabilities, removing order information.</p></caption><graphic xlink:href="EMS200145-f007"/></fig><fig id="F8" position="float"><label>Figure 8</label><caption><p>Correct classification probability of models using Naive Bayes classifier. Lines connect iterations based on the same training/test dataset split. Red lines indicate average performances of classifiers. Prediction based on two units removed (A), previous unit (B), two preceding units (AB), and two preceding units without order information (rAB).</p></caption><graphic xlink:href="EMS200145-f008"/></fig><fig id="F9" position="float"><label>Figure 9</label><caption><p>Comparison of unit-level prediction accuracy for the Naive Bayes classifier for each gesture - prediction based on the single antecedent B in blue, for the two-antecedent AB combination in red, with the difference on the right. Where only one dot is visible, the predictive power was the same.</p></caption><graphic xlink:href="EMS200145-f009"/></fig><table-wrap id="T1" orientation="portrait" position="float"><label>Table 1</label><caption><title>Number of sequences with a given number of units depending on time-window</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" align="left">Sequence Length</th><th valign="top" align="left">5 seconds</th><th valign="top" align="left">Rapid-fire Sequences</th><th valign="top" align="left">Overlap</th><th valign="top" align="left">Solitary Gestures</th></tr></thead><tbody><tr><td valign="top" align="left" style="border-bottom: 1px dashed">1</td><td valign="top" align="left" style="border-bottom: 1px dashed">3616</td><td valign="top" align="left" style="border-bottom: 1px dashed">5343</td><td valign="top" align="left" style="border-bottom: 1px dashed">6279</td><td valign="top" align="left" style="border-bottom: 1px dashed">6406</td></tr><tr><td valign="top" align="left">2</td><td valign="top" align="left">1006</td><td valign="top" align="left">772</td><td valign="top" align="left">589</td><td valign="top" align="left">420</td></tr><tr><td valign="top" align="left">3</td><td valign="top" align="left">302</td><td valign="top" align="left">152</td><td valign="top" align="left">64</td><td valign="top" align="left">97</td></tr><tr><td valign="top" align="left">4</td><td valign="top" align="left">109</td><td valign="top" align="left">56</td><td valign="top" align="left">13</td><td valign="top" align="left">23</td></tr><tr><td valign="top" align="left">5</td><td valign="top" align="left">67</td><td valign="top" align="left">23</td><td valign="top" align="left">8</td><td valign="top" align="left">13</td></tr><tr><td valign="top" align="left">6</td><td valign="top" align="left">28</td><td valign="top" align="left">6</td><td valign="top" align="left">1</td><td valign="top" align="left">1</td></tr><tr><td valign="top" align="left">7</td><td valign="top" align="left">15</td><td valign="top" align="left">3</td><td valign="top" align="left">0</td><td valign="top" align="left">4</td></tr><tr><td valign="top" align="left">8</td><td valign="top" align="left">8</td><td valign="top" align="left">1</td><td valign="top" align="left">0</td><td valign="top" align="left">0</td></tr><tr><td valign="top" align="left">9</td><td valign="top" align="left">3</td><td valign="top" align="left">0</td><td valign="top" align="left">0</td><td valign="top" align="left">1</td></tr><tr><td valign="top" align="left">10</td><td valign="top" align="left">3</td><td valign="top" align="left">0</td><td valign="top" align="left">0</td><td valign="top" align="left">1</td></tr><tr><td valign="top" align="left">11</td><td valign="top" align="left">1</td><td valign="top" align="left">0</td><td valign="top" align="left">0</td><td valign="top" align="left">0</td></tr><tr><td valign="top" align="left">12</td><td valign="top" align="left">2</td><td valign="top" align="left">0</td><td valign="top" align="left">0</td><td valign="top" align="left">0</td></tr><tr><td valign="top" align="left" style="border-bottom: 1px solid #000000">13</td><td valign="top" align="left" style="border-bottom: 1px solid #000000">1</td><td valign="top" align="left" style="border-bottom: 1px solid #000000">0</td><td valign="top" align="left" style="border-bottom: 1px solid #000000">0</td><td valign="top" align="left" style="border-bottom: 1px solid #000000">0</td></tr><tr><td valign="top" align="left" style="border-bottom: 1.5px solid #000000"><bold>Total Number of Sequences &gt; 1</bold></td><td valign="top" align="left" style="border-bottom: 1.5px solid #000000">1545</td><td valign="top" align="left" style="border-bottom: 1.5px solid #000000">1013</td><td valign="top" align="left" style="border-bottom: 1.5px solid #000000">675</td><td valign="top" align="left" style="border-bottom: 1.5px solid #000000">560</td></tr></tbody></table></table-wrap><table-wrap id="T2" orientation="portrait" position="float"><label>Table 3</label><caption><p>Bigrams where one unit was more commonly observed to precede the other than would be expected given random distribution within sequences, arranged by the strength of effect. For the 5-second time frame only.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" align="left">Antecedent</th><th valign="top" align="left">Consequent</th><th valign="top" align="left">Count</th><th valign="top" align="left">Conditional probability</th><th valign="top" align="left">Expected</th><th valign="top" align="left">Improvement</th></tr></thead><tbody><tr><td valign="top" align="left">Embrace</td><td valign="top" align="left">Bite</td><td valign="top" align="left">16</td><td valign="top" align="left">0.64</td><td valign="top" align="left">0.39</td><td valign="top" align="left">0.25</td></tr><tr><td valign="top" align="left">GrabHold</td><td valign="top" align="left">Bite</td><td valign="top" align="left">18</td><td valign="top" align="left">0.42</td><td valign="top" align="left">0.23</td><td valign="top" align="left">0.19</td></tr><tr><td valign="top" align="left">Grab</td><td valign="top" align="left">Bite</td><td valign="top" align="left">18</td><td valign="top" align="left">0.25</td><td valign="top" align="left">0.13</td><td valign="top" align="left">0.12</td></tr><tr><td valign="top" align="left">Bite</td><td valign="top" align="left">HitOther</td><td valign="top" align="left">6</td><td valign="top" align="left">0.17</td><td valign="top" align="left">0.1</td><td valign="top" align="left">0.08</td></tr><tr><td valign="top" align="left">Raise</td><td valign="top" align="left">BigLoudScratch</td><td valign="top" align="left">23</td><td valign="top" align="left">0.34</td><td valign="top" align="left">0.28</td><td valign="top" align="left">0.07</td></tr><tr><td valign="top" align="left">Push</td><td valign="top" align="left">Touch</td><td valign="top" align="left">15</td><td valign="top" align="left">0.16</td><td valign="top" align="left">0.11</td><td valign="top" align="left">0.05</td></tr><tr><td valign="top" align="left">Raise</td><td valign="top" align="left">StompObject</td><td valign="top" align="left">5</td><td valign="top" align="left">0.07</td><td valign="top" align="left">0.04</td><td valign="top" align="left">0.04</td></tr><tr><td valign="top" align="left">Pull</td><td valign="top" align="left">Push</td><td valign="top" align="left">7</td><td valign="top" align="left">0.11</td><td valign="top" align="left">0.07</td><td valign="top" align="left">0.04</td></tr><tr><td valign="top" align="left">Swing</td><td valign="top" align="left">HitOther</td><td valign="top" align="left">11</td><td valign="top" align="left">0.09</td><td valign="top" align="left">0.06</td><td valign="top" align="left">0.03</td></tr><tr><td valign="top" align="left">ObjectShake</td><td valign="top" align="left">StompObject</td><td valign="top" align="left">35</td><td valign="top" align="left">0.13</td><td valign="top" align="left">0.1</td><td valign="top" align="left">0.03</td></tr><tr><td valign="top" align="left">HitOther</td><td valign="top" align="left">StompOther</td><td valign="top" align="left">8</td><td valign="top" align="left">0.06</td><td valign="top" align="left">0.03</td><td valign="top" align="left">0.03</td></tr><tr><td valign="top" align="left">Push</td><td valign="top" align="left">Raise</td><td valign="top" align="left">6</td><td valign="top" align="left">0.07</td><td valign="top" align="left">0.04</td><td valign="top" align="left">0.03</td></tr><tr><td valign="top" align="left">BigLoudScratch</td><td valign="top" align="left">Push</td><td valign="top" align="left">14</td><td valign="top" align="left">0.05</td><td valign="top" align="left">0.03</td><td valign="top" align="left">0.02</td></tr><tr><td valign="top" align="left">HitObject</td><td valign="top" align="left">Jump</td><td valign="top" align="left">9</td><td valign="top" align="left">0.04</td><td valign="top" align="left">0.02</td><td valign="top" align="left">0.02</td></tr><tr><td valign="top" align="left">ObjectMove</td><td valign="top" align="left">ThrowObject</td><td valign="top" align="left">5</td><td valign="top" align="left">0.04</td><td valign="top" align="left">0.02</td><td valign="top" align="left">0.02</td></tr><tr><td valign="top" align="left">Touch</td><td valign="top" align="left">HitOther</td><td valign="top" align="left">5</td><td valign="top" align="left">0.04</td><td valign="top" align="left">0.02</td><td valign="top" align="left">0.02</td></tr><tr><td valign="top" align="left">Touch</td><td valign="top" align="left">GrabHold</td><td valign="top" align="left">7</td><td valign="top" align="left">0.05</td><td valign="top" align="left">0.04</td><td valign="top" align="left">0.02</td></tr><tr><td valign="top" align="left">StompObject</td><td valign="top" align="left">HeadStand</td><td valign="top" align="left">5</td><td valign="top" align="left">0.02</td><td valign="top" align="left">0.01</td><td valign="top" align="left">0.02</td></tr><tr><td valign="top" align="left" style="border-bottom: hidden">BigLoudScratch</td><td valign="top" align="left" style="border-bottom: hidden">PresentGenitals</td><td valign="top" align="left" style="border-bottom: hidden">5</td><td valign="top" align="left" style="border-bottom: hidden">0.02</td><td valign="top" align="left" style="border-bottom: hidden">0.01</td><td valign="top" align="left" style="border-bottom: hidden">0.01</td></tr></tbody></table></table-wrap><table-wrap id="T3" orientation="portrait" position="float"><label>Table 4</label><caption><title>Observed and expected entropies for the four time-windows, considering the full dataset for each.</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" align="left">Time-Window</th><th valign="top" align="right">Entropy</th><th valign="top" align="right">Expected Entropy</th><th valign="top" align="right">Entropy Ratio</th></tr></thead><tbody><tr><td valign="top" align="left" style="border-top:solid 1px #000000"><bold>5 seconds</bold></td><td valign="top" align="right" style="border-top:solid 1px #000000">2.42</td><td valign="top" align="right" style="border-top:solid 1px #000000">3.96</td><td valign="top" align="right" style="border-top:solid 1px #000000">0.62</td></tr><tr><td valign="top" align="left"><bold>overlap</bold></td><td valign="top" align="right">1.72</td><td valign="top" align="right">3.29</td><td valign="top" align="right">0.52</td></tr><tr><td valign="top" align="left"><bold>rapid-fire</bold></td><td valign="top" align="right">2.11</td><td valign="top" align="right">3.66</td><td valign="top" align="right">0.58</td></tr><tr><td valign="top" align="left" style="border-bottom:solid 1px #000000"><bold>solitary-gestures-with-waiting</bold></td><td valign="top" align="right" style="border-bottom: 1px solid">1.88</td><td valign="top" align="right" style="border-bottom: 1px solid">3.29</td><td valign="top" align="right" style="border-bottom: 1px solid">0.57</td></tr></tbody></table></table-wrap><table-wrap id="T4" orientation="portrait" position="float"><label>Table 4</label><caption><p>Results of pairwise comparisons of prediction performances (measured using prediction accuracy) of Naive Bayes classifier using paired samples t-tests with False Discovery Rate correction of p-values. Significantly different pairings marked in italics bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" align="left">Predictor 1</th><th valign="top" align="left">Predictor 2</th><th valign="top" align="left">Adjusted p-value</th></tr></thead><tbody><tr><td valign="top" align="left" style="border-top:solid 1px #000000">A</td><td valign="top" align="left" style="border-top:solid 1px #000000">B</td><td valign="top" align="left" style="border-top:solid 1px #000000">0.17</td></tr><tr><td valign="top" align="left"><bold><italic>A</italic></bold></td><td valign="top" align="left"><bold><italic>AB</italic></bold></td><td valign="top" align="left"><bold><italic>&lt; 0.001</italic></bold></td></tr><tr><td valign="top" align="left"><bold><italic>A</italic></bold></td><td valign="top" align="left"><bold><italic>rBA</italic></bold></td><td valign="top" align="left"><bold><italic>&lt; 0.001</italic></bold></td></tr><tr><td valign="top" align="left"><bold><italic>B</italic></bold></td><td valign="top" align="left"><bold><italic>AB</italic></bold></td><td valign="top" align="left"><bold><italic>&lt; 0.001</italic></bold></td></tr><tr><td valign="top" align="left"><bold><italic>B</italic></bold></td><td valign="top" align="left"><bold><italic>rBA</italic></bold></td><td valign="top" align="left"><bold><italic>0.003</italic></bold></td></tr><tr><td valign="top" align="left" style="border-bottom:solid 1px #000000"><bold><italic>AB</italic></bold></td><td valign="top" align="left" style="border-bottom:solid 1px #000000"><bold><italic>rBA</italic></bold></td><td valign="top" align="left" style="border-bottom:solid 1px #000000"><bold><italic>0.015</italic></bold></td></tr></tbody></table></table-wrap></floats-group></article>