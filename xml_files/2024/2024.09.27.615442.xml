<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS199155</article-id><article-id pub-id-type="doi">10.1101/2024.09.27.615442</article-id><article-id pub-id-type="archive">PPR917742</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A depth map of visual space in the primary visual cortex</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>He</surname><given-names>Yiran</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Nieto</surname><given-names>Antonio Colas</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="fn" rid="FN1">2</xref></contrib><contrib contrib-type="author"><name><surname>Blot</surname><given-names>Antonin</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Znamenskiy</surname><given-names>Petr</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><aff id="A1"><label>1</label>Specification and Function of Neural Circuits Laboratory, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/04tnbqb63</institution-id><institution>The Francis Crick Institute</institution></institution-wrap>, <addr-line>1 Midland Road</addr-line>, <city>London</city><postal-code>NW1 1AT</postal-code>, <country country="GB">UK</country></aff></contrib-group><author-notes><corresp id="CR1"><bold><italic>Correspondence: <email>petr.znamenskiy@crick.ac.uk</email></italic></bold></corresp><fn id="FN1" fn-type="present-address"><label>2</label><p id="P1">Present address: Sainsbury Wellcome Centre, 25 Howland Street, London W1T 4JG, UK</p></fn></author-notes><pub-date pub-type="nihms-submitted"><day>04</day><month>10</month><year>2024</year></pub-date><pub-date pub-type="preprint"><day>30</day><month>09</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P2">Depth perception is essential for visually-guided behavior. Computer vision algorithms use depth maps to encode distances in three-dimensional scenes but it is unknown whether such depth maps are generated by animal visual systems. To answer this question, we focused on motion parallax, a depth cue relying on visual motion resulting from movement of the observer. As neurons in the mouse primary visual cortex (V1) are broadly modulated by locomotion, we hypothesized that they may integrate vision- and locomotion-related signals to estimate depth from motion parallax. Using recordings in a three-dimensional virtual reality environment, we found that conjunctive coding of visual and self-motion speeds gave rise to depth-selective neuronal responses. Depth-selective neurons could be characterized by three-dimensional receptive fields, responding to specific virtual depths and retinotopic locations. Neurons tuned to a broad range of virtual depths were found across all sampled retinotopic locations, showing that motion parallax generates a depth map of visual space in V1.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P3">To guide behavior using vision, the visual system must infer the three-dimensional location of visual cues based on the two-dimensional images formed on the retinae. The importance of this computation is underscored by the fact that the capacity for depth perception is innate in many animals (<xref ref-type="bibr" rid="R1">1</xref>). However, our understanding of how the visual system parses the organization of three-dimensional visual scenes to infer the depth of visual cues remains limited. Computer vision algorithms for depth estimation use two-dimensional input images to generate depth maps of the visual scene, where every pixel represents the estimate of the depth at that location (<xref ref-type="bibr" rid="R2">2</xref>). Such explicit representations of depth help guide the behavior of artificial agents (<xref ref-type="bibr" rid="R3">3</xref>). Do animal visual systems also generate depth maps of the visual scene and how are such depth maps encoded in neural activity?</p><p id="P4">In mammals, depth perception is supported by multiple cues relying on both binocular and monocular information (<xref ref-type="bibr" rid="R4">4</xref>–<xref ref-type="bibr" rid="R9">9</xref>). While binocular vision supports fine depth judgments (<xref ref-type="bibr" rid="R10">10</xref>), experiments in animals and humans show that it is not necessary for depth perception (<xref ref-type="bibr" rid="R1">1</xref>, <xref ref-type="bibr" rid="R11">11</xref>–<xref ref-type="bibr" rid="R13">13</xref>). In humans, motion parallax, a monocular depth cue that relies on visual motion arising from movement of the observer in the environment, is sufficient to generate a sensation of depth in the absence of other depth cues (<xref ref-type="bibr" rid="R14">14</xref>). When animals move, nearby and far visual cues appear to move at different velocities (<xref ref-type="bibr" rid="R15">15</xref>) making it possible to estimate their depth based on the speed of movement of the observer and the rate of the resulting visual motion. Previous work in passively translated macaques has found evidence for representation of depth from motion parallax in area MT (<xref ref-type="bibr" rid="R6">6</xref>, <xref ref-type="bibr" rid="R16">16</xref>, <xref ref-type="bibr" rid="R17">17</xref>). However, little is known about how visual circuits in mammals process motion parallax resulting from active movements of the observer and how the resulting representations are mapped across the visual field.</p><p id="P5">In mammals with limited binocular overlap, such as rodents, motion parallax is thought to be one of the major sources of depth information (<xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R13">13</xref>). In mice, eye movements stabilize the direction of gaze during locomotion to compensate for head rotation (<xref ref-type="bibr" rid="R18">18</xref>–<xref ref-type="bibr" rid="R20">20</xref>). Consequently, self-generated visual motion is primarily driven by translation of the animal, and its speed is proportional to the ratio of locomotion speed and depth (<xref ref-type="fig" rid="F1">Figure 1A</xref>). When performing tasks that require depth perception, mice can make monocular depth judgments that depend on the intact activity of the primary visual cortex (V1) (<xref ref-type="bibr" rid="R13">13</xref>), but how V1 neurons support depth perception from monocular cues such as motion parallax is unknown. Visual responses in mouse V1 are broadly modulated by locomotion (<xref ref-type="bibr" rid="R21">21</xref>–<xref ref-type="bibr" rid="R23">23</xref>) and the functional role of this modulation in visual processing remains debated (<xref ref-type="bibr" rid="R24">24</xref>, <xref ref-type="bibr" rid="R25">25</xref>). As estimation of depth from motion parallax requires both visual and self-motion information, we hypothesized that it is supported by locomotion-related modulation of visual responses in V1.</p><p id="P6">To test this hypothesis, we designed a virtual reality environment for head-fixed mice, where motion parallax acted as a cue of virtual depth, and recorded the activity of excitatory neurons in layer 2/3 of V1 using two-photon calcium imaging as mice navigated through this environment. We found that selectivity for virtual depth from motion parallax was widespread among V1 neurons. This depth selectivity could not be explained by visual-evoked responses alone but resulted from the integration of optic flow and locomotion-related signals.</p><p id="P7">Furthermore, by reconstructing visual stimuli presented in the virtual environment, we mapped the three-dimensional receptive fields of V1 neurons – tuned for both retinotopic location and virtual depth of visual cues. By characterizing depth selectivity across V1, we show that V1 neurons responding to stimuli across all sampled visual field locations encode the full range of virtual depths indicating that motion parallax generates a depth map of visual space in the visual cortex. However, depth selectivity was not homogeneously distributed across V1, with regions representing the upper visual field enriched for neurons tuned to near depths. We speculate that this bias reflects the ethological significance of the upper visual field for detection of nearby threats.</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>Depth selectivity from motion parallax in virtual reality</title><p id="P8">To test whether neurons in mouse primary visual cortex (V1) integrate visual- and locomotion-related signals to estimate depth from motion parallax, we trained head-fixed mice to navigate a virtual reality (VR) environment where visual stimuli were presented at different virtual distances from the mouse and motion parallax acted as a cue of depth (<xref ref-type="fig" rid="F1">Figure 1B-E</xref>). The environment contained a floor texture that the mouse ran across and black spheres presented in the monocular visual field (<xref ref-type="fig" rid="F1">Figure 1B,E</xref>). The position of the mouse in the VR environment was updated in closed loop based on its running speed on a styrofoam cylinder (<xref ref-type="supplementary-material" rid="SD1">Figure S1</xref>). Virtual depth of spheres varied from trial to trial (logarithmically spaced, with either 5 depths ranging from 6 cm to 600 cm in 25 sessions, or 8 depths ranging from 5 cm to 640 cm in 60 sessions). Consequently, animals’ locomotion on the wheel generated different speeds of optic flow depending on the virtual depth on a given trial. New spheres were presented in front of the mice as they advanced through the virtual environment to provide constant visual stimulation throughout the trial. Each trial was terminated once mice traveled 6 m through the VR, after which the spheres disappeared. To motivate mice to run, they were food-restricted and received a probabilistic soy milk reward after the spheres disappeared on 60-80% of trials (<xref ref-type="fig" rid="F1">Figure 1D</xref>). The size and density of spheres on each trial were adjusted such that spheres subtended the same visual angle across depths and had similar spacing from the animals’ perspective (<xref ref-type="fig" rid="F1">Figure 1C</xref>). The animals running speed was similar across trials of different depths (<xref ref-type="supplementary-material" rid="SD1">Figure S2A-D</xref>) and the sphere stimulus did not elicit any stereotyped eye movements (<xref ref-type="supplementary-material" rid="SD1">Figure S2I-L</xref>).</p><p id="P9">We recorded the activity of neurons in layer 2/3 of V1 in transgenic mice expressing GCaMP6f or GCaMP6s in excitatory cortical neurons using two-photon calcium imaging (60,357 neurons from 85 sessions in 7 mice) and determined whether neuronal responses were selective for virtual depth. To this end, we quantified virtual depth tuning by calculating the mean calcium fluorescence for trials of different virtual depths (<xref ref-type="fig" rid="F1">Figure 1F-H</xref>). Depth selectivity was widespread in among layer 2/3 excitatory cells, with individual neurons tuned to different depths (<xref ref-type="fig" rid="F1">Figure 1I-J</xref>). Across the population, 51.4% of cells (31,013 of 60,357 neurons) exhibited significant depth selectivity and virtual depth preferences of depth-selective neurons spanned the full range of depth values that we tested (<xref ref-type="fig" rid="F1">Figure 1K-L</xref>).</p><p id="P10">Depth-selective responses were absent during periods when mice were stationary (maximum speed during preceding 1 second &lt;5 cm/s, <xref ref-type="fig" rid="F1">Figure 1H-I</xref>). At the population level, virtual depth could be decoded from V1 population activity during locomotion but not during stationary periods (<xref ref-type="fig" rid="F1">Figure 1M-N</xref>). This demonstrates that motion parallax resulting from animals’ locomotion gives rise to a representation of depth in mouse V1.</p><p id="P11">To ensure that sphere stimuli subtended the same visual angle across different virtual depths, their size in the virtual environment was proportional to depth on a given trial (<xref ref-type="fig" rid="F1">Figure 1C</xref>). Therefore, depth selectivity could in fact reflect selectivity for virtual size. To test this possibility, we simultaneously varied virtual depth and virtual size of the spheres in a subset of recordings, such that the spheres subtended either 5, 10 or 20 degrees of visual angle (<xref ref-type="supplementary-material" rid="SD1">Figure S3</xref>). The magnitude of neuronal responses was modulated by the visual angle of the spheres (<xref ref-type="supplementary-material" rid="SD1">Figure S3B</xref>), consistent with the widespread size tuning of V1 responses (<xref ref-type="bibr" rid="R26">26</xref>, <xref ref-type="bibr" rid="R27">27</xref>). However, the preferred virtual depth was consistent across trials with 5, 10, and 20 degree spheres (<xref ref-type="supplementary-material" rid="SD1">Figure S3B-C</xref>), demonstrating that selectivity for virtual depth is invariant to stimulus size.</p></sec><sec id="S4"><title>Depth selectivity arises from the integration of optic flow and locomotion-related signals</title><p id="P12">Statistics of optic flow speed differed across virtual depths – near depths generate faster optic flow than far depths (<xref ref-type="supplementary-material" rid="SD1">Figure S2E-H</xref>). Consequently, depth selective responses could arise solely as the product of selectivity for optic flow speed. On the other hand, as the speed of self-generated optic flow depends on the ratio of the speed of locomotion and virtual depth, depth-selective neurons may preferentially respond to the conjunction of these variables.</p><p id="P13">To determine how the depth selectivity arose as a function of optic flow and locomotion speeds, we calculated the optic flow speed for each imaging frame as the ratio of running speed and virtual depth, which corresponds to the rate of visual motion at 90 degrees azimuth. We then visualized the activity of depth-selective neurons as a function of running speed and optic flow speed (<xref ref-type="fig" rid="F2">Figure 2A-B</xref>, <xref ref-type="supplementary-material" rid="SD1">S4A-B,E-F</xref>). If optic flow responses were the only determinant of depth selectivity, optic flow speed tuning should not change across virtual depths. However, this analysis showed that depth selectivity persisted when controlling for the speed of optic flow (<xref ref-type="fig" rid="F2">Figure 2B</xref>, <xref ref-type="supplementary-material" rid="SD1">S4B,F</xref>). As mice have to run at different speeds to generate the same rate of optic flow across different depths, this supported the hypothesis that depth-selectivity arose from the interaction of optic flow and locomotion-related signals. To examine this directly, we analyzed neuronal responses as a function of both optic flow and running speeds. This analysis revealed that depth-selective neurons in V1 responded to a specific conjunction of running speed and optic flow speed (<xref ref-type="fig" rid="F2">Figure 2C</xref>, <xref ref-type="supplementary-material" rid="SD1">S4C,G</xref>).</p><p id="P14">To quantify how running speed and optic flow speed tuning were integrated, we fitted neuronal responses as a function of running speed and optic flow speed using five classes of models: models tuned to either optic flow or running speed in isolation; an additive model, which models neuronal responses as linear summation of optic flow and locomotion-related responses; a conjunctive model, where neurons are tuned to a specific conjunction of optic flow and running speeds defined by a two-dimensional Gaussian function; and an idealized depth tuning model, where neuronal responses depend on the ratio of locomotion and optic flow speed (<xref ref-type="fig" rid="F2">Figure 2D</xref>, <xref ref-type="supplementary-material" rid="SD1">S4D,H</xref>). To control for differences in model complexity, we used K-fold cross-validation to assess the performance of each of the 5 models on test data not used for parameter optimization. We found that the pure optic flow and running speed models could not explain neuronal responses, showing that integration of visuo-motor signals is required to account for depth selectivity. The conjunctive model consistently outperformed the other models (<xref ref-type="fig" rid="F2">Figure 2E</xref>; <italic>p &lt;</italic> 0.0001 vs optic flow model; <italic>p &lt;</italic> 0.0001 vs running model; <italic>p &lt;</italic> 0.0001 vs additive model; <italic>p &lt;</italic> 0.0001 vs ratio model). Therefore, responses of depth selective neurons are best explained by selectivity for a specific combination of running and optic flow speeds.</p><p id="P15">We next asked how the preferred virtual depth of individual neurons related to their preferred optic flow and running speeds. To this end, we split the trials recorded for each neuron into two sets: one half of the trials was used to estimate each neuron’s preferred virtual depth, while the other half was used to estimate preferred optic flow and running speeds using the conjunctive model described above. Due to motion parallax, the ratio of running speed and the resulting optic flow is equal to virtual depth (<xref ref-type="fig" rid="F1">Figure 1A</xref>). Consistent with this, we found that the ratio of the preferred running and optic flow speeds of individual V1 neurons could predict their preferred virtual depth estimated using hold out data (<italic>r</italic> = 0.794, <italic>p &lt;</italic> 0.0001, <xref ref-type="fig" rid="F2">Figure 2F</xref>).</p><p id="P16">To further explore the origins of depth selectivity, we visualized running speed and optic flow speed preferences for neuronal populations preferring different virtual depths in logarithmic space (<xref ref-type="fig" rid="F2">Figures 2G</xref>, <xref ref-type="supplementary-material" rid="SD1">S2I-J</xref>). Since different depths generate different ratios of running and optic flow speeds, log-optic flow speeds associated with each depth lie along a line as a function of log-running speed with a different intercept (<xref ref-type="fig" rid="F2">Figure 2G</xref>, inset). Responses of V1 neurons mirrored this principle – neurons tuned to the same virtual depth have a wide range of preferred running and optic flow speed with a given ratio, which lie along along a line in log-running speed / log-optic flow speed space (<xref ref-type="fig" rid="F2">Figure 2G</xref>). Consequently, neurons with similar depth selectively prefer different combinations of running and optic flow speeds with a fixed ratio. This may enable V1 to encode depth independent of the animal’s speed of locomotion.</p></sec><sec id="S5"><title>Closed loop coupling of locomotion and optic flow enables accurate representation of depth</title><p id="P17">In the experiments described above, locomotion and optic flow were coupled on fast time scales (<xref ref-type="supplementary-material" rid="SD1">Figure S1D</xref>). Next, we examined whether this closed loop coupling was required for conjunctive coding of optic flow and running speed underlying depth-selective responses. To break the coupling between locomotion and optic flow while still maintaining the overall statistics of the visual stimulus in open loop conditions, the movement of the mouse through the VR environment was determined by a previously recorded running trajectory instead of the current running speed (<xref ref-type="fig" rid="F2">Figure 2H</xref>, <xref ref-type="supplementary-material" rid="SD1">S5C-D</xref>). We recorded neuronal responses during blocks of closed loop and open loop trials and compared running speed and optic flow speed preferences between these conditions. Preferred optic flow and running speeds of individual neurons were correlated between closed loop and open loop trials (<xref ref-type="fig" rid="F2">Figure 2I-K</xref>, running speed <italic>r</italic> = 0.578, <italic>p &lt;</italic> 0.0001, optic flow speed <italic>r</italic> = 0.700, <italic>p &lt;</italic> 0.0001). The magnitude of responses at the preferred running and optic flow speeds was similar on closed loop and open loop trials (<xref ref-type="fig" rid="F2">Figure 2L</xref>, <italic>p</italic> = 0.681). Overall these results show that the conjunctive coding of optic flow and locomotion speeds does not depend on their closed loop coupling.</p><p id="P18">Next, we asked whether the closed loop coupling of locomotion and optic flow helps maintain an accurate representation of depth at the population level. When animals run at different speeds, stimuli at different depths can give rise to the same speed of optic flow. During closed loop trials, optic flow inputs are accompanied by locomotion-related modulation corresponding to the running speed that produced them, which may help disambiguate the depth of such stimuli. On the other hand under open loop conditions, the locomotion-related modulation no longer matches the current optic flow speed. Therefore, open loop recordings allow us to experimentally “scramble” locomotion-related modulation in V1 and reveal its role in the representation of virtual depth from motion parallax. To this end, we trained a linear SVM classifier to decode virtual depth from the responses of simultaneously recorded neurons under closed or open loop conditions. Classifier accuracy was consistently higher on closed loop trials (<xref ref-type="fig" rid="F2">Figure 2M-N</xref>), demonstrating that the coupling of locomotion and optic flow is necessary for accurate representation of virtual depth in V1.</p></sec><sec id="S6"><title>V1 neurons have three-dimensional receptive fields</title><p id="P19">We hypothesised that depth-selective neurons are driven by the presentation of visual cues at specific retinotopic locations and at specific virtual depths. To test this hypothesis, we reconstructed the stimulus presented during every imaging frame and fit neuronal responses using a linear model to estimate neuronal receptive fields (RFs) while accounting for depth (<xref ref-type="fig" rid="F3">Figure 3A-B</xref>; see <xref ref-type="sec" rid="S14">Methods</xref>).</p><p id="P20">Many neurons responded to stimuli at specific retinotopic locations (<xref ref-type="fig" rid="F3">Figure 3C</xref>). Moreover, these spatially selective responses were modulated by virtual depth in accordance with depth selectivity observed based on trial-average responses (<xref ref-type="fig" rid="F3">Figure 3C</xref>). Consequently, neuronal responses could be described by a three-dimensional receptive fields (<xref ref-type="fig" rid="F3">Figure 3D</xref>). Across the population, the majority (66.3%, 20,554 / 31,013) of depth-selective neurons had detectable receptive fields (<xref ref-type="fig" rid="F3">Figure 3E</xref>), which represents a lower bound on the proportion of spatially tuned depth-selective neurons as receptive fields may be difficult to detect for neurons with lower signal-to-noise responses. As expected given the retinotopic organization of V1, the spatial location of RFs recorded within the same field of view was clustered in a particular region of the visual field and their preferred azimuth and elevation followed expected retinotopic gradients (<xref ref-type="fig" rid="F3">Figure 3F-G</xref>, <xref ref-type="supplementary-material" rid="SD1">S6A-B</xref>). In contrast, depth selectivity followed a salt-and-pepper pattern, with nearby neurons often preferring different virtual depths (<xref ref-type="fig" rid="F3">Figure 3H</xref>, <xref ref-type="supplementary-material" rid="SD1">S6C</xref>). These results demonstrate that when navigating through a three-dimensional virtual environment, responses of V1 neurons can be characterized by three-dimensional receptive fields defined by the conjunction of retinotopic location and preferred virtual depth.</p></sec><sec id="S7"><title>Depth selectivity across V1</title><p id="P21">We next asked how virtual depth was represented across V1. To this end, we first used widefield calcium imaging to identify the location of V1 and higher visual areas (<xref ref-type="fig" rid="F4">Figure 4A</xref>). We then characterized depth-selective responses across V1 and aligned the recordings across animals to a common reference frame to visualize their distribution at different cortical locations (see <xref ref-type="sec" rid="S14">Methods</xref>). Neurons’ preferred azimuth (<xref ref-type="fig" rid="F4">Figure 4B</xref>) and elevation (<xref ref-type="fig" rid="F4">Figure 4C</xref>) followed the expected gradients across V1. While neurons preferring different depths were intermingled, we found that neurons tuned to near virtual depths were over-represented in posterior V1 (<xref ref-type="fig" rid="F4">Figure 4D</xref>). Across our dataset, the preferred virtual depth of depth-selective neurons was negatively correlated with their anterior-posterior location in V1 (<italic>r</italic> = −0.239, <italic>p</italic> = 0.0007).</p><p id="P22">We next asked how the preferred virtual depth of individual neurons relates to their RF location. So far, we have defined virtual depth as the virtual distance to the sphere stimuli when the animal passes them at 90 degrees of azimuth. However, the depth of the spheres varies as a function of retinotopic location as spheres that are not at 90 degrees azimuth are further away. To control for this, for each depth-selective neuron with a significant RF, we calculated the distance of the spheres at its preferred azimuth and elevation (see <xref ref-type="sec" rid="S14">Methods</xref>). The resulting adjustment had the largest effect at small azimuths, where spheres were ~2 times further than at the closest point at 90 degrees azimuth. However, it was substantially smaller than the &gt;100 fold range of depths sampled across trials.</p><p id="P23">Analyzing virtual depth preferences as a function of neurons’ preferred elevation and azimuth revealed that near depths were over-represented in the upper and posterior visual field (<xref ref-type="fig" rid="F4">Figure 4E</xref>). This effect was significant when controlling for variability between mice and recording sessions (<italic>p</italic> = 6.51× 10<sup>−5</sup>, see <xref ref-type="sec" rid="S14">Methods</xref>) and persisted whether or not we applied the retinotopic correction described above (<xref ref-type="supplementary-material" rid="SD1">Figure S6D-E</xref>, <italic>p</italic> = 0.00761).</p><p id="P24">While virtual depth preferences displayed a spatial bias across the visual field, neurons tuned to the same retinotopic locations displayed a broad range of preferred virtual depths, consistent with our observation that nearby neurons often have distinct depth tuning (<xref ref-type="fig" rid="F1">Figures 1I</xref>, <xref ref-type="fig" rid="F3">3H</xref>, <xref ref-type="supplementary-material" rid="SD1">S6C</xref>). To illustrate this, we divided depth-selective neurons into 3 populations – near- (&lt;20 cm), mid- (20-100 cm), and far (&gt;100 cm) preferring cells. Neurons from all 3 populations were intermingled across most of the retinotopic locations sampled by our stimuli (<xref ref-type="fig" rid="F4">Figure 4F</xref>). The only notable exception was the relative absence of near-preferring cells in the anterior visual field, in part as a consequence of the bias inherent to the layout of the virtual environment. Therefore, our results indicate the V1 contains a depth map of visual space from motion parallax, with most retinotopic locations represented by distinct populations of neurons tuned to near, intermediate and far depths, and over-representation of near depths in the posterior upper visual field.</p></sec></sec><sec id="S8" sec-type="discussion"><title>Discussion</title><sec id="S9"><title>Representation of depth in V1</title><p id="P25">Visual responses of V1 neurons have traditionally been studied using two-dimensional visual stimuli, such as gratings or natural images. This has led to the view of V1 as a collection of spatiotemporal filters selective for specific visual features, such as spatial and temporal frequency, orientation, and direction, in the two-dimensional visual field (<xref ref-type="bibr" rid="R28">28</xref>–<xref ref-type="bibr" rid="R31">31</xref>). However, under naturalistic conditions the visual system is faced with the challenge of parsing the structure of three-dimensional visual scenes. Moreover, vision is an active sense – the optic flow generated by movements of the observer carries rich information about the organization of the surrounding space. Here we probed responses of V1 neurons in a three-dimensional virtual reality environment, where closed loop optic flow feedback based on the animal’s locomotion acted as a cue of the depth of visual stimuli. We found that a large fraction (51.4%) of V1 neurons were selectively tuned for the virtual depth of stimuli in VR. This depth selectivity was driven by motion parallax and was absent during periods when animals were stationary. Moreover, consistent with the retinotopic organization of V1, most depth-selective neurons responded to specific retinotopic locations, and could thus be characterized by three-dimensional receptive fields. Neurons with different depth preferences were intermingled, such that most retinotopic locations were represented by neurons spanning a broad range of virtual depths.</p><p id="P26">These findings support a conceptually novel view of how mouse V1 represents the visual scene in actively moving animals. They demonstrate that motion parallax generates a depth map of visual space in V1, with distinct neuronal populations responding to near versus far visual cues. We speculate that these populations may play different roles in visually guided behaviors. For example, neurons tuned to far depths may provide information about visual landmarks for navigation, while those selective for near and intermediate depths may support detection of nearby threats.</p></sec><sec id="S10"><title>A gradient of depth selectivity across V1</title><p id="P27">While we found that nearby neurons exhibited a broad range of virtual depth preferences, depth selectivity was not homogeneously distributed across V1. Most saliently, near depths were overrepresented in the upper lateral visual field. This contrasts to the distribution of binocular disparity preferences in the binocular region of V1, where near depths are overrepresented at low elevations (<xref ref-type="bibr" rid="R9">9</xref>). These differences may reflect distinct ethological roles of monocular and binocular regions in visually guided behaviors. The binocular zone appears to play a specialized role in hunting (<xref ref-type="bibr" rid="R32">32</xref>). During pursuit of prey, mice use their head and eye movements to maintain the location of their target in a region with minimal optic flow (<xref ref-type="bibr" rid="R20">20</xref>). Consequently, mice must rely on binocular cues for hunting (<xref ref-type="bibr" rid="R32">32</xref>) as motion parallax provides little depth information. On the other hand, the overrepresentation of near depths in the upper lateral visual field, where binocular cues are not available, may support detection of threats. Finally, overrepresentation of far depths in the lower visual field may help animals avoid hazardous falls when navigating over uneven terrain.</p><p id="P28">We speculate that this bias in the distribution of near- and far-preferring neurons may reflect biases already present in the signals conveyed by the retina. Retinal ganglion cells (RGCs) are not evenly distributed across the mouse retina, with the highest density of RGCs in the ventronasal retina (<xref ref-type="bibr" rid="R33">33</xref>), corresponding to upper lateral visual field. This bias is shared by W3 retinal ganglion cells which are responsive to local visual motion against a featureless or stationary background (<xref ref-type="bibr" rid="R34">34</xref>). These cells have primarily been viewed as sensitive to object motion, such as detection of predators. However, self-generated optic flow resulting from near visual cues also gives rise to local motion distinct from background and would also likely activate these neurons.</p></sec><sec id="S11"><title>Function of locomotion-related signals in V1</title><p id="P29">Since its first characterization over a decade ago (<xref ref-type="bibr" rid="R21">21</xref>), the modulation of mouse V1 neurons by locomotion and other movements has been extensively studied (<xref ref-type="bibr" rid="R23">23</xref>, <xref ref-type="bibr" rid="R35">35</xref>). However, its role in visual processing has remained a subject of debate (<xref ref-type="bibr" rid="R22">22</xref>, <xref ref-type="bibr" rid="R24">24</xref>, <xref ref-type="bibr" rid="R25">25</xref>, <xref ref-type="bibr" rid="R36">36</xref>–<xref ref-type="bibr" rid="R38">38</xref>). By analyzing neuronal responses to motion parallax as a function of both running and optic flow speeds, we found that depth selective responses arise as the product of integration of these signals. Locomotion-related signals determine the gain of optic flow responses, with the largest responses occurring on coincidence of the neuron’s preferred locomotion and optic flow speeds. V1 contains a diverse population of neurons preferring a wide range of running and optic flow speeds, and the ratio between the two speeds defines their depth preferences. Therefore, we propose that locomotion-related signals in V1 support estimation of depth from motion parallax through locomotion-dependent gain modulation of optic flow responses. Gain modulation has been proposed to support coordinate transforms, such as from retinal to body-centered (<xref ref-type="bibr" rid="R39">39</xref>), or from to egocentric to allocentric coordinates (<xref ref-type="bibr" rid="R40">40</xref>).</p><p id="P30">Here, gain modulation may support the transformation from retinotopic to three-dimensional coordinates by generating set of basis functions selective for different combinations of optic flow and locomotion speeds. Downstream neurons in deep layers of V1 or in higher visual areas may generate depth-selective representations that are invariant to the animals’ speed of locomotion by integrating inputs tuned to the same depth but preferring different running speeds.</p><p id="P31">Locomotion-related modulation has been previously proposed to encode the predicted sensory feedback arising from animals’ movements (<xref ref-type="bibr" rid="R22">22</xref>, <xref ref-type="bibr" rid="R41">41</xref>) and in combination with feed-forward visual motion signals in V1, to enable the detection of mismatches between predicted and experienced visual motion (<xref ref-type="bibr" rid="R22">22</xref>, <xref ref-type="bibr" rid="R42">42</xref>, <xref ref-type="bibr" rid="R43">43</xref>). However, predicting optic flow based on running speed requires knowledge of the depth of objects in the visual scene. Our results suggest that locomotion-related modulation enables visual cortical circuits to infer the depth of the visual cues, which could later be used to make accurate predictions of visual feedback.</p></sec><sec id="S12"><title>Circuit mechanisms of depth selectivity</title><p id="P32">The integration of optic flow and locomotion-related signals underlying depth-selective responses may occur in layer 2/3 of V1 <italic>de novo</italic> or be inherited from other sources, including the dorsal lateral geniculate nucleus (dLGN), or superior colliculus, as their spontaneous and visually-driven responses are modulated by locomotion (<xref ref-type="bibr" rid="R44">44</xref>–<xref ref-type="bibr" rid="R46">46</xref>). Several circuit mechanisms have been put forward as pathways for locomotion-related gain modulation in V1. Fu et al. proposed that locomotion modulates responses of excitatory cells through <italic>Vip</italic>-positive interneuron-mediated disinhibition. Leinweber et al. found that feedback projections from the secondary motor cortex are correlated with the expected optic flow resulting from animals’ locomotion, suggesting that locomotion-related modulation is conveyed by corticocortical pathways. In addition, inputs from both dLGN and the lateral posterior nucleus were shown to encode a combination of locomotion and optic flow signals (<xref ref-type="bibr" rid="R48">48</xref>), supporting the idea that locomotion-related modulation in depth-selective neurons may be inherited from the thalamus. Finally, locomotion has also been found to modulate effective recurrent connectivity within the cortical microcircuit (<xref ref-type="bibr" rid="R27">27</xref>), although the mechanism of this modulation is unclear.</p><p id="P33">Mouse primary visual cortex sends projections to several higher visual areas, which display different preferences for the speed of visual motion – anterior visual areas prefer fast moving grating stimuli, while posterior ones prefer slow ones (<xref ref-type="bibr" rid="R49">49</xref>, <xref ref-type="bibr" rid="R50">50</xref>). As optic flow speed preferences of V1 neurons correlate with their depth selectivity, higher visual areas may be specialized for processing visual signals from different depths – with anterior areas preferentially responding to near visual cues, and posterior areas responding to far ones. Consistent with this, higher visual areas have distinct binocular disparity preferences, with anterior area RL tuned to disparities related to near visual cues (<xref ref-type="bibr" rid="R9">9</xref>).</p></sec><sec id="S13"><title>Summary</title><p id="P34">By manipulating the depth of visual cues in a virtual reality environment, we found that selectivity for depth from motion parallax is widespread in the mouse primary visual cortex. While in the virtual environment used in our experiments self-generated optic flow acts as the only source of visual motion, in the real world visual motion results from both movements of the observer and that of visual cues. Our results raise the question of how the conjunctive responses of V1 neurons enable discrimination between self-generated and external motion and support depth estimation of moving visual cues. For a moving visual stimulus, the total visual motion is the sum of external and self-generated components. When the observer varies its self-motion speed, the depth of the stimulus determines the slope of the relationship between visual motion speed and self-motion, while external motion determines the intercept. We speculate that the conjunctive code of optic flow and locomotion speed in V1 may serve as a basis set for more abstract three-dimensional representations of the visual scene further along the visual cortical hierarchy that are invariant to locomotion speed and discriminate between self-generated and external motion.</p></sec></sec><sec id="S14" sec-type="methods"><title>Methods</title><sec id="S15"><title>Animals</title><p id="P35">All experimental procedures were performed in accordance to the UK Animals (Scientific Procedures) Act of 1986 (PPL PP4882546) and approved by the Animal Welfare Ethical Review Body at the Francis Crick Institute. 7 transgenic mice were used in the experiments, including 6 Emx1-Cre × Ai95D mice (JAX stock #028865 (<xref ref-type="bibr" rid="R51">51</xref>) and #005628 (<xref ref-type="bibr" rid="R52">52</xref>)) and one CamKII-tTA × TRE-GCaMP6s line G6s2 mouse (JAX stock #003010 (<xref ref-type="bibr" rid="R53">53</xref>) and #024742 (<xref ref-type="bibr" rid="R54">54</xref>)) expressing genetically encoded calcium indicators GCaMP6f or GCaMP6s respectively in cortical excitatory neurons. Both male and female mice were used (1 male and 6 females).</p></sec><sec id="S16"><title>Cranial window implantation</title><p id="P36">To implant the cranial window for chronic two-photon calcium imaging, mice were anesthetized with isofluorane (1.5-2.5%). They received a subcutaneous injection of dexamethasone (0.02 ml at 2 mg/ml) to reduce brain adaema. The skull was exposed and a metal headplate was affixed using dental cement. A craniotomy (4 mm in diameter) was performed over the left visual cortex and the bone flap was replaced by a 4 mm glass coverslip. Imaging was typically performed at the age of 3 to 7 months (up to 10 months).</p></sec><sec id="S17"><title>Virtual reality setup and visual stimuli</title><p id="P37">During the recordings, mice were head-fixed on a polystyrene cylindrical wheel and presented with a virtual reality (VR) setup on four monitor screens (MSI Optix G241) surrounding the mouse. The monitors were arranged in portrait orientation approximately along four sides of a hexagon centered on the mouse and covered a visual field of ~240 degrees horizontally and ~80 degrees vertically. The animals’ position in the VR environment was updated in closed loop based on the distance they traveled on the wheel recorded using a rotary encoder (Kubler 2400). The virtual reality environment was rendered in Bonsai software (<xref ref-type="bibr" rid="R55">55</xref>) using the BonVision package (<xref ref-type="bibr" rid="R56">56</xref>). The 3D environment was rendered using cube mapping, which projected a 360° visual scene onto the view ports around the mouse (<xref ref-type="bibr" rid="R56">56</xref>). The view ports were defined as the positions of the screens relative to the mouse, whose translation and rotational vectors were calibrated using ArUco markers (<xref ref-type="bibr" rid="R57">57</xref>, <xref ref-type="bibr" rid="R58">58</xref>) displayed on the screens and placed at the location of the mouse. The right eye was monitored with a camera (Basler acA1440-220um) synchronized with the two-photon acquisition (~ 15 Hz) and illuminated with an infrared light placed above the camera.</p><p id="P38">The VR environment had a gray background (11.9 cd/m<sup>2</sup>), a 5 cm wide floor with a checkerboard texture positioned 2.5 cm below the mouse in the VR, and 24 black spheres presented at varying virtual distances from the animal. The locations of the centers of the spheres were selected in cylindrical coordinates (<xref ref-type="supplementary-material" rid="SD1">Figure S1A</xref>). The longitudinal axis of the cylinder was aligned to the animals path of travel in the VR, while its radius was equal to the virtual depth of a given trial chosen pseudorandomly from a range of logarithmically spaced values (either 5 virtual depth values between 6 cm and 600 cm, or 8 depths between 5 and 640 cm). The spheres were spaced along the axis of the animal’s locomotion by 0.15 × <italic>d</italic> with 19 spheres ahead of and 5 spheres behind the mouse. The cylindrical azimuth of the spheres, which determines their position relative to the horizon, was randomly chosen from a uniform distribution between -40 and 40 degrees or between 140 and 220 degrees, i.e. around the horizon on either the left-hand side or right-hand side of the animal (<xref ref-type="supplementary-material" rid="SD1">Figure S1A</xref>). To maintain constant visual stimulation during the trial, whenever the animal moved the distance of 0.15 × <italic>d</italic> in the VR, resulting in a sphere disappearing outside of the monitors’ field of view behind the mouse, a new sphere was generated 2.7 × <italic>d</italic> ahead of the mouse at a randomized cylindrical azimuth. New spheres gradually faded in from gray over 0.3 s. The virtual radius of the spheres on trials of different virtual depths was equal to tan 5° × <italic>d</italic>, so that they covered the same angular extent in the visual field of the mice across different virtual depths (10 degrees when the mice passed closest to each sphere in the VR). The spheres were a solid black color without specular highlights.</p><p id="P39">Each trial continued until the mouse traveled 6 m after which the spheres faded to gray and a 10 s inter-trial period commenced. A probabilistic reward of soy milk (~8 <italic>μ</italic>l of 10% SMA Wysoy) was delivered from a spout in front of the mouse 2 s into the inter-trial interval on 60-80% of trials. Each session consisted of at least 10 trials for each virtual depth.</p><p id="P40">During open loop sessions, the animal’s movement on the wheel was recorded but did not control its virtual position in VR. Instead, the running trajectory of a previous closed loop session of the animal was replayed and determined the animal’s moment-to-moment virtual position and the updating of the sphere stimulus.</p><p id="P41">The nominal refresh rate of the four monitors was 144 Hz. To synchronize the visual stimuli with imaging data, and measure the actual frame rate, a small square (3 cm × 3 cm) with varying luminance was displayed in the bottom left corner of the leftmost monitor (<xref ref-type="supplementary-material" rid="SD1">Figure S1B</xref>). The luminance changes of this square were recorded with a photodiode at 1 kHz (HARP Behavior device, Champalimaud Research Scientific Hardware Platform). For 2 mice (25 sessions), the square alternated at every frame between black and white and was used to detect dropped frames. 89.7 ± 5.1% of the frame were presented at 144 Hz, resulting in a average frame rate per session above 124 ± 9 Hz. To determine the closed loop latency between rotary encoder inputs and updates of the visual stimulus, for the remaining 5 mice (60 sessions), we updated the brightness of the square following an irregular predefined sequence of 5 values (<xref ref-type="supplementary-material" rid="SD1">Figure S1B</xref>). The sequence was selected to ensure that brightness increases and decreases were alternating at each frame. We then detected frames on the photodiode trace and computed the cross-correlation between the filtered predefined sequence and the photodiode recording to identify which frame was presented at each time point and to continuously measure display lag. The average display lag per session was 26.6 ± 1.4 ms (<xref ref-type="supplementary-material" rid="SD1">Figure S1C-D</xref>).</p></sec><sec id="S18"><title>Behavioral training</title><p id="P42">To encourage mice to run in the VR environment, they underwent a 2-4 week training period under food restriction. Mice were acclimatized to the behavioral setup for at least 3 sessions of 5-15 min. Mice initiated their training with a minimum distance traveled per trial of 1 m that was gradually increased to 6 m depending on the animals’ running behavior. Each training session lasted 30 min - 1 h. Imaging commenced once the mice could reliably complete ~2 trials per minute. After the training period, food restriction continued during the imaging experiments.</p></sec><sec id="S19"><title>Two-photon calcium imaging</title><p id="P43">Fluorescence signals from neurons in layer 2/3 of the primary visual cortex (150-300 <italic>μ</italic>m below pia) were recorded using a custom-built resonant scanning two-photon microscope (Independent NeuroScience Services) and a Nikon 16x water-immersion objective (NA 0.8). The microscope was controlled with ScanImage 2020 software. GCaMP6f and GCaMP6s fluorescence signals were excited with a 930 nm laser (SpectraPhysics MaiTai DeepSee eHP) at 70-100 mW and acquired with a 525/50 emission filter (FF03-525/50-25, Semrock). Imaging frames (1024 x 1024 pixels) were acquired at ~15 Hz. Individual recordings covered a field of view of ~ 661 <italic>μ</italic>m x 661 <italic>μ</italic>m. To minimize the interference from the monitors’ during imaging, a custom electronic circuit activated the backlights only during the turn-around period of the resonant X mirror.</p></sec><sec id="S20"><title>Widefield calcium imaging</title><p id="P44">The locations of V1 and higher visual areas (HVAs) were determined using widefield calcium imaging of the whole visual cortical window. Widefield calcium imaging was performed using a camera integrated into the two-photon microscope (Independent NeuroScience Services) using a Nikon AF Nikkor 85mm f/1.8D camera lens as the objective and a 100 mm tube lens (Thorlabs AC508-100-A-ML). Two excitation LEDs at 470 nm (M470L3, Thorlabs, with excitation filter FF02-447/60-25, Semrock) and 405 nm (M405L4, Thorlabs, with excitation filter FF01-405/10-25, Semrock) were combined using a dichroic mirror (FF458-Di02-25x36, Semrock) and coupled to the infinity focused imaging path (FF495-Di03 BrightLine primary dichroic mirror). Images were acquired with an emission filter (FF03-525/50-25, Semrock) and a CMOS camera (Basler acA2040-120um) at 60 Hz. Illumination of the two LEDs was interleaved for hemodynamic correction using a microcontroller (Arduino Uno) triggered by the camera’s FlashWindow output.</p><p id="P45">During imaging, mice were head-fixed on the wheel and presented with a circular square-wave grating patch of 20 degrees diameter (at a spatial frequency of 0.2 cpd and a temporal frequency of 2 Hz) at 30 or 90 degrees azimuth and 0 degrees elevation on a black background. Each grating patch was presented for 4 seconds and grating direction (8 directions in 45 degree increments) changed every 500 ms in pseudorandom order. The grating patches were repeated 50 times for each location in pseudorandom order with 8 seconds of blank screen between presentations.</p><p id="P46">Images were first deinterleaved to separate 405 nm and 470 nm channels. Hemodynamic signal correction was carried out by performing linear regression on each pixel of the 470 nm channel’s signal using the 405 nm channel signal as the independent variable. The residuals of the regression added to the mean fitted signal of the 470 nm channel was used as the corrected widefield signal for each pixel. Corrected signals were detrended by applying a high-pass filter of 0.001 Hz. The signal at each pixel was normalized by its average value. The responses to the grating patches at each retinotopic location were averaged across repetitions to determine the locations of V1 and HVAs.</p></sec><sec id="S21"><title>Data analysis</title><p id="P47">To account for the nested structure of the experimental data, statistical comparisons were conducted using hierarchical bootstrap (<xref ref-type="bibr" rid="R59">59</xref>) unless specified otherwise. In analyses at the level of individual neurons, for each bootstrap sample we resampled mice, followed by experimental sessions for each mouse, and neurons for each session and calculated the statistic of interest, such as correlation coefficient between two variables, or difference in medians between two conditions, for each sample. We then determined the quantile <italic>q</italic> of the bootstrap distribution at 0 and calculated the two-sided bootstrap p-value as 2 min{<italic>q</italic>, 1 − <italic>q</italic>}.</p><sec id="S22"><title>Eye movements</title><p id="P48">To determine if the visual stimulation triggered eye movements, in a subset of sessions (N = 16 sessions from 2 mice) we reconstructed the gaze direction during behaviour. Eyelid position, the reflection of the infrared light and 12 key points around the pupil border were tracked on each frame using DeepLabCut (<xref ref-type="bibr" rid="R60">60</xref>). Frames in which the eye was closed (distance between top eyelid and bottom eyelid &lt; mean distance - 3 standard deviations, or DeepLabCut likelihood &lt;0.88) were excluded. For the remaining frames, the 12 markers around the pupil were fitted with an ellipse which was used to perform gaze reconstruction as previously described (<xref ref-type="bibr" rid="R61">61</xref>). Briefly, the eye was modeled as a 3D sphere with a circular pupil rotating at a constant radius around the the eye center. Translation of the eye relative to the camera was corrected using the infrared light reflection as the origin for each frame. The position of the eye center on the camera frame was defined as the least square estimate of the intersection of all minor axes of the fitted ellipses. The scale factor, which depends on the radius of the eye, was estimated based on the ratio of the short and long axis of the fitted ellipses and the position of the eye center (<xref ref-type="bibr" rid="R61">61</xref>). With these parameters, the pupil radius <italic>r</italic> and rotation angles relative to the camera axes, <italic>ϕ</italic> and <italic>θ</italic> define the reprojected pupil border in the camera frame. We performed a grid search for each frame to find the (<italic>r, ϕ, θ</italic>) minimizing the difference between the fitted and reprojected ellipses.</p><p id="P49">To estimate the azimuth and elevation from the gaze vector in camera coordinates, we calibrated the camera position using an ArUco marker (<xref ref-type="bibr" rid="R58">58</xref>) placed parallel to the floor in the camera field of view and pointing in the animal’s direction of travel. The pose of the marker relative to the camera was estimated using OpenCV (<xref ref-type="bibr" rid="R57">57</xref>) and used to rotate the gaze vector from camera coordinates to world coordinates.</p><p id="P50">Angular velocity (<xref ref-type="supplementary-material" rid="SD1">Figure S2K</xref>) was calculated on the mean filtered azimuth and elevation traces (rolling window of 3 frames). Saccades (<xref ref-type="supplementary-material" rid="SD1">Figure S2L</xref>) were defined as instances when the filtered velocity traces (calculated on median-filtered azimuth and elevation traces with a rolling window of 5 frames) exceeded 75 degrees per second, i.e. a 5 degrees change in a frame.</p></sec><sec id="S23"><title>Preprocessing of two-photon imaging data</title><p id="P51">Imaging frames were registered and segmented using the suite2p package (<xref ref-type="bibr" rid="R62">62</xref>) with anatomical segmentation (Cellpose (<xref ref-type="bibr" rid="R63">63</xref>)) to avoid bias towards active cells. Fluorescence traces were detrended by subtracting a rolling baseline calculated as the 20th percentile in a moving window of 900 frames. To remove neuropil contamination of the fluorescence trace for the ROIs, we fitted the fluorescence of the ROIs and their surrounding areas with asymmetric Student-t distributions. The mean of these distributions depended on a shared neuropil signal that affected both ROI and surrounding fluorescence (<xref ref-type="bibr" rid="R64">64</xref>). Finally, we estimated <italic>F</italic><sub>0</sub>, by fitting a Gaussian mixture model with 2 components to the fluorescence time series and identified <italic>F</italic><sub>0</sub> as the mean of the smaller Gaussian. Δ<italic>F</italic> /<italic>F</italic><sub>0</sub> was defined as Δ<italic>F/F</italic><sub>0</sub> = (<italic>F</italic> − <italic>F</italic><sub>0</sub>)<italic>/F</italic><sub>0</sub>, where <italic>F</italic> was the fluorescence value after neuropil correction.</p></sec><sec id="S24"><title>Depth selectivity</title><p id="P52">To determine the preferred virtual depth for each ROI, we calculated the mean Δ<italic>F</italic>/<italic>F</italic><sub>0</sub> across each trial for each neuron, and fitted a Gaussian model to the average single trial responses <italic>f</italic> and the corresponding log-transformed virtual depth <italic>d</italic> displayed in each trial: <disp-formula id="FD1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>a</italic> was the peak response amplitude, <italic>d</italic><sub>0</sub> was the log-preferred depth, <italic>σ</italic> was the tuning width, and <italic>b</italic> was the baseline fluorescence. The preferred virtual depth <italic>d</italic><sub>0</sub> was constrained between ln2 and ln2000 cm, and the peak amplitude <italic>a</italic> was constrained to be positive. The tuning width <italic>σ</italic><sup>2</sup> was constrained to &gt;0.5 to avoid overfitting. All depth tuning curves were plotted using this fit.</p><p id="P53">To identify depth-selective neurons, we fitted the Gaussian model using 5-fold cross validation. On each fold, 80% of trials were assigned to the training set, which was used to estimate model parameters and 20% were assigned to the test set, which was used to evaluate model predictions. Then we calculated the Spearman’s correlation coefficient of predicted and observed fluorescence on test trials across all 5 folds. Depth-selective neurons were defined as having a Spearman’s correlation p value &lt;0.05 and the Spearman’s correlation coefficient &gt;0.1.</p><p id="P54">Near-preferring cells and far-preferring cells (<xref ref-type="fig" rid="F1">Figure 1K</xref>) were defined as the cells with preferred virtual depths close to the bounds of <italic>d</italic><sub>0</sub> (log-preferred virtual depth &lt; ln2 + 10<sup>−4</sup> for near-preferring cells, and &gt; ln2000 − 10<sup>−4</sup> for far-preferring cells).</p></sec><sec id="S25"><title>Optic flow and running speed tuning</title><p id="P55">To visualize the optic flow speed and running speed tuning curves in <xref ref-type="fig" rid="F2">Figure 2B</xref> and <xref ref-type="supplementary-material" rid="SD1">S4B,F</xref>, we first smoothed the sum of responses from all frames and the number of frames at different bins of running speed or log-transformed optic flow speed respectively, using a 1D Gaussian kernel with a standard deviation of 1 bin width (10 cm/s for running speed, ~ 0.691 for log-transformed optic flow speed). The tuning curves were calculated by dividing the smoothed sum of response by the smoothed number of frames to account for the number of samples in different bins. Only frames with a running speed above 1 cm/s were included for the optic flow speed tuning curves to facilitate the logarithmic transformation of the optic flow speeds.</p><p id="P56">To determine how optic flow and locomotion-related signals were integrated and estimate preferred optic flow and running speeds, we fitted the Δ<italic>F/F</italic><sub>0</sub> trace as a function of log-transformed optic flow speed <italic>v</italic> and running speed <italic>r</italic> on individual imaging frames when mice were moving at &gt;1 cm/s using 5 models.</p><p id="P57">The optic flow only and running speed only models were Gaussian functions of log-optic flow speed <italic>v</italic> and log-running speed <italic>r</italic>, respectively: <disp-formula id="FD2"><label>(2)</label><mml:math id="M2"><mml:mrow><mml:mtext>Δ</mml:mtext><mml:mi>F</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> <disp-formula id="FD3"><label>(3)</label><mml:math id="M3"><mml:mrow><mml:mtext>Δ</mml:mtext><mml:mi>F</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P58">The additive model fitted responses as the sum of Gaussian functions of log-optic flow speed <italic>v</italic> and log-running speed <italic>r</italic>: <disp-formula id="FD4"><label>(4)</label><mml:math id="M4"><mml:mrow><mml:mtext>Δ</mml:mtext><mml:mi>F</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>σ</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mrow/><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P59">The conjunctive model consisted of a bivariate Gaussian tuned to both log-optic flow speed <italic>v</italic> and log-running speed <disp-formula id="FD5"><label>(5)</label><mml:math id="M5"><mml:mrow><mml:mtext>Δ</mml:mtext><mml:mi>F</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> Where <disp-formula id="FD6"><label>(6)</label><mml:math id="M6"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mtext>cos</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mtext>sin</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> <disp-formula id="FD7"><label>(7)</label><mml:math id="M7"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mtext>sin</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mtext>cos</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> <disp-formula id="FD8"><label>(8)</label><mml:math id="M8"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>sin</mml:mtext><mml:mn>2</mml:mn><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mtext>sin</mml:mtext><mml:mn>2</mml:mn><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula> <italic>a, a</italic><sub><italic>r</italic></sub>, and <italic>a</italic><sub><italic>v</italic></sub> were the peak amplitude of the overall response and the running speed and optic flow components, respectively. <italic>σ</italic><sub><italic>v</italic></sub> and <italic>σ</italic><sub><italic>r</italic></sub> were the tuning width for optic flow and running speed respectively, <italic>b</italic> was the baseline activity, and <italic>v</italic><sub>0</sub> and <italic>r</italic><sub>0</sub> were the log-preferred optic flow and log-preferred running speeds. For the conjunctive model, <italic>θ</italic> determined the orientation of the axes of the bivariate Gaussian, and <italic>σ</italic><sub>1</sub> and <italic>σ</italic><sub>2</sub> were the standard deviation for its two axes.</p><p id="P60">The ratio model was a Gaussian function of the difference between log-running speed and log-optic flow speed, equivalent to the logarithm of their ratio: <disp-formula id="FD9"><label>(9)</label><mml:math id="M9"><mml:mrow><mml:mtext>Δ</mml:mtext><mml:mi>F</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:mi>v</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>σ</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mrow/><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>d</italic><sub>0</sub> was the log-preferred virtual depth and <italic>σ</italic><sub><italic>d</italic></sub> was the depth tuning width. This model is similar to the depth tuning model in <xref ref-type="disp-formula" rid="FD1">Eq. 1</xref> fitted on individual imaging frames rather than trial-averages.</p><p id="P61">The preferred optic flow speed <inline-formula><mml:math id="M10"><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> was constrained between 0.03 to 3000 degrees/s. The preferred running speed <inline-formula><mml:math id="M11"><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> was constrained between 0.5 to 500 cm/s. <italic>d</italic><sub>0</sub> was constrained between 0.0095 and 9.5 × 10<sup>5</sup> cm according to the lower and upper bounds of <inline-formula><mml:math id="M12"><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M13"><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="M14"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> were constrained to be &gt;0.25. <italic>θ</italic> was constrained between 0 and 90 degrees. <italic>a, a</italic><sub><italic>r</italic></sub>, and <italic>a</italic><sub><italic>v</italic></sub> were constrained to be &gt;0.</p><p id="P62">To facilitate the comparison of the models in <xref ref-type="fig" rid="F2">Figure 2E</xref>, we selected strongly responsive neurons defined as depth-selective neurons (defined as above) with a peak response at their preferred depth &gt;0.2 (6,277 of 31,013 depth-selective neurons from 85 sessions). To determine which model best described neuronal activity for individual neurons, we fitted each model with 5-fold cross validation. On each fold, individual trials were assigned to the training set (80%), which was used to estimate model parameters, or test set (20%), which was used to evaluate model predictions. We then computed the predicted fluorescence <inline-formula><mml:math id="M15"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mtext>Δ</mml:mtext><mml:mi>F</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> on test trials using parameters estimated on the training set for each fold and compared it to observed Δ<italic>F/F</italic><sub>0</sub> to compute <italic>R</italic><sup>2</sup> as the fraction of variance explained by the model: <disp-formula id="FD10"><label>(10)</label><mml:math id="M16"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mtext>Var</mml:mtext><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtext>Δ</mml:mtext><mml:mi>F</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mrow><mml:mtext>Δ</mml:mtext><mml:mi>F</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mtext>Var</mml:mtext><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtext>Δ</mml:mtext><mml:mi>F</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P63">The best model for each neuron was selected as the model with the highest <italic>R</italic><sup>2</sup>. To compare performance between models across the dataset, we first calculated the proportion of neurons best fitted by each model for each recording session and then used hierarchical bootstrap (<xref ref-type="bibr" rid="R59">59</xref>) to resample mice and sessions 20000 times. We then compared the difference of the median proportions between models.</p><p id="P64">To estimate the preferred running speed and optic flow speed for each neuron, we fitted the Δ<italic>F</italic> /<italic>F</italic><sub>0</sub> trace, the corresponding optic flow speed and running speed from all trials using the conjunctive model. Closed loop recordings and open loop recordings were fitted separately. The neurons plotted in <xref ref-type="fig" rid="F2">Figure 2F-G,J-L</xref> were the depth-selective neurons well-fitted by the conjunctive model in closed loop recordings (cross-validated <italic>R</italic><sup>2</sup>&gt;0.02 computed as described above, 5,737 of 31,013 depth-selective neurons recorded in closed loop conditions from 84 out of 85 sessions; no neurons passed this threshold in 1 session). To quantify the correlation between preferred depth, preferred running speed, preferred optic flow speed or the ratio between preferred running and optic flow speed in <xref ref-type="fig" rid="F2">Figure 2F</xref> and <xref ref-type="supplementary-material" rid="SD1">Figure S4I, J</xref>, we first calculated the Spearman’s correlation coefficient between the two variables and calculated the p-value using hierarchical bootstrap by resampling mice, sessions, and neurons as described above.</p><p id="P65">To compare the preferred running speed and preferred optic flow speed of depth-selective neurons in closed loop vs. open loop trials in <xref ref-type="fig" rid="F2">Figure 2J-K</xref>, we chose depth-selective neurons with good fits to the conjunctive model (cross-validated <italic>R</italic><sup>2</sup>&gt;0.02) in both closed and open loop conditions (1,234 of 10,850 depth-selective neurons recorded in open loop from 34 sessions). Then, we computed the Spearman’s correlation coefficient between the variables and calculated the p-value using hierarchical bootstrap by resampling mice, sessions, and neurons as described above.</p><p id="P66">To compare the peak response amplitudes of depth-selective neurons in closed loop vs. open loop trials in <xref ref-type="fig" rid="F2">Figure 2L</xref>, we calculated the peak response amplitude using the conjunctive model fit (sum of amplitude <italic>a</italic> and offset <italic>b</italic>). The selection criteria for neurons was the same as <xref ref-type="fig" rid="F2">Figure 2J-K</xref>. Then we compared the median ratio between the peak response amplitude of closed loop and open loop trials to 1 using hierarchical bootstrap by resampling mice, sessions, and neurons as described above.</p></sec><sec id="S26"><title>Receptive field estimation</title><p id="P67">To estimate the receptive field for each neuron, we used the recorded sphere locations and the animal’s position in the VR to reconstruct the three-dimensional visual stimulus presented for each imaging frame. The first two dimensions corresponded to retinotopic location in spherical coordinates with a resolution of 5 degrees in azimuth and elevation, while the third dimension corresponded to the virtual depth of the stimulus on a given trial. We then constructed a design matrix representing the visual stimuli <italic>S</italic>, where each row contained the stimulus reconstruction for the corresponding imaging frame and for each virtual depth, as well as a column of 1s to account for a bias term. The number of rows corresponded to the number of imaging frames. Therefore, the dimension of <italic>S</italic> was <italic>N</italic><sub>frames</sub> × (<italic>N</italic><sub>pixels</sub> ∗<italic>N</italic><sub>depths</sub>) + 1, where <italic>N</italic><sub>pixels</sub> was the number of pixels on each visual stimulus frame, <italic>N</italic><sub>depths</sub> was the number of virtual depths, and the <italic>N</italic><sub>frames</sub> was the number of imaging frames. <italic>S</italic> was reconstructed based on the stimuli presented on either the two screens on the right-hand side of the mouse or the two screens on the left-hand side of the mouse. We used a linear model to fit Δ<italic>F/F</italic><sub>0</sub>: <disp-formula id="FD11"><label>(11)</label><mml:math id="M17"><mml:mrow><mml:mtext>Δ</mml:mtext><mml:mi>F</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mtext>b</mml:mtext></mml:mrow></mml:math></disp-formula></p><p id="P68">We used the regularized pseudoinverse method (<xref ref-type="bibr" rid="R65">65</xref>, <xref ref-type="bibr" rid="R66">66</xref>) to impose a smoothness constraint on the receptive field in azimuth and elevation and in virtual depth. The constraints aimed to make the Laplacian of the receptive field close to zero at all points. To impose this constraint, we constructed matrices <italic>L</italic><sub><italic>xy</italic></sub> and <italic>L</italic><sub><italic>depth</italic></sub>. <italic>L</italic><sub><italic>xy</italic></sub> aimed to smooth the receptive fields across azimuth and elevation. To assemble the matrix <italic>L</italic><sub><italic>xy</italic></sub>, we first generated Laplace matrices that contained the values <disp-formula id="FD12"><label>(12)</label><mml:math id="M18"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mn>4</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula> corresponding to adjacent <italic>x</italic> and <italic>y</italic> pixels for a single depth embedded in <italic>N</italic><sub><italic>y</italic></sub> × <italic>N</italic><sub><italic>x</italic></sub> × <italic>N</italic><sub>depth</sub> matrix of zeros for each <italic>x</italic> and <italic>y</italic> pixel location and depth. These matrices were flattened and assigned to individual rows of <italic>L</italic><sub><italic>xy</italic></sub>.</p><p id="P69"><italic>L</italic><sub><italic>depth</italic></sub> controlled the smoothness the receptive fields across different virtual depths. To assemble <italic>L</italic><sub><italic>depth</italic></sub>, we generated Laplace matrices that contained the values [−1 2 −1] corresponding to adjacent depths for a single pixel embedded in a <italic>N</italic><sub><italic>y</italic></sub> × <italic>N</italic><sub><italic>x</italic></sub> × <italic>N</italic><sub>depth</sub> matrix of zeros for each <italic>x</italic> and <italic>y</italic> pixel location and depth. These matrices were flattened and assigned to individual rows of <italic>L</italic><sub><italic>depth</italic></sub>.</p><p id="P70">We appended a column of 0s to <italic>L</italic><sub><italic>xy</italic></sub> and <italic>L</italic><sub><italic>depth</italic></sub> so as not to regularize the bias term. We then assembled the design matrix <italic>X</italic> containing the stimulus matrix <italic>S</italic> and the smoothness constraints <italic>L</italic><sub><italic>xy</italic></sub> and <italic>L</italic><sub><italic>depth</italic></sub>: <disp-formula id="FD13"><label>(13)</label><mml:math id="M19"><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>S</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula> <italic>λ</italic><sub><italic>xy</italic></sub> and <italic>λ</italic><sub><italic>depth</italic></sub> are scalars that control the strength of regularization of receptive field smoothness across azimuth and elevation (<italic>λ</italic><sub><italic>xy</italic></sub>) and across virtual depths (<italic>λ</italic><sub><italic>depth</italic></sub>). We constructed an augmented response vector by appending 0s to the Δ<italic>F/F</italic><sub>0</sub> fluorescence vector corresponding to each row of <italic>L</italic><sub><italic>xy</italic></sub> and <italic>L</italic><sub><italic>depth</italic></sub> <disp-formula id="FD14"><label>(14)</label><mml:math id="M20"><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mtext>Δ</mml:mtext><mml:mi>F</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋅</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋅</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋅</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P71">To estimate the receptive field we found the least squares solution for the equation <bold>y</bold> = <italic>X</italic><bold>b</bold> as <disp-formula id="FD15"><label>(15)</label><mml:math id="M21"><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mover accent="true"><mml:mi>b</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mstyle><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>X</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P72">The first <italic>N</italic><sub>pixels</sub> elements of <inline-formula><mml:math id="M22"><mml:mrow><mml:mover accent="true"><mml:mtext mathvariant="bold">b</mml:mtext><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> correspond to coefficients of the receptive field, while the last element represents the bias term.</p><p id="P73">To optimize hyperparameters <italic>λ</italic><sub><italic>xy</italic></sub> and <italic>λ</italic><sub><italic>depth</italic></sub>, we used 5-fold cross validation to split imaging trials into training and test sets and estimated <inline-formula><mml:math id="M23"><mml:mrow><mml:mover accent="true"><mml:mtext mathvariant="bold">b</mml:mtext><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> on each training set. We then computed the predicted fluorescence <disp-formula id="FD16"><label>(16)</label><mml:math id="M24"><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mstyle><mml:mo>=</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mtext>test</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mover accent="true"><mml:mi>b</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mstyle></mml:mrow></mml:math></disp-formula> for each test set and found the combination of <italic>λ</italic><sub><italic>xy</italic></sub> and <italic>λ</italic><sub><italic>depth</italic></sub> which yielded the highest fraction of variance explained by the model for each ROI by searching over a 13 x 13 grid of values logarithmically spaced between 2.5 and 10240. The receptive field for each neuron was calculated based on the best combination of <italic>λ</italic><sub><italic>xy</italic></sub> and <italic>λ</italic><sub><italic>depth</italic></sub> by averaging the coefficients <inline-formula><mml:math id="M25"><mml:mrow><mml:mover accent="true"><mml:mtext mathvariant="bold">b</mml:mtext><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> across 5 folds. The peak azimuth and elevation of the receptive field (RF<sub>azi</sub> and RF<sub>ele</sub>) were defined as the position with the maximum coefficient value.</p><p id="P74">To identify the neurons with significant receptive fields, we used the procedure above to optimize <italic>λ</italic><sub><italic>xy</italic></sub> and <italic>λ</italic><sub><italic>depth</italic></sub> and fit the receptive field using the stimuli in the right visual field (RF<sub>right</sub>). To estimate the null distribution for receptive field coefficients for each neuron, we used these values of <italic>λ</italic><sub><italic>xy</italic></sub> and <italic>λ</italic><sub><italic>depth</italic></sub> to fit the receptive field using stimuli in the left visual field (RF<sub>left</sub>), which have the same statistics but are not expected to drive neuronal activity recorded in the left visual cortex. We calculated the mean and standard deviation of the coefficients in RF<sub>left</sub> and defined the ROIs with significant receptive fields as those with a maximum value of the RF<sub>right</sub> which was 6 standard deviations above the mean of RF<sub>left</sub>. The 6 standard deviations threshold was chosen such that ~ 5% of neurons passed this threshold if it was applied to RF<sub>left</sub>.</p></sec><sec id="S27"><title>Cortical location</title><p id="P75">To determine the spatial location of imaged neurons within V1 in individual animals, we first found the location of neurons within the field of view (FOV) for each recording session based on their cell masks. We then aligned the location of each FOV within the whole visual cortical window by matching the blood vessel patterns between the surface of the FOVs and the overview image of the window acquired using the widefield camera.</p><p id="P76">To align the location of imaged neurons across animals, we assumed that the RF azimuth and elevation followed the same linear gradient in V1 across animals. Under this assumption, the overview location <italic>x</italic><sub>overview</sub> and <italic>y</italic><sub>overview</sub> of individual neurons depends linearly on their RF azimuth and elevation with a translational offset that can vary across mice: <disp-formula id="FD17"><label>(17)</label><mml:math id="M26"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mtext>overview</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mtext>R</mml:mtext><mml:msub><mml:mtext>F</mml:mtext><mml:mrow><mml:mtext>azi</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mtext>R</mml:mtext><mml:msub><mml:mtext>F</mml:mtext><mml:mrow><mml:mtext>ele</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mtext>1</mml:mtext><mml:mo>,</mml:mo><mml:mtext>mouse</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula> <disp-formula id="FD18"><label>(18)</label><mml:math id="M27"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mtext>overview</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mtext>R</mml:mtext><mml:msub><mml:mtext>F</mml:mtext><mml:mrow><mml:mtext>azi</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mtext>R</mml:mtext><mml:msub><mml:mtext>F</mml:mtext><mml:mrow><mml:mtext>ele</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mtext>mouse</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p id="P77">We used Huber regression to fit the coefficients and individual mouse offsets ensuring robustness to outliers. We then chose a reference mouse and aligned the neurons’ coordinates of all other mice to the reference mouse by subtracting the offset of the original mouse and adding back the offset of the reference mouse: <disp-formula id="FD19"><label>(19)</label><mml:math id="M28"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mtext>aligned</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mtext>overview</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mtext>1</mml:mtext><mml:mo>,</mml:mo><mml:mtext>mouse</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mtext>1</mml:mtext><mml:mo>,</mml:mo><mml:mtext>reference</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula> <disp-formula id="FD20"><label>(20)</label><mml:math id="M29"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mtext>aligned </mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mtext>overview </mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mtext>mouse </mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mtext>reference</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p></sec><sec id="S28"><title>Depth selectivity in relation to RF location</title><p id="P78">To illustrate the spatial distribution of RF azimuth, elevation and the preferred depth in <xref ref-type="fig" rid="F4">Figure 4B-D</xref>, we plotted these parameters of all depth-selective neurons with a significant RF at the location of individual neurons. In the lower panels, we smoothed the spatial distribution map with a 113 <italic>μ</italic>m Gaussian kernel. To set the transparency of the smoothed map, we calculated the sum of Gaussian weights for all plotted neurons for each pixel, normalized by its maximum value. The alpha for each pixel was set to either 5 times this value or 1, whichever was smaller.</p><p id="P79">As the virtual distance between the spheres and the mouse varied based on the spheres’ positions, in the analyses in in <xref ref-type="fig" rid="F4">Figure 4E-F</xref> we calculated the corrected log-preferred virtual depth <inline-formula><mml:math id="M30"><mml:mrow><mml:msubsup><mml:mi>d</mml:mi><mml:mn>0</mml:mn><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>, corresponding to the distance to the spheres at the center of the neurons’ receptive field at the peak of their depth tuning curve (<xref ref-type="disp-formula" rid="FD1">Eq. 1</xref>): <disp-formula id="FD21"><label>(21)</label><mml:math id="M31"><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msubsup><mml:mi>d</mml:mi><mml:mn>0</mml:mn><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mtext>sin</mml:mtext></mml:mrow><mml:mtext>2</mml:mtext></mml:msup><mml:msub><mml:mrow><mml:mtext>RF</mml:mtext></mml:mrow><mml:mrow><mml:mtext>azi</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mtext>cos</mml:mtext></mml:mrow><mml:mtext>2</mml:mtext></mml:msup><mml:msub><mml:mrow><mml:mtext>RF</mml:mtext></mml:mrow><mml:mrow><mml:mtext>ele</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mtext>+sin</mml:mtext></mml:mrow><mml:mtext>2</mml:mtext></mml:msup><mml:msub><mml:mrow><mml:mtext>RF</mml:mtext></mml:mrow><mml:mrow><mml:mtext>ele</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P80">To test for the presence of a gradient in preferred depth as a function of azimuth and elevation, we used hierarchical bootstrap (<xref ref-type="bibr" rid="R59">59</xref>) to resample neurons across mice and recording sessions. For each bootstrap sample, we performed linear regression between the RF<sub>azi</sub> and RF<sub>ele</sub> of the center of the neurons’ receptive fields and the neurons’ corrected log-preferred virtual depth: <disp-formula id="FD22"><label>(22)</label><mml:math id="M32"><mml:mrow><mml:msubsup><mml:mi>d</mml:mi><mml:mn>0</mml:mn><mml:mo>′</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mtext>azi</mml:mtext></mml:mrow></mml:msub><mml:mtext>R</mml:mtext><mml:msub><mml:mtext>F</mml:mtext><mml:mrow><mml:mtext>azi</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mtext>ele</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mtext>R</mml:mtext><mml:msub><mml:mtext>F</mml:mtext><mml:mrow><mml:mtext>ele</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>k</italic><sub><italic>azi</italic></sub> and <italic>k</italic><sub><italic>ele</italic></sub> defined the gradient of depth as a function of azimuth and elevation of the neurons’ receptive fields, respectively.</p><p id="P81">To test for statistical significance, we modeled the coefficients (<italic>k</italic><sub>azi</sub>, <italic>k</italic><sub>ele</sub>) obtained from different bootstrap samples using a bivariate Normal distribution and used a multivariate generalization of the Z-test to compare their mean to 0. To this end, we first computed the mean <bold><italic>μ</italic></bold> and covariance Σ of bootstrap samples (<italic>k</italic><sub>azi</sub>, <italic>k</italic><sub>ele</sub>). As the square of the Mahalanobis distance <inline-formula><mml:math id="M33"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mtext>Σ</mml:mtext><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">μ</mml:mi></mml:mrow></mml:math></inline-formula> follows the chi-squared distribution with 2 degrees of freedom, the p-value is calculated as <inline-formula><mml:math id="M34"><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mtext>Σ</mml:mtext><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="S29"><title>Population decoding</title><p id="P82">To decode virtual depth from population activity, we trained a linear SVM classifier using Δ<italic>F</italic> /<italic>F</italic><sub>0</sub> values of simultaneously recorded neurons in a session. We split imaging trials into training (64%), validation (16%) and test (20%) sets with 5-fold cross validation such that each trial was included in one test set. We then trained the classifier on individual imaging frames from trials in the training set using the validation set to optimize the hyperparameter <italic>C</italic>. We used the test sets to evaluate the performance of the optimized classifier to obtain the accuracy and the confusion matrix of predicted and true virtual depths. Confusion matrices in <xref ref-type="fig" rid="F1">Figures 1M</xref> and <xref ref-type="fig" rid="F2">2N</xref> show decoder performance as the proportion of imaging frames for each true depth.</p><p id="P83">To determine how decoding accuracy depends on the animals’ running speed, we trained a single decoder using data across all running speeds for each session. We then evaluated decoder performance on imaging frames from trials in the test set belonging to different running speed bins. <xref ref-type="fig" rid="F1">Figure 1M</xref> shows mean decoding error across sessions. Confidence intervals were computed using bootstrap by resampling sessions.</p><p id="P84">To compare decoding accuracy between closed loop and open loop trials, we trained and evaluated separate SVM classifiers on closed loop and open loop data.</p></sec></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Materials</label><media xlink:href="EMS199155-supplement-Supplementary_Materials.pdf" mimetype="application" mime-subtype="pdf" id="d101aAcEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S30"><title>Acknowledgments</title><p>This work was supported by the Francis Crick Institute which receives its core funding from Cancer Research UK (CC2108), the UK Medical Research Council (CC2108), and the Wellcome Trust (CC2108). We thank M. Florencia Iacaruso, Chunyu Ann Duan, and Ivana Orsolic for comments on the manuscript. We thank the Francis Crick Institute’s Making Lab, Biological Research Facility, and Scientific Computing for technical support. This preprint was typeset in LaTeX based on <ext-link ext-link-type="uri" xlink:href="https://www.overleaf.com/latex/templates/henriqueslab-biorxiv-template/nyprsybwffws">a template created by Ricardo Henriques</ext-link>.</p></ack><sec id="S31" sec-type="data-availability"><title>Data and code availability</title><p id="P85">Preprocessed data, including fluorescence traces of individual neurons and metadata required to reproduce the analyses will be deposited on Figshare and made publicly available prior to publication. Both preprocessed and raw image data is available from Petr Znamenskiy (<email>petr.znamenskiy@crick.ac.uk</email>) upon request. Original code required to reproduce the analyses is available on Github at <ext-link ext-link-type="uri" xlink:href="https://github.com/znamlab/v1_depth_map">https://github.com/znamlab/v1_depth_map</ext-link>.</p><p id="P86">Bonsai workflows for visual stimulus presentation are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/znamlab/vis-stim-depth">https://github.com/znamlab/vis-stim-depth</ext-link>. Original code for preprocessing of two-photon imaging data is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/znamlab/2p-preprocess">https://github.com/znamlab/2p-preprocess</ext-link>.</p></sec><fn-group><fn fn-type="con" id="FN2"><p id="P87"><bold>Author contributions</bold></p><p id="P88">Y.H. and P.Z. designed the experiments; Y.H. and A.B. developed the experimental setup; Y.H. and A.C.N. performed the experiments; Y.H., A.B. and P.Z. analyzed the data; Y.H and P.Z. wrote the manuscript.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walk</surname><given-names>RD</given-names></name><name><surname>Gibson</surname><given-names>EJ</given-names></name></person-group><article-title>A comparative and analytical study of visual depth perception</article-title><source>Psychological Monographs: General and Applied</source><year>1961</year><volume>75</volume><issue>15</issue><fpage>1</fpage><lpage>44</lpage><pub-id pub-id-type="doi">10.1037/h0093827</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ranftl</surname><given-names>R</given-names></name><name><surname>Lasinger</surname><given-names>K</given-names></name><name><surname>Hafner</surname><given-names>D</given-names></name><name><surname>Schindler</surname><given-names>K</given-names></name><name><surname>Koltun</surname><given-names>V</given-names></name></person-group><article-title>Towards robust monocular depth estimation: mixing datasets for zero-shot cross-dataset transfer</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><year>2022</year><volume>44</volume><issue>3</issue><fpage>1623</fpage><lpage>1637</lpage><pub-id pub-id-type="pmid">32853149</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>B</given-names></name><name><surname>Krähenbühl</surname><given-names>P</given-names></name><name><surname>Koltun</surname><given-names>V</given-names></name></person-group><article-title>Does computer vision matter for action?</article-title><source>Science Robotics</source><year>2019</year><volume>4</volume><issue>30</issue><elocation-id>eaaw6661</elocation-id><pub-id pub-id-type="pmid">33137779</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trychin</surname><given-names>S</given-names></name><name><surname>Walk</surname><given-names>RD</given-names></name></person-group><article-title>A study of the depth perception of monocular hooded rats on the visual cliff</article-title><source>Psychonomic Science</source><year>1964</year><volume>1</volume><issue>1</issue><fpage>53</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.3758/BF03342786</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barlow</surname><given-names>HB</given-names></name><name><surname>Blakemore</surname><given-names>C</given-names></name><name><surname>Pettigrew</surname><given-names>JD</given-names></name></person-group><article-title>The neural mechanism of binocular depth discrimination</article-title><source>The Journal of Physiology</source><year>1967</year><volume>193</volume><issue>2</issue><fpage>327</fpage><lpage>342</lpage><pub-id pub-id-type="pmcid">PMC1365600</pub-id><pub-id pub-id-type="pmid">6065881</pub-id><pub-id pub-id-type="doi">10.1113/jphysiol.1967.sp008360</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nadler</surname><given-names>JW</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><article-title>A neural representation of depth from motion parallax in macaque visual cortex</article-title><source>Nature</source><year>2008</year><volume>452</volume><issue>7187</issue><fpage>642</fpage><lpage>645</lpage><pub-id pub-id-type="pmcid">PMC2422877</pub-id><pub-id pub-id-type="pmid">18344979</pub-id><pub-id pub-id-type="doi">10.1038/nature06814</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scholl</surname><given-names>B</given-names></name><name><surname>Burge</surname><given-names>J</given-names></name><name><surname>Priebe</surname><given-names>NJ</given-names></name></person-group><article-title>Binocular integration and disparity selectivity in mouse primary visual cortex</article-title><source>Journal of Neurophysiology</source><year>2013</year><volume>109</volume><issue>12</issue><fpage>3013</fpage><lpage>3024</lpage><pub-id pub-id-type="pmcid">PMC3680810</pub-id><pub-id pub-id-type="pmid">23515794</pub-id><pub-id pub-id-type="doi">10.1152/jn.01021.2012</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Samonds</surname><given-names>JM</given-names></name><name><surname>Choi</surname><given-names>V</given-names></name><name><surname>Priebe</surname><given-names>NJ</given-names></name></person-group><article-title>Mice discriminate stereoscopic surfaces without fixating in depth</article-title><source>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</source><year>2019</year><volume>39</volume><issue>41</issue><fpage>8024</fpage><lpage>8037</lpage><pub-id pub-id-type="pmcid">PMC6786824</pub-id><pub-id pub-id-type="pmid">31462533</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0895-19.2019</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>La Chioma</surname><given-names>A</given-names></name><name><surname>Bonhoeffer</surname><given-names>T</given-names></name><name><surname>Hübener</surname><given-names>M</given-names></name></person-group><article-title>Area-specific mapping of binocular disparity across mouse visual cortex</article-title><source>Current Biology</source><year>2019</year><volume>29</volume><issue>17</issue><fpage>2954</fpage><lpage>2960</lpage><elocation-id>e5</elocation-id><pub-id pub-id-type="pmid">31422884</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McKee</surname><given-names>SP</given-names></name><name><surname>Taylor</surname><given-names>DG</given-names></name></person-group><article-title>The precision of binocular and monocular depth judgments in natural settings</article-title><source>Journal of vision</source><year>2010</year><volume>10</volume><issue>10</issue><fpage>5</fpage><pub-id pub-id-type="pmcid">PMC2951307</pub-id><pub-id pub-id-type="pmid">20884470</pub-id><pub-id pub-id-type="doi">10.1167/10.10.5</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walk</surname><given-names>RD</given-names></name><name><surname>Dodge</surname><given-names>SH</given-names></name></person-group><article-title>Visual depth perception of a 10-month-old monocular human infant</article-title><source>Science</source><year>1962</year><volume>137</volume><issue>3529</issue><fpage>529</fpage><lpage>530</lpage><pub-id pub-id-type="pmid">14004557</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ellard</surname><given-names>CG</given-names></name><name><surname>Goodale</surname><given-names>MA</given-names></name><name><surname>Timney</surname><given-names>B</given-names></name></person-group><article-title>Distance estimation in the mongolian gerbil: The role of dynamic depth cues</article-title><source>Behavioural Brain Research</source><year>1984</year><volume>14</volume><issue>1</issue><fpage>29</fpage><lpage>39</lpage><pub-id pub-id-type="pmid">6518079</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parker</surname><given-names>PRL</given-names></name><name><surname>Abe</surname><given-names>ETT</given-names></name><name><surname>Beatie</surname><given-names>NT</given-names></name><name><surname>Leonard</surname><given-names>ESP</given-names></name><name><surname>Martins</surname><given-names>DM</given-names></name><name><surname>Sharp</surname><given-names>SL</given-names></name><name><surname>Wyrick</surname><given-names>DG</given-names></name><name><surname>Mazzucato</surname><given-names>L</given-names></name><name><surname>Niell</surname><given-names>CM</given-names></name></person-group><article-title>Distance estimation from monocular cues in an ethological visuomotor task</article-title><source>eLife</source><year>2022</year><volume>11</volume><elocation-id>e74708</elocation-id><pub-id pub-id-type="pmcid">PMC9489205</pub-id><pub-id pub-id-type="pmid">36125119</pub-id><pub-id pub-id-type="doi">10.7554/eLife.74708</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rogers</surname><given-names>B</given-names></name><name><surname>Graham</surname><given-names>M</given-names></name></person-group><article-title>Motion parallax as an independent cue for depth perception</article-title><source>Perception</source><year>1979</year><volume>8</volume><issue>2</issue><fpage>125</fpage><lpage>134</lpage><pub-id pub-id-type="pmid">471676</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kral</surname><given-names>K</given-names></name></person-group><article-title>Behavioural–analytical studies of the role of head movements in depth perception in insects, birds and mammals</article-title><source>Behavioural Processes</source><year>2003</year><volume>64</volume><issue>1</issue><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="pmid">12914988</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nadler</surname><given-names>JW</given-names></name><name><surname>Nawrot</surname><given-names>M</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><article-title>MT neurons combine visual motion with a smooth eye movement signal to code depth-sign from motion parallax</article-title><source>Neuron</source><year>2009</year><volume>63</volume><issue>4</issue><fpage>523</fpage><lpage>532</lpage><pub-id pub-id-type="pmcid">PMC2736789</pub-id><pub-id pub-id-type="pmid">19709633</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2009.07.029</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>HR</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><article-title>A functional link between MT neurons and depth perception based on motion parallax</article-title><source>Journal of Neuroscience</source><year>2015</year><volume>35</volume><issue>6</issue><fpage>2766</fpage><lpage>2777</lpage><pub-id pub-id-type="pmcid">PMC4323539</pub-id><pub-id pub-id-type="pmid">25673864</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3134-14.2015</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname><given-names>AF</given-names></name><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Poort</surname><given-names>J</given-names></name></person-group><article-title>Two distinct types of eye-head coupling in freely moving mice</article-title><source>Current Biology</source><year>2020</year><volume>30</volume><issue>11</issue><fpage>2116</fpage><lpage>2130</lpage><elocation-id>e6</elocation-id><pub-id pub-id-type="pmcid">PMC7284311</pub-id><pub-id pub-id-type="pmid">32413309</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2020.04.042</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Michaiel</surname><given-names>AM</given-names></name><name><surname>Abe</surname><given-names>ET</given-names></name><name><surname>Niell</surname><given-names>CM</given-names></name></person-group><article-title>Dynamics of gaze control during prey capture in freely moving mice</article-title><source>eLife</source><year>2020</year><volume>9</volume><elocation-id>e57458</elocation-id><pub-id pub-id-type="pmcid">PMC7438109</pub-id><pub-id pub-id-type="pmid">32706335</pub-id><pub-id pub-id-type="doi">10.7554/eLife.57458</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holmgren</surname><given-names>CD</given-names></name><name><surname>Stahr</surname><given-names>P</given-names></name><name><surname>Wallace</surname><given-names>DJ</given-names></name><name><surname>Voit</surname><given-names>KM</given-names></name><name><surname>Matheson</surname><given-names>EJ</given-names></name><name><surname>Sawinski</surname><given-names>J</given-names></name><name><surname>Bassetto</surname><given-names>G</given-names></name><name><surname>Kerr</surname><given-names>JN</given-names></name></person-group><article-title>Visual pursuit behavior in mice maintains the pursued prey on the retinal region with least optic flow</article-title><source>eLife</source><year>2021</year><volume>10</volume><elocation-id>e70838</elocation-id><pub-id pub-id-type="pmcid">PMC8547958</pub-id><pub-id pub-id-type="pmid">34698633</pub-id><pub-id pub-id-type="doi">10.7554/eLife.70838</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niell</surname><given-names>CM</given-names></name><name><surname>Stryker</surname><given-names>MP</given-names></name></person-group><article-title>Modulation of visual responses by behavioral state in mouse visual cortex</article-title><source>Neuron</source><year>2010</year><volume>65</volume><issue>4</issue><fpage>472</fpage><lpage>479</lpage><pub-id pub-id-type="pmcid">PMC3184003</pub-id><pub-id pub-id-type="pmid">20188652</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2010.01.033</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keller</surname><given-names>GB</given-names></name><name><surname>Bonhoeffer</surname><given-names>T</given-names></name><name><surname>Hübener</surname><given-names>M</given-names></name></person-group><article-title>Sensorimotor mismatch signals in primary visual cortex of the behaving mouse</article-title><source>Neuron</source><year>2012</year><volume>74</volume><issue>5</issue><fpage>809</fpage><lpage>815</lpage><pub-id pub-id-type="pmid">22681686</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saleem</surname><given-names>AB</given-names></name><name><surname>Ayaz</surname><given-names>A</given-names></name><name><surname>Jeffery</surname><given-names>KJ</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name></person-group><article-title>Integration of visual motion and locomotion in mouse visual cortex</article-title><source>Nature Neuroscience</source><year>2013</year><volume>16</volume><issue>12</issue><fpage>1864</fpage><lpage>1869</lpage><pub-id pub-id-type="pmcid">PMC3926520</pub-id><pub-id pub-id-type="pmid">24185423</pub-id><pub-id pub-id-type="doi">10.1038/nn.3567</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muzzu</surname><given-names>T</given-names></name><name><surname>Saleem</surname><given-names>AB</given-names></name></person-group><article-title>Feature selectivity can explain mismatch signals in mouse visual cortex</article-title><source>Cell Reports</source><year>2021</year><volume>37</volume><issue>1</issue><elocation-id>109772</elocation-id><pub-id pub-id-type="pmcid">PMC8655498</pub-id><pub-id pub-id-type="pmid">34610298</pub-id><pub-id pub-id-type="doi">10.1016/j.celrep.2021.109772</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vasilevskaya</surname><given-names>A</given-names></name><name><surname>Widmer</surname><given-names>FC</given-names></name><name><surname>Keller</surname><given-names>GB</given-names></name><name><surname>Jordan</surname><given-names>R</given-names></name></person-group><article-title>Locomotion-induced gain of visual responses cannot explain visuomotor mismatch responses in layer 2/3 of primary visual cortex</article-title><source>Cell Reports</source><year>2023</year><volume>42</volume><issue>3</issue><elocation-id>112096</elocation-id><pub-id pub-id-type="pmcid">PMC9945359</pub-id><pub-id pub-id-type="pmid">36821437</pub-id><pub-id pub-id-type="doi">10.1016/j.celrep.2023.112096</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adesnik</surname><given-names>H</given-names></name><name><surname>Bruns</surname><given-names>W</given-names></name><name><surname>Taniguchi</surname><given-names>H</given-names></name><name><surname>Huang</surname><given-names>ZJ</given-names></name><name><surname>Scanziani</surname><given-names>M</given-names></name></person-group><article-title>A neural circuit for spatial summation in visual cortex</article-title><source>Nature</source><year>2012</year><volume>490</volume><issue>7419</issue><fpage>226</fpage><lpage>231</lpage><pub-id pub-id-type="pmcid">PMC3621107</pub-id><pub-id pub-id-type="pmid">23060193</pub-id><pub-id pub-id-type="doi">10.1038/nature11526</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dipoppa</surname><given-names>M</given-names></name><name><surname>Ranson</surname><given-names>A</given-names></name><name><surname>Krumin</surname><given-names>M</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><article-title>Vision and locomotion shape the interactions between neuron types in mouse visual cortex</article-title><source>Neuron</source><year>2018</year><volume>98</volume><issue>3</issue><fpage>602</fpage><lpage>615</lpage><elocation-id>e8</elocation-id><pub-id pub-id-type="pmcid">PMC5946730</pub-id><pub-id pub-id-type="pmid">29656873</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2018.03.037</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adelson</surname><given-names>EH</given-names></name><name><surname>Bergen</surname><given-names>JR</given-names></name></person-group><article-title>Spatiotemporal energy models for the perception of motion</article-title><source>Journal of the Optical Society of America A</source><year>1985</year><volume>2</volume><issue>2</issue><fpage>284</fpage><pub-id pub-id-type="pmid">3973762</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Basole</surname><given-names>A</given-names></name><name><surname>White</surname><given-names>LE</given-names></name><name><surname>Fitzpatrick</surname><given-names>D</given-names></name></person-group><article-title>Mapping multiple features in the population response of visual cortex</article-title><source>Nature</source><year>2003</year><volume>423</volume><issue>6943</issue><fpage>986</fpage><lpage>990</lpage><pub-id pub-id-type="pmid">12827202</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>TI</given-names></name><name><surname>Issa</surname><given-names>NP</given-names></name></person-group><article-title>Cortical maps of separable tuning properties predict population responses to complex visual stimuli</article-title><source>Journal of Neurophysiology</source><year>2005</year><volume>94</volume><issue>1</issue><fpage>775</fpage><lpage>787</lpage><pub-id pub-id-type="pmid">15758052</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mante</surname><given-names>V</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name></person-group><article-title>Mapping of stimulus energy in primary visual cortex</article-title><source>Journal of Neurophysiology</source><year>2005</year><volume>94</volume><issue>1</issue><fpage>788</fpage><lpage>798</lpage><pub-id pub-id-type="pmid">15758051</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>KP</given-names></name><name><surname>Fitzpatrick</surname><given-names>MJ</given-names></name><name><surname>Zhao</surname><given-names>L</given-names></name><name><surname>Wang</surname><given-names>B</given-names></name><name><surname>McCracken</surname><given-names>S</given-names></name><name><surname>Williams</surname><given-names>PR</given-names></name><name><surname>Kerschensteiner</surname><given-names>D</given-names></name></person-group><article-title>Cell-type-specific binocular vision guides predation in mice</article-title><source>Neuron</source><year>2021</year><volume>109</volume><issue>9</issue><fpage>1527</fpage><lpage>1539</lpage><elocation-id>e4</elocation-id><pub-id pub-id-type="pmcid">PMC8112612</pub-id><pub-id pub-id-type="pmid">33784498</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2021.03.010</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dräger</surname><given-names>UC</given-names></name><name><surname>Olsen</surname><given-names>JF</given-names></name></person-group><article-title>Ganglion cell distribution in the retina of the mouse</article-title><source>Investigative Ophthalmology &amp; Visual Science</source><year>1981</year><volume>20</volume><issue>3</issue><fpage>285</fpage><lpage>293</lpage><pub-id pub-id-type="pmid">6162818</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Kim</surname><given-names>IJ</given-names></name><name><surname>Sanes</surname><given-names>JR</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><article-title>The most numerous ganglion cell type of the mouse retina is a selective feature detector</article-title><source>Proceedings of the National Academy of Sciences</source><year>2012</year><volume>109</volume><issue>36</issue><fpage>E2391</fpage><lpage>E2398</lpage><pub-id pub-id-type="pmcid">PMC3437843</pub-id><pub-id pub-id-type="pmid">22891316</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1211547109</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Steinmetz</surname><given-names>N</given-names></name><name><surname>Reddy</surname><given-names>CB</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><article-title>Spontaneous behaviors drive multidimensional, brainwide activity</article-title><source>Science</source><year>2019</year><volume>364</volume><issue>6437</issue><elocation-id>eaav7893</elocation-id><pub-id pub-id-type="pmcid">PMC6525101</pub-id><pub-id pub-id-type="pmid">31000656</pub-id><pub-id pub-id-type="doi">10.1126/science.aav7893</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dadarlat</surname><given-names>MC</given-names></name><name><surname>Stryker</surname><given-names>MP</given-names></name></person-group><article-title>Locomotion enhances neural encoding of visual stimuli in mouse v1</article-title><source>Journal of Neuroscience</source><year>2017</year><volume>37</volume><issue>14</issue><fpage>3764</fpage><lpage>3775</lpage><pub-id pub-id-type="pmcid">PMC5394894</pub-id><pub-id pub-id-type="pmid">28264980</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2728-16.2017</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christensen</surname><given-names>AJ</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name></person-group><article-title>Reduced neural activity but improved coding in rodent higher-order visual cortex during locomotion</article-title><source>Nature Communications</source><year>2022</year><volume>13</volume><issue>1</issue><elocation-id>1676</elocation-id><pub-id pub-id-type="pmcid">PMC8967903</pub-id><pub-id pub-id-type="pmid">35354804</pub-id><pub-id pub-id-type="doi">10.1038/s41467-022-29200-z</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muzzu</surname><given-names>T</given-names></name><name><surname>Saleem</surname><given-names>AB</given-names></name></person-group><article-title>Redefining sensorimotor mismatch selectivity in the visual cortex</article-title><source>Cell Reports</source><year>2023</year><volume>42</volume><issue>3</issue><elocation-id>112098</elocation-id><pub-id pub-id-type="pmcid">PMC10632662</pub-id><pub-id pub-id-type="pmid">36821444</pub-id><pub-id pub-id-type="doi">10.1016/j.celrep.2023.112098</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pouget</surname><given-names>A</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><article-title>Spatial transformations in the parietal cortex using basis functions</article-title><source>Journal of Cognitive Neuroscience</source><year>1997</year><volume>9</volume><issue>2</issue><fpage>222</fpage><lpage>237</lpage><pub-id pub-id-type="pmid">23962013</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lyu</surname><given-names>C</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Maimon</surname><given-names>G</given-names></name></person-group><article-title>Building an allocentric travelling direction signal via vector computation</article-title><source>Nature</source><year>2022</year><volume>601</volume><issue>7891</issue><fpage>92</fpage><lpage>97</lpage><pub-id pub-id-type="pmcid">PMC11104186</pub-id><pub-id pub-id-type="pmid">34912112</pub-id><pub-id pub-id-type="doi">10.1038/s41586-021-04067-0</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leinweber</surname><given-names>M</given-names></name><name><surname>Ward</surname><given-names>DR</given-names></name><name><surname>Sobczak</surname><given-names>JM</given-names></name><name><surname>Attinger</surname><given-names>A</given-names></name><name><surname>Keller</surname><given-names>GB</given-names></name></person-group><article-title>A sensorimotor circuit in mouse cortex for visual flow predictions</article-title><source>Neuron</source><year>2017</year><volume>95</volume><issue>6</issue><fpage>1420</fpage><lpage>1432</lpage><elocation-id>e5</elocation-id><pub-id pub-id-type="pmid">28910624</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Attinger</surname><given-names>A</given-names></name><name><surname>Wang</surname><given-names>B</given-names></name><name><surname>Keller</surname><given-names>GB</given-names></name></person-group><article-title>Visuomotor coupling shapes the functional development of mouse visual cortex</article-title><source>Cell</source><year>2017</year><volume>169</volume><issue>7</issue><fpage>1291</fpage><lpage>1302</lpage><elocation-id>e14</elocation-id><pub-id pub-id-type="pmid">28602353</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jordan</surname><given-names>R</given-names></name><name><surname>Keller</surname><given-names>GB</given-names></name></person-group><article-title>Opposing influence of top-down and bottom-up input on excitatory layer 2/3 neurons in mouse primary visual cortex</article-title><source>Neuron</source><year>2020</year><volume>108</volume><issue>6</issue><fpage>1194</fpage><lpage>1206</lpage><elocation-id>e5</elocation-id><pub-id pub-id-type="pmcid">PMC7772056</pub-id><pub-id pub-id-type="pmid">33091338</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2020.09.024</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erisken</surname><given-names>S</given-names></name><name><surname>Vaiceliunaite</surname><given-names>A</given-names></name><name><surname>Jurjut</surname><given-names>O</given-names></name><name><surname>Fiorini</surname><given-names>M</given-names></name><name><surname>Katzner</surname><given-names>S</given-names></name><name><surname>Busse</surname><given-names>L</given-names></name></person-group><article-title>Effects of locomotion extend throughout the mouse early visual system</article-title><source>Current Biology</source><year>2014</year><volume>24</volume><issue>24</issue><fpage>2899</fpage><lpage>2907</lpage><pub-id pub-id-type="pmid">25484299</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aydın</surname><given-names>C</given-names></name><name><surname>Couto</surname><given-names>J</given-names></name><name><surname>Giugliano</surname><given-names>M</given-names></name><name><surname>Farrow</surname><given-names>K</given-names></name><name><surname>Bonin</surname><given-names>V</given-names></name></person-group><article-title>Locomotion modulates specific functional cell types in the mouse visual thalamus</article-title><source>Nature Communications</source><year>2018</year><volume>9</volume><issue>1</issue><elocation-id>4882</elocation-id><pub-id pub-id-type="pmcid">PMC6242985</pub-id><pub-id pub-id-type="pmid">30451819</pub-id><pub-id pub-id-type="doi">10.1038/s41467-018-06780-3</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Savier</surname><given-names>EL</given-names></name><name><surname>Chen</surname><given-names>H</given-names></name><name><surname>Cang</surname><given-names>J</given-names></name></person-group><article-title>Effects of locomotion on visual responses in the mouse superior colliculus</article-title><source>The Journal of Neuroscience</source><year>2019</year><volume>39</volume><issue>47</issue><fpage>9360</fpage><lpage>9368</lpage><pub-id pub-id-type="pmcid">PMC6867823</pub-id><pub-id pub-id-type="pmid">31570535</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1854-19.2019</pub-id></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>Y</given-names></name><name><surname>Tucciarone</surname><given-names>JM</given-names></name><name><surname>Espinosa</surname><given-names>JS</given-names></name><name><surname>Sheng</surname><given-names>N</given-names></name><name><surname>Darcy</surname><given-names>DP</given-names></name><name><surname>Nicoll</surname><given-names>RA</given-names></name><name><surname>Huang</surname><given-names>ZJ</given-names></name><name><surname>Stryker</surname><given-names>MP</given-names></name></person-group><article-title>A cortical circuit for gain control by behavioral state</article-title><source>Cell</source><year>2014</year><volume>156</volume><issue>6</issue><fpage>1139</fpage><lpage>1152</lpage><pub-id pub-id-type="pmcid">PMC4041382</pub-id><pub-id pub-id-type="pmid">24630718</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2014.01.050</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roth</surname><given-names>MM</given-names></name><name><surname>Dahmen</surname><given-names>JC</given-names></name><name><surname>Muir</surname><given-names>DR</given-names></name><name><surname>Imhof</surname><given-names>F</given-names></name><name><surname>Martini</surname><given-names>FJ</given-names></name><name><surname>Hofer</surname><given-names>SB</given-names></name></person-group><article-title>Thalamic nuclei convey diverse contextual information to layer 1 of visual cortex</article-title><source>Nature Neuroscience</source><year>2016</year><volume>19</volume><issue>2</issue><fpage>299</fpage><lpage>307</lpage><pub-id pub-id-type="pmcid">PMC5480596</pub-id><pub-id pub-id-type="pmid">26691828</pub-id><pub-id pub-id-type="doi">10.1038/nn.4197</pub-id></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andermann</surname><given-names>ML</given-names></name><name><surname>Kerlin</surname><given-names>AM</given-names></name><name><surname>Roumis</surname><given-names>DK</given-names></name><name><surname>Glickfeld</surname><given-names>LL</given-names></name><name><surname>Reid</surname><given-names>RC</given-names></name></person-group><article-title>Functional specialization of mouse higher visual cortical areas</article-title><source>Neuron</source><year>2011</year><volume>72</volume><issue>6</issue><fpage>1025</fpage><lpage>1039</lpage><pub-id pub-id-type="pmcid">PMC3876958</pub-id><pub-id pub-id-type="pmid">22196337</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2011.11.013</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marshel</surname><given-names>JH</given-names></name><name><surname>Garrett</surname><given-names>ME</given-names></name><name><surname>Nauhaus</surname><given-names>I</given-names></name><name><surname>Callaway</surname><given-names>EM</given-names></name></person-group><article-title>Functional specialization of seven mouse visual cortical areas</article-title><source>Neuron</source><year>2011</year><volume>72</volume><issue>6</issue><fpage>1040</fpage><lpage>1054</lpage><pub-id pub-id-type="pmcid">PMC3248795</pub-id><pub-id pub-id-type="pmid">22196338</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2011.12.004</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Madisen</surname><given-names>L</given-names></name><name><surname>Garner</surname><given-names>AR</given-names></name><name><surname>Shimaoka</surname><given-names>D</given-names></name><name><surname>Chuong</surname><given-names>AS</given-names></name><name><surname>Klapoetke</surname><given-names>NC</given-names></name><name><surname>Li</surname><given-names>L</given-names></name><name><surname>van der Bourg</surname><given-names>A</given-names></name><name><surname>Niino</surname><given-names>Y</given-names></name><name><surname>Egolf</surname><given-names>L</given-names></name><name><surname>Monetti</surname><given-names>C</given-names></name><name><surname>Gu</surname><given-names>H</given-names></name><etal/></person-group><article-title>Transgenic mice for intersectional targeting of neural sensors and effectors with high specificity and performance</article-title><source>Neuron</source><year>2015</year><volume>85</volume><issue>5</issue><fpage>942</fpage><lpage>958</lpage><pub-id pub-id-type="pmcid">PMC4365051</pub-id><pub-id pub-id-type="pmid">25741722</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2015.02.022</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorski</surname><given-names>JA</given-names></name><name><surname>Talley</surname><given-names>T</given-names></name><name><surname>Qiu</surname><given-names>M</given-names></name><name><surname>Puelles</surname><given-names>L</given-names></name><name><surname>Rubenstein</surname><given-names>JLR</given-names></name><name><surname>Jones</surname><given-names>KR</given-names></name></person-group><article-title>Cortical excitatory neurons and glia, but not GABAergic neurons, are produced in the Emx1-expressing lineage</article-title><source>The Journal of Neuroscience</source><year>2002</year><volume>22</volume><issue>15</issue><fpage>6309</fpage><lpage>6314</lpage><pub-id pub-id-type="pmcid">PMC6758181</pub-id><pub-id pub-id-type="pmid">12151506</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.22-15-06309.2002</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mayford</surname><given-names>M</given-names></name><name><surname>Bach</surname><given-names>ME</given-names></name><name><surname>Huang</surname><given-names>YY</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Hawkins</surname><given-names>RD</given-names></name><name><surname>Kandel</surname><given-names>ER</given-names></name></person-group><article-title>Control of memory formation through regulated expression of a CaMKII transgene</article-title><source>Science</source><year>1996</year><volume>274</volume><issue>5293</issue><fpage>1678</fpage><lpage>1683</lpage><pub-id pub-id-type="pmid">8939850</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wekselblatt</surname><given-names>JB</given-names></name><name><surname>Flister</surname><given-names>ED</given-names></name><name><surname>Piscopo</surname><given-names>DM</given-names></name><name><surname>Niell</surname><given-names>CM</given-names></name></person-group><article-title>Large-scale imaging of cortical dynamics during sensory perception and behavior</article-title><source>Journal of Neurophysiology</source><year>2016</year><volume>115</volume><issue>6</issue><fpage>2852</fpage><lpage>2866</lpage><pub-id pub-id-type="pmcid">PMC4922607</pub-id><pub-id pub-id-type="pmid">26912600</pub-id><pub-id pub-id-type="doi">10.1152/jn.01056.2015</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lopes</surname><given-names>G</given-names></name><name><surname>Bonacchi</surname><given-names>N</given-names></name><name><surname>Frazão</surname><given-names>J</given-names></name><name><surname>Neto</surname><given-names>JP</given-names></name><name><surname>Atallah</surname><given-names>BV</given-names></name><name><surname>Soares</surname><given-names>S</given-names></name><name><surname>Moreira</surname><given-names>L</given-names></name><name><surname>Matias</surname><given-names>S</given-names></name><name><surname>Itskov</surname><given-names>PM</given-names></name><name><surname>Correia</surname><given-names>PA</given-names></name><name><surname>Medina</surname><given-names>RE</given-names></name><etal/></person-group><article-title>Bonsai: an event-based framework for processing and controlling data streams</article-title><source>Frontiers in Neuroinformatics</source><year>2015</year><volume>9</volume><fpage>7</fpage><pub-id pub-id-type="pmcid">PMC4389726</pub-id><pub-id pub-id-type="pmid">25904861</pub-id><pub-id pub-id-type="doi">10.3389/fninf.2015.00007</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lopes</surname><given-names>G</given-names></name><name><surname>Farrell</surname><given-names>K</given-names></name><name><surname>Horrocks</surname><given-names>EA</given-names></name><name><surname>Lee</surname><given-names>CY</given-names></name><name><surname>Morimoto</surname><given-names>MM</given-names></name><name><surname>Muzzu</surname><given-names>T</given-names></name><name><surname>Papanikolaou</surname><given-names>A</given-names></name><name><surname>Rodrigues</surname><given-names>FR</given-names></name><name><surname>Wheatcroft</surname><given-names>T</given-names></name><name><surname>Zucca</surname><given-names>S</given-names></name><name><surname>Solomon</surname><given-names>SG</given-names></name><etal/></person-group><article-title>Creating and controlling visual environments using BonVision</article-title><source>eLife</source><year>2021</year><volume>10</volume><elocation-id>e65541</elocation-id><pub-id pub-id-type="pmcid">PMC8104957</pub-id><pub-id pub-id-type="pmid">33880991</pub-id><pub-id pub-id-type="doi">10.7554/eLife.65541</pub-id></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bradski</surname><given-names>G</given-names></name></person-group><article-title>The OpenCV Library</article-title><source>Dr Dobb’s Journal of Software Tools</source><year>2000</year></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garrido-Jurado</surname><given-names>S</given-names></name><name><surname>Muñoz-Salinas</surname><given-names>R</given-names></name><name><surname>Madrid-Cuevas</surname><given-names>FJ</given-names></name><name><surname>Marín-Jiménez</surname><given-names>MJ</given-names></name></person-group><article-title>Automatic generation and detection of highly reliable fiducial markers under occlusion</article-title><source>Pattern Recognition</source><year>2014</year><volume>47</volume><issue>6</issue><fpage>2280</fpage><lpage>2292</lpage><pub-id pub-id-type="doi">10.1016/j.patcog.2014.01.005</pub-id></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saravanan</surname><given-names>V</given-names></name><name><surname>Berman</surname><given-names>GJ</given-names></name><name><surname>Sober</surname><given-names>SJ</given-names></name></person-group><article-title>Application of the hierarchical bootstrap to multi-level data in neuroscience</article-title><source>Neurons, Behavior, Data Analysis, and Theory</source><year>2020</year><volume>3</volume><issue>5</issue><pub-id pub-id-type="pmcid">PMC7906290</pub-id><pub-id pub-id-type="pmid">33644783</pub-id></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname><given-names>A</given-names></name><name><surname>Mamidanna</surname><given-names>P</given-names></name><name><surname>Cury</surname><given-names>KM</given-names></name><name><surname>Abe</surname><given-names>T</given-names></name><name><surname>Murthy</surname><given-names>VN</given-names></name><name><surname>Mathis</surname><given-names>MW</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name></person-group><article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title><source>Nature Neuroscience</source><year>2018</year><volume>21</volume><issue>9</issue><fpage>1281</fpage><lpage>1289</lpage><pub-id pub-id-type="pmid">30127430</pub-id></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallace</surname><given-names>DJ</given-names></name><name><surname>Greenberg</surname><given-names>DS</given-names></name><name><surname>Sawinski</surname><given-names>J</given-names></name><name><surname>Rulla</surname><given-names>S</given-names></name><name><surname>Notaro</surname><given-names>G</given-names></name><name><surname>Kerr</surname><given-names>JND</given-names></name></person-group><article-title>Rats maintain an overhead binocular field at the expense of constant fusion</article-title><source>Nature</source><year>2013</year><volume>498</volume><issue>7452</issue><fpage>65</fpage><lpage>69</lpage><pub-id pub-id-type="pmid">23708965</pub-id></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Dipoppa</surname><given-names>M</given-names></name><name><surname>Schröder</surname><given-names>S</given-names></name><name><surname>Rossi</surname><given-names>LF</given-names></name><name><surname>Dalgleish</surname><given-names>H</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><article-title>Suite2p: beyond 10,000 neurons with standard two-photon microscopy</article-title><source>bioRxiv</source><year>2017</year><pub-id pub-id-type="doi">10.1101/061507</pub-id></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Wang</surname><given-names>T</given-names></name><name><surname>Michaelos</surname><given-names>M</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name></person-group><article-title>Cellpose: a generalist algorithm for cellular segmentation</article-title><source>Nature Methods</source><year>2021</year><volume>18</volume><issue>1</issue><fpage>100</fpage><lpage>106</lpage><pub-id pub-id-type="pmid">33318659</pub-id></element-citation></ref><ref id="R64"><label>64</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orsolic</surname><given-names>I</given-names></name><name><surname>Rio</surname><given-names>M</given-names></name><name><surname>Mrsic-Flogel</surname><given-names>TD</given-names></name><name><surname>Znamenskiy</surname><given-names>P</given-names></name></person-group><article-title>Mesoscale cortical dynamics reflect the interaction of sensory evidence and temporal expectation during perceptual decision-making</article-title><source>Neuron</source><year>2021</year><volume>109</volume><issue>11</issue><fpage>1861</fpage><lpage>1875</lpage><elocation-id>e10</elocation-id><pub-id pub-id-type="pmcid">PMC8186564</pub-id><pub-id pub-id-type="pmid">33861941</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2021.03.031</pub-id></element-citation></ref><ref id="R65"><label>65</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smyth</surname><given-names>D</given-names></name><name><surname>Willmore</surname><given-names>B</given-names></name><name><surname>Baker</surname><given-names>GE</given-names></name><name><surname>Thompson</surname><given-names>ID</given-names></name><name><surname>Tolhurst</surname><given-names>DJ</given-names></name></person-group><article-title>The receptive-field organization of simple cells in primary visual cortex of ferrets under natural scene stimulation</article-title><source>Journal of Neuroscience</source><year>2003</year><volume>23</volume><issue>11</issue><fpage>4746</fpage><lpage>4759</lpage><pub-id pub-id-type="pmcid">PMC6740783</pub-id><pub-id pub-id-type="pmid">12805314</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.23-11-04746.2003</pub-id></element-citation></ref><ref id="R66"><label>66</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cossell</surname><given-names>L</given-names></name><name><surname>Iacaruso</surname><given-names>MF</given-names></name><name><surname>Muir</surname><given-names>DR</given-names></name><name><surname>Houlton</surname><given-names>R</given-names></name><name><surname>Sader</surname><given-names>EN</given-names></name><name><surname>Ko</surname><given-names>H</given-names></name><name><surname>Hofer</surname><given-names>SB</given-names></name><name><surname>Mrsic-Flogel</surname><given-names>TD</given-names></name></person-group><article-title>Functional organization of excitatory synaptic strength in primary visual cortex</article-title><source>Nature</source><year>2015</year><volume>518</volume><issue>7539</issue><fpage>399</fpage><lpage>403</lpage><pub-id pub-id-type="pmcid">PMC4843963</pub-id><pub-id pub-id-type="pmid">25652823</pub-id><pub-id pub-id-type="doi">10.1038/nature14182</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Depth selectivity from motion parallax in layer 2/3 of mouse primary visual cortex.</title><p><bold>(A)</bold> Schematic of the relationship between locomotion and visual motion speed due to motion parallax. <bold>(B-C)</bold> Virtual reality setup and stimulus schematic. <bold>(D)</bold> Schematic of trial structure. <bold>(E)</bold> Illustration of stimuli at two virtual depths from the animal’s perspective and viewed from above. The mouse (not to scale) is shown to indicate the animals position in the virtual environment. <bold>(F)</bold> Raster plot of responses of an example neuron in mouse V1 across virtual depths as a function of distance traveled during the trial including 3 m of inter-stimulus interval before and after each trial. Dashed lines separate different trial types. <bold>(G)</bold> Mean responses of the example neuron (top) and mean running speed (bottom) across virtual depths. <bold>(H)</bold> Virtual depth tuning of the neuron in <bold>F-G</bold> during locomotion (&gt;5 cm/s) or stationary (max speed during preceding 1 second &lt;5 cm/s) periods. Error bar – 95% confidence interval. <bold>(I)</bold> Virtual depth tuning of 6 additional example neurons during running (blue) and stationary (gray) periods (left) and spatial location of depth-tuned neurons in an example imaging session (right). Scale bar – 100 <italic>μ</italic>m. Error bar – 95% confidence interval. <bold>(J)</bold> Proportion of depth-tuned neurons across imaging sessions (N = 85 sessions). Triangle indicates the median value. <bold>(K)</bold> Distribution of preferred virtual depths of depth-selective neurons (N = 31,013 neurons). N.P., near-preferring neurons; F.P., far-preferring neurons. <bold>(L)</bold> Raster plot of z-scored responses of all depth-selective neurons from sessions with 8 virtual depths in mouse V1, in a [-1 m, +1 m] window around stimulus presentation. Neurons are sorted by virtual depth preferences estimated using hold-out data not used in calculation of the raster plot. <bold>(M)</bold> Mean error of virtual depth decoded from population activity of simultaneously recorded neurons when mice were stationary and as a function of running speed (N = 85 sessions). Dashed line – chance level; shading – 95% confidence interval. <bold>(N)</bold> Average confusion matrices of decoded virtual depth when animals were stationary or running at a speed of 40-60 cm/s (N = 60 sessions with 8 virtual depths).</p></caption><graphic xlink:href="EMS199155-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Depth selectivity arises from conjunctive coding of optic flow speed and running speed.</title><p><bold>(A-B)</bold> Virtual depth tuning <bold>(A)</bold> and responses of an example depth-selective neuron binned by running speed <bold>(B</bold> left<bold>)</bold> or optic flow speed <bold>(B</bold> right<bold>)</bold>. Gray line <bold>(B</bold> left<bold>)</bold> – running speed tuning during the inter-trial interval, when sphere stimuli were absent. <bold>(C)</bold> Responses of the example neuron in <bold>A-B</bold> and as a function of both running and optic flow speeds. <bold>(D)</bold> Running speed and optic flow tuning of the example neuron in <bold>A-C</bold> fitted with the pure optic flow, pure running speed, additive, conjunctive or ratio models (see <xref ref-type="sec" rid="S14">methods</xref>). RS – running speed, OF – optic flow speed. <bold>(E)</bold> Proportion of depth-selective neurons best explained by the pure optic flow, pure running speed, additive, conjunctive and ratio models in each session (N = 85 sessions). <bold>(F)</bold> Preferred virtual depths as a function of the ratio between preferred running and optic flow speeds estimated using hold-out data not used in calculation of preferred virtual depths (N = 5,737 neurons). Contours – kernel density estimate. <bold>(G)</bold> Preferred optic flow speed and running speed of neurons estimated using hold-out data color coded by their preferred virtual depth (N = 5,737 neurons). Inset – optic flow speed as a function of running speed across virtual depths. <bold>(H)</bold> Relationship between running speed and optic flow speed on closed loop and open loop trials and example traces. Shaded areas color-coded by depth indicate stimulus presentation. <bold>(I)</bold> Responses of an example neuron as a function of both running and optic flow speeds in closed loop (left) and open loop (right). <bold>(J-K)</bold> Scatter plot of preferred running speed <bold>(J)</bold>, preferred optic flow speed <bold>(K)</bold> in open loop and closed loop for depth-selective neurons (N = 1,234 neurons). Contours – kernel density estimates. <bold>(L)</bold> Histogram of ratios of peak response amplitude in closed loop over open loop for neurons in <bold>J-K. (M)</bold> Accuracy of decoding virtual depth from population activity of depth-selective neurons in closed loop and open loop recordings (N = 34 sessions). <bold>(N)</bold> Confusion matrices of decoded virtual depth in closed loop and open loop recordings with 8 virtual depths (N = 27 sessions).</p></caption><graphic xlink:href="EMS199155-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Depth-selective neurons have three-dimensional receptive fields.</title><p><bold>(A-B)</bold> Linear receptive field model schematic (<bold>A</bold>) and an example reconstructed stimulus frame (<bold>B</bold>). Stimuli were downsampled to 5 degrees for analysis. <bold>(C-D)</bold> Receptive fields of 3 example V1 neurons as a function of virtual depth <bold>(C)</bold> and visualized in 3D <bold>(D)</bold>. The rendered volume in <bold>D</bold> represents the receptive field locations with responses greater than half of maximum for each cell. <bold>(E)</bold> Proportion of neurons with significant receptive fields across imaging sessions (N = 85 sessions). Triangle indicates the median value. <bold>(F-H)</bold> Preferred azimuth <bold>(F)</bold>, elevation <bold>(G)</bold>, and depth <bold>(H)</bold> of depth-selective neurons across the field of view in an example imaging session. Scale bar – 100 <italic>μ</italic>m. Arrow – direction of gradient of increase in preferred azimuth or elevation. Inset – mean fluorescence image.</p></caption><graphic xlink:href="EMS199155-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Distribution of depth preferences across the visual field.</title><p><bold>(A)</bold> Location of V1 and higher visual areas identified using widefield calcium imaging. Cyan and yellow indicate responses to grating patches centered at 30 and 90 degrees of azimuth. Scale bar – 1 mm. LM, lateromedial; AL, anterolateral; RL, rostrolateral; A, anterior; AM, anteromedial; PM, posteromedial. <bold>(B-D)</bold> Top – spatial distributions of receptive field (RF) azimuths <bold>(B)</bold> and elevations <bold>(C)</bold>, and preferred virtual depths <bold>(D)</bold> of depth at the location of individual neurons; bottom – RF azimuth, elevation and preferred virtual depth smoothed with a Gaussian kernel (N = 20,554 neurons from 85 sessions). <bold>(E)</bold> Median corrected preferred virtual depth as a function of RF location (N = 20,554 neurons from 85 sessions). Only locations with at least 20 neurons are shown. Inset – direction of the gradient of preferred virtual depth across the visual field estimated across hierarchical bootstrap samples of the dataset. <bold>(F)</bold> RF locations of depth-selective neurons tuned to near (&lt;20 cm, N = 3,753 neurons), intermediate (20-100 cm, N = 8,412 neurons) and far (&gt;100 cm, N = 8,389 neurons) virtual depths (top) after correcting for RF location, and proportion of depth-selective neurons in each category as a function of RF location (bottom). RF locations were estimated with 5 degree resolution and was plotted with 2 degree jitters.</p></caption><graphic xlink:href="EMS199155-f004"/></fig></floats-group></article>