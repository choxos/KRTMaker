<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS199963</article-id><article-id pub-id-type="doi">10.1101/2024.11.05.622169</article-id><article-id pub-id-type="archive">PPR936594</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">2</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>PLM-interact: extending protein language models to predict protein-protein interactions</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Dan</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Young</surname><given-names>Francesca</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Lamb</surname><given-names>Kieran D.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Quiros</surname><given-names>Adalberto Claudio</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Pancheva</surname><given-names>Alexandrina</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Miller</surname><given-names>Crispin</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Macdonald</surname><given-names>Craig</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Robertson</surname><given-names>David L</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">#</xref></contrib><contrib contrib-type="author"><name><surname>Yuan</surname><given-names>Ke</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref><xref ref-type="corresp" rid="CR1">#</xref></contrib></contrib-group><aff id="A1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03vaer060</institution-id><institution>MRC-University of Glasgow Centre for Virus Research</institution></institution-wrap>, <city>Glasgow</city>, <country country="GB">United Kingdom</country></aff><aff id="A2"><label>2</label>School of Cancer Sciences, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/00vtgdb53</institution-id><institution>University of Glasgow</institution></institution-wrap>, <city>Glasgow</city>, <country country="GB">United Kingdom</country></aff><aff id="A3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03pv69j64</institution-id><institution>Cancer Research UK Scotland Institute</institution></institution-wrap>, <city>Glasgow</city>, <country country="GB">United Kingdom</country></aff><aff id="A4"><label>4</label>School of Computing Science, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/00vtgdb53</institution-id><institution>University of Glasgow</institution></institution-wrap>, <city>Glasgow</city>, <country country="GB">United Kingdom</country></aff><author-notes><corresp id="CR1"><label>#</label>Shared senior/corresponding authors: <email>david.l.robertson@glasgow.ac.uk</email>, <email>ke.yuan@glasgow.ac.uk</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>09</day><month>11</month><year>2024</year></pub-date><pub-date pub-type="preprint"><day>07</day><month>11</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Computational prediction of protein structure from amino acid sequences alone has been achieved with unprecedented accuracy, yet the prediction of protein-protein interactions (PPIs) remains an outstanding challenge. Here we assess the ability of protein language models (PLMs), routinely applied to protein folding, to be retrained for PPI prediction. Existing PPI prediction models that exploit PLMs use a pre-trained PLM feature set, ignoring that the proteins are physically interacting. Our novel method, PLM-interact, goes beyond a single protein, jointly encoding protein pairs to learn their relationships, analogous to the next-sentence prediction task from natural language processing. This approach provides a significant improvement in performance: Trained on human-human PPIs, PLM-interact predicts mouse, fly, worm, <italic>E. coli</italic> and yeast PPIs, with 16-28% improvements in AUPR compared with state-of-the-art PPI models. Additionally, it can detect changes that disrupt or cause PPIs and be applied to virus-host PPI prediction. Our work demonstrates that large language models can be extended to learn the intricate relationships among biomolecules from their sequences alone.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Proteins are the main structural components of cells and mediate biological processes by interacting with other proteins<sup><xref ref-type="bibr" rid="R1">1</xref></sup>. Disruption of these protein-protein interactions (PPIs), e.g., mediated by mutations can underlie human disease<sup><xref ref-type="bibr" rid="R2">2</xref></sup>. In virology PPIs are particularly important as viruses depend entirely on the host cell for replication, achieved mainly through specific interactions with host proteins. Understanding PPI mechanisms offers the potential for developing novel therapy strategies for both human disease and pathogen infections<sup><xref ref-type="bibr" rid="R3">3</xref></sup>. Unfortunately, experimentally identifying PPIs is both costly and time-consuming, such that interaction datasets remain sparse with only a few species having comprehensive coverage<sup><xref ref-type="bibr" rid="R4">4</xref>,<xref ref-type="bibr" rid="R5">5</xref></sup>.</p><p id="P3">Computational algorithms offer an efficient alternative to the prediction of PPIs at scale. Existing prediction approaches mainly leverage protein properties such as protein structures, sequence composition and evolutionary information<sup><xref ref-type="bibr" rid="R6">6</xref>,<xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R8">8</xref>,<xref ref-type="bibr" rid="R9">9</xref></sup>. Applying these features to pairs of proteins, classifiers have been trained using classical machine learning<sup><xref ref-type="bibr" rid="R10">10</xref></sup> and deep learning approaches<sup><xref ref-type="bibr" rid="R11">11</xref></sup>. Recently, protein language models (PLMs) trained on large public protein sequence databases have been used for encoding sequence composition, evolutionary and structural features<sup><xref ref-type="bibr" rid="R12">12</xref>–<xref ref-type="bibr" rid="R14">14</xref></sup>, becoming the method of choice for representing proteins in state-of-the-art PPI predictors. A typical PPI prediction architecture uses a pre-trained PLM to represent each protein in a pair separately, then a classification head is trained for a binary task that discriminates interacting pairs from non-interacting pairs<sup><xref ref-type="bibr" rid="R13">13</xref>,<xref ref-type="bibr" rid="R15">15</xref></sup> (<xref ref-type="fig" rid="F1">Figure 1a</xref>). Despite this use of PLMs, identifying positive PPIs remains challenging.</p><p id="P4">The main issue is PLMs are primarily trained using single protein sequences, i.e., while they learn to identify contact points within a single protein<sup><xref ref-type="bibr" rid="R16">16</xref></sup>, they are not ‘aware’ of interaction partners. In a conventional PLM-based PPI predictor architecture, a classification head is used to extrapolate the signals of inter-protein interactions by grouping common patterns of intra-protein contacts in interacting and non-interacting pairs, respectively (<xref ref-type="fig" rid="F1">Figure 1a</xref>). This strategy relies on the classification head being generalisable. Unfortunately, with a feedforward neural network being the dominant option, these classifiers often don’t generalise well.</p><p id="P5">To address the lack of inter-protein context in training, we propose a novel PPI prediction model, PLM-interact, that directly models PPIs by extending and fine-tuning a pre-trained PLM, ESM-2<sup><xref ref-type="bibr" rid="R17">17</xref></sup>. PLM-interact (trained on human PPI data) achieves a significant improvement compared to other predictors when applied to mouse, fly, worm, yeast and <italic>E. coli</italic> datasets. We demonstrate that PLM-interact is capable at identifying mutations that cause and disrupt interactions. We also show PLM-interact’s performance in a virus-human PPI prediction task.</p><sec id="S2"><title>PLM-interact</title><p id="P6">To directly model PPIs, two extensions to ESM-2 are introduced (<xref ref-type="fig" rid="F1">Figure 1b</xref>): (1) longer permissible sequence lengths in paired masked-language training to accommodate amino acid residues from both proteins; (2) implementation of the “next sentence” prediction task<sup><xref ref-type="bibr" rid="R18">18</xref></sup> to fine-tune ESM-2 where the model is trained with a binary label indicating whether the protein pair is interacting or not (see <xref ref-type="sec" rid="S7">Methods</xref> for more details). Our training task is, thus, a mixture of the next sentence prediction and the mask language modelling tasks. This architecture allows amino acids in a protein to be linked by amino acids from a different protein through transformer’s attention mechanism.</p><p id="P7">The training of PLM-interact begins with the pre-trained large language model ESM-2. We fine-tune it for PPIs, by showing it pairs of known interacting and non-interacting proteins. In contrast to similar training strategies in machine learning<sup><xref ref-type="bibr" rid="R18">18</xref></sup>, we find the next sentence prediction and mask language modelling objectives need to be balanced. We therefore conducted comprehensive benchmarking for different weighting options, before selecting a 1:10 ratio between classification loss and mask loss, combined with initialisation using the ESM-2 (with 650M parameters), as this achieved the best performance (see <xref ref-type="sec" rid="S7">Methods</xref> and <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 1</xref>).</p></sec><sec id="S3"><title>PLM-interact improves prediction performance</title><p id="P8">To examine the performance of PLM-interact, we benchmark the model against five PPI prediction approaches: TT3D<sup><xref ref-type="bibr" rid="R13">13</xref></sup>, Topsy-Turvy<sup><xref ref-type="bibr" rid="R19">19</xref></sup>, D-SCRIPT<sup><xref ref-type="bibr" rid="R15">15</xref></sup>, PIPR<sup><xref ref-type="bibr" rid="R6">6</xref></sup> and DeepPPI<sup><xref ref-type="bibr" rid="R20">20</xref></sup>. We use a multi-species dataset created by Sledzieski et al.<sup><xref ref-type="bibr" rid="R15">15</xref></sup>. Each model is trained on human protein interaction data and tested on five other species. The human training dataset in this multi-species dataset includes 421,792 protein pairs (38,344 positive interaction pairs and 383,448 negative pairs), human validation includes 52,725 protein pairs (4,794 positive interaction pairs and 47,931 negative pairs) and the mouse, worm, fly and yeast test datasets each includes 55,000 pairs (5,000 positives interaction pairs and 50,000 negative pairs), except for the <italic>E. coli</italic> test dataset, which includes 22,000 pairs (2,000 positive interaction pairs and 20,000 negative pairs). The positive PPIs in these datasets are experimentally-derived physical interactions, while the negative pairs are randomly paired proteins not reported to interact.</p><p id="P9">PLM-interact achieves the highest AUPR (area under the precision-recall curve)<sup><xref ref-type="bibr" rid="R23">23</xref></sup> with the next best performer, TT3D<sup><xref ref-type="bibr" rid="R13">13</xref></sup>, although similar performance to Topsy-Turvy<sup><xref ref-type="bibr" rid="R19">19</xref></sup> (<xref ref-type="fig" rid="F2">Figure 2b</xref>). Tested on mouse, fly and worm test species datasets, PLM-interact has an improvement of 16%, 21% and 20% AUPR compared to TT3D<sup><xref ref-type="bibr" rid="R13">13</xref></sup>, respectively. The predictions for yeast and <italic>E. coli</italic> PPIs are more challenging because they are more evolutionarily divergent from the human proteins used for training than the other species (see <xref ref-type="fig" rid="F2">Figure 2b</xref>): Our model achieved an AUPR of 0.706 on yeast, a 28% improvement over TT3D’s AUPR of 0.553 and a 19% improvement on <italic>E. coli</italic> with an AUPR of 0.722.</p><p id="P10">Importantly, the improvement in PLM-interact is due to its ability to correctly identify positive PPIs: Comparing the prediction interaction probability of PLM-interact with the second-best predictor TT3D, PLM-interact consistently assigned higher probabilities of interaction to true positive PPIs. TT3D, in contrast, despite using a broader feature set, produces a bimodal distribution for interaction probabilities in all held-out species (more details see <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 2</xref>).</p><p id="P11">Next, we showcase five positive PPI instances, one for each test species, for which our model produces a correct prediction, but TT3D produces an incorrect prediction (<xref ref-type="fig" rid="F3">Figure 3</xref>). These PPIs are necessary for essential biology processes including ubiquinone biosynthesis, RNA polymerisation, ATP catalysis, transcriptional activation and protein transportation. We use Chai-1<sup><xref ref-type="bibr" rid="R21">21</xref></sup> and AlphaFold3<sup><xref ref-type="bibr" rid="R22">22</xref></sup> to predict and visualise these interacting protein structures (<xref ref-type="fig" rid="F3">Figure 3</xref> and <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 3</xref>). Notably, both Chai-1 and AlphaFold3 have only 1 out 5 structures with close to high-confidence prediction (ipTM close to 0.8). Both methods give failed prediction scores (ipTM &lt;0.6) for 4 out 5 structures. PLM-interact gives correct predictions with high confidence in all cases.</p></sec><sec id="S4"><title>PLM-interact can identify the impact of mutations on interactions</title><p id="P12">Here, we demonstrate examples of PLM-interact correctly predicting the consequences of mutations on PPIs. Canonical protein sequences (ie proteins without mutations) are retrieved from UniProt<sup><xref ref-type="bibr" rid="R24">24</xref></sup>. Amino acid substitutions associated with changes in PPIs are obtained from InAct<sup><xref ref-type="bibr" rid="R25">25</xref></sup>. We obtained predicted interaction probabilities for canonical and mutants from PLM-interact trained on human data (see <xref ref-type="sec" rid="S7">Methods</xref>). For mutations associated with interaction gain ‘mutation-causing’ interactions (see <xref ref-type="fig" rid="F4">Figure 4a</xref>), PLM-interact predicted interaction probabilities of the negative pair are below 0.5, whereas the interaction probabilities of the mutant PPI exceed 0.5. For mutation-disrupting PPIs (<xref ref-type="fig" rid="F4">Figure 4b</xref>), the predicted interaction probabilities of the positive PPI exceed 0.5, whereas the interaction probabilities of the mutant PPI are below 0.5. Examples are presented in <xref ref-type="fig" rid="F4">Figure 4</xref>, where complex structures are predicted by Chai-1<sup><xref ref-type="bibr" rid="R21">21</xref></sup>.</p><p id="P13">We show two examples of mutations that cause PPIs in <xref ref-type="fig" rid="F4">Figure 4a</xref>, the protein Ataxin-1 and SOD1 (Superoxide dismutase [Cu-Zn]), which in humans are encoded by the ATXN1 and SOD1 genes separately. Ataxin-1 interacts with many other proteins and the expansion of a glutamine(Q)-encoding repeat can affect the function of PPIs and cause the genetic disease spinocerebellar ataxia type1 (SCA1) and other polyglutamine diseases<sup><xref ref-type="bibr" rid="R26">26</xref></sup>. Predictions from PLM-interact show that prior to mutation, this PPI has a predicted interaction probability of 0.411, correctly indicating non-interaction with E2s (Ubiquitin-conjugating enzyme E2 E3). Following mutation, PLM-interact increases this score to 0.771, correctly predicting that the mutation induces interaction. In the second example (<xref ref-type="fig" rid="F4">Figure 4a</xref>), the SOD1 gene encodes superoxide dismutase enzymes that break down human superoxide radicals. SOD1 is linked to the nervous system disease amyotrophic lateral sclerosis (ALS)<sup><xref ref-type="bibr" rid="R27">27</xref></sup>, with the A4V mutation being the most common variant in North America<sup><xref ref-type="bibr" rid="R28">28</xref></sup>. PLM-interact predicts 0.465 and 0.851 before and after the mutation, correctly capturing the change in interaction with CDK4 (Cyclin-dependent kinase 4).</p><p id="P14">Next, we show two examples of mutations that disrupt PPIs (<xref ref-type="fig" rid="F4">Figure 4b</xref>). First, GRB2 (growth factor receptor bound protein 2) is associated with signal transduction. GRB2 mutations are associated with multiple cancers, including breast cancer<sup><xref ref-type="bibr" rid="R33">33</xref></sup> and leukaemia<sup><xref ref-type="bibr" rid="R34">34</xref></sup>. The canonical protein interacts with PLEKHS1 (Pleckstrin homology domain-containing family S member 1). PLM-interact predicts that the missense mutation R86K reduces the interaction probability from 0.503 to 0.477. Second, CLCP1 is a transmembrane protein that regulates cell growth and this protein is identified as a cancer marker, exhibiting up-regulated expression in lung cancer<sup><xref ref-type="bibr" rid="R35">35</xref></sup>. Again, PLM-interact predicts that the missense mutation Y732F reduces the interaction probability from 0.873 to 0.425.</p></sec><sec id="S5"><title>Improved virus-human PPI prediction</title><p id="P15">To study virus-host PPI prediction, we train PLM-interact on a virus-human PPIs dataset from Tsukiyama et al.<sup><xref ref-type="bibr" rid="R11">11</xref></sup>. The dataset is derived from the Host-Pathogen Interaction Database (HPIDB) 3.0<sup><xref ref-type="bibr" rid="R36">36</xref></sup>, comprising a total of 22,383 PPIs, which include 5,882 human and 996 virus proteins. We compare our model with three recent virus-human PPI models: PLM-based approach STEP<sup><xref ref-type="bibr" rid="R14">14</xref></sup> and the protein embeddings-based approaches LSTM-PHV<sup><xref ref-type="bibr" rid="R11">11</xref></sup> and InterSPPI<sup><xref ref-type="bibr" rid="R37">37</xref></sup>. STEP is similar to existing PPI models benchmarked previously in our study; it leverages protein sequence embedding extracted by the pre-trained PLM ProtBERT<sup><xref ref-type="bibr" rid="R38">38</xref></sup>. The results show that PLM-interact outperforms the other models. For the STEP comparison this corresponds to improvements in AUPR, F1 and MCC scores of 5.7%, 10.9% and 11.9%, respectively (<xref ref-type="fig" rid="F5">Figure 5a</xref>). The length of virus, human proteins and the combined length of virus-human PPIs are shown in <xref ref-type="fig" rid="F5">Figure 5b</xref>. To further analyse our model’s performance, we select three pairs of virus-human PPIs from our test data, all with corresponding experimental virus-human complex structures available in the HVIDB<sup><xref ref-type="bibr" rid="R39">39</xref></sup>. We then use ChimeraX<sup><xref ref-type="bibr" rid="R23">23</xref></sup> to visualise these structures and present PLM-interact’s predicted interaction probability for each example (see <xref ref-type="fig" rid="F5">Figure 5c</xref>).</p></sec></sec><sec id="S6" sec-type="discussion"><title>Discussion</title><p id="P16">In this study we have developed a novel PPI prediction method, PLM-interact, that extends single protein-focused PLMs to their interacting protein partner. We report significant improvements in held-out species comparisons and highlight successful examples of predicting mutational effects on protein interactions. We further demonstrate PLM-interact’s performance in a virus-human PPI prediction task, showing a significant improvement over state-of-the-art prediction approaches.</p><p id="P17">Underlying the benefit of PLM-interact is the improved capability of correctly predicting positive PPIs in the held-out species. However, positive PPIs are challenging to predict due to a lack of high-quality PPI data for training. Our improved performance relative to TT3D is particularly impressive given that we only use sequences. TT3D<sup><xref ref-type="bibr" rid="R13">13</xref></sup> includes explicit structural information, the per-residual structural alphabet in Foldseek<sup><xref ref-type="bibr" rid="R40">40</xref></sup>, to improve its prediction over Topsy-Turvy, which incorporated network data.</p><p id="P18">Furthermore, our results show the potential of predicting mutational effects on PPIs from sequence alone. This could lead to a new generation of interaction aware in-silico variant effect predictors where methods rely on PLMs of the single proteins<sup><xref ref-type="bibr" rid="R41">41</xref>–<xref ref-type="bibr" rid="R43">43</xref></sup>. However, current training data remains limited. The number of high-quality structures of mutant proteins and their interaction partners are low. Algorithmically, models with long and multimodal context<sup><xref ref-type="bibr" rid="R44">44</xref>–<xref ref-type="bibr" rid="R46">46</xref></sup> that include multiple proteins, structures and nucleotides could be specialised for interaction tasks.</p><p id="P19">Finally, effective sequence-based virus-host PPI predictors could provide the much-needed molecular detail to conventional virus-host species prediction tools, which tend to rely on genome composition signals ignoring host molecules that are interacting physically with viral molecules<sup><xref ref-type="bibr" rid="R47">47</xref>–<xref ref-type="bibr" rid="R50">50</xref></sup>. In those approaches, the host species only act as labels. Recent progress within SARS-CoV-2 PPI studies mapped out a complex interaction landscape between the virus and human proteome<sup><xref ref-type="bibr" rid="R51">51</xref>,<xref ref-type="bibr" rid="R52">52</xref></sup>. Other viruses are likely to have similarly complex interactions with human and animal hosts. Leveraging these interactions could lead to tools that are better at predicting zoonotic events and the potential for emergence of novel viruses. While PLM-interact has demonstrated significant improvements there is much to do in terms of generating reliable predictions, in particular the need for high-quality virus-host experimental PPI data for training. What is clear is attention applied to longer-range sequence interactions is enhancing our understanding of protein interactions, the fundamental ‘language’ of molecules.</p></sec><sec id="S7" sec-type="methods"><title>Methods</title><sec id="S8"><title>Datasets</title><p id="P20">The benchmarking human PPI dataset, from Sledzieski et al.<sup><xref ref-type="bibr" rid="R15">15</xref></sup>, comprises human training and validation data and test data from five other species: mouse, <italic>Mus musculus</italic>; fly, <italic>Drosophila melanogaster</italic>; worm, <italic>Caenorhabditis elegans</italic>; yeast, <italic>Saccharomyces cerevisia</italic>e; and <italic>E. coli</italic>, <italic>Escherichia coli</italic>), all retrieved from STRING V11<sup><xref ref-type="bibr" rid="R53">53</xref></sup>. We train and validate our model on human PPIs and then conduct inference on PPIs from five other species. All training, validation and test datasets maintain a 1:10 ratio of positive to negative pairs, reflecting the fact that positive PPIs are significantly fewer than negative pairs in the actual host PPI networks. The length of protein sequences ranges from 50 to 800 and PPIs are clustered at 40% identity using CD-HIT<sup><xref ref-type="bibr" rid="R54">54</xref></sup> to remove the redundant PPIs. The human training dataset includes 38,344 positive PPIs, whereas the validation set includes 4,794 positive PPIs. Each of the five species includes 5,000 positive interactions, except for <italic>E. coli</italic>, which only has 2,000 positive interactions due to the fewer positive PPIs in the STRING dataset used<sup><xref ref-type="bibr" rid="R15">15</xref></sup>.</p><p id="P21">The benchmarking dataset of 22,383 virus-human PPIs includes 5,882 human and 996 virus proteins. This dataset was obtained from Tsukiyama et al.<sup><xref ref-type="bibr" rid="R11">11</xref></sup>, sourced from the HPIDB 3.0 database<sup><xref ref-type="bibr" rid="R36">36</xref></sup>; the ratio of positive to negative pairs is 1:10 and negative pairs are chosen based on sequence dissimilarities. The length of protein sequences ranges from 30 to 1,000 and the redundant PPIs are filtered based on a threshold of 95% identity using CD-HIT<sup><xref ref-type="bibr" rid="R54">54</xref></sup>.</p><p id="P22">In addition, we provide models trained on human PPIs from STRING V12<sup><xref ref-type="bibr" rid="R55">55</xref></sup>. The positive PPIs are selected by collecting physical links with positive experimental scores, while excluding PPIs with positive homology scores and confidence scores below 400. Previous studies have typically limited the maximum length of protein sequences to 800 or 1000 due to GPU memory limitations. We process the length of protein sequences up to 2000, with a combined length threshold for PPIs of nearly 2500. This human dataset includes 60,308 positive PPIs for training and 15,124 positive PPIs for testing. Furthermore, protein sequences are clustered at 40% identity using MMSeq2<sup><xref ref-type="bibr" rid="R56">56</xref></sup> and only PPIs from the distinct clusters are chosen to eliminate redundant PPIs. Again, the positive-to-negative protein pair ratio is 1:10, consistent with the aforementioned two benchmarking datasets.</p></sec><sec id="S9"><title>Model architecture</title><p id="P23">We use ESM-2 as the base model in PLM-interact. ESM-2 is an encoder transformer model with a parameter size range from 8 million to 15 billion. The results presented are PLM-interact based on ESM-2 with 650M parameters. We also provided PLM-interact models with ESM-2 35M on our GitHub repository to help with quick testing. The input representation contains amino acid token representations from two proteins. This setup is similar to the original BERT model<sup><xref ref-type="bibr" rid="R57">57</xref></sup>, also known as the <italic>cross-encoder</italic> which simultaneously encodes a pair of query and answer sentences.</p><p id="P24">A standard input sequence of PLM-interact, <italic>x</italic>, can be shown as the following: <disp-formula id="FD1"><label>(1)</label><mml:math id="M1"><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mi>L</mml:mi><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mi>O</mml:mi><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mi>O</mml:mi><mml:mi>S</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula> where <italic>CLS</italic> is the classification token, <italic>P</italic><sub>1</sub> contains amino acid tokens of protein 1, <italic>P<sub>2</sub></italic> contains amino acid tokens of protein 2, and <italic>EOS</italic> is the end-of-sentence token. The first EOS token marks the end of the amino acid sequence in protein 1. This setting allows us to use the original ESM-2 tokenizer to generate embedding vectors <italic>e</italic>, and pass them to the transformer encoder of the ESM-2: <disp-formula id="FD2"><label>(2)</label><mml:math id="M2"><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula> where <italic>f</italic> is ESM-2, <italic>e</italic> contains the token embeddings of <italic>x</italic>, and <italic>h</italic> contains the output embeddings of all input tokens. h can be presented as: <disp-formula id="FD3"><label>(3)</label><mml:math id="M3"><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>O</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>O</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula> where <italic>h<sub>a<sub>1</sub></sub></italic> and <italic>h<sub>a<sub>n</sub></sub></italic> represent amino acid tokens in proteins 1 and 2. Then, we use the <italic>CLS</italic> token embedding to aggregate the representation of the entire sequence pair and as the features for a linear classification function <italic>φ</italic>, and parameterised as a single feed forward layer with a ReLU activation function. The output of the FF layer is converted by the sigmoid function <italic>σ</italic> to obtain the predicted interaction probability <italic>g</italic>, <disp-formula id="FD4"><label>(4)</label><mml:math id="M4"><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula></p></sec><sec id="S10"><title>Model Training</title><p id="P25">PLM-interact is trained with two tasks: 1) a mask language modelling (MLM) task predicting randomly masked amino acids and 2) a binary classification task predicting the interaction label of a pair of proteins. PLM-interact is trained for 10 epochs using a batch size of 128 on both benchmarking datasets of human PPIs and virus-human PPIs. For all training runs, the input protein pairs are trained on both orders as the interaction between protein 1 and protein 2 is the same as the protein 2 and protein 1, which leads to double the size of the training set. The validation and testing sets are not subject to the same data argumentation. The learning rate is 2e-5, weight decay is 0.01, warm up is 2000 steps, and the schedular is WarmupLinear which linearly increasing the learning rate over the warmup steps. During training, we evaluate the model’s performance at every 2000 steps on the validation set. For every evaluation, a set of 128 protein pairs are randomly sampled from the validation set and the results are averaged over 100 times to ensure metric reliability. Here, we use both masking and classification losses to optimize our model, the loss function for each data point <italic>l</italic> can be represented as: <disp-formula id="FD5"><label>(5)</label><mml:math id="M5"><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>l</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>β</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math></disp-formula> where <italic>l</italic><sub>mlm</sub> and <italic>l<sub>ce</sub></italic> are separately represent the MLM loss and classification (ie cross entropy) loss. <italic>I</italic> can be written as: <disp-formula id="FD6"><label>(6)</label><mml:math id="M6"><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mi>α</mml:mi><mml:mi>M</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mrow><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula> where <italic>M</italic> is the number of the masked tokens, <italic>x<sub>i</sub></italic> is the true token at position <italic>i</italic>, <italic>p(X<sub>i</sub>|X<sub>-i</sub>)</italic> is the probability of the true token <italic>x<sub>i</sub></italic> given the unmasked amino acid <italic>x<sub>-i</sub></italic>. <italic>y</italic> is the label of the interaction and <italic>g</italic> is the predicted probability for <italic>y = 1</italic>, obtained from <xref ref-type="disp-formula" rid="FD4">equation 4</xref>. <italic>α</italic> and <italic>β</italic> are weights for the MLM and classification losses.</p><p id="P26">All of the models are trained on the DiRAC Extreme Scaling GPU cluster Tursa. A typical 10-epoch training run of the model with ESM-2 (650M) with human PPIs takes 31.1 hours on 16 A100-80 GPUs. A typical 10-epoch training run of the model with ESM-2 (650M) trained on virus-human PPIs takes 30.5 hours on 8 A100-80 GPUs. The model with ESM-2 (650M) trained on STRING V12 human PPIs used 16 A100-80 GPUs for 86.4 hours. For model training time with different ratios and model sizes, see the following section optimisation experiment and <xref ref-type="supplementary-material" rid="SD1">Supplementary Table 1</xref>.</p><p id="P27">We provide model checkpoints that include human PPI models trained on Sledzieski et al.’s benchmarking datasets retrieved from STRING V11<sup><xref ref-type="bibr" rid="R53">53</xref></sup>, the virus-human PPIs model trained on the benchmarking virus-human PPIs created by Tsukiyama et al.<sup><xref ref-type="bibr" rid="R11">11</xref></sup>, sourced from HPIDB 3.0<sup><xref ref-type="bibr" rid="R36">36</xref></sup>, as well as human PPIs collected by us from STRING V12<sup><xref ref-type="bibr" rid="R53">53</xref></sup>.</p></sec><sec id="S11"><title>PLM-interact optimisation experiments</title><p id="P28">To find the optimal value of <italic>α</italic> and <italic>β</italic> in <xref ref-type="disp-formula" rid="FD6">equation (6)</xref>, we benchmark a range of different options between mask loss and classification loss on human benchmarking data. For each ESM-2-35M and ESM-2-650M model, we train five models with different setting of ratios <italic>α</italic> : <italic>β</italic> between mask loss and classification loss. The ratios are <italic>α</italic> : <italic>β</italic> = 1:1, 1:5, 1:10, 0:1 (with mask), and 0:1 (without mask, denoted as classification). (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 1 b, c</xref>). We used the human validation set for each model to identify the optimal epoch checkpoint achieving the best AUPR. Next, the final model is selected based on testing on five other host PPIs.</p><p id="P29">For ESM-2-650M, the 1:10 ratio is the optimal choice (McNemar test, the greatest counts of p-values less than 0.05) compared to other options (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 1 d</xref>). The model with a 1:5 ratio of mask to classification loss based on ESM-2-35M achieved the highest number of significant improvements (McNemar test, the greatest counts of p-values less than 0.05) compared to other models (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 1 e</xref>). According to these results, we select a loss ratio of 1:10 for ESM-2-650M and 1:5 for ESM-2-35M. The ratio setting is implemented in both benchmarking of human PPI training and virus-human PPI training, as well as human PPI training using the STRING V12 database.</p></sec><sec id="S12"><title>Baselines</title><p id="P30">We compute the prediction interaction probabilities based on checkpoints of TT3D<sup><xref ref-type="bibr" rid="R13">13</xref></sup>, Topsy-Turvy<sup><xref ref-type="bibr" rid="R19">19</xref></sup> and D-SCRIPT<sup><xref ref-type="bibr" rid="R15">15</xref></sup> to generate precision-recall (PR) curves in <xref ref-type="fig" rid="F2">Figure 2b</xref>. Due to the absence of publicly available checkpoints for DeepPPI<sup><xref ref-type="bibr" rid="R20">20</xref></sup> and PIPR<sup><xref ref-type="bibr" rid="R6">6</xref></sup>, these methods are excluded from the PR curve comparison. The AUPR values for DeepPPI and Topsy-Turvy are sourced from the Topsy-Turvy paper<sup><xref ref-type="bibr" rid="R19">19</xref></sup>, those for D-SCRIPT and PIPR are from the D-SCRIPT paper<sup><xref ref-type="bibr" rid="R15">15</xref></sup>, and the AUPR value of TT3D is obtained through email communication. A complete list of the main features, architectures, reference, and code link of each baseline method can be found in <xref ref-type="supplementary-material" rid="SD1">Supplementary Table 2</xref>.</p></sec><sec id="S13"><title>MMseq2</title><p id="P31">We used MMseq2 to obtain the protein sequence-based alignment results between each pair of proteins; the parameters setting is:</p><p id="P32">--threads 128 --min-seq-id 0.4 --alignment-mode 3 --cov-mode 1.</p></sec><sec id="S14"><title>Chai-1</title><p id="P33">Chai-1<sup><xref ref-type="bibr" rid="R21">21</xref></sup> is a state-of-the-art model for molecular structure prediction, available at <ext-link ext-link-type="uri" xlink:href="https://lab.chaidiscovery.com/">https://lab.chaidiscovery.com/</ext-link>. We use Chai-1 with the “specify restraints” option to predict protein-protein structure complexes and visualise predicted PPI structures using the molecular visualisation program ChimeraX<sup><xref ref-type="bibr" rid="R23">23</xref></sup>.</p></sec><sec id="S15"><title>AlphaFold3</title><p id="P34">AlphaFold3 is a tool to predict the biomolecular interactions including protein, DNA, small molecules, ions and modified residues<sup><xref ref-type="bibr" rid="R22">22</xref></sup>, available at <ext-link ext-link-type="uri" xlink:href="https://alphafoldserver.com/">https://alphafoldserver.com/</ext-link>. We use AlphaFold3 in its PPI mode to predict protein structure complexes. The results are visualised with the molecular visualisation program ChimeraX<sup><xref ref-type="bibr" rid="R23">23</xref></sup>.</p></sec><sec id="S16"><title>McNemar’s test</title><p id="P35">McNemar’s Chi-Square test<sup><xref ref-type="bibr" rid="R58">58</xref></sup> is a statistical test that determines if there are significant differences between paired nominal data. <disp-formula id="FD7"><label>(7)</label><mml:math id="M7"><mml:mtext> </mml:mtext><mml:mi>M</mml:mi><mml:mi>c</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mo>'</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mtext> </mml:mtext><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:msub><mml:mtext>1</mml:mtext><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mtext> </mml:mtext><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:msub><mml:mtext>2</mml:mtext><mml:mrow><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mo> </mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:msub><mml:mn>1</mml:mn><mml:mrow><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mtext> </mml:mtext><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:msub><mml:mn>2</mml:mn><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mtext> </mml:mtext><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:msub><mml:mn>1</mml:mn><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo> </mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mtext> </mml:mtext><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:msub><mml:mtext>2</mml:mtext><mml:mrow><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mo> </mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:msub><mml:mn>1</mml:mn><mml:mrow><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mo> </mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mtext> </mml:mtext><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:msub><mml:mtext>2</mml:mtext><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:math></disp-formula> Here, <italic>model</italic>1<italic><sub>true</sub></italic> and <italic>model</italic>2<italic><sub>true</sub></italic> separately represent the count of correct prediction obtained by <italic>model</italic>1 or <italic>model</italic>2, while <italic>model</italic>1<italic><sub>fasle</sub></italic> and <italic>model</italic>2<italic><sub>false</sub></italic> separately represent the count of incorrect prediction obtained by <italic>model</italic>1 and <italic>model</italic>2.</p><p id="P36">To investigate if our models with varying ratios of mask-to-classification loss perform significant differences, we conducted a McNemar’s test for any pairs of models in optimisation experiments. This test is based on the number of correct and incorrect between two models. Predicted interaction probabilities from each model are used to get predicted labels, which are used to obtain the counts of correct and incorrect predictions. A McNemar’s test p-value ≤ 0.05 indicates a significant difference between the predictive performance of two models. The model with more correct predictions is considered superior to the other.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary material</label><media xlink:href="EMS199963-supplement-Supplementary_material.pdf" mimetype="application" mime-subtype="pdf" id="d64aAcDbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S17"><title>Acknowledgements</title><p>The authors acknowledge funding from the European Union’s Horizon 2020 research and innovation 562 program, under the Marie Sklodowska-Curie Actions Innovative Training Networks 563 grant agreement no. 955974 (VIROINF) for DL, a UK Medical Research Council (MRC) Doctoral Training Programme in Precision Medicine studentship (MR/N013166/1) for KDL and MRC grants: MC_UU_00034/5, MC_UU_00034/6 and MR/V01157X/1. KY acknowledges support from Cancer Research UK (EDDPGM-Nov21\100001 and DRCMDP-Nov23/100010), Biotechnology and Biological Sciences Research Council (BBSRC) BB/V016067/1, Prostate Cancer UK MA-TIA22-001 and EU Horizon 2020 grant ID 101016851. This work used the DiRAC Extreme Scaling service (Tursa) at the University of Edinburgh, managed by the Edinburgh Parallel Computing Centre on behalf of the STFC DiRAC HPC Facility (<ext-link ext-link-type="uri" xlink:href="http://www.dirac.ac.uk">www.dirac.ac.uk</ext-link>). The DiRAC service at Edinburgh was funded by BEIS, UKRI and STFC capital funding and STFC operations grants. DiRAC is part of the UKRI Digital Research Infrastructure.</p></ack><sec id="S18" sec-type="data-availability"><title>Data availability</title><p id="P37">Sledzieski et al.’s benchmarking PPI data is available at <ext-link ext-link-type="uri" xlink:href="https://d-script.readthedocs.io/en/stable/data.html">https://d-script.readthedocs.io/en/stable/data.html</ext-link></p><p id="P38">Tsukiyama et al.’s virus-human benchmarking PPIs dataset is available at: <ext-link ext-link-type="uri" xlink:href="http://kurata35.bio.kyutech.ac.jp/LSTM-PHV/download_page">http://kurata35.bio.kyutech.ac.jp/LSTM-PHV/download_page</ext-link></p><p id="P39">STRING V12 PPIs database: <ext-link ext-link-type="uri" xlink:href="https://stringdb-downloads.org/download/protein.physical.links.v12.0.txt.gz">https://stringdb-downloads.org/download/protein.physical.links.v12.0.txt.gz</ext-link> The mutations that cause or disrupt PPIs are from IntAct<sup><xref ref-type="bibr" rid="R25">25</xref></sup> Database; the link is <ext-link ext-link-type="uri" xlink:href="https://ftp.ebi.ac.uk/pub/databases/intact/current/various/mutations.tsv">https://ftp.ebi.ac.uk/pub/databases/intact/current/various/mutations.tsv</ext-link> UniProt: <ext-link ext-link-type="uri" xlink:href="https://www.uniprot.org/">https://www.uniprot.org/</ext-link></p><p id="P40">HVIDB: <ext-link ext-link-type="uri" xlink:href="http://zzdlab.com/hvidb/download.php">http://zzdlab.com/hvidb/download.php</ext-link></p></sec><sec id="S19" sec-type="data-availability"><title>Code availability</title><p id="P41">The code and trained models are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/liudan111/PLM-interact">https://github.com/liudan111/PLM-interact</ext-link></p></sec><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berggård</surname><given-names>T</given-names></name><name><surname>Linse</surname><given-names>S</given-names></name><name><surname>James</surname><given-names>P</given-names></name></person-group><article-title>Methods for the detection and analysis of protein–protein interactions</article-title><source>Proteomics</source><year>2007</year><volume>7</volume><fpage>2833</fpage><lpage>2842</lpage><pub-id pub-id-type="pmid">17640003</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>David</surname><given-names>A</given-names></name><name><surname>Sternberg</surname><given-names>MJE</given-names></name></person-group><article-title>The Contribution of Missense Mutations in Core and Rim Residues of Protein–Protein Interfaces to Human Disease</article-title><source>Journal of Molecular Biology</source><year>2015</year><volume>427</volume><fpage>2886</fpage><lpage>2898</lpage><pub-id pub-id-type="pmcid">PMC4548493</pub-id><pub-id pub-id-type="pmid">26173036</pub-id><pub-id pub-id-type="doi">10.1016/j.jmb.2015.07.004</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vassilev</surname><given-names>LT</given-names></name><etal/></person-group><article-title>In Vivo Activation of the p53 Pathway by Small-Molecule Antagonists of MDM2</article-title><source>Science</source><year>2004</year><volume>303</volume><fpage>844</fpage><lpage>848</lpage><pub-id pub-id-type="pmid">14704432</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kotlyar</surname><given-names>M</given-names></name><name><surname>Pastrello</surname><given-names>C</given-names></name><name><surname>Sheahan</surname><given-names>N</given-names></name><name><surname>Jurisica</surname><given-names>I</given-names></name></person-group><source>Integrated interactions database: tissue-specific view of the human and model organism interactomes</source><year>2015</year><pub-id pub-id-type="pmcid">PMC4702811</pub-id><pub-id pub-id-type="pmid">26516188</pub-id><pub-id pub-id-type="doi">10.1093/nar/gkv1115</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Shin</surname><given-names>W-H</given-names></name><name><surname>Kumazawa</surname><given-names>K</given-names></name><name><surname>Imai</surname><given-names>K</given-names></name><name><surname>Hirokawa</surname><given-names>T</given-names></name><name><surname>Kihara</surname><given-names>D</given-names></name></person-group><source>Current Challenges and Opportunities in Designing Protein–Protein Interaction Targeted Drugs</source><year>2020</year><pub-id pub-id-type="pmcid">PMC7669531</pub-id><pub-id pub-id-type="pmid">33209039</pub-id><pub-id pub-id-type="doi">10.2147/AABC.S235542</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>M</given-names></name><etal/></person-group><article-title>Multifaceted protein–protein interaction prediction based on Siamese residual RCNN</article-title><source>Bioinformatics</source><year>2019</year><volume>35</volume><fpage>i305</fpage><lpage>i314</lpage><pub-id pub-id-type="pmcid">PMC6681469</pub-id><pub-id pub-id-type="pmid">31510705</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btz328</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hashemifar</surname><given-names>S</given-names></name><name><surname>Neyshabur</surname><given-names>B</given-names></name><name><surname>Khan</surname><given-names>AA</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name></person-group><article-title>Predicting protein–protein interactions through sequence-based deep learning</article-title><source>Bioinformatics</source><year>2018</year><volume>34</volume><fpage>i802</fpage><lpage>i810</lpage><pub-id pub-id-type="pmcid">PMC6129267</pub-id><pub-id pub-id-type="pmid">30423091</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/bty573</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>Y</given-names></name><name><surname>Wuchty</surname><given-names>S</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name></person-group><article-title>SGPPI: structure-aware prediction of protein–protein interactions in rigorous conditions with graph convolutional network</article-title><source>Briefings in Bioinformatics</source><year>2023</year><volume>24</volume><elocation-id>bbad020</elocation-id><pub-id pub-id-type="pmid">36682013</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yuan</surname><given-names>Q</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Zhao</surname><given-names>H</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name></person-group><article-title>Structure-aware protein–protein interaction site prediction using deep graph convolutional network</article-title><source>Bioinformatics</source><year>2021</year><volume>38</volume><fpage>125</fpage><lpage>132</lpage><pub-id pub-id-type="pmid">34498061</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>J</given-names></name><etal/></person-group><article-title>Predicting protein–protein interactions based only on sequences information</article-title><source>Proceedings of the National Academy of Sciences</source><year>2007</year><volume>104</volume><fpage>4337</fpage><lpage>4341</lpage><pub-id pub-id-type="pmcid">PMC1838603</pub-id><pub-id pub-id-type="pmid">17360525</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0607879104</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsukiyama</surname><given-names>S</given-names></name><name><surname>Hasan</surname><given-names>MM</given-names></name><name><surname>Fujii</surname><given-names>S</given-names></name><name><surname>Kurata</surname><given-names>H</given-names></name></person-group><article-title>LSTM-PHV: prediction of human-virus protein–protein interactions by LSTM with word2vec</article-title><source>Briefings in Bioinformatics</source><year>2021</year><volume>22</volume><elocation-id>bbab228</elocation-id><pub-id pub-id-type="pmcid">PMC8574953</pub-id><pub-id pub-id-type="pmid">34160596</pub-id><pub-id pub-id-type="doi">10.1093/bib/bbab228</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hallee</surname><given-names>L</given-names></name><name><surname>Gleghorn</surname><given-names>JP</given-names></name></person-group><article-title>Protein–Protein Interaction Prediction is Achievable with Large Language Models</article-title><year>2023</year><elocation-id>2023.06.07.544109</elocation-id><comment>Preprint at</comment><pub-id pub-id-type="doi">10.1101/2023.06.07.544109</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sledzieski</surname><given-names>S</given-names></name><name><surname>Devkota</surname><given-names>K</given-names></name><name><surname>Singh</surname><given-names>R</given-names></name><name><surname>Cowen</surname><given-names>L</given-names></name><name><surname>Berger</surname><given-names>B</given-names></name></person-group><article-title>TT3D: Leveraging precomputed protein 3D sequence models to predict protein–protein interactions</article-title><source>Bioinformatics</source><year>2023</year><volume>39</volume><elocation-id>btad663</elocation-id><pub-id pub-id-type="pmcid">PMC10640393</pub-id><pub-id pub-id-type="pmid">37897686</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btad663</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Madan</surname><given-names>S</given-names></name><name><surname>Demina</surname><given-names>V</given-names></name><name><surname>Stapf</surname><given-names>M</given-names></name><name><surname>Ernst</surname><given-names>O</given-names></name><name><surname>Fröhlich</surname><given-names>H</given-names></name></person-group><article-title>Accurate prediction of virus-host protein-protein interactions via a Siamese neural network using deep protein sequence embeddings</article-title><source>Patterns</source><year>2022</year><volume>3</volume><elocation-id>100551</elocation-id><pub-id pub-id-type="pmcid">PMC9481957</pub-id><pub-id pub-id-type="pmid">36124304</pub-id><pub-id pub-id-type="doi">10.1016/j.patter.2022.100551</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sledzieski</surname><given-names>S</given-names></name><name><surname>Singh</surname><given-names>R</given-names></name><name><surname>Cowen</surname><given-names>L</given-names></name><name><surname>Berger</surname><given-names>B</given-names></name></person-group><article-title>D-SCRIPT translates genome to phenome with sequence-based, structure-aware, genome-scale predictions of protein-protein interactions</article-title><source>Cell Systems</source><year>2021</year><volume>12</volume><fpage>969</fpage><lpage>982</lpage><elocation-id>e6</elocation-id><pub-id pub-id-type="pmcid">PMC8586911</pub-id><pub-id pub-id-type="pmid">34536380</pub-id><pub-id pub-id-type="doi">10.1016/j.cels.2021.08.010</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Z</given-names></name><etal/></person-group><article-title>Protein language models learn evolutionary statistics of interacting sequence motifs</article-title><year>2024</year><elocation-id>2024.01.30.577970</elocation-id><comment>Preprint at</comment><pub-id pub-id-type="pmcid">PMC11551344</pub-id><pub-id pub-id-type="pmid">39467119</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2406285121</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>Z</given-names></name><etal/></person-group><article-title>Evolutionary-scale prediction of atomic-level protein structure with a language model</article-title><source>Science</source><year>2023</year><volume>379</volume><fpage>1123</fpage><lpage>1130</lpage><pub-id pub-id-type="pmid">36927031</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Reimers</surname><given-names>N</given-names></name><name><surname>Gurevych</surname><given-names>I</given-names></name></person-group><article-title>Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</article-title><year>2019</year><comment>Preprint at <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1908.10084">http://arxiv.org/abs/1908.10084</ext-link></comment></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singh</surname><given-names>R</given-names></name><name><surname>Devkota</surname><given-names>K</given-names></name><name><surname>Sledzieski</surname><given-names>S</given-names></name><name><surname>Berger</surname><given-names>B</given-names></name><name><surname>Cowen</surname><given-names>L</given-names></name></person-group><article-title>Topsy-Turvy: integrating a global view into sequence-based PPI prediction</article-title><source>Bioinformatics</source><year>2022</year><volume>38</volume><fpage>i264</fpage><lpage>i272</lpage><pub-id pub-id-type="pmcid">PMC9235477</pub-id><pub-id pub-id-type="pmid">35758793</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btac258</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Richoux</surname><given-names>F</given-names></name><name><surname>Servantie</surname><given-names>C</given-names></name><name><surname>Borès</surname><given-names>C</given-names></name><name><surname>Téletchéa</surname><given-names>S</given-names></name></person-group><source>Comparing two deep learning sequence-based models for protein–protein interaction prediction</source><year>2019</year><pub-id pub-id-type="doi">10.48550/arXiv.1901.06268</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="book"><collab>Chai Discovery</collab><source>Chai-1: Decoding the molecular interactions of life</source><year>2024</year><comment>Preprint at</comment><pub-id pub-id-type="doi">10.1101/2024.10.10.615955</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abramson</surname><given-names>J</given-names></name><etal/></person-group><article-title>Accurate structure prediction of biomolecular interactions with AlphaFold 3</article-title><source>Nature</source><year>2024</year><volume>630</volume><fpage>493</fpage><lpage>500</lpage><pub-id pub-id-type="pmcid">PMC11168924</pub-id><pub-id pub-id-type="pmid">38718835</pub-id><pub-id pub-id-type="doi">10.1038/s41586-024-07487-w</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pettersen</surname><given-names>EF</given-names></name><etal/></person-group><article-title>UCSF ChimeraX: Structure visualization for researchers, educators, and developers</article-title><source>Protein Science</source><year>2021</year><volume>30</volume><fpage>70</fpage><lpage>82</lpage><pub-id pub-id-type="pmcid">PMC7737788</pub-id><pub-id pub-id-type="pmid">32881101</pub-id><pub-id pub-id-type="doi">10.1002/pro.3943</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><collab>UniProt Consortium</collab><article-title>UniProt: a worldwide hub of protein knowledge</article-title><source>Nucleic Acids Res</source><year>2019</year><volume>47</volume><fpage>D506</fpage><lpage>D515</lpage><pub-id pub-id-type="pmcid">PMC6323992</pub-id><pub-id pub-id-type="pmid">30395287</pub-id><pub-id pub-id-type="doi">10.1093/nar/gky1049</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kerrien</surname><given-names>S</given-names></name><etal/></person-group><article-title>The IntAct molecular interaction database in 2012</article-title><source>Nucleic Acids Research</source><year>2012</year><volume>40</volume><fpage>D841</fpage><lpage>D846</lpage><pub-id pub-id-type="pmcid">PMC3245075</pub-id><pub-id pub-id-type="pmid">22121220</pub-id><pub-id pub-id-type="doi">10.1093/nar/gkr1088</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lim</surname><given-names>J</given-names></name><etal/></person-group><source>Opposing effects of polyglutamine expansion on native protein complexes contribute to SCA1</source><year>2008</year><pub-id pub-id-type="pmcid">PMC2377396</pub-id><pub-id pub-id-type="pmid">18337722</pub-id><pub-id pub-id-type="doi">10.1038/nature06731</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosen</surname><given-names>DR</given-names></name><etal/></person-group><article-title>Mutations in Cu/Zn superoxide dismutase gene are associated with familial amyotrophic lateral sclerosis</article-title><source>Nature</source><year>1993</year><volume>362</volume><fpage>59</fpage><lpage>62</lpage><pub-id pub-id-type="pmid">8332197</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wj</surname><given-names>B</given-names></name><etal/></person-group><article-title>SOD1A4V-mediated ALS: absence of a closely linked modifier gene and origination in Asia</article-title><source>PubMed</source><year>2008</year><pub-id pub-id-type="pmid">18055113</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>C</surname><given-names>H</given-names></name><etal/></person-group><article-title>Interactome Mapping Provides a Network of Neurodegenerative Disease Proteins and Uncovers Widespread Protein Aggregation in Affected Brains</article-title><source>PubMed</source><year>2020</year><pub-id pub-id-type="pmid">32814053</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kabuta</surname><given-names>T</given-names></name><etal/></person-group><article-title>Ubiquitin C-terminal Hydrolase L1 (UCH-L1) Acts as a Novel Potentiator of Cyclin-dependent Kinases to Enhance Cell Proliferation Independently of Its Hydrolase Activity*</article-title><source>Journal of Biological Chemistry</source><year>2013</year><volume>288</volume><fpage>12615</fpage><lpage>12626</lpage><pub-id pub-id-type="pmcid">PMC3642309</pub-id><pub-id pub-id-type="pmid">23543736</pub-id><pub-id pub-id-type="doi">10.1074/jbc.M112.435701</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grossmann</surname><given-names>A</given-names></name><etal/></person-group><article-title>Phospho-tyrosine dependent protein–protein interaction network</article-title><source>Molecular Systems Biology</source><year>2015</year><volume>11</volume><fpage>794</fpage><pub-id pub-id-type="pmcid">PMC4380928</pub-id><pub-id pub-id-type="pmid">25814554</pub-id><pub-id pub-id-type="doi">10.15252/msb.20145968</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aten</surname><given-names>TM</given-names></name><etal/></person-group><article-title>Tyrosine phosphorylation of the orphan receptor ESDN/DCBLD2 serves as a scaffold for the signaling adaptor CrkL</article-title><source>FEBS Lett</source><year>2013</year><volume>587</volume><fpage>2313</fpage><lpage>2318</lpage><pub-id pub-id-type="pmcid">PMC3759512</pub-id><pub-id pub-id-type="pmid">23770091</pub-id><pub-id pub-id-type="doi">10.1016/j.febslet.2013.05.064</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daly</surname><given-names>RJ</given-names></name><name><surname>Binder</surname><given-names>MD</given-names></name><name><surname>Sutherland</surname><given-names>RL</given-names></name></person-group><article-title>Overexpression of the Grb2 gene in human breast cancer cell lines</article-title><source>Oncogene</source><year>1994</year><volume>9</volume><fpage>2723</fpage><lpage>2727</lpage><pub-id pub-id-type="pmid">8058337</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ohanian</surname><given-names>M</given-names></name><etal/></person-group><source>Liposomal Grb2 antisense oligodeoxynucleotide (BP1001) in patients with refractory or relapsed haematological malignancies: a single-centre, open-label, dose-escalation, phase 1/1b trial</source><year>2018</year><pub-id pub-id-type="pmid">29550383</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koshikawa</surname><given-names>K</given-names></name><etal/></person-group><article-title>Significant up-regulation of a novel gene, CLCP1, in a highly metastatic lung cancer subline as well as in lung cancers in vivo</article-title><source>Oncogene</source><year>2002</year><volume>21</volume><fpage>2822</fpage><lpage>2828</lpage><pub-id pub-id-type="pmid">11973641</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ammari</surname><given-names>MG</given-names></name><name><surname>Gresham</surname><given-names>CR</given-names></name><name><surname>McCarthy</surname><given-names>FM</given-names></name><name><surname>Nanduri</surname><given-names>B</given-names></name></person-group><article-title>HPIDB 2.0: a curated database for host–pathogen interactions</article-title><source>Database</source><year>2016</year><volume>2016</volume><elocation-id>baw103</elocation-id><pub-id pub-id-type="pmcid">PMC4930832</pub-id><pub-id pub-id-type="pmid">27374121</pub-id><pub-id pub-id-type="doi">10.1093/database/baw103</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>X</given-names></name><name><surname>Yang</surname><given-names>S</given-names></name><name><surname>Li</surname><given-names>Q</given-names></name><name><surname>Wuchty</surname><given-names>S</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name></person-group><article-title>Prediction of human-virus protein-protein interactions through a sequence embedding-based machine learning method</article-title><source>Computational and Structural Biotechnology Journal</source><year>2020</year><volume>18</volume><fpage>153</fpage><lpage>161</lpage><pub-id pub-id-type="pmcid">PMC6961065</pub-id><pub-id pub-id-type="pmid">31969974</pub-id><pub-id pub-id-type="doi">10.1016/j.csbj.2019.12.005</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Elnaggar</surname><given-names>A</given-names></name><etal/></person-group><source>ProtTrans: Towards Cracking the Language of Life’s Code Through Self-Supervised Learning</source><year>2021</year><elocation-id>2020.07.12.199554</elocation-id><comment>Preprint at</comment><pub-id pub-id-type="doi">10.1101/2020.07.12.199554</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>X</given-names></name><etal/></person-group><article-title>HVIDB: a comprehensive database for human–virus protein–protein interactions</article-title><source>Briefings in Bioinformatics</source><year>2021</year><volume>22</volume><fpage>832</fpage><lpage>844</lpage><pub-id pub-id-type="pmid">33515030</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Kempen</surname><given-names>M</given-names></name><etal/></person-group><article-title>Fast and accurate protein structure search with Foldseek</article-title><source>Nat Biotechnol</source><year>2023</year><pub-id pub-id-type="pmcid">PMC10869269</pub-id><pub-id pub-id-type="pmid">37156916</pub-id><pub-id pub-id-type="doi">10.1038/s41587-023-01773-0</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frazer</surname><given-names>J</given-names></name><etal/></person-group><article-title>Disease variant prediction with deep generative models of evolutionary data</article-title><source>Nature</source><year>2021</year><volume>599</volume><fpage>91</fpage><lpage>95</lpage><pub-id pub-id-type="pmid">34707284</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rives</surname><given-names>A</given-names></name><etal/></person-group><article-title>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</article-title><source>Proc Natl Acad Sci U S A</source><year>2021</year><volume>118</volume><elocation-id>e2016239118</elocation-id><pub-id pub-id-type="pmcid">PMC8053943</pub-id><pub-id pub-id-type="pmid">33876751</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2016239118</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>J</given-names></name><etal/></person-group><article-title>Accurate proteome-wide missense variant effect prediction with AlphaMissense</article-title><source>Science</source><year>2023</year><volume>381</volume><elocation-id>eadg7492</elocation-id><pub-id pub-id-type="pmid">37733863</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hayes</surname><given-names>T</given-names></name><etal/></person-group><source>Simulating 500 million years of evolution with a language model</source><year>2024</year><comment>Preprint at</comment><pub-id pub-id-type="doi">10.1101/2024.07.01.600583</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cornman</surname><given-names>A</given-names></name><etal/></person-group><source>The OMG dataset: An Open MetaGenomic corpus for mixed-modality genomic language modeling</source><year>2024</year><pub-id pub-id-type="doi">10.1101/2024.08.14.607850</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y</given-names></name><etal/></person-group><source>LC-PLM: Long-context Protein Language Model</source><year>2024</year><pub-id pub-id-type="doi">10.1101/2024.10.29.620988</pub-id></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Babayan</surname><given-names>SA</given-names></name><name><surname>Orton</surname><given-names>RJ</given-names></name><name><surname>Streicker</surname><given-names>DG</given-names></name></person-group><article-title>Predicting reservoir hosts and arthropod vectors from evolutionary signatures in RNA virus genomes</article-title><source>Science</source><year>2018</year><volume>362</volume><fpage>577</fpage><lpage>580</lpage><pub-id pub-id-type="pmcid">PMC6536379</pub-id><pub-id pub-id-type="pmid">30385576</pub-id><pub-id pub-id-type="doi">10.1126/science.aap9072</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Young</surname><given-names>F</given-names></name><name><surname>Rogers</surname><given-names>S</given-names></name><name><surname>Robertson</surname><given-names>DL</given-names></name></person-group><source>Predicting host taxonomic information from viral genomes: A comparison of feature representations</source><year>2022</year><pub-id pub-id-type="pmcid">PMC7307784</pub-id><pub-id pub-id-type="pmid">32453718</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007894</pub-id></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>D</given-names></name><name><surname>Young</surname><given-names>F</given-names></name><name><surname>Robertson</surname><given-names>DL</given-names></name><name><surname>Yuan</surname><given-names>K</given-names></name></person-group><source>Prediction of virus-host associations using protein language models and multiple instance learning</source><year>2023</year><pub-id pub-id-type="pmcid">PMC11614202</pub-id><pub-id pub-id-type="pmid">39561204</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1012597</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Roux</surname><given-names>S</given-names></name><etal/></person-group><source>iPHoP: An integrated machine learning framework to maximize host prediction for metagenome-derived viruses of archaea and bacteria</source><year>2023</year><pub-id pub-id-type="pmcid">PMC10155999</pub-id><pub-id pub-id-type="pmid">37083735</pub-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3002083</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stukalov</surname><given-names>A</given-names></name><etal/></person-group><article-title>Multilevel proteomics reveals host perturbations by SARS-CoV-2 and SARS-CoV</article-title><source>Nature</source><year>2021</year><volume>594</volume><fpage>246</fpage><lpage>252</lpage><pub-id pub-id-type="pmid">33845483</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>J</surname><given-names>L</given-names></name><etal/></person-group><article-title>Virus-Host Interactome and Proteomic Survey Reveal Potential Virulence Factors Influencing SARS-CoV-2 Pathogenesis</article-title><source>PubMed</source><year>2021</year><pub-id pub-id-type="pmcid">PMC7373048</pub-id><pub-id pub-id-type="pmid">32838362</pub-id><pub-id pub-id-type="doi">10.1016/j.medj.2020.07.002</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szklarczyk</surname><given-names>D</given-names></name><etal/></person-group><article-title>STRING v11: protein–protein association networks with increased coverage, supporting functional discovery in genome-wide experimental datasets</article-title><source>Nucleic Acids Research</source><year>2019</year><volume>47</volume><fpage>D607</fpage><lpage>D613</lpage><pub-id pub-id-type="pmcid">PMC6323986</pub-id><pub-id pub-id-type="pmid">30476243</pub-id><pub-id pub-id-type="doi">10.1093/nar/gky1131</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>L</given-names></name><name><surname>Niu</surname><given-names>B</given-names></name><name><surname>Zhu</surname><given-names>Z</given-names></name><name><surname>Wu</surname><given-names>S</given-names></name><name><surname>Li</surname><given-names>W</given-names></name></person-group><article-title>CD-HIT: accelerated for clustering the next-generation sequencing data</article-title><source>Bioinformatics</source><year>2012</year><volume>28</volume><fpage>3150</fpage><lpage>3152</lpage><pub-id pub-id-type="pmcid">PMC3516142</pub-id><pub-id pub-id-type="pmid">23060610</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/bts565</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szklarczyk</surname><given-names>D</given-names></name><etal/></person-group><article-title>The STRING database in 2023: protein–protein association networks and functional enrichment analyses for any sequenced genome of interest</article-title><source>Nucleic Acids Research</source><year>2023</year><volume>51</volume><fpage>D638</fpage><lpage>D646</lpage><pub-id pub-id-type="pmcid">PMC9825434</pub-id><pub-id pub-id-type="pmid">36370105</pub-id><pub-id pub-id-type="doi">10.1093/nar/gkac1000</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinegger</surname><given-names>M</given-names></name><name><surname>Söding</surname><given-names>J</given-names></name></person-group><article-title>MMseqs2 enables sensitive protein sequence searching for the analysis of massive data sets</article-title><source>Nature Biotechnology</source><year>2017</year><volume>35</volume><fpage>1026</fpage><lpage>1028</lpage><pub-id pub-id-type="pmid">29035372</pub-id></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Devlin</surname><given-names>J</given-names></name><name><surname>Chang</surname><given-names>M-W</given-names></name><name><surname>Lee</surname><given-names>K</given-names></name><name><surname>Toutanova</surname><given-names>K</given-names></name></person-group><article-title>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</article-title><year>2019</year><comment>Preprint at <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1810.04805">http://arxiv.org/abs/1810.04805</ext-link></comment></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McNemar</surname><given-names>Q</given-names></name></person-group><article-title>Note on the sampling error of the difference between correlated proportions or percentages</article-title><source>Psychometrika</source><year>1947</year><volume>12</volume><fpage>153</fpage><lpage>157</lpage><pub-id pub-id-type="pmid">20254758</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>A comparison of PLM-interact to the typical existing PPI prediction framework.</title><p><bold>a</bold>. PPI prediction models that use pre-trained protein language models to extract single protein embeddings. Then an interaction classifier is trained use these single protein embeddings. <bold>b</bold>. PLM-interact uses a protein language model with a longer context to handle a pair of protein sequences directly. Both the mask language modelling task and a binary classification task predicting interaction status are used to train the model. (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 1</xref>).</p></caption><graphic xlink:href="EMS199963-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>PLM-interact achieves the highest PPI prediction performance.</title><p>The benchmarking results of PLM-interact with state-of-the-art PPI prediction models. <bold>a.</bold> The data size of training, validation and test PPIs. <bold>b.</bold> The taxonomic tree of the training and test species, precision-recall curves of each test species and a bar plot of AUPR values on PPI prediction benchmarking. <bold>c</bold>. Violin plots of predicted interaction probabilities of PLM-interact and TT3D on positive and negative pairs, respectively. (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 2</xref> for more information).</p></caption><graphic xlink:href="EMS199963-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>PPI example for each species that was predicted correctly by PLM-interact but not by TT3D.</title><p>Protein-protein structures are predicted by Chai-1<sup><xref ref-type="bibr" rid="R21">21</xref></sup> and visualised with ChimeraX<sup><xref ref-type="bibr" rid="R23">23</xref></sup>. Both models’ prediction interaction probabilities range between 0 and 1. A predicted interaction probability &gt;0.5, is predicted as a positive PPI, while &lt;0.5 is a negative pair. Interacting proteins are shown from left (yellow) to right (green), respectively, for <bold>mouse</bold>: Q8K1Z0 (Ubiquinone biosynthesis protein COQ9, mitochondrial) and Q8R1S0 (Ubiquinone biosynthesis monooxygenase COQ6, mitochondrial); <bold>Worm</bold>: Q21955 (Mediator of RNA polymerase II transcription subunit 15) and Q9N4F2 (Mediator of RNA polymerase II transcription subunit 19); <bold>Fly</bold>: Q9V3J1 (V-type proton ATPase subunit H) and Q9V7D2 (V-type proton ATPase subunit D 1); <bold>Yeast</bold>: P22149 (Iron-regulated transcriptional activator AFT1) and P53040 (Transcription initiation factor TFIID subunit 6); and <bold><italic>E. coli</italic></bold>: A0A454A7G5 (ABC transporter permease protein) and A0A454A7H5 (Possible ABC-transport protein, ATP-binding component). See <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 3</xref> for AlphaFold3<sup><xref ref-type="bibr" rid="R22">22</xref></sup> predicted structures. The ipTMs for both Chai-1 and AlphaFold3 are shown for each structure, where ipTM &lt; 0.6 indicates failed predictions and ipTM &gt;0.8 indicates high confidence predictions.</p></caption><graphic xlink:href="EMS199963-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Demonstration of PLM-interact detecting changes in human PPIs associated with mutations.</title><p><bold>a</bold> shows two mutation-causing interaction examples, while <bold>b</bold> shows two mutationdisrupting PPI examples. These PPI structures are predicted using Chai-1<sup><xref ref-type="bibr" rid="R21">21</xref></sup> and visualised with ChimeraX<sup><xref ref-type="bibr" rid="R23">23</xref></sup>; here, the mutated amino acids are highlighted in purple. Prediction interaction probabilities exceeding 0.5 indicate the proteins interact, while below 0.5 indicate non-interact. Chai-1’s ipTM scores give the structure prediction confidence where &lt;0.6 indicates failed predictions. Interacting protein structures are shown from left (yellow) to right (green): <bold>a.</bold> Residue 225 Glutamine (Q) of P54253 (Ataxin-1) is mutated to 50 Q, causing interaction with Q969T4 (Ubiquitin-conjugating enzyme E2 E3)<sup><xref ref-type="bibr" rid="R29">29</xref></sup>; Residue 5 Alanine (A) of P00441 (Superoxide dismutase [Cu-Zn]) is mutated to Valine (V), causing interaction with P11802 (Cyclin-dependent kinase 4)<sup><xref ref-type="bibr" rid="R30">30</xref></sup>. <bold>b.</bold> Residue 86 Arginine(R) of P62993 (Growth factor receptor-bound protein 2) is mutated to Lysine (K), disrupting its interaction with Q5SXH7-1 (Pleckstrin homology domain-containing family S member 1)<sup><xref ref-type="bibr" rid="R31">31</xref></sup>; Residue 732 Tyrosine (Y) of Q96PD2 (Discoidin, CUB and LCCL domain-containing protein 2) is mutated to Phenylalanine (F), disrupting its interaction with P46109 (Crk-like protein)<sup><xref ref-type="bibr" rid="R32">32</xref></sup>. See <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 4</xref> for AlphaFold3<sup><xref ref-type="bibr" rid="R22">22</xref></sup> predicted structures.</p></caption><graphic xlink:href="EMS199963-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><p><bold>a.</bold> Comparison of AUPR, F1 and MCC metrics of PLM-interact against recent virus-human PPI models. <bold>b</bold>. The distribution of the length of virus proteins, human proteins and virus-human protein pairs. <bold>c</bold>. The virus-human PPIs are correctly predicted by our model and the 3D complex structures of virus-human PPIs are experimentally verified structures collected from the human-virus PPI database (<bold>HVIDB</bold><sup><xref ref-type="bibr" rid="R39">39</xref></sup>). From left (green) to right (yellow), these interacting protein structures are: Tumour necrosis factor receptor superfamily member 14 (Human protein: Q92956) with Envelope glycoprotein D (human herpes simplex virus 1: P57083), Ephrin-B2 (human protein: P52799) with Glycoprotein G (Nipah virus protein: Q9IH62) and Retinoblastoma-associated protein (human protein: P06400) with Large T antigen (Simian virus 40: P03070). Note: The metrics results of the other three models in panel <bold>a</bold> are taken from STEP<sup><xref ref-type="bibr" rid="R14">14</xref></sup> paper.</p></caption><graphic xlink:href="EMS199963-f005"/></fig></floats-group></article>