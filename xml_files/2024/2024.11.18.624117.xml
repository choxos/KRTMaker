<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS200373</article-id><article-id pub-id-type="doi">10.1101/2024.11.18.624117</article-id><article-id pub-id-type="archive">PPR942662</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Detecting interspecific positive selection using convolutional neural networks</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-2094-3324</contrib-id><name><surname>West</surname><given-names>Charlotte</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5617-5086</contrib-id><name><surname>Walker</surname><given-names>Conor R.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">https://orcid.org/0009-0005-1607-5300</contrib-id><name><surname>Arasti</surname><given-names>Shayesteh</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="fn" rid="FN1">a</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">https://orcid.org/0009-0007-9368-7524</contrib-id><name><surname>Vasilev</surname><given-names>Viacheslav</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="fn" rid="FN2">b</xref></contrib><contrib contrib-type="author"><name><surname>Xu</surname><given-names>Xingze</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="fn" rid="FN3">c</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-1776-8564</contrib-id><name><surname>De Maio</surname><given-names>Nicola</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-8486-2211</contrib-id><name><surname>Goldman</surname><given-names>Nick</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">**</xref></contrib></contrib-group><aff id="A1"><label>1</label>European Molecular Biology Laboratory, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02catss52</institution-id><institution>European Bioinformatics Institute (EMBL-EBI)</institution></institution-wrap>, Wellcome Genome Campus, <city>Hinxton</city>, <postal-code>CB10 1SD</postal-code>, <country country="GB">United Kingdom</country></aff><aff id="A2"><label>2</label>Department of Genetics, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>University of Cambridge</institution></institution-wrap>, <city>Cambridge</city>, <postal-code>CB2 3EH</postal-code>, <country country="GB">United Kingdom</country></aff><author-notes><corresp id="CR1">
<label>**</label>Corresponding author: <email>goldman@ebi.ac.uk</email>.</corresp><fn fn-type="present-address" id="FN1"><label>a</label><p id="P1">Current address: Computer Science and Engineering Department, University of California San Diego, La Jolla, CA 92093, USA</p></fn><fn fn-type="present-address" id="FN2"><label>b</label><p id="P2">Current address: Moscow Institute of Physics and Technology, Dolgoprudny, 141701 Russia</p></fn><fn fn-type="present-address" id="FN3"><label>c</label><p id="P3">Current address: Wellcome-MRC Cambridge Stem Cell Institute, Cambridge, CB2 0AW, UK</p></fn></author-notes><pub-date pub-type="nihms-submitted"><day>21</day><month>11</month><year>2024</year></pub-date><pub-date pub-type="preprint"><day>19</day><month>11</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nd/4.0/">CC BY-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P4">Traditional statistical methods using maximum likelihood and Bayesian inference can detect positive selection from an interspecific phylogeny and a codon sequence alignment based on model assumptions, but they are prone to false positives due to alignment errors and can lack power. These problems are particularly pronounced when faced with high levels of indels and divergence. Leveraging the feature-detection capabilities of convolutional neural networks (CNNs), we achieve higher accuracy in detecting selection across a specific range of phylogenetic scenarios and evolutionary modes. This advantage is particularly evident with noisy data prone to misalignments. Our method shows some ability to account for these errors, where most statistical frameworks fail to do so in a tractable manner. We explore generalisability and identify future avenues to achieve broader utility. Once trained, our CNN model is faster at test time, making it a scalable alternative to traditional statistical methods for large-scale, multi-gene analyses. In addition to binary classification (inference of the presence or absence of positive selection during the evolution of the sequences), we use saliency maps to understand what the model learns and observe how this could be leveraged for sitewise inference of positive selection.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><label>1</label><title>Introduction</title><sec id="S2" sec-type="intro"><label>1.1</label><title>Background</title><p id="P5">Natural selection is a crucial aspect of evolution. We study it to uncover evolutionary mechanisms that drive biodiversity, adaptation, and the development of life on Earth. A deeper understanding of selection allows us to make insights across biological fields, including drivers of cancer evolution (<xref ref-type="bibr" rid="R40">Martincorena et al., 2017</xref>), host-pathogen co-evolution (<xref ref-type="bibr" rid="R4">Bishop, Dean, and Mitchell-Olds, 2000</xref>) and adaptation to extreme environments (<xref ref-type="bibr" rid="R56">Shin et al., 2014</xref>). These insights offer practical applications, from innovative cancer treatments to effective climate change management. However, the urgency and impact of such practicalities and the growing throughput of sequencing technologies necessitates fast and accurate methods for studying selection.</p><p id="P6">Natural selection is typically studied either at the population level (microevolutionary) or at the interspecies level (macroevolutionary). Within these temporal frames, we may be interested in particular genes or genomic loci targeted by selection. In this study we focus on selection inference of protein-coding genes at the interspecies scale. This involves firstly gathering homologous genes from different species and aligning these sequences into a multiple sequence alignment (MSA). Then a phylogenetic tree that describes the evolutionary history of these genes is inferred from the MSA. Current state-of-the-art methods then take the MSA, tree and an explicit model of molecular evolution and apply statistical methods such as maximum likelihood or Bayesian inference to estimate parameters of the model that are interpreted to signal the presence or absence of selection.</p><p id="P7">Nucleotide models of sequence evolution neglect the coding and non-independent aspects of the evolution of adjacent sequence positions and lack a direct parameter through which we can model Darwinian selection of amino acid sequences. Amino acid models do not consider synonymous mutations and are largely unable to help in the detection of selection (although <xref ref-type="bibr" rid="R70">Weber et al., 2020</xref>, demonstrate that some information about non-synonymous mutations remains when amino acid sequence data are embedded in a genetic code-aware model). Codon-based models were proposed to address these issues. The codon substitution models proposed by <xref ref-type="bibr" rid="R16">Goldman and Yang (1994)</xref> and <xref ref-type="bibr" rid="R44">Muse and Gaut (1994)</xref> and further developed by (e.g.) <xref ref-type="bibr" rid="R80">Yang, Nielsen, et al. (2000)</xref> are continuous-time Markov chains defined in terms of the substitution rates from each codon to any other that can be realised by a single nucleotide change (excluding stop codons). The framework simultaneously utilises both nucleotide and amino acid-level information available in an alignment of protein-coding DNA sequences, allowing for parameters through which selection can be modelled. In particular, the parameter <italic>ω</italic>, frequently referred to as <bold><italic>dN/dS</italic></bold> and representing the ratio of the rates of non-synonymous to synonymous substitutions, has been widely used as a proxy for selective pressure (<xref ref-type="bibr" rid="R77">Yang, 2014</xref>). Values of ω larger than 1 suggest that substitutions leading to an amino acid change, and therefore presumably an alteration in protein structure or function, are occurring at a higher rate than synonymous (and supposedly neutral) substitutions. Typically <italic>ω</italic> &gt; 1 is interpreted as positive selection, <italic>ω</italic> = 1 as neutral evolution, and <italic>ω</italic> &lt; 1 as purifying selection.</p><p id="P8">Tools such as CODEML from the PAML package (<xref ref-type="bibr" rid="R76">Yang, 2007</xref>; <xref ref-type="bibr" rid="R2">Álvarez-Carretero, Kapli, and Yang, 2023</xref>) and HyPhy (<xref ref-type="bibr" rid="R51">Pond, Frost, and Muse, 2005</xref>) use maximum likelihood methods to infer whether natural selection affected the evolution of given protein-coding sequences through hypothesis tests and the estimation of values of <italic>ω</italic>. Indeed, in CODEML <italic>ω</italic> can be estimated for the whole tree and alignment, or per branch, per site, or both (<xref ref-type="bibr" rid="R16">Goldman and Yang, 1994</xref>; <xref ref-type="bibr" rid="R44">Muse and Gaut, 1994</xref>; <xref ref-type="bibr" rid="R75">Yang, 1998</xref>; <xref ref-type="bibr" rid="R80">Yang, Nielsen, et al., 2000</xref>; <xref ref-type="bibr" rid="R79">Yang and Nielsen, 2002</xref>; <xref ref-type="bibr" rid="R82">Zhang, Nielsen, and Yang, 2005</xref>). However, these statistical methods are prone to some issues. Firstly, they can be overly conservative (<xref ref-type="bibr" rid="R74">Wong, Yang, et al., 2004</xref>; <xref ref-type="bibr" rid="R78">Yang, Goldman, and Nielsen, 2009</xref>), resulting in the failure to detect positive selection. This is especially prevalent with sequences of very low or high divergence where the signal for positive selection is weak (<xref ref-type="bibr" rid="R21">Jordan and Goldman, 2012</xref>). Secondly, these methods can return false positives due to alignment errors in the MSAs (<xref ref-type="bibr" rid="R12">Fletcher and Yang, 2010</xref>). This is because the statistical models interpreting the MSAs do not account for misalignments and take the MSA as truth (<xref ref-type="bibr" rid="R73">Wong, Suchard, and Huelsenbeck, 2008</xref>). Misalignments, particularly over-alignment where non-homologous residues are placed in the same column (<xref ref-type="bibr" rid="R36">Löytynoja and Goldman, 2008</xref>), can result in extra inferred non-synonymous mutations and thus cause inflation of the estimated <italic>ω</italic> value.</p><p id="P9">On the other hand, misalignments can also result in false negatives. For example, if synonymous but non-homologous codons are aligned at a site under positive selection, this can lead to an apparent increased synonymous substitution rate and thus decreased <italic>ω</italic>. Alternatively, if truly homologous codons are not correctly aligned at a positively selected site, this may result in less supporting evidence for positive selection there. Both instances reduce statistical power at that site (<xref ref-type="bibr" rid="R21">Jordan and Goldman, 2012</xref>).</p><p id="P10">One way to account for alignment errors is not to assume a fixed alignment, but instead to integrate over alignment uncertainty with Bayesian methods — jointly estimating the alignment, model parameters, and the presence or absence of positive selection (<xref ref-type="bibr" rid="R53">Redelings, 2014</xref>). Although this approach can perform well in minimising the problem of high false positive rates (<xref ref-type="bibr" rid="R53">Redelings, 2014</xref>), it is computationally demanding and application is typically limited to small datasets (<xref ref-type="bibr" rid="R49">Pečerska, Gil, and Anisimova, 2021</xref>). Misalignment thus remains a considerable problem in selection inference analyses.</p></sec><sec id="S3"><label>1.2</label><title>AI as an alternative</title><p id="P11">A promising alternative to these model-based statistical methods for the inference of selection at the interspecies level is the use of machine learning. Specifically, convolutional neural networks (CNNs) have been successfully applied to a range of problems, most notably in the field of image recognition (<xref ref-type="bibr" rid="R26">Krizhevsky, Sutskever, and Hinton, 2017</xref>). Since genomic MSAs have intuitive image representations, and because CNNs have an innate ability for feature detection, questions of evolution lend naturally to CNNs.</p><p id="P12">CNNs have been successfully applied in population genetics, which often relies on complex models to describe evolutionary processes affecting genetic variation within and between populations, including mutation, recombination, migration, selection, and demographic history. Consequently, calculating the likelihood of a population genetic dataset can be computationally intractable (<xref ref-type="bibr" rid="R3">Bertl et al., 2017</xref>). CNNs offer computationally tractable solutions to these problems. For example, <xref ref-type="bibr" rid="R10">Flagel, Brandvain, and Schrider (2019)</xref> use a supervised CNN to infer selective sweeps and recombination rates, where the input ‘image’ is a population genetic alignment of binary values representing allele status. ImaGene (<xref ref-type="bibr" rid="R68">Torada et al., 2019</xref>) uses CNNs to quantify natural selection from aligned population genomic data, with distinct alleles represented by colours.</p><p id="P13">In the context of interspecies evolutionary genetic inference, a number of machine learning approaches have recently been proposed for addressing the problem of phylogenetic tree inference (<xref ref-type="bibr" rid="R65">Suvorov, Hochuli, and Schrider, 2020</xref>; <xref ref-type="bibr" rid="R83">Zou et al., 2020</xref>; <xref ref-type="bibr" rid="R62">Solis-Lemus, Yang, and Zepeda-Nunez, 2022</xref>; <xref ref-type="bibr" rid="R45">Nesterenko et al., 2024</xref>) and phylogenetic model selection (<xref ref-type="bibr" rid="R5">Burgstaller-Muehlbacher et al., 2023</xref>). To our knowledge, such approaches have not previously been used for studying interspecific selection.</p><p id="P14">CNNs present a compelling alternative to traditional statistical methods for detecting interspecific positive selection. Genetic data often exhibit complex, non-linear relationships between features, challenging conventional statistical approaches. Combining feature extraction and non-linearity (obtained through layers of non-linear transformations) means that CNNs are well-equipped to deal with the intricate patterns that occur in an MSA when alignment error interacts with selection. The features learned by a CNN are based on the data it encounters during training. In the study of selection, the range and complexity of simulation models available for creating realistic training data (e.g. <xref ref-type="bibr" rid="R60">Sipos et al., 2011</xref>; <xref ref-type="bibr" rid="R39">Mallo, De Oliveira Martins, and Posada, 2016</xref>; <xref ref-type="bibr" rid="R17">Haller and Messer, 2019</xref>) surpass those of models tractable for maximum likelihood inference. Moreover, CNNs are highly adaptable, allowing for task- or model-specific optimisation. Training an AI model is resource- and time-intensive but, once trained, AI methods typically result in faster inference compared with maximum likelihood or Bayesian methods. This means that AI could also address the problem of excessive computational resource consumption of classical phylogenetic methods when used on large genomic datasets (see <xref ref-type="bibr" rid="R28">Kumar, 2022</xref>; <xref ref-type="bibr" rid="R25">Kozlov and Stamatakis, 2024</xref>, and references therein).</p></sec><sec id="S4"><label>1.3</label><title>Introducing OmegaAI</title><p id="P15">Here we tackle the problem of inferring selection at the interspecies level while accounting for alignment uncertainty and errors. We introduce OmegaAI, a CNN trained to produce a binary classification indicating the presence or absence of positive selection given an MSA of homologous, protein-coding nucleotide sequences from different species. We aim to address the problems caused by alignment errors by training our CNN on alignments containing such errors. We show that our approach can achieve high levels of accuracy when inferring positive selection, especially at greater divergences, even with high levels of alignment error. Further, data evaluation time is approximately four orders of magnitude faster on average compared with state-of-the-art maximum likelihood methods. This work aims to be a proof of principle, so whilst the method’s application is limited to a specific set of problems, we nevertheless illustrate the promise of machine learning methods for studying selection, and discuss possibilities for future advances that may be even more effective.</p></sec></sec><sec id="S5"><label>2</label><title>New Approaches</title><sec id="S6"><label>2.1</label><title>CNN architecture</title><p id="P16">Following previous studies that have applied deep learning methods to evolutionary questions (<xref ref-type="bibr" rid="R10">Flagel, Brandvain, and Schrider, 2019</xref>; <xref ref-type="bibr" rid="R68">Torada et al., 2019</xref>; <xref ref-type="bibr" rid="R65">Suvorov, Hochuli, and Schrider, 2020</xref>), we choose a CNN model for our problem of detecting positive selection from input data which, before pre-processing, is an MSA of protein-coding, nucleotide sequences. CNNs are characterised by convolutions. The OmegaAI architecture (<xref ref-type="fig" rid="F1">Figure 1</xref>) begins with a series of convolutional blocks. The first layer uses a filter size of <italic>m</italic> × 3, where <italic>m</italic> is the number of sequence rows (taxa) and 3 corresponds to one codon alignment column (i.e. 3 nucleotide or gap characters), with a stride of 3 to evaluate each codon column separately, with the intent to encode the concept of a codon in this initial layer. There are a total of six convolutional layers with ReLU activation, batch normalization, dropout for regularization, and average pooling — operations that are standard with regards to CNN architectures (<xref ref-type="bibr" rid="R7">De Andrade, 2019</xref>). A global pooling layer condenses the feature maps into a single vector, which is then processed by a fully connected layer. The final layer is a dense layer with a sigmoid activation function, producing a single binary prediction (selection, 1, vs. no selection, 0), indicative for the whole MSA. As a proof of principle study, our aim was to create a concise yet effective machine learning model, hence our choice to use a conservative number of layers and parameters. Once fully trained, this CNN is what we refer to as an OmegaAI model. Throughout this work, several OmegaAI models were trained under various simulation scenarios, each reflecting aspects of biological reality.</p></sec><sec id="S7"><label>2.2</label><title>Simulation</title><p id="P17">In the absence of large amounts of biological data known to have been subject to positive, neutral or purifying selection, we use simulations under a variety of conditions to generate datasets — both with and without positive selection — in the quantities required to train and test our CNN. To reduce the vast parameter space of possible simulation scenarios, we restrict our attention to the evolution of a protein-coding gene sequence on a symmetric 8-taxon tree (<xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 1</xref>).</p><p id="P18">We simulated nucleotide sequence evolution on this phylogeny with INDELible (<xref ref-type="bibr" rid="R11">Fletcher and Yang, 2009</xref>), using standard codon and site variation models that allow for different purifying, neutral, or positive selective pressures (those used for inference in PAML; <xref ref-type="bibr" rid="R76">Yang, 2007</xref>) as well as insertions and deletions. Many different random model parameter values were used to generate alignments with varying root ancestral protein length, strength of selection, number of sites affected, and rate of evolution. By resampling parameters across numerous simulations, we generated a comprehensive training set covering a variety of realistic selective scenarios.</p><p id="P19">To further mimic real-life scenarios, we cannot rely on knowledge of the true sequence alignments but rather we must re-align the simulated sequence datasets. We primarily use Clustal Omega v1.2.4 (<xref ref-type="bibr" rid="R58">Sievers, Wilm, et al., 2011</xref>) for this since it is fast and widely used (<xref ref-type="bibr" rid="R57">Sievers and Higgins, 2018</xref>), although we also consider other alignment tools for comparison purposes. By training OmegaAI on MSAs containing alignment errors, we hope OmegaAI will learn to account for them in its predictions.</p></sec><sec id="S8"><label>2.3</label><title>Training</title><p id="P20">Our aim is to train OmegaAI to produce a single binary classification for the whole MSA, stating whether or not the gene has undergone positive selection. The CNN is trained on 1,000,000 MSAs, with exposure to roughly equal amounts of genes with and without positive selection. Although in reality most genes might not have sites under positive selection, balanced training data is used to avoid bias towards the majority class (<xref ref-type="bibr" rid="R71">Wei and Dunbrack, 2013</xref>; <xref ref-type="bibr" rid="R14">Ghosh et al., 2024</xref>).</p><p id="P21">Simulated MSAs are converted into one-hot encoded tensors, a binary representation suitable for the numeric nature of neural networks, before being input into the model. During training, these tensors pass through the convolutional layers in batches, where feature extraction and transformation occur, followed by pooling layers that reduce spatial dimensions. Batch normalization and dropout layers regularise the model to prevent overfitting. The model weights are adjusted based on the loss computed from the difference between the predicted output and the true label, using backpropagation and the update rules of the Adam optimiser (<xref ref-type="bibr" rid="R24">Kingma and Ba, 2017</xref>). This process is repeated for multiple epochs, with each epoch exposing the CNN to the entire training dataset exactly once. A CNN requires multiple epochs to iteratively refine its weights through gradient descent, allowing it to effectively learn and generalise from the training data. The training dataset is divided into training and validation subsets to monitor performance, resulting in the final model.</p></sec><sec id="S9"><label>2.4</label><title>Testing</title><p id="P22">A further 2,000 MSAs are simulated in the same manner to test each OmegaAI model, guaranteeing avoidance of train-test contamination (<xref ref-type="bibr" rid="R38">Magar and Schwartz, 2022</xref>). Test data analysed by the CNN is also evaluated using CODEML, and results are compared. We chose CODEML for benchmarking OmegaAI’s machine learning results as it implements a state-of-the-art maximum likelihood method for detecting selection.</p><p id="P23">Deep learning methods are often criticised for being “black boxes”. In order to gain insight into what the OmegaAI CNN learns, we visualise saliency maps (<xref ref-type="bibr" rid="R59">Simonyan, Vedaldi, and Zisserman, 2014</xref>) that indicate which parts of each input dataset, and in particular which alignment columns, are contributing to the prediction obtained for that dataset. We compare these findings to the sitewise inference of positive selection offered by CODEML.</p><p id="P24">An overview of the method workflow is given in <xref ref-type="fig" rid="F2">Figure 2</xref> and technical details regarding all methods are discussed in the <xref ref-type="sec" rid="S21">Methods</xref> section.</p></sec></sec><sec id="S10" sec-type="results"><label>3</label><title>Results</title><sec id="S11"><label>3.1</label><title>OmegaAI performs well with baseline parameters</title><p id="P25">We choose the symmetric 8-taxon tree to be the known, true phylogeny (<xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 1</xref>) along which sequence evolution is simulated. Our baseline parameter set describes a tree scaling of 0.2 substitutions per site per branch, with an indel rate of 0.1 per substitution. This choice serves as our reference point for analysis, chosen based on literature and empirical data (<xref ref-type="bibr" rid="R47">Ogurtsov, Sunyaev, and Kondrashov, 2004</xref>; <xref ref-type="bibr" rid="R12">Fletcher and Yang, 2010</xref>; <xref ref-type="bibr" rid="R21">Jordan and Goldman, 2012</xref>), and due to the fact that these parameters produce realistic alignments (<xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 2 and Supplementary Fig. 3</xref>) with the divergence being high enough that we obtain a reasonable signal of positive selection when it is present, without encountering the excessive information loss and alignment error that we see with higher divergences. Along this phylogeny, data is simulated with and without selection with parameters as described in detail in Methods. OmegaAI is trained using 1,000,000 MSAs, with performance of trained models assessed using 2,000 independently simulated MSAs (see New Approaches and Methods for details).</p><p id="P26">To obtain binary classifications from real-valued data, it is necessary to determine thresholds that delineate the categorical boundaries between positive selection and no selection. These decision thresholds influence the trade-off between precision and recall, allowing for optimisation of a preferred metric. In statistical methods like CODEML, a significance level is set to control the Type I error rate. For instance, a 95% confidence level is often chosen to detect positive selection, ensuring a false positive rate of 5% based on true alignments (<xref ref-type="bibr" rid="R74">Wong, Yang, et al., 2004</xref>) (see <xref ref-type="sec" rid="S21">Methods</xref>). This approach provides a statistically grounded framework for decision-making.</p><p id="P27">In contrast, OmegaAI lacks an intrinsic statistical framework to establish a decision threshold. Therefore, we have selected 0.5 as a natural threshold, representing the midpoint of the output value range between 0 and 1. Unlike CODEML, which aims to control its error rate directly through its significance level, OmegaAI’s performance is evaluated using various metrics, and its error rates are empirically determined. The flexibility of adjusting the decision threshold in OmegaAI allows for optimisation based on specific application needs, but it also requires thorough benchmarking to identify the most appropriate threshold. Some applications may prioritise specificity over recall or vice versa.</p><p id="P28">To assess and compare the efficacy of detecting positive selection by OmegaAI and CODEML, in <xref ref-type="fig" rid="F3">Figure 3</xref> we show Receiver Operating Characteristic (ROC) curves which illustrate the trade-off between true positive and false positive rates when varying the methods’ decision thresholds between 0 and 1. Precision-recall curves are also presented, showing the trade-off between these metrics as decision thresholds are varied. For both metrics, the total area under the curve (AUC) provides a comprehensive evaluation of the method’s performance, encompassing all potential classification thresholds. <xref ref-type="fig" rid="F3">Figure 3 A–B</xref> compares the performance of OmegaAI and CODEML when evaluated with test data aligned using Clustal. The OmegaAI curve sits above the CODEML curve in both, showing that the performance of OmegaAI dominates that of CODEML over the whole threshold range. The higher AUC value for OmegaAI in both the ROC and precision-recall plots provides a summary of this superior performance. Dots on the curves represent results obtained at typical thresholds. For example, comparing OmegaAI with a decision threshold of 0.5 with the 0.95 p-value threshold commonly used in CODEML, we see that OmegaAI has a higher true positive rate (TPR) and lower false positive rate (FPR) (<xref ref-type="fig" rid="F3">Figure 3 A</xref>) and higher precision and recall (<xref ref-type="fig" rid="F3">Figure 3 B</xref>) than CODEML.</p><p id="P29">In <xref ref-type="fig" rid="F3">Figure 3 C–D</xref> we repeat this analysis using test datasets aligned with three alternative aligners, again evaluating each set with both OmegaAI and CODEML. This approach allows us to assess the performance of the selection detection methods across various alignment algorithms and qualities, and to evaluate OmegaAI’s ability to generalise to MSAs aligned by algorithms it did not encounter during training. OmegaAI consistently outperforms CODEML based on both the ROC and precision-recall metrics, for every aligner (solid lines for OmegaAI always above corresponding dashed lines for CODEML, with each colour pair representing analysis with a different aligner), as summarised by the AUC values.</p><p id="P30">The results show that OmegaAI performs similarly on PRANKc, PRANKaa and MAFFT alignments, whilst exhibiting lowest performance for Clustal alignments. Whilst the model was trained exclusively on Clustal alignments, the alignments from the other three aligners tend to be of significantly higher quality (as shown in <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 2 and Supplementary Fig. 3</xref>). This suggests that alignment quality is more important for accurate prediction by our method than the CNN model learning alignment error patterns specific to any one particular alignment algorithm. Further, it appears OmegaAI’s learned ability to account for alignment error applies across alignment types.</p><p id="P31">For CODEML, there is a clearer ordering of performance by alignment type: PRANKc giving best results, followed by PRANKaa, MAFFT and then Clustal proving most difficult. This outcome agrees with the observed alignment quality rankings in <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 2 and Supplementary Fig. 3</xref>. Since CODEML cannot compare data across alignment columns, more alignment errors will lead to greater information loss regarding the occurrence of selection. In particular, PRANKc and PRANKaa, as evolutionary-aware aligners, produce higher quality alignments (<xref ref-type="bibr" rid="R21">Jordan and Goldman, 2012</xref>) resulting in more reliable CODEML inferences.</p></sec><sec id="S12"><label>3.2</label><title>OmegaAI performs well with increasing divergence and higher indel rates</title><p id="P32">We repeated this analysis across different scalings of the simulation tree, representing different divergence levels among the taxa. This approach tests simulated sequences of genes evolving over a range of rates or times. Greater divergence reduces sequence similarity and can make alignment and accurate selection signal extraction more challenging due to the increased noise.</p><p id="P33">We present 10 OmegaAI models, each trained on one of 10 datasets. The topology of the tree underlying sequence evolution simulation is unchanged, but each training dataset has incrementally increasing divergence for the branches (0.1, 0.2, …, 1.0 substitutions per codon site per branch).</p><p id="P34">Likewise, we employ 10 test datasets, each exhibiting the same incremental levels of divergence to test each corresponding OmegaAI model.</p><p id="P35">We compare OmegaAI and CODEML across increasing divergences in <xref ref-type="fig" rid="F4">Figure 4</xref>. We show results for test data aligned with the four different aligners, as previously. Both OmegaAI and CODEML show superior predictive power for detecting positive selection at lower divergences compared to higher ones. However, OmegaAI achieves higher accuracy and TPR compared to CODEML, whilst maintaining low FPR, across most of the aligner-divergence combinations tested.</p><p id="P36">CODEML’s predictive power falls with increasing divergence, as evidenced by TPR and FPR that converge towards zero. OmegaAI detects selection with TPR above zero and generally produces higher accuracies, but makes errors as evidenced by having slightly higher FPR at higher divergences, with PRANKc results being an exception of considerably higher FPR. Since OmegaAI’s AUC values are well above 0.5, we could choose to adjust decision thresholds to reduce FPRs whilst sacrificing some power, in situations where precision is prioritised over recall.</p><p id="P37">Evaluating ROC and precision-recall AUC data shows a method’s performance independent of the choice of any particular threshold. OmegaAI produces AUCs greater than CODEML across the range of divergence and aligners that we test. These results indicate that OmegaAI’s performance can be very good, comparable and frequently better than state-of-the-art maximum likelihood methods in CODEML, in various use-cases, whether emphasizing maximal true positive recovery, high precision, or minimizing false positive rates, contingent upon selecting an appropriate threshold.</p><p id="P38">Where OmegaAI has been trained using Clustal alignments, we hypothesise that the model is able to account for some amount of misalignment, and perhaps even draw information from this (see <xref ref-type="sec" rid="S20">Discussion</xref>). Despite not being trained on the other aligners, OmegaAI remains able to achieve higher accuracy when evaluating these alignments than evaluation of Clustal alignments. In contrast, CODEML performs particularly poorly on Clustal alignments. This is likely due to the relatively high amount of misalignment that one can expect from Clustal alignments (<xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 2 and Supplementary Fig. 3</xref>) driving up its FPR.</p><p id="P39">We note the unusual behaviour at divergence 0.8, where there is a dip in OmegAI’s TPR across aligners, interrupting the otherwise smooth trend. We suspect that this is an artefact arising from an unidentified feature of the Clustal alignment algorithm, since only Clustal alignments are used to train the CNN but the artefact is observed when analysing data aligned by all aligners studied. This theory is corroborated by <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 4 and Supplementary Fig. 5</xref>, since we do not witness this behaviour on these models where OmegaAI has been trained on other alignment types.</p><p id="P40">Reduced alignment quality makes the challenge of detecting positive selection harder (<xref ref-type="bibr" rid="R21">Jordan and Goldman, 2012</xref>), especially for models that do not account for alignment error. Over the range that we consider, increasing divergence between sequences tends to increase the difficulty of the alignment problem and thus the selection inference problem. Another challenge to aligners that we consider is higher rates of indels. We repeated the same analysis as in <xref ref-type="fig" rid="F4">Figure 4</xref> but increased the in-del rates from 0.1 to 0.2 and 0.3. Overall performance decreases for both OmegaAI and CODEML, as expected, with more variable trends (<xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 6 and Supplementary Fig. 7</xref>). However, we again see that OmegaAI generally outperforms CODEML across the binary classifier metrics for various alignment algorithms and tree divergence scalings at these higher indel rates.</p></sec><sec id="S13"><label>3.3</label><title>Training using PRANKc alignments</title><p id="P41">OmegaAI models presented so far have been trained on alignments inferred by the fast, widely used Clustal aligner. However, as noted above, Clustal can be relatively error-prone compared to other aligners whereas PRANKc is a highly accurate alignment tool which integrates phylogenetic construction and information into its alignment algorithm (<xref ref-type="bibr" rid="R36">Löytynoja and Goldman, 2008</xref>) but is orders of magnitude slower than Clustal. By training OmegaAI models with PRANKc alignments and comparing their performance with OmegaAI models trained with Clustal alignments, we can explore the effects of training with alignments produced from different algorithms and assess how this affects performance, including generalisability to other alignment types. With PRANKc’s higher accuracy coming at the cost of orders of magnitude longer runtimes than Clustal, its use for model training becomes very costly. Nevertheless, extensive resources were expended to test its value.</p><p id="P42">For clarity, in this section we refer to the models trained on Clustal or PRANKc alignments as OmegaAI(Clustal) and OmegaAI(PRANKc), respectively. Training sequences for both were simulated under baseline conditions (divergence 0.2, indel rate 0.1). Both models were tested with the standard test dataset used previously, where 2,000 MSAs are realigned by each of our considered aligners: Clustal, MAFFT, PRANKaa and PRANKc. ROC and precision-recall curves from this analysis are shown in <xref ref-type="fig" rid="F5">Figure 5</xref> (with cross-divergence/aligner results in <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 4</xref>, and for comparison to CODEML see <xref ref-type="fig" rid="F3">Figure 3</xref> and <xref ref-type="fig" rid="F4">Figure 4</xref>). The curves, and in particular the AUC values, show that both OmegaAI(Clustal) and OmegaAI(PRANKc) perform well, and to a similar level, when tested on MAFFT, PRANKaa and PRANKc alignments. However, OmegaAI(Clustal) achieves notably higher AUC values for Clustal alignments than OmegaAI(PRANKc).</p><p id="P43">These findings indicate that precise alignment is not imperative during training to enhance our models’ predictive capacities, and that training on error-prone alignments makes the model more robust to alignment errors. However, during testing, alignment accuracy does significantly influence predictive accuracy, with superior results typically associated with higher-quality alignments, irrespective of training data alignment accuracy. This is evidenced by baseline tests performed on higher quality alignments producing higher AUC values than for Clustal alignments, which are less accurate (see above). <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 4</xref> illustrates a similar trend across all divergence levels studied, with OmegaAI(Clustal) generally performing better on Clustal alignments than OmegaAI(PRANKc). While OmegaAI(Clustal) excels with Clustal alignments, OmegaAI(PRANKc) occasionally outperforms OmegaAI(Clustal) on PRANKc alignments for certain divergences, with comparable performance observed for MAFFT and PRANKaa alignments when tested with the two models. These results suggest that OmegaAI models may benefit from more exposure to alignment error during training.</p></sec><sec id="S14"><label>3.4</label><title>Benchmarking with the true alignment</title><p id="P44">CODEML applied to true, simulated alignments represents a gold-standard for positive selection inference because the true alignment contains no errors, artefacts or other features that CODEML’s models do not account for. The true alignment and the simulation tree should permit a highly accurate inference of the evolutionary events leading to a given set of protein coding sequences. Results from the following analysis are likely unattainable in any real-life case, since data will never conform exactly to these generative models or be perfectly aligned. However, they can help us better understand what factors contribute to the performance of different methods.</p><p id="P45">For comparison with CODEML’s inference of positive selection on true alignments we trained a new OmegaAI model solely on true simulated alignments across our divergence set. We also took the models previously trained on Clustal alignments and tested all of these on true alignments. In <xref ref-type="fig" rid="F6">Figure 6</xref> we show the ROC and precision-recall results from baseline parameter simulations (results for other divergence levels are shown in <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 5</xref>). All three methods perform comparably well on the true alignment baseline test dataset, with very similar AUCs for both ROC and precision-recall. CODEML produces slightly better TPR at lower FPR than both OmegaAI methods, while the OmegaAI model trained on true alignments achieves the highest ROC AUC. These results further support the idea that the superior performance of OmegaAI in previous analyses is attributable to its ability to account for alignment errors.</p><p id="P46">So far when evaluating the performance of OmegaAI in terms of accuracy, TPR and FPR, we have used a decision threshold of 0.5 as a natural choice to make inferences on weights between 0 and 1 being output by OmegaAI. This choice leads to desirable results in some cases, but could be adjusted for others. For example, in <xref ref-type="fig" rid="F4">Figure 4</xref> we see that when OmegaAI evaluates Clustal, MAFFT or PRANKaa alignments, the accuracy is high whilst maintaining low FPR, compared to CODEML, for most divergences. However, whilst the accuracy of results from OmegaAI applied to PRANKc alignments are some of the highest, so too are the FPRs. We observe a similar trend when OmegaAI (trained on either Clustal or true alignments) evaluates true alignments (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 5</xref>). From these observations, it appears that higher quality MSAs may benefit from being evaluated by OmegaAI using a lower decision threshold.</p></sec><sec id="S15"><label>3.5</label><title>Generalisability</title><p id="P47">We have shown that OmegaAI models trained on Clustal alignments generalise effectively to the other aligners we have tested. However, up to this point our CNNs have been trained and tested on specific trees and divergence levels. In real-world scenarios, the data will be more varied.</p><p id="P48">AI methods can be difficult to generalise without very large and diverse training data (<xref ref-type="bibr" rid="R9">Eche et al., 2021</xref>). Whilst there exist strategies to mitigate this, such as fine-tuning and domain adaptation (<xref ref-type="bibr" rid="R13">Ganin and Lempitsky, 2015</xref>), there is a common struggle between creating models which perform well at a specific task but fail to generalise outside of it, versus models that offer wide general use but whose performance falls short for any specific subgroup (<xref ref-type="bibr" rid="R9">Eche et al., 2021</xref>; <xref ref-type="bibr" rid="R18">Huisman and Hannink, 2023</xref>). We found it valuable to consider whether OmegaAI as already studied was able to cope with a greater diversity of use cases. Here, we investigate its performance for different divergences of the 8-taxon simulation tree and in the absence of information of the underlying tree topology.</p><sec id="S16"><title>Training OmegaAI with reduced phylogenetic information</title><p id="P49">So far, all simulated MSAs have had the same row ordering relative to their underlying tree. As a result, the CNN is injected with learnable information about the relative relatedness of the sequences in the MSA during training (in addition to what it can learn from sequence similarity, indel patterns, and the consistent use of the same simulation tree). This structure is also predictable at test time, since the same tree and row ordering is used for the test data. Using a consistent tree and alignment row ordering during training and testing is akin to how we give CODEML a pre-estimated tree topology while testing for selection. We sought to test how well OmegaAI would perform with reduced information regarding the underlying simulation tree. This was achieved by randomly shuffling the row ordering of all MSAs before use.</p><p id="P50">Results for OmegaAI trained and tested on baseline parameters, with row shuffling, referred to here as OmegaAI(shuffled), are shown in <xref ref-type="fig" rid="F7">Figure 7</xref> (with cross-divergence/aligner results in <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 8</xref>). Removing any information about an underlying tree during training negatively impacted OmegaAI(shuffled)’s prediction power, reflected by the lower ROC and precision-recall curves and reduced AUC values for the OmegaAI model trained on shuffled data. However, OmegaAI(shuffled) still outperforms CODEML by these same metrics. For some mid-range divergences and aligners the accuracy achieved by OmegaAI(shuffled) is similar to that of OmegaAI when given consistent row ordering at training and test time (<xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 8</xref>). However, this OmegaAI(shuffled) result, whilst exhibiting a higher TPR, comes with a dramatically increased FPR for all divergences, especially for tests performed on PRANKc alignments (a case where adjusting the decision threshold to lower the FPR and sacrifice some power could be valuable). Overall, these results suggest that the tree information is important to OmegaAI’s ability to predict and that indeed the model learns some representation of the underlying tree topology when MSA row ordering contains information. Further, it is evident that OmegaAI models, as currently trained, would likely not generalise well to a range of tree topologies to a point where they would compete with the models trained for a specific topology.</p></sec><sec id="S17"><title>Training and testing across divergence levels</title><p id="P51">We also tested generalisability with respect to the divergence parameter of the simulation tree. By keeping the simulation tree and row ordering constant, the model can learn a representation of the topology of the sequences’ evolutionary relationships, but the rate of evolution will vary: challenging the model to deal with different levels of signal from mutations and noise from misalignment. We describe two experiments that tested this.</p><p id="P52">We tested the OmegaAI model trained on the central value (0.5) of our divergence range on data from all divergences in our set, with other parameters held constant. The results are shown in <xref ref-type="fig" rid="F8">Figure 8</xref> (left). In terms of accuracy, OmegaAI produces poor scores when tested on trees other than those with branch lengths of 0.5 (the value for which it was trained). There is a tendency in these cases for OmegaAI to infer all datasets with a lower divergence than it was trained with to have undergone positive selection, and all datasets with higher divergence to have undergone neutral/purifying selection. Comparison with an analogous approach in CODEML is shown in <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 9</xref>, where CODEML also performs poorly when the assumed divergence level differs from what is being tested. However, ROC AUC values from this analysis indicate that the OmegaAI model trained on 0.5 divergence trees still effectively ranks instances of positive and negative selection for some divergences around 0.5, but the default threshold is not optimal for classifying these other divergence sets.</p><p id="P53">Next we created a model exposed to data from all of the 10 divergence levels in our divergence set during training, to see if this increased OmegaAI’s ability to generalise to other divergences without having to optimise for different thresholds in different testing scenarios. The training dataset was kept the same size (1,000,000 trees), but now consisting of equal numbers from each of the 10 divergences. This creates a single OmegaAI model which has learnt from phylogenies with a range of divergence levels, not just a single divergence level as with all models previously described. We tested this model on each test dataset, each simulated under a single divergence. Results are shown in <xref ref-type="fig" rid="F8">Figure 8</xref> (right).</p><p id="P54">The model exhibits similar behaviour to our previous experiment where the model trained on 0.5 divergence trees was tested across the range of divergence test sets. The difference in this case is that this new model shows reasonable performance at divergences 0.7 and 0.8, indicated by accuracy values similar to when an OmegaAI model is trained solely on trees of a single divergence level and tested on trees of the same divergence. Compared to the model trained on 0.5 divergence trees, the mixed divergence model demonstrates broader applicability across the divergence set, as indicated by its higher accuracy and especially its AUC values. This suggests that the mixed divergence OmegaAI model has learned to detect positive selection across a range of divergences, with its best performance centered at the 0.7 level, in contrast to the 0.5 model’s much narrower effective range. The TPR and AUC values together indicate that the decision threshold is too low for lower divergences and too high for higher divergences and it happens to be the case that the default threshold of 0.5 is optimal around divergences 0.7 and 0.8. More results from this analysis are shown in <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 10</xref>.</p><p id="P55">In these two analyses, where accuracy is low but the ROC AUC value is good, it is evident that there is a thresholding issue. While the need to optimise thresholds in each testing scenario indicates that the OmegaAI models are not immediately generalisable, it also demonstrates that the information learned by OmegaAI in one scenario is transferable to others.</p></sec></sec><sec id="S18"><label>3.6</label><title>Alignment saliency: visualising and quantifying what OmegaAI learns</title><p id="P56">Saliency in the context of CNNs applied to image recognition tasks refers to the identification and visualisation of the most relevant pixels of an input image that contribute to the model’s prediction (<xref ref-type="bibr" rid="R59">Simonyan, Vedaldi, and Zisserman, 2014</xref>). It has been used as a strategy to aid understanding of what a neural network learns. In our case, where the input is an MSA instead of an image, a saliency map highlights the alignment positions that were most influential in OmegaAI’s prediction for a given MSA. Saliency is computed by taking the gradient of the output with respect to the input: the magnitude of the gradient shows which characters need the least change to most affect the output, and these can be visualized in a saliency map where brighter regions highlight key positions. For more details, see Methods.</p><p id="P57">In <xref ref-type="fig" rid="F9">Figure 9</xref> we present an example saliency map for an MSA with sequences simulated under baseline conditions with positive selection and aligned using Clustal. In such an alignment, each site evolves under purifying, neutral or positive selection, with most sites evolving under purifying or neutral selection (see <xref ref-type="sec" rid="S21">Methods</xref> for details). Alignment sites are coloured according to their selection class in <xref ref-type="fig" rid="F9">Figure 9</xref>, but note that more than one colour can occur in an alignment column due to misalignment. In this example, sites evolving under positive selection have <italic>ω</italic> = 4.28. <xref ref-type="fig" rid="F9">Figure 9</xref> also shows CODEML’s sitewise predictions of positive selection, inferred using a Bayes empirical Bayes (BEB) approach (see <xref ref-type="sec" rid="S21">Methods</xref> for details). The figure reveals that high saliency tends to correlate with sites evolving under positive selection, indicating that the OmegaAI model finds these sites most informative to its predictions.</p><p id="P58">The correlation between sites under positive selection and high saliency can be leveraged as a proxy for sitewise detection of positive selection by OmegaAI, analogous to the BEB approach used by CODEML. Assuming this proxy, we can contrast the differences in sitewise predictions between CODEML and OmegaAI. Whilst the two methods tend to agree overall, it is informative to look at some of the details. In <xref ref-type="fig" rid="F9">Figure 9</xref> we highlight two examples: firstly, a site (labelled TP-TP) where CODEML and OmegaAI both correctly infer a site with <italic>ω</italic> &gt; 1, despite some misalignment in the area; and secondly, a site (labelled FN-TP) where the OmegaAI saliency correctly suggests positive selection that CODEML fails to detect. <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 11</xref> provides an extended analysis of classification instances where CODEML and OmegaAI agree and disagree for this saliency map. We note the low saliency in the far right of the MSA and postulate that this is a result of zero-padding during training (see <xref ref-type="sec" rid="S20">Discussion</xref>).</p></sec><sec id="S19"><label>3.7</label><title>Computational resource measurements</title><p id="P59">A drawback of likelihood-based methods is the computational time required for parameter estimation. This becomes increasingly significant when scaling up the analysis to many genes or trees. Here we show that machine learning approaches can offer an alternative that is time and compute efficient.</p><p id="P60">In <xref ref-type="fig" rid="F10">Figure 10</xref>, we present the computational time and memory usage for evaluating a standard test set of 2,000 alignments using CODEML and OmegaAI with baseline parameters. OmegaAI is evaluated in two scenarios: one using only CPU resources, and another utilising a single GPU in addition to a CPU.</p><p id="P61">OmegaAI exhibits better speed, being four orders of magnitude faster with an average processing time of approximately 32 seconds to evaluate the 2,000 alignments when given access to the parallel processing capability of a GPU. In contrast, CODEML takes around 65 hours to evaluate these alignments when executed serially on a single CPU. When OmegaAI is restricted to a single CPU its processing time increases somewhat, to approximately 137 seconds in this example.</p><p id="P62">While the resource demands during training OmegaAI models (<xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 12</xref>) are substantial, the advantage of OmegaAI during test time is evident. This efficiency comes with a trade-off: OmegaAI requires more memory than CODEML, although the amount remains well within the capacity of modern personal computers. However, since OmegaAI lacks generalisability over tree shapes and divergences and would require retraining or at least fine-tuning for specific use-case utility, OmegaAI does not yet realise large advantages over CODEML in terms of resources. A possible exception to this is genome-scale detection of positive selection (see <xref ref-type="sec" rid="S20">Discussion</xref>).</p></sec></sec><sec id="S20" sec-type="discussion"><label>4</label><title>Discussion</title><p id="P63">The OmegaAI CNN, trained to perform positive selection detection on a set of aligned, interspecific, protein-coding nucleotide sequences, appears to outperform the state-of-the-art maximum likelihood methods, as embodied in the CODEML program in the PAML package (<xref ref-type="bibr" rid="R76">Yang, 2007</xref>), when trained for a specific phylogenetic scenario. This is particularly clear when dealing with high divergences and indel rates.</p><p id="P64">Using Clustal alignments to train OmegaAI is time efficient and, because Clustal alignments are more error-prone, potentially provides a richer resource from which the CNN model can learn to account for alignment error. Once an OmegaAI model has been trained, it provides a significant time advantage for dataset evaluation compared to maximum likelihood methods. These are promising results, suggesting that machine learning approaches could be valuable for detecting selection in multiple sequence alignment datasets. It is noteworthy that using a quicker, albeit less accurate, aligner during training for OmegaAI does not seem to present significant drawbacks. In fact, it substantially reduces training data generation time and the resulting OmegaAI model exhibits good performance even when evaluating lower-quality alignments, without a considerable drop in performance with higher-quality alignments. Where OmegaAI has been trained on MSAs containing alignment errors, we postulate that it has been able to learn patterns associated with positive selection, including selection signals made noisy by alignment error, effectively allowing OmegaAI to learn to accommodate alignment errors.</p><p id="P65">Although OmegaAI is not yet generalised to deal with more varied scenarios, particularly with respect to the number and evolutionary relationships of input sequences, this work serves as a proof of concept. Through a deliberately constrained version of the positive selection inference problem, we have begun to explore the capabilities of AI methods applied to this task. At present, the efficacy of OmegaAI relies on retraining for parameters specific to a given problem. For example, a feature already available in CODEML is that it can estimate divergence level during selection detection. Our results suggest that for OmegaAI it may be necessary to train new models using data simulated under a divergence level close to that of the target data, which in of itself requires approximate prior knowledge of the divergence level of the test data, or using more extensive data drawn from a range of divergence levels. Another mitigating strategy could be to fine-tune existing models instead of fully retraining, which appears to be a promising strategy based on the results from the mixed divergence OmegaAI model.</p><p id="P66">The prospects for creating a generalised AI model capable of high performance over many different problems look promising and could conceivably be aided by an attention-based architecture or by increasing the size of the model. Attention-based architectures have proven to be promising in a range of tasks, including applications to MSAs (<xref ref-type="bibr" rid="R52">Rao et al., 2021</xref>; <xref ref-type="bibr" rid="R22">Jumper et al., 2021</xref>). Larger machine learning models — those with more parameters and deeper networks — tend to perform better in terms of generalisation because they have more capacity to learn complex patterns and representations from data, allowing them to better capture the underlying distribution and make accurate predictions on new, unseen examples.</p><p id="P67">One use-case for which OmegaAI suggests immediate utility may be genome-wide detection of positive selection. In these investigations, typically thousands of genes are tested for positive selection. For example, Lee et al. (2017) query over 1,000 genes in nine primates using likelihood-based methods. The underlying species tree is constant and mammalian gene evolutionary rates tend to be approximately normally distributed with a similar range to the divergence levels that we tested (<xref ref-type="bibr" rid="R29">Kumar and Subramanian, 2002</xref>), and so a small number of OmegaAI models could be trained to cover this range. At this scale, even including the time it takes to simulate training data and train the model(s), OmegaAI could present a time-efficient solution that is potentially able to provide more accurate inferences, according to our results.</p><p id="P68">Our results show that OmegaAI retains some predictive power with difficult datasets, where the likelihood methods that we have tested perform poorly. In particular, it shows promise for noisy data that result from high divergence and indel rates. This too is an encouraging potential use-case for OmegaAI, where current statistical models are likely to be of little use because of their failure to account for alignment errors.</p><p id="P69">Another interesting avenue to investigate would be alternative evolutionary models for simulating data. The simulation model should reflect biological reality in order for OmegaAI models to perform well on empirical data. The space of models available for simulating sequence evolution is larger and more complex than those used for maximum likelihood optimisation (<xref ref-type="bibr" rid="R42">McGuire, Denham, and Balding, 2001</xref>) and increasingly complex models can better capture evolutionary mechanisms and emergent properties (<xref ref-type="bibr" rid="R8">De Maio et al., 2013</xref>; <xref ref-type="bibr" rid="R50">Perron et al., 2019</xref>; <xref ref-type="bibr" rid="R37">Lucaci et al., 2021</xref>). Since our AI model is trained on simulated sequences, this allows for training on sequences generated under complex and realistic models, even those intractable for likelihood inference. This could lead to a machine learning model that makes more accurate inferences on empirical data. For example, we could simulate the phenomenon of multiple nucleotide substitutions (knowledge of which has been shown to reduce the rate of false inference of positive selection: <xref ref-type="bibr" rid="R8">De Maio et al., 2013</xref>), take into account the growing evidence for the non-neutrality of synonymous substitutions (<xref ref-type="bibr" rid="R48">Parmley, Chamary, and Hurst, 2006</xref>; <xref ref-type="bibr" rid="R27">Kubatko et al., 2016</xref>; <xref ref-type="bibr" rid="R55">Shen et al., 2022</xref>; <xref ref-type="bibr" rid="R54">Rosenberg, Marx, and Bronstein, 2022</xref>), and model synonymous rate variation (<xref ref-type="bibr" rid="R72">Wisotsky et al., 2020</xref>) and interlocus gene conversion (<xref ref-type="bibr" rid="R20">Ji, Griffing, and Thorne, 2016</xref>). Machine learning approaches are often preferable to approximate Bayesian computing (ABC) — an alternative to likelihood-based estimation for intractable models — because they automate the data-to-parameter mapping, eliminating the need for selecting summary statistics and comparison metrics. Additionally, machine learning methods are generally more scalable, particularly for large datasets (<xref ref-type="bibr" rid="R33">Lenzi et al., 2023</xref>).</p><p id="P70">Finally, it is a natural extension to imagine a machine learning model whereby we try to infer sitewise <bold><italic>dN/dS</italic></bold>, a feature already available in the likelihood framework, for example as implemented within CODEML through empirical Bayes approaches or in SLR (<xref ref-type="bibr" rid="R41">Massingham and Goldman, 2005</xref>) through maximum likelihood. In the CNN context, this problem is akin to a pixel-classification problem already attempted in <xref ref-type="bibr" rid="R30">Längkvist et al. (2016)</xref>, or could be approached using an attention-based architecture, similar to <xref ref-type="bibr" rid="R45">Nesterenko et al. (2024)</xref>. To simultaneously achieve sitewise detection of positive selection and sequence length generalisability, however, our saliency maps have uncovered that extra care is needed beyond crude zero-padding (see <xref ref-type="sec" rid="S21">Methods</xref>). We regularly observe weaker saliency towards the far right end of MSAs, likely attributable to zero-padding that was necessary to accommodate variable lengths in sequences during training. The CNN, regularly encountering zeros in these positions across numerous training datasets, appears to have learned to prioritise areas most frequently containing relevant information. There is enough signal in higher saliency regions that binary classification performance has not been strongly impacted, but in sitewise inference we would require uniformly high signal across the whole length of an MSA. A sliding window approach, although potentially less efficient, could be used to address this problem by querying sections of the MSA with some overlap to capture spreading of horizontal information from misalignment.</p><p id="P71">In conclusion, machine learning methods applied to detecting interspecific positive selection have demonstrated superior accuracy and time efficiency compared to likelihood methods, when trained appropriately for a specific problem. In this study, we concentrated on training and testing models for particular use-cases, observing some limitations in generalisation outside of the trained scenarios. The application of current and future machine learning techniques, which are more sophisticated, is expected to enhance generalisability and thus increase their utility for a broader range of phylogenetic topologies and questions. Although machine learning methods still face “black box” issues, we have begun to address this through saliency map exploration. Whilst likelihood methods remain valuable due to their strong statistical foundations and extensive usage, the adoption of learning-based methods is likely to increase due to their growing transparency, scope, and sophistication.</p></sec><sec id="S21" sec-type="methods"><label>5</label><title>Methods</title><sec id="S22"><label>5.1</label><title>Simulating multiple sequence alignments</title><p id="P72">Following earlier studies which investigated the effects of insertions, deletions, and alignment errors on tests of positive selection (<xref ref-type="bibr" rid="R12">Fletcher and Yang, 2010</xref>; <xref ref-type="bibr" rid="R21">Jordan and Goldman, 2012</xref>), we simulate codon sequences under artificial trees for a range of testing scenarios using INDELible (<xref ref-type="bibr" rid="R11">Fletcher and Yang, 2009</xref>). This tool, given a variety of user-defined parameters, outputs both unaligned sequences and the true alignment of those sequences. Alignments of these simulated sequences act as our training, validation, and test datasets for our CNN and as our test sets for evaluation using the likelihood-based methods for detecting selection implemented in CODEML from the Phylogenetic Analysis by Maximum Likelihood (PAML; <xref ref-type="bibr" rid="R76">Yang, 2007</xref>) package. Unlike traditional classification tasks in deep learning, which are limited by the amount of available training data, we are able to use simulations to generate as many pairs of alignments and true class labels (selection or no selection) as desired for each testing scenario.</p><p id="P73">Considering a constrained version of the problem, we used an 8-taxon symmetric topology rooted at its midpoint (similar to those used in <xref ref-type="bibr" rid="R12">Fletcher and Yang, 2010</xref>; <xref ref-type="bibr" rid="R21">Jordan and Goldman, 2012</xref>). The intention is to mimic the evolution of a protein-coding gene sequence along an idealised tree under varying selection pressures. Some parameters, and particularly sampling distribution parameters and ranges, are fixed across all simulations. The sequence length at the root of the tree is sampled from a gamma distribution with parameters <italic>k</italic> = 4.2 and <italic>θ</italic> = 85, which approximates bacterial gene length distributions as outlined in <xref ref-type="bibr" rid="R61">Skovgaard et al. (2001)</xref>. To prevent sampling of extreme lengths, we impose minimum and maximum root sequence lengths of 100 and 600 codons, respectively. Additionally, the ratio of transitions to transversions, <italic>κ</italic>, is sampled from <bold><italic>U</italic></bold>(2, 3), the continuous uniform distribution with a minumum value of 2 and a maximum value of 3. This range is close to that which is observed in empirical studies (<xref ref-type="bibr" rid="R69">Wang et al., 2015</xref>).</p><p id="P74">Sequences were simulated using a version of the codon model of evolution with site variation in selective pressure from <xref ref-type="bibr" rid="R46">Nielsen and Yang (1998)</xref>. The model assumes that each codon alignment position is a member of a discrete site class <italic>S</italic>. Each site class is assigned a different level of selective pressure as measured by the nonsynonymous to synonymous substitution ratio, <italic>ω</italic>, with selective pressure being constant across all branches of the tree at a considered codon position. We assume three site classes <italic>S</italic> ∈ <italic>S</italic><sub>0</sub>, <italic>S</italic><sub>1</sub>, <italic>S</italic><sub>2</sub> occurring in proportions <italic>p</italic><sub>0</sub>, <italic>p</italic><sub>1</sub>, <italic>p</italic><sub>2</sub>, respectively, each associated with different <italic>ω</italic> values. The value of <italic>ω</italic><sub>0</sub> for <italic>S</italic><sub>0</sub> is chosen from <italic>U</italic>(0.1, 0.5), and <italic>ω</italic><sub>1</sub> for <italic>S</italic><sub>1</sub> is chosen from <italic>U</italic>(0.5, 0.9). Sites in <italic>S</italic><sub>0</sub> are under strong purifying selection and those in <italic>S</italic><sub>1</sub> are under slight purifying selection. The value of <italic>ω</italic><sub>2</sub> for <italic>S</italic><sub>2</sub> is chosen differently depending on the type of gene we are simulating; genes that are predominantly undergoing purifying selection (<italic>ω</italic><sub>2</sub> ∈ <bold><italic>U</italic></bold>(0.9,1.0)), genes containing strictly neutral sites (<italic>ω</italic><sub>2</sub> = 1), and genes where some sites are evolving under positive selection (<italic>ω</italic><sub>2</sub> ∈ <bold><italic>U</italic></bold>(1.5, 5)). For simplicity, in this study we refer to these three types of simulated genes as purifying, neutral and positive selection genes, respectively. The model used to simulate genes under purifying and neutral evolution is similar to the models M1a and M7 implemented in CODEML, while the model used to simulate genes under positive selection is similar to M2a and M8 (<xref ref-type="bibr" rid="R46">Nielsen and Yang, 1998</xref>; <xref ref-type="bibr" rid="R80">Yang, Nielsen, et al., 2000</xref>; <xref ref-type="bibr" rid="R81">Yang, Wong, and Nielsen, 2005</xref>).</p><p id="P75">For every gene, we sample one value each for <italic>ω</italic><sub>0</sub>, <italic>ω</italic><sub>1</sub>, and <italic>ω</italic><sub>2</sub> from their corresponding distributions. This ensures that for each gene, every codon site will evolve under one of three <italic>ω</italic> values and sites of the same site class will experience identical selection pressure. For clarity, the distributions of <italic>ω</italic> are shown for our baseline parameters for each type of selection in <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 13</xref>. The proportion of class S<sub>0</sub> sites, <bold><italic>p</italic></bold><sub>0</sub>, was chosen independently for each gene from <bold><italic>U</italic></bold>(0.5, 0.8), and <bold><italic>p</italic></bold><sub>2</sub> was sampled from U(0.01, 0.1); p<sub>1</sub> is then set as p<sub>1</sub> = 1 – p<sub>0</sub> – p<sub>2</sub>.</p><p id="P76">The branch lengths of the ultrametric simulation tree are all equal, varying between 0.1 and 1.0 in different simulations, with a baseline value of 0.2 (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 1</xref>). These branch lengths represent the expected number of substitutions per codon site per branch. The ratio of insertions to deletions was fixed at 1, and the rate of insertions and deletions (indels) to substitutions was set at 0.1 (close to the empirically determined value for coding regions in mammals; <xref ref-type="bibr" rid="R6">Chen et al., 2009</xref>) as part of the baseline parameter set. More extreme values of 0.2 and 0.3 were also tested (rates closer to those observed in bacteria: <xref ref-type="bibr" rid="R6">Chen et al., 2009</xref>), with results shown in <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 6 and Supplementary Fig. 7</xref>, respectively. As in <xref ref-type="bibr" rid="R12">Fletcher and Yang (2010)</xref>, indel lengths are drawn from a geometric distribution with parameter <italic>q</italic> = 1 − <italic>p</italic> = 0.35, as this provides a good fit to observed indel length distributions in protein coding sequences (<xref ref-type="bibr" rid="R67">Taylor, Ponting, and Copley, 2004</xref>).</p><p id="P77">Using the procedure outlined above, for each unique set of parameter values, we simulated 1,000,000 alignments for training and validating our network, and an additional 2,000 alignments to test and compare the performance of our network and the likelihood-based methods provided by CODEML (<xref ref-type="bibr" rid="R76">Yang, 2007</xref>). For training and testing simulations, 40% of the genes for each set of parameter values are evolved under negative selection, 10% with neutral selection, and 50% with positive selection. These proportions are chosen to achieve a balanced training dataset and avoid bias towards a majority class (<xref ref-type="bibr" rid="R71">Wei and Dunbrack, 2013</xref>; <xref ref-type="bibr" rid="R14">Ghosh et al., 2024</xref>).</p><p id="P78">All generated nucleotide sequences are converted into their corresponding amino acid sequences before further processing in order to maintain reading frame between sequences. The training and validation sets are aligned using Clustal Omega v1.2.4 (<xref ref-type="bibr" rid="R58">Sievers, Wilm, et al., 2011</xref>), which is fast, but is known to be capable of producing a high false positive rate when inferring selection using traditional methods (<xref ref-type="bibr" rid="R36">Löytynoja and Goldman, 2008</xref>; <xref ref-type="bibr" rid="R12">Fletcher and Yang, 2010</xref>). For each of the 2,000 testing alignments, we retain the true alignment, re-align each set of sequences using Clustal Omega, and additionally align using MAFFT v7.475 (<xref ref-type="bibr" rid="R23">Katoh and Standley, 2013</xref>) and two variants of PRANK v170427 (<xref ref-type="bibr" rid="R36">Löytynoja and Goldman, 2008</xref>), one using an amino acid model (subsequently referred to as PRANKaa) and one using an empirical codon model (subsequently referred to as PRANKc). All amino acid alignments are converted back into their original nucleotide sequences using PAL2NAL v14 (<xref ref-type="bibr" rid="R66">Suyama, Torrents, and Bork, 2006</xref>).</p></sec><sec id="S23"><label>5.2</label><title>Alignment transformation</title><p id="P79">Multiple sequence alignments are typically represented as an <italic>m</italic> × <italic>n</italic> matrix, of <italic>m</italic> sequences and <italic>n</italic> alignment columns, with each alignment position in this matrix occupied by a nucleotide or gap character. It is necessary to encode this information numerically before passing it as input to any CNN. Ideally, encoding should avoid introducing any numerical relationships between entries in the input data which were not present before encoding; for example, encoding nucleotides with integers 0, 1, 2, and 3 would artificially introduce different distances between different pairs of nucleotides. To avoid this, we use one-hot encoding of our input alignment matrix, where each nucleotide/gap character is converted into one of five unique bit patterns (<italic>A</italic> : 00001, <italic>C</italic> : 00010, <italic>G</italic> : 00100, <italic>T</italic> : 01000, – : 10000). We then convert our <italic>m</italic> × <italic>n</italic> matrices (where <italic>m</italic> = 8 and <italic>n</italic> = 3 × number of codons in the alignment) into three-dimensional <italic>m</italic> × <italic>n</italic> × 5 tensors recognised by the CNN, where the third dimension of each alignment position contains the five bits just described, corresponding to the character present at that position. (This is analogous to the common 3D encoding of RGB images in which the third dimension represents composite colour via three distinct colour channels.) For training, we store these encoded alignments (along with their true class label) in TFRecord format, a binary file format used for efficient serialisation of structured data (<xref ref-type="bibr" rid="R1">Abadi et al., 2016</xref>).</p></sec><sec id="S24"><label>5.3</label><title>OmegaAI: architecture</title><p id="P80">Here we describe our final network structure, which we refer to as OmegaAI. Before arriving at our final architecture and hyperparameter selection we conducted preliminary tests using 100,000 Clustal Omega alignments, simulated under baseline conditions. We selected the architecture with the highest F<sub>0</sub>.<sub>5</sub> score on this test set, which was an architecture with six convolutional layers. The F<sub>0</sub>.<sub>5</sub> score provides a measures of accuracy for binary classification tasks that places a greater weight on precision rather than recall. The learning rate is a hyperparameter that determines the size of the steps the model takes to adjust its weights during training, affecting the speed and stability of the learning process. A learning rate of 0.001 was opted for as the baseline, having tested a standard range. See <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 14</xref> for details of these preliminary tests.</p><p id="P81">All network implementation was done in Python v3.6.12, using Keras v2.3.0, which is packaged with TensorFlow v2.2.0 (<xref ref-type="bibr" rid="R1">Abadi et al., 2016</xref>). Our network is structured similarly to that of <xref ref-type="bibr" rid="R65">Suvorov, Hochuli, and Schrider (2020)</xref>, but uses six convolutional layers instead of eight. The first layer uses a filter size of <italic>m</italic> × 3, where <italic>m</italic> is the number of sequence rows (taxa) in each dataset, and 3 corresponds to the three nucleotides of each codon alignment column. We use a stride of 3 in the first layer to evaluate each codon column separately. We tested using a smaller filter size of 3 × 3 for the first layer, but found that convolutions over all <italic>m</italic> sequence rows resulted in better performance. We also tested a smaller stride size to allow neighbouring partial codons to be evaluated (i.e. still with filter size <italic>m</italic> × 3, but now the convolution can also covers 1 or 2 columns from one codon, and correspondingly 2 or 1 from an adjacent codon), but noticed no appreciable improvement in performance. Note also that operating over each <italic>m</italic> × 3 column with no overlap permits faster training than using smaller values for filter size and stride.</p><p id="P82">After the convolutional layers, we use global average pooling across the third dimension of the weight tensors. We chose this pooling operation over the commonly used flatten layer, as it reduces the spatial dependency between specific input alignment regions and the resulting classifications (<xref ref-type="bibr" rid="R34">Lin, Chen, and Yan, 2013</xref>). As our problem is one of binary classification (selection, 1 vs. no selection, 0), we use a sigmoid activation function in our final dense layer. Other than for our initial layer, in which convolutions are over codon columns (see above), filter sizes follow those outlined in <xref ref-type="bibr" rid="R65">Suvorov, Hochuli, and Schrider (2020)</xref>. After each convolutional layer, we use a standard batch normalisation (<xref ref-type="bibr" rid="R19">Ioffe and Szegedy, 2015</xref>), dropout regularisation (<italic>p</italic> = 0.5; <xref ref-type="bibr" rid="R64">Srivastava et al., 2014</xref>), then average pooling. This relatively high dropout probability of 0.5 was used because lower rates tended to produce overfitted networks which performed worse when evaluating test datasets (<xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 14</xref>).</p><p id="P83">Our aim was to enhance accuracy and reduce false positives in Clustal Omega alignments simultaneously. Each model underwent 50 epochs of training, with evaluation conducted on the model at the 50th epoch. Stopping at 50 epochs means we stop before validation accuracy falls due to over-fitting (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 15</xref>).</p></sec><sec id="S25"><label>5.4</label><title>OmegaAI: training</title><p id="P84">All training runs were performed on a single GPU (NVIDIA Tesla V100 PCIe 32 GB), within a high-performance computing environment. Weights in our network are initialised using Glorot uniform initialization (<xref ref-type="bibr" rid="R15">Glorot and Bengio, 2010</xref>). As is typical when working with large training datasets (here, storing 950,000 training alignments in memory is almost always unfeasible), we use non-overlapping, subsets or “batches” of alignments for training. Each of these batches consists of 512 randomly chosen alignments, zero-padded to the length of the longest alignment in that batch, as batched inputs to CNNs are required to have equal dimensions. For each batch, weight updates are performed using the Adam optimisation algorithm (<xref ref-type="bibr" rid="R24">Kingma and Ba, 2017</xref>) to minimise a binary cross-entropy loss function that distinguishes positive selection (1) from no selection (0). This procedure is performed for a total of 50 epochs, where one epoch has passed when all 950,000 training alignments have been seen once. Note that the alignments in each batch change at each epoch to avoid overfitting on any particular batch. Adam optimisation is well-established and generally performs well across a range of learning tasks (<xref ref-type="bibr" rid="R63">Soydaner, 2020</xref>). We also tried training using AdamW (<xref ref-type="bibr" rid="R35">Loshchilov and Hutter, 2017</xref>), a popular extension of Adam, but noticed no appreciable improvement in performance.</p><p id="P85">For the standard baseline OmegaAI model, solely Clustal alignments are used to train the CNN. Clustal is fast (<xref ref-type="bibr" rid="R58">Sievers, Wilm, et al., 2011</xref>) which is crucial for the scalability needed in generating large amounts of training data. In addition, the tool is also widely used and accessible. Despite being the most error-prone among the aligners tested (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 2 and Supplementary Fig. 3</xref>), this characteristic presents an opportunity for the CNN’s training process and may even be beneficial (see <xref ref-type="sec" rid="S10">Results</xref> and <xref ref-type="sec" rid="S20">Discussion</xref>).</p></sec><sec id="S26"><label>5.5</label><title>Comparison of OmegaAI with maximum likelihood methods</title><p id="P86">To evaluate the performance of both OmegaAI and likelihood-based tests for selection, we use the true, Clustal Omega, MAFFT, PRANKc, and PRANKaa alignments generated for each parameter set (2,000 alignments for each aligner).</p><p id="P87">In all tests, we perform classification under OmegaAI on each of the 2,000 Clustal Omega alignments, and in most cases, we also classify the alignments of the same sequences obtained from all other aligners. For almost all test sets, and consistent with our network training, we apply a threshold of 0.5: the classification of an alignment is ‘positive selection’ if the output value from the final sigmoid activation layer, <italic>Z</italic>, satisfies <italic>Z</italic> ≽ 0.5, and ‘no selection’ otherwise. For a subset of test sets, we also explore increasing the threshold for calling positive selection in order to reduce the false positive rate.</p><p id="P88">Likelihood-based tests are facilitated by the CODEML program implemented in PAML (<xref ref-type="bibr" rid="R76">Yang, 2007</xref>), which provides statistical methods for detecting the presence of positive selection in alignments of protein-coding nucleotide sequences under a variety of statistical models of selection. We used the two recommended pairs of models for detecting selection in scenarios acknowledging the possible presence of sitewise variation in <italic>ω</italic>: M1a/M2a (<xref ref-type="bibr" rid="R46">Nielsen and Yang, 1998</xref>; <xref ref-type="bibr" rid="R81">Yang, Wong, and Nielsen, 2005</xref>) and M7/M8 (<xref ref-type="bibr" rid="R80">Yang, Nielsen, et al., 2000</xref>). Each pair enables a likelihood ratio test (LRT) between hypotheses of all sites evolving with no positive selection (<italic>ω</italic> ≤ 1), and evolution with positive selection (<italic>ω</italic> &gt; 1 at some sites). Briefly, in the first LRT, M1a assumes two site classes, <italic>ω</italic><sub>0</sub> &lt; 1 and <italic>ω</italic><sub>1</sub> = 1, while M2a assumes three site classes, <italic>ω</italic><sub>0</sub> &lt; 1, <italic>ω</italic><sub>1</sub> = 1, and <italic>ω</italic><sub>2</sub> &gt; 1. Note the correspondence between the assumptions of M1a and M2a and the distributions shown in <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 13</xref>. In the second LRT, M7 assumes that all sites are conserved with <italic>ω</italic> ~ beta(<italic>p, q</italic>), while M8 assumes that a proportion (<italic>p</italic><sub>0</sub>) of sites are conserved with <italic>ω</italic><sub>0</sub> ~ beta(<italic>p</italic>, <italic>q</italic>), and all other sites (<italic>p<sub>1</sub></italic> = 1 – <italic>p</italic><sub>0</sub>) evolved under positive selection with <italic>ω</italic> &gt; 1. The parameters in all of these models are estimated by maximising their likelihood functions (see <xref ref-type="bibr" rid="R80">Yang, Nielsen, et al., 2000</xref>), and the significance of the LRT between no selection and selection is evaluated using a <bold><italic>χ</italic></bold><sup>2</sup> test with two degrees of freedom in each case. In this study, a significance level of 0.05 is chosen, indicating a 95% confidence level for determining the presence of positive selection, which is a common threshold in the literature (<xref ref-type="bibr" rid="R74">Wong, Yang, et al., 2004</xref>). We define CODEML’s classification of positive selection to be the result where both the M1a/M2a and M7/M8 LRTs produce significant, positive results. We do not classify a gene to be under positive selection if one or both LRTs are non-significant. CODEML is given the simulation topology as a start tree and is allowed to re-estimate branch lengths during the likelihood maximising process: CODEML tends to perform better under this regime compared to enforcing fixed branch lengths (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 16</xref>).</p><p id="P89">In addition to enabling LRTs for binary classification of a whole MSA, CODEML can conduct a Bayes empirical Bayes analysis to estimate sitewise <italic>ω</italic> values in cases where we want to study the specific location(s) of signal for selection within alignments (<xref ref-type="bibr" rid="R76">Yang, 2007</xref>; <xref ref-type="bibr" rid="R2">Álvarez-Carretero, Kapli, and Yang, 2023</xref>). We use this for comparison with the saliency approach for CNNs, described in the next section.</p></sec><sec id="S27"><label>5.6</label><title>Alignment saliency</title><p id="P90">We are interested in where the most informative positions lie within an MSA input, and in particular what the characteristics of these informative positions are. We can investigate this by quantifying the importance of each position in an input alignment on the prediction from our network using saliency maps (<xref ref-type="bibr" rid="R59">Simonyan, Vedaldi, and Zisserman, 2014</xref>). For example, we can expect that the most informative sites for predicting positive selection are those evolving with <italic>ω</italic> &gt; 1. We can also examine the saliency of misaligned and neighboring columns, particularly where codon sites that have evolved under different selection pressures are incorrectly placed in the same column. Given a CNN producing an output score <italic>Z</italic> (between 0 and 1, ultimately thresholded to infer selection or no selection) from any input alignment <italic>A</italic> (of size <italic>m</italic> sequences × <italic>n</italic> alignment columns × 5 nucleotide/gap characters), a saliency map is a function <italic>M</italic> which assigns an importance <italic>M(A)</italic><sub><italic>ij</italic></sub> ∈ ℝ to each alignment position (<italic>i,j</italic>) based on the influence of that position on the output score <italic>Z</italic>(<italic>A</italic>). We use the saliency function of <xref ref-type="bibr" rid="R59">Simonyan, Vedaldi, and Zisserman (2014)</xref> as it provides a suitable method for calculating the sitewise saliency for one-hot encoded values. Given a specific alignment A<sub>0</sub> and associated binary score <italic>Z</italic>(<italic>A</italic><sub>0</sub>), the (non-linear) function <italic>Z</italic>(<italic>A</italic>) is approximated as a linear function in the neighbourhood of <italic>A</italic><sub>0</sub> using the first-order Taylor expansion: <disp-formula id="FD1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:mi>Z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≈</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P91">where <italic>w</italic> is the derivative of <italic>Z</italic> with respect to alignment <italic>A</italic> at <italic>A</italic><sub>0</sub>: <disp-formula id="FD2"><label>(2)</label><mml:math id="M2"><mml:mrow><mml:mi>w</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P92">Note that in our application this derivative is three-dimensional (<italic>w</italic> = <italic>w<sub>ijc</sub></italic>), because <italic>A</italic> has dimensions <italic>m</italic> × <italic>n</italic> × 5. To compute the full saliency map (<italic>M</italic> ∈ ℝ<sup><italic>m</italic></sup>×<sup><italic>n</italic></sup>) for each alignment, we first calculate the derivative <italic>w</italic> (<xref ref-type="disp-formula" rid="FD2">Equation 2</xref>) by back-propagation, and then take the maximum magnitude of <italic>w</italic> across all nucleotide channels <italic>c</italic> at each alignment position (<italic>i,j</italic>) using <disp-formula id="FD3"><label>(3)</label><mml:math id="M3"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mi>C</mml:mi></mml:munder><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Information</label><media xlink:href="EMS200373-supplement-Supplementary_Information.pdf" mimetype="application" mime-subtype="pdf" id="d142aAcFbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S28"><title>Acknowledgments</title><p>We thank José Almeida for assistance with technical details of CNN architecture design and interpretation of results.</p><sec id="S29"><title>Funding</title><p>C.W., C.R.W., S.A., N.D.M. and N.G. were supported by the European Molecular Biology Laboratory. C.R.W. was supported by the National Institute of Health Research (NIHR) Cambridge Biomedical Research Centre; grant number IS-BRC-1215-20014. X.X. was supported by the Cambridge Mathematics Placements programme.</p></sec></ack><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abadi</surname><given-names>M</given-names></name><etal/></person-group><article-title>TensorFlow: large-scale machine learning on heterogeneous distributed systems</article-title><source>arXiv</source><year>2016</year><elocation-id>1603.04467</elocation-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Álvarez-Carretero</surname><given-names>S</given-names></name><name><surname>Kapli</surname><given-names>P</given-names></name><name><surname>Yang</surname><given-names>Z</given-names></name></person-group><article-title>Beginner’s guide on the use of PAML to detect positive selection</article-title><source>Molecular Biology and Evolution</source><year>2023</year><volume>40</volume><elocation-id>msad041</elocation-id><pub-id pub-id-type="pmcid">PMC10127084</pub-id><pub-id pub-id-type="pmid">37096789</pub-id><pub-id pub-id-type="doi">10.1093/molbev/msad041</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bertl</surname><given-names>J</given-names></name><name><surname>Ewing</surname><given-names>G</given-names></name><name><surname>Kosiol</surname><given-names>C</given-names></name><name><surname>Futschik</surname><given-names>A</given-names></name></person-group><article-title>Approximate maximum likelihood estimation for population genetic inference</article-title><source>Statistical Applications in Genetics and Molecular Biology</source><year>2017</year><volume>16</volume><fpage>291</fpage><lpage>312</lpage><pub-id pub-id-type="pmid">29095700</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bishop</surname><given-names>J</given-names></name><name><surname>Dean</surname><given-names>A</given-names></name><name><surname>Mitchell-Olds</surname><given-names>T</given-names></name></person-group><article-title>Rapid evolution in plant chitinases: molecular targets of selection in plant-pathogen coevolution</article-title><source>Proceedings of the National Academy of Sciences U.S.A</source><year>2000</year><volume>97</volume><fpage>5322</fpage><lpage>5327</lpage><pub-id pub-id-type="pmcid">PMC25827</pub-id><pub-id pub-id-type="pmid">10805791</pub-id><pub-id pub-id-type="doi">10.1073/pnas.97.10.5322</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burgstaller-Muehlbacher</surname><given-names>S</given-names></name><etal/></person-group><article-title>ModelRevelator: fast phylogenetic model estimation via deep learning</article-title><source>Molecular Phylogenetics and Evolution</source><year>2023</year><volume>188</volume><elocation-id>107905</elocation-id><pub-id pub-id-type="pmid">37595933</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>J-Q</given-names></name><etal/></person-group><article-title>Variation in the ratio of nucleotide substitution and indel rates across genomes in mammals and bacteria</article-title><source>Molecular Biology and Evolution</source><year>2009</year><volume>26</volume><fpage>1523</fpage><lpage>1531</lpage><pub-id pub-id-type="pmid">19329651</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Andrade</surname><given-names>A</given-names></name></person-group><article-title>Best practices for convolutional neural networks applied to object recognition in images</article-title><source>arXiv</source><year>2019</year><elocation-id>1910.13029</elocation-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Maio</surname><given-names>N</given-names></name><name><surname>Holmes</surname><given-names>I</given-names></name><name><surname>Schlötterer</surname><given-names>C</given-names></name><name><surname>Kosiol</surname><given-names>C</given-names></name></person-group><article-title>Estimating empirical codon hidden Markov models</article-title><source>Molecular Biology and Evolution</source><year>2013</year><volume>30</volume><fpage>725</fpage><lpage>736</lpage><pub-id pub-id-type="pmcid">PMC3563974</pub-id><pub-id pub-id-type="pmid">23188590</pub-id><pub-id pub-id-type="doi">10.1093/molbev/mss266</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eche</surname><given-names>T</given-names></name><name><surname>Schwartz</surname><given-names>LH</given-names></name><name><surname>Mokrane</surname><given-names>F-Z</given-names></name><name><surname>Dercle</surname><given-names>L</given-names></name></person-group><article-title>Toward generalizability in the deployment of artificial intelligence in radiology: role of computation stress testing to overcome underspecification</article-title><source>Radiology: Artificial Intelligence</source><year>2021</year><volume>3</volume><elocation-id>e210097</elocation-id><pub-id pub-id-type="pmcid">PMC8637230</pub-id><pub-id pub-id-type="pmid">34870222</pub-id><pub-id pub-id-type="doi">10.1148/ryai.2021210097</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flagel</surname><given-names>L</given-names></name><name><surname>Brandvain</surname><given-names>Y</given-names></name><name><surname>Schrider</surname><given-names>DR</given-names></name></person-group><article-title>The unreasonable effectiveness of convolutional neural networks in population genetic inference</article-title><source>Molecular Biology and Evolution</source><year>2019</year><volume>36</volume><fpage>220</fpage><lpage>238</lpage><pub-id pub-id-type="pmcid">PMC6367976</pub-id><pub-id pub-id-type="pmid">30517664</pub-id><pub-id pub-id-type="doi">10.1093/molbev/msy224</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fletcher</surname><given-names>W</given-names></name><name><surname>Yang</surname><given-names>Z</given-names></name></person-group><article-title>INDELible: a flexible simulator of biological sequence evolution</article-title><source>Molecular Biology and Evolution</source><year>2009</year><volume>26</volume><fpage>1879</fpage><lpage>1888</lpage><pub-id pub-id-type="pmcid">PMC2712615</pub-id><pub-id pub-id-type="pmid">19423664</pub-id><pub-id pub-id-type="doi">10.1093/molbev/msp098</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fletcher</surname><given-names>W</given-names></name><name><surname>Yang</surname><given-names>Z</given-names></name></person-group><article-title>The effect of insertions, deletions, and alignment errors on the branch-site test of positive selection</article-title><source>Molecular Biology and Evolution</source><year>2010</year><volume>27</volume><fpage>2257</fpage><lpage>2267</lpage><pub-id pub-id-type="pmid">20447933</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ganin</surname><given-names>Y</given-names></name><name><surname>Lempitsky</surname><given-names>V</given-names></name></person-group><article-title>Unsupervised domain adaptation by backpropagation</article-title><source>Proceedings of Machine Learning Research</source><year>2015</year><volume>37</volume><fpage>1180</fpage><lpage>1189</lpage></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghosh</surname><given-names>K</given-names></name><etal/></person-group><article-title>The class imbalance problem in deep learning</article-title><source>Machine Learning</source><year>2024</year><volume>113</volume><fpage>4845</fpage><lpage>4901</lpage></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glorot</surname><given-names>X</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><article-title>Understanding the difficulty of training deep feedforward neural networks</article-title><source>Proceedings of Machine Learning Research</source><year>2010</year><volume>9</volume><fpage>249</fpage><lpage>256</lpage></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldman</surname><given-names>N</given-names></name><name><surname>Yang</surname><given-names>Z</given-names></name></person-group><article-title>A codon-based model of nucleotide substitution for proteincoding DNA sequences</article-title><source>Molecular Biology and Evolution</source><year>1994</year><volume>11</volume><fpage>725</fpage><lpage>736</lpage><pub-id pub-id-type="pmid">7968486</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haller</surname><given-names>BC</given-names></name><name><surname>Messer</surname><given-names>PW</given-names></name></person-group><article-title>SLiM 3: forward genetic simulations beyond the Wright-Fisher model</article-title><source>Molecular Biology and Evolution</source><year>2019</year><volume>36</volume><fpage>632</fpage><lpage>637</lpage><pub-id pub-id-type="pmcid">PMC6389312</pub-id><pub-id pub-id-type="pmid">30517680</pub-id><pub-id pub-id-type="doi">10.1093/molbev/msy228</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huisman</surname><given-names>M</given-names></name><name><surname>Hannink</surname><given-names>G</given-names></name></person-group><article-title>The AI generalization gap: one size does not fit all</article-title><source>Radiology: Artificial Intelligence</source><year>2023</year><volume>5</volume><elocation-id>e230246</elocation-id><pub-id pub-id-type="pmcid">PMC10546357</pub-id><pub-id pub-id-type="pmid">37795134</pub-id><pub-id pub-id-type="doi">10.1148/ryai.230246</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ioffe</surname><given-names>S</given-names></name><name><surname>Szegedy</surname><given-names>C</given-names></name></person-group><article-title>Batch normalization: accelerating deep network training by reducing internal covariate shift</article-title><source>arXiv</source><year>2015</year><elocation-id>1502.03167</elocation-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ji</surname><given-names>X</given-names></name><name><surname>Griffing</surname><given-names>A</given-names></name><name><surname>Thorne</surname><given-names>JL</given-names></name></person-group><article-title>A phylogenetic approach finds abundant interlocus gene conversion in yeast</article-title><source>Molecular Biology and Evolution</source><year>2016</year><volume>33</volume><fpage>2469</fpage><lpage>2476</lpage><pub-id pub-id-type="pmcid">PMC6398807</pub-id><pub-id pub-id-type="pmid">27297467</pub-id><pub-id pub-id-type="doi">10.1093/molbev/msw114</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jordan</surname><given-names>G</given-names></name><name><surname>Goldman</surname><given-names>N</given-names></name></person-group><article-title>The effects of alignment error and alignment filtering on the sitewise detection of positive selection</article-title><source>Molecular Biology and Evolution</source><year>2012</year><volume>29</volume><fpage>1125</fpage><lpage>1139</lpage><pub-id pub-id-type="pmid">22049066</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jumper</surname><given-names>J</given-names></name><etal/></person-group><article-title>Highly accurate protein structure prediction with AlphaFold</article-title><source>Nature</source><year>2021</year><volume>596</volume><fpage>583</fpage><lpage>589</lpage><pub-id pub-id-type="pmcid">PMC8371605</pub-id><pub-id pub-id-type="pmid">34265844</pub-id><pub-id pub-id-type="doi">10.1038/s41586-021-03819-2</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Katoh</surname><given-names>K</given-names></name><name><surname>Standley</surname><given-names>DM</given-names></name></person-group><article-title>MAFFT multiple sequence alignment software version 7: improvements in performance and usability</article-title><source>Molecular Biology and Evolution</source><year>2013</year><volume>30</volume><fpage>772</fpage><lpage>780</lpage><pub-id pub-id-type="pmcid">PMC3603318</pub-id><pub-id pub-id-type="pmid">23329690</pub-id><pub-id pub-id-type="doi">10.1093/molbev/mst010</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><article-title>Adam: a method for stochastic optimization</article-title><source>arXiv</source><year>2017</year><elocation-id>1412.6980</elocation-id></element-citation></ref><ref id="R25"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kozlov</surname><given-names>OM</given-names></name><name><surname>Stamatakis</surname><given-names>A</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Weiland</surname><given-names>M</given-names></name><name><surname>Neuwirth</surname><given-names>S</given-names></name><name><surname>Kruse</surname><given-names>C</given-names></name><name><surname>Weinzierl</surname><given-names>T</given-names></name></person-group><source>EcoFreq: compute with cheaper, cleaner energy via carbon-aware power scaling. High Performance Computing. Lecture Notes in Computer Science</source><conf-name>ISC High Performance 2024 Research International Workshops</conf-name><conf-loc>Cham</conf-loc><publisher-name>Springer Nature Switzerland</publisher-name><year>2024</year><volume>15058</volume><fpage>1</fpage><lpage>12</lpage></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><article-title>ImageNet classification with deep convolutional neural networks</article-title><source>Communications of the ACM</source><year>2017</year><volume>60</volume><fpage>84</fpage><lpage>90</lpage></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kubatko</surname><given-names>L</given-names></name><name><surname>Shah</surname><given-names>P</given-names></name><name><surname>Herbei</surname><given-names>R</given-names></name><name><surname>Gilchrist</surname><given-names>MA</given-names></name></person-group><article-title>A codon model of nucleotide substitution with selection on synonymous codon usage</article-title><source>Molecular Phylogenetics and Evolution</source><year>2016</year><volume>94</volume><fpage>290</fpage><lpage>297</lpage><pub-id pub-id-type="pmid">26358614</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumar</surname><given-names>S</given-names></name></person-group><article-title>Embracing green computing in molecular phylogenetics</article-title><source>Molecular Biology and Evolution</source><year>2022</year><volume>39</volume><elocation-id>msac043</elocation-id><pub-id pub-id-type="pmcid">PMC8894743</pub-id><pub-id pub-id-type="pmid">35243506</pub-id><pub-id pub-id-type="doi">10.1093/molbev/msac043</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumar</surname><given-names>S</given-names></name><name><surname>Subramanian</surname><given-names>S</given-names></name></person-group><article-title>Mutation rates in mammalian genomes</article-title><source>Proceedings of the National Academy of Sciences U.S.A</source><year>2002</year><volume>99</volume><fpage>803</fpage><lpage>808</lpage><pub-id pub-id-type="pmcid">PMC117386</pub-id><pub-id pub-id-type="pmid">11792858</pub-id><pub-id pub-id-type="doi">10.1073/pnas.022629899</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Längkvist</surname><given-names>M</given-names></name><name><surname>Alirezaie</surname><given-names>M</given-names></name><name><surname>Kiselev</surname><given-names>A</given-names></name><name><surname>Loutfi</surname><given-names>A</given-names></name></person-group><source>Interactive learning with convolutional neural networks for image labeling</source><conf-name>Presented at International Joint Conference on Artificial Intelligence (IJCAI)</conf-name><conf-loc>New York, USA</conf-loc><conf-date>9–15th July, 2016</conf-date><year>2016</year></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Lee</surname><given-names>R</given-names></name><name><surname>Wiel</surname><given-names>L</given-names></name><name><surname>van Dam</surname><given-names>TJ</given-names></name><name><surname>Huynen</surname><given-names>MA</given-names></name></person-group><article-title>Genome-scale detection of positive selection in nine primates predicts human-virus evolutionary conflicts</article-title><source>Nucleic Acids Research</source><year>2017</year><volume>45</volume><fpage>10634</fpage><lpage>10648</lpage><pub-id pub-id-type="pmcid">PMC5737536</pub-id><pub-id pub-id-type="pmid">28977405</pub-id><pub-id pub-id-type="doi">10.1093/nar/gkx704</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeNail</surname><given-names>A</given-names></name></person-group><article-title>NN-SVG: publication-ready neural network architecture schematics</article-title><source>Journal of Open Source Software</source><year>2019</year><volume>4</volume><fpage>747</fpage></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lenzi</surname><given-names>A</given-names></name><name><surname>Bessac</surname><given-names>J</given-names></name><name><surname>Rudi</surname><given-names>J</given-names></name><name><surname>Stein</surname><given-names>ML</given-names></name></person-group><article-title>Neural networks for parameter estimation in intractable models</article-title><source>Computational Statistics and Data Analysis</source><year>2023</year><volume>185</volume><elocation-id>107762</elocation-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>M</given-names></name><name><surname>Chen</surname><given-names>Q</given-names></name><name><surname>Yan</surname><given-names>S</given-names></name></person-group><article-title>Network in network</article-title><source>arXiv</source><year>2013</year><elocation-id>1312.4400</elocation-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loshchilov</surname><given-names>I</given-names></name><name><surname>Hutter</surname><given-names>F</given-names></name></person-group><article-title>Decoupled weight decay regularization</article-title><source>arXiv</source><year>2017</year><elocation-id>1711.05101</elocation-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Löytynoja</surname><given-names>A</given-names></name><name><surname>Goldman</surname><given-names>N</given-names></name></person-group><article-title>Phylogeny-aware gap placement prevents errors in sequence alignment and evolutionary analysis</article-title><source>Science</source><year>2008</year><volume>320</volume><fpage>1632</fpage><lpage>1635</lpage><pub-id pub-id-type="pmid">18566285</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lucaci</surname><given-names>AG</given-names></name><name><surname>Wisotsky</surname><given-names>SR</given-names></name><name><surname>Shank</surname><given-names>SD</given-names></name><name><surname>Weaver</surname><given-names>S</given-names></name><name><surname>Kosakovsky Pond</surname><given-names>SL</given-names></name></person-group><article-title>Extra base hits: widespread empirical support for instantaneous multiple-nucleotide changes</article-title><source>PLoS ONE</source><year>2021</year><volume>16</volume><elocation-id>e0248337</elocation-id><pub-id pub-id-type="pmcid">PMC7954308</pub-id><pub-id pub-id-type="pmid">33711070</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0248337</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Magar</surname><given-names>I</given-names></name><name><surname>Schwartz</surname><given-names>R</given-names></name></person-group><article-title>Data contamination: from memorization to exploitation</article-title><source>arXiv</source><year>2022</year><elocation-id>2203.08242</elocation-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mallo</surname><given-names>D</given-names></name><name><surname>De Oliveira Martins</surname><given-names>L</given-names></name><name><surname>Posada</surname><given-names>D</given-names></name></person-group><article-title>SimPhy: phylogenomic simulation of gene, locus, and species trees</article-title><source>Systematic Biology</source><year>2016</year><volume>65</volume><fpage>334</fpage><lpage>344</lpage><pub-id pub-id-type="pmcid">PMC4748750</pub-id><pub-id pub-id-type="pmid">26526427</pub-id><pub-id pub-id-type="doi">10.1093/sysbio/syv082</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martincorena</surname><given-names>I</given-names></name><etal/></person-group><article-title>Universal patterns of selection in cancer and somatic tissues</article-title><source>Cell</source><year>2017</year><volume>171</volume><fpage>1029</fpage><lpage>1041</lpage><pub-id pub-id-type="pmcid">PMC5720395</pub-id><pub-id pub-id-type="pmid">29056346</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2017.09.042</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Massingham</surname><given-names>T</given-names></name><name><surname>Goldman</surname><given-names>N</given-names></name></person-group><article-title>Detecting amino acid sites under positive selection and purifying selection</article-title><source>Genetics</source><year>2005</year><volume>169</volume><fpage>1753</fpage><lpage>1762</lpage><pub-id pub-id-type="pmcid">PMC1449526</pub-id><pub-id pub-id-type="pmid">15654091</pub-id><pub-id pub-id-type="doi">10.1534/genetics.104.032144</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McGuire</surname><given-names>G</given-names></name><name><surname>Denham</surname><given-names>MC</given-names></name><name><surname>Balding</surname><given-names>DJ</given-names></name></person-group><article-title>Models of sequence evolution for DNA sequences containing gaps</article-title><source>Molecular Biology and Evolution</source><year>2001</year><volume>18</volume><fpage>481</fpage><lpage>490</lpage><pub-id pub-id-type="pmid">11264399</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mirarab</surname><given-names>S</given-names></name><name><surname>Warnow</surname><given-names>T</given-names></name></person-group><article-title>FastSP: linear time calculation of alignment accuracy</article-title><source>Bioinformatics</source><year>2011</year><volume>27</volume><fpage>3250</fpage><lpage>3258</lpage><pub-id pub-id-type="pmid">21984754</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muse</surname><given-names>SV</given-names></name><name><surname>Gaut</surname><given-names>BS</given-names></name></person-group><article-title>A likelihood approach for comparing synonymous and nonsynonymous nucleotide substitution rates, with application to the chloroplast genome</article-title><source>Molecular Biology and Evolution</source><year>1994</year><volume>11</volume><fpage>715</fpage><lpage>724</lpage><pub-id pub-id-type="pmid">7968485</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nesterenko</surname><given-names>L</given-names></name><name><surname>Blassel</surname><given-names>L</given-names></name><name><surname>Veber</surname><given-names>P</given-names></name><name><surname>Boussau</surname><given-names>B</given-names></name><name><surname>Jacob</surname><given-names>L</given-names></name></person-group><article-title>Phyloformer: fast, accurate and versatile phylogenetic reconstruction with deep neural networks</article-title><source>bioRxiv</source><year>2024</year><pub-id pub-id-type="doi">10.1101/2024.06.17.599404</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nielsen</surname><given-names>R</given-names></name><name><surname>Yang</surname><given-names>Z</given-names></name></person-group><article-title>Likelihood models for detecting positively selected amino acid sites and applications to the HIV-1 envelope gene</article-title><source>Genetics</source><year>1998</year><volume>148</volume><fpage>929</fpage><lpage>936</lpage><pub-id pub-id-type="pmcid">PMC1460041</pub-id><pub-id pub-id-type="pmid">9539414</pub-id><pub-id pub-id-type="doi">10.1093/genetics/148.3.929</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ogurtsov</surname><given-names>AY</given-names></name><name><surname>Sunyaev</surname><given-names>S</given-names></name><name><surname>Kondrashov</surname><given-names>AS</given-names></name></person-group><article-title>Indel-based evolutionary distance and mouse–human divergence</article-title><source>Genome Research</source><year>2004</year><volume>14</volume><fpage>1610</fpage><lpage>1616</lpage><pub-id pub-id-type="pmcid">PMC509270</pub-id><pub-id pub-id-type="pmid">15289479</pub-id><pub-id pub-id-type="doi">10.1101/gr.2450504</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parmley</surname><given-names>JL</given-names></name><name><surname>Chamary</surname><given-names>J-V</given-names></name><name><surname>Hurst</surname><given-names>LD</given-names></name></person-group><article-title>Evidence for purifying selection against synonymous mutations in mammalian exonic splicing enhancers</article-title><source>Molecular Biology and Evolution</source><year>2006</year><volume>23</volume><fpage>301</fpage><lpage>309</lpage><pub-id pub-id-type="pmid">16221894</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pečerska</surname><given-names>J</given-names></name><name><surname>Gil</surname><given-names>M</given-names></name><name><surname>Anisimova</surname><given-names>M</given-names></name></person-group><article-title>Joint alignment and tree inference</article-title><source>bioRxiv</source><year>2021</year><pub-id pub-id-type="doi">10.1101/2021.09.28.462230</pub-id></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perron</surname><given-names>U</given-names></name><name><surname>Kozlov</surname><given-names>AM</given-names></name><name><surname>Stamatakis</surname><given-names>A</given-names></name><name><surname>Goldman</surname><given-names>N</given-names></name><name><surname>Moal</surname><given-names>IH</given-names></name></person-group><article-title>Modeling structural constraints on protein evolution via side-chain conformational states</article-title><source>Molecular Biology and Evolution</source><year>2019</year><volume>36</volume><fpage>2086</fpage><lpage>2103</lpage><pub-id pub-id-type="pmcid">PMC6736381</pub-id><pub-id pub-id-type="pmid">31114882</pub-id><pub-id pub-id-type="doi">10.1093/molbev/msz122</pub-id></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pond</surname><given-names>SLK</given-names></name><name><surname>Frost</surname><given-names>SD</given-names></name><name><surname>Muse</surname><given-names>SV</given-names></name></person-group><article-title>HyPhy: hypothesis testing using phylogenies</article-title><source>Bioinformatics</source><year>2005</year><volume>21</volume><fpage>676</fpage><lpage>679</lpage><pub-id pub-id-type="pmid">15509596</pub-id></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>RM</given-names></name><etal/></person-group><article-title>MSA transformer</article-title><source>Proceedings of Machine Learning Research</source><year>2021</year><volume>139</volume><fpage>8844</fpage><lpage>8856</lpage></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Redelings</surname><given-names>B</given-names></name></person-group><article-title>Erasing errors due to alignment ambiguity when estimating positive selection</article-title><source>Molecular Biology and Evolution</source><year>2014</year><volume>31</volume><fpage>1979</fpage><lpage>1993</lpage><pub-id pub-id-type="pmcid">PMC4155473</pub-id><pub-id pub-id-type="pmid">24866534</pub-id><pub-id pub-id-type="doi">10.1093/molbev/msu174</pub-id></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenberg</surname><given-names>AA</given-names></name><name><surname>Marx</surname><given-names>A</given-names></name><name><surname>Bronstein</surname><given-names>AM</given-names></name></person-group><article-title>Codon-specific Ramachandran plots show amino acid backbone conformation depends on identity of the translated codon</article-title><source>Nature Communications</source><year>2022</year><volume>13</volume><elocation-id>2815</elocation-id><pub-id pub-id-type="pmcid">PMC9123026</pub-id><pub-id pub-id-type="pmid">35595777</pub-id><pub-id pub-id-type="doi">10.1038/s41467-022-30390-9</pub-id></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>X</given-names></name><name><surname>Song</surname><given-names>S</given-names></name><name><surname>Li</surname><given-names>C</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name></person-group><article-title>Synonymous mutations in representative yeast genes are mostly strongly non-neutral</article-title><source>Nature</source><year>2022</year><volume>606</volume><fpage>725</fpage><lpage>731</lpage><pub-id pub-id-type="pmcid">PMC9650438</pub-id><pub-id pub-id-type="pmid">35676473</pub-id><pub-id pub-id-type="doi">10.1038/s41586-022-04823-w</pub-id></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shin</surname><given-names>SC</given-names></name><etal/></person-group><article-title>The genome sequence of the Antarctic bullhead notothen reveals evolutionary adaptations to a cold environment</article-title><source>Genome Biology</source><year>2014</year><volume>15</volume><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="pmcid">PMC4192396</pub-id><pub-id pub-id-type="pmid">25252967</pub-id><pub-id pub-id-type="doi">10.1186/s13059-014-0468-1</pub-id></element-citation></ref><ref id="R57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sievers</surname><given-names>F</given-names></name><name><surname>Higgins</surname><given-names>DG</given-names></name></person-group><article-title>Clustal Omega for making accurate alignments of many protein sequences</article-title><source>Protein Science</source><year>2018</year><volume>27</volume><fpage>135</fpage><lpage>145</lpage><pub-id pub-id-type="pmcid">PMC5734385</pub-id><pub-id pub-id-type="pmid">28884485</pub-id><pub-id pub-id-type="doi">10.1002/pro.3290</pub-id></element-citation></ref><ref id="R58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sievers</surname><given-names>F</given-names></name><name><surname>Wilm</surname><given-names>A</given-names></name><etal/></person-group><article-title>Fast, scalable generation of high-quality protein multiple sequence alignments using Clustal Omega</article-title><source>Molecular Systems Biology</source><year>2011</year><volume>7</volume><fpage>539</fpage><pub-id pub-id-type="pmcid">PMC3261699</pub-id><pub-id pub-id-type="pmid">21988835</pub-id><pub-id pub-id-type="doi">10.1038/msb.2011.75</pub-id></element-citation></ref><ref id="R59"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K</given-names></name><name><surname>Vedaldi</surname><given-names>A</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>LeCun</surname><given-names>Y</given-names></name></person-group><source>Deep inside convolutional networks: visualising image classification models and saliency maps</source><conf-name>2nd International Conference on Learning Representations (ICLR 2014), Workshop Track Proceedings</conf-name><conf-loc>Banff, Canada</conf-loc><conf-date>14–16 April, 2014</conf-date><year>2014</year></element-citation></ref><ref id="R60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sipos</surname><given-names>B</given-names></name><name><surname>Massingham</surname><given-names>T</given-names></name><name><surname>Jordan</surname><given-names>GE</given-names></name><name><surname>Goldman</surname><given-names>N</given-names></name></person-group><article-title>PhyloSim — Monte Carlo simulation of sequence evolution in the R statistical computing environment</article-title><source>BMC Bioinformatics</source><year>2011</year><volume>12</volume><fpage>104</fpage><pub-id pub-id-type="pmcid">PMC3102636</pub-id><pub-id pub-id-type="pmid">21504561</pub-id><pub-id pub-id-type="doi">10.1186/1471-2105-12-104</pub-id></element-citation></ref><ref id="R61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skovgaard</surname><given-names>M</given-names></name><name><surname>Jensen</surname><given-names>LJ</given-names></name><name><surname>Brunak</surname><given-names>S</given-names></name><name><surname>Ussery</surname><given-names>D</given-names></name><name><surname>Krogh</surname><given-names>A</given-names></name></person-group><article-title>On the total number of genes and their length distribution in complete microbial genomes</article-title><source>Trends in Genetics</source><year>2001</year><volume>17</volume><fpage>425</fpage><lpage>428</lpage><pub-id pub-id-type="pmid">11485798</pub-id></element-citation></ref><ref id="R62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Solis-Lemus</surname><given-names>C</given-names></name><name><surname>Yang</surname><given-names>S</given-names></name><name><surname>Zepeda-Nunez</surname><given-names>L</given-names></name></person-group><article-title>Accurate phylogenetic inference with a symmetry-preserving neural network model</article-title><source>arXiv</source><year>2022</year><elocation-id>2201.04663</elocation-id><pub-id pub-id-type="pmcid">PMC11026143</pub-id><pub-id pub-id-type="pmid">38638281</pub-id><pub-id pub-id-type="doi">10.1093/bioadv/vbae022</pub-id></element-citation></ref><ref id="R63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soydaner</surname><given-names>D</given-names></name></person-group><article-title>A comparison of optimization algorithms for deep learning</article-title><source>International Journal of Pattern Recognition and Artificial Intelligence</source><year>2020</year><volume>34</volume><fpage>1</fpage><lpage>26</lpage></element-citation></ref><ref id="R64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srivastava</surname><given-names>N</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Salakhutdinov</surname><given-names>R</given-names></name></person-group><article-title>Dropout: a simple way to prevent neural networks from overfitting</article-title><source>Journal of Machine Learning Research</source><year>2014</year><volume>15</volume><fpage>1929</fpage><lpage>1958</lpage></element-citation></ref><ref id="R65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suvorov</surname><given-names>A</given-names></name><name><surname>Hochuli</surname><given-names>J</given-names></name><name><surname>Schrider</surname><given-names>DR</given-names></name></person-group><article-title>Accurate inference of tree topologies from multiple sequence alignments using deep learning</article-title><source>Systematic Biology</source><year>2020</year><volume>69</volume><fpage>221</fpage><lpage>233</lpage><pub-id pub-id-type="pmcid">PMC8204903</pub-id><pub-id pub-id-type="pmid">31504938</pub-id><pub-id pub-id-type="doi">10.1093/sysbio/syz060</pub-id></element-citation></ref><ref id="R66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suyama</surname><given-names>M</given-names></name><name><surname>Torrents</surname><given-names>D</given-names></name><name><surname>Bork</surname><given-names>P</given-names></name></person-group><article-title>PAL2NAL: robust conversion of protein sequence alignments into the corresponding codon alignments</article-title><source>Nucleic Acids Research</source><year>2006</year><volume>34</volume><fpage>W609</fpage><lpage>W612</lpage><pub-id pub-id-type="pmcid">PMC1538804</pub-id><pub-id pub-id-type="pmid">16845082</pub-id><pub-id pub-id-type="doi">10.1093/nar/gkl315</pub-id></element-citation></ref><ref id="R67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taylor</surname><given-names>MS</given-names></name><name><surname>Ponting</surname><given-names>CP</given-names></name><name><surname>Copley</surname><given-names>RR</given-names></name></person-group><article-title>Occurrence and consequences of coding sequence insertions and deletions in mammalian genomes</article-title><source>Genome Research</source><year>2004</year><volume>14</volume><fpage>555</fpage><lpage>566</lpage><pub-id pub-id-type="pmcid">PMC383299</pub-id><pub-id pub-id-type="pmid">15059996</pub-id><pub-id pub-id-type="doi">10.1101/gr.1977804</pub-id></element-citation></ref><ref id="R68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Torada</surname><given-names>L</given-names></name><etal/></person-group><article-title>ImaGene: a convolutional neural network to quantify natural selection from genomic data</article-title><source>BMC Bioinformatics</source><year>2019</year><volume>20</volume><fpage>337</fpage><pub-id pub-id-type="pmcid">PMC6873651</pub-id><pub-id pub-id-type="pmid">31757205</pub-id><pub-id pub-id-type="doi">10.1186/s12859-019-2927-x</pub-id></element-citation></ref><ref id="R69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Raskin</surname><given-names>L</given-names></name><name><surname>Samuels</surname><given-names>DC</given-names></name><name><surname>Shyr</surname><given-names>Y</given-names></name><name><surname>Guo</surname><given-names>Y</given-names></name></person-group><article-title>Genome measures used for quality control are dependent on gene function and ancestry</article-title><source>Bioinformatics</source><year>2015</year><volume>31</volume><fpage>318</fpage><lpage>323</lpage><pub-id pub-id-type="pmcid">PMC4308666</pub-id><pub-id pub-id-type="pmid">25297068</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btu668</pub-id></element-citation></ref><ref id="R70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weber</surname><given-names>CC</given-names></name><name><surname>Perron</surname><given-names>U</given-names></name><name><surname>Casey</surname><given-names>D</given-names></name><name><surname>Yang</surname><given-names>Z</given-names></name><name><surname>Goldman</surname><given-names>N</given-names></name></person-group><article-title>Ambiguity coding allows accurate inference of evolutionary parameters from alignments in an aggregated state-space</article-title><source>Systematic Biology</source><year>2020</year><volume>70</volume><fpage>21</fpage><lpage>32</lpage><pub-id pub-id-type="pmcid">PMC7744038</pub-id><pub-id pub-id-type="pmid">32353118</pub-id><pub-id pub-id-type="doi">10.1093/sysbio/syaa036</pub-id></element-citation></ref><ref id="R71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>Q</given-names></name><name><surname>Dunbrack</surname><given-names>RLJ</given-names></name></person-group><article-title>The role of balanced training and testing data sets for binary classifiers in bioinformatics</article-title><source>PLoS ONE</source><year>2013</year><volume>8</volume><elocation-id>e67863</elocation-id><pub-id pub-id-type="pmcid">PMC3706434</pub-id><pub-id pub-id-type="pmid">23874456</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0067863</pub-id></element-citation></ref><ref id="R72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wisotsky</surname><given-names>SR</given-names></name><name><surname>Pond Kosakovsky</surname><given-names>SL</given-names></name><name><surname>Shank</surname><given-names>SD</given-names></name><name><surname>Muse</surname><given-names>SV</given-names></name></person-group><article-title>Synonymous site-to-site substitution rate variation dramatically inflates false positive rates of selection analyses: ignore at your own peril</article-title><source>Molecular Biology and Evolution</source><year>2020</year><volume>37</volume><fpage>2430</fpage><lpage>2439</lpage><pub-id pub-id-type="pmcid">PMC7403620</pub-id><pub-id pub-id-type="pmid">32068869</pub-id><pub-id pub-id-type="doi">10.1093/molbev/msaa037</pub-id></element-citation></ref><ref id="R73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wong</surname><given-names>KM</given-names></name><name><surname>Suchard</surname><given-names>MA</given-names></name><name><surname>Huelsenbeck</surname><given-names>JP</given-names></name></person-group><article-title>Alignment uncertainty and genomic analysis</article-title><source>Science</source><year>2008</year><volume>319</volume><fpage>473</fpage><lpage>476</lpage><pub-id pub-id-type="pmid">18218900</pub-id></element-citation></ref><ref id="R74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wong</surname><given-names>WS</given-names></name><name><surname>Yang</surname><given-names>Z</given-names></name><name><surname>Goldman</surname><given-names>N</given-names></name><name><surname>Nielsen</surname><given-names>R</given-names></name></person-group><article-title>Accuracy and power of statistical methods for detecting adaptive evolution in protein coding sequences and for identifying positively selected sites</article-title><source>Genetics</source><year>2004</year><volume>168</volume><fpage>1041</fpage><lpage>1051</lpage><pub-id pub-id-type="pmcid">PMC1448811</pub-id><pub-id pub-id-type="pmid">15514074</pub-id><pub-id pub-id-type="doi">10.1534/genetics.104.031153</pub-id></element-citation></ref><ref id="R75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>Z</given-names></name></person-group><article-title>Likelihood ratio tests for detecting positive selection and application to primate lysozyme evolution</article-title><source>Molecular Biology and Evolution</source><year>1998</year><volume>15</volume><fpage>568</fpage><lpage>573</lpage><pub-id pub-id-type="pmid">9580986</pub-id></element-citation></ref><ref id="R76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>Z</given-names></name></person-group><article-title>PAML 4: Phylogenetic Analysis by Maximum Likelihood</article-title><source>Molecular Biology and Evolution</source><year>2007</year><volume>24</volume><fpage>1586</fpage><lpage>1591</lpage><pub-id pub-id-type="pmid">17483113</pub-id></element-citation></ref><ref id="R77"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>Z</given-names></name></person-group><source>Molecular Evolution: a Statistical Approach</source><publisher-name>Oxford University Press</publisher-name><publisher-loc>Oxford, UK</publisher-loc><year>2014</year></element-citation></ref><ref id="R78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>Z</given-names></name><name><surname>Goldman</surname><given-names>N</given-names></name><name><surname>Nielsen</surname><given-names>R</given-names></name></person-group><article-title>In defense of statistical methods for detecting positive selection</article-title><source>Proceedings of the National Academy of Sciences U.S.A</source><year>2009</year><volume>106</volume><fpage>E95</fpage><pub-id pub-id-type="pmcid">PMC2741286</pub-id><pub-id pub-id-type="pmid">19805241</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0904550106</pub-id></element-citation></ref><ref id="R79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>Z</given-names></name><name><surname>Nielsen</surname><given-names>R</given-names></name></person-group><article-title>Codon-substitution models for detecting molecular adaptation at individual sites along specific lineages</article-title><source>Molecular Biology and Evolution</source><year>2002</year><volume>19</volume><fpage>908</fpage><lpage>917</lpage><pub-id pub-id-type="pmid">12032247</pub-id></element-citation></ref><ref id="R80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>Z</given-names></name><name><surname>Nielsen</surname><given-names>R</given-names></name><name><surname>Goldman</surname><given-names>N</given-names></name><name><surname>Pedersen</surname><given-names>A-MK</given-names></name></person-group><article-title>Codon-substitution models for heterogeneous selection pressure at amino acid sites</article-title><source>Genetics</source><year>2000</year><volume>155</volume><fpage>431</fpage><lpage>449</lpage><pub-id pub-id-type="pmcid">PMC1461088</pub-id><pub-id pub-id-type="pmid">10790415</pub-id><pub-id pub-id-type="doi">10.1093/genetics/155.1.431</pub-id></element-citation></ref><ref id="R81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>Z</given-names></name><name><surname>Wong</surname><given-names>WS</given-names></name><name><surname>Nielsen</surname><given-names>R</given-names></name></person-group><article-title>Bayes empirical Bayes inference of amino acid sites under positive selection</article-title><source>Molecular Biology and Evolution</source><year>2005</year><volume>22</volume><fpage>1107</fpage><lpage>1118</lpage><pub-id pub-id-type="pmid">15689528</pub-id></element-citation></ref><ref id="R82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Nielsen</surname><given-names>R</given-names></name><name><surname>Yang</surname><given-names>Z</given-names></name></person-group><article-title>Evaluation of an improved branch-site likelihood method for detecting positive selection at the molecular level</article-title><source>Molecular Biology and Evolution</source><year>2005</year><volume>22</volume><fpage>2472</fpage><lpage>2479</lpage><pub-id pub-id-type="pmid">16107592</pub-id></element-citation></ref><ref id="R83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zou</surname><given-names>Z</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Guan</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name></person-group><article-title>Deep residual neural networks resolve quartet molecular phylogenies</article-title><source>Molecular Biology and Evolution</source><year>2020</year><volume>37</volume><fpage>1495</fpage><lpage>1507</lpage><pub-id pub-id-type="pmcid">PMC8453599</pub-id><pub-id pub-id-type="pmid">31868908</pub-id><pub-id pub-id-type="doi">10.1093/molbev/msz307</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>OmegaAI model architecture.</title><p>After sequence evolution is simulated along an 8-taxon tree and aligned to result in an MSA (top), the MSA is converted into a one-hot encoded tensor in which each nucleotide has a binary representation in the depth dimension (middle). Batches of these tensors are fed to the CNN in order to train it. There are 6 convolution layers followed by global average pooling, a fully connected layer and finally a dense sigmoid layer, resulting in a single output of 1 (positive selection) or 0 (no selection) (bottom). Each convolution block consists of convolutions followed by batch normalisation, drop out regularisation and average pooling. CNN architecture diagram was created using NN-SVG (<xref ref-type="bibr" rid="R32">LeNail, 2019</xref>).</p></caption><graphic xlink:href="EMS200373-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Graphical summary of the OmegaAI training and testing workflows.</title><p>The workflow to train an OmegaAI model is shown in green. Sequence evolution is simulated along a phylogenetic tree under positive or purifying/neutral selection using INDELible, then typically aligned using Clustal. These alignments are one-hot encoded and collated into TFrecord format for efficient batching. The OmegaAI model is trained for 50 epochs. Sequences for the testing workflow (lilac) are simulated under the same parameters as training, and aligned using four aligners: Clustal, MAFFT, PRANKaa and PRANKc. These are also similarly transformed for testing the OmegaAI model, or passed directly in fasta format to CODEML (yellow). Binary classifier metrics ACC, TP, TN, FP, FN, AUC are calculated for both methods and then compared. See main text for full details.</p></caption><graphic xlink:href="EMS200373-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>OmegaAI vs. CODEML ROC and precision-recall curves for baseline parameters.</title><p><bold>A:</bold> ROC curves for OmegaAI (solid line) and CODEML (dashed). The red dot on the solid line represents the OmegaAI result at the 0.5 decision threshold used throughout this work. The red dot on the dashed line represents the 0.95 p-value threshold typically used for determining positive selection with CODEML. The grey dots indicate results for p-value thresholds of 0.99 (left) and 0.9 (right). <bold>B:</bold> Precision-recall curves for OmegaAI (solid) and CODEML (dashed). Red and grey dots are as in (A). <bold>C:</bold> ROC curves when the same OmegaAI model as in (A–B), trained exclusively on Clustal alignments, is used to evaluate simulated test data that has been aligned with four different aligners: Clustal (as in A–B), MAFFT, PRANKaa and PRANKc. This test data is evaluated by both OmegaAI (solid lines) and CODEML (dashed lines). <bold>D:</bold> As in (C), now showing precision-recall curves.</p></caption><graphic xlink:href="EMS200373-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>OmegaAI vs. CODEML across divergences.</title><p>Various binary classifier performance metrics are presented to compare the two methods. In the left column the accuracy (defined as the number of correct predictions divided by the total number of predictions), TPR and FPR are calculated using thresholds of 0.5 and p=0.95 for OmegaAI and CODEML, respectively. In the right column AUC values for ROC and precision-recall are presented. The x-axis refers to the divergence scaling of branches of the 8-taxon symmetric tree (<xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 1</xref>) used for simulation. The baseline indel rate, 0.1, is used throughout. Each OmegaAI model is individually trained for each divergence using Clustal alignments. The test data is aligned by the four different aligners indicated and tested by both methods. Baseline results that were shown in <xref ref-type="fig" rid="F3">Figure 3</xref> are therefore now points shown at the relative divergence of 0.2.</p></caption><graphic xlink:href="EMS200373-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>OmegaAI(PRANKc) vs. OmegaAI(Clustal).</title><p>Training data was simulated under baseline parameters for both models. See main text for details of training regimes for OmegaAI(PRANKc) vs. OmegaAI(Clustal). Both models were tested on the same test data, which were aligned using four aligners. ROC (left) and precision-recall (right) curves are shown to illustrate the methods’ performance.</p></caption><graphic xlink:href="EMS200373-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><title>Analysing true alignments.</title><p>ROC and precision-recall curves are shown to illustrate different methods’ performances. Lines labelled ‘OmegaAI(Clustal)’ refer to the OmegaAI model that has been exclusively trained on Clustal alignments. ‘OmegaAI(true)’ indicates training was on true alignments. Both are trained using data simulated under our baseline parameters. Both CNN models and CODEML are tested using true alignments, where the same test dataset is used for all methods.</p></caption><graphic xlink:href="EMS200373-f006"/></fig><fig id="F7" position="float"><label>Figure 7</label><caption><title>Comparing OmegaAI models with and without row shuffling.</title><p>CNN models are trained with baseline parameters (divergence 0.2, indel rate 0.1). In OmegaAI(shuffled), MSA rows are randomly shuffled in both training and test data to remove some tree topology information. ROC (left) and precision-recall (right) curves are shown to illustrate the methods’ performance. CODEML represented by dashed lines for comparison; OmegaAI and CODEML results are as in <xref ref-type="fig" rid="F3">Figure 3</xref>.</p></caption><graphic xlink:href="EMS200373-f007"/></fig><fig id="F8" position="float"><label>Figure 8</label><caption><title>OmegaAI divergence generalisation.</title><p><bold>Left column:</bold> The single OmegaAI model trained with trees of divergence scaling 0.5 is tested on data from trees of each divergence level in the set (bold lines), and compared with multiple standard OmegaAI models retrained and tested for each individual divergence, as in Figure 4 (semi-transparent lines). The accuracy and TPR are computed using a decision threshold of 0.5. ROC AUC gives a measure of performance across possible thresholds. <bold>Right column:</bold> A single OmegaAI model trained on 1,000,000 trees containing an equal number from each divergence level in the set. It is tested across divergences (bold) and compared to the standard OmegaAI model retrained and tested for each divergence level (semi-transparent). The accuracy and TPR result from a decision threshold of 0.5 and ROC AUC is shown to illustrate performance independently of threshold choice.</p></caption><graphic xlink:href="EMS200373-f008"/></fig><fig id="F9" position="float"><label>Figure 9</label><caption><title>Saliency map for an MSA of protein-coding sequences evolving under positive selection, compared with the MSA and CODEML sitewise predictions of positive selection.</title><p><bold>Top panel:</bold> CODEML sitewise predictions of positive selection. Following significant results from both M1a/M2a and M7/M8 LRTs (as described in <xref ref-type="sec" rid="S21">Methods</xref>), a Bayes empirical Bayes approach is used to calculate the posterior probability that each site is from a particular site class. Inferences of sites belonging to site classes with <italic>ω</italic> &gt; 1 with <bold><italic>p</italic></bold> ≽ 0.95 or <bold><italic>p</italic></bold> ≽ 0.99 are shown in grey and red bars, respectively. <bold>Middle panel:</bold> Colour coded Clustal alignment. Blue, yellow and red sites indicate the three site classes described in the Methods, with red indicating sites with <italic>ω</italic> &gt; 1. Grey indicates gaps in the alignment. <bold>Bottom panel:</bold> Saliency map, with bright regions (high saliency) showing the regions of the MSA that are most influential for OmegaAI’s classification. <bold>Vertical lines:</bold> The vertical line labelled TP-TP is an example alignment location of both CODEML and OmegaAI correctly identifying a site undergoing positive selection. FN-TP is an example of CODEML failing to detect a site undergoing positive selection whereas OmegaAI correctly detects positive selection at this location. Further examples are highlighted in <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 11</xref>.</p></caption><graphic xlink:href="EMS200373-f009"/></fig><fig id="F10" position="float"><label>Figure 10</label><caption><title>OmegaAI vs CODEML resource usage.</title><p>These results are from 2,000 test MSAs (simulated under baseline parameters) sequentially evaluated by OmegaAI and CODEML. OmegaAI results are given for two scenarios: one where there is only a single CPU available and the other where both a CPU and GPU are utilised. OmegaAI requires the test data to be in TFrecord format (see Methods) before analysis and measurements presented here exclude the minimal resources required for pre-processing into this condensed format.</p></caption><graphic xlink:href="EMS200373-f010"/></fig></floats-group></article>