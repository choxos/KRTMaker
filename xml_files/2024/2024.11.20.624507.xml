<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS200694</article-id><article-id pub-id-type="doi">10.1101/2024.11.20.624507</article-id><article-id pub-id-type="archive">PPR943911</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Differential Engagement of Associative-Limbic and Sensorimotor Regions of the Cerebellum and Basal Ganglia in Explicit vs. Implicit Emotional Processing</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Ceravolo</surname><given-names>Leonardo</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Thomasson</surname><given-names>Marine</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Constantin</surname><given-names>Ioana Medeleine</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Chassot</surname><given-names>Émilie</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Pierce</surname><given-names>Jordan</given-names></name><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Cionca</surname><given-names>Alexandre</given-names></name><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Grandjean</surname><given-names>Didier</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Assal</surname><given-names>Frédéric</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A7">7</xref></contrib><contrib contrib-type="author"><name><surname>Péron</surname><given-names>Julie</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A4">4</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>Neuroscience of Emotion and Affective Dynamics Laboratory, Department of Psychology and Swiss Centre for Affective Sciences, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01swzsf04</institution-id><institution>University of Geneva</institution></institution-wrap>, <country country="CH">Switzerland</country></aff><aff id="A2"><label>2</label>Cognitive Neurology Unit, Department of Neurology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01m1pv723</institution-id><institution>University Hospitals of Geneva</institution></institution-wrap>, <city>Geneva</city>, <country country="CH">Switzerland</country></aff><aff id="A3"><label>3</label>School of Physical and Occupational Therapy, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01pxwe438</institution-id><institution>McGill University</institution></institution-wrap>, <city>Montréal, QC</city>, <country country="CA">Canada</country></aff><aff id="A4"><label>4</label>Clinical and Experimental Neuropsychology Laboratory, Department of Psychology and Swiss Centre for Affective Sciences, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01swzsf04</institution-id><institution>University of Geneva</institution></institution-wrap>, <country country="CH">Switzerland</country></aff><aff id="A5"><label>5</label>Cognitive and Affective Neuroscience Lab, Department of Psychology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/043mer456</institution-id><institution>University of Nebraska-Lincoln</institution></institution-wrap>, <country country="US">USA</country></aff><aff id="A6"><label>6</label>Medical Image Processing Laboratory, Neuro-X Institute, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02s376052</institution-id><institution>École Polytechnique Fédérale De Lausanne</institution></institution-wrap> (EPFL), <city>Geneva</city>, <country country="CH">Switzerland</country></aff><aff id="A7"><label>7</label>Faculty of Medicine, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01swzsf04</institution-id><institution>University of Geneva</institution></institution-wrap>, <country country="CH">Switzerland</country></aff><author-notes><corresp id="CR1"><label>*</label>Corresponding author: Professor Julie Péron, Clinical and Experimental Neuropsychology Laboratory, Faculté de Psychologie et des Sciences de l’Education, Université de Genève, 40 bd du Pont d’Arve, 1205 Geneva, Switzerland. Tel.: +4122 379 94 55 ; <email>julie.peron@unige.ch</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>23</day><month>11</month><year>2024</year></pub-date><pub-date pub-type="preprint"><day>21</day><month>11</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nd/4.0/">CC BY-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Emotional prosody processing involves multiple brain regions, but the specific roles of the cerebellum and basal ganglia in explicit (conscious) and implicit (incidental) tasks are not well known or understood. This study investigated how the cerebellum and basal ganglia contribute to explicit (emotion categorization) and implicit (gender categorization) processing of emotional prosody. Twenty-eight healthy French-speaking participants underwent high-resolution functional MRI while performing a vocal emotion processing task under such implicit and explicit conditions. Behavioral data analyses indicated greater accuracy in the gender discrimination task (implicit processing). Neuroimaging partially supported our hypothesis according to which explicit emotion processing yielded increased activations in associative-limbic regions (e.g., inferior frontal gyrus, Crus I and caudate) linked to higher-order functions, while implicit emotion processing engaged sensorimotor regions (primary motor cortex, primary somatosensory cortex) and areas associated with automatic processing (putamen, posterior insula, cerebellar lobules VIIIa-b and IX). Unexpected activity during task conditions suggest motor preparation effects and more complex brain network dynamics. These results challenge modular views of brain function and highlight the need to consider emotional processing as complex, dynamic, network-based interactions.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Recent studies about non-motor roles of the basal ganglia and cerebellum provide a holistic view of the neural basis of emotional stimuli processing (<xref ref-type="bibr" rid="R33">Manto et al., 2024</xref>). At the behavioral level, it suggests that organisms must not only respond to their current conditions but also anticipate future changes in their environment to survive and evolve effectively, by constructing what are known as internal models (<xref ref-type="bibr" rid="R29">Koziol et al., 2014</xref>). These predictions are then compared with the outcomes of their actions. Such predictive mechanisms allow organisms to prepare adaptive responses even before environmental changes occur. Neuroanatomically, studies suggested that the cerebellum is involved in this form of predictive adaptation, especially in relation to the temporal structure of unfolding events (<xref ref-type="bibr" rid="R10">Cabaraux et al., 2020</xref>; <xref ref-type="bibr" rid="R28">Ito, 2008</xref>). Over time, experiences are grouped into functional units known as "chunks" (<xref ref-type="bibr" rid="R25">Graybiel, 1998</xref>). These chunks are cognitive structures that facilitate the processing and management of information. By consolidating internal models and creating chunks, organisms establish habits that reduce the need for conscious processing in routine tasks (<xref ref-type="bibr" rid="R40">Pierce &amp; Péron, 2020</xref>). It is suggested that this metacognitive function applies to sensorimotor, cognitive, but also emotional processes (<xref ref-type="bibr" rid="R33">Manto et al., 2024</xref>) explaining why neuroanatomical (<xref ref-type="bibr" rid="R1">Adamaszek et al., 2017</xref>; <xref ref-type="bibr" rid="R47">Schutter &amp; Van Honk, 2005</xref>), cerebellar patient observation and clinical studies (<xref ref-type="bibr" rid="R51">Thomasson et al., 2022</xref>) have suggested the involvement of the cerebellum in emotional processing (<xref ref-type="bibr" rid="R12">Ceravolo et al., 2021</xref>; <xref ref-type="bibr" rid="R50">Thomasson et al., 2021</xref>). The shift from a goal-directed and controlled mode to a habitual mode is supported by the transition from the associative to the sensorimotor pathway (<xref ref-type="bibr" rid="R26">Graybiel, 2008</xref>). The associative-limbic loops include the associative and limbic parts of the cerebral cortex, as well as associative and limbic parts of the basal ganglia and cerebellum (notably, the caudate nucleus, ventral striatum, ventral part of the subthalamic nucleus, dorsomedial globus pallidus, lateral cerebellar hemispheres: Crus I-II, as well as the fastigial and dentate nuclei) (<xref ref-type="bibr" rid="R27">Habas et al., 2009</xref>). The sensorimotor loop includes the sensorimotor, somatosensory and motor parts of the cerebral cortex, as well as the sensorimotor parts of the basal ganglia and cerebellum (notably, the putamen, dorsolateral globus pallidus, dorsal part of the subthalamic nucleus, anterior cerebellum, interposed nuclei, anterior part of the vermis, as well as lobules VIIIa-b) (<xref ref-type="bibr" rid="R30">Krack et al., 2010</xref>; <xref ref-type="bibr" rid="R34">Middleton &amp; Strick, 2000</xref>). In the domain of emotion processing, certain vocal patterns, such as tone or intonation used to convey emotions (i.e., emotional prosody), may shift from a consciously controlled to an automatic response, illustrating a potential transition from associative-limbic to sensorimotor circuits—though this shift in the emotional domain has yet to be empirically studied.</p><p id="P3">Exploring such a holistic proposition through the lens of emotional auditory perception appears to be a promising approach, as emotional prosody involves processing temporal structures in events. This requires both sensory brain regions for analyzing low-level acoustic features and cognitive regions for evaluative judgments (<xref ref-type="bibr" rid="R20">Frühholz et al., 2012</xref>; <xref ref-type="bibr" rid="R21">Frühholz et al., 2016</xref>; <xref ref-type="bibr" rid="R23">Grandjean, 2021</xref>; <xref ref-type="bibr" rid="R49">Steiner et al., 2022</xref>). Additionally, manipulating attentional focus during emotional prosody processing could offer a nuanced way to compare brain activity when participants consciously process emotions versus when emotion processing happens automatically or incidentally, without the need for conscious recognition of affective states. The former, known as ‘explicit emotion processing’, occurs when participants are asked to make decisions about the emotional aspects of what they hear or see. In contrast, ‘implicit emotion processing’ involves tasks in which attention is directed away from the emotional content, focusing on another aspect of the stimulus (e.g., speaker’s gender), while emotional tone remains present (<xref ref-type="bibr" rid="R4">Bach et al., 2008</xref>; <xref ref-type="bibr" rid="R24">Grandjean et al., 2005</xref>).</p><p id="P4">Therefore, it can be reasonably hypothesized that large-scale cortico-subcortico-cerebellar networks involved in sensorimotor loops (including the cerebellum and basal ganglia) would be particularly engaged during implicit emotional processing, where attention is not directly focused on the emotion itself. In contrast, explicit emotional processing, which requires detailed and conscious evaluation, should involve the cortico-subcortico-cerebellar associative-limbic loop that is crucial for integrating emotions within a goal-directed cognitive context (<xref ref-type="bibr" rid="R41">Pierce &amp; Péron, 2022</xref>). By conducting a retrospective analysis of studies that have investigated the brain bases of emotional prosody (by distinguishing between implicit and explicit processing), we can observe that literature aligns with this perspective, even if reported results were not originally interpreted within this theoretical framework. <xref ref-type="bibr" rid="R18">Ethofer et al. (2009)</xref> highlighted that explicit tasks tend to activate the associative regions of the basal ganglia, such as the caudate nucleus and the anterior putamen. <xref ref-type="bibr" rid="R4">Bach et al. (2008)</xref> showed differential activations within the basal ganglia, where the associative parts are more active during tasks that require explicit judgments of emotional content. <xref ref-type="bibr" rid="R54">Wildgruber et al. (2005)</xref> found that explicit emotional tasks activated not only cortical areas but also associative regions of the basal ganglia, supporting the hypothesis that these regions play a role in tasks requiring conscious evaluation of emotional stimuli. Interestingly, a recent meta-analysis (<xref ref-type="bibr" rid="R42">Pierce et al., 2023</xref>) reported for explicit processing of emotional stimuli that the right cerebellar lobule VI, left vermis/lobule VI, and left lobule VI were more specifically involved. In contrast, implicit processing was processed indirectly and showed a specific activation in a small cluster in the left cerebellar lobule VI.</p><p id="P5">With the abovementioned literature in mind, we hypothesized that activity within the cerebellum and the basal ganglia would be sensitive to the level of processing required by the task—here, explicit and implicit vocal emotion processing tasks. Specifically, we expected that associative-limbic regions (e.g., the caudate nucleus, ventral striatum, ventral part of the subthalamic nucleus, dorsomedial globus pallidus, lateral cerebellar hemispheres: Crus I-II, as well as the fastigial and dentate nuclei) would be more active during explicit emotional processing, while sensorimotor-related regions (e.g., the putamen, dorsolateral globus pallidus, dorsal part of the subthalamic nucleus, anterior cerebellum, interposed nuclei, anterior part of the vermis, as well as lobules VIIIa-b) would be predominantly engaged during implicit emotional processing. We tested this hypothesis by using high-resolution fMRI on twenty-eight healthy participants to explore the involvement of specific parts of the cerebellum and basal ganglia, as well as their relationship with cortical networks involved in goal-directed (explicit) versus habit-driven (implicit) emotional prosody processing.</p></sec><sec id="S2" sec-type="materials | methods"><title>Materials and Methods</title><sec id="S3"><title>Participants</title><p id="P6">Twenty-eight healthy participants were recruited for the study. They were all French speakers with a mean age of 65.4 years (SE = 8.9, range = 48-77). According to the Edinburgh Handedness Inventory criteria (<xref ref-type="bibr" rid="R37">Oldfield, 1971</xref>), 26 participants were right-handed and 2 were left-handed. Their mean education level was 16.4 years (SE = 3.0, range = 12-22). Exclusion criteria for the recruitment were: 1) history of neurological disorders, 2) head trauma, 3) anoxia, stroke or major cognitive deterioration, as attested by their score on the Montreal Cognitive Assessment (<xref ref-type="bibr" rid="R36">Nasreddine &amp; Patel, 2016</xref>) (mean score = 28, SD = 1.3, range = 25-30). Moreover, none of them wore hearing aids or had a history of tinnitus or a hearing impairment, as attested either by their PEGA score (mean = 28.7, SD = 1.4, range = 25-30). All participants gave their written informed consent and the study was approved by the local ethics committee. Using a two-tailed testing method with an effect size of 0.5, an alpha of 0.05 and our sample of N=28, we obtain a power of 72.27% as calculated in G*Power 3.1.9.7 (<xref ref-type="bibr" rid="R19">Faul et al., 2007</xref>).</p></sec><sec id="S4"><title>Vocal emotion recognition task procedure</title><p id="P7">The vocal stimuli consisted of two speech-like but semantically meaningless sentences extracted from Banse and Scherer’s validated database (<xref ref-type="bibr" rid="R6">Banse &amp; Scherer, 1996</xref>). These pseudo-sentences were spoken in a neutral, happy or angry tone by two male and two female speakers, resulting in 18 different stimuli. During scanning, these binaurally recorded auditory stimuli were played through MRI-compatible headphones and displayed using E-Prime (<ext-link ext-link-type="uri" xlink:href="http://www.pstnet.com/eprime.cfm">http://www.pstnet.com/eprime.cfm</ext-link>). Participants performed a categorization task with two conditions for the Task factor (implicit, explicit) and three for the Emotion factor (anger, happiness, neutral). This paradigm was divided into six counterbalanced blocks (16 trials per block). For three of these blocks (implicit), participants had to do a gender decision (participants had to indicate “male” or “female” by pressing a key with their index or middle finger). For the other three blocks (explicit), participants performed an emotion categorization task (they had to indicate “happiness” or “neutral” / “anger” or “neutral” by pressing a key with their index or middle finger).</p><p id="P8">We assumed that during the gender decision task, participants would focus on the gender of the voice, but the emotional tone of the voice still would be processed implicitly. Before each block, a message indicating “gender” or “emotion” was displayed on the screen for 1 sec as an instruction. The vocal stimuli were then played (duration: 2.5 s) and a fixation cross was presented at the center of the screen, and the two possible responses displayed on the screen for 2 s. Each of the two runs contained all 96 trials and lasted approximatively 10 minutes while functional MRI was acquired.</p></sec><sec id="S5"><title>Behavioral data analysis</title><p id="P9">Behavioral results illustrate the probability of accurately recognizing voice gender or emotion (Task factor) as a function of the Emotion factor. These analyses were computed using mixed effects, regression analyses for the response accuracy (of-interest) and for the associated reaction times (of-no-interest) in R studio (<xref ref-type="bibr" rid="R3">Allaire, 2012</xref>).</p><p id="P10">For the ‘response’ dependent variable, we used a logistic regression using the lme4 ‘glmer’ package (<xref ref-type="bibr" rid="R8">Bates et al., 2015</xref>) with Voice emotion (Emo) interacting with Task and Run as fixed effects and participant identity (ParticipantID) and gender (ParticipantGender), stimuli, stimuli order (StimuliOrder), voice actor identity (ActorID) and Gender (ActorGender) as random factors. The model formula was the following: <disp-formula id="FD1"><mml:math id="M1"><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mo>~</mml:mo><mml:mi>E</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mo>*</mml:mo><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>G</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>O</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>G</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P11">For the ‘reaction times’ dependent variable, we used a linear regression using the lme4 ‘lmer’ package with the exact same formula as above.</p><p id="P12">For our planned comparisons, we used a confidence interval of 97.5% and a Bonferroni correction for multiple comparisons. Specific contrasts were computed using the ‘phia’ package (<xref ref-type="bibr" rid="R17">De Rosario-Martinez et al., 2015</xref>).</p><p id="P13">No difference was observed between models including or excluding participant gender as a random effect (p&gt;.1).</p></sec><sec id="S6"><title>MRI data acquisition</title><p id="P14">All functional imaging data were recorded on a 3-T Siemens Trio System (Siemens, Erlangen, Germany) scanner equipped with a 32-channel antenna. A 3D sequence was used to acquire a high-resolution T1-weighted image (1mm isotropic voxels; TR=1900msec; TE=2.27ms; matrix resolution=256 x 256; FA=9degrees; 192 total slices).</p><p id="P15">For the task, functional images were acquired in descending order using a multi-band echo-planar imaging (EPI) sequence of 54 slices aligned along the anterior-posterior commissure (voxel size: 2.5mm isotropic; TR=1300msec; TE=20ms; FOV=205 x 205mm; matrix resolution=84 x 84; FA=64degrees; BW=1952Hz/px). Functional image acquisition was continuous throughout each run of the task.</p></sec><sec id="S7"><title>MRI data analysis</title><p id="P16">Data were preprocessed using a mix of SPM12 (SPM12, Wellcome Trust Centre for Neuroimaging, London, UK), the CONN toolbox (<xref ref-type="bibr" rid="R53">Whitfield-Gabrieli &amp; Nieto-Castanon, 2012</xref>) and the Artifact detection tools (ART) toolbox (<ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/artifact_detect/">https://www.nitrc.org/projects/artifact_detect/</ext-link>). Functional data were first converted to 4D Nifti in SPM12 for each run separately to get one single file per run, for more efficient computation performance and decrease storage volume. Preprocessing steps included the following: realignment and unwarp, slice-timing correction, artifact detection followed by an additional denoising (scrubbing including functional regression and functional bandpass [0.01-0.1Hz]). At this stage, direct segmentation and normalization into the Montreal Neurological Institute (MNI) space (<xref ref-type="bibr" rid="R13">Collins et al., 1994</xref>) was performed for improved comparison between the brain tissues of our sample of participants and with regards to other existing fMRI studies. Finally, data were spatially smoothed with an isotropic Gaussian filter of 8 mm full width at half maximum. Following an initial step consisting in the creation of a participant-specific matrix including the onset and reaction times of each trial of each run, preprocessed, smoothed functional images were then analyzed with SPM12 both at the participant-level (‘first-level’) as well as at the group-level (‘second-level’). A single general linear model was used to compute first-level statistics, using a convolution with the Hemodynamic Response Function (HRF). Each event was therefore modelled using HRF function, locked to the onset of each voice stimulus using continuous MRI data acquisition. The design matrix included three columns per run representing the Emotion factor (anger, happiness, neutral) as well as six motion parameters—included as regressors of no interest to account for movement in the data—as well as trial-level reaction times, also as a regressors of no interest—since accuracy was of interest, not response times. Trials in which an evaluation error was made for each task was also included—regressors of no interest. Regressors of interest were used to compute simple contrasts for each participant, leading to separate main effects of Angry, Neutral and Happy voices for each task, for a total of six simple contrasts (Implicit task: Angry, Neutral and Happy voices; Explicit task: Angry, Neutral and Happy voices).</p><p id="P17">This model yielded two flexible factorial second-level analyses, with factors Task and Emotion. We also had the Participants factor (Factor 1 in each analysis) to consider interindividual variability, with data independence set to ‘yes’, variance to ‘unequal’ while for the within factors of interest data independence set to ‘no’, variance to ‘unequal’. Second-level models focused on: the Task factor (Model 1) and on the interaction between Task and Emotion (Model 2, Task * Emotion).</p><p id="P18">Activations were ultimately thresholded in SPM12 by using a voxel-wise FDR correction at <italic>p</italic>&lt;.05. Thresholded contrast activations were then rendered on brains from the CONN toolbox (<xref ref-type="bibr" rid="R53">Whitfield-Gabrieli &amp; Nieto-Castanon, 2012</xref>). Regions were labelled using the latest version of the ‘automated anatomical labelling’ (‘aal3’) atlas (<xref ref-type="bibr" rid="R44">Rolls et al., 2020</xref>; <xref ref-type="bibr" rid="R52">Tzourio-Mazoyer et al., 2002</xref>).</p></sec></sec><sec id="S8" sec-type="results"><title>Results</title><p id="P19">This section reports the behavioral and neuroimaging results of our sample of participants (N=28), who were asked to categorize the gender (implicit emotion processing) or the emotion (explicit emotion processing) of angry, happy or neutral voices presented to them via headphones while lying in the fMRI scanner. The explicit and implicit tasks illustrate the Task factor while the emotion expressed by each voice stimulus (anger, happiness, neutral) represents the levels of the Emotion factor. Main effects as well as partial and full interaction effects are described below.</p><sec id="S9"><title>Behavioral data</title><p id="P20">Behavioral data included one dependent variable of interest—response accuracy (<xref ref-type="fig" rid="F1">Fig.1</xref>)—and one of no-interest, namely reaction times (<xref ref-type="supplementary-material" rid="SD1">Fig.S1</xref>). These results are presented below and summarized in <xref ref-type="supplementary-material" rid="SD1">Table S1</xref>.</p><sec id="S10"><title>Response patterns and accuracy</title><p id="P21">Response data of each task were treated as ‘correct’ or ‘incorrect’ and coded as ‘1’ or ‘0’, respectively. Logistic regression was used to better characterize data and allow for the interpretation of the probability of a correct response as a function of our Task and Emotion factors (fixed effects)—including also the split into two runs, and taking into consideration relevant random effects (see Methods).</p><p id="P22">The probability of a correct response was predicted by the Task with higher accuracy for the gender task (χ<sup>2</sup>(1)=93.33, <italic>p</italic>&lt;.001), voice Emotion (χ<sup>2</sup>(2)=19,11, <italic>p</italic>&lt;.001), Run (with higher accuracy in run 2, χ<sup>2</sup>(1)=8.81, <italic>p</italic>&lt;.01) and the interaction between Task and Emotion (χ<sup>2</sup>(2)=85.73, <italic>p</italic>&lt;.001). Across tasks, neutral and angry voices yielded a higher probability of correct response compared to happy voices for the Emotion factor (χ<sup>2</sup>(1)=5.54, <italic>p</italic>&lt;.05). The interaction between Task and Emotion was mainly explained by an inverted pattern of response between tasks, in which higher accuracy probability was observed for angry compared to neutral and happy voices in the emotion discrimination task. The opposite trend occurred in the gender task paired with a general higher accuracy for the gender task ([Task Gender &gt; Task Emotion * Angry &gt; Neutral+Happy voices]: χ<sup>2</sup>(1)=84.88, <italic>p</italic>&lt;.001). Additionally, differences were observed in accurate response probability between angry vs happy voices in the Emotion task (χ<sup>2</sup>(1)=32.59, <italic>p</italic>&lt;.001)—but not in the Gender task (χ<sup>2</sup>(1)=1.74, <italic>p</italic>=.19)—and also between tasks, with higher accuracy for angry vs happy in the Gender task as opposed to the inverse in the Emotion task (χ<sup>2</sup>(1)=71.97, <italic>p</italic>&lt;.001). Response probability for angry vs neutral voices also differed in both tasks (Gender: χ<sup>2</sup>(1)=6.49, <italic>p</italic>&lt;.05; Emotion: χ<sup>2</sup>(1)=9.23, <italic>p</italic>&lt;.01) and between tasks (χ<sup>2</sup>(1)=53.46, <italic>p</italic>&lt;.001) as well as for happy vs neutral voices in the Emotion task only (χ<sup>2</sup>(1)=8.99, <italic>p</italic>&lt;.01; Gender task: χ<sup>2</sup>(1)=1.21, <italic>p</italic>&gt;.10; Gender vs Emotion task: χ<sup>2</sup>(1)=3.15, <italic>p</italic>&lt;.10). Variance explained by the fixed effects was 38.24% (R<sup>2</sup>c) while the full model including random effects (R<sup>2</sup>c) explained 61.67% of the variance of the response probability data. See <xref ref-type="fig" rid="F1">Fig.1</xref> for details and illustration.</p></sec><sec id="S11"><title>Reaction times data</title><p id="P23">Reaction times data of each task were analyzed next. Linear regression was used as a function of the Task and Emotion factors (fixed effects)—including the split into two runs, and also taking into consideration relevant random effects (see Methods). Since the tasks are rather ‘easy’ to perform and the fact that no instruction concerning response time was given to the participants, reaction times data were of no interest in the present paper, but were examined anyway for completeness.</p><p id="P24">Reaction times were predicted by the Task with faster values for the gender task (χ<sup>2</sup>(1)=19.96, <italic>p</italic>&lt;.001), voice Emotion (χ<sup>2</sup>(2)=7.03, <italic>p</italic>&lt;.05), Run (χ<sup>2</sup>(1)=23.06, <italic>p</italic>&lt;.001) and the interaction between Task and Emotion (χ<sup>2</sup>(2)=17.53, <italic>p</italic>&lt;.001). Across tasks, angry and happy voices yielded slower reaction times compared to neutral voices for the Emotion factor (χ<sup>2</sup>(1)=5.92, <italic>p</italic>&lt;.05). The interaction between Task and Emotion was again mainly explained by a partly inverted pattern of data between tasks, in which happy and neutral compared to angry voices led to slower reaction times in the emotion discrimination task while the inverse was observed in the gender task ([Task Gender &gt; Task Emotion * Angry &gt; Neutral + Happy voices]: χ<sup>2</sup>(1)=16.53, <italic>p</italic>&lt;.001). Additionally, differences were observed between angry vs happy voices with slower reaction times for happy voices in the Emotion task (χ<sup>2</sup>(1)=5.21, <italic>p</italic>&lt;.05) but not in the Gender task (χ<sup>2</sup>(1)=0.94, <italic>p&gt;</italic>.10)—but differences were also observed between tasks (slower reaction times for angry vs happy in the Gender task as opposed to the inverse in the Emotion task (χ<sup>2</sup>(1)=16.83, <italic>p</italic>&lt;.001). Reaction times were also slower for angry vs neutral voices the Gender task (χ<sup>2</sup>(1)=7.01, <italic>p</italic>&lt;.01) but not in the Emotion task (χ<sup>2</sup>(1)=0.26, <italic>p&gt;</italic>.10), and between tasks (χ<sup>2</sup>(1)=9.46, <italic>p</italic>&lt;.01) as well as slower reaction times for happy vs neutral voices in the Emotion task only (χ<sup>2</sup>(1)=8.40, <italic>p</italic>&lt;.01; Gender task: χ<sup>2</sup>(1)=2.50, <italic>p</italic>&gt;.10; Gender vs Emotion task: χ<sup>2</sup>(1)=2.56, <italic>p</italic>&lt;.10). Variance explained by the fixed effects was 31.25% (R<sup>2</sup>c) while the full model including random effects (R<sup>2</sup>c) explained 48.33% of the variance of the reaction times data. See <xref ref-type="supplementary-material" rid="SD1">Fig.S1</xref> for details and illustration.</p></sec></sec><sec id="S12"><title>Neuroimaging data</title><p id="P25">Functional images were acquired continuously during the session of each participant to assess brain correlates across tasks (explicit and implicit vocal emotion processing), emotions when contrasting the tasks (angry, happy and neutral voices for explicit &gt; implicit and implicit &gt; explicit) and the other interactions between tasks and emotions. Wholebrain voxel-wise statistics are presented below with correction for multiple tests (false-discovery rate, ‘FDR’). Since reaction times were of no-interest and participants were not instructed to respond as fast as possible, reaction times were used as a trial-level covariate of no interest in the neuroimaging statistical models. Error trials—although accounting for a very small percentage of the trials—were also concatenated per task into a regressor of no interest.</p><sec id="S13"><title>Effects of the type of task (explicit, implicit emotion processing)</title><p id="P26">Both tasks were designed to highlight and analyzed with respect to the levels of vocal emotion processing: focusing on voice gender or on expressed emotion to implicitly or explicitly process vocal emotion, respectively.</p><p id="P27">The contrast between explicit vs implicit processing of vocal emotions showed enhanced activations in the lateral temporal and inferior frontal cortex, especially in the STG, STS and anterior insula (<xref ref-type="fig" rid="F2">Fig.2AC</xref>) and in all subparts of the IFG (<xref ref-type="fig" rid="F2">Fig.2ABCH</xref>). Additional activations were observed in the pre-supplementary motor area (preSMA, <xref ref-type="fig" rid="F2">Fig.2GI</xref>), the basal ganglia (caudate nucleus, putamen, <xref ref-type="fig" rid="F2">Fig.2M</xref>), the thalamus and in widely spread regions of the cerebellum (Crus I and II, <xref ref-type="fig" rid="F2">Fig.2N</xref>). See also <xref ref-type="supplementary-material" rid="SD1">Table S2</xref>.</p><p id="P28">In the inverse contrast, implicit vs explicit vocal emotion processing, we observed a very distinct network of activations involving the primary motor and somatosensory cortices (<xref ref-type="fig" rid="F2">Fig.2DEF</xref>), the ventromedial (<xref ref-type="fig" rid="F2">Fig.2JL</xref>) and dorsolateral (<xref ref-type="fig" rid="F2">Fig.2DEF</xref>) prefrontal cortices, the orbitofrontal cortex (<xref ref-type="fig" rid="F2">Fig.2K</xref>), the posterior insula (<xref ref-type="fig" rid="F2">Fig.2DF</xref>) as well as the parahippocampal gyrus (<xref ref-type="fig" rid="F2">Fig.2K</xref>), basal ganglia (putamen, caudate nucleus; <xref ref-type="fig" rid="F2">Fig.2O</xref>) and the cerebellar lobules IX, VIIIa-b and Crus II (<xref ref-type="fig" rid="F2">Fig.2P</xref>). See <xref ref-type="supplementary-material" rid="SD1">Table S3</xref>.</p></sec><sec id="S14"><title>Task effects for each emotion (angry, happy, neutral voices)</title><p id="P29">We then contrasted tasks and looked at each emotion individually. These results are overlaid in <xref ref-type="fig" rid="F3">Fig.3</xref> (<xref ref-type="supplementary-material" rid="SD1">Table S4</xref>) for the Explicit &gt; Implicit contrast and reported in <xref ref-type="supplementary-material" rid="SD1">Fig.S2-4</xref> (<xref ref-type="supplementary-material" rid="SD1">Tables S5-7</xref>) for the Implicit &gt; Explicit contrast.</p><p id="P30">When contrasting Explicit to Implicit emotion processing, we observed a large overlap for angry, happy and neutral voices, especially in the bilateral inferior frontal cortex <italic>pars triangularis</italic> (<xref ref-type="fig" rid="F3">Fig.3A-G</xref>), bilateral pre-supplementary motor area (<xref ref-type="fig" rid="F3">Fig.3H-J, L-N</xref>) and in the cerebellum—specifically in the right Crus I and cerebellar lobule VI (<xref ref-type="fig" rid="F3">Fig.3O-Q</xref>). Specific overlaps between angry and neutral voices for this contrast were observed in the bilateral STG and STS (<xref ref-type="fig" rid="F3">Fig.3ACFG</xref>), orbitofrontal cortex (<xref ref-type="fig" rid="F3">Fig.3K</xref>), right Crus II (<xref ref-type="fig" rid="F3">Fig.3O-Q</xref>) and in the basal ganglia (caudate nucleus, <xref ref-type="fig" rid="F3">Fig.3K</xref>). Overlaps shared by happy and neutral voices in this contrast were mainly observed in the left IFG (<xref ref-type="fig" rid="F3">Fig.3BC</xref>), preSMA (<xref ref-type="fig" rid="F3">Fig.3HI, LM</xref>) and in the cerebellum, especially in the left hemisphere Crus I and Crus II and in the vermis Crus II (<xref ref-type="fig" rid="F3">Fig.3O-Q</xref>).</p><p id="P31">Other results pertaining to the Task * Emotion interactions—happy vs neutral voices in both tasks as well as angry vs happy (and inverse) in the explicit task—are reported in <xref ref-type="supplementary-material" rid="SD1">Fig.S5</xref> (<xref ref-type="supplementary-material" rid="SD1">Tables S8</xref> and <xref ref-type="supplementary-material" rid="SD1">S9</xref>) and in <xref ref-type="supplementary-material" rid="SD1">Fig.S6</xref> (<xref ref-type="supplementary-material" rid="SD1">Tables S10</xref> and <xref ref-type="supplementary-material" rid="SD1">S11</xref>), respectively.</p></sec></sec></sec><sec id="S15" sec-type="discussion"><title>Discussion</title><p id="P32">The present study partially sustains the hypothesis that the level of emotional processing required by a task influences brain activation patterns in regions within the cerebellum, basal ganglia, and associated cortical networks. Activation in some other brain areas are however challenging the present hypotheses. Based on our detailed interpretation below, these results are not mutually exclusive but provide further evidence that cognitive and emotional processes arise from the dynamic interplay of large-scale brain networks, where the cerebellum and basal ganglia play roles as crucial as the cerebral cortex, amygdala, or other widely recognized regions of the "affective brain" (<xref ref-type="bibr" rid="R16">Damasio, 1994</xref>; <xref ref-type="bibr" rid="R32">LeDoux, 1998</xref>).</p><p id="P33">The results for the explicit task showed enhanced activations in associative-limbic regions, such as the lateral temporal cortex, including the superior temporal gyrus (STG) and superior temporal sulcus (STS) and the inferior frontal cortex, specifically the inferior frontal gyrus (IFG). Additionally, activations were observed in the cerebellum, including Crus I, Crus II, and the vermis, as well as the basal ganglia, notably the caudate nucleus, and the thalamus. These areas are associated with complex emotional processing (<xref ref-type="bibr" rid="R31">Kringelbach &amp; Rolls, 2004</xref>), decision-making (<xref ref-type="bibr" rid="R9">Bechara et al., 2000</xref>), and social cognition (<xref ref-type="bibr" rid="R2">Adolphs, 2009</xref>), which aligns with the idea that they are more engaged when explicit emotional processing is required. The results for the implicit task showed activations in sensorimotor regions, such as the primary motor cortex (M1), primary somatosensory cortex (S1), and the cerebellum, specifically lobules IX and VIIIa-b. Additionally, activations were observed in the basal ganglia, including the putamen, and in regions involved in automatic processing, such as the posterior insula (<xref ref-type="bibr" rid="R5">Bamiou et al., 2003</xref>) and precuneus (<xref ref-type="bibr" rid="R11">Cavanna &amp; Trimble, 2006</xref>). This fits with the hypothesis that sensorimotor-related regions are more engaged during implicit tasks where the emotional content of the voice is not the focus but is processed incidentally (<xref ref-type="bibr" rid="R38">Pessoa &amp; Adolphs, 2010</xref>) and / or when integrating sensory and emotional information (<xref ref-type="bibr" rid="R15">Craig, 2009</xref>). This task-dependent engagement may reflect the brain's adaptive mechanism for managing emotional information efficiently. For explicit tasks, the engagement of higher-order cognitive and associative regions facilitates deliberate analysis and conscious decision-making. For implicit tasks, the engagement of sensorimotor and more automatic processing regions suggests a streamlined pathway for handling emotional information when the primary task does not demand conscious focus on emotions. This distinction enhances our understanding of how different types of emotional processing can be integrated into broader cognitive processes and how emotional ‘chunk-like’ habits can develop in response to repeated experiences. Such habits could be particularly relevant in real-life scenarios where emotional information is processed in the background while attention is focused on another task.</p><p id="P34">Some results are however surprising with respect to our formulated hypotheses. First, results also showed activations in sensorimotor regions for explicit processing. For example, the "explicit &gt; implicit" contrast shows activations in the pre-supplementary motor area (preSMA), which is unexpected as these regions should be less active during explicit processing (<xref ref-type="bibr" rid="R22">Fuster, 2008</xref>). Those activations are, however, coherent with a higher involvement of such regions in the embodiment of vocal happiness, since preSMA underlies its production (<xref ref-type="bibr" rid="R48">Selosse et al., 2024</xref>). This could also be due to motor preparation or response mechanisms (<xref ref-type="bibr" rid="R35">Nachev et al., 2008</xref>; <xref ref-type="bibr" rid="R39">Picard &amp; Strick, 1996</xref>) associated with the explicit task. Participants might have used motor strategies or mental imagery processes that activate motor areas, even when the task requires explicit emotional processing. Second, we observed activations spread across several cerebellar regions (Crus I and II for explicit, lobules IX, VIIIa-b, and Crus II for implicit) without a clear distinction between associative-limbic and sensorimotor regions. The cerebellum is known to be involved in a wide range of cognitive and emotional functions (<xref ref-type="bibr" rid="R46">Schmahmann, 2019</xref>), and such diffuse cerebellar activations may reflect an integration of these functions rather than a segregation between associative-limbic and sensorimotor functions. This is in accordance with previous literature that has shown a cerebellar involvement in both the sensory and cognitive processing of emotions, suggesting a broader metacognitive role of this structure (<xref ref-type="bibr" rid="R40">Pierce &amp; Péron, 2020</xref>). Another possibility is that the sensitivity of sensorimotor processes to the implicit and explicit conditions may not be sufficient. Third, some cortical (lateral occipital cortex, superior parietal cortex) and subcortical regions (caudate nucleus) showed significant activations in both types of tasks (explicit and implicit). While these activations are not clearly explained by the hypothesis of differentiation between associative-limbic and sensorimotor networks, they might represent additional attentional, cognitive control, or perceptual processing required by the emotional stimuli used in both tasks, rather than strictly emotional processes (<xref ref-type="bibr" rid="R14">Corbetta &amp; Shulman, 2002</xref>). They might also suggest that the stimuli have multisensory or complex characteristics that require integration into broader sensory and attentional networks.</p><p id="P35">According to dynamic systems theories, brain activity reflects smooth transitions between different network states rather than static and isolated processing modules (<xref ref-type="bibr" rid="R7">Bassett &amp; Sporns, 2017</xref>). Widespread activation in cerebellar regions without a clear distinction between associative-limbic and sensorimotor regions suggest that the cerebellum plays a more integrated and context-dependent role in emotional processing (<xref ref-type="bibr" rid="R45">Schmahmann, 1997</xref>).</p><p id="P36">This supports the idea that brain functions are not segregated but rather that different cerebellar regions can dynamically engage depending on the specific demands of the task, thereby integrating cognitive, emotional, and sensorimotor processing. Classical cognitive theories often propose a dichotomy between explicit (associative-limbic) and implicit (sensorimotor) processing, with each associated with distinct neural circuits. However, our findings challenge this view by showing overlapping activations across different networks, such as the lateral occipital cortex and superior parietal cortex, for both explicit and implicit tasks. These overlaps are not necessarily contradictory; instead, they may reflect the brain's flexible use of resources to adapt to complex stimuli that require both emotional and perceptual processing. Thus, rather than being strictly separated, explicit and implicit processing may coexist and interact, dynamically influencing each other based on the context and the individual's strategies. The brain dynamically reorganizes itself based on task demands, participant strategies, and contextual factors. Such implications challenge the way we understand emotional processing in the brain, not as a linear or modular process, but as a result of complex, dynamic interactions among various neural networks (<xref ref-type="bibr" rid="R43">Rizzolatti &amp; Sinigaglia, 2010</xref>).</p><p id="P37">While our study provides valuable insights into the neural correlates of explicit and implicit emotional processing, methodological limitations need to be considered when interpreting the results. The small sample size and narrow age range (mean age 65.4) limit generalization across broader and younger populations. The use of a restricted set of emotions (anger, happiness, neutral) may reduce ecological validity, while task design assumptions about implicit processing may not hold true for all participants, potentially confounding results. Temporal fMRI resolution may also obscure more subtle neural dynamics. Furthermore, experimental settings, task constraints, and potential biases related to task-switching further challenge the interpretability results. These aspects were however kept constant and proven to be identical across participants. Finally, individual strategies used by our participants could be different and should not be neglected. Addressing these limitations in future research would enhance the robustness and applicability of the findings.</p><p id="P38">This study investigated the neural correlates of explicit versus implicit emotional processing, focusing on the cerebellum, basal ganglia, and associated cortical networks. Our results confirm that different levels of emotional processing engage distinct large scale brain networks involving the basal ganglia and the cerebellum. However, overlapping activations also emphasize that parts of these networks are not strictly polarized between the two emotional processes, with different regions interacting based on task demands and individual strategies. The results highlight the importance of considering the cerebellum in models of emotion functions and going beyond traditional modular theories to embrace more integrated and dynamic frameworks.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary figures and tables</label><media xlink:href="EMS200694-supplement-Supplementary_figures_and_tables.docx" mimetype="application" mime-subtype="vnd.openxmlformats-officedocument.wordprocessingml.document" id="d308aAcEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S16"><title>Acknowledgments</title><p>We warmly thank the team of the Brain and Behavioral Laboratory of the University of Geneva, Switzerland, for their help with setting-up MRI sequences and with MRI data acquisition.</p><sec id="S17"><title>Funding</title><p>The present research was supported by Swiss National Science Foundation (SNSF) grants to JAP (PI) and FA (Co-PI) within the project "Influence of top-down mechanisms on cerebellar activity during vocal emotion decoding" (2023-2026), Grant N°: 105314_215015 through the University of Geneva. The funders had no role in data collection, discussion of content, preparation of the manuscript, or decision to publish.</p></sec></ack><sec id="S18" sec-type="data-availability"><title>Data availability</title><p id="P39">Data and codes will be made available on the free repository YARETA, which meets the FAIR guidelines, upon acceptance of the manuscript for publication.</p></sec><fn-group><fn id="FN1" fn-type="conflict"><p id="P40">Disclosure: The authors report no conflicts of interest.</p></fn><fn id="FN2"><p id="P41"><bold>Ethics</bold></p><p id="P42">This study was conducted following the written informed consent of all participants (N=28), and according to the strict regulations of the University of Geneva and the declaration of Helsinki. Participants were informed they could exit the study without justification any time they wanted or felt it was necessary for them. The state-wise ethics committee of Geneva University Hospital (Comité Cantonal d’Ethique en Recherche, CCER) reviewed and accepted our ethics application (number is 2024-00174).</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adamaszek</surname><given-names>M</given-names></name><name><surname>D’Agata</surname><given-names>F</given-names></name><name><surname>Ferrucci</surname><given-names>R</given-names></name><name><surname>Habas</surname><given-names>C</given-names></name><name><surname>Keulen</surname><given-names>S</given-names></name><name><surname>Kirkby</surname><given-names>K</given-names></name><name><surname>Leggio</surname><given-names>M</given-names></name><name><surname>Mariën</surname><given-names>P</given-names></name><name><surname>Molinari</surname><given-names>M</given-names></name><name><surname>Moulton</surname><given-names>E</given-names></name></person-group><article-title>Consensus paper: cerebellum and emotion</article-title><source>The Cerebellum</source><year>2017</year><volume>16</volume><fpage>552</fpage><lpage>576</lpage><pub-id pub-id-type="pmid">27485952</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adolphs</surname><given-names>R</given-names></name></person-group><article-title>The social brain: neural basis of social knowledge</article-title><source>Annual review of psychology</source><year>2009</year><volume>60</volume><issue>1</issue><fpage>693</fpage><lpage>716</lpage><pub-id pub-id-type="pmcid">PMC2588649</pub-id><pub-id pub-id-type="pmid">18771388</pub-id><pub-id pub-id-type="doi">10.1146/annurev.psych.60.110707.163514</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allaire</surname><given-names>J</given-names></name></person-group><article-title>RStudio: integrated development environment for R</article-title><source>Boston, MA</source><year>2012</year><volume>770</volume><issue>394</issue><fpage>165</fpage><lpage>171</lpage></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bach</surname><given-names>DR</given-names></name><name><surname>Grandjean</surname><given-names>D</given-names></name><name><surname>Sander</surname><given-names>D</given-names></name><name><surname>Herdener</surname><given-names>M</given-names></name><name><surname>Strik</surname><given-names>WK</given-names></name><name><surname>Seifritz</surname><given-names>E</given-names></name></person-group><article-title>The effect of appraisal level on processing of emotional prosody in meaningless speech</article-title><source>Neuroimage</source><year>2008</year><volume>42</volume><issue>2</issue><fpage>919</fpage><lpage>927</lpage><pub-id pub-id-type="pmid">18586524</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bamiou</surname><given-names>D-E</given-names></name><name><surname>Musiek</surname><given-names>FE</given-names></name><name><surname>Luxon</surname><given-names>LM</given-names></name></person-group><article-title>The insula (Island of Reil) and its role in auditory processing: literature review</article-title><source>Brain research reviews</source><year>2003</year><volume>42</volume><issue>2</issue><fpage>143</fpage><lpage>154</lpage><pub-id pub-id-type="pmid">12738055</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Banse</surname><given-names>R</given-names></name><name><surname>Scherer</surname><given-names>KR</given-names></name></person-group><article-title>Acoustic profiles in vocal emotion expression</article-title><source>Journal of personality and social psychology</source><year>1996</year><volume>70</volume><issue>3</issue><fpage>614</fpage><pub-id pub-id-type="pmid">8851745</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bassett</surname><given-names>DS</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name></person-group><article-title>Network neuroscience</article-title><source>Nature neuroscience</source><year>2017</year><volume>20</volume><issue>3</issue><fpage>353</fpage><lpage>364</lpage><pub-id pub-id-type="pmcid">PMC5485642</pub-id><pub-id pub-id-type="pmid">28230844</pub-id><pub-id pub-id-type="doi">10.1038/nn.4502</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bates</surname><given-names>D</given-names></name><name><surname>Maechler</surname><given-names>M</given-names></name><name><surname>Bolker</surname><given-names>B</given-names></name><name><surname>Walker</surname><given-names>S</given-names></name><name><surname>Christensen</surname><given-names>RHB</given-names></name><name><surname>Singmann</surname><given-names>H</given-names></name><name><surname>Dai</surname><given-names>B</given-names></name><name><surname>Grothendieck</surname><given-names>G</given-names></name><name><surname>Green</surname><given-names>P</given-names></name><name><surname>Bolker</surname><given-names>MB</given-names></name></person-group><article-title>Package ‘lme4’</article-title><source>convergence</source><year>2015</year><volume>12</volume><issue>1</issue><fpage>2</fpage></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bechara</surname><given-names>A</given-names></name><name><surname>Damasio</surname><given-names>H</given-names></name><name><surname>Damasio</surname><given-names>AR</given-names></name></person-group><article-title>Emotion, decision making and the orbitofrontal cortex</article-title><source>Cerebral cortex</source><year>2000</year><volume>10</volume><issue>3</issue><fpage>295</fpage><lpage>307</lpage><pub-id pub-id-type="pmid">10731224</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cabaraux</surname><given-names>P</given-names></name><name><surname>Gandini</surname><given-names>J</given-names></name><name><surname>Kakei</surname><given-names>S</given-names></name><name><surname>Manto</surname><given-names>M</given-names></name><name><surname>Mitoma</surname><given-names>H</given-names></name><name><surname>Tanaka</surname><given-names>H</given-names></name></person-group><article-title>Dysmetria and errors in predictions: the role of internal forward model</article-title><source>International journal of molecular sciences</source><year>2020</year><volume>21</volume><issue>18</issue><fpage>6900</fpage><pub-id pub-id-type="pmcid">PMC7555030</pub-id><pub-id pub-id-type="pmid">32962256</pub-id><pub-id pub-id-type="doi">10.3390/ijms21186900</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cavanna</surname><given-names>AE</given-names></name><name><surname>Trimble</surname><given-names>MR</given-names></name></person-group><article-title>The precuneus: a review of its functional anatomy and behavioural correlates</article-title><source>Brain</source><year>2006</year><volume>129</volume><issue>3</issue><fpage>564</fpage><lpage>583</lpage><pub-id pub-id-type="pmid">16399806</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ceravolo</surname><given-names>L</given-names></name><name><surname>Frühholz</surname><given-names>S</given-names></name><name><surname>Pierce</surname><given-names>J</given-names></name><name><surname>Grandjean</surname><given-names>D</given-names></name><name><surname>Péron</surname><given-names>J</given-names></name></person-group><article-title>Basal ganglia and cerebellum contributions to vocal emotion processing as revealed by high-resolution fMRI</article-title><source>Scientific reports</source><year>2021</year><volume>11</volume><issue>1</issue><elocation-id>10645</elocation-id><pub-id pub-id-type="pmcid">PMC8138027</pub-id><pub-id pub-id-type="pmid">34017050</pub-id><pub-id pub-id-type="doi">10.1038/s41598-021-90222-6</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname><given-names>DL</given-names></name><name><surname>Neelin</surname><given-names>P</given-names></name><name><surname>Peters</surname><given-names>TM</given-names></name><name><surname>Evans</surname><given-names>AC</given-names></name></person-group><article-title>Automatic 3D intersubject registration of MR volumetric data in standardized Talairach space</article-title><source>Journal of computer assisted tomography</source><year>1994</year><volume>18</volume><issue>2</issue><fpage>192</fpage><lpage>205</lpage><pub-id pub-id-type="pmid">8126267</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corbetta</surname><given-names>M</given-names></name><name><surname>Shulman</surname><given-names>GL</given-names></name></person-group><article-title>Control of goal-directed and stimulus-driven attention in the brain</article-title><source>Nature Reviews Neuroscience</source><year>2002</year><volume>3</volume><issue>3</issue><fpage>201</fpage><lpage>215</lpage><pub-id pub-id-type="pmid">11994752</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Craig</surname><given-names>AD</given-names></name></person-group><article-title>How do you feel—now? The anterior insula and human awareness</article-title><source>Nature Reviews Neuroscience</source><year>2009</year><volume>10</volume><issue>1</issue><fpage>59</fpage><lpage>70</lpage><pub-id pub-id-type="pmid">19096369</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Damasio</surname><given-names>A</given-names></name></person-group><source>Descartes’ Error. Emotion, Reason and the Human Brain</source><publisher-loc>New York</publisher-loc><publisher-name>(Grosset/Putnam)</publisher-name><year>1994</year></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Rosario-Martinez</surname><given-names>H</given-names></name><name><surname>Fox</surname><given-names>J</given-names></name><name><surname>Team</surname><given-names>RC</given-names></name><name><surname>De Rosario-Martinez</surname><given-names>MH</given-names></name></person-group><article-title>Package ‘phia’</article-title><source>CRAN Repos Retrieved</source><year>2015</year><volume>1</volume></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ethofer</surname><given-names>T</given-names></name><name><surname>Kreifelts</surname><given-names>B</given-names></name><name><surname>Wiethoff</surname><given-names>S</given-names></name><name><surname>Wolf</surname><given-names>J</given-names></name><name><surname>Grodd</surname><given-names>W</given-names></name><name><surname>Vuilleumier</surname><given-names>P</given-names></name><name><surname>Wildgruber</surname><given-names>D</given-names></name></person-group><article-title>Differential influences of emotion, task, and novelty on brain regions underlying the processing of speech melody</article-title><source>Journal of cognitive neuroscience</source><year>2009</year><volume>21</volume><issue>7</issue><fpage>1255</fpage><lpage>1268</lpage><pub-id pub-id-type="pmid">18752404</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faul</surname><given-names>F</given-names></name><name><surname>Erdfelder</surname><given-names>E</given-names></name><name><surname>Lang</surname><given-names>A-G</given-names></name><name><surname>Buchner</surname><given-names>A</given-names></name></person-group><article-title>G* Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences</article-title><source>Behavior research methods</source><year>2007</year><volume>39</volume><issue>2</issue><fpage>175</fpage><lpage>191</lpage><pub-id pub-id-type="pmid">17695343</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frühholz</surname><given-names>S</given-names></name><name><surname>Ceravolo</surname><given-names>L</given-names></name><name><surname>Grandjean</surname><given-names>D</given-names></name></person-group><article-title>Specific brain networks during explicit and implicit decoding of emotional prosody</article-title><source>Cerebral cortex</source><year>2012</year><volume>22</volume><issue>5</issue><fpage>1107</fpage><lpage>1117</lpage><pub-id pub-id-type="pmid">21750247</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frühholz</surname><given-names>S</given-names></name><name><surname>Trost</surname><given-names>W</given-names></name><name><surname>Kotz</surname><given-names>SA</given-names></name></person-group><article-title>The sound of emotions—Towards a unifying neural network perspective of affective sound processing</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><year>2016</year><volume>68</volume><fpage>96</fpage><lpage>110</lpage><pub-id pub-id-type="pmid">27189782</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fuster</surname><given-names>J</given-names></name></person-group><source>The Prefrontal Cortex</source><publisher-name>Academic Press</publisher-name><publisher-loc>New York</publisher-loc><year>2008</year></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grandjean</surname><given-names>D</given-names></name></person-group><article-title>Brain networks of emotional prosody processing</article-title><source>Emotion Review</source><year>2021</year><volume>13</volume><issue>1</issue><fpage>34</fpage><lpage>43</lpage></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grandjean</surname><given-names>D</given-names></name><name><surname>Sander</surname><given-names>D</given-names></name><name><surname>Pourtois</surname><given-names>G</given-names></name><name><surname>Schwartz</surname><given-names>S</given-names></name><name><surname>Seghier</surname><given-names>ML</given-names></name><name><surname>Scherer</surname><given-names>KR</given-names></name><name><surname>Vuilleumier</surname><given-names>P</given-names></name></person-group><article-title>The voices of wrath: brain responses to angry prosody in meaningless speech</article-title><source>Nature neuroscience</source><year>2005</year><volume>8</volume><issue>2</issue><fpage>145</fpage><lpage>146</lpage><pub-id pub-id-type="pmid">15665880</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graybiel</surname><given-names>AM</given-names></name></person-group><article-title>The basal ganglia and chunking of action repertoires</article-title><source>Neurobiology of learning and memory</source><year>1998</year><volume>70</volume><issue>1-2</issue><fpage>119</fpage><lpage>136</lpage><pub-id pub-id-type="pmid">9753592</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graybiel</surname><given-names>AM</given-names></name></person-group><article-title>Habits, rituals, and the evaluative brain</article-title><source>Annu Rev Neurosci</source><year>2008</year><volume>31</volume><issue>1</issue><fpage>359</fpage><lpage>387</lpage><pub-id pub-id-type="pmid">18558860</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Habas</surname><given-names>C</given-names></name><name><surname>Kamdar</surname><given-names>N</given-names></name><name><surname>Nguyen</surname><given-names>D</given-names></name><name><surname>Prater</surname><given-names>K</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Menon</surname><given-names>V</given-names></name><name><surname>Greicius</surname><given-names>MD</given-names></name></person-group><article-title>Distinct cerebellar contributions to intrinsic connectivity networks</article-title><source>Journal of neuroscience</source><year>2009</year><volume>29</volume><issue>26</issue><fpage>8586</fpage><lpage>8594</lpage><pub-id pub-id-type="pmcid">PMC2742620</pub-id><pub-id pub-id-type="pmid">19571149</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1868-09.2009</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ito</surname><given-names>M</given-names></name></person-group><article-title>Control of mental activities by internal models in the cerebellum</article-title><source>Nature Reviews Neuroscience</source><year>2008</year><volume>9</volume><issue>4</issue><fpage>304</fpage><lpage>313</lpage><pub-id pub-id-type="pmid">18319727</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koziol</surname><given-names>LF</given-names></name><name><surname>Budding</surname><given-names>D</given-names></name><name><surname>Andreasen</surname><given-names>N</given-names></name><name><surname>D’Arrigo</surname><given-names>S</given-names></name><name><surname>Bulgheroni</surname><given-names>S</given-names></name><name><surname>Imamizu</surname><given-names>H</given-names></name><name><surname>Ito</surname><given-names>M</given-names></name><name><surname>Manto</surname><given-names>M</given-names></name><name><surname>Marvel</surname><given-names>C</given-names></name><name><surname>Parker</surname><given-names>K</given-names></name></person-group><article-title>Consensus paper: the cerebellum’s role in movement and cognition</article-title><source>The Cerebellum</source><year>2014</year><volume>13</volume><fpage>151</fpage><lpage>177</lpage><pub-id pub-id-type="pmcid">PMC4089997</pub-id><pub-id pub-id-type="pmid">23996631</pub-id><pub-id pub-id-type="doi">10.1007/s12311-013-0511-x</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krack</surname><given-names>P</given-names></name><name><surname>Hariz</surname><given-names>MI</given-names></name><name><surname>Baunez</surname><given-names>C</given-names></name><name><surname>Guridi</surname><given-names>J</given-names></name><name><surname>Obeso</surname><given-names>JA</given-names></name></person-group><article-title>Deep brain stimulation: from neurology to psychiatry?</article-title><source>Trends in neurosciences</source><year>2010</year><volume>33</volume><issue>10</issue><fpage>474</fpage><lpage>484</lpage><pub-id pub-id-type="pmid">20832128</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kringelbach</surname><given-names>ML</given-names></name><name><surname>Rolls</surname><given-names>ET</given-names></name></person-group><article-title>The functional neuroanatomy of the human orbitofrontal cortex: evidence from neuroimaging and neuropsychology</article-title><source>Progress in Neurobiology</source><year>2004</year><volume>72</volume><issue>5</issue><fpage>341</fpage><lpage>372</lpage><pub-id pub-id-type="pmid">15157726</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>LeDoux</surname><given-names>JE</given-names></name></person-group><source>The emotional brain: The mysterious underpinnings of emotional life</source><publisher-name>Simon and Schuster</publisher-name><year>1998</year></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manto</surname><given-names>M</given-names></name><name><surname>Adamaszek</surname><given-names>M</given-names></name><name><surname>Apps</surname><given-names>R</given-names></name><name><surname>Carlson</surname><given-names>E</given-names></name><name><surname>Guarque-Chabrera</surname><given-names>J</given-names></name><name><surname>Heleven</surname><given-names>E</given-names></name><name><surname>Kakei</surname><given-names>S</given-names></name><name><surname>Khodakhah</surname><given-names>K</given-names></name><name><surname>Kuo</surname><given-names>S-H</given-names></name><name><surname>Lin</surname><given-names>C-YR</given-names></name></person-group><article-title>Consensus Paper: Cerebellum and Reward</article-title><source>The Cerebellum</source><year>2024</year><fpage>1</fpage><lpage>24</lpage><pub-id pub-id-type="pmid">38769243</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Middleton</surname><given-names>FA</given-names></name><name><surname>Strick</surname><given-names>PL</given-names></name></person-group><article-title>Basal ganglia and cerebellar loops: motor and cognitive circuits</article-title><source>Brain research reviews</source><year>2000</year><volume>31</volume><issue>2-3</issue><fpage>236</fpage><lpage>250</lpage><pub-id pub-id-type="pmid">10719151</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nachev</surname><given-names>P</given-names></name><name><surname>Kennard</surname><given-names>C</given-names></name><name><surname>Husain</surname><given-names>M</given-names></name></person-group><article-title>Functional role of the supplementary and pre-supplementary motor areas</article-title><source>Nature Reviews Neuroscience</source><year>2008</year><volume>9</volume><issue>11</issue><fpage>856</fpage><lpage>869</lpage><pub-id pub-id-type="pmid">18843271</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nasreddine</surname><given-names>ZS</given-names></name><name><surname>Patel</surname><given-names>BB</given-names></name></person-group><article-title>Validation of Montreal cognitive assessment, MoCA, alternate French versions</article-title><source>Canadian Journal of Neurological Sciences</source><year>2016</year><volume>43</volume><issue>5</issue><fpage>665</fpage><lpage>671</lpage><pub-id pub-id-type="pmid">27670209</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oldfield</surname><given-names>R</given-names></name></person-group><article-title>Edinburgh handedness inventory</article-title><source>Journal of Abnormal Psychology</source><year>1971</year></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pessoa</surname><given-names>L</given-names></name><name><surname>Adolphs</surname><given-names>R</given-names></name></person-group><article-title>Emotion processing and the amygdala: from a’low road’to’many roads’ of evaluating biological significance</article-title><source>Nature Reviews Neuroscience</source><year>2010</year><volume>11</volume><issue>11</issue><fpage>773</fpage><lpage>782</lpage><pub-id pub-id-type="pmcid">PMC3025529</pub-id><pub-id pub-id-type="pmid">20959860</pub-id><pub-id pub-id-type="doi">10.1038/nrn2920</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Picard</surname><given-names>N</given-names></name><name><surname>Strick</surname><given-names>PL</given-names></name></person-group><article-title>Motor areas of the medial wall: a review of their location and functional activation</article-title><source>Cerebral cortex</source><year>1996</year><volume>6</volume><issue>3</issue><fpage>342</fpage><lpage>353</lpage><pub-id pub-id-type="pmid">8670662</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pierce</surname><given-names>JE</given-names></name><name><surname>Péron</surname><given-names>J</given-names></name></person-group><article-title>The basal ganglia and the cerebellum in human emotion</article-title><source>Social cognitive and affective neuroscience</source><year>2020</year><volume>15</volume><issue>5</issue><fpage>599</fpage><lpage>613</lpage><pub-id pub-id-type="pmcid">PMC7328022</pub-id><pub-id pub-id-type="pmid">32507876</pub-id><pub-id pub-id-type="doi">10.1093/scan/nsaa076</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pierce</surname><given-names>JE</given-names></name><name><surname>Péron</surname><given-names>JA</given-names></name></person-group><chapter-title>Reward-based learning and emotional habit formation in the cerebellum</chapter-title><source>The Emotional Cerebellum</source><publisher-name>Springer</publisher-name><year>2022</year><fpage>125</fpage><lpage>140</lpage><pub-id pub-id-type="pmid">35902469</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pierce</surname><given-names>JE</given-names></name><name><surname>Thomasson</surname><given-names>M</given-names></name><name><surname>Voruz</surname><given-names>P</given-names></name><name><surname>Selosse</surname><given-names>G</given-names></name><name><surname>Péron</surname><given-names>J</given-names></name></person-group><article-title>Explicit and implicit emotion processing in the cerebellum: a meta-analysis and systematic review</article-title><source>The Cerebellum</source><year>2023</year><volume>22</volume><issue>5</issue><fpage>852</fpage><lpage>864</lpage><pub-id pub-id-type="pmcid">PMC10485090</pub-id><pub-id pub-id-type="pmid">35999332</pub-id><pub-id pub-id-type="doi">10.1007/s12311-022-01459-4</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rizzolatti</surname><given-names>G</given-names></name><name><surname>Sinigaglia</surname><given-names>C</given-names></name></person-group><article-title>The functional role of the parieto-frontal mirror circuit: interpretations and misinterpretations</article-title><source>Nature Reviews Neuroscience</source><year>2010</year><volume>11</volume><issue>4</issue><fpage>264</fpage><lpage>274</lpage><pub-id pub-id-type="pmid">20216547</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolls</surname><given-names>ET</given-names></name><name><surname>Huang</surname><given-names>C-C</given-names></name><name><surname>Lin</surname><given-names>C-P</given-names></name><name><surname>Feng</surname><given-names>J</given-names></name><name><surname>Joliot</surname><given-names>M</given-names></name></person-group><article-title>Automated anatomical labelling atlas 3</article-title><source>Neuroimage</source><year>2020</year><volume>206</volume><elocation-id>116189</elocation-id><pub-id pub-id-type="pmid">31521825</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schmahmann</surname><given-names>JD</given-names></name></person-group><source>The cerebellum and cognition</source><publisher-name>Academic Press</publisher-name><year>1997</year><pub-id pub-id-type="pmid">29997061</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmahmann</surname><given-names>JD</given-names></name></person-group><article-title>The cerebellum and cognition</article-title><source>Neuroscience letters</source><year>2019</year><volume>688</volume><fpage>62</fpage><lpage>75</lpage></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schutter</surname><given-names>DJ</given-names></name><name><surname>Van Honk</surname><given-names>J</given-names></name></person-group><article-title>The cerebellum on the rise in human emotion</article-title><source>The Cerebellum</source><year>2005</year><volume>4</volume><fpage>290</fpage><lpage>294</lpage><pub-id pub-id-type="pmid">16321885</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Selosse</surname><given-names>G</given-names></name><name><surname>Grandjean</surname><given-names>D</given-names></name><name><surname>Ceravolo</surname><given-names>L</given-names></name></person-group><article-title>Neural correlates of embodied and vibratory mechanisms associated with vocal emotion production</article-title><source>bioRxiv</source><year>2024</year><elocation-id>20242005 2014594073</elocation-id></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steiner</surname><given-names>F</given-names></name><name><surname>Fernandez</surname><given-names>N</given-names></name><name><surname>Dietziker</surname><given-names>J</given-names></name><name><surname>Stämpfli</surname><given-names>P</given-names></name><name><surname>Seifritz</surname><given-names>E</given-names></name><name><surname>Rey</surname><given-names>A</given-names></name><name><surname>Frühholz</surname><given-names>S</given-names></name></person-group><article-title>Affective speech modulates a cortico-limbic network in real time</article-title><source>Progress in Neurobiology</source><year>2022</year><volume>214</volume><elocation-id>102278</elocation-id><pub-id pub-id-type="pmid">35513165</pub-id></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thomasson</surname><given-names>M</given-names></name><name><surname>Benis</surname><given-names>D</given-names></name><name><surname>Saj</surname><given-names>A</given-names></name><name><surname>Voruz</surname><given-names>P</given-names></name><name><surname>Ronchi</surname><given-names>R</given-names></name><name><surname>Grandjean</surname><given-names>D</given-names></name><name><surname>Assal</surname><given-names>F</given-names></name><name><surname>Péron</surname><given-names>J</given-names></name></person-group><article-title>Sensory contribution to vocal emotion deficit in patients with cerebellar stroke</article-title><source>NeuroImage: Clinical</source><year>2021</year><volume>31</volume><elocation-id>102690</elocation-id><pub-id pub-id-type="pmcid">PMC8138671</pub-id><pub-id pub-id-type="pmid">34000647</pub-id><pub-id pub-id-type="doi">10.1016/j.nicl.2021.102690</pub-id></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thomasson</surname><given-names>M</given-names></name><name><surname>Benis</surname><given-names>D</given-names></name><name><surname>Voruz</surname><given-names>P</given-names></name><name><surname>Saj</surname><given-names>A</given-names></name><name><surname>Vérin</surname><given-names>M</given-names></name><name><surname>Assal</surname><given-names>F</given-names></name><name><surname>Grandjean</surname><given-names>D</given-names></name><name><surname>Péron</surname><given-names>J</given-names></name></person-group><article-title>Crossed functional specialization between the basal ganglia and cerebellum during vocal emotion decoding: insights from stroke and Parkinson’s disease</article-title><source>Cognitive, Affective, &amp; Behavioral Neuroscience</source><year>2022</year><volume>22</volume><issue>5</issue><fpage>1030</fpage><lpage>1043</lpage><pub-id pub-id-type="pmcid">PMC9458588</pub-id><pub-id pub-id-type="pmid">35474566</pub-id><pub-id pub-id-type="doi">10.3758/s13415-022-01000-4</pub-id></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tzourio-Mazoyer</surname><given-names>N</given-names></name><name><surname>Landeau</surname><given-names>B</given-names></name><name><surname>Papathanassiou</surname><given-names>D</given-names></name><name><surname>Crivello</surname><given-names>F</given-names></name><name><surname>Etard</surname><given-names>O</given-names></name><name><surname>Delcroix</surname><given-names>N</given-names></name><name><surname>Mazoyer</surname><given-names>B</given-names></name><name><surname>Joliot</surname><given-names>M</given-names></name></person-group><article-title>Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain</article-title><source>Neuroimage</source><year>2002</year><volume>15</volume><issue>1</issue><fpage>273</fpage><lpage>289</lpage><pub-id pub-id-type="pmid">11771995</pub-id></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whitfield-Gabrieli</surname><given-names>S</given-names></name><name><surname>Nieto-Castanon</surname><given-names>A</given-names></name></person-group><article-title>Conn: a functional connectivity toolbox for correlated and anticorrelated brain networks</article-title><source>Brain connectivity</source><year>2012</year><volume>2</volume><issue>3</issue><fpage>125</fpage><lpage>141</lpage><pub-id pub-id-type="pmid">22642651</pub-id></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wildgruber</surname><given-names>D</given-names></name><name><surname>Riecker</surname><given-names>A</given-names></name><name><surname>Hertrich</surname><given-names>I</given-names></name><name><surname>Erb</surname><given-names>M</given-names></name><name><surname>Grodd</surname><given-names>W</given-names></name><name><surname>Ethofer</surname><given-names>T</given-names></name><name><surname>Ackermann</surname><given-names>H</given-names></name></person-group><article-title>Identification of emotional intonation evaluated by fMRI</article-title><source>Neuroimage</source><year>2005</year><volume>24</volume><issue>4</issue><fpage>1233</fpage><lpage>1241</lpage><pub-id pub-id-type="pmid">15670701</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Fig. 1</label><caption><title>Behavioral results for response probability in the Gender (Implicit) and the Emotion (Explicit) discrimination task.</title><p>Circles and triangles represent individual values for each participant (N=28; circles: Emotion task, triangles: Gender task), with data curves as violin distribution and mean values with error bars representing the standard error of the mean (Y axis) as a function of voice emotion (X axis). Chance level for response probability is 50% (0.5, X axis, blue) since all tasks involved a 2-alternate forced choice type of response. *** <italic>p</italic>&lt;.001, ** <italic>p</italic>&lt;.01, * <italic>p</italic>&lt;.05. Descriptive values are reported in <xref ref-type="supplementary-material" rid="SD1">Table S1</xref>.</p></caption><graphic xlink:href="EMS200694-f001"/></fig><fig id="F2" position="float"><label>Fig. 2</label><caption><title>Whole brain activations contrasting the levels of the Task factor.</title><p>Lateral and medial cortical as well as basal ganglia and cerebellar activations were identified when contrasting explicit to implicit emotion processing tasks (Explicit&gt;Implicit: red-to-yellow activations, lateral cortex ‘A,B,C,H’; medial cortex ‘G,I’; basal ganglia ‘M’; cerebellum ‘N’; Implicit&gt;Explicit: blue-to-green activations, lateral cortex ‘D,E,F,K’; medial cortex ‘J,L’; basal ganglia ‘O’; cerebellum ‘P’). All activations reported using an FDR corrected voxel-wise threshold of <italic>p</italic>&lt;.05. Color bars represent T-values. IFG: inferior frontal gyrus; INS: insula; STG: superior temporal gyrus; STS: superior temporal sulcus; preSMA: pre-supplementary motor area; SMA: supplementary motor area; preM: premotor cortex; M1: primary motor cortex; S1: primary somatosensory cortex; SMG: supramarginal gyrus. Caud: caudate nucleus; Put: putamen; Thal: thalamus; OFC: orbitofrontal cortex; Cer: cerebellar lobule; Hipp: hippocampus. Suffixes: orb, pars orbitalis; tri, pars triangularis; op, pars opercularis; sup, superior; mid, middle; med, medial.</p></caption><graphic xlink:href="EMS200694-f002"/></fig><fig id="F3" position="float"><label>Fig. 3</label><caption><title>Whole brain activations contrasting the levels of the Task factor for each emotion.</title><p>Lateral and medial cortical as well as basal ganglia and cerebellar activations were highlighted when contrasting explicit to implicit emotion processing tasks (lateral cortex ‘A-G, K’; medial cortex ‘H-J, L-N’; basal ganglia ‘K’; cerebellum ‘O-Q’) for angry (‘Ang’; blue-to-red colormap), happy (‘Hap’; pink-to-yellow colormap) and neutral (‘Neu’; black-to-white colormap) voices. All activations reported using an FDR corrected voxel-wise threshold of <italic>p</italic>&lt;.05. Color bars represent T-values. White outline in panels ‘O-Q’: cerebellar overlap between all three emotions. IFG: inferior frontal gyrus; INS: insula; STG: superior temporal gyrus; STS: superior temporal sulcus; preSMA: pre-supplementary motor area; SMA: supplementary motor area; preM: premotor cortex; M1: primary motor cortex; S1: primary somatosensory cortex; SMG: supramarginal gyrus. Caud: caudate nucleus; Put: putamen; Thal: thalamus; OFC: orbitofrontal cortex; Brainstem CST: cortico-spinal tract of the brainstem; Ver: Vermis. Suffixes: orb, pars orbitalis; tri, pars triangularis; op, pars opercularis; sup, superior; mid, middle; med, medial.</p></caption><graphic xlink:href="EMS200694-f003"/></fig></floats-group></article>