<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS199509</article-id><article-id pub-id-type="doi">10.1101/2024.10.15.618378</article-id><article-id pub-id-type="archive">PPR926273</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">2</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>KinPFN: Bayesian Approximation of RNA Folding Kinetics using Prior-Data Fitted Networks</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><name><surname>Scheuer</surname><given-names>Dominik</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><name><surname>Runge</surname><given-names>Frederic</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Franke</surname><given-names>Jörg K.H.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Wolfinger</surname><given-names>Michael T.</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Flamm</surname><given-names>Christoph</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Hutter</surname><given-names>Frank</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A4">4</xref></contrib></contrib-group><aff id="A1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0245cg223</institution-id><institution>University of Freiburg</institution></institution-wrap>, <country country="DE">Germany</country></aff><aff id="A2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03prydq77</institution-id><institution>University of Vienna</institution></institution-wrap>, <country country="AT">Austria</country></aff><aff id="A3"><label>3</label>RNA Forecast e.U., Vienna, Austria</aff><aff id="A4"><label>4</label>ELLIS Institute Tübingen, Germany</aff><author-notes><corresp id="CR1">Correspondence to <email>runget@cs.uni-freiburg.de</email> or <email>dom.scheuer@gmail.com</email>.</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>19</day><month>10</month><year>2024</year></pub-date><pub-date pub-type="preprint"><day>17</day><month>10</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">RNA is a dynamic biomolecule crucial for cellular regulation, with its function largely determined by its folding into complex structures, while misfolding can lead to multifaceted biological sequelae. During the folding process, RNA traverses through a series of intermediate structural states, with each transition occurring at variable rates that collectively influence the time required to reach the functional form. Understanding these folding kinetics is vital for predicting RNA behavior and optimizing applications in synthetic biology and drug discovery. While <italic>in silico</italic> kinetic RNA folding simulators are often computationally intensive and time-consuming, accurate approximations of the folding times can already be very informative to assess the efficiency of the folding process. In this work, we present <italic>KinPFN</italic>, a novel approach that leverages prior-data fitted networks to directly model the posterior predictive distribution of RNA folding times. By training on synthetic data representing arbitrary prior folding times, <italic>KinPFN</italic> efficiently approximates the cumulative distribution function of RNA folding times in a single forward pass, given only a few initial folding time examples. Our method offers a modular extension to existing RNA kinetics algorithms, promising significant computational speed-ups orders of magnitude faster, while achieving comparable results. We showcase the effectiveness of <italic>KinPFN</italic> through extensive evaluations and real-world case studies, demonstrating its potential for RNA folding kinetics analysis, its practical relevance, and generalization to other biological data.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><label>1</label><title>Introduction</title><p id="P2">Ribonucleic acid (RNA) plays a pivotal role in various biological processes, serving as a crucial intermediary between DNA and proteins while exerting significant regulatory functions through diverse mechanisms (<xref ref-type="bibr" rid="R20">Fu, 2014</xref>). Composed of four nucleotides – Adenine (A), Cytosine (C), Guanine (G), and Uracil (U) – the functionality of RNA is closely tied to its structure (<xref ref-type="bibr" rid="R30">Lodish et al., 2005</xref>): An RNA molecule adopts one or more native conformations that are essential for its biological activity (<xref ref-type="bibr" rid="R12">Fang et al., 2015</xref>). The dynamic process of how RNAs acquire their functional structure is known as the kinetic folding of RNA. During this process, the RNA strand transitions through several intermediate structural states, driven by intra-molecular interactions (<xref ref-type="bibr" rid="R15">Flamm et al., 2000</xref>; <xref ref-type="bibr" rid="R49">Yu et al., 2018</xref>). Since misfolding can lead to significant dysfunctions and diseases (<xref ref-type="bibr" rid="R8">Conlon &amp; Manley, 2017</xref>), the study of RNA folding kinetics is highly relevant for biomedical applications.</p><p id="P3">An important aspect of folding dynamics is the study of the rates and pathways through which RNA molecules achieve their native structures (<xref ref-type="bibr" rid="R7">Chen, 2008</xref>). A common measure to quantify these processes are first passage times (FPTs), i.e. the time required to acquire a certain structure for the first time, and their cumulative distribution functions (CDFs) (<xref ref-type="bibr" rid="R15">Flamm et al., 2000</xref>; <xref ref-type="bibr" rid="R47">Wolfinger et al., 2004</xref>). These functions are derived from extensive simulations, requiring thousands of folding iterations to capture the probabilistic behavior of RNA molecules. While essential for understanding RNA dynamics, calculating FPT CDFs is computationally expensive (<xref ref-type="bibr" rid="R47">Wolfinger et al., 2004</xref>; <xref ref-type="bibr" rid="R3">Badelt et al., 2023</xref>), posing a significant barrier to real-time applications such as kinetic RNA design, which is critical for drug discovery. While deep learning methods could improve the state of the art in RNA folding (<xref ref-type="bibr" rid="R19">Fu et al., 2022</xref>; <xref ref-type="bibr" rid="R18">Franke et al., 2024</xref>) and RNA design (<xref ref-type="bibr" rid="R42">Runge et al., 2024</xref>; <xref ref-type="bibr" rid="R37">Patil et al., 2024</xref>), they are not yet used in modeling RNA kinetics.</p><p id="P4">In this work, we present <italic>KinPFN</italic>, a novel deep learning-based approach that dramatically accelerates the computation of RNA first passage times. <italic>KinPFN</italic> leverages prior-data fitted networks (PFNs) (<xref ref-type="bibr" rid="R35">Müller et al., 2022</xref>) trained on synthetic datasets of RNA folding times to predict the entire CDF of folding times from just a few context examples in a single forward pass. By providing fast and accurate distribution approximations, <italic>KinPFN</italic> can be integrated with existing RNA kinetics simulators, offering comparable performance at a fraction of the computational cost. These speed-ups make KinPFN a valuable tool for the study of RNA folding kinetics, offering novel routes for applications in kinetic RNA design, which was previously intractable due to exponential runtimes of kinetic folding simulators, and promising fast analysis of RNA folding behaviors across multiple applications in drug discovery, medicine, biotechnology and synthetic biology.</p><p id="P5">Our main contributions are summarized as follows:</p><list list-type="bullet" id="L1"><list-item><p id="P6">We propose a new synthetic prior to sample datasets of RNA folding times. We use this synthetic data to train a prior-data fitted network to learn to predict the distribution of RNA first passage times, conditioned on a small set of context examples (<xref ref-type="sec" rid="S7">Section 4.1</xref>).</p></list-item><list-item><p id="P7">We introduce <italic>KinPFN</italic>, a new deep learning model for RNA kinetics. <italic>KinPFN</italic> provides accurate predictions of RNA first passage time distributions, accelerating kinetic simulations by orders of magnitude (<xref ref-type="sec" rid="S8">Section 4.2</xref>).</p></list-item><list-item><p id="P8">We evaluate <italic>KinPFN</italic>’s performance on synthetic and real-world RNA data (<xref ref-type="sec" rid="S10">Section 5</xref>), demonstrating its practical utility through two case studies: an analysis of eukaryotic RNAs (<xref ref-type="sec" rid="S12">Section 5.2</xref>) and a study of RNA folding efficiency (<xref ref-type="sec" rid="S13">Section 5.3</xref>).</p></list-item><list-item><p id="P9">In addition to its application to RNA folding kinetics, we assess <italic>KinPFN</italic>’s ability to generalize to different biological data sources by approximating gene expression data obtained from a previous smFISH (<xref ref-type="bibr" rid="R13">Femino et al., 1998</xref>; <xref ref-type="bibr" rid="R39">Raj et al., 2008</xref>) wet-lab analysis (<xref ref-type="bibr" rid="R4">Bagnall et al., 2020</xref>), demonstrating its potential to accelerate experimental protocols (<xref ref-type="sec" rid="S17">Section 5.4</xref>).</p></list-item></list><p id="P10">We provide an overview of <italic>KinPFN</italic> in <xref ref-type="fig" rid="F1">Figure 1</xref>. Our source code, data, and trained models are publicly available at <ext-link ext-link-type="uri" xlink:href="https://github.com/automl/KinPFN">https://github.com/automl/KinPFN</ext-link>.</p></sec><sec id="S2" sec-type="intro"><label>2</label><title>Background</title><sec id="S3"><title>RNA First Passage Times</title><p id="P11">Kinetic RNA folding is typically approximated by Monte-Carlo simulation techniques (<xref ref-type="bibr" rid="R14">Flamm &amp; Hofacker, 2008</xref>). However, this is computationally expensive since enough stochastic simulations need to be accumulated to get a statistically representative time evolution of the state probabilities. Depending on the number of different structural states, which is typically huge, the path during folding, and the energy barriers between the states, the time to reach a certain structure for the first time, i.e., the <italic>first passage time</italic>, can differ across multiple kinetic simulations for a given RNA. By comparing the first passage time CDFs of different RNA molecules or under varying conditions, differences in the folding dynamics can be revealed and better understood. This comparison provides insights into the efficiency and stability of the different folding processes, examining the impact of various modifications, such as chemical alternations or evolutionary changes (<xref ref-type="bibr" rid="R15">Flamm et al., 2000</xref>). In this work, we use the term folding time as a synonym for first passage time and focus on RNA folding kinetics based on secondary structure information.</p></sec><sec id="S4"><title>Prior-Data Fitted Networks</title><p id="P12">Prior-data fitted networks (PFNs) (<xref ref-type="bibr" rid="R35">Müller et al., 2022</xref>) use a transformer-based model to perform approximate Bayesian inference. PFNs are trained to predict an output <inline-formula><mml:math id="M1"><mml:mrow><mml:mi>y</mml:mi><mml:mo>∈</mml:mo><mml:mi>ℝ</mml:mi></mml:mrow></mml:math></inline-formula>, conditioned on an input <italic>x</italic> and a training set <italic>D</italic><sub>train</sub> of input-output pairs. During training, these samples are drawn from a prior distribution over datasets <inline-formula><mml:math id="M2"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="script">D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, optimizing the Cross-Entropy loss for a PFN <italic>q<sub>θ</sub></italic> with parameters <italic>θ</italic>, <disp-formula id="FD1"><label>(1)</label><mml:math id="M3"><mml:mrow><mml:msub><mml:mi>ℓ</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∪</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mtext>train</mml:mtext></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="script">D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>log</mml:mi><mml:msub><mml:mi>q</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mspace width="0.2em"/><mml:mo>(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>∣</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mtext>train</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P13">for predicting the label <italic>y</italic>, given <italic>x</italic> and <italic>D</italic><sub>train</sub>. As shown by <xref ref-type="bibr" rid="R35">Müller et al. (2022)</xref>, this approach directly minimizes the Kullback-Leibler (KL) divergence between the prediction of the PFN and the true posterior predictive distribution when training on many samples of the form (<italic>x, y</italic>) ∪ <italic>D</italic><sub>train</sub>. In this work, we adapt this strategy to tackle the prediction of RNA first passage time distributions, accounting for the specific challenges of the probabilistic behavior of RNA molecules that is also reflected in kinetic simulators by renouncing quantile information. For more details, see <xref ref-type="sec" rid="S8">Section 4.2</xref>.</p></sec></sec><sec id="S5"><label>3</label><title>Related Work</title><p id="P14"><italic>In silico</italic> analysis of RNA folding kinetics can be divided into nucleotide-resolution and coarse-grained approaches. While the first yields a high level of simulation details, the latter typically allows studying larger systems, i.e. longer RNA chain lengths. The first publicly available tool for computing RNA folding kinetics at nucleotide resolution is <italic>Kinfold</italic> (<xref ref-type="bibr" rid="R15">Flamm et al., 2000</xref>), a Markov-chain Monte Carlo (MCMC) method that is still considered one of the most accurate approaches available (<xref ref-type="bibr" rid="R21">Fukunaga &amp; Hamada, 2019</xref>). This accuracy, however, comes at the cost of runtime as <italic>Kinfold</italic> MCMC simulations typically require a large number of trajectories to obtain reliable results. While it is possible to simulate the folding kinetics of RNA chains of several hundreds of nucleotides, such calculations require substantial compute (<xref ref-type="bibr" rid="R21">Fukunaga &amp; Hamada, 2019</xref>). This limitation inspired accelerating techniques like memoization and parallelization (<xref ref-type="bibr" rid="R2">Aviram et al., 2012</xref>), or shortcuts for the energy calculations of RNA secondary structures as implemented in <italic>Kfold</italic> (<xref ref-type="bibr" rid="R11">Dykeman, 2015</xref>). In contrast, we develop <italic>KinPFN</italic> as an extension to existing kinetic RNA folding simulators to massively speed up every kinetic simulator that produces first passage times.</p><p id="P15">An Alternative to <italic>KinPFN</italic> are probabilistic density estimators like kernel density estimation (KDE) (<xref ref-type="bibr" rid="R5">Bishop, 2006</xref>), Gaussian Mixture Models (GMM) (<xref ref-type="bibr" rid="R5">Bishop, 2006</xref>) or Bayesian Gaussian Mixture Models, also known as Dirichlet Process GMMs (DP-GMM), which utilize a Variational Bayesian estimation of Gaussian mixtures (<xref ref-type="bibr" rid="R6">Blei &amp; Jordan, 2006</xref>). Similar to <italic>KinPFN</italic>, GMM and DP-GMM aim to model the posterior predictive distribution as a multi-modal Gaussian distribution. While GMMs struggle with complex data structures, especially when the number of modes is unknown, Bayesian approaches like DP-GMM can dynamically adjust the number of mixture components (<xref ref-type="bibr" rid="R32">McLachlan et al., 2019</xref>; <xref ref-type="bibr" rid="R36">Neal, 2000</xref>). Alternatively, kernel density estimation (KDE) offers a non-parametric approach by estimating probability densities through the summation of kernels, like Gaussians, over data points (<xref ref-type="bibr" rid="R5">Bishop, 2006</xref>). From a deep learning perspective, methods based on normalizing flows (<xref ref-type="bibr" rid="R41">Rezende &amp; Mohamed, 2015</xref>), variational autoencoders (VAEs) (<xref ref-type="bibr" rid="R27">Kingma, 2013</xref>), or a probabilistic transformer as proposed in <xref ref-type="bibr" rid="R17">Franke et al. (2022)</xref>, would be well suited for probability density estimation of RNA folding kinetics. However, these methods typically require large amounts of training data which is not available for RNA folding kinetics. Instead, we approach the problem of folding time prediction using a synthetic prior to train a PFN for direct approximation of the CDF of folding time distributions.</p><p id="P16">While, to the best of our knowledge, <italic>KinPFN</italic> is the first deep learning approach for RNA folding kinetics, PFNs were previously applied to multiple problems like few shot image classification (<xref ref-type="bibr" rid="R35">Müller et al., 2022</xref>), classification for small tabular datasets (<xref ref-type="bibr" rid="R35">Müller et al., 2022</xref>; <xref ref-type="bibr" rid="R26">Hollmann et al., 2023</xref>), extrapolation of learning curves (<xref ref-type="bibr" rid="R1">Adriaensen et al., 2023</xref>), Bayesian optimization and hyperparameter optimization (<xref ref-type="bibr" rid="R34">Müller et al., 2023</xref>; <xref ref-type="bibr" rid="R40">Rakotoarison et al., 2024</xref>), and time series forecasting (<xref ref-type="bibr" rid="R10">Dooley et al., 2024</xref>). For more discussions on related work, please see <xref ref-type="supplementary-material" rid="SD1">Appendix A</xref>.</p></sec><sec id="S6"><label>4</label><title>Approximation of RNA Folding Time Distributions</title><p id="P17">We consider the problem of learning the posterior predictive distribution (PPD) of first passage times for an RNA molecule <italic>ϕ</italic> ∈ {<italic>A</italic>, <italic>G, C, U</italic>}<sup><italic>l</italic></sup> of length <italic>l</italic>, conditioned on a small set of initial examples, to approximate the cumulative distribution function (CDF). Formally, the first passage time <italic>t</italic> is the time required for the RNA <italic>ϕ</italic> to fold from an initial structure <italic>ω</italic><sub>start</sub> into a stop structure <italic>ω</italic><sub>stop</sub> while transitioning through arbitrary intermediate structural states. Running <italic>M</italic> folding simulations under the same conditions (for RNA sequence <italic>ϕ</italic>, <italic>ω</italic><sub>start</sub>, and <italic>ω</italic><sub>stop</sub>) yields distinct first passage times <italic>t</italic><sub>1</sub>, …, <italic>t<sub>M</sub></italic>. By aggregating these times, we compute the fraction of molecules <italic>ϕ</italic> folded by time <italic>T</italic>, denoted <italic>F<sup>ϕ</sup></italic>(<italic>T</italic>), where <inline-formula><mml:math id="M4"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mi>t</mml:mi><mml:mi>ϕ</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>≤</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> represents the CDF of the stochastic variable <italic>t</italic>.</p><p id="P18">The problem we consider in this work can be formulated as follows: Given <italic>N</italic> ≪ <italic>M</italic> observed first passage times <italic>t</italic><sub>1</sub>,…, <italic>t<sub>N</sub></italic> and a prior distribution over first passage times from which we can generate samples, we aim to approximate the PPD <italic>q</italic>(<italic>t</italic> | <italic>t</italic><sub>1</sub>, …, <italic>t<sub>N</sub></italic>). With an approximated PPD, we can compute the predicted CDF <inline-formula><mml:math id="M5"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>ϕ</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, which approximates the true CDF <italic>F<sup>ϕ</sup></italic> (<italic>T</italic>); the fraction of molecules folded by time <italic>T</italic>.</p><p id="P19">In the following sections, we describe our approach to define a synthetic prior of first passage time distributions that allows us to approximate the PPD of folding times (<xref ref-type="sec" rid="S7">Section 4.1</xref>) and explain the development of <italic>KinPFN</italic> in detail (<xref ref-type="sec" rid="S8">Section 4.2</xref>).</p><sec id="S7"><label>4.1</label><title>A Synthetic Prior for RNA Folding Time Distributions</title><p id="P20">Obtaining large amounts of prior RNA kinetics data to train a deep learning model, particularly for longer RNAs, is currently infeasible due to the exponential runtime of accurate kinetic simulators (see <xref ref-type="supplementary-material" rid="SD1">Figure 7 in Appendix B</xref>). This hinders us from using traditional Bayesian approaches for the approximation of RNA first passage times, e.g., by training a variational autoencoder (VAE) (<xref ref-type="bibr" rid="R27">Kingma, 2013</xref>). Therefore, we take an alternative approach, training a PFN solely on a synthetic prior of RNA first passage time distributions. However, developing a synthetic prior for molecular problems is challenging since it seems impossible to generate meaningful synthetic combinations of molecule features with posterior information from a process depending on these features. We, therefore, develop <italic>KinPFN</italic> independent of molecular features and restrict its input to first passage times only. This offers the advantage that we can apply <italic>KinPFN</italic> to predict first passage time distributions at test time, independent of the underlying data-generating process.</p><p id="P21">For the development of our synthetic FPT prior, we leverage the observation that RNA first passage time distributions often exhibit CDFs with regions of slower growth interspersed with steeper transitions, leading to distinct plateaus and multiple changes between convex and concave sections representing inefficiencies in the corresponding folding pathway (<xref ref-type="bibr" rid="R15">Flamm et al., 2000</xref>; <xref ref-type="bibr" rid="R47">Wolfinger et al., 2004</xref>). These patterns make multi-modal distributions a natural choice to model the complexity of such processes synthetically, as they are designed to capture data with multiple local maxima or modes (<xref ref-type="bibr" rid="R23">Hartigan &amp; Hartigan, 1985</xref>). We thus construct a prior distribution over RNA first passage times as a family of multi-modal Gaussian distributions {<italic>P<sub>ψ<sub>k</sub></sub></italic> | <italic>k</italic> ∈ {2,3,4, 5}, <italic>ψ<sub>k</sub> ∈</italic> Ψ<sub><italic>k</italic></sub>}. Each multi-modal distribution in this family comprises <italic>k</italic> Gaussian components, each characterized by its own mean <italic>μ<sub>i</sub></italic> and standard deviation <italic>σ<sub>i</sub></italic>, <italic>i</italic> = 1, …, <italic>k</italic>. The parameter space Ψ<sub><italic>k</italic></sub> thus defines the family of distributions, with each specific distribution parameterized by a vector ψ<sub><bold>k</bold></sub> = ((<italic>μ</italic><sub>1</sub>, <italic>σ</italic><sub>1</sub>), (<italic>μ</italic><sub>2</sub>, <italic>σ</italic><sub>2</sub>), …, (<italic>μ<sub>k</sub></italic>, <italic>σ<sub>k</sub></italic>)) within Ψ<sub><italic>k</italic></sub>. We illustrate a synthetic bi-modal PDF alongside its corresponding CDF and examples of synthetic first passage time CDFs in <xref ref-type="supplementary-material" rid="SD1">Figure 8</xref>.</p><p id="P22">Since we cannot make any further assumptions about the distribution of folding times, especially when generating synthetic data, <italic>x</italic> and <italic>y</italic> of a prior distribution <italic>p</italic>(<italic>ψ<sub>k</sub></italic>) are considered completely independent. Consequently, we decide to assign a value of zero to all variables <italic>x</italic>, representing no prior information, while the <italic>y</italic> variables are ultimately sampled from the aforementioned multimodal distributions. As the targets <italic>y</italic> represent synthetic first passage times, they will be referred to as <italic>t</italic> from this point forward. We set the range of possible first passage time values <italic>t</italic> ~ <italic>p</italic>(<italic>ψ<sub>k</sub>)</italic> to [10<sup>-6</sup>, 10<sup>15</sup>], a range that covers a large fraction of possible folding processes based on observations from preliminary kinetic simulations. To mimic realistic first passage time distributions, we choose bounded uniform base means <inline-formula><mml:math id="M6"><mml:mrow><mml:msubsup><mml:mi>μ</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>base</mml:mtext></mml:mrow></mml:msubsup><mml:mo>~</mml:mo><mml:mi mathvariant="script">U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>16</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, and uniformly distributed standard deviations <inline-formula><mml:math id="M7"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>~</mml:mo><mml:mi mathvariant="script">U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>4.2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> based on preliminary experiments. To increase the variability of the prior, we introduce a uniformly distributed shifting parameter <inline-formula><mml:math id="M8"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>~</mml:mo><mml:mi mathvariant="script">U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>6</mml:mn><mml:mo>,</mml:mo><mml:mn>15</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, which is sampled only once and fixed for all <italic>i</italic> = 1 <italic>k</italic>. The final means <italic>μ<sub>i</sub></italic> are then given by: <disp-formula id="FD2"><label>(2)</label><mml:math id="M9"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>μ</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>base</mml:mtext></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>δ</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P23">with the probability density function (PDF) of the multi-modal Gaussian distribution parameterized by <italic>ψ<sub>k</sub></italic> expressed as <disp-formula id="FD3"><label>(3)</label><mml:math id="M10"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover><mml:mrow><mml:mi>exp</mml:mi></mml:mrow></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P24">for a value <italic>x</italic>.</p><p id="P25">To sample first passage times (FPTs) from these PDFs, we generate the PDF over a logarithmically spaced range of <italic>x</italic>-values within the provided FPT bounds and employ the inverse transformation method, known as the Smirnov transformation. The required series of calculations to derive the CDF, its quantile function CDF<sup>−1</sup>, different normalizations to properly scale the functions, and logarithmic transformations are detailed in <xref ref-type="supplementary-material" rid="SD1">Appendix C.1</xref>. The prior distribution over synthetic RNA first passage times used in this work is then represented by the log-encoded samples from a multi-modal Gaussian distribution <italic>p</italic>(<italic>ψ<sub>k</sub></italic>) ∈ <italic>P<sub>ψ<sub>k</sub></sub></italic> : <disp-formula id="FD4"><label>(4)</label><mml:math id="M11"><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Y</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>log</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>CDF</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="script">U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>∣</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p></sec><sec id="S8"><label>4.2</label><title>PFNs for the Approximation of RNA Folding Time Distributions</title><p id="P26">We propose to use PFNs (<xref ref-type="bibr" rid="R35">Müller et al., 2022</xref>) to accelerate kinetic simulations for RNA first passage time distributions. During training, the PFN <italic>q<sub>θ</sub></italic> with model parameters <italic>θ</italic> is presented with <italic>M</italic> synthetic first passage times, <inline-formula><mml:math id="M12"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mn>0</mml:mn><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, sampled from the prior distribution <italic>p</italic>(<italic>ψ<sub>k</sub></italic>). To enable the model to generalize across varying amounts of training data instead of a fixed number of context folding times, this example set is split at a random cutoff point <inline-formula><mml:math id="M13"><mml:mrow><mml:mi>N</mml:mi><mml:mo>~</mml:mo><mml:mi mathvariant="script">U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, resulting in a training subset <inline-formula><mml:math id="M14"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mtext>train</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mn>0</mml:mn><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, while the remaining first passage times are held out via masking. These held-out times, t<sub>test</sub> = {<italic>t</italic><sub><italic>N</italic> +1</sub>, …, <italic>t<sub>M</sub></italic>}, are then used as targets for prediction by minimizing the prior-data negative log-likelihood (NLL) according to Equation 1: <disp-formula id="FD5"><label>(5)</label><mml:math id="M15"><mml:mrow><mml:msub><mml:mi>ℓ</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext>test</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∪</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mtext>train</mml:mtext></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>log</mml:mi><mml:msub><mml:mi>q</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext>test</mml:mtext></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mn>0</mml:mn><mml:mrow><mml:mtext>test</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mtext>train</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P27"><xref ref-type="fig" rid="F2">Figure 2</xref> schematically illustrates this training process of <italic>KinPFN</italic> for a single batch of size <italic>B</italic>, along with its application in approximating the posterior predictive distribution (PPD) of RNA first passage times using <italic>N</italic> real folding times as context obtained from a kinetic simulator.</p><sec id="S9"><title><italic>KinPFN</italic> Architecture and Hyperparameters</title><p id="P28">We adopt the transformer-based (<xref ref-type="bibr" rid="R45">Vaswani et al., 2017</xref>) PFN architecture as proposed by <xref ref-type="bibr" rid="R35">Müller et al. (2022)</xref> and treat each pair (0, <italic>t</italic>) as a separate token. To learn the distribution of the targets rather than their specific ordering, we deliberately omit positional encoding to maintain permutation invariance according to <xref ref-type="bibr" rid="R35">Müller et al. (2022)</xref>. Since the first passage times <italic>t</italic> have already been log-encoded to the range [−6, 15] in the prior distribution <italic>p</italic>(<italic>ψ<sub>k</sub></italic>) (see <xref ref-type="sec" rid="S7">Section 4.1</xref>), we encode the input with a linear layer after normalizing the data to zero mean and a standard deviation of one while preserving the distributional properties. Following <xref ref-type="bibr" rid="R35">Müller et al. (2022)</xref>, we mask the attention matrix s.t. each position only attends to the training positions. This ensures that only training examples influence each other while test samples remain independent. We use the Adam optimizer (<xref ref-type="bibr" rid="R28">Kingma &amp; Ba, 2015</xref>) with a cosine decay (<xref ref-type="bibr" rid="R31">Loshchilov &amp; Hutter, 2017</xref>) and a linear learning rate warm-up over 25% of the training steps as previously proposed (<xref ref-type="bibr" rid="R35">Müller et al., 2022</xref>; <xref ref-type="bibr" rid="R1">Adriaensen et al., 2023</xref>). <italic>KinPFN</italic> outputs a discretized distribution <italic>q<sub>θ</sub></italic> (<italic>t</italic>|0, <italic>D</italic><sub>train</sub>) (<italic>Riemann distribution</italic>; see <xref ref-type="bibr" rid="R35">Müller et al. (2022)</xref>) using a finite number of buckets with equal likelihood of containing <italic>t</italic>; a hyperparameter that is included in our hyperparameter optimization (HPO) procedure leading to a final number of 1,000 buckets for <italic>KinPFN</italic>, initialized on a batch of 100,000 prior samples. A visualization of the discretized distribution <italic>q<sub>θ</sub></italic> can be found in <xref ref-type="supplementary-material" rid="SD1">Appendix H.4</xref>. Further hyperparameters, like the number of layers, the embedding size, or the learning rate are inherited from the Transformer architecture. Given the infinite nature of synthetic training data, we set the dropout rate and the weight decay to zero. We tune hyperparameters in two separate runs using Neural Pipeline search (NePS) (<xref ref-type="bibr" rid="R43">Stoll et al., 2023</xref>). More details regarding hyperparameters, hyperparameter optimization, and the final configuration of <italic>KinPFN</italic> can be found in <xref ref-type="supplementary-material" rid="SD1">Appendix D</xref>. The final model of <italic>KinPFN</italic> was trained for roughly five hours on a single A40 GPU.</p></sec></sec></sec><sec id="S10"><label>5</label><title>Experiments</title><p id="P29"><italic>KinPFN</italic> was trained on synthetic datasets of RNA folding times to learn to predict the distribution of first passage times, conditioned on a few examples. Therefore, the predictions only depend on example folding times for a given RNA but not on other features, e.g., its length, sequence composition, structure, or energy parameters. In this section, we show that this feature of <italic>KinPFN</italic> is a main contributor to its practical relevance. First, we confirm its ability to transfer from the synthetic prior data to realistic scenarios using a test set of simulations for randomly generated RNAs (<xref ref-type="sec" rid="S11">Section 5.1</xref>). Then, we demonstrate the practical importance of <italic>KinPFN</italic> in two case studies: We show that <italic>KinPFN</italic> is capable of approximating first passage time distributions of natural RNAs (<xref ref-type="sec" rid="S13">Section 5.2</xref>) and analyze the folding efficiency of different RNA sequences (<xref ref-type="sec" rid="S15">Section 5.3</xref>). Finally, we assess <italic>KinPFN</italic>’s ability to generalize to different biological data by approximating gene expression data from a previous study (<xref ref-type="bibr" rid="R4">Bagnall et al., 2020</xref>) (<xref ref-type="sec" rid="S17">Section 5.4</xref>). Preliminary evaluations for the predictions on samples from the synthetic prior are shown in <xref ref-type="supplementary-material" rid="SD1">Appendix H.1</xref>. We report performance in terms of prior-data negative log-likelihood (NLL) between the approximated posterior predictive distribution (PPD) and the true first passage time distribution and mean absolute error (MAE) between the CDF of the approximated PPD <inline-formula><mml:math id="M16"><mml:mrow><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and the true target CDF <italic>F</italic>(<italic>t</italic>). More information about these measures can be found in <xref ref-type="supplementary-material" rid="SD1">Appendix F</xref>. All experiments analyzing runtimes were benchmarked on a single AMD Milan EPYC 7513 CPU with 2,6 GHz.</p><sec id="S11"><label>5.1</label><title><italic>KinPFN</italic> Transfers to Real-World Scenarios</title><p id="P30">We assess the general capabilities of <italic>KinPFN</italic> to transfer from synthetic data to data obtained from kinetic simulators. In particular, we analyze the robustness of <italic>KinPFN</italic> to changes in the sequence length of the RNA, the start and stop structure, and different kinetic simulators. To do so, we create a novel test set of 635 randomly generated RNA sequences with lengths between 15 and 147 nucleotides, run <italic>Kinfold</italic> (<xref ref-type="bibr" rid="R15">Flamm et al., 2000</xref>) for 1,000 simulations on each of the test samples and extract first passage times (FPTs) from the simulations. We compare <italic>KinPFN</italic> to GMMs and DP-GMMs across multiple modes as well as KDE on this dataset, evaluating their performance across varying amounts of context using identical context first passage times. The modes we use for the GMMs align with our assumption in the synthetic prior for <italic>KinPFN</italic> (<xref ref-type="sec" rid="S7">Section 4.1</xref>). Information about hyperparameter optimization for the competitors can be found in <xref ref-type="supplementary-material" rid="SD1">Appendix E</xref>. To analyze the performance of <italic>KinPFN</italic> for arbitrary folding paths that do not include the unfolded or minimum free energy structural states, we additionally run <italic>Kinfold</italic> on a randomly generated RNA sequence of 75 nucleotides and predict the PPD of first passage times for alternative folding paths. For the evaluation of <italic>KinPFN</italic>’s robustness to changes of the simulator, we use the <italic>Kfold</italic> (<xref ref-type="bibr" rid="R11">Dykeman, 2015</xref>) kinetic simulator to obtain FPTs for a randomly generated RNA of length 56. We provide more details about our novel test set in <xref ref-type="supplementary-material" rid="SD1">Appendix G</xref>. Predictions with different context lengths, more competitor evaluations, and results for additional RNAs are reported in <xref ref-type="supplementary-material" rid="SD1">Appendix H.2</xref> and <xref ref-type="supplementary-material" rid="SD1">H.3</xref>.</p><sec id="S12" sec-type="results"><title>Results</title><p id="P31"><xref ref-type="table" rid="T1">Table 1</xref> provides a comparison of <italic>KinPFN</italic> with the GMM, DP-GMM and KDE on our introduced test set with respect to NLL. Please find results for mean absolute error (MAE) and Kolmogorov-Smirnov (KS) statistic as well as explanations about these metrics in <xref ref-type="supplementary-material" rid="SD1">Appendix H.2</xref> and <xref ref-type="supplementary-material" rid="SD1">F</xref>, respectively. Across all three metrics, <italic>KinPFN</italic> consistently demonstrates lower mean losses from a sample size of 25 onwards, outperforming the other approaches across various context first passage times (<italic>N</italic> ∈ {25, 50, 75, 100}). Consistent with our expectations, the performance of <italic>KinPFN</italic> constantly improves with more context FPTs. For a context size of ten, <italic>KinPFN</italic> performs slightly worse than KDE in terms of MAE and KS, achieving the second best performance while still outperforming KDE in terms of NLL. However, while we do observe visually strong approximations with a context size of ten in later experiments with <italic>KinPFN</italic> (see e.g. <xref ref-type="sec" rid="S15">Section 5.3</xref>), we note that these results should be taken with care due to relatively large KS values, indicating that the predicted distributions do not strongly match with the ground truth distributions at a context size of ten. As shown in <xref ref-type="fig" rid="F3">Figure 3a</xref>, <italic>KinPFN</italic> performs well across all sequences of the test set independent of the sequence length, given only 25 context points. This is an important finding since especially simulations for long RNAs could benefit from accelerations with <italic>KinPFN</italic>. Similarly, we observe a very good fit of the approximation of the CDF of first passage times for folding paths between alternative structures (<xref ref-type="fig" rid="F3">Figure 3b</xref>) and the application of <italic>KinPFN</italic> to simulations obtained from <italic>Kfold</italic> instead of <italic>Kinfold</italic> (<xref ref-type="fig" rid="F3">Figure 3c</xref>). Our results thus indicate that <italic>KinPFN</italic> seems to generalize across different sequence lengths, start and stop structures, and different simulators. Notably, the approximations with <italic>KinPFN</italic> only require 2,5% of the compute budget of the original simulators to achieve comparable results. However, the accuracy of the <italic>KinPFN</italic> approximations across all experiments can be further improved as we observe better performance with an increasing number of context examples (see also <xref ref-type="supplementary-material" rid="SD1">Table 8, 9</xref>, and <xref ref-type="supplementary-material" rid="SD1">10</xref> in <xref ref-type="supplementary-material" rid="SD1">Appendix H.2</xref>). This, however, comes at the cost of additional simulator runtime.</p></sec></sec><sec id="S13"><label>5.2</label><title><italic>KinPFN</italic> Approximates First Passage Times of Eukaryotic RNAs</title><p id="P32">While we observed robust performance of <italic>KinPFN</italic> for randomly generated RNA sequences, predictions for natural RNAs might be more challenging. In particular, highly structured RNAs like transfer RNAs (tRNA) or ribosomal RNAs (rRNA) might show different folding behavior compared to random RNA sequences due to million years of evolutionary pressure (<xref ref-type="bibr" rid="R46">Vicens &amp; Kieft, 2022</xref>; <xref ref-type="bibr" rid="R24">Herschlag, 1995</xref>). We, therefore, decide to evaluate <italic>KinPFN</italic> on a tRNA<sup>phe</sup> of 76 nucleotides (RNAcentral Id: URS000011107D_4932) and a 5S rRNA of 121 nucleotides (RNAcentral Id: URS000055688D_559292) from <italic>Saccharomyces cerevisiae</italic>, one of the most extensively studied eukaryotic model organisms in molecular and cell biology, commonly known as brewer’s yeast. For our experiments, we again use 1,000 <italic>Kinfold</italic> simulations as the ground truth data.</p><sec id="S14" sec-type="results"><title>Results</title><p id="P33"><xref ref-type="fig" rid="F4">Figure 4</xref> shows the first passage time CDF approximations of <italic>KinPFN</italic> for the tRNA<sup>phe</sup> (left). We observe that <italic>KinPFN</italic> is capable of approximating the ground truth data nearly perfectly using only 50 context first passage times. The runtime plot in <xref ref-type="fig" rid="F4">Figure 4</xref> (right) visualizes the decrease of the computational demands as the approximations of <italic>KinPFN</italic> result from using only 5% of the original compute budget, reducing the required CPU time from approximately 2,686 minutes (1000 simulations) to 170 minutes (50 simulations) while achieving nearly identical results. More predictions with different context times, as well as similar results for the 5S rRNA and further RNA types, are shown in <xref ref-type="supplementary-material" rid="SD1">Appendix H.5</xref> and <xref ref-type="supplementary-material" rid="SD1">H.6</xref>. We conclude that <italic>KinPFN</italic> is capable of accurately approximating the CDFs of first passage times for real-world, structured RNAs like tRNA and rRNA.</p></sec></sec><sec id="S15"><label>5.3</label><title>Case Study: RNA Folding Efficiency Analysis</title><p id="P34">To demonstrate the utility of <italic>KinPFN</italic>, we conduct a case study focused on comparing the folding efficiency of three 43 nucleotide long RNA molecules (<italic>ϕ</italic><sub>0</sub>, <italic>ϕ</italic><sub>1</sub>, <italic>ϕ</italic><sub>2</sub>) that are predicted to fold into the same minimum free energy (MFE) structure. Alterations in the RNA sequences, such as mutations or modifications – often driven by evolutionary optimization – can have a significant effect on the folding dynamics (<xref ref-type="bibr" rid="R15">Flamm et al., 2000</xref>). A comparison of the CDFs of first passage times can distinguish molecules that fold more or less efficiently and provide information about how alternations in the molecules impact the folding behavior, an important aspect for RNA-based therapeutics (<xref ref-type="bibr" rid="R33">Mollica et al., 2022</xref>). For our experiment, we simulate 1,000 folding trajectories from the open chain to the MFE structure using <italic>Kinfold</italic> and calculate the ground truth first passage time CDFs shown in the left plot of <xref ref-type="fig" rid="F5">Figure 5</xref> for each of the 3 RNA molecules.</p><sec id="S16" sec-type="results"><title>Results</title><p id="P35">We find that <italic>KinPFN</italic> captures the general folding behavior of the RNAs accurately, as shown in <xref ref-type="fig" rid="F5">Figure 5</xref> (right). However, while it captures the saddle points of the CDFs of <italic>ϕ</italic><sub>1</sub> (orange) and <italic>ϕ</italic><sub>2</sub> (green) arguably well, it is slightly less accurate for the most efficiently folding RNA, <italic>ϕ</italic><sub>0</sub> (blue). Remarkably, the <italic>KinPFN</italic> approximations were obtained using only ten context times, marking a 100× speed-up compared to each of the three individual simulation trajectories. Results for more approximations using different context lengths are shown in <xref ref-type="supplementary-material" rid="SD1">Appendix H.7</xref>.</p></sec></sec><sec id="S17"><label>5.4</label><title><italic>KinPFN</italic> Generalizes to Gene Expression Data</title><p id="P36">Besides their usage in RNA folding kinetics analysis, CDFs of different distributions are a common tool for the analysis of biological data. For example, <xref ref-type="bibr" rid="R4">Bagnall et al. (2020)</xref> analyzed the messenger RNA (mRNA) expression of interleukin-1-<italic>α</italic> (<italic>IL-1α</italic>), interleukin-1-<italic>β</italic> (<italic>IL-1β</italic>), and tumor necrosis factor-alpha (<italic>TNF-α</italic>) to study inducible gene expression in the immune toll-like receptor (TLR) system. Using single-molecule fluorescence <italic>in situ</italic> hybridization (smFISH) (<xref ref-type="bibr" rid="R13">Femino et al., 1998</xref>; <xref ref-type="bibr" rid="R39">Raj et al., 2008</xref>) analysis of the cumulative probability distribution of <italic>IL-1α</italic>, <italic>IL-1β</italic>, and <italic>TNF-α</italic> mRNA expression in two cell lines (established RAW 264.7 macrophage cells and bone-marrow-derived macrophages (BMDM)) stimulated with lipid A, <xref ref-type="bibr" rid="R4">Bagnall et al. (2020)</xref> demonstrate conserved variability in the TLR system across cell types, suggesting different modes of regulation of <italic>IL-1β</italic> and <italic>TNF-α</italic> expression. We use this experiment to analyze the capability of <italic>KinPFN</italic> to generalize to different biological data. Specifically, we use the raw count data of 447, 718, and 356 RAW 264.7 and 447, 732, and 322 BMDM cells for <italic>IL-1α, IL-1β</italic>, and <italic>TNF-α</italic>, respectively, to predict the cumulative probability functions of mRNA expression to replicate the outcome of the smFISH experiment of <xref ref-type="bibr" rid="R4">Bagnall et al. (2020)</xref> with <italic>KinPFN</italic> while using only a fraction of the data.</p><sec id="S18" sec-type="results"><title>Results</title><p id="P37"><xref ref-type="fig" rid="F6">Figure 6</xref> illustrates the approximations of the mRNA expression of <italic>IL-1α, IL-1β</italic>, and <italic>TNF-α</italic>. We observe that <italic>KinPFN</italic> can approximate the gene expression with high accuracy, using only roughly 8% of the expression data. These results suggest that – besides its application to RNA folding kinetics – <italic>KinPFN</italic> could be a valuable tool for different types of analysis across biological questions, including the potential to speed up even wet-lab experiments (see also <xref ref-type="supplementary-material" rid="SD1">Appendix H.8</xref>).</p></sec></sec></sec><sec id="S19"><label>6</label><title>Conclusion, Limitations &amp; Future Work</title><p id="P38">We present <italic>KinPFN</italic>, the first work that uses prior-data fitted networks for biological data. Trained on a synthetic prior, we show that our novel approach can accurately model RNA folding kinetics while accelerating RNA first passage time analysis by orders of magnitude. Moreover, we demonstrate that <italic>KinPFN</italic> generalizes to gene expression data obtained from wet-lab smFISH analysis, suggesting that <italic>KinPFN</italic> could be applicable to the analysis of a wide range of different biological questions.</p><sec id="S20"><title>Limitations</title><p id="P39">While showing impressive accuracy across multiple tasks, <italic>KinPFN</italic> also has limitations. Since it is purely trained on synthetic first passage time data, it depends on a data-generating approach like kinetic simulators during inference. Consequently, <italic>KinPFN</italic>’s performance is bounded by the accuracy of the simulator. Incorporating other features, like the RNA sequence, structure, or energy information, could mitigate this issue. However, it is an open problem to implement the required information in a synthetic prior without using external data sources. Additionally, <italic>KinPFN</italic> would benefit from larger-scale evaluations, e.g., on longer RNAs, to confirm its independence of RNA features like sequence length. However, obtaining this kind of data is currently infeasible due to the large computing demands of available simulators and the problem’s complexity. Further, <italic>KinPFN</italic> is limited to a bounded range of time values; however, so far, we have not experienced this limitation to be a major problem, and the training time of <italic>KinPFN</italic> is moderate, allowing retraining on adapted ranges. Similar to GMMs and KDEs, the performance of <italic>KinPFN</italic> strongly depends on the provided context. We tried to compensate for that by showing mean and standard deviation around the mean across 20 context inputs to quantify the variation in <italic>KinPFN</italic> approximations.</p></sec><sec id="S21"><title>Future Work</title><p id="P40">Using synthetic data for biological applications appears very promising. Unlike GMMs or standard KDEs, <italic>KinPFN</italic> is not limited to predefined kernels or Gaussian distributions; we consider the definition of synthetic priors using different distributions as future work. Generally, PFNs could play an important role in the field of structural biology, with the potential to substantially impact biological analysis, offering tremendous possibilities to accelerate scientific discovery.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Appendix</label><media xlink:href="EMS199509-supplement-Appendix.pdf" mimetype="application" mime-subtype="pdf" id="d82aAcGbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S22"><title>Acknowledgments</title><p>Dominik Scheuer and Frederic Runge would like to thank Samuel Müller and Steven Adriaensen for helpful discussions and valuable comments. This work is supported in part by the European Union’s Horizon Europe Doctoral Network programme under the Marie-Skłodowska-Curie grant agreement No 101072930 (TACsy), the Novo Nordisk Foundation grant NNF21OC0066551 (MATOMIC), and the Austrian Science Fund FWF grant I-6440 N. The authors further acknowledge funding by the German Research Foundation (DFG) under SFB 1597 (SmallData), grant no. 499552394, and through grant no. 417962828 as well as support by the state of Baden-Württemberg through bwHPC and the German Research Foundation (DFG) through grant no INST 39/963-1 FUGG (bwForCluster NEMO) and grant INST 35/1597-1 FUGG (bwForCluster Helix). Frank Hutter acknowledges the financial support of the Hector Foundation. This research was funded by the European Union (via ERC Consolidator Grant DeepLearning 2.0, grant no. 101045765). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them.</p></ack><fn-group><fn id="FN1"><p id="P41">Reproducibility Statement</p><p id="P42">To ensure the reproducibility of our results, we have made our source code, the trained model, and datasets publicly available at <ext-link ext-link-type="uri" xlink:href="https://github.com/automl/KinPFN">https://github.com/automl/KinPFN</ext-link>. The repository contains detailed instructions for setting up the required conda environment and package installs (see README.md). Model checkpoints of <italic>KinPFN</italic> are provided in the models directory. The validation and test sets are stored in the <monospace>neps_validation_set</monospace> and <monospace>kinpfn_testing_set</monospace> directories, respectively. We provide notebooks (along with the required experiment data) to demonstrate the training and evaluation of <italic>KinPFN</italic> and for reproducing results in the <monospace>notebooks</monospace> directory. We recommend using a single GPU with at least 48GB of memory for training <italic>KinPFN</italic>. However, for inference, a single CPU should be sufficient. Following the provided instructions, it should be straightforward to reproduce our environment, train and evaluate <italic>KinPFN</italic>, and replicate our experiments with minimal effort.</p></fn><fn id="FN2" fn-type="con"><p id="P43">Author Contributions</p><p id="P44">F.R. conceptualized the study and developed the methodology. D.S. and F.R. wrote the manuscript, designed figures, and were responsible for data curation. D.S. implemented the model and conducted all experiments. J.F. contributed to the experimental design and baseline selection. M.W. and C.F. provided expertise in RNA kinetics simulations and theory. J.F., M.W., and C.F. assisted with manuscript refinement and figure layout. F.H. provided project supervision and secured funding.</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Adriaensen</surname><given-names>Steven</given-names></name><name><surname>Rakotoarison</surname><given-names>Herilalaina</given-names></name><name><surname>Müller</surname><given-names>Samuel</given-names></name><name><surname>Hutter</surname><given-names>Frank</given-names></name></person-group><source>Efficient bayesian learning curve extrapolation using prior-data fitted networks</source><conf-name>Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS 2023)</conf-name><year>2023</year><comment>URL <ext-link ext-link-type="uri" xlink:href="https://openreview.net/forum?id=xgTV6rmH6n">https://openreview.net/forum?id=xgTV6rmH6n</ext-link></comment></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aviram</surname><given-names>Iddo</given-names></name><name><surname>Veltman</surname><given-names>Ilia</given-names></name><name><surname>Churkin</surname><given-names>Alexander</given-names></name><name><surname>Barash</surname><given-names>Danny</given-names></name></person-group><article-title>Efficient procedures for the numerical simulation of mid-size rna kinetics</article-title><source>Algorithms for Molecular Biology</source><year>2012</year><volume>7</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="pmcid">PMC3463434</pub-id><pub-id pub-id-type="pmid">22958879</pub-id><pub-id pub-id-type="doi">10.1186/1748-7188-7-24</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badelt</surname><given-names>Stefan</given-names></name><name><surname>Lorenz</surname><given-names>Ronny</given-names></name><name><surname>Hofacker</surname><given-names>Ivo L</given-names></name></person-group><article-title>Drtransformer: heuristic cotranscriptional rna folding using the nearest neighbor energy model</article-title><source>Bioinformatics</source><year>2023</year><volume>39</volume><issue>1</issue><elocation-id>btad034</elocation-id><pub-id pub-id-type="pmcid">PMC9889959</pub-id><pub-id pub-id-type="pmid">36655786</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btad034</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bagnall</surname><given-names>James</given-names></name><name><surname>Rowe</surname><given-names>William</given-names></name><name><surname>Alachkar</surname><given-names>Nissrin</given-names></name><name><surname>Roberts</surname><given-names>James</given-names></name><name><surname>England</surname><given-names>Hazel</given-names></name><name><surname>Clark</surname><given-names>Christopher</given-names></name><name><surname>Platt</surname><given-names>Mark</given-names></name><name><surname>Jackson</surname><given-names>Dean A</given-names></name><name><surname>Muldoon</surname><given-names>Mark</given-names></name><name><surname>Paszek</surname><given-names>Pawel</given-names></name></person-group><article-title>Gene-specific linear trends constrain transcriptional variability of the toll-like receptor signaling</article-title><source>Cell Systems</source><year>2020</year><volume>11</volume><issue>3</issue><fpage>300</fpage><lpage>314</lpage><pub-id pub-id-type="pmcid">PMC7521480</pub-id><pub-id pub-id-type="pmid">32918862</pub-id><pub-id pub-id-type="doi">10.1016/j.cels.2020.08.007</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bishop</surname><given-names>Christopher</given-names></name></person-group><source>Pattern Recognition and Machine Learning</source><year>2006</year><month>01</month><volume>16</volume><fpage>140</fpage><lpage>155</lpage><pub-id pub-id-type="doi">10.1117/1.2819119</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blei</surname><given-names>David M</given-names></name><name><surname>Jordan</surname><given-names>Michael I</given-names></name></person-group><article-title>Variational inference for Dirichlet process mixtures</article-title><source>Bayesian Analysis</source><year>2006</year><volume>1</volume><issue>1</issue><fpage>121</fpage><lpage>143</lpage><comment>URL <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1214/06-BA104">https://doi.org/10.1214/06-BA104</ext-link></comment><pub-id pub-id-type="doi">10.1214/06-BA104</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Shi-Jie</given-names></name></person-group><article-title>Rna folding: conformational statistics, folding kinetics, and ion electrostatics</article-title><source>Annual Review of Biophysics</source><year>2008</year><volume>37</volume><fpage>197</fpage><lpage>214</lpage><pub-id pub-id-type="pmcid">PMC2473866</pub-id><pub-id pub-id-type="pmid">18573079</pub-id><pub-id pub-id-type="doi">10.1146/annurev.biophys.37.032807.125957</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Conlon</surname><given-names>Elizabeth G</given-names></name><name><surname>Manley</surname><given-names>James L</given-names></name></person-group><article-title>RNA-binding proteins in neurodegeneration: mechanisms in aggregate</article-title><source>Genes &amp; Development</source><year>2017</year><month>August</month><volume>31</volume><issue>15</issue><fpage>1509</fpage><lpage>1528</lpage><pub-id pub-id-type="pmcid">PMC5630017</pub-id><pub-id pub-id-type="pmid">28912172</pub-id><pub-id pub-id-type="doi">10.1101/gad.304055.117</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Danilova</surname><given-names>Ludmila V</given-names></name><name><surname>Pervouchine</surname><given-names>Dmitri D</given-names></name><name><surname>Favorov</surname><given-names>Alexander V</given-names></name><name><surname>Mironov</surname><given-names>Andrei A</given-names></name></person-group><article-title>Rnakinetics: a web server that models secondary structure kinetics of an elongating rna</article-title><source>Journal of bioinformatics and computational biology</source><year>2006</year><volume>4</volume><issue>02</issue><fpage>589</fpage><lpage>596</lpage><pub-id pub-id-type="pmid">16819804</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dooley</surname><given-names>Samuel</given-names></name><name><surname>Khurana</surname><given-names>Gurnoor Singh</given-names></name><name><surname>Mohapatra</surname><given-names>Chirag</given-names></name><name><surname>Naidu</surname><given-names>Siddartha V</given-names></name><name><surname>White</surname><given-names>Colin</given-names></name></person-group><article-title>Forecastpfn: Synthetically-trained zero-shot forecasting</article-title><source>Advances in Neural Information Processing Systems</source><year>2024</year><volume>36</volume></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dykeman</surname><given-names>Eric C</given-names></name></person-group><article-title>An implementation of the Gillespie algorithm for RNA kinetics with logarithmic time update</article-title><source>Nucleic Acids Research</source><year>2015</year><month>05</month><volume>43</volume><issue>12</issue><fpage>5708</fpage><lpage>5715</lpage><comment>ISSN 0305-1048 URL <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/nar/gkv480">https://doi.org/10.1093/nar/gkv480</ext-link></comment><pub-id pub-id-type="pmcid">PMC4499123</pub-id><pub-id pub-id-type="pmid">25990741</pub-id><pub-id pub-id-type="doi">10.1093/nar/gkv480</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fang</surname><given-names>Xianyang</given-names></name><name><surname>Stagno</surname><given-names>Jason R</given-names></name><name><surname>Bhandari</surname><given-names>Yuba R</given-names></name><name><surname>Zuo</surname><given-names>Xiaobing</given-names></name><name><surname>Wang</surname><given-names>Yun-Xing</given-names></name></person-group><article-title>Small-angle x-ray scattering: a bridge between rna secondary structures and three-dimensional topological structures</article-title><source>Current Opinion in Structural Biology</source><year>2015</year><volume>30</volume><fpage>147</fpage><lpage>160</lpage><comment>ISSN 0959-440X URL <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0959440X15000196">https://www.sciencedirect.com/science/article/pii/S0959440X15000196</ext-link>. Folding and binding/Nucleic acids and their protein complexes</comment><pub-id pub-id-type="pmcid">PMC6676907</pub-id><pub-id pub-id-type="pmid">25765781</pub-id><pub-id pub-id-type="doi">10.1016/j.sbi.2015.02.010</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Femino</surname><given-names>Andrea M</given-names></name><name><surname>Fay</surname><given-names>Fredric S</given-names></name><name><surname>Fogarty</surname><given-names>Kevin</given-names></name><name><surname>Singer</surname><given-names>Robert H</given-names></name></person-group><article-title>Visualization of single rna transcripts in situ</article-title><source>Science</source><year>1998</year><volume>280</volume><issue>5363</issue><fpage>585</fpage><lpage>590</lpage><pub-id pub-id-type="pmid">9554849</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flamm</surname><given-names>Christoph</given-names></name><name><surname>Hofacker</surname><given-names>Ivo Ludwig</given-names></name></person-group><article-title>Beyond energy minimization: Approaches to the kinetic folding of RNA</article-title><source>Chemical Monthly</source><year>2008</year><volume>139</volume><issue>4</issue><fpage>447</fpage><lpage>457</lpage><pub-id pub-id-type="doi">10.1007/s00706-008-0895-3</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flamm</surname><given-names>Christoph</given-names></name><name><surname>Fontana</surname><given-names>Walter</given-names></name><name><surname>Hofacker</surname><given-names>Ivo L</given-names></name><name><surname>Schuster</surname><given-names>Peter</given-names></name></person-group><article-title>RNA folding at elementary step resolution</article-title><source>RNA</source><year>2000</year><month>March</month><volume>6</volume><issue>3</issue><fpage>325</fpage><lpage>338</lpage><pub-id pub-id-type="pmcid">PMC1369916</pub-id><pub-id pub-id-type="pmid">10744018</pub-id><pub-id pub-id-type="doi">10.1017/s1355838200992161</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flamm</surname><given-names>Christoph</given-names></name><name><surname>Hofacker</surname><given-names>Ivo L</given-names></name><name><surname>Stadler</surname><given-names>Peter F</given-names></name><name><surname>Wolfinger</surname><given-names>Michael T</given-names></name></person-group><article-title>Barrier trees of degenerate landscapes</article-title><source>Zeitschrift für Physikalische Chemie</source><year>2002</year><volume>216</volume><issue>2</issue><fpage>155</fpage><comment>URL <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1524/zpch.2002.216.2.155">https://doi.org/10.1524/zpch.2002.216.2.155</ext-link></comment><pub-id pub-id-type="doi">10.1524/zpch.2002.216.2.155</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franke</surname><given-names>Jörg</given-names></name><name><surname>Runge</surname><given-names>Frederic</given-names></name><name><surname>Hutter</surname><given-names>Frank</given-names></name></person-group><article-title>Probabilistic transformer: Modelling ambiguities and distributions for rna folding and molecule design</article-title><source>Advances in Neural Information Processing Systems</source><year>2022</year><volume>35</volume><fpage>26856</fpage><lpage>26873</lpage></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franke</surname><given-names>Jörg KH</given-names></name><name><surname>Runge</surname><given-names>Frederic</given-names></name><name><surname>Köksal</surname><given-names>Ryan</given-names></name><name><surname>Backofen</surname><given-names>Rolf</given-names></name><name><surname>Hutter</surname><given-names>Frank</given-names></name></person-group><article-title>Rnaformer: A simple yet effective deep learning model for rna secondary structure prediction</article-title><source>bioRxiv</source><year>2024</year><pub-id pub-id-type="doi">10.1101/2024.02.12.579881</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>Laiyi</given-names></name><name><surname>Cao</surname><given-names>Yingxin</given-names></name><name><surname>Wu</surname><given-names>Jie</given-names></name><name><surname>Peng</surname><given-names>Qinke</given-names></name><name><surname>Nie</surname><given-names>Qing</given-names></name><name><surname>Xie</surname><given-names>Xiaohui</given-names></name></person-group><article-title>Ufold: fast and accurate rna secondary structure prediction with deep learning</article-title><source>Nucleic acids research</source><year>2022</year><volume>50</volume><issue>3</issue><fpage>e14</fpage><pub-id pub-id-type="pmcid">PMC8860580</pub-id><pub-id pub-id-type="pmid">34792173</pub-id><pub-id pub-id-type="doi">10.1093/nar/gkab1074</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>Xiang-Dong</given-names></name></person-group><article-title>Non-coding RNA: a new frontier in regulatory biology</article-title><source>National Science Review</source><year>2014</year><volume>1</volume><issue>2</issue><fpage>190</fpage><lpage>204</lpage><pub-id pub-id-type="pmcid">PMC4374487</pub-id><pub-id pub-id-type="pmid">25821635</pub-id><pub-id pub-id-type="doi">10.1093/nsr/nwu008</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fukunaga</surname><given-names>Tsukasa</given-names></name><name><surname>Hamada</surname><given-names>Michiaki</given-names></name></person-group><article-title>Computational approaches for alternative and transient secondary structures of ribonucleic acids</article-title><source>Briefings in Functional Genomics</source><year>2019</year><volume>18</volume><issue>3</issue><fpage>182</fpage><lpage>191</lpage><pub-id pub-id-type="pmid">30689706</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Gilmer</surname><given-names>Justin</given-names></name><name><surname>Schoenholz</surname><given-names>Samuel S</given-names></name><name><surname>Riley</surname><given-names>Patrick F</given-names></name><name><surname>Vinyals</surname><given-names>Oriol</given-names></name><name><surname>Dahl</surname><given-names>George E</given-names></name></person-group><source>Neural message passing for quantum chemistry</source><conf-name>International conference on machine learning</conf-name><conf-sponsor>PMLR</conf-sponsor><year>2017</year><fpage>1263</fpage><lpage>1272</lpage></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hartigan</surname><given-names>JA</given-names></name><name><surname>Hartigan</surname><given-names>PM</given-names></name></person-group><article-title>The dip test of unimodality</article-title><source>The Annals of Statistics</source><year>1985</year><volume>13</volume><issue>1</issue><fpage>70</fpage><lpage>84</lpage><comment>ISSN 00905364, 21688966 URL <ext-link ext-link-type="uri" xlink:href="http://www.jstor.org/stable/2241144">http://www.jstor.org/stable/2241144</ext-link></comment></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herschlag</surname><given-names>Daniel</given-names></name></person-group><article-title>Rna chaperones and the rna folding problem</article-title><source>Journal of Biological Chemistry</source><year>1995</year><volume>270</volume><issue>36</issue><fpage>20871</fpage><lpage>20874</lpage><pub-id pub-id-type="pmid">7545662</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hofacker</surname><given-names>Ivo L</given-names></name><name><surname>Fontana</surname><given-names>Walter</given-names></name><name><surname>Stadler</surname><given-names>Peter F</given-names></name><name><surname>Sebastian Bonhoeffer</surname><given-names>L</given-names></name><name><surname>Tacker</surname><given-names>Manfred</given-names></name><name><surname>Schuster</surname><given-names>Peter</given-names></name><etal/></person-group><article-title>Fast folding and comparison of rna secondary structures</article-title><source>Monatshefte fur chemie</source><year>1994</year><volume>125</volume><fpage>167</fpage></element-citation></ref><ref id="R26"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hollmann</surname><given-names>Noah</given-names></name><name><surname>Müller</surname><given-names>Samuel</given-names></name><name><surname>Eggensperger</surname><given-names>Katharina</given-names></name><name><surname>Hutter</surname><given-names>Frank</given-names></name></person-group><source>Tabpfn: A transformer that solves small tabular classification problems in a second</source><conf-name>The Eleventh International Conference on Learning Representations (ICLR)</conf-name><year>2023</year></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>Diederik P</given-names></name></person-group><article-title>Auto-encoding variational bayes</article-title><source>arXiv preprint</source><year>2013</year><elocation-id>arXiv:1312.6114</elocation-id></element-citation></ref><ref id="R28"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>Diederik P</given-names></name><name><surname>Ba</surname><given-names>Jimmy</given-names></name></person-group><source>Adam: A method for stochastic optimization</source><conf-name>Proceedings of the 3rd International Conference on Learning Representations (ICLR)</conf-name><year>2015</year><comment>URL <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link></comment></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Lisha</given-names></name><name><surname>Jamieson</surname><given-names>Kevin</given-names></name><name><surname>DeSalvo</surname><given-names>Giulia</given-names></name><name><surname>Rostamizadeh</surname><given-names>Afshin</given-names></name><name><surname>Talwalkar</surname><given-names>Ameet</given-names></name></person-group><article-title>Hyperband: a novel bandit-based approach to hyperparameter optimization</article-title><source>J Mach Learn Res</source><year>2017</year><month>jan</month><volume>18</volume><issue>1</issue><fpage>6765</fpage><lpage>6816</lpage><comment>ISSN 1532-4435</comment></element-citation></ref><ref id="R30"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lodish</surname><given-names>Harvey</given-names></name><name><surname>Berk</surname><given-names>Arnold</given-names></name><name><surname>Matsudaira</surname><given-names>Paul</given-names></name><name><surname>Kaiser</surname><given-names>Chris A</given-names></name><name><surname>Krieger</surname><given-names>Monty</given-names></name><name><surname>Scott</surname><given-names>Matthew P</given-names></name><etal/></person-group><source>Molecular Cell Biology</source><publisher-name>W.H. Freeman and Co</publisher-name><publisher-loc>New York</publisher-loc><edition>5th edition</edition><year>2005</year></element-citation></ref><ref id="R31"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Loshchilov</surname><given-names>Ilya</given-names></name><name><surname>Hutter</surname><given-names>Frank</given-names></name></person-group><source>SGDR: Stochastic gradient descent with warm restarts</source><year>2017</year><comment>URL <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1608.03983">https://arxiv.org/abs/1608.03983</ext-link></comment></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McLachlan</surname><given-names>Geoffrey J</given-names></name><name><surname>Lee</surname><given-names>Sharon X</given-names></name><name><surname>Rathnayake</surname><given-names>Suren I</given-names></name></person-group><article-title>Finite mixture models</article-title><source>Annual Review of Statistics and Its Application</source><year>2019</year><volume>6</volume><fpage>355</fpage><lpage>378</lpage><comment>ISSN 2326-831X URL <ext-link ext-link-type="uri" xlink:href="https://www.annualreviews.org/content/journals/10.1146/annurev-statistics-031017-100325">https://www.annualreviews.org/content/journals/10.1146/annurev-statistics-031017-100325</ext-link>2019</comment><pub-id pub-id-type="doi">10.1146/annurev-statistics-031017-100325</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mollica</surname><given-names>Luca</given-names></name><name><surname>Cupaioli</surname><given-names>Francesca Anna</given-names></name><name><surname>Rossetti</surname><given-names>Grazisa</given-names></name><name><surname>Chiappori</surname><given-names>Federica</given-names></name></person-group><article-title>An overview of structural approaches to study therapeutic rnas</article-title><source>Frontiers in Molecular Biosciences</source><year>2022</year><volume>9</volume><elocation-id>1044126</elocation-id><pub-id pub-id-type="pmcid">PMC9649582</pub-id><pub-id pub-id-type="pmid">36387283</pub-id><pub-id pub-id-type="doi">10.3389/fmolb.2022.1044126</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Müller</surname><given-names>Samuel</given-names></name><name><surname>Feurer</surname><given-names>Matthias</given-names></name><name><surname>Hollmann</surname><given-names>Noah</given-names></name><name><surname>Hutter</surname><given-names>Frank</given-names></name></person-group><source>Pfns4bo: In-context learning for bayesian optimization</source><conf-name>International Conference on Machine Learning</conf-name><conf-sponsor>PMLR</conf-sponsor><year>2023</year><fpage>25444</fpage><lpage>25470</lpage></element-citation></ref><ref id="R35"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Müller</surname><given-names>Samuel</given-names></name><name><surname>Hollmann</surname><given-names>Noah</given-names></name><name><surname>Arango</surname><given-names>Sebastian Pineda</given-names></name><name><surname>Grabocka</surname><given-names>Josif</given-names></name><name><surname>Hutter</surname><given-names>Frank</given-names></name></person-group><source>Transformers can do bayesian inference</source><conf-name>International Conference on Learning Representations (ICLR)</conf-name><year>2022</year><comment>URL <ext-link ext-link-type="uri" xlink:href="https://openreview.net/forum?id=KSugKcbNf9">https://openreview.net/forum?id=KSugKcbNf9</ext-link></comment></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neal</surname><given-names>Radford M</given-names></name></person-group><article-title>Markov chain sampling methods for dirichlet process mixture models</article-title><source>Journal of Computational and Graphical Statistics</source><year>2000</year><volume>9</volume><issue>2</issue><fpage>249</fpage><lpage>265</lpage><comment>URL <ext-link ext-link-type="uri" xlink:href="https://www.tandfonline.com/doi/abs/10.1080/10618600.2000.10474879">https://www.tandfonline.com/doi/abs/10.1080/10618600.2000.10474879</ext-link></comment><pub-id pub-id-type="doi">10.1080/10618600.2000.10474879</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Patil</surname><given-names>Sharat</given-names></name><name><surname>Runge</surname><given-names>Frederic</given-names></name><name><surname>Franke</surname><given-names>Jörg KH</given-names></name><name><surname>Hutter</surname><given-names>Frank</given-names></name></person-group><source>Towards generative RNA design with tertiary interactions</source><conf-name>ICLR 2024 Workshop on Generative and Experimental Perspectives for Biomolecular Design</conf-name><year>2024</year><comment>URL <ext-link ext-link-type="uri" xlink:href="https://openreview.net/forum?id=pLzoHOceHN">https://openreview.net/forum?id=pLzoHOceHN</ext-link></comment></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prašnikar</surname><given-names>Eva</given-names></name><name><surname>Ljubič</surname><given-names>Martin</given-names></name><name><surname>Perdih</surname><given-names>Andrej</given-names></name><name><surname>Borišek</surname><given-names>Jure</given-names></name></person-group><article-title>Machine learning heralding a new development phase in molecular dynamics simulations</article-title><source>Artificial intelligence review</source><year>2024</year><volume>57</volume><issue>4</issue><fpage>102</fpage></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raj</surname><given-names>Arjun</given-names></name><name><surname>Van Den Bogaard</surname><given-names>Patrick</given-names></name><name><surname>Rifkin</surname><given-names>Scott A</given-names></name><name><surname>Van Oudenaarden</surname><given-names>Alexander</given-names></name><name><surname>Tyagi</surname><given-names>Sanjay</given-names></name></person-group><article-title>Imaging individual mrna molecules using multiple singly labeled probes</article-title><source>Nature methods</source><year>2008</year><volume>5</volume><issue>10</issue><fpage>877</fpage><lpage>879</lpage><pub-id pub-id-type="pmcid">PMC3126653</pub-id><pub-id pub-id-type="pmid">18806792</pub-id><pub-id pub-id-type="doi">10.1038/nmeth.1253</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rakotoarison</surname><given-names>Herilalaina</given-names></name><name><surname>Adriaensen</surname><given-names>Steven</given-names></name><name><surname>Mallik</surname><given-names>Neeratyoy</given-names></name><name><surname>Garibov</surname><given-names>Samir</given-names></name><name><surname>Bergman</surname><given-names>Eddie</given-names></name><name><surname>Hutter</surname><given-names>Frank</given-names></name></person-group><source>In-context freeze-thaw bayesian optimization for hyperparameter optimization</source><conf-name>Forty-first International Conference on Machine Learning (ICML)</conf-name><year>2024</year></element-citation></ref><ref id="R41"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rezende</surname><given-names>Danilo</given-names></name><name><surname>Mohamed</surname><given-names>Shakir</given-names></name></person-group><source>Variational inference with normalizing flows</source><conf-name>International conference on machine learning</conf-name><conf-sponsor>PMLR</conf-sponsor><year>2015</year><fpage>1530</fpage><lpage>1538</lpage></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Runge</surname><given-names>Frederic</given-names></name><name><surname>Franke</surname><given-names>Jörg</given-names></name><name><surname>Fertmann</surname><given-names>Daniel</given-names></name><name><surname>Backofen</surname><given-names>Rolf</given-names></name><name><surname>Hutter</surname><given-names>Frank</given-names></name></person-group><article-title>Partial rna design</article-title><source>Bioinformatics</source><year>2024</year><volume>40</volume><issue>Supplement_1</issue><fpage>i437</fpage><lpage>i445</lpage><pub-id pub-id-type="pmcid">PMC11256918</pub-id><pub-id pub-id-type="pmid">38940170</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btae222</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Stoll</surname><given-names>Danny</given-names></name><name><surname>Mallik</surname><given-names>Neeratyoy</given-names></name><name><surname>Schrodi</surname><given-names>Simon</given-names></name><name><surname>Janowski</surname><given-names>Maciej</given-names></name><name><surname>Garibov</surname><given-names>Samir</given-names></name><name><surname>Chakra</surname><given-names>Tarek Abou</given-names></name><name><surname>Rogalla</surname><given-names>Daniel</given-names></name><name><surname>Bergman</surname><given-names>Eddie</given-names></name><name><surname>Hvarfner</surname><given-names>Carl</given-names></name><name><surname>Ru</surname><given-names>Binxin</given-names></name><name><surname>Kober</surname><given-names>Nils</given-names></name><etal/></person-group><source>Neural pipeline search (NePS)</source><year>2023</year><month>October</month><comment>URL <ext-link ext-link-type="uri" xlink:href="https://github.com/automl/neps">https://github.com/automl/neps</ext-link></comment></element-citation></ref><ref id="R44"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>Richard S</given-names></name></person-group><source>Reinforcement learning: An introduction A Bradford Book</source><year>2018</year></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaswani</surname><given-names>Ashish</given-names></name><name><surname>Shazeer</surname><given-names>Noam</given-names></name><name><surname>Parmar</surname><given-names>Niki</given-names></name><name><surname>Uszkoreit</surname><given-names>Jakob</given-names></name><name><surname>Jones</surname><given-names>Llion</given-names></name><name><surname>Gomez</surname><given-names>Aidan N</given-names></name><name><surname>Kaiser</surname><given-names>Lukasz</given-names></name><name><surname>Polosukhin</surname><given-names>Illia</given-names></name></person-group><article-title>Attention is all you need</article-title><source>CoRR</source><year>2017</year><elocation-id>abs/1706.03762</elocation-id><comment>URL <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1706.03762">http://arxiv.org/abs/1706.03762</ext-link></comment></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vicens</surname><given-names>Quentin</given-names></name><name><surname>Kieft</surname><given-names>Jeffrey S</given-names></name></person-group><article-title>Thoughts on how to think (and talk) about rna structure</article-title><source>Proceedings of the National Academy of Sciences</source><year>2022</year><volume>119</volume><issue>17</issue><elocation-id>e2112677119</elocation-id><pub-id pub-id-type="pmcid">PMC9169933</pub-id><pub-id pub-id-type="pmid">35439059</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2112677119</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolfinger</surname><given-names>Michael T</given-names></name><name><surname>Andreas Svrcek-Seiler</surname><given-names>W</given-names></name><name><surname>Flamm</surname><given-names>Christoph</given-names></name><name><surname>Hofacker</surname><given-names>Ivo L</given-names></name><name><surname>Stadler</surname><given-names>Peter F</given-names></name></person-group><article-title>Efficient computation of RNA folding dynamics</article-title><source>Journal of Physics A: Mathematical and General</source><year>2004</year><month>April</month><volume>37</volume><issue>17</issue><elocation-id>4731</elocation-id><comment>URL <ext-link ext-link-type="uri" xlink:href="https://dx.doi.org/10.1088/0305-4470/37/17/005">https://dx.doi.org/10.1088/0305-4470/37/17/005</ext-link></comment><pub-id pub-id-type="doi">10.1088/0305-4470/37/17/005</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xayaphoummine</surname><given-names>Alain</given-names></name><name><surname>Bucher</surname><given-names>T</given-names></name><name><surname>Isambert</surname><given-names>Herve</given-names></name></person-group><article-title>Kinefold web server for rna/dna folding path and structure prediction including pseudoknots and knots</article-title><source>Nucleic acids research</source><year>2005</year><volume>33</volume><issue>suppl_2</issue><fpage>W605</fpage><lpage>W610</lpage><pub-id pub-id-type="pmcid">PMC1160208</pub-id><pub-id pub-id-type="pmid">15980546</pub-id><pub-id pub-id-type="doi">10.1093/nar/gki447</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>Angela M</given-names></name><name><surname>Gasper</surname><given-names>Paul M</given-names></name><name><surname>Strobel</surname><given-names>Eric J</given-names></name><name><surname>Watters</surname><given-names>Kyle E</given-names></name><name><surname>Chen</surname><given-names>Alan A</given-names></name><name><surname>Lucks</surname><given-names>Julius B</given-names></name></person-group><article-title>Computationally reconstructing cotranscriptional rna folding pathways from experimental data reveals rearrangement of non-native folding intermediates</article-title><source>bioRxiv</source><year>2018</year><comment>URL <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/early/2018/07/28/379222">https://www.biorxiv.org/content/early/2018/07/28/379222</ext-link></comment><pub-id pub-id-type="pmcid">PMC8061711</pub-id><pub-id pub-id-type="pmid">33453165</pub-id><pub-id pub-id-type="doi">10.1016/j.molcel.2020.12.017</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Graphical abstract.</title><p><bold>a</bold>: <italic>KinPFN</italic> is trained on synthetic RNA folding time distributions drawn from parameterized multi-modal Gaussians by minimizing the negative log-likelihood (NLL). <bold>b</bold>: <italic>KinPFN</italic> accelerates RNA kinetics simulators by predicting the RNA folding time distribution in a single forward pass, given a few folding times as context.</p></caption><graphic xlink:href="EMS199509-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>A schematic visualization of <italic>KinPFN</italic>. Diagram based on <xref ref-type="bibr" rid="R35">Müller et al. (2022)</xref>.</title></caption><graphic xlink:href="EMS199509-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title><italic>KinPFN</italic> approximations of first passage time distributions for simulation data of random RNA sequences across different settings.</title><p><bold>a</bold>: <italic>KinPFN</italic> testing set PPD mean NLL losses along with the CDF MAEs across RNA sequence length ranges. Error bars show the standard deviation of the losses. <bold>b</bold>: Example approximation for an alternative folding path of a 75 nucleotide RNA sequence with ground truth data obtained from <italic>Kinfold</italic> simulations. <bold>c</bold>: Example approximation for a 56 nucleotide RNA using <italic>Kfold</italic> simulation data as ground truth. We use <italic>N</italic> = 25 context first passage times for all experiments. Approximation examples show the mean and standard deviation around the mean for 20 predictions with different context examples sampled at random.</p></caption><graphic xlink:href="EMS199509-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title><italic>KinPFN</italic> first passage time CDF approximations for <italic>Saccharomyces cerevisiae</italic> tRNA<sup>phe</sup>.</title><p>We show the mean and standard deviation for 20 predictions of <italic>KinPFN</italic>, each using 50 randomly sampled context times (left). On the right side, we show the runtime of <italic>Kinfold</italic> for 50 and 1,000 kinetic simulations for the tRNA<sup>phe</sup>.</p></caption><graphic xlink:href="EMS199509-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>RNA folding efficiency analysis.</title><p>The left plot shows the ground truth CDFs <italic>F</italic>(<italic>t</italic>) for three sequences <italic>ϕ<sub>0</sub>, ϕ1</italic> and <italic>ϕ</italic><sub>2</sub>, representing the fraction of molecules folded into the MFE conformation (shown in dot-bracket notation (<xref ref-type="bibr" rid="R25">Hofacker et al., 1994</xref>)) over time <italic>t</italic>. The right plot displays the <italic>KinPFN</italic> approximations <inline-formula><mml:math id="M17"><mml:mrow><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> with ten <italic>Kinfold</italic> times as context.</p></caption><graphic xlink:href="EMS199509-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><p>Approximation of mRNA expression of <italic>IL-1α</italic>, <italic>IL-1β</italic> and <italic>TNF-α</italic> in RAW 264.7 and BMDM cells. We plot approximations using only 25 context data points per gene.</p></caption><graphic xlink:href="EMS199509-f006"/></fig><table-wrap id="T1" orientation="portrait" position="float"><label>Table 1</label><caption><p>Evaluation of <italic>KinPFN</italic>, <italic>KDE</italic>, and multiple <italic>GMM<sub>k</sub></italic> and <italic>DP-GMM<sub>k</sub></italic> models with different initial modality assumptions <italic>k</italic> ∈ {2, 3,4, 5} on a newly introduced testing set comprising 635 real-world first passage time distributions in terms of prior-data negative log-likelihood loss (lower is better) with context first passage time cutoffs <italic>N ∈</italic> {10, 25, 50, 75,100}.</p></caption><table frame="above" rules="groups"><thead><tr><th align="left" valign="top" rowspan="2">Method</th><th align="left" valign="top" colspan="5" style="border-bottom:solid thin">First Passage Times <italic>N</italic></th></tr><tr><th align="left" valign="top">10</th><th align="left" valign="top">25</th><th align="left" valign="top">50</th><th align="left" valign="top">75</th><th align="left" valign="top">100</th></tr></thead><tbody><tr><td align="left" valign="top"><italic>KinPFN</italic></td><td align="left" valign="top"><bold>1.3739</bold></td><td align="left" valign="top"><bold>1.2435</bold></td><td align="left" valign="top"><bold>1.2047</bold></td><td align="left" valign="top"><bold>1.1916</bold></td><td align="left" valign="top"><bold>1.1858</bold></td></tr><tr><td align="left" valign="top"><italic>GMM</italic><sub>2</sub></td><td align="left" valign="top">2.3122</td><td align="left" valign="top">1.3612</td><td align="left" valign="top">1.2355</td><td align="left" valign="top">1.2036</td><td align="left" valign="top">1.1933</td></tr><tr><td align="left" valign="top"><italic>GMM</italic><sub>3</sub></td><td align="left" valign="top">5.2469</td><td align="left" valign="top">1.5830</td><td align="left" valign="top">1.2838</td><td align="left" valign="top">1.2132</td><td align="left" valign="top">1.1910</td></tr><tr><td align="left" valign="top"><italic>GMM</italic><sub>4</sub></td><td align="left" valign="top">13.1325</td><td align="left" valign="top">1.9922</td><td align="left" valign="top">1.3676</td><td align="left" valign="top">1.2480</td><td align="left" valign="top">1.2119</td></tr><tr><td align="left" valign="top"><italic>GMM</italic><sub>5</sub></td><td align="left" valign="top">37.5845</td><td align="left" valign="top">2.7708</td><td align="left" valign="top">1.4957</td><td align="left" valign="top">1.2953</td><td align="left" valign="top">1.2374</td></tr><tr><td align="left" valign="top"><italic>DP-GMM</italic><sub>2</sub></td><td align="left" valign="top">1.6285</td><td align="left" valign="top">1.3529</td><td align="left" valign="top">1.2618</td><td align="left" valign="top">1.2305</td><td align="left" valign="top">1.2150</td></tr><tr><td align="left" valign="top"><italic>DP-GMM</italic><sub>3</sub></td><td align="left" valign="top">1.6268</td><td align="left" valign="top">1.3549</td><td align="left" valign="top">1.2653</td><td align="left" valign="top">1.2323</td><td align="left" valign="top">1.2155</td></tr><tr><td align="left" valign="top"><italic>DP-GMM</italic><sub>4</sub></td><td align="left" valign="top">1.6294</td><td align="left" valign="top">1.3558</td><td align="left" valign="top">1.2663</td><td align="left" valign="top">1.2337</td><td align="left" valign="top">1.2169</td></tr><tr><td align="left" valign="top"><italic>DP-GMM</italic><sub>5</sub></td><td align="left" valign="top">1.6256</td><td align="left" valign="top">1.3572</td><td align="left" valign="top">1.2675</td><td align="left" valign="top">1.2337</td><td align="left" valign="top">1.2175</td></tr><tr><td align="left" valign="top"><italic>KDE</italic></td><td align="left" valign="top">1.4370</td><td align="left" valign="top">1.2559</td><td align="left" valign="top">1.2133</td><td align="left" valign="top">1.2003</td><td align="left" valign="top">1.1957</td></tr></tbody></table></table-wrap></floats-group></article>