<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS202897</article-id><article-id pub-id-type="doi">10.1101/2024.12.11.628035</article-id><article-id pub-id-type="archive">PPR962245</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">2</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Adaptation shapes the representational geometry in mouse V1 to efficiently encode the environment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Dipoppa</surname><given-names>Mario</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="corresp" rid="CR1">✉</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Nogueira</surname><given-names>Ramon</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A4">4</xref><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Bugeon</surname><given-names>Stéphane</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Friedman</surname><given-names>Yoni</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Reddy</surname><given-names>Charu B.</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Harris</surname><given-names>Kenneth D.</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Ringach</surname><given-names>Dario L.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A7">7</xref></contrib><contrib contrib-type="author"><name><surname>Miller</surname><given-names>Kenneth D.</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A8">8</xref></contrib><contrib contrib-type="author"><name><surname>Carandini</surname><given-names>Matteo</given-names></name><xref ref-type="aff" rid="A9">9</xref><xref ref-type="fn" rid="FN1">+</xref><xref ref-type="corresp" rid="CR1">✉</xref></contrib><contrib contrib-type="author"><name><surname>Fusi</surname><given-names>Stefano</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A8">8</xref><xref ref-type="fn" rid="FN1">+</xref><xref ref-type="corresp" rid="CR1">✉</xref></contrib><aff id="A1"><label>1</label>Department of Neurobiology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/046rm7j60</institution-id><institution>University of California</institution></institution-wrap>, <city>Los Angeles</city>, <state>CA</state>, <country country="US">USA</country></aff><aff id="A2"><label>2</label>Center for Theoretical Neuroscience, Zuckerman Institute for Brain Mind and Behavior, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Columbia University</institution></institution-wrap>, <city>NY</city>, <country country="US">USA</country></aff><aff id="A3"><label>3</label>Institute of Neurology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap>, <country country="GB">UK</country></aff><aff id="A4"><label>4</label>Grossman Center for Quantitative Biology and Human Behavior, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/024mw5h28</institution-id><institution>University of Chicago</institution></institution-wrap>, <city>Chicago</city>, <state>IL</state>, <country country="US">USA</country></aff><aff id="A5"><label>5</label>Department of Neurobiology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/024mw5h28</institution-id><institution>University of Chicago</institution></institution-wrap>, <city>Chicago</city>, <state>IL</state>, <country country="US">USA</country></aff><aff id="A6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/042nb2s44</institution-id><institution>Massachusetts Institute of Technology</institution></institution-wrap>, <state>MA</state>, <country country="US">USA</country></aff><aff id="A7"><label>7</label>Department of Psychology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/046rm7j60</institution-id><institution>University of California</institution></institution-wrap>, <city>Los Angeles</city>, <state>CA</state>, <country country="US">USA</country></aff><aff id="A8"><label>8</label>Kavli Institute for Brain Science, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Columbia University</institution></institution-wrap>, <city>NY</city>, <country country="US">USA</country></aff><aff id="A9"><label>9</label>Institute of Ophthalmology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap>, <country country="GB">UK</country></aff></contrib-group><author-notes><corresp id="CR1">
<label>✉</label>
<email>mdipoppa@g.ucla.edu</email>; <email>m.carandini@ucl.ac.uk</email>; <email>sf2237@columbia.edu</email>.</corresp><fn id="FN1"><label>+</label><p id="P1">co-senior authors.</p></fn></author-notes><pub-date pub-type="nihms-submitted"><day>28</day><month>01</month><year>2025</year></pub-date><pub-date pub-type="preprint"><day>24</day><month>01</month><year>2025</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nd/4.0/">CC BY-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P2">Sensory adaptation dynamically changes neural responses as a function of previous stimuli, profoundly impacting perception. The response changes induced by adaptation have been characterized in detail in individual neurons and at the population level after averaging across trials. However, it is not clear how adaptation modifies the aspects of the representations that relate more directly to the ability to perceive stimuli, such as their geometry and the noise structure in individual trials. To address this question, we recorded from a population of neurons in the mouse visual cortex and presented one stimulus (an oriented grating) more frequently than the others. We then analyzed these data in terms of representational geometry and studied the ability of a linear decoder to discriminate between similar visual stimuli based on the single-trial population responses. Surprisingly, the discriminability of stimuli near the adaptor increased, even though the responses of individual neurons to these stimuli decreased. Similar changes were observed in artificial neural networks trained to reconstruct the visual stimulus under metabolic constraints. We conclude that the paradoxical effects of adaptation are consistent with the efficient coding framework, allowing the brain to improve the representation of frequent stimuli while limiting the associated metabolic cost.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P3">Visual perception is profoundly affected by adaptation to previous stimuli, which can induce changes in our ability to detect and discriminate stimuli <sup><xref ref-type="bibr" rid="R1">1</xref>,<xref ref-type="bibr" rid="R2">2</xref></sup>, as well as generate visual illusions such as the tilt aftereffect <sup><xref ref-type="bibr" rid="R3">3</xref></sup>. Adaptation-induced changes in perception have been connected to changes in visual responses <sup><xref ref-type="bibr" rid="R4">4</xref></sup> and have been observed in different species <sup><xref ref-type="bibr" rid="R5">5</xref>–<xref ref-type="bibr" rid="R7">7</xref></sup>, sensory systems <sup><xref ref-type="bibr" rid="R6">6</xref>,<xref ref-type="bibr" rid="R8">8</xref></sup>, and at different stages of sensory processing <sup><xref ref-type="bibr" rid="R5">5</xref>,<xref ref-type="bibr" rid="R6">6</xref>,<xref ref-type="bibr" rid="R9">9</xref>,<xref ref-type="bibr" rid="R10">10</xref></sup>. Despite these studies providing a link between changes in perception and visual responses, the crucial question of whether adaptation makes certain stimuli more discriminable and whether it does so to a specific set of stimuli has only partial answers.</p><p id="P4">A connection between adaptation-induced changes in perception and visual responses was obtained in studies of tuning curves in single neurons <sup><xref ref-type="bibr" rid="R11">11</xref></sup> or neural populations <sup><xref ref-type="bibr" rid="R12">12</xref></sup>. These studies showed that adaptation can increase discriminability between stimuli <sup><xref ref-type="bibr" rid="R11">11</xref></sup> and decorrelate tuning curves <sup><xref ref-type="bibr" rid="R12">12</xref></sup>. However, it is not clear how all these observed changes affect the ability of a downstream population to discriminate stimuli. In particular, any realistic readout of a population of neurons should generate a response for each stimulus every time it is presented (in a single trial). Noise correlations across different neurons might play an important role, and the average response properties of individual neurons, like their tuning curve, can only partially predict the discrimination performance. The tuning curve is typically estimated by looking at multiple trials, and hence, it is an average property of the neuronal response that cannot be read out in a single trial <sup><xref ref-type="bibr" rid="R13">13</xref>,<xref ref-type="bibr" rid="R14">14</xref></sup>. Moreover, the typical tuning curves of individual neurons are affected in multiple ways by adaptation (e.g. by the stimulus presented in the present and by the activity of neurons driven in the past, see <sup><xref ref-type="bibr" rid="R12">12</xref></sup>), and it is often difficult to capture all these changes using a simple model of the responses of individual neurons. Moreover, it is not clear how these multiple changes affect discriminability.</p><p id="P5">A more interpretable description of the activity of a neuronal population can be obtained by considering its representational geometry, i.e. the arrangement of all the points in the neural space corresponding to the different experimental conditions <sup><xref ref-type="bibr" rid="R15">15</xref></sup>. This description of the data is interpretable in terms of perception and discriminability because it directly relates to the ability of a linear readout to discriminate between similar stimuli. Moreover, unlike with tuning curves, this description of the data can be applied to single trials. It might, therefore, reveal how adaptation-induced changes in representational geometry interact with other forms of modulation, such as those induced by running. Running strongly affects the visual responses <sup><xref ref-type="bibr" rid="R16">16</xref>,<xref ref-type="bibr" rid="R17">17</xref></sup> and their geometry <sup><xref ref-type="bibr" rid="R18">18</xref></sup>, and reflects changes in behavioral state that might, in turn, affect adaptation <sup><xref ref-type="bibr" rid="R19">19</xref></sup>. Overall, only a few studies have focused on adaptation-induced changes at the population level, and they lacked trial-to-trial resolution <sup><xref ref-type="bibr" rid="R20">20</xref>–<xref ref-type="bibr" rid="R22">22</xref></sup>, single-unit resolution <sup><xref ref-type="bibr" rid="R12">12</xref></sup>, or an analysis of population geometry <sup><xref ref-type="bibr" rid="R23">23</xref></sup>. Applying a geometrical approach at the single trial level can reveal how adaptation depends on orientation similarity and whether adaptation-induced and running-induced changes interact in the neural code.</p><p id="P6">Furthermore, describing adaptation-induced changes from a geometrical perspective could shed light on the computational advantages of adaptation. Computational theories <sup><xref ref-type="bibr" rid="R4">4</xref>,<xref ref-type="bibr" rid="R24">24</xref></sup> have proposed that adaptation can efficiently represent stimuli through population homeostasis maintenance <sup><xref ref-type="bibr" rid="R12">12</xref></sup>, optimization of information transmission <sup><xref ref-type="bibr" rid="R25">25</xref></sup>, decorrelation <sup><xref ref-type="bibr" rid="R26">26</xref></sup>, response-product homeostasis <sup><xref ref-type="bibr" rid="R27">27</xref></sup>, and the trade-off between precision of the representation and metabolic cost <sup><xref ref-type="bibr" rid="R28">28</xref>,<xref ref-type="bibr" rid="R29">29</xref></sup>.</p><p id="P7">For all these reasons, we investigated how adaptation modifies the geometry of representations and how it affects the ability of a linear readout to discriminate between similar stimuli on single trials. We presented to awake mice sequences of visual stimuli in the form of oriented gratings characterized by two distributions: uniform and biased <sup><xref ref-type="bibr" rid="R12">12</xref></sup>. As a first step towards answering which orientations become more discriminable, we first found that in the uniform environment, vertical orientations lead to the lowest discriminability, in contrast with cat <sup><xref ref-type="bibr" rid="R30">30</xref></sup> and primate <sup><xref ref-type="bibr" rid="R31">31</xref></sup> V1 neurophysiology, where visual acuity is lowest for oblique orientations. Then, we found that in the biased environment, discriminability increases for stimuli near the adaptor, consistent with human psychophysics studies <sup><xref ref-type="bibr" rid="R2">2</xref>,<xref ref-type="bibr" rid="R32">32</xref></sup>, while responses near the adaptor decreased. This scenario is consistent with earlier phenomenological theories of adaptation <sup><xref ref-type="bibr" rid="R33">33</xref></sup>. We observed that running expanded the geometry of representations. Running had a stronger effect in magnitude than adaptation on the geometry of neural representations. However, the increase in discriminability and decrease in responses around the adapted orientation was observed across different locomotion states.</p><p id="P8">Finally, we leveraged these data to constrain a theoretical model that predicts changes in discriminability in single trials, and that reveals a computational role for the response changes induced by adaptation. Following an efficient coding approach <sup><xref ref-type="bibr" rid="R34">34</xref>,<xref ref-type="bibr" rid="R35">35</xref></sup>, we trained an artificial neural network to reconstruct stimuli presented in environments with the same distributions as those used in our experiments (uniform and biased). The network minimized a stimulus reconstruction cost and a metabolic cost to represent the stimuli efficiently. Consistent with our experimental data, we observed an increase in discriminability and a decrease in responses around the adaptor. Our model thus suggests that population responses to stimuli efficiently adapt to the environment statistics.</p></sec><sec id="S2" sec-type="results"><title>Results</title><p id="P9">We presented sequences of static gratings interleaved by blank stimuli to awake head-fixed mice that were free to run on an air-suspended ball (<xref ref-type="fig" rid="F1">Fig. 1a</xref>). The orientation of the gratings was sampled from a uniform distribution (<xref ref-type="fig" rid="F1">Fig. 1b</xref>). We simultaneously recorded hundreds of neurons using two-photon imaging in layers 2/3 of V1 in these mice (<xref ref-type="fig" rid="F1">Fig. 1c</xref>) <sup><xref ref-type="bibr" rid="R16">16</xref></sup>.</p><p id="P10">As a first characterization of how oriented gratings are encoded in the neural population, we studied the neurons’ tuning curves. We estimated the distribution of preferred orientations in the population of recorded neurons when orientations were sampled from a uniform distribution (<xref ref-type="fig" rid="F1">Fig. 1d</xref>). Consistent with previous reports in mammals <sup><xref ref-type="bibr" rid="R30">30</xref>,<xref ref-type="bibr" rid="R31">31</xref>,<xref ref-type="bibr" rid="R36">36</xref></sup>, we found an overrepresentation of neurons with a preference for the horizontal (90 deg) or vertical (0 deg) orientations (no significant difference in fraction of neurons with a preference for horizontal or vertical orientations across experiments: p = 0.09, Wilcoxon signed-ranked test). A larger number of neurons with a preference for 90 or 0 deg suggests a higher signal-to-noise ratio for these orientations, which appear consistent with a reduced discrimination threshold at horizontal and vertical orientations in human psychophysics experiments<sup><xref ref-type="bibr" rid="R1">1</xref>,<xref ref-type="bibr" rid="R30">30</xref>,<xref ref-type="bibr" rid="R37">37</xref></sup>. We also characterized the tuning width of the neurons by fitting von Mises functions to the responses of neurons to estimate the concentration parameters (high concentration corresponds to sharper tuning curves) (<xref ref-type="fig" rid="F1">Fig. 1e</xref>). We found sharper tuning curves in neurons preferring horizontal orientations but wider tuning curves in neurons preferring vertical orientations. In conclusion, different tuning curves’ features (distribution of preferred orientations and tuning sharpness) suggest different patterns of orientation-dependent discriminability in the neural population.</p><p id="P11">To have a full picture, instead of considering single-tuning curves separately, we next focused on the full-dimensional neural activity space (i.e. the space in which the coordinate axes represent the activities of the different neurons). We projected the population responses into low-dimensional space and quantified distances in the full-dimensional activity space. The representations of the stimuli in the activity space reflected the circular symmetry of the visual stimuli (<xref ref-type="fig" rid="F1">Fig. 1f</xref>), but as expected from the inhomogeneities in the tuning curve distribution and properties (<xref ref-type="fig" rid="F1">Fig. 1d,e</xref>), they were not spaced uniformly around a circle. Instead, the distance in the activity space between stimuli with a horizontal orientation or similar was larger than for stimuli with a vertical orientation or similar. This can be seen in visualizations of the geometrical structure of the representations (<xref ref-type="fig" rid="F1">Fig. 1f,g,h</xref>), which we created by using PCA to reduce the dimensionality of the activity space. The same results are also observed in the original full-dimensional activity space by comparing the Euclidean distance between stimuli near the horizontal orientation with the distance between those near the vertical orientation (p &lt; 10<sup>-5</sup>, Wilcoxon signed-ranked test; <xref ref-type="supplementary-material" rid="SD1">Fig. S1a,b</xref>).</p><p id="P12">To characterize the geometry of the neural representations in a way that directly reflects the information that can be read out by a downstream population, we then measured the ability of linear decoders to discriminate pairs of stimuli (<xref ref-type="fig" rid="F1">Fig. 1i,j</xref>), starting from the case in which all the orientations are presented with equal probability (uniform distribution) The performance of these decoders depends on the representations in the original full-dimensional activity space and, importantly, on the noise’s strength and structure. In general, we observed a simple relation between discriminability and Euclidean distances: the larger the distances, the more discriminable the stimuli (<xref ref-type="supplementary-material" rid="SD1">Fig. S1c</xref>), When considering pairs of stimuli 15 deg apart, we consistently observed a peak in discrimination accuracy around the horizontal orientation, while the lowest decoding performance was seen for the vertical orientations. These observations suggest that discriminability is primarily modulated by the distances between orientations, rather than by the size or the structure of the noise. Indeed, the noise level of horizontal stimuli was not clearly distinguished from the noise level of vertical stimuli (p = 0.21, Wilcoxon signed-ranked test; <xref ref-type="supplementary-material" rid="SD1">Fig. S1a,b</xref>). Moreover, the position of the discrimination peak did not depend on the noise structure, as removing correlations modestly increased the discrimination accuracy but not the peak position (<xref ref-type="supplementary-material" rid="SD1">Fig. S1d-f</xref>). Shorter Euclidean distances at vertical orientations are consistent with wider tuning curves at those orientations (<xref ref-type="fig" rid="F1">Fig. 1e</xref>).</p><p id="P13">To understand the contribution not only of the tuning width but also of the distribution of preferred orientations to the orientation-dependent discriminability, we considered a lower-dimensional phenomenological model constructed from the average tuning curves of neurons in each of the 12 bins of preferred orientation. We fitted a von Mises function to each of these average tuning curves and then multiplied the fitted curves by the probability density of the associated preferred orientation (<xref ref-type="supplementary-material" rid="SD1">Fig. S2c</xref>). Alternatively, we only considered the preferred orientation distribution (<xref ref-type="supplementary-material" rid="SD1">Fig. S2a</xref>) or only the tuning width (<xref ref-type="supplementary-material" rid="SD1">Fig. S2b</xref>). We then considered the Euclidean distance between pairs of stimuli in the lower dimensional space of 12 superneurons (one superneuron’s tuning curve is the average tuning curve of neurons that approximately prefer one specific orientation out of the 12 possible ones considered in this study) with responses determined by these average tuning curves (<xref ref-type="supplementary-material" rid="SD1">Fig. S2d-f</xref>). When considering nearby stimuli (<xref ref-type="supplementary-material" rid="SD1">Fig. S2g-i</xref>), we could observe the decoding peak at the horizontal orientation only when taking into account the tuning width. However, the distribution of preferred orientations was also informative when considering the relation between discriminability and distances across all pairs of orientations (<xref ref-type="supplementary-material" rid="SD1">Fig. S2j-l</xref>). Hence, we concluded that both the tuning width and the response magnitudes contribute to the peak of discriminability at the horizontal orientation.</p><sec id="S3"><title>Adaptation decreases responses of neurons tuned to the adaptor orientation while increasing discriminability around the adaptor</title><p id="P14">To understand how adaptation changes the individual neural responses and the geometry of neural representations, we presented sequences of oriented gratings with different statistics (<xref ref-type="fig" rid="F2">Fig. 2a</xref>): stimulus orientation was sampled from either a uniform or a biased sequence <sup><xref ref-type="bibr" rid="R12">12</xref></sup>, defining two environments that are characterized by different stimulus distributions. In the case of a biased sequence, one orientation was presented 50% of the time, which produced strong adaptation effects around that orientation. When projecting the activity in a space with reduced dimensionality, we observed that adaptation changes the representational geometry in a structured way (<xref ref-type="fig" rid="F2">Fig. 2b</xref>). This geometry in the reduced activity space indicates that the discriminability might actually increase around the adaptor.</p><p id="P15">We started by investigating how the tuning properties of neurons change in a biased environment. Consistent with previous reports <sup><xref ref-type="bibr" rid="R12">12</xref></sup>, when averaging across recording sessions, we observed that adaptation induced a response decrease in neurons tuned to stimuli near the adaptor orientation (<xref ref-type="fig" rid="F2">Fig. 2c-f</xref>, <xref ref-type="supplementary-material" rid="SD1">S3a,b, S4a</xref>). We also observed a response increase in neurons whose orientation preference was farther from the adaptor. In the original full-dimensional space, the Euclidean distance between the response to a given orientation and one 15 degrees away had the highest increase for the adaptor orientation (<xref ref-type="supplementary-material" rid="SD1">Fig. S3c,d</xref>).</p><p id="P16">We then compared the discrimination accuracy in a uniform environment to that in a biased environment. After estimating the accuracy in the two environments in the full-dimensional neural space (<xref ref-type="fig" rid="F2">Fig. 2g</xref>), we computed the difference between the two environments (<xref ref-type="fig" rid="F2">Fig. 2h,i</xref>, <xref ref-type="supplementary-material" rid="SD1">S4b</xref>). When considering pairs of similar stimuli that were 15 deg apart, we observed increased discrimination accuracy around the adaptor (<xref ref-type="fig" rid="F2">Fig. 2j</xref>; p &lt; 10<sup>-4</sup>, 1-sample t-test). This result was consistent across different adaptor orientations (adaptor at 0 deg: p &lt; 10<sup>-4</sup>, 1-sample t-test; adaptor at 45 deg: p = 0.026, 1-sample t-test; <xref ref-type="supplementary-material" rid="SD1">Fig. S3e</xref>) as well as with increased cosine distance around the adaptor (<xref ref-type="supplementary-material" rid="SD1">Fig. S3f</xref>). Overall, changes in discriminability were qualitatively similar to observations in human psychophysics experiments <sup><xref ref-type="bibr" rid="R2">2</xref>,<xref ref-type="bibr" rid="R32">32</xref></sup> (but see also <sup><xref ref-type="bibr" rid="R38">38</xref></sup>). These results did not depend strongly on the noise structure as they were qualitatively similar when correlations were removed by shuffling the activity of each neuron independently across trials for a given stimulus condition (shuffling, see <sup><xref ref-type="bibr" rid="R13">13</xref>,<xref ref-type="bibr" rid="R14">14</xref>,<xref ref-type="bibr" rid="R39">39</xref></sup>, <xref ref-type="supplementary-material" rid="SD1">Fig. S3g,h</xref>). The fact that the noise structure did not play a major role in adaptation-induced changes is consistent with previous reports <sup><xref ref-type="bibr" rid="R12">12</xref></sup> (but see: <sup><xref ref-type="bibr" rid="R23">23</xref></sup>).</p><p id="P17">In the previous analysis, the decoder was aware of changes in environments <sup><xref ref-type="bibr" rid="R40">40</xref></sup>. We then asked whether a decoder unaware of changes in environments can deal with the changes in representational geometry <sup><xref ref-type="bibr" rid="R40">40</xref></sup>. We trained linear decoders in a uniform environment and tested them in a biased environment or vice versa (<xref ref-type="supplementary-material" rid="SD1">Fig. S5a</xref>). The discrimination accuracy for pairs of approximately opposite stimuli relative to the adaptor orientation typically increased, including those near the adaptor (<xref ref-type="supplementary-material" rid="SD1">Fig. S5a</xref>). In other words, it would increase for pairs of stimuli whose orientation was approximately ±<italic>θ</italic> from that of the adaptor. Although the structure of the changes was not exactly the same compared to when a decoder was trained and tested in the same environment, there was a common result: near the adaptor, discriminability increased in biased environments, which was an even more surprising result when the decoder was unaware of a change in context. We also trained a regression model in a uniform environment to estimate the orientation of the stimulus presented (<xref ref-type="supplementary-material" rid="SD1">Fig. S5b</xref>). We confirmed a classical result showing, for stimuli near the adaptor, a repulsion of the estimated orientations away from the adaptor orientation <sup><xref ref-type="bibr" rid="R6">6</xref></sup> when tested in a biased environment.</p><p id="P18">Representations might change over time for reasons independent of the specific task we are considering (‘representational drift’ <sup><xref ref-type="bibr" rid="R41">41</xref>,<xref ref-type="bibr" rid="R42">42</xref></sup>). As the biased sequence followed the uniform one, one might wonder whether this geometric change could be explained simply by the temporal separation of the two environments. We thus asked if we could discriminate the same orientation in two blocks of time. The two time blocks could have the same statistics (uniform) or different statistics (uniform and biased). Discriminating two blocks of time with uniform statistics was larger than chance (<xref ref-type="supplementary-material" rid="SD1">Fig. S5c,d</xref>). However, the discrimination accuracy was greater when discriminating two blocks of time with different statistics. This difference in discrimination accuracy indicated that the passage of time alone was insufficient to explain changes in geometry (<xref ref-type="supplementary-material" rid="SD1">Fig. S5c,d</xref>).</p><p id="P19">We generalized the previous analysis by training and testing linear decoders in environments separated by different periods of time. Training and testing could be in the same (uniform) or different (uniform vs. biased) environments but always separated by varying time blocks. As expected, based on the potential presence of representational drifts or recording drift, the discrimination accuracy decreased with time (<xref ref-type="supplementary-material" rid="SD1">Fig. S5e</xref>).</p></sec><sec id="S4"><title>Running expands the representational geometry along directions different from those encoding stimulus orientation</title><p id="P20">Running is one of the main drivers of visual response modulation in the mouse brain <sup><xref ref-type="bibr" rid="R16">16</xref>,<xref ref-type="bibr" rid="R17">17</xref>,<xref ref-type="bibr" rid="R43">43</xref></sup>. Running increased the distance between stimuli (p &lt; 10<sup>-11</sup>, Wilcoxon signed-ranked test; <xref ref-type="fig" rid="F3">Fig. 3a</xref>). In the activity space, this expansion appears to be along directions that are approximately orthogonal to the dimensions spanned by all the visual stimuli (<xref ref-type="fig" rid="F3">Fig. 3a</xref>), which is consistent with previous reports <sup><xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R36">36</xref></sup>. This change in geometry enables the encoding of whether the animal is running or not without interfering with a linear readout of the orientation of the stimulus.</p><p id="P21">Running strongly modulated the neural activity, and the locomotion state accounted for more variability than the identity of the visual stimulus. We observed a general increase in Euclidean distances during running (<xref ref-type="supplementary-material" rid="SD1">Fig. S6a</xref>). We then computed the angle between the coding direction of running (i.e., the vector linking responses during the stationary condition to those during the running condition) and the main direction of variability (as measured by PCA) (<xref ref-type="supplementary-material" rid="SD1">Fig. S6b</xref>). We then compared this angle with the angle between the main direction of variability and the coding directions of different stimuli (the vector linking average responses to two orientations 15 deg apart, same locomotion condition). We found that the main direction of variability was more aligned with the running direction than the stimulus directions (<xref ref-type="supplementary-material" rid="SD1">Fig. S6c</xref>).</p><p id="P22">We compared the discrimination accuracy between any pair of stimuli for stationary and running conditions separately in a uniform environment. We observed a general increase in discrimination accuracy in the running condition (<xref ref-type="fig" rid="F3">Fig. 3b,c</xref>) consistent with an increase in Euclidean distances (<xref ref-type="supplementary-material" rid="SD1">Fig. S6a</xref>). We then asked if the neural code for orientation is preserved across locomotion conditions. More specifically, we tested if coding directions for different orientations were approximately the same for running and stationary states. We computed the cross-condition generalization performance (CCGP) <sup><xref ref-type="bibr" rid="R15">15</xref>,<xref ref-type="bibr" rid="R44">44</xref></sup>: we trained a linear model to discriminate any two angles in one locomotion condition (stationary or running) and test it in the other condition (<xref ref-type="fig" rid="F3">Fig. 3d</xref>). The CCGP (<xref ref-type="fig" rid="F3">Fig. 3e</xref>) was similar to the performance achieved when we trained and tested the decoder using the same locomotion condition (<xref ref-type="fig" rid="F3">Fig. 3b</xref>), supporting the notion that locomotion and stimulus representation in V1 are disentangled (or mostly disentangled <sup><xref ref-type="bibr" rid="R18">18</xref></sup>).</p><p id="P23">We then asked whether the coding direction of running was preserved across stimulus orientations. We performed another CCGP analysis by estimating the ability to discriminate between stationary and running trials, training the decoder on one stimulus orientation, and testing on another orientation (<xref ref-type="supplementary-material" rid="SD1">Fig. S6d</xref>). We found that it was possible to decode locomotion, and we did not find a large difference across angles. We also found that destroying correlations increased the discrimination accuracy of the previous analysis (<xref ref-type="supplementary-material" rid="SD1">Fig. S6e,f</xref>), which is consistent with the main axis of co-variability being aligned with running (<xref ref-type="supplementary-material" rid="SD1">Fig. S6b,c</xref>). These analyses indicate that there is a subspace in which the locomotion state of the animal is approximately invariant with respect to the stimulus identity.</p><p id="P24">From the reduced dimensionality representations (<xref ref-type="fig" rid="F3">Fig. 3a</xref>), it seems that the transformation of the geometry from stationary to running can be described as an expansion accompanied by a shift. It is then natural to ask whether this transformation can be explained by a simple scaling model in which all responses in the stationary case are simply multiplied by the same factor. We fit a scaling geometrical model (see <xref ref-type="sec" rid="S9">Methods</xref> for more details), which can be described as a truncated cone, to the neural data. This model accounts only partially (averaged normalized error: 37%) for the change in responses by running (<xref ref-type="supplementary-material" rid="SD1">Fig. S6g</xref>). In conclusion, while our results on the scaling model indicate that the geometry is more complex than a truncated cone, the results on CCGP show that the expansion of the geometry by running does not interfere substantially with the encoding of the stimulus orientation.</p></sec><sec id="S5"><title>Adaptation increases discriminability across locomotion conditions</title><p id="P25">Locomotion mostly preserved adaptation-induced changes. The analysis of the activity space with reduced dimensionality suggests that the changes in representations are approximately similar in different locomotion conditions (<xref ref-type="fig" rid="F4">Fig. 4a,b</xref>). We then studied in which directions of the original full-dimensional neural space the locomotion-induced and adaptation-induced changes happen across orientations. For each orientation, we define the direction of adaptation as the vector between the points in the activity space that represent the responses in uniform and biased environments. We then estimated the angle between the coding direction of stationary/running and the direction of adaptation (<xref ref-type="supplementary-material" rid="SD1">Fig. S7a</xref>. This angle tended to be negative, although not significantly so. We also estimated the angle between the direction of maximum variability and the direction of adaptation (<xref ref-type="supplementary-material" rid="SD1">Fig. S7b</xref>). In this case, the angle was significantly negative, indicating that adaptation is oriented toward a decrease of this vector, which typically represents the average activity.</p><p id="P26">The coding directions for adaptation were preserved when the locomotion state changed. We computed the CCGP for adaptation by training the model to discriminate responses to the adaptor between the uniform and biased environment. We either trained the model in the stationary condition and tested it in the running condition or vice versa (<xref ref-type="supplementary-material" rid="SD1">Fig. S7c</xref>). We compared the CCGP with the one obtained by discriminating two different uniform blocks so that the only difference would be the passage of time. We found it easier to discriminate between biased and uniform environments than between two uniform environments (<xref ref-type="supplementary-material" rid="SD1">Fig. S7d</xref>). This shows that adaptation shifted the manifold in a direction not perfectly aligned with the locomotion axis. It would have been more difficult to discriminate between the two conditions if they had been perfectly aligned. When performing a similar operation—discriminating between stationary and running by training linear decoders in a uniform environment and testing them in a biased environment (<xref ref-type="supplementary-material" rid="SD1">Fig. S7e</xref>)—we did not observe a significant difference in the case of training and testing models in different uniform environments (<xref ref-type="supplementary-material" rid="SD1">Fig. S7f</xref>).</p><p id="P27">Finally, we tested how locomotion changes neural responses and discrimination accuracy. We observed decreased responses (<xref ref-type="fig" rid="F4">Fig. 4c,d</xref>; <xref ref-type="supplementary-material" rid="SD1">S7g,h</xref>) and increased discrimination accuracy (<xref ref-type="fig" rid="F4">Fig. 4e,f</xref>) around the adaptor during running and stationary periods. Thus, even though running had a stronger effect in magnitude on the geometry of representations than adaptation, we observed in either locomotion condition changes similar to those reported without separating locomotion conditions (<xref ref-type="fig" rid="F2">Fig. 2e,i</xref>).</p></sec><sec id="S6"><title>An artificial neural network reproduces the changes in discriminability and population tuning observed in mouse V1</title><p id="P28">Since theoretical work has shown that adaptation can increase the efficiency of neural representations, we next asked if our findings could be explained by efficient coding <sup><xref ref-type="bibr" rid="R35">35</xref></sup>. We trained 6,000 autoencoder models with metabolic constraints (L1 norm in the hidden layer; <xref ref-type="fig" rid="F5">Fig. 5a</xref>) and different hyperparameters (e.g. noise level, number of neurons, metabolic coefficient) to reconstruct the stimuli. We trained the autoencoder separately on the uniform and biased statistics (<xref ref-type="fig" rid="F5">Fig. 5b</xref>). In the biased environment, because of biased statistics, the autoencoder would be penalized more when misclassifying the adaptor’s orientation as the adaptor is presented more often. The biased statistics also force the autoencoder to represent the adaptor in a more-efficient manner than other orientations.</p><p id="P29">Despite the simplicity of the model and the minimal number of assumptions, adaptation changed the population geometry in most models in a way that is similar to the data (<xref ref-type="fig" rid="F5">Fig. 5c</xref>). Within a broad hyperparameter region (<xref ref-type="supplementary-material" rid="SD1">Figure S8e</xref>), we observed a decrease in responses around the adaptor for neurons tuned to the adaptor (<xref ref-type="fig" rid="F5">Fig. 5d,e</xref>) and an increase in discrimination accuracy around the adaptor (<xref ref-type="fig" rid="F5">Fig. 5f,g</xref>) consistent with our experimental observations (<xref ref-type="fig" rid="F2">Fig. 2e,f,i,j</xref>).</p><p id="P30">Among the different hyperparameters of the model, we focused on the metabolic penalty and its impact on the metabolic cost, discrimination accuracy, and response magnitude (<xref ref-type="supplementary-material" rid="SD1">Fig. S8a-f</xref>). We observed that a decreased metabolic penalty increased response magnitudes (<xref ref-type="supplementary-material" rid="SD1">Fig. S8b</xref>) and discriminability (<xref ref-type="supplementary-material" rid="SD1">Fig. S8c</xref>) when averaged across all orientations or pairs of orientations. Furthermore, a decrease in metabolic penalty leads to a weaker (and negative) change in response magnitude (<xref ref-type="supplementary-material" rid="SD1">Fig. S8e</xref>) and an increase in change in discrimination accuracy (<xref ref-type="supplementary-material" rid="SD1">Fig. S8f</xref>) around the adaptor. These results can be visualized in reduced dimensions as an expansion of the geometry (<xref ref-type="supplementary-material" rid="SD1">Fig. S8g</xref>) and suggest that the running-induced changes could be interpreted as a decrease in metabolic penalty.</p></sec><sec id="S7"><title>Relation between changes in responses and discriminability induced by the adaptor</title><p id="P31">The decrease of responses by adaptation in neurons tuned to the adaptor orientation in the data (<xref ref-type="fig" rid="F2">Fig. 2e</xref>) and the normative model (<xref ref-type="fig" rid="F5">Fig. 5d</xref>) may appear at odds with increased discriminability around the adaptor observed in our data (<xref ref-type="fig" rid="F2">Fig. 2i</xref>) and in the normative model (<xref ref-type="fig" rid="F5">Fig. 5f</xref>). We now show that what we observed in the data reflects an interesting computational strategy that allows the system to better discriminate more frequent stimuli without increasing the metabolic cost. We will compare this strategy with two other scenarios: one where the adaptor discriminability increases but also the metabolic cost, and another where the energy consumption is reduced but also the discriminability of the adaptor decreases.</p><p id="P32">We started by considering a hypothetical adaptation of tuning curves where the peak responses of the neurons tuned to the adaptor were either depressed (<xref ref-type="fig" rid="F6">Fig. 6a</xref>, <xref ref-type="supplementary-material" rid="SD1">S9a</xref>) or facilitated (<xref ref-type="fig" rid="F6">Fig. 6b</xref>, <xref ref-type="supplementary-material" rid="SD1">S9b</xref>). We also considered another hypothetical adaptation of tuning curves (<xref ref-type="fig" rid="F6">Fig. 6c</xref>) consistent with changes observed in the data (<xref ref-type="fig" rid="F2">Fig 2h</xref>) but applied to homogeneous tuning curves. In this way, we would abstract our results away from the inhomogeneities we observed in a uniform environment (<xref ref-type="fig" rid="F1">Fig. 1</xref>) and focus on the changes in a few response properties.</p><p id="P33">What is the advantage of a simple response depression (<xref ref-type="fig" rid="F6">Fig. 6a</xref>)? We measured the metabolic cost as the average firing rate over time based on the biased environment in the experiments. Assuming this biased environment, we compared the metabolic cost if the responses were uniform (non-adapted) or depressed (<xref ref-type="fig" rid="F6">Fig. 6d</xref>). As expected, a response depression around the adaptor led to a decrease in metabolic cost compared to the non-adapted responses. We then estimated discrimination accuracy to different pairs of stimuli, assuming independent and identically distributed noise when responses were depressed compared to when were non-adapted. As expected, because of a decrease in signal-to-noise ratio, discrimination accuracy decreased around the adaptor, thus to the most frequent stimuli. Thus, response depression has the advantage of decreasing metabolic cost but the disadvantage of decreasing discriminability. The reverse emerges when considering response facilitation (<xref ref-type="fig" rid="F6">Fig. 6b</xref>): facilitation has the advantage of increasing discrimination accuracy around the adaptor (<xref ref-type="fig" rid="F6">Fig. 6h</xref>) but the disadvantage of increasing metabolic cost (<xref ref-type="fig" rid="F6">Fig. 6e</xref>).</p><p id="P34">Finally, we estimated the metabolic cost and discrimination accuracy as the average firing rate based on the biased distribution used in the experiments. What are the consequences of these tuning curve changes in a biased environment? The changes in responses observed in the data and applied to a homogenous population present two advantages: they decrease the metabolic cost (<xref ref-type="fig" rid="F6">Fig. 6f</xref>) and, at the same time, increase discrimination accuracy around the adaptor (<xref ref-type="fig" rid="F6">Fig. 6f</xref>).</p><p id="P35">To find an intuitive explanation for how these two seemingly incompatible changes can happen simultaneously, we inspected the representations projected in a reduced dimensionality activity space (<xref ref-type="fig" rid="F6">Fig. 6l</xref>). We observed that while responses near the adaptor got closer to the center, as expected from a decrease in responses, the stimuli near the adaptor got farther away from it, leading to a local increase in discrimination accuracy. These results can be compatible with a combination of changes in gain and warping of tuning curves <sup><xref ref-type="bibr" rid="R45">45</xref></sup> and they can be contrasted to the cases of simple depression (<xref ref-type="fig" rid="F6">Fig. 6j</xref>) or facilitation (<xref ref-type="fig" rid="F6">Fig. 6k</xref>), in which there was an increase or a decrease in both responses and discrimination accuracy.</p><p id="P36">The adaptation-induced changes observed in the data and reproduced by the model have two benefits: reduction in metabolic cost and increase in overall discriminability. Is there any cost? Let us assume that the visual system decodes the stimulus orientation by estimating the angle from the neural population vector. Let us also assume that the decoder is unaware of changes in representational geometry <sup><xref ref-type="bibr" rid="R40">40</xref></sup> (<xref ref-type="supplementary-material" rid="SD1">Fig. S5a,b</xref>). This means that a decoder trained in a default environment (e.g. a uniform environment) could be tested in another environment (e.g. one specific biased environment) differently from a decoder trained and tested in the same environment (<xref ref-type="fig" rid="F2">Fig. 2</xref>). Then, for the unaware decoder, there is a strong repulsion of the estimated angle near the adapted orientation (<xref ref-type="fig" rid="F6">Fig. 6l,o</xref>), i.e. the estimated angle is farther from the adaptor than it really is, creating a bias. This repulsion is what we observed in the data (<xref ref-type="supplementary-material" rid="SD1">Fig. S5b</xref>) and is consistent with a stronger reduction of tuning curves at the flank near the adaptor <sup><xref ref-type="bibr" rid="R12">12</xref></sup>.</p><p id="P37">Repulsion of orientations near adaptation is consistent with the tilt aftereffect <sup><xref ref-type="bibr" rid="R33">33</xref>,<xref ref-type="bibr" rid="R46">46</xref>,<xref ref-type="bibr" rid="R47">47</xref></sup>, where a long exposure to a stimulus (e.g., vertical) can produce the illusion that a different but similar stimulus (e.g., slightly oblique) will be perceived more different than what it is (e.g., more oblique). This issue would be less prominent in the other hypothetical adaptation-induced changes in geometry (<xref ref-type="fig" rid="F6">Fig. 6j,k,m,n</xref>), suggesting that having an unbiased representation of the orientation may be, to some degree, less important than a robust discriminability between two angles and a metabolic cost under control. In other words, adaptation-induced changes may improve the ability to tell apart two stimuli (focusing on their relative difference of orientations) while decreasing the ability to identify the absolute orientation of those stimuli.</p></sec></sec><sec id="S8" sec-type="discussion"><title>Discussion</title><p id="P38">We investigated how the representational geometry in mouse V1 is shaped by adaptation under environments with different stimulus statistics. More specifically, we presented oriented gratings sampled from uniform and biased distributions. Consistently with previous studies in cat V1 <sup><xref ref-type="bibr" rid="R12">12</xref></sup>, we observed that adaptation decreases the neurons’ responses to the adaptor orientation. However, we also observed increased discriminability around the adaptor. A normative model could explain these results, suggesting that the observed changes in the representational geometry emerge from a trade-off between improving the representation of frequent stimuli and reducing the metabolic cost of responding to these more frequent stimuli.</p><p id="P39">The analysis of the geometry in the uniform case is not fully consistent with human psychophysics studies. In humans, visual acuity is higher for both horizontal and vertical orientations and lower for oblique stimuli <sup><xref ref-type="bibr" rid="R1">1</xref>,<xref ref-type="bibr" rid="R48">48</xref></sup>, a phenomenon known as “oblique effect” <sup><xref ref-type="bibr" rid="R37">37</xref></sup>, which is consistent with several neurophysiology studies in cat <sup><xref ref-type="bibr" rid="R30">30</xref></sup> and primate V1 <sup><xref ref-type="bibr" rid="R31">31</xref></sup>. However, we found that vertical orientations were less discriminable than oblique orientations when reading out from the neural populations we recorded. The reason for this discrepancy is not clear. One possible explanation is that in the natural scene observed by mice, especially those observed while running, vertical shapes are less frequent or important.</p><p id="P40">The geometric analysis in the biased case revealed that despite the decrease in responses, discriminability increases between the stimuli around the adaptor, consistent with human psychophysics studies <sup><xref ref-type="bibr" rid="R2">2</xref>,<xref ref-type="bibr" rid="R32">32</xref></sup> and theoretical models <sup><xref ref-type="bibr" rid="R32">32</xref>,<xref ref-type="bibr" rid="R49">49</xref></sup>. A reduced dimensionality analysis shows intuitively how the decrease in responses and increase in discriminability can coexist through a non-uniform transformation of the circle representing the uniform environment. The geometry in the low-dimensional space is consistent with earlier phenomenological theories of adaptation <sup><xref ref-type="bibr" rid="R33">33</xref></sup> but was not until now shown in neural population data. In previous studies in primate V1 <sup><xref ref-type="bibr" rid="R11">11</xref></sup> only an analysis at the level of single neurons was performed, except for a study on neuron pairs <sup><xref ref-type="bibr" rid="R23">23</xref></sup>, which focused only on local discrimination and thus did not on the full population geometry. Furthermore, the results on pairwise noise correlations in primate V1 in <sup><xref ref-type="bibr" rid="R23">23</xref></sup> were different from those in cat V1 <sup><xref ref-type="bibr" rid="R12">12</xref></sup>. It remains to be understood whether our population analysis of mouse V1 would match that of primates and cats.</p><p id="P41">Running strongly affects visual responses <sup><xref ref-type="bibr" rid="R16">16</xref>,<xref ref-type="bibr" rid="R17">17</xref>,<xref ref-type="bibr" rid="R43">43</xref></sup>, and adaptation effects can depend on the animal’s behavioral state <sup><xref ref-type="bibr" rid="R19">19</xref></sup>. We observed that, while running expands the stimulus representations, the increase in the discriminability of an adapted stimulus and the decrease in its responses are present during both stationary and running periods.</p><p id="P42">To understand how orientation, adaptation, and running information are formatted in V1, we performed several CCGP analyses <sup><xref ref-type="bibr" rid="R15">15</xref>,<xref ref-type="bibr" rid="R44">44</xref>,<xref ref-type="bibr" rid="R50">50</xref></sup>. We found that the discriminability of a pair of stimuli depended on the test locomotion condition (higher during running). However, the coding directions for pairs of stimuli in the two locomotion conditions were approximately the same, enabling a decoder trained on stationary stimuli to work also in the running condition (and vice versa). The same geometry supported a higher-than-chance discriminability of stationary vs. running conditions when training a decoder in one orientation and testing it in another one, suggesting that the coding direction of running was approximately preserved across stimulus orientations. Another CCGP analysis suggested that it was also preserved across environments, and thus, adaptation did not affect the running direction. Finally, a different CCGP analysis showed that adaptation shifts the manifold in a direction not perfectly aligned with the locomotion axis.</p><p id="P43">All these results indicate that the locomotion state is encoded, and despite that it is not strongly interfering with the readout of the stimulus orientation. In other words, stimulus orientation and locomotion states are two approximately disentangled variables, enabling a simple linear readout to generalize across multiple situations without any need for retraining. This indicates that the geometry is dominated by a relatively low-dimensional structure <sup><xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R51">51</xref></sup>, which is not trivial to observe in activity spaces whose ambient dimensionality is elevated (i.e. when considering a large number of neurons). This structure has been observed in multiple brain areas across different species <sup><xref ref-type="bibr" rid="R15">15</xref>,<xref ref-type="bibr" rid="R44">44</xref>,<xref ref-type="bibr" rid="R50">50</xref>,<xref ref-type="bibr" rid="R52">52</xref>,<xref ref-type="bibr" rid="R53">53</xref></sup>.</p><p id="P44">Normative theories of adaptation have been based on different frameworks, not necessarily mutually exclusive, including redundancy reduction, predictive coding, surprise salience, inference, and efficient coding <sup><xref ref-type="bibr" rid="R24">24</xref></sup>. Following an efficient coding approach <sup><xref ref-type="bibr" rid="R35">35</xref>,<xref ref-type="bibr" rid="R54">54</xref></sup>, we trained an artificial neural network <sup><xref ref-type="bibr" rid="R34">34</xref></sup> to represent stimuli in different environments under energy constraints. Variations of an efficient coding approach have considered different objective functions, such as the maximization of mutual information <sup><xref ref-type="bibr" rid="R55">55</xref></sup>. Here, similarly to previous work, we considered a tradeoff between representation fidelity and metabolic cost <sup><xref ref-type="bibr" rid="R28">28</xref>,<xref ref-type="bibr" rid="R34">34</xref>,<xref ref-type="bibr" rid="R56">56</xref></sup>. Differently from previous studies, we not only considered its effect on changes in tuning curves and perceptual effects but also on the full population geometry. After comparing networks trained under biased or uniform statistics, we observed an increase in discriminability and a decrease in responses around the adaptor, consistently with our experimental data. It would be interesting to understand how our theory would apply to different forms of adaptation, such as contrast adaptation <sup><xref ref-type="bibr" rid="R20">20</xref>,<xref ref-type="bibr" rid="R22">22</xref></sup> or even affecting other sensory modalities, such as the auditory one <sup><xref ref-type="bibr" rid="R57">57</xref></sup>, and compare it to other related normative theories <sup><xref ref-type="bibr" rid="R58">58</xref>,<xref ref-type="bibr" rid="R59">59</xref></sup>.</p><p id="P45">In conclusion, our model suggests that the stimuli representation is efficiently encoded in a way that considers the stimulus statistics. Several open questions stem from our study. The first question is to understand the detailed neural mechanisms underlying the observed phenomena in the data. The second question is whether the mouse perception reflects the finding in the neural population. Answering this question will require the animal to perform a discrimination task.</p></sec><sec id="S9" sec-type="methods"><title>Methods</title><p id="P46">All experimental procedures were conducted in accordance with the UK Animals (Scientific Procedures Act) 1986. Experiments were performed at University College London under personal and project licenses released by the Home Office following appropriate ethics review.</p><sec id="S10"><title>Mice</title><p id="P47">We recorded neural activity from 12 transgenic animals (5 males, 7 females) in which specific cell types were labeled by a functional or structural indicator. In this study, we focused on all neurons recorded, independently of cell type. Experiments in which an interneuron class was labeled with tdTomato and recorded together with other cells were conducted in double-transgenic mice obtained by crossing Gt(ROSA)26Sor &lt; tm14(CAG-tdTomato)Hze &gt; reporters with appropriate drivers: <italic>Pvalb</italic>&lt;tm1(cre)Arbr &gt; (1 male, 1 female), <italic>Vip</italic>&lt;tm1(cre)Zjh &gt; (1 female), <italic>Sst</italic>&lt;tm2.1(cre)Zjh &gt; (3 males, 1 female), and GAD-nls-mCherry (1 male, 2 females). Experiments in which indicator was expressed uniquely in one neuron class were conducted in single transgenic mice: <italic>Scnn1a-Cre</italic> (1 female). Mice were used for experiments at adult postnatal ages (P59-214).</p></sec><sec id="S11"><title>Animal preparation and virus injection</title><p id="P48">The surgeries were performed in adult mice in a stereotaxic frame and under isoflurane anesthesia (5% for induction, 0.5%–3% during the surgery). During the surgery, we implanted a head-plate for later head fixation, made a craniotomy with a cranial window implant for optical access, and, on relevant experiments, performed virus injections, all during the same surgical procedure. In experiments where an interneuron class was recorded with other cells, mice were injected with an unconditional GCaMP6m virus, AAV1.Syn.GCaMP6m.WPRE.SV40 (#100841; concentration 2.23 10<sup>12</sup>). In experiments where a cell type (excitatory L4 neurons) was labeled by unique expression, mice were injected with AAV-Flex-hSyn-GCaMP6m (#100845; concentration 2.23 10<sup>12</sup>). In a subset of mice crossed with GAD-nls-mCherry (n = 2 females), a sparse set of unspecified neurons (most of them excitatory) were labeled, and the following viruses were injected: pAAV-FLEX-tdTomato (#28306-AAV1; concentration 2.5 10<sup>12</sup>); pENN.AAV.CamKII 0.4.Cre.SV40 (#105558; concentration 4 10<sup>8</sup>). All viruses were acquired from University of Pennsylvania Viral Vector Core. Viruses were injected with a beveled micropipette using a Nanoject II injector (Drummond Scientific Company, Broomall, PA 1) attached to a stereotaxic micromanipulator. Six to seven boli of 100-200 nL virus were slowly (23 nl/min) injected unilaterally into monocular V1, 2.1-3.3 mm laterally and 3.5-4.0mm posteriorly from Bregma and at a depth of L2/3 (200-400 mm).</p></sec><sec id="S12"><title>Visual stimuli</title><p id="P49">Stimuli were horizontal static two-dimensional Gabor functions presented in a location adjusted to match the center of GCaMP expression on one of two screens that spanned 45 to +135 of the horizontal visual field and ± 42.5 of the vertical visual field. During the gray screen presentation (duration 0.5 s), the screens were set to a steady gray level equal to the background of all the stimuli presented for visual response protocols. Gabor functions were presented for 0.5 s, with a spatial frequency of 0.1 cycles/deg and a width of 13 Deg.</p></sec><sec id="S13"><title>Stimulus environments</title><p id="P50">The static gratings presented in the sequences were sampled from either uniform or biased distributions. In the biased distribution, one orientation (45 deg, n=12 recordings; 0 deg, n=7 recordings) was presented 50% of the time. The other stimuli were sampled from a uniform distribution in the remaining trials. In n=6 recordings, the contrast of 7% of the repeats was set to 0, otherwise the contrast was always at 100% at the center of the Gabor stimuli. The number of trials in each sequence was approximately either 1,000 (n=1 recording), 2,000 (n=12 recordings) or 2,500 (n=6 recordings). We always presented first a uniform environment, then a biased environment and lastly another uniform environment.</p></sec><sec id="S14"><title>Imaging</title><p id="P51">Experiments were performed at least two weeks after the virus injection. We used a commercial two-photon microscope with a resonant-galvo scanhead (B-scope, ThorLabs, Ely UK) controlled by ScanImage <sup><xref ref-type="bibr" rid="R60">60</xref></sup>, with an acquisition frame rate of about 30Hz (at 512 by 512 pixels, corresponding to a rate of 4.28-7.5 Hz per plane), which was later interpolated to a frequency of 20 Hz, common to all planes. Recordings were performed in the area where expression was strongest. In most recordings (n = 16) this location was in the monocular zone (MZ, horizontal visual field preference &gt; 30 deg) <sup><xref ref-type="bibr" rid="R61">61</xref></sup>. Other recordings (n = 11) were performed in the callosal binocular zone (CBZ, n = 4, 0-15 deg) <sup><xref ref-type="bibr" rid="R62">62</xref></sup> and others (n = 7) in the acallosal binocular zone (ABZ, 15-30 deg).</p></sec></sec><sec id="S15"><title>Data analysis</title><sec id="S16"><title>Data processing</title><p id="P52">We analyzed raw calcium movies using Suite2p, which performs several processing stages <sup><xref ref-type="bibr" rid="R63">63</xref></sup>. First, Suite2p registers the movies to account for brain motion, then clusters neighboring pixels with similar time courses into regions of interest (ROIs). Based on their morphology, we manually curated ROIs in the Suite2p GUI to distinguish somata from dendritic processes. For spike deconvolution from the calcium traces, we used the default method in Suite2p <sup><xref ref-type="bibr" rid="R63">63</xref></sup>. The outcome of spike deconvolution was the inferred spike probability up to an unknown multiplicative constant independent for each neuron. We later normalized all neural responses at the end of the preprocessing stage, and thus, the unknown multiplicative constant was not influential.</p></sec><sec id="S17"><title>Retinotopic mapping</title><p id="P53">We initially mapped the retinotopy before the adaptation experiments to determine where to place a stimulus in a given recording. To do this mapping, we used sparse noise stimuli, consisting of black or white squares with a width of 6 deg visual angle on a grey background, which were presented to the mouse for 30 min. Squares appeared randomly at fixed positions in a 15 by 45 grid spanning the retinotopic range of the computer screens. At any one time, 2% of the squares were shown.</p></sec><sec id="S18"><title>Normalization of neural responses</title><p id="P54">For each neuron <italic>i</italic>, trial <italic>t</italic>, and environment <italic>l</italic>, spontaneous activity <italic>x</italic><sub><italic>s</italic>,<italic>i</italic>,<italic>l</italic></sub>(<italic>t</italic>) was computed as the average inferred spike probability over 250 ms before the stimulus onset while evoked activity <italic>x</italic><sub><italic>e</italic>,<italic>i</italic>,<italic>l</italic></sub>(<italic>t</italic>) was computed as the average inferred spike probability for the whole stimulus duration of 500 ms.</p><p id="P55">We then detrended the activity in the way described in this paragraph. We considered the average spontaneous activity, averaged across neurons <inline-formula><mml:math id="M1"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≡</mml:mo><mml:msub><mml:mrow><mml:mo>〈</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>〉</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, for each of the three environments (uniform #1, biased, uniform #2). We then separated the values of <inline-formula><mml:math id="M2"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> for each trial depending on the running speed (stationary: v &lt; 1cm/s; low speed: 1 cm/s &lt; v &lt; 15 cm/s; high speed: v &gt; 15 cm/s). In each environment, we considered the locomotion condition out of these three (stationary, low speed, high speed) with the largest number of trials. We finally fit an exponential <italic>f</italic><sub><italic>l</italic></sub>(<italic>t</italic>) <italic>= ae</italic><sup>−<italic>bt</italic></sup> + <italic>c</italic> on the values <inline-formula><mml:math id="M3"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> of this condition. We then divided <italic>f</italic><sub><italic>l</italic></sub>(<italic>t</italic>) to all neurons from spontaneous <italic>x</italic><sub><italic>s</italic>,<italic>i</italic>,<italic>l</italic></sub>(<italic>t</italic>) and evoked <italic>x</italic><sub><italic>e</italic>,<italic>i</italic>,<italic>l</italic></sub>(<italic>t</italic>) by that in the given environment <italic>l</italic>, giving the new values <italic>r</italic><sub><italic>s</italic>,<italic>i</italic>,<italic>l</italic></sub>(<italic>t</italic>) <italic>= x</italic><sub><italic>s</italic>,<italic>i</italic>,<italic>l</italic></sub>(<italic>t</italic>)<italic>/f</italic><sub><italic>l</italic></sub> and <italic>r</italic><sub><italic>e</italic>,<italic>i</italic>,<italic>l</italic></sub>(<italic>t</italic>) <italic>= x</italic><sub><italic>e</italic>,<italic>i</italic>,<italic>l</italic></sub>(<italic>t</italic>)<italic>/f</italic><sub><italic>l</italic></sub> for spontaneous and evoked activity.</p><p id="P56">Finally, we z-scored the evoked activity as follows. For each neuron <italic>i</italic> and environment <italic>l</italic>, we computed the average spontaneous activity, averaged across environments <inline-formula><mml:math id="M4"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>〈</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>〉</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and the standard deviation averaged across environments <inline-formula><mml:math id="M5"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>〈</mml:mo><mml:mi>σ</mml:mi><mml:msub><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>〉</mml:mo></mml:mrow><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> When then computed for each neuron <inline-formula><mml:math id="M6"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="S19"><title>Tuning parameters</title><p id="P57">For each neuron <italic>i</italic> and environment <italic>l</italic>, we computed the preferred orientation by first considering the average response of a neuron across repeats of the same stimuli in a uniform environment <italic>z</italic><sub><italic>i</italic>,<italic>l</italic></sub>(<italic>θ</italic>) <italic>=</italic> ⟨<italic>z</italic><sub><italic>i</italic>,<italic>l</italic></sub>(<italic>t</italic>)⟩<sub><italic>t</italic>∈θ</sub> to an orientation <italic>θ</italic>. We then computed <inline-formula><mml:math id="M7"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:munder><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow><mml:mi>θ</mml:mi></mml:munder><mml:mtext> </mml:mtext><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Then, the preferred orientation corresponded to the circular mean <inline-formula><mml:math id="M8"><mml:mrow><mml:msub><mml:mi>φ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext>atan</mml:mtext><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mstyle displaystyle="true"><mml:mo>Σ</mml:mo></mml:mstyle><mml:mi>θ</mml:mi></mml:msub><mml:msub><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext>cos</mml:mtext><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mstyle displaystyle="true"><mml:mo>Σ</mml:mo></mml:mstyle><mml:mi>θ</mml:mi></mml:msub><mml:msub><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext>sin</mml:mtext><mml:mi>θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic>n</italic><sub><italic>s</italic></sub> <italic>=</italic> 12 is the number of stimuli.</p><p id="P58">To estimate average tuning curves based on preferred orientations (also known as super-neurons), we grouped each neuron into one of <italic>n</italic><sub><italic>s</italic></sub> groups based on the closer discrete orientation. We then took the average responses across neurons without normalizing their responses (other than the normalization already described in a previous session), giving rise to <italic>z</italic><sub><italic>e</italic>,<italic>l</italic></sub>(<italic>θ, φ</italic>), where <italic>φ</italic> it the assigned preferred orientation of a super-neuron.</p><p id="P59">To compute a phenomenological model of the Euclidean distances between average tuning curves we proceeded as follows: we first fitted these average tuning curves to von Mises function and estimated the amplitude a<sub>φ</sub> and concentration parameter κ<sub>φ</sub> : <italic>m</italic>(<italic>θ, φ</italic>) <italic>=</italic> a<sub>φ</sub>exp[κ<sub>φ</sub>(<italic>θ</italic> − <italic>φ</italic>] (we dropped the environment index <italic>l</italic> as this analysis was done only in the uniform environment). Then, we computed an average concentration parameter <inline-formula><mml:math id="M9"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>κ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>φ</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, together with an average amplitude ā<sub>φ</sub>, by fitting a single von Mises function <inline-formula><mml:math id="M10"><mml:mrow><mml:mover accent="true"><mml:mi>m</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>φ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mtext>a</mml:mtext><mml:mi>φ</mml:mi></mml:msub><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mi>φ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:mi>φ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> after averaging all individual responses, which were circularly shifted based on their preferred orientation. After this, we considered two cases: (i) fixed concentration but variable amplitudes or (ii) fixed amplitude but variable width.</p></sec><sec id="S20"><title>Discriminability</title><p id="P60">We decoded the orientation of the static gratings presented to the mice using the population activity from V1 neurons. Let us consider environments with different stimulus statistics, i.e. different probability of stimulus presentation with a given orientation. For each environment, we computed the discrimination accuracy between any pair of stimuli. When considering the same environment, we equalized the number of trials for each recording session in the following way: we computed the minimum number of repeats for a given orientation across all 12 orientations. Then, we used 2/3 of these repeats for training the model and 1/3 for testing the model. We used a Linear Support Vector Classification as a model trained on all neurons recorded for a particular session with a regularization parameter equal to 0.1. We then tested the discrimination accuracy of the model on the test data.</p><p id="P61">In other analyses, training and testing data were from different environments (e.g., training in a uniform condition and testing in a biased condition). We still computed the minimum number of repeats per stimulus in the training and testing environment separately in those cases. We did not need to take only a fraction of this minimum number.</p></sec><sec id="S21"><title>Cross-condition generalization performance</title><p id="P62">To compute the cross-condition generalization performance (CCGP) <sup><xref ref-type="bibr" rid="R15">15</xref>,<xref ref-type="bibr" rid="R44">44</xref></sup>, we typically considered two different variables, for example, stimulus orientation <italic>θ</italic> and locomotion condition (stationary and running). We then trained, for example, a linear decoder to discriminate between two angles, <italic>θ</italic><sub>1</sub> and <italic>θ</italic><sub>2</sub>, during the stationary condition and tested the decoder to discriminate the same angles in the running condition. We sampled the number of trials to balance them across classes separately in the training and test set.</p></sec><sec id="S22"><title>Normative Model</title><sec id="S23"><title>Model training</title><p id="P63">We trained an autoencoder model with three different layers: an input layer <bold><italic>x</italic></bold> with <italic>n</italic><sub><italic>s</italic></sub> <italic>=</italic> 121 units (corresponding to a vectorized 11x11 image), a hidden layer <bold><italic>r</italic></bold> with <italic>n</italic><sub><italic>x</italic></sub> units, an output layer <bold><italic>y</italic></bold> with <italic>n</italic><sub><italic>y</italic></sub> <italic>= n</italic><sub><italic>x</italic></sub> units. We used sigmoid activation functions. The goal of the autoencoder was to minimize a cost function <italic>E = E</italic><sub>,</sub> + <italic>λE</italic><sub>2</sub> where <inline-formula><mml:math id="M11"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mi>Σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M12"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:msub><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mstyle displaystyle="true"><mml:mo>Σ</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, which, since <italic>r</italic> &gt; 0, represents an L1 sparsity constraint (metabolic cost). We added Gaussian noise with strengths <italic>ε</italic><sub><italic>x</italic></sub> and <italic>ε</italic><sub><italic>r</italic></sub> = 1.0 in the input and hidden layers. We trained the autoencoder using Pytorch using a batch size of 500 and a max number of epochs of 3,000 if convergence was not reached. We set the convergence error to <inline-formula><mml:math id="M13"><mml:mrow><mml:mn>0.05</mml:mn><mml:mo>⋅</mml:mo><mml:msubsup><mml:mi>ε</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>. We implemented gradient descent with the Adam algorithm with a learning rate of 10<sup>-4</sup>.</p><p id="P64">We scanned through three different hyperparameters: the penalty factor on the metabolic cost <italic>λ</italic> ∈ {0., 0.1,0.25,0.5,1.}, the noise in the input <italic>ε</italic><sub><italic>x</italic></sub> ∈ {0.25,0.5,1., 2.}, the number of hidden units <italic>n</italic><sub><italic>x</italic></sub> ∈ {500, 1,000, 2,000, 3,000, 5,000} for a total of 100 of different sets of hyperparameters. We repeated 30 trainings with random initializations for each of these parameters.</p></sec></sec><sec id="S24"><title>Stimuli presented to the model</title><p id="P65">Similarly to the experiments, we presented sequences of oriented gratings plus blank stimuli with zero contrast to the model. Both uniform and biased environments had 2,000 blank stimuli. Then, 120 trials were added with 10 repeats of one orientation for each of the 12 considered orientations. In addition, 1,000 other stimuli were drawn from a von Mises distribution with a concentration parameter of 0 in the uniform environment and a concentration parameter of 150 in the biased environment. Stimuli had a size of 11×11 pixels and corresponded to a Gabor function with a spatial scale of 10 pixels and spatial frequency of 1.5 pixels.</p></sec><sec id="S25"><title>Decoding of model responses</title><p id="P66">We computed discrimination accuracy using the same method used for the neural data but we only used a subset <italic>n</italic><sub><italic>d</italic></sub> <italic>=</italic> 50 of the artificial neurons to decode the direction of motion from the population activity. In contrast to the training procedure, for the decoder we used the trained models as forward models but changed the level of noise in the input to <inline-formula><mml:math id="M14"><mml:mrow><mml:msubsup><mml:mi>ε</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:math></inline-formula> and, similarly, in the hidden layer to <inline-formula><mml:math id="M15"><mml:mrow><mml:msubsup><mml:mi>ε</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula>. The values of <inline-formula><mml:math id="M16"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>ε</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="M17"><mml:mrow><mml:msubsup><mml:mi>ε</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, where chosen to ensure compatibility on the experimental results on discrimination accuracy.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Material</label><media xlink:href="EMS202897-supplement-Supplementary_Materials.pdf" mimetype="application" mime-subtype="pdf" id="d2aAcFbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S26"><title>Acknowledgments</title><p>This work was supported by NIH (R21 EY035064 to MD and DLR and U19 NS107613 to KDM); NSF (NeuroNex Award 1707398 DBI to KDM, and SF); the Gatsby Charitable Foundation (GAT3708 to KDM, and SF); the Simons Foundation to SF; the Swartz Foundation to SF and KDM; Kavli Foundation to MD, SF and KDM; and the Wellcome Trust (223144/Z/21/Z to MC and KDH). We thank Lauren Wool for her support in designing a set of visual stimuli used in this work. We also thank Joram Keijser and Xuexin Wei for helpful comments on the manuscript.</p></ack><fn-group><fn fn-type="con" id="FN2"><p id="P67"><bold>Author contributions</bold></p><p id="P68">MD and MC designed the experiments; MD and SB performed the experiments; SB and CBR performed surgery; MD, RN, and YF preprocessed the data; MD, RN, DLR, KDM, MC, and SF designed the data analysis; MD performed the data analysis; MD, RN, and SF designed the model; MD and RN developed the model; MD, RN, KDH, DLR, KDM, MC, and SF interpreted results; MD wrote the original draft; MD, RN, SB, DLR, KDM, MC, and SF edited the manuscript.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caelli</surname><given-names>T</given-names></name><name><surname>Brei-El</surname><given-names>H</given-names></name><name><surname>Rentschler</surname><given-names>I</given-names></name></person-group><article-title>Discrimina:on thresholds in the two-dimensional spatial frequency domain</article-title><source>Vision Research</source><year>1983</year><volume>23</volume><fpage>129</fpage><lpage>133</lpage><pub-id pub-id-type="pmid">6868387</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Regan</surname><given-names>D</given-names></name><name><surname>Beverley</surname><given-names>KI</given-names></name></person-group><article-title>Postadaptation orientation discrimination</article-title><source>J Opt Soc Am A</source><year>1985</year><volume>2</volume><fpage>147</fpage><pub-id pub-id-type="pmid">3973752</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clifford</surname><given-names>CWG</given-names></name></person-group><article-title>The tilt illusion: Phenomenology and functional implications</article-title><source>Vision Research</source><year>2014</year><volume>104</volume><fpage>3</fpage><lpage>11</lpage><pub-id pub-id-type="pmid">24995379</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kohn</surname><given-names>A</given-names></name></person-group><article-title>Visual Adaptation: Physiology, Mechanisms, and Functional Benefits</article-title><source>Journal of Neurophysiology</source><year>2007</year><volume>97</volume><fpage>3155</fpage><lpage>3164</lpage><pub-id pub-id-type="pmid">17344377</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barlow</surname><given-names>HB</given-names></name><name><surname>Hill</surname><given-names>RM</given-names></name></person-group><article-title>Evidence for a Physiological Explanation of the Waterfall Phenomenon and Figural After-effects</article-title><source>Nature</source><year>1963</year><volume>200</volume><fpage>1345</fpage><lpage>1347</lpage><pub-id pub-id-type="pmid">14098503</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dragoi</surname><given-names>V</given-names></name><name><surname>Sharma</surname><given-names>J</given-names></name><name><surname>Sur</surname><given-names>M</given-names></name></person-group><article-title>Adaptation-Induced Plasticity of Orientation Tuning in Adult Visual Cortex</article-title><source>Neuron</source><year>2000</year><volume>28</volume><fpage>287</fpage><lpage>298</lpage><pub-id pub-id-type="pmid">11087001</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jin</surname><given-names>M</given-names></name><name><surname>Beck</surname><given-names>JM</given-names></name><name><surname>Glickfeld</surname><given-names>LL</given-names></name></person-group><article-title>Neuronal Adaptation Reveals a Suboptimal Decoding of Orientation Tuned Populations in the Mouse Visual Cortex</article-title><source>J Neurosci</source><year>2019</year><volume>39</volume><fpage>3867</fpage><lpage>3881</lpage><pub-id pub-id-type="pmcid">PMC6520502</pub-id><pub-id pub-id-type="pmid">30833509</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3172-18.2019</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Angeloni</surname><given-names>C</given-names></name><name><surname>Geffen</surname><given-names>M</given-names></name></person-group><article-title>Contextual modulation of sound processing in the auditory cortex</article-title><source>Current Opinion in Neurobiology</source><year>2018</year><volume>49</volume><fpage>8</fpage><lpage>15</lpage><pub-id pub-id-type="pmcid">PMC6037899</pub-id><pub-id pub-id-type="pmid">29125987</pub-id><pub-id pub-id-type="doi">10.1016/j.conb.2017.10.012</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kohn</surname><given-names>A</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><article-title>Adaptation changes the direction tuning of macaque MT neurons</article-title><source>Nat Neurosci</source><year>2004</year><volume>7</volume><fpage>764</fpage><lpage>772</lpage><pub-id pub-id-type="pmid">15195097</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shapley</surname><given-names>R</given-names></name><name><surname>Enroth-Cugell</surname><given-names>C</given-names></name></person-group><article-title>Chapter 9 Visual adaptation and retinal gain controls</article-title><source>Progress in Retinal Research</source><year>1984</year><volume>3</volume><fpage>263</fpage><lpage>346</lpage></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dragoi</surname><given-names>V</given-names></name><name><surname>Sharma</surname><given-names>J</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Sur</surname><given-names>M</given-names></name></person-group><article-title>Dynamics of neuronal sensitivity in visual cortex and local feature discrimination</article-title><source>Nat Neurosci</source><year>2002</year><volume>5</volume><fpage>883</fpage><lpage>891</lpage><pub-id pub-id-type="pmid">12161755</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benucci</surname><given-names>A</given-names></name><name><surname>Saleem</surname><given-names>AB</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name></person-group><article-title>Adaptation maintains population homeostasis in primary visual cortex</article-title><source>Nat Neurosci</source><year>2013</year><volume>16</volume><fpage>724</fpage><lpage>729</lpage><pub-id pub-id-type="pmcid">PMC3665725</pub-id><pub-id pub-id-type="pmid">23603708</pub-id><pub-id pub-id-type="doi">10.1038/nn.3382</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Averbeck</surname><given-names>BB</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><article-title>Neural correlations, population coding and computation</article-title><source>Nat Rev Neurosci</source><year>2006</year><volume>7</volume><fpage>358</fpage><lpage>366</lpage><pub-id pub-id-type="pmid">16760916</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nogueira</surname><given-names>R</given-names></name><etal/></person-group><article-title>The Effects of Population Tuning and Trial-by-Trial Variability on Information Encoding and Behavior</article-title><source>J Neurosci</source><year>2020</year><volume>40</volume><fpage>1066</fpage><lpage>1083</lpage><pub-id pub-id-type="pmcid">PMC6989000</pub-id><pub-id pub-id-type="pmid">31754013</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0859-19.2019</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernardi</surname><given-names>S</given-names></name><etal/></person-group><article-title>The Geometry of Abstraction in the Hippocampus and Prefrontal Cortex</article-title><source>Cell</source><year>2020</year><volume>183</volume><fpage>954</fpage><lpage>967</lpage><elocation-id>e21</elocation-id><pub-id pub-id-type="pmcid">PMC8451959</pub-id><pub-id pub-id-type="pmid">33058757</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2020.09.031</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dipoppa</surname><given-names>M</given-names></name><etal/></person-group><article-title>Vision and Locomotion Shape the Interactions between Neuron Types in Mouse Visual Cortex</article-title><source>Neuron</source><year>2018</year><volume>98</volume><fpage>602</fpage><lpage>615</lpage><elocation-id>e8</elocation-id><pub-id pub-id-type="pmcid">PMC5946730</pub-id><pub-id pub-id-type="pmid">29656873</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2018.03.037</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niell</surname><given-names>CM</given-names></name><name><surname>Stryker</surname><given-names>MP</given-names></name></person-group><article-title>Modulation of Visual Responses by Behavioral State in Mouse Visual Cortex</article-title><source>Neuron</source><year>2010</year><volume>65</volume><fpage>472</fpage><lpage>479</lpage><pub-id pub-id-type="pmcid">PMC3184003</pub-id><pub-id pub-id-type="pmid">20188652</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2010.01.033</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name><etal/></person-group><article-title>Spontaneous behaviors drive multidimensional, brainwide activity</article-title><source>Science</source><year>2019</year><volume>364</volume><elocation-id>eaav7893</elocation-id><pub-id pub-id-type="pmcid">PMC6525101</pub-id><pub-id pub-id-type="pmid">31000656</pub-id><pub-id pub-id-type="doi">10.1126/science.aav7893</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keller</surname><given-names>AJ</given-names></name><etal/></person-group><article-title>Stimulus relevance modulates contrast adaptation in visual cortex</article-title><source>eLife</source><year>2017</year><volume>6</volume><elocation-id>e21589</elocation-id><pub-id pub-id-type="pmcid">PMC5298877</pub-id><pub-id pub-id-type="pmid">28130922</pub-id><pub-id pub-id-type="doi">10.7554/eLife.21589</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tring</surname><given-names>E</given-names></name><name><surname>Dipoppa</surname><given-names>M</given-names></name><name><surname>Ringach</surname><given-names>DL</given-names></name></person-group><article-title>On the contrast response function of adapted neural populations</article-title><source>Journal of Neurophysiology</source><year>2024</year><volume>131</volume><fpage>446</fpage><lpage>453</lpage><pub-id pub-id-type="pmcid">PMC11305633</pub-id><pub-id pub-id-type="pmid">38264786</pub-id><pub-id pub-id-type="doi">10.1152/jn.00413.2023</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tring</surname><given-names>E</given-names></name><name><surname>Dipoppa</surname><given-names>M</given-names></name><name><surname>Ringach</surname><given-names>DL</given-names></name></person-group><article-title>A power law describes the magnitude of adaptation in neural populations of primary visual cortex</article-title><source>Nat Commun</source><year>2023</year><volume>14</volume><fpage>8366</fpage><pub-id pub-id-type="pmcid">PMC10724159</pub-id><pub-id pub-id-type="pmid">38102113</pub-id><pub-id pub-id-type="doi">10.1038/s41467-023-43572-w</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tring</surname><given-names>E</given-names></name><name><surname>Moosavi</surname><given-names>SA</given-names></name><name><surname>Dipoppa</surname><given-names>M</given-names></name><name><surname>Ringach</surname><given-names>DL</given-names></name></person-group><article-title>Contrast gain control is a reparameterization of a population response curve</article-title><source>Journal of Neurophysiology</source><year>2024</year><pub-id pub-id-type="pmcid">PMC11573281</pub-id><pub-id pub-id-type="pmid">39292873</pub-id><pub-id pub-id-type="doi">10.1152/jn.00336.2024</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gutnisky</surname><given-names>DA</given-names></name><name><surname>Dragoi</surname><given-names>V</given-names></name></person-group><article-title>Adaptive coding of visual information in neural populations</article-title><source>Nature</source><year>2008</year><volume>452</volume><fpage>220</fpage><lpage>224</lpage><pub-id pub-id-type="pmid">18337822</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weber</surname><given-names>AI</given-names></name><name><surname>Krishnamurthy</surname><given-names>K</given-names></name><name><surname>Fairhall</surname><given-names>AL</given-names></name></person-group><article-title>Coding Principles in Adaptation</article-title><source>Annu Rev Vis Sci</source><year>2019</year><volume>5</volume><fpage>427</fpage><lpage>449</lpage><pub-id pub-id-type="pmid">31283447</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wainwright</surname><given-names>MJ</given-names></name></person-group><article-title>Visual adaptation as optimal information transmission</article-title><source>Vision Research</source><year>1999</year><volume>39</volume><fpage>3960</fpage><lpage>3974</lpage><pub-id pub-id-type="pmid">10748928</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Barlow</surname><given-names>H</given-names></name><name><surname>Földiák</surname><given-names>P</given-names></name></person-group><chapter-title>Adaptation and decorrelation in the cortex</chapter-title><source>The Computing Neuron</source><publisher-name>Addison Wesley</publisher-name><year>1989</year><fpage>54</fpage><lpage>72</lpage></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Westrick</surname><given-names>ZM</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name><name><surname>Landy</surname><given-names>MS</given-names></name></person-group><article-title>Pattern Adaptation and Normalization Reweighting</article-title><source>J Neurosci</source><year>2016</year><volume>36</volume><fpage>9805</fpage><lpage>9816</lpage><pub-id pub-id-type="pmcid">PMC5030348</pub-id><pub-id pub-id-type="pmid">27656020</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1067-16.2016</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duong</surname><given-names>LR</given-names></name><name><surname>Bredenberg</surname><given-names>C</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><article-title>Adaptive coding efficiency in recurrent cortical circuits via gain control</article-title><year>2023</year><comment>Preprint at <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2305.19869">http://arxiv.org/abs/2305.19869</ext-link></comment></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gutierrez</surname><given-names>GJ</given-names></name><name><surname>Denève</surname><given-names>S</given-names></name></person-group><article-title>Population adaptation in efficient balanced networks</article-title><source>eLife</source><year>2019</year><volume>8</volume><elocation-id>e46926</elocation-id><pub-id pub-id-type="pmcid">PMC6759354</pub-id><pub-id pub-id-type="pmid">31550233</pub-id><pub-id pub-id-type="doi">10.7554/eLife.46926</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>B</given-names></name><name><surname>Peterson</surname><given-names>MR</given-names></name><name><surname>Freeman</surname><given-names>RD</given-names></name></person-group><article-title>Oblique Effect: A Neural Basis in the Visual Cortex</article-title><source>Journal of Neurophysiology</source><year>2003</year><volume>90</volume><fpage>204</fpage><lpage>217</lpage><pub-id pub-id-type="pmid">12611956</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Valois</surname><given-names>RL</given-names></name><name><surname>William Yund</surname><given-names>E</given-names></name><name><surname>Hepler</surname><given-names>N</given-names></name></person-group><article-title>The orientation and direction selectivity of cells in macaque visual cortex</article-title><source>Vision Research</source><year>1982</year><volume>22</volume><fpage>531</fpage><lpage>544</lpage><pub-id pub-id-type="pmid">7112953</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mao</surname><given-names>J</given-names></name><name><surname>Rothkopf</surname><given-names>CA</given-names></name><name><surname>Stocker</surname><given-names>AA</given-names></name></person-group><article-title>Adaptation optimizes sensory encoding for future stimuli</article-title><source>PLoS Comput Biol</source><year>2025</year><volume>21</volume><elocation-id>e1012746</elocation-id><pub-id pub-id-type="pmcid">PMC11771873</pub-id><pub-id pub-id-type="pmid">39823517</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1012746</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clifford</surname><given-names>CWG</given-names></name><name><surname>Wenderoth</surname><given-names>P</given-names></name><name><surname>Spehar</surname><given-names>B</given-names></name></person-group><article-title>A functional angle on some after-effects in cortical vision</article-title><source>Proc R Soc Lond B</source><year>2000</year><volume>267</volume><fpage>1705</fpage><lpage>1710</lpage><pub-id pub-id-type="pmcid">PMC1690741</pub-id><pub-id pub-id-type="pmid">12233765</pub-id><pub-id pub-id-type="doi">10.1098/rspb.2000.1198</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamin</surname><given-names>AS</given-names></name><name><surname>Zhang</surname><given-names>L-Q</given-names></name><name><surname>Qiu</surname><given-names>C</given-names></name><name><surname>Stocker</surname><given-names>AA</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name></person-group><article-title>Efficient neural codes naturally emerge through gradient descent learning</article-title><source>Nat Commun</source><year>2022</year><volume>13</volume><fpage>7972</fpage><pub-id pub-id-type="pmcid">PMC9800366</pub-id><pub-id pub-id-type="pmid">36581618</pub-id><pub-id pub-id-type="doi">10.1038/s41467-022-35659-7</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname><given-names>BA</given-names></name><name><surname>Field</surname><given-names>DJ</given-names></name></person-group><article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title><source>Nature</source><year>1996</year><volume>381</volume><pub-id pub-id-type="pmid">8637596</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name></person-group><article-title>High-precision coding in visual cortex</article-title><source>Cell</source><year>2021</year><volume>184</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="pmid">33857423</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Appelle</surname><given-names>S</given-names></name></person-group><article-title>Perception and discrimination as a function of stimulus orientation: The ‘oblique effect’ in man and animals</article-title><source>Psychological Bulletin</source><year>1972</year><volume>78</volume><fpage>266</fpage><lpage>278</lpage><pub-id pub-id-type="pmid">4562947</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Westheimer</surname><given-names>G</given-names></name><name><surname>Gee</surname><given-names>A</given-names></name></person-group><article-title>Orthogonal adaptation and orientation discrimination</article-title><source>Vision Research</source><year>2002</year><volume>42</volume><fpage>2339</fpage><lpage>2343</lpage><pub-id pub-id-type="pmid">12350422</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stefanini</surname><given-names>F</given-names></name><etal/></person-group><article-title>A Distributed Neural Code in the Dentate Gyrus and in CA1</article-title><source>Neuron</source><year>2020</year><volume>107</volume><fpage>703</fpage><lpage>716</lpage><elocation-id>e4</elocation-id><pub-id pub-id-type="pmcid">PMC7442694</pub-id><pub-id pub-id-type="pmid">32521223</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2020.05.022</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seriès</surname><given-names>P</given-names></name><name><surname>Stocker</surname><given-names>AA</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><article-title>Is the Homunculus “Aware” of Sensory Adaptation?</article-title><source>Neural Computation</source><year>2009</year><volume>21</volume><fpage>3271</fpage><lpage>3304</lpage><pub-id pub-id-type="pmcid">PMC3134250</pub-id><pub-id pub-id-type="pmid">19686064</pub-id><pub-id pub-id-type="doi">10.1162/neco.2009.09-08-869</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deitch</surname><given-names>D</given-names></name><name><surname>Rubin</surname><given-names>A</given-names></name><name><surname>Ziv</surname><given-names>Y</given-names></name></person-group><article-title>Representational drift in the mouse visual cortex</article-title><source>Current Biology</source><year>2021</year><volume>31</volume><fpage>4327</fpage><lpage>4339</lpage><elocation-id>e6</elocation-id><pub-id pub-id-type="pmid">34433077</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rule</surname><given-names>ME</given-names></name><name><surname>O’Leary</surname><given-names>T</given-names></name><name><surname>Harvey</surname><given-names>CD</given-names></name></person-group><article-title>Causes and consequences of representational drift</article-title><source>Current Opinion in Neurobiology</source><year>2019</year><volume>58</volume><fpage>141</fpage><lpage>147</lpage><pub-id pub-id-type="pmcid">PMC7385530</pub-id><pub-id pub-id-type="pmid">31569062</pub-id><pub-id pub-id-type="doi">10.1016/j.conb.2019.08.005</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horrocks</surname><given-names>EAB</given-names></name><name><surname>Rodrigues</surname><given-names>FR</given-names></name><name><surname>Saleem</surname><given-names>AB</given-names></name></person-group><article-title>Flexible neural population dynamics govern the speed and stability of sensory encoding in mouse visual cortex</article-title><source>Nat Commun</source><year>2024</year><volume>15</volume><fpage>6415</fpage><pub-id pub-id-type="pmcid">PMC11289260</pub-id><pub-id pub-id-type="pmid">39080254</pub-id><pub-id pub-id-type="doi">10.1038/s41467-024-50563-y</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nogueira</surname><given-names>R</given-names></name><name><surname>Rodgers</surname><given-names>CC</given-names></name><name><surname>Bruno</surname><given-names>RM</given-names></name><name><surname>Fusi</surname><given-names>S</given-names></name></person-group><article-title>The geometry of cortical representations of touch in rodents</article-title><source>Nat Neurosci</source><year>2023</year><volume>26</volume><fpage>239</fpage><lpage>250</lpage><pub-id pub-id-type="pmid">36624277</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Wei</surname><given-names>X-X</given-names></name></person-group><article-title>Neural tuning and representational geometry</article-title><source>Nat Rev Neurosci</source><year>2021</year><volume>22</volume><fpage>703</fpage><lpage>718</lpage><pub-id pub-id-type="pmid">34522043</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gibson</surname><given-names>JJ</given-names></name><name><surname>Radner</surname><given-names>M</given-names></name></person-group><article-title>Adaptation, after-effect and contrast in the perception of tilted lines. I. Quantitative studies</article-title><source>Journal of Experimental Psychology</source><year>1937</year><volume>20</volume><fpage>453</fpage><lpage>467</lpage></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jin</surname><given-names>DZ</given-names></name><name><surname>Dragoi</surname><given-names>V</given-names></name><name><surname>Sur</surname><given-names>M</given-names></name><name><surname>Seung</surname><given-names>HS</given-names></name></person-group><article-title>Tilt Aftereffect and Adaptation-Induced Changes in Orientation Tuning in Visual Cortex</article-title><source>Journal of Neurophysiology</source><year>2005</year><volume>94</volume><fpage>4038</fpage><lpage>4050</lpage><pub-id pub-id-type="pmid">16135549</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamblin</surname><given-names>JR</given-names></name><name><surname>Winser</surname><given-names>TH</given-names></name></person-group><article-title>On the resolution of gratings by the astigmatic eye</article-title><source>Trans Opt Soc</source><year>1927</year><volume>29</volume><fpage>28</fpage><lpage>42</lpage></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stocker</surname><given-names>AA</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><article-title>Visual motion aftereffects arise from a cascade of two isomorphic adaptation mechanisms</article-title><source>Journal of Vision</source><year>2009</year><volume>9</volume><fpage>9</fpage><pub-id pub-id-type="pmcid">PMC3718883</pub-id><pub-id pub-id-type="pmid">19761342</pub-id><pub-id pub-id-type="doi">10.1167/9.9.9</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boyle</surname><given-names>LM</given-names></name><name><surname>Posani</surname><given-names>L</given-names></name><name><surname>Irfan</surname><given-names>S</given-names></name><name><surname>Siegelbaum</surname><given-names>SA</given-names></name><name><surname>Fusi</surname><given-names>S</given-names></name></person-group><article-title>Tuned geometries of hippocampal representations meet the computational demands of social memory</article-title><source>Neuron</source><year>2024</year><volume>112</volume><fpage>1358</fpage><lpage>1371</lpage><elocation-id>e9</elocation-id><pub-id pub-id-type="pmcid">PMC11186585</pub-id><pub-id pub-id-type="pmid">38382521</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2024.01.021</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Musall</surname><given-names>S</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Juavinei</surname><given-names>AL</given-names></name><name><surname>Gluf</surname><given-names>S</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name></person-group><article-title>Single-trial neural dynamics are dominated by richly varied movements</article-title><source>Nat Neurosci</source><year>2019</year><volume>22</volume><fpage>1677</fpage><lpage>1686</lpage><pub-id pub-id-type="pmcid">PMC6768091</pub-id><pub-id pub-id-type="pmid">31551604</pub-id><pub-id pub-id-type="doi">10.1038/s41593-019-0502-4</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Courellis</surname><given-names>HS</given-names></name><etal/></person-group><article-title>Abstract representations emerge in human hippocampal neurons during inference</article-title><source>Nature</source><year>2024</year><volume>632</volume><fpage>841</fpage><lpage>849</lpage><pub-id pub-id-type="pmcid">PMC11338822</pub-id><pub-id pub-id-type="pmid">39143207</pub-id><pub-id pub-id-type="doi">10.1038/s41586-024-07799-x</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Higgins</surname><given-names>I</given-names></name><etal/></person-group><article-title>Unsupervised deep learning identifies semantic disentanglement in single inferotemporal face patch neurons</article-title><source>Nat Commun</source><year>2021</year><volume>12</volume><fpage>6456</fpage><pub-id pub-id-type="pmcid">PMC8578601</pub-id><pub-id pub-id-type="pmid">34753913</pub-id><pub-id pub-id-type="doi">10.1038/s41467-021-26751-5</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ganguli</surname><given-names>D</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><article-title>Efficient Sensory Encoding and Bayesian Inference with Heterogeneous Neural Populations</article-title><source>Neural Computation</source><year>2014</year><volume>26</volume><fpage>2103</fpage><lpage>2134</lpage><pub-id pub-id-type="pmcid">PMC4167880</pub-id><pub-id pub-id-type="pmid">25058702</pub-id><pub-id pub-id-type="doi">10.1162/NECO_a_00638</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>X-X</given-names></name><name><surname>Stocker</surname><given-names>AA</given-names></name></person-group><article-title>A Bayesian observer model constrained by efficient coding can explain ‘anti-Bayesian’ percepts</article-title><source>Nat Neurosci</source><year>2015</year><volume>18</volume><fpage>1509</fpage><lpage>1517</lpage><pub-id pub-id-type="pmid">26343249</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Wei</surname><given-names>X-X</given-names></name><name><surname>Stocker</surname><given-names>AA</given-names></name><name><surname>Lee</surname><given-names>DD</given-names></name></person-group><article-title>Efficient Neural Codes under Metabolic Constraints</article-title><source>Advances in Neural Information Processing Systems</source><year>2016</year><volume>29</volume><fpage>9</fpage></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Natan</surname><given-names>RG</given-names></name><etal/></person-group><article-title>Complementary control of sensory adaptation by two types of cortical interneurons</article-title><source>eLife</source><year>2015</year><volume>4</volume><elocation-id>e09868</elocation-id><pub-id pub-id-type="pmcid">PMC4641469</pub-id><pub-id pub-id-type="pmid">26460542</pub-id><pub-id pub-id-type="doi">10.7554/eLife.09868</pub-id></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Angeloni</surname><given-names>CF</given-names></name><etal/></person-group><article-title>Dynamics of cortical contrast adaptation predict perception of signals in noise</article-title><source>Nat Commun</source><year>2023</year><volume>14</volume><fpage>4817</fpage><pub-id pub-id-type="pmcid">PMC10412650</pub-id><pub-id pub-id-type="pmid">37558677</pub-id><pub-id pub-id-type="doi">10.1038/s41467-023-40477-6</pub-id></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mlynarski</surname><given-names>WF</given-names></name><name><surname>Hermundstad</surname><given-names>AM</given-names></name></person-group><article-title>Efficient and adaptive sensory codes</article-title><source>Nat Neurosci</source><year>2021</year><volume>24</volume><fpage>998</fpage><lpage>1009</lpage><pub-id pub-id-type="pmid">34017131</pub-id></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pologruto</surname><given-names>TA</given-names></name><name><surname>Sabatini</surname><given-names>BL</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><article-title>ScanImage: Flexible software for operating laser scanning microscopes</article-title><source>BioMed Eng OnLine</source><year>2003</year><volume>2</volume><fpage>13</fpage><pub-id pub-id-type="pmcid">PMC161784</pub-id><pub-id pub-id-type="pmid">12801419</pub-id><pub-id pub-id-type="doi">10.1186/1475-925X-2-13</pub-id></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wagor</surname><given-names>E</given-names></name><name><surname>Mangini</surname><given-names>NJ</given-names></name><name><surname>Pearlman</surname><given-names>AL</given-names></name></person-group><article-title>Retinotopic organization of striate and extrastriate visual cortex in the mouse</article-title><source>J of Comparative Neurology</source><year>1980</year><volume>193</volume><fpage>187</fpage><lpage>202</lpage><pub-id pub-id-type="pmid">6776164</pub-id></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Burkhalter</surname><given-names>A</given-names></name></person-group><article-title>Area map of mouse visual cortex</article-title><source>J of Comparative Neurology</source><year>2007</year><volume>502</volume><fpage>339</fpage><lpage>357</lpage><pub-id pub-id-type="pmid">17366604</pub-id></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pachitariu</surname><given-names>M</given-names></name><etal/></person-group><article-title>Suite2p: beyond 10,000 neurons with standard two-photon microscopy</article-title><source>bioRxiv</source><year>2017</year><elocation-id>061507</elocation-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Relation between tuning curves and geometry in V1.</title><p><bold>a</bold>) Two-photon recordings of V1 neurons in head-fixed mice freely moving on a ball while viewing static gratings of different orientations. <bold>b)</bold> Example segment of oriented grating sequences drawn from a uniform distribution. <bold>c</bold>) Average normalized activity of all neurons recorded in an example session sorted by their preferred orientation. Orientation at 0 deg is vertical, while at 90 deg is horizontal. <bold>d)</bold> Distribution of preferred orientations across neurons for individual recording sessions (gray) and averaged across sessions (black). <bold>e</bold>) Orientation tuning sharpness (concentration parameter of fitted von Mises functions) as a function of the preferred orientation of the neurons for individual recording sessions (gray) and for averaged across recording sessions (black). <bold>f</bold>) The first two Principal Components (PCs) of the population responses in (c), showing responses in each repeat (small dots) and their averages for each stimulus (large circles). <bold>g</bold>) Covariance across trials of the population responses. For each stimulus orientation, the ellipse is centered on the average responses, and its axes are proportional to the square root of eigenvalues of the stimulus-conditioned covariance matrix of the trial responses in PC space. <bold>h</bold>) The first three PCs of the population responses, with lines indicating the main axis of the ellipsoid in (g) in three dimensions. <bold>i</bold>) Discrimination accuracy of any pair of stimuli, defined as discrimination accuracy of a linear classifier, averaged across all recording sessions. <bold>j</bold>) Discrimination accuracy for pairs of orientations differing by 15 deg (squares outlined in green in (i)) for each recording session (light green) and averaged across all sessions (dark green).</p></caption><graphic xlink:href="EMS202897-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Adaptation increases discriminability around the adaptor while reducing neural responses.</title><p><bold>a</bold>) Example segment of oriented grating sequences drawn from a uniform (black, same as in <xref ref-type="fig" rid="F1">Fig. 1b</xref>) or biased distribution (red). <bold>b</bold>) Projection of neural activity in PC space, similar to <xref ref-type="fig" rid="F1">Fig. 1f</xref>, but in a biased environment (colored dots, black lines), gray lines and dots correspond to the uniform environment; insets in the left and center panels focus on the adaptor orientations and orientations that are 15 deg distant from it. <bold>c)</bold> Average responses of tuned neurons in a single recording session (same as in panel b) whose preferred orientation has a given distance from the adaptor orientation (y-axis) to stimuli with a given distance from the adaptor orientation (x-axis); left: uniform environment; right: biased environment; white square: response to the adaptor stimulus of neurons tuned to the adapter. <bold>d</bold>) Difference in normalized responses between biased and uniform environments in (c). <bold>e</bold>) Same as in (d) but across recording sessions; pluses (resp. minuses) correspond to a significant increase (resp. decrease) in responses (p &lt; 0.05, 1-sample t-test); green squares correspond to average values in (f); <bold>f</bold>) change in average responses between biased and uniform environments at the preferred orientation of each group of neurons (light green: single recording sessions; dark green: average across sessions; red asterisks: significant increase in responses as in (e); blue asterisks: significant decrease in responses as in (e)). <bold>g</bold>) Discrimination accuracy for any pair of stimuli in a uniform environment for one example session; right: same as the left panel but in a biased environment; green square corresponds to adapter orientation; <bold>h</bold>) Difference in population discrimination accuracy in an example recording between the biased and the uniform environment in (g). <bold>i</bold>) Similar to the example session in (h) but averaged across sessions; green squares correspond to average values in (j); pluses correspond to a significant increase in discrimination (p &lt; 0.05, 1-sample t-test). <bold>j</bold>) Change in discrimination accuracy between biased and uniform environments for stimuli 15 deg apart as a function of the distance of the stimuli from the adapter; light green: individual recording sessions, dark green: average across all sessions; asterisks are a significant increase in discrimination accuracy (p &lt; 10<sup>-4</sup>, 1-sample t-test).</p></caption><graphic xlink:href="EMS202897-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Running expands the geometry of representations.</title><p><bold>a</bold>) Same format as in <xref ref-type="fig" rid="F1">Fig. 1f-h</xref>, comparing average stimulus responses measured when the mouse was running (squares, solid lines) vs. stationary (diamonds, dashed lines). Dots represent individual trials and are shown only during running. <bold>b</bold>) Discrimination accuracy between pairs of orientations for a model that has been trained and tested during stationary (left) or running (right) periods; <bold>c)</bold> Euclidean distance of population responses between any pair of stimuli (averaged across recording sessions). <bold>d)</bold> Cartoon illustrating the measurement of cross-condition generalization performance (CCGP); left: a model was trained (black contour and black line) to discriminate two orientations (more opaque colors) during the running condition (squares) and then tested (blue contours) to discriminate the same orientation during the stationary condition (diamonds); for illustration purposes, we also plotted the hyperplane separating the points trained in the other condition (blue dotted line); right: same as before but model was trained in the stationary condition and tested in the running condition. <bold>e</bold>) Discrimination accuracy between pairs of orientations during running for a model that has been trained during stationary periods (left) or vice versa (right).</p></caption><graphic xlink:href="EMS202897-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Interaction between adaptation and running.</title><p><bold>a</bold>) Projection of neural activity in PC space, similar to <xref ref-type="fig" rid="F2">Fig. 2b</xref>, but for the stationary condition only. Colored dots and black lines correspond to a biased environment, while gray lines and dots correspond to a uniform environment. <bold>b</bold>) Similar to (a) but for the running condition. <bold>c)</bold> Difference in normalized responses between biased and uniform environments averaged across experiments similar to <xref ref-type="fig" rid="F2">Fig. 2e</xref>, but only for the stationary condition. <bold>d</bold>) Similar to (c) but for the running condition. <bold>e</bold>) Difference in population discrimination accuracy between the biased and the uniform environment averaged across recording sessions similar to <xref ref-type="fig" rid="F2">Fig. 2i</xref> but only for the stationary conditions; <bold>f</bold>) Similar to (e) but for the running condition.</p></caption><graphic xlink:href="EMS202897-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>A normative model reproduces the changes in discriminability and tuning observed in mouse V1.</title><p><bold>a</bold>) Training set of the model matching the distribution of stimuli in the data. <bold>b</bold>) Autoencoder trained to minimize a multi-objective function composed of a reconstruction error in the output layer and energy cost (L1-norm sparsity) in the hidden layer; <bold>c</bold>) PCA on responses in the hidden layer (compare with <xref ref-type="fig" rid="F2">Fig. 2b</xref> and 6l) for one typical model; note the decrease in responses (purple arrow) and increase in distances (green arrows) near the adaptor. <bold>d</bold>) Difference in average responses between biased and uniform environment computed as in <xref ref-type="supplementary-material" rid="SD1">Fig. S3a</xref> but for the model. <bold>e</bold>) Change in average responses between biased and uniform environments at the preferred orientation of each group of neurons, computed as in <xref ref-type="supplementary-material" rid="SD1">Fig. S3b</xref> but for the example model in (c). <bold>f</bold>) Difference in discrimination accuracy in an example recording between the biased and the uniform environment, computed similarly to <xref ref-type="fig" rid="F2">Fig. 2h</xref> but for the example model in (c). <bold>g</bold>) Change in discrimination accuracy between biased and uniform environments for stimuli 15 deg apart as a function of the distance of the stimuli from the adapter, computed similarly to <xref ref-type="fig" rid="F2">Fig. 2i</xref> but for the example model in (c).</p></caption><graphic xlink:href="EMS202897-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><title>Relationship between changes in responses and discriminability.</title><p><bold>a</bold>) Normalized population-average responses to different orientations. Each curve corresponds to the average responses of neurons tuned to a specific orientation in a hypothetical, biased environment where the adaptor decreases responses in neurons tuned to the adaptor at the adaptor location. <bold>b</bold>) Similar to (a) but the adaptor increases responses in neurons tuned to the adaptor at the adaptor location. <bold>c</bold>) Similar to (a) and (b) but the increase and decrease in responses reflects that observed in the data (see <xref ref-type="sec" rid="S9">Methods</xref>). <bold>d-f</bold>) hypothetical average firing rate of neurons if their responses were not adapted but the distribution of orientation was biased (black) and firing rate of neurons when their responses are adapted as in (a-c) consistently with a biased distribution of orientations (red). <bold>g-i</bold>) difference in discrimination accuracy (here, computed based on population discriminability between any pair of stimuli) between a homogenous population (representing biased condition) and the same population after applying the perturbation in tuning curves in (a-c). Black squares correspond to adapter orientation; <bold>j</bold>) Projected neural activity in PCA space before and after changes in (a) decreases responses (purple dashed arrow toward the center) and reduces distances and discrimination accuracy near the adaptor (purple inward arrows). <bold>k</bold>) Same as in (j), but changes in (b) are applied, which increase responses (green dashed arrow farther from the center) and enhance distances between stimuli near the adaptor (green outward arrows) as well as discrimination accuracy. <bold>l</bold>) Despite the response decrease (purple dashed line), the changes in (c) enhance distances and discrimination accuracy near the adaptor (green outward arrows). <bold>m-o</bold>) Angle prediction in the biased environments of (a-c) after training a model in a uniform environment. The angle is calculated from a linear regression of cos <italic>θ</italic> and sin <italic>θ</italic>, where <italic>θ</italic> is the stimulus orientation, followed by computing arctan(sin <italic>θ/</italic>cos <italic>θ</italic>).</p></caption><graphic xlink:href="EMS202897-f006"/></fig></floats-group></article>