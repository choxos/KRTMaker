<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS201767</article-id><article-id pub-id-type="doi">10.1101/2024.12.03.626542</article-id><article-id pub-id-type="archive">PPR951110</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">2</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Between-movie variability severely limits generalizability of “naturalistic” neuroimaging</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Leipold</surname><given-names>Simon</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Rao</surname><given-names>Rajat Ravi</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Schoffelen</surname><given-names>Jan-Mathijs</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Bögels</surname><given-names>Sara</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Toni</surname><given-names>Ivan</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib></contrib-group><aff id="A1"><label>1</label>Donders Institute for Brain, Cognition, and Behaviour, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/016xsfp80</institution-id><institution>Radboud University</institution></institution-wrap></aff><aff id="A2"><label>2</label>Social Brain Sciences Lab, Department of Humanities, Social and Political Sciences, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/05a28rw58</institution-id><institution>ETH Zürich</institution></institution-wrap></aff><aff id="A3"><label>3</label>Cognitive Psychology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/027bh9e22</institution-id><institution>Leiden University</institution></institution-wrap></aff><aff id="A4"><label>4</label>Centre for Language Studies, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/016xsfp80</institution-id><institution>Radboud University</institution></institution-wrap></aff><author-notes><corresp id="CR1">
<bold>Corresponding authors:</bold> Simon Leipold &amp; Ivan Toni, Radboud University, Donders Institute for Brain, Cognition, and Behaviour, Centre for Cognitive Neuroimaging, Kapittelweg 29, 6525 EN Nijmegen, The Netherlands, <email>simon.leipold@gess.ethz.ch</email>; <email>ivan.toni@donders.ru.nl</email>
</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>08</day><month>12</month><year>2024</year></pub-date><pub-date pub-type="preprint"><day>06</day><month>12</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">“Naturalistic” neuroimaging paradigms, where subjects watch movies during fMRI, have become increasingly popular. Movie fMRI data is often analyzed using inter-subject correlation (ISC), which measures the similarity of neural time series across individuals. Differences in ISC during movie-watching have been linked to individual traits and states. However, movies are complex cultural artifacts that differ in content, structure, and style, raising concerns about the generalizability of ISC findings across movies.</p><p id="P2">Using fMRI data from 112 subjects watching eight animated movies, we quantified between-movie variability in ISC and examined its implications for trait- or state-like effects. ISC varied substantially across movies and brain regions, with the strongest ISC observed in regions showing the greatest variability. Consequently, associations between behavior and ISC differed markedly across movies.</p><p id="P3">These findings suggest that movie-based paradigms should be treated like task-based designs, requiring clear specification of movie features. Broad claims about “movie watching” are not warranted.</p></abstract><kwd-group><kwd>inter-subject correlation (ISC)</kwd><kwd>neural synchronization</kwd><kwd>movie-watching fMRI</kwd><kwd>individual differences</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P4">In the last two decades, “naturalistic” paradigms have gained enormous popularity in neuroscience <sup><xref ref-type="bibr" rid="R1">1</xref>–<xref ref-type="bibr" rid="R6">6</xref></sup>. These paradigms include subjects viewing movies <sup><xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R8">8</xref></sup> or listening to narratives <sup><xref ref-type="bibr" rid="R9">9</xref>–<xref ref-type="bibr" rid="R11">11</xref></sup> or music <sup><xref ref-type="bibr" rid="R12">12</xref>,<xref ref-type="bibr" rid="R13">13</xref></sup> while undergoing functional brain imaging, most commonly functional magnetic resonance imaging (fMRI). Although the term “naturalistic” is used variably across subfields <sup><xref ref-type="bibr" rid="R14">14</xref></sup>, here we use it specifically to refer to the use of media materials such as movies or audiobooks in neuroimaging research. It has been argued that naturalistic paradigms more closely resemble everyday life experiences <sup><xref ref-type="bibr" rid="R5">5</xref>,<xref ref-type="bibr" rid="R15">15</xref></sup>, thereby addressing the limited “ecological validity” <sup><xref ref-type="bibr" rid="R16">16</xref>–<xref ref-type="bibr" rid="R18">18</xref></sup> of traditional, cognitively controlled neuroscience experiments. Specifically, the simple and arbitrarily ordered stimuli used in conventional experiments might restrict the generalizability of findings to everyday-life settings <sup><xref ref-type="bibr" rid="R15">15</xref></sup>. However, to date, the generalizability of findings from naturalistic paradigms has been largely overlooked, despite their reliance on culturally contingent and idiosyncratic artifacts such as movies and literary narratives, which are forms of media deliberately designed to serve specific purposes, such as entertaining or informing audiences <sup><xref ref-type="bibr" rid="R14">14</xref>,<xref ref-type="bibr" rid="R19">19</xref></sup>.</p><p id="P5">A popular method for analyzing fMRI data from naturalistic paradigms is inter-subject correlation (ISC) analysis <sup><xref ref-type="bibr" rid="R7">7</xref></sup>. In ISC analysis, the fMRI time series of a brain region (e.g., a single voxel or brain atlas parcel) in one subject is used as a model for the fMRI time series in the same region of another subject <sup><xref ref-type="bibr" rid="R20">20</xref></sup>: Calculating the correlation coefficient of the two time series yields a measure of synchronization of brain activation between the different subjects <sup><xref ref-type="bibr" rid="R21">21</xref></sup>. Summary statistics, such as the average correlation between all possible pairs of subjects, are calculated to express the level of synchronization across all subjects per brain region. These summary statistics themselves are commonly referred to as ISC.</p><p id="P6">Prior work has demonstrated that many naturalistic materials, particularly movies, evoke strong ISC across the cortex, with particularly strong synchronization in visual and auditory brain regions <sup><xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R21">21</xref></sup>. The spatial distribution of ISC across the brain is highly reliable when subjects view the same movie more than once <sup><xref ref-type="bibr" rid="R2">2</xref></sup>. An advantage of ISC over general linear model analysis is that it is completely data-driven with no need to build regressors of specific event types, such as events marking the appearance of faces during the movie. Crucially, ISC is <italic>stimulus-driven</italic>, as activation changes that are not consistently related to processing of the naturalistic material across subjects remain largely uncorrelated and do not contribute to the ISC <sup><xref ref-type="bibr" rid="R20">20</xref>,<xref ref-type="bibr" rid="R21">21</xref></sup>. However, it remains unclear whether different naturalistic materials, for example different movie clips, evoke consistent patterns of ISC.</p><p id="P7">In recent years, research employing naturalistic paradigms has increasingly focused on explaining state-like and trait-like differences in neural synchronization between subjects. In a typical study, a measure of neural synchronization, most commonly ISC, is assessed while subjects watch a movie or listen to a narrative. Neural synchronization is compared between (sub-)groups of subjects classified based on certain characteristics, such as psychological states, traits, or clinical diagnoses. For example, prior work suggests that subjects with similar psychological perspectives, such as watching a movie from the perspective of a “detective” versus an “interior decorator” <sup><xref ref-type="bibr" rid="R22">22</xref></sup>, or having similar beliefs about the characters in a story <sup><xref ref-type="bibr" rid="R23">23</xref></sup>, show higher ISC than subjects with different perspectives or beliefs. Groups of subjects that reach consensus about the interpretation of ambiguous movie clips show higher ISC than groups that do not reach consensus <sup><xref ref-type="bibr" rid="R24">24</xref></sup>. Friends show higher ISC than individuals who are further away from each other in a social network <sup><xref ref-type="bibr" rid="R25">25</xref></sup>. Sisters show higher ISC than friends <sup><xref ref-type="bibr" rid="R26">26</xref></sup>, and married couples show higher ISC than random couples <sup><xref ref-type="bibr" rid="R27">27</xref></sup>. In the clinical domain, alterations in ISC during movie watching have been reported in individuals with autism <sup><xref ref-type="bibr" rid="R28">28</xref>–<xref ref-type="bibr" rid="R30">30</xref></sup>, psychosis <sup><xref ref-type="bibr" rid="R31">31</xref></sup>, depression <sup><xref ref-type="bibr" rid="R32">32</xref></sup>, and social anxiety <sup><xref ref-type="bibr" rid="R33">33</xref></sup>, among others. This research line was recently extended within the framework of inter-subject representational similarity analysis (IS-RSA), which allows for a dimensional assessment of the relation between subjects’ neural synchronization patterns and subjects’ behavioral characteristics <sup><xref ref-type="bibr" rid="R34">34</xref>,<xref ref-type="bibr" rid="R35">35</xref></sup>. For example, subjects who are similar in age <sup><xref ref-type="bibr" rid="R36">36</xref>–<xref ref-type="bibr" rid="R38">38</xref></sup>, personality traits <sup><xref ref-type="bibr" rid="R39">39</xref>,<xref ref-type="bibr" rid="R40">40</xref></sup>, political orientation <sup><xref ref-type="bibr" rid="R41">41</xref>–<xref ref-type="bibr" rid="R44">44</xref></sup>, sexual preferences <sup><xref ref-type="bibr" rid="R45">45</xref></sup>, or reading ability <sup><xref ref-type="bibr" rid="R46">46</xref></sup> show higher neural synchronization than subjects who are dissimilar in these characteristics while watching a movie or listening to a narrative.</p><p id="P8">The majority of studies investigating trait- or state-like differences in ISC have drawn conclusions from the presentation of a single movie or narrative to the subjects. However, a few studies presenting more than one movie suggest that different movies might evoke different patterns of neural synchronization across the brain <sup><xref ref-type="bibr" rid="R24">24</xref>,<xref ref-type="bibr" rid="R27">27</xref>,<xref ref-type="bibr" rid="R33">33</xref>,<xref ref-type="bibr" rid="R44">44</xref></sup>. The same group of subjects might have higher synchronization than another group in brain region A for movie #1, higher neural synchronization in region B during movie #2, and no significant synchronization differences for movie #3.</p><p id="P9">By their very nature, movies are spatially and temporally complex, as well as highly multidimensional <sup><xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R47">47</xref></sup>. Different movies vary along multiple features. Considering that the main measure of neural synchronization, ISC, is strongly stimulus-driven <sup><xref ref-type="bibr" rid="R20">20</xref>,<xref ref-type="bibr" rid="R21">21</xref></sup>, different movies might lead to different trait- or state-like effects in ISC. Indeed, the extraordinary sensitivity of ISC to varying features of movies was already highlighted in the earliest studies using naturalistic paradigms <sup><xref ref-type="bibr" rid="R2">2</xref>,<xref ref-type="bibr" rid="R7">7</xref></sup>. Notably, it has been suggested that ISC could be a useful tool for directors or producers of movies to assess how engaging their movies are to an audience <sup><xref ref-type="bibr" rid="R48">48</xref></sup>, as feature films evoke much stronger ISC than filmed segments of reality <sup><xref ref-type="bibr" rid="R48">48</xref></sup> or educational movies <sup><xref ref-type="bibr" rid="R38">38</xref></sup>.</p><p id="P10">Surprisingly, this systematic variation in ISC between movies has been neglected when considering trait- and state-like ISC effects. More precisely, it is unclear how spatial patterns of ISC vary from movie to movie, a crucial element when considering the scope of findings based on a single movie <sup><xref ref-type="bibr" rid="R5">5</xref>,<xref ref-type="bibr" rid="R6">6</xref></sup>. How generalizable or “ecologically valid” are naturalistic paradigms in neuroscience, if trait- or state-like differences in ISC are bound to the movie being shown?</p><p id="P11">Here, we leverage a dataset of 112 adults watching eight movies each <sup><xref ref-type="bibr" rid="R49">49</xref></sup> to (i) quantify between-movie variability in ISC across the brain and (ii) assess the potential consequences of this variability in ISC for trait- or state-like differences in ISC (<xref ref-type="fig" rid="F1">Figure 1</xref>). Specifically, we assessed how between-movie variability affects association between a cognitive characteristic, similarity in semantic judgments of novel objects, and a neural synchronization characteristic, measured using ISC.</p><p id="P12">We find substantial between-movie variability in ISC across the whole brain, with the extent of variability differing strongly across brain regions: regions with higher levels of ISC showed higher variability. Crucially, applying IS-RSA for each movie separately revealed strongly varying associations of subjects’ similarity in semantic judgments with neural synchronization. The localization of effects was different for each movie, which puts in doubt the generalizability of findings obtained from a single or a few movies.</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>Between-movie variability in ISC is substantial</title><p id="P13">Our first aim was to quantify the variability in ISC evoked by different movies across the brain. We analyzed fMRI data from 112 adults (76 females; mean age = 22.72) from a published dataset <sup><xref ref-type="bibr" rid="R49">49</xref></sup>. The data was acquired while subjects watched eight animated movies (detailed in <xref ref-type="supplementary-material" rid="SD1">Supplementary Table 1</xref>). Animated movies are an ideal limiting case for assessing movie-related variability in ISC, as they are stylistically and thematically similar. For each subject and movie, we extracted a preprocessed and spatially averaged fMRI time series from each of the 210 cortical parcels of the Brainnetome atlas <sup><xref ref-type="bibr" rid="R50">50</xref></sup>. For each parcel, we calculated the ISC between all subject pairs by computing the Pearson correlation coefficient between the extracted fMRI time series. Subsequently, we averaged the pairwise correlation coefficients to obtain a mean ISC value per parcel.</p><p id="P14">As shown in <xref ref-type="fig" rid="F2">Figure 2</xref>, the overall spatial distribution of ISC across the brain evoked by each of the eight movies resembled the spatial pattern found in numerous studies using movies <sup><xref ref-type="bibr" rid="R2">2</xref>,<xref ref-type="bibr" rid="R6">6</xref>,<xref ref-type="bibr" rid="R7">7</xref></sup>. The strongest ISC was evoked in the extended visual system, including face-sensitive areas in the fusiform gyrus, and the auditory system, including Heschl’s gyrus and the planum temporale on the superior temporal plane. Additionally, there was strong ISC in higher-order areas in the temporal, parietal, and frontal cortices. The ISC values for each parcel and movie are available online (<ext-link ext-link-type="uri" xlink:href="https://dx.doi.org/10.17605/OSF.IO/S78WU">https://dx.doi.org/10.17605/OSF.IO/S78WU</ext-link>). At first glance, the overall ISC patterns appeared qualitatively similar across movies. In prior work, such coarse similarities have been interpreted as evidence that different naturalistic materials evoke broadly comparable synchronization across the brain <sup><xref ref-type="bibr" rid="R51">51</xref></sup>. However, closer visual inspection of the ISC maps in <xref ref-type="fig" rid="F2">Figure 2</xref> suggests that different movies elicited distinct neural synchronization patterns. For example, Movie #3 appeared to elicit generally higher ISC values than Movie #5.</p><p id="P15">Building on these qualitative observations, we next quantified the variability in ISC between movies through statistical analysis. First, we investigated if the extent of whole-brain ISC varies between movies. To do this, we averaged the ISC values across all cortical parcels to obtain one whole-brain ISC value per subject pair and movie. This pair-wise approach is one of the standard procedures when computing ISC <sup><xref ref-type="bibr" rid="R20">20</xref></sup>. Then, we submitted whole-brain ISC values to a repeated-measures analysis of variance (ANOVA) with Movie as the within-subject factor. To meet the assumptions of the ANOVA, we used the values from a subset of subject pairs, ensuring each subject uniquely contributed to a single subject pair. The results of the ANOVA revealed a significant main effect of Movie (<italic>F</italic>(7,385) = 4.93, <italic>p</italic> &lt; 0.001, generalized eta-squared [η<sup>2</sup><sub>G</sub>] = 0.051), indicating considerable variability in whole-brain ISC across movies. <xref ref-type="fig" rid="F3">Figure 3A</xref> visualizes the means and standard errors of the whole-brain ISC values for each movie separately. Notably, the visualization shows that the differences in ISC values between movies are not driven by one particular movie being different from all the others; rather, ISC values consistently differ across all movies.</p><p id="P16">Next, we investigated the spatial distribution of ISC variability by quantifying the between-movie variability in ISC values across all parcels of the cortex. We conducted a repeated-measures ANOVA with Movie as the within-subject factor for each of the 210 cortical parcels defined by the Brainnetome atlas. We found a statistically significant main effect of Movie (family-wise error (FWE)-adjusted <italic>p</italic> &lt; 0.05) for 97 of the 210 cortical parcels (46.2%). The strongest variability was found in the primary and associative visual cortex, as well as the primary and associative auditory cortex. However, there was also substantial variability in higher-order areas, such as the precuneus and inferior parietal lobule. The statistical values quantifying variability for each parcel are available online (<ext-link ext-link-type="uri" xlink:href="https://dx.doi.org/10.17605/OSF.IO/S78WU">https://dx.doi.org/10.17605/OSF.IO/S78WU</ext-link>). <xref ref-type="fig" rid="F3">Figure 3B</xref> shows the spatial distribution of between-movie ISC variability across the brain.</p><p id="P17">To further test whether ISC patterns carry movie-specific information, we ran a cross-validated multivariate classification analysis to distinguish the movies based on parcel-wise ISC patterns, achieving an accuracy of 62.9% (chance level = 12.5%; <italic>p</italic> = 0.001, permutation test). This demonstrates that the spatial distribution of ISC values contains sufficient information to robustly differentiate between movies. The analysis is described in detail in the <xref ref-type="supplementary-material" rid="SD1">Supplementary Results</xref> and illustrated in <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 4</xref>.</p></sec><sec id="S4"><title>Brain regions with stronger ISC show higher between-movie variability in ISC</title><p id="P18">We followed up on the finding of widespread and substantial between-movie variability in ISC across the cortex by inspecting brain regions that exhibited particularly large and small variability. We discovered that brain regions showing high variability, such as the superior temporal cortex, also had relatively higher levels of ISC. Conversely, brain regions with low variability, such as the parahippocampal gyrus, had lower levels of ISC (<xref ref-type="fig" rid="F3">Figure 3C</xref>). To quantify this relationship, we calculated the Pearson correlation coefficient between the level of ISC and between-movie variability of ISC across the 210 brain regions. As shown in <xref ref-type="fig" rid="F3">Figure 3D</xref>, ISC levels and between-movie variability were strongly positively correlated (<italic>r</italic> = 0.73, <italic>t</italic>(208) = 15.62, <italic>p</italic> &lt; 0.001, <italic>R</italic><sup>2</sup> = 0.54). While this relationship is not surprising from a statistical standpoint, since regions with higher ISC allow for more variability, it is still important to highlight. It indicates that brain regions with stronger ISC also show greater differences in ISC between movies. In other words, the regions that most consistently synchronize across individuals are also the ones most affected by between-movie variability.</p></sec><sec id="S5"><title>Between-movie variability in ISC affects generalizability of trait- or state-like differences in ISC</title><p id="P19">Our next objective was to assess consequences of between-movie variability in ISC for the generalizability of trait- or state-like differences in ISC. Our dataset encompasses behavioral data on subjects’ conceptual-semantic representations of novel objects (“Fribbles”, <xref ref-type="fig" rid="F1">Figure 1C</xref>), which were estimated in a <italic>Features task</italic> <sup><xref ref-type="bibr" rid="R49">49</xref>,<xref ref-type="bibr" rid="R52">52</xref></sup>. In this task, subjects used a linear visual analog scale to report how well each novel object matched 29 different features (e.g., pointy, symmetrical, human, related to movement; full list in <xref ref-type="supplementary-material" rid="SD1">Supplementary Table 2</xref>; see <xref ref-type="supplementary-material" rid="SD1">Supplementary Methods</xref> for details on the Features task). Prior work has shown that subjects with similar interpretations of a narrative show higher neural synchronization than subjects with different interpretations <sup><xref ref-type="bibr" rid="R23">23</xref></sup>. Furthermore, subjects who share their interpretations of an animation of abstract geometric shapes show high neural synchronization during narrative listening in the default-mode network <sup><xref ref-type="bibr" rid="R53">53</xref></sup>. Inspired by these previous studies, we hypothesized that subjects with similar conceptual representations of novel objects would show higher ISC during movie watching. As these particular movies were selected to contain categories of objects that were mentioned often in pilot tests to be associated with the novel objects <sup><xref ref-type="bibr" rid="R49">49</xref></sup>, we anticipated a “spill-over” effect from representations of novel objects to naturalistic viewing of conventional objects.</p><p id="P20">We used IS-RSA to discover brain regions in which similarity in conceptual representations of novel objects (as measured by the Features task) across pairs of subjects are associated with similarity in ISC during movie watching. In other words, we looked for brain regions that synchronize more between subjects that have similar semantic representations of the novel objects. For each movie separately, we performed regression-based IS-RSA using linear mixed-effects models <sup><xref ref-type="bibr" rid="R54">54</xref></sup> to estimate the association between subject similarity in semantic representations and ISC values during movie watching, while controlling for subjects’ similarities in the names they gave the novel objects (<italic>Naming task</italic>) and similarities in age and sex. Linear mixed-effects models are commonly used for quantifying trait- or state-like differences in ISC <sup><xref ref-type="bibr" rid="R37">37</xref>,<xref ref-type="bibr" rid="R39">39</xref>,<xref ref-type="bibr" rid="R41">41</xref>,<xref ref-type="bibr" rid="R44">44</xref>,<xref ref-type="bibr" rid="R46">46</xref>,<xref ref-type="bibr" rid="R55">55</xref>,<xref ref-type="bibr" rid="R56">56</xref></sup>. If the effects of between-movie variability in ISC are negligible, we would expect a similar pattern of IS-RSA results across movies. By contrast, we would expect a distinct pattern of IS-RSA results for each movie if between-movie variability in ISC is consequential for assessing trait- or state-like differences in ISC.</p><p id="P21">As shown in <xref ref-type="fig" rid="F4">Figure 4</xref>, the results of the IS-RSA revealed associations between semantic representations of novel objects and ISC in distinct brain regions for each movie. For Movie #1, we found that similarities between subjects in semantic representations were statistically significantly associated with neural synchronization between subjects in parcels within bilateral insula (right hemisphere: <italic>p</italic><sub>FWE</sub> = 0.004 and left hemisphere: <italic>p</italic><sub>FWE</sub> = 0.007), a parcel in the left inferior parietal lobule (<italic>p</italic><sub>FWE</sub> = 0.006), the left precentral gyrus (<italic>p</italic><sub>FWE</sub> = 0.007), the right superior frontal gyrus (<italic>p</italic><sub>FWE</sub> = 0.02), the left inferior temporal gyrus (<italic>p</italic><sub>FWE</sub> = 0.03), and the right anterior cingulate (<italic>p</italic><sub>FWE</sub> = 0.03). In contrast, for Movie #2, we found no statistically significant association between semantic representations and neural synchronization between subjects. For Movie #3, we found significant associations of similarities in semantic representations with ISC in the left precuneus (<italic>p</italic><sub>FWE</sub> &lt; 0.05), which did not overlap with any of the regions that were found for Movie #1. For Movie #4, we found significant associations in bilateral middle frontal gyrus (right hemisphere: <italic>p</italic><sub>FWE</sub> &lt; 0.001 and left hemisphere: <italic>p</italic><sub>FWE</sub> = 0.03) and the right orbital gyrus (<italic>p</italic><sub>FWE</sub> = 0.02), which again did not overlap with regions found in Movie #1 or #3. Likewise, for Movie #5 through Movie #8, we found non-overlapping brain regions that showed an association between semantic representations and neural synchronization except for a single parcel in the right middle temporal gyrus, which showed an association both for Movie #5 (<italic>p</italic><sub>FWE</sub> = 0.004) and Movie #6 (<italic>p</italic><sub>FWE</sub> &lt; 0.001). Details on the statistically significant IS-RSA associations for all movies are listed in <xref ref-type="supplementary-material" rid="SD1">Supplementary Table 3</xref>. Details on all parcels and movies are available online (<ext-link ext-link-type="uri" xlink:href="https://dx.doi.org/10.17605/OSF.IO/S78WU">https://dx.doi.org/10.17605/OSF.IO/S78WU</ext-link>). Thus, between-movie variability in ISC has detrimental downstream effects for generalizability of findings with one movie as these effects likely do not generalize to other movies.</p></sec><sec id="S6"><title>A subset of brain regions show IS-RSA effects that generalize across movies</title><p id="P22">To examine whether any associations generalized across movies, we conducted an additional IS-RSA that included a random effect for movie. This analysis identified three brain regions, including bilateral posterior inferior temporal gyrus and right posterior superior temporal sulcus, where ISC was significantly associated with semantic representations of novel objects across movies (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Results</xref>).</p></sec></sec><sec id="S7" sec-type="discussion"><title>Discussion</title><p id="P23">Here, we quantified the extent of variability in ISC between different movies, and we assessed the consequences of this variability for investigations of trait-like or state-like differences in neural synchronization during movie watching. Our analyses revealed substantial between-movie variability in whole-brain ISC and statistically significant variability in almost half of all cortical regions. Between-movie variability in ISC affected not only early auditory and visual brain regions but also higher-order brain regions, including those belonging to the default-mode network. Using IS-RSA, we showed that associations between subjects’ similarity in semantic-conceptual representations of novel objects and ISC during movie watching have a distinct spatial distribution in the brain for each of the movies, with minimal overlap between movies. Thus, findings of trait-like and state-like differences in ISC based on a single movie are not generalizable to different movies.</p><sec id="S8"><title>What features of a movie drive neural synchronization?</title><p id="P24">A major issue in using naturalistic material to investigate trait-like and state-like differences in neural synchronization is that it is unknown which features of a movie might drive the neural synchronization. As resting-state fMRI time series between different subjects are generally uncorrelated <sup><xref ref-type="bibr" rid="R7">7</xref></sup>, there must be features of the naturalistic material that drive the neural synchronization. However, ever since the first studies used free viewing of movies as a paradigm in neuroscience <sup><xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R47">47</xref></sup>, it has been clear that movies are a highly complex and multidimensional sequence of stimuli, making it challenging to isolate specific features that drive neural synchronization <sup><xref ref-type="bibr" rid="R2">2</xref>,<xref ref-type="bibr" rid="R7">7</xref></sup>. This is further complicated by the fact that many features in movies are correlated, making it difficult to isolate their individual contributions <sup><xref ref-type="bibr" rid="R57">57</xref></sup>. Thus, we argue that even when movies are grouped by an experimenter-selected feature, for example “child-oriented animations”, different movie clips within this specific “category” will still vary in a myriad of features that might differentially drive brain responses and lead to different neural synchronization between subjects.</p></sec><sec id="S9"><title>Including the stimulus as a random effect in the statistical model</title><p id="P25">A potential solution to the limited generalizability of findings based on a single or small set of movies is to include movie as a “random effect” in the statistical model. Following Yarkoni <sup><xref ref-type="bibr" rid="R58">58</xref></sup>, “random effects are used to model […] variables that are assumed to be stochastically sampled from some underlying population and can vary across replications without meaningfully altering the research question.” <sup>58 p. 3</sup>. With rare exceptions <sup><xref ref-type="bibr" rid="R44">44</xref></sup>, studies investigating trait- or state-like differences in ISC that include multiple movie clips have either modeled the stimulus as a fixed effect <sup><xref ref-type="bibr" rid="R24">24</xref></sup> or run separate models per movie, effectively treating the stimulus as fixed. This limits the ability to generalize beyond the specific stimulus set used, a problem long recognized in neuroscience <sup><xref ref-type="bibr" rid="R59">59</xref></sup> and other fields <sup><xref ref-type="bibr" rid="R58">58</xref>,<xref ref-type="bibr" rid="R60">60</xref>,<xref ref-type="bibr" rid="R61">61</xref></sup>. It is therefore possible that the phenomenon we describe here reflects a specific instance of the “stimulus-as-fixed-effect fallacy” which, in principle, can be addressed by including the movie as a random effect in the statistical model. To explore this approach, we tested an IS-RSA model that included movie as a random effect. This analysis revealed consistent associations in a small number of brain regions, but most effects remained movie specific.</p><p id="P26">These findings raise a deeper issue: what exactly are we generalizing across? Can we reasonably assume that all movies sample from the same underlying population, given their substantial variability in structure, content, and style? Do different movies address the same research question, as the random-effects framework assumes, or do they address distinct ones? The central challenge persists: even when between-movie variability is statistically modeled, we lack a principled understanding of which features within these complex, multidimensional sequences of stimuli give rise to ISC and IS-RSA effects. Without identifying and quantifying the relevant features, generalization remains difficult.</p></sec><sec id="S10"><title>Movies are more akin to tasks than to rest</title><p id="P27">Naturalistic paradigms have been proposed as an alternative to resting-state paradigms <sup><xref ref-type="bibr" rid="R5">5</xref></sup>, and movie watching addresses several limitations of resting-state acquisitions. For example, ISC during movie viewing shows higher test-retest reliability when the same movie is watched multiple times <sup><xref ref-type="bibr" rid="R62">62</xref></sup>. Movies are also well-suited for pediatric populations <sup><xref ref-type="bibr" rid="R6">6</xref></sup>, as they enhance attentional engagement and reduce head motion compared to resting-state scans <sup><xref ref-type="bibr" rid="R36">36</xref>,<xref ref-type="bibr" rid="R63">63</xref></sup>.</p><p id="P28">However, our findings suggest that different movies elicit distinct neural synchronization patterns across individuals, making them more comparable to separate tasks than to resting-state conditions <sup><xref ref-type="bibr" rid="R14">14</xref></sup>. Consequently, studies using movie paradigms should document stimulus properties with the same level of rigor applied in task-based research, even though the relevant features may be difficult to isolate <sup><xref ref-type="bibr" rid="R6">6</xref></sup>. Researchers should explicitly specify which perceptual, cognitive, or affective processes a given movie is intended to engage. Machine learning-based annotation tools, such as those implemented in <italic>pliers</italic> <sup><xref ref-type="bibr" rid="R64">64</xref></sup> or <italic>NeuroScout</italic> <sup><xref ref-type="bibr" rid="R65">65</xref></sup>, can help address this by automatically extracting auditory, visual, linguistic, or affective features from naturalistic materials. These tools provide a data-driven foundation for identifying features that may contribute to ISC and its variability across movies, offering a promising path toward greater interpretability and generalizability of ISC-based findings. Recent work also highlights the potential of combining stimulus-based annotations with multi-dimensional experience sampling, which captures subjects’ ongoing thought patterns without disrupting the movie-watching experience <sup><xref ref-type="bibr" rid="R66">66</xref></sup>. Combining such subjective reports with automated feature annotations could clarify how both stimulus features and internal cognitive states shape ISC.</p><p id="P29">By contrast, when the goal is to evoke states closer to rest, researchers may turn to specially designed audiovisual materials such as <italic>Inscapes</italic>, which have been shown to elicit more rest-like brain activation patterns than conventional movie clips <sup><xref ref-type="bibr" rid="R63">63</xref></sup>.</p></sec><sec id="S11"><title>Movies are not a naturalistic depiction of reality</title><p id="P30">It has been argued that the rich and complex stimulation provided by movies more closely resembles everyday-life perception than the custom-designed stimuli typically used in laboratory settings <sup><xref ref-type="bibr" rid="R7">7</xref></sup>. In a seminal study where subjects watched a James Bond movie, the paradigm was described as observing a “dynamic natural scene” <sup><xref ref-type="bibr" rid="R47">47</xref></sup>. However, a Hollywood-produced action movie is far from natural. At best, movies are abstracted representations of daily-life experiences. They contain elements deliberately manipulated by filmmakers to capture and maintain the audience’s attention, such as camera movements, zooms, editing techniques like cuts, and soundtracks. Indeed, researchers previously noted that critically acclaimed movies evoke much higher ISC than actual natural scenes <sup><xref ref-type="bibr" rid="R48">48</xref>,<xref ref-type="bibr" rid="R67">67</xref></sup> or educational films <sup><xref ref-type="bibr" rid="R38">38</xref></sup>. While naturalistic paradigms may appear more “ecologically valid” than traditional controlled experiments, it remains unclear to what extent ISC during movie watching is driven by stylistic, artificial features such as cuts and zooms <sup><xref ref-type="bibr" rid="R68">68</xref>–<xref ref-type="bibr" rid="R70">70</xref></sup>. Variation in these features across films is directly relevant to the issue of between-movie variability, as they may contribute to differences in neural synchronization.</p></sec><sec id="S12"><title>Rethinking the use of “naturalistic” in movie fMRI research</title><p id="P31">The term “naturalistic” is widely used in neuroscience to describe paradigms intended to approximate everyday-life experiences <sup><xref ref-type="bibr" rid="R5">5</xref></sup>. However, the term is highly ambiguous <sup><xref ref-type="bibr" rid="R14">14</xref></sup>. In vision science, “naturalistic” might refer to static images of everyday-life objects or scenes <sup><xref ref-type="bibr" rid="R71">71</xref>–<xref ref-type="bibr" rid="R73">73</xref></sup>, while in social neuroscience it may describe mobile neuroimaging conducted in everyday-life settings<sup><xref ref-type="bibr" rid="R74">74</xref>,<xref ref-type="bibr" rid="R75">75</xref></sup>. In the context of fMRI, the term is often applied to studies using movies or narratives because such media are part of everyday life for many individuals. Yet, as others have argued <sup><xref ref-type="bibr" rid="R14">14</xref>,<xref ref-type="bibr" rid="R19">19</xref></sup>, movies are deliberately constructed artifacts, designed to entertain, inform, or persuade. Labeling them as “naturalistic” risks obscuring their inherently artificial, highly structured, and culturally specific nature. Rather than relying on this broad and inconsistently applied term, we suggest that researchers describe movie stimuli in terms of their specific properties, such as being dynamic, high-dimensional, and temporally extended, and explicitly articulate how these properties relate to the perceptual, cognitive, or behavioral phenomena under investigation <sup>cf. 76</sup>.</p></sec><sec id="S13"><title>Constraints on generality, limitations, and future directions</title><p id="P32">It remains an open question to what extent our findings generalize across different uses of naturalistic materials in neuroscience. For example, studies employing control conditions such as scrambled movies <sup><xref ref-type="bibr" rid="R8">8</xref>,<xref ref-type="bibr" rid="R9">9</xref></sup> or scrambled music <sup><xref ref-type="bibr" rid="R12">12</xref></sup> may be less affected by the stimulus-specificity in ISC observed here. The rationale behind scrambled conditions is that many (though likely not all) features of naturalistic materials that drive variability in ISC are held constant between the intact and scrambled versions, isolating only those of interest to the researcher. However, if this assumption is not fully met, for example because it is difficult to match all features that variably drive ISC between the real and control conditions, stimulus-specificity could still pose a problem. Our findings may also be less consequential for studies focusing on specific types of events embedded within naturalistic materials, such as those investigating memory or event segmentation <sup><xref ref-type="bibr" rid="R77">77</xref>–<xref ref-type="bibr" rid="R79">79</xref></sup>, many of which use the general linear model to examine brain responses to specific event types <sup><xref ref-type="bibr" rid="R47">47</xref>,<xref ref-type="bibr" rid="R80">80</xref>–<xref ref-type="bibr" rid="R82">82</xref></sup>. In such cases, variability between entire movies may have minimal impact on the targeted event-level effects. Our findings are likely not relevant for studies using voxel-wise encoding models to map neural responses to annotated stimulus features, as these models are typically evaluated on independent datasets, providing a direct assessment of generalizability across different stimuli <sup><xref ref-type="bibr" rid="R83">83</xref>,<xref ref-type="bibr" rid="R84">84</xref></sup>.</p><p id="P33">Within-subject functional connectivity measures may be more consistent across different movies than ISC-based measures <sup><xref ref-type="bibr" rid="R85">85</xref></sup>. This relative stability is likely because functional connectivity is largely driven by intrinsic network architecture <sup><xref ref-type="bibr" rid="R86">86</xref></sup>, which is commonly measured during rest and shows only modest modulation by movies <sup><xref ref-type="bibr" rid="R63">63</xref></sup> or task conditions <sup><xref ref-type="bibr" rid="R87">87</xref>,<xref ref-type="bibr" rid="R88">88</xref></sup>. However, movie-specific differences in functional connectivity may still be meaningful in certain contexts <sup><xref ref-type="bibr" rid="R89">89</xref>,<xref ref-type="bibr" rid="R90">90</xref></sup>, such as when using functional connectivity patterns to predict individual phenotypes <sup><xref ref-type="bibr" rid="R91">91</xref></sup>.</p><p id="P34">More broadly, the adverse effects of between-stimulus variability on the generalizability of trait- and state-like differences in ISC are unlikely to be limited to movie fMRI. They may extend to other classes of naturalistic materials, such as narratives or music; to analytical approaches such as inter-subject functional correlation <sup>cf. 92</sup> and ISC-based predictive modeling of individual phenotypes <sup>cf. 67</sup>; and to neuroimaging modalities beyond fMRI, including magnetoencephalography (MEG), electroencephalography (EEG) <sup>cf. 38</sup>, or functional near-infrared spectroscopy (fNIRS). Most of these possibilities remain to be tested and require systematic empirical investigation in future research.</p><p id="P35">A limitation of the current study is that the dataset was not designed to test whether more natural stimuli, such as unedited natural scenes, evoke less variable ISC than feature films. Prior work suggests that natural scenes elicit substantially lower ISC than Hollywood movies <sup><xref ref-type="bibr" rid="R48">48</xref>,<xref ref-type="bibr" rid="R67">67</xref></sup>. In our data, brain regions with lower ISC also show less between-movie variability (<xref ref-type="fig" rid="F2">Figure 2D</xref>), raising the possibility that more natural scenes may yield reduced ISC variability. This remains an important question for future research to address.</p><p id="P36">A further limitation is that our study did not include direct measures or manipulations of subjects’ psychological states during movie viewing. While the semantic representations assessed here may contain state-like components <sup><xref ref-type="bibr" rid="R49">49</xref></sup>, we cannot directly assess transient state-like effects or their relationship with ISC. Future work should aim to manipulate such states more directly and test whether their associations with ISC generalize across different movies.</p><p id="P37">Future work could also compare variability in responses to movies or narratives with variability across cognitive tasks that target similar underlying processes <sup><xref ref-type="bibr" rid="R93">93</xref>–<xref ref-type="bibr" rid="R95">95</xref></sup>. Such comparisons may help contextualize movie-induced variability relative to more traditional task-based paradigms. Making such a comparison meaningful would require identifying the specific perceptual or cognitive processes that a given set of naturalistic materials is likely to engage. More generally, if the goal is to identify common neural patterns that emerge across different movies during free viewing, data-driven approaches such as factor analysis may be useful for isolating components that reflect shared structure beyond movie-specific effects.</p></sec><sec id="S14" sec-type="conclusions"><title>Conclusion</title><p id="P38">We find substantial between-movie variability in ISC across the cortex, leading to distinct spatial distributions of trait- or state-like differences in the brain for separate movies, with minimal overlap between movies. Consequently, findings of trait-like or state-like differences in ISC based on a single movie are not generalizable across different movies, raising questions about findings from previous studies that relied on a single or limited set of movies.</p></sec></sec><sec id="S15" sec-type="methods"><title>Methods</title><sec id="S16" sec-type="subjects"><title>Subjects</title><p id="P39">We analyzed data from <italic>N</italic> = 112 subjects from the Communicative Alignment of Brain and Behaviour (CABB) dataset <sup><xref ref-type="bibr" rid="R49">49</xref></sup>. These subjects were the same as those included in the fMRI analysis in the dataset description paper by Eijk et al. <sup><xref ref-type="bibr" rid="R49">49</xref></sup> and did not include subjects who were excluded due to technical malfunctions, incomplete data, or poor data quality. For our main analysis, a repeated-measures ANOVA on ISC values of 56 subject pairs, we had 80% statistical power to detect differences in ISC between movies with an effect size of at least Cohen’s <italic>f</italic> &gt; 0.51. Statistical power was calculated using the <italic>WebPower</italic> package (version 0.9.2) <sup><xref ref-type="bibr" rid="R96">96</xref></sup> in <italic>R</italic> (version 4.1.0; RRID:SCR_001905).</p></sec><sec id="S17"><title>fMRI movie task</title><p id="P40">Subjects viewed eight animated movies (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Table 1</xref> for details), which were presented using Presentation software (version 20.2; RRID:SCR_002521). The movies were selected to include categories of objects frequently mentioned in pilot tests of the novel objects (“Fribbles”, <xref ref-type="fig" rid="F1">Figure 1C</xref>), such as humans, plants, tools, toys, and food. The movies were presented on a portion of the screen slightly below the center, on a black background with a height of 360 pixels and a width of 640 pixels, with a visual angle of approximately 9–11° vertically and 16–20° horizontally. The movies were played at 30 frames per second. Each movie was played in its entirety, except for the start and end (i.e., titles and credits), which were cut off to ensure no text was shown to subjects. The average duration of the movies was 4.1 minutes (range = 2.2-6.1 minutes), totaling 35 minutes, including 12-second breaks between subsequent movies. The movies were preceded by a filler video clip that lasted a few seconds. Subjects were instructed to simply attend to the movies and to lie still. The movies did not contain spoken language, but the original soundtrack was provided to subjects via earphones. Before scanning began, the sound levels were adjusted to each subject’s individual preferences.</p></sec><sec id="S18"><title>Imaging data acquisition</title><p id="P41">Magnetic resonance images were acquired using two 3T MAGNETOM magnetic resonance imaging scanners: Prisma and PrismaFit (Siemens AG, Healthcare Sector, Erlangen, Germany). Blood oxygenation level dependent (BOLD) functional images were acquired using a multi-band 2D-echo planar imaging (EPI) sequence (Human Connectome Project) <sup><xref ref-type="bibr" rid="R97">97</xref></sup> with repetition time (TR) = 1000 ms, echo time (TE) = 34 ms, flip angle = 60º, 2mm isotropic resolution, field of view (FOV) = 208x208x132 mm. A multi-band acceleration factor of 6 was used in the slice direction, and no parallel imaging was applied in-plane. Phase encoding was applied in the anterior-posterior (AP) direction with a partial Fourier coverage of 7/8. About 2074 volumes were acquired during movie watching. Anatomical images were acquired with a T1-weighted 3D MPRAGE sequence, integrated parallel acquisition technique (iPAT) acceleration factor = 2, with TR = 2200 ms, TE = 2 ms, inversion time (TI) = 1100 ms, flip angle = 11°, 0.8 mm isotropic resolution, FOV = 256x240x166 mm; and with a T2-weighted turbo spin echo (TSE) sequence with variable flip angle, TR = 3200 ms, TE = 569 ms, echo spacing = 4.12 ms, turbo factor = 314, 0.8 mm isotropic resolution, and FOV = 256x240x166 mm. Raw imaging data were converted to Brain Imaging Data Structure (BIDS) format <sup><xref ref-type="bibr" rid="R98">98</xref></sup> using <italic>BIDScoin</italic> (version 1.5; RRID:SCR_022839) <sup><xref ref-type="bibr" rid="R99">99</xref></sup>.</p></sec><sec id="S19"><title>Imaging data preprocessing</title><p id="P42">Each subject’s raw imaging data underwent a quality control assessment for excessive motion, susceptibility artifacts, or other sources of noise using <italic>MRIQC</italic> (version 23.1.0; RRID:SCR_022942). Imaging data preprocessing was implemented using <italic>fMRIPrep</italic> (version 20.2.7; RRID:SCR_016216) <sup><xref ref-type="bibr" rid="R100">100</xref></sup>, which is based on <italic>Nipype</italic> (version 1.7.0; RRID:SCR_002502)<sup><xref ref-type="bibr" rid="R101">101</xref></sup>. The main preprocessing steps included segmentation, surface reconstruction, and spatial normalization of the T1-weighted anatomical image; and slice timing correction, susceptibility distortion correction, head-motion correction, and spatial normalization of the fMRI data. Details of the preprocessing pipeline are included in the <xref ref-type="supplementary-material" rid="SD1">Supplementary Methods</xref>.</p></sec><sec id="S20"><title>fMRI data denoising</title><p id="P43">Each subject’s preprocessed fMRI data underwent further denoising, implemented using <italic>Nilearn</italic> (version 0.9.2; RRID:SCR_001362) in <italic>Python</italic> (version 3.7.13; RRID:SCR_008394). The voxel-wise fMRI time series were detrended and low-pass filtered at 0.08 Hz. Next, the following confounds estimated by fMRIprep during preprocessing were regressed out from the voxel-wise time series: six motion parameters (three translations, three rotations) and their first temporal derivatives, as well as cerebrospinal fluid and white matter time series and their first temporal derivatives. In addition, scans that exceeded a threshold of 0.5 mm framewise displacement (FD) or 1.5 standardized DVARS were annotated as motion outliers and included as nuisance regressors. Subsequently, the time series were standardized to have a mean of zero and unit variance. After denoising, we segmented the fMRI time series and extracted a separate time series for each movie clip based on the onset and offset times recorded in the Presentation log files. The denoised fMRI data for each movie was then spatially smoothed with a 6 mm full-width-at-half-maximum three-dimensional Gaussian kernel.</p></sec><sec id="S21"><title>Inter-subject correlation (ISC) analysis</title><p id="P44">To calculate parcel-wise ISC values, we first extracted a spatially averaged parcel-wise time series for each subject and movie from each of the 210 cortical parcels defined by the Brainnetome atlas <sup><xref ref-type="bibr" rid="R50">50</xref></sup> using <italic>NLtools</italic> (version 0.4.7) <sup><xref ref-type="bibr" rid="R102">102</xref></sup> in <italic>Python</italic>. For each movie and cortical parcel, we then calculated the ISC values for all possible subject pairs by computing the Pearson correlation coefficient between the parcel-wise fMRI time series of subject [i] and subject [j] of each pair [i,j] using <italic>scikit-learn</italic> (version 1.0.2; RRID:SCR_002577) in <italic>Python</italic>. To obtain a mean ISC value for each parcel, we averaged the ISC values across all subject pairs. Visualization of the ISC values in the brain was implemented using <italic>Nilearn</italic>.</p></sec><sec id="S22"><title>Statistical analyses of ISC values</title><p id="P45">Prior to statistical analysis, all ISC values were transformed using Fisher’s r-to-z transformation to normalize the distribution of correlation coefficients. To quantify between-movie variability in ISC across the whole brain, we averaged the ISC values across all cortical parcels to obtain a single ISC value per subject pair and movie. Then, we entered the whole-brain ISC values into a repeated-measures ANOVA with Movie as the within-subject factor. We used data from 56 subject pairs, ensuring that each subject contributed only to a single ISC value per movie and that the assumptions of the repeated-measures ANOVA were not violated. Next, we repeated the ANOVA for each parcel separately to obtain a spatial distribution of between-movie variability across the brain. The ANOVAs were performed using the <italic>ez</italic> (version 4.4-0; RRID:SCR_020990) and <italic>rstatix</italic> (version 0.7.2; RRID:SCR_021240) packages in <italic>R</italic>.</p><p id="P46">To investigate whether brain regions with high levels of ISC also show high variability in ISC, we correlated the parcel-wise level of ISC with the parcel-wise between-movie variability in ISC. Specifically, we extracted the <italic>F</italic> values from the repeated-measures ANOVAs for each parcel as a measure of between-movie variability. We then calculated the Pearson correlation coefficient between the F values and the ISC values for each parcel in <italic>R</italic>.</p></sec><sec id="S23"><title>Inter-subject representational similarity analysis (IS-RSA)</title><p id="P47">We conducted a regression-based IS-RSA <sup><xref ref-type="bibr" rid="R34">34</xref>,<xref ref-type="bibr" rid="R35">35</xref></sup> using linear mixed-effects models <sup><xref ref-type="bibr" rid="R44">44</xref>,<xref ref-type="bibr" rid="R54">54</xref></sup> to link subject similarity in conceptual representations of the novel objects, estimated in the <italic>Features task</italic>, with neural synchronization during movie watching, separately for each movie. The linear mixed-effects models were estimated using the <italic>lme4</italic> package (version 1.1-33; RRID:SCR_015654) in <italic>R</italic> (version 4.1.0; RRID:SCR_001905). We estimated a full model to simultaneously assess the main effects of subjects’ similarities in conceptual representations on their ISC during movie watching, while controlling for subject similarity in lexical representations of the novel objects, estimated in a <italic>Naming task</italic>, and also controlling for subject similarities in sex and age. Details on the novel object stimuli, Features and Naming tasks, and the IS-RSA are given in the <xref ref-type="supplementary-material" rid="SD1">Supplementary Methods</xref>.</p></sec><sec id="S24"><title>Control analyses</title><p id="P48">We ran multiple additional analyses to test the robustness of our findings, which are detailed in the <xref ref-type="supplementary-material" rid="SD1">Supplementary Results</xref>. These include: (1) re-running all ISC and IS-RSA analyses using truncated time series to control for differences in movie length; (2) implementing a resampling approach to assess the stability of between-movie variability in ISC across different non-overlapping subject-pair configurations; (3) incorporating a random effect for movie in the IS-RSA model to evaluate whether any effects generalize across our set of movies; (4) re-running IS-RSA analyses without covariates; and (5) repeating IS-RSA analyses using an alternative distance measure to quantify subject similarity in conceptual representations.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Information</label><media xlink:href="EMS201767-supplement-Supplementary_Information.pdf" mimetype="application" mime-subtype="pdf" id="d2aAcEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S25"><title>Acknowledgments</title><p>This work was supported by the Dutch Research Council (NWO; Gravitation Grant 024.001.006 to the Language in Interaction Consortium), by the Swiss National Science Foundation (SNSF; Grants 206557 and 222073 to Simon Leipold), and the European Research Council (ERC; Advanced Grant 101054559 to Ivan Toni).</p></ack><sec id="S26" sec-type="data-availability"><title>Data availability</title><p id="P49">Raw data is available in the Radboud Data Repository, as described in the accompanying data descriptor (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2022.119734">https://doi.org/10.1016/j.neuroimage.2022.119734</ext-link>).</p></sec><sec id="S27" sec-type="data-availability"><title>Code availability</title><p id="P50">Python and R code is available online (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.15929487">https://doi.org/10.5281/zenodo.15929487</ext-link>).</p></sec><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eickhoff</surname><given-names>SB</given-names></name><name><surname>Milham</surname><given-names>M</given-names></name><name><surname>Vanderwal</surname><given-names>T</given-names></name></person-group><article-title>Towards clinical applications of movie fMRI</article-title><source>NeuroImage</source><year>2020</year><volume>217</volume><elocation-id>116860</elocation-id><pub-id pub-id-type="pmid">32376301</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><article-title>Reliability of cortical activity during natural stimulation</article-title><source>Trends Cogn Sci</source><year>2010</year><volume>14</volume><fpage>40</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2009.10.011</pub-id><pub-id pub-id-type="pmcid">PMC2818432</pub-id><pub-id pub-id-type="pmid">20004608</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jääskeläinen</surname><given-names>IP</given-names></name><name><surname>Sams</surname><given-names>M</given-names></name><name><surname>Glerean</surname><given-names>E</given-names></name><name><surname>Ahveninen</surname><given-names>J</given-names></name></person-group><article-title>Movies and narratives as naturalistic stimuli in neuroimaging</article-title><source>NeuroImage</source><year>2021</year><volume>224</volume><elocation-id>117445</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117445</pub-id><pub-id pub-id-type="pmcid">PMC7805386</pub-id><pub-id pub-id-type="pmid">33059053</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Redcay</surname><given-names>E</given-names></name><name><surname>Moraczewski</surname><given-names>D</given-names></name></person-group><article-title>Social cognition in context: A naturalistic imaging approach</article-title><source>NeuroImage</source><year>2020</year><volume>216</volume><elocation-id>116392</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.116392</pub-id><pub-id pub-id-type="pmcid">PMC7244370</pub-id><pub-id pub-id-type="pmid">31770637</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sonkusare</surname><given-names>S</given-names></name><name><surname>Breakspear</surname><given-names>M</given-names></name><name><surname>Guo</surname><given-names>C</given-names></name></person-group><article-title>Naturalistic Stimuli in Neuroscience: Critically Acclaimed</article-title><source>Trends Cogn Sci</source><year>2019</year><volume>23</volume><fpage>699</fpage><lpage>714</lpage><pub-id pub-id-type="pmid">31257145</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vanderwal</surname><given-names>T</given-names></name><name><surname>Eilbott</surname><given-names>J</given-names></name><name><surname>Castellanos</surname><given-names>FX</given-names></name></person-group><article-title>Movies in the magnet: Naturalistic paradigms in developmental functional neuroimaging</article-title><source>Dev Cogn Neurosci</source><year>2019</year><volume>36</volume><elocation-id>100600</elocation-id><pub-id pub-id-type="doi">10.1016/j.dcn.2018.10.004</pub-id><pub-id pub-id-type="pmcid">PMC6969259</pub-id><pub-id pub-id-type="pmid">30551970</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Nir</surname><given-names>Y</given-names></name><name><surname>Levy</surname><given-names>I</given-names></name><name><surname>Fuhrmann</surname><given-names>G</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name></person-group><article-title>Intersubject Synchronization of Cortical Activity during Natural Vision</article-title><source>Science</source><year>2004</year><volume>303</volume><fpage>1634</fpage><lpage>1640</lpage><pub-id pub-id-type="pmid">15016991</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Yang</surname><given-names>E</given-names></name><name><surname>Vallines</surname><given-names>I</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name><name><surname>Rubin</surname><given-names>N</given-names></name></person-group><article-title>A Hierarchy of Temporal Receptive Windows in Human Cortex</article-title><source>J Neurosci</source><year>2008</year><volume>28</volume><fpage>2539</fpage><lpage>2550</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5487-07.2008</pub-id><pub-id pub-id-type="pmcid">PMC2556707</pub-id><pub-id pub-id-type="pmid">18322098</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lerner</surname><given-names>Y</given-names></name><name><surname>Honey</surname><given-names>CJ</given-names></name><name><surname>Silbert</surname><given-names>LJ</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name></person-group><article-title>Topographic Mapping of a Hierarchy of Temporal Receptive Windows Using a Narrated Story</article-title><source>J Neurosci</source><year>2011</year><volume>31</volume><fpage>2906</fpage><lpage>2915</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3684-10.2011</pub-id><pub-id pub-id-type="pmcid">PMC3089381</pub-id><pub-id pub-id-type="pmid">21414912</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willems</surname><given-names>RM</given-names></name><name><surname>Nastase</surname><given-names>SA</given-names></name><name><surname>Milivojevic</surname><given-names>B</given-names></name></person-group><article-title>Narratives for Neuroscience</article-title><source>Trends Neurosci</source><year>2020</year><volume>43</volume><fpage>271</fpage><lpage>273</lpage><pub-id pub-id-type="pmid">32353331</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>SM</given-names></name><name><surname>Molnar-Szakacs</surname><given-names>I</given-names></name><name><surname>Iacoboni</surname><given-names>M</given-names></name></person-group><article-title>Beyond Superior Temporal Cortex: Intersubject Correlations in Narrative Speech Comprehension</article-title><source>Cereb Cortex</source><year>2008</year><volume>18</volume><fpage>230</fpage><lpage>242</lpage><pub-id pub-id-type="pmid">17504783</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abrams</surname><given-names>DA</given-names></name><etal/></person-group><article-title>Inter-subject synchronization of brain responses during natural music listening</article-title><source>Eur J Neurosci</source><year>2013</year><volume>37</volume><fpage>1458</fpage><lpage>1469</lpage><pub-id pub-id-type="doi">10.1111/ejn.12173</pub-id><pub-id pub-id-type="pmcid">PMC4487043</pub-id><pub-id pub-id-type="pmid">23578016</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alluri</surname><given-names>V</given-names></name><etal/></person-group><article-title>Large-scale brain networks emerge from dynamic processing of musical timbre, key and rhythm</article-title><source>NeuroImage</source><year>2012</year><volume>59</volume><fpage>3677</fpage><lpage>3689</lpage><pub-id pub-id-type="pmid">22116038</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grall</surname><given-names>C</given-names></name><name><surname>Finn</surname><given-names>ES</given-names></name></person-group><article-title>Leveraging the power of media to drive cognition: a media-informed approach to naturalistic neuroscience</article-title><source>Soc Cogn Affect Neurosci</source><year>2022</year><volume>17</volume><fpage>598</fpage><lpage>608</lpage><pub-id pub-id-type="doi">10.1093/scan/nsac019</pub-id><pub-id pub-id-type="pmcid">PMC9164202</pub-id><pub-id pub-id-type="pmid">35257180</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nastase</surname><given-names>SA</given-names></name><name><surname>Goldstein</surname><given-names>A</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name></person-group><article-title>Keep it real: rethinking the primacy of experimental control in cognitive neuroscience</article-title><source>NeuroImage</source><year>2020</year><volume>222</volume><elocation-id>117254</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117254</pub-id><pub-id pub-id-type="pmcid">PMC7789034</pub-id><pub-id pub-id-type="pmid">32800992</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunswik</surname><given-names>E</given-names></name></person-group><article-title>Representative design and probabilistic theory in a functional psychology</article-title><source>Psychol Rev</source><year>1955</year><volume>62</volume><fpage>193</fpage><lpage>217</lpage><pub-id pub-id-type="pmid">14371898</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holleman</surname><given-names>GA</given-names></name><name><surname>Hooge</surname><given-names>ITC</given-names></name><name><surname>Kemner</surname><given-names>C</given-names></name><name><surname>Hessels</surname><given-names>RS</given-names></name></person-group><article-title>The ‘Real-World Approach’ and Its Problems: A Critique of the Term Ecological Validity</article-title><source>Front Psychol</source><year>2020</year><volume>11</volume><pub-id pub-id-type="doi">10.3389/fpsyg.2020.00721</pub-id><pub-id pub-id-type="pmcid">PMC7204431</pub-id><pub-id pub-id-type="pmid">32425850</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kihlstrom</surname><given-names>JF</given-names></name></person-group><article-title>Ecological Validity and “Ecological Validity”</article-title><source>Perspect Psychol Sci</source><year>2021</year><volume>16</volume><fpage>466</fpage><lpage>471</lpage><pub-id pub-id-type="pmid">33593121</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmälzle</surname><given-names>R</given-names></name><name><surname>Huskey</surname><given-names>R</given-names></name></person-group><article-title>Integrating media content analysis, reception analysis, and media effects studies</article-title><source>Front Neurosci</source><year>2023</year><volume>17</volume><pub-id pub-id-type="doi">10.3389/fnins.2023.1155750</pub-id><pub-id pub-id-type="pmcid">PMC10173883</pub-id><pub-id pub-id-type="pmid">37179563</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nastase</surname><given-names>SA</given-names></name><name><surname>Gazzola</surname><given-names>V</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Keysers</surname><given-names>C</given-names></name></person-group><article-title>Measuring shared responses across subjects using intersubject correlation</article-title><source>Soc Cogn Affect Neurosci</source><year>2019</year><volume>14</volume><fpage>667</fpage><lpage>685</lpage><pub-id pub-id-type="doi">10.1093/scan/nsz037</pub-id><pub-id pub-id-type="pmcid">PMC6688448</pub-id><pub-id pub-id-type="pmid">31099394</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nummenmaa</surname><given-names>L</given-names></name><name><surname>Lahnakoski</surname><given-names>JM</given-names></name><name><surname>Glerean</surname><given-names>E</given-names></name></person-group><article-title>Sharing the social world via intersubject neural synchronisation</article-title><source>Curr Opin Psychol</source><year>2018</year><volume>24</volume><fpage>7</fpage><lpage>14</lpage><pub-id pub-id-type="pmid">29550395</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lahnakoski</surname><given-names>JM</given-names></name><etal/></person-group><article-title>Synchronous brain activity across individuals underlies shared psychological perspectives</article-title><source>NeuroImage</source><year>2014</year><volume>100</volume><fpage>316</fpage><lpage>324</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.06.022</pub-id><pub-id pub-id-type="pmcid">PMC4153812</pub-id><pub-id pub-id-type="pmid">24936687</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeshurun</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Same Story, Different Story: The Neural Representation of Interpretive Frameworks</article-title><source>Psychol Sci</source><year>2017</year><volume>28</volume><fpage>307</fpage><lpage>319</lpage></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sievers</surname><given-names>B</given-names></name><name><surname>Welker</surname><given-names>C</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Kleinbaum</surname><given-names>AM</given-names></name><name><surname>Wheatley</surname><given-names>T</given-names></name></person-group><article-title>Consensus-building conversation leads to neural alignment</article-title><source>Nat Commun</source><year>2024</year><volume>15</volume><elocation-id>3936</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-023-43253-8</pub-id><pub-id pub-id-type="pmcid">PMC11087652</pub-id><pub-id pub-id-type="pmid">38729961</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parkinson</surname><given-names>C</given-names></name><name><surname>Kleinbaum</surname><given-names>AM</given-names></name><name><surname>Wheatley</surname><given-names>T</given-names></name></person-group><article-title>Similar neural responses predict friendship</article-title><source>Nat Commun</source><year>2018</year><volume>9</volume><fpage>332</fpage><pub-id pub-id-type="doi">10.1038/s41467-017-02722-7</pub-id><pub-id pub-id-type="pmcid">PMC5790806</pub-id><pub-id pub-id-type="pmid">29382820</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bacha-Trams</surname><given-names>M</given-names></name><etal/></person-group><article-title>Sisterhood predicts similar neural processing of a film</article-title><source>NeuroImage</source><year>2024</year><volume>297</volume><elocation-id>120712</elocation-id><pub-id pub-id-type="pmid">38945181</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>L</given-names></name><etal/></person-group><article-title>Neural synchronization predicts marital satisfaction</article-title><source>Proc Natl Acad Sci</source><year>2022</year><volume>119</volume><elocation-id>e2202515119</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2202515119</pub-id><pub-id pub-id-type="pmcid">PMC9407484</pub-id><pub-id pub-id-type="pmid">35981139</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname><given-names>U</given-names></name><etal/></person-group><article-title>Shared and idiosyncratic cortical activation patterns in autism revealed under continuous real-life viewing conditions</article-title><source>Autism Res</source><year>2009</year><volume>2</volume><fpage>220</fpage><lpage>231</lpage><pub-id pub-id-type="doi">10.1002/aur.89</pub-id><pub-id pub-id-type="pmcid">PMC2775929</pub-id><pub-id pub-id-type="pmid">19708061</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salmi</surname><given-names>J</given-names></name><etal/></person-group><article-title>The brains of high functioning autistic individuals do not synchronize with those of others</article-title><source>NeuroImage Clin</source><year>2013</year><volume>3</volume><fpage>489</fpage><lpage>497</lpage><pub-id pub-id-type="doi">10.1016/j.nicl.2013.10.011</pub-id><pub-id pub-id-type="pmcid">PMC3830058</pub-id><pub-id pub-id-type="pmid">24273731</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mangnus</surname><given-names>M</given-names></name><etal/></person-group><article-title>Preserved Spontaneous Mentalizing Amid Reduced Intersubject Variability in Autism During a Movie Narrative</article-title><source>Biol Psychiatry Cogn Neurosci Neuroimaging</source><year>2024</year><pub-id pub-id-type="pmid">39490786</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mäntylä</surname><given-names>T</given-names></name><etal/></person-group><article-title>Aberrant Cortical Integration in First-Episode Psychosis During Natural Audiovisual Processing</article-title><source>Biol Psychiatry</source><year>2018</year><volume>84</volume><fpage>655</fpage><lpage>664</lpage><pub-id pub-id-type="pmid">29885763</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>CC</given-names></name><name><surname>Nguyen</surname><given-names>VT</given-names></name><name><surname>Hyett</surname><given-names>MP</given-names></name><name><surname>Parker</surname><given-names>GB</given-names></name><name><surname>Breakspear</surname><given-names>MJ</given-names></name></person-group><article-title>Out-of-sync: disrupted neural activity in emotional circuitry during film viewing in melancholic depression</article-title><source>Sci Rep</source><year>2015</year><volume>5</volume><elocation-id>11605</elocation-id><pub-id pub-id-type="doi">10.1038/srep11605</pub-id><pub-id pub-id-type="pmcid">PMC4481375</pub-id><pub-id pub-id-type="pmid">26112251</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Camacho</surname><given-names>MC</given-names></name><etal/></person-group><article-title>Higher Intersubject Variability in Neural Response to Narrative Social Stimuli Among Youth With Higher Social Anxiety</article-title><source>J Am Acad Child Adolesc Psychiatry</source><year>2023</year><pub-id pub-id-type="doi">10.1016/j.jaac.2023.08.020</pub-id><pub-id pub-id-type="pmcid">PMC12035772</pub-id><pub-id pub-id-type="pmid">38070872</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Finn</surname><given-names>ES</given-names></name><etal/></person-group><article-title>Idiosynchrony: From shared responses to individual differences during naturalistic neuroimaging</article-title><source>NeuroImage</source><year>2020</year><volume>215</volume><elocation-id>116828</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.116828</pub-id><pub-id pub-id-type="pmcid">PMC7298885</pub-id><pub-id pub-id-type="pmid">32276065</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nummenmaa</surname><given-names>L</given-names></name><etal/></person-group><article-title>Emotions promote social interaction by synchronizing brain activity across individuals</article-title><source>Proc Natl Acad Sci</source><year>2012</year><volume>109</volume><fpage>9599</fpage><lpage>9604</lpage><pub-id pub-id-type="doi">10.1073/pnas.1206095109</pub-id><pub-id pub-id-type="pmcid">PMC3386135</pub-id><pub-id pub-id-type="pmid">22623534</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cantlon</surname><given-names>JF</given-names></name><name><surname>Li</surname><given-names>R</given-names></name></person-group><article-title>Neural Activity during Natural Viewing of Sesame Street Statistically Predicts Test Scores in Early Childhood</article-title><source>PLOS Biol</source><year>2013</year><volume>11</volume><elocation-id>e1001462</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1001462</pub-id><pub-id pub-id-type="pmcid">PMC3536813</pub-id><pub-id pub-id-type="pmid">23300385</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moraczewski</surname><given-names>D</given-names></name><name><surname>Chen</surname><given-names>G</given-names></name><name><surname>Redcay</surname><given-names>E</given-names></name></person-group><article-title>Inter-subject synchrony as an index of functional specialization in early childhood</article-title><source>Sci Rep</source><year>2018</year><volume>8</volume><elocation-id>2252</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-018-20600-0</pub-id><pub-id pub-id-type="pmcid">PMC5797124</pub-id><pub-id pub-id-type="pmid">29396415</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petroni</surname><given-names>A</given-names></name><etal/></person-group><article-title>The Variability of Neural Responses to Naturalistic Videos Change with Age and Sex</article-title><source>eNeuro</source><year>2018</year><volume>5</volume><pub-id pub-id-type="doi">10.1523/ENEURO.0244-17.2017</pub-id><pub-id pub-id-type="pmcid">PMC5786826</pub-id><pub-id pub-id-type="pmid">29379880</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Finn</surname><given-names>ES</given-names></name><name><surname>Corlett</surname><given-names>PR</given-names></name><name><surname>Chen</surname><given-names>G</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name><name><surname>Constable</surname><given-names>RT</given-names></name></person-group><article-title>Trait paranoia shapes inter-subject synchrony in brain activity during an ambiguous social narrative</article-title><source>Nat Commun</source><year>2018</year><volume>9</volume><elocation-id>2043</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-04387-2</pub-id><pub-id pub-id-type="pmcid">PMC5966466</pub-id><pub-id pub-id-type="pmid">29795116</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matz</surname><given-names>SC</given-names></name><name><surname>Hyon</surname><given-names>R</given-names></name><name><surname>Baek</surname><given-names>EC</given-names></name><name><surname>Parkinson</surname><given-names>C</given-names></name><name><surname>Cerf</surname><given-names>M</given-names></name></person-group><article-title>Personality similarity predicts synchronous neural responses in fMRI and EEG data</article-title><source>Sci Rep</source><year>2022</year><volume>12</volume><elocation-id>14325</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-022-18237-1</pub-id><pub-id pub-id-type="pmcid">PMC9395427</pub-id><pub-id pub-id-type="pmid">35995958</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Bruin</surname><given-names>D</given-names></name><name><surname>van Baar</surname><given-names>JM</given-names></name><name><surname>Rodríguez</surname><given-names>PL</given-names></name><name><surname>FeldmanHall</surname><given-names>O</given-names></name></person-group><article-title>Shared neural representations and temporal segmentation of political content predict ideological similarity</article-title><source>Sci Adv</source><year>2023</year><volume>9</volume><elocation-id>eabq5920</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.abq5920</pub-id><pub-id pub-id-type="pmcid">PMC9891706</pub-id><pub-id pub-id-type="pmid">36724226</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Katabi</surname><given-names>N</given-names></name><etal/></person-group><article-title>Deeper Than You Think: Partisanship-Dependent Brain Responses in Early Sensory and Motor Brain Regions</article-title><source>J Neurosci</source><year>2023</year><volume>43</volume><fpage>1027</fpage><lpage>1037</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0895-22.2022</pub-id><pub-id pub-id-type="pmcid">PMC9908315</pub-id><pub-id pub-id-type="pmid">36599681</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leong</surname><given-names>YC</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Willer</surname><given-names>R</given-names></name><name><surname>Zaki</surname><given-names>J</given-names></name></person-group><article-title>Conservative and liberal attitudes drive polarized neural responses to political content</article-title><source>Proc Natl Acad Sci</source><year>2020</year><volume>117</volume><fpage>27731</fpage><lpage>27739</lpage><pub-id pub-id-type="doi">10.1073/pnas.2008530117</pub-id><pub-id pub-id-type="pmcid">PMC7959490</pub-id><pub-id pub-id-type="pmid">33082227</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Baar</surname><given-names>JM</given-names></name><name><surname>Halpern</surname><given-names>DJ</given-names></name><name><surname>FeldmanHall</surname><given-names>O</given-names></name></person-group><article-title>Intolerance of uncertainty modulates brain-to-brain synchrony during politically polarized perception</article-title><source>Proc Natl Acad Sci</source><year>2021</year><volume>118</volume><elocation-id>e2022491118</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2022491118</pub-id><pub-id pub-id-type="pmcid">PMC8157931</pub-id><pub-id pub-id-type="pmid">33986114</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>P-HA</given-names></name><name><surname>Jolly</surname><given-names>E</given-names></name><name><surname>Cheong</surname><given-names>JH</given-names></name><name><surname>Chang</surname><given-names>LJ</given-names></name></person-group><article-title>Intersubject representational similarity analysis reveals individual variations in affective experience when watching erotic movies</article-title><source>NeuroImage</source><year>2020</year><volume>216</volume><elocation-id>116851</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.116851</pub-id><pub-id pub-id-type="pmcid">PMC7955800</pub-id><pub-id pub-id-type="pmid">32294538</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jangraw</surname><given-names>DC</given-names></name><etal/></person-group><article-title>Inter-subject correlation during long narratives reveals widespread neural correlates of reading ability</article-title><source>NeuroImage</source><year>2023</year><volume>282</volume><elocation-id>120390</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2023.120390</pub-id><pub-id pub-id-type="pmcid">PMC10783814</pub-id><pub-id pub-id-type="pmid">37751811</pub-id></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartels</surname><given-names>A</given-names></name><name><surname>Zeki</surname><given-names>S</given-names></name></person-group><article-title>Functional brain mapping during free viewing of natural scenes</article-title><source>Hum Brain Mapp</source><year>2004</year><volume>21</volume><fpage>75</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1002/hbm.10153</pub-id><pub-id pub-id-type="pmcid">PMC6872023</pub-id><pub-id pub-id-type="pmid">14755595</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname><given-names>U</given-names></name><etal/></person-group><article-title>Neurocinematics: The Neuroscience of Film</article-title><source>Projections</source><year>2008</year><volume>2</volume><fpage>1</fpage><lpage>26</lpage></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eijk</surname><given-names>L</given-names></name><etal/></person-group><article-title>The CABB dataset: A multimodal corpus of communicative interactions for behavioural and neural analyses</article-title><source>NeuroImage</source><year>2022</year><volume>264</volume><elocation-id>119734</elocation-id><pub-id pub-id-type="pmid">36343884</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fan</surname><given-names>L</given-names></name><etal/></person-group><article-title>The Human Brainnetome Atlas: A New Brain Atlas Based on Connectional Architecture</article-title><source>Cereb Cortex</source><year>2016</year><volume>26</volume><fpage>3508</fpage><lpage>3526</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw157</pub-id><pub-id pub-id-type="pmcid">PMC4961028</pub-id><pub-id pub-id-type="pmid">27230218</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nastase</surname><given-names>SA</given-names></name><etal/></person-group><article-title>The “Narratives” fMRI dataset for evaluating models of naturalistic language comprehension</article-title><source>Sci Data</source><year>2021</year><volume>8</volume><fpage>250</fpage><pub-id pub-id-type="doi">10.1038/s41597-021-01033-3</pub-id><pub-id pub-id-type="pmcid">PMC8479122</pub-id><pub-id pub-id-type="pmid">34584100</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Binder</surname><given-names>JR</given-names></name><etal/></person-group><article-title>Toward a brain-based componential semantic representation</article-title><source>Cogn Neuropsychol</source><year>2016</year><volume>33</volume><fpage>130</fpage><lpage>174</lpage><pub-id pub-id-type="pmid">27310469</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nguyen</surname><given-names>M</given-names></name><name><surname>Vanderwal</surname><given-names>T</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name></person-group><article-title>Shared understanding of narratives is correlated with shared neural responses</article-title><source>NeuroImage</source><year>2019</year><volume>184</volume><fpage>161</fpage><lpage>170</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.09.010</pub-id><pub-id pub-id-type="pmcid">PMC6287615</pub-id><pub-id pub-id-type="pmid">30217543</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>G</given-names></name><name><surname>Taylor</surname><given-names>PA</given-names></name><name><surname>Shin</surname><given-names>Y-W</given-names></name><name><surname>Reynolds</surname><given-names>RC</given-names></name><name><surname>Cox</surname><given-names>RW</given-names></name></person-group><article-title>Untangling the relatedness among correlations, Part II: Inter-subject correlation group analysis through linear mixed-effects modeling</article-title><source>NeuroImage</source><year>2017</year><volume>147</volume><fpage>825</fpage><lpage>840</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.08.029</pub-id><pub-id pub-id-type="pmcid">PMC5303634</pub-id><pub-id pub-id-type="pmid">27751943</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baek</surname><given-names>EC</given-names></name><etal/></person-group><article-title>In-degree centrality in a social network is linked to coordinated neural activity</article-title><source>Nat Commun</source><year>2022</year><volume>13</volume><elocation-id>1118</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-022-28432-3</pub-id><pub-id pub-id-type="pmcid">PMC8891270</pub-id><pub-id pub-id-type="pmid">35236835</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baek</surname><given-names>EC</given-names></name><etal/></person-group><article-title>Lonely Individuals Process the World in Idiosyncratic Ways</article-title><source>Psychol Sci</source><year>2023</year><volume>34</volume><fpage>683</fpage><lpage>695</lpage><pub-id pub-id-type="doi">10.1177/09567976221145316</pub-id><pub-id pub-id-type="pmcid">PMC10404901</pub-id><pub-id pub-id-type="pmid">37027033</pub-id></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rust</surname><given-names>NC</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><article-title>In praise of artifice</article-title><source>Nat Neurosci</source><year>2005</year><volume>8</volume><fpage>1647</fpage><lpage>1650</lpage><pub-id pub-id-type="pmid">16306892</pub-id></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yarkoni</surname><given-names>T</given-names></name></person-group><article-title>The generalizability crisis</article-title><source>Behav Brain Sci</source><year>2022</year><volume>45</volume><fpage>e1</fpage><pub-id pub-id-type="doi">10.1017/S0140525X20001685</pub-id><pub-id pub-id-type="pmcid">PMC10681374</pub-id><pub-id pub-id-type="pmid">33342451</pub-id></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Westfall</surname><given-names>J</given-names></name><name><surname>Nichols</surname><given-names>T</given-names></name><name><surname>Yarkoni</surname><given-names>T</given-names></name></person-group><article-title>Fixing the stimulus-as-fixed-effect fallacy in task fMRI</article-title><source>Wellcome Open Res</source><year>2017</year><volume>1</volume><pub-id pub-id-type="doi">10.12688/wellcomeopenres.10298.2</pub-id><pub-id pub-id-type="pmcid">PMC5428747</pub-id><pub-id pub-id-type="pmid">28503664</pub-id></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>HH</given-names></name></person-group><article-title>The language-as-fixed-effect fallacy: A critique of language statistics in psychological research</article-title><source>J Verbal Learn Verbal Behav</source><year>1973</year><volume>12</volume><fpage>335</fpage><lpage>359</lpage></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coleman</surname><given-names>EB</given-names></name></person-group><article-title>Generalizing to a Language Population</article-title><source>Psychol Rep</source><year>1964</year><volume>14</volume><fpage>219</fpage><lpage>226</lpage></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>J</given-names></name><etal/></person-group><article-title>Test–retest reliability of functional connectivity networks during naturalistic fMRI paradigms</article-title><source>Hum Brain Mapp</source><year>2017</year><volume>38</volume><fpage>2226</fpage><lpage>2241</lpage><pub-id pub-id-type="doi">10.1002/hbm.23517</pub-id><pub-id pub-id-type="pmcid">PMC6867176</pub-id><pub-id pub-id-type="pmid">28094464</pub-id></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vanderwal</surname><given-names>T</given-names></name><name><surname>Kelly</surname><given-names>C</given-names></name><name><surname>Eilbott</surname><given-names>J</given-names></name><name><surname>Mayes</surname><given-names>LC</given-names></name><name><surname>Castellanos</surname><given-names>FX</given-names></name></person-group><article-title>Inscapes: A movie paradigm to improve compliance in functional magnetic resonance imaging</article-title><source>NeuroImage</source><year>2015</year><volume>122</volume><fpage>222</fpage><lpage>232</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.07.069</pub-id><pub-id pub-id-type="pmcid">PMC4618190</pub-id><pub-id pub-id-type="pmid">26241683</pub-id></element-citation></ref><ref id="R64"><label>64</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>McNamara</surname><given-names>Q</given-names></name><name><surname>De La Vega</surname><given-names>A</given-names></name><name><surname>Yarkoni</surname><given-names>T</given-names></name></person-group><source>Developing a Comprehensive Framework for Multimodal Feature Extraction</source><conf-name>Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</conf-name><conf-sponsor>Association for Computing Machinery</conf-sponsor><conf-loc>New York, NY, USA</conf-loc><year>2017</year><fpage>1567</fpage><lpage>1574</lpage><pub-id pub-id-type="doi">10.1145/3097983.3098075</pub-id></element-citation></ref><ref id="R65"><label>65</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de la Vega</surname><given-names>A</given-names></name><etal/></person-group><article-title>Neuroscout, a unified platform for generalizable and reproducible fMRI research</article-title><source>eLife</source><year>2022</year><volume>11</volume><elocation-id>e79277</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.79277</pub-id><pub-id pub-id-type="pmcid">PMC9489206</pub-id><pub-id pub-id-type="pmid">36040302</pub-id></element-citation></ref><ref id="R66"><label>66</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallace</surname><given-names>RS</given-names></name><etal/></person-group><article-title>Mapping patterns of thought onto brain activity during movie-watching</article-title><source>eLife</source><year>2025</year><volume>13</volume><elocation-id>RP97731</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.97731</pub-id><pub-id pub-id-type="pmcid">PMC11723579</pub-id><pub-id pub-id-type="pmid">39792001</pub-id></element-citation></ref><ref id="R67"><label>67</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Eickhoff</surname><given-names>SB</given-names></name><name><surname>Weis</surname><given-names>S</given-names></name></person-group><article-title>Stimulus Selection Influences Prediction of Individual Phenotypes in Naturalistic Conditions</article-title><source>Hum Brain Mapp</source><year>2025</year><volume>46</volume><elocation-id>e70164</elocation-id><pub-id pub-id-type="doi">10.1002/hbm.70164</pub-id><pub-id pub-id-type="pmcid">PMC11831449</pub-id><pub-id pub-id-type="pmid">39960115</pub-id></element-citation></ref><ref id="R68"><label>68</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herbec</surname><given-names>A</given-names></name><name><surname>Kauppi</surname><given-names>J-P</given-names></name><name><surname>Jola</surname><given-names>C</given-names></name><name><surname>Tohka</surname><given-names>J</given-names></name><name><surname>Pollick</surname><given-names>FE</given-names></name></person-group><article-title>Differences in fMRI intersubject correlation while viewing unedited and edited videos of dance performance</article-title><source>Cortex</source><year>2015</year><volume>71</volume><fpage>341</fpage><lpage>348</lpage><pub-id pub-id-type="pmid">26298503</pub-id></element-citation></ref><ref id="R69"><label>69</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heimann</surname><given-names>KS</given-names></name><etal/></person-group><article-title>“Cuts in Action”: A High-Density EEG Study Investigating the Neural Correlates of Different Editing Techniques in Film</article-title><source>Cogn Sci</source><year>2017</year><volume>41</volume><fpage>1555</fpage><lpage>1588</lpage><pub-id pub-id-type="pmid">27882594</pub-id></element-citation></ref><ref id="R70"><label>70</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>TJ</given-names></name><name><surname>Levin</surname><given-names>D</given-names></name><name><surname>Cutting</surname><given-names>JE</given-names></name></person-group><article-title>A Window on Reality: Perceiving Edited Moving Images</article-title><source>Curr Dir Psychol Sci</source><year>2012</year><volume>21</volume><fpage>107</fpage><lpage>113</lpage></element-citation></ref><ref id="R71"><label>71</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>EJ</given-names></name><etal/></person-group><article-title>A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence</article-title><source>Nat Neurosci</source><year>2022</year><volume>25</volume><fpage>116</fpage><lpage>126</lpage><pub-id pub-id-type="pmid">34916659</pub-id></element-citation></ref><ref id="R72"><label>72</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>MN</given-names></name><etal/></person-group><article-title>THINGS: A database of 1,854 object concepts and more than 26,000 naturalistic object images</article-title><source>PLOS ONE</source><year>2019</year><volume>14</volume><elocation-id>e0223792</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0223792</pub-id><pub-id pub-id-type="pmcid">PMC6793944</pub-id><pub-id pub-id-type="pmid">31613926</pub-id></element-citation></ref><ref id="R73"><label>73</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>MN</given-names></name><etal/></person-group><article-title>THINGS-data, a multimodal collection of large-scale datasets for investigating object representations in human brain and behavior</article-title><source>eLife</source><year>2023</year><volume>12</volume><elocation-id>e82580</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.82580</pub-id><pub-id pub-id-type="pmcid">PMC10038662</pub-id><pub-id pub-id-type="pmid">36847339</pub-id></element-citation></ref><ref id="R74"><label>74</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vigliocco</surname><given-names>G</given-names></name><etal/></person-group><article-title>Ecological brain: reframing the study of human behaviour and cognition</article-title><source>R Soc Open Sci</source><year>2024</year><volume>11</volume><elocation-id>240762</elocation-id><pub-id pub-id-type="doi">10.1098/rsos.240762</pub-id><pub-id pub-id-type="pmcid">PMC11544371</pub-id><pub-id pub-id-type="pmid">39525361</pub-id></element-citation></ref><ref id="R75"><label>75</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shamay-Tsoory</surname><given-names>SG</given-names></name><name><surname>Mendelsohn</surname><given-names>A</given-names></name></person-group><article-title>Real-Life Neuroscience: An Ecological Approach to Brain and Behavior Research</article-title><source>Perspect Psychol Sci</source><year>2019</year><volume>14</volume><fpage>841</fpage><lpage>859</lpage><pub-id pub-id-type="pmid">31408614</pub-id></element-citation></ref><ref id="R76"><label>76</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holleman</surname><given-names>GA</given-names></name><name><surname>Hooge</surname><given-names>ITC</given-names></name><name><surname>Kemner</surname><given-names>C</given-names></name><name><surname>Hessels</surname><given-names>RS</given-names></name></person-group><article-title>The Reality of “Real-Life” Neuroscience: A Commentary on Shamay-Tsoory and Mendelsohn (2019</article-title><source>Perspect Psychol Sci</source><year>2021</year><volume>16</volume><fpage>461</fpage><lpage>465</lpage><pub-id pub-id-type="doi">10.1177/1745691620917354</pub-id><pub-id pub-id-type="pmcid">PMC7961613</pub-id><pub-id pub-id-type="pmid">32316849</pub-id></element-citation></ref><ref id="R77"><label>77</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baldassano</surname><given-names>C</given-names></name><etal/></person-group><article-title>Discovering Event Structure in Continuous Narrative Perception and Memory</article-title><source>Neuron</source><year>2017</year><volume>95</volume><fpage>709</fpage><lpage>721</lpage><elocation-id>e5</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuron.2017.06.041</pub-id><pub-id pub-id-type="pmcid">PMC5558154</pub-id><pub-id pub-id-type="pmid">28772125</pub-id></element-citation></ref><ref id="R78"><label>78</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ben-Yakov</surname><given-names>A</given-names></name><name><surname>Henson</surname><given-names>RN</given-names></name></person-group><article-title>The Hippocampal Film Editor: Sensitivity and Specificity to Event Boundaries in Continuous Experience</article-title><source>J Neurosci</source><year>2018</year><volume>38</volume><fpage>10057</fpage><lpage>10068</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0524-18.2018</pub-id><pub-id pub-id-type="pmcid">PMC6246887</pub-id><pub-id pub-id-type="pmid">30301758</pub-id></element-citation></ref><ref id="R79"><label>79</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>J</given-names></name><etal/></person-group><article-title>Shared memories reveal shared structure in neural activity across individuals</article-title><source>Nat Neurosci</source><year>2017</year><volume>20</volume><fpage>115</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1038/nn.4450</pub-id><pub-id pub-id-type="pmcid">PMC5191958</pub-id><pub-id pub-id-type="pmid">27918531</pub-id></element-citation></ref><ref id="R80"><label>80</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacoby</surname><given-names>N</given-names></name><name><surname>Bruneau</surname><given-names>E</given-names></name><name><surname>Koster-Hale</surname><given-names>J</given-names></name><name><surname>Saxe</surname><given-names>R</given-names></name></person-group><article-title>Localizing Pain Matrix and Theory of Mind networks with both verbal and non-verbal stimuli</article-title><source>NeuroImage</source><year>2016</year><volume>126</volume><fpage>39</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.11.025</pub-id><pub-id pub-id-type="pmcid">PMC4733571</pub-id><pub-id pub-id-type="pmid">26589334</pub-id></element-citation></ref><ref id="R81"><label>81</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richardson</surname><given-names>H</given-names></name></person-group><article-title>Development of brain networks for social functions: Confirmatory analyses in a large open source dataset</article-title><source>Dev Cogn Neurosci</source><year>2019</year><volume>37</volume><elocation-id>100598</elocation-id><pub-id pub-id-type="doi">10.1016/j.dcn.2018.11.002</pub-id><pub-id pub-id-type="pmcid">PMC6969289</pub-id><pub-id pub-id-type="pmid">30522854</pub-id></element-citation></ref><ref id="R82"><label>82</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richardson</surname><given-names>H</given-names></name><name><surname>Lisandrelli</surname><given-names>G</given-names></name><name><surname>Riobueno-Naylor</surname><given-names>A</given-names></name><name><surname>Saxe</surname><given-names>R</given-names></name></person-group><article-title>Development of the social brain from age three to twelve years</article-title><source>Nat Commun</source><year>2018</year><volume>9</volume><elocation-id>1027</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-03399-2</pub-id><pub-id pub-id-type="pmcid">PMC5847587</pub-id><pub-id pub-id-type="pmid">29531321</pub-id></element-citation></ref><ref id="R83"><label>83</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huth</surname><given-names>AG</given-names></name><name><surname>De Heer</surname><given-names>WA</given-names></name><name><surname>Griffiths</surname><given-names>TL</given-names></name><name><surname>Theunissen</surname><given-names>FE</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><article-title>Natural speech reveals the semantic maps that tile human cerebral cortex</article-title><source>Nature</source><year>2016</year><volume>532</volume><fpage>453</fpage><lpage>458</lpage><pub-id pub-id-type="doi">10.1038/nature17637</pub-id><pub-id pub-id-type="pmcid">PMC4852309</pub-id><pub-id pub-id-type="pmid">27121839</pub-id></element-citation></ref><ref id="R84"><label>84</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dupré la Tour</surname><given-names>T</given-names></name><name><surname>Visconti di Oleggio Castello</surname><given-names>M</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><article-title>The Voxelwise Encoding Model framework: A tutorial introduction to fitting encoding models to fMRI data</article-title><source>Imaging Neurosci</source><year>2025</year><volume>3</volume><elocation-id>imag_a_00575</elocation-id></element-citation></ref><ref id="R85"><label>85</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tian</surname><given-names>L</given-names></name><name><surname>Ye</surname><given-names>M</given-names></name><name><surname>Chen</surname><given-names>C</given-names></name><name><surname>Cao</surname><given-names>X</given-names></name><name><surname>Shen</surname><given-names>T</given-names></name></person-group><article-title>Consistency of functional connectivity across different movies</article-title><source>NeuroImage</source><year>2021</year><volume>233</volume><elocation-id>117926</elocation-id><pub-id pub-id-type="pmid">33675997</pub-id></element-citation></ref><ref id="R86"><label>86</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buckner</surname><given-names>RL</given-names></name><name><surname>Krienen</surname><given-names>FM</given-names></name><name><surname>Yeo</surname><given-names>BTT</given-names></name></person-group><article-title>Opportunities and limitations of intrinsic functional connectivity MRI</article-title><source>Nat Neurosci</source><year>2013</year><volume>16</volume><fpage>832</fpage><lpage>837</lpage><pub-id pub-id-type="pmid">23799476</pub-id></element-citation></ref><ref id="R87"><label>87</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cole</surname><given-names>MW</given-names></name><name><surname>Bassett</surname><given-names>DS</given-names></name><name><surname>Power</surname><given-names>JD</given-names></name><name><surname>Braver</surname><given-names>TS</given-names></name><name><surname>Petersen</surname><given-names>SE</given-names></name></person-group><article-title>Intrinsic and Task-Evoked Network Architectures of the Human Brain</article-title><source>Neuron</source><year>2014</year><volume>83</volume><fpage>238</fpage><lpage>251</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.05.014</pub-id><pub-id pub-id-type="pmcid">PMC4082806</pub-id><pub-id pub-id-type="pmid">24991964</pub-id></element-citation></ref><ref id="R88"><label>88</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gratton</surname><given-names>C</given-names></name><etal/></person-group><article-title>Functional Brain Networks Are Dominated by Stable Group and Individual Factors, Not Cognitive or Daily Variation</article-title><source>Neuron</source><year>2018</year><volume>98</volume><fpage>439</fpage><lpage>452</lpage><elocation-id>e5</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuron.2018.03.035</pub-id><pub-id pub-id-type="pmcid">PMC5912345</pub-id><pub-id pub-id-type="pmid">29673485</pub-id></element-citation></ref><ref id="R89"><label>89</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vanderwal</surname><given-names>T</given-names></name><etal/></person-group><article-title>Individual differences in functional connectivity during naturalistic viewing conditions</article-title><source>NeuroImage</source><year>2017</year><volume>157</volume><fpage>521</fpage><lpage>530</lpage><pub-id pub-id-type="pmid">28625875</pub-id></element-citation></ref><ref id="R90"><label>90</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kröll</surname><given-names>JP</given-names></name><etal/></person-group><article-title>Naturalistic viewing increases individual identifiability based on connectivity within functional brain networks</article-title><source>NeuroImage</source><year>2023</year><volume>273</volume><elocation-id>120083</elocation-id><pub-id pub-id-type="pmid">37015270</pub-id></element-citation></ref><ref id="R91"><label>91</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Finn</surname><given-names>ES</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name></person-group><article-title>Movie-watching outperforms rest for functional connectivity-based prediction of behavior</article-title><source>NeuroImage</source><year>2021</year><volume>235</volume><elocation-id>117963</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2021.117963</pub-id><pub-id pub-id-type="pmcid">PMC8204673</pub-id><pub-id pub-id-type="pmid">33813007</pub-id></element-citation></ref><ref id="R92"><label>92</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ye</surname><given-names>M</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Guan</surname><given-names>Y</given-names></name><name><surname>Ma</surname><given-names>H</given-names></name><name><surname>Tian</surname><given-names>L</given-names></name></person-group><article-title>Are inter-subject functional correlations consistent across different movies?</article-title><source>Brain Imaging Behav</source><year>2023</year><volume>17</volume><fpage>44</fpage><lpage>53</lpage><pub-id pub-id-type="pmid">36418674</pub-id></element-citation></ref><ref id="R93"><label>93</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andermane</surname><given-names>N</given-names></name><name><surname>Bosten</surname><given-names>JM</given-names></name><name><surname>Seth</surname><given-names>AK</given-names></name><name><surname>Ward</surname><given-names>J</given-names></name></person-group><article-title>Individual differences in change blindness are predicted by the strength and stability of visual representations</article-title><source>Neurosci Conscious</source><year>2019</year><volume>2019</volume><elocation-id>niy010</elocation-id><pub-id pub-id-type="doi">10.1093/nc/niy010</pub-id><pub-id pub-id-type="pmcid">PMC6345093</pub-id><pub-id pub-id-type="pmid">30697440</pub-id></element-citation></ref><ref id="R94"><label>94</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fedorenko</surname><given-names>E</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><article-title>Broad domain generality in focal regions of frontal and parietal cortex</article-title><source>Proc Natl Acad Sci</source><year>2013</year><volume>110</volume><fpage>16616</fpage><lpage>16621</lpage><pub-id pub-id-type="doi">10.1073/pnas.1315235110</pub-id><pub-id pub-id-type="pmcid">PMC3799302</pub-id><pub-id pub-id-type="pmid">24062451</pub-id></element-citation></ref><ref id="R95"><label>95</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname><given-names>W</given-names></name><name><surname>Taghia</surname><given-names>J</given-names></name><name><surname>Menon</surname><given-names>V</given-names></name></person-group><article-title>A multi-demand operating system underlying diverse cognitive tasks</article-title><source>Nat Commun</source><year>2024</year><volume>15</volume><elocation-id>2185</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-024-46511-5</pub-id><pub-id pub-id-type="pmcid">PMC10928152</pub-id><pub-id pub-id-type="pmid">38467606</pub-id></element-citation></ref><ref id="R96"><label>96</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>Yuan</surname><given-names>K-H</given-names></name></person-group><source>Practical Statistical Power Analysis Using Webpower and R</source><publisher-name>ISDSA Press</publisher-name><year>2018</year><pub-id pub-id-type="doi">10.35566/power</pub-id></element-citation></ref><ref id="R97"><label>97</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uğurbil</surname><given-names>K</given-names></name><etal/></person-group><article-title>Pushing spatial and temporal resolution for functional and diffusion MRI in the Human Connectome Project</article-title><source>NeuroImage</source><year>2013</year><volume>80</volume><fpage>80</fpage><lpage>104</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.05.012</pub-id><pub-id pub-id-type="pmcid">PMC3740184</pub-id><pub-id pub-id-type="pmid">23702417</pub-id></element-citation></ref><ref id="R98"><label>98</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name><etal/></person-group><article-title>The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments</article-title><source>Sci Data</source><year>2016</year><volume>3</volume><elocation-id>160044</elocation-id><pub-id pub-id-type="doi">10.1038/sdata.2016.44</pub-id><pub-id pub-id-type="pmcid">PMC4978148</pub-id><pub-id pub-id-type="pmid">27326542</pub-id></element-citation></ref><ref id="R99"><label>99</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zwiers</surname><given-names>MP</given-names></name><name><surname>Moia</surname><given-names>S</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><article-title>BIDScoin: A User-Friendly Application to Convert Source Data to Brain Imaging Data Structure</article-title><source>Front Neuroinformatics</source><year>2022</year><volume>15</volume><pub-id pub-id-type="doi">10.3389/fninf.2021.770608</pub-id><pub-id pub-id-type="pmcid">PMC8792932</pub-id><pub-id pub-id-type="pmid">35095452</pub-id></element-citation></ref><ref id="R100"><label>100</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esteban</surname><given-names>O</given-names></name><etal/></person-group><article-title>fMRIPrep: a robust preprocessing pipeline for functional MRI</article-title><source>Nat Methods</source><year>2019</year><volume>16</volume><fpage>111</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0235-4</pub-id><pub-id pub-id-type="pmcid">PMC6319393</pub-id><pub-id pub-id-type="pmid">30532080</pub-id></element-citation></ref><ref id="R101"><label>101</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name><etal/></person-group><article-title>Nipype: A Flexible, Lightweight and Extensible Neuroimaging Data Processing Framework in Python</article-title><source>Front Neuroinformatics</source><year>2011</year><volume>5</volume><pub-id pub-id-type="doi">10.3389/fninf.2011.00013</pub-id><pub-id pub-id-type="pmcid">PMC3159964</pub-id><pub-id pub-id-type="pmid">21897815</pub-id></element-citation></ref><ref id="R102"><label>102</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>L</given-names></name><etal/></person-group><article-title>cosanlab/nltools: 0.4.7</article-title><source>Zenodo</source><year>2022</year><pub-id pub-id-type="doi">10.5281/zenodo.7015135</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Overview.</title><p><bold>A)</bold> Subjects (<italic>N</italic> = 112) viewed eight animated movies during fMRI scanning. The movies were selected to include diverse object categories, such as humans, plants, tools, toys, and food. <bold>B)</bold> We quantified the variability in inter-subject correlation (ISC) between movies across the brain. For each subject and movie, we extracted a preprocessed and spatially averaged fMRI time series from 210 cortical parcels. ISC was computed for each parcel by correlating the time series between all subject pairs. <bold>C)</bold> To assess the consequences of between-movie variability for generalizability, we examined whether trait- or state-like differences in ISC were consistent across movies. Specifically, we used inter-subject representational similarity (IS-RSA) to relate neural synchronization during movie watching to the similarity of subjects’ conceptual-semantic representations of novel objects (“Fribbles”, in blue), with analyses conducted separately for each movie.</p></caption><graphic xlink:href="EMS201767-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Inter-subject correlation for individual movies.</title><p>Spatial distribution and magnitude of inter-subject correlations (ISC) across the cortex, shown separately for each of the eight movie clips.</p></caption><graphic xlink:href="EMS201767-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Variability in inter-subject correlation across movies.</title><p><bold>A)</bold> Mean and standard error of Fisher <italic>z</italic>-transformed whole-brain inter-subject correlation (ISC) values for each of the eight movies. Whole-brain ISC was computed by averaging ISC values across all cortical parcels for each subject and movie. A repeated-measures ANOVA revealed a significant main effect of Movie, indicating considerable variability in whole-brain ISC across movies. <bold>B)</bold> Spatial distribution of between-movie ISC variability across the brain. A significant main effect of Movie was found for 97 out of 210 cortical parcels (46.2%). The highest variability was observed in primary and associative visual cortices, as well as in primary and associative auditory cortices. Significant variability was also found in higher-order regions such as the precuneus and inferior parietal lobule. <bold>C)</bold> Brain regions with high between-movie variability, such as the superior temporal gyrus, tended to show relatively higher ISC levels, while regions with low variability, such as the parahippocampal gyrus, showed lower ISC. <bold>D)</bold> A strong positive correlation between ISC level (y-axis) and between-movie variability (x-axis) across brain regions indicates that regions with higher ISC also exhibit greater variability across movies.</p></caption><graphic xlink:href="EMS201767-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Between-movie ISC variability leads to varying IS-RSA associations between behavior and neural synchronization.</title><p>We used inter-subject representational similarity analysis (IS-RSA) to identify brain regions where similarity in conceptual representations of novel objects was associated with similarity in neural synchronization during movie watching. The results revealed distinct sets of brain regions for each movie (family-wise error-adjusted <italic>p</italic> &lt; 0.05). These findings indicate that the association between behavior and ISC varies substantially across stimuli, limiting the generalizability of IS-RSA results based on a single movie clip.</p></caption><graphic xlink:href="EMS201767-f004"/></fig></floats-group></article>