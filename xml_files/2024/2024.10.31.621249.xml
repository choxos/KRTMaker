<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS199793</article-id><article-id pub-id-type="doi">10.1101/2024.10.31.621249</article-id><article-id pub-id-type="archive">PPR933389</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>WTools: a MATLAB-based toolbox for time-frequency analysis</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ferrari</surname><given-names>Ambra</given-names></name><email>ambra.ferrari@unitn.it</email><xref ref-type="aff" rid="A1">a</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Filippin</surname><given-names>Luca</given-names></name><email>luca.filippin@gmail.com</email><xref ref-type="aff" rid="A2">b</xref></contrib><contrib contrib-type="author"><name><surname>Buiatti</surname><given-names>Marco</given-names></name><email>marco.buiatti@unitn.it</email><xref ref-type="aff" rid="A3">c</xref></contrib><contrib contrib-type="author"><name><surname>Parise</surname><given-names>Eugenio</given-names></name><email>eugenio.parise@unitn.it</email><xref ref-type="aff" rid="A4">d</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>a</label>CIMeC, Center for Mind/Brain Sciences, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/05trd4x28</institution-id><institution>University of Trento</institution></institution-wrap>, <addr-line>Corso Bettini 31</addr-line>, <postal-code>38068</postal-code>, <city>Rovereto</city>, <country country="IT">Italy</country>; <institution-wrap><institution-id institution-id-type="ror">https://ror.org/00671me87</institution-id><institution>Max Plank Institute for Psycholinguistics</institution></institution-wrap>, <addr-line>Wundtlaan 1</addr-line>, <postal-code>6525 XD</postal-code>, <city>Nijmegen</city>, <country country="NL">The Netherlands</country></aff><aff id="A2"><label>b</label>CIMeC, Center for Mind/Brain Sciences, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/05trd4x28</institution-id><institution>University of Trento</institution></institution-wrap>, <addr-line>Corso Bettini 31</addr-line>, <postal-code>38068</postal-code>, <city>Rovereto</city>, <country country="IT">Italy</country></aff><aff id="A3"><label>c</label>CIMeC, Center for Mind/Brain Sciences, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/05trd4x28</institution-id><institution>University of Trento</institution></institution-wrap>, <addr-line>Piazza Manifattura 1</addr-line>, <postal-code>38068</postal-code>, <city>Rovereto</city>, <country country="IT">Italy</country></aff><aff id="A4"><label>d</label>CIMeC, Center for Mind/Brain Sciences, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/05trd4x28</institution-id><institution>University of Trento</institution></institution-wrap>, <addr-line>Corso Bettini 31</addr-line>, <postal-code>38068</postal-code>, <city>Rovereto</city>, <country country="IT">Italy</country></aff><author-notes><corresp id="CR1"><label>*</label>Corresponding authors</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>02</day><month>11</month><year>2024</year></pub-date><pub-date pub-type="preprint"><day>31</day><month>10</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Electroencephalography (EEG) is an established method for investigating neurocognitive functions during human development. In cognitive neuroscience, time-frequency analysis of the EEG is a widely used analytical approach. This paper introduces WTools, a new MATLAB-based toolbox capable of performing time-frequency analysis of EEG signals using complex wavelet transformation. WTools features an intuitive GUI that guides users through the analysis steps, focusing on essential parameters. Being free and open-source, it can be integrated and expanded with new features, making it a handy tool that is growing its popularity in developmental cognitive neuroscience. Here, we provide a detailed description of the WTools algorithm for wavelet transformation and we compare it with state-of-the-art methods implemented in EEGLAB. Alongside the official tool release, we also offer a comprehensive illustrated tutorial to enhance accessibility and promote the transparent and reproducible usage of WTools by the scientific community.</p></abstract><kwd-group><kwd>EEG</kwd><kwd>Time-frequency analysis</kwd><kwd>Morlet wavelet</kwd><kwd>Infants</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><label>1</label><title>Introduction</title><p id="P2">Electroencephalography (EEG) is an established and practical tool for studying brain function and dysfunction across the lifespan. Over the past decades, time-frequency analysis of the EEG has become increasingly popular in cognitive neuroscience. Often implemented via wavelet transformation (<xref ref-type="bibr" rid="R6">Cohen, 2014</xref>; <xref ref-type="bibr" rid="R7">Csibra et al., 2000</xref>; <xref ref-type="bibr" rid="R21">Jensen and Tesche, 2002</xref>), time-frequency analysis attempts to unfold the EEG signal into the frequency domain. Different from pure Fourier transformation, it includes some temporal support, allowing us to see the frequencies in the EEG signal spread over time relative to a time-locked stimulation. Consequently, time-frequency analysis can finely characterize the temporal dynamics embedded in EEG oscillations in terms of frequency, power, and phase (<xref ref-type="bibr" rid="R6">Cohen, 2014</xref>). This analysis provides invaluable information on a variety of cognitive processes such as attention (<xref ref-type="bibr" rid="R20">Jensen and Mazaheri, 2010</xref>; <xref ref-type="bibr" rid="R23">Klimesch, 2012</xref>), learning and prediction of upcoming information (<xref ref-type="bibr" rid="R1">Arnal and Giraud, 2012</xref>; <xref ref-type="bibr" rid="R12">Engel et al., 2001</xref>), memory (<xref ref-type="bibr" rid="R15">Griffiths and Jensen, 2023</xref>; <xref ref-type="bibr" rid="R18">Hanslmayr et al., 2016</xref>), language (<xref ref-type="bibr" rid="R11">Drijvers and Mazzini, 2023</xref>; <xref ref-type="bibr" rid="R34">Poeppel, 2014</xref>; <xref ref-type="bibr" rid="R41">Wang et al., 2012</xref>) and motor activation (<xref ref-type="bibr" rid="R4">Brittain and Brown, 2014</xref>; <xref ref-type="bibr" rid="R29">Muthukumaraswamy et al., 2004</xref>), pointing to the centrality of cortical rhythms in cognition (<xref ref-type="bibr" rid="R5">Buzsáki and Draguhn, 2004</xref>; <xref ref-type="bibr" rid="R42">Wang, 2010</xref>).</p><p id="P3">Over the past two decades, time-frequency analysis of the electroencephalogram (EEG) has become increasingly popular in the field of developmental cognitive neuroscience (<xref ref-type="bibr" rid="R3">Begus et al., 2016</xref>; <xref ref-type="bibr" rid="R2">Begus and Bonawitz, 2020</xref>; <xref ref-type="bibr" rid="R7">Csibra et al., 2000</xref>; <xref ref-type="bibr" rid="R22">Kaufman et al., 2005</xref>; <xref ref-type="bibr" rid="R25">Maguire and Abel, 2013</xref>; <xref ref-type="bibr" rid="R26">Meyer et al., 2020</xref>; <xref ref-type="bibr" rid="R38">Southgate et al., 2010</xref>; <xref ref-type="bibr" rid="R43">Xie et al., 2018</xref>). Yet, most studies in this field currently rely on ERPs and Fourier-based power, possibly because time-frequency analyses can be both computationally intensive and analytically complex (<xref ref-type="bibr" rid="R27">Morales and Bowers, 2022</xref>). Different from event-related potential (ERP) analysis, time-frequency analysis can be done with many different mathematical approaches (Fast Fourier Transformation, Hilbert transformation, wavelet transformation) and requires the manipulation of several parameters. Different approaches and small changes in the parameters can lead to very different results.</p><p id="P4">There is a large number of free software (e.g. EEGLAB: <xref ref-type="bibr" rid="R10">Delorme and Makeig, 2004</xref>; MNE-Python: <xref ref-type="bibr" rid="R13">Gramfort et al., 2013</xref>; ERPWAVELAB: <xref ref-type="bibr" rid="R28">Mørup et al., 2007</xref>; FieldTrip: <xref ref-type="bibr" rid="R30">Oostenveld et al., 2011</xref>) and commercial software (NetStation, Brain Vision Analyzer) to perform time-frequency analysis of the EEG. However, commercial software does not disclose the exact algorithm for the signal processing. As a result, it is virtually impossible to understand what the software does exactly, undermining flexibility, transparency and reproducibility. Free software on the other hand can be difficult to use because of the lack of a graphic user interface (GUI), or because they require to manually set several parameters. By developing in complexity and integrating an increasing number of functionalities, these softwares are difficult to use for non-experienced researchers and students.</p><p id="P5">This paper presents WTools, a new MATLAB-based toolbox for time-frequency analysis of the EEG signal. WTools is designed to be very user-friendly through its simple GUI; it is easy to learn and use because it keeps the number of parameters to manipulate at a minimum; it is open source and flexible as the code is freely available. WTools uses wavelet transformation and the algorithm is fully described. It works in combination with EEGLAB, exploiting its functionalities to import several different EEG data formats, as well as some of the EEGLAB plotting functionalities. Its data structure is derived from ERPWAVELAB (<xref ref-type="bibr" rid="R28">Mørup et al., 2007</xref>) and therefore WTools files are fully compatible with ERPWAVELAB. It handles multi-channel time-frequency analysis and multi-subject projects. It computes within-subject time-frequency differences among conditions and plots the results with time-frequency plots, as well as 2D and 3D scalp maps. It is possible to integrate and extend the toolbox with personalized configurations for developmental research (i.e. newborn and infant scalp maps). Finally, WTools includes a function to export numerical datasets into tabulated text files that can be easily analysed with any statistical software. For these reasons combined, an increasing number of studies in the field of developmental cognitive neuroscience have adopted beta versions of WTools before its official release (<xref ref-type="bibr" rid="R9">de Klerk et al., 2016</xref>; <xref ref-type="bibr" rid="R24">Kolesnik et al., 2019</xref>; <xref ref-type="bibr" rid="R31">Ortiz-Barajas et al., 2023</xref>; <xref ref-type="bibr" rid="R33">Piccardi et al., 2020</xref>; <xref ref-type="bibr" rid="R35">Pomiechowska and Csibra, 2017</xref>; <xref ref-type="bibr" rid="R37">Quadrelli et al., 2021</xref>, <xref ref-type="bibr" rid="R36">2019</xref>; <xref ref-type="bibr" rid="R39">Southgate and Vernetti, 2014</xref>; <xref ref-type="bibr" rid="R44">Yin et al., 2020</xref>). With the present work, we aim to formalize the toolbox, publicly release it together with a clear, step-by-step user-friendly tutorial and therefore promote its usage from the wider developmental cognitive neuroscience community and beyond.</p><p id="P6">To provide a comprehensive overview of WTools, the paper is structured as follows. First, we describe how WTools implements wavelet transformation and how the signal is processed. Then, we illustrate the main visualization functionalities of WTools. Finally, we compare its time-frequency functionality with state-of-the-art methods implemented in EEGLAB, an established popular free software (<xref ref-type="bibr" rid="R10">Delorme and Makeig, 2004</xref>). WTools scripts are made freely available for the users (under GPL-3.0 license) on GitHub (github.com/cogdevtools/WTools). A user-friendly step-by-step tutorial of the entire pipeline for the use of WTools is provided on GitHub, along with an anonymized example dataset (see section “WTools availability”).</p></sec><sec id="S2" sec-type="materials | methods"><label>2</label><title>Materials and Methods</title><sec id="S3"><label>2.1</label><title>WTools description</title><p id="P7">The processing pipeline (<xref ref-type="fig" rid="F1">Fig. 1</xref>) consists of a sequence of steps, each controlled through a GUI and executed by an independent code module. The paper focuses mainly on the core analytical section of the toolbox. Accordingly, in the following sections we describe the wavelet transformation implemented in WTools. The description is kept at the minimum possible complexity to provide a simple and clear explanation of how WTools transforms the EEG signal into the time-frequency domain.</p><sec id="S4"><label>2.1.1</label><title>Morlet complex wavelets</title><p id="P8">A wavelet (small wave) is a mathematical function of time (<italic>t</italic>) and frequency (<italic>f</italic>), with the following two features. First, its amplitude approaches zero at positive and negative infinity and it differs from zero only around the origin of the cartesian axes. That is, differently from other mathematical functions such as sinusoidal or infinite waves, a wavelet has a defined temporal length. Second, the sum of the area of a wavelet above the x axis is equal to the sum of the area of the wavelet below the x axis, hence the total sum of the area of a wavelet is equal to zero. Therefore, differently from a filter, a wavelet will not change the power spectrum of a signal. WTools computes Morlet complex wavelets at a 1 Hz sampling frequency according to: <disp-formula id="FD1"><mml:math id="M1"><mml:mrow><mml:mi>w</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo>∗</mml:mo><mml:mi>exp</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mi>π</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula> where <inline-formula><mml:math id="M2"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>n</mml:mi><mml:mtext>cycles</mml:mtext><mml:mo>/</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>exp</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> is a Gaussian bell curve, <italic>exp</italic>(2<italic>iπ f t</italic>) is a complex sinusoid. Wavelets are by default normalized so that their total energy is 1, the normalization factor <italic>A</italic> being equal to <inline-formula><mml:math id="M3"><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msqrt><mml:mi>π</mml:mi></mml:msqrt></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="R40">Tallon-Baudry et al., 1996</xref>). This is common practice in time-frequency analysis (e.g. in EEGLAB), which we strongly recommend as it allows direct quantitative comparisons of frequency content across frequency values. Nevertheless, we still provide the option to omit wavelet normalization for back-compatibility with previous (beta) versions of WTools. As a sanity check, there is a perfect match between the wavelets specified by WTools and EEGLAB once wavelet normalization is applied to the WTools wavelet (<xref ref-type="fig" rid="F2">Fig. 2</xref>).</p><p id="P9">The last term of the equation can be written as a sum of <italic>sin</italic> and <italic>cos</italic> waves: <disp-formula id="FD2"><mml:math id="M4"><mml:mrow><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mi>π</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>cos</mml:mi><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mi>sin</mml:mi><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></disp-formula></p><p id="P10">In practice, a wavelet is a complex sinusoidal wave enveloped into a Gaussian bell curve. Because a wavelet is a function depending on a frequency <italic>f</italic>, at each <italic>f</italic> the corresponding wavelet works as a magnifier that can reveal how much that frequency is present into the EEG signal. At the same time, because a wavelet has a defined temporal length, it can preserve some time information after the transformation of the signal.</p><p id="P11">Wavelets are wide at lower frequencies, that is the temporal information is relatively imprecise but the frequency information is preserved. Wavelets become progressively narrower at higher frequencies, that is the time information is more accurate but the frequency information is less precise. Time and frequency information respond to Heisenberg's indetermination principle: the more accurately we know one of them, the less accurately we know the other. Wavelet transformation attempts to find the best possible compromise to squeeze as much information as possible in terms of both time and frequency, depending on the frequency of interest.</p></sec><sec id="S5"><label>2.1.2</label><title>Time-frequency transformation</title><p id="P12">After computing all the wavelets for each frequency in a range of interest, WTools uses continuous wavelet transformation. By convolving the signal <italic>s(t)</italic> (cleared of its DC component) with the wavelet <italic>w(t, f)</italic> and taking the modulus of the resulting complex coefficients, WTools obtains the coefficients (the mean square root) of the wavelet transformation at a specific frequency <italic>f</italic>. The resulting vector represents how well the signal fits the wavelet at a the frequency <italic>f</italic>, that is how much of the frequency <italic>f</italic> is in that signal (<xref ref-type="bibr" rid="R19">Herrmann et al., 2005</xref>). WTools performs the convolution for each experimental condition and channel, of each EEG segment and subject, for all frequencies of interest. Transformed segments are then averaged across trials: <disp-formula id="FD3"><mml:math id="M5"><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>W</mml:mi><mml:mi>T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mstyle><mml:mi>∑</mml:mi></mml:mstyle><mml:mi>n</mml:mi><mml:mi>N</mml:mi></mml:munderover><mml:mi>X</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></disp-formula> where <italic>X</italic>(<italic>c, f, t, n</italic>) is the time-frequency coefficient at channel <italic>c</italic>, frequency <italic>f</italic>, time <italic>t</italic> and segment <italic>n</italic> of the EEG signal given by <italic>X</italic>(<italic>c, t, n</italic>). Following the ERPWAVELAB data structure (<xref ref-type="bibr" rid="R28">Mørup et al., 2007</xref>), the resulting MATLAB variable “WT” is a 3D matrix (Channel × Frequency × Time) that contains the average of the transformed segments. Notice that, since WTools takes the modulus of the complex coefficients, the data are still expressed in <italic>μ</italic>V rather than in power (i.e. <italic>μ</italic>V<sup>2</sup>). As a result, the measure is slightly more sensitive to small frequency changes.</p><p id="P13">WTools allows computing both total-induced and evoked oscillations (<xref ref-type="bibr" rid="R8">David et al., 2006</xref>; <xref ref-type="bibr" rid="R40">Tallon-Baudry et al., 1996</xref>). Total-induced oscillations are computed by wavelet transformation of each epoch before averaging all of them together (i.e. the pipeline we just described above): in this way, the oscillatory activity non-phase-locked to the onset of the stimulus is preserved. Total-induced oscillations are richer measures compared to evoked oscillations. Evoked oscillations are computed by running the wavelet transformation after averaging all the epochs together, that is the wavelet transformation is performed on the average (i.e. it is the wavelet transformation of the ERP): in this way, the oscillatory activity that is non-phase-locked to the stimulus onset is averaged out and lost before the wavelet transformation. Evoked oscillations are poorer measures compared to total-induced oscillations and are slightly more sensitive to low-level features of the stimulus.</p></sec><sec id="S6"><label>2.1.3</label><title>Edges chopping</title><p id="P14">The continuous wavelet transformation assumes signal continuity, yet EEG segments inherently lack this continuity: at the segment's start, a shift from no signal to signal with amplitude occurs, mirrored at the segment's end, causing significant frequency changes. This discontinuity distorts coefficient vectors at the edges, particularly noticeable at lower frequencies where wavelets are broader (see <xref ref-type="fig" rid="F2">Fig. 2</xref>). To address the impact of this distortion on EEG signal analysis, it is advisable to pre-cut wider segments. Alternatively, WTools offers an edge padding feature to compensate for signal absence at segment edges. Based on empirical evaluation, the minimally necessary length of extra signal needed at each edge can be estimated with the current formula (Gergely Csibra, personal communication): <disp-formula id="FD4"><mml:math id="M6"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2000</mml:mn><mml:mo>/</mml:mo><mml:mi>min</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>f</italic> is the frequency range of interest. For instance, if one wants to look at frequencies between 5-15 Hz, then it is necessary to consider <italic>t</italic> = 400 ms of extra signal at both left and right edges of the segment.</p></sec></sec><sec id="S7"><label>2.1.4</label><title>Baseline correction</title><p id="P15">Differently from other toolboxes that adopt a divisive baseline (<xref ref-type="bibr" rid="R14">Grandchamp and Delorme, 2011</xref>), WTools performs a subtractive baseline similarly to baseline correction for ERPs. The average amplitude in a given time window of the pre-stimulus interval is computed and subtracted from the whole time-varying signal. Notice that there is not a common baseline for all frequencies; instead, each frequency of interest has its own baseline value that is used for correction only for that frequency. This approach makes the resulting spectrograms easy to read and highly comparable to ERPs, which is particularly handy for developmental research. Furthermore, adopting a divisive baseline assumes a multiplicative model, in which oscillations and 1 / <italic>f</italic> activity scale by the same factor relative to baseline for each frequency (<xref ref-type="bibr" rid="R17">Gyurkovics et al., 2021</xref>). This assumption may be incorrect when signal and noise contribute independently to the power spectrum (i.e. additive model; <xref ref-type="bibr" rid="R17">Gyurkovics et al., 2021</xref>), a very common problem in developmental studies for which a subtractive baseline may be more suitable.</p></sec><sec id="S8"><label>2.2</label><title>Dataset description</title><p id="P16">To examine the time-frequency functionality of WTools, we employed a dataset from a previously published study investigating EEG responses to multimodal ostensive signals in 5-month-old infants (Experiment 1 in <xref ref-type="bibr" rid="R32">Parise and Csibra, 2013</xref>). The study was approved by the United Ethical Review Committee for Research in Psychology (EPKEB) at Central European University, and the parents of all participants provided written informed consent. Eighteen infants (9 females; average age=148.17 days, range=136 to 157 days) were included in the study. All infants were born full-term (gestational age: 37 to 41 weeks) and in the normal weight range (&gt;2500 g). We provide free access to an anonymized dataset containing three representative participants from this previously published study (see section “WTools availability”). The figures contained in the present paper pertain to one of these participants.</p><p id="P17">The experiment conformed to a 2×2 within-subject factorial design, corresponding to the orthogonal crossing of the factors Modality (visual vs. auditory) and Ostension (ostensive vs. non-ostensive). As a result, the experiment included four within-subject conditions: ostensive visual stimulus of direct gaze (DG); non-ostensive visual stimulus of averted gaze (AG); ostensive auditory stimulus of infant-directed speech (IDS); non-ostensive auditory stimulus of adult-directed speech (ADS).</p><p id="P18">Visual stimuli (female faces) were presented on a 19-inch CRT monitor operating at 100 Hz refresh rate using PsychToolBox (v. 3.0.8) and custom-made MATLAB scripts. Auditory stimuli (pseudo-words pronounced by a female voice) were presented by a pair of computer speakers located behind the monitor. A remote-controlled video camera located below the monitor allowed the recording of infants' behaviour during the experiment. For further details on the stimuli and setup, please refer to <xref ref-type="bibr" rid="R32">Parise and Csibra, 2013</xref>.</p><p id="P19">Infants sat on their parent’s lap at 70 cm from the monitor. At the beginning of each trial, a dynamic visual stimulus appeared on top of the face with closed eyes for 600 ms to grab the infant’s attention. Afterwards, the attention grabber stopped moving and stayed on screen for a random interval between 600 and 800 ms. The attention grabber then disappeared and a visual (DG or AG) or auditory (IDS or ADS) stimulus was presented for 1000 ms. An inter-trial interval between 1100 and 1300 ms was inserted between successive trials. Infants were presented with a maximum of 192 trials divided into 4 blocks. The behaviour of the infants was video-recorded throughout the session for offline trial-by-trial editing.</p><p id="P20">High-density EEG was recorded continuously using Hydrocel Geodesic Sensor Nets (Electrical Geodesics Inc., Eugene, OR, USA) at 124 scalp locations referenced to the vertex (Cz). The ground electrode was at the rear of the head (between Cz and Pz). Electrophysiological signals were acquired at the sampling rate of 500 Hz by an Electrical Geodesics Inc. amplifier with a band-pass filter of 0.1–200 Hz.</p></sec><sec id="S9"><label>2.3</label><title>Data analysis</title><p id="P21">The digitized EEG was band-pass filtered between 0.3-100 Hz and was segmented into epochs including 500 ms before stimulus onset and 1500 ms following stimulus onset for each trial. EEG epochs were automatically rejected for body and eye movements whenever the average amplitude of an 80 ms gliding window exceeded 55 µV at horizontal EOG channels or 200 µV at any other channel. Additional rejection of bad recording was performed by visual inspection of each individual epoch. Bad channels were interpolated in epochs in which ≤10% of the channels contained artefacts; epochs in which &gt;10% of the channels contained artefacts were rejected. Infants contributed on average 12.11 artefact-free trials to the DG condition (range: 10 to 19), 11.67 to the AG condition (10 to 15), 11.67 to the IDS condition (10 to 19), 12.61 to the ADS condition (10 to 22). The artefact-free segments were subjected to time-frequency analysis using WTools and EEGLAB respectively.</p><sec id="S10"><label>2.3.1</label><title>WTools analysis</title><p id="P22">To import data into WTools, epochs were transformed from the native EGI format to the MATLAB format using the appropriate NetStation Waveform export tool by EGI, and then the correspondent EEGLAB import plugin (v. 2021.1). In general, WTools calls existing EEGLAB plugins to import data. Each plugin takes as input a specific data format. Users must save the data in a format that is compatible with the EEGLAB plugin of interest, which is called by the WTools import routine (see guidelines: eeglab.org/tutorials/04_Import/Import.html). Note that data to be imported into WTools must conform to an event-related within-subject design with one single trigger per segment/epoch indicating the onset of the key event. Epochs were re-referenced to average reference and then subjected to time-frequency analysis. Data were analysed using both the standard WTools pipeline (whose code is released with this publication) and custom pipelines that allowed a direct comparison with the EEGLAB output, as outlined below.</p><p id="P23">In the following, we describe the standard WTools pipeline. We computed complex Morlet wavelets for the frequencies 10-90 Hz in 1 Hz steps. We calculated total-induced oscillations by performing a continuous wavelet transformation of all the epochs using convolution with each normalized wavelet (number of cycles = 7) and taking the absolute value (i.e., the amplitude) of the results (<xref ref-type="bibr" rid="R7">Csibra et al., 2000</xref>). To remove the distortion introduced by the convolution, we chopped 300 ms at each edge of the epochs, resulting in 1400 ms long segments, including 200 ms before and 1200 ms after stimulus onset. We used the average amplitude of the 200 ms pre-stimulus window as baseline, subtracting it from the whole epoch at each frequency.</p><p id="P24">To compare the WTools output to the EEGLAB output, we carried out a custom analysis that deviated from the standard WTools pipeline in two ways. First, we took the squared value (i.e. the power) of the complex wavelet coefficient, in line with the EEGLAB pipeline. Second, we skipped baseline correction to avoid a mismatch between the two toolboxes (while WTools performs a subtractive baseline, EEGLAB implements a divisive baseline).</p></sec></sec><sec id="S11"><label>2.3.2</label><title>EEGLAB analysis</title><p id="P25">Epochs were transformed from the native EGI format to the MATLAB format using the NetStation export tool and then imported by using the correspondent EEGLAB import plugin (v. 2021.1). Epochs were re-referenced to average reference and then subjected to time-frequency analysis. Data were analysed using various EEGLAB pipelines that allowed a direct comparison with the WTools output. For all analyses, we computed Morlet complex wavelets for the frequencies 10-90 Hz in 1 Hz steps. We calculated total-induced oscillations by performing a continuous wavelet transformation of all the epochs using convolution with each normalized wavelet (number of cycles = 7) and taking the squared value (i.e., the power) of the complex coefficients. Data were automatically chopped by EEGLAB to remove the distortion introduced by the convolution, resulting in 1400 ms long segments, including 200 ms before and 1200 ms after stimulus onset. For baseline correction, we entered the 200 ms pre-stimulus window as baseline.</p><p id="P26">To compare the EEGLAB output to the WTools output, we carried out a custom analysis that deviated from the standard EEGLAB pipeline in two ways. First, we did not log-transform the results of the wavelet transformation (typically done to bring the results to the standard dB scale). Second, we skipped baseline correction to avoid a mismatch between the two toolboxes.</p></sec></sec><sec id="S12" sec-type="results"><label>3</label><title>Results</title><sec id="S13"><label>3.1</label><title>Illustrating WTools</title><p id="P27">We illustrate the main visualization functionalities available in WTools using results obtained under the standard WTools pipeline. For details of all the available visualization settings, please refer to the step-by-step tutorial of the entire pipeline that is provided on GitHub (see section “WTools availability”). Three main types of plots are available: time-frequency graphs, 2D scalp maps and 3D scalp maps. For time-frequency graphs, it is possible to plot results from individual channels, from the average of a desired subset of channels, or from all available channels in a topographical arrangement. For each of these graphs, users can also plot the standard error of the effect relative to baseline, which allows a preliminary descriptive evaluation of the effect size. For 2D and 3D scalp maps, please note that WTools expects to assign a 2D topoplot configuration to any given dataset during the initial data import routine. Optionally, the user can decide to additionally select a spline configuration for 3D scalp maps. Currently, WTools offers several 2D and 3D configuration files (ANT, BIOSEMI, GSN) for 2D and 3D maps of adult and infant data. It is possible to integrate and extend the toolbox with personalized configurations for developmental research (i.e. newborn and infant scalp maps). For 2D scalp maps, the user can obtain a plot corresponding to various possibilities: one timepoint at one specific frequency; average of timepoints at a specific frequency; average of a frequency range at one timepoint. It is also possible to plot a series of scalp maps, either for time or frequency series. For 3D scalp maps, the user can similarly obtain a plot corresponding to various possibilities: one timepoint at one certain frequency; average of timepoints at a certain frequency; average of a frequency range at one timepoint. Finally, all types of graphs allow plotting results of individual conditions or results of user-defined pair-wise differences between conditions.</p><p id="P28">For illustrative purposes, we show the time-frequency plots and 2D scalp maps for one representative participant and channel both for individual conditions and for their difference (<xref ref-type="fig" rid="F3">Fig. 3</xref>). Furthermore, we show the 3D infant scalp map of the same difference (<xref ref-type="fig" rid="F4">Fig. 4</xref>). Results show a higher synchronization for Condition 1 (DG) than Condition 2 (AG) in the gamma frequency-band at 250-350 ms post-stimulus. This result is representative of the final finding of the original paper (<xref ref-type="bibr" rid="R32">Parise and Csibra, 2013</xref>), which replicates and extends previous work (<xref ref-type="bibr" rid="R16">Grossmann et al., 2007</xref>). Time, frequency and scale ranges are user-defined before all plotting.</p><p id="P29">Finally, WTools has a built-in functionality that allows exporting summary data for later statistical inference. Specifically, users can extract numeric values for each desired subject, condition and channel, computing the average value both in a time window and in a frequency range (via a double average). Values are stored in a text-tabulated file (.tab) that can be treated directly by third-party statistical packages.</p></sec><sec id="S14"><label>3.2</label><title>Comparison with EEGLAB</title><p id="P30">The analysis aimed to examine the time-frequency functionality of WTools via comparison with state-of-the-art methods implemented in EEGLAB, an established popular free software (<xref ref-type="bibr" rid="R10">Delorme and Makeig, 2004</xref>). First, we report the time-frequency results for one representative participant (“04”), channel (“22”, corresponding to Fp2) and condition (“DG”) under the custom pipelines that allowed a direct comparison of the WTools and EEGLAB outputs. Second, we report the time-frequency results under the standard WTools and EEGLAB pipelines, which are practically used in real research settings.</p><p id="P31">By removing baseline correction and taking the squared value of the complex wavelet coefficient (see section “Data analysis” for further details), the WTools output perfectly matched the EEGLAB output (<xref ref-type="fig" rid="F5">Fig. 5A</xref>). This analysis confirms that WTools produces time-frequency results in line with state-of-the-art EEGLAB outputs (our gold standard for the present work), representing an important sanity check. Then, as expected, the standard pipelines generated slightly different results across WTools and EEGLAB (<xref ref-type="fig" rid="F5">Fig. 5B</xref>). Importantly, the two toolboxes still showed qualitatively compatible results, with a pronounced increase of induced gamma-band activity over the forehead (channel 22, corresponding to Fp2) in the 250 to 350 ms time window, and 30 to 40 Hz frequency window (<xref ref-type="bibr" rid="R32">Parise and Csibra, 2013</xref>). Please see the Supplementary material for a comprehensive review of the results under additional analyses pipelines, which derived from the orthogonal crossing of the settings being manipulated (i.e. baseline correction, complex wavelet manipulation, log-transformation).</p></sec></sec><sec id="S15" sec-type="discussion"><label>4</label><title>Discussion</title><p id="P32">This paper introduced WTools, a new MATLAB-based toolbox for time-frequency analysis of the EEG signal that implements complex wavelet transformation. We provided a detailed description of the WTools algorithm for wavelet transformation, as well as an overview of the main visualization functionalities for data exploration and characterization. Crucially, we compared WTools with state-of-the-art methods implemented in EEGLAB (<xref ref-type="bibr" rid="R10">Delorme and Makeig, 2004</xref>). Finally, we created a step-by-step illustrated and user-friendly tutorial of the entire processing pipeline for easy and transparent access to the toolbox’s functionalities (see section “WTools availability”).</p><p id="P33">In terms of usability, WTools features a straightforward, transparent and user-friendly GUI that is particularly handy for non-advanced users (with little to no experience in coding). Furthermore, WTools guarantees high flexibility in data handling, since it takes a huge range of input formats thanks to its direct compatibility with EEGLAB: any dataset that can be imported into EEGLAB can also be imported into WTools.</p><p id="P34">In terms of data analysis, WTools produces time-frequency results in units of <italic>μ</italic>V and it performs a subtractive baseline correction, very similar to baseline correction for ERPs. This approach makes the resulting spectrograms easy to read and highly comparable to ERPs, which is particularly handy for developmental research. Plotting functions allow for a straightforward visualization and evaluation of the data in terms of time-frequency graphs, 2D and 3D scalp maps (including both infant and adult 3D configurations). Users can also plot the standard error of an effect relative to baseline, which allows a preliminary descriptive evaluation of the effect size. Finally, WTools features a built-in tool for exporting numerical data that can be treated directly by third-party statistical packages.</p><p id="P35">For these reasons combined, an increasing number of studies in the field of developmental cognitive neuroscience have adopted beta versions of WTools (<xref ref-type="bibr" rid="R9">de Klerk et al., 2016</xref>; <xref ref-type="bibr" rid="R24">Kolesnik et al., 2019</xref>; <xref ref-type="bibr" rid="R31">Ortiz-Barajas et al., 2023</xref>; <xref ref-type="bibr" rid="R33">Piccardi et al., 2020</xref>; <xref ref-type="bibr" rid="R35">Pomiechowska and Csibra, 2017</xref>; <xref ref-type="bibr" rid="R37">Quadrelli et al., 2021</xref>, <xref ref-type="bibr" rid="R36">2019</xref>; <xref ref-type="bibr" rid="R39">Southgate and Vernetti, 2014</xref>; <xref ref-type="bibr" rid="R44">Yin et al., 2020</xref>). With the current official release, we further foster the transparent and reproducible usage of WTools by the developmental cognitive neuroscience community and beyond.</p></sec><sec id="S16"><title>WTools availability</title><p id="P36">WTools is publicly available as open-source software on GitHub (github.com/cogdevtools/WTools/tree/v2.0) under GNU General Public License (GPL-3.0). A step-by-step tutorial for the use of WTools is provided in the wiki section of the GitHub project. An example anonymized dataset (data from <xref ref-type="bibr" rid="R32">Parise and Csibra, 2013</xref>) has been deposited in the Open Science Framework and is freely available at osf.io/jtudr.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Figure S1</label><media xlink:href="EMS199793-supplement-Supplementary_Figure_S1.pdf" mimetype="application" mime-subtype="pdf" id="d95aAcFbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S17"><title>Acknowledgements</title><p>When developing WTools, E.P. and L.F. were supported by an Advanced Investigator Grant (#249519, OSTREFCOM) from the European Research Council to Gergely Csibra. E.P. was partially supported by the International Centre for Language and Communicative Development (LuCiD) at Lancaster University, funded by the Economic and Social Research Council (UK) [ES/L008955/1]. We thank John E. Richards for providing E.P. with the MRI scan to create the 3D infant head model embedded in WTools. We thank Teodora Gliga and Morgan Whitworth for testing preliminary versions of this WTools release.</p></ack><fn-group><fn id="FN1" fn-type="conflict"><p id="P37"><bold>Declaration of Competing Interest</bold></p><p id="P38">The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arnal</surname><given-names>LH</given-names></name><name><surname>Giraud</surname><given-names>A-L</given-names></name></person-group><article-title>Cortical oscillations and sensory predictions</article-title><source>Trends Cogn Sci</source><year>2012</year><volume>16</volume><fpage>390</fpage><lpage>398</lpage><pub-id pub-id-type="pmid">22682813</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Begus</surname><given-names>K</given-names></name><name><surname>Bonawitz</surname><given-names>E</given-names></name></person-group><article-title>The rhythm of learning: Theta oscillations as an index of active learning in infancy</article-title><source>Dev Cogn Neurosci</source><year>2020</year><volume>45</volume><elocation-id>100810</elocation-id><pub-id pub-id-type="pmcid">PMC7371744</pub-id><pub-id pub-id-type="pmid">33040970</pub-id><pub-id pub-id-type="doi">10.1016/j.dcn.2020.100810</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Begus</surname><given-names>K</given-names></name><name><surname>Gliga</surname><given-names>T</given-names></name><name><surname>Southgate</surname><given-names>V</given-names></name></person-group><article-title>Infants’ preferences for native speakers are associated with an expectation of information</article-title><source>Proc Natl Acad Sci</source><year>2016</year><volume>113</volume><fpage>12397</fpage><lpage>12402</lpage><pub-id pub-id-type="pmcid">PMC5098614</pub-id><pub-id pub-id-type="pmid">27791064</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1603261113</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brittain</surname><given-names>JS</given-names></name><name><surname>Brown</surname><given-names>P</given-names></name></person-group><article-title>Oscillations and the basal ganglia: Motor control and beyond</article-title><source>Neuroimage</source><year>2014</year><volume>85</volume><fpage>637</fpage><lpage>647</lpage><pub-id pub-id-type="pmcid">PMC4813758</pub-id><pub-id pub-id-type="pmid">23711535</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.05.084</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname><given-names>G</given-names></name><name><surname>Draguhn</surname><given-names>A</given-names></name></person-group><article-title>Neuronal Oscillations in Cortical Networks</article-title><source>Science (80-)</source><year>2004</year><volume>304</volume><fpage>1926</fpage><lpage>1929</lpage><pub-id pub-id-type="pmid">15218136</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>MX</given-names></name></person-group><source>Analyzing Neural Time Series Data, Analyzing Neural Time Series Data</source><publisher-name>The MIT Press</publisher-name><year>2014</year><pub-id pub-id-type="doi">10.7551/mitpress/9609.001.0001</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Csibra</surname><given-names>G</given-names></name><name><surname>Davis</surname><given-names>G</given-names></name><name><surname>Spratling</surname><given-names>MW</given-names></name><name><surname>Johnson</surname><given-names>MH</given-names></name></person-group><article-title>Gamma Oscillations and Object Processing in the Infant Brain</article-title><source>Science (80-)</source><year>2000</year><volume>290</volume><fpage>1582</fpage><lpage>1585</lpage><pub-id pub-id-type="pmid">11090357</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>David</surname><given-names>O</given-names></name><name><surname>Kilner</surname><given-names>JM</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><article-title>Mechanisms of evoked and induced responses in MEG/EEG</article-title><source>Neuroimage</source><year>2006</year><volume>31</volume><fpage>1580</fpage><lpage>1591</lpage><pub-id pub-id-type="pmid">16632378</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Klerk</surname><given-names>CCJM</given-names></name><name><surname>Southgate</surname><given-names>V</given-names></name><name><surname>Csibra</surname><given-names>G</given-names></name></person-group><article-title>Predictive action tracking without motor experience in 8-month-old infants</article-title><source>Brain Cogn</source><year>2016</year><volume>109</volume><fpage>131</fpage><lpage>139</lpage><pub-id pub-id-type="pmcid">PMC5090050</pub-id><pub-id pub-id-type="pmid">27693999</pub-id><pub-id pub-id-type="doi">10.1016/j.bandc.2016.09.010</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Delorme</surname><given-names>A</given-names></name><name><surname>Makeig</surname><given-names>S</given-names></name></person-group><article-title>EEGLAB: an open source toolbox for analysis of single-trial EEG dynamics including independent component analysis</article-title><source>J Neurosci Methods</source><year>2004</year><volume>134</volume><fpage>9</fpage><lpage>21</lpage><pub-id pub-id-type="pmid">15102499</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Drijvers</surname><given-names>L</given-names></name><name><surname>Mazzini</surname><given-names>S</given-names></name></person-group><chapter-title>Neural oscillations in audiovisual language and communication</chapter-title><person-group person-group-type="editor"><name><surname>Murray Sherman</surname><given-names>M</given-names></name></person-group><source>Oxford Research Encyclopedia of Neurocience</source><publisher-name>Oxford University Press</publisher-name><year>2023</year><fpage>1</fpage><lpage>45</lpage></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engel</surname><given-names>AK</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Singer</surname><given-names>W</given-names></name></person-group><article-title>Dynamic predictions: Oscillations and synchrony in top–down processing</article-title><source>Nat Rev Neurosci</source><year>2001</year><volume>2</volume><fpage>704</fpage><lpage>716</lpage><pub-id pub-id-type="pmid">11584308</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Luessi</surname><given-names>M</given-names></name><name><surname>Larson</surname><given-names>E</given-names></name><name><surname>Engemann</surname><given-names>DA</given-names></name><name><surname>Strohmeier</surname><given-names>D</given-names></name><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Goj</surname><given-names>R</given-names></name><name><surname>Jas</surname><given-names>M</given-names></name><name><surname>Brooks</surname><given-names>T</given-names></name><name><surname>Parkkonen</surname><given-names>L</given-names></name><name><surname>Hämäläinen</surname><given-names>M</given-names></name></person-group><article-title>MEG and EEG data analysis with MNE-Python</article-title><source>Front Neurosci</source><year>2013</year><volume>7</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="pmcid">PMC3872725</pub-id><pub-id pub-id-type="pmid">24431986</pub-id><pub-id pub-id-type="doi">10.3389/fnins.2013.00267</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grandchamp</surname><given-names>R</given-names></name><name><surname>Delorme</surname><given-names>A</given-names></name></person-group><article-title>Single-Trial Normalization for Event-Related Spectral Decomposition Reduces Sensitivity to Noisy Trials</article-title><source>Front Psychol</source><year>2011</year><volume>2</volume><fpage>236</fpage><pub-id pub-id-type="pmcid">PMC3183439</pub-id><pub-id pub-id-type="pmid">21994498</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2011.00236</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Griffiths</surname><given-names>BJ</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name></person-group><article-title>Gamma oscillations and episodic memory</article-title><source>Trends Neurosci</source><year>2023</year><volume>46</volume><fpage>832</fpage><lpage>846</lpage><pub-id pub-id-type="pmid">37550159</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grossmann</surname><given-names>T</given-names></name><name><surname>Johnson</surname><given-names>MH</given-names></name><name><surname>Farroni</surname><given-names>T</given-names></name><name><surname>Csibra</surname><given-names>G</given-names></name></person-group><article-title>Social perception in the infant brain: gamma oscillatory activity in response to eye gaze</article-title><source>Soc Cogn Affect Neurosci</source><year>2007</year><volume>2</volume><fpage>284</fpage><lpage>291</lpage><pub-id pub-id-type="pmcid">PMC2566755</pub-id><pub-id pub-id-type="pmid">18985134</pub-id><pub-id pub-id-type="doi">10.1093/scan/nsm025</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gyurkovics</surname><given-names>M</given-names></name><name><surname>Clements</surname><given-names>GM</given-names></name><name><surname>Low</surname><given-names>KA</given-names></name><name><surname>Fabiani</surname><given-names>M</given-names></name><name><surname>Gratton</surname><given-names>G</given-names></name></person-group><article-title>The impact of 1/f activity and baseline correction on the results and interpretation of time-frequency analyses of EEG/MEG data: A cautionary tale</article-title><source>Neuroimage</source><year>2021</year><volume>237</volume><elocation-id>118192</elocation-id><pub-id pub-id-type="pmcid">PMC8354524</pub-id><pub-id pub-id-type="pmid">34048899</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2021.118192</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanslmayr</surname><given-names>S</given-names></name><name><surname>Staresina</surname><given-names>BP</given-names></name><name><surname>Bowman</surname><given-names>H</given-names></name></person-group><article-title>Oscillations and Episodic Memory: Addressing the Synchronization/Desynchronization Conundrum</article-title><source>Trends Neurosci</source><year>2016</year><volume>39</volume><fpage>16</fpage><lpage>25</lpage><pub-id pub-id-type="pmcid">PMC4819444</pub-id><pub-id pub-id-type="pmid">26763659</pub-id><pub-id pub-id-type="doi">10.1016/j.tins.2015.11.004</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Herrmann</surname><given-names>CS</given-names></name><name><surname>Grigutsch</surname><given-names>M</given-names></name><name><surname>Busch</surname><given-names>NA</given-names></name></person-group><chapter-title>EEG oscillations and wavelet analysis</chapter-title><source>Event-Related Potentials: A Methods Handbook</source><publisher-name>MIT Press</publisher-name><year>2005</year></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jensen</surname><given-names>O</given-names></name><name><surname>Mazaheri</surname><given-names>A</given-names></name></person-group><article-title>Shaping functional architecture by oscillatory alpha activity: gating by inhibition</article-title><source>Front Hum Neurosci</source><year>2010</year><volume>4</volume><fpage>186</fpage><pub-id pub-id-type="pmcid">PMC2990626</pub-id><pub-id pub-id-type="pmid">21119777</pub-id><pub-id pub-id-type="doi">10.3389/fnhum.2010.00186</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jensen</surname><given-names>O</given-names></name><name><surname>Tesche</surname><given-names>CD</given-names></name></person-group><article-title>Frontal theta activity in humans increases with memory load in a working memory task</article-title><source>Eur J Neurosci</source><year>2002</year><volume>15</volume><fpage>1395</fpage><lpage>1399</lpage><pub-id pub-id-type="pmid">11994134</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaufman</surname><given-names>J</given-names></name><name><surname>Csibra</surname><given-names>G</given-names></name><name><surname>Johnson</surname><given-names>MH</given-names></name></person-group><article-title>Oscillatory activity in the infant brain reflects object maintenance</article-title><source>Proc Natl Acad Sci</source><year>2005</year><volume>102</volume><fpage>15271</fpage><lpage>15274</lpage><pub-id pub-id-type="pmcid">PMC1257741</pub-id><pub-id pub-id-type="pmid">16230640</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0507626102</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klimesch</surname><given-names>W</given-names></name></person-group><article-title>Alpha-band oscillations, attention, and controlled access to stored information</article-title><source>Trends Cogn Sci</source><year>2012</year><volume>16</volume><fpage>606</fpage><lpage>617</lpage><pub-id pub-id-type="pmcid">PMC3507158</pub-id><pub-id pub-id-type="pmid">23141428</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2012.10.007</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kolesnik</surname><given-names>A</given-names></name><name><surname>Begum Ali</surname><given-names>J</given-names></name><name><surname>Gliga</surname><given-names>T</given-names></name><name><surname>Guiraud</surname><given-names>J</given-names></name><name><surname>Charman</surname><given-names>T</given-names></name><name><surname>Johnson</surname><given-names>MH</given-names></name><name><surname>Jones</surname><given-names>EJH</given-names></name></person-group><article-title>Increased cortical reactivity to repeated tones at 8 months in infants with later ASD</article-title><source>Transl Psychiatry</source><year>2019</year><volume>9</volume><fpage>16</fpage><lpage>18</lpage><pub-id pub-id-type="pmcid">PMC6353960</pub-id><pub-id pub-id-type="pmid">30700699</pub-id><pub-id pub-id-type="doi">10.1038/s41398-019-0393-x</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maguire</surname><given-names>MJ</given-names></name><name><surname>Abel</surname><given-names>AD</given-names></name></person-group><article-title>What changes in neural oscillations can reveal about developmental cognitive neuroscience: Language development as a case in point</article-title><source>Dev Cogn Neurosci</source><year>2013</year><volume>6</volume><fpage>125</fpage><lpage>136</lpage><pub-id pub-id-type="pmcid">PMC3875138</pub-id><pub-id pub-id-type="pmid">24060670</pub-id><pub-id pub-id-type="doi">10.1016/j.dcn.2013.08.002</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname><given-names>M</given-names></name><name><surname>Endedijk</surname><given-names>HM</given-names></name><name><surname>Hunnius</surname><given-names>S</given-names></name></person-group><article-title>Intention to imitate: Top-down effects on 4-year-olds’ neural processing of others’ actions</article-title><source>Dev Cogn Neurosci</source><year>2020</year><volume>45</volume><pub-id pub-id-type="pmcid">PMC7481529</pub-id><pub-id pub-id-type="pmid">32890960</pub-id><pub-id pub-id-type="doi">10.1016/j.dcn.2020.100851</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morales</surname><given-names>S</given-names></name><name><surname>Bowers</surname><given-names>ME</given-names></name></person-group><article-title>Time-frequency analysis methods and their application in developmental EEG data</article-title><source>Dev Cogn Neurosci</source><year>2022</year><volume>54</volume><elocation-id>101067</elocation-id><pub-id pub-id-type="pmcid">PMC8784307</pub-id><pub-id pub-id-type="pmid">35065418</pub-id><pub-id pub-id-type="doi">10.1016/j.dcn.2022.101067</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mørup</surname><given-names>M</given-names></name><name><surname>Hansen</surname><given-names>LK</given-names></name><name><surname>Arnfred</surname><given-names>SM</given-names></name></person-group><article-title>ERPWAVELAB: A toolbox for multi-channel analysis of time–frequency transformed event related potentials</article-title><source>J Neurosci Methods</source><year>2007</year><volume>161</volume><fpage>361</fpage><lpage>368</lpage><pub-id pub-id-type="pmid">17204335</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muthukumaraswamy</surname><given-names>SD</given-names></name><name><surname>Johnson</surname><given-names>BW</given-names></name><name><surname>McNair</surname><given-names>NA</given-names></name></person-group><article-title>Mu rhythm modulation during observation of an object-directed grasp</article-title><source>Cogn Brain Res</source><year>2004</year><volume>19</volume><fpage>195</fpage><lpage>201</lpage><pub-id pub-id-type="pmid">15019715</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Schoffelen</surname><given-names>J-M</given-names></name></person-group><article-title>FieldTrip: Open Source Software for Advanced Analysis of MEG, EEG, and Invasive Electrophysiological Data</article-title><source>Comput Intell Neurosci</source><year>2011</year><volume>2011</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="pmcid">PMC3021840</pub-id><pub-id pub-id-type="pmid">21253357</pub-id><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ortiz-Barajas</surname><given-names>MC</given-names></name><name><surname>Guevara</surname><given-names>R</given-names></name><name><surname>Gervain</surname><given-names>J</given-names></name></person-group><article-title>Neural oscillations and speech processing at birth</article-title><source>iScience</source><year>2023</year><volume>26</volume><pub-id pub-id-type="pmcid">PMC10641252</pub-id><pub-id pub-id-type="pmid">37965146</pub-id><pub-id pub-id-type="doi">10.1016/j.isci.2023.108187</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parise</surname><given-names>E</given-names></name><name><surname>Csibra</surname><given-names>G</given-names></name></person-group><article-title>Neural Responses to Multimodal Ostensive Signals in 5-Month-Old Infants</article-title><source>PLoS One</source><year>2013</year><volume>8</volume><elocation-id>e72360</elocation-id><pub-id pub-id-type="pmcid">PMC3747163</pub-id><pub-id pub-id-type="pmid">23977289</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0072360</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piccardi</surname><given-names>ES</given-names></name><name><surname>Johnson</surname><given-names>MH</given-names></name><name><surname>Gliga</surname><given-names>T</given-names></name></person-group><article-title>Explaining individual differences in infant visual sensory seeking</article-title><source>Infancy</source><year>2020</year><volume>25</volume><fpage>677</fpage><lpage>698</lpage><pub-id pub-id-type="pmcid">PMC7496506</pub-id><pub-id pub-id-type="pmid">32748567</pub-id><pub-id pub-id-type="doi">10.1111/infa.12356</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>The neuroanatomic and neurophysiological infrastructure for speech and language</article-title><source>Curr Opin Neurobiol</source><year>2014</year><volume>28</volume><fpage>142</fpage><lpage>149</lpage><pub-id pub-id-type="pmcid">PMC4177440</pub-id><pub-id pub-id-type="pmid">25064048</pub-id><pub-id pub-id-type="doi">10.1016/j.conb.2014.07.005</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pomiechowska</surname><given-names>B</given-names></name><name><surname>Csibra</surname><given-names>G</given-names></name></person-group><article-title>Motor activation during action perception depends on action interpretation</article-title><source>Neuropsychologia</source><year>2017</year><volume>105</volume><fpage>84</fpage><lpage>91</lpage><pub-id pub-id-type="pmcid">PMC5447367</pub-id><pub-id pub-id-type="pmid">28189494</pub-id><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2017.01.032</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quadrelli</surname><given-names>E</given-names></name><name><surname>Geangu</surname><given-names>E</given-names></name><name><surname>Turati</surname><given-names>C</given-names></name></person-group><article-title>Human action sounds elicit sensorimotor activation early in life</article-title><source>Cortex</source><year>2019</year><volume>117</volume><fpage>323</fpage><lpage>335</lpage><pub-id pub-id-type="pmid">31200126</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quadrelli</surname><given-names>E</given-names></name><name><surname>Roberti</surname><given-names>E</given-names></name><name><surname>Polver</surname><given-names>S</given-names></name><name><surname>Bulf</surname><given-names>H</given-names></name><name><surname>Turati</surname><given-names>C</given-names></name></person-group><article-title>Sensorimotor activity and network connectivity to dynamic and static emotional faces in 7-month-old infants</article-title><source>Brain Sci</source><year>2021</year><volume>11</volume><pub-id pub-id-type="pmcid">PMC8615901</pub-id><pub-id pub-id-type="pmid">34827394</pub-id><pub-id pub-id-type="doi">10.3390/brainsci11111396</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Southgate</surname><given-names>V</given-names></name><name><surname>Johnson</surname><given-names>MH</given-names></name><name><surname>Karoui</surname><given-names>IEl</given-names></name><name><surname>Csibra</surname><given-names>G</given-names></name></person-group><article-title>Motor System Activation Reveals Infants’ On-Line Prediction of Others’ Goals</article-title><source>Psychol Sci</source><year>2010</year><volume>21</volume><fpage>355</fpage><lpage>359</lpage><pub-id pub-id-type="pmid">20424068</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Southgate</surname><given-names>V</given-names></name><name><surname>Vernetti</surname><given-names>A</given-names></name></person-group><article-title>Belief-based action prediction in preverbal infants</article-title><source>Cognition</source><year>2014</year><volume>130</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="pmcid">PMC3857687</pub-id><pub-id pub-id-type="pmid">24140991</pub-id><pub-id pub-id-type="doi">10.1016/j.cognition.2013.08.008</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tallon-Baudry</surname><given-names>C</given-names></name><name><surname>Bertrand</surname><given-names>O</given-names></name><name><surname>Delpuech</surname><given-names>C</given-names></name><name><surname>Pernier</surname><given-names>J</given-names></name></person-group><article-title>Stimulus specificity of phase-locked and nonphase-locked 40 Hz visual responses in human</article-title><source>J Neurosci</source><year>1996</year><volume>16</volume><fpage>4240</fpage><lpage>4249</lpage><pub-id pub-id-type="pmcid">PMC6579008</pub-id><pub-id pub-id-type="pmid">8753885</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.16-13-04240.1996</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name><name><surname>Van den Brink</surname><given-names>D</given-names></name><name><surname>Weder</surname><given-names>N</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name><name><surname>Magyari</surname><given-names>L</given-names></name><name><surname>Hagoort</surname><given-names>P</given-names></name><name><surname>Bastiaansen</surname><given-names>M</given-names></name></person-group><article-title>Beta oscillations relate to the N400m during language comprehension</article-title><source>Hum Brain Mapp</source><year>2012</year><volume>33</volume><fpage>2898</fpage><lpage>2912</lpage><pub-id pub-id-type="pmcid">PMC6870343</pub-id><pub-id pub-id-type="pmid">22488914</pub-id><pub-id pub-id-type="doi">10.1002/hbm.21410</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><article-title>Neurophysiological and computational principles of cortical rhythms in cognition</article-title><source>Physiol Rev</source><year>2010</year><volume>90</volume><fpage>1195</fpage><lpage>1268</lpage><pub-id pub-id-type="pmcid">PMC2923921</pub-id><pub-id pub-id-type="pmid">20664082</pub-id><pub-id pub-id-type="doi">10.1152/physrev.00035.2008</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>W</given-names></name><name><surname>Mallin</surname><given-names>BM</given-names></name><name><surname>Richards</surname><given-names>JE</given-names></name></person-group><article-title>Development of infant sustained attention and its relation to EEG oscillations: an EEG and cortical source analysis study</article-title><source>Dev Sci</source><year>2018</year><volume>21</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="pmcid">PMC5628078</pub-id><pub-id pub-id-type="pmid">28382759</pub-id><pub-id pub-id-type="doi">10.1111/desc.12562</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yin</surname><given-names>J</given-names></name><name><surname>Tatone</surname><given-names>D</given-names></name><name><surname>Csibra</surname><given-names>G</given-names></name></person-group><article-title>Giving, but not taking, actions are spontaneously represented as social interactions: Evidence from modulation of lower alpha oscillations</article-title><source>Neuropsychologia</source><year>2020</year><volume>139</volume><elocation-id>107363</elocation-id><pub-id pub-id-type="pmid">32007510</pub-id></element-citation></ref></ref-list></back><floats-group><boxed-text id="BX1" position="float" orientation="portrait"><caption><title>Highlights</title></caption><p><list list-type="bullet" id="L1"><list-item><p>WTools is a new MATLAB-based toolbox for time-frequency analysis of the EEG signal</p></list-item><list-item><p>It uses wavelet transformation and the algorithm is fully described</p></list-item><list-item><p>It is designed to be very user friendly through its simple GUI</p></list-item><list-item><p>It is open source and flexible as the code is freely available</p></list-item><list-item><p>We provide a step-by-step tutorial and an example dataset to learn the toolbox</p></list-item></list>
</p></boxed-text><fig id="F1" position="float"><label>Fig. 1</label><caption><p>WTools GUI (<bold>A</bold>) and schematic representation of the processing pipeline (<bold>B</bold>). The toolbox is divided into five main sections: Project management (initialize project and import data); Signal processing (run time-frequency analysis); Plots (create time-frequency plots, 2D and 3D scalp maps); Statistics (export numeric values for statistical analyses); Application (configure and monitor the toolbox’s functionalities). The paper focuses mainly on the core analytical section of the toolbox (i.e. Signal processing); for further details on the remaining sections, please consult the step-by-step tutorial accessible from the WTools GitHub project website (see section “WTools availability”).</p></caption><graphic xlink:href="EMS199793-f001"/></fig><fig id="F2" position="float"><label>Fig. 2</label><caption><p>Comparison of complex Morlet wavelets between WTools (on the left) and EEGLAB (on the right) for the frequency range used in the present work (10-90 Hz; see section “Data analysis”). Please note that the standard WTools pipeline computes wavelets with 1 Hz sampling frequency; here we plot wavelets with 10 Hz sampling frequency for visualization purposes only. The red plots show non-normalized wavelets for WTools. The blue plots show normalized wavelets for WTools and EEGLAB, with a perfect correspondence between the two. X axis: Time (s); Y axis: Wavelet amplitude (a.u.).</p></caption><graphic xlink:href="EMS199793-f002"/></fig><fig id="F3" position="float"><label>Fig. 3</label><caption><p>Time-frequency plots (<bold>A</bold>) and 2D scalp maps (<bold>B</bold>) for one representative participant (“04”) and channel (“22”, corresponding to Fp2) for individual conditions (“DG”, “AG”) and for their difference (“DG – AG”), under the standard WTools pipeline. Scalp maps show the topographies of the time-frequency results in the 250-350 ms time window and 30-40 Hz frequency window (as highlighted with dashed lines in panel A).</p></caption><graphic xlink:href="EMS199793-f003"/></fig><fig id="F4" position="float"><label>Fig. 4</label><caption><p>3D infant scalp maps for the pair-wise condition difference “DG – AG” of one representative participant (“04”) under the standard WTools pipeline. Scalp maps show the topographies of the time-frequency results in the 250-350 ms time window and 30-40 Hz frequency window (as highlighted in <xref ref-type="fig" rid="F3">Fig. 3A</xref>). In the GUI, users can rotate scalp maps in any directions to optimize scalp visualization as desired.</p></caption><graphic xlink:href="EMS199793-f004"/></fig><fig id="F5" position="float"><label>Fig. 5</label><caption><p>Time-frequency plots for one representative participant (“04”), channel (“22”, corresponding to Fp2) and condition (“DG”) under the custom pipelines that allowed a direct comparison of the WTools and EEGLAB outputs (<bold>A</bold>) and under the standard WTools and EEGLAB pipelines (<bold>B</bold>).</p></caption><graphic xlink:href="EMS199793-f005"/></fig></floats-group></article>