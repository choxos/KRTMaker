<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS200378</article-id><article-id pub-id-type="doi">10.1101/2024.11.18.624090</article-id><article-id pub-id-type="archive">PPR942730</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">2</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Eye and hand coarticulation during problem solving reveals hierarchically organized planning</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Eluchans</surname><given-names>Mattia</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Maselli</surname><given-names>Antonella</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Lancia</surname><given-names>Gian Luca</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Pezzulo</surname><given-names>Giovanni</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>Institute of Cognitive Sciences and Technologies, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/04zaypm56</institution-id><institution>National Research Council</institution></institution-wrap>, <city>Rome</city>, <country country="IT">Italy</country></aff><aff id="A2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05ctdxz19</institution-id><institution>University of Messina</institution></institution-wrap>, <country country="IT">Italy</country></aff><author-notes><corresp id="CR1"><label>*</label>To whom correspondence should be addressed. <email>giovanni.pezzulo@istc.cnr.it</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>21</day><month>11</month><year>2024</year></pub-date><pub-date pub-type="preprint"><day>19</day><month>11</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license><license><ali:license_ref>https://europepmc.org/downloads/openaccess</ali:license_ref><license-p>This preprint is made available via the <ext-link ext-link-type="uri" xlink:href="https://europepmc.org/downloads/openaccess">Europe PMC open access subset</ext-link>, for unrestricted research re-use and secondary analysis in any form or by any means with acknowledgement of the original preprint source.</license-p></license></permissions><abstract><p id="P1">While the ways in which we plan ahead have been investigated since the inception of cognitive science, much remains to be understood about how we plan and coordinate sequences of actions (e.g., eye and hand movements) necessary for effective problem-solving. This study investigates how participants use gaze and cursor movements to plan and execute problem-solving tasks, revealing three key findings. First, participants segment the problem into sequences of gestures; within each gesture, gaze selects a target and remains fixed until the cursor reaches it, then moves to the next target. Second, we observed coarticulation in both cursor-cursor and gaze-cursor movements, occurring within individual gestures and, to a lesser degree, between successive gestures. Third, the angular position of gaze reliably predicts the direction of the next fixation, indicating forward-looking coarticulation between successive gaze fixations. Together, these findings suggest that participants employ a hierarchical planning strategy: they divide the problem into gesture sequences and plan multiple eye and cursor movements in advance to efficiently reach both current and upcoming gesture targets. This hierarchical motor plan demonstrates a structure where targets (or subgoals) are defined and achieved through the coordinated actions of the eyes and hand, highlighting the importance of integrated eye-hand planning in complex task performance.</p></abstract></article-meta></front><body><p id="P2">During everyday activities, such as when preparing a cup of coffee or traveling to a novel destination, we often plan ahead sequences of actions. While the ways we plan ahead has been investigated since the inception of cognitive science [<xref ref-type="bibr" rid="R29">29</xref>], much remains to be understood about how we plan and coordinate sequences of actions (e.g., eye and hand movements) necessary to solve cognitive problems.</p><p id="P3">Previous studies investigated human planning during the solution of various types of problems, such as when navigating mazes, solving games like chess, or puzzles like the tower of Hanoi. Some studies used the time taken before making a choice as an index of planning complexity [<xref ref-type="bibr" rid="R13">13</xref>, <xref ref-type="bibr" rid="R27">27</xref>, <xref ref-type="bibr" rid="R33">33</xref>, <xref ref-type="bibr" rid="R38">38</xref>]. Other studies used computational models to analyse participants’ sequential action selection (e.g., the route taken in a maze), revealing various approximations to optimal planning strategies that emerge due to limited temporal and cognitive constraints [<xref ref-type="bibr" rid="R9">9</xref>, <xref ref-type="bibr" rid="R10">10</xref>, <xref ref-type="bibr" rid="R16">16</xref>, <xref ref-type="bibr" rid="R25">25</xref>, <xref ref-type="bibr" rid="R39">39</xref>].</p><p id="P4">Most of the above studies focus on planning sequences of abstract actions. However, in real-life situations, planning abstract action sequences often needs to be translated into the execution of co-ordinated movements. For example, making a cup of coffee requires not only identifying and ordering the correct preparation steps – such as adding water and coffee grounds, lighting the stove, and serving the coffee – but also translating these steps into sequences of coordinated eye and hand movements [<xref ref-type="bibr" rid="R5">5</xref>, <xref ref-type="bibr" rid="R22">22</xref>, <xref ref-type="bibr" rid="R24">24</xref>, <xref ref-type="bibr" rid="R41">41</xref>]. The close relationship between abstract cognitive planning and real-time movement control raises the possibility of using movement kinematics as a window into planning processes.</p><p id="P5">In keeping, one line of research investigates planning processes by analysing gaze behaviour before and during the execution of complex action sequences [<xref ref-type="bibr" rid="R3">3</xref>, <xref ref-type="bibr" rid="R4">4</xref>, <xref ref-type="bibr" rid="R28">28</xref>, <xref ref-type="bibr" rid="R32">32</xref>, <xref ref-type="bibr" rid="R34">34</xref>, <xref ref-type="bibr" rid="R43">43</xref>]. These studies reveal that gaze behaviour is driven by both salient visual stimuli and (present and future) goals [<xref ref-type="bibr" rid="R14">14</xref>, <xref ref-type="bibr" rid="R15">15</xref>, <xref ref-type="bibr" rid="R19">19</xref>, <xref ref-type="bibr" rid="R23">23</xref>, <xref ref-type="bibr" rid="R30">30</xref>, <xref ref-type="bibr" rid="R31">31</xref>, <xref ref-type="bibr" rid="R36">36</xref>, <xref ref-type="bibr" rid="R42">42</xref>]. During preparatory phases before the execution of action sequences, gaze can be used to scan the environment and to gather information about the problem [<xref ref-type="bibr" rid="R6">6</xref>, <xref ref-type="bibr" rid="R11">11</xref>]. Subsequently, during the motor implementation phase, most gaze fixations are on objects immediately needed but some are on objects needed in the future [<xref ref-type="bibr" rid="R32">32</xref>]. For example, gaze during a tic-tac-toe game could predict participants’ future moves, with accuracy increasing with their expertise [<xref ref-type="bibr" rid="R18">18</xref>]. Further analysis of gaze behavior in route planning tasks reveals the presence of backward and forward planning [<xref ref-type="bibr" rid="R43">43</xref>]. Moreover, studies on attention allocation during saccade sequences reveal that saccades are planned simultaneously rather than sequentially, and their amplitude and direction are affected by subsequent saccades in a sequence [<xref ref-type="bibr" rid="R1">1</xref>, <xref ref-type="bibr" rid="R2">2</xref>, <xref ref-type="bibr" rid="R3">3</xref>, <xref ref-type="bibr" rid="R8">8</xref>, <xref ref-type="bibr" rid="R26">26</xref>, <xref ref-type="bibr" rid="R35">35</xref>].</p><p id="P6">Another line of research reveals that it is possible to identify sequential planning by analysing movement <italic>coarticulation</italic>, or the fact that when two movements in a sequence are planned, the execution of the first movement changes as a function of the second movement to be executed. For example, phonology studies assessed that the same letters are pronounced differently, depending on the next letters to be uttered [<xref ref-type="bibr" rid="R7">7</xref>, <xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R40">40</xref>]. Coarticulation has been reported in various other domains, such as in fingerspelling and motor skill acquisition, suggesting its generality [<xref ref-type="bibr" rid="R21">21</xref>, <xref ref-type="bibr" rid="R37">37</xref>].</p><p id="P7">While the above studies show that movement kinematics provide information about covert planning dynamics, they mostly focused on familiar and simple tasks. Planning dynamics during novel and challenging problem solving tasks remain largely unaddressed. To address this challenge, we tracked participants’ gaze and mouse cursor kinematics while they solved novel problem solving tasks on a PCs (see <xref ref-type="fig" rid="F1">Figure 1</xref> for an example).</p><p id="P8">We aimed to address three main questions. First, we asked whether gaze and cursor dynamics reveal if and how participants segment a motor plan into sequences of movements. Second, we asked whether participants’ gaze position and/or cursor kinematics provides enough information about the rest of the plan to predict next cursor direction, therefore showing cursor-cursor and/or gaze-cursor coarticulation. Third, we asked whether gaze fixations provide information about the next saccade direction, therefore showing gaze-gaze coarticulation. Examples of cursor-cursor, gaze-cursor and gaze-gaze coarticulation (or a lack of it) in our task are shown in <xref ref-type="fig" rid="F2">Figure 2</xref>.</p><sec id="S1" sec-type="results"><title>Results</title><sec id="S2"><title>The planning task</title><p id="P9">Participants solved 90 planning problems, each requiring them to find a path that connected all the ”gems” on a grid, without revisiting any nodes (though they were allowed to backtrack to unselect nodes) (<xref ref-type="fig" rid="F1">Figure 1</xref>). Each problem had to be completed within 60 seconds, with more points awarded for quicker solutions. Participants solved the problems by controlling a computer mouse, while the grid and the cursor were always visible on a PC screen.</p></sec><sec id="S3"><title>Participants segment problems into sequences of gestures and alternate gaze and cursor movements</title><p id="P10">A standard solution for a problem of the task often comprises two phases, see <xref ref-type="fig" rid="F3">Figure 3A</xref> for an example. In the <italic>preplanning</italic> phase, which is not included in the analysis, gaze (in blue) scans the problem while the cursor (in orange) stays still. In the subsequent <italic>execution</italic> phase, which we consider in our analysis, gaze-cursor coordination follows an alternating pattern: gaze position anticipates the cursor position most of the time and tends to remain fixed on (or close to) a target node, until the cursor reaches it (or the cursor-fixation distance becomes very small). The alternating pattern can be visually appreciated in <xref ref-type="fig" rid="F3">Figure 3D</xref>, which shows that cursor-fixation distance increases (as the gaze moves from one target to the next, while the cursor tends to remain stationary) and then decreases (as the cursor moves towards the next target node, and the gaze tends to remain stationary), multiple times.</p><p id="P11">To test the statistical significance of a regime that alternates gaze fixations and cursor movements to the same target, we considered the variation in gaze-cursor distances between two consecutive fixations, defined as:
<disp-formula id="FD1"><mml:math id="M1"><mml:mrow><mml:mtext>Δ</mml:mtext><mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P12">Positive (negative) values of this difference imply that fixations move away (towards) the current cursor position.</p><p id="P13">The variation in gaze-cursor distances between consecutive fixations was on average significantly greater than zero (<italic>τ</italic> = 14.21, <italic>p<sub>value</sub></italic> &lt; 10<sup>−3</sup>, one-sample t-test), implying that fixations move away from the current cursor position (<xref ref-type="fig" rid="F3">Figure 3B</xref>).</p><p id="P14">We next tested whether gaze-cursor distance △<italic>d</italic> decreases during a fixation, as expected if gaze precedes the cursor on the same target. We computed the temporal derivative of △<italic>d</italic> during a fixation: a negative (positive) value in the derivative implies that gaze-cursor distance decreases (increases) over time. To compare distances during fixations having different duration, we first defined a normalized time <inline-formula><mml:math id="M2"><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula> given by the absolute time <italic>t</italic> shifted to start from 0 and normalized by the fixation duration: <disp-formula id="FD2"><mml:math id="M3"><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="italic">start</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="italic">end</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi mathvariant="italic">start</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></disp-formula> where <italic>t<sub>start</sub></italic> and <italic>t<sub>end</sub></italic> are the start and end of the fixation, respectively. The normalize time <inline-formula><mml:math id="M4"><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula> indicates the percentage of fixation duration and is comprised between 0 and 1.</p><p id="P16">The time derivative of gaze-cursor distance was negative throughout the fixation duration <inline-formula><mml:math id="M5"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo>&lt;</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>∀</mml:mo><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, implying that, on average, the cursor reduces its distance from gaze position, therefore reaching the gaze on the same target (<xref ref-type="fig" rid="F3">Figure 3D</xref>).</p><p id="P17">To summarize, during movement execution, gaze and cursor show alternating dynamics, with gaze fixating a target node until the cursor reached it – and then fixating the next target node. This alternating structure segments the problem into a sequence of <italic>gestures</italic>, defined as the intervals of time between two consecutive fixations, during which gaze remains stable on the target node while the cursor reaches it.</p></sec><sec id="S4"><title>Coarticulation in cursor movements: both gaze position and current cursor kinematics predict the next cursor direction</title><p id="P18">We asked whether it is possible to use gaze position and the kinematics of cursor movement until a certain node to predict the direction of the next cursor movement after it leaves the node. We collapsed cursor trajectories into three possible directions: <bold>N</bold>orth, <bold>E</bold>ast, or <bold>W</bold>est.</p><p id="P19">Since the above analysis reveals that problem solving is naturally decomposed into a sequence of <italic>gestures</italic>, we performed separate prediction tasks for the cases <italic>within gestures</italic> and <italic>between gestures</italic>. Predicting cursor direction <italic>within gestures</italic> implies that participants coarticulate consecutive movements to their current target, whereas predicting cursor movement <italic>between gestures</italic> implies that they coarticulate movements to the next target while they reach their current target.</p><p id="P20">We reformulated the four prediction tasks as classification tasks, by considering how well LDA models classify the current cursor kinematics into the three next cursor directions: North, East or West (<xref ref-type="fig" rid="F4">Figure 4A</xref>). Both gaze position and the kinematics of cursor movement predict the next cursor direction above chance (dotted line), both <italic>within gestures</italic> and <italic>between gestures</italic>. Two control analyses indicate that the same results hold when splitting data across all the three game levels (<xref ref-type="supplementary-material" rid="SD2">Figure S4</xref> and <xref ref-type="supplementary-material" rid="SD2">Figure S3</xref>) and when in the analysis cases in which the endpoint of the cursor in the gesture does not match the gaze target, for example because it leaved current target before being reached (<xref ref-type="supplementary-material" rid="SD2">Figure S6</xref>), which are not considered in <xref ref-type="fig" rid="F4">Figure 4A</xref>.</p><p id="P21">To obtain more information about the quality of the classification, we plotted confusion matrices for the LDA models corresponding to the four prediction tasks (<xref ref-type="fig" rid="F4">Figure 4</xref>; see also <xref ref-type="supplementary-material" rid="SD2">Figure S7</xref> for the control analysis. In all cases except one (i.e., cursor kinematics in the north direction between gestures) the diagonal shows greater values than the off-diagonal terms, implying a discriminability above chance.</p><p id="P22">To summarize, we found coarticulation in cursor-cursor and gaze-cursor movements, when participants made consecutive movements <italic>within gestures</italic> and <italic>between gestures</italic>. These results imply that participants plan ahead multiple cursor movements to both the current and the next targets.</p></sec><sec id="S5"><title>Coarticulation in gaze fixations: gaze angular position between consecutive fixations</title><p id="P23">We finally asked whether the angle formed by the gaze position and the last node of a gesture shared information with the next fixation angle – which would imply the coarticulation between sequential fixations.</p><p id="P24">We used a circular-to-circular model to regress current fixation angle to the next fixation angle (<xref ref-type="fig" rid="F5">Figure 5A</xref>). We estimated the accuracy of this regression in classifying the discretized (8 equal bins) current fixation angle. We considered two controls (<xref ref-type="fig" rid="F5">Figure 5(B)</xref>): a model using a random point around the remaining (unvisited) nodes and a model using a random point in an interval around the remaining (uncollected) rewards. The two control models only use available visual information (i.e., node and gem positions). The accuracy of the three models is greater than chance (dotted line; 1/8), but the regression to the next fixation angle shows significantly greater prediction accuracy compared to the two control models (<italic>A<sub>Next</sub></italic> = 0.350 ± 0.008, <italic>p<sub>value</sub></italic> &lt; 10<sup>−3</sup><italic>A<sub>Unvis</sub></italic> = 0.183 ± 0.006, <italic>p<sub>value</sub></italic> &lt; 10<sup>−3</sup>, <italic>A<sub>Uncoll</sub></italic> = 0.225 ± 0.006, <italic>p<sub>value</sub></italic> &lt; 10<sup>−3</sup>) (<xref ref-type="fig" rid="F5">Figure 5B</xref>).</p><p id="P25">To summarize, we found coarticulation between consecutive gaze fixations, indicating that participants plan ahead fixations to multiple targets.</p></sec></sec><sec id="S6" sec-type="discussions"><title>Discussion</title><p id="P26">Assessing how we plan sequences of actions to solve challenging problems has been a central question of cognitive science since its inception [<xref ref-type="bibr" rid="R29">29</xref>]. Here, we investigated human planning strategies by recording cursor and gaze kinematics while participants solve problem solving tasks requiring navigating in a grid with the computer mouse, to collect multiple ”gems”. We used the coarticuation of cursor-cursor, gaze-cursor and gaze-gaze movements as an index of planning. The rationale is that, to the extent that participants plan ahead multiple movements, the kinematics of their first movement in a sequence should carry information about – and hence predict – the next movements.</p><p id="P27">We report three main findings. First, after a pre-planning phase in which the eyes scan the problem while the cursor remains fixed at the start position, gaze fixations and cursor movements show a robust alternating pattern, with gaze selecting a target node and fixating it until the cursor reached it – and then moving to the next target node. This finding suggests that plan execution is naturally segmented into a sequence of <italic>gestures</italic> – defined as the interval of time between two consecutive fixations.</p><p id="P28">Second, both gaze position and cursor kinematics predict the next cursor direction, which implies coarticulation both within a single effector (cursor-cursor) and across effectors (gaze-cursor). For example, the cursor movements directed towards the next node typically deviate from the beginning in the direction of the next node (<xref ref-type="fig" rid="F2">Figure 2</xref>). Similarly, gaze fixations do not simply lie close to the next target but were also slightly deviated in the direction of the next target. Interestingly, not only coarticulation effects arise within gestures, but also between consecutive gestures. This finding indicates that participants plan ahead multiple cursor movements, to reach both the current and the next targets.</p><p id="P29">Third, and interestingly, the fixation angle with respect to a fixated node predicts the next fixation angle, implying coarticulation in gaze. This finding indicates that while participants segment the task into a sequence of targets to be fixated and then reached with the cursor, they plan fixations to multiple targets in advance. While sequential saccade planning was previously reported [<xref ref-type="bibr" rid="R17">17</xref>, <xref ref-type="bibr" rid="R44">44</xref>], this study shows that it supports the solution of novel problems.</p><p id="P30">Overall, these results suggest that participants form hierarchically organized plans to coordinate eye and hand movements required for problem-solving. Participants segment the problem into a sequence of gestures and then achieve them by alternating between gaze and cursor movements. Specifically, gaze selects the target for each cursor movement (i.e., the endpoint of a gesture) and remains fixed on it to guide the cursor until the target is reached, then transitions to the next target. This alternation implies a hierarchical planning structure, with a higher level organizing sequences of gestures to define targets (or subgoals) for gaze and cursor, while a lower level supports the detailed sequence of cursor movements needed to reach each target. Notably, the coarticulation analyses indicate that participants not only plan multiple movements within a gesture but also plan multiple gestures in advance. This finding suggests that sequential planning operates simultaneously at both the higher (gesture) and lower (cursor movement) hierarchical levels, supporting complex coordination across the task.</p><p id="P31">One limitation of the current study is that it only focuses on the plan execution phase. Future studies could address the relations between the pre-planning phase, in which eye movements scan the problem to establish a (partial) plan and the subsequent execution phase. Furthermore, future studies could investigate the events that break the alternation of gaze and cursor movements during the execution phase (e.g., pauses, backtracking, movement errors), which we excluded from the analysis. These events could reveal more fine-grained aspects of planning and replanning during problem solving.</p></sec><sec id="S7" sec-type="materials | methods"><title>Materials and Methods</title><sec id="S8"><title>Experimental Setup and Procedure</title><p id="P32">We recruited 31 participants (19 M age = 27 ± 4 years, 12 F age=24 ± 5 years) with normal-to-corrected vision. Participants were free to leave the experiment at any moment. All gave informed consent to our procedures which were approved by the Ethics Committee of the National Research Council. All of them were rewarded for taking part to the experiment with vouchers. All the participants completed the experiment.</p><p id="P33">Participants were shown a video tutorial and informed about the task structure (4 training problems followed by 90 problems divided into 3 levels of 30 problems each [<xref ref-type="bibr" rid="R10">10</xref>]).</p><p id="P34">We used an Eye-Link 1000+ tracker in a chin-rest desktop mount configuration with a 16 mm lens. The experiment was performed inside an isolated room with no source of light except the IR camera of the gaze tracker and the (LED) light of the screen where the task app was running.</p><p id="P35">Participants could control the movement in the PC via a mouse cursor.</p><p id="P36">For each participant, a standard 9+1 points calibration was performed on a grid with the same area of the task problems at the beginning of the experiment. A 9+1 point validation was performed after each calibration. Participants were asked to move as little as possible once the calibration was validated. The acceptance criterion for the calibration was the one suggested by the producer: a maximum error in calibration of 1° and an average error smaller than 0.5°. Eye-tracking starting sampling frequency was 2000 Hz, while mouse-tracker sampling frequency was 60 Hz.</p></sec><sec id="S9"><title>Data Preprocessing: cursor trajectory resampling and dimensionality reduction</title><p id="P37">To enable comparison across gestures of varying durations (and because of the fixed sampling frequency, of varying number of points), we fitted a linear spline with no smoothing factor to each original tra jectory and resampled a fixed number (N = 50) of 2D coordinates. Resampled points were spatially distributed according to the original velocity profile, ensuring that regions with fewer points (due to higher speeds) maintained the same relative density in the resampled trajectory.</p><p id="P38">The resampled trajectories were translated such that the starting node coincided with the the Cartesian plane (0,0), and rotated such that the first crossed link direction was along the positive y-axis (”North” direction). The gaze position during the execution of this trajectory was described by a 2D point in the Cartesian plane and transformed (translated and rotated) according to the same amounts of the corresponding trajectory.</p><p id="P39">After standardizing each trajectory into a 2(Cartesian) x50(Sampled points) dimensions, we looked for a lower-dimensional description via a <italic>spatio-temporal principal component analysis</italic> (st-PCA). The st-PCA is a method that explains trajectory variability by identifying a basis of trajectories, or <italic>spatio-temporal principal components</italic>, whose linear combinations can reconstruct the original trajectories.</p><p id="P40">At the base of this method is the idea to treat each point of the (resampled) trajectory as a distinct variable. The 2D coordinates are then flattened in a single vector, resulting in each trajectory of N points being represented as a 2N-vector (<italic>x</italic><sub>1</sub>, . . . , <italic>x<sub>n</sub></italic>, <italic>y</italic><sub>1</sub>, . . . , <italic>y<sub>n</sub></italic>). Stacking all the considered trajectories creates an <italic>N<sub>trajectories</sub></italic> × 2N matrix, where a standard PCA can be carried on.</p><p id="P41">Interestingly, we found the PCs emerged from the analysis of cursor movements to be functionally interpretable (<xref ref-type="supplementary-material" rid="SD2">Figure S9</xref>), supporting its appropriateness.</p></sec><sec id="S10"><title>Classification of the next cursor direction</title><p id="P42">We train four linear classifiers to predict the next cursor direction based upon two different effectors (the low-dimensional representation of cursor kinematics and gaze position) and for two different conditions (<italic>within gestures</italic> and <italic>between gestures</italic>).</p><p id="P43">For all the classifiers, we collapse the cursor directions into the three discrete orientations (<bold>N</bold>orth, <bold>E</bold>ast, <bold>W</bold>est) relative to the first movement, depending on the underlying sequence of nodes that are crossed (e.g. from (0,1) to (0,2) would correspond to the North direction).</p><p id="P44">The former, <italic>within gestures</italic> condition, considers whether the cursor kinematics up until a node can be used to predict the cursor direction towards the next node, when the two nodes belong to the same gesture. For the <italic>within gestures</italic> prediction, we consider all periods of time in which at least two links have been crossed and we use the cursor trajectory from start to the first node to train a classifier and the cursor trajectory from the first to the second node as the test for classification. The second, <italic>between gestures</italic> condition, considers whether the cursor kinematics up until a node can be used to predict the cursor direction towards the next node, when the two nodes belong to two consecutive gestures. For the <italic>between gestures</italic> prediction, we consider all the cases in which a single link was crossed during the first gesture (used to train the classifier) and at least one link was crossed during the second gesture. We exclude from analyses all the cases in which the participant paused or backtracked.</p><p id="P45">For classification, we use <italic>Linear Discriminant Analysis</italic> (LDA) that identifies a linear combination of features that optimally differentiate between two or more classes, while minimizing the variance within each class. To assess the statistical significance of the LDA model accuracy in the different cases, we used a <italic>repeated stratified K-fold cross validation</italic> procedure (number of folds = 3, number of replicas = 100). For all the cases, we balanced the classes’ volume by a downsampling procedure, in order to avoid possible biases in the classifier. Since the downsample is a stochastic procedure we considered 30 (down)samples of the whole data, and for each of them performed the cross-validation procedure previously described. The errors estimated for the average accuracy are obtained as the standard deviation across all the (30 samples x 100 K-fold replicas) accuracies. This is a more conservative estimation with respect to taking the standard deviations of the (30 samples) accuracies.</p></sec><sec id="S11"><title>Prediction of the next saccade direction</title><p id="P46">Finally, we study the information that the gaze position shares across consecutive fixations. In particular, during each gesture, we can define the current angular gaze position as the direction formed by the last node reached during that gesture and the current gaze position. Analogously we can define the next angular gaze position as the direction formed by last node reached during that gesture and the gaze position of the next fixation.</p><p id="P47">To fit the relation between these two angles we used a circular-to-circular regression [<xref ref-type="bibr" rid="R20">20</xref>]. The goodness of the fit was evaluated by testing the accuracy of the classification of the next angular direction into current angular direction, once discretized into 8 possible angular bins.</p><p id="P48">The estimated accuracy was compared to two different random models: the first one classifies the direction of a point randomly sampled around the nodes that had not been visited (rather then the next fixation position); the second one with a point randomly sampled in an interval around the ”gems” that have not yet been collected.</p><p id="P49">To estimate the accuracy of the model regressing the current fixation angle to the next fixation angle we used a 3-fold cross-validation procedure repeated 50 times.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Video</label><media xlink:href="EMS200378-supplement-Video.mp4" mimetype="video" mime-subtype="mp4" id="d215aAcKbB" position="anchor"/></supplementary-material><supplementary-material content-type="local-data" id="SD2"><label>Supplementary Information</label><media xlink:href="EMS200378-supplement-Supplementary_Information.pdf" mimetype="application" mime-subtype="pdf" id="d215aAcKcB" position="anchor"/></supplementary-material></sec></body><back><ack id="S"><title>Acknowledgments</title><p id="P50">This research received funding from the European Research Council under the Grant Agreement No. 820213 (ThinkAhead), the Italian National Recovery and Resilience Plan (NRRP), M4C2, funded by the European Union – NextGenerationEU (Pro ject IR0000011, CUP B51E22000150006, “EBRAINS-Italy”; Pro ject PE0000013, “FAIR”; Pro ject PE0000006, “MNESYS”), and the PRIN PNRR P20224FESY. The GEFORCE Quadro RTX6000 and Titan GPU cards used for this research were donated by the NVIDIA Corporation.</p></ack><fn-group><fn id="FN1" fn-type="con"><p id="P51"><bold>Author Contributions</bold></p><p id="P52">Conceptualization: ME, GP; Methodology: ME, GLL, AM; Validation: ME, GLL, AM, GP; Formal analysis: ME; Investigation: ME; Software: ME, AM; Data Curation: ME; Writing - Original Draft: ME, GP; Writing - Review &amp; Editing: ME, GLL, AM, GP; Supervision: GP; Project administration: GP; Funding acquisition: GP.</p></fn><fn id="FN2"><p id="P53"><bold>Author Declaration</bold></p><p id="P54">We have no competing interests to declare.</p></fn></fn-group><ref-list><ref id="R1"><label>[1]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ames</surname><given-names>Kevin C</given-names></name><name><surname>Ryu</surname><given-names>StephenI</given-names></name><name><surname>Shenoy</surname><given-names>Krishna V</given-names></name></person-group><article-title>Simultaneous motor preparation and execution in a last-moment reach correction task</article-title><source>Nature Communications</source><year>2019</year><volume>10</volume><elocation-id>2718</elocation-id><pub-id pub-id-type="pmcid">PMC6586876</pub-id><pub-id pub-id-type="pmid">31221968</pub-id><pub-id pub-id-type="doi">10.1038/s41467-019-10772-2</pub-id></element-citation></ref><ref id="R2"><label>[2]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Azadi</surname><given-names>Reza</given-names></name><name><surname>Zhu</surname><given-names>Elizabeth Y</given-names></name><name><surname>McPeek</surname><given-names>Robert M</given-names></name></person-group><article-title>Modulation of saccade trajectories during sequential saccades</article-title><source>Journal of Neurophysiology</source><year>2021</year><volume>125</volume><issue>3</issue><fpage>796</fpage><lpage>804</lpage><pub-id pub-id-type="pmcid">PMC7988749</pub-id><pub-id pub-id-type="pmid">33471606</pub-id><pub-id pub-id-type="doi">10.1152/jn.00106.2020</pub-id></element-citation></ref><ref id="R3"><label>[3]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baldauf</surname><given-names>Daniel</given-names></name><name><surname>Deubel</surname><given-names>Heiner</given-names></name></person-group><article-title>Properties of attentional selection during the preparation of sequential saccades</article-title><source>Experimental Brain Research</source><year>2008</year><volume>184</volume><fpage>411</fpage><lpage>425</lpage><pub-id pub-id-type="pmid">17846754</pub-id></element-citation></ref><ref id="R4"><label>[4]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Binder</surname><given-names>FJ</given-names></name><name><surname>Mattar</surname><given-names>MG</given-names></name><name><surname>Kirsh</surname><given-names>D</given-names></name><name><surname>Fan</surname><given-names>JE</given-names></name></person-group><article-title>Humans choose visual subgoals to reduce cognitive cost</article-title><conf-name>Proceedings of the Annual Meeting of the Cognitive Science Society</conf-name><year>2023</year><volume>45</volume></element-citation></ref><ref id="R5"><label>[5]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cisek</surname><given-names>Paul</given-names></name><name><surname>Pastor-Bernier</surname><given-names>Alexandre</given-names></name></person-group><article-title>On the challenges and mechanisms of embodied decisions</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><year>2014</year><volume>369</volume><issue>1655</issue><elocation-id>20130479</elocation-id><pub-id pub-id-type="pmcid">PMC4186232</pub-id><pub-id pub-id-type="pmid">25267821</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2013.0479</pub-id></element-citation></ref><ref id="R6"><label>[6]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crowe</surname><given-names>Douglas A</given-names></name><name><surname>Averbeck</surname><given-names>BrunoB</given-names></name><name><surname>Chafee</surname><given-names>Matthew V</given-names></name><name><surname>Anderson</surname><given-names>John H</given-names></name><name><surname>Georgopoulos</surname><given-names>Apostolos P</given-names></name></person-group><article-title>Mental maze solving</article-title><source>Journal of Cognitive Neuroscience</source><year>2000</year><volume>12</volume><issue>5</issue><fpage>813</fpage><lpage>827</lpage><pub-id pub-id-type="pmid">11054923</pub-id></element-citation></ref><ref id="R7"><label>[7]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daniloff</surname><given-names>RG</given-names></name><name><surname>Hammarberg</surname><given-names>RE</given-names></name></person-group><article-title>On defining coarticulation</article-title><source>Journal of Phonetics</source><year>1973</year><volume>1</volume><issue>3</issue><fpage>239</fpage><lpage>248</lpage></element-citation></ref><ref id="R8"><label>[8]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Vries</surname><given-names>Joseph  P</given-names></name><name><surname>Hooge</surname><given-names>Ignace T</given-names></name><name><surname>Verstraten</surname><given-names>Frans A</given-names></name></person-group><article-title>Saccades toward the target are planned as sequences rather than as single steps</article-title><source>Psychological Science</source><year>2014</year><volume>25</volume><issue>1</issue><fpage>215</fpage><lpage>223</lpage><pub-id pub-id-type="pmid">24166857</pub-id></element-citation></ref><ref id="R9"><label>[9]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donnarumma</surname><given-names>Francesco</given-names></name><name><surname>Maisto</surname><given-names>Domenico</given-names></name><name><surname>Pezzulo</surname><given-names>Giovanni</given-names></name></person-group><article-title>Problem solving as probabilistic inference with subgoaling: Explaining human successes and pitfalls in the tower of hanoi</article-title><source>PLOS Computational Biology</source><year>2016</year><volume>12</volume><issue>4</issue><elocation-id>e1004864</elocation-id><pub-id pub-id-type="pmcid">PMC4830581</pub-id><pub-id pub-id-type="pmid">27074140</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004864</pub-id></element-citation></ref><ref id="R10"><label>[10]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eluchans</surname><given-names>Mattia</given-names></name><name><surname>Lancia</surname><given-names>GianLuca</given-names></name><name><surname>Maselli</surname><given-names>Antonella</given-names></name><name><surname>D’Alessando</surname><given-names>Marco</given-names></name><name><surname>Gordon</surname><given-names>Jeremy</given-names></name><name><surname>Pezzulo</surname><given-names>Giovanni</given-names></name></person-group><article-title>Adaptive planning depth in human problem solving</article-title><source>bioRxiv</source><year>2024</year></element-citation></ref><ref id="R11"><label>[11]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiedler</surname><given-names>Susann</given-names></name><name><surname>Glöckner</surname><given-names>Andreas</given-names></name></person-group><article-title>The dynamics of decision making in risky choice: An eye-tracking analysis</article-title><source>Frontiers in Psychology</source><year>2012</year><volume>3</volume><pub-id pub-id-type="pmcid">PMC3498888</pub-id><pub-id pub-id-type="pmid">23162481</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2012.00335</pub-id></element-citation></ref><ref id="R12"><label>[12]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fowler</surname><given-names>Carol A</given-names></name><name><surname>Saltzman</surname><given-names>Elliot</given-names></name></person-group><article-title>Coordination and coarticulation in speech production</article-title><source>Language and Speech</source><year>1993</year><volume>36</volume><issue>2-3</issue><fpage>171</fpage><lpage>195</lpage><pub-id pub-id-type="pmid">8277807</pub-id></element-citation></ref><ref id="R13"><label>[13]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fudenberg</surname><given-names>D</given-names></name><name><surname>Newey</surname><given-names>Whitney</given-names></name><name><surname>Strack</surname><given-names>P</given-names></name><name><surname>Strzalecki</surname><given-names>Tomasz</given-names></name></person-group><article-title>Testing the drift-diffusion model</article-title><source>Proceedings of the National Academy of Sciences</source><year>2019</year><volume>117</volume><fpage>33141</fpage><lpage>33148</lpage><pub-id pub-id-type="pmcid">PMC7776861</pub-id><pub-id pub-id-type="pmid">33310903</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2011446117</pub-id></element-citation></ref><ref id="R14"><label>[14]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayhoe</surname><given-names>Mary</given-names></name><name><surname>Ballard</surname><given-names>Dana</given-names></name></person-group><article-title>Eye movements in natural behavior</article-title><source>Trends in Cognitive Sciences</source><year>2005</year><month>April</month><volume>9</volume><issue>4</issue><fpage>188</fpage><lpage>194</lpage><pub-id pub-id-type="pmid">15808501</pub-id></element-citation></ref><ref id="R15"><label>[15]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayhoe</surname><given-names>Mary M</given-names></name><name><surname>Rothkopf</surname><given-names>Constantin A</given-names></name></person-group><article-title>Vision in the natural world</article-title><source>Wiley Interdisciplinary Reviews: Cognitive Science</source><year>2011</year><volume>2</volume><issue>2</issue><fpage>158</fpage><lpage>166</lpage><pub-id pub-id-type="pmid">26302007</pub-id></element-citation></ref><ref id="R16"><label>[16]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ho</surname><given-names>ManKit</given-names></name><name><surname>Abel</surname><given-names>David</given-names></name><name><surname>Correa</surname><given-names>Camilo Gerardo</given-names></name><name><surname>Littman</surname><given-names>Michael L</given-names></name><name><surname>Cohen</surname><given-names>Jonathan D</given-names></name><name><surname>Griffiths</surname><given-names>Thomas L</given-names></name></person-group><article-title>People construct simplified mental representations to plan</article-title><source>Nature</source><year>2022</year><volume>606</volume><issue>7912</issue><fpage>129</fpage><lpage>136</lpage><pub-id pub-id-type="pmid">35589843</pub-id></element-citation></ref><ref id="R17"><label>[17]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoppe</surname><given-names>David</given-names></name><name><surname>Rothkopf</surname><given-names>Constantin A</given-names></name></person-group><article-title>Multi-step planning of eye movements in visual search</article-title><source>Scientific reports</source><year>2019</year><volume>9</volume><issue>1</issue><fpage>144</fpage><pub-id pub-id-type="pmcid">PMC6333838</pub-id><pub-id pub-id-type="pmid">30644423</pub-id><pub-id pub-id-type="doi">10.1038/s41598-018-37536-0</pub-id></element-citation></ref><ref id="R18"><label>[18]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>Jie</given-names></name><name><surname>Velarde</surname><given-names>Iris</given-names></name><name><surname>Ma</surname><given-names>Wei Ji</given-names></name><name><surname>Baldassano</surname><given-names>Christopher</given-names></name></person-group><article-title>Schema-based predictive eye movements support sequential memory encoding</article-title><source>eLife</source><year>2023</year><volume>12</volume><elocation-id>e82599</elocation-id><pub-id pub-id-type="pmcid">PMC10097418</pub-id><pub-id pub-id-type="pmid">36971343</pub-id><pub-id pub-id-type="doi">10.7554/eLife.82599</pub-id></element-citation></ref><ref id="R19"><label>[19]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Itti</surname><given-names>Laurent</given-names></name><name><surname>Koch</surname><given-names>Christof</given-names></name></person-group><article-title>Computational modeling of visual attention</article-title><source>Nature Reviews Neuroscience</source><year>2001</year><volume>2</volume><fpage>194</fpage><lpage>203</lpage><pub-id pub-id-type="pmid">11256080</pub-id></element-citation></ref><ref id="R20"><label>[20]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rao Jammalamadaka</surname><given-names>S</given-names></name><name><surname>SenGupta</surname><given-names>A</given-names></name></person-group><source>Topics in Circular Statistics</source><publisher-name>World Scientific Press</publisher-name><publisher-loc>Singapore</publisher-loc><year>2001</year><comment>Section 8.3</comment></element-citation></ref><ref id="R21"><label>[21]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jerde</surname><given-names>Thomas E</given-names></name><name><surname>Soechting</surname><given-names>John F</given-names></name><name><surname>Flanders</surname><given-names>Martha</given-names></name></person-group><article-title>Coarticulation in fluent fingerspelling</article-title><source>Journal of Neuroscience</source><year>2003</year><volume>23</volume><issue>6</issue><fpage>2383</fpage><lpage>2393</lpage><pub-id pub-id-type="pmcid">PMC6742053</pub-id><pub-id pub-id-type="pmid">12657698</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.23-06-02383.2003</pub-id></element-citation></ref><ref id="R22"><label>[22]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lepora</surname><given-names>Nathan F</given-names></name><name><surname>Pezzulo</surname><given-names>Giovanni</given-names></name></person-group><article-title>Embodied choice: how action influences perceptual decision making</article-title><source>PLoS computational biology</source><year>2015</year><volume>11</volume><issue>4</issue><elocation-id>e1004110</elocation-id><pub-id pub-id-type="pmcid">PMC4388485</pub-id><pub-id pub-id-type="pmid">25849349</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004110</pub-id></element-citation></ref><ref id="R23"><label>[23]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mannan</surname><given-names>SK</given-names></name><name><surname>Ruddock</surname><given-names>KH</given-names></name><name><surname>Wooding</surname><given-names>DS</given-names></name></person-group><article-title>Fixation patterns made during brief examination of two-dimensional images</article-title><source>Perception</source><year>1997</year><volume>26</volume><fpage>1059</fpage><lpage>1072</lpage><pub-id pub-id-type="pmid">9509164</pub-id></element-citation></ref><ref id="R24"><label>[24]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maselli</surname><given-names>Antonella</given-names></name><name><surname>Gordon</surname><given-names>Jeremy</given-names></name><name><surname>Eluchans</surname><given-names>Mattia</given-names></name><name><surname>Lancia</surname><given-names>GianLuca</given-names></name><name><surname>Thiery</surname><given-names>Thomas</given-names></name><name><surname>Moretti</surname><given-names>Riccardo</given-names></name><name><surname>Cisek</surname><given-names>Paul</given-names></name><name><surname>Pezzulo</surname><given-names>Giovanni</given-names></name></person-group><article-title>Beyond simple laboratory studies: developing sophisticated models to study rich behavior</article-title><source>Physics of Life Reviews</source><year>2023</year><pub-id pub-id-type="pmid">37499620</pub-id></element-citation></ref><ref id="R25"><label>[25]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mattar</surname><given-names>Marcelo G</given-names></name><name><surname>Lengyel</surname><given-names>Máté</given-names></name></person-group><article-title>Planning in the brain</article-title><source>Neuron</source><year>2022</year><pub-id pub-id-type="pmid">35041804</pub-id></element-citation></ref><ref id="R26"><label>[26]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McPeek</surname><given-names>Robert M</given-names></name><name><surname>Skavenski</surname><given-names>Alexander A</given-names></name><name><surname>Nakayama</surname><given-names>Ken</given-names></name></person-group><article-title>Concurrent processing of saccades in visual search</article-title><source>Vision Research</source><year>2000</year><volume>40</volume><issue>18</issue><fpage>2499</fpage><lpage>2516</lpage><pub-id pub-id-type="pmid">10915889</pub-id></element-citation></ref><ref id="R27"><label>[27]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Myers</surname><given-names>C</given-names></name><name><surname>Interian</surname><given-names>A</given-names></name><name><surname>Moustafa</surname><given-names>A</given-names></name></person-group><article-title>A practical introduction to using the drift diffusion model of decision-making in cognitive psychology, neuroscience, and health sciences</article-title><source>Frontiers in Psychology</source><year>2022</year><volume>13</volume><pub-id pub-id-type="pmcid">PMC9784241</pub-id><pub-id pub-id-type="pmid">36571016</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2022.1039172</pub-id></element-citation></ref><ref id="R28"><label>[28]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Najemnik</surname><given-names>John</given-names></name><name><surname>Geisler</surname><given-names>Wilson S</given-names></name></person-group><article-title>Optimal eye movement strategies in visual search</article-title><source>Nature</source><year>2005</year><volume>434</volume><issue>7031</issue><fpage>387</fpage><lpage>391</lpage><pub-id pub-id-type="pmid">15772663</pub-id></element-citation></ref><ref id="R29"><label>[29]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Newell</surname><given-names>Allen</given-names></name><name><surname>Simon</surname><given-names>Herbert Alexander</given-names></name><etal/></person-group><source>Human problem solving</source><publisher-name>Prenticehall Englewood Cliffs</publisher-name><publisher-loc>NJ</publisher-loc><year>1972</year><volume>104</volume></element-citation></ref><ref id="R30"><label>[30]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parkhurst</surname><given-names>Derrick</given-names></name><name><surname>Law</surname><given-names>Effie</given-names></name><name><surname>Niebur</surname><given-names>Ernst</given-names></name></person-group><article-title>Modeling the role of salience in the allocation of overt visual attention</article-title><source>Vision Research</source><year>2002</year><volume>42</volume><fpage>107</fpage><lpage>123</lpage><pub-id pub-id-type="pmid">11804636</pub-id></element-citation></ref><ref id="R31"><label>[31]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parkhurst</surname><given-names>Derrick J</given-names></name><name><surname>Niebur</surname><given-names>Ernst</given-names></name></person-group><article-title>Scene content selected by active vision</article-title><source>Spatial Vision</source><year>2003</year><volume>16</volume><issue>2</issue><fpage>125</fpage><lpage>154</lpage><pub-id pub-id-type="pmid">12696858</pub-id></element-citation></ref><ref id="R32"><label>[32]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelz</surname><given-names>Jeff B</given-names></name><name><surname>Canosa</surname><given-names>Roxanne</given-names></name></person-group><article-title>Oculomotor behavior and perceptual strategies in complex tasks</article-title><source>Vision Research</source><year>2001</year><volume>41</volume><issue>25</issue><fpage>3587</fpage><lpage>3596</lpage><pub-id pub-id-type="pmid">11718797</pub-id></element-citation></ref><ref id="R33"><label>[33]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname><given-names>Roger</given-names></name></person-group><article-title>A theory of memory retrieval</article-title><source>Psychological review</source><year>1978</year><volume>85</volume><issue>2</issue><fpage>59</fpage></element-citation></ref><ref id="R34"><label>[34]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rayner</surname><given-names>Keith</given-names></name></person-group><article-title>Eye movements and attention in reading, scene perception, and visual search</article-title><source>Quarterly Journal of Experimental Psychology</source><year>2009</year><volume>62</volume><issue>8</issue><fpage>1457</fpage><lpage>1506</lpage><pub-id pub-id-type="pmid">19449261</pub-id></element-citation></ref><ref id="R35"><label>[35]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robinson</surname><given-names>David A</given-names></name></person-group><article-title>A method of measuring eye movemnent using a scieral search coil in a magnetic field</article-title><source>IEEE Transactions on Biomedical Electronics</source><year>1963</year><volume>10</volume><issue>4</issue><fpage>137</fpage><lpage>145</lpage><pub-id pub-id-type="pmid">14121113</pub-id></element-citation></ref><ref id="R36"><label>[36]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rothkopf</surname><given-names>Constantin A</given-names></name><name><surname>Ballard</surname><given-names>Dana H</given-names></name><name><surname>Hayhoe</surname><given-names>Mary M</given-names></name></person-group><article-title>Task and context determine where you look</article-title><source>Journal of Vision</source><year>2016</year><volume>7</volume><issue>14</issue><fpage>16</fpage><pub-id pub-id-type="pmid">18217811</pub-id></element-citation></ref><ref id="R37"><label>[37]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shah</surname><given-names>Ashvin</given-names></name><name><surname>Barto</surname><given-names>Andrew G</given-names></name><name><surname>Fagg</surname><given-names>Andrew H</given-names></name></person-group><article-title>A dual process account of coarticulation in motor skill acquisition</article-title><source>Journal of motor behavior</source><year>2013</year><volume>45</volume><issue>6</issue><fpage>531</fpage><lpage>549</lpage><pub-id pub-id-type="pmid">24116847</pub-id></element-citation></ref><ref id="R38"><label>[38]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stone</surname><given-names>Mervyn</given-names></name></person-group><article-title>Models for choice-reaction time</article-title><source>Psychometrika</source><year>1960</year><volume>25</volume><issue>3</issue><fpage>251</fpage><lpage>260</lpage></element-citation></ref><ref id="R39"><label>[39]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tomov</surname><given-names>Martin S</given-names></name><name><surname>Yagati</surname><given-names>Sai</given-names></name><name><surname>Kumar</surname><given-names>Ankit</given-names></name><name><surname>Yang</surname><given-names>William</given-names></name><name><surname>Gershman</surname><given-names>Samuel J</given-names></name></person-group><article-title>Discovery of hierarchical representations for efficient planning</article-title><source>PLoS Computational Biology</source><year>2020</year><volume>16</volume><issue>4</issue><elocation-id>e1007594</elocation-id><pub-id pub-id-type="pmcid">PMC7162548</pub-id><pub-id pub-id-type="pmid">32251444</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007594</pub-id></element-citation></ref><ref id="R40"><label>[40]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whalen</surname><given-names>DH</given-names></name></person-group><article-title>Coarticulation is largely planned</article-title><source>Journal of Phonetics</source><year>1990</year><volume>18</volume><issue>1</issue><fpage>3</fpage><lpage>35</lpage></element-citation></ref><ref id="R41"><label>[41]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoo</surname><given-names>Seng BumMichael</given-names></name><name><surname>Hayden</surname><given-names>Benjamin Yost</given-names></name><name><surname>Pearson</surname><given-names>John M</given-names></name></person-group><article-title>Continuous decisions</article-title><source>Philosophical Transactions of the Royal Society B</source><year>2021</year><volume>376</volume><issue>1819</issue><elocation-id>20190664</elocation-id><pub-id pub-id-type="pmcid">PMC7815426</pub-id><pub-id pub-id-type="pmid">33423634</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2019.0664</pub-id></element-citation></ref><ref id="R42"><label>[42]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>Min</given-names></name><name><surname>Marquez</surname><given-names>Andre G</given-names></name></person-group><article-title>Understanding humans’ strategies in maze solving</article-title><source>arXiv</source><year>2013</year><elocation-id>arXiv:1307.5713</elocation-id><comment>preprint</comment></element-citation></ref><ref id="R43"><label>[43]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>Seren</given-names></name><name><surname>Lakshminarasimhan</surname><given-names>Kaushik J</given-names></name><name><surname>Arfaei</surname><given-names>Nastaran</given-names></name><name><surname>Angelaki</surname><given-names>Dora E</given-names></name></person-group><article-title>Eye movements reveal spatiotemporal dynamics of visually-informed planning in navigation</article-title><source>eLife</source><year>2022</year><volume>11</volume><elocation-id>e73097</elocation-id><pub-id pub-id-type="pmcid">PMC9135400</pub-id><pub-id pub-id-type="pmid">35503099</pub-id><pub-id pub-id-type="doi">10.7554/eLife.73097</pub-id></element-citation></ref><ref id="R44"><label>[44]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zingale</surname><given-names>Carolina M</given-names></name><name><surname>Kowler</surname><given-names>Eileen</given-names></name></person-group><article-title>Planning sequences of saccades</article-title><source>Vision research</source><year>1987</year><volume>27</volume><issue>8</issue><fpage>1327</fpage><lpage>1341</lpage><pub-id pub-id-type="pmid">3424681</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>An example planning problem.</title><p>The problem requires finding a path in the grid that starts from the start location (yellow node) and collects all the “gems” (red nodes), without passing through the same node twice. Participants solved the problems by controlling a computer mouse on a PC. The figure shows four time steps of the solution. The azure path shows the path taken by a participant (which was visible to participants), the azure dots show the cursor trajectory sampled at 60 Hz and the small red dots show gaze positions (both of which were not visible to participants). The participant sees the grid (A) and chooses an incorrect path toward two gems on the right (B), then back-tracks to the starting point (C) and finally identifies the correct path to solve the problem (D).</p></caption><graphic xlink:href="EMS200378-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Examples of the three types of coarticulation that we investigate.</title><p>(A-B) Two hypothetical sequences of cursor movements, from the yellow to the blue and then the red nodes, without coarticulation (A) and with coarticulation (B). In the latter case, rather than moving directly from the yellow to the blue note, a shortcut to the red node is taken. Hence, the kinematics of the first cursor movement are influenced by the next cursor direction (to the <bold>East</bold>) and carry some predictive information about it. (C-D) Two hypothetical sequences of saccade fixations, indicated by numbers, without cross-effector (gaze-cursor) coarticulation (C) and with cross-effector (gaze-cursor) coarticulation (D). In the former case, the fixation points are close to the target nodes, without a specific direction. Rather, in the latter case, fixation points slightly deviate towards the next target. (E-F) Two hypothetical sequences of saccade fixations, indicated by numbers, without (E) and with gaze coarticulation (F). While in the first case the angular position of the first and the second fixation points (relative to the first target node) are different, in the latter case they are very similar.</p></caption><graphic xlink:href="EMS200378-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Gaze fixations and cursor movements during the solution of an example problem.</title><p>(A) A visualization of the gaze-cursor coordination during the solution of a problem. On the y-axis nodes of the graph named after their coordinates; on the x-axis time. The blue and orange lines represent gaze and cursor positions collapsed onto the closest node of the graph. (B) Difference in gaze-cursor euclidean distance between consecutive fixations. (solid red line) <italic>μ</italic> is the mean of the distribution; (dashed red line) <italic>σ</italic> the standard deviation. (C) Temporal derivative of the distance between the gaze and the cursor position during a fixation over a normalized time (percentile of the fixation duration). (solid blue line) <italic>μ</italic> is the mean of the distribution; (shaded blue area) <italic>σ</italic> is the standard deviation interval; the (darker) red line is the estimated <italic>p<sub>value</sub></italic> at each time (one-sample t-test), and the (lighter) red line is a threshold <italic>p<sub>value</sub></italic>(<italic>t</italic>) = 0.05. (D) gaze-cursor euclidean distance during the implementation phase.</p></caption><graphic xlink:href="EMS200378-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Quantification of the coarticulation effect in the next cursor direction.</title><p>(A, D) Accuracy of the LDA models <italic>within</italic> and <italic>between</italic> gestures for (A) the gaze position and (D) the cursor trajectory. Error bars are the standard deviation obtained across 10 replicas of the cross-validation. (B-C; E-F) Confusion matrices for the LDA models <italic>within</italic> and <italic>between</italic> gestures for all classes (<italic>E</italic>, <italic>N</italic>, <italic>W</italic>) for (B-C) gaze position and (E-F) Cursor trajectory.</p></caption><graphic xlink:href="EMS200378-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Quantification of the coarticulation effect in the next saccade direction.</title><p>(A) Relation between the current fixation angle and the next fixation angle, in a Cartesian plane. The orange line represents the fitted values of the circular-to-circular regression. (B) Accuracy of the circular-to-circular regression model in the general case. The boxplots represent the accuracy of the models. The dashed line is the chance level.</p></caption><graphic xlink:href="EMS200378-f005"/></fig></floats-group></article>