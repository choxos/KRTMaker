<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS199617</article-id><article-id pub-id-type="doi">10.1101/2024.10.21.619532</article-id><article-id pub-id-type="archive">PPR928929</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Quality assessment and control of unprocessed anatomical, functional, and diffusion MRI of the human brain using <italic>MRIQC</italic></article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Hagen</surname><given-names>McKenzie P.</given-names></name><xref ref-type="corresp" rid="CR1">*</xref><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Provins</surname><given-names>Céline</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>MacNicol</surname><given-names>Eilidh</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Jamie K.</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Gomez</surname><given-names>Teresa</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Garcia</surname><given-names>Mélanie</given-names></name><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Seeley</surname><given-names>Saren H.</given-names></name><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Legarreta</surname><given-names>Jon Haitz</given-names></name><xref ref-type="aff" rid="A7">7</xref></contrib><contrib contrib-type="author"><name><surname>Norgaard</surname><given-names>Martin</given-names></name><xref ref-type="aff" rid="A8">8</xref><xref ref-type="aff" rid="A9">9</xref></contrib><contrib contrib-type="author"><name><surname>Bissett</surname><given-names>Patrick G.</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Poldrack</surname><given-names>Russell A.</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Rokem</surname><given-names>Ariel</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A10">10</xref></contrib><contrib contrib-type="author"><name><surname>Esteban</surname><given-names>Oscar</given-names></name><xref ref-type="corresp" rid="CR1">*</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A4">4</xref></contrib><aff id="A1"><label>1</label>Department of Psychology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cvxb145</institution-id><institution>University of Washington</institution></institution-wrap>, <city>Seattle</city>, <state>WA</state>, <country country="US">USA</country></aff><aff id="A2"><label>2</label>Department of Radiology, Lausanne University Hospital and <institution-wrap><institution-id institution-id-type="ror">https://ror.org/019whta54</institution-id><institution>University of Lausanne</institution></institution-wrap>, <city>Lausanne</city>, <country country="CH">Switzerland</country></aff><aff id="A3"><label>3</label>Department of Neuroimaging, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/0220mzb33</institution-id><institution>King’s College London</institution></institution-wrap>, <city>London</city>, <country country="GB">UK</country></aff><aff id="A4"><label>4</label>Department of Psychology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Stanford University</institution></institution-wrap>; <city>Palo Alto</city>, <state>CA</state>, <country country="US">USA</country></aff><aff id="A5"><label>5</label>Department of Psychiatry, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/002pd6e78</institution-id><institution>Massachusetts General Hospital</institution></institution-wrap>, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/03wevmz92</institution-id><institution>Harvard Medical School</institution></institution-wrap>, <city>Boston</city>, <state>MA</state>, <country country="US">USA</country></aff><aff id="A6"><label>6</label>Department of Psychiatry, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/04a9tmd77</institution-id><institution>Icahn School of Medicine at Mount Sinai</institution></institution-wrap>, <city>New York</city>, <state>NY</state>, <country country="US">USA</country></aff><aff id="A7"><label>7</label>Department of Radiology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/04b6nzv94</institution-id><institution>Brigham and Women’s Hospital</institution></institution-wrap>, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/04py2rh25</institution-id><institution>Mass General Brigham</institution></institution-wrap>/<institution-wrap><institution-id institution-id-type="ror">https://ror.org/03wevmz92</institution-id><institution>Harvard Medical School</institution></institution-wrap>, <city>Boston</city>, <state>MA</state>, <country country="US">USA</country></aff><aff id="A8"><label>8</label>Department of Computer Science, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/035b05819</institution-id><institution>University of Copenhagen</institution></institution-wrap>, <city>Copenhagen</city>, <country country="DK">Denmark</country></aff><aff id="A9"><label>9</label>Molecular Imaging Branch, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/04xeg9z08</institution-id><institution>National Institute of Mental Health</institution></institution-wrap>, <city>Bethesda</city>, <state>MD</state>, <country country="US">USA</country></aff><aff id="A10"><label>10</label>eScience Institute, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cvxb145</institution-id><institution>University of Washington</institution></institution-wrap>, <city>Seattle</city>, <state>WA</state>, <country country="US">USA</country></aff></contrib-group><author-notes><corresp id="CR1">
<label>*</label><email>mphagen@uw.edu</email>; <email>phd@oscaresteban.es</email>.</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>25</day><month>10</month><year>2024</year></pub-date><pub-date pub-type="preprint"><day>22</day><month>10</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nd/4.0/">CC BY-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Quality control of MRI data prior to preprocessing is fundamental, as substandard data are known to increase variability spuriously. Currently, no automated or manual method reliably identifies subpar images, given pre-specified exclusion criteria. In this work, we propose a protocol describing how to carry out the visual assessment of T1-weighted, T2-weighted, functional, and diffusion MRI scans of the human brain with the visual reports generated by <italic>MRIQC</italic>. The protocol describes how to execute the software on all the images of the input dataset using typical research settings (i.e., a high-performance computing cluster). We then describe how to screen the visual reports generated with <italic>MRIQC</italic> to identify artifacts and potential quality issues and annotate the latter with the “rating widget” ─ a utility that enables rapid annotation and minimizes bookkeeping errors. Integrating proper quality control checks on the unprocessed data is fundamental to producing reliable statistical results and crucial to identifying faults in the scanning settings, preempting the acquisition of large datasets with persistent artifacts that should have been addressed as they emerged.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Ensuring the quality of acquired data is a crucial initial step for any magnetic resonance imaging (MRI) analysis workflow because low-quality images may alter the statistical outcomes<sup><xref ref-type="bibr" rid="R2">2</xref>–<xref ref-type="bibr" rid="R5">5</xref></sup>. Therefore, a quality assurance (QA) check on the reconstructed images as they are produced by the scanner ─ i.e., “<italic>acquired</italic>” or “<italic>unprocessed</italic>” data─ is fundamental to identify subpar instances and prevent their progression through analysis when they meet predefined exclusion criteria. Data curation protocols<sup><xref ref-type="bibr" rid="R6">6</xref></sup> for MRI data are still being established, and automated tools to carry out quality control (QC) are still in their infancy <sup><xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R8">8</xref></sup>. Laboratories currently address the challenge of QC by applying their internal knowledge to visual assessments or by accounting for artifacts and other quality issues in the subsequent statistical analysis. One common practice for doing QC of unprocessed data entails screening every slice of every scan individually<sup><xref ref-type="bibr" rid="R9">9</xref></sup>, which is time-consuming, subjective, prone to human errors, and variable within and between raters. Additionally, QA/QC protocols may vary substantially across MRI modalities due to available visualization software or knowledge about modality-specific artifacts. These unstandardized protocols can result in low intra- and inter-rater reliability, hindering the definition of objective exclusion criteria in studies. Intra-rater variability derives from aspects such as training, subjectivity, varying annotation settings and protocols, fatigue, or bookkeeping errors. The difficulty in calibrating between experts and the lack of agreed exclusion criteria, which are contingent on each particular application<sup><xref ref-type="bibr" rid="R10">10</xref></sup>, lies at the heart of inter-rater variability. Adhering to a well-developed standard operating procedure (SOP) that describes the QC process can minimize these variabilities.</p><p id="P3">With the current data deluge in neuroimaging, manual QC of every scan has become onerous for typically-sized datasets and impractical for consortium-sized datasets, exacerbating the problems mentioned above. For the smaller datasets, QC can be streamlined by using informative visualizations to rate images and minimize bookkeeping effort. For large-scale databases, early approaches<sup><xref ref-type="bibr" rid="R11">11</xref>–<xref ref-type="bibr" rid="R14">14</xref></sup> to objectively estimate image quality have employed “image quality metrics” (IQMs) that quantify objectively defined, although variably interpretable, aspects of image quality (e.g., summary statistics of image intensities, signal-to-noise ratio, coefficient of joint variation, Euler angle, etc.) Importantly, these IQMs are defined without a <italic>canonical</italic> (i.e., artifact-free) reference. These IQMs can also be used for automated QC<sup><xref ref-type="bibr" rid="R11">11</xref>–<xref ref-type="bibr" rid="R14">14</xref></sup> using machine learning models to predict scan quality <sup><xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R15">15</xref>–<xref ref-type="bibr" rid="R21">21</xref></sup>.</p><p id="P4">This work proposes a protocol outlining best practices for developing and integrating QA/QC of functional, structural, and diffusion-weighted MRI (sMRI, fMRI, and dMRI, respectively) within the Standard Operating Procedures (SOPs) of whole-brain neuroimaging studies. It describes the execution of <italic>MRIQC</italic> to generate IQMs and visual reports designed to assess data quality. Once generated, these visual reports can be used to view and rate individual scans expeditiously. Finally, these IQMs and ratings can be used to curate datasets.</p><sec id="S2"><title>Development of the protocol</title><p id="P5">This protocol describes how to carry out the visual assessment of T<sub>1</sub>-weighted (T1w), T<sub>2</sub>-weighted (T2w) sMRI, BOLD (blood-oxygen-level-dependent) fMRI, and dMRI scans of the human brain with the visual reports generated by <italic>MRIQC</italic><sup><xref ref-type="bibr" rid="R7">7</xref><xref ref-type="fn" rid="FN1">a</xref></sup>. We outline how to execute the software on all the images of the input dataset using typical research settings (i.e., a high-performance computing cluster). We then detail how to screen the visual reports to identify artifacts and potential quality issues. We report the usage of the <italic>rating widget</italic> utility, which enables rapid annotation and minimizes bookkeeping errors by allowing the ratings and annotations to be locally downloaded as JSON files. These expert annotations can additionally be submitted to the <italic>MRIQC Web-API</italic><sup><xref ref-type="bibr" rid="R8">8</xref></sup>, a web service for crowdsourcing and sharing MRI image quality metrics and QC visual assessments. A visual abstract of the proposed protocol is given in <xref ref-type="fig" rid="F1">Figure 1</xref>. Note that depending on your data collection procedures, you may run these steps in a different order, or you may run some steps multiple times.</p><p id="P6">As further described in the paper originally presenting the tool<sup><xref ref-type="bibr" rid="R22">22</xref></sup>, <italic>MRIQC</italic> leverages the Brain Imaging Data Structure<sup><xref ref-type="bibr" rid="R23">23</xref></sup> (BIDS) to understand the input dataset’s particular features and available metadata (i.e., imaging parameters). BIDS allows <italic>MRIQC</italic> to configure an appropriate workflow without manual intervention automatically. To do so, <italic>MRIQC</italic> self-adapts to the dataset by applying a set of heuristics that account for irregularities such as missing acquisitions or runs. Adaptiveness is implemented with modularity: <italic>MRIQC</italic> comprises sub-workflows, which are dynamically assembled into appropriate configurations. These building blocks combine tools from widely used, open-source neuroimaging packages. The workflow engine <italic>Nipype</italic><sup><xref ref-type="bibr" rid="R24">24</xref></sup> is used to stage the workflows and deal with execution details (such as resource management). Then <italic>NiReports</italic> visual reporting system is used to create informative “reportlets”, which are atomic visualizations of the imaging data, such as mosaic views. Finally, <italic>NiReports</italic> composes them in the final “individual report”.</p><sec id="S3"><title>Applications of the protocol and target audience</title><p id="P7">A robust QA/QC strategy is vital to any neuroimaging research workflow (for instance, as shown in our tandem protocol<sup><xref ref-type="bibr" rid="R25">25</xref></sup> for the case of preprocessed fMRI). However, QC of the data immediately after its acquisition and before any subsequent processing is frequently overlooked. Instead, it is merged with quality checkpoints set after data have been preprocessed or revisited only after the results have been obtained to explain failures a posteriori. While this might seem sufficient, only considering QC after analyses can enable “cherry-picking” subjects to skew analysis results. Even if QC is done earlier in an analytics workflow, before statistical analysis but after preprocessing, artifacts that render datasets unsuitable may go unnoticed. For example, brain masking can hide artifacts most evident in the background of an MRI image.</p><p id="P8">Therefore, this protocol shows how to streamline a QA/QC checkpoint immediately after image acquisition, highlighting the importance of carrying out the step before any other data inspection or computation of derived summary statistics. For example, responsive QA/QC enables decisions such as recalling a participant to repeat an otherwise failed acquisition in complex experimental designs. The protocol also proposes checklists to standardize the assessment and mechanisms to specify exclusion criteria pre-existing before the data are screened, which is critical to minimize intra- and inter-rater variabilities and to standardize QA/QC. This protocol can also be used on fully collected datasets when there may already be hundreds of subjects to screen with the use of the visual reports generated by <italic>MIRQC</italic>. For even larger datasets, this protocol enables data aggregation for future use in automated QC using machine learning regression and classification models.</p><p id="P9">This protocol targets MRI practitioners who routinely include sMRI, fMRI, and/or dMRI in their imaging protocols. In particular, this protocol will be of special interest to researchers starting a career in neuroimaging, such as graduate students and research coordinators who may be delegated to carry out the curation and QC of acquired data. These “newcomer” roles are likely less familiar with the impact of low data quality on subsequent analyses or what would make one particular scan “unusable” in comparison with a poor (albeit acceptable for the purpose of the study) scan. We anticipate this protocol will also be useful for PIs aiming to standardize early and reliable QC within their laboratories. This protocol can also serve as a resource to Research Directors, Engineers, and Managers of scanning centers to outsource some of the burdens in QA and early detection of scanner-related artifacts. In the long term, this protocol sets the foundations for implementing real-time QA strategies and streamlining QC within the MRI scanner pipeline. Finally, the present approach will serve for reference in the development of QA/QC protocols for other modalities, such as Positron Emission Tomography (PET).</p></sec><sec id="S4"><title>Advantages and adaptations</title><p id="P10">This protocol leverages <italic>MRIQC</italic>, which is a widely adopted and consolidated tool as evidenced by millions of IQMs already crowdsourced by the <italic>MRIQC Web-API</italic> service (<xref ref-type="fig" rid="F5">Fig. 4</xref>, right panel of Poldrack et al., 2024<sup><xref ref-type="bibr" rid="R26">26</xref></sup>), and the over 10,000 downloads of <italic>MRIQC</italic>’s <italic>Docker</italic> image. <italic>MRIQC</italic> is a part of the <italic>NiPreps (</italic><italic><bold>N</bold>euro<bold>i</bold>maging <bold>Prep</bold>rocessing tool<bold>s</bold></italic>)<sup><xref ref-type="fn" rid="FN2">b</xref></sup> initiative, which comes along with major advantages such as a large user base, a standardized approach to development, and complementary companion QA/QC resources. <italic>NiPreps</italic> provide comprehensive tooling and documentation for reporting and research protocol management. For example, we have presented a previous protocol for <italic>fMRIPrep</italic><sup><xref ref-type="bibr" rid="R25">25</xref></sup>. <italic>NiPreps</italic>’ <italic>NiReports</italic> module provides standardized visualization components that allow users to leverage knowledge and training across tools to do robust QC at multiple necessary stages. Notably, the “<italic>SOPs-cookiecutter</italic>” framework stands out in implementing version-controlled, collaborative, and eventually publicly shared study SOPs.</p><p id="P11">The present protocol addresses T1w, T2w, BOLD, and dMRI of the human brain, with adaptations to other imaging modalities and animal imaging existing<sup><xref ref-type="bibr" rid="R27">27</xref></sup> or planned. One significant advantage is the consistency of visualizations and procedures across these modalities, decreasing the time required for researcher training and uniformizing rater experience. <italic>MRIQC</italic> additionally has a rating widget to rate and annotate images easily. Finally, the <italic>MRIQC Web-API</italic> service enables the development of machine learning models for a more reliable and semi-manually —if not fully-automated— QA/QC.</p><p id="P12"><italic>MRIQC</italic> has been comprehensively tested on images acquired at 1T-3T field strengths, as showcased by the millions of unique records crowdsourced by the <italic>MRIQC Web-API</italic> database. We are not aware of specific issues preventing <italic>MRIQC</italic> from performing on high-field systems. However, given the limited availability of 7T systems for human research, these users must use caution and are specifically encouraged to report issues.</p><sec id="S5"><title>Imaging acceleration</title><p id="P13">Echo-planar imaging (EPI) acceleration techniques such as standard in-plane acceleration or multi-band EPI sequences employed in fMRI and dMRI, are supported. Nonetheless, these acceleration techniques may introduce specific artifacts that may be difficult to assess with <italic>MRIQC</italic>’s reports, requiring additional QA/QC actions.</p></sec><sec id="S6"><title>Animal species</title><p id="P14"><italic>MRIQC</italic> for rodent brains runs on anatomical and BOLD fMRI<sup><xref ref-type="bibr" rid="R27">27</xref></sup>. These adaptations bridge human and preclinical QA/QC protocols, and the generation of homologous IQMs may also lead to better information integration across different model species. Because rodent MRI data typically employ high-field MRI, rodent QA/QC is also key to bridging gaps towards assessing the rapidly increasing volumes of 7T MRI of the human brain. Other species, such as nonhuman primates, are not currently supported.</p></sec><sec id="S7"><title>Positron Emission Tomography</title><p id="P15">Development of <italic>MRIQC</italic> sibling tools for other imaging modalities such as <italic>PETQC</italic> PET will be able to standardly replicate the framework. With the advent of simultaneous PET/MRI scanners, the upcoming integration of PET with structural and functional MRI data further emphasizes the need for consistent and multimodal QA/QC metrics and tools.</p></sec></sec><sec id="S8"><title>Limitations</title><sec id="S9"><title>Multi-echo BOLD fMRI</title><p id="P16">Recent versions of <italic>MRIQC</italic> (above 23.1.1) generate a single report for BOLD fMRI scans employing multi-echo EPI, facilitating and expediting the assessment of these images. Within each report, every echo is visualized separately. However, defining a unique set of IQMs extracted from all echos in the scan remains as an active line of development.</p></sec><sec id="S10"><title>Other MRI techniques</title><p id="P17"><italic>MRIQC</italic> currently only runs on T1w, T2w, dMRI, and BOLD fMRI, and does not run on other modalities such as quantitative MRI data, or non-BOLD fMRI data such as ASL.</p></sec><sec id="S11"><title><italic>MRIQC</italic> is intended for <italic>unprocessed</italic> data</title><p id="P18">The purpose of <italic>MRIQC</italic> is to assess the quality of the original data before any processing is performed. To assess the results of processing steps, other tools must be used. We refer the reader to previously published fMRI QA/QC guidelines<sup><xref ref-type="bibr" rid="R10">10</xref></sup> to see how to assess sMRI and fMRI data preprocessed by <italic>fMRIPrep</italic> using its visual report, and the <ext-link ext-link-type="uri" xlink:href="https://qsiprep.readthedocs.io/en/latest/preprocessing.html#visual-reports"><italic>QSIprep</italic> documentation</ext-link><sup><xref ref-type="fn" rid="FN3">c</xref></sup> <italic>for dMRI preprocessing</italic>.</p></sec><sec id="S12"><title><italic>MRIQC</italic> is not a preprocessing tool</title><p id="P19">Despite that <italic>MRIQC</italic>’s workflow implements standard preprocessing steps such as head-motion estimation, neither the outputs of internal processing steps nor final outcomes should be employed in downstream analysis other than to implement QC exclusion decisions.</p></sec><sec id="S13"><title>Other limitations</title><p id="P20">We have evaluated the biases introduced by the process of defacing (i.e., removal of facial features from anatomical scans to protect privacy prior to data sharing) into both human ratings and automatically computed IQMs<sup><xref ref-type="bibr" rid="R28">28</xref></sup>. Our results indicate that it is recommended to run QA/QC on “nondefaced” data when available<sup><xref ref-type="bibr" rid="R29">29</xref></sup>.</p></sec></sec><sec id="S14"><title>Approaches complementing <italic>MRIQC</italic> in QA/QC</title><sec id="S15"><title>QA/QC of the Human Connectome Project (HCP)</title><p id="P21">The HCP has maintained a rigorous MRI QA/QC protocol tailored for the project<sup><xref ref-type="bibr" rid="R30">30</xref></sup>. For researchers following the HCP image acquisition and processing protocols, the HCP QA/QC may be a better (although mutually non-exclusive) and more comprehensive option.</p></sec><sec id="S16"><title>QC protocol of the UK Biobank (UKB)</title><p id="P22">The massive scale of the UKB required an automated solution to exclude subpar images from analyses. Alfaro-Almagro et al.<sup><xref ref-type="bibr" rid="R18">18</xref></sup> developed an ensemble classifier to determine image quality based on a number of <italic>image-derived phenotypes</italic> (e.g., 190 features for the case of T1w images). A relevant aspect of the UKB protocol is that rather than assessing/controlling for the quality of input images, the goal is to QC the outcome of the UKB preprocessing pipelines, discarding input images that will make downstream processing pipelines fail or generate results of insufficient quality.</p></sec><sec id="S17"><title>Swipes for Science</title><p id="P23">Keshavan et al.<sup><xref ref-type="bibr" rid="R31">31</xref></sup> proposed a creative solution to the problem of visually assessing large datasets. They were able to annotate over 80,000 two-dimensional slices extracted from 722 brain 3D images using <italic>Braindr</italic>, a smartphone application for crowdsourcing (<ext-link ext-link-type="uri" xlink:href="https://braindr.us/">https://braindr.us/</ext-link>). They also proposed a novel approach to the QC problem by training a convolutional neural network on <italic>Braindr</italic> ratings, with excellent results (area under the curve, 0.99). Their QC tool performed as well as an <italic>MRIQC</italic> classifier<sup><xref ref-type="bibr" rid="R7">7</xref></sup> (which uses IQMs and a random forests classifier to decide which images should be excluded) on their single-site dataset. By collecting several ratings per screened entity, they were able to effectively minimize the noisy label problem with the averaging of expert ratings. Limitations of this work include the use of 2D images for annotation.</p></sec><sec id="S18"><title>Healthy Brain Network Preprocessed Open Diffusion Derivatives (HBN-POD2)</title><p id="P24">Ritchie-Halford et al.<sup><xref ref-type="bibr" rid="R32">32</xref></sup> also used crowdsourced QC annotations from processed diffusion images to train a deep learning classifier. They designed <ext-link ext-link-type="uri" xlink:href="https://www.nipreps.org/dmriprep-viewer/"><italic>dmriprep-viewer</italic></ext-link><sup><xref ref-type="fn" rid="FN4">d</xref></sup> and <ext-link ext-link-type="uri" xlink:href="https://fibr.dev/#/"><italic>fibr</italic></ext-link><sup><xref ref-type="fn" rid="FN5">e</xref></sup>, two web applications that display visual reports from the preprocessed data, for experts and community scientists, respectively, to rate the images. These ratings and IQMs extracted from <italic>QSIprep</italic> were combined to successfully train a model predicting expert ratings from both image quality metrics and preprocessed data. In contrast to <italic>MRIQC</italic>, this work used preprocessed data.</p></sec><sec id="S19"><title>BrainQCNet</title><p id="P25">Author MG and colleagues developed a deep learning solution to predict manual QC annotations assigned by experts<sup><xref ref-type="bibr" rid="R33">33</xref></sup>. For moderately-sized datasets, these tools could complement manual assessment (e.g., <italic>MRIQC</italic>’s visual reports) to reduce human errors and maximize inter-rater agreements. In large-scale datasets where screening of every image is not feasible, tools like <italic>BrainQCNet</italic> could be the only way to implement objective exclusion criteria consistent across studies, sites, and samples.</p></sec><sec id="S20"><title>Fetal MRI QA/QC</title><p id="P26">Author OE and colleagues developed <italic>FETMRQC</italic><sup><xref ref-type="bibr" rid="R34">34</xref></sup>, a tool derived from <italic>MRIQC</italic> and specifically tailored for fetal brain imaging. <italic>FETMRQC</italic> builds on <italic>MRIQC</italic>’s machine-learning framework to address the unique challenges of fetal imaging, mostly relating to uncontrolled fetal motion and heterogeneous acquisition protocols across clinical centers. Standardizing QA/QC in populations at risk, which typically are affected by accute data scarcity, is critical to ensure data reliability.</p></sec><sec id="S21"><title>Real-time QA</title><p id="P27">Complementing <italic>MRIQC</italic> with online QA during scanning is strongly recommended. Indeed, real-time QA<sup><xref ref-type="bibr" rid="R35">35</xref></sup> is an effective way of identifying quality issues early, shortening the time window they remain undetected, and allowing rapid reaction (e.g., repeating a particular acquisition within the session) to minimize data exclusion. In addition to visual inspection by trained technicians, there are automated alternatives such as <italic>AFNI</italic>’s real-time tooling<sup><xref ref-type="bibr" rid="R36">36</xref></sup>, or <italic>OpenNFT</italic><sup><xref ref-type="bibr" rid="R37">37</xref></sup>. In addition to data quality monitoring, motion can be monitored during the scan using software like FIRMM<sup><xref ref-type="bibr" rid="R38">38</xref></sup>, so that researchers can intervene when participants have high motion.</p></sec><sec id="S22"><title>Other manual QC utilities</title><p id="P28">Several alternatives for manual assessment exist, such as <italic>MindControl</italic><sup><xref ref-type="bibr" rid="R39">39</xref></sup>, or <italic>Qoala-T</italic><sup><xref ref-type="bibr" rid="R21">21</xref></sup>, however, they are typically designed for the assessment of surface reconstruction and other derivatives (e.g., segmentations and parcellations) extracted from T1w images. Indeed, reconstructed surfaces have been demonstrated to be a reliable proxy for some aspects of image quality of anatomical images<sup><xref ref-type="bibr" rid="R40">40</xref></sup>. As argued by Niso et al.<sup><xref ref-type="bibr" rid="R41">41</xref></sup>, researchers should include QC checkpoints at the most relevant points of the processing pipeline. In line with this recommendation, <italic>MRIQC</italic> should be used as an QA/QC checkpoint of unprocessed data in addition to (rather than instead of) other checkpoints at later steps (e.g., T1w preprocessing or <italic>fMRIPrep</italic> outputs) of the pipeline. Instead, these utilities should be used in addition to <italic>MRIQC</italic>.</p></sec></sec></sec></sec><sec id="S23" sec-type="materials"><title>Materials</title><sec id="S24"><title>Subject Data</title><p id="P29"><styled-content style="color:#980000">▲<bold>CRITICAL</bold></styled-content> The study must use data acquired after approval by the appropriate ethical review board. If the data are intended to be shared in a public repository such as OpenNeuro (<styled-content style="color:#4A86E8">■<bold>RECOMMENDED</bold></styled-content>), the consent form submitted to the ethical review board should explicitly state that data will be publicly shared (e.g., the Open Brain consent<sup><xref ref-type="bibr" rid="R42">42</xref></sup>) and, if appropriate, the consent form and the data management plan must also comply with any relevant privacy laws regarding pseudo-anonymization (e.g., GDPR in the EU and HIPAA in the USA).</p><p id="P30"><styled-content style="color:#980000">▲<bold>CRITICAL</bold></styled-content> All subjects’ data must be organized according to the BIDS specification. The dataset can be validated (<styled-content style="color:#4A86E8">■<bold>RECOMMENDED</bold></styled-content>) using the <italic>BIDS-Validator</italic>. Conversion to BIDS, and the BIDS-Validator steps are further described below. In this protocol, we use <italic>ds002785</italic> - an open dataset accessed through OpenNeuro.org<sup><xref ref-type="bibr" rid="R43">43</xref></sup>.</p></sec><sec id="S25"><title>Equipment Setup</title><sec id="S26"><title>MRI scanner</title><p id="P31">If the study is acquiring new data, then a standard whole-head scanner is required. <italic>MRIQC</italic> autonomously adapts the preprocessing workflow to the input data, affording researchers the possibility to fine-tune their MR protocols to their experimental needs and design.</p></sec><sec id="S27"><title>Computing hardware</title><p id="P32"><italic>MRIQC</italic> can be executed on almost any platform with enough memory: conventional desktop or laptop hardware, high-performance computing (HPC), or cloud computing. Some elements of the workflow will require a minimum of 8GB RAM, although 16GB is recommended. <italic>MRIQC</italic> is able to optimize the workflow execution via parallelization. The use of 8-16 CPUs is recommended for optimal performance. To store interim results, <italic>MRIQC</italic> requires approximately 250MB per scan. For our example dataset, each subject generated approximately 2GB of interim results. This storage can be temporary, for example a”local scratch” filesystem of a compute node in HPC, which is a fast, local hard-disk that gets cleared after execution. If using other storage, these results can be removed after successfully running <italic>MRIQC</italic>.</p></sec><sec id="S28"><title>Visualization hardware</title><p id="P33">The tools used in this protocol generate HTML reports to carry out visual quality control. These reports contain dynamic, rich visual elements to inspect the data and results from processing steps. Therefore, a high resolution, high static contrast, and widescreen monitor above 30” should be used if available (<styled-content style="color:#4A86E8">■<bold>RECOMMENDED</bold></styled-content>). Visual reports can be opened with standard Web browsers, with <italic>Mozilla Firefox</italic> and <italic>Google Chrome</italic> being routinely tested (<styled-content style="color:#4A86E8">■<bold>RECOMMENDED</bold></styled-content>)., Graphics acceleration support (<styled-content style="color:#4A86E8">■<bold>RECOMMENDED</bold></styled-content>) improves report visualization.</p></sec><sec id="S29"><title>Computing software</title><p id="P34"><italic>MRIQC</italic> can be manually installed (“bare-metal” installation as per its documentation) on <italic>GNU/Linux, Windows Subsystem for Linux (WSL)</italic>, and <italic>macOS</italic> systems, or executed via containers (e.g., using <italic>Docker</italic> for <italic>Windows</italic>). When setting up manually, all software dependencies must also be correctly installed (e.g., <italic>AFNI</italic><sup><xref ref-type="bibr" rid="R36">36</xref></sup>, <italic>ANTs</italic><sup><xref ref-type="bibr" rid="R44">44</xref></sup>, <italic>FSL</italic><sup><xref ref-type="bibr" rid="R45">45</xref></sup>, <italic>Nilearn</italic><sup><xref ref-type="bibr" rid="R46">46</xref></sup>, <italic>Nipype</italic><sup><xref ref-type="bibr" rid="R24">24</xref></sup>, etc.). When using containers (<styled-content style="color:#4A86E8">■<bold>RECOMMENDED</bold></styled-content>), a new container image is distributed from the <italic>Docker Hub</italic> service for each new release of <italic>MRIQC</italic>, which includes all the dependencies pinned to specific versions to ensure the reproducibility of the computational framework. Containers encapsulate all necessary software required to run a particular data processing pipeline akin to virtual machines. However, containers leverage some lightweight virtualization features of the <italic>Linux</italic> kernel without incurring much of the performance penalties of hardware-level virtualization. For these two reasons (reproducibility and computational performance), container execution is <styled-content style="color:#4A86E8">■<bold>RECOMMENDED</bold></styled-content>.</p></sec><sec id="S30"><title>Report evaluation interface</title><p id="P35"><italic>Q’kay</italic><sup><xref ref-type="bibr" rid="R47">47</xref></sup> is a web server interface for viewing and rating <italic>MRIQC</italic>, and other reports generated by <italic>NiReports</italic> (<styled-content style="color:#4A86E8">■<bold>RECOMMENDED)</bold></styled-content>. It collates ratings for each report in a <italic>MongoDB</italic> database for review. See the <ext-link ext-link-type="uri" xlink:href="https://github.com/nipreps/qkay">Q’kay documentation</ext-link><sup><xref ref-type="fn" rid="FN6">f</xref></sup> for installation and usage details.</p></sec></sec></sec><sec id="S31" sec-type="methods"><title>Procedure</title><sec id="S32"><label>0</label><title>Before starting data collection</title><sec id="S33"><label>0.1</label><title>Specify rating procedure and scan exclusion criteria in the study SOPs</title><p id="P36"><styled-content style="color:#980000">▲<bold>CRITICAL</bold></styled-content> Exclusion criteria should be tailored towards a study’s specific analysis plan<sup><xref ref-type="bibr" rid="R10">10</xref></sup>. Since every analysis will have different requirements, there are no formal guidelines for what constitutes a “usable” scan, or a “unusable” scan, and a scan that is suboptimal for one analysis might be fine for another. However, some general qualities to assess are as follows: participant motion, presence of visually identifiable artifacts (especially artifacts that are ubiquitous across participants), and signal to noise ratio (SNR). <italic>MRIQC</italic>’s default “threshold” for plotting framewise displacement (FD) is set at 0.2 mm (see <xref ref-type="fig" rid="F8">Figure 7</xref>), but “acceptable” levels of participant motion vary for different populations of participants. The impact of motion on data quality can be variable, often exacerbated by specific acquisition parameters and may interact with other artifacts like susceptibility distortion. While participant motion and SNR can be evaluated quantitatively, some artifacts require visual identification and may be difficult to conclusively diagnose.</p><p id="P37">Exclusion criteria should be explicitly detailed in the study SOPs<sup><xref ref-type="bibr" rid="R48">48</xref></sup>, which can then be referenced throughout data collection and QC, and shared to increase transparency. These SOPs can also be used to help train new researchers on the QC task. SOPs must contain QA/QC sections including, e.g., checklists of artifacts to look for, definitions for what constitutes an unusable scan, and prescriptions on how discrepancies between raters can be addressed.</p><p id="P38">SOPs can be created, managed, and shared using word processing software like <italic>Word</italic> or <italic>Google Documents</italic>. If version control is desired, <ext-link ext-link-type="uri" xlink:href="https://github.com/"><italic>GitHub</italic></ext-link><sup><xref ref-type="fn" rid="FN7">g</xref></sup> can be used to host SOPs (<styled-content style="color:#4A86E8">■<bold>RECOMMENDED</bold></styled-content>). See the <ext-link ext-link-type="uri" xlink:href="https://www.nipreps.org/sops-cookiecutter/"><italic>NiPreps‘</italic> documentation</ext-link><sup><xref ref-type="fn" rid="FN8">h</xref></sup> for an example of SOPs derived from the <italic>SOPs-cookiecutter</italic> template, with usage details.</p><p id="P39"><styled-content style="color:#980000">▲<bold>CRITICAL</bold></styled-content> If using <italic>GitHub</italic> or the <italic>SOPs-cookiecutter</italic> when creating the new repository, the researcher will very likely want to start a private repository as the study is unlikely to be publicly shareable at this point. <italic>SOPs-cookiecutter</italic> affords find-grained control over private information (e.g., researchers’ phone numbers or contact information for institutional resources), keeping these details separately from the SOPs and marking them as redacted placeholders if the SOPs are openly shared.</p></sec><sec id="S34"><label>0.2</label><title>Researcher training</title><p id="P40">Researchers will need to be familiar with <italic>MRIQC</italic>’s typical outputs and have visualized and assessed a sufficient number of <italic>MRIQC</italic> reports to be able to identify problematic scans and recognize common artifacts. The previously published fMRI QC guidelines<sup><xref ref-type="bibr" rid="R10">10</xref></sup> and in particular its <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/articles/10.3389/fnimg.2022.1073734/full#supplementary-material">Supplementary Material</ext-link><sup><xref ref-type="fn" rid="FN9">i</xref></sup> can help for this training as it provides visual examples of what some problematic artifacts look like.</p></sec><sec id="S35"><label>0.3</label><title>Researcher calibration</title><p id="P41">Whenever possible, more than one researcher should assess each scan to avoid rater bias, and to ensure a robust QC process (<styled-content style="color:#4A86E8">■<bold>RECOMMENDED</bold></styled-content>). In general, ratings and artifact categorizations do not need to be exactly the same, but large discrepancies, especially discrepancies between categorizing scans as unacceptable or acceptable, should be discussed in the context of the data analysis plans and rectified.</p></sec></sec><sec id="S36"><label>1</label><title>Data acquisition and curation</title><sec id="S37"><label>1.1</label><title>Participant preparation</title><p id="P42">Data collection is an integral part of the QC process. Obtain informed consent from subjects, collect prescribed phenotypic information (sex, handedness, etc.), and prepare the participant for the scanning session. The SOPs should include a script for the interaction with the participant and a final checklist to be followed during the preparation of the experiment and setting up (<styled-content style="color:#4A86E8">■<bold>RECOMMENDED</bold></styled-content>; see <ext-link ext-link-type="uri" xlink:href="https://www.axonlab.org/hcph-sops/data-collection/participant-prep/">these SOPs</ext-link><sup><xref ref-type="fn" rid="FN10">j</xref></sup> for an example). Due to the negative impact of motion on data, participants should be thoroughly informed of the importance of staying still, and care should be taken to ensure padding is placed properly for their comfort. Additionally, for populations where increased motion or scan discontinuation due to discomfort or anxiety is likely (children, elderly, some clinical populations), a mock scanner should be utilized prior to the acquisition scan, so that the participants can get acclimated to the scanner bore. Participants should also be informed of the optimal time for swallowing and adjusting position prior to any scans.</p></sec><sec id="S38"><label>1.2</label><title>MRI acquisition</title><p id="P43">Run the prescribed scan protocol. Between acquisitions, continue to check in on participants and remind them to stay still. Researchers may want to visually monitor motion during the scan, especially for high motion populations. Use <italic>ReproIn</italic><sup><xref ref-type="bibr" rid="R49">49</xref></sup> naming conventions when defining the prescribe sequences to ease later conversion to BIDS (<styled-content style="color:#4A86E8">■<bold>RECOMMENDED</bold></styled-content>).</p><p id="P44"><styled-content style="color:#980000">▲<bold>CRITICAL</bold></styled-content> Keep a pristine copy of the original data and metadata.</p></sec></sec><sec id="S39"><label>1.3</label><title>BIDS conversion</title><p id="P45">Store all imaging data in NIfTI-1 or NIfTI-2 file formats as per BIDS specifications, ensuring all metadata are correctly encoded. The process can be made much more reliable and consistent with conversion tools such as <italic>dcm2niix</italic><sup><xref ref-type="bibr" rid="R50">50</xref></sup> or <italic>HeuDiConv</italic><sup><xref ref-type="bibr" rid="R45">45</xref></sup>. The <italic>ReproIn</italic><sup><xref ref-type="bibr" rid="R18">18</xref></sup> naming convention automates the conversion to BIDS with <italic>HeudiConv</italic>, ensuring the shareability and version control of the data starting from the earliest steps of the pipeline. For larger datasets with heterogeneous acquisition parameters <italic>CuBIDS (“Curation of BIDS”)</italic><sup><xref ref-type="bibr" rid="R52">52</xref></sup> can be used to identify all permutations of acquisition parameters.</p><p id="P46"><styled-content style="color:#980000">▲<bold>CRITICAL</bold></styled-content> If data are to be shared publicly, they must be anonymized<sup><xref ref-type="bibr" rid="R53">53</xref></sup> and facial features must be removed from the anatomical images (some tools and recommendations are found with the Open Brain consent project<sup><xref ref-type="bibr" rid="R42">42</xref></sup> and White et al.<sup><xref ref-type="bibr" rid="R54">54</xref></sup>). Despite defacing, it is <styled-content style="color:#4A86E8">■<bold>RECOMMENDED</bold></styled-content> to execute <italic>MRIQC</italic> on the original images before defacing (“nondefaced”). In such cases, maintaining a “private” copy of the dataset in BIDS will be necessary.</p><p id="P47">To ensure that the dataset is BIDS-compliant, use the online <ext-link ext-link-type="uri" xlink:href="https://bids-standard.github.io/bids-validator/">BIDS-Validator</ext-link><sup><xref ref-type="fn" rid="FN11">k</xref></sup> or some up-to-date <ext-link ext-link-type="uri" xlink:href="https://github.com/bids-standard/bids-validator?tab=readme-ov-file#quickstart">local native or containerized installation</ext-link><sup><xref ref-type="fn" rid="FN12">l</xref></sup>, specifying the path to the top-level directory of the dataset (<styled-content style="color:#4A86E8">■<bold>RECOMMENDED</bold></styled-content>). The online BIDS-Validator can be run in any modern browser without uploading any data.</p></sec><sec id="S40"><label>2</label><title>Execute <italic>MRIQC</italic></title><p id="P48">The protocol is described assuming that execution takes place on an HPC cluster with the <italic>Bash</italic> shell, the <italic>SLURM</italic> job scheduler<sup><xref ref-type="bibr" rid="R55">55</xref></sup> and the <italic>Apptainer</italic> container framework<sup><xref ref-type="bibr" rid="R56">56</xref></sup> (v3.0 or higher) installed. With appropriate modifications to the batch-submission directives, the protocol can also be deployed on HPC clusters with alternative job management systems such as <italic>SGE, PBS</italic> or <italic>LSF</italic>. For execution in the cloud or on conventional desktop or laptop, please refer to <italic>MRIQC’s</italic> <ext-link ext-link-type="uri" xlink:href="https://mriqc.readthedocs.io/en/stable/docker.html">documentation</ext-link><sup><xref ref-type="fn" rid="FN13">m</xref></sup>.</p><sec id="S41"><label>2.1</label><title>Setting up the computing environment</title><p id="P49">(<styled-content style="color:#FF9900">●<bold>TIMING</bold></styled-content> 30-45 min). First, define <styled-content style="color:#1C4587; background-color:#EFEFEF"><monospace>$STUDY</monospace></styled-content>, an environment variable pointing at the directory containing all study materials:</p><p id="P50"><preformat preformat-type="computer code">
<styled-content style="color:#1C4587">$ export STUDY=/path/to/study/directory
$ mkdir -p $STUDY
$ cd $STUDY
</styled-content>
</preformat></p><p id="P51">When running <italic>MRIQC</italic> for the first time in a new computing environment, begin by creating a container image. If this is being done on an HPC, be sure to use a compute node rather than a login node to avoid your process being killed for using too many resources. As of <italic>Apptainer</italic> 2.5, it is straightforward to download the container image via the Docker registry:</p><p id="P52"><preformat preformat-type="computer code">
<styled-content style="color:#1C4587">apptainer pull docker://nipreps/mriqc:24.0.0
</styled-content>
</preformat></p><p id="P53">This command line will create an Apptainer image file at the current working directory</p><p id="P54"><preformat preformat-type="computer code">
<styled-content style="color:#1C4587">($STUDY/mriqc_24.0.0.sif).
</styled-content>
</preformat></p><p id="P55"><styled-content style="color:#980000">▲<bold>CRITICAL</bold></styled-content> Be sure to indicate a specific version of <italic>MRIQC</italic> (version 24.0.0, in this example, but the most up to date <italic>MRIQC</italic> is <styled-content style="color:#4A86E8">■<bold>RECOMMENDED</bold></styled-content>). The quality of all datasets and subjects used in any study should be assessed consistently, using the same version of <italic>MRIQC</italic>. The version of <italic>MRIQC</italic> previously used to process any dataset can be identified by consulting the <styled-content style="color:#1C4587; background-color:#EFEFEF"><monospace>GeneratedBy</monospace></styled-content> field of the <styled-content style="color:#1C4587; background-color:#EFEFEF"><monospace>dataset_description.json</monospace></styled-content> file in the top level of <italic>MRIQC</italic>’s output directory or by consulting the <styled-content style="color:#1C4587; background-color:#EFEFEF"><monospace>MRIQC version</monospace></styled-content> field in the <styled-content style="color:#1C4587; background-color:#EFEFEF"><monospace>Summary</monospace></styled-content> box at the top of any visual reports.</p></sec><sec id="S42"><label>2.2</label><title>Stage the dataset on the computing platform</title><p id="P56">(<styled-content style="color:#FF9900">●<bold>TIMING</bold></styled-content> 15 min). Transfer a copy of the nondefaced (if available) BIDS dataset to the designated filesystem that will be accessible from compute nodes. In this protocol case, this path will be represented by the environment variable <styled-content style="color:#1C4587; background-color:#EFEFEF"><monospace>$STUDY</monospace></styled-content>.</p><p id="P57">If you’re using data that you’ve collected, you can copy, move, or download your dataset into <styled-content style="color:#1C4587; background-color:#EFEFEF"><monospace>$STUDY</monospace></styled-content>. If you’re using data available from <italic>OpenNeuro</italic> you can download it using the code snippets provided on <italic>OpenNeuro</italic> for Amazon Web Service S3 or <italic>Datalad</italic>.</p><p id="P58"><styled-content style="color:#4A86E8">■<bold>RECOMMENDED</bold></styled-content> To download the example dataset used in this protocol, deploy a lightweight <italic>Python</italic> environment in the cluster using <italic>Conda, Anaconda, Miniconda</italic>, or <italic>Mamba</italic> and install <italic>DataLad</italic> and its dependency <italic>git-annex</italic>.</p><p id="P59"><preformat preformat-type="computer code">
<styled-content style="color:#1C4587">$ module load Miniconda3 # Replace with the appropriate module name
$ conda create -n datamgmt python=3.12
$ conda install -n datamgmt -c conda-forge git-annex
$ conda activate datamgmt
$ python -m pip install datalad
</styled-content>
</preformat></p><p id="P60"><italic>DataLad</italic> can be used for more advanced dataset management, but for this particular protocol, <italic>DataLad</italic> is only used to download a dataset from <italic>OpenNeuro</italic>.</p><p id="P61"><preformat preformat-type="computer code">
<styled-content style="color:#1C4587">$ conda activate datamgmt
$ cd $STUDY
$ datalad install <ext-link ext-link-type="uri" xlink:href="https://github.com/OpenNeuroDatasets/ds002785.git">https://github.com/OpenNeuroDatasets/ds002785.git</ext-link></styled-content>
</preformat></p><p id="P62">When downloading <italic>DataLad</italic> datasets as above, <italic>DataLad</italic> will not download large imaging files onto the hard-disk, only the dataset structure and human-readable textual files. <italic>MRIQC</italic> integrates <italic>DataLad</italic> starting with its 22.0.3 release and will transparently pull images from the remote storage locations as they are submitted for processing, minimizing storage needs for large datasets. In order to use <italic>DataLad</italic> to manage other datasets from scratch, refer to the <ext-link ext-link-type="uri" xlink:href="https://handbook.datalad.org/en/latest/"><italic>DataLad</italic>Handbook</ext-link><sup><xref ref-type="fn" rid="FN14">n</xref></sup> for further information and reference.</p></sec><sec id="S43"><label>2.3</label><title><italic>Run MRIQC</italic>’s “participant” level</title><p id="P63">(<styled-content style="color:#FF9900">●<bold>TIMING</bold></styled-content> 10-15 minutes per scan, depending on the number, length, and size of imaging schemes in the protocol). Container instances can make use of multiple CPUs to accelerate subject level processing, and multiple container instances can be distributed across compute nodes to parallelize processing across subjects (<styled-content style="color:#4A86E8">■<bold>RECOMMENDED</bold></styled-content>). To run <italic>MRIQC</italic>, first create a batch prescription file with the preferred text file editor, such as <italic>Nano</italic> (nano <styled-content style="color:#1C4587; background-color:#EFEFEF"><monospace>$STUDY/mriqc.sbatch</monospace></styled-content>). <xref ref-type="boxed-text" rid="BX1">Box 2</xref> describes an example of a batch prescription file <styled-content style="color:#1C4587; background-color:#EFEFEF"><monospace>$STUDY/mriqc.sbatch</monospace></styled-content>, and the elements that may be customized for the particular execution environment. After adapting the example script to your HPC using your preferred text editor, submit the job to the scheduler: sbatch <styled-content style="color:#1C4587; background-color:#EFEFEF"><monospace>$STUDY/mriqc.sbatch</monospace></styled-content>. Although the default options are typically sufficient, the <ext-link ext-link-type="uri" xlink:href="https://mriqc.readthedocs.io/en/stable/">documentation of <italic>MRIQC</italic></ext-link><sup><xref ref-type="fn" rid="FN15">o</xref></sup> provides more specific guidelines.</p></sec><sec id="S44"><label>2.4</label><title>Run <italic>MRIQC</italic>’s “group” level</title><p id="P64">(<styled-content style="color:#FF9900">●<bold>TIMING</bold></styled-content> &gt;5 min). Once all “participant” level jobs have completed, run the “group” level using the same paths and commands defined in <xref ref-type="boxed-text" rid="BX1">Box 2</xref> to aggregate IQMs and generate the group-level report.</p><p id="P65"><preformat preformat-type="computer code">
<styled-content style="color:#1C4587">$ $APPTAINER_CMD $BIDS_DIR $OUTPUT_DIR group
</styled-content>
</preformat></p></sec></sec><sec id="S45"><label>3</label><title>Visual inspection of reports</title><sec id="S46"><label>3.1</label><title>Inspect all visual reports generated by <italic>MRIQC</italic></title><p id="P66">(<styled-content style="color:#FF9900">●<bold>TIMING</bold></styled-content> 1-5 min per scan). After running <italic>MRIQC</italic>, inspect all the generated visual reports to identify images with insufficient quality for analysis, according to predefined exclusion criteria. Each scan gets an HTML report, consisting of “reportlet” visualizations that highlight a specific aspect of the scan quality. Refer to the <ext-link ext-link-type="uri" xlink:href="https://mriqc.s3.amazonaws.com/index.html#aomic-piop1/mriqc-23.0.0rc0/">shared reports</ext-link><sup><xref ref-type="fn" rid="FN16">p</xref></sup> for examples, <xref ref-type="boxed-text" rid="BX2">Box 3</xref>, <xref ref-type="boxed-text" rid="BX3">Box 4</xref>, or <xref ref-type="boxed-text" rid="BX4">Box 5</xref> for an inspection protocol, and refer to up-to-date <ext-link ext-link-type="uri" xlink:href="https://mriqc.readthedocs.io/en/latest/reports.html">documentation</ext-link>. For interested readers, more details about the artifacts to inspect, notably the explanation behind their emergence and how to differentiate artifacts that look similar, can be found in the fMRI QC guidelines<sup><xref ref-type="bibr" rid="R10">10</xref></sup>. The built-in rating widget should be used to record overall image quality rating and specific artifacts (<xref ref-type="fig" rid="F2">Figure 2</xref>). <italic>Q’kay</italic> can be used to manage reports and ratings (<styled-content style="color:#4A86E8">■<bold>RECOMMENDED</bold></styled-content>). To minimize bias, reports should be viewed in a random order, and environmental variables (such as screen brightness or size) should be consistent.</p><p id="P67">Some examples of artifacts that could grant exclusion of images from a study are T1w images showing extreme ringing as a result of head motion, irrecoverable signal dropout derived from susceptibility distortions across regions of interest in fMRI or dMRI, excessive N/2 ghosting within fMRI scans, or excessive signal leakage through slices in multiband fMRI reconstructions. Some artifacts may be more obvious in certain visualizations and more subtle in others, so inspecting reports is not strictly a linear process (i.e., once an artifact is identified in one component, checking other components for evidence can help diagnose the problem or evaluate the severity).</p><p id="P68">Visualizations generated with the <styled-content style="color:#1C4587; background-color:#EFEFEF"><monospace>--verbose-reports</monospace></styled-content> flag should be used for debugging software errors, rather than for evaluating scan quality. <italic>MRIQC</italic> implements a quick and coarse workflow, so those visualizations checking the validity of intermediate steps should not be considered to evaluate overall quality.</p><boxed-text id="BX1" position="anchor" orientation="portrait"><label>Box 2</label><caption><title>Running <italic>MRIQC</italic> on HPC.</title></caption><p id="P69">Execution of BIDS-Apps<sup><xref ref-type="bibr" rid="R57">57</xref></sup> (such as <italic>MRIQC</italic> or <italic>fMRIPrep</italic>58) is easy to configure on HPC clusters. We provide below an example execution script for a SLURM-based cluster. An up-to-date, complete version of the script is distributed within <ext-link ext-link-type="uri" xlink:href="https://github.com/nipreps/mriqc/tree/master/docs/source/resources">the documentation</ext-link><sup><xref ref-type="fn" rid="FN17">q</xref></sup>.</p><p id="P70"><preformat preformat-type="computer code">
<styled-content style="color:#000000">#!/bin/bash
##NOTE: These should work with Slurm HPC systems,
# but these specific parameters have only been tested on
# Stanford’s Sherlock. Some parameters may need to be
# adjusted for other HPCs, specifically --partition.
#SBATCH --job-name mriqc
#SBATCH --partition normal #TODO: update for your HPC
#NOTE: The --array parameter allows multiple jobs to be launched at once,
# and is generally recommended to efficiently run several hundred jobs
# at once.
##TODO: adjust the range for your dataset; 1-n%j where n is the number of
# participants and j is the maximum number of concurrent jobs you’d like
# to run.
#SBATCH --array=1-216%50
#SBATCH --time=1:00:00 #NOTE: likely longer than generally needed
#SBATCH --ntasks 1
#SBATCH --cpus-per-task=16
#SBATCH --mem-per-cpu=4G
# Outputs ----------------------------------
#SBATCH --output log/%x-%A-%a.out
#SBATCH --error log/%x-%A-%a.err
#SBATCH --mail-user=%u@stanford.edu #TODO: update for your email domain
#SBATCH --mail-type=ALL
# ------------------------------------------
STUDY=“/scratch/users/mphagen/mriqc-protocol” #TODO: replace with your path
MRIQC_VERSION=“24.0.2” #TODO: update if using a different version
BIDS_DIR=“${STUDY}/ds002785” # TODO: replace with path to your dataset
OUTPUT_DIR=“${BIDS_DIR}/derivatives/mriqc-${MRIQC_VERSION}”
APPTAINER_CMD=“apptainer run -e mriqc_${MRIQC_VERSION}.sif”
# Offset subject index by 1 because of header in participants.tsv
subject_idx=$(( ${SLURM_ARRAY_TASK_ID} + 1))

##NOTE: The first clause in this line selects a row in participants.tsv
  # using the system generated array index variable SLURM_ARRAY_TASK_ID.
  # This is piped to grep to isolate the subject id. The regex should
  # work for most subject naming conventions, but may need to be modified.
subject=$( sed -n ${subject_idx}p ${BIDS_DIR}/participants.tsv \
| grep -oP “sub-[A-Za-z0-9_]*”)
echo Subject $subject
cmd=“${APPTAINER_CMD} ${BIDS_DIR} ${OUTPUT_DIR} participant \
      --participant-label $subject \
      -w $PWD/work/ \
      --omp-nthreads 10 --nprocs 12”
echo Running task ${SLURM_ARRAY_TASK_ID}
echo Commandline: $cmd
eval $cmd
exitcode=$?
echo “sub-$subject ${SLURM_ARRAY_TASK_ID} $exitcode” \
      &gt;&gt; ${SLURM_ARRAY_JOB_ID}.tsv
echo Finished tasks ${SLURM_ARRAY_TASK_ID} with exit code $exitcode exit
$exitcode</styled-content>
</preformat></p></boxed-text><boxed-text id="BX2" position="anchor" orientation="portrait"><label>Box 3</label><caption><title>Individual Anatomical Reports.</title></caption><sec id="S47"><title>Basic visual report</title><p id="P71">The visual report consists of various detailed visualizations of the raw data. We detail below which reportlets are presented for anatomical images and explain potential pitfalls in each visualization.</p></sec><sec id="S48"><title>About</title><p id="P72">Contains metadata (filename, report creation date and time), <italic>MRIQC</italic> version information and specific workflow details. May contain Warnings or Errors, which can be searched on <ext-link ext-link-type="uri" xlink:href="https://neurostars.org/">NeuroStars.org</ext-link><sup><xref ref-type="fn" rid="FN18">r</xref></sup> or the <ext-link ext-link-type="uri" xlink:href="https://github.com/nipreps/mriqc/issues?q=is%3Aissue"><italic>MRIQC</italic> Github Repository</ext-link><sup><xref ref-type="fn" rid="FN19">s</xref></sup>.</p><p id="P73">Extracted Image Quality Metrics (IQMs): Metrics calculated by <italic>MRIQC</italic> that relate to the quality of the images. These are also available in each subject’s directory as a JSON file. Definitions for each IQM can be found in the <italic>MRIQC</italic> <ext-link ext-link-type="uri" xlink:href="https://mriqc.readthedocs.io/en/stable/measures.html">documentation</ext-link><sup><xref ref-type="fn" rid="FN20">t</xref></sup>.</p><p id="P74">Metadata: Scan metadata from the BIDS sidecar such as RepetitionTime, EchoTime and ScannerSequence.</p><p id="P75">Provenance information: Information related to the version of <italic>MRIQC</italic> used to generate the visual report.</p></sec></boxed-text><boxed-text id="BX3" position="anchor" orientation="portrait"><label>Box 4</label><caption><title>Individual Functional Reports.</title></caption><sec id="S49"><title>Basic echo-wise reports</title><p id="P76">The report once again consists of various detailed visualizations of the raw data. For each reportlet presented, we detail below quality aspects for careful consideration. If the data contains multiple echoes, each echo is visualized separately for these reportlets.</p><p id="P77">The severity of artifacts identified in the standard deviation map can be evaluated by looking for evidence of them in the BOLD average or raw NifTI.</p></sec><sec id="S50"><title>About</title><p id="P78">Identical to <xref ref-type="boxed-text" rid="BX3">Box 4</xref> “About”.</p></sec></boxed-text><boxed-text id="BX4" position="anchor" orientation="portrait"><label>Box 5</label><caption><title>Individual Diffusion Reports.</title></caption><sec id="S51"><title>Summary</title><sec id="S52"><title>DWI shells</title><p id="P79">The following reportlets are created for each shell.</p></sec><sec id="S53"><title>About</title><p id="P80">Identical to <xref ref-type="boxed-text" rid="BX3">Box 4</xref> “About”.</p></sec></sec></boxed-text></sec><sec id="S54"><label>3.2</label><title>Visual inspection of group reports</title><p id="P81">(<styled-content style="color:#FF9900">●<bold>TIMING</bold></styled-content> 10-15 min). The group reports can be used to investigate the range and clustering of each IQM, as well as identify participants that are outliers in some IQM distribution. Individual reports can be viewed by clicking on data points, so that researchers can see how the visual reports are impacted by high, low, and average IQM values.</p></sec></sec><sec id="S55"><label>4</label><title>Post assessment of IQMs and ratings</title><sec id="S56"><label>4.1</label><title>QC results management</title><p id="P82">(<styled-content style="color:#FF9900">●<bold>TIMING</bold></styled-content> 10-15 min). After all scans have been inspected, ratings and notes can be exported from <italic>Q’kay</italic>, or collated from downloaded rating JSONs. If more than one rater inspected reports, inter-rater reliability can be assessed, and discrepancies (especially on scans flagged to be excluded) should be noted and resolved. Ratings and exclusion status can be added to the BIDS-required <styled-content style="color:#1C4587; background-color:#EFEFEF"><monospace>participants.tsv</monospace></styled-content> file to select scans to feed into downstream preprocessing tools (like <italic>fMRIPREP</italic>) or incorporation into analytic workflows.</p><p id="P83"><styled-content style="color:#980000">▲<bold>CRITICAL</bold></styled-content> In general, “unuseable” data should not be deleted in case it may be suitable for another analysis, or with future preprocessing advances. Sharing “unuseable” data also supports the development and improvement of automatic QC algorithms. <italic>MRIQC</italic> reports, rating JSONs, and SOPs detailing exclusion criteria should be stored with the raw data to facilitate data sharing and reuse.</p></sec><sec id="S57"><label>4.2</label><title>Automated classification of images (optional)</title><p id="P84">For large datasets, automated QC may be desired to decrease the researcher time required to manually rate images. This can be achieved by training a classifier to predict ratings from the IQMs, using a set of labeled ratings as the training data. Previous experiments using IQMs to predict image quality found that these models can be influenced by the sensitivity of IQMs to scan parameters and site, and may not generalize from one dataset/site to another<sup><xref ref-type="bibr" rid="R7">7</xref></sup>. More details about the IQMs and how to create a classifier can be found in the <ext-link ext-link-type="uri" xlink:href="https://www.nipreps.org/qc-book/welcome.html"><italic>NiPreps</italic> ISMRM QC book</ext-link><sup><xref ref-type="fn" rid="FN21">u</xref></sup>.</p></sec></sec></sec><sec id="S58"><title>Troubleshooting</title><p id="P85">Some of the most common pitfalls encountered by <italic>MRIQC</italic> users relate to resource management and other set-up settings (step 1.3 in <xref ref-type="sec" rid="S36">Data acquisition</xref> or step 2.1 in <xref ref-type="sec" rid="S40">Execute <italic>MRIQC</italic></xref> subsection), as suggested by the many questions the <ext-link ext-link-type="uri" xlink:href="https://github.com/nipreps/mriqc/issues?q=is%3Aissue">source code repository</ext-link><sup><xref ref-type="fn" rid="FN22">v</xref></sup> and the <ext-link ext-link-type="uri" xlink:href="https://NeuroStars.org">NeuroStars.org</ext-link> channel each receive weekly. In particular, the limitations imposed by each HPC system, and the particularities of the <italic>Apptainer</italic> container framework may require some troubleshooting.</p><sec id="S59"><title>Invalid BIDS dataset</title><p id="P86">A fairly common reason for <italic>MRIQC</italic> to fail is the attempt to use non-BIDS data. Therefore, the first troubleshooting step is running the <italic>BIDS-Validator</italic>. When using containers to run <italic>MRIQC</italic>, if the container does not have access to the data, the validator will flag the dataset as invalid. Containers are a confined computation environment and they are not allowed to access the host’s filesystems, unless explicit measures are taken to grant access (i.e., mounting or binding filesystems using the <styled-content style="color:#1C4587; background-color:#EFEFEF"><monospace>-v</monospace></styled-content> flag for <italic>Docker</italic> or the equivalent <styled-content style="color:#1C4587; background-color:#EFEFEF"><monospace>--bind</monospace></styled-content> for <italic>Apptainer</italic>). Therefore, when using containers with a valid BIDS dataset, the “invalid BIDS dataset” could be a symptom of failure to access the data from the host.</p></sec><sec id="S60"><title>Network filesystem errors</title><p id="P87"><italic>MRIQC</italic> is built on <italic>Nipype</italic><sup><xref ref-type="bibr" rid="R24">24</xref></sup>, a neuroimaging workflow framework that uses the filesystem to coordinate the data flow during execution. Network filesystems may exhibit large latencies and temporary inconsistencies that may break execution. Setting the “working directory” option to a local, synchronized filesystem will preempt these issues.</p></sec><sec id="S61"><title>Memory errors</title><p id="P88">When running on systems with restrictive memory overcommit policies (frequently found in multi-tenant HPC systems), the <italic>MRIQC</italic> virtual memory footprint may become too large, and the process will be stopped by the job scheduler or the operating system kernel. The recommendation in this scenario is to split (parallelize) processing across subjects (<xref ref-type="boxed-text" rid="BX1">Box 2</xref> showcases a solution). Alternatively, when running on a system with 8GB RAM or less, <italic>MRIQC</italic> is likely to exceed physical memory limits. This scenario is particularly common when running the container version of <italic>MRIQC</italic>, because the container has access to a very low physical memory allocation. For example, <italic>Docker</italic> typically limits memory to 2GB by default on <italic>macOS</italic> and <italic>Windows</italic> systems. In this case, the solution is to increase the memory allocation available to <italic>MRIQC</italic> (via adequate settings of the container engine and/or upgrading the hardware).</p></sec><sec id="S62"><title>Hard disk quotas</title><p id="P89">Shared systems generally limit the hard disk space a user can use. Please allocate enough space for both intermediate and final results. Remove intermediate results as soon as satisfied with the final results to free up scratch space.</p></sec><sec id="S63"><title>PyBIDS indexing</title><p id="P90">For large datasets, <italic>PyBIDS</italic> indexing inflates run time. To optimize run time, a pre-indexed cache of the BIDS structure and metadata can be created with <italic>PyBIDS</italic>, and will substantially speed <italic>MRIQC</italic> up:</p><p id="P91"><preformat preformat-type="computer code">
<styled-content style="color:#1C4587">$ python -m pip install pybids
$ mkdir $STUDY/ds002785/.bids-index/
$ pybids layout --no-validate --index-metadata. $STUDY/ds002785/.bids-index/
</styled-content>
</preformat></p><p id="P92">Once the pre-indexed cache is built, you should inform <italic>MRIQC</italic> of its location by setting the command line argument <styled-content style="color:#1C4587; background-color:#EFEFEF"><monospace>--bids-database-dir $STUDY/ds002785/.bids-index/</monospace></styled-content>.</p></sec><sec id="S64"><title>NeuroStars forum</title><p id="P93">Many other frequently asked questions are found and responded to at <ext-link ext-link-type="uri" xlink:href="https://neurostars.org/">NeuroStars.org</ext-link>. New support requests are welcome via this platform.</p></sec></sec><sec id="S65"><title>Anticipated results</title><p id="P94">The successful application of this protocol produces the following outcomes: visual reports and qualitative ratings from assessing those reports, and quantitative metrics of image quality for each scan in the dataset. These can be used to prepare the dataset for analysis by identifying scans that do or do not meet the exclusion criteria. Additionally, this information can be used to generate data to train a classifier for your scanner, enabling automated image quality assessment.</p></sec></body><back><ack id="S66"><title>Acknowledgements</title><p>We thank the invaluable feedback on this report from Jo Etzel, Ph.D., as well as her continued support to our QA/QC endeavors. This work was supported by Chan-Zuckerberg-Initiative (EOSS5-266, O.E.), the NIMH (R24MH114705 and R24MH117179, R.A.P.; RF1MH121867, O.E., A.R., and R.A.P.). C.P. and O.E. receive support from the Swiss National Science Foundation —SNSF— (grant #185872, O.E.). This material is based upon work supported by the U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, Department of Energy Computational Science Graduate Fellowship under Award Number DE-SC0023112 (M.P.H). This work was supported in part by the NIH National Institute on Mental Health (T32MH122394) and National Institute on Aging (F31AG062067, S.S.). MN was supported by the BRAIN initiative (MH002977-01).</p></ack><sec id="S67" sec-type="data-availability"><title>Data availability</title><p id="P95">The MRI data used in this protocol are publicly available at <ext-link ext-link-type="uri" xlink:href="https://OpenNeuro.org">OpenNeuro.org</ext-link> (doi: 10.18112/openneuro.ds002785.v2.0.0). <italic>MRIQC</italic> reports generated as part of this protocol are available at <ext-link ext-link-type="uri" xlink:href="https://mriqc.s3.amazonaws.com/index.html#Hagen2024/">https://mriqc.s3.amazonaws.com/index.html#Hagen2024/</ext-link>.</p></sec><fn-group><fn id="FN1"><label>a</label><p id="P96"><ext-link ext-link-type="uri" xlink:href="https://mriqc.readthedocs.io/en/latest/#">https://mriqc.readthedocs.io/en/latest/#</ext-link></p></fn><fn id="FN2"><label>b</label><p id="P97"><ext-link ext-link-type="uri" xlink:href="https://www.nipreps.org/">https://www.nipreps.org/</ext-link></p></fn><fn id="FN3"><label>c</label><p id="P98"><ext-link ext-link-type="uri" xlink:href="https://qsiprep.readthedocs.io/en/latest/preprocessing.html#visual-reports">https://qsiprep.readthedocs.io/en/latest/preprocessing.html#visual-reports</ext-link></p></fn><fn id="FN4"><label>d</label><p id="P99"><ext-link ext-link-type="uri" xlink:href="https://www.nipreps.org/dmriprep-viewer/">www.nipreps.org/dmriprep-viewer/</ext-link></p></fn><fn id="FN5"><label>e</label><p id="P100"><ext-link ext-link-type="uri" xlink:href="https://fibr.dev/">https://fibr.dev/</ext-link></p></fn><fn id="FN6"><label>f</label><p id="P101"><ext-link ext-link-type="uri" xlink:href="https://github.com/nipreps/qkay">https://github.com/nipreps/qkay</ext-link></p></fn><fn id="FN7"><label>g</label><p id="P102"><ext-link ext-link-type="uri" xlink:href="https://www.github.com">www.github.com</ext-link></p></fn><fn id="FN8"><label>h</label><p id="P103"><ext-link ext-link-type="uri" xlink:href="https://www.nipreps.org/sops-cookiecutter/">www.nipreps.org/sops-cookiecutter/</ext-link></p></fn><fn id="FN9"><label>i</label><p id="P104"><ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/articles/10.3389/fnimg.2022.1073734/full#supplementary-material">www.frontiersin.org/articles/10.3389/fnimg.2022.1073734/full#supplementary-material</ext-link></p></fn><fn id="FN10"><label>j</label><p id="P105"><ext-link ext-link-type="uri" xlink:href="https://www.axonlab.org/hcph-sops/data-collection/participant-prep/">www.axonlab.org/hcph-sops/data-collection/participant-prep/</ext-link></p></fn><fn id="FN11"><label>k</label><p id="P106"><ext-link ext-link-type="uri" xlink:href="https://bids-standard.github.io/bids-validator/">https://bids-standard.github.io/bids-validator/</ext-link></p></fn><fn id="FN12"><label>l</label><p id="P107"><ext-link ext-link-type="uri" xlink:href="https://github.com/bids-standard/bids-validator?tab=readme-ov-file#quickstart">https://github.com/bids-standard/bids-validator?tab=readme-ov-file#quickstart</ext-link></p></fn><fn id="FN13"><label>m</label><p id="P108"><ext-link ext-link-type="uri" xlink:href="https://mriqc.readthedocs.io/en/stable/docker.html">https://mriqc.readthedocs.io/en/stable/docker.html</ext-link></p></fn><fn id="FN14"><label>n</label><p id="P109"><ext-link ext-link-type="uri" xlink:href="https://handbook.datalad.org/en/latest/">https://handbook.datalad.org/en/latest/</ext-link></p></fn><fn id="FN15"><label>o</label><p id="P110"><ext-link ext-link-type="uri" xlink:href="https://mriqc.readthedocs.io/en/stable/">https://mriqc.readthedocs.io/en/stable/</ext-link></p></fn><fn id="FN16"><label>p</label><p id="P111"><ext-link ext-link-type="uri" xlink:href="https://mriqc.s3.amazonaws.com/index.html#aomic-piop1/mriqc-23.0.0rc0/">https://mriqc.s3.amazonaws.com/index.html#aomic-piop1/mriqc-23.0.0rc0/</ext-link></p></fn><fn id="FN17"><label>q</label><p id="P112"><ext-link ext-link-type="uri" xlink:href="https://github.com/nipreps/mriqc/tree/master/docs/source/resources">https://github.com/nipreps/mriqc/tree/master/docs/source/resources</ext-link></p></fn><fn id="FN18"><label>r</label><p id="P113"><ext-link ext-link-type="uri" xlink:href="https://neurostars.org/">https://neurostars.org/</ext-link></p></fn><fn id="FN19"><label>s</label><p id="P114"><ext-link ext-link-type="uri" xlink:href="https://github.com/nipreps/mriqc/issues?q=is%3Aissue">https://github.com/nipreps/mriqc/issues?q=is%3Aissue</ext-link></p></fn><fn id="FN20"><label>t</label><p id="P115"><ext-link ext-link-type="uri" xlink:href="https://mriqc.readthedocs.io/en/stable/measures.html">https://mriqc.readthedocs.io/en/stable/measures.html</ext-link></p></fn><fn id="FN21"><label>u</label><p id="P116"><ext-link ext-link-type="uri" xlink:href="https://www.nipreps.org/qc-book/">https://www.nipreps.org/qc-book/</ext-link></p></fn><fn id="FN22"><label>v</label><p id="P117"><ext-link ext-link-type="uri" xlink:href="https://github.com/nipreps/mriqc/issues?q=is%3Aissue">https://github.com/nipreps/mriqc/issues?q=is%3Aissue</ext-link></p></fn><fn id="FN23"><p id="P118"><bold>Disclaimer</bold></p><p id="P119">This report was prepared as an account of work sponsored by an agency of the United States Government. Neither the United States Government nor any agency thereof, nor any of their employees, makes any warranty, express or implied, or assumes any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or represents that its use would not infringe privately owned rights. Reference herein to any specific commercial product, process, or service by trade name, trademark, manufacturer, or otherwise does not necessarily constitute or imply its endorsement, recommendation, or favoring by the United States Government or any agency thereof. The views and opinions of authors expressed herein do not necessarily state or reflect those of the United States Government or any agency thereof.</p><p id="P120">This content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.</p></fn><fn fn-type="con" id="FN24"><p id="P121"><bold>Author contribution statement</bold></p><p id="P122">MPH, CP, AR and OE contributed formal analysis, investigation, methodology, validation. MPH, CP and OE wrote the original draft. MPH, CP, and JKL developed and validated the protocol. PB, RAP, AR, and OE contributed to conceptualization, interpretation, overall framing of the protocol, funding acquisition, project administration, resources, and supervision. EM, JHL, MN, MG, SS and TG contributed to software, methodology and validation. All the authors have contributed software and/or documentation, read the manuscript, and edited/revised the original draft and previous versions.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snoek</surname><given-names>L</given-names></name><etal/></person-group><article-title>The Amsterdam Open MRI Collection, a set of multimodal MRI datasets for individual difference analyses</article-title><source>Sci Data</source><year>2021</year><volume>8</volume><fpage>85</fpage><pub-id pub-id-type="pmcid">PMC7979787</pub-id><pub-id pub-id-type="pmid">33741990</pub-id><pub-id pub-id-type="doi">10.1038/s41597-021-00870-6</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Power</surname><given-names>JD</given-names></name><name><surname>Barnes</surname><given-names>KA</given-names></name><name><surname>Snyder</surname><given-names>AZ</given-names></name><name><surname>Schlaggar</surname><given-names>BL</given-names></name><name><surname>Petersen</surname><given-names>SE</given-names></name></person-group><article-title>Spurious but systematic correlations in functional connectivity MRI networks arise from subject motion</article-title><source>NeuroImage</source><year>2012</year><volume>59</volume><fpage>2142</fpage><lpage>2154</lpage><pub-id pub-id-type="pmcid">PMC3254728</pub-id><pub-id pub-id-type="pmid">22019881</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.10.018</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yendiki</surname><given-names>A</given-names></name><name><surname>Koldewyn</surname><given-names>K</given-names></name><name><surname>Kakunoori</surname><given-names>S</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><article-title>Spurious group differences due to head motion in a diffusion MRI study</article-title><source>NeuroImage</source><year>2014</year><volume>88</volume><fpage>79</fpage><lpage>90</lpage><pub-id pub-id-type="pmcid">PMC4029882</pub-id><pub-id pub-id-type="pmid">24269273</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.11.027</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reuter</surname><given-names>M</given-names></name><etal/></person-group><article-title>Head motion during MRI acquisition reduces gray matter volume and thickness estimates</article-title><source>NeuroImage</source><year>2015</year><volume>107</volume><fpage>107</fpage><lpage>115</lpage><pub-id pub-id-type="pmcid">PMC4300248</pub-id><pub-id pub-id-type="pmid">25498430</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.12.006</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alexander-Bloch</surname><given-names>A</given-names></name><etal/></person-group><article-title>Subtle in-scanner motion biases automated measurement of brain anatomy from in vivo MRI</article-title><source>Hum Brain Mapp</source><year>2016</year><volume>37</volume><fpage>2385</fpage><lpage>2397</lpage><pub-id pub-id-type="pmcid">PMC5110234</pub-id><pub-id pub-id-type="pmid">27004471</pub-id><pub-id pub-id-type="doi">10.1002/hbm.23180</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Etzel</surname><given-names>JA</given-names></name></person-group><article-title>Efficient evaluation of the Open QC task fMRI dataset</article-title><source>Front Neuroimaging</source><year>2023</year><volume>2</volume><pub-id pub-id-type="pmcid">PMC10406291</pub-id><pub-id pub-id-type="pmid">37554632</pub-id><pub-id pub-id-type="doi">10.3389/fnimg.2023.1070274</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esteban</surname><given-names>O</given-names></name><etal/></person-group><article-title>MRIQC: Advancing the automatic prediction of image quality in MRI from unseen sites</article-title><source>PLOS ONE</source><year>2017</year><volume>12</volume><elocation-id>e0184661</elocation-id><pub-id pub-id-type="pmcid">PMC5612458</pub-id><pub-id pub-id-type="pmid">28945803</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0184661</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esteban</surname><given-names>O</given-names></name><etal/></person-group><article-title>Crowdsourced MRI quality metrics and expert quality annotations for training of humans and machines</article-title><source>Sci Data</source><year>2019</year><volume>6</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="pmcid">PMC6472378</pub-id><pub-id pub-id-type="pmid">30975998</pub-id><pub-id pub-id-type="doi">10.1038/s41597-019-0035-4</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lepping</surname><given-names>RJ</given-names></name><etal/></person-group><article-title>Quality control in resting-state fMRI: the benefits of visual inspection</article-title><source>Front Neurosci</source><year>2023</year><volume>17</volume><pub-id pub-id-type="pmcid">PMC10192849</pub-id><pub-id pub-id-type="pmid">37214404</pub-id><pub-id pub-id-type="doi">10.3389/fnins.2023.1076824</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Provins</surname><given-names>C</given-names></name><name><surname>MacNicol</surname><given-names>E</given-names></name><name><surname>Seeley</surname><given-names>SH</given-names></name><name><surname>Hagmann</surname><given-names>P</given-names></name><name><surname>Esteban</surname><given-names>O</given-names></name></person-group><article-title>Quality control in functional MRI studies with MRIQC and fMRIPrep</article-title><source>Front Neuroimaging</source><year>2023</year><volume>1</volume><pub-id pub-id-type="pmcid">PMC10406249</pub-id><pub-id pub-id-type="pmid">37555175</pub-id><pub-id pub-id-type="doi">10.3389/fnimg.2022.1073734</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gardner</surname><given-names>EA</given-names></name><etal/></person-group><article-title>Detection of degradation of magnetic resonance (MR) images: Comparison of an automated MR image-quality analysis system with trained human observers</article-title><source>Acad Radiol</source><year>1995</year><volume>2</volume><fpage>277</fpage><lpage>281</lpage><pub-id pub-id-type="pmid">9419562</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woodard</surname><given-names>JP</given-names></name><name><surname>Carley-Spencer</surname><given-names>MP</given-names></name></person-group><article-title>No-Reference image quality metrics for structural MRI</article-title><source>Neuroinformatics</source><year>2006</year><volume>4</volume><fpage>243</fpage><lpage>262</lpage><pub-id pub-id-type="pmid">16943630</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gedamu</surname><given-names>EL</given-names></name><name><surname>Collins</surname><given-names>Dl</given-names></name><name><surname>Arnold</surname><given-names>DL</given-names></name></person-group><article-title>Automated quality control of brain MR images</article-title><source>J Magn Reson Imaging</source><year>2008</year><volume>28</volume><fpage>308</fpage><lpage>319</lpage><pub-id pub-id-type="pmid">18666143</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mortamet</surname><given-names>B</given-names></name><etal/></person-group><article-title>Automatic quality assessment in structural brain magnetic resonance imaging</article-title><source>Magn Reson Med</source><year>2009</year><volume>62</volume><fpage>365</fpage><lpage>372</lpage><pub-id pub-id-type="pmcid">PMC2780021</pub-id><pub-id pub-id-type="pmid">19526493</pub-id><pub-id pub-id-type="doi">10.1002/mrm.21992</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shehzad</surname><given-names>Z</given-names></name><etal/></person-group><article-title>The Preprocessed Connectomes Project Quality Assessment Protocol - a resource for measuring the quality of MRI data</article-title><source>INCF Neuroinformatics (Front Neurosci, Cairns, Australia</source><year>2015</year><pub-id pub-id-type="doi">10.3389/conf.fnins.2015.91.00047</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pizarro</surname><given-names>RA</given-names></name><etal/></person-group><article-title>Automated Quality Assessment of Structural Magnetic Resonance Brain Images Based on a Supervised Machine Learning Algorithm</article-title><source>Front Neuroinformatics</source><year>2016</year><volume>10</volume><pub-id pub-id-type="pmcid">PMC5165041</pub-id><pub-id pub-id-type="pmid">28066227</pub-id><pub-id pub-id-type="doi">10.3389/fninf.2016.00052</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Backhausen</surname><given-names>LL</given-names></name><etal/></person-group><article-title>Quality Control of Structural MRI Images Applied Using FreeSurfer—A Hands-On Workflow to Rate Motion Artifacts</article-title><source>Front Neurosci</source><year>2016</year><volume>10</volume><pub-id pub-id-type="pmcid">PMC5138230</pub-id><pub-id pub-id-type="pmid">27999528</pub-id><pub-id pub-id-type="doi">10.3389/fnins.2016.00558</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alfaro-Almagro</surname><given-names>F</given-names></name><etal/></person-group><article-title>Image processing and Quality Control for the first 10,000 brain imaging datasets from UK Biobank</article-title><source>NeuroImage</source><year>2017</year><pub-id pub-id-type="pmcid">PMC5770339</pub-id><pub-id pub-id-type="pmid">29079522</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.10.034</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname><given-names>T</given-names></name><etal/></person-group><article-title>Automated quality assessment of structural magnetic resonance images in children: Comparison with visual inspection and surface-based reconstruction</article-title><source>Hum Brain Mapp</source><year>2018</year><volume>39</volume><fpage>1218</fpage><lpage>1231</lpage><pub-id pub-id-type="pmcid">PMC6866370</pub-id><pub-id pub-id-type="pmid">29206318</pub-id><pub-id pub-id-type="doi">10.1002/hbm.23911</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keshavan</surname><given-names>A</given-names></name><name><surname>Yeatman</surname><given-names>J</given-names></name><name><surname>Rokem</surname><given-names>A</given-names></name></person-group><article-title>Combining citizen science and deep learning to amplify expertise in neuroimaging</article-title><source>bioRxiv</source><year>2018</year><elocation-id>363382</elocation-id><pub-id pub-id-type="pmcid">PMC6517786</pub-id><pub-id pub-id-type="pmid">31139070</pub-id><pub-id pub-id-type="doi">10.3389/fninf.2019.00029</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klapwijk</surname><given-names>ET</given-names></name><name><surname>van de Kamp</surname><given-names>F</given-names></name><name><surname>van der Meulen</surname><given-names>M</given-names></name><name><surname>Peters</surname><given-names>S</given-names></name><name><surname>Wierenga</surname><given-names>LM</given-names></name></person-group><article-title>Qoala-T: A supervised-learning tool for quality control of FreeSurfer segmented MRI data</article-title><source>NeuroImage</source><year>2019</year><volume>189</volume><fpage>116</fpage><lpage>129</lpage><pub-id pub-id-type="pmid">30633965</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esteban</surname><given-names>O</given-names></name><etal/></person-group><article-title>MRIQC: Advancing the automatic prediction of image quality in MRI from unseen sites</article-title><source>PLOS ONE</source><year>2017</year><volume>12</volume><elocation-id>e0184661</elocation-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name><etal/></person-group><article-title>The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments</article-title><source>Sci Data</source><year>2016</year><volume>3</volume><elocation-id>160044</elocation-id><pub-id pub-id-type="pmcid">PMC4978148</pub-id><pub-id pub-id-type="pmid">27326542</pub-id><pub-id pub-id-type="doi">10.1038/sdata.2016.44</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorgolewski</surname><given-names>K</given-names></name><etal/></person-group><article-title>Nipype: a flexible, lightweight and extensible neuroimaging data processing framework in Python</article-title><source>Front Neuroinformatics</source><year>2011</year><volume>5</volume><fpage>13</fpage><pub-id pub-id-type="pmcid">PMC3159964</pub-id><pub-id pub-id-type="pmid">21897815</pub-id><pub-id pub-id-type="doi">10.3389/fninf.2011.00013</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esteban</surname><given-names>O</given-names></name><etal/></person-group><article-title>Analysis of task-based functional MRI data preprocessed with fMRIPrep</article-title><source>Nat Protoc</source><year>2020</year><volume>15</volume><fpage>2186</fpage><lpage>2202</lpage><pub-id pub-id-type="pmcid">PMC7404612</pub-id><pub-id pub-id-type="pmid">32514178</pub-id><pub-id pub-id-type="doi">10.1038/s41596-020-0327-3</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poldrack</surname><given-names>RA</given-names></name><etal/></person-group><article-title>The Past, Present, and Future of the Brain Imaging Data Structure (BIDS)</article-title><source>ArXiv</source><year>2024</year><elocation-id>arXiv:2309.05768v2</elocation-id><pub-id pub-id-type="pmcid">PMC11415029</pub-id><pub-id pub-id-type="pmid">39308505</pub-id><pub-id pub-id-type="doi">10.1162/imag_a_00103</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>MacNicol</surname><given-names>EE</given-names></name><etal/></person-group><source>Extending MRIQC to rodents: image quality metrics for rat MRI</source><conf-name>Annual Meeting of the European Society for Molecular Imaging (EMIM)</conf-name><conf-loc>Thessaloniki, Greece</conf-loc><year>2022</year><volume>17</volume><fpage>PW23</fpage><lpage>913</lpage></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Provins</surname><given-names>C</given-names></name><etal/></person-group><article-title>Defacing biases in manual and automatic quality assessments of structural MRI with MRIQC</article-title><source>Peer Community Regist Rep Regist Rep Consid Stage</source><volume>1</volume></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Provins</surname><given-names>C</given-names></name><etal/></person-group><article-title>Defacing biases manual and automated quality assessments of structural MRI with MRIQC</article-title><year>2022</year><pub-id pub-id-type="doi">10.31219/osf.io/t9ehk</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marcus</surname><given-names>DS</given-names></name><etal/></person-group><article-title>Human Connectome Project informatics: Quality control, database services, and data visualization</article-title><source>NeuroImage</source><year>2013</year><volume>80</volume><fpage>202</fpage><lpage>219</lpage><pub-id pub-id-type="pmcid">PMC3845379</pub-id><pub-id pub-id-type="pmid">23707591</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.05.077</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keshavan</surname><given-names>A</given-names></name><name><surname>Yeatman</surname><given-names>JD</given-names></name><name><surname>Rokem</surname><given-names>A</given-names></name></person-group><article-title>Combining Citizen Science and Deep Learning to Amplify Expertise in Neuroimaging</article-title><source>Front Neuroinformatics</source><year>2019</year><volume>13</volume><fpage>29</fpage></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richie-Halford</surname><given-names>A</given-names></name><etal/></person-group><article-title>An analysis-ready and quality controlled resource for pediatric brain white-matter research</article-title><source>Sci Data</source><year>2022</year><volume>9</volume><fpage>616</fpage><pub-id pub-id-type="pmcid">PMC9556519</pub-id><pub-id pub-id-type="pmid">36224186</pub-id><pub-id pub-id-type="doi">10.1038/s41597-022-01695-7</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garcia</surname><given-names>M</given-names></name><name><surname>Dosenbach</surname><given-names>N</given-names></name><name><surname>Kelly</surname><given-names>C</given-names></name></person-group><article-title>BrainQCNet: a Deep Learning attention-based model for the automated detection of artifacts in brain structural MRI scans</article-title><source>Imaging Neurosci</source><year>2024</year><pub-id pub-id-type="doi">10.1162/imag_a_00300</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanchez</surname><given-names>T</given-names></name><etal/></person-group><article-title>FetMRQC: A robust quality control system for multi-centric fetal brain MRI</article-title><source>Med Image Anal</source><year>2024</year><volume>97</volume><elocation-id>103282</elocation-id><pub-id pub-id-type="pmid">39053168</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heunis</surname><given-names>S</given-names></name><etal/></person-group><article-title>Quality and denoising in real-time functional magnetic resonance imaging neurofeedback: A methods review</article-title><source>Hum Brain Mapp</source><year>2020</year><volume>41</volume><fpage>3439</fpage><lpage>3467</lpage><pub-id pub-id-type="pmcid">PMC7375116</pub-id><pub-id pub-id-type="pmid">32333624</pub-id><pub-id pub-id-type="doi">10.1002/hbm.25010</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>RW</given-names></name><name><surname>Hyde</surname><given-names>JS</given-names></name></person-group><article-title>Software tools for analysis and visualization of fMRI data</article-title><source>NMR Biomed</source><year>1997</year><volume>10</volume><fpage>171</fpage><lpage>178</lpage><pub-id pub-id-type="pmid">9430344</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koush</surname><given-names>Y</given-names></name><etal/></person-group><article-title>OpenNFT: An open-source Python/Matlab framework for real-time fMRI neurofeedback training based on activity, connectivity and multivariate pattern analysis</article-title><source>NeuroImage</source><year>2017</year><volume>156</volume><fpage>489</fpage><lpage>503</lpage><pub-id pub-id-type="pmid">28645842</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dosenbach</surname><given-names>NUF</given-names></name><etal/></person-group><article-title>Real-time motion analytics during brain MRI improve data quality and reduce costs</article-title><source>NeuroImage</source><year>2017</year><volume>161</volume><fpage>80</fpage><lpage>93</lpage><pub-id pub-id-type="pmcid">PMC5731481</pub-id><pub-id pub-id-type="pmid">28803940</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.08.025</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keshavan</surname><given-names>A</given-names></name><etal/></person-group><article-title>Mindcontrol: A web application for brain segmentation quality control</article-title><source>NeuroImage</source><year>2018</year><volume>170</volume><fpage>365</fpage><lpage>372</lpage><pub-id pub-id-type="pmid">28365419</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosen</surname><given-names>AFG</given-names></name><etal/></person-group><article-title>Quantitative assessment of structural image quality</article-title><source>NeuroImage</source><year>2018</year><volume>169</volume><fpage>407</fpage><lpage>418</lpage><pub-id pub-id-type="pmcid">PMC5856621</pub-id><pub-id pub-id-type="pmid">29278774</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.12.059</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niso</surname><given-names>G</given-names></name><etal/></person-group><article-title>Open and reproducible neuroimaging: from study inception to publication</article-title><source>NeuroImage</source><year>2022</year><elocation-id>119623</elocation-id><pub-id pub-id-type="pmcid">PMC10008521</pub-id><pub-id pub-id-type="pmid">36100172</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119623</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Halchenko</surname><given-names>YO</given-names></name><etal/></person-group><article-title>Open Brain Consent: make open data sharing a no-brainer for ethics committees</article-title><source>Zenodo</source><year>2018</year><pub-id pub-id-type="doi">10.5281/zenodo.1411525</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markiewicz</surname><given-names>CJ</given-names></name><etal/></person-group><article-title>The OpenNeuro resource for sharing of neuroscience data</article-title><source>eLife</source><year>2021</year><volume>10</volume><elocation-id>e71774</elocation-id><pub-id pub-id-type="pmcid">PMC8550750</pub-id><pub-id pub-id-type="pmid">34658334</pub-id><pub-id pub-id-type="doi">10.7554/eLife.71774</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avants</surname><given-names>BB</given-names></name><etal/></person-group><article-title>A reproducible evaluation of ANTs similarity metric performance in brain image registration</article-title><source>NeuroImage</source><year>2011</year><volume>54</volume><fpage>2033</fpage><lpage>44</lpage><pub-id pub-id-type="pmcid">PMC3065962</pub-id><pub-id pub-id-type="pmid">20851191</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.09.025</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name></person-group><article-title>FSL</article-title><source>NeuroImage</source><year>2012</year><volume>62</volume><fpage>782</fpage><lpage>790</lpage><pub-id pub-id-type="pmid">21979382</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abraham</surname><given-names>A</given-names></name><etal/></person-group><article-title>Machine learning for neuroimaging with scikit-learn</article-title><source>Front Neuroinformatics</source><year>2014</year><volume>8</volume><pub-id pub-id-type="pmcid">PMC3930868</pub-id><pub-id pub-id-type="pmid">24600388</pub-id><pub-id pub-id-type="doi">10.3389/fninf.2014.00014</pub-id></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Savary</surname><given-names>E</given-names></name><name><surname>Provins</surname><given-names>C</given-names></name><name><surname>Sanchez</surname><given-names>T</given-names></name><name><surname>Esteban</surname><given-names>O</given-names></name></person-group><source>Q’kay: a manager for the quality assessment of large neuroimaging studies</source><conf-name>29th Annual Meeting of the Organization for Human Brain Mapping (OHBM)</conf-name><year>2023</year><pub-id pub-id-type="doi">10.31219/osf.io/edx6t</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hollmann</surname><given-names>S</given-names></name><etal/></person-group><article-title>Ten simple rules on how to write a standard operating procedure</article-title><source>PLoS Comput Biol</source><year>2020</year><volume>16</volume><elocation-id>e1008095</elocation-id><pub-id pub-id-type="pmcid">PMC7470745</pub-id><pub-id pub-id-type="pmid">32881868</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008095</pub-id></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="web"><collab>OHBM2018 - ReproIn</collab><source>Google Docs</source><comment><ext-link ext-link-type="uri" xlink:href="https://docs.google.com/document/d/1EsQH3kVTdIstvjNoB-9vJxB8kGhtG4guKNaPVe-EP54/edit?usp=embed_facebook">https://docs.google.com/document/d/1EsQH3kVTdIstvjNoB-9vJxB8kGhtG4guKNaPVe-EP54/edit?usp=embed_facebook</ext-link></comment></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Morgan</surname><given-names>PS</given-names></name><name><surname>Ashburner</surname><given-names>J</given-names></name><name><surname>Smith</surname><given-names>J</given-names></name><name><surname>Rorden</surname><given-names>C</given-names></name></person-group><article-title>The first step for neuroimaging data analysis: DICOM to NIfTI conversion</article-title><source>J Neurosci Methods</source><year>2016</year><volume>264</volume><fpage>47</fpage><lpage>56</lpage><pub-id pub-id-type="pmid">26945974</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Halchenko</surname><given-names>YO</given-names></name><etal/></person-group><article-title>Open Source Software: Heudiconv</article-title><source>Zenodo</source><year>2018</year><pub-id pub-id-type="doi">10.5281/zenodo.1306159</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Covitz</surname><given-names>S</given-names></name><etal/></person-group><article-title>Curation of BIDS (CuBIDS): A workflow and software package for streamlining reproducible curation of large BIDS datasets</article-title><source>NeuroImage</source><year>2022</year><volume>263</volume><elocation-id>119609</elocation-id><pub-id pub-id-type="pmcid">PMC9981813</pub-id><pub-id pub-id-type="pmid">36064140</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119609</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwarz</surname><given-names>CG</given-names></name><etal/></person-group><article-title>Changing the face of neuroimaging research: Comparing a new MRI de-facing technique with popular alternatives</article-title><source>NeuroImage</source><year>2021</year><volume>231</volume><elocation-id>117845</elocation-id><pub-id pub-id-type="pmcid">PMC8154695</pub-id><pub-id pub-id-type="pmid">33582276</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2021.117845</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname><given-names>T</given-names></name><name><surname>Blok</surname><given-names>E</given-names></name><name><surname>Calhoun</surname><given-names>VD</given-names></name></person-group><article-title>Data sharing and privacy issues in neuroimaging research: Opportunities, obstacles, challenges, and monsters under the bed</article-title><source>Hum Brain Mapp</source><year>2022</year><volume>43</volume><fpage>278</fpage><lpage>291</lpage><pub-id pub-id-type="pmcid">PMC8675413</pub-id><pub-id pub-id-type="pmid">32621651</pub-id><pub-id pub-id-type="doi">10.1002/hbm.25120</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Yoo</surname><given-names>AB</given-names></name><name><surname>Jette</surname><given-names>MA</given-names></name><name><surname>Grondona</surname><given-names>M</given-names></name></person-group><chapter-title>SLURM: Simple Linux Utility for Resource Management</chapter-title><person-group person-group-type="editor"><name><surname>Feitelson</surname><given-names>D</given-names></name><name><surname>Rudolph</surname><given-names>L</given-names></name><name><surname>Schwiegelshohn</surname><given-names>U</given-names></name></person-group><source>Job Scheduling Strategies for Parallel Processing</source><publisher-name>Springer Berlin Heidelberg</publisher-name><publisher-loc>Seattle, WA, USA</publisher-loc><year>2003</year><fpage>44</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1007/10968987_3</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kurtzer</surname><given-names>GM</given-names></name><name><surname>Sochat</surname><given-names>V</given-names></name><name><surname>Bauer</surname><given-names>MW</given-names></name></person-group><article-title>Singularity: Scientific containers for mobility of compute</article-title><source>mPLOS ONE</source><year>2017</year><volume>12</volume><elocation-id>e0177459</elocation-id><pub-id pub-id-type="pmcid">PMC5426675</pub-id><pub-id pub-id-type="pmid">28494014</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0177459</pub-id></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name><etal/></person-group><article-title>BIDS Apps: Improving ease of use, accessibility, and reproducibility of neuroimaging data analysis methods</article-title><source>PLOS Comput Biol</source><year>2017</year><volume>13</volume><elocation-id>e1005209</elocation-id><pub-id pub-id-type="pmcid">PMC5363996</pub-id><pub-id pub-id-type="pmid">28278228</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005209</pub-id></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esteban</surname><given-names>O</given-names></name><etal/></person-group><article-title>fMRIPrep: a robust preprocessing pipeline for functional MRI</article-title><source>Nat Methods</source><year>2019</year><volume>16</volume><fpage>111</fpage><lpage>116</lpage><pub-id pub-id-type="pmcid">PMC6319393</pub-id><pub-id pub-id-type="pmid">30532080</pub-id><pub-id pub-id-type="doi">10.1038/s41592-018-0235-4</pub-id></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Power</surname><given-names>JD</given-names></name></person-group><article-title>A simple but useful way to assess fMRI scan qualities</article-title><source>NeuroImage</source><year>2017</year><volume>154</volume><fpage>150</fpage><lpage>158</lpage><pub-id pub-id-type="pmcid">PMC5296400</pub-id><pub-id pub-id-type="pmid">27510328</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.08.009</pub-id></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patriat</surname><given-names>R</given-names></name><name><surname>Molloy</surname><given-names>E</given-names></name><name><surname>Birn</surname><given-names>R</given-names></name><name><surname>Guitchev</surname><given-names>T</given-names></name><name><surname>Popov</surname><given-names>A</given-names></name></person-group><article-title>Using Edge Voxel Information to Improve Motion Regression for rs-fMRI Connectivity Studies</article-title><source>Brain Connect</source><year>2015</year><volume>5</volume><fpage>582</fpage><lpage>595</lpage><pub-id pub-id-type="pmcid">PMC4652211</pub-id><pub-id pub-id-type="pmid">26107049</pub-id><pub-id pub-id-type="doi">10.1089/brain.2014.0321</pub-id></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gudbjartsson</surname><given-names>H</given-names></name><name><surname>Patz</surname><given-names>S</given-names></name></person-group><article-title>The rician distribution of noisy mri data</article-title><source>Magn Reson Med</source><year>1995</year><volume>34</volume><fpage>910</fpage><lpage>914</lpage><pub-id pub-id-type="pmcid">PMC2254141</pub-id><pub-id pub-id-type="pmid">8598820</pub-id><pub-id pub-id-type="doi">10.1002/mrm.1910340618</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Quality Control protocol for the assessment of MRI with <italic>MRIQC</italic>.</title><p>Step 0: Before assessing data quality, standard operating procedures (SOPs) for QC should be defined and researchers should be trained on those procedures. Step 1: After data acquisition the dataset needs to be organized following the BIDS specification. Formal verification of the BIDS structure with the <italic>BIDS Validator</italic> is optional but strongly recommended. Step 2: Researchers should use SLURM or other HPC job scheduler to execute <italic>MRIQC</italic> software on every participant for assessment. <italic>MRIQC</italic> generates a visual report for each of the input scans. Step 3: Researchers need to inspect those reports for quality assurance and assign a quality score in accordance with SOPs. The final rating can be shared via <italic>MRIQC Web-API</italic> and/or saved as a JSON text file. Step 4: Finally, users should review and store those QC reports alongside the data. Optionally, users can also train a classification or regression model on their data to operate within-site quality rating prediction to aid the assessment of future collection efforts.</p></caption><graphic xlink:href="EMS199617-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><p>Ratings Widget With the ratings widget, you can take note of which artifacts are present in the scan, and give it an overall quality score using the slider ranging on a continuous scale from 1 to 4 (1 : exclude, 4 : excellent quality). This information will be locally downloaded as a JSON file, which can be saved along with other dataset metadata, and used for final dataset curation. This data can also optionally be uploaded to the MRIQC Web-API for crowdsourced QC projects.</p></caption><graphic xlink:href="EMS199617-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>View of the background of the anatomical image.</title><p>This visualization is an enhancement of the background noise. Any artifacts identified in this image may also be visible in the zoomed-in image. Look for: <list list-type="bullet" id="L1"><list-item><p>Excessive visual noise in the background of the image, specifically in or around the brain, or “structured” noise (global or local noise, image reconstruction errors, EM interference).</p></list-item><list-item><p>Motion artifacts, commonly seen as “waves” coming out from the skull (head motion artifacts).</p></list-item><list-item><p>Faint and shifted copies of the brain in the background (aliasing ghosts).</p></list-item></list></p></caption><graphic xlink:href="EMS199617-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Zoomed-in mosaic view of the brain.</title><p>This visualization is the T1w or T2w scan, sliced, and plotted in a mosaic view, allowing you to look at the whole brain at once. Look for: <list list-type="bullet" id="L2"><list-item><p>Brain not displayed in the right orientation (axis flipped or switched because of data formatting issues).</p></list-item><list-item><p>Ripples caused by head motion or Gibbs ringing that blurs some brain areas.</p></list-item><list-item><p>Brain areas that are distorted, or extreme deviations from typical anatomy, which may indicate an incidental finding, or an artifact such as susceptibility distortion.</p></list-item><list-item><p>Inconsistency in the contrast between light and dark areas of the brain that does not reflect actual gray matter/white matter anatomy. It is typically caused by intensity non-uniformity, which manifests as a slow and smooth drift in image intensity throughout the brain.</p></list-item><list-item><p>“Wrap-around” where a piece of the head gets cut-off and folded over on the opposite extreme of the image. It is a problem only if the folded region contains or overlaps with the region of interest (problematic FOV).</p></list-item><list-item><p>Eye-spillover, where eye movements trigger signal leakage that might overlap with signal from regions of interest (eye spillover through PE axis).</p></list-item></list></p></caption><graphic xlink:href="EMS199617-f004"/></fig><fig id="F5" position="float"><label>Figure 4</label><caption><title>Standard deviation of signal through time.</title><p>This visualization corresponds to the standard deviation of the BOLD signal in each voxel, plotted as sagittal and axial cross sections, with yellow representing the most extreme values. The eyes and arteries will typically be the brightest yellow, a result of physiological motion. Look for: <list list-type="bullet" id="L3"><list-item><p>An extra outline of the brain shifted on the acquisition-axis (aliasing ghost if it overlaps with the actual image, other ghost if it does not).</p></list-item><list-item><p>Piece of the head (usually the front or back) outside of the field of view that folds over on the opposite extreme of the image (problematic FOV prescription / wrap around). This is a problem only if the folded region contains or overlaps with the region of interest.</p></list-item><list-item><p>Symmetric brightness on the edges of the skull (head motion).</p></list-item><list-item><p>Any excessive variability as indicated by patterns of brightness that is unlikely to be genuine BOLD activity, such as vertical strikes in the sagittal plane that extends hyperintensities through the whole plane.</p></list-item></list></p></caption><graphic xlink:href="EMS199617-f005"/></fig><fig id="F6" position="float"><label>Figure 5</label><caption><title>View of the background of the voxel-wise average of the BOLD timeseries.</title><p>Mosaic view of the average BOLD signal, with background enhancement. Look for: <list list-type="bullet" id="L4"><list-item><p>An extra outline of the brain shifted on the acquisition-axis (aliasing ghost if it overlaps with the actual image, other ghost if it does not).</p></list-item><list-item><p>Excessive visual noise in the background of the image, specifically in or around the brain, or “structured” noise (global or local noise, image reconstruction errors, EM interference).</p></list-item></list></p></caption><graphic xlink:href="EMS199617-f006"/></fig><fig id="F7" position="float"><label>Figure 6</label><caption><title>Voxel-wise average of BOLD time-series, zoomed-in covering just the brain.</title><p>This visualization is the average values of the BOLD signal in each voxel across the entire scan duration, plotted as sagittal and axial cross sections. Look for: <list list-type="bullet" id="L5"><list-item><p>Brain not displayed in the right orientation (axis flipped or switched because of data formatting issues).</p></list-item><list-item><p>Piece of the head (usually the front or back) outside of the field of view that folds over on the opposite extreme of the image (problematic FOV prescription / wrap around). It is a problem only if the folded region contains or overlaps with the region of interest.</p></list-item><list-item><p>Missing or particularly blurry slices (coil failure, local noise).</p></list-item><list-item><p>Signal drop-outs or brain distortions, especially close to brain/air interfaces such as prefrontal cortex or the temporal lobe next to the ears (susceptibility distortions), or from unremoved metallic items (EM interference).</p></list-item><list-item><p>Uneven image brightness, especially near anatomical areas that would be closer to the head coils (intensity non-uniformity).</p></list-item><list-item><p>Blurriness (local noise if confined to one area of the scan, global noise if present in the entire image).</p></list-item></list></p></caption><graphic xlink:href="EMS199617-f007"/></fig><fig id="F8" position="float"><label>Figure 7</label><caption><title>Carpetplot and nuisance signals.</title><p>This reportlet is aimed at emphasizing changes in voxel intensity throughout an fMRI scan. The core of this visualization is the carpet plot, which works by plotting voxel time series in close spatial proximity so that the eye notes temporal coincidence<sup><xref ref-type="bibr" rid="R59">59</xref></sup>. The carpet plot is segmented into relevant regions, notably the “crown” or brain-edge area that corresponds to voxels located on a closed band around the brain<sup><xref ref-type="bibr" rid="R60">60</xref></sup>. Above the carpet plot, several time series are represented to support the interpretation of the carpet. The time series plotted are the slice-wise noise average on background, the % outliers, DVARS, and framewise displacement (FD). Look for: <list list-type="bullet" id="L6"><list-item><p>Prolonged dark deflections on the carpet plot paired with peaks in FD (motion artifacts caused by sudden movements).</p></list-item><list-item><p>Periodic modulations of the carpet plot (motion artifacts caused by regular, slow movement like respiration).</p></list-item><list-item><p>Sudden change in overall signal intensity on the carpet plot not paired with FD peaks, and generally sustained through the end of the scan (coil failure).</p></list-item><list-item><p>Strongly polarized structure in the crown region of the carpet plot is a sign of artifacts, because those voxels should not present signal as they are outside the brain.</p></list-item><list-item><p>Consistently high values and large peaks in FD or DVARS (motion artifacts).</p></list-item><list-item><p>An average FD above your pre-defined threshold (motion artifacts).</p></list-item><list-item><p>Sudden spikes in the slice-wise noise average on background that affect a single slice (motion artifact or white-pixel noise depending on whether it is paired with a peak in FD or DVARS or not).</p></list-item></list></p></caption><graphic xlink:href="EMS199617-f008"/></fig><fig id="F9" position="float"><label>Figure 8</label><caption><title>Shell-wise joint distribution of SNR vs FA in every voxel.</title><p>The top visualization shows a heatmap of the estimated SNR and FA for every voxel by shell. The higher shells should have lower SNRs. The bottom visualization shows the histogram of SNR values independent of FA, separated by shell. Look for: <list list-type="bullet" id="L7"><list-item><p>Overlap between the SNR distributions for each shell (can indicate suboptimal acquisition parameters).</p></list-item><list-item><p>Linear correlations between FA and SNR, which indicates noise contamination.</p></list-item><list-item><p>Non-normal distributions of SNR for each shell; MRI noise is Rician, which means that SNR distributions that are mostly above approximately 2 should resemble a normal distribution in each shell<sup><xref ref-type="bibr" rid="R61">61</xref></sup>.</p></list-item></list></p></caption><graphic xlink:href="EMS199617-f009"/></fig><fig id="F10" position="float"><label>Figure 9</label><caption><title>Fractional anisotropy (FA) map.</title><p>The reconstructed FA values for each voxel, plotted by slices in a mosaic layout. Look for: <list list-type="bullet" id="L8"><list-item><p>Brain not displayed in the right orientation (axis flipped or switched because of data formatting issues).</p></list-item><list-item><p>Piece of the head (usually the front or back) outside of the field of view that folds over on the opposite extreme of the image (problematic FOV prescription / wrap around). It is a problem only if the folded part overlaps with the region of interest.</p></list-item><list-item><p>Blurriness or lack of contrast between white matter and gray matter. You should be able to identify major white matter structures, such as the corpus callosum. FA should be higher in white matter tissue, and especially high in white matter areas where there is a predominant fiber orientation (e.g., the corpus callosum).,</p></list-item><list-item><p>Excessive white speckles, especially if they’re located in the interior of the brain (reconstruction error).</p></list-item></list></p></caption><graphic xlink:href="EMS199617-f010"/></fig><fig id="F11" position="float"><label>Figure 10</label><caption><title>Mean diffusivity (MD) map.</title><p>The reconstructed MD values for each voxel, plotted by slices in a mosaic layout. Look for: <list list-type="bullet" id="L9"><list-item><p>Brain not displayed in the right orientation (axis flipped or switched because of data formatting issues).</p></list-item><list-item><p>Piece of the head (usually the front or back) outside of the field of view that folds over on the opposite extreme of the image (problematic FOV prescription / wrap around).</p></list-item><list-item><p>Blurriness or lack of contrast between white matter, gray matter, and ventricles (local noise if confined to one area of the scan, global noise if present in the entire image).</p></list-item></list></p></caption><graphic xlink:href="EMS199617-f011"/></fig><fig id="F12" position="float"><label>Figure 11</label><caption><title>Voxel-wise average and standard deviation across volumes in this DWI shell.</title><p>This visualization transitions between the voxel-wise average and the standard deviation values for each voxel, with yellow indicating higher variance. A red line delineates the calculated brain mask. As the b-value increases, the yellow signal should clearly resemble white matter tracts. Note that shells with a b-value of 0 may not have any variability if only one gradient was collected. Look for: <list list-type="bullet" id="L10"><list-item><p>An extra outline of the brain shifted on the acquisition-axis (aliasing ghost if it overlaps with the actual image, other ghost if there’s no overlap).</p></list-item><list-item><p>Piece of the head (usually the front or back) outside of the field of view that folds over on the opposite extreme of the image (problematic FOV prescription / wrap around). This is a problem only if the folded region contains or overlaps with the region of interest.</p></list-item><list-item><p>Symmetric brightness on the edges of the skull (head motion).</p></list-item><list-item><p>Any excessive variability as indicated by patterns of brightness, such as brightness localized to one section of the brain or outside of white matter tracts (global or local noise).</p></list-item></list></p></caption><graphic xlink:href="EMS199617-f012"/></fig><fig id="F13" position="float"><label>Figure 12</label><caption><title>View of the background of the voxel-wise average of this DWI shell.</title><p>This visualization is an enhancement of the background noise. Any artifacts identified in this image may also be visible in the FA or MD maps. Look for: <list list-type="bullet" id="L11"><list-item><p>An extra outline of the brain shifted on the acquisition-axis (aliasing ghost if it overlaps with the actual image, other ghost if it does not).</p></list-item><list-item><p>Excessive visual noise in the background of the image, specifically in or around the brain, or “structured” noise (global or local noise, image reconstruction errors, EM interference).</p></list-item></list></p></caption><graphic xlink:href="EMS199617-f013"/></fig><fig id="F14" position="float"><label>Figure 13</label><caption><title>MRIQC: group T1w report.</title><p>The group report for each MRI modality shows the distribution of each IQM across participants.</p></caption><graphic xlink:href="EMS199617-f014"/></fig></floats-group></article>