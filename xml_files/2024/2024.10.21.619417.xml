<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS199633</article-id><article-id pub-id-type="doi">10.1101/2024.10.21.619417</article-id><article-id pub-id-type="archive">PPR929550</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">2</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Visual objects refine head direction coding</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Siegenthaler</surname><given-names>Dominique</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Denny</surname><given-names>Henry</given-names></name><xref ref-type="aff" rid="A4">4</xref><xref ref-type="fn" rid="FN1">#</xref></contrib><contrib contrib-type="author"><name><surname>Skromne Carrasco</surname><given-names>Sofía</given-names></name><xref ref-type="aff" rid="A4">4</xref><xref ref-type="fn" rid="FN1">#</xref></contrib><contrib contrib-type="author"><name><surname>Mayer</surname><given-names>Johanna Luise</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="fn" rid="FN1">#</xref></contrib><contrib contrib-type="author"><name><surname>Levenstein</surname><given-names>Daniel</given-names></name><xref ref-type="aff" rid="A4">4</xref><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Peyrache</surname><given-names>Adrien</given-names></name><xref ref-type="aff" rid="A4">4</xref><xref ref-type="fn" rid="FN2">*</xref></contrib><contrib contrib-type="author"><name><surname>Trenholm</surname><given-names>Stuart</given-names></name><xref ref-type="aff" rid="A4">4</xref><xref ref-type="fn" rid="FN2">*</xref></contrib><contrib contrib-type="author"><name><surname>Macé</surname><given-names>Émilie</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="corresp" rid="CR1">†</xref><xref ref-type="fn" rid="FN2">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>Dynamics of Excitable Cell Networks Group, Department of Ophthalmology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/021ft0n22</institution-id><institution>University Medical Center Göttingen</institution></institution-wrap>, <city>Göttingen</city>, <country country="DE">Germany</country></aff><aff id="A2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05xy1nn52</institution-id><institution>Cluster of Excellence “Multiscale Bioimaging: from Molecular Machines to Networks of Excitable Cells” (MBExC)</institution></institution-wrap>, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01y9bpm73</institution-id><institution>University of Göttingen</institution></institution-wrap>, <country country="DE">Germany</country></aff><aff id="A3"><label>3</label>Brain-Wide Circuits for Behavior Research Group, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/03g267s60</institution-id><institution>Max Planck Institute for Biological Intelligence</institution></institution-wrap>, <city>Planegg</city>, <country country="DE">Germany</country></aff><aff id="A4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05ghs6f64</institution-id><institution>Montreal Neurological Institute</institution></institution-wrap>, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01pxwe438</institution-id><institution>McGill University</institution></institution-wrap>, <city>Montreal</city>, <country country="CA">Canada</country></aff><aff id="A5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05c22rx21</institution-id><institution>Mila</institution></institution-wrap>, <city>Montreal</city>, <country country="CA">Canada</country></aff><author-notes><corresp id="CR1">
<label>†</label>Corresponding author; <email>emilie.mace@med.uni-goettingen.de</email>
</corresp><fn id="FN1"><label>#</label><p id="P1">co-second authors</p></fn><fn id="FN2"><label>*</label><p id="P2">co-last authors</p></fn></author-notes><pub-date pub-type="nihms-submitted"><day>25</day><month>10</month><year>2024</year></pub-date><pub-date pub-type="preprint"><day>22</day><month>10</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P3">Animals use visual objects to guide navigation-related behaviors, from hunting prey, to escaping predators, to exploring the world. However, little is known about where visual objects are encoded in the mouse brain or how they impact processing in the spatial navigation system. Using functional ultrasound imaging in mice, we conducted a brain-wide screen and identified brain areas that were preferentially activated by images of objects compared to scrambled versions of the same images. While visual cortical areas did not show a significant preference, regions associated with spatial navigation were preferentially activated by visual objects. Electrophysiological recordings in postsubiculum, the primary cortical area of the head direction (HD) system, further confirmed a preference for visual objects, which was present in both HD cells and fast-spiking interneurons. Finally, we found that visual objects dynamically modulated HD cells, selectively increasing firing rates of HD cells aligned with a visual landmark’s direction, while decreasing activity in HD cells coding for other directions. These results reveal that visual objects refine population-level coding of head direction.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P4">The spatial navigation system is comprised of neurons whose firing rate is modulated by variables including the direction an animal is facing and the location it occupies in an environment<sup><xref ref-type="bibr" rid="R1">1</xref>–<xref ref-type="bibr" rid="R3">3</xref></sup>. In turn, these neurons are believed to support the cognitive map<sup><xref ref-type="bibr" rid="R4">4</xref>,<xref ref-type="bibr" rid="R5">5</xref></sup>. However, in addition to spatial variables, neurons within the spatial navigation system are modulated by visual inputs<sup><xref ref-type="bibr" rid="R6">6</xref></sup>, as visual objects<sup><xref ref-type="bibr" rid="R7">7</xref></sup> can serve as environmental landmarks<sup><xref ref-type="bibr" rid="R8">8</xref></sup>. Moving a visual landmark in an environment can result in a corresponding shift in the tuning of head direction (HD) cells<sup><xref ref-type="bibr" rid="R9">9</xref></sup>, place cells<sup><xref ref-type="bibr" rid="R10">10</xref>,<xref ref-type="bibr" rid="R11">11</xref></sup>, and grid cells<sup><xref ref-type="bibr" rid="R3">3</xref></sup>. Furthermore, placing an animal in the dark can lead to tuning instability<sup><xref ref-type="bibr" rid="R12">12</xref>,<xref ref-type="bibr" rid="R13">13</xref></sup>. Thus, visual landmarks can anchor the internal representation of space<sup><xref ref-type="bibr" rid="R14">14</xref></sup>. However, other than helping orientate the tuning of these cells, much remains to be known about how visual objects modulate the firing properties of neurons within the spatial navigation system.</p><p id="P5">A detailed understanding of how the brain encodes visual objects has been gleaned from decades of work, primarily in humans and non-human primates<sup><xref ref-type="bibr" rid="R15">15</xref></sup>. Briefly, spanning from primary visual cortex (V1) to inferotemporal cortex, there are a series of brain areas arranged in a hierarchical manner, referred to as the ‘ventral visual stream’, tasked with encoding increasingly high-level representations of visual objects<sup><xref ref-type="bibr" rid="R15">15</xref>,<xref ref-type="bibr" rid="R16">16</xref></sup>. Along these lines, studies in rodents have found that lateral visual cortical areas appear to exhibit certain ventral-steam-like properties<sup><xref ref-type="bibr" rid="R17">17</xref>–<xref ref-type="bibr" rid="R20">20</xref></sup> and that rodents are able to adapt their behaviors in response to visual objects<sup><xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R21">21</xref>,<xref ref-type="bibr" rid="R22">22</xref></sup>. However, a clear understanding of how and where visual objects are encoded in the rodent brain is largely missing.</p><p id="P6">To address these issues, we directly examined where representations of visual objects can be detected in the mouse brain and how visual objects impact processing within the spatial navigation system. First, we performed a brain-wide screen, using functional ultrasound (fUS) imaging<sup><xref ref-type="bibr" rid="R23">23</xref>–<xref ref-type="bibr" rid="R25">25</xref></sup>, to look for areas activated more strongly by visual objects than by scrambled versions of the same images. While we did not find a preference for visual objects in the mouse’s ventral visual cortical areas, we found an enrichment of visual object preferring areas within the spatial navigation system, which we then validated with single-cell electrophysiology. Next, focusing on one of the top hits of our fUS screen, the postsubiculum (PoSub), the cortical hub of the HD system, we examined the firing properties of neurons in freely moving mice and found that a preference for visual objects was present in both HD cells and fast spiking (FS) interneurons. Finally, we found that visual objects act to dynamically refine population coding of HD in PoSub, increasing the firing rate of HD cells whose preferred firing directions correspond to a visual landmark, while decreasing the firing rate of HD cells coding other directions.</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>A brain-wide screen for areas that preferentially respond to visual objects</title><p id="P7">We set out to screen for brain areas in the mouse that were preferentially modulated by visual objects. Functional brain imaging experiments in humans, comparing activity driven by the presentation of visual objects versus scrambled versions of the same images, were important in revealing the importance of ventral stream cortical areas in encoding visual objects<sup><xref ref-type="bibr" rid="R26">26</xref>,<xref ref-type="bibr" rid="R27">27</xref></sup>. Inspired by those experiments, here we performed a brain-wide screen in mice using fUS imaging<sup><xref ref-type="bibr" rid="R23">23</xref>–<xref ref-type="bibr" rid="R25">25</xref></sup> to search for areas that responded preferentially to visual objects. fUS is a neuroimaging method that monitors changes in blood volume, a proxy for neuronal activity<sup><xref ref-type="bibr" rid="R28">28</xref></sup>, and enables brain-wide volumetric recordings with a spatial resolution of ∼250 μm and a temporal resolution of 2 Hz<sup><xref ref-type="bibr" rid="R29">29</xref></sup>.</p><p id="P8">Mice were implanted with a COMBO cranial window<sup><xref ref-type="bibr" rid="R30">30</xref></sup> and head-fixed under the ultrasound probe (<xref ref-type="fig" rid="F1">Fig. 1a</xref>). Visual stimuli were presented while fUS signals were measured simultaneously from almost the whole mouse brain (excluding olfactory bulbs, cerebellum and parts of the hindbrain). Images were presented 60° in height, centered along the horizontal meridian and 20° above the vertical meridian (<xref ref-type="fig" rid="F1">Fig. 1a</xref>), corresponding to the ‘focea’, a part of the visual field thought to be particularly behaviorally relevant for mice<sup><xref ref-type="bibr" rid="R31">31</xref></sup> and that has been hypothesized to be important for processing visual landmarks<sup><xref ref-type="bibr" rid="R32">32</xref></sup>. Images of 48 different objects were shown, covering a range of object categories (<xref ref-type="fig" rid="F5">Extended Data Fig. 1</xref>). Each image contained a single object centered on a grey background and was shown for 0.5s. For image scrambling, we used a texture scrambling method<sup><xref ref-type="bibr" rid="R33">33</xref></sup> (which we refer to as Scrambled<sub>T</sub>) that has previously been used in studies of the primate ventral visual stream<sup><xref ref-type="bibr" rid="R34">34</xref>,<xref ref-type="bibr" rid="R35">35</xref></sup>. The 48 images were split into 4 blocks of 12 images, and 4 blocks of corresponding scrambled images (<xref ref-type="fig" rid="F1">Fig. 1b</xref>, <xref ref-type="fig" rid="F5">Extended Data Fig. 1</xref>).</p><p id="P9">First, we examined the visual response, in anesthetized mice, to all 8 blocks (i.e. combining object and scrambled stimuli). We found visual stimulus evoked activity in voxels throughout the mouse visual system, including visual cortex, superior colliculus, and lateral geniculate nucleus of the thalamus (<xref ref-type="fig" rid="F1">Fig. 1c</xref>). After anatomically segmenting the brain into ∼100 areas, we found visual activity in 27 brain areas (<xref ref-type="fig" rid="F5">Extended Data Fig. 1</xref>). Next, we computed a visual object sensitivity index (VOSI<sub>T</sub>), to quantify preferences for object vs. texture scrambled images. Positive VOSI<sub>T</sub> values indicate stronger responses for object stimuli and negative VOSI<sub>T</sub> indicates stronger responses for scrambled stimuli. We noted an enrichment in positive VOSI<sub>T</sub> voxels in the hippocampal formation and retrosplenial cortex (RSC; <xref ref-type="fig" rid="F1">Fig. 1d</xref>, <xref ref-type="fig" rid="F5">Extended Data Fig. 1</xref>). VOSI<sub>T</sub> analysis at the scale of brain areas revealed a small number of brain areas with a statistically significant preference for visual objects: the PoSub, the parasubiculum, and the area prostriata (<xref ref-type="fig" rid="F1">Fig. 1d-f</xref>). In contrast, many areas, including V1 did not exhibit a preference for objects or scrambled images (<xref ref-type="fig" rid="F1">Fig. 1d-f</xref>). We found one area, the superior colliculus (SC), responded significantly more strongly to scrambled images (<xref ref-type="fig" rid="F1">Fig. 1d-f</xref>). Importantly, we found similar results when we repeated experiments in awake animals (<xref ref-type="fig" rid="F6">Extended Data Fig. 2</xref>).</p><p id="P10">Next, to examine if there was a systematic bias for visual object sensitivity at the brain network-level, we rank-ordered visually responsive brain areas according to their VOSI<sub>T</sub> values. Then, based on literature review, we categorized brain areas as belonging to either 1) the spatial navigation system, 2) the visual system or 3) other (<xref ref-type="fig" rid="F1">Fig. 1g</xref>, see <xref ref-type="supplementary-material" rid="SD1">Supplementary Table 1</xref> for the list of brain areas and their assigned systems). Within the top ten areas with highest VOSI<sub>T</sub> values, eight are closely associated with the spatial navigation system (<xref ref-type="fig" rid="F1">Fig. 1g</xref>). Consistent with this, the average VOSI<sub>T</sub> value of all spatial navigation related areas was significantly larger than zero (<xref ref-type="fig" rid="F1">Fig. 1h</xref>). We found a similar result—that a preference for visual objects over scrambled images was restricted to spatial navigation areas—when we clustered brain areas, or visually responsive pixels, based solely on their object and scrambled response kernels (<xref ref-type="fig" rid="F7">Extended Data Fig. 3</xref>).</p><p id="P11">Finally, as some studies indicate that, similar to primates, visual object processing in mouse cortex is biased towards ventral visual cortical areas<sup><xref ref-type="bibr" rid="R17">17</xref>,<xref ref-type="bibr" rid="R18">18</xref></sup>, we subdivided the ‘visual system’ group into ventral and dorsal visual cortical areas. However, we did not detect a difference in activation by object versus scrambled images for either ventral or dorsal visual cortical areas (<xref ref-type="fig" rid="F8">Extended Data Fig. 4</xref>). Taken together, these results reveal a preference for visual objects within the mouse’s spatial navigation system, not its visual system.</p></sec><sec id="S4"><title>A preference for visual objects in postsubicular neurons</title><p id="P12">To ensure that visual object preferences identified with fUS were driven by neuronal firing<sup><xref ref-type="bibr" rid="R28">28</xref></sup>, we performed acute electrophysiological recordings from several brain areas in head-fixed anesthetized mice (<xref ref-type="fig" rid="F2">Fig. 2a</xref>, <xref ref-type="fig" rid="F9">Extended Data Fig. 5</xref>). We recorded from regions of the spatial navigation system: PoSub (which had a positive VOSI<sub>T</sub> in fUS), as well as dorsal and ventral retrosplenial cortex (RSCd and RSCv). We chose RSC because from our fUS experiments RSCd, but not RSCv, trended towards preferring object images, and RSCd was among the most object preferring regions outside the subiculum. Additionally, previous work suggests RSC is a spatial navigation area involved with processing visual landmarks<sup><xref ref-type="bibr" rid="R36">36</xref>,<xref ref-type="bibr" rid="R37">37</xref></sup>. As control regions, we recorded from two visual areas: V1 (which did not exhibit a preference in fUS experiments), and SC (which had a texture preference in fUS experiments). We presented animals with the same object images as in fUS experiments, but now included a second form of scrambling. The texture scrambling method used above corrupts object identity but can also introduce local contrast changes into background areas that were previously solid gray. To control for this, taking advantage of the larger number of trials possible with electrophysiology, we added a second scrambling method, diffeomorphic transformation<sup><xref ref-type="bibr" rid="R38">38</xref></sup>, which maintains overall object shape while modifying local pixel-level spatial organization (Scrambled<sub>D</sub>; <xref ref-type="fig" rid="F2">Fig. 2b</xref>; <xref ref-type="fig" rid="F10">Extended Data Fig. 6</xref>).</p><p id="P13">First, we compared fUS recordings to single unit spike recordings, focusing on the difference between responses to objects and texture scrambled images (<xref ref-type="fig" rid="F2">Fig. 2c-e</xref>). For single cell responses, we tested for a preference for visual objects by subtracting the response to Scrambled<sub>T</sub> images from the response to Objects (Object-Scrambled<sub>T</sub>). We found that neurons in PoSub and RSCd exhibited a preference for visual objects. Neurons in SC, V1 and RSCv cortex did not exhibit a preference for visual objects, though in accordance with the fUS results there was a negative trend in the SC Object-Scrambled<sub>T</sub> response (<xref ref-type="fig" rid="F2">Fig. 2d</xref>). For all five regions, spiking recapitulated fUS preferences for object versus texture scrambled images, with a strong correlation between the object preferences calculated with both recording modalities (<xref ref-type="fig" rid="F2">Fig. 2e</xref>, R = 0.97). Next, we examined responses to diffeomorphic transformations (<xref ref-type="fig" rid="F2">Fig. 2f</xref>). From this point forward, we considered brain areas to prefer visual objects if they showed a preference for Objects over both Scrambled<sub>T</sub> and Scrambled<sub>D</sub>. This was the case for PoSub and RSCd (<xref ref-type="fig" rid="F2">Fig. 2f-h</xref>). In contrast, we found that V1 and RSCv cortex did not show a preference for objects (<xref ref-type="fig" rid="F2">Fig. 2g,h</xref>). Lastly, SC neurons did not show a preference for diffeomorphic scrambles, suggesting that the preference for texture scrambled images in fUS recordings is more related to the texture scrambling method itself than to the disruption of object identity (<xref ref-type="fig" rid="F2">Fig. 2g,h</xref>).</p><p id="P14">Next, we wondered if PoSub and RSCd exhibited other visual feature preferences that differentiated them from non-visual object preferring areas. We measured additional visual properties (receptive field size, spatial and temporal frequency tuning) of neurons from the five brain areas outlined above. We found that neurons in PoSub and RSCd exhibited larger receptive fields and lower temporal frequency preferences compared to SC and V1, but had similar tuning properties to non-object preferring neurons in RSCv (<xref ref-type="fig" rid="F2">Fig 2i,j</xref>; <xref ref-type="fig" rid="F11">Extended Data Fig. 7</xref>).</p></sec><sec id="S5"><title>In PoSub, head-direction and fast spiking cells exhibit a preference for visual objects</title><p id="P15">How do visual objects modulate the processing of spatial information? As PoSub was one of the most visual object preferring regions in our screen, and as it is known to possess HD cells<sup><xref ref-type="bibr" rid="R2">2</xref>,<xref ref-type="bibr" rid="R39">39</xref></sup> and be modulated by visual landmarks<sup><xref ref-type="bibr" rid="R14">14</xref></sup>, we focused further on this region. To obtain freely-moving recordings and head-fixed visual stimulation recordings from the same neurons, we chronically implanted a silicon probe in PoSub (<xref ref-type="fig" rid="F12">Extended Data Fig. 8</xref>). We then introduced mice to an open field arena, with one visual landmark on the wall, for freely moving recording sessions (<xref ref-type="fig" rid="F3">Fig. 3a</xref>). Immediately following open field recordings, animals were kept awake, head-restrained, and shown the same set of visual stimuli as outlined above for anesthetized animals (<xref ref-type="fig" rid="F3">Fig. 3a</xref>).</p><p id="P16">First, we examined the nature of light-evoked activity in PoSub. We found that visual stimulation increased the firing rate of some PoSub neurons, whereas it decreased the firing rate of other neurons, a phenomenon we did not observe in anesthetized animals (<xref ref-type="fig" rid="F3">Fig. 3b,c</xref>; also, some temporal properties of the light response also changed between awake and anesthetized conditions; <xref ref-type="fig" rid="F13">Extended data Fig. 9</xref>). If we split PoSub neurons into positively and negatively visually modulated cells, we found that a preference for visual objects was specific to positively visually modulated cells (<xref ref-type="fig" rid="F3">Fig. 3d,e</xref>).</p><p id="P17">Next, using the spike waveform, baseline firing rate, and head direction information encoded by each cell, we classified PoSub neurons into three cell types: HD cells, fast-spiking (FS) inhibitory cells and slow non-HD cells (<xref ref-type="fig" rid="F3">Fig. 3f</xref>), as done previously<sup><xref ref-type="bibr" rid="R39">39</xref></sup>. We found that positively and negatively modulated cells were present in each group, however the majority of negatively modulated cells were HD cells (<xref ref-type="fig" rid="F3">Fig. 3g</xref>). Furthermore, focusing solely on positively modulated cells, we found that a preference for visual objects (i.e. positive Object-Scrambled<sub>T</sub> and positive Object-Scrambled<sub>D</sub> responses) was present in HD cells and FS cells, but not slow non-HD cells (<xref ref-type="fig" rid="F3">Fig. 3h-j</xref>).</p></sec><sec id="S6"><title>Visual objects dynamically refine population-level encoding of head direction</title><p id="P18">We next focused on PoSub HD cells to understand what was controlling the differential effects of visual stimulation on the firing rate of HD cells. We examined if there was population-level organization of visual responses. For a given animal, from freely moving recordings, we calculated the preferred firing direction of all simultaneously recorded HD cells and positioned each HD cell on a ring, with the angular position of each cell corresponding to its preferred firing direction (<xref ref-type="fig" rid="F4">Fig. 4a</xref>). Next, for the same HD cells, from the head-fixed recordings we calculated each cell’s visual response (positive values indicate that visual stimulation increased the firing rate; negative values indicate that visual stimulation decreased the firing rate; <xref ref-type="fig" rid="F4">Fig. 4a</xref>). We found that HD cells with similar preferred firing directions in the open field exhibited similar visual responses in the head-fixed experiment (<xref ref-type="fig" rid="F4">Fig. 4a</xref>). In other words, HD cells that were positively modulated by visual stimulation had similar preferred firing directions, and HD cells that were negatively modulated by visual stimulation had similar preferred firing directions (<xref ref-type="fig" rid="F4">Fig. 4a</xref>). To quantify this effect, for the ring generated for each mouse we computed the average preferred firing direction of positively visually responding HD cells and aligned this to 0° (see Methods; <xref ref-type="fig" rid="F4">Fig. 4b,c</xref>). We found a correlation between visual response and angular position on the ring following alignment to 0° (<xref ref-type="fig" rid="F4">Fig. 4c, R</xref>=0.294 (circular correlation), and <xref ref-type="fig" rid="F14">Extended Data Fig. 10</xref>, R=0.316 (linear correlation)) that significantly differed from chance (<xref ref-type="fig" rid="F14">Extended Data Fig. 10</xref>). Similarly, we found that HD cells whose preferred firing directions pointed towards 0° (preferred firing direction of 0° ± 45°) exhibited stronger visual responses than the remaining HD cells (preferred firing direction of 180° ± 135°; <xref ref-type="fig" rid="F4">Fig. 4d</xref>). Thus, visual stimuli differentially modulate the firing rate of HD cells in PoSub as a function of preferred firing direction.</p><p id="P19">However, because the HD tuning curves above were calculated in one environment and the head-fixed recordings were performed in a different environment, it was unclear how the above-described visual responses related to the animal’s head-fixed HD. To answer this question, we took advantage of existing knowledge about population responses in the HD system. The HD system exhibits several hallmarks of a continuous ring attractor<sup><xref ref-type="bibr" rid="R40">40</xref></sup>: only HD cells with similar preferred firing directions—those encoding the mouse’s current head direction—can be active at a given time, resulting in a localized ‘bump of activity’ in the ring<sup><xref ref-type="bibr" rid="R5">5</xref>,<xref ref-type="bibr" rid="R41">41</xref></sup>. Furthermore, the angular offset between HD cells is maintained across environments<sup><xref ref-type="bibr" rid="R42">42</xref></sup>, making it possible to compare responses of similarly tuned neurons between different conditions. Thus, we tested whether the skewed distribution of positively and negatively visually responsive HD cells around 360° related to the ‘bump of activity’ in the HD ring attractor. Since HD cells continue to fire even when an animal is stationary<sup><xref ref-type="bibr" rid="R2">2</xref>,<xref ref-type="bibr" rid="R43">43</xref></sup>, the ‘bump of activity’ in baseline firing (in the absence of visual stimulation) should indicate which HD cells are encoding the animal’s current (head-fixed) head direction. We calculated the ‘bump of activity’ from baseline firing rates during head-fixation, indicating the current head direction, and found that it closely aligned with the region of the ring that exhibited positive visual responses (<xref ref-type="fig" rid="F4">Fig. 4e</xref>). In other words, HD cells with preferred firing directions that pointed towards the visual stimuli (which were presented directly in front of the animal) were positively modulated by visual stimulation, whereas HD cells coding other directions tended to be unmodulated or negatively modulated by visual stimulation.</p><p id="P20">The visual stimulation experiments described above were performed in head-fixed conditions. Do visual objects similarly modulate HD cell firing in freely moving conditions? To test this, we set the direction of the wall of the open field arena that contained the visual landmark to be 0°. Next, we restricted our analysis to time points when the animal’s HD was within the range of 0° ± 45°, to focus on HDs over which it was possible for the mouse to directly face the visual landmark. Finally, we split the data into time points when the mouse was either directly facing the landmark (On landmark) or facing away from the landmark (Off landmark; <xref ref-type="fig" rid="F4">Fig. 4f</xref>), while matching the distributions of head directions included in the two conditions (<xref ref-type="fig" rid="F14">Extended Data Fig. 10</xref>). We then subtracted the average Off landmark firing rate from the average On landmark firing rate to obtain a visual response value in freely-moving conditions. We found that the visual response of HD cells with preferred firing directions aligned with the direction of the landmark (0° ± 45°) was significantly larger than the visual response of HD cells that coded for directions not aligned with the landmark (180° ± 135°; <xref ref-type="fig" rid="F4">Fig. 4g</xref>). This arose because HD cells with preferred firing directions aligned with the direction of the landmark were positively modulated by the landmark, whereas responses of HD cells that coded for directions not aligned with the landmark were negatively modulated by the landmark (<xref ref-type="fig" rid="F4">Fig. 4h</xref>). Together, these results show that visual landmarks dynamically modulate HD population activity as the animal navigates an environment, boosting the ‘bump of activity’ of the HD ring attractor coding the current HD when the animal is facing the landmark, while simultaneously decreasing activity on the ring away from the ‘bump of activity’.</p><p id="P21">Lastly, we asked whether our finding of a differential effect of visual input on HD cell firing as a function of preferred firing direction could arise from an untuned input (i.e. the same visual input to all PoSub cells, regardless of preferred firing direction). Indeed, it seems implausible that the visual input reaching PoSub would already be tuned by HD. We reasoned that a supralinear stabilized network<sup><xref ref-type="bibr" rid="R44">44</xref></sup> (SSN) might recapitulate our observation seeing as 1) the SSN has previously accounted for tuning-specific experimental results when model units were organized in a ring architecture<sup><xref ref-type="bibr" rid="R44">44</xref>,<xref ref-type="bibr" rid="R45">45</xref></sup> and 2) the SSN activity properties differ depending on the strength of the input<sup><xref ref-type="bibr" rid="R44">44</xref></sup>. This is due an architecture that relies on recurrent inhibition to stabilize the effects of strong recurrent excitation and a supralinear activation function, in which gain increases as stronger inputs are received (<xref ref-type="fig" rid="F4">Fig. 4i-k</xref>). Here, the units of the SSN were provided with a spatially localized bump of activity, modeling activity within the HD ring attractor representing the current HD (<xref ref-type="fig" rid="F4">Fig. 4i-k</xref>). Next, we examined the effect of the visual input to PoSub, modeled as an untuned input to the ring SSN network (<xref ref-type="fig" rid="F4">Fig. 4l</xref>). We found that, despite the visual input to all units being the same, it evoked differential effects depending on each model unit’s angular distance from the current HD (<xref ref-type="fig" rid="F4">Fig. 4m</xref>). Notably, the model recapitulated the experimental data, with visual input producing an increase in firing of model units coding for the animal’s current HD (represented by the location of the bump of activity in the ring), and a decrease in firing of model units coding HDs away from the current HD. Intriguingly, this effect arose only when visual stimulation corresponded to a decrease in global input to the SSN. These results were robust to parameter variations (<xref ref-type="fig" rid="F15">Extended Data Fig. 11</xref>).</p></sec></sec><sec id="S7" sec-type="discussion"><title>Discussion</title><p id="P22">Combining brain-wide functional ultrasound imaging and both head-fixed and freely-moving electrophysiological recordings in mice, we made two central findings related to visual objects and the brain’s spatial navigation system. First, at the level of mean activity in brain areas, the spatial navigation system exhibits a preference for visual objects over scrambled versions of the same images. Second, visual objects directly modulate head direction coding in PoSub: a visual landmark boosts the firing rate of HD cells with preferred firing directions pointing toward the visual landmark, whereas it decreases the firing rate of HD cells that encode other HDs.</p><p id="P23">Interestingly, unlike in humans and non-human primates<sup><xref ref-type="bibr" rid="R26">26</xref>,<xref ref-type="bibr" rid="R27">27</xref></sup>, our brain-wide screen did not reveal that ventral visual cortical areas were more strongly activated by objects compared to scrambled images. While it is possible that spatial resolution limitations imposed by volumetric fUS (∼250 µm) could impede our ability to detect visual object preferences in particularly small brain areas, our findings are consistent with single cell calcium imaging experiments from mouse ventral stream areas that, to date, have failed to reveal individual neurons with preferred stimuli that resemble objects<sup><xref ref-type="bibr" rid="R20">20</xref></sup> (though a slight preference for textures may exist in certain rodent ventral areas<sup><xref ref-type="bibr" rid="R19">19</xref>,<xref ref-type="bibr" rid="R46">46</xref></sup>). However, the lack of a clear preference for visual objects in the rodent ventral pathway does not mean that neurons in these areas are not involved in encoding visual objects. Indeed, previous studies in rodents trained on invariant visual object recognition tasks found that the amount of information contained about object identity increased along the ventral stream hierarchy<sup><xref ref-type="bibr" rid="R17">17</xref>,<xref ref-type="bibr" rid="R18">18</xref></sup>. Furthermore, in mouse V1, previous work has shown that while phase scrambling natural images does not affect mean population activity or response variance, it does affects higher-order correlations between neurons<sup><xref ref-type="bibr" rid="R42">42</xref></sup>. Thus, while mouse visual cortex appears capable of encoding information about visual objects and passing this information onto the spatial navigation system, unlike primates there may not be specific visual cortical areas dedicated to encoding specific classes of visual objects. Interestingly, beyond a potential role in visual object processing, it has been hypothesized that the rodent ventral visual stream may be important for visually-guided spatial navigation, as ventral visual cortical areas mostly encode regions of the visual field directly in front of the mouse, which could help them process visual landmarks<sup><xref ref-type="bibr" rid="R32">32</xref></sup>.</p><p id="P24">Our experiments demonstrate that several regions within the spatial navigation system, notably dorsal RSC and PoSub, preferentially respond to visual objects. Both of these areas are known to receive substantial input from the visual system and be involved with processing of landmarks<sup><xref ref-type="bibr" rid="R14">14</xref>,<xref ref-type="bibr" rid="R47">47</xref>,<xref ref-type="bibr" rid="R48">48</xref></sup>. RSC is critical for aligning reference frames, translating egocentric signals—such as viewpoint-specific visual inputs—into allocentric information, including the HD signal<sup><xref ref-type="bibr" rid="R48">48</xref>,<xref ref-type="bibr" rid="R49">49</xref></sup>. The RSC is thought to detect visual landmarks in the environment and integrate them with subcortical HD inputs, which are primarily driven by vestibular and proprioceptive signals, anchoring the HD signal to the external world<sup><xref ref-type="bibr" rid="R37">37</xref>,<xref ref-type="bibr" rid="R50">50</xref>–<xref ref-type="bibr" rid="R54">54</xref></sup>. PoSub is the cortical hub of the HD system<sup><xref ref-type="bibr" rid="R55">55</xref></sup> and updates the HD signal with visual orientation cues<sup><xref ref-type="bibr" rid="R9">9</xref>,<xref ref-type="bibr" rid="R56">56</xref></sup>. It broadcasts the HD signal to downstream navigation structures such as the medial entorhinal cortex and hippocampus<sup><xref ref-type="bibr" rid="R57">57</xref></sup>. While the head-direction signal is ubiquitous in the medial entorhinal cortex<sup><xref ref-type="bibr" rid="R58">58</xref>,<xref ref-type="bibr" rid="R59">59</xref></sup>, hippocampal neurons also show tuning to head-direction in the presence of salient visual cues<sup><xref ref-type="bibr" rid="R60">60</xref></sup>. Furthermore, while our study mainly focused on the PoSub and RSC, we found other areas of the spatial navigation system that were modulated by objects, and it is possible that visual objects impact HD coding in those other areas as well. For example, our fUS screen identified the parasubiculum as preferring visual objects, and this area is known to play a role in conveying a directional signal to spatially tuned cells of the entorhinal cortex<sup><xref ref-type="bibr" rid="R61">61</xref></sup>.</p><p id="P25">As noted above, the preference for visual objects in the spatial navigation system that we describe does not appear to result from highly specific responses to individual objects by individual neurons. Instead, it appears to reflect a general preference for objects over scrambled images, which could arise from a preference for natural image statistics present in objects. This object preference that we describe could be useful for isolating objects from a noisy, visually cluttered environment and facilitate the use of visual objects as spatial landmarks. This preference for visual objects was present in both awake and anesthetized conditions, similar to what has been found for visual feature selectivity in IT cortex of primates<sup><xref ref-type="bibr" rid="R62">62</xref></sup>. Furthermore, seeing as the preference for visual objects was maintained in anesthetized conditions, it indicates that the response preference in spatial navigation areas represents a sensory, not behaviorally, driven phenomenon. Next, an interesting feature of our computational model of the HD system in PoSub is that it suggests that net input to PoSub decreases when the animal faces a visual cue, which in turn leads to increased firing of HD cells coding the current HD, but future work is required to test this model prediction. Lastly, while the anatomical connectivity between primary visual cortex, higher visual areas, and RSC and PoSub is well mapped out<sup><xref ref-type="bibr" rid="R48">48</xref>,<xref ref-type="bibr" rid="R47">47</xref>,<xref ref-type="bibr" rid="R63">63</xref>–<xref ref-type="bibr" rid="R65">65</xref></sup>, the specific pathways and computations that enable visual objects to get encoded and modulate the spatial navigation system remain to be elucidated.</p><p id="P26">Our findings in PoSub show that the firing rate of HD cells coding the current head direction increases when an animal is facing a visual object. At the same time, the firing rate decreases for HD cells that are not coding the current head direction. This indicates that in moments when the visual landmark is in the centre of the mouse’s visual field, the certainty about the current heading direction is increased. However, whether this visual object-mediated increase in gain is specific to the HD system, or also extends to other types of spatially tuned neurons throughout the spatial navigation system remains an open question.</p></sec><sec id="S8" sec-type="methods"><title>Methods</title><sec id="S9"><title>Animals</title><p id="P27">Experiments performed in Germany complied with the institutional guidelines of the Max Planck Society and were approved by the local government (Regierung von Oberbayern). Experiments performed in Canada were done in accordance with the Canadian Council on Animal Care and approved by the Montreal Neurological Institute’s Animal Care Committee. Male and female C57BL/6 mice aged 8-12 weeks were used in all experiments. They were housed in groups under a 12-hour light-dark cycle with unrestricted access to standard diet and water.</p></sec><sec id="S10"><title>Cranial Window Surgery</title><p id="P28">Surgeries were performed as described previously<sup><xref ref-type="bibr" rid="R30">30</xref></sup>. Briefly, mice were anesthetized with a subcutaneous injection of fentanyl (0.05 mg/kg), midazolam (5 mg/kg), and medetomidine (0.5 mg/kg) (FMM cocktail). They were secured using a bite bar and kept at 37°C with a temperature controller (Supertech). To prevent their eyes from drying, a hydration gel (Bayer, Bepanthen) was applied. A large cranial window was created in the skull using a dental drill, extending from bregma +2.25 mm AP to bregma -4.00 mm AP, while ensuring the dura remained intact. A pre-prepared COMBO window<sup><xref ref-type="bibr" rid="R30">30</xref></sup> was adhered to the exposed bone using cyanoacrylate glue (Pattex) and the contact was reinforced with dental cement (Super-Bond). Anesthesia was reversed after surgery by administering subcutaneous flumazenil (0.5 mg/kg) and atipamezole (2.5 mg/kg). Postoperative analgesia was managed with subcutaneous injections of buprenorphine (0.1 mg/kg). After a 7-day recovery period, the FMM cocktail was administered again, and a head plate was attached to the COMBO window. Mice were allowed at least 3 days of additional recovery before being gradually acclimated to the experimenter, the behavioral setups, and tasks.</p></sec><sec id="S11"><title>Habituation to Head Fixation</title><p id="P29">For awake fUS recording sessions, all animals underwent gradual habituation to a head-fixed context. For the first 1 to 2 days, they were allowed to explore the behavior rig. Over the next 5+ days, the duration of head fixation increased daily from 10 minutes to 60 minutes.</p></sec><sec id="S12"><title>fUS Acquisition</title><p id="P30">fUS-imaging data was collected using a 32 × 32 channel matrix probe (15 MHz, 1024 total elements, spatial resolution: 220 × 280 × 175 μm<sup>3</sup>, Vermon) connected to a Vantage 256 system (Verasonics, Inc.) and controlled by a custom volumetric fUS acquisition module (AUTC). A 4× multiplexer linked the 1024 channel probe to the 256-channel system, with the beamforming and sequences adjusted accordingly. For fUS recordings under anesthesia, mice were injected with the FMM cocktail and kept at 37° with a temperature controller. Next, the mouse was head-fixed in a holding tube and the matrix probe was positioned to cover the entire cranial window using a three-way translation stage. A compound ultrasound image was created from the summation of plane wave emissions at -4.5, -3, -1.5, 0, 1.5, 3, and 4.5 degrees. A power Doppler image was generated from the incoherent average of 160 compound ultrasound images acquired at 400 Hz. Clutter filtering was performed in real time by decomposing the ultrasound stack using singular value decomposition, removing the first 20% of singular vectors. This resulted in a power Doppler image approximately every 500 ms.</p></sec><sec id="S13"><title>Electrophysiology Recordings under anesthesia</title><p id="P31">Data was recorded at 20 kHz using RHX software from 32-channel silicon probes (Cambridge NeuroTech, type H10b) connected via a SPI interface cable (Intan technologies, C3216) to a USB interface board (Intan technologies, C3100). Mice were anesthetized with the FMM cocktail and head-fixed in a holding tube, aligned to the probe manipulator’s coordinate system. Small perforations (0.5 mm diameter) were made above the region of interest, and the probe was lowered into the brain at 2 μm/s. Recordings began 10 minutes after reaching the target depth. For separate recordings on different days, perforations were sealed with Kwik-Cast silicone sealant. CM-DiI (Thermo Fisher, CellTracker) was applied to the probe before brain insertion to identify recording sites post hoc.</p></sec><sec id="S14"><title>Silicon probe implantation for chronic (awake) electrophysiology</title><p id="P32">Animals were anesthetized with 5% isoflurane and positioned in a stereotaxic apparatus. Anesthesia during the surgery was maintained with 1.5-2% isoflurane. Animals were given a subcutaneous saline injection (∼0.8 ml) and carprofen (20mg/kg), as well as a subcutaneous 1:1 lidocaine-bupivacaine mixture underneath the incision site as soon as they were placed in the stereotaxic apparatus.</p><p id="P33">A thin layer of light-cured adhesive (Kerr OptiBond) was applied to the skull to help implants better adhere to the skull. A head-fixation bar was cemented to the skull using a dental acrylic cement (Unifast Trad). A silver wire was then implanted into the cerebellum to serve as a reference electrode. Two types of silicon probes were utilized for the experiments: a single-shank probe (Cambridge NeuroTech H5) or a single shank probe with a Molex connector (Cambridge NeuroTech H5). Both probes were mounted on a moveable microdrive.</p><p id="P34">A craniotomy was performed above the target area, creating a hole slightly larger than the probe itself, approximately 0.1 mm wider on each side. The silicon probes were implanted to the following coordinates: AP (from Lambda) +0.30, ML -2.35, DV -1.10. The probe was carefully lowered into position and affixed to the skull using dental acrylic cement (Unifast Trad). To minimize electrical interference during recordings, a copper mesh was attached to the skull surrounding the implant using a light-cured flowable composite (Fusion Flo).</p><p id="P35">After surgery, animals were given a second subcutaneous saline injection (∼0.8 ml) and placed on a heated pad until fully recovered. Post-surgery, animals were housed individually and received daily carprofen injections for at least three days. Animals were allowed to recover for 7 days post-surgery before further experimentation.</p></sec><sec id="S15"><title>Electrophysiological recordings in awake animals</title><p id="P36">The probe was lowered slowly into the postsubiculum using the microdrive over the course of 2-3 hours. Brain tissue was allowed to recover for 2 hours before the start of the recording. Neurophysiological signals were recorded continuously at 20 kHz using a 256-channel RHD USB Interface board (Intan Technologies) and captured with Intan RHX software.</p><p id="P37">Behaviour of the mouse during freely-moving open field arena sessions was tracked using reflective markers that were mounted on the copper mesh or pre-amplifier. These were tracked in 3D with infrared cameras and Motive 2.0 motion capture system (Optitrack). Video was captured by an additional camera mounted overhead. Behavioural tracking was acquired at a rate of 120 Hz and synchronized with the electrophysiological signal using voltage pulses registered by the RHD USB Interface Board. The mice were allowed to roam freely in a circular or square environment for 15-20 minutes. Following the freely-moving session, animals were immediately head-fixed and presented with the visual stimulation task.</p></sec><sec id="S16"><title>Visual stimulation</title><p id="P38">Images were presented at a size of 60° with the center of the image at 0° azimuth and +20° elevation on a 61 cm computer monitor (Dell, U2415b) using the PsychoPy toolbox. The monitor was placed 18 cm in front of the mouse. For fUS experiments, we employed a block design consisting of 12 s of gray followed by 12 s of images (12 images, each presented for 0.5s interleaved with 0.5s gray, with images presented in random order across trials). Each recording run consisted of eight repetitions of eight blocks (four object blocks and four texture scrambling blocks) in random order. For object presentation in electrophysiological experiments, images (48 objects, 48 textures, 48 diffeomorphic transformations) were presented in pseudorandom order for ten times for 0.5s and interleaved with at least 0.5s of gray.</p></sec><sec id="S17"><title>Image preparation</title><p id="P39">Original images were resized to 1024x1024 pixels and put on a medium gray background. For each of these images we created a control image with matching statistics using the described texture synthesis algorithm<sup><xref ref-type="bibr" rid="R33">33</xref></sup> with 25 iterations. Subsequently, all images were gamma corrected and the SHINE toolbox<sup><xref ref-type="bibr" rid="R66">66</xref></sup> was used to luminance match all images. Finally, to avoid edge effects a small border (7 pixels around the image) was blurred using a gaussian filter. For stimuli used in electrophysiological experiments we additionally created a second set of control images (diffeomorphic transformations) using a previously published algorithm<sup><xref ref-type="bibr" rid="R38">38</xref></sup>. Additionally, to avoid potential changes in spatial frequency or luminance content introduced by this procedure, for electrophysiology experiments with two scrambling methods, the whole amplitude spectra (specMatch.m) as well as the luminance histogram (histMatch.m) of all images and their respective controls were equated in ten iterations using the SHINE toolbox.</p></sec><sec id="S18"><title>Analysis fUS</title><sec id="S19"><title>fUS Preprocessing</title><p id="P40">The fUS time series was first registered to the Allen Brain Institute’s mouse brain atlas. For this, first all sessions of a mouse were aligned using an automated approach based on the imregtform.m function. Then, a high-resolution power Doppler image was created by averaging 100 fUS frames from a single session, which was then manually registered to the atlas using anatomical landmarks to create a transformation matrix using a previously published toolbox<sup><xref ref-type="bibr" rid="R25">25</xref></sup>. This matrix was applied to data from other sessions of the same mouse. The fUS time series was preprocessed using custom MATLAB scripts on a voxel-by-voxel basis. Temporal interpolation was performed to achieve a constant frame rate of 2 Hz. The relative change in power Doppler signal was calculated by subtracting the baseline signal from each time point and dividing by the baseline signal. Slow drifts were removed using a fifth-order high-pass Butterworth filter with a cutoff frequency of 0.056 Hz. To eliminate movement artifacts during awake experiments, the top 10% of temporal principal components extracted from non-brain voxels was regressed out from all voxels.</p></sec></sec><sec id="S20"><title>fUS activation</title><p id="P41">To assess brain activation at the voxel-wise level, the preprocessed fUS data was temporally smoothed (four frames) and fitted using a General Linear Model (GLM) using the MATLAB function glmfit.m. Model regressors included the visual stimulation block stimuli, convolved with a single-gamma hemodynamic response function. The resulting T-scores where averaged across animals/sessions and plotted on top of the Allen Brain Atlas (thresholded by p-values, see below).</p><p id="P42">To assess activation at the brain region level, first the preprocessed and trial-averaged (only trials of the same type of stimulus; objects or texture) whole-brain data was segmented into individual brain regions. For this, anatomical regions from the Allen reference brain atlas were consolidated into 100 brain regions and the data from all voxels within each region was averaged. Next, the correlation coefficient between the stimulus timing and the fUS signal of each brain region was calculated. To not remove regions that only respond to object or texture blocks respectively, the correlation was performed three times (objects only, scrambled only and both combined). Regions with significant p-values (see below) for one or more of the three correlations were defined as active.</p><p id="P43">Similarly, to find brain regions with differences between object and scrambled stimuli, a preference index for each region (or voxel) was calculated on the preprocessed and trial-averaged fUS data. For this, the area under the curve during the response window for texture stimuli was subtracted from the response to object stimuli and normalized by the sum of the two responses (VOSI<sub>T</sub>). To unbiasedly define a response window for each region/voxel, we averaged all data of a given experiment (sessions, animals, stimuli, all trials) to end up with a single response kernel. The start of the response window was defined as the point with maximum rate of change. The end of the response was defined as the point where the response dropped below 70% of the peak response (30% for awake fUS data due to different temporal dynamics). In case this method did not result in a meaningful response window (window very short or start/stop point before/after stimulus onset/offset), a generic stimulus window (from stimulus onset to stimulus offset) was used.</p><p id="P44">To determine significance, a mixed effects model was fitted on T-scores, correlation scores or preference indices for each voxel/region using the fitlme.m function. Resulting p values were corrected for multiple comparisons using False Discovery Rate (FDR)-correction.</p></sec><sec id="S21"><title>Region and voxel clustering</title><p id="P45">Preprocessed, segmented and trial-averaged (trials of the same stimulus type) fUS data of visually active regions were clustered using a MATLAB UMAP<sup><xref ref-type="bibr" rid="R67">67</xref></sup> implementation (Meehan, Version 4.4; with correlation as distance metric and the following parameters: n_neighbours=5, cluster_detail=’very low’). Similarly, for hierarchical clustering, the segmented and trial-averaged fUS data of visually active regions was clustered using the MATLAB function dendrogram.m (distance metric = correlation, linkage method = weighted). For clustering at the level of voxels, preprocessed and trial-averaged fUS data of visually active voxels (defined by the above described GLM approach) were clustered using k-means with correlation as distance metric and number of clusters defined by the silhouette method.</p></sec><sec id="S22"><title>Analysis electrophysiology</title><sec id="S23"><title>Preprocessing</title><p id="P46">Electrophysiological recordings were processed with Kilosort3, and the spike sorting results were manually refined using the Phy software (<ext-link ext-link-type="uri" xlink:href="https://github.com/cortex-lab/phy">https://github.com/cortex-lab/phy</ext-link>). Only units that exhibited stable responses throughout the session and achieved high Kilosort quality scores were included in the analysis. To estimate continuous firing rates, single-unit responses were binned with a 10 ms bin size. Except for raster plots, the ten trials of the same image were averaged. Raster plots were plotted using the Gramm toolbox<sup><xref ref-type="bibr" rid="R68">68</xref></sup>.</p></sec><sec id="S24"><title>Visual object and scrambled stimuli</title><p id="P47">To select only visually responsive units, we averaged all trials and stimuli of a cell, temporally smoothed the response (10ms window) and determined the response amplitude as the difference of the peak of this overall averaged response minus the baseline firing rate. Cells with an amplitude below 1Hz were excluded from the analysis (in awake electrophysiology all cells were included). To determine the difference between object and control stimulus presentation, the average firing rate during the response window (stimulus onset to stimulus offset) was calculated and the median response to control stimuli (Scrambled<sub>T</sub> or Scrambled<sub>D</sub>) was subtracted from the median response to object.</p></sec><sec id="S25"><title>Receptive field mapping</title><p id="P48">White squares (30° x 30° visual angle) were flashed at 15 different positions (3 elevation positions x 5 azimuth positions) of the screen at random order. Each position was repeated 20 times. The stimulus was presented for 0.5s followed by at least 0.5s of gray. Only units that had a response amplitude larger than 1 Hz for at least one of the screen positions were included in the analysis. To determine how many positions of the screen resulted in a significant activation, we tested at each position whether the firing rates during the response (40ms around peak response) was significantly different from the firing rates before and after stimulus presentation across the 20 repetitions of the stimulus (Wilcoxon signed rank test, B-H corrected).</p></sec><sec id="S26"><title>Spatial frequency tuning</title><p id="P49">Black and white striped stimuli of different spatial frequencies and two orientations (horizontal and vertical) were presented at random order. Like object stimuli, the area of these stimuli spanned 60° visual angle and were centered at 0° azimuth and +20° elevation, and were displayed for 0.5s, interleaved with 0.5 s full-field gray. Each stimulus was shown 20 times. Only units with a significant response to at least one stimulus (Wilcoxon signed rank test, FDR corrected) were included in the analysis. For each cell and spatial frequency, the maximum firing rate during the response window in response to the more preferred orientation was calculated. This resulted in a per cell spatial frequency tuning curve (the maximum represents the preferred spatial frequency) that, for visualization of units with different firing rates, was rescaled between zero and one.</p></sec><sec id="S27"><title>Chirp stimulus</title><p id="P50">A full-field temporal chirp stimulus was presented 20 times, interleaved with ten seconds of full-field gray. The stimulus intensity smoothly transited from black to white with increasing frequency from around 0.3Hz up to 4Hz. Only units with a response amplitude larger than 3 Hz were included for analysis. To determine when a cell stopped responding to the chirp stimulus, first, the normalized firing rates of each cell were decomposed in time-frequency using 1-D wavelet transformation (cwt.m function). Then, the power across all frequencies was summed and the point in time during the stimulus where the power dropped below 1.5 times of the pre-stimulus power was calculated. The last chirp frequency presented before this time point represents the maximal frequency the cell responded to.</p></sec><sec id="S28"><title>HD tuning metrics</title><p id="P51">HD tuning curves were calculated as previously described<sup><xref ref-type="bibr" rid="R39">39</xref></sup>. Briefly, markers on the animal’s headcap were tracked and connected to a three-dimensional polygon allowing calculation of head direction as the horizontal orientation (yaw) of the polygon. The yaw was measured in global coordinates (i.e., aligned with the environment’s axes, which remained consistent throughout the study). HD tuning curves were generated by taking the ratio of histograms of spike counts to the total time spent in each direction, using 1° bins and smoothing with a Gaussian kernel (3° standard deviation). The preferred firing direction of a cell is defined as the mean direction of the tuning curve.</p><p id="P52">To determine how strongly a cell is tuned to head direction, for each cell we determined the head direction information as previously described<sup><xref ref-type="bibr" rid="R39">39</xref></sup> with this formula: <disp-formula id="FD1"><mml:math id="M1"><mml:mrow><mml:mtext>I</mml:mtext><mml:mo>=</mml:mo><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mi>λ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mo>Θ</mml:mo><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mi>log</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>λ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mo>Θ</mml:mo><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>λ</mml:mi></mml:mfrac></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mtext>p</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mo>Θ</mml:mo><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula> Here, n represents the total number of angular bins and λ(Θ<sub>i</sub>) represents the firing rate of the cell in the ith angular bin, λ denotes the neuron’s overall average firing rate during exploration, and p(Θ<sub>i</sub>) indicates the occupancy (normalized time spent) in direction Θ<sub>i</sub>. The information rate (I, expressed in bits per second) was then adjusted by the neuron’s average firing rate to yield an information content metric (head direction information, measured in bits per spike).</p></sec><sec id="S29"><title>Cell type classification</title><p id="P53">PoSub cells were classified as previously described<sup><xref ref-type="bibr" rid="R39">39</xref></sup>. Briefly, cells with a baseline firing rate larger than 10 Hz (during freely moving condition) and a trough to peak duration shorter than 0.4 ms were classified as fast-spiking neurons. In contrast, cells with baseline firing rates below 10 Hz and trough to peak duration above 0.4 Hz were considered slow-spiking neurons. This group was further subdivided into HD cells (head direction information above 0.1) or slow non-HD cells (head direction information below 0.1). All other cells were not classified.</p></sec><sec id="S30"><title>Visual responses in head-fixed conditions, and realigning HD based on visual responses</title><p id="P54">To determine the overall visual response of a cell, first all trials of the same images were averaged and then the median firing rate during the baseline period was subtracted from the median response to all images during the stimulus period. Cells with a visual response value above 0.2 Hz were assigned to the positively modulated group. Cells with a visual response smaller than -0.2 Hz were assigned to the negatively modulated group. To visualize the visual responses with respect to the cell’s HD direction tuning (activity bump, center ring in <xref ref-type="fig" rid="F4">Fig. 4b</xref>), the realigned cells of all mice were binned into 20 angular bins based on their preferred head direction. Then the smoothed and binned visual responses were plotted as scaled deviation (0.1 scaling factor) from the average response (dashed line circle).</p><p id="P55">In each mouse we observed an accumulation of cells with similar head direction tuning and increased visual activity (and baseline activity), however the center of this activity was differently oriented for each mouse likely because there was no systematic remapping when moving the mice from the freely moving condition to the head-fixation setup. As such, we realigned the HD cells’ preferred directions such that the cluster of positively visually modulated HD cells was centered around 0°. For this, in each mouse the circular mean of the preferred directions of the cells that positively responded to the visual stimulus was determined, and this mean direction was subtracted from the preferred directions of all cells, concurrently shifting the whole ring. For shuffled distributions, the realignment process per mouse was repeated 100,000 times, with the visual responses of all cells of a mouse randomly shuffled and the preferred directions kept constant. To calculate the center of baseline firing in head-fixed condition, the circular mean of the cells with high baseline firing rate (&gt; median of all cells of a mouse) was determined.</p></sec><sec id="S31"><title>Visual responses in freely moving mice</title><p id="P56">For this analysis, only recording sessions performed in a rectangular arena were considered (4 out 5 mice), so that the geometry was the same for all analyses (one session in a circular arena was excluded here). First, positional and head direction data was resampled to match 25 ms binning of the spiking data (resample.m). Then, to identify time points when the mouse was looking directly at the visual landmark (On landmark) or looking away from the landmark (Off landmark), at each spatial location we checked if and where the extension of the head direction immediately in front of the animal intersected with the north wall positions where the landmark was placed. Spatial locations and head directions with intersections on the landmark (but not on the edges: 1 cm buffer zone) were considered “On landmark”. Similarly, spatial locations and head directions with intersections beside the landmark were considered “Off landmark”. To ensure the intersections were “Off landmark”, intersections closer than 10cm from a landmark edge were excluded. To exclude positions and head directions with extreme cue viewing angles, which would result in a strong distortion of visual cue, in both cases, only time points, where the absolute head direction was 0° ± 45° were included. Moreover, to ensure that the selection of the time points did not result in a bias in selection of head directions (which could result in differences between “Off landmark” and “On landmark” due to HD tuning of the cells), we subsampled the larger of the two indices and matched the histogram of head directions for each session. Finally, we averaged the firing rates during “On landmark” and “Off landmark” time points and subtracted the two to get the visual response per cell.</p></sec><sec id="S32"><title>Response delay</title><p id="P57">For each HD cell, all trials and stimuli presentations were averaged to get an overall response of the cell. The response delay was defined as the point when the firing rate of the cell rose above two times the standard deviation of the pre-stimulus period.</p></sec><sec id="S33"><title>Perfusion and Histology</title><p id="P58">Following the termination of the electrophysiological experiments, animals were deeply anesthetized using sodium pentobarbital or ketamine (120 mg/kg) and xylazine (16 mg/kg) cocktail and perfused transcardially first with 0.9% phosphate-buffered saline solution followed by 4% paraformaldehyde solution.</p><p id="P59">In case of chronic recordings, the microdrive was retracted to remove the probe from the brain. Then brains were isolated and kept in a 4% paraformaldehyde solution for 24 hours, after which the solution was changed to 30% sucrose in phosphate-buffered saline until sinking. After freezing in a -80° freezer, brains were sectioned with a freezing microtome coronally in 40 μm slices. Sections were washed, counterstained with DAPI and mounted on glass slides with ProlongGold fluorescence antifade medium. Sections containing probe tracts were additionally incubated with a Cy3 anti-mouse secondary antibody (1:200 dilution; Cedarlane, 715-165-150) to help visualize the electrode tract. Widefield fluorescence microscope (Leica) was used to obtain images of sections and verify the tracks of silicon probe shanks.</p><p id="P60">In case of acute recordings, after perfusion, brains were collected and incubated in 4% PFA at 4°C overnight. Then brains were washed and embedded in 4% agarose. Coronal slices were cut using a vibratome (Leica microsystems, VT1000S) with a thickness of 100 μm and stained with DAPI (Thermo Fisher Scientific, D1306 or R37606). Brain slices were washed and mounted on microscope slides in Vectashield (Vector Laboratories, H-1200). Images were captured using a Leica TCS SP8 laser scanning confocal microscope with a 20x air objectives (Leica Microsystems). Images were processed using LAS X (Leica Microsystems) and Adobe Photoshop software.</p></sec><sec id="S34"><title>SSN model</title><p id="P61">The supralinear stabilized ring network model followed Hennequin et al.<sup><xref ref-type="bibr" rid="R45">45</xref></sup> Specifically, the network contained <italic>N<sub>E</sub></italic> = 50 excitatory and <italic>N<sub>I</sub></italic> = 50 inhibitory neurons evenly spaced around a ring with angle <italic>θ</italic> ∈ [−π, π]. The circuit dynamics followed <disp-formula id="FD2"><mml:math id="M2"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:mi>E</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>θ</mml:mi><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:mi>I</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>θ</mml:mi><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>V<sub>θ</sub></italic> and τ<sub><italic>θ</italic></sub> correspond to the membrane potential and time constant of a model unit at angle <italic>θ</italic>, <italic>V<sub>rest</sub></italic>, is the neuronal resting potential, ℎ<sub><italic>θ</italic></sub> is the external input, and <inline-formula><mml:math id="M3"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>θ</mml:mi><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mo>∝</mml:mo><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:mi>cos</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msubsup><mml:mi>ℓ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> the strength of synaptic connection from neuron <italic>θ</italic>′ to neuron <italic>θ</italic>, which is further scaled such that the sum of incoming weights corresponds to <italic>W</italic><sub>EE</sub>, <italic>W</italic><sub>IE</sub>, −<italic>W</italic><sub>EI</sub> and −<italic>W</italic><sub>II</sub>. The input was taken to be <disp-formula id="FD3"><mml:math id="M4"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>I</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mtext mathvariant="italic">max</mml:mtext></mml:mrow></mml:msub><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:mi>cos</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msubsup><mml:mi>ℓ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula> which consists of a tuned component at <italic>θ<sub>stim</sub></italic> = 0 with half width <italic>ℓ<sub>stim</sub></italic> and magnitude <italic>A<sub>max</sub></italic>, and an untuned component of magnitude <italic>I</italic>. We assumed that E and I model units at a given HD are driven equally strongly by the stimulus.</p><p id="P62">Finally, the firing rate of each cell is a threshold-power law function of its membrane potential: <disp-formula id="FD4"><mml:math id="M5"><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:msubsup><mml:mrow><mml:mo>⌊</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>⌋</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></disp-formula> Parameters were taken as the default in Hennequin et al.<sup><xref ref-type="bibr" rid="R45">45</xref></sup> (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Table 5</xref>). To test the parameter robustness of our results, a population of 1250 networks with different weight hyperparameters was sampled from a uniform distribution (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Table 5</xref> for bounds). For each of these networks, the change in rate of neurons aligned with the input HD (0°, corresponding to the current HD; ‘in-field’) and opposite to that HD (180°; ‘out-field’) was calculated for a small change in input (<italic>∂I</italic> = 0.1).</p></sec></sec></sec><sec id="S35" sec-type="extended-data"><title>Extended Data</title><fig id="F5" position="anchor"><label>Extended Data Fig. 1</label><caption><title>Brain-wide fUS imaging in anesthetized animals using object and control stimuli.</title><p id="P63"><bold>a</bold>, All images used for visual stimulation in fUS experiments. Each block of images contains 12 object images (left side) or the same images scrambled using texture scrambling (right side). Blocks of images as well as images within blocks were presented pseudo-randomly. <bold>b</bold>, Area-segmented data shown as an average across all image blocks and sessions. Significant areas (correlation analysis, see methods) are indicated by area names (Allen Brain Atlas names), p &lt; 0.05, FDR-corrected, mixed effects model (n = 56 sessions from 7 anesthetized animals). <bold>c</bold>, All significantly visually-responsive brain areas (as in b) color coded by the region-average T-score shown on coronal brain slices at indicated positions. <bold>d</bold>, GLM contrast between all Object and all ScrambledT blocks, p&lt;0.01, uncorrected, mixed effects model (n = 56 sessions from 7 anesthetized animals).</p></caption><graphic xlink:href="EMS199633-f005"/></fig><fig id="F6" position="anchor"><label>Extended Data Fig. 2</label><caption><title>fUS imaging in awake animals is consistent with findings in anesthetized animals.</title><p id="P64"><bold>a</bold>, Visually-evoked fUS responses (GLM, all image blocks as regressors) overlaid on coronal brain images at indicated positions. Only T-scores significantly different from zero are shown, p&lt;0.05, FDR-corrected, mixed effects model (n = 83 sessions from 7 awake animals) <bold>b</bold>, All significantly visually-responsive brain areas (correlation analysis, see methods) color coded by the region-average T-score, p&lt;0.05, FDR-corrected, mixed effects model (n = 83 sessions from 7 awake animals). <bold>c</bold>, Areas with a VOSI<sub>T</sub> significantly different from zero are colored by their VOSI<sub>T</sub> values. p&lt;0.05, FDR-corrected, mixed effects model (n = 83 sessions from 7 awake animals) <bold>d</bold>, Session-averaged fUS responses (shaded area represents s.e.m.) to Objects and ScrambledT images from postsubiculum and SC. p(PoSub)=0.002, p(SC)=8.4*10<sup>-4</sup>, Bonferroni-Holm (B-H) corrected, mixed effects model (n = 83 sessions from 7 awake animals) <bold>e</bold>, Rank ordering of visually responsive brain areas according to the VOSI<sub>T</sub> values (<italic>black dots</italic>), color-coded (<italic>squares</italic>) according to whether they belong to Spatial navigation (<italic>green</italic>), Vision (<italic>orange</italic>) or Other (<italic>grey</italic>) brain networks. <bold>f</bold>, Comparison of VOSI<sub>T</sub> values for 8 spatial navigation areas (<italic>green</italic>) and 12 visual areas (<italic>orange</italic>) brain, p=0.003, Mann-Whitney U-test.</p></caption><graphic xlink:href="EMS199633-f006"/></fig><fig id="F7" position="anchor"><label>Extended Data Fig. 3</label><caption><title>Unbiased clustering of brain regions, and voxels, groups spatial navigation areas as visual object preferring.</title><p id="P65"><bold>a</bold>, UMAP-based clustering of the 27 visually active areas results in three clusters. Cluster 1 is dominated by regions of the spatial navigation system, while Clusters 2 and 3 mostly contain regions belonging to the visual system. <bold>b</bold>, Consistent with <xref ref-type="fig" rid="F1">Figure 1</xref>, and the presence of many spatial navigation areas in Cluster 1, only regions of Cluster 1 display significant objects sensitivity, p (from left to right) = 0.014, 1.391, 1.391, B-H corrected, one-sample Wilcoxon signed rank test (n (from left to right) = 11, 6, 10). <bold>c</bold>, Similarly, hierarchical clustering of all visually-responsive areas separates spatial navigation areas from visual areas. <bold>d</bold>, K-means clustering of all visually-responsive voxels into three clusters results in one cluster (Cluster 1) mostly containing voxels belonging to Spatial navigation areas, another cluster (Cluster 2) dominated by voxels of Visual areas and the last cluster exclusively populated by voxels of Other brain areas. Time-resolved fUS traces show the average response for all voxels of indicated clusters separated by image type. <bold>e</bold>, Clusters as in (d) shown in the brain at indicated coronal positions.</p></caption><graphic xlink:href="EMS199633-f007"/></fig><fig id="F8" position="anchor"><label>Extended Data Fig. 4</label><caption><title>No evidence for a preference for object over scrambled images in dorsal or ventral visual cortical areas.</title><p id="P66"><bold>a</bold>, VOSI<sub>T</sub> values as in <xref ref-type="fig" rid="F1">Fig. 1h</xref>, with more refined grouping for “Vision” areas, p (from left to right) = 0.02, 1.688. 1.25, 1.688, 0.5, B-H corrected, one-sample Wilcoxon signed rank test (n (from left to right) = 9, 6, 3, 5, 4). <bold>b</bold>, Session-averaged fUS responses (mean ± s.e.m.) to Objects and ScrambledT averaged across all areas of indicated groups (ventral: VISl, VISli, VISpl; dorsal: VISrl, VISa, VISal, VISam, VISpm), p (form left to right) = 1.212, 1.212, B-H corrected, mixed effects model (n = 56 sessions from 7 anesthetized animals).</p></caption><graphic xlink:href="EMS199633-f008"/></fig><fig id="F9" position="anchor"><label>Extended Data Fig. 5</label><caption><title>Coronal section showing insertion sites for electrophysiological experiments in <xref ref-type="fig" rid="F2">Figure 2</xref>.</title><p id="P67"><bold>a-d</bold>, Representative coronal sections showing the track of the electrophysiological probes dyed using a CM-DiI (red) for the five target regions. For reference, cell nuclei were stained using antibodies against DAPI (cyan). Scale bars represent 1 mm.</p></caption><graphic xlink:href="EMS199633-f009"/></fig><fig id="F10" position="anchor"><label>Extended Data Fig. 6</label><caption><title>All objects and control stimuli used in electrophysiological experiments.</title><p id="P68"><bold>a</bold>, The same 48 object images as in fUS experiments were presented in electrophysiological experiments. For electrophysiological experiments the images were presented individually (not in blocks as for fUS) and additional illuminance and spatial frequency matching was performed across all stimuli (see methods). <bold>b</bold>, The same 48 object images scrambled using the diffeomorphic transformation algorithm<sup><xref ref-type="bibr" rid="R38">38</xref></sup>. <bold>c</bold>, Again the same 48 objects images scrambled using a texture synthetizing algorithm<sup><xref ref-type="bibr" rid="R33">33</xref></sup>.</p></caption><graphic xlink:href="EMS199633-f010"/></fig><fig id="F11" position="anchor"><label>Extended Data Fig. 7</label><caption><title>Neurons in spatial navigation areas have larger receptive fields and respond less well to high temporal frequencies.</title><p id="P69"><bold>a</bold>, A white square was randomly presented at 15 different positions of the screen. <italic>Bottom left</italic>, Trial-averaged responses (mean ± s.e.m.) of a single cell in PoSub that responded to the majority of the 15 stimulus positions (gray boxes represent stimulus presentation). <italic>Right</italic>, Quantification of the number of squares that elicited a significant response for all analyzed cells. Significant differences are indicated by a black horizontal line, p &lt; 0.05 (all p values are in <xref ref-type="supplementary-material" rid="SD1">Supplementary Table 2</xref>), B-H corrected, Mann-U-Whitney test, (n (from left to right) = 195, 165, 135, 210, 128 cells. <bold>b</bold>, Full-field chirp stimulus with increasing frequency was presented to the mouse, lasting 20 s. <italic>Bottom left</italic>, Trial-averaged response (mean ± s.e.m.) of an example cell in PoSub (gray box indicates duration of the chirp). <italic>Right</italic>, Quantification of the maximum frequency for all analyzed cells of indicated regions. Significant differences are indicated by a black line, p &lt; 0.05 (all p values are in <xref ref-type="supplementary-material" rid="SD1">Supplementary Table 2</xref>), B-H corrected, Mann-Whitney U-test, (n (from left to right = 128, 114, 67, 167, 94). <bold>c</bold>, Different spatial frequencies (SFs) were presented at two orthogonal orientations (not shown). <italic>Bottom left</italic>, Trial-averaged responses (mean ± s.e.m.) to five SFs of a representative cell in PoSub indicating the relatively weak tuning and tendency to low SF preference observed in PoSub. <italic>Right</italic>, For each cell the responses were normalized to the maximal response and the mean ± s.e.m. across cells is displayed here (n (from left to right = 34, 81, 21, 89, 88 cells). <bold>d</bold>, Distributions of preferred SF frequency across all regions showing few significant differences indicated by a black line, p &lt; 0.05 (all p values are in <xref ref-type="supplementary-material" rid="SD1">Supplementary Tables 2-4</xref>), B-H corrected, Mann-Whitney U-test (n’s as in c).</p></caption><graphic xlink:href="EMS199633-f011"/></fig><fig id="F12" position="anchor"><label>Extended Data Fig. 8</label><caption><title>Coronal section showing position of chronically implanted silicon probe in PoSub</title><p id="P70"><bold>a-b</bold>, Overview (<bold>a</bold>) and zoom in (<bold>b</bold>) of an example coronal section stained with DAPI. The tract of the silicon probe is visible as a line of high intensity DAPI staining due to tissue scaring. Scale bars represent 1mm.</p></caption><graphic xlink:href="EMS199633-f012"/></fig><fig id="F13" position="anchor"><label>Extended Data Fig. 9</label><caption><title>State-dependent differences of visual response properties in PoSub.</title><p id="P71"><bold>a</bold>, Comparison of light-evoked electrophysiological recordings in PoSub from anesthetized animals (<xref ref-type="fig" rid="F2">Fig. 2</xref>) and awake animals (<xref ref-type="fig" rid="F3">Fig. 3</xref>/<xref ref-type="fig" rid="F4">4</xref>). <bold>b-c</bold>, Shown are the normalized, trial-averaged responses (across all images) of all recorded cells in both conditions. Bottom panels show the average across all cells, showing slower and more sustained light-evoked responses under anesthesia. Under anesthesia, no negatively modulated cells were found and a large fraction of cells showed only responses after stimulus offset. <bold>d-e</bold>, To directly compare response dynamics between conditions, the top 100 responding cells (dashed line in b,c) were selected and the average of these cells is shown. <bold>f</bold>, Cells in PoSub respond significantly slower to visual stimuli under anesthesia than in awake condition, p = 1.5*10<sup>-10</sup>, Mann-Whitney U-test (n = 100/100 cells). <bold>g</bold>, In awake condition almost no OFF responses were observed, while cells had strong OFF responses under anesthesia, p=5.3*10<sup>-25</sup>, Mann-Whitney U-test (n=100/100 cells). Note that under anesthesia, the OFF response often merged with the slow ON response. <bold>h,i</bold>, As we found substantial OFF responses under anesthesia, we wondered whether the object sensitivity in PoSub described in <xref ref-type="fig" rid="F2">Fig. 2</xref> (significant difference of object responses in comparison to both scrambled image types) was restricted to the ON response. We found that in the OFF response, despite a trend towards a preference for Objects over ScrambledT, there was not a significant object preference. p (Object-ScrambledT, ON / OFF) = 9.6*10<sup>-9</sup> / 0.097, p (Object-ScrambledD, ON / OFF) = 7.4*10<sup>-7</sup> / 0.269, B-H corrected, Wilcoxon signed rank test (n = 192 cells).</p></caption><graphic xlink:href="EMS199633-f013"/></fig><fig id="F14" position="anchor"><label>Extended Data Fig. 10</label><caption><title>Analysis of HD population tuning and visual responses.</title><p id="P72"><bold>a</bold>, Results of permutation test to ensure that the ring realignment procedure does not account for the circular correlation coefficients observed in <xref ref-type="fig" rid="F4">Fig. 4</xref>. The histogram displays circular correlation coefficients after 100,000 permutations of visual response shuffling before ring alignment. The dotted red line indicates the observed circular correlation coefficient between visual response and preferred firing direction (p = 0.044). <bold>b</bold>, Pearson correlation between visual response and absolute preferred firing direction, p = 2.9*10<sup>-4</sup> (n = 127 cells). Red dotted line represents a linear fit. <bold>c</bold>, Similar to (a), the distribution of Pearson correlation coefficients after shuffling the visual responses before ring alignments (100,000 permutations, p = 0.015). <bold>d</bold>, Related to <xref ref-type="fig" rid="F4">Fig. 4f,g</xref>. To ensure that the differences we observed between the On and Off landmark activity were not driven by a potential bias in selection of time points (which could result in differences in firing activity due to head direction-dependent firing alone), we matched the head directions included in the analysis for each session, Mann-Whitney U-test (uncorrected), p (from left to right) = 0.988, 0.986, 0.99, 0.996 (n (from left to right) = 213/213, 751/ 751, 983/983, 1195/1195).</p></caption><graphic xlink:href="EMS199633-f014"/></fig><fig id="F15" position="anchor"><label>Extended Data Fig. 11</label><caption><title>SSN model results are robust to hyperparameter variations.</title><p id="P73"><bold>a,</bold> Equilibrium firing rate of in-field (model HD cells with a preferred firing direction of 0°, corresponding to the current HD) and out-field model units (corresponding to HD cells with a preferred firing direction of 180°) as a function of the level of untuned visual input. Increasing visual input always decreased the firing rate of in-field model units and increased the firing rate of out-field model HD neurons. Visual input only recapitulated our experimental finding if it resulted in a decrease in network input. <bold>b</bold>, The effect of varying the strength of untuned visual input on the firing rates of in-field (∂<sub>rin-field</sub>/∂Input) and out-field (∂<sub>rout-field</sub>/∂Input) model HD units over a population of networks with different randomly selected recurrent strength parameters. While the firing rate of out-field model HD units always increased as the visual input increased, the firing rate of in-field neurons decreased in most network instantiations as the visual input strength increased. <bold>c</bold>, ∂<sub>rin-field/∂</sub> Input for all network instantiations in the random parameter population, as a function of the E-&gt;E, E-&gt;I, I-&gt;E, and I-&gt;I weight. <bold>d</bold>, Same as <xref ref-type="fig" rid="F4">Fig. 4l,m</xref>, but for inhibitory model units. Unlike the excitatory neurons, the firing rate of inhibitory model units always increased with increasing visual input.</p></caption><graphic xlink:href="EMS199633-f015"/></fig></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Tables</label><media xlink:href="EMS199633-supplement-Supplementary_Tables.pdf" mimetype="application" mime-subtype="pdf" id="d46aAcFbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S36"><title>Acknowledgements</title><p>We thank L. Mainville for performing histology for post hoc identification of electrode tracks. We thank T. Frank, A. Krishnaswamy, J-M. Martinez de Paz, and P. Wanken, T. Gollisch and N. Gogolla for helpful discussions and feedback on the manuscript. We thank G. Montaldo and A. Urban for technical support with fUS data acquisition. We thank H. Lin and A. Aliyeva for their contributions on aspects of this project not included in the manuscript. D.S. was supported by the Swiss National Science Foundation (SNSF) Early Postdoc.Mobility no. 194957, SNSF Postdoc.Mobility no. 211087 and Deutsche Forschungsgemeinschaft Walter-Benjamin Programm (Stelle) no. SI 2831/1-1. E. M. was funded by the Max Planck Society and the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy - EXC 2067/1-390729940. We acknowledge funding from the Vanier Canadian Graduate Scholarship to S.S.C. Canada Research Chairs to A.P. and S.T.; Canadian Institutes of Health Research Project Grants (190289 and 180330) and National Sciences and Engineering Research Council Discovery Grant (RGPIN-2018-04600) to A.P.; Alfred P. Sloan Foundation Research Fellowship, Human Frontiers Science Program Career Development Award, Office of Naval Research Global Grant, and a Canadian Institutes of Health Research Project Grant (185933) to S.T.</p></ack><sec id="S37" sec-type="data-availability"><title>Data and code availability</title><p id="P74">All data and code will be made available on public repositories upon publication.</p></sec><fn-group><fn id="FN3" fn-type="con"><p id="P75"><bold>Author Contributions</bold></p><p id="P76">Experiments were designed by D.S., S.T. and E.M. fUS experiments were performed by D.S. Anesthetized electrophysiological experiments were performed by D.S. and J.L.M. Awake electrophysiological experiments were performed by H.D. and S.S.C. Data was analyzed by D.S. SNN modelling work was performed by D.L. The manuscript was written by D.S., A.P., S.T. and E.M.</p></fn><fn id="FN4" fn-type="conflict"><p id="P77"><bold>Competing interests</bold></p><p id="P78">The authors have no competing interests to report.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Dostrovsky</surname><given-names>J</given-names></name></person-group><article-title>The hippocampus as a spatial map: Preliminary evidence from unit activity in the freely-moving rat</article-title><source>Brain Res</source><year>1971</year><volume>34</volume><fpage>171</fpage><lpage>175</lpage><pub-id pub-id-type="pmid">5124915</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taube</surname><given-names>JS</given-names></name><name><surname>Muller</surname><given-names>RU</given-names></name><name><surname>Ranck</surname><given-names>JB</given-names></name></person-group><article-title>Head-direction cells recorded from the postsubiculum in freely moving rats. I. Description and quantitative analysis</article-title><source>J Neurosci</source><year>1990</year><volume>10</volume><fpage>420</fpage><lpage>435</lpage><pub-id pub-id-type="pmcid">PMC6570151</pub-id><pub-id pub-id-type="pmid">2303851</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.10-02-00420.1990</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hafting</surname><given-names>T</given-names></name><name><surname>Fyhn</surname><given-names>M</given-names></name><name><surname>Molden</surname><given-names>S</given-names></name><name><surname>Moser</surname><given-names>M-B</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><article-title>Microstructure of a spatial map in the entorhinal cortex</article-title><source>Nature</source><year>2005</year><volume>436</volume><fpage>801</fpage><lpage>806</lpage><pub-id pub-id-type="pmid">15965463</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tolman</surname><given-names>EC</given-names></name></person-group><article-title>Cognitive maps in rats and men</article-title><source>Psychol Rev</source><year>1948</year><volume>55</volume><fpage>189</fpage><lpage>208</lpage><pub-id pub-id-type="pmid">18870876</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McNaughton</surname><given-names>BL</given-names></name><name><surname>Battaglia</surname><given-names>FP</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name><name><surname>Moser</surname><given-names>M-B</given-names></name></person-group><article-title>Path integration and the neural basis of the ‘cognitive map’</article-title><source>Nat Rev Neurosci</source><year>2006</year><volume>7</volume><fpage>663</fpage><lpage>678</lpage><pub-id pub-id-type="pmid">16858394</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saleem</surname><given-names>AB</given-names></name><name><surname>Busse</surname><given-names>L</given-names></name></person-group><article-title>Interactions between rodent visual and spatial systems during navigation</article-title><source>Nat Rev Neurosci</source><year>2023</year><volume>24</volume><fpage>487</fpage><lpage>501</lpage><pub-id pub-id-type="pmid">37380885</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feldman</surname><given-names>J</given-names></name></person-group><article-title>What is a visual object?</article-title><source>Trends Cogn Sci</source><year>2003</year><volume>7</volume><fpage>252</fpage><lpage>256</lpage><pub-id pub-id-type="pmid">12804691</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chan</surname><given-names>E</given-names></name><name><surname>Baumann</surname><given-names>O</given-names></name><name><surname>Bellgrove</surname><given-names>MA</given-names></name><name><surname>Mattingley</surname><given-names>JB</given-names></name></person-group><article-title>From Objects to Landmarks: The Function of Visual Location Information in Spatial Navigation</article-title><source>Front Psychol</source><year>2012</year><volume>3</volume><pub-id pub-id-type="pmcid">PMC3427909</pub-id><pub-id pub-id-type="pmid">22969737</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2012.00304</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taube</surname><given-names>JS</given-names></name><name><surname>Muller</surname><given-names>RU</given-names></name><name><surname>Ranck</surname><given-names>JB</given-names></name></person-group><article-title>Head-direction cells recorded from the postsubiculum in freely moving rats. II. Effects of environmental manipulations</article-title><source>J Neurosci</source><year>1990</year><volume>10</volume><fpage>436</fpage><lpage>447</lpage><pub-id pub-id-type="pmcid">PMC6570161</pub-id><pub-id pub-id-type="pmid">2303852</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.10-02-00436.1990</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muller</surname><given-names>RU</given-names></name><name><surname>Kubie</surname><given-names>JL</given-names></name></person-group><article-title>The effects of changes in the environment on the spatial firing of hippocampal complex-spike cells</article-title><source>J Neurosci</source><year>1987</year><volume>7</volume><fpage>1951</fpage><lpage>1968</lpage><pub-id pub-id-type="pmcid">PMC6568940</pub-id><pub-id pub-id-type="pmid">3612226</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.07-07-01951.1987</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jeffery</surname><given-names>KJ</given-names></name></person-group><article-title>Learning of landmark stability and instability by hippocampal place cells</article-title><source>Neuropharmacology</source><year>1998</year><volume>37</volume><fpage>677</fpage><lpage>687</lpage><pub-id pub-id-type="pmid">9705005</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Asumbisa</surname><given-names>K</given-names></name><name><surname>Peyrache</surname><given-names>A</given-names></name><name><surname>Trenholm</surname><given-names>S</given-names></name></person-group><article-title>Flexible cue anchoring strategies enable stable head direction coding in both sighted and blind animals</article-title><source>Nat Commun</source><year>2022</year><volume>13</volume><fpage>5483</fpage><pub-id pub-id-type="pmcid">PMC9485117</pub-id><pub-id pub-id-type="pmid">36123333</pub-id><pub-id pub-id-type="doi">10.1038/s41467-022-33204-0</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waaga</surname><given-names>T</given-names></name><etal/></person-group><article-title>Grid-cell modules remain coordinated when neural activity is dissociated from external sensory cues</article-title><source>Neuron</source><year>2022</year><volume>110</volume><fpage>1843</fpage><lpage>1856</lpage><elocation-id>e6</elocation-id><pub-id pub-id-type="pmcid">PMC9235855</pub-id><pub-id pub-id-type="pmid">35385698</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2022.03.011</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoder</surname><given-names>RM</given-names></name><name><surname>Clark</surname><given-names>BJ</given-names></name><name><surname>Taube</surname><given-names>JS</given-names></name></person-group><article-title>Origins of landmark encoding in the brain</article-title><source>Trends Neurosci</source><year>2011</year><volume>34</volume><fpage>561</fpage><lpage>571</lpage><pub-id pub-id-type="pmcid">PMC3200508</pub-id><pub-id pub-id-type="pmid">21982585</pub-id><pub-id pub-id-type="doi">10.1016/j.tins.2011.08.004</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name><name><surname>Rust</surname><given-names>NC</given-names></name></person-group><article-title>How does the brain solve visual object recognition?</article-title><source>Neuron</source><year>2012</year><volume>73</volume><fpage>415</fpage><lpage>434</lpage><pub-id pub-id-type="pmcid">PMC3306444</pub-id><pub-id pub-id-type="pmid">22325196</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2012.01.010</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ungerleider</surname><given-names>LG</given-names></name><name><surname>Mishkin</surname><given-names>Mortimer</given-names></name></person-group><chapter-title>Two cortical visual systems</chapter-title><source>Analysis of visual behavior</source><fpage>549</fpage><lpage>586</lpage><publisher-name>MIT Press</publisher-name><publisher-loc>Cambridge, Mass</publisher-loc></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tafazoli</surname><given-names>S</given-names></name><etal/></person-group><article-title>Emergence of transformation-tolerant representations of visual objects in rat lateral extrastriate cortex</article-title><source>eLife</source><year>2017</year><volume>6</volume><pub-id pub-id-type="pmcid">PMC5388540</pub-id><pub-id pub-id-type="pmid">28395730</pub-id><pub-id pub-id-type="doi">10.7554/eLife.22794</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Froudarakis</surname><given-names>E</given-names></name><etal/></person-group><article-title>Object manifold geometry across the mouse cortical visual hierarchy</article-title><year>2021</year><elocation-id>20200820.258798</elocation-id><pub-id pub-id-type="doi">10.1101/2020.08.20.258798</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>Y</given-names></name><name><surname>Stirman</surname><given-names>JN</given-names></name><name><surname>Dorsett</surname><given-names>CR</given-names></name><name><surname>Smith</surname><given-names>SL</given-names></name></person-group><article-title>Selective representations of texture and motion in mouse higher visual areas</article-title><source>Curr Biol</source><year>2022</year><volume>32</volume><fpage>2810</fpage><lpage>2820</lpage><elocation-id>e5</elocation-id><pub-id pub-id-type="pmcid">PMC9283294</pub-id><pub-id pub-id-type="pmid">35609609</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2022.04.091</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tong</surname><given-names>R</given-names></name><etal/></person-group><article-title>The feature landscape of visual cortex</article-title><year>2023</year><elocation-id>2023.1103.565500</elocation-id><pub-id pub-id-type="doi">10.1101/2023.11.03.565500</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zoccolan</surname><given-names>D</given-names></name><name><surname>Oertelt</surname><given-names>N</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Cox</surname><given-names>DD</given-names></name></person-group><article-title>A rodent model for the study of invariant visual object recognition</article-title><source>Proc Natl Acad Sci U. S. A</source><year>2009</year><volume>106</volume><fpage>8748</fpage><lpage>8753</lpage></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoy</surname><given-names>JL</given-names></name><name><surname>Yavorska</surname><given-names>I</given-names></name><name><surname>Wehr</surname><given-names>M</given-names></name><name><surname>Niell</surname><given-names>CM</given-names></name></person-group><article-title>Vision Drives Accurate Approach Behavior during Prey Capture in Laboratory Mice</article-title><source>Curr Biol</source><year>2016</year><volume>26</volume><fpage>3046</fpage><lpage>3052</lpage><pub-id pub-id-type="pmcid">PMC5121011</pub-id><pub-id pub-id-type="pmid">27773567</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2016.09.009</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Macé</surname><given-names>É</given-names></name><etal/></person-group><article-title>Functional ultrasound imaging of the brain</article-title><source>Nat Methods</source><year>2011</year><volume>8</volume><fpage>662</fpage><lpage>664</lpage><pub-id pub-id-type="pmid">21725300</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Macé</surname><given-names>É</given-names></name><etal/></person-group><article-title>Whole-Brain Functional Ultrasound Imaging Reveals Brain Modules for Visuomotor Integration</article-title><source>Neuron</source><year>2018</year><volume>100</volume><fpage>1241</fpage><lpage>1251</lpage><elocation-id>e7</elocation-id><pub-id pub-id-type="pmcid">PMC6292977</pub-id><pub-id pub-id-type="pmid">30521779</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2018.11.031</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunner</surname><given-names>C</given-names></name><etal/></person-group><article-title>Whole-brain functional ultrasound imaging in awake head-fixed mice</article-title><source>Nat Protoc</source><year>2021</year><volume>16</volume><fpage>3547</fpage><lpage>3571</lpage><pub-id pub-id-type="pmid">34089019</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Malach</surname><given-names>R</given-names></name><etal/></person-group><article-title>Object-related activity revealed by functional magnetic resonance imaging in human occipital cortex</article-title><source>Proc Natl Acad Sci</source><year>1995</year><volume>92</volume><fpage>8135</fpage><lpage>8139</lpage></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>McDermott</surname><given-names>J</given-names></name><name><surname>Chun</surname><given-names>MM</given-names></name></person-group><article-title>The fusiform face area: a module in human extrastriate cortex specialized for face perception</article-title><source>J Neurosci Off J Soc Neurosci</source><year>1997</year><volume>17</volume><fpage>4302</fpage><lpage>4311</lpage><pub-id pub-id-type="pmcid">PMC6573547</pub-id><pub-id pub-id-type="pmid">9151747</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.17-11-04302.1997</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nunez-Elizalde</surname><given-names>AO</given-names></name><etal/></person-group><article-title>Neural correlates of blood flow measured by ultrasound</article-title><source>Neuron</source><year>2022</year><volume>110</volume><fpage>1631</fpage><lpage>1640</lpage><elocation-id>e4</elocation-id><pub-id pub-id-type="pmcid">PMC9235295</pub-id><pub-id pub-id-type="pmid">35278361</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2022.02.012</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunner</surname><given-names>C</given-names></name><etal/></person-group><article-title>A Platform for Brain-wide Volumetric Functional Ultrasound Imaging and Analysis of Circuit Dynamics in Awake Mice</article-title><source>Neuron</source><year>2020</year><volume>108</volume><fpage>861</fpage><lpage>875</lpage><elocation-id>e7</elocation-id><pub-id pub-id-type="pmid">33080230</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Edelman</surname><given-names>BJ</given-names></name><etal/></person-group><article-title>The COMBO window: A chronic cranial implant for multiscale circuit interrogation in mice</article-title><source>PLOS Biol</source><year>2024</year><volume>22</volume><elocation-id>e3002664</elocation-id><pub-id pub-id-type="pmcid">PMC11185485</pub-id><pub-id pub-id-type="pmid">38829885</pub-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3002664</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Beest</surname><given-names>EH</given-names></name><etal/></person-group><article-title>Mouse visual cortex contains a region of enhanced spatial resolution</article-title><source>Nat Commun</source><year>2021</year><volume>12</volume><fpage>4029</fpage><pub-id pub-id-type="pmcid">PMC8242089</pub-id><pub-id pub-id-type="pmid">34188047</pub-id><pub-id pub-id-type="doi">10.1038/s41467-021-24311-5</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saleem</surname><given-names>AB</given-names></name></person-group><article-title>Two stream hypothesis of visual processing for navigation in mouse</article-title><source>Curr Opin Neurobiol</source><year>2020</year><volume>64</volume><fpage>70</fpage><lpage>78</lpage><pub-id pub-id-type="pmid">32294570</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Portilla</surname><given-names>J</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><article-title>A Parametric Texture Model Based on Joint Statistics of Complex Wavelet Coefficients</article-title><source>Int J Comput Vis</source><year>2000</year><volume>40</volume><fpage>49</fpage><lpage>70</lpage></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rust</surname><given-names>NC</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><article-title>Selectivity and Tolerance (“Invariance”) Both Increase as Visual Information Propagates from Cortical Area V4 to IT</article-title><source>J Neurosci</source><year>2010</year><volume>30</volume><fpage>12978</fpage><lpage>12995</lpage><pub-id pub-id-type="pmcid">PMC2975390</pub-id><pub-id pub-id-type="pmid">20881116</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0179-10.2010</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rust</surname><given-names>NC</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><article-title>Balanced increases in selectivity and tolerance produce constant sparseness along the ventral visual stream</article-title><source>J Neurosci Off J Soc Neurosci</source><year>2012</year><volume>32</volume><fpage>10170</fpage><lpage>10182</lpage><pub-id pub-id-type="pmcid">PMC3485085</pub-id><pub-id pub-id-type="pmid">22836252</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6125-11.2012</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischer</surname><given-names>LF</given-names></name><name><surname>Mojica Soto-Albors</surname><given-names>R</given-names></name><name><surname>Buck</surname><given-names>F</given-names></name><name><surname>Harnett</surname><given-names>MT</given-names></name></person-group><article-title>Representation of visual landmarks in retrosplenial cortex</article-title><source>eLife</source><year>2020</year><volume>9</volume><elocation-id>e51458</elocation-id><pub-id pub-id-type="pmcid">PMC7064342</pub-id><pub-id pub-id-type="pmid">32154781</pub-id><pub-id pub-id-type="doi">10.7554/eLife.51458</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sit</surname><given-names>KK</given-names></name><name><surname>Goard</surname><given-names>MJ</given-names></name></person-group><article-title>Coregistration of heading to visual cues in retrosplenial cortex</article-title><source>Nat Commun</source><year>2023</year><volume>14</volume><fpage>1992</fpage><pub-id pub-id-type="pmcid">PMC10082791</pub-id><pub-id pub-id-type="pmid">37031198</pub-id><pub-id pub-id-type="doi">10.1038/s41467-023-37704-5</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stojanoski</surname><given-names>B</given-names></name><name><surname>Cusack</surname><given-names>R</given-names></name></person-group><article-title>Time to wave good-bye to phase scrambling: Creating controlled scrambled images using diffeomorphic transformations</article-title><source>J Vis</source><year>2014</year><volume>14</volume><fpage>6</fpage><pub-id pub-id-type="pmid">25301014</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duszkiewicz</surname><given-names>AJ</given-names></name><etal/></person-group><article-title>Local origin of excitatory–inhibitory tuning equivalence in a cortical network</article-title><source>Nat Neurosci</source><year>2024</year><volume>27</volume><fpage>782</fpage><lpage>792</lpage><pub-id pub-id-type="pmcid">PMC11001581</pub-id><pub-id pub-id-type="pmid">38491324</pub-id><pub-id pub-id-type="doi">10.1038/s41593-024-01588-5</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chaudhuri</surname><given-names>R</given-names></name><name><surname>Gerçek</surname><given-names>B</given-names></name><name><surname>Pandey</surname><given-names>B</given-names></name><name><surname>Peyrache</surname><given-names>A</given-names></name><name><surname>Fiete</surname><given-names>I</given-names></name></person-group><article-title>The intrinsic attractor manifold and population dynamics of a canonical cognitive circuit across waking and sleep</article-title><source>Nat Neurosci</source><year>2019</year><volume>22</volume><fpage>1512</fpage><lpage>1520</lpage><pub-id pub-id-type="pmid">31406365</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knierim</surname><given-names>JJ</given-names></name><name><surname>Zhang</surname><given-names>K</given-names></name></person-group><article-title>Attractor Dynamics of Spatially Correlated Neural Activity in the Limbic System</article-title><source>Annu Rev Neurosci</source><year>2012</year><volume>35</volume><fpage>267</fpage><lpage>285</lpage><pub-id pub-id-type="pmcid">PMC5613981</pub-id><pub-id pub-id-type="pmid">22462545</pub-id><pub-id pub-id-type="doi">10.1146/annurev-neuro-062111-150351</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carrasco</surname><given-names>SS</given-names></name><name><surname>Viejo</surname><given-names>G</given-names></name><name><surname>Peyrache</surname><given-names>A</given-names></name></person-group><article-title>Months-long stability of the head-direction system</article-title><year>2024</year><elocation-id>2024.06.13.598909</elocation-id><pub-id pub-id-type="doi">10.1101/2024.06.13.598909</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taube</surname><given-names>JS</given-names></name><name><surname>Muller</surname><given-names>RU</given-names></name></person-group><article-title>Comparisons of head direction cell activity in the postsubiculum and anterior thalamus of freely moving rats</article-title><source>Hippocampus</source><year>1998</year><volume>8</volume><fpage>87</fpage><lpage>108</lpage><pub-id pub-id-type="pmid">9572715</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubin</surname><given-names>DB</given-names></name><name><surname>Van Hooser</surname><given-names>SD</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name></person-group><article-title>The Stabilized Supralinear Network: A Unifying Circuit Motif Underlying Multi-Input Integration in Sensory Cortex</article-title><source>Neuron</source><year>2015</year><volume>85</volume><fpage>402</fpage><lpage>417</lpage><pub-id pub-id-type="pmcid">PMC4344127</pub-id><pub-id pub-id-type="pmid">25611511</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2014.12.026</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hennequin</surname><given-names>G</given-names></name><name><surname>Ahmadian</surname><given-names>Y</given-names></name><name><surname>Rubin</surname><given-names>DB</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name></person-group><article-title>The Dynamical Regime of Sensory Cortex: Stable Dynamics around a Single Stimulus-Tuned Attractor Account for Patterns of Noise Variability</article-title><source>Neuron</source><year>2018</year><volume>98</volume><fpage>846</fpage><lpage>860</lpage><elocation-id>e5</elocation-id><pub-id pub-id-type="pmcid">PMC5971207</pub-id><pub-id pub-id-type="pmid">29772203</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2018.04.017</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bolaños</surname><given-names>F</given-names></name><etal/></person-group><article-title>Efficient coding of natural images in the mouse visual cortex</article-title><source>Nat Commun</source><year>2024</year><volume>15</volume><fpage>2466</fpage><pub-id pub-id-type="pmcid">PMC10951403</pub-id><pub-id pub-id-type="pmid">38503746</pub-id><pub-id pub-id-type="doi">10.1038/s41467-024-45919-3</pub-id></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>S-L</given-names></name></person-group><article-title>Comparative anatomy of the prosubiculum, subiculum, presubiculum, postsubiculum, and parasubiculum in human, monkey, and rodent</article-title><source>J Comp Neurol</source><year>2013</year><volume>521</volume><fpage>4145</fpage><lpage>4162</lpage><pub-id pub-id-type="pmid">23839777</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alexander</surname><given-names>AS</given-names></name><name><surname>Place</surname><given-names>R</given-names></name><name><surname>Starrett</surname><given-names>MJ</given-names></name><name><surname>Chrastil</surname><given-names>ER</given-names></name><name><surname>Nitz</surname><given-names>DA</given-names></name></person-group><article-title>Rethinking retrosplenial cortex: Perspectives and predictions</article-title><source>Neuron</source><year>2023</year><volume>111</volume><fpage>150</fpage><lpage>175</lpage><pub-id pub-id-type="pmid">36460006</pub-id></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vann</surname><given-names>SD</given-names></name><name><surname>Aggleton</surname><given-names>JP</given-names></name><name><surname>Maguire</surname><given-names>EA</given-names></name></person-group><article-title>What does the retrosplenial cortex do?</article-title><source>Nat Rev Neurosci</source><year>2009</year><volume>10</volume><fpage>792</fpage><lpage>802</lpage><pub-id pub-id-type="pmid">19812579</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>BJ</given-names></name><name><surname>Bassett</surname><given-names>JP</given-names></name><name><surname>Wang</surname><given-names>SS</given-names></name><name><surname>Taube</surname><given-names>JS</given-names></name></person-group><article-title>Impaired Head Direction Cell Representation in the Anterodorsal Thalamus after Lesions of the Retrosplenial Cortex</article-title><source>J Neurosci</source><year>2010</year><volume>30</volume><fpage>5289</fpage><lpage>5302</lpage><pub-id pub-id-type="pmcid">PMC2861549</pub-id><pub-id pub-id-type="pmid">20392951</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3380-09.2010</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bicanski</surname><given-names>A</given-names></name><name><surname>Burgess</surname><given-names>N</given-names></name></person-group><article-title>Environmental Anchoring of Head Direction in a Computational Model of Retrosplenial Cortex</article-title><source>J Neurosci</source><year>2016</year><volume>36</volume><fpage>11601</fpage><lpage>11618</lpage><pub-id pub-id-type="pmcid">PMC5125222</pub-id><pub-id pub-id-type="pmid">27852770</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0516-16.2016</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacob</surname><given-names>P-Y</given-names></name><etal/></person-group><article-title>An independent, landmark-dominated head-direction signal in dysgranular retrosplenial cortex</article-title><source>Nat Neurosci</source><year>2017</year><volume>20</volume><fpage>173</fpage><lpage>175</lpage><pub-id pub-id-type="pmcid">PMC5274535</pub-id><pub-id pub-id-type="pmid">27991898</pub-id><pub-id pub-id-type="doi">10.1038/nn.4465</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Page</surname><given-names>HJI</given-names></name><name><surname>Jeffery</surname><given-names>KJ</given-names></name></person-group><article-title>Landmark-Based Updating of the Head Direction System by Retrosplenial Cortex: A Computational Model</article-title><source>Front Cell Neurosci</source><year>2018</year><volume>12</volume><pub-id pub-id-type="pmcid">PMC6055052</pub-id><pub-id pub-id-type="pmid">30061814</pub-id><pub-id pub-id-type="doi">10.3389/fncel.2018.00191</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Auger</surname><given-names>SD</given-names></name><name><surname>Mullally</surname><given-names>SL</given-names></name><name><surname>Maguire</surname><given-names>EA</given-names></name></person-group><article-title>Retrosplenial Cortex Codes for Permanent Landmarks</article-title><source>PLOS ONE</source><year>2012</year><volume>7</volume><elocation-id>e43620</elocation-id><pub-id pub-id-type="pmcid">PMC3422332</pub-id><pub-id pub-id-type="pmid">22912894</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0043620</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taube</surname><given-names>JS</given-names></name></person-group><article-title>The head direction signal: origins and sensory-motor integration</article-title><source>Annu Rev Neurosci</source><year>2007</year><volume>30</volume><fpage>181</fpage><lpage>207</lpage><pub-id pub-id-type="pmid">17341158</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodridge</surname><given-names>JP</given-names></name><name><surname>Taube</surname><given-names>JS</given-names></name></person-group><article-title>Interaction between the Postsubiculum and Anterior Thalamus in the Generation of Head Direction Cell Activity</article-title><source>J Neurosci</source><year>1997</year><volume>17</volume><fpage>9315</fpage><lpage>9330</lpage><pub-id pub-id-type="pmcid">PMC6573595</pub-id><pub-id pub-id-type="pmid">9364077</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.17-23-09315.1997</pub-id></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Preston-Ferrer</surname><given-names>P</given-names></name><name><surname>Coletta</surname><given-names>S</given-names></name><name><surname>Frey</surname><given-names>M</given-names></name><name><surname>Burgalossi</surname><given-names>A</given-names></name></person-group><article-title>Anatomical organization of presubicular head-direction circuits</article-title><source>eLife</source><year>2016</year><volume>5</volume><elocation-id>e14592</elocation-id><pub-id pub-id-type="pmcid">PMC4927294</pub-id><pub-id pub-id-type="pmid">27282390</pub-id><pub-id pub-id-type="doi">10.7554/eLife.14592</pub-id></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sargolini</surname><given-names>F</given-names></name><etal/></person-group><article-title>Conjunctive Representation of Position, Direction, and Velocity in Entorhinal Cortex</article-title><source>Science</source><year>2006</year><volume>312</volume><fpage>758</fpage><lpage>762</lpage><pub-id pub-id-type="pmid">16675704</pub-id></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giocomo</surname><given-names>LM</given-names></name><etal/></person-group><article-title>Topography of Head Direction Cells in Medial Entorhinal Cortex</article-title><source>Curr Biol</source><year>2014</year><volume>24</volume><fpage>252</fpage><lpage>262</lpage><pub-id pub-id-type="pmid">24440398</pub-id></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Acharya</surname><given-names>L</given-names></name><name><surname>Aghajan</surname><given-names>ZM</given-names></name><name><surname>Vuong</surname><given-names>C</given-names></name><name><surname>Moore</surname><given-names>JJ</given-names></name><name><surname>Mehta</surname><given-names>MR</given-names></name></person-group><article-title>Causal Influence of Visual Cues on Hippocampal Directional Selectivity</article-title><source>Cell</source><year>2016</year><volume>164</volume><fpage>197</fpage><lpage>207</lpage><pub-id pub-id-type="pmid">26709045</pub-id></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vollan</surname><given-names>AZ</given-names></name><name><surname>Gardner</surname><given-names>RJ</given-names></name><name><surname>Moser</surname><given-names>M-B</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><article-title>Left-right-alternating theta sweeps in the entorhinal-hippocampal spatial map</article-title><year>2024</year><elocation-id>2024.05.16.594473</elocation-id><pub-id pub-id-type="doi">10.1101/2024.05.16.594473</pub-id></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tamura</surname><given-names>H</given-names></name><name><surname>Tanaka</surname><given-names>K</given-names></name></person-group><article-title>Visual Response Properties of Cells in the Ventral and Dorsal Parts of the Macaque Inferotemporal Cortex</article-title><source>Cereb Cortex</source><year>2001</year><volume>11</volume><fpage>384</fpage><lpage>399</lpage><pub-id pub-id-type="pmid">11313291</pub-id></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oh</surname><given-names>SW</given-names></name><etal/></person-group><article-title>A mesoscale connectome of the mouse brain</article-title><source>Nature</source><year>2014</year><volume>508</volume><fpage>207</fpage><lpage>214</lpage><pub-id pub-id-type="pmcid">PMC5102064</pub-id><pub-id pub-id-type="pmid">24695228</pub-id><pub-id pub-id-type="doi">10.1038/nature13186</pub-id></element-citation></ref><ref id="R64"><label>64</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name><name><surname>Burkhalter</surname><given-names>A</given-names></name></person-group><article-title>Network Analysis of Corticocortical Connections Reveals Ventral and Dorsal Processing Streams in Mouse Visual Cortex</article-title><source>J Neurosci</source><year>2012</year><volume>32</volume><fpage>4386</fpage><lpage>4399</lpage><pub-id pub-id-type="pmcid">PMC3328193</pub-id><pub-id pub-id-type="pmid">22457489</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6063-11.2012</pub-id></element-citation></ref><ref id="R65"><label>65</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>D’Souza</surname><given-names>RD</given-names></name><etal/></person-group><article-title>Hierarchical and nonhierarchical features of the mouse visual cortical network</article-title><source>Nat Commun</source><year>2022</year><volume>13</volume><fpage>503</fpage><pub-id pub-id-type="pmcid">PMC8791996</pub-id><pub-id pub-id-type="pmid">35082302</pub-id><pub-id pub-id-type="doi">10.1038/s41467-022-28035-y</pub-id></element-citation></ref><ref id="R66"><label>66</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willenbockel</surname><given-names>V</given-names></name><etal/></person-group><article-title>Controlling low-level image properties: The SHINE toolbox</article-title><source>Behav Res Methods</source><year>2010</year><volume>42</volume><fpage>671</fpage><lpage>684</lpage><pub-id pub-id-type="pmid">20805589</pub-id></element-citation></ref><ref id="R67"><label>67</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McInnes</surname><given-names>L</given-names></name><name><surname>Healy</surname><given-names>J</given-names></name><name><surname>Melville</surname><given-names>J</given-names></name></person-group><article-title>UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction</article-title><year>2020</year><pub-id pub-id-type="doi">10.48550/arXiv.1802.03426</pub-id></element-citation></ref><ref id="R68"><label>68</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morel</surname><given-names>P</given-names></name></person-group><article-title>Gramm: grammar of graphics plotting in Matlab</article-title><source>J Open Source Softw</source><year>2018</year><volume>3</volume><fpage>568</fpage></element-citation></ref><ref id="R69"><label>69</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Burkhalter</surname><given-names>A</given-names></name></person-group><article-title>Area map of mouse visual cortex</article-title><source>J Comp Neurol</source><year>2007</year><volume>502</volume><fpage>339</fpage><lpage>357</lpage><pub-id pub-id-type="pmid">17366604</pub-id></element-citation></ref><ref id="R70"><label>70</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name><name><surname>Burkhalter</surname><given-names>A</given-names></name></person-group><article-title>Network analysis of corticocortical connections reveals ventral and dorsal processing streams in mouse visual cortex</article-title><source>J Neurosci</source><year>2012</year><volume>32</volume><fpage>4386</fpage><lpage>4399</lpage></element-citation></ref><ref id="R71"><label>71</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marshel</surname><given-names>JH</given-names></name><name><surname>Garrett</surname><given-names>ME</given-names></name><name><surname>Nauhaus</surname><given-names>I</given-names></name><name><surname>Callaway</surname><given-names>EM</given-names></name></person-group><article-title>Functional Specialization of Seven Mouse Visual Cortical Areas</article-title><source>Neuron</source><year>2011</year><volume>72</volume><fpage>1040</fpage><lpage>1054</lpage><pub-id pub-id-type="pmcid">PMC3248795</pub-id><pub-id pub-id-type="pmid">22196338</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2011.12.004</pub-id></element-citation></ref><ref id="R72"><label>72</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siegle</surname><given-names>JH</given-names></name><etal/></person-group><article-title>Survey of spiking in the mouse visual system reveals functional hierarchy</article-title><source>Nature</source><year>2021</year><volume>592</volume><fpage>86</fpage><lpage>92</lpage><pub-id pub-id-type="pmcid">PMC10399640</pub-id><pub-id pub-id-type="pmid">33473216</pub-id><pub-id pub-id-type="doi">10.1038/s41586-020-03171-x</pub-id></element-citation></ref><ref id="R73"><label>73</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morin</surname><given-names>LP</given-names></name><name><surname>Studholme</surname><given-names>KM</given-names></name></person-group><article-title>Retinofugal projections in the mouse</article-title><source>J Comp Neurol</source><year>2014</year><volume>522</volume><fpage>3733</fpage><lpage>3753</lpage><pub-id pub-id-type="pmcid">PMC4142087</pub-id><pub-id pub-id-type="pmid">24889098</pub-id><pub-id pub-id-type="doi">10.1002/cne.23635</pub-id></element-citation></ref><ref id="R74"><label>74</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>AE</given-names></name><name><surname>Procyk</surname><given-names>CA</given-names></name><name><surname>Howarth</surname><given-names>M</given-names></name><name><surname>Walmsley</surname><given-names>L</given-names></name><name><surname>Brown</surname><given-names>TM</given-names></name></person-group><article-title>Visual input to the mouse lateral posterior and posterior thalamic nuclei: photoreceptive origins and retinotopic order</article-title><source>J Physiol</source><year>2016</year><volume>594</volume><fpage>1911</fpage><lpage>1929</lpage><pub-id pub-id-type="pmcid">PMC4818601</pub-id><pub-id pub-id-type="pmid">26842995</pub-id><pub-id pub-id-type="doi">10.1113/JP271707</pub-id></element-citation></ref><ref id="R75"><label>75</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seabrook</surname><given-names>TA</given-names></name><name><surname>Burbridge</surname><given-names>TJ</given-names></name><name><surname>Crair</surname><given-names>MC</given-names></name><name><surname>Huberman</surname><given-names>AD</given-names></name></person-group><article-title>Architecture, Function, and Assembly of the Mouse Visual System</article-title><year>2017</year><pub-id pub-id-type="pmid">28772103</pub-id></element-citation></ref><ref id="R76"><label>76</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Bonito</surname><given-names>M</given-names></name><name><surname>Studer</surname><given-names>M</given-names></name></person-group><article-title>Frontiers | Cellular and Molecular Underpinnings of Neuronal Assembly in the Central Auditory System during Mouse Development</article-title><pub-id pub-id-type="pmcid">PMC5395578</pub-id><pub-id pub-id-type="pmid">28469562</pub-id><pub-id pub-id-type="doi">10.3389/fncir.2017.00018</pub-id></element-citation></ref><ref id="R77"><label>77</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>H</given-names></name><etal/></person-group><article-title>Infralimbic medial prefrontal cortex signalling to calbindin 1 positive neurons in posterior basolateral amygdala suppresses anxiety- and depression-like behaviours</article-title><source>Nat Commun</source><year>2022</year><volume>13</volume><fpage>5462</fpage><pub-id pub-id-type="pmcid">PMC9482654</pub-id><pub-id pub-id-type="pmid">36115848</pub-id><pub-id pub-id-type="doi">10.1038/s41467-022-33139-6</pub-id></element-citation></ref><ref id="R78"><label>78</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mukherjee</surname><given-names>A</given-names></name><name><surname>Caroni</surname><given-names>P</given-names></name></person-group><article-title>Infralimbic cortex is required for learning alternatives to prelimbic promoted associations through reciprocal connectivity</article-title><source>Nat Commun</source><year>2018</year><volume>9</volume><fpage>2727</fpage><pub-id pub-id-type="pmcid">PMC6045592</pub-id><pub-id pub-id-type="pmid">30006525</pub-id><pub-id pub-id-type="doi">10.1038/s41467-018-05318-x</pub-id></element-citation></ref><ref id="R79"><label>79</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tovote</surname><given-names>P</given-names></name><etal/></person-group><article-title>Midbrain circuits for defensive behaviour</article-title><source>Nature</source><year>2016</year><volume>534</volume><fpage>206</fpage><lpage>212</lpage><pub-id pub-id-type="pmid">27279213</pub-id></element-citation></ref><ref id="R80"><label>80</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bandler</surname><given-names>R</given-names></name><name><surname>Shipley</surname><given-names>MT</given-names></name></person-group><article-title>Columnar organization in the midbrain periaqueductal gray: modules for emotional expression?</article-title><source>Trends Neurosci</source><year>1994</year><volume>17</volume><fpage>379</fpage><lpage>389</lpage><pub-id pub-id-type="pmid">7817403</pub-id></element-citation></ref><ref id="R81"><label>81</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Connell</surname><given-names>LA</given-names></name><name><surname>Hofmann</surname><given-names>HA</given-names></name></person-group><article-title>The Vertebrate mesolimbic reward system and social behavior network: A comparative synthesis</article-title><source>J Comp Neurol</source><year>2011</year><volume>519</volume><fpage>3599</fpage><lpage>3639</lpage><pub-id pub-id-type="pmid">21800319</pub-id></element-citation></ref><ref id="R82"><label>82</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geiller</surname><given-names>T</given-names></name><name><surname>Priestley</surname><given-names>JB</given-names></name><name><surname>Losonczy</surname><given-names>A</given-names></name></person-group><article-title>A local circuit-basis for spatial navigation and memory processes in hippocampal area CA1</article-title><source>Curr Opin Neurobiol</source><year>2023</year><volume>79</volume><elocation-id>102701</elocation-id><pub-id pub-id-type="pmcid">PMC10020891</pub-id><pub-id pub-id-type="pmid">36878147</pub-id><pub-id pub-id-type="doi">10.1016/j.conb.2023.102701</pub-id></element-citation></ref><ref id="R83"><label>83</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Strien</surname><given-names>NM</given-names></name><name><surname>Cappaert</surname><given-names>NLM</given-names></name><name><surname>Witter</surname><given-names>MP</given-names></name></person-group><article-title>The anatomy of memory: an interactive overview of the parahippocampal–hippocampal network</article-title><source>Nat Rev Neurosci</source><year>2009</year><volume>10</volume><fpage>272</fpage><lpage>282</lpage><pub-id pub-id-type="pmid">19300446</pub-id></element-citation></ref><ref id="R84"><label>84</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Subramanian</surname><given-names>DL</given-names></name><name><surname>Miller</surname><given-names>AMP</given-names></name><name><surname>Smith</surname><given-names>DM</given-names></name></person-group><article-title>A comparison of hippocampal and retrosplenial cortical spatial and contextual firing patterns</article-title><source>Hippocampus</source><year>2024</year><volume>34</volume><fpage>357</fpage><lpage>377</lpage><pub-id pub-id-type="pmid">38770779</pub-id></element-citation></ref><ref id="R85"><label>85</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mitchell</surname><given-names>AS</given-names></name><name><surname>Czajkowski</surname><given-names>R</given-names></name><name><surname>Zhang</surname><given-names>N</given-names></name><name><surname>Jeffery</surname><given-names>K</given-names></name><name><surname>Nelson</surname><given-names>AJD</given-names></name></person-group><article-title>Retrosplenial cortex and its role in spatial cognition</article-title><source>Brain Neurosci Adv</source><year>2018</year><volume>2</volume><elocation-id>2398212818757098</elocation-id><pub-id pub-id-type="pmcid">PMC6095108</pub-id><pub-id pub-id-type="pmid">30221204</pub-id><pub-id pub-id-type="doi">10.1177/2398212818757098</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Fig 1</label><caption><title>Visual object sensitive brain areas revealed with brain-wide fUS imaging.</title><p><bold>a</bold>, Schematic of fUS imaging with visual stimulation. <bold>b</bold>, <italic>top</italic>, Schematic of the stimulus design, with blocks of object and scrambled images, interleaved with full-field grey stimuli. <bold>b,</bold> <italic>bottom</italic>, Pooled fUS responses (mean ± s.e.m.) from V1 for each stimulus block (n = 56 sessions from 7 anesthetized animals; same data for all analyses in this figure). <bold>c</bold>, Visually-evoked fUS responses (all image blocks as regressors) overlaid on coronal brain images at indicated positions. Only T-scores significantly different from zero are shown, p &lt; 0.05, FDR-corrected, mixed effects model. <bold>d</bold>, Visually-active voxels color coded by VOSI<sub>T</sub> values are displayed on coronal brain images (only absolute VOSI<sub>T</sub> values larger than 0.015 are shown). <bold>e</bold>, Brain areas with a VOSI<sub>T</sub> significantly different from zero are colored by their VOSI<sub>T</sub> values, p &lt; 0.05, FDR-corrected, mixed effects model. <bold>f</bold>, Average fUS responses to Object and ScrambledT images, from PoSub, V1 and SC. p (from left to right) = 0.001, 0.359, 6.3*10<sup>-7</sup>, Bonferroni-Holm (B-H) corrected, mixed effects model. <bold>g</bold>, Rank ordering of visually-responsive brain areas according to VOSI<sub>T</sub> values (<italic>black dots</italic>), color-coded (<italic>squares</italic>) according to Spatial navigation (<italic>green</italic>), Vision (<italic>orange</italic>) or Other (<italic>grey</italic>) brain networks. <bold>h</bold>, VOSI<sub>T</sub> values of all brain regions belonging to either Spatial navigation (<italic>green</italic>) or Vision networks (<italic>orange</italic>). For statistical comparison between the two networks, p =7 .5*10<sup>-4</sup>, Mann-Whitney U-test. VOSI<sub>T</sub> values for Spatial navigation but not Vision networks statistically differed from 0. p(Spatial navigation) = 0.008, p(Vision) = 0.952, B-H corrected, one-sample Wilcoxon signed rank test.</p></caption><graphic xlink:href="EMS199633-f001"/></fig><fig id="F2" position="float"><label>Fig 2</label><caption><title>A preference for visual objects in postsubiculum and dorsal retrosplenial cortex in single cell recordings.</title><p><bold>a</bold>, Schematic of the experimental approach and the recording sites <bold>b</bold>, <italic>left</italic>, Example Object (<italic>red box</italic>), ScrambledT (texture; <italic>blue box</italic>) and ScrambledD (diffeomorphic transformations; <italic>green box</italic>) images. <bold>c</bold>, <italic>left</italic>, Raster plot showing spiking responses from a single neuron in PoSub to Object (<italic>red</italic>) and ScrambledT (<italic>blue</italic>) images. <bold>c</bold>, <italic>right top</italic>, Mean (<italic>solid line</italic>) <italic>±</italic> s.e.m. (<italic>shaded area</italic>) responses to Object (<italic>red</italic>) and ScrambledT (<italic>blue</italic>) images from the same cell. <bold>c</bold>, <italic>right bottom</italic>, Mean (<italic>solid line</italic>) <italic>±</italic> s.e.m. (<italic>shaded area</italic>) response difference of Object-Scrambled<sub>T</sub> responses. Black solid line with shaded area shows the mean <italic>±</italic> s.e.m. for 100 permutations. <bold>d</bold>, Traces of Object-ScrambledT (<italic>purple line,</italic> mean ± s.e.m. across cells<italic>)</italic>. A shuffled response distribution is shown for visualization purposes (<italic>black line,</italic> mean ± s.e.m. of 100 permutations of trial labels shuffling), p (from top to bottom) = 2.4*10<sup>-8</sup>, 2.4*10<sup>-5</sup>, 0.595, 0.511, 0.351, B-H corrected, Wilcoxon signed rank test, (n (from top to bottom) = 192, 120, 81, 145, 61 cells). <bold>e</bold>, Object preference compared to texture stimuli for all five regions calculated using fUS data (VOSI<sub>T</sub>, x-axis, mean <italic>±</italic> s.e.m. n = 57 sessions) and ephys data (Object-ScrambledT, y-axis, mean <italic>±</italic> s.e.m., n’s as in (d)). <bold>f-g</bold>, The same as (<bold>c-d</bold>), except for Object vs. ScrambledD, p (from top to bottom) = 2.2*10<sup>-6</sup>, 0.017, 0.595, 0.456, 0.456, B-H corrected, Wilcoxon signed rank test, (n’s as in d). <bold>h</bold>, Object-ScrambledT and Object-ScrambledD values for neurons from the five different brain areas (mean <italic>±</italic> s.e.m., n’s and p’s as in (d,g)). <bold>i</bold>, Schematic of the visual stimulus protocols for measuring receptive field size, and spatial and temporal frequency preferences. <bold>j</bold>, Results of the receptive field and temporal frequency experiments described in (i) for all five regions (mean <italic>±</italic> s.e.m., see <xref ref-type="fig" rid="F11">Extended data Fig. 7</xref> for details).</p></caption><graphic xlink:href="EMS199633-f002"/></fig><fig id="F3" position="float"><label>Fig 3</label><caption><title>A preference for visual objects is present in HD and FS neurons in PoSub.</title><p><bold>a</bold>, <italic>left</italic>, Schematic of the two awake experimental paradigms, involving freely moving exploration of an open-field arena (<italic>top</italic>) followed by head-fixation and presentation of visual stimuli (<italic>bottom</italic>). <bold>a</bold>, <italic>right,</italic> Polar plots for four recorded HD cells in PoSub (<italic>top</italic>), and responses of the same cells to 500 randomly selected visual stimulus presentations (<italic>bottom</italic>), plotted both as raster plots and as a mean visually-evoked firing rate (FR). <bold>b</bold>, The average visually-evoked responses of all cells recorded in PoSub in awake animals (n = 319 cells from 5 animals), ordered from top-to-bottom by visual response strength. <bold>c</bold>, Example raster plots and average firing rates of an example neuron that was positively modulated by visual stimulation (<italic>left</italic>) and an example neuron that was negatively modulated by visual stimulation (<italic>right</italic>). Shown are 500 randomly selected trials on the raster plot and mean firing rate across all trials below. <bold>d</bold>, Mean (<italic>solid line</italic>) ± s.e.m. (<italic>shaded area</italic>) across positively or negatively modulated cells for Object-Scrambled<sub>T</sub> and Object-Scrambled<sub>D</sub> responses (n (from top to bottom) = 170, 86 cells). Black solid line with shaded area shows the mean ± s.e.m. for 100 permutations. <bold>e</bold>, Object-Scrambled<sub>T</sub> and Object-Scrambled<sub>D</sub> values for positively and negatively visually modulated PoSub neurons (mean ± s.e.m.), p (VOSI<sub>T</sub>, from left to right) = 8.56*10<sup>-6</sup>, 0.98, p (VOSI<sub>D</sub>, from left to right) = 2.2*10<sup>-5</sup>, 0.9, B-H corrected, Wilcoxon signed rank test, (n (from top to bottom) = 170, 86 cells). <bold>f</bold>, PoSub neurons from freely-moving recordings grouped into three categories (FS, fast spiking; HD, head direction; slow non-HD), based on baseline firing rate, spike waveform (trough to peak) and head direction information (bits per spike). <bold>g</bold>, Cell type distributions for negatively and positively modulated cells. <bold>h</bold>, Example raster plots (<italic>left</italic>) and mean ± s.e.m. firing rate (<italic>right</italic>) of an example PoSub HD cell in awake conditions to Object, Scrambled<sub>D</sub> and Scrambled<sub>T</sub> stimuli. <bold>i</bold>, Same as (g), but for an example FS neuron. <bold>j</bold>, VOSI<sub>T</sub> and VOSI<sub>D</sub> values for different types of positively modulated PoSub neurons (mean ± s.e.m.), p (VOSI<sub>D</sub>, from left to right) = 0.032, 0.555, 0.017, p (VOSI<sub>T</sub>, from left to right) = 0.003, 0.985, 0.047, B-H corrected, Wilcoxon signed rank test (n (from left to right) = 58, 14, 57 cells).</p></caption><graphic xlink:href="EMS199633-f003"/></fig><fig id="F4" position="float"><label>Fig 4</label><caption><title>Visual objects dynamically refine population-level HD encoding.</title><p><bold>a</bold>, Schematic outlining how, for each mouse, we compared the visual response and preferred firing direction for all simultaneously recorded HD cells. In brief, the preferred firing direction is indicated by the angular position of each cell (dot) on the ring, and the visual response is indicated by the dot color (orange, positive; green, negative). Data here is from one example mouse. <bold>b</bold>, The rings (dashed circles with colored dots) are shown for 5 mice, with the center of the positive visual response regions aligned to 0°. Central ring: Dashed line represents mean visual response across all bins and bold line the deviations from it (20 angular bins). <bold>c</bold>, Scatterplot of HD cell visual response as a function of preferred firing direction following alignment to 0° (n = 127 cells from 5 animals). <bold>d</bold>, Comparison between the visual responses of HD cells with preferred firing directions 0° ± 45° (blue) or 180 ± 135° (red), p = 1.13*10<sup>-4</sup>, Mann-Whitney U-test (n = 51 and 76 cells). <bold>e</bold>, <italic>left</italic>, For an example mouse, the visual response as a function of HD preferred firing direction is compared to the baseline firing rate (in head-fixed) as a function of HD preferred firing direction (before realignment to 0°). Violet and green dotted lines represent the circular mean for the example mouse. <bold>e</bold>, <italic>right</italic>, For each mouse, the center of positive visual responses is plotted against the center of region with the highest baseline firing (n = 5 mice). <bold>f</bold>, <italic>left</italic>, Schematic illustrating how for HDs of 0° ± 45° (with the direction of the wall containing the visual landmark set as 0°), from some positions the mice look directly at the landmark (On landmark) whereas from other positions mice do not directly look at the landmark (Off landmark). <bold>f</bold>, <italic>right</italic>, For an example mouse, its location over time is shown (orange) with HDs within the range of 0° ± 45° colored by whether the mouse is On landmark (black arrows) or Off landmark (grey arrows). The visual cue is represented by the thick brown line. <bold>g</bold>, Visual responses in freely-moving conditions for HD cells with preferred firing directions within the range of 0° ± 45 ° and 180° ± 135°, p = 0.002, Mann-Whitney U-test (n = 23, 68 cells). <bold>h</bold>, For HD cells with preferred firing directions within the range of 0° ± 45 ° and 180° ± 135°, the firing rate for both On and Off landmark positions, p (from left to right) = 0.037, 0.016, Wilcoxon signed rank test (n (from left to right) =23, 68 cells). <bold>i,</bold> Supralinear stabilized ring network. Excitatory and inhibitory neurons are assigned a preferred head direction at which they receive the strongest tuned input (current HD input). In addition, all neurons receive an untuned input (visual input). <bold>j-k</bold>, Neurons in the network have a supralinear activation function (<bold>j</bold>), and their local weights are determined by the difference in their preferred firing direction (<bold>k</bold>). Weight and activation function parameters were set following Hennequin et al.<sup><xref ref-type="bibr" rid="R44">44</xref></sup> and were varied to verify model robustness (see <xref ref-type="fig" rid="F15">Extended Data Fig. 11</xref>). <bold>l, m,</bold> Increasing/decreasing the level of untuned input (<bold>l</bold>) has a differential effect on model HD cell firing rates (<bold>m</bold>), depending if the preferred head direction is in-field (current HD: 0 degrees), or out-field (180 degrees).</p></caption><graphic xlink:href="EMS199633-f004"/></fig></floats-group></article>