<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS201993</article-id><article-id pub-id-type="doi">10.1101/2024.12.06.627189</article-id><article-id pub-id-type="archive">PPR954040</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Loom response in mouse superior colliculus depends on sensorimotor context</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zucca</surname><given-names>Stefano</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="fn" rid="FN1">7</xref></contrib><contrib contrib-type="author"><name><surname>Schulz</surname><given-names>Auguste</given-names></name><xref ref-type="aff" rid="A3">3</xref><xref ref-type="fn" rid="FN1">7</xref></contrib><contrib contrib-type="author"><name><surname>Gonçalves</surname><given-names>Pedro J.</given-names></name><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Macke</surname><given-names>Jakob H.</given-names></name><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Saleem</surname><given-names>Aman B.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="fn" rid="FN2">8</xref></contrib><contrib contrib-type="author"><name><surname>Solomon</surname><given-names>Samuel G.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="fn" rid="FN2">8</xref><xref ref-type="corresp" rid="CR1">☨</xref></contrib><aff id="A1"><label>1</label>Institute of Behavioural Neuroscience and Department of Experimental Psychology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap>, <addr-line>26 Bedford Way</addr-line>, <city>London</city>, <postal-code>WC1H 0AP</postal-code></aff><aff id="A2"><label>2</label>Department of Life Sciences and Systems Biology (DBIOS), <institution-wrap><institution-id institution-id-type="ror">https://ror.org/048tbm396</institution-id><institution>University of Turin</institution></institution-wrap>, <addr-line>via Accademia Albertina 13</addr-line>,<postal-code>10123</postal-code><city>Turin</city>, <country country="IT">Italy</country></aff><aff id="A3"><label>3</label>Machine Learning in Science, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/03a1kwz48</institution-id><institution>University of Tübingen</institution></institution-wrap> and <institution-wrap><institution-id institution-id-type="ror">https://ror.org/0107nyd78</institution-id><institution>Tübingen AI Center</institution></institution-wrap>, <addr-line>Maria-von-Linden-Str. 6</addr-line>, <postal-code>72076</postal-code><city>Tübingen</city>, <country country="DE">Germany</country></aff><aff id="A4"><label>4</label>VIB-Neuroelectronics Research Flanders (NERF) and imec, Remisebosweg 1, 3001 Leuven, Belgium</aff><aff id="A5"><label>5</label>Department of Computer Science and Department of Electrical Engineering, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/05f950310</institution-id><institution>KU Leuven</institution></institution-wrap>, <addr-line>Oude Markt 13</addr-line>, <postal-code>3000</postal-code><city>Leuven</city>, <country country="BE">Belgium</country></aff><aff id="A6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04fq9j139</institution-id><institution>Max Planck Institute for Intelligent Systems</institution></institution-wrap>, <addr-line>Max-Planck-Ring 4</addr-line>, <postal-code>72076</postal-code><city>Tübingen</city>, <country country="DE">Germany</country></aff></contrib-group><author-notes><corresp id="CR1">
<label>☨</label>Corresponding author: <email>s.solomon@ucl.ac.uk</email>
</corresp><fn id="FN1" fn-type="equal"><label>7</label><p id="P1">These authors contributed equally</p></fn><fn id="FN2" fn-type="equal"><label>8</label><p id="P2">These authors contributed equally</p></fn></author-notes><pub-date pub-type="nihms-submitted"><day>18</day><month>12</month><year>2024</year></pub-date><pub-date pub-type="preprint"><day>12</day><month>12</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P3">Visual motion is produced both by an organism’s movement through the world, and by objects moving in the world such as potential predators. Choosing appropriate behaviour therefore requires organisms to distinguish these sources of visual motion. Here we asked how mice integrate self-movement with looming visual motion by combining virtual reality and neural recordings from superior colliculus (SC), a brain area important in visually-guided approach and avoidance behaviours. We first measured locomotion behaviour and neural activity while animals approached an object in virtual reality, and while the same object loomed at them. In both cases, vision dominated activity in superficial layers (SCs), while locomotion had more influence on activity in intermediate layers (SCim). In addition, animals instinctively slowed their locomotion when nearing the object, or when the object neared them. To directly test animals’ ability to distinguish self- from object motion we replayed the visual images generated during object approach. Locomotion behaviour often changed during replay, showing animals are able to establish if visual motion is matched to their self-movement. Further, decoders trained on locomotion behaviour, or on population activity in SC, particularly in SCim, were able to reliably discriminate epochs of replay and object approach. We conclude that both mouse behaviour and SC activity encode whether looming visual motion arises from self- or object movement, with implications for understanding sensorimotor coordination in dynamic environments.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P4">The meaning of sensory activity is often ambiguous, because multiple contexts produce identical patterns of sensory stimulation. For example, identical sequences of retinal images can be produced by an object moving (looming) towards an observer, or an observer’s movement toward that object. Accounting for this ambiguity requires organisms to establish whether a pattern of sensory stimulation matches that expected from self-movement, or is more likely to arise from object movement, such as a potential prey or predator. The importance of distinguishing sensory stimulation arising from self- or object movement to organism survival may explain why there are widespread neural correlates of self-movement, such as locomotion, that converge with other sensory signals in multiple brain areas <sup><xref ref-type="bibr" rid="R1">1</xref>–<xref ref-type="bibr" rid="R5">5</xref></sup>.</p><p id="P5">Superior colliculus (SC) is one such multimodal brain area, that is implicated in visually-guided approach of potential prey or other objects, and avoidance of potential visual threats <sup><xref ref-type="bibr" rid="R6">6</xref>,<xref ref-type="bibr" rid="R7">7</xref></sup>. SC has access to visual sensory signals directly from retina <sup><xref ref-type="bibr" rid="R8">8</xref></sup> and via visual sensory cortices <sup><xref ref-type="bibr" rid="R9">9</xref></sup>, and can access locomotion-related and high-level contextual signals from multiple brain regions <sup><xref ref-type="bibr" rid="R10">10</xref>,<xref ref-type="bibr" rid="R11">11</xref></sup>. Retinal and visual cortical inputs are biased to superficial layers of SC (‘SCs’). Non-visual inputs are instead biased towards intermediate (‘SCim’) and deeper layers, which are also more tightly linked to behavioural outputs (see <sup><xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R12">12</xref>–<xref ref-type="bibr" rid="R15">15</xref></sup> for reviews).</p><p id="P6">The anatomical connections suggest that SCs should be primarily sensitive to visual signals, while activity in SCim should be more dependent on the interaction of locomotion and vision, and more able to distinguish self- from object movement. There is, however, limited understanding of how locomotion and visual signals combine in SC. Visual responses in SCs are strong, but can increase or decrease during locomotion <sup><xref ref-type="bibr" rid="R16">16</xref>–<xref ref-type="bibr" rid="R20">20</xref></sup>. Visual responses in SCim are weaker <sup><xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R21">21</xref>–<xref ref-type="bibr" rid="R25">25</xref></sup>, and we know little about locomotion’s impact on SCim activity <sup><xref ref-type="bibr" rid="R17">17</xref>,<xref ref-type="bibr" rid="R18">18</xref></sup>. In addition, these measurements have rarely been made in contexts where animal locomotion is relevant to the visual stimulus. To establish how SC responds during self- and object movement, we therefore measured activity in SCs and SCim in mice immersed in a virtual reality environment.</p></sec><sec id="S2" sec-type="results"><title>Results</title><p id="P7">Mice were head-restrained over a treadmill, which we configured to control movement in a virtual reality (VR) environment. Mice could move along a 100 cm long platform in VR by moving on the treadmill (<xref ref-type="fig" rid="F1">Figs 1A,B</xref>). We used 32-channel multielectrode arrays to measure spiking activity from populations of single-units in SCs (30 sessions in 7 mice) and SCim (35 sessions in 9 mice) (<xref ref-type="fig" rid="F1">Fig 1C</xref>).</p><sec id="S3"><title>Neurons in SCs and SCim respond to looming visual objects</title><p id="P8">We first measured response to an object that loomed at the animal. On each trial, a dark round object (8 cm diameter) appeared at the distant end of the platform and then moved towards the animal at a speed of 100 cm/s. We analysed responses where activity was consistent across trials (see <xref ref-type="sec" rid="S12">Methods</xref>), yielding 672 units in SCs, and 583 in SCim (<xref ref-type="fig" rid="F1">Fig 1D</xref>). We found that the looming object increased activity in most SCs units. Responses in SCim were more diverse, such that the looming object increased activity in some SCim units, but decreased activity in others. Consequently, activity was on average higher in SCs (mean z–score 0.85, s.d. 1.03) than in SCim (0.07, s.d. 0.56; <italic>p</italic> &lt;&lt; 0.001, Student’s t-test).</p><p id="P9">Vigorous response to object loom in SCs is consistent with previous measurements that show highly responsive, spatially-localised visual receptive fields in most SCs neurons (e.g. <sup><xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R26">26</xref>–<xref ref-type="bibr" rid="R29">29</xref></sup>). Conversely, fewer neurons in SCim show measurable visual receptive fields <sup><xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R21">21</xref>,<xref ref-type="bibr" rid="R23">23</xref></sup>, potentially explaining reduced response to looming objects in SCim. To understand the relationship between loom response and receptive fields, we determined receptive field maps by presenting flashed black and white squares at different positions in the visual field, and fit the maps for each unit with two-dimensional Gaussians. In these recordings we could extract receptive fields for at least one luminance polarity in most SCs units (607/711, 85%; see <xref ref-type="sec" rid="S12">Methods</xref>), and fewer SCim units (230/830, 28%) (examples in <xref ref-type="fig" rid="F1">Fig 1E</xref>). Most of these units responded to the black squares, and we found that receptive field size (s.d. of the Gaussian) for black squares was smaller in SCs (mean 7.3 degrees, s.d. 3.9, n = 569) than SCim (mean 14.4, s.d. 6.2, n = 219; <italic>p</italic> &lt; 0.001, Student’s t-test).</p><p id="P10">Visual neurons should respond when the object’s edge passes through their receptive field, and receptive field location should therefore predict the relative timing of neural responses to the loom stimulus. We tested this prediction in units with a receptive field map, which also increased activity in response to the looming object. We averaged loom response among units with receptive fields at similar positions along the nasal-temporal axis, and found the time at which loom response peaked (<xref ref-type="fig" rid="F1">Fig 1E</xref>). Since objects looming at 100 cm/s expanded very rapidly, there was little difference in time to response peak across the visual field. We therefore also presented objects that approached at slower speed (25 cm/s; see also <xref ref-type="supplementary-material" rid="SD1">Suppl. Fig 1</xref>). At slower speeds, we found that loom response peaked earlier among neurons with more nasal receptive fields, in both SCs and SCim. The temporal pattern of responses to a looming object is therefore consistent with that expected from neurons’ receptive field locations.</p></sec><sec id="S4"><title>Self-movement modulates response to looming visual objects</title><p id="P11">Locomotion influenced the response of SC neurons to object movement. Animals moved sporadically during presentation of object looms, and so we defined each trial as either ‘running’ (locomotion speed at least 2 cm/s) or ‘stationary’ (less than 2 cm/s) and calculated average activity in each condition, over the 0.2 s preceding object collision. We found that locomotion could either increase or decrease activity in SCs neurons, with no net effect (<xref ref-type="fig" rid="F1">Fig 1F</xref>; mean difference in z-scores of -0.05, s.d. 0.80; <italic>p</italic> = 0.141, paired Student’s t-test). In contrast, locomotion consistently increased activity in SCim, by a mean 0.23 (s.d. 0.72; <italic>p</italic> &lt; 0.001; SCs vs. SCim: <italic>p</italic> &lt; 0.001, Student’s t-test). Our measurements therefore reveal that individual neurons throughout SC process both visual and locomotion signals: visual response is strongest in SCs, but locomotion more consistently increases SCim response.</p></sec><sec id="S5"><title>Vision and self-motion signals also converge during object approach</title><p id="P12">Visual looming also arises when an animal approaches an object. To characterise response to looms caused by self-movement we made a simple change to the virtual reality (VR) environment. On each trial the same dark object still appeared at the distant end of the platform, but now it stayed there. As animals moved along the platform, their self-movement brought them closer to the object; the upshot was that the object again loomed in the visual field, but now that looming was caused by animal self-movement. Time to reach the object varied between trials, so here we analysed neuronal activity as a function of distance between animal and object.</p><p id="P13">We found a consistent pattern of response in SCs as animals approached the object in VR: population neuronal activity increased to a peak, and then subsided (<xref ref-type="fig" rid="F2">Figs 2A,B</xref>). Object approach produced a different pattern of responses in SCim. Some neurons showed a distinct peak in activity as the animal neared the object. Other neurons showed distinct reduction in activity at similar positions. In most SCim neurons, however, we saw a ramp in activity (either increase or decrease) as animals moved along the platform. Activity patterns in these neurons usually showed a peak or trough as the animal neared the object. Overall, we saw a consistent pattern of response in SCs and a diversity of responses in SCim.</p><p id="P14">Locomotion speed influenced activity during object approach, particularly in SCim. SCim activity depended on approach speed throughout the trial, even early in the trial when the object was still distant (examples in <xref ref-type="fig" rid="F2">Fig 2B</xref>). SCs activity, by contrast, was similar during both slower- and faster approaches. We used ridge regression to establish the relative contribution of visual and self-movement signals to SC activity during object approach. With these regressions we estimated the amount of variance in neural activity that could be explained by the distance between animal and object (equivalently, the location of object edges in the visual field), or by locomotion speed (see <xref ref-type="sec" rid="S12">Methods</xref>). To assess the relative contribution of each factor, we normalized the cross-validated explained variance attained when using either locomotion speed or distance, to that attained when using both. Note that these normalized values can be greater than one due to the cross-validation. We found a variety of responses across the population, such that some neurons were more dependent on distance to object, while others were more dependent on locomotion speed (<xref ref-type="fig" rid="F2">Fig 2C</xref>). Neurons in SCs showed stronger dependence on distance (median normalized explained variance = 0.706) compared to approach speed (0.256; <italic>p</italic> &lt;&lt; 0.001, n = 627, Wilcoxon Rank Sum Test). Neurons in SCim instead showed stronger dependence on approach speed (median = 0.888) compared to distance (0.402; <italic>p</italic> &lt;&lt; 0.001, n = 468).</p></sec><sec id="S6"><title>Vision drives instinctive locomotion behaviours</title><p id="P15">Mice were free to choose how they moved along the virtual platform. Locomotion behaviour was, however, dependent on vision and stereotypical: mice consistently slowed down as they approached the object (<xref ref-type="fig" rid="F3">Fig 3A</xref>). This slow-down behaviour could be observed on the first day of exposure to an object, and then became more pronounced as animals gained experience and ran faster down the platform (<xref ref-type="fig" rid="F3">Fig 3C</xref>). Slow-down behaviour was not limited to black objects - in separate experiments we found that animals showed the slow-down behaviour when they instead encountered a white object (not shown), and continued to show slow-down behaviour when the white object was subsequently replaced with a black one.</p><p id="P16">How does locomotion behaviour differ when visual stimuli match or conflict with that predicted by self-movement? To address this, we stored the sequence of visual images generated as an animal approached the object, and replayed those sequences later in the session. This allowed us to compare two conditions: locomotion behaviour during VR, where the visual stimulus matched that predicted by self-movement, and during replay, where it did not. We found that behaviour changed during the replay condition, such that animals often ran slower or even ceased moving during one or both replay epochs (<xref ref-type="fig" rid="F3">Figs 3E,F</xref>, Wilcoxon signed-rank test; <italic>p</italic> &lt; 0.02 for all conditions relative to the previous condition, n=48<bold>;</bold> see also <xref ref-type="supplementary-material" rid="SD1">Suppl. Figs S2,S3</xref>). When animals ran during the replay condition, their patterns of movement resembled that in the VR condition - animals slowed down when the object was close (<xref ref-type="fig" rid="F3">Figs 3B,D</xref>). Similarly, presentation of independently looming objects (as in <xref ref-type="fig" rid="F1">Fig 1</xref>) could evoke slow-down behaviour at each of the loom speeds we tested (<xref ref-type="supplementary-material" rid="SD1">Suppl. Fig S4</xref>).</p><p id="P17">The similar slow-downs observed for VR and replay conditions, and looming objects, suggests that slow-downs are an instinctive sensorimotor response to a looming visual stimulus (at least in head-restrained animals), whether that looming arises from self-movement or object movement. The changes in locomotion behaviour between VR and replay conditions, however, suggests that mice become aware - at some level - of whether the visual experience is a result of their own movement or not.</p></sec><sec id="S7"><title>SC activity is sensitive to match between vision and self-movement</title><p id="P18">We hypothesised that patterns of neural activity in SC, particularly SCim, depend on whether the visual stimulus is matched to self-movement. Because the temporal pattern of visual stimulation when presenting the looming object as in <xref ref-type="fig" rid="F1">Fig 1</xref> (where object movement was smooth) could be very different to that during object approach (where object movement depended on animal speed), we tested the hypothesis by analysing behaviour and neural responses during the VR and replay conditions described above. Consistent with the overall impact of locomotion shown in <xref ref-type="fig" rid="F2">Fig 2</xref>, SCim activity often reduced dramatically when the animal simply stopped moving during replay of the visual stimulus (<xref ref-type="supplementary-material" rid="SD1">Suppl. Fig S5</xref>). To provide a stricter test of the hypothesis, we restricted the analyses to sessions in which animals showed a similar range of locomotion in the VR and replay conditions (<xref ref-type="supplementary-material" rid="SD1">Suppl. Fig S2</xref>), and only when the animal was within 50 cm of the object. We divided each trial into non-overlapping 0.2 s epochs, and performed logistic regression to classify each measurement epoch as arising from VR or replay conditions, using behavioural and neural variables (<xref ref-type="fig" rid="F4">Fig 4A</xref>, see <xref ref-type="sec" rid="S12">Methods</xref>).</p><p id="P19">We first measured classification performance when the regression model was blind to neural activity. Even in sessions where overall locomotion was similar, logistic regression was able to predict if the animal was in a VR or replay condition from combinations of behavioural variables, including locomotion speed, distance to the object, and time in the session (Wilcoxon signed-rank test; covariates: <italic>p</italic> &lt; 0.001, n = 12 SCs sessions; <italic>p</italic> &lt; 0.001, n = 15 SCim sessions; <xref ref-type="fig" rid="F4">Figs 4B,C</xref>). Classification with locomotion speed alone was sometimes sufficient (speed: SCs sessions, <italic>p</italic> = 0.034; SCim sessions, <italic>p</italic> = 0.277). Time into the experiment also had some predictive power (time: SCs sessions, <italic>p</italic> &lt; 0.001; SCim sessions, <italic>p</italic> = 0.035), but this may be an indirect effect of locomotion speed, as animals usually ran faster earlier in the session. Above chance classification persisted in many sessions even after ensuring average time into the experiment and locomotion speed ranges were matched between VR and replay conditions (covariates cor.: SCs sessions, <italic>p</italic> = 0.009; SCim sessions, <italic>p</italic> = 0.389). We tested the classifier after shuffling the trials between VR and replay conditions, and found that performance dropped to chance levels for all variables tested (covariates cor.: SCs sessions, <italic>p</italic> = 0.204; SCim sessions, <italic>p</italic> = 0.470). The distributions of locomotion speed in the two conditions remained different even after speed-matching (<xref ref-type="fig" rid="F4">Fig 4D</xref>), so in the following we compare classification performance when including neural activity, to that obtained from the covariates alone.</p><p id="P20">We tested whether inclusion of neural activity during these measurement epochs would improve model performance (<xref ref-type="fig" rid="F4">Figs 4E,F</xref>). We recorded spiking activity of populations of neurons alongside behaviour, and calculated the mean firing rate in the same epochs. We trained the classifier on subsets of the data for which measurement time and locomotion speed ranges were similar across VR and replay conditions. We found that neural activity in SCim improved classifier performance (mean ± s.d.: 56.1 ± 23.0 % increase, <italic>p</italic> &lt; 0.001, n = 15 sessions; <xref ref-type="fig" rid="F4">Fig 4F</xref>). Neural activity in SCs also improved classifier performance (28.8 ± 27.7% increase, <italic>p</italic> &lt; 0.001; n = 12 sessions; <xref ref-type="fig" rid="F4">Fig 4E</xref>), but to a lesser extent (SCim vs. SCs; <italic>p</italic> = 0.005, Mann-Whitney U test: 32.0). These improvements in SCim and SCs were not dependent on the range of distance-to-object included in the analyses (<xref ref-type="supplementary-material" rid="SD1">Suppl. Fig S6</xref>). Thus, neural activity in SC, particularly SCim, depends on whether the visual stimulus is matched to that expected from the animal’s locomotion.</p></sec></sec><sec id="S8" sec-type="discussion"><title>Discussion</title><p id="P21">We found that neuronal activity in SCs was dominated by vision, but that vision and self-movement signals contribute to neuronal activity in SCim, for both looming objects and during object approach. Animals instinctively slowed-down when an object was near, and further altered their behaviour when visual experience did not match that expected from their locomotion. This difference between visual experience and expectation from locomotion behaviour was also encoded by population neural activity in SC, particularly in SCim.</p><p id="P22">Mice often ceased moving during replay of the sequence of images they had previously generated while approaching an object. The behaviour likely reflects their instinctive ability to predict and account for the visual stimulation brought about by their self-movement. Virtual reality allowed us to generate a mismatch between predicted and actual visual stimulus, and mice were sensitive to this discrepancy. Neural activity in SC was also sensitive to this mismatch. Although our analyses revealed behavioural differences between VR and replay conditions, SC activity could still distinguish between the two conditions when behavioural differences were accounted for. We infer that both animals and neural activity in SC, particularly SCim, are able to tell the difference between visual stimuli produced by object- and self-movement, even when they give rise to identical patterns of visual stimulation. Previous work has described subpopulations of neurons in primary visual cortex that are sensitive to brief disruptions (mismatch events) between predicted and actual visual stimulus (<sup><xref ref-type="bibr" rid="R30">30</xref></sup>; but see <sup><xref ref-type="bibr" rid="R31">31</xref></sup>). Similarly, response of neurons in visual cortical areas POR and LI (but not V1) to a small moving spot depends on whether the spot’s movement is coupled to animal locomotion <sup><xref ref-type="bibr" rid="R16">16</xref></sup>. These latter cortical responses appear to depend on a subpopulation of SCs neurons (‘widefield cells’), and the pooled calcium signal of populations of widefield SCs neurons is increased when a small spot’s movement is uncoupled (either slower or faster) from animal movement <sup><xref ref-type="bibr" rid="R16">16</xref></sup>. Widefield cells, which project from SCs to the pulvinar (lateral posterior nucleus), show weak responses to large visual stimuli like the looming object we presented <sup><xref ref-type="bibr" rid="R28">28</xref>,<xref ref-type="bibr" rid="R32">32</xref></sup>, but we cannot rule out a contribution from widefield cells to the observed differences in VR and replay conditions.</p><p id="P23">Where comparison can be made our neurophysiological measurements appear consistent with previous work. Receptive fields were large in SCim units where we could recover them, as in previous work <sup><xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R22">22</xref>–<xref ref-type="bibr" rid="R25">25</xref></sup>. SCim receptive fields were larger than those of most SCs neurons, but similar in size to widefield and ‘horizontal’ SCs cell classes <sup><xref ref-type="bibr" rid="R26">26</xref>,<xref ref-type="bibr" rid="R28">28</xref>,<xref ref-type="bibr" rid="R32">32</xref></sup>. We also found weaker responses to visual looms in SCim than SCs. Response to visual looms is often measured after centering the stimulus on a neuron’s receptive field (e.g. <sup><xref ref-type="bibr" rid="R23">23</xref>,<xref ref-type="bibr" rid="R33">33</xref>,<xref ref-type="bibr" rid="R34">34</xref></sup>), but here objects always loomed from in front of the animal, and the edge of the stimulus therefore always moved across the visual field in the nasal-temporal direction. Direction-selectivity is a common feature of receptive fields in SCs <sup><xref ref-type="bibr" rid="R35">35</xref></sup>, but can also be found in deeper SC <sup><xref ref-type="bibr" rid="R21">21</xref></sup>, so we cannot rule out the possibility that neurons in SCim (or SCs) would have been more responsive if we had tested looms centred at other locations.</p><p id="P24">Finally, the substantial impact of locomotion on activity in SCim seems consistent with very limited previous work (<sup><xref ref-type="bibr" rid="R18">18</xref></sup>; but see <sup><xref ref-type="bibr" rid="R17">17</xref></sup>). We also found that locomotion had limited impact on net populational activity in SCs, consistent with previous measurements <sup><xref ref-type="bibr" rid="R17">17</xref>–<xref ref-type="bibr" rid="R20">20</xref></sup>, though we note that specific subpopulations in SCs may show more consistent facilitation <sup><xref ref-type="bibr" rid="R36">36</xref></sup>.</p><p id="P25">The relatively strong dependence of SCim activity on locomotion and related features is consistent with the differential inputs to SC layers. SCs receives strong visual input from retina <sup><xref ref-type="bibr" rid="R8">8</xref></sup>, primary visual cortex and some secondary visual areas (LI, LM, P, POR; <sup><xref ref-type="bibr" rid="R9">9</xref></sup>). By contrast, SCim receives weak if any retinal input and cortical visual inputs are mainly derived from other secondary visual areas (A, AM, PM, RL; <sup><xref ref-type="bibr" rid="R9">9</xref></sup>) that may form a ‘dorsal’ stream through mouse visual cortex. SCim also receives multi-modal sensory input from auditory and somatosensory pathways (e.g. <sup><xref ref-type="bibr" rid="R22">22</xref>,<xref ref-type="bibr" rid="R37">37</xref></sup>), as well as multiple non-sensory signals from subcortical and cortical areas (see <sup><xref ref-type="bibr" rid="R10">10</xref>,<xref ref-type="bibr" rid="R11">11</xref></sup>) including inputs from retrosplenial cortex, prefrontal cortex and striatum among others (e.g. <sup><xref ref-type="bibr" rid="R38">38</xref>–<xref ref-type="bibr" rid="R40">40</xref></sup>). These inputs to the SCim make it an ideal site for establishing the VR context, and broadcasting those signals.</p><p id="P26">In freely-moving mice, overhead looming objects usually induce escape when a refuge is present, and freezing behaviour when a refuge is absent <sup><xref ref-type="bibr" rid="R41">41</xref>–<xref ref-type="bibr" rid="R43">43</xref></sup>. Looming objects in the frontal visual field can also induce escape to refuge, though usually after initial freezing behaviour <sup><xref ref-type="bibr" rid="R44">44</xref></sup>. Our behavioural measurements show that head-restrained mice slow-down when presented with looming objects in the frontal visual field, whether or not that visual loom is matched to their locomotion. Other visual behaviours that are readily elicited in untrained, head-restrained animals include ‘vidgetting’ and locomotion changes produced by novel grating patterns <sup><xref ref-type="bibr" rid="R45">45</xref>,<xref ref-type="bibr" rid="R46">46</xref></sup>; ‘locomotion arrest’ produced by flashed lights (<sup><xref ref-type="bibr" rid="R47">47</xref></sup>; see also <sup><xref ref-type="bibr" rid="R48">48</xref></sup>); and ‘burrow ingress’ produced by overhead looms <sup><xref ref-type="bibr" rid="R49">49</xref></sup>. Slow-down behaviour adds to this repertoire of instinctive behaviours in head-restrained animals; whether there is a relationship between them needs to be established.</p><p id="P27">Animals explored the virtual environment at moderate locomotion speeds of about 20 cm/s. The instinctive slow-downs we observed when animals were near an object may be adaptive, providing both time and motor state for a greater range of subsequent behaviours. Objects and agents (conspecifics, or potential threat or prey) approaching at slow speed will evoke a wave of activity across SC during this period (<xref ref-type="fig" rid="F1">Fig 1E</xref>), analysis of which could allow animals to choose among appropriate approach or avoidance behaviours (cf. <sup><xref ref-type="bibr" rid="R38">38</xref></sup>). Striking predators will likely move more quickly (e.g. <sup><xref ref-type="bibr" rid="R50">50</xref></sup>), and a rapidly approaching immediate threat will near simultaneously activate a large fraction of visually responsive neurons in both SCs and SCim (<xref ref-type="fig" rid="F1">Fig 1D</xref>). The different spatiotemporal dynamics of population activity during slower object approach and fast object loom may be sufficient to allow rapid avoidance behaviours in presence of immediate threat, while reducing the likelihood of false-alarms.</p><p id="P28">Our measurements leveraged immersive virtual reality to afford tight experimental control, while retaining expression of instinctive behaviours. Our findings demonstrate that both animals and neurons in SC can distinguish whether current visual experience matches that expected from the animal’s actions. The convergence of visual and locomotion signals onto SC, and the fact that SC is well placed to instruct behaviour, indicate that contextual variation in SC activity could be important in allowing animals to shape their behaviour.</p></sec><sec id="S9"><title>Resource availability</title><sec id="S10"><title>Lead contact</title><p id="P29">Further information and requests for resources and reagents should be directed to and will be fulfilled by the Lead Contact, Samuel Solomon (<email>s.solomon@ucl.ac.uk</email>).</p></sec><sec id="S11"><title>Materials availability</title><p id="P30">This study did not generate new unique reagents. Commercially available reagents are indicated in the ‘Key resources table’.</p></sec></sec><sec id="S12" sec-type="methods"><title>Methods</title><sec id="S13"><title>Experimental model and subject details</title><p id="P31">All experiments were performed in accordance with the Animals (Scientific Procedures) Act 1986 (United Kingdom) and Home Office (United Kingdom) approved project and personal licences. Mice (n = 16 C57BL6/J male wild-type, age 12-16 weeks) were housed in groups of maximum five under a 12-hour light/dark cycle, with free access to food and water. All electrophysiological recordings were carried out during the dark phase of the cycle.</p></sec><sec id="S14"><title>Method details</title><sec id="S15"><title>Surgery and recording</title><p id="P32">Mice were anaesthetised with isoflurane and the skull exposed under aseptic conditions. A custom-built stainless-steel metal plate was attached to the skull with dental cement and a metal screw was implanted over the somatosensory cortex on the right hemisphere, for future use as a reference electrode. The skull above SC was left accessible. Animals were allowed to recover from surgery for at least seven days; analgesia was provided for at least the first three days. Mice were then habituated to the experimental apparatus, as described in Visual Stimulus section below. Following habituation, mice were briefly re-anesthetized with isoflurane and a craniotomy was performed over the SC on the right hemisphere, centred at 0.75 mm lateral to sagittal midline, and at lambda), and the dura was left intact. The craniotomy was sealed with silicon elastomer (KwikCast, World Precision Instruments).</p><p id="P33">Mice were allowed to recover for at least 4 hours before the first recording session. Multiple, daily recordings (3 - 6 sessions, one session per day) were then made in each animal. In each session, the animal was head-restrained in the apparatus, the craniotomy was exposed and a silicon probe, comprising two shanks each with 16 electrodes (spacing 250 µm between shanks, 40 µm between sites; ASSY-37 E-1, Cambridge Neurotech Ltd, Cambridge, UK), was implanted using a vertical micromanipulator (Sensapex, Finland). Electrophysiological signals were acquired using an OpenEphys acquisition board <sup><xref ref-type="bibr" rid="R51">51</xref></sup> at a rate of 30 kHz. The electrodes were lowered rapidly to a depth of approximately 0.5 mm below the brain surface, and then lowered slowly while displaying a large flickering (2 Hz) checkerboard across the visual field. The depth of the dorsal surface of SC was identified by the appearance of robust neural activity modulated at the stimulus frequency; the electrodes were then lowered further until the tip was 500 µm below SC surface (for recordings from SCs) or was 800 µm below (for recordings from SCim). At the end of the recording session the electrodes were retracted and the craniotomy was resealed as above. To confirm electrode targeting the electrode was immersed into a DiI solution before the last recording session. Following that session, mice were deeply anaesthetized and perfused transcardially, and brains were extracted for histological analysis.</p></sec><sec id="S16"><title>Visual Stimulus</title><p id="P34">We used an experimental apparatus described previously <sup><xref ref-type="bibr" rid="R31">31</xref></sup>. Mice were head-fixed above a polystyrene wheel (radius 10 cm), such that their head was in the centre of a truncated spherical dome. Visual stimuli were produced by shining the light of a projector (Casio Green Slim XJ-A257-UJ DLP; 60 Hz refresh rate), onto the internal surface of the dome via an hemispherical mirror; the projected image spanned 240° azimuth (from -120° to 120°) and 120° elevation (from -30° and 90°) with mean luminance ∼10 cd.m<sup>2</sup>. Mesh-mapped and gamma-calibrated visual stimuli were produced by the package BonVision <sup><xref ref-type="bibr" rid="R52">52</xref></sup>, in the Bonsai framework <sup><xref ref-type="bibr" rid="R53">53</xref></sup>. Movement of the polystyrene wheel was sensed with a rotary encoder (2400 pulses/rotation, Kübler, Germany), the output of which was copied to both the OpenEphys acquisition board and to an Arduino connected to the stimulus computer, so as to allow locomotion-dependent updating of the visual scene where required. A synchronising signal was sent to both the OpenEphys board and the stimulus computer. The OpenEphys board also acquired the signals of a photodiode (PDA25K2, Thorlabs Inc., USA) that detected light in a small region of the dome hidden from the animal, and provided additional confirmation of stimulus timing.</p><p id="P35">We presented three classes of stimulus in the virtual environment: 1) platform: a static virtual platform formed by a smoothed random visual texture (8 cm wide x 100 cm long, rendered 2 cm below the animal’s eye); 2) static object: a round object (8 cm diameter) sitting on the distal end of the platform; 3) looming object: the same round object sitting on the distal end of the platform, but which then moved along the platform towards the animal at one of 12.5, 25, 50 or 100 cm/s, and disappeared after it had collided with the animal. For looms of 100 cm/s, the object appeared on the platform and remained stationary for 1s before moving towards the animal.</p><p id="P36">Animals were habituated to the virtual environment for 8-13 days (one session per day) before recordings started. On each habituation day the animal was placed in the virtual reality apparatus for up to 30 mins, during which it moved along the virtual platform by running on the treadmill. On each of the last 4 habituation days, the static object was presented at the end of the virtual platform on a subset of randomly interleaved trials. Animals (n = 16) experienced an average 55±30 (mean ± s.d.) trials of the platform, and 52±23 trials with the static object, on each of these days. Each trial was separated by 2s of grey screen. Subsequent recording sessions started with 5 consecutive platform trials, then 40 trials of the static object that were randomly interleaved with 20 trials of a looming object moving at 100 cm/s. In the virtual reality (‘VR’, or ‘closed-loop’) condition, the distance travelled on the polystyrene wheel was used by the stimulus generator to control the position of the animal on the virtual platform. In the ‘replay’ (or ‘open-loop’) condition, the movies produced during the VR condition were replayed to the animal, independent of the animal’s movement on the treadmill. In 2/16 animals, the 40 VR trials and 20 looming object trials were presented in a single block, and these trials then replayed in a single block. In 14/16 animals, we split the 40 trials so that animals experienced 20 trials in VR then 20 trials in replay condition, and then repeated this process. Subsequently, we presented randomly interleaved trials of a looming object moving at different speeds (30 trials/speed); each trial was preceded by 5s of grey screen. To map receptive fields we then obtained responses to flashed black or white squares (15° wide). On each of 3000 0.1s trials (no interstimulus interval), we presented black or white squares at each of 5 random locations drawn randomly from an 8x8 grid centred in the left hemifield. The stimuli covered -15:105° azimuth, and -30:90° elevation, where 0° is directly ahead of the animal and at eye height.</p></sec><sec id="S17"><title>Spike sorting and clustering</title><p id="P37">Electrophysiological signals from all recordings in a session were concatenated and processed using Kilosort 2 and Phy <sup><xref ref-type="bibr" rid="R54">54</xref>,<xref ref-type="bibr" rid="R55">55</xref></sup>. We kept for further analysis those units in which minimum inter-spike interval was greater than 1 ms, yielding a total of 828 units in SCs (28±10 per session, mean±s.d.; 30 sessions in 7 animals), and 1024 in SCim (29±8 per session; 35 sessions in 9 animals). Subsequent analyses were performed in MATLAB R2021b or 2022a. Neural firing rate was resampled to the refresh-rate of the visual environment (60 Hz). Except for receptive field analyses, these binned rates were transformed into z-scores by normalising to the mean and standard deviation of firing rate across all stimulus conditions. To assess running behaviour, locomotion speed was also resampled at 60Hz and animals were defined as ‘stationary’ if locomotion speed was less than 2 cm/s, and as ‘running’ otherwise.</p></sec><sec id="S18"><title>Receptive field analysis</title><p id="P38">Units were included for further analysis if they produced at least 100 spikes during the stimulus sequence (yielding 711 units in SCs, and 830 in SCim). We analysed responses to black and white stimuli separately. For each unit, a linear regression (function <italic>fitlm</italic> in Matlab) was conducted between a vector representation of the stimulus contrast at each location, and mean firing rate over the frame. The five parameters of a circular two-dimensional Gaussian (x- and y-location, standard deviation, amplitude, and offset) that best predicted the spatial pattern of regression weights were found using constrained least-squares optimisation (function <italic>lsqcurvefit</italic> in Matlab). Size (standard deviation) of the receptive field was constrained to be at least 3.75°. Fits were included for further analysis if the predicted location of the receptive field centre was at least 7.5° from the edge of the stimulus grid, and the fits predicted at least 33% of the variance in the regression weights. Of the 711 SCs units, 569 yielded acceptable fits for black stimuli and 351 for white, of which 38 were acceptable only for white; of the 830 SCim units, 219 yielded acceptable fits for black stimuli and 46 for white, of which 11 were acceptable only for white. Receptive fields could be defined for black squares in most of these units. Since the looming object was also black, we used responses to black squares for further analysis. Note we will overestimate receptive field size, particularly in SCs, because receptive field size was poorly constrained when units responded to only one location in the stimulus grid. The model returned receptive field size less than 5 degrees in 240 SCs units, and 10 SCim units.</p></sec><sec id="S19"><title>Response during object loom and object approach</title><p id="P39">For analyses of responses to object looms in <xref ref-type="fig" rid="F1">Fig 1</xref>, binned and z-scored firing rates on each trial were aligned to the time at which the object collided with the animal in the virtual environment. Units were included for further analysis if they met the criteria for response consistency described below.</p><p id="P40">Response amplitude was calculated as the mean z-score in the 0.2 s (for 100 cm/s objects) or 0.8s (for 25 cm/s) preceding collision. For analyses of responses during object approach in <xref ref-type="fig" rid="F2">Fig 2</xref>, the binned and z-scored firing rate was first smoothed with a Gaussian filter (0.3 s). We then discretized the distance between the animal and object into non-overlapping bins of 1cm, and found the time-bins in each trial where the animal was at each of those distances. We then calculated time- and trial-averaged neural activity, and locomotion speed, at each of the 100 distances-to-object. Units were included for further analysis if they met the criteria for response consistency as below.</p></sec><sec id="S20"><title>Relative contribution of speed and distance-to-object during object approach</title><p id="P41">To evaluate the relative contribution of locomotion speed and distance-to-object in explaining SC responses in <xref ref-type="fig" rid="F2">Fig 2C</xref>, we used methods previously applied to responses in mouse primary visual cortex <sup><xref ref-type="bibr" rid="R56">56</xref></sup>. Distance-to-object was discretised into 110 bins; locomotion speed into 20 bins. We fit a linear model to get weights for each bin of the predictor variables, i.e. <italic>y</italic><sub><italic>n</italic></sub> = <italic>w</italic><sub><italic>n</italic></sub><italic>X</italic>, where <italic>y</italic><sub><italic>n</italic></sub> is the response of the <italic>n</italic><sup><italic>th</italic></sup> neuron over time, <italic>X</italic> is the matrix of predictor variable bins, and <italic>w</italic><sub><italic>n</italic></sub> is an array of weights acting on the predictor variable bins to capture the response of the <italic>n</italic><sup><italic>th</italic></sup> neuron. We used linear ridge regression to calculate the weights based on the equation: <italic>w′</italic><sub><italic>n</italic></sub> = (<italic>X</italic><sup><italic>T</italic></sup><italic>X</italic> + <italic>λI</italic>)<sup>−1</sup><italic>X</italic><sup><italic>T</italic></sup><italic>y</italic><sub><italic>n</italic></sub>, where <italic>λ</italic> was the ridge parameter tested (one of 0.001, 0.1, 1, 5, 10). We chose <italic>λ</italic> to minimise cross-validated explained variance. To calculate cross-validated explained variance, we first calculated the weights using the training data (80% of data) and then used the following equation to calculate performance in the test data: <disp-formula id="FD1"><mml:math id="M1"><mml:mrow><mml:mspace width="0.2em"/><mml:mi>P</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.2em"/><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mtext>Σ</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi><mml:mo>′</mml:mo></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mi>X</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mtext>Σ</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
where <italic>μ</italic><sub><italic>train</italic></sub> is the mean of the response in the training period. Performance of each of three models (locomotion speed only, distance-to-object only, or both) was calculated on the best <italic>λ</italic> of each of the models independently.</p></sec><sec id="S21"><title>Response consistency</title><p id="P42">For each of 30 iterations we split trials randomly into two equally-sized sets and binned activity as a function of time to collision (object loom) or distance-to-object (object approach). We averaged activity in each bin over trials and calculated Pearson’s correlation coefficient between the two resultant vectors. We then took one of these sets and randomly shuffled the time- or distance-bins on each trial before creating the average vector, and then calculated the Pearson’s correlation with the other unshuffled, trial-averaged vector as above. The procedure was also repeated 30 times and yielded 30 estimates of raw correlations and 900 estimates of shuffle correlations. Units in which the mean raw correlation exceeded 95% of shuffle correlations were considered consistent.</p></sec><sec id="S22"><title>Logistic regression</title><p id="P43">To classify whether epochs of 0.2 s belonged to a VR or replay condition in <xref ref-type="fig" rid="F4">Fig 4</xref>, we performed logistic regression based on purely behavioural covariates or when also including neural population activity in SCim or SCs. We applied control analyses and exclusion criteria to prevent inflated classification performance that could result from temporal structure of the trials or behavioural differences between environments.</p></sec><sec id="S23"><title>Inclusion criteria and preprocessing</title><p id="P44">We included sessions where we ran two repetitions of VR and replay conditions. Running speed strongly influenced neural activity (<xref ref-type="fig" rid="F1">Figs 1</xref>&amp;<xref ref-type="fig" rid="F2">2</xref>), and running speed varied between VR and replay conditions in many sessions (<xref ref-type="fig" rid="F3">Fig. 3</xref>). We therefore excluded 5 SCs and 16 SCim sessions during which the animal had very different running profiles or paused for long periods of time during the first replay condition compared to VR conditions (<xref ref-type="supplementary-material" rid="SD1">Suppl. Figure S3</xref>). Missing values in running speed data were imputed using a monotonic cubic interpolation method <sup><xref ref-type="bibr" rid="R57">57</xref></sup>. We applied a moving average filter over 81 time steps to correct for slight jitter in wheel movements, followed by a median filter with a kernel of 51 time steps. Neural activity from units that contained missing values were excluded from further analyses.</p><sec id="S24"><title>Model details</title><p id="P45">We divided data into non-overlapping 0.2 s epochs, for each of which we calculated different features (e.g., locomotion speed, z-scored neural activity, etc.) as predictors. We classified these epochs as belonging to VR or replay conditions using logistic regression with sparsity-inducing L1-regularisation (C=1.0). For each session, we trained a separate logistic regression model. We split the data into four train and test splits and report accuracy values averaged over the different data splits (mean and s.d.). We balanced the number of epochs across conditions. We introduced a buffer period between training and test epochs to account for autocorrelations in the neural data and prevent information leakage between training and test sets. Thus, 40 consecutive training epochs were separated from 10 test epochs by a buffer phase of 5 seconds. In control analyses, we tested classifier performance by shuffling test labels between the VR and replay conditions.</p></sec><sec id="S25"><title>Feature selection</title><p id="P46">We first established classification performance for non-neural features (‘behavioural covariates’: locomotion speed, distance to object, time into the experiment), and combinations of these features. Because these covariates could successfully classify many epochs, we applied feature-specific corrections (‘covariates cor.’) to minimise their influence when evaluating the contribution of neural activity to classification accuracy. First, we restricted analyses to epochs where the animal was within 50 cm of the object. Second, we selected epochs to ensure a consistent range of locomotion speed across conditions. Third, to balance average time into the experiment, we only included data from the second half of the first VR condition, the first replay condition, and the first half of the second VR condition (<xref ref-type="supplementary-material" rid="SD1">Suppl. Fig S7</xref>). Note that this control will not be beneficial for more complex non-linear classifiers. Corrected covariates alone still yielded good classification performance in some sessions. We thus concatenated neural features with covariates and assessed the improvement in classification accuracy over that provided by the covariates alone. We performed min-max scaling for each feature (neural and covariate) separately to ensure fair feature weighting.</p></sec><sec id="S26"><title>Statistical Testing</title><p id="P47">To evaluate whether performance accuracy was above chance level for different predictors, we performed a one-sample Wilcoxon signed-rank test comparing the mean accuracy of all sessions to a chance level of 0.5 using the implementation of the scipy.stats package. To assess the difference in prediction performance between models incorporating neural activity and covariates versus covariates alone, we used a paired Wilcoxon signed-rank test. This test compared the mean classification accuracy of all sessions with and without neural activity as a predictor.</p></sec><sec id="S27"><title>Code</title><p id="P48">(Pre)processing, analysis, and classification code was written in Python using the Sklearn toolbox.</p></sec></sec></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Figures</label><media xlink:href="EMS201993-supplement-Supplementary_Figures.pdf" mimetype="application" mime-subtype="pdf" id="d59aAcFbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S28"><title>Acknowledgements</title><p>This work was supported by the Biotechnology and Biological Sciences Research Council (BBSRC; BB/R004765/1), by the UKRI Frontier Research Grant (EU underwrite; EP/Y024656/1) and by the Human Frontier Science Program (RGY0076/2018). This work was also supported by the German Research Foundation (DFG) through Germany’s Excellence Strategy (EXC-Number 2064/1, PN 390727645), the German Federal Ministry of Education and Research (Tübingen AI Center, FKZ: 01IS18039). AS is a member of the International Max Planck Research School for Intelligent Systems (IMPRS-IS). We thank Sarah Ruediger for comments on the manuscript.</p></ack><fn-group><fn fn-type="con" id="FN3"><p id="P49"><bold>Author contributions</bold></p><p id="P50">This work was conceptualized by S.Z, A.B.S. and S.G.S.; experiments were performed by S.Z.; formal analysis and visualization by S.Z., A.S. and S.G.S.; formal logistic regression analyses and visualisation by A.S., P.J.D. and J.H.M.; original draft writing by S.Z, A.S., A.B.S. and S.G.S.; review and feedback by all authors; supervision and funding acquisition by S.G.S, A.B.S and J.H.M.</p></fn><fn fn-type="conflict" id="FN4"><p id="P51"><bold>Declaration of interests</bold></p><p id="P52">The authors declare no competing interests.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Busse</surname><given-names>L</given-names></name><name><surname>Cardin</surname><given-names>JA</given-names></name><name><surname>Chiappe</surname><given-names>ME</given-names></name><name><surname>Halassa</surname><given-names>MM</given-names></name><name><surname>McGinley</surname><given-names>MJ</given-names></name><name><surname>Yamashita</surname><given-names>T</given-names></name><name><surname>Saleem</surname><given-names>AB</given-names></name></person-group><article-title>Sensation during Active Behaviors</article-title><source>J Neurosci</source><year>2017</year><volume>37</volume><fpage>10826</fpage><lpage>10834</lpage><pub-id pub-id-type="pmcid">PMC5678015</pub-id><pub-id pub-id-type="pmid">29118211</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1828-17.2017</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clancy</surname><given-names>KB</given-names></name><name><surname>Orsolic</surname><given-names>I</given-names></name><name><surname>Mrsic-Flogel</surname><given-names>TD</given-names></name></person-group><article-title>Locomotion-dependent remapping of distributed cortical networks</article-title><source>Nat Neurosci</source><year>2019</year><volume>22</volume><fpage>778</fpage><lpage>786</lpage><pub-id pub-id-type="pmcid">PMC6701985</pub-id><pub-id pub-id-type="pmid">30858604</pub-id><pub-id pub-id-type="doi">10.1038/s41593-019-0357-8</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Musall</surname><given-names>S</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Juavinett</surname><given-names>AL</given-names></name><name><surname>Gluf</surname><given-names>S</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name></person-group><article-title>Single-trial neural dynamics are dominated by richly varied movements</article-title><source>Nat Neurosci</source><year>2019</year><volume>22</volume><fpage>1677</fpage><lpage>1686</lpage><pub-id pub-id-type="pmcid">PMC6768091</pub-id><pub-id pub-id-type="pmid">31551604</pub-id><pub-id pub-id-type="doi">10.1038/s41593-019-0502-4</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niell</surname><given-names>CM</given-names></name><name><surname>Stryker</surname><given-names>MP</given-names></name></person-group><article-title>Modulation of visual responses by behavioral state in mouse visual cortex</article-title><source>Neuron</source><year>2010</year><volume>65</volume><fpage>472</fpage><lpage>479</lpage><pub-id pub-id-type="pmcid">PMC3184003</pub-id><pub-id pub-id-type="pmid">20188652</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2010.01.033</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Steinmetz</surname><given-names>N</given-names></name><name><surname>Reddy</surname><given-names>CB</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><article-title>Spontaneous behaviors drive multidimensional, brainwide activity</article-title><source>Science</source><year>2019</year><volume>364</volume><fpage>255</fpage><pub-id pub-id-type="pmcid">PMC6525101</pub-id><pub-id pub-id-type="pmid">31000656</pub-id><pub-id pub-id-type="doi">10.1126/science.aav7893</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dean</surname><given-names>P</given-names></name><name><surname>Redgrave</surname><given-names>P</given-names></name><name><surname>Westby</surname><given-names>GW</given-names></name></person-group><article-title>Event or emergency? Two response systems in the mammalian superior colliculus</article-title><source>Trends Neurosci</source><year>1989</year><volume>12</volume><fpage>137</fpage><lpage>147</lpage><pub-id pub-id-type="pmid">2470171</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wheatcroft</surname><given-names>T</given-names></name><name><surname>Saleem</surname><given-names>AB</given-names></name><name><surname>Solomon</surname><given-names>SG</given-names></name></person-group><article-title>Functional Organisation of the Mouse Superior Colliculus</article-title><source>Front Neural Circuits</source><year>2022</year><volume>16</volume><elocation-id>792959</elocation-id><pub-id pub-id-type="pmcid">PMC9118347</pub-id><pub-id pub-id-type="pmid">35601532</pub-id><pub-id pub-id-type="doi">10.3389/fncir.2022.792959</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ellis</surname><given-names>EM</given-names></name><name><surname>Gauvain</surname><given-names>G</given-names></name><name><surname>Sivyer</surname><given-names>B</given-names></name><name><surname>Murphy</surname><given-names>GJ</given-names></name></person-group><article-title>Shared and distinct retinal input to the mouse superior colliculus and dorsal lateral geniculate nucleus</article-title><source>J Neurophysiol</source><year>2016</year><volume>116</volume><fpage>602</fpage><lpage>610</lpage><pub-id pub-id-type="pmcid">PMC4982907</pub-id><pub-id pub-id-type="pmid">27169509</pub-id><pub-id pub-id-type="doi">10.1152/jn.00227.2016</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Burkhalter</surname><given-names>A</given-names></name></person-group><article-title>Stream-related preferences of inputs to the superior colliculus from areas of dorsal and ventral streams of mouse visual cortex</article-title><source>J Neurosci</source><year>2013</year><volume>33</volume><fpage>1696</fpage><lpage>1705</lpage><pub-id pub-id-type="pmcid">PMC3711538</pub-id><pub-id pub-id-type="pmid">23345242</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3067-12.2013</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benavidez</surname><given-names>NL</given-names></name><name><surname>Bienkowski</surname><given-names>MS</given-names></name><name><surname>Zhu</surname><given-names>M</given-names></name><name><surname>Garcia</surname><given-names>LH</given-names></name><name><surname>Fayzullina</surname><given-names>M</given-names></name><name><surname>Gao</surname><given-names>L</given-names></name><name><surname>Bowman</surname><given-names>I</given-names></name><name><surname>Gou</surname><given-names>L</given-names></name><name><surname>Khanjani</surname><given-names>N</given-names></name><name><surname>Cotter</surname><given-names>KR</given-names></name><etal/></person-group><article-title>Organization of the inputs and outputs of the mouse superior colliculus</article-title><source>Nat Commun</source><year>2021</year><volume>12</volume><elocation-id>4004</elocation-id><pub-id pub-id-type="pmcid">PMC8239028</pub-id><pub-id pub-id-type="pmid">34183678</pub-id><pub-id pub-id-type="doi">10.1038/s41467-021-24241-2</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doykos</surname><given-names>TK</given-names></name><name><surname>Gilmer</surname><given-names>JI</given-names></name><name><surname>Person</surname><given-names>AL</given-names></name><name><surname>Felsen</surname><given-names>G</given-names></name></person-group><article-title>Monosynaptic inputs to specific cell types of the intermediate and deep layers of the superior colliculus</article-title><source>J Comp Neurol</source><year>2020</year><volume>528</volume><fpage>2254</fpage><lpage>2268</lpage><pub-id pub-id-type="pmcid">PMC8032550</pub-id><pub-id pub-id-type="pmid">32080842</pub-id><pub-id pub-id-type="doi">10.1002/cne.24888</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Basso</surname><given-names>MA</given-names></name><name><surname>Bickford</surname><given-names>ME</given-names></name><name><surname>Cang</surname><given-names>J</given-names></name></person-group><article-title>Unraveling circuits of visual perception and cognition through the superior colliculus</article-title><source>Neuron</source><year>2021</year><volume>109</volume><fpage>918</fpage><lpage>937</lpage><pub-id pub-id-type="pmcid">PMC7979487</pub-id><pub-id pub-id-type="pmid">33548173</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2021.01.013</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isa</surname><given-names>T</given-names></name><name><surname>Marquez-Legorreta</surname><given-names>E</given-names></name><name><surname>Grillner</surname><given-names>S</given-names></name><name><surname>Scott</surname><given-names>EK</given-names></name></person-group><article-title>The tectum/superior colliculus as the vertebrate solution for spatial sensory integration and action</article-title><source>Curr Biol</source><year>2021</year><volume>31</volume><fpage>R741</fpage><lpage>R762</lpage><pub-id pub-id-type="pmcid">PMC8190998</pub-id><pub-id pub-id-type="pmid">34102128</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2021.04.001</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>X</given-names></name><name><surname>Huang</surname><given-names>H</given-names></name><name><surname>Snutch</surname><given-names>TP</given-names></name><name><surname>Cao</surname><given-names>P</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Wang</surname><given-names>F</given-names></name></person-group><article-title>The Superior Colliculus: Cell Types, Connectivity, and Behavior</article-title><source>Neurosci Bull</source><year>2022</year><volume>38</volume><fpage>1519</fpage><lpage>1540</lpage><pub-id pub-id-type="pmcid">PMC9723059</pub-id><pub-id pub-id-type="pmid">35484472</pub-id><pub-id pub-id-type="doi">10.1007/s12264-022-00858-1</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>May</surname><given-names>PJ</given-names></name></person-group><article-title>The mammalian superior colliculus: laminar structure and connections</article-title><source>Prog Brain Res</source><year>2006</year><volume>151</volume><fpage>321</fpage><lpage>378</lpage><pub-id pub-id-type="pmid">16221594</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brenner</surname><given-names>JM</given-names></name><name><surname>Beltramo</surname><given-names>R</given-names></name><name><surname>Gerfen</surname><given-names>CR</given-names></name><name><surname>Ruediger</surname><given-names>S</given-names></name><name><surname>Scanziani</surname><given-names>M</given-names></name></person-group><article-title>A genetically defined tecto-thalamic pathway drives a system of superior-colliculus-dependent visual cortices</article-title><source>Neuron</source><year>2023</year><volume>111</volume><fpage>2247</fpage><lpage>2257</lpage><elocation-id>e2247</elocation-id><pub-id pub-id-type="pmcid">PMC10524301</pub-id><pub-id pub-id-type="pmid">37172584</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2023.04.022</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>H</given-names></name><name><surname>Savier</surname><given-names>EL</given-names></name><name><surname>DePiero</surname><given-names>VJ</given-names></name><name><surname>Cang</surname><given-names>J</given-names></name></person-group><article-title>Lack of Evidence for Stereotypical Direction Columns in the Mouse Superior Colliculus</article-title><source>J Neurosci</source><year>2021</year><volume>41</volume><fpage>461</fpage><lpage>473</lpage><pub-id pub-id-type="pmcid">PMC7821859</pub-id><pub-id pub-id-type="pmid">33214319</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1155-20.2020</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ito</surname><given-names>S</given-names></name><name><surname>Feldheim</surname><given-names>DA</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name></person-group><article-title>Segregation of Visual Response Properties in the Mouse Superior Colliculus and Their Modulation during Locomotion</article-title><source>J Neurosci</source><year>2017</year><volume>37</volume><fpage>8428</fpage><lpage>8443</lpage><pub-id pub-id-type="pmcid">PMC5577856</pub-id><pub-id pub-id-type="pmid">28760858</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3689-16.2017</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Savier</surname><given-names>EL</given-names></name><name><surname>Chen</surname><given-names>H</given-names></name><name><surname>Cang</surname><given-names>J</given-names></name></person-group><article-title>Effects of Locomotion on Visual Responses in the Mouse Superior Colliculus</article-title><source>J Neurosci</source><year>2019</year><volume>39</volume><fpage>9360</fpage><lpage>9368</lpage><pub-id pub-id-type="pmcid">PMC6867823</pub-id><pub-id pub-id-type="pmid">31570535</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1854-19.2019</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schroder</surname><given-names>S</given-names></name><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Krumin</surname><given-names>M</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Rizzi</surname><given-names>M</given-names></name><name><surname>Lagnado</surname><given-names>L</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name></person-group><article-title>Arousal Modulates Retinal Output</article-title><source>Neuron</source><year>2020</year><volume>107</volume><fpage>487</fpage><lpage>495</lpage><elocation-id>e489</elocation-id><pub-id pub-id-type="pmcid">PMC7427318</pub-id><pub-id pub-id-type="pmid">32445624</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2020.04.026</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gonzalez-Rueda</surname><given-names>A</given-names></name><name><surname>Jensen</surname><given-names>K</given-names></name><name><surname>Noormandipour</surname><given-names>M</given-names></name><name><surname>de Malmazet</surname><given-names>D</given-names></name><name><surname>Wilson</surname><given-names>J</given-names></name><name><surname>Ciabatti</surname><given-names>E</given-names></name><name><surname>Kim</surname><given-names>J</given-names></name><name><surname>Williams</surname><given-names>E</given-names></name><name><surname>Poort</surname><given-names>J</given-names></name><name><surname>Hennequin</surname><given-names>G</given-names></name><name><surname>Tripodi</surname><given-names>M</given-names></name></person-group><article-title>Kinetic features dictate sensorimotor alignment in the superior colliculus</article-title><source>Nature</source><year>2024</year><volume>631</volume><fpage>378</fpage><lpage>385</lpage><pub-id pub-id-type="pmcid">PMC11236723</pub-id><pub-id pub-id-type="pmid">38961292</pub-id><pub-id pub-id-type="doi">10.1038/s41586-024-07619-2</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ito</surname><given-names>S</given-names></name><name><surname>Si</surname><given-names>Y</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name><name><surname>Feldheim</surname><given-names>DA</given-names></name></person-group><article-title>Nonlinear visuoauditory integration in the mouse superior colliculus</article-title><source>PLoS Comput Biol</source><year>2021</year><volume>17</volume><elocation-id>e1009181</elocation-id><pub-id pub-id-type="pmcid">PMC8584769</pub-id><pub-id pub-id-type="pmid">34723955</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1009181</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>KH</given-names></name><name><surname>Tran</surname><given-names>A</given-names></name><name><surname>Turan</surname><given-names>Z</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><article-title>The sifting of visual information in the superior colliculus</article-title><source>Elife</source><year>2020</year><volume>9</volume><pub-id pub-id-type="pmcid">PMC7237212</pub-id><pub-id pub-id-type="pmid">32286224</pub-id><pub-id pub-id-type="doi">10.7554/eLife.50678</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>YT</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><article-title>Functional cell types in the mouse superior colliculus</article-title><source>Elife</source><year>2023</year><volume>12</volume><pub-id pub-id-type="pmcid">PMC10121220</pub-id><pub-id pub-id-type="pmid">37073860</pub-id><pub-id pub-id-type="doi">10.7554/eLife.82367</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Herman</surname><given-names>JP</given-names></name><name><surname>Krauzlis</surname><given-names>RJ</given-names></name></person-group><article-title>Neuronal modulation in the mouse superior colliculus during covert visual selective attention</article-title><source>Sci Rep</source><year>2022</year><volume>12</volume><elocation-id>2482</elocation-id><pub-id pub-id-type="pmcid">PMC8847498</pub-id><pub-id pub-id-type="pmid">35169189</pub-id><pub-id pub-id-type="doi">10.1038/s41598-022-06410-5</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Franceschi</surname><given-names>G</given-names></name><name><surname>Solomon</surname><given-names>SG</given-names></name></person-group><article-title>Visual response properties of neurons in the superficial layers of the superior colliculus of awake mouse</article-title><source>J Physiol</source><year>2018</year><volume>596</volume><fpage>6307</fpage><lpage>6332</lpage><pub-id pub-id-type="pmcid">PMC6292807</pub-id><pub-id pub-id-type="pmid">30281795</pub-id><pub-id pub-id-type="doi">10.1113/JP276964</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drager</surname><given-names>UC</given-names></name><name><surname>Hubel</surname><given-names>DH</given-names></name></person-group><article-title>Responses to visual stimulation and relationship between visual, auditory, and somatosensory inputs in mouse superior colliculus</article-title><source>J Neurophysiol</source><year>1975</year><volume>38</volume><fpage>690</fpage><lpage>713</lpage><pub-id pub-id-type="pmid">1127462</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gale</surname><given-names>SD</given-names></name><name><surname>Murphy</surname><given-names>GJ</given-names></name></person-group><article-title>Distinct representation and distribution of visual information by specific cell types in mouse superficial superior colliculus</article-title><source>J Neurosci</source><year>2014</year><volume>34</volume><fpage>13458</fpage><lpage>13471</lpage><pub-id pub-id-type="pmcid">PMC4180477</pub-id><pub-id pub-id-type="pmid">25274823</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2768-14.2014</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Sarnaik</surname><given-names>R</given-names></name><name><surname>Rangarajan</surname><given-names>K</given-names></name><name><surname>Liu</surname><given-names>X</given-names></name><name><surname>Cang</surname><given-names>J</given-names></name></person-group><article-title>Visual receptive field properties of neurons in the superficial superior colliculus of the mouse</article-title><source>J Neurosci</source><year>2010</year><volume>30</volume><fpage>16573</fpage><lpage>16584</lpage><pub-id pub-id-type="pmcid">PMC3073584</pub-id><pub-id pub-id-type="pmid">21147997</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3305-10.2010</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keller</surname><given-names>GB</given-names></name><name><surname>Bonhoeffer</surname><given-names>T</given-names></name><name><surname>Hubener</surname><given-names>M</given-names></name></person-group><article-title>Sensorimotor mismatch signals in primary visual cortex of the behaving mouse</article-title><source>Neuron</source><year>2012</year><volume>74</volume><fpage>809</fpage><lpage>815</lpage><pub-id pub-id-type="pmid">22681686</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muzzu</surname><given-names>T</given-names></name><name><surname>Saleem</surname><given-names>AB</given-names></name></person-group><article-title>Feature selectivity can explain mismatch signals in mouse visual cortex</article-title><source>Cell Rep</source><year>2021</year><volume>37</volume><elocation-id>109772</elocation-id><pub-id pub-id-type="pmcid">PMC8655498</pub-id><pub-id pub-id-type="pmid">34610298</pub-id><pub-id pub-id-type="doi">10.1016/j.celrep.2021.109772</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoy</surname><given-names>JL</given-names></name><name><surname>Bishop</surname><given-names>HI</given-names></name><name><surname>Niell</surname><given-names>CM</given-names></name></person-group><article-title>Defined Cell Types in Superior Colliculus Make Distinct Contributions to Prey Capture Behavior in the Mouse</article-title><source>Curr Biol</source><year>2019</year><volume>29</volume><fpage>4130</fpage><lpage>4138</lpage><elocation-id>e4135</elocation-id><pub-id pub-id-type="pmcid">PMC6925587</pub-id><pub-id pub-id-type="pmid">31761701</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2019.10.017</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shang</surname><given-names>C</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Liu</surname><given-names>A</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Qu</surname><given-names>B</given-names></name><name><surname>Yan</surname><given-names>F</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>W</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name><etal/></person-group><article-title>Divergent midbrain circuits orchestrate escape and freezing responses to looming stimuli in mice</article-title><source>Nat Commun</source><year>2018</year><volume>9</volume><elocation-id>1232</elocation-id><pub-id pub-id-type="pmcid">PMC5964329</pub-id><pub-id pub-id-type="pmid">29581428</pub-id><pub-id pub-id-type="doi">10.1038/s41467-018-03580-7</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>X</given-names></name><name><surname>Liu</surname><given-names>M</given-names></name><name><surname>Cang</surname><given-names>J</given-names></name></person-group><article-title>Visual cortex modulates the magnitude but not the selectivity of looming-evoked responses in the superior colliculus of awake mice</article-title><source>Neuron</source><year>2014</year><volume>84</volume><fpage>202</fpage><lpage>213</lpage><pub-id pub-id-type="pmcid">PMC4184914</pub-id><pub-id pub-id-type="pmid">25220812</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2014.08.037</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shi</surname><given-names>X</given-names></name><name><surname>Barchini</surname><given-names>J</given-names></name><name><surname>Ledesma</surname><given-names>HA</given-names></name><name><surname>Koren</surname><given-names>D</given-names></name><name><surname>Jin</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>X</given-names></name><name><surname>Wei</surname><given-names>W</given-names></name><name><surname>Cang</surname><given-names>J</given-names></name></person-group><article-title>Retinal origin of direction selectivity in the superior colliculus</article-title><source>Nat Neurosci</source><year>2017</year><volume>20</volume><fpage>550</fpage><lpage>558</lpage><pub-id pub-id-type="pmcid">PMC5374021</pub-id><pub-id pub-id-type="pmid">28192394</pub-id><pub-id pub-id-type="doi">10.1038/nn.4498</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>C</given-names></name><name><surname>Kuhn</surname><given-names>NK</given-names></name><name><surname>Alkislar</surname><given-names>I</given-names></name><name><surname>Sans-Dublanc</surname><given-names>A</given-names></name><name><surname>Zemmouri</surname><given-names>F</given-names></name><name><surname>Paesmans</surname><given-names>S</given-names></name><name><surname>Calzoni</surname><given-names>A</given-names></name><name><surname>Ooms</surname><given-names>F</given-names></name><name><surname>Reinhard</surname><given-names>K</given-names></name><name><surname>Farrow</surname><given-names>K</given-names></name></person-group><article-title>Pathway-specific inputs to the superior colliculus support flexible responses to visual threat</article-title><source>Sci Adv</source><year>2023</year><volume>9</volume><elocation-id>eade3874</elocation-id><pub-id pub-id-type="pmcid">PMC10468139</pub-id><pub-id pub-id-type="pmid">37647395</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.ade3874</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>Z</given-names></name><name><surname>Wang</surname><given-names>M</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Shang</surname><given-names>C</given-names></name><name><surname>Zhang</surname><given-names>C</given-names></name><name><surname>Sun</surname><given-names>L</given-names></name><name><surname>Gu</surname><given-names>H</given-names></name><name><surname>Ran</surname><given-names>G</given-names></name><name><surname>Pei</surname><given-names>Q</given-names></name><name><surname>Ma</surname><given-names>Q</given-names></name><etal/></person-group><article-title>Transcriptomic encoding of sensorimotor transformation in the midbrain</article-title><source>Elife</source><year>2021</year><volume>10</volume><pub-id pub-id-type="pmcid">PMC8341986</pub-id><pub-id pub-id-type="pmid">34318750</pub-id><pub-id pub-id-type="doi">10.7554/eLife.69825</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campagner</surname><given-names>D</given-names></name><name><surname>Vale</surname><given-names>R</given-names></name><name><surname>Tan</surname><given-names>YL</given-names></name><name><surname>Iordanidou</surname><given-names>P</given-names></name><name><surname>Pavon Arocas</surname><given-names>O</given-names></name><name><surname>Claudi</surname><given-names>F</given-names></name><name><surname>Stempel</surname><given-names>AV</given-names></name><name><surname>Keshavarzi</surname><given-names>S</given-names></name><name><surname>Petersen</surname><given-names>RS</given-names></name><name><surname>Margrie</surname><given-names>TW</given-names></name><name><surname>Branco</surname><given-names>T</given-names></name></person-group><article-title>A cortico-collicular circuit for orienting to shelter during escape</article-title><source>Nature</source><year>2023</year><volume>613</volume><fpage>111</fpage><lpage>119</lpage><pub-id pub-id-type="pmcid">PMC7614651</pub-id><pub-id pub-id-type="pmid">36544025</pub-id><pub-id pub-id-type="doi">10.1038/s41586-022-05553-9</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>W</given-names></name><name><surname>Sabatini</surname><given-names>BL</given-names></name></person-group><article-title>Anatomically segregated basal ganglia pathways allow parallel behavioral modulation</article-title><source>Nat Neurosci</source><year>2020</year><volume>23</volume><fpage>1388</fpage><lpage>1398</lpage><pub-id pub-id-type="pmcid">PMC7606600</pub-id><pub-id pub-id-type="pmid">32989293</pub-id><pub-id pub-id-type="doi">10.1038/s41593-020-00712-5</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ritter</surname><given-names>A</given-names></name><name><surname>Habusha</surname><given-names>S</given-names></name><name><surname>Givon</surname><given-names>L</given-names></name><name><surname>Edut</surname><given-names>S</given-names></name><name><surname>Klavir</surname><given-names>O</given-names></name></person-group><article-title>Prefrontal control of superior colliculus modulates innate escape behavior following adversity</article-title><source>Nat Commun</source><year>2024</year><volume>15</volume><elocation-id>2158</elocation-id><pub-id pub-id-type="pmcid">PMC10925020</pub-id><pub-id pub-id-type="pmid">38461293</pub-id><pub-id pub-id-type="doi">10.1038/s41467-024-46460-z</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Franceschi</surname><given-names>G</given-names></name><name><surname>Vivattanasarn</surname><given-names>T</given-names></name><name><surname>Saleem</surname><given-names>AB</given-names></name><name><surname>Solomon</surname><given-names>SG</given-names></name></person-group><article-title>Vision Guides Selection of Freeze or Flight Defense Strategies in Mice</article-title><source>Curr Biol</source><year>2016</year><volume>26</volume><fpage>2150</fpage><lpage>2154</lpage><pub-id pub-id-type="pmid">27498569</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>P</given-names></name><name><surname>Liu</surname><given-names>N</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>Liu</surname><given-names>X</given-names></name><name><surname>Tang</surname><given-names>Y</given-names></name><name><surname>He</surname><given-names>X</given-names></name><name><surname>Wu</surname><given-names>B</given-names></name><name><surname>Zhou</surname><given-names>Z</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><etal/></person-group><article-title>Processing of visually evoked innate fear by a non-canonical thalamic pathway</article-title><source>Nat Commun</source><year>2015</year><volume>6</volume><elocation-id>6756</elocation-id><pub-id pub-id-type="pmcid">PMC4403372</pub-id><pub-id pub-id-type="pmid">25854147</pub-id><pub-id pub-id-type="doi">10.1038/ncomms7756</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yilmaz</surname><given-names>M</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><article-title>Rapid innate defensive responses of mice to looming visual stimuli</article-title><source>Curr Biol</source><year>2013</year><volume>23</volume><fpage>2011</fpage><lpage>2015</lpage><pub-id pub-id-type="pmcid">PMC3809337</pub-id><pub-id pub-id-type="pmid">24120636</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2013.08.015</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Solomon</surname><given-names>SG</given-names></name><name><surname>Janbon</surname><given-names>H</given-names></name><name><surname>Bimson</surname><given-names>A</given-names></name><name><surname>Wheatcroft</surname><given-names>T</given-names></name></person-group><article-title>Visual spatial location influences selection of instinctive behaviours in mouse</article-title><source>R Soc Open Sci</source><year>2023</year><volume>10</volume><elocation-id>230034</elocation-id><pub-id pub-id-type="pmcid">PMC10130721</pub-id><pub-id pub-id-type="pmid">37122945</pub-id><pub-id pub-id-type="doi">10.1098/rsos.230034</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cooke</surname><given-names>SF</given-names></name><name><surname>Komorowski</surname><given-names>RW</given-names></name><name><surname>Kaplan</surname><given-names>ES</given-names></name><name><surname>Gavornik</surname><given-names>JP</given-names></name><name><surname>Bear</surname><given-names>MF</given-names></name></person-group><article-title>Visual recognition memory, manifested as long-term habituation, requires synaptic plasticity in V1</article-title><source>Nat Neurosci</source><year>2015</year><volume>18</volume><fpage>262</fpage><lpage>271</lpage><pub-id pub-id-type="pmcid">PMC4383092</pub-id><pub-id pub-id-type="pmid">25599221</pub-id><pub-id pub-id-type="doi">10.1038/nn.3920</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Papanikolaou</surname><given-names>A</given-names></name><name><surname>Rodrigues</surname><given-names>FR</given-names></name><name><surname>Holeniewska</surname><given-names>J</given-names></name><name><surname>Phillips</surname><given-names>KG</given-names></name><name><surname>Saleem</surname><given-names>AB</given-names></name><name><surname>Solomon</surname><given-names>SG</given-names></name></person-group><article-title>Plasticity in visual cortex is disrupted in a mouse model of tauopathy</article-title><source>Commun Biol</source><year>2022</year><volume>5</volume><fpage>77</fpage><pub-id pub-id-type="pmcid">PMC8776781</pub-id><pub-id pub-id-type="pmid">35058544</pub-id><pub-id pub-id-type="doi">10.1038/s42003-022-03012-9</pub-id></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liang</surname><given-names>F</given-names></name><name><surname>Xiong</surname><given-names>XR</given-names></name><name><surname>Zingg</surname><given-names>B</given-names></name><name><surname>Ji</surname><given-names>XY</given-names></name><name><surname>Zhang</surname><given-names>LI</given-names></name><name><surname>Tao</surname><given-names>HW</given-names></name></person-group><article-title>Sensory Cortical Control of a Visually Induced Arrest Behavior via Corticotectal Projections</article-title><source>Neuron</source><year>2015</year><volume>86</volume><fpage>755</fpage><lpage>767</lpage><pub-id pub-id-type="pmcid">PMC4452020</pub-id><pub-id pub-id-type="pmid">25913860</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2015.03.048</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roseberry</surname><given-names>T</given-names></name><name><surname>Kreitzer</surname><given-names>A</given-names></name></person-group><article-title>Neural circuitry for behavioural arrest</article-title><source>Philos Trans R Soc Lond B Biol Sci</source><year>2017</year><volume>372</volume><pub-id pub-id-type="pmcid">PMC5332856</pub-id><pub-id pub-id-type="pmid">28242731</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2016.0197</pub-id></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fink</surname><given-names>AJ</given-names></name><name><surname>Axel</surname><given-names>R</given-names></name><name><surname>Schoonover</surname><given-names>CE</given-names></name></person-group><article-title>A virtual burrow assay for head-fixed mice measures habituation, discrimination, exploration and avoidance without training</article-title><source>Elife</source><year>2019</year><volume>8</volume><pub-id pub-id-type="pmcid">PMC6469927</pub-id><pub-id pub-id-type="pmid">30994457</pub-id><pub-id pub-id-type="doi">10.7554/eLife.45658</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garland</surname><given-names>TJ</given-names></name></person-group><article-title>The relation between maximal running speed and body mass in terrestrial mammals</article-title><source>Journal of Zoology</source><year>1983</year><volume>199</volume><fpage>157</fpage><lpage>170</lpage></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siegle</surname><given-names>JH</given-names></name><name><surname>Lopez</surname><given-names>AC</given-names></name><name><surname>Patel</surname><given-names>YA</given-names></name><name><surname>Abramov</surname><given-names>K</given-names></name><name><surname>Ohayon</surname><given-names>S</given-names></name><name><surname>Voigts</surname><given-names>J</given-names></name></person-group><article-title>Open Ephys: an open-source, plugin-based platform for multichannel electrophysiology</article-title><source>J Neural Eng</source><year>2017</year><volume>14</volume><elocation-id>045003</elocation-id><pub-id pub-id-type="pmid">28169219</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lopes</surname><given-names>G</given-names></name><name><surname>Farrell</surname><given-names>K</given-names></name><name><surname>Horrocks</surname><given-names>EA</given-names></name><name><surname>Lee</surname><given-names>CY</given-names></name><name><surname>Morimoto</surname><given-names>MM</given-names></name><name><surname>Muzzu</surname><given-names>T</given-names></name><name><surname>Papanikolaou</surname><given-names>A</given-names></name><name><surname>Rodrigues</surname><given-names>FR</given-names></name><name><surname>Wheatcroft</surname><given-names>T</given-names></name><name><surname>Zucca</surname><given-names>S</given-names></name><etal/></person-group><article-title>Creating and controlling visual environments using BonVision</article-title><source>Elife</source><year>2021</year><volume>10</volume><pub-id pub-id-type="pmcid">PMC8104957</pub-id><pub-id pub-id-type="pmid">33880991</pub-id><pub-id pub-id-type="doi">10.7554/eLife.65541</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lopes</surname><given-names>G</given-names></name><name><surname>Bonacchi</surname><given-names>N</given-names></name><name><surname>Frazao</surname><given-names>J</given-names></name><name><surname>Neto</surname><given-names>JP</given-names></name><name><surname>Atallah</surname><given-names>BV</given-names></name><name><surname>Soares</surname><given-names>S</given-names></name><name><surname>Moreira</surname><given-names>L</given-names></name><name><surname>Matias</surname><given-names>S</given-names></name><name><surname>Itskov</surname><given-names>PM</given-names></name><name><surname>Correia</surname><given-names>PA</given-names></name><etal/></person-group><article-title>Bonsai: an event-based framework for processing and controlling data streams</article-title><source>Front Neuroinform</source><year>2015</year><volume>9</volume><fpage>7</fpage><pub-id pub-id-type="pmcid">PMC4389726</pub-id><pub-id pub-id-type="pmid">25904861</pub-id><pub-id pub-id-type="doi">10.3389/fninf.2015.00007</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Sridhar</surname><given-names>S</given-names></name><name><surname>Pennington</surname><given-names>J</given-names></name><name><surname>Stringer</surname><given-names>C</given-names></name></person-group><article-title>Spike sorting with Kilosort4</article-title><source>Nat Methods</source><year>2024</year><volume>21</volume><fpage>914</fpage><lpage>921</lpage><pub-id pub-id-type="pmcid">PMC11093732</pub-id><pub-id pub-id-type="pmid">38589517</pub-id><pub-id pub-id-type="doi">10.1038/s41592-024-02232-7</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rossant</surname><given-names>C</given-names></name><name><surname>Kadir</surname><given-names>SN</given-names></name><name><surname>Goodman</surname><given-names>DFM</given-names></name><name><surname>Schulman</surname><given-names>J</given-names></name><name><surname>Hunter</surname><given-names>MLD</given-names></name><name><surname>Saleem</surname><given-names>AB</given-names></name><name><surname>Grosmark</surname><given-names>A</given-names></name><name><surname>Belluscio</surname><given-names>M</given-names></name><name><surname>Denfield</surname><given-names>GH</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name><etal/></person-group><article-title>Spike sorting for large, dense electrode arrays</article-title><source>Nat Neurosci</source><year>2016</year><volume>19</volume><fpage>634</fpage><lpage>641</lpage><pub-id pub-id-type="pmcid">PMC4817237</pub-id><pub-id pub-id-type="pmid">26974951</pub-id><pub-id pub-id-type="doi">10.1038/nn.4268</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saleem</surname><given-names>AB</given-names></name><name><surname>Ayaz</surname><given-names>A</given-names></name><name><surname>Jeffery</surname><given-names>KJ</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name></person-group><article-title>Integration of visual motion and locomotion in mouse visual cortex</article-title><source>Nat Neurosci</source><year>2013</year><volume>16</volume><fpage>1864</fpage><lpage>1869</lpage><pub-id pub-id-type="pmcid">PMC3926520</pub-id><pub-id pub-id-type="pmid">24185423</pub-id><pub-id pub-id-type="doi">10.1038/nn.3567</pub-id></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fritsch</surname><given-names>NF</given-names></name><name><surname>Butland</surname><given-names>A</given-names></name></person-group><article-title>A method for constructing local monotone piecewise cubic interpolants</article-title><source>SIAM Journal on Scientific and Statistical Computing</source><year>1984</year><volume>5</volume><fpage>300</fpage><lpage>304</lpage><pub-id pub-id-type="doi">10.1137/0905021</pub-id></element-citation></ref></ref-list></back><floats-group><boxed-text id="BX1" position="float" orientation="portrait"><caption><title>Highlights</title></caption><list list-type="bullet" id="L1"><list-item><p>We recorded from superficial (SCs) and intermediate (SCim) superior colliculus in VR</p></list-item><list-item><p>Vision dominated SCs, while SCim was modulated by both vision and locomotion</p></list-item><list-item><p>Mice altered behaviour when visual experience did not match that expected from their locomotion</p></list-item><list-item><p>Population activity differed between matched and unmatched visual experiences, particularly in SCim</p></list-item></list></boxed-text><fig id="F1" position="float"><label>Figure 1</label><caption><title>Loom response in SC combines visual and self-motion signals.</title><p><bold>A</bold>. Animals were head-restrained above a treadmill in an immersive virtual reality environment. <bold>B</bold>. A round black object loomed towards the animal at constant speed. Schematics show the approximate view of the virtual object to the animal at each distance; dashed line indicates the visual limit of the virtual environment. <bold>C</bold>. Extracellular recordings were made from SCs and SCim using high-density probes. Images of sections through SC showing red fluorescence deposited by a DiI-coated electrode. Inferred electrode tracks are indicated by vertical lines. SCs image is flipped on the horizontal axis. <bold>D</bold>. Visual response is strongest in SCs. (<italic>upper</italic>) Responses (z-scores) of consistent units to object loom (100 cm/s), sorted by response in 0.2 s preceding collision. (<italic>lower</italic>) Average response over all units in SCs or SCim, and for units where looms increased (‘Exc’) or decreased (‘Inhib’, dashed line) activity. <bold>E</bold>. Visual receptive fields predict timing of loom response. (<italic>upper</italic>) Example units showing response to slow object loom (25 cm/s), and receptive field maps for black squares. Receptive field maps are flipped on the horizontal axis so that the nasal visual field is on the left, and temporal visual field is on the right. Darker indicates larger positive weight at that location. Red circle indicates estimates of location and size of excitatory receptive fields in each case. (<italic>lower left)</italic> Average responses to slow loom, for units where the looming object increased activity. Units were grouped by the azimuthal locations of their receptive field; the four lines of varying colour show average response in each of four of the azimuthal groups. (<italic>lower right</italic>) Time to response peak as a function of receptive field azimuth. Units with more nasal receptive fields (azimuth closer to 0) respond earlier to the looming object. Numerals indicate relevant curves in the lower left plot, from which the indices were derived. <bold>F</bold>. Locomotion has a more consistent impact on SCim. Response to fast object looms over the 0.2 s preceding collision, in SCs and SCim, during stationary and running conditions.</p></caption><graphic xlink:href="EMS201993-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>SCs is dominated by vision, and SCim is dominated by self-motion, during object approach.</title><p><bold>A</bold>. Patterns of activity were stereotypical in SCs, but diverse in SCim, as animals approached a stationary object at the end of the virtual platform. (<italic>upper</italic>) Response (z-scores) of consistent units, sorted by response in the 0–10 cm (SCs) or 10–20 cm (SCim) preceding collision. Colourbar indicates unit response in z-scores. (<italic>lower</italic>) Average response over all units in SCs or SCim, or over units in which object approach increased (‘Exc’) or decreased (‘Inhib’) activity. <bold>B</bold>. Speed of approach had more impact on activity in SCim and less impact in SCs. Example units showing average activity during trials of high locomotion speed (darker colours; 5 fastest trials), or low locomotion speed (lighter colours; 5 slowest trials). Horizontal scale bar 20 cm; Vertical scale bar 1 z-score. <bold>C</bold>. Relative contribution of locomotion speed and distance-to-object to activity in individual units in SCs (<italic>left</italic>) and SCim (<italic>right</italic>). Each point represents the performance (‘Perf.’; explained variance) of a ridge regression model when only locomotion speed (abscissa) or distance (ordinate) was allowed to vary, normalised to the performance of the model when both were allowed to vary. These normalized values could be greater than one due to the cross-validation. Histograms show the difference between these performance indices.</p></caption><graphic xlink:href="EMS201993-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Vision and sensorimotor context drive instinctive locomotion behaviours.</title><p><bold>A</bold>. (<italic>top</italic>) Animals reduce locomotion speed (slow-down) as they approach an object in virtual reality (VR). Each row of the image represents the locomotion speed of one animal, averaged across all VR trials in all post-training sessions. Colorbar indicates animal speed (cm/s). (<italic>bottom</italic>) Slow-down behaviour was present during object-approach but not when the object was absent (platform trials). Mean and s.e.m. of locomotion speed across animals. Shaded regions indicate locations used for estimates of locomotion speed used in panels C-D. <bold>B</bold>. Same as A but during replay, where the visual stimulus was not predicted by current locomotion behaviour. Slow-down behaviour remained but overall locomotion speed was reduced during replay of object-approach trials. <bold>C</bold>. Slow-down behaviour emerged in the earliest stages of exposure to virtual objects. Each point shows locomotion speed (mean and s.e.m. across animals) when animals were near the object (blue-grey regions in panels A,B), relative to that at larger distances (pink regions in A,B). Equivalent measurements were made during platform trials. The last point shows the data from the post-training sessions shown in A. <bold>D</bold>. Comparison of slow-down behaviour in platform, VR and replay trials post-training. Small symbols show individual animals. <bold>E</bold>. Temporal dynamics of locomotion behaviour in two example sessions. (<italic>left</italic>) Session in which locomotion speed was similar during VR (dark green) and replay (light green) blocks of trials. (<italic>right</italic>) Session in which the animal ceased locomotion upon exposure to replay trials, and resumed locomotion when reintroduced into VR. <bold>F</bold>. (<italic>left</italic>) Fraction of time in each condition where animal locomotion speed was at least 0.1 cm/s. Data represent 48 sessions in 14 animals. Red squares and triangles indicate example sessions in E. (<italic>right</italic>) Average locomotion speed in each condition, relative to that in the first exposure to VR.</p></caption><graphic xlink:href="EMS201993-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Neural activity in SC discriminates virtual reality and replay conditions.</title><p><bold>A</bold>. Logistic regression was used to classify epochs of time as belonging to Virtual Reality (VR) or replay given neural activity from SCs or SCim and/or other covariates, including running speed, distance to object, or time into the experiment. <bold>B</bold>. Test classification accuracy of SCs sessions for individual covariates (speed, distance, time) and all covariates combined. ‘Covariates cor.’ indicates classification performance when using a corrected subset of the data (confining analyses to samples of VR and replay conditions with the same speed range, the same average time into the experiment, and confining analyses to distances within 50 cm of the object). Circles indicate mean performance in each session, and triangles indicate mean of shuffled controls for each session. Mean and s.d. Lines were obtained across four train-test splits. Symbols of the same hue are sessions from the same animal. <bold>C</bold>. Same as B but for SCim sessions. <bold>D</bold>. (<italic>top</italic>) Distributions of locomotion speed differs for VR and replay conditions. (<italic>bottom</italic>) Distributions of distance-to-object are matched, by design, in VR and replay. <bold>E</bold>. Test classification accuracy of SCs sessions when including all corrected covariates, or when including neural activity as well as those covariates. Symbols of the same hue are sessions from the same animal. <bold>F</bold>. Same as E but for SCim sessions.</p></caption><graphic xlink:href="EMS201993-f004"/></fig></floats-group></article>