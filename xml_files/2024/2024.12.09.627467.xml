<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS201828</article-id><article-id pub-id-type="doi">10.1101/2024.12.09.627467</article-id><article-id pub-id-type="archive">PPR951905</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Repetition of rhythmic patterns fosters neural representation of musical meter</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Coulon</surname><given-names>Emmanuel</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Baum</surname><given-names>Sacha</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Lenc</surname><given-names>Tomas</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Polak</surname><given-names>Rainer</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Nozaradan</surname><given-names>Sylvie</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A4">4</xref></contrib></contrib-group><aff id="A1"><label>1</label>Institute of Neuroscience (IoNS), <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02495e989</institution-id><institution>Université Catholique de Louvain (UCLouvain)</institution></institution-wrap>, <city>Brussels</city>, <country country="BE">Belgium</country></aff><aff id="A2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01a28zg77</institution-id><institution>Basque Center on Cognition, Brain and Language</institution></institution-wrap>, <country country="ES">Spain</country></aff><aff id="A3"><label>3</label>RITMO Centre for Interdisciplinary Studies in Rhythm, Time and Motion, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01xtthb56</institution-id><institution>University of Oslo</institution></institution-wrap>, <country country="NO">Norway</country></aff><aff id="A4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05yfz9t60</institution-id><institution>International Laboratory for Brain, Music and Sound Research (BRAMS)</institution></institution-wrap>, <city>Montreal, QC</city>, <country country="CA">Canada</country></aff><author-notes><corresp id="CR1">Correspondence concerning this article should be addressed to Emmanuel Coulon (<email>emmanuel.coulon@uclouvain.be</email>).</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>11</day><month>12</month><year>2024</year></pub-date><pub-date pub-type="preprint"><day>09</day><month>12</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Music often entails perception of periodic pulses (hereafter meter) which serve as an internal temporal reference to coordinate movements to music. Crucially, meter perception arises even when the musical rhythm only weakly cues meter periodicities (i.e., syncopated rhythms). However, syncopated rhythms are often looped in music, suggesting that repetition of rhythmic patterns may facilitate meter perception by providing periodic cues at a slower, supra-second timescale. Here, we tested this hypothesis by recording separately electroencephalographic (EEG) and behavioral responses (finger tapping) while participants listened to different syncopated rhythmic sequences. These sequences either consisted of a repeated pattern (repetition of 4.8 and 9.6-s patterns) or were generated without repetition. EEG responses showed overall periodization of the rhythmic input, at periodicities corresponding to those expressed as the meter in behavioral responses, and in contrast with the weak cues to these periodicities in the rhythmic inputs. Most importantly, pattern repetition strengthened this neural representation of the meter, demonstrating that supra-second periodicities in the rhythmic input further enhance sub-second periodicities in neural activity. These findings thus highlight the multiscale nature of temporal processes at stake in processing musical rhythm, and, more generally, complex rhythmic inputs involved in interpersonal interaction and communication.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><label>1</label><title>Introduction</title><p id="P2">Music has a profound and universal effect on humans. This effect often results from movement (e.g., instrument playing, hand-clapping, singing, etc.), and also motivates movements (e.g., dancing, head bobbing, foot tapping, co-performers instrument playing, etc.) that are coordinated with the rhythmic input<sup>[<xref ref-type="bibr" rid="R1">1</xref>,<xref ref-type="bibr" rid="R2">2</xref>]</sup>. For this rhythmic movement coordination to occur, an internal representation of time is required that is, to some degree, invariant to the specific temporal arrangement of musical events. This internal representation typically takes the form of meter, usually conceptualized as a set of temporally recurrent pulses. These pulses often form a nested scaffolding over multiple (sub- to supra-second) timescales, including a pulse layer standing as a particularly prominent time reference, usually called the beat<sup>[<xref ref-type="bibr" rid="R3">3</xref>,<xref ref-type="bibr" rid="R4">4</xref>]</sup>.</p><p id="P3">Notably, a representation of meter can arise in response to a wide variety of rhythmic stimuli, ranging from controlled isochronous sequences (e.g., metronomes) to spectro-temporally complex real-world performances (e.g., marching bands, large orchestras, etc.). Hence, the brain does not stop at building an internal representation of meter that corresponds one-to-one to periodic recurrences physically present in the rhythmic input, but rather goes beyond. This phenomenon is illustrated well with syncopated rhythms, which typically feature only weak acoustic cues to meter periodicities. Yet, syncopated rhythms are commonly used in music and do not prevent listeners from perceiving a metric structure, as embodied in the production of periodic movement along with these rhythms<sup>[<xref ref-type="bibr" rid="R5">5</xref>–<xref ref-type="bibr" rid="R7">7</xref>]</sup>.</p><p id="P4">Analogously, studies using electrophysiology have repeatedly demonstrated that brain activity elicited while participants listen to syncopated rhythms shows a selective enhancement of periodicities corresponding to the perceived meter. In other words, there is robust evidence that meter perception is tied to processes that periodize the rhythmic sensory input, thus shaping the neural representation towards a behaviorally-relevant internal template<sup>[<xref ref-type="bibr" rid="R8">8</xref>–<xref ref-type="bibr" rid="R12">12</xref>]</sup>. Together, these findings highlight the fact that internal representations of meter cannot be solely explained by lower-level sensory processing of prominent temporal cues of the input, but at least partially involve processes that enable a higher degree of invariance with respect to the input<sup>[<xref ref-type="bibr" rid="R13">13</xref>]</sup>.</p><p id="P5">Critically, meter perception concerns a specific temporal range lying approximately between 100 and 1800 ms (as delimited through ethnomusicological work, and behavioral experiments where participants were instructed to tap in synchrony along with isochronous rhythms<sup>[<xref ref-type="bibr" rid="R14">14</xref>–<xref ref-type="bibr" rid="R18">18</xref>]</sup>). Given that this temporal range covers a large span, from sub-second to supra-second scales, meter perception has been hypothesized to involve multiscale temporal scaffolding processes whereby the temporal structure of the rhythmic input over this whole temporal range, from slow to fast, would serve as temporal cues to map metric pulses<sup>[<xref ref-type="bibr" rid="R4">4</xref>,<xref ref-type="bibr" rid="R19">19</xref>–<xref ref-type="bibr" rid="R21">21</xref>]</sup>.</p><p id="P6">Here, we aimed to move a critical step forward in investigating this multiscale temporal scaffolding with a focus on its slower, supra-second end. We first propose a scheme where this temporal range for meter is organized into three temporal timescales or layers, from slow to fast (<xref ref-type="fig" rid="F1">Fig. 1</xref>). This scheme then serves as a basis to manipulate the prominence of the temporal cues at the slow supra-second layer while keeping parameters at the faster sub-second layers constant, thus informing on the contribution of the supra-second layer to the multiscale temporal scaffolding process at stake in meter perception.</p><p id="P7">As depicted in <xref ref-type="fig" rid="F1">Figure 1</xref>, the proposed scheme comprises a faster timescale (hereafter referred to as <italic>subdivision layer</italic>) concerned with single time intervals delimited by the successive events making up the rhythmic input, which can serve as a basis to map fast metric pulses<sup>[<xref ref-type="bibr" rid="R4">4</xref>,<xref ref-type="bibr" rid="R16">16</xref>,<xref ref-type="bibr" rid="R17">17</xref>]</sup>. The intermediate timescale (hereafter called <italic>beat layer</italic>) is concerned with the local arrangement of these successive events (also referred to as groups)<sup>[<xref ref-type="bibr" rid="R4">4</xref>]</sup>. These local arrangements can be considered as unsyncopated or syncopated (also referred to as strongly periodic or weakly periodic in the beat layer), depending on the extent to which these groups match the periodicity of the perceived beat<sup>[<xref ref-type="bibr" rid="R22">22</xref>,<xref ref-type="bibr" rid="R23">23</xref>]</sup>. Finally, the slower timescale (hereafter referred to as <italic>cycle layer</italic>) is concerned with longer, seamlessly repeating, or looped, rhythmic patterns, which can serve as a basis to map slow metric pulses<sup>[<xref ref-type="bibr" rid="R4">4</xref>,<xref ref-type="bibr" rid="R15">15</xref>,<xref ref-type="bibr" rid="R24">24</xref>]</sup>. Notably, the boundaries between these three timescales certainly depend on a multitude of factors pertaining to the stimulus (e.g., speed, timbre, etc.) and to the listener (e.g., cross-cultural or inter-individual differences, knowledge of the perceptual conventions inherent in musical genres, etc.). Nevertheless, there is generally a considerable overlap between the range of temporal cues of the rhythmic input and the range of perceived metric pulses, particularly at the beat and subdivision layers<sup>[<xref ref-type="bibr" rid="R2">2</xref>,<xref ref-type="bibr" rid="R4">4</xref>,<xref ref-type="bibr" rid="R12">12</xref>]</sup>.</p><p id="P8">Thus far, most neuroimaging studies focused on the role of temporal cues at the intermediate timescale corresponding to the beat layer, while keeping periodic recurrence at the slower timescale highly prominent through the use of looped rhythmic patterns<sup>[<xref ref-type="bibr" rid="R8">8</xref>,<xref ref-type="bibr" rid="R10">10</xref>,<xref ref-type="bibr" rid="R11">11</xref>,<xref ref-type="bibr" rid="R13">13</xref>,<xref ref-type="bibr" rid="R25">25</xref>,<xref ref-type="bibr" rid="R26">26</xref>]</sup>. However, little effort has been focused on evaluating the effect of supra-second temporal cues on the internal representation of meter. Yet, repetition of rhythmic patterns represents a core component of music<sup>[<xref ref-type="bibr" rid="R24">24</xref>,<xref ref-type="bibr" rid="R27">27</xref>–<xref ref-type="bibr" rid="R29">29</xref>]</sup>, one that is key to the aesthetic appeal and social power of music<sup>[<xref ref-type="bibr" rid="R5">5</xref>,<xref ref-type="bibr" rid="R30">30</xref>]</sup>. Repeating rhythmic patterns are often easily recognized and learned without requiring explicit training, and this periodic recurrence can help to structure the musical piece and anticipate future events<sup>[<xref ref-type="bibr" rid="R4">4</xref>,<xref ref-type="bibr" rid="R27">27</xref>,<xref ref-type="bibr" rid="R31">31</xref>]</sup>.</p><p id="P9">Therefore, we hypothesize that the periodicity provided by the repetition of rhythmic patterns serves as temporal cues to the meter. These cues would offer a temporal anchor point onto which faster metric pulses at the beat and/or subdivision layers can be mapped by interpolation, through multiscale temporal scaffolding potentially involved in meter perception<sup>[<xref ref-type="bibr" rid="R4">4</xref>,<xref ref-type="bibr" rid="R32">32</xref>,<xref ref-type="bibr" rid="R33">33</xref>]</sup>. We tested the role of these supra-second temporal cues by recording behavioral and neural responses in human healthy adult volunteers. Behavioral measures of the internal meter representation were obtained by asking participants to tap the finger along with the meter they would perceive while listening to rhythmic sequences. In a separate session, neural responses were captured using electroencephalography (EEG) while the same participants were listening to the same rhythmic sequences, from which neural representation of the perceived meter were measured using frequency analysis (see<sup>[<xref ref-type="bibr" rid="R12">12</xref>,<xref ref-type="bibr" rid="R34">34</xref>]</sup> for reviews).</p><p id="P10">These behavioral and neural measures were compared across three sequences presented in separate conditions. In the three sequences, temporal cues to the beat layer were invariantly low (i.e. weakly periodic, syncopated rhythms), whereas the periodic recurrence in the slower timescale corresponding to rhythmic patterns was manipulated such as to gradually decrease across stimuli (sequence #1: repetition of a 4.8 s pattern, sequence #2: repetition of a 9.6 s pattern, and sequence #3: no pattern repetition). A decrease of the behavioral and neural measures of internal meter representation across conditions would thus demonstrate a critical role of slow periodic cues in meter processing. Alternatively, observing comparable meter-related responses across conditions would suggest that periodic cues at the faster beat and/or subdivision layers may be sufficient to enable meter representation.</p><p id="P11">We also tested how these supra-second temporal cues interacted with other factors assumed to play a role in meter processing, namely short-term prior context<sup>[<xref ref-type="bibr" rid="R35">35</xref>]</sup> and long-term musical expertise. To this aim, different presentation orders of the conditions were compared, thus allowing to test whether supra-second temporal cues would be used as temporal anchor for meter not only when these slow cues are actually present in the sequences but also in subsequent sequences lacking these cues (<xref ref-type="fig" rid="F3">Fig. 3</xref>). Additionally, group comparison across professional musicians and non-musicians aimed to test the extent to which long-term musical practice overrides the lack of slow temporal cues in the rhythmic input, allowing for the representation of a stable internal metric template irrespective of sensory input degradation<sup>[<xref ref-type="bibr" rid="R2">2</xref>,<xref ref-type="bibr" rid="R36">36</xref>]</sup>.</p></sec><sec id="S2" sec-type="materials | methods"><label>2</label><title>Materials and Methods</title><sec id="S3" sec-type="subjects"><title>Participants</title><p id="P12">To measure long-term shaping of meter perception by music practice, two groups of 26 healthy volunteers recruited from Brussels area (Belgium) participated in this study. The first group consisted of professional musicians (17 females, 5 left-handed, 27.54 ± 6.18 years old, 19.5 ± 5.46 years of musical practice, and 26.75 ± 7.8 hours of musical practice per week - mean ± standard deviation) who were either studying or had graduated from a music conservatory (i.e., bachelor’s or master’s degree) at the time of the experiment. The second group consisted of non-musicians (17 females, 2 left-handed, 25.96 ± 5.53 years old, 0 ± 0 years of musical practice, and 0 ± 0 hours of musical practice per week). Both groups were matched for dance experience (Musicians: 2.34 ± 3.89 years of dance experience and 1.15 ± 2.96 hours of weekly practice; Non-musicians: 0.81 ± 2.00 years of dance experience and 0.60 ± 1.16 hours of weekly practice; both p-values &gt; 0.05). Participants had no history of hearing, neurological or psychiatric disorders, and gave a written informed consent before the start of the experiment. This study was approved by the local ethics committee (ref. 2018-353) and all experiments were performed in accordance with relevant guidelines and regulations.</p></sec><sec id="S4"><title>Auditory Stimuli</title><p id="P13">A set of 14 seed rhythms was created using Matlab (R2020a, <italic>The MathWorks, Natick, USA</italic>; <xref ref-type="fig" rid="F2">Fig. 2</xref>). Each rhythm was based on a 12-interval grid structure made of 200 ms evenly spaced time points and consisted of eight sound events, and four silent events. For each seed rhythm, the arrangement of sound and silent events on the 12-interval grid was manipulated to obtain 2.4 s syncopated (i.e., weakly periodic in the beat layer) rhythms.</p><p id="P14">Sound events consisted in a pure tone of 200 ms duration (10 ms and 20 ms linear ramps up and down). The carrier frequency of the pure tone varied between 150 and 200 Hz ([150.00, 161.19, 173.21, 186.12, 200] Hz) in each successive trial to increase participants’ vigilance due to the change in tone between trials. These relatively low tone frequencies were chosen since low tones have been shown to elicit brain responses with higher signal-to-noise ratios<sup>[<xref ref-type="bibr" rid="R37">37</xref>]</sup>, and stronger meter-related neural responses<sup>[<xref ref-type="bibr" rid="R8">8</xref>]</sup>. To account for the spectral sensitivity of the auditory system<sup>[<xref ref-type="bibr" rid="R38">38</xref>]</sup>, the intensity of each pure tone was equalized to 70dB<sub>A</sub> using a Bruel &amp; Kjaer (type 2250, <italic>Denmark</italic>) sound level meter.</p><p id="P15">To confirm that temporal cues to beat-layer periodicities were weak in these stimuli (i.e., syncopation criterion), we measured the prominence of meter periodicities in the modulation spectrum of each rhythm. This was done by extracting the envelope of the acoustic signal of each 2.4 s rhythmic pattern using a Hilbert transform and subsequently applying a Fast Fourier Transform (FFT). The distribution of energy within the first 12 frequency bins (from the frequency corresponding to the duration of the rhythmic pattern, here 1/2.4 s = 0.41 Hz, to the 11th harmonics corresponding to the period of the grid timepoints, 1/0.2 s = 5 Hz) reflects periodic recurrences in the amplitude modulations of the acoustic signal<sup>[<xref ref-type="bibr" rid="R9">9</xref>,<xref ref-type="bibr" rid="R34">34</xref>,<xref ref-type="bibr" rid="R39">39</xref>,<xref ref-type="bibr" rid="R40">40</xref>]</sup>.</p><p id="P16">From this set of frequencies of interest, frequencies were classified as meter-related when corresponding to the most plausible metric pulses (1.25Hz and harmonics, based on tapping behavior observed in studies using similar rhythms<sup>[<xref ref-type="bibr" rid="R8">8</xref>,<xref ref-type="bibr" rid="R10">10</xref>,<xref ref-type="bibr" rid="R13">13</xref>]</sup>), and as meter-unrelated frequencies otherwise. To measure the relative prominence of meter-related frequencies, amplitudes at all frequencies of interest were converted to z-scores and the z-scores at meter-related frequencies were averaged to serve as an index of their relative prominence in the modulation spectra (see<sup>[<xref ref-type="bibr" rid="R34">34</xref>]</sup> for further details on this approach). All seed rhythms showed negative meter-related z-scores, indicating that the amplitude at meter-related frequencies was, on average, smaller than the average amplitude across all frequencies of interest. In other words, meter-related frequencies did not stand out relative to the whole set of frequencies characterizing the temporal structure of the rhythmic patterns, thus confirming the syncopation criterion.</p><p id="P17">The 14 seed rhythms were then concatenated in three 67.2s sequences that varied in terms of pattern repetition. The “medium pattern repetition” sequence consisted of a 4.8-s rhythmic pattern made of the concatenation of two of the seed rhythms (seed rhythms #5 and 10), which was seamlessly looped 14 times within the sequence. The “long pattern repetition” sequence comprised the same 4.8s pattern from the medium pattern repetition sequence followed by two additional seed rhythms, thus forming a longer rhythmic pattern of 9.6s (seed rhythms #5, 10, 8, 9) which was seamlessly looped 7 times within the sequence. Lastly, the “no pattern repetition” sequence was made by randomly concatenating the 14 seed rhythms to generate the 67.2s sequence (seed rhythms #6, 8, 11, 5, 12, 5, 7, 6, 11, 7 etc.).</p><p id="P18">All sequences were controlled (i) to yield similar meter-related z-scores, (ii) to not present identical rhythms following each other within a pattern, and (iii) so that the no-pattern repetition sequence would not reproduce parts of the pattern from the other two sequences. Note that the label “short pattern repetition” sequence was purposely avoided here, as it better refers to the duration of the seed rhythms (i.e., 2.4-s), which corresponds to the typical duration of rhythms used in previous similar neuroimaging studies<sup>[<xref ref-type="bibr" rid="R8">8</xref>,<xref ref-type="bibr" rid="R10">10</xref>,<xref ref-type="bibr" rid="R13">13</xref>,<xref ref-type="bibr" rid="R41">41</xref>]</sup>. Note also that a condition with repetition of such a 2.4 s rhythmic pattern was not included in the current study to maintain a reasonable experiment duration, and because longer patterns were hypothesized to more effectively target the 6-8 s limit for the implicit learning of rhythmic patterns<sup>[<xref ref-type="bibr" rid="R15">15</xref>,<xref ref-type="bibr" rid="R42">42</xref>]</sup>. Audio files of the stimuli are available on an online repository (<ext-link ext-link-type="uri" xlink:href="https://zenodo.org/records/13870035?preview=1">https://zenodo.org/records/13870035?preview=1</ext-link>).</p><p id="P19">To test whether low-level sensory processing from early stages of the auditory pathway could explain substantial enhancement of meter frequencies as compared to the input<sup>[<xref ref-type="bibr" rid="R43">43</xref>]</sup>, all three sequences were analyzed using a cochlear model to estimate the sound representation at the level of auditory nerve fibers (as implemented in the Auditory Toolbox for Matlab, with 128 characteristic frequencies distributed from 130 to 16.000Hz<sup>[<xref ref-type="bibr" rid="R44">44</xref>]</sup>). The output of this model was averaged across the cochlear channels, and the signal was transformed into the frequency domain by applying an FFT. The meter-related z-score was then computed for each stimulus condition to confirm the lack of prominent meter-related frequencies, thus excluding possible low-level confounds in the EEG results.</p></sec><sec id="S5"><title>Experimental design</title><p id="P20">The experiment took place in a single session. Each participant listened to all three sequences presented in blocks, and the block corresponding to the “long pattern repetition” condition was presented twice (<xref ref-type="fig" rid="F3">Fig. 3</xref>) to test for effects of short-term context. Another way to test for possible effects of short-term context was to compare two different fixed orders of the blocks, with the “no pattern repetition” condition placed either at the beginning (i.e., no prior context), or at the end of the experiment (i.e., maximum prior context). Within each group of participants (musicians vs. non-musicians), half of each group was assigned to a different condition order, thus yielding a mixed factorial design.</p><p id="P21">Each of the four resulting blocks consisted of 10 EEG trials followed by 3 tapping trials. During EEG trials, participants were seated in a comfortable chair and were asked to listen carefully to the rhythmic sequences while fixating a visual target placed in front of them and avoiding any unnecessary head or body movements. To maintain participants’ attention to the temporal properties of the stimuli during EEG trials, participants were asked to report whether they heard a tempo change, and if so the direction of this change, at the end of each trial<sup>[<xref ref-type="bibr" rid="R8">8</xref>,<xref ref-type="bibr" rid="R35">35</xref>,<xref ref-type="bibr" rid="R44">44</xref>]</sup>. These tempo changes were introduced in an additional sequence snippet appended to the end of 6 out of 10 trials (3 accelerations, 3 decelerations, and 4 trials without tempo change). Modulations of the tempo were implemented following a 4.8-s long cosine waveform with a fixed magnitude for musicians (minimum and maximum grid intervals of 188.7 and 217.4 ms respectively), and an adaptive two-down, one-up staircase method<sup>[<xref ref-type="bibr" rid="R45">45</xref>]</sup> (grid intervals starting at 169.5 for accelerations and 238.1 ms for decelerations with adaptative steps of 8ms) for non-musicians to account for their reduced ability to detect tempo modulations as compared to musicians<sup>[<xref ref-type="bibr" rid="R46">46</xref>]</sup>. Note that these additional snippets were excluded from further analyses, thus exclusively leaving responses to rhythmic input with a steady tempo for the analyses. Before the start of the experiment, participants were familiarized with several examples of these tempo changes applied to syncopated rhythms that were not used in the actual experiment.</p><p id="P22">For the tapping trials, participants were instructed to tap the regular pulse that was spontaneously perceived while listening to the stimulus by producing up and down movements with the index finger of their preferred hand on a custom-built response box (i.e., the exact instructions were: “<italic>Tap the index finger of your preferred hand in time with the regular pulse that you perceive when listening to the rhythm, as if you were clapping your hands on the music during a live performance</italic>”). The experimenter remained in the recording room with the participant at all times to monitor compliance with the procedure and instructions.</p></sec><sec id="S6"><title>Tapping Recording and Analysis</title><p id="P23">Tapping responses were recorded using a custom-built response box made of a hard tapping surface (i.e. producing auditory feedback, mitigated by the ear inserts used to deliver acoustic inputs simultaneously) and a pressure sensor underneath. From the tap time series, intertap intervals (ITIs) were calculated as durations between successive taps. For each trial, participant and condition, the median ITI was taken as an index of the perceived beat period<sup>[<xref ref-type="bibr" rid="R2">2</xref>]</sup>.</p><p id="P24">Additionally, we converted the tap time series to a sequence of phases relative to the predicted beat-layer pulse frequency, determined by the meter periodicity closest to each participant’s median ITI. The circular mean was then taken as an index of tapping stability, with values ranging from 0 to 1, respectively, indicating low and high synchronization with the rhythmic input<sup>[<xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R47">47</xref>]</sup>.</p></sec><sec id="S7"><title>EEG Recording and Preprocessing</title><p id="P25">The EEG was recorded using a Biosemi Active-Two system (Biosemi, Amsterdam, Netherlands) with 64 Ag-AgCl electrodes placed on the scalp according to the international 10/20 system, and two additional electrodes placed on the mastoids. EEG recordings were preprocessed using Matlab (R2020a, The MathWorks, Natick, USA) and Letswave (<ext-link ext-link-type="uri" xlink:href="http://letswave.org">http://letswave.org</ext-link>).</p><p id="P26">First, signals were digitized at a sampling rate of 512 Hz. To remove slow drifts, EEG recordings were high-pass filtered offline at 0.1 Hz (4th order Butterworth filter), segmented from 0 to 67.2 s relative to the trial onset (excluding any appended snippets with tempo changes relevant for the perceptual task) and re-referenced to the average of mastoid electrodes to increase the signal-to-noise ratio of neural responses to acoustic rhythms in frontocentral channels, which would be later selected<sup>[<xref ref-type="bibr" rid="R48">48</xref>,<xref ref-type="bibr" rid="R49">49</xref>]</sup>.</p><p id="P27">Excessively noisy channels were visually selected and linearly interpolated across all trials and conditions, separately for each participant (maximum 3 channels interpolated in 2 participants). Artifacts attributed to eye blinks and horizontal eye movements were identified and removed using independent component analysis<sup>[<xref ref-type="bibr" rid="R50">50</xref>]</sup> based on visual inspection of their typical waveform shape and topographic distribution. For each participant and condition separately, the epochs were averaged across trials in the time-domain to reduce the contribution of background noise that was not time-locked to the stimuli, thus improving the signal-to-noise ratio of the EEG activity<sup>[<xref ref-type="bibr" rid="R51">51</xref>]</sup>.</p><p id="P28">The signal was then averaged across a set of 9 frontocentral channels (‘F1’, ‘Fz’, ‘F2’, ‘FC1’, ‘FCz’, ‘FC2’, ‘C1’, ‘Cz’, ‘C2’ in the 10-20 international system) based on prior observations of brain responses to rhythmic sound patterns being predominantly captured by these channels<sup>[<xref ref-type="bibr" rid="R8">8</xref>,<xref ref-type="bibr" rid="R10">10</xref>,<xref ref-type="bibr" rid="R35">35</xref>]</sup>. Finally, the averaged waveforms were transformed into the frequency domain using an FFT, and baseline corrected by subtracting the average amplitude at the 2nd to the 5th neighboring bin on both sides relative to each frequency bin to locally correct for the background noise in the EEG signal<sup>[<xref ref-type="bibr" rid="R51">51</xref>]</sup>.</p></sec><sec id="S8"><title>Relative amplitude of the meter-related neural activity in the frequency domain</title><p id="P29">A vast majority of previous EEG studies using a frequency-tagging approach to investigate the neural representation of meter have used repeated rhythmic patterns<sup>[<xref ref-type="bibr" rid="R8">8</xref>,<xref ref-type="bibr" rid="R10">10</xref>,<xref ref-type="bibr" rid="R11">11</xref>,<xref ref-type="bibr" rid="R13">13</xref>,<xref ref-type="bibr" rid="R25">25</xref>,<xref ref-type="bibr" rid="R26">26</xref>]</sup>. In such a context, the selection of frequencies of interest is relatively straightforward, and directly based on the fact that the response to a repeated rhythmic pattern is elicited repeatedly with a fixed a priori known period. Hence, any response reliably elicited by the pattern should itself repeat at the rate of pattern repetition in the sensory input. Such a systematically repeated response can be conveniently isolated in the frequency domain, since it only contains peaks at frequencies corresponding to the pattern repetition rate and harmonics.</p><p id="P30">Critically, the distribution of amplitudes across these frequencies of interest is related to the shape of the response elicited <italic>within</italic> each repetition of the rhythmic pattern. Moreover, the degree of periodic recurrence of the response (i.e., meter periodicities) within each pattern cycle is reflected in the relative prominence of amplitudes at a subset of frequencies of interest (i.e., meter-related frequencies), which correspond to the rate of this nested recurrence and its harmonics. To measure such prominence at the rate of the perceived metric periodicities (as directly informed by the group-averaged median ITI), the amplitudes across all frequencies of interest are typically standardized as z-scores and averaged across meter-related frequencies (i.e. frequencies corresponding to the rates and harmonics of the metric pulses). Notably, the resulting mean z-score quantifies the amplitude at meter-related frequencies <italic>relative to</italic> the amplitude at other, meter-unrelated frequencies, which are expected to emerge in the response spectrum due to stimulus repetition, but do not capture meter periodicities. The mean meter-related z-score thus constitutes a normalized index of the prominence of meter periodicities in the EEG signal<sup>[<xref ref-type="bibr" rid="R9">9</xref>,<xref ref-type="bibr" rid="R12">12</xref>,<xref ref-type="bibr" rid="R34">34</xref>,<xref ref-type="bibr" rid="R52">52</xref>]</sup>.</p><p id="P31">Along this line, we determined a set of meter-related frequencies based on the tapping, namely the observed convergent ITIs across conditions. However, determining meter-unrelated frequencies based on the harmonics of the pattern repetition rate would be ill-suited here, as each condition contained a different repetition rate (up to a complete absence of pattern repetition), thus precluding comparisons across conditions due to the profound differences in the number of meter-unrelated frequencies. Yet, meter-unrelated frequencies are key to standardizing response amplitudes at meter-related frequencies, thus capturing the prominence of meter periodicities irrespective of unit and scale<sup>[<xref ref-type="bibr" rid="R34">34</xref>]</sup>. Therefore, we adopted a new approach to selecting frequencies of interest and overcoming this imbalance without any prior assumption about possible meter-unrelated frequencies.</p><p id="P32">First, we identified the most prominent peaks from the group-averaged EEG spectrum of each condition. These were obtained by running the <italic>findpeak</italic> function implemented in Matlab, with a minimum peak prominence set as the mean plus two standard deviations of the spectrum between 0 and 30Hz. This measure was computed separately for each group of participants, and only frequencies present in both groups were retained. The same procedure was applied to the modulation spectrum of each of the three rhythmic sequences, and the series of prominent peaks from the three stimuli were merged with those extracted from their corresponding EEG spectrum to better capture the input/output transformation. This way, small peaks at the stimulus level that are boosted in the EEG, or otherwise, prominent peaks in the input that are reduced in the output are both considered as frequencies of interest.</p><p id="P33">Furthermore, peaks obtained below 1.25 Hz (i.e., pulse period corresponding to 4 events, or 800ms) were excluded from further analysis since they were expected to be particularly affected by 1/f noise in the EEG signal. In addition, peaks at 5Hz and harmonics were also excluded as they are likely to be mostly driven by the shape of the average response to individual sound events making up the stimuli.</p><p id="P34">The steps describe thus far could, in principle, yield a rather different number of frequencies of interest across conditions. To avoid such imbalance, we retained only the smallest number of frequencies of interest among conditions by (i) selecting those with the largest amplitude in the modulation spectrum of each stimulus, and (ii) forcing the inclusion of meter-related frequencies corresponding to median ITIs tapped by participants (i.e., 1.25 and 2.5 Hz), resulting in 3 meter-related and 24 meter-unrelated frequencies.</p><p id="P35">Finally, the amplitudes of all these frequencies of interest were converted into z-scores and averaged separately for the meter-related frequencies (frequencies corresponding to the period of the median ITIs, and harmonics – 1.25, 2.5 and 3.75 Hz) and meter-unrelated frequencies (all non-meter-related frequencies of interest). As expected, this procedure yielded a selection of different meter-unrelated frequencies across conditions. However, the fact that this selection was conducted within an identical spectral range and was restricted to an identical number of these frequencies across conditions ensured fair comparison of the relative prominence of meter-related frequencies across conditions.</p></sec><sec id="S9"><title>Statistical analysis</title><p id="P36">Statistical analyses were performed using R (version 4.2.1) with the significance level set at p &lt; 0.05. To account for the absence of normal distribution in behavioral results, non-parametric Mann-Whitney tests were used for comparisons across the two groups, and between the two condition orders, and a non-parametric Friedman test was used to compare across conditions. For neural responses, a three-way mixed ANOVA was conducted to evaluate the main effect and interaction of the factors condition, musical expertise, and condition order. Where relevant, the Greenhouse–Geisser correction was used to correct for violations of sphericity in the performed ANOVAs. Post-hoc comparisons were carried out using paired t-tests, and a false discovery rate (FDR) correction was applied when needed to adjust for multiple comparisons.</p></sec></sec><sec id="S10" sec-type="results"><title>Results</title><sec id="S11"><title>Behavioral results</title><p id="P37"><xref ref-type="fig" rid="F4">Figure 4A</xref> depicts the distribution of inter-tap intervals (ITIs) for both groups of participants across all four conditions. The observed convergent median ITIs at the group level and across conditions were thus used to inform the selection of meter-related frequencies, corresponding here to 1.25 Hz (i.e., 1/800 ms, or 4 times the grid interval) and harmonics (thus including also 2.5 Hz, corresponding to 1/400 ms, or 2 times the grid interval) for the frequency analysis of neural responses, in line with previous studies<sup>[<xref ref-type="bibr" rid="R13">13</xref>,<xref ref-type="bibr" rid="R35">35</xref>]</sup>.</p><p id="P38">To evaluate behavioral responses, the tapping stability (i.e., vector strength of the circular asynchrony between tapping onsets and the nearest hypothetical pulse position) was compared between groups, condition orders, and across conditions (<xref ref-type="fig" rid="F4">Fig. 4B</xref>). Professional musicians showed higher stability than non-musicians in all conditions (Medium Pattern Repetition: W = 564, p = 1.442e-05, η<sup>2</sup> = 0.574 ; Long Pattern Repetition #1: W = 578, p = 3.304e-06, η<sup>2</sup> = 0.609; Long Pattern Repetition #2: W = 574, p = 5.111e-06, η<sup>2</sup> = 0.599; No Pattern Repetition: W = 580, p = 2.644e-06, η<sup>2</sup> = 0.614). However, within-group comparisons across conditions, and condition orders showed no significant effect (Condition, Musicians: χ<sup>2</sup>(3) = 1.57, p = 0.67; Condition, Non-musicians: χ<sup>2</sup>(3) = 3, p = 0.39; Condition Order: W = 5784, p = 0.387), though some non-musicians appeared to strongly benefit from the order offering maximum prior context, leading to a bimodal distribution of tapping asynchronies and a close to significant interaction (Order, Non-musicians: W = 1608, p = 0.09).</p><p id="P39">Together, these results suggest that degradation of pattern repetition, as well as prior context, did not play a significant role in tapping performance. Only musical expertise had a positive effect, as observed by the more clustered ITIs and increased tapping stability measured in the musicians group.</p></sec><sec id="S12"><title>Frequency Analysis of the Stimulus and EEG responses</title><p id="P40"><xref ref-type="fig" rid="F5">Figure 5</xref> displays the modulation spectra across stimulus conditions obtained from the cochlear model (top row) and EEG responses (musicians and non-musicians in the middle and bottom rows respectively). As expected, the degradation of pattern recurrence causes the acoustic energy (and the corresponding output of the cochlear model) to be spread across a larger number of frequency bins that is directly determined by the duration of the repeated pattern in the sequence (<xref ref-type="fig" rid="F5">Fig. 5</xref>, top row), hence the necessity for a proper adjustment of a method for selecting frequencies of interest as described in the <xref ref-type="sec" rid="S2">materials and methods</xref> section. After running the findpeak function on the modulation spectrum of each stimulus condition and their corresponding group-averaged EEG spectra, the medium pattern repetition condition showed the fewest prominent peaks (n=27), which determined the number of frequencies of interest that would then be selected in each condition. After classifying 1.25Hz and harmonics as meter-related frequencies (<xref ref-type="fig" rid="F5">Fig. 5</xref> in red, n=3, as informed by tapping results), and all other frequencies of interest as meter-unrelated (<xref ref-type="fig" rid="F5">Fig.5</xref> in blue, n=24), the modulation spectrum of the cochlear model responses showed a weaker prominence of meter-related frequencies compared to meter-unrelated frequencies. This observation was confirmed by the negative meter-related z-score obtained in all four conditions (<xref ref-type="fig" rid="F6">Fig. 6</xref>, horizontal black line), indicating that the rhythmic sequences contained little cues to the metric pulses as tapped by most participants.</p><p id="P41">One-sample one-tailed t-tests were then performed to estimate whether meter-related frequencies were more prominent than predicted by the cochlear model, which could not be explained by lower-level tracking of prominent acoustic features of the stimuli<sup>[<xref ref-type="bibr" rid="R12">12</xref>]</sup> (<xref ref-type="fig" rid="F6">Fig. 6</xref>, black horizontal line). Both groups showed a selective enhancement of meter-related frequencies in the EEG in all four conditions and for both condition orders (all p-values &lt; 0.05, FDR corrected, <xref ref-type="table" rid="T1">Table 1</xref>). These results align with previous work showing a periodization of the input, even when the input shows weak meter periodicities in its modulation spectrum<sup>[<xref ref-type="bibr" rid="R9">9</xref>,<xref ref-type="bibr" rid="R11">11</xref>,<xref ref-type="bibr" rid="R12">12</xref>]</sup>. However, these results go one step further by showing that the processes underlying periodization persist in response to stimuli with degraded periodicities at slower timescales, where periodic recurrences associated with pattern repetitions typically occur.</p><p id="P42">Finally, and more central to the aim of the current study, we estimated the effect of these slow temporal recurrences, along with effects of musical expertise and prior context, on the amplitude of EEG responses at meter-related frequencies (<xref ref-type="fig" rid="F6">Fig. 6</xref>). The three-way mixed ANOVA with the factors condition, group, and condition order showed a main effect of condition (F(3,48) = 2.711, p = 0.0470), with larger meter-related z-scores observed in the medium-pattern repetition compared to the first presentation of the long-pattern repetition condition (t(51) = 3.411, p = 0.006, η<sup>2</sup> = 0.472), as well as a main effect of condition order (F(1,48) = 4.828, p = 0.033), with greater enhancement of meter-related frequencies in the order that provided maximum prior context (i.e. starting with the medium pattern repetition condition; t(103) = 2.553, p = 0.006, η<sup>2</sup> = 0.250). However, no main effect or interaction was found for the group factor, suggesting that listeners’ neural responses may be similar irrespective of levels of musical expertise.</p></sec></sec><sec id="S13" sec-type="discussion"><title>Discussion</title><p id="P43">The current study investigated the role of rhythmic pattern repetition in meter processing. As hypothesized, pattern repetition was found to strengthen neural representation of the meter. Moreover, we observed stronger neural emphasis of the meter periodicities when the highly repetitive stimuli were presented first, indicating a carry-over effect from the directly preceding condition, which offered maximal prior short-term context in terms of pattern repetition<sup>[<xref ref-type="bibr" rid="R35">35</xref>]</sup>.</p><sec id="S14"><title>Periodization of rhythmic input in neural activity</title><p id="P44">Results revealed that the relative prominence of meter frequencies was enhanced in neural activity as compared to their relative prominence in the input. Notably, this enhancement was observed in all conditions, irrespective of the degree of pattern repetition in the input. Crucially, these neural responses cannot be explained by a mere tracking of prominent periodicities of the rhythmic input, as all stimuli were purposely designed to lack prominent temporal cues in the intermediate timescale corresponding to the beat layer (i.e., syncopated rhythms). This periodization of the rhythmic input observed in neural activity as captured with EEG thus arguably reflects processes related to meter perception, beyond lower-level sensory tracking of prominent features of the input.</p><p id="P45">Most importantly, the selective enhancement of meter frequencies was observed even in the non-repeating condition, which only offered a low degree of periodic recurrence in the input. This result suggests that periodic recurrence, even when limited only to faster timescales, is already sufficient to elicit significant neural emphasis of the meter. Together, these findings thus reveal processes through which the brain would leverage periodic recurrence restricted to a narrow timescale in the rhythmic input, to map a rich metric template covering wider timescales<sup>[<xref ref-type="bibr" rid="R20">20</xref>,<xref ref-type="bibr" rid="R21">21</xref>,<xref ref-type="bibr" rid="R53">53</xref>]</sup>. This multiscale temporal scaffolding would thus play a critical role in the neural periodization of rhythmic input that could be experienced as the meter<sup>[<xref ref-type="bibr" rid="R9">9</xref>]</sup>.</p><p id="P46">To some extent, these findings may be related to the phenomenon of subjective meter (or “tick tock” effect), which refers to perception of periodic accents in unaccented isochronous rhythms<sup>[<xref ref-type="bibr" rid="R19">19</xref>,<xref ref-type="bibr" rid="R20">20</xref>,<xref ref-type="bibr" rid="R54">54</xref>]</sup>. More specifically, it could be hypothesized that, in the no-pattern repetition condition, interpolation of the prominent periodic recurrences at the fastest inter-onset-interval timescale served as a basis to map an internal representation of a periodic structure at slower rates, thus yielding a set of nested periodic pulses. Previous studies<sup>[<xref ref-type="bibr" rid="R55">55</xref>,<xref ref-type="bibr" rid="R56">56</xref>]</sup> on Western populations have shown a clear tendency towards tapping rates corresponding to groupings of 2, 4, and 8 notes (compared to groupings of 3, 5, 6, and 7 notes). Corroborating these observations, the current behavioral results showed a convergence towards duple meters.</p><p id="P47">However, the current study provides a nuanced differentiation from previous findings. Specifically, we found convergent tapping across participants at a rate corresponding to two times the fastest inter-onset intervals (<xref ref-type="fig" rid="F4">Fig. 4</xref>). This tapping rate is thus overall faster, as compared to previous studies which mostly found tapping rates corresponding to four times these intervals in response to repeated rhythms at the same tempo<sup>[<xref ref-type="bibr" rid="R8">8</xref>,<xref ref-type="bibr" rid="R10">10</xref>]</sup>. The faster rate observed here thus likely reflects an attraction toward the most prominent periodic recurrence available in the sensory input, here located at the fastest timescale.</p></sec><sec id="S15"><title>Discrepancy between neural and behavioral responses</title><p id="P48">The behavioral measures showed less stable sensorimotor synchronization performances in non-musicians as compared to musicians, consistent with previous research<sup>[<xref ref-type="bibr" rid="R2">2</xref>]</sup>. This cross-group difference in the ability to tap a periodic beat could appear contradictory with the neural measures which did not show such a contrast between the two groups. Rather, the neural measures showed “periodization” of the input that was significantly influenced by pattern repetition and prior context, but not musical expertise.</p><p id="P49">Yet, these discrepancies between behavioral and neural measures actually align with the view of a default periodization of the rhythmic input in neural activity, which would contribute to meter perception irrespective of the musical expertise. Corroborating this view, previous research using intracerebral recordings in humans has provided evidence for such a periodization of rhythmic inputs at the earliest cortical stage of auditory processing<sup>[<xref ref-type="bibr" rid="R26">26</xref>]</sup> (i.e., Heschl’s gyrus). These findings are also consistent with previous research showing periodization of syncopated rhythms in the EEG of human adults even when the attention is focused away from the rhythmic input<sup>[<xref ref-type="bibr" rid="R43">43</xref>]</sup>, as well as in the EEG of 5-6 months old human infants, despite relative immaturity of higher-level associative and sensorimotor cortices at this developmental stage<sup>[<xref ref-type="bibr" rid="R41">41</xref>]</sup>.</p><p id="P50">Taken together, these studies suggest that the capacity to move to a periodic beat does not only rely on a default periodized neural representation of the rhythmic input as produced in the primary auditory cortex. Rather, this ability would also require strong functional coupling between auditory and higher-level associative and sensorimotor cortices, allowing this periodized representation to guide movement timing, thus in line with the functional definition of meter in music<sup>[<xref ref-type="bibr" rid="R57">57</xref>,<xref ref-type="bibr" rid="R58">58</xref>]</sup>. In return, these processes would be reinforced through self-entrainment, especially in the context of syncopated rhythms. The critical role of such a functional coupling between these brain regions is further corroborated by the evidence that body movement can shape the internal representation of meter<sup>[<xref ref-type="bibr" rid="R6">6</xref>,<xref ref-type="bibr" rid="R44">44</xref>,<xref ref-type="bibr" rid="R59">59</xref>]</sup>.</p><p id="P51">In addition to the cross-group difference discussed above, we also found a cross-task difference. The positive effect of pattern repetition on the prominence of meter periodicities was found in the neural but not tapping response. This difference could be attributed to differences in task demands between behavioral and neural measurements involving, or not, explicitly instructed movement to the beat. Since the ability to move to the beat tends to be particularly developed in highly trained musicians due to lifelong music practice<sup>[<xref ref-type="bibr" rid="R36">36</xref>,<xref ref-type="bibr" rid="R60">60</xref>]</sup>, it could be hypothesized that musicians showed a ceiling effect in their behavioral responses, thus masking potential effects of pattern repetition. In non-musicians, in contrast, the overall weaker sensorimotor synchronization performances<sup>[<xref ref-type="bibr" rid="R2">2</xref>]</sup>, and the relatively low recurrence at slow supra-second timescales compared to previous similar studies<sup>[<xref ref-type="bibr" rid="R8">8</xref>,<xref ref-type="bibr" rid="R10">10</xref>,<xref ref-type="bibr" rid="R43">43</xref>]</sup>, may thus have proven too limited at enabling consistent motor entrainment to the beat, thus yielding a floor effect.</p></sec><sec id="S16"><title>Pattern repetition in music: capitalizing on implicit learning to foster meter perception</title><p id="P52">The current study shows that repetition of rhythmic patterns facilitates the internal representation of meter. This facilitation could stem from the capacity of the brain to subdivide the slow, supra-second periodicities provided by repetition of the rhythmic patterns into faster pulses, that is, a process of interpolation. Yet, this effect of pattern repetition is of a relatively small effect size. This relatively small effect could be explained by the fact that the slow timescales used here may have been already close to the limit beyond which periodic recurrence no longer facilitates meter perception<sup>[<xref ref-type="bibr" rid="R4">4</xref>]</sup> (but see<sup>[<xref ref-type="bibr" rid="R61">61</xref>]</sup> for extremely long timescale meter perception based on explicit theorization and training).</p><p id="P53">However, the human brain has proven to be remarkably proficient at recognizing and learning recurring patterns over time<sup>[<xref ref-type="bibr" rid="R42">42</xref>,<xref ref-type="bibr" rid="R62">62</xref>,<xref ref-type="bibr" rid="R63">63</xref>]</sup>. This phenomenon, referred to as implicit learning occurs on a short timescale already<sup>[<xref ref-type="bibr" rid="R64">64</xref>]</sup>, and does not require any specific training (see<sup>[<xref ref-type="bibr" rid="R65">65</xref>]</sup> for a review of implicit learning in the context of music). Supporting this, effects of implicit learning were observed with auditory stimuli lasting up to 8.4 seconds<sup>[<xref ref-type="bibr" rid="R42">42</xref>]</sup>, thus comprising the duration of our “medium pattern repetition” condition (i.e. 4.8 s) used in the current study. The current results thus open to future research comparing neural and behavioral responses across a finer-grained range of durations of pattern repetition. For example, starting with approximately 2 s long patterns (as in<sup>[<xref ref-type="bibr" rid="R8">8</xref>,<xref ref-type="bibr" rid="R10">10</xref>,<xref ref-type="bibr" rid="R26">26</xref>,<xref ref-type="bibr" rid="R41">41</xref>,<xref ref-type="bibr" rid="R43">43</xref>]</sup>) could further clarify the role of implicit learning processes in fostering meter perception.</p></sec></sec><sec id="S17" sec-type="conclusions"><title>Conclusion</title><p id="P54">Despite being a core component of music worldwide<sup>[<xref ref-type="bibr" rid="R24">24</xref>,<xref ref-type="bibr" rid="R28">28</xref>–<xref ref-type="bibr" rid="R30">30</xref>]</sup>, the repetition of musical patterns (melodic phrases, rhythmic figures, etc.) has often been overlooked in studies of rhythm processing. The current experiment aims to fill this gap by showing that the repetition of rhythmic patterns plays a critical role in fostering neural representations of temporal structures that can be experienced as the meter. Importantly, and beyond implications in music contexts, the obtained results reveal the capacity of the human brain to exploit slow periodicities available in the rhythmic input in the supra-second scale, to map internal representations of faster periodicities in sub-second timescales through interpolation. By highlighting the multiscale nature of temporal processes involved in rhythm processing, this study contributes to progress in our understanding of high-level perceptual processes allowing groups of individuals to coordinate interaction and communication in a multitude of fundamentally human joint actions ranging from music and dance performance to ritual, work, and play.</p></sec></body><back><ack id="S18"><title>Acknowledgements</title><p>The study is receiving financial support from the European Research Council (ERC Starting Grant Rhythm and Brains, ref. 801872). We would like to thank CATL, the technical support team of the Institute of Neurosciences at the Université Catholique de Louvain for providing the custom-built tapping box used in this experiment.</p></ack><sec id="S19" sec-type="data-availability"><title>Code and data availability</title><p id="P55">Preprocessed and analyzed data are available on a public Zenodo repository (<ext-link ext-link-type="uri" xlink:href="https://zenodo.org/records/13870035?preview=1">https://zenodo.org/records/13870035?preview=1</ext-link>). Additionally, anonymized raw data is available upon request.</p><p id="P56">The scripts used to conduct the analyses and figures are available on a public GitHub repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/Manu-RnB/pattern_repetiti_on.git">https://github.com/Manu-RnB/pattern_repetiti_on.git</ext-link>).</p></sec><fn-group><fn id="FN1" fn-type="con"><p id="P57"><bold>Author Contributions</bold></p><p id="P58">EC, TL and SN designed the study, and EC and SB collected and analyzed the data. EC, TL, RP and SN contributed to writing and editing the manuscript. All authors reviewed the manuscript.</p></fn><fn id="FN2" fn-type="conflict"><p id="P59"><bold>Competing interests</bold></p><p id="P60">The authors have no competing financial interests to declare.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehr</surname><given-names>SA</given-names></name><etal/></person-group><article-title>Universality and diversity in human song</article-title><source>Science</source><year>2019</year><volume>366</volume><pub-id pub-id-type="pmcid">PMC7001657</pub-id><pub-id pub-id-type="pmid">31753969</pub-id><pub-id pub-id-type="doi">10.1126/science.aax0868</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Repp</surname><given-names>BH</given-names></name><name><surname>Su</surname><given-names>YH</given-names></name></person-group><article-title>Sensorimotor synchronization: A review of recent research (2006-2012)</article-title><source>Psychon Bull Rev</source><year>2013</year><volume>20</volume><fpage>403</fpage><lpage>452</lpage><pub-id pub-id-type="pmid">23397235</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cohn</surname><given-names>R</given-names></name></person-group><chapter-title>Meter</chapter-title><person-group person-group-type="editor"><name><surname>Alexander</surname><given-names>Rehding</given-names></name><name><surname>Rings</surname><given-names>Steven</given-names></name></person-group><source>The Oxford Handbook of Critical Concepts in Music Theory</source><publisher-name>Oxford Academic</publisher-name><year>2020</year><fpage>207</fpage><lpage>233</lpage></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>London</surname><given-names>J</given-names></name></person-group><source>Hearing in Time: Psychological Aspects of Musical Meter</source><publisher-name>Oxford University Press</publisher-name><year>2012</year></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Danielsen</surname><given-names>A</given-names></name></person-group><source>Presence and Pleasure: The Funk Grooves of James Brown and Parliament</source><publisher-name>Wesleyan University Press</publisher-name><year>2006</year></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fitch</surname><given-names>WT</given-names></name></person-group><article-title>Dance, music, meter and groove: A forgotten partnership</article-title><source>Front Hum Neurosci</source><year>2016</year><volume>10</volume><pub-id pub-id-type="pmcid">PMC4771755</pub-id><pub-id pub-id-type="pmid">26973489</pub-id><pub-id pub-id-type="doi">10.3389/fnhum.2016.00064</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Large</surname><given-names>EW</given-names></name><name><surname>Herrera</surname><given-names>JA</given-names></name><name><surname>Velasco</surname><given-names>MJ</given-names></name></person-group><article-title>Neural networks for beat perception in musical rhythm</article-title><source>Front Syst Neurosci</source><year>2015</year><volume>9</volume><pub-id pub-id-type="pmcid">PMC4658578</pub-id><pub-id pub-id-type="pmid">26635549</pub-id><pub-id pub-id-type="doi">10.3389/fnsys.2015.00159</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lenc</surname><given-names>T</given-names></name><name><surname>Keller</surname><given-names>PE</given-names></name><name><surname>Varlet</surname><given-names>M</given-names></name><name><surname>Nozaradan</surname><given-names>S</given-names></name></person-group><article-title>Neural tracking of the musical beat is enhanced by low-frequency sounds</article-title><source>Proc Natl Acad Sci</source><year>2018</year><volume>115</volume><fpage>8221</fpage><lpage>8226</lpage><pub-id pub-id-type="pmcid">PMC6094140</pub-id><pub-id pub-id-type="pmid">30037989</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1801421115</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lenc</surname><given-names>T</given-names></name><etal/></person-group><article-title>Mapping between sound, brain and behaviour: Four-level framework for understanding rhythm processing in humans and non-human primates</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><year>2021</year><volume>376</volume><pub-id pub-id-type="pmcid">PMC8380981</pub-id><pub-id pub-id-type="pmid">34420381</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2020.0325</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nozaradan</surname><given-names>S</given-names></name><name><surname>Peretz</surname><given-names>I</given-names></name><name><surname>Mouraux</surname><given-names>A</given-names></name></person-group><article-title>Selective neuronal entrainment to the beat and meter embedded in a musical rhythm</article-title><source>Journal of Neuroscience</source><year>2012</year><volume>32</volume><fpage>17572</fpage><lpage>17581</lpage><pub-id pub-id-type="pmcid">PMC6621650</pub-id><pub-id pub-id-type="pmid">23223281</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3203-12.2012</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tal</surname><given-names>I</given-names></name><etal/></person-group><article-title>Neural entrainment to the beat: The “missing-pulse” phenomenon</article-title><source>Journal of Neuroscience</source><year>2017</year><volume>37</volume><fpage>6331</fpage><lpage>6341</lpage><pub-id pub-id-type="pmcid">PMC5490067</pub-id><pub-id pub-id-type="pmid">28559379</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2500-16.2017</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nozaradan</surname><given-names>S</given-names></name><name><surname>Keller</surname><given-names>PE</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name><name><surname>Mouraux</surname><given-names>A</given-names></name></person-group><article-title>EEG frequency-tagging and input–output comparison in rhythm perception</article-title><source>BrainTopography</source><year>2018</year><volume>31</volume><fpage>153</fpage><lpage>160</lpage><pub-id pub-id-type="pmid">29127530</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nozaradan</surname><given-names>S</given-names></name><name><surname>Schönwiesner</surname><given-names>M</given-names></name><name><surname>Keller</surname><given-names>PE</given-names></name><name><surname>Lenc</surname><given-names>T</given-names></name><name><surname>Lehman</surname><given-names>A</given-names></name></person-group><article-title>Neural bases of rhythmic entrainment in humans: critical transformation between cortical and lower-level representations of auditory rhythm</article-title><source>Eur J Neurosci</source><year>2018</year><volume>47</volume><fpage>321</fpage><lpage>332</lpage><pub-id pub-id-type="pmid">29356161</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miyake</surname><given-names>Y</given-names></name><name><surname>Onishi</surname><given-names>Y</given-names></name><name><surname>Pöppel</surname><given-names>E</given-names></name></person-group><article-title>Two Types of Anticipation in Synchronization Tapping</article-title><source>Acta Neurobiol Exp</source><year>2004</year><volume>64</volume><pub-id pub-id-type="pmid">15283483</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>London</surname><given-names>J</given-names></name></person-group><article-title>Cognitive Constraints on Metric Systems: Some Observations and Hypotheses</article-title><source>Music Percept</source><year>2002</year><volume>19</volume><fpage>529</fpage><lpage>550</lpage></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polak</surname><given-names>R</given-names></name></person-group><article-title>The lower limit for meter in dance drumming from West Africa</article-title><source>Empirical Musicology Review</source><year>2017</year><volume>12</volume><fpage>205</fpage><lpage>226</lpage></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Repp</surname><given-names>BH</given-names></name></person-group><article-title>Rate limits in sensorimotor synchronization with auditory and visual sequences: The synchronization threshold and the benefits and costs of interval subdivision</article-title><source>J Mot Behav</source><year>2003</year><volume>35</volume><fpage>355</fpage><lpage>370</lpage><pub-id pub-id-type="pmid">14607773</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Repp</surname><given-names>BH</given-names></name><name><surname>Doggett</surname><given-names>R</given-names></name></person-group><article-title>Tapping to a very slow beat: A comparison of musicians and nonmusicians</article-title><source>Music Percept</source><year>2007</year><volume>24</volume><fpage>367</fpage><lpage>376</lpage></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Repp</surname><given-names>BH</given-names></name></person-group><article-title>Multiple temporal references in sensorimotor synchronization with metrical auditory sequences</article-title><source>Psychol Res</source><year>2008</year><volume>72</volume><fpage>79</fpage><lpage>98</lpage><pub-id pub-id-type="pmid">16786353</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Large</surname><given-names>EW</given-names></name></person-group><chapter-title>Resonating to musical rhythm: Theory and experiment</chapter-title><person-group person-group-type="editor"><name><surname>Grondin</surname><given-names>S</given-names></name></person-group><source>The psychology of time</source><year>2008</year></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Povel</surname><given-names>D-J</given-names></name></person-group><article-title>Internal Representation of Simple Temporal Patterns</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>1981</year><volume>7</volume><pub-id pub-id-type="pmid">6452500</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Longuet-Higgins</surname><given-names>HC</given-names></name><name><surname>Lee</surname><given-names>CS</given-names></name></person-group><article-title>The rhythmic interpretation of monophonic music</article-title><source>Music Percept</source><year>1984</year><volume>1</volume><fpage>424</fpage><lpage>441</lpage></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fitch</surname><given-names>WT</given-names></name><name><surname>Rosenfeld</surname><given-names>AJ</given-names></name></person-group><article-title>Perception and production of syncopated rhythms</article-title><source>Music Percept</source><year>2007</year><volume>25</volume><fpage>43</fpage><lpage>58</lpage></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Margulis</surname><given-names>E</given-names></name></person-group><chapter-title>Hellmuth</chapter-title><source>On Repeat: How Music Plays the Mind</source><publisher-name>Oxford University Press</publisher-name><year>2014</year></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cirelli</surname><given-names>LK</given-names></name><name><surname>Spinelli</surname><given-names>C</given-names></name><name><surname>Nozaradan</surname><given-names>S</given-names></name><name><surname>Trainor</surname><given-names>LJ</given-names></name></person-group><article-title>Measuring neural entrainment to beat and meter in infants: Effects of music background</article-title><source>Front Neurosci</source><year>2016</year><volume>10</volume><pub-id pub-id-type="pmcid">PMC4877507</pub-id><pub-id pub-id-type="pmid">27252619</pub-id><pub-id pub-id-type="doi">10.3389/fnins.2016.00229</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nozaradan</surname><given-names>S</given-names></name><etal/></person-group><article-title>Intracerebral evidence of rhythm transform in the human auditory cortex</article-title><source>Brain Struct Funct</source><year>2017</year><volume>222</volume><fpage>2389</fpage><lpage>2404</lpage><pub-id pub-id-type="pmid">27990557</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Margulis</surname><given-names>EH</given-names></name></person-group><chapter-title>Repetition</chapter-title><person-group person-group-type="editor"><name><surname>Alexander</surname><given-names>Rehding</given-names></name><name><surname>Rings</surname><given-names>Steven</given-names></name></person-group><source>The Oxford Handbook of Critical Concepts in Music Theory</source><publisher-name>Oxford Academic</publisher-name><year>2020</year><fpage>187</fpage><lpage>206</lpage></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tenzer</surname><given-names>M</given-names></name></person-group><article-title>Generalized representations of musical time and periodic structures</article-title><source>Ethnomusicology</source><year>2011</year><volume>55</volume><fpage>369</fpage><lpage>386</lpage></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zbikowski</surname><given-names>LM</given-names></name></person-group><article-title>Modelling the Groove: Conceptual Structure and Popular Music</article-title><source>Journal of the Royal Musical Association</source><year>2004</year><volume>129</volume><fpage>272</fpage><lpage>297</lpage></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Agawu</surname><given-names>K</given-names></name></person-group><source>The African Imagination in Music</source><publisher-name>Oxford University Press</publisher-name><year>2016</year></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Margulis</surname><given-names>EH</given-names></name></person-group><article-title>Musical repetition detection across multiple exposures</article-title><source>Music Perception: An Interdisciplinary Journal</source><year>2012</year><volume>29</volume><fpage>377</fpage><lpage>385</lpage></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lerdahl</surname><given-names>F</given-names></name><name><surname>Jackendoff</surname><given-names>R</given-names></name></person-group><article-title>On the theory of grouping and meter</article-title><source>The Musical Quarterly</source><year>1981</year><volume>67</volume><fpage>479</fpage><lpage>506</lpage></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Temperley</surname><given-names>D</given-names></name><name><surname>Bartlette</surname><given-names>C</given-names></name></person-group><article-title>Parallelism as a factor in metrical analysis</article-title><source>Music Percept</source><year>2002</year><volume>20</volume><fpage>117</fpage><lpage>149</lpage></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Lenc</surname><given-names>T</given-names></name><etal/></person-group><source>Measuring self-similarity in empirical signals to understand musical beat perception</source><year>2024</year><comment>Preprint at <ext-link ext-link-type="uri" xlink:href="https://osf.io/preprints/psyarxiv/yptsr">https://osf.io/preprints/psyarxiv/yptsr</ext-link></comment></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lenc</surname><given-names>T</given-names></name><name><surname>Keller</surname><given-names>PE</given-names></name><name><surname>Varlet</surname><given-names>M</given-names></name><name><surname>Nozaradan</surname><given-names>S</given-names></name></person-group><article-title>Neural and behavioral evidence for frequency-selective context effects in rhythm processing in humans</article-title><source>Cereb Cortex Commun</source><year>2020</year><volume>1</volume><pub-id pub-id-type="pmcid">PMC8152888</pub-id><pub-id pub-id-type="pmid">34296106</pub-id><pub-id pub-id-type="doi">10.1093/texcom/tgaa037</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zatorre</surname><given-names>RJ</given-names></name></person-group><article-title>Predispositions and plasticity in music and speech learning: neural correlates and implications</article-title><source>Science</source><year>2013</year><volume>342</volume><fpage>585</fpage><lpage>589</lpage><pub-id pub-id-type="pmid">24179219</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wunderlich</surname><given-names>JL</given-names></name><name><surname>Cone-Wesson</surname><given-names>BK</given-names></name></person-group><article-title>Effects of stimulus frequency and complexity on the mismatch negativity and other components of the cortical auditory-evoked potential</article-title><source>J Acoust Soc Am</source><year>2001</year><volume>109</volume><fpage>1526</fpage><lpage>1537</lpage><pub-id pub-id-type="pmid">11325124</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zwicker</surname><given-names>E</given-names></name><name><surname>Fasti</surname><given-names>H</given-names></name></person-group><source>Psychoacoustics - Facts and Models</source><publisher-name>Springer Series in Information Sciences</publisher-name><year>1999</year></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norcia</surname><given-names>AM</given-names></name><name><surname>Appelbaum</surname><given-names>LG</given-names></name><name><surname>Ales</surname><given-names>JM</given-names></name><name><surname>Cottereau</surname><given-names>BR</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><article-title>The steady-state visual evoked potential in vision research: A review</article-title><source>J Vis</source><year>2015</year><volume>15</volume><fpage>1</fpage><lpage>46</lpage><pub-id pub-id-type="pmcid">PMC4581566</pub-id><pub-id pub-id-type="pmid">26024451</pub-id><pub-id pub-id-type="doi">10.1167/15.6.4</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chiu</surname><given-names>MG</given-names></name></person-group><source>Form as meter: metric forms through Fourier space</source><publisher-name>thesis, Boston University</publisher-name><year>2018</year><comment>at <ext-link ext-link-type="uri" xlink:href="https://open.bu.edu/handle/2144/30655">https://open.bu.edu/handle/2144/30655</ext-link></comment></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lenc</surname><given-names>T</given-names></name><etal/></person-group><article-title>Infants show enhanced neural responses to musical meter frequencies beyond low-level features</article-title><source>Dev Sci</source><year>2023</year><volume>26</volume><pub-id pub-id-type="pmid">36415027</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brandon</surname><given-names>M</given-names></name><name><surname>Terry</surname><given-names>J</given-names></name><name><surname>Stevens</surname><given-names>CJ</given-names></name><name><surname>Tillmann</surname><given-names>B</given-names></name></person-group><article-title>Incidental learning of temporal structures conforming to a metrical framework</article-title><source>Front Psychol</source><year>2012</year><volume>3</volume><pub-id pub-id-type="pmcid">PMC3425964</pub-id><pub-id pub-id-type="pmid">22936921</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2012.00294</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Lenc</surname><given-names>T</given-names></name><name><surname>Keller</surname><given-names>PE</given-names></name><name><surname>Varlet</surname><given-names>M</given-names></name><name><surname>Nozaradan</surname><given-names>S</given-names></name></person-group><source>Attention affects overall gain but not selective contrast at meter frequencies in the neural processing of rhythm</source><year>2020</year><comment>Preprint at <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2020.09.23.309443v1.full">https://www.biorxiv.org/content/10.1101/2020.09.23.309443v1.full</ext-link></comment></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Slaney</surname><given-names>M</given-names></name></person-group><source>Auditory Toolbox, Version 2</source><year>1998</year><comment><ext-link ext-link-type="uri" xlink:href="https://engineering.purdue.edu/~malcolm/interval/1998-010/AuditoryToolboxTechReport.pdf">https://engineering.purdue.edu/~malcolm/interval/1998-010/AuditoryToolboxTechReport.pdf</ext-link></comment></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chemin</surname><given-names>B</given-names></name><name><surname>Mouraux</surname><given-names>A</given-names></name><name><surname>Nozaradan</surname><given-names>S</given-names></name></person-group><article-title>Body movement selectively shapes the neural representation of musical rhythms</article-title><source>Psychol Sci</source><year>2014</year><volume>25</volume><fpage>2147</fpage><lpage>2159</lpage><pub-id pub-id-type="pmid">25344346</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leek</surname><given-names>MR</given-names></name></person-group><article-title>Adaptive procedures in psychophysical research</article-title><source>Perception &amp; Psychophysics</source><year>2001</year><volume>63</volume><fpage>1279</fpage><lpage>1292</lpage><pub-id pub-id-type="pmid">11800457</pub-id></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Madsen</surname><given-names>CK</given-names></name></person-group><article-title>Modulated beat discrimination among musicians and nonmusicians</article-title><source>Journal of Research in Music Education</source><year>1979</year><volume>27</volume><fpage>57</fpage><lpage>67</lpage></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tranchant</surname><given-names>P</given-names></name><name><surname>Vuvan</surname><given-names>DT</given-names></name><name><surname>Peretz</surname><given-names>I</given-names></name></person-group><article-title>Keeping the beat: A large sample study of bouncing and clapping to music</article-title><source>PLoS One</source><year>2016</year><volume>11</volume><pub-id pub-id-type="pmcid">PMC4966945</pub-id><pub-id pub-id-type="pmid">27471854</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0160178</pub-id></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skoe</surname><given-names>E</given-names></name><name><surname>Kraus</surname><given-names>N</given-names></name></person-group><article-title>Auditory brainstem response to complex sound: a tutorial</article-title><source>Ear Hear</source><year>2010</year><volume>3</volume><fpage>302</fpage><lpage>324</lpage><pub-id pub-id-type="pmcid">PMC2868335</pub-id><pub-id pub-id-type="pmid">20084007</pub-id><pub-id pub-id-type="doi">10.1097/AUD.0b013e3181cdb272</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nozaradan</surname><given-names>S</given-names></name><name><surname>Schönwiesner</surname><given-names>M</given-names></name><name><surname>Caron-Desrochers</surname><given-names>L</given-names></name><name><surname>Lehmann</surname><given-names>A</given-names></name></person-group><article-title>Enhanced brainstem and cortical encoding of sound during synchronized movement</article-title><source>Neuroimage</source><year>2016</year><volume>142</volume><fpage>231</fpage><lpage>240</lpage><pub-id pub-id-type="pmid">27397623</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jung</surname><given-names>T-P</given-names></name><etal/></person-group><article-title>Removal of eye activity artifacts from visual event-related potentials in normal and clinical subjects</article-title><source>Clinical Neurophysiology</source><year>2000</year><volume>111</volume><fpage>1745</fpage><lpage>1758</lpage><pub-id pub-id-type="pmid">11018488</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rossion</surname><given-names>B</given-names></name><name><surname>Retter</surname><given-names>TL</given-names></name><name><surname>Liu-Shuang</surname><given-names>J</given-names></name></person-group><article-title>Understanding human individuation of unfamiliar faces with oddball fast periodic visual stimulation and electroencephalography</article-title><source>European Journal of Neuroscience</source><year>2020</year><volume>52</volume><fpage>4283</fpage><lpage>4344</lpage><pub-id pub-id-type="pmid">32542962</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nozaradan</surname><given-names>S</given-names></name><name><surname>Peretz</surname><given-names>I</given-names></name><name><surname>Missal</surname><given-names>M</given-names></name><name><surname>Mouraux</surname><given-names>A</given-names></name></person-group><article-title>Tagging the neuronal entrainment to beat and meter</article-title><source>Journal of Neuroscience</source><year>2011</year><volume>31</volume><fpage>10234</fpage><lpage>10240</lpage><pub-id pub-id-type="pmcid">PMC6623069</pub-id><pub-id pub-id-type="pmid">21753000</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0411-11.2011</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tomic</surname><given-names>ST</given-names></name><name><surname>Janata</surname><given-names>P</given-names></name></person-group><article-title>Beyond the beat: Modeling metric structure in music and performance</article-title><source>J Acoust Soc Am</source><year>2008</year><volume>124</volume><fpage>4024</fpage><lpage>4041</lpage><pub-id pub-id-type="pmid">19206825</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brochard</surname><given-names>R</given-names></name><name><surname>Abecasis</surname><given-names>D</given-names></name><name><surname>Potter</surname><given-names>D</given-names></name><name><surname>Ragot</surname><given-names>R</given-names></name><name><surname>Drake</surname><given-names>C</given-names></name></person-group><article-title>The ‘ticktock’ of our internal clock: direct brain evidence of subjective accents in isochronous sequences</article-title><source>Psychol Sci</source><year>2003</year><volume>14</volume><fpage>362</fpage><lpage>366</lpage><pub-id pub-id-type="pmid">12807411</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Vos</surname><given-names>PGMM</given-names></name></person-group><source>Waarneming van metrische toonreeksen</source><publisher-name>thesis, Radboud University</publisher-name><year>1973</year><comment>at <ext-link ext-link-type="uri" xlink:href="https://repository.ubn.ru.nl/bitstream/handle/2066/148457/mmubn000001_048420638.pdf?sequence=1">https://repository.ubn.ru.nl/bitstream/handle/2066/148457/mmubn000001_048420638.pdf?sequence=1</ext-link></comment></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bååth</surname><given-names>R</given-names></name></person-group><article-title>Subjective rhythmization: a replication and an assessment of two theoretical explanations</article-title><source>Music Percept</source><year>2015</year><volume>33</volume><fpage>244</fpage><lpage>254</lpage></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Merchant</surname><given-names>H</given-names></name><name><surname>Honing</surname><given-names>H</given-names></name></person-group><article-title>Are non-human primates capable of rhythmic entrainment? Evidence for the gradual audiomotor evolution hypothesis</article-title><source>Front Neurosci</source><year>2014</year><volume>7</volume><pub-id pub-id-type="pmcid">PMC3894452</pub-id><pub-id pub-id-type="pmid">24478618</pub-id><pub-id pub-id-type="doi">10.3389/fnins.2013.00274</pub-id></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patel</surname><given-names>AD</given-names></name><name><surname>Iversen</surname><given-names>JR</given-names></name></person-group><article-title>The evolutionary neuroscience of musical beat perception: The Action Simulation for Auditory Prediction (ASAP) hypothesis</article-title><source>Front Syst Neurosci</source><year>2014</year><volume>8</volume><pub-id pub-id-type="pmcid">PMC4026735</pub-id><pub-id pub-id-type="pmid">24860439</pub-id><pub-id pub-id-type="doi">10.3389/fnsys.2014.00057</pub-id></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Phillips-Silver</surname><given-names>J</given-names></name><name><surname>Trainor</surname><given-names>LJ</given-names></name></person-group><article-title>Hearing what the body feels: Auditory encoding of rhythmic movement</article-title><source>Cognition</source><year>2007</year><volume>105</volume><fpage>533</fpage><lpage>546</lpage><pub-id pub-id-type="pmid">17196580</pub-id></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harry</surname><given-names>BB</given-names></name><name><surname>Margulies</surname><given-names>DS</given-names></name><name><surname>Falkiewicz</surname><given-names>M</given-names></name><name><surname>Keller</surname><given-names>PE</given-names></name></person-group><article-title>Brain networks for temporal adaptation, anticipation, and sensory-motor integration in rhythmic human behavior</article-title><source>Neuropsychologia</source><year>2023</year><volume>183</volume><pub-id pub-id-type="pmid">36868500</pub-id></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clayton</surname><given-names>M</given-names></name></person-group><article-title>Theory and practice of long-form non-isochronous meters: The case of the North Indian rūpak tāl</article-title><source>Music Theory Online</source><year>2020</year><volume>26</volume></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salidis</surname><given-names>J</given-names></name></person-group><article-title>Nonconscious temporal cognition: learning rhythms implicitly</article-title><source>Mem Cognit</source><year>2001</year><volume>29</volume><fpage>1111</fpage><lpage>1119</lpage><pub-id pub-id-type="pmid">11913747</pub-id></element-citation></ref><ref id="R64"><label>64</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tóth</surname><given-names>B</given-names></name><etal/></person-group><article-title>Auditory learning of recurrent tone sequences is present in the newborn’s brain</article-title><source>Neuroimage</source><year>2023</year><volume>281</volume><pub-id pub-id-type="pmid">37739198</pub-id></element-citation></ref><ref id="R65"><label>65</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saffran</surname><given-names>JR</given-names></name><name><surname>Aslin</surname><given-names>RN</given-names></name><name><surname>Newport</surname><given-names>EL</given-names></name></person-group><article-title>Statistical learning by 8-month-old infants</article-title><source>Science</source><year>1996</year><volume>274</volume><fpage>1926</fpage><lpage>1928</lpage><pub-id pub-id-type="pmid">8943209</pub-id></element-citation></ref><ref id="R66"><label>66</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohrmeier</surname><given-names>M</given-names></name><name><surname>Rebuschat</surname><given-names>P</given-names></name></person-group><article-title>Implicit learning and acquisition of music</article-title><source>Top Cogn Sci</source><year>2012</year><volume>4</volume><fpage>525</fpage><lpage>553</lpage><pub-id pub-id-type="pmid">23060126</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Three-layer scheme of the temporal range for meter processing, showing the physical space of the rhythmic input and corresponding metric perceptual space.</title><p>The figure depicts an example rhythmic sequence with prominent periodic cues in the faster layer, formed by recurrent intervals between successive sound events (dashed vertical lines) and prominent periodic cues in the slower layer, formed by the seamless repetition of the rhythmic pattern (as depicted with squared brackets). In contrast, the intermediate layer does not show prominent periodic cues due to the local arrangement of sound events forming a syncopated rhythm.</p></caption><graphic xlink:href="EMS201828-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Temporal envelope of the fourteen seed rhythms.</title><p>(a) All rhythms were based on a twelve-interval 200 ms grid structure (i.e., 2.4 s in total), with eight of the intervals consisting in sound events and the remaining four intervals consisting in silent events. The sound and silent intervals were arranged such as to yield syncopated rhythms.</p><p>(b) Weak prominence of beat periodicities was confirmed through frequency analysis of the amplitude modulation of the rhythms, from which meter z-scores were calculated All seed rhythms showed negative meter-related zscores, indicating weak prominence of meter-related periodicities in their modulation spectrum compared to meter-unrelated periodicities.</p></caption><graphic xlink:href="EMS201828-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><p>Experimental design. Musician and non-musician groups of participants were presented with four blocks in two possible fixed orders - the top and bottom condition orders providing maximum and minimum prior context respectively, with the “no pattern repetition “ condition positioned either at the end or the beginning of the block design. Half of each group was assigned to each condition order, and for each condition neural activity was recorded using electroencephalography, followed by a tapping session.</p></caption><graphic xlink:href="EMS201828-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Behavioral results: non-musicians showed overall poorer stability at tapping the beat along with the rhythmic inputs, as compared to musicians.</title><p>(A) Distribution of intertap intervals (ITIs) across participant groups and conditions. Horizontal lines correspond to integer multiples of the 200-ms grid structure on which rhythmic patterns were built, thus corresponding to plausible beat rates participants could have synchronized their tapping to. Dashed lines highlight 400- and 800-ms, corresponding to the convergent tapping rates across participants. (B) Tapping stability. Values ranging from 0 to 1 indicate low and high synchronization with a steady periodic beat, respectively. The horizontal line and limits of the boxplot indicate the median and 25<sup>th</sup> and 75<sup>th</sup> percentiles respectively.</p></caption><graphic xlink:href="EMS201828-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><p>Modulation spectra obtained using cochlear model for each stimulus condition (top row), and their corresponding EEG responses in musicians (middle row), and non-musicians (bottom row). Frequencies of interest are respectively highlighted in red or blue for meter-related or meter-unrelated frequencies. EEG spectra correspond to a pool of 9 frontocentral channels and are averages over the 26 participants in each group.</p></caption><graphic xlink:href="EMS201828-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><p>Mean z-score of EEG responses at meter-related frequencies across groups (left plot: musicians, right plot: non-musicians), for all conditions and condition order (circles: condition order maximizing prior context, squares: condition order minimizing prior context). Corresponding values obtained from the cochlear model output are shown as vertical black lines.</p></caption><graphic xlink:href="EMS201828-f006"/></fig><table-wrap id="T1" position="float" orientation="portrait"><label>Table 1</label><caption><title>Selective enhancement of EEG meter-related zscores observed in each group, condition and condition order.</title><p>The p-value is flagged with one star (*) if lower than 0.05, two stars (**) if lower than 0.01, and three stars (***) if lower than 0.001.</p></caption><table frame="void" rules="groups"><thead><tr><th align="center" valign="top">Musicians</th><th align="center" valign="top"/><th align="center" valign="top">Mean</th><th align="center" valign="top">Standard error of the mean</th><th align="center" valign="top">T-statistic</th><th align="center" valign="top">p-value (FDR corrected)</th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="2">Medium pattern repetition</td><td align="left" valign="top">Maximum prior context</td><td align="left" valign="top">0.798</td><td align="left" valign="top">0.150</td><td align="left" valign="top">5.324</td><td align="left" valign="top">2.621e-04***</td></tr><tr><td align="left" valign="top">No prior context</td><td align="left" valign="top">0.423</td><td align="left" valign="top">0.112</td><td align="left" valign="top">3.765</td><td align="left" valign="top">1.797e-03**</td></tr><tr><td align="left" valign="top" rowspan="2">Long pattern repetition #1</td><td align="left" valign="top">Maximum prior context</td><td align="left" valign="top">0.698</td><td align="left" valign="top">0.086</td><td align="left" valign="top">8.149</td><td align="left" valign="top">1.244e-05***</td></tr><tr><td align="left" valign="top">No prior context</td><td align="left" valign="top">0.380</td><td align="left" valign="top">0.083</td><td align="left" valign="top">4.607</td><td align="left" valign="top">6.036e-04***</td></tr><tr><td align="left" valign="top" rowspan="2">Long pattern repetition #2</td><td align="left" valign="top">Maximum prior context</td><td align="left" valign="top">0 574</td><td align="left" valign="top">0.128</td><td align="left" valign="top">4.501</td><td align="left" valign="top">6 452e-04***</td></tr><tr><td align="left" valign="top">No prior context</td><td align="left" valign="top">0.521</td><td align="left" valign="top">0.146</td><td align="left" valign="top">3.559</td><td align="left" valign="top">2.420e-03***</td></tr><tr><td align="left" valign="top" rowspan="2">No pattern repetition</td><td align="left" valign="top">Maximum prior context</td><td align="left" valign="top">0.575</td><td align="left" valign="top">0.130</td><td align="left" valign="top">4.425</td><td align="left" valign="top">6.628e-04***</td></tr><tr><td align="left" valign="top">No prior context</td><td align="left" valign="top">0.636</td><td align="left" valign="top">0.185</td><td align="left" valign="top">3.435</td><td align="left" valign="top">2.822e-03**</td></tr><tr style="border-bottom: solid thin"><td align="left" valign="top" colspan="6"><bold>Non-musicians</bold></td></tr><tr><td align="left" valign="top" rowspan="2">Medium pattern repetition</td><td align="left" valign="top">Maximum prior context</td><td align="left" valign="top">0.923</td><td align="left" valign="top">0.096</td><td align="left" valign="top">9.651</td><td align="left" valign="top">4.197e-06***</td></tr><tr><td align="left" valign="top">No prior context</td><td align="left" valign="top">0.631</td><td align="left" valign="top">0.120</td><td align="left" valign="top">5.274</td><td align="left" valign="top">2.621e-04***</td></tr><tr><td align="left" valign="top" rowspan="2">Long pattern repetition #1</td><td align="left" valign="top">Maximum prior context</td><td align="left" valign="top">0.638</td><td align="left" valign="top">0.107</td><td align="left" valign="top">5.965</td><td align="left" valign="top">1.313e-04***</td></tr><tr><td align="left" valign="top">No prior context</td><td align="left" valign="top">0.485</td><td align="left" valign="top">0.069</td><td align="left" valign="top">7.032</td><td align="left" valign="top">3.658e-05***</td></tr><tr><td align="left" valign="top" rowspan="2">Long pattern repetition #2</td><td align="left" valign="top">Maximum prior context</td><td align="left" valign="top">0.591</td><td align="left" valign="top">0.126</td><td align="left" valign="top">4.675</td><td align="left" valign="top">6.036e-04***</td></tr><tr><td align="left" valign="top">No prior context</td><td align="left" valign="top">0.599</td><td align="left" valign="top">0.140</td><td align="left" valign="top">4.275</td><td align="left" valign="top">7.846e-04***</td></tr><tr><td align="left" valign="top" rowspan="2">No pattern repetition</td><td align="left" valign="top">Maximum prior context</td><td align="left" valign="top">0.576</td><td align="left" valign="top">0.179</td><td align="left" valign="top">3.225</td><td align="left" valign="top">3 887e-03***</td></tr><tr><td align="left" valign="top">No prior context</td><td align="left" valign="top">0.411</td><td align="left" valign="top">0.150</td><td align="left" valign="top">2.738</td><td align="left" valign="top">8.994e-03***</td></tr></tbody></table></table-wrap></floats-group></article>