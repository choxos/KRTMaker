<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS201582</article-id><article-id pub-id-type="doi">10.1101/2024.11.29.625898</article-id><article-id pub-id-type="archive">PPR947619</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Deep learning super-resolution of paediatric ultra-low-field MRI without paired high-field scans</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Briski</surname><given-names>Ula</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Bourke</surname><given-names>Niall J.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Donald</surname><given-names>Kirsten A.</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Bradford</surname><given-names>Layla E.</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Williams</surname><given-names>Simone R.</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Zieff</surname><given-names>Michal R.</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Deoni</surname><given-names>Sean C.L.</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Williams</surname><given-names>Steven C.R.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><collab>Khula South Africa Study Team</collab><xref ref-type="aff" rid="A2">2</xref><xref ref-type="fn" rid="FN2">†</xref></contrib><contrib contrib-type="author"><name><surname>Moran</surname><given-names>Rosalyn J.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Baljer</surname><given-names>Levente</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="fn" rid="FN1">*</xref></contrib><contrib contrib-type="author"><name><surname>Váša</surname><given-names>František</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="fn" rid="FN1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>Department of Neuroimaging, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/0220mzb33</institution-id><institution>King’s College London</institution></institution-wrap>, <country country="GB">UK</country></aff><aff id="A2"><label>2</label>Department of Paediatrics and Child Health, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/04d6eav07</institution-id><institution>Red Cross War Memorial Children’s Hospital</institution></institution-wrap>, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/03p74gp79</institution-id><institution>University of Cape Town</institution></institution-wrap>, <country country="ZA">South Africa</country></aff><aff id="A3"><label>3</label>Neuroscience Institute, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/03p74gp79</institution-id><institution>University of Cape Town</institution></institution-wrap>, <country country="ZA">South Africa</country></aff><aff id="A4"><label>4</label>Maternal Newborn and Child Nutrition and Health (MNCNH), <institution-wrap><institution-id institution-id-type="ror">https://ror.org/0456r8d26</institution-id><institution>Bill &amp; Melinda Gates Foundation</institution></institution-wrap>, <city>Seattle</city>, <state>Washington</state>, <country country="US">USA</country></aff><author-notes><fn id="FN1"><label>*</label><p id="P1">Joint senior authors</p></fn><fn id="FN2"><label>†</label><p id="P2">For the full list of Khula South Africa Study Team authors, see the Supplementary Information.</p></fn></author-notes><pub-date pub-type="nihms-submitted"><day>01</day><month>12</month><year>2024</year></pub-date><pub-date pub-type="preprint"><day>30</day><month>11</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nd/4.0/">CC BY-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P3">Brain magnetic resonance imaging (MRI) is essential for diagnosis and neurodevelopmental research, but the high cost and infrastructure demands of high-field MRI scanners restrict their use to high-income settings. To address this, more affordable and energy-efficient ultra-low-field MRI scanners have been developed. However, the reduced resolution and signal-to-noise ratio of the resulting scans limit their clinical utility, motivating the development of super-resolution techniques. The current state-of-the-art super-resolution methods require either three anisotropic ultra-low-field scans acquired at different orientations (axial, coronal, sagittal) to reconstruct a higher-resolution image using multi-resolution registration (MRR), or the training of deep learning super-resolution models using paired ultra-low- and high-field scans. Since acquiring three high-quality ultra-low-field scans is not always feasible, and paired high-field data may not be available for the target population, this study explores the efficacy of using a deep learning model, the 3D UNet, to generate higher-resolution brain scans from just one ultra-low-field scan. The model was trained to receive a single ultra-low-field brain scan of 6-month-old infants and produce a scan of MRR quality. Results showed a significant improvement in the quality of output scans compared to input scans, including increased image quality metrics, stronger correlations in tissue volume estimates across participants, and greater Dice overlap of the underlying tissue segmentations to those of target scans. The study demonstrates that the 3D UNet effectively enhances the resolution of ultra-low-field infant MRI scans. Generating higher-resolution brain scans from single ultra-low-field scans, without needing paired high-field data, reduces scanning time and supports wider MRI use in low- and middle-income countries. Additionally, this approach allows for easier model training on a site- and population-specific basis, enhancing adaptability in diverse settings.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P4">Magnetic resonance imaging (MRI) is widely used for medical diagnosis and research (<xref ref-type="bibr" rid="R33">Ryan &amp; Jaju, 2023</xref>). Unlike other neuroimaging techniques, such as computed tomography and positron emission tomography, MRI is non-invasive and does not use ionizing radiation, making it ideal for studying, for example, fetal and early infant neurodevelopment (<xref ref-type="bibr" rid="R11">Deoni et al., 2021</xref>). Despite its utility, MRI technology is not equally accessible to everyone. In low- and middle-income countries (LMICs), fewer than one MRI scanner is available per million individuals (<xref ref-type="bibr" rid="R2">Anazodo et al., 2023</xref>). For example, there are only 84 MRI scanners available to 373 million people in the West African sub-region (<xref ref-type="bibr" rid="R28">Ogbole et al., 2018</xref>). Conversely, high-income countries (HICs) have about one MRI unit for every 25,000 people (<xref ref-type="bibr" rid="R28">Ogbole et al., 2018</xref>). Moreover, MRI scanners in LMICs are frequently outdated or predominantly located in urban areas (<xref ref-type="bibr" rid="R18">Jalloul et al., 2023</xref>), further reducing their accessibility. The main reasons for this are the high capital equipment costs and infrastructural requirements (<xref ref-type="bibr" rid="R36">Wald et al., 2019</xref>). These include purchasing the system, installing it in a shielded suite, powering it, the need for cryogenic cooling, and ongoing maintenance costs, as well as the need for trained specialists (<xref ref-type="bibr" rid="R3">Arnold et al., 2022</xref>; <xref ref-type="bibr" rid="R34">van Beek et al., 2018</xref>). In addition to significantly limiting clinical use, this disparity also negatively affects research efforts. For example, studies on neurodevelopment and neural correlates of disorders often rely on small sample sizes and are predominantly conducted on participants from HICs, thus limiting their generalizability (<xref ref-type="bibr" rid="R1">Abate et al., 2024</xref>; <xref ref-type="bibr" rid="R11">Deoni et al., 2021</xref>).</p><p id="P5">One possible way to address this problem is the adoption of ultra-low-field (ULF) MRI scanners that use substantially lower magnetic field strength (e.g. &lt;0.1 T) than conventional high-field (HF) scanners (1.5-3 T). ULF scanners are considerably cheaper and easier to maintain, and additionally require less power and no specific facilities (<xref ref-type="bibr" rid="R36">Wald et al., 2019</xref>). Therefore, these systems could offer better paediatric clinical care in LIMCs and opportunities for carrying out larger studies with diverse populations. <xref ref-type="bibr" rid="R11">Deoni and colleagues (2021)</xref> demonstrated that a 64mT ULF MRI scanner could reliably replicate brain volume and developmental estimates from a 3T HF scanner, with additional benefits including quieter scan acquisition and allowing parents to be present, resulting in reduced movement artefacts. Despite the many benefits of ULF systems and the potential for their global adoption, the primary drawback is that resulting scans have significantly lower signal-to-noise ratio (SNR) and resolution compared to HF systems. For example, scans acquired with the 64 mT Hyperfine Swoop have a spatial resolution of 1.5 × 1.5 × 5 mm<sup>3</sup> compared to standard 1 × 1 × 1 mm<sup>3</sup> in HF scanners. This yields anisotropic images where the resolution is high within one plane (axial, coronal, or sagittal) with reasonably small pixels (1.5 × 1.5 mm<sup>2</sup>), but with considerably thicker slices (5 mm). Each anisotropic image takes 3-6 minutes to acquire, depending on contrast and SNR, making it particularly useful for infants who cannot keep still for long periods.</p><p id="P6">One approach to improve the clinical and research utility of lower-quality ultra-low-field scans is super-resolution (SR). SR algorithms aim to increase the resolution and SNR of an image and their use has been strongly driven by the development of new deep learning techniques in recent years (<xref ref-type="bibr" rid="R25">Li et al., 2021</xref>), primarily through the use of convolutional neural networks (CNN). The UNet was originally developed for the segmentation of biomedical images but is now also widely used for SR problems. <xref ref-type="bibr" rid="R16">Iglesias and colleagues (2021)</xref> introduced a 3D UNet super-resolution method for reconstruction of high-quality MRI brain scans from inputs of varied contrasts and resolutions, allowing for the analysis of a wide range of MRI data. This model, however, was trained on synthetically generated scans of healthy adults, hindering its applicability to paediatric and/or clinical data. Several other approaches have been proposed, including a multi-orientation UNet (<xref ref-type="bibr" rid="R5">Baljer et al., 2024</xref>), generative adversarial networks (GANs) (<xref ref-type="bibr" rid="R17">Islam et al., 2023</xref>), and SRDenseNet (<xref ref-type="bibr" rid="R10">de Leeuw den Bouter et al., 2022</xref>); however, they all require paired ULF and HF data for training.</p><p id="P7">A study by <xref ref-type="bibr" rid="R12">Deoni and colleagues (2022)</xref> presents a successful SR approach for reconstructing higher-resolution isotropic images from three low-resolution anisotropic images (axial, coronal, and sagittal) acquired at low magnetic field strengths, without the use of deep learning, called multi-resolution registration (MRR). MRR involves combining three ULF scans with dimensions of 1.5 × 1.5 × 5 mm<sup>3</sup>, each with high-resolution within one orthogonal plane (axial, coronal, sagittal), into a single 1.5 × 1.5 × 1.5 mm<sup>3</sup> higher-resolution image. The MRR approach to SR, however, has the limitation of requiring acquisition of all three anisotropic ULF scans to reconstruct a higher-resolution isotropic scan. This results in a long scanning protocol (9-18 min) compared to the modern HF scanners. Moreover, it is not always possible to acquire good-quality scans from all three orientations due to head motion or other artefacts, particularly in paediatric or clinical populations.</p><p id="P8">To overcome this problem, this study investigates the effectiveness of a UNet at reconstructing higher-resolution scans of MRR quality (1.5 × 1.5 × 1.5 mm<sup>3</sup>) from any anisotropic ULF scan (1.5 × 1.5 × 5 mm<sup>3</sup>) using T2-weighted 64 mT ULF scans of 6-month-old infants. Higher-resolution images were reconstructed using a particular type of CNN, namely a UNet (<xref ref-type="bibr" rid="R9">Çiçek et al., 2016</xref>; <xref ref-type="bibr" rid="R32">Ronneberger et al., 2015</xref>) without the need for paired ULF and HF data. The quality of super-resolved model outputs shows increased correlations of tissue volume and greater Dice overlap of segmented brain regions (<xref ref-type="bibr" rid="R8">Billot et al., 2023</xref>) to MRR reference scans. Obtaining a 1.5 × 1.5 × 1.5 mm<sup>3</sup> scan from a single anisotropic low-resolution scan can help reduce scanning time, making this approach especially useful for scanning neonates, infants and any other participants who struggle to remain still during lengthy scanner sessions. Additionally, less time in the scanner means lower costs, further widening accessibility to MRI in LMICs.</p></sec><sec id="S2" sec-type="methods"><title>Methods</title><sec id="S3"><title>MRI Data</title><p id="P9">The MRI data used in the study was acquired in Cape Town, South Africa as part of the ‘Khula Study’ (<xref ref-type="bibr" rid="R38">Zieff et al., 2024</xref>), which is a multi-site longitudinal birth cohort study investigating executive function emergence and development in the first 2 years of life in South Africa and Malawi. To be eligible for participation, women had to be in their third trimester of pregnancy or up to three months postpartum. Additionally, women had to be older than 18 years, had to have had a singleton pregnancy, and not use psychotropic drugs during pregnancy. Women with major complications during delivery or infants with the presence of congenital malformations or abnormalities were also excluded. In South Africa, researchers recruited 394 participants. A range of data types were collected, including demographic and health-related information, neuroimaging, electrophysiological and behavioural data, as well as biospecimens. MRI data was acquired at both ultra-low-field (ULF), using a 64 mT Hyperfine Swoop system, and at high-field (HF), using a 3T Siemens system. In the present study, T2-weighted ULF scans from 6-month-old infants were used. We only used data from a single age group as the contrast between grey and white matter changes significantly during the first two years of life due to rapid myelination of white matter (<xref ref-type="bibr" rid="R6">Barkovich, 2005</xref>; <xref ref-type="bibr" rid="R31">Prastawa et al., 2005</xref>), and including multiple age groups could make the learning task more difficult.</p><p id="P10">Each ULF scan has high resolution within one plane – axial (AXI), coronal (COR) or sagittal (SAG) – but low resolution along the remaining dimension (i.e., 1.5 × 1.5 × 5 mm<sup>3</sup>). In the present study, we only included participants with all three orthogonal anisotropic scans (referred to as AXI, COR and SAG), yielding a sample size of 200 participants. We performed visual quality control (QC) to exclude any participants with at least one low-quality anisotropic ULF scan. Participants were removed from the dataset if any one of their three scans exhibited severe motion artefacts or if part of the brain was out of the field of view. As a result, 35 participants were excluded, leaving a final sample of 165 participants. See <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S1</xref> for examples of excluded scans.</p></sec><sec id="S4"><title>Pre-processing</title><p id="P11">We performed rigid registration of each participant’s ULF scans to a HF template using the SPM12 software package (<xref ref-type="bibr" rid="R30">Penny et al., 2006</xref>). The template, serving as the reference image, was developed by <xref ref-type="bibr" rid="R5">Baljer and colleagues (2024)</xref> from HF scans of age-corresponding participants. Following this step, an additional 33 participants were excluded due to failed registration, resulting in a final dataset of 132 participants, each contributing three ULF scans (a total of 396 scans). Following registration, the three anisotropic ULF scans (AXI, COR, SAG) from each participant were combined into a single higher-resolution MRR scan, using multi-resolution registration (<xref ref-type="bibr" rid="R12">Deoni et al., 2022</xref>). Similarly to <xref ref-type="bibr" rid="R12">Deoni and colleagues (2022)</xref>, we used the <italic>antsMultivariateTemplateConstruction2.sh</italic> function from the Advanced Normalization Tools (ANTs) software package (<xref ref-type="bibr" rid="R4">Avants et al., 2009</xref>) to repeatedly co-register and average the three ULF scans, finally generating a higher-resolution scan with an effective voxel resolution of 1.5 × 1.5 × 1.5 mm<sup>3</sup>. The resulting dataset comprised of three anisotropic scans (AXI, COR, SAG) and one corresponding MRR scan per participant that would later be used to sample paired data for training and evaluation of the SR model.</p></sec><sec id="S5"><title>Model Architecture</title><p id="P12">The proposed deep learning method for super-resolution of ULF scans utilises a 3D UNet (<xref ref-type="fig" rid="F1">Figure 1</xref>), based on the architecture introduced by <xref ref-type="bibr" rid="R9">Çiçek and colleagues (2016)</xref>. In the present study, we trained the model to accept a single ULF scan (AXI, COR or SAG) as input and generate a super-resolved scan as the output, targeting the quality of a MRR ULF scan. We implemented the model on a GPU using PyTorch. For a detailed description of model architecture, see the <xref ref-type="supplementary-material" rid="SD1">supplementary information</xref>.</p></sec><sec id="S6"><title>Training and Inference</title><p id="P13">Training of our model minimises the negative peak-signal-to-noise ratio with respect to the neural network parameters θ: <disp-formula id="FD1"><mml:math id="M1"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mtext>PSNR</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>;</mml:mo><mml:mover accent="true"><mml:mtext>Y</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mtext>Y</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:mo>∗</mml:mo><mml:msub><mml:mrow><mml:mtext>log</mml:mtext></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mtext>MAX</mml:mtext></mml:mrow><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:mtext>MSE</mml:mtext></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula> Where <italic>MAX<sub>I</sub></italic> denotes the maximum possible pixel value of the input and output image (a pre-defined input parameter) and <italic>MSE</italic> is the mean-squared error, or: <disp-formula id="FD2"><mml:math id="M2"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mtext>MSE</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>;</mml:mo><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mtext>Y</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mstyle><mml:mtext>∑</mml:mtext></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula> Where <inline-formula><mml:math id="M3"><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula> represents the model output and <italic>y</italic> represents the target (ground truth) image.</p><p id="P14">Pairs of three ULF (AXI, COR, SAG) and corresponding target MRR scans were randomly divided into training (<italic>n</italic> = 318) and testing (<italic>n</italic> = 78) sets, such that the former consisted of 106 participants with three possible pairing combinations, representing ≈ 80% of the whole dataset, and the latter of 26 participants with three possible pairing combinations, representing ≈ 20% of the whole dataset. Due to GPU capacity restrictions, both the input and output scans were reduced in size from 256 × 256 × 256 to 160 × 160 × 160 voxels before training the model.</p><p id="P15">Pairs of one ULF (AXI, COR or SAG) and one MRR scan were used to train the model where one of the three ULF scans was randomly sampled as an input in each epoch and the corresponding MRR scan served as the target image. The model was trained with a learning rate of 10<sup>-4</sup> using the Adam optimizer (<xref ref-type="bibr" rid="R22">Kingma &amp; Ba, 2014</xref>). Using a batch size of 1 across 106 samples, the model underwent 1000 epochs of training (approximately 59 hours using an NVidia GeForce RTX 3090 GPU). A larger batch size could not be used due to limitations in GPU capacity. After training, the test set was used to assess the model’s performance. This was done by taking the final weights of the model and running inference on each of the three ULF scans per test participant, to produce three super-resolved output scans for subsequent comparison to MRR ground truth scans. This resulted in 78 predictions. Inference took approximately 1 minute per participant on a modern CPU and ∼1 second per participant on a GPU.</p></sec><sec id="S7"><title>Model Evaluation</title><p id="P16">We assessed the performance of the UNet by comparing its outputs to both the original ULF scans to quantify improvement in image quality, and to the target MRR scans to quantify how well the model learns to approximate these higher-resolution scans. In addition, we also assessed whether the quality of model outputs is influenced by the orientation of the input scan (AXI, COR or SAG).</p><p id="P17">We first evaluated the image quality using two metrics: normalised root mean squared error (NRMSE) and the structural similarity index (SSIM). These metrics were computed between the original ULF scans and the corresponding ground truth MRR scans and between the UNet predicted outputs and the corresponding ground truth MRR scans. The analysis was conducted across all 26 test participants.</p><p id="P18">We used SynthSeg+ to segment all brain scans and estimate tissue volume (<xref ref-type="bibr" rid="R8">Billot et al., 2023</xref>) for subsequent evaluation of performance using both the Dice overlap of segmented brain regions within participants and correlations of tissue volumes across participants. SynthSeg+ is a deep learning tool trained to handle brain scans acquired at any resolution or contrast, without the need for retraining or fine-tuning, making it particularly suitable for the segmentation of low-resolution 64 mT scans (<xref ref-type="bibr" rid="R35">Váša et al. 2024</xref>). SynthSeg+ generates 98 labels for distinct brain regions. We performed our analyses on 4 global tissue types, namely, white matter (WM), cerebrospinal fluid (CSF), cortical grey matter (GM<sub>cort</sub>) and subcortical grey matter (GM<sub>subcort</sub>) by aggregating multiple labels. In addition to the 4 global tissue types, we repeated the analyses on 8 bilateral subcortical regions: thalamus, amygdala, accumbens, hippocampus, putamen, pallidum, caudate and ventral diencephalon. Example SynthSeg+ segmentation masks for one test participant can be seen in <xref ref-type="fig" rid="F2">Figure 2</xref>. Before conducting the analyses, we performed visual QC of segmentations and excluded 4 participants with failed segmentations from further analyses (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S2</xref> for examples of excluded participants). SynthSeg+ also outputs automated QC scores for three global tissue types (WM, CSF, GM<sub>cort</sub>) and a subset of subcortical regions (thalamus, putamen + pallidum, hippocampus + amygdala). As SynthSeg+ was trained on adult scans, we did not exclude segmentations based on automated QC scores. Still, we calculated average QC scores for each region and scan type across the 22 participants with segmentations which were visually of high quality(see <xref ref-type="supplementary-material" rid="SD1">Supplementary Table S1</xref>).</p><p id="P19">Before running SynthSeg+, each participant had their original ULF scans and the three model predictions rigid-registered to their MRR scan using FSL FLIRT, a tool for linear brain image registration (<xref ref-type="bibr" rid="R19">Jenkinson et al., 2002</xref>). This was done to ensure the segmentations being compared are perfectly aligned, which is necessary for the Dice score to be meaningful.</p></sec><sec id="S8"><title>Statistical Analyses</title><p id="P20">Given the small sample size (<italic>n</italic> = 22 for individual scan orientations) and given its robustness to outliers and data distributions, we used a non-parametric Wilcoxon signed-rank test (<xref ref-type="bibr" rid="R37">Wilcoxon, 1945</xref>) to quantify the improvement in image quality following super-resolution, by comparing the Dice overlap between segmentations of ULF and MRR scans, to Dice overlap between segmentations of predicted and MRR scans. We assessed tissue volume correlations across participants using Pearson’s r (<xref ref-type="bibr" rid="R29">Pearson, 1896</xref>) to measure the strength of linear correlation, as well as Lin’s concordance correlation coefficient (CCC) (<xref ref-type="bibr" rid="R26">Lin, 1989</xref>) to quantify exact agreement (i.e., alignment with the identity line, y=x).</p></sec></sec><sec id="S9" sec-type="results"><title>Results</title><p id="P21">The outputs of the UNet model are visually superior and show increased sharpness compared to the original ULF scans (<xref ref-type="fig" rid="F2">Figure 2</xref>). Model predictions show the most prominent enhancement in quality along the two low-resolution orthogonal planes, resulting in a sharper image with increased detail as seen in the enlarged portion of the brain in <xref ref-type="fig" rid="F2">Figure 2</xref>. These visual differences are also reflected in lower NRMSE and higher SSIM values for UNet outputs compared to ULF input scans (<xref ref-type="table" rid="T1">Table 1</xref>).</p><p id="P22">We quantified improvement in tissue segmentation from SR by comparing the Dice overlap of UNet predictions and MRR scans to the Dice overlap of ULF and MRR scans. We tested the significance of differences in individual overlap using the Wilcoxon signed-rank test (WSR) with Bonferroni correction for multiple comparisons. For all 4 main tissue types, the median Dice score increased significantly (WSR <italic>p</italic> &lt; 0.001): from 0.81 to 0.82 for WM, 0.60 to 0.65 for CSF, 0.76 to 0.78 for GM<sub>cort</sub> and 0.87 to 0.88 for GM<sub>subcort</sub> (<xref ref-type="fig" rid="F3">Figure 3</xref> and <xref ref-type="table" rid="T2">Table 2</xref>). Bonferroni correction for multiple comparisons was applied (i.e. <italic>p</italic> &lt; 0.05/4 = 0.0125). Within individual orientations, the most noticeable improvement in SR outputs was observed for SAG scans, with significant increases in Dice overlap across all tissue types and COR scans with significant increases in Dice overlap across all tissue types except GM<sub>subcort</sub> (<xref ref-type="fig" rid="F3">Figure 3</xref> and <xref ref-type="table" rid="T2">Table 2</xref>). AXI scans showed the highest baseline Dice score, with significant improvements only in the CSF. Additionally, we tested whether there is a significant difference between Dice scores of scans predicted from AXI, COR or SAG orientations. For all 4 main tissue types, predictions generated using AXI ULF scans showed significantly higher Dice overlap to MRR targets than predictions generated using SAG ULF scans (<xref ref-type="table" rid="T2">Table 2</xref>). Similarly, except for GM<sub>subcort</sub>, predictions generated from COR ULF scans exhibited significantly higher Dice overlap with MRR targets than those from SAG ULF scans (<xref ref-type="table" rid="T2">Table 2</xref>). We found no significant difference between Dice scores of predictions generated using AXI and COR ULF scans (<xref ref-type="table" rid="T2">Table 2</xref>).</p><p id="P23">We performed the same analyses on 8 bilateral subcortical brain regions and here too Bonferroni correction for multiple comparisons was used (i.e. <italic>p</italic> &lt; 0.05/8 = 0.00625). Considering all orientations together, three out of eight regions showed a significant increase in Dice overlap; namely, the accumbens, putamen and caudate (<xref ref-type="table" rid="T2">Table 2</xref> and <xref ref-type="supplementary-material" rid="SD1">Figure S3</xref>). When examining individual orientations, the most noticeable improvement of Dice overlap following SR was observed in accumbens, putamen, pallidum and caudate for COR and hippocampus, putamen and caudate for SAG scans. Interestingly, Dice overlap did not improve significantly for AXI scans in any of the subcortical brain regions. Nevertheless, AXI scans had the highest median baseline Dice score (<xref ref-type="supplementary-material" rid="SD1">Figure S1</xref>).</p><p id="P24">We further inspected correlations of tissue volume across participants, quantifying improvements in image quality following SR by comparing correlations between UNet outputs and MRR targets, to correlations between ULF inputs and MRR. Within the 4 global tissue types, across all scan orientations combined, we observed the greatest increases in correlation in GM<sub>subc</sub>, including both linear correlation (Pearson’s r rose from 0.79 to 0.84) and exact agreement (CCC rose from 0.59 to 0.79) (<xref ref-type="fig" rid="F4">Figure 4</xref>). The remaining tissue types (GMcort, WM, CSF) already showed high linear correlation and agreement at baseline (before SR), and therefore also smaller differences in both measures following SR (<xref ref-type="fig" rid="F4">Figure 4</xref>). When examining correlations within each specific orientation separately, a less obvious improvement was observed (<xref ref-type="table" rid="T3">Table 3</xref>). COR scans showed an increase in volume correlations for all 4 global tissue types except CSF, while AXI and SAG scans showed an increase in volume correlations for only one global tissue type (CSF and WM, respectively) (<xref ref-type="table" rid="T3">Table 3</xref>). In line with aggregate results across all scans, GM<sub>subc</sub> showed the most prominent improvement for all three scan orientations, including greatest increases for COR scans (<xref ref-type="table" rid="T3">Table 3</xref>).</p><p id="P25">We next inspected differences in volume correlations following SR in individual (bilateral) subcortical regions. Across the three orientations combined, improvements were observed for all subcortical regions except for accumbens and pallidum (<xref ref-type="supplementary-material" rid="SD1">Figure S4</xref>), with the greatest improvements in the amygdala, hippocampus and caudate (Pearson’s r rose from 0.54 to 0.64, from 0.40 to 0.49 and from 0.83 to 0.90, respectively) (<xref ref-type="supplementary-material" rid="SD1">Figure S4</xref>). Looking at individual orientations, coronal scans showed improvement in linear correlation for all subcortical regions except the accumbens, caudate and ventral diencephalon and improvement in exact agreement for all subcortical regions except the accumbens (<xref ref-type="table" rid="T4">Table 4</xref>).</p></sec><sec id="S10" sec-type="discussion"><title>Discussion</title><p id="P26">ULF MRI scanners offer a more accessible and affordable alternative for neuroimaging in LMICs, but come at the cost of reduced image quality. This study explores whether deep learning, specifically a 3D UNet model, can enhance the resolution of individual ULF scans to match MRR scans, which traditionally require three orthogonal acquisitions. The results show that the model significantly improved the quality of ULF scans, both globally and locally. Dice overlap scores for global tissue types and most subcortical regions were significantly higher in UNet outputs compared to ULF inputs, with the greatest improvements in coronal and sagittal scans. Additionally, volume correlations between predicted and MRR scans were significantly stronger for subcortical grey matter, both globally and in most individual deep-brain structures. Overall, our findings support the effectiveness of the 3D UNet in enhancing the quality of individual ULF scans and capturing regions more closely to how they appear in MRR outputs.</p><p id="P27">The ability to reconstruct a higher-resolution scan from any single ULF scan (axial, coronal or sagittal) is an improvement to existing SR approaches, that either require all three ULF scans to reconstruct a higher-resolution scan (<xref ref-type="bibr" rid="R12">Deoni et al., 2022</xref>), or require paired ULF and HF data for model training (<xref ref-type="bibr" rid="R5">Baljer et al. 2024</xref>; <xref ref-type="bibr" rid="R17">Islam et al., 2023</xref>; <xref ref-type="bibr" rid="R10">de Leeuw den Bouter et al., 2022</xref>). Our approach has multiple advantages. Firstly, the ability to generate higher-quality scans from single ULF acquisitions could reduce scanning time, making MRI data acquisition more suitable for scanning infants who cannot keep still for long periods of time and often struggle with extended scanning sessions (<xref ref-type="bibr" rid="R7">Barkovich et al., 2019</xref>). Shorter scanning sessions are also cheaper, making this approach accessible for adoption in LMICs, which could further boost neurodevelopmental research and clinical applications. Moreover, since the model requires only a single ULF scan, our approach could save significant amounts of data in instances where participants may be excluded from studies due to not having high-quality scans from all three orientations.</p><p id="P28">While our approach requires high-quality ULF data from all three orientations to train a super-resolution model, it does not require paired HF scans. One advantage of this approach is that our model can be easily retrained at individual ULF scanner sites without access to HF MRI, on age- and/or diagnosis-specific populations. The other important benefit is that we avoid the domain gap between input and target data observed in several other super-resolution studies where models are trained on paired ULF and HF scans, or even HF scans paired with synthetically down-sampled HF counterparts, which may not generalise well to empirical ULF scans (<xref ref-type="bibr" rid="R5">Baljer et al., 2024</xref>; <xref ref-type="bibr" rid="R17">Islam et al., 2023</xref>; <xref ref-type="bibr" rid="R15">Iglesias et al., 2023</xref>). The domain gap is problematic as morphometric measurements are highly dependent on the MR image contrast and noise, both of which are influenced by the field strength of the MR system (<xref ref-type="bibr" rid="R24">Laguna et al., 2022</xref>; <xref ref-type="bibr" rid="R35">Váša et al., 2024</xref>). Additionally, the presence of the domain gap presents previously used SR models with an additional translation task that they have to learn. Alongside translation from low-resolution to higher-resolution and from low SNR to high SNR, these models have to learn to bridge the domain gap by mapping between scans of subtly different contrast that depends on field strength. Our model is not presented with such a challenge and therefore has a slightly easier task to learn. It is worth acknowledging, however, that models trained on paired ULF and HF scans can learn to output higher-quality images compared to ours.</p><p id="P29">Still, our model was presented with other challenges. The contrast between grey and white matter is the least distinct at 6 months of age due to active myelination processes (<xref ref-type="bibr" rid="R6">Barkovich, 2005</xref>; <xref ref-type="bibr" rid="R13">Dubois et al., 2020</xref>; <xref ref-type="bibr" rid="R31">Prastawa et al., 2005</xref>), making the learning particularly demanding. Additionally, to be able to super-resolve any ULF scan, our model had to learn three transformations to the target MRR scan (namely AXI to MRR, COR to MRR and SAG to MRR), arguably making the task more difficult compared to only using one ULF scan orientation as an input. However, this approach enables the use of three times more training data than if training the model on a single input orientation (e.g. only AXI to MRR). In spite of the two challenges, our model improved the quality of ULF scans, closely approximating MRR scans.</p><p id="P30">We found that when looking at 4 global tissue types, among the three orientations, median Dice scores were the highest in axial scans for both ULF and UNet scans, however, the absolute improvement following SR was the greatest in coronal and sagittal scans. This suggests that in situations where only a single ULF anisotropic scan can be acquired, acquisition of an axial scan should potentially be prioritised. Some differences between orientations were also observed in the study by <xref ref-type="bibr" rid="R35">Váša and colleagues (2024)</xref>, where T2-weighted ULF COR scans showed the highest correspondence to scans acquired with a 3 T HF scanner, followed by AXI and SAG scans. It is important to note, however, that this latter study focused on adult participants.</p><p id="P31">The complexity of working with infant brain data goes beyond resolution improvement, particularly when it comes to accurate segmentation. An important limitation of our study was that scans were segmented using the widely employed SynthSeg+ segmentation tool (<xref ref-type="bibr" rid="R8">Billot et al., 2023</xref>), which was trained on adult MRI data. The infant anatomy of our scans could result in inaccurate segmentations and volume estimates, which may be especially likely because brains at such a young age exhibit an almost complete lack of contrast between grey and white matter compared to adults (<xref ref-type="bibr" rid="R13">Dubois et al., 2020</xref>). To overcome this limitation, future tools developed specifically for the segmentation of infant scans could be used to assess the model’s performance more accurately.</p><p id="P32">Another limitation of this study is that the data used to train the 3D U-Net was acquired from a single scanning site in Cape Town, from neurologically healthy 6-month-old infants only. Having such a uniform sample likely limits the generalisability of the model, especially because age can introduce significant anatomical variations that are not captured by our training set (<xref ref-type="bibr" rid="R23">Knickmeyer et al., 2008</xref>). However, it is important to note this cohort are from an context where there are high instances of psychosocial adversity and disease burden, including exposure to viruses such as HIV and substances such as alcohol during pregnancy. These adversities may contribute to greater variability in brain development within the cohort (<xref ref-type="bibr" rid="R27">Miguel et al., 2019</xref>), potentially enhancing the diversity of the training data. To assess the generalisability of the model, inference could be run on ULF scans from different scanner sites. Our model can, however, easily be retrained on a wide range of populations as it does not require paired ULF and HF data, but only three ULF scans that are much cheaper to obtain on a site-specific basis, including in LMICs.</p><p id="P33">Finally, while we show that the UNet can successfully super-resolve brain MRI scans, future research could investigate other deep learning architectures for the super-resolution of single ULF MRI scans without paired HF data. Such architectures include, but are not limited to, diffusion models, which have already been proposed as a paradigm for medical image reconstruction and enhancement (<xref ref-type="bibr" rid="R20">Kazerouni et al., 2023</xref>, <xref ref-type="bibr" rid="R21">Kim &amp; Park, 2023</xref>), and vision transformers, which has shown great promise in MRI reconstruction and SR tasks (<xref ref-type="bibr" rid="R14">Feng et al., 2021</xref>).</p><p id="P34">In conclusion, the current study demonstrates the efficacy of a deep learning 3D UNet model in enhancing the resolution of ULF infant MRI scans, significantly improving tissue volume estimation and segmentation accuracy compared to original scans. By enabling the reconstruction of higher resolution brain scans from a single ULF scan without paired HF data, the model not only reduces the scanning time–which is critical for paediatric imaging–but also paves the way for global adoption of MRI technology in LMICs, potentially advancing neurodevelopmental research in these regions.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Information</label><media xlink:href="EMS201582-supplement-Supplementary_Information.pdf" mimetype="application" mime-subtype="pdf" id="d199aAcEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S11"><title>Funding Acknowledgments</title><p>This study was supported by the Bill and Melinda Gates Foundation UNITY project [INV-032788; INV-047888], and by the Wellcome Leap 1kD programme (The First 1000 Days) [222076/Z/20/Z].</p></ack><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abate</surname><given-names>F</given-names></name><name><surname>Adu-Amankwah</surname><given-names>A</given-names></name><name><surname>Ae-Ngibise</surname><given-names>K</given-names></name><name><surname>Agbokey</surname><given-names>F</given-names></name><name><surname>Agyemang</surname><given-names>V</given-names></name><name><surname>Agyemang</surname><given-names>C</given-names></name><etal/><name><surname>Deoni</surname><given-names>S</given-names></name><name><surname>Williams</surname><given-names>S</given-names></name></person-group><article-title>UNITY: A Low-Field Magnetic Resonance Neuroimaging Initiative to Characterize Neurodevelopment in Low and Middle-Income Settings</article-title><source>Developmental Cognitive Neuroscience</source><year>2024</year><volume>69</volume><elocation-id>101397</elocation-id><pub-id pub-id-type="pmcid">PMC11315107</pub-id><pub-id pub-id-type="pmid">39029330</pub-id><pub-id pub-id-type="doi">10.1016/j.dcn.2024.101397</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anazodo</surname><given-names>UC</given-names></name><name><surname>Ng</surname><given-names>JJ</given-names></name><name><surname>Ehiogu</surname><given-names>B</given-names></name><name><surname>Obungoloch</surname><given-names>J</given-names></name><name><surname>Fatade</surname><given-names>A</given-names></name><name><surname>Mutsaerts</surname><given-names>HJMM</given-names></name><name><surname>Secca</surname><given-names>MF</given-names></name><name><surname>Diop</surname><given-names>M</given-names></name><name><surname>Opadele</surname><given-names>A</given-names></name><name><surname>Alexander</surname><given-names>DC</given-names></name><name><surname>Dada</surname><given-names>MO</given-names></name><etal/></person-group><article-title>A framework for advancing sustainable magnetic resonance imaging access in Africa</article-title><source>NMR in Biomedicine</source><year>2023</year><volume>36</volume><issue>3</issue><elocation-id>e4846</elocation-id><pub-id pub-id-type="pmid">36259628</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arnold</surname><given-names>TC</given-names></name><name><surname>Freeman</surname><given-names>CW</given-names></name><name><surname>Litt</surname><given-names>B</given-names></name><name><surname>Stein</surname><given-names>JM</given-names></name></person-group><article-title>Low-field MRI: Clinical Promise and Challenges</article-title><source>Journal of Magnetic Resonance Imaging</source><year>2022</year><volume>57</volume><issue>1</issue><pub-id pub-id-type="pmcid">PMC9771987</pub-id><pub-id pub-id-type="pmid">36120962</pub-id><pub-id pub-id-type="doi">10.1002/jmri.28408</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avants</surname><given-names>BB</given-names></name><name><surname>Tustison</surname><given-names>N</given-names></name><name><surname>Song</surname><given-names>G</given-names></name></person-group><article-title>Advanced normalization tools (ANTS)</article-title><source>Insight Journal</source><year>2009</year><volume>2</volume><issue>365</issue><fpage>1</fpage><lpage>35</lpage></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baljer</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Bourke</surname><given-names>NJ</given-names></name><name><surname>Donald</surname><given-names>KA</given-names></name><name><surname>Bradford</surname><given-names>LE</given-names></name><name><surname>Ringshaw</surname><given-names>JE</given-names></name><name><surname>Williams</surname><given-names>SR</given-names></name><name><surname>Deoni</surname><given-names>SC</given-names></name><name><surname>Williams</surname><given-names>SC</given-names></name><name><surname>Váša</surname><given-names>F</given-names></name><name><surname>Moran</surname><given-names>RJ</given-names></name></person-group><article-title>Ultra-low-field paediatric MRI in low- and middle-income countries: super-resolution using a multi-orientation U-Net</article-title><source>BioRxiv Preprint</source><year>2024</year><pub-id pub-id-type="doi">10.1101/2024.02.16.580639</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barkovich</surname><given-names>AJ</given-names></name></person-group><article-title>Magnetic resonance techniques in the assessment of myelin and myelination</article-title><source>Journal of Inherited Metabolic Disease</source><year>2005</year><volume>28</volume><issue>3</issue><fpage>311</fpage><lpage>343</lpage><pub-id pub-id-type="pmid">15868466</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barkovich</surname><given-names>MJ</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Desikan</surname><given-names>RS</given-names></name><name><surname>Barkovich</surname><given-names>AJ</given-names></name><name><surname>Xu</surname><given-names>D</given-names></name></person-group><article-title>Challenges in pediatric neuroimaging</article-title><source>NeuroImage</source><year>2019</year><volume>185</volume><fpage>793</fpage><lpage>801</lpage><pub-id pub-id-type="pmcid">PMC6197938</pub-id><pub-id pub-id-type="pmid">29684645</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.04.044</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Billot</surname><given-names>B</given-names></name><name><surname>Magdamo</surname><given-names>C</given-names></name><name><surname>Cheng</surname><given-names>Y</given-names></name><name><surname>Arnold</surname><given-names>SE</given-names></name><name><surname>Das</surname><given-names>S</given-names></name><name><surname>Iglesias</surname><given-names>JE</given-names></name></person-group><article-title>Robust machine learning segmentation for large-scale analysis of heterogeneous clinical brain MRI datasets</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2023</year><volume>120</volume><issue>9</issue><pub-id pub-id-type="pmcid">PMC9992854</pub-id><pub-id pub-id-type="pmid">36802420</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2216399120</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Çiçek</surname><given-names>Ö</given-names></name><name><surname>Abdulkadir</surname><given-names>A</given-names></name><name><surname>Lienkamp</surname><given-names>SS</given-names></name><name><surname>Brox</surname><given-names>T</given-names></name><name><surname>Ronneberger</surname><given-names>Olaf</given-names></name></person-group><source>3D u-net: Learning dense volumetric segmentation from sparse annotation</source><pub-id pub-id-type="doi">10.48550/arxiv.1606.06650</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Leeuw den Bouter</surname><given-names>ML</given-names></name><name><surname>Ippolito</surname><given-names>G</given-names></name><name><surname>O’Reilly</surname><given-names>TPA</given-names></name><name><surname>Remis</surname><given-names>RF</given-names></name><name><surname>van Gijzen</surname><given-names>MB</given-names></name><name><surname>Webb</surname><given-names>AG</given-names></name></person-group><article-title>Deep learning-based single image super-resolution for low-field MR brain images</article-title><source>Scientific Reports</source><year>2022</year><volume>12</volume><issue>1</issue><elocation-id>6362</elocation-id><pub-id pub-id-type="pmcid">PMC9013376</pub-id><pub-id pub-id-type="pmid">35430586</pub-id><pub-id pub-id-type="doi">10.1038/s41598-022-10298-6</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deoni</surname><given-names>SCL</given-names></name><name><surname>Bruchhage</surname><given-names>MMK</given-names></name><name><surname>Beauchemin</surname><given-names>J</given-names></name><name><surname>Volpe</surname><given-names>A</given-names></name><name><surname>D’Sa</surname><given-names>V</given-names></name><name><surname>Huentelman</surname><given-names>M</given-names></name><name><surname>Williams</surname><given-names>SCR</given-names></name></person-group><article-title>Accessible pediatric neuroimaging using a low field strength MRI scanner</article-title><source>NeuroImage</source><year>2021</year><volume>238</volume><elocation-id>118273</elocation-id><pub-id pub-id-type="pmid">34146712</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deoni</surname><given-names>SCL</given-names></name><name><surname>O’Muircheartaigh</surname><given-names>J</given-names></name><name><surname>Ljungberg</surname><given-names>E</given-names></name><name><surname>Huentelman</surname><given-names>M</given-names></name><name><surname>Williams</surname><given-names>SCR</given-names></name></person-group><article-title>Simultaneous high-resolution T <sub>2</sub> -weighted imaging and quantitative T <sub>2</sub> mapping at low magnetic field strengths using a multiple TE and multi-orientation acquisition approach</article-title><source>Magnetic Resonance in Medicine</source><year>2022</year><volume>88</volume><issue>3</issue><fpage>1273</fpage><lpage>1281</lpage><pub-id pub-id-type="pmcid">PMC9322579</pub-id><pub-id pub-id-type="pmid">35553454</pub-id><pub-id pub-id-type="doi">10.1002/mrm.29273</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dubois</surname><given-names>J</given-names></name><name><surname>Alison</surname><given-names>M</given-names></name><name><surname>Counsell</surname><given-names>SJ</given-names></name><name><surname>Hertz-Pannier</surname><given-names>L</given-names></name><name><surname>Hüppi</surname><given-names>PS</given-names></name><name><surname>Benders</surname><given-names>MJNL</given-names></name></person-group><article-title>MRI of the Neonatal Brain: A Review of Methodological Challenges and Neuroscientific Advances</article-title><source>Journal of Magnetic Resonance Imaging</source><year>2020</year><volume>53</volume><fpage>1318</fpage><lpage>1343</lpage><pub-id pub-id-type="pmcid">PMC8247362</pub-id><pub-id pub-id-type="pmid">32420684</pub-id><pub-id pub-id-type="doi">10.1002/jmri.27192</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feng</surname><given-names>C-M</given-names></name><name><surname>Yan</surname><given-names>Y</given-names></name><name><surname>Fu</surname><given-names>H</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Xu</surname><given-names>Y</given-names></name></person-group><article-title>Task Transformer Network for Joint MRI Reconstruction and Super-Resolution</article-title><source>Lecture Notes in Computer Science</source><year>2021</year><fpage>307</fpage><lpage>317</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-87231-1_30</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iglesias</surname><given-names>JE</given-names></name><name><surname>Billot</surname><given-names>B</given-names></name><name><surname>Balbastre</surname><given-names>Y</given-names></name><name><surname>Magdamo</surname><given-names>C</given-names></name><name><surname>Arnold</surname><given-names>SE</given-names></name><name><surname>Das</surname><given-names>S</given-names></name><name><surname>Edlow</surname><given-names>BL</given-names></name><name><surname>Alexander</surname><given-names>DC</given-names></name><name><surname>Golland</surname><given-names>P</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><article-title>SynthSR: A public AI tool to turn heterogeneous clinical brain scans into high-resolution T1-weighted images for 3D morphometry</article-title><source>Science Advances</source><year>2023</year><volume>9</volume><issue>5</issue><pub-id pub-id-type="pmcid">PMC9891693</pub-id><pub-id pub-id-type="pmid">36724222</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.add3607</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iglesias</surname><given-names>JE</given-names></name><name><surname>Billot</surname><given-names>B</given-names></name><name><surname>Balbastre</surname><given-names>Y</given-names></name><name><surname>Tabari</surname><given-names>A</given-names></name><name><surname>Conklin</surname><given-names>J</given-names></name><name><surname>González</surname><given-names>RG</given-names></name><name><surname>Alexander</surname><given-names>DC</given-names></name><name><surname>Golland</surname><given-names>P</given-names></name><name><surname>Edlow</surname><given-names>BL</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><collab>Alzheimer’s Disease Neuroimaging Initiative</collab></person-group><article-title>Joint super-resolution and synthesis of 1 mm isotropic MP-RAGE volumes from clinical MRI exams with scans of different orientation, resolution and contrast</article-title><source>NeuroImage</source><year>2021</year><volume>237</volume><elocation-id>118206</elocation-id><pub-id pub-id-type="pmcid">PMC8354427</pub-id><pub-id pub-id-type="pmid">34048902</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2021.118206</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Islam</surname><given-names>KT</given-names></name><name><surname>Zhong</surname><given-names>S</given-names></name><name><surname>Zakavi</surname><given-names>P</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Kavnoudias</surname><given-names>H</given-names></name><name><surname>Farquharson</surname><given-names>S</given-names></name><name><surname>Durbridge</surname><given-names>G</given-names></name><name><surname>Barth</surname><given-names>M</given-names></name><name><surname>McMahon</surname><given-names>KL</given-names></name><name><surname>Parizel</surname><given-names>PM</given-names></name><name><surname>Dwyer</surname><given-names>A</given-names></name><etal/></person-group><article-title>Improving portable low-field MRI image quality through image-to-image translation using paired low- and high-field images</article-title><source>Scientific Reports</source><year>2023</year><volume>13</volume><issue>1</issue><elocation-id>21183</elocation-id><pub-id pub-id-type="pmcid">PMC10692211</pub-id><pub-id pub-id-type="pmid">38040835</pub-id><pub-id pub-id-type="doi">10.1038/s41598-023-48438-1</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jalloul</surname><given-names>M</given-names></name><name><surname>Miranda-Schaeubinger</surname><given-names>M</given-names></name><name><surname>Noor</surname><given-names>AM</given-names></name><name><surname>Stein</surname><given-names>JM</given-names></name><name><surname>Amiruddin</surname><given-names>R</given-names></name><name><surname>Derbew</surname><given-names>HM</given-names></name><name><surname>Mango</surname><given-names>VL</given-names></name><name><surname>Akinola</surname><given-names>A</given-names></name><name><surname>Hart</surname><given-names>K</given-names></name><name><surname>Weygand</surname><given-names>J</given-names></name><name><surname>Pollack</surname><given-names>E</given-names></name><etal/></person-group><article-title>MRI scarcity in low- and middle-income countries</article-title><source>NMR in Biomedicine</source><year>2023</year><volume>36</volume><issue>12</issue><pub-id pub-id-type="pmid">37574441</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Bannister</surname><given-names>P</given-names></name><name><surname>Brady</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name></person-group><article-title>Improved Optimization for the Robust and Accurate Linear Registration and Motion Correction of Brain Images</article-title><source>NeuroImage</source><year>2002</year><volume>17</volume><issue>2</issue><fpage>825</fpage><lpage>841</lpage><pub-id pub-id-type="pmid">12377157</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kazerouni</surname><given-names>A</given-names></name><name><surname>Aghdam</surname><given-names>EK</given-names></name><name><surname>Heidari</surname><given-names>M</given-names></name><name><surname>Azad</surname><given-names>R</given-names></name><name><surname>Fayyaz</surname><given-names>M</given-names></name><name><surname>Hacihaliloglu</surname><given-names>I</given-names></name><name><surname>Merhof</surname><given-names>D</given-names></name></person-group><article-title>Diffusion models in medical imaging: A comprehensive survey</article-title><source>Medical Image Analysis</source><year>2023</year><volume>88</volume><elocation-id>102846</elocation-id><pub-id pub-id-type="pmid">37295311</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>J</given-names></name><name><surname>Park</surname><given-names>H</given-names></name></person-group><article-title>Adaptive Latent Diffusion Model for 3D Medical Image to Image Translation: Multi-modal Magnetic Resonance Imaging Study</article-title><source>ArXiv</source><year>2023</year><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2311.00265">https://arxiv.org/abs/2311.00265</ext-link></comment></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>D</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><article-title>Adam: A Method for Stochastic Optimization</article-title><source>Computer Science</source><year>2014</year><pub-id pub-id-type="doi">10.48550/arXiv.1412.6980</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knickmeyer</surname><given-names>RC</given-names></name><name><surname>Gouttard</surname><given-names>S</given-names></name><name><surname>Kang</surname><given-names>C</given-names></name><name><surname>Evans</surname><given-names>D</given-names></name><name><surname>Wilber</surname><given-names>K</given-names></name><name><surname>Smith</surname><given-names>JK</given-names></name><name><surname>Hamer</surname><given-names>RM</given-names></name><name><surname>Lin</surname><given-names>W</given-names></name><name><surname>Gerig</surname><given-names>G</given-names></name><name><surname>Gilmore</surname><given-names>JH</given-names></name></person-group><article-title>A Structural MRI Study of Human Brain Development from Birth to 2 Years</article-title><source>Journal of Neuroscience</source><year>2008</year><volume>28</volume><issue>47</issue><fpage>12176</fpage><lpage>12182</lpage><pub-id pub-id-type="pmcid">PMC2884385</pub-id><pub-id pub-id-type="pmid">19020011</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3479-08.2008</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laguna</surname><given-names>S</given-names></name><name><surname>Riana</surname><given-names>S</given-names></name><name><surname>Billot</surname><given-names>B</given-names></name><name><surname>Schaefer</surname><given-names>P</given-names></name><name><surname>McKaig</surname><given-names>B</given-names></name><name><surname>Goldstein</surname><given-names>JN</given-names></name><name><surname>Sheth</surname><given-names>KN</given-names></name><name><surname>Rosen</surname><given-names>MS</given-names></name><name><surname>Kimberly</surname><given-names>WT</given-names></name><name><surname>Iglesias</surname><given-names>JE</given-names></name></person-group><article-title>Super-resolution of portable low-field MRI in real scenarios: integration with denoising and domain adaptation</article-title><source>Medical Imaging with Deep Learning</source><year>2022</year></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Z</given-names></name><name><surname>Yu</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Zhou</surname><given-names>H</given-names></name><name><surname>Yang</surname><given-names>H</given-names></name><name><surname>Qiao</surname><given-names>Z</given-names></name></person-group><article-title>DeepVolume: Brain Structure and Spatial Connection-Aware Network for Brain MRI Super-Resolution</article-title><source>IEEE Transactions on Cybernetics</source><year>2021</year><volume>51</volume><issue>7</issue><fpage>3441</fpage><lpage>3454</lpage><pub-id pub-id-type="pmid">31484151</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>LI-Kuei</given-names></name></person-group><article-title>A Concordance Correlation Coefficient to Evaluate Reproducibility</article-title><source>Biometrics</source><year>1989</year><volume>45</volume><issue>1</issue><fpage>255</fpage><pub-id pub-id-type="pmid">2720055</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miguel</surname><given-names>PM</given-names></name><name><surname>Pereira</surname><given-names>LO</given-names></name><name><surname>Silveira</surname><given-names>PP</given-names></name><name><surname>Meaney</surname><given-names>MJ</given-names></name></person-group><article-title>Early environmental influences on the development of children’s brain structure and function</article-title><source>Developmental Medicine &amp; Child Neurology</source><year>2019</year><volume>61</volume><issue>10</issue><fpage>1127</fpage><lpage>1133</lpage><pub-id pub-id-type="pmid">30740660</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ogbole</surname><given-names>GI</given-names></name><name><surname>Adeyomoye</surname><given-names>AO</given-names></name><name><surname>Badu-Peprah</surname><given-names>A</given-names></name><name><surname>Mensah</surname><given-names>Y</given-names></name><name><surname>Nzeh</surname><given-names>DA</given-names></name></person-group><article-title>Survey of magnetic resonance imaging availability in West Africa</article-title><source>Pan African Medical Journal</source><year>2018</year><volume>30</volume><issue>240</issue><pub-id pub-id-type="pmcid">PMC6295297</pub-id><pub-id pub-id-type="pmid">30574259</pub-id><pub-id pub-id-type="doi">10.11604/pamj.2018.30.240.14000</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearson</surname><given-names>K</given-names></name></person-group><article-title>Mathematical contributions to the theory of evolution. III. Regression, heredity, and panmixia</article-title><source>Philosophical Transactions A</source><year>1896</year><volume>373</volume><fpage>253</fpage><lpage>318</lpage></element-citation></ref><ref id="R30"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Penny</surname><given-names>WD</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Ashburner</surname><given-names>JT</given-names></name><name><surname>Kiebel</surname><given-names>SJK</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name></person-group><source>Statistical Parametric Mapping: The Analysis of Functional Brain Images</source><edition>1</edition><publisher-name>Academic Press</publisher-name><year>2006</year></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prastawa</surname><given-names>M</given-names></name><name><surname>Gilmore</surname><given-names>JH</given-names></name><name><surname>Lin</surname><given-names>W</given-names></name><name><surname>Gerig</surname><given-names>G</given-names></name></person-group><article-title>Automatic segmentation of MR images of the developing newborn brain</article-title><source>Medical Image Analysis</source><year>2005</year><volume>9</volume><issue>5</issue><fpage>457</fpage><lpage>466</lpage><pub-id pub-id-type="pmid">16019252</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Ronneberger</surname><given-names>O</given-names></name><name><surname>Fischer</surname><given-names>P</given-names></name><name><surname>Brox</surname><given-names>T</given-names></name></person-group><article-title>U-Net: Convolutional Networks for Biomedical Image Segmentation</article-title><year>2015</year><month>May</month><day>18</day><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1505.04597">https://arxiv.org/abs/1505.04597</ext-link></comment></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ryan</surname><given-names>ME</given-names></name><name><surname>Jaju</surname><given-names>A</given-names></name></person-group><article-title>Revolutionizing pediatric neuroimaging: the era of CT, MRI, and beyond</article-title><source>Child’s Nervous System</source><year>2023</year><volume>39</volume><issue>10</issue><pub-id pub-id-type="pmid">37380927</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Beek</surname><given-names>EJR</given-names></name><name><surname>Kuhl</surname><given-names>C</given-names></name><name><surname>Anzai</surname><given-names>Y</given-names></name><name><surname>Desmond</surname><given-names>P</given-names></name><name><surname>Ehman</surname><given-names>RL</given-names></name><name><surname>Gong</surname><given-names>Q</given-names></name><name><surname>Gold</surname><given-names>G</given-names></name><name><surname>Gulani</surname><given-names>V</given-names></name><name><surname>Hall-Craggs</surname><given-names>M</given-names></name><name><surname>Leiner</surname><given-names>T</given-names></name><name><surname>Lim</surname><given-names>CCT</given-names></name><etal/></person-group><article-title>Value of MRI in medicine: More than just another test?</article-title><source>Journal of Magnetic Resonance Imaging</source><year>2018</year><volume>49</volume><issue>7</issue><fpage>e14</fpage><lpage>e25</lpage><pub-id pub-id-type="pmcid">PMC7036752</pub-id><pub-id pub-id-type="pmid">30145852</pub-id><pub-id pub-id-type="doi">10.1002/jmri.26211</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Váša</surname><given-names>F</given-names></name><name><surname>Bennallick</surname><given-names>C</given-names></name><name><surname>Bourke</surname><given-names>NJ</given-names></name><name><surname>Padormo</surname><given-names>F</given-names></name><name><surname>Baljer</surname><given-names>L</given-names></name><name><surname>Briski</surname><given-names>U</given-names></name><name><surname>Cawley</surname><given-names>P</given-names></name><name><surname>Arichi</surname><given-names>T</given-names></name><name><surname>Wood</surname><given-names>TC</given-names></name><name><surname>Lythgoe</surname><given-names>DJ</given-names></name><name><surname>Dell’Acqua</surname><given-names>F</given-names></name><etal/></person-group><article-title>Ultra-low-field brain MRI morphometry: test-retest reliability and correspondence to high-field MRI</article-title><source>BioRxiv</source><year>2024</year><pub-id pub-id-type="doi">10.1101/2024.08.14.607942</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wald</surname><given-names>LL</given-names></name><name><surname>McDaniel</surname><given-names>PC</given-names></name><name><surname>Witzel</surname><given-names>T</given-names></name><name><surname>Stockmann</surname><given-names>JP</given-names></name><name><surname>Cooley</surname><given-names>CZ</given-names></name></person-group><article-title>Low-cost and portable MRI</article-title><source>Journal of Magnetic Resonance Imaging</source><year>2019</year><volume>52</volume><issue>3</issue><fpage>686</fpage><lpage>696</lpage><pub-id pub-id-type="pmcid">PMC10644353</pub-id><pub-id pub-id-type="pmid">31605435</pub-id><pub-id pub-id-type="doi">10.1002/jmri.26942</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilcoxon</surname><given-names>F</given-names></name></person-group><article-title>Individual Comparisons by Ranking Methods</article-title><source>Biometrics Bulletin</source><year>1945</year><volume>1</volume><issue>6</issue><fpage>80</fpage><lpage>83</lpage><pub-id pub-id-type="pmid">18903631</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zieff</surname><given-names>MR</given-names></name><name><surname>Miles</surname><given-names>M</given-names></name><name><surname>Mbale</surname><given-names>E</given-names></name><name><surname>Eastman</surname><given-names>E</given-names></name><name><surname>Ginnell</surname><given-names>L</given-names></name><name><surname>Williams</surname><given-names>SCR</given-names></name><name><surname>Jones</surname><given-names>DK</given-names></name><name><surname>Alexander</surname><given-names>DC</given-names></name><name><surname>Wijeratne</surname><given-names>PA</given-names></name><name><surname>Gabard-Durnam</surname><given-names>LJ</given-names></name><name><surname>Klepac-Ceraj</surname><given-names>V</given-names></name><etal/></person-group><article-title>Characterizing developing executive functions in the first 1000 days in South Africa and Malawi: The Khula Study</article-title><source>Wellcome Open Research</source><year>2024</year><volume>9</volume><issue>157</issue><pub-id pub-id-type="doi">10.12688/wellcomeopenres.19638.1</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Overview of 3D U-Net architecture.</title><p>The model takes either AXI, COR or SAG scan as an input and generates a super-resolved MRR scan as an output. Blue blocks represent convolution followed by group normalisation and ReLU. Orange blocks represent max pooling and yellow boxes represent up-sampling. Abbreviations: GroupNorm = group normalisation, ReLU = rectified linear unit.</p></caption><graphic xlink:href="EMS201582-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Scans from a single example test subject.</title><p>The first row displays a ULF (AXI) input scan, the middle row shows the 3D UNet output, and the bottom row shows the MRR scan, which serves as both the super-resolution target and reference standard. Left to right: Axial slice, coronal slice, sagittal slice, magnified features from a coronal slice, segmentation masks on axial slice, and segmentation masks on coronal slice.</p></caption><graphic xlink:href="EMS201582-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Dice overlap scores between UNet predictions and MRR scans, and between ULF and MRR scans, for 4 global tissue types.</title><p>The ‘All orientations’ plots represent Dice scores obtained by combining axial, coronal and sagittal scans and have 66 data points. ‘Axial’, ‘Coronal’ and ‘Sagittal’ plots represent Dice scores obtained from each orientation separately and have 22 data points each.</p></caption><graphic xlink:href="EMS201582-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Tissue volume correlations between MRR and ULF versus UNet scans for 4 global tissue types.</title><p>Blue boxes under the correlation plots report Pearson’s correlation coefficient and Lin’s concordance correlation coefficient (CCC).</p></caption><graphic xlink:href="EMS201582-f004"/></fig><table-wrap id="T1" position="float" orientation="portrait"><label>Table 1</label><caption><title>Image quality metrics for input ULF and output UNet scans.</title><p>Normalized root-mean-squared error (NRMSE) and structural similarity index measure (SSIM) were calculated by comparing model inputs and outputs to reference MRR scans. Values correspond to mean ± standard deviation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="top">Comparison</th><th align="left" valign="top">NRMSE (↓)</th><th align="left" valign="top">SSIM (↑)</th></tr></thead><tbody><tr><td align="left" valign="top">ULF vs MRR</td><td align="center" valign="top">0.538 ± 0.102</td><td align="center" valign="top">0.564 ± 0.197</td></tr><tr><td align="left" valign="top">UNet vs MRR</td><td align="center" valign="top">0.521 ± 0.0911</td><td align="center" valign="top">0.752 ± 0.133</td></tr></tbody></table></table-wrap><table-wrap id="T2" position="float" orientation="portrait"><label>Table 2</label><caption><title>Wilcoxon signed-rank test applied to Dice scores.</title><p>Wilcoxon signed-rank test applied to Dice scores between original and predicted scans (all orientations combined and across AXI, COR, and SAG), as well as between predicted Dice scores for each pair of orientations. p-value and RBC are reported for each comparison. RBC is a measure of effect size (strength of the difference between two groups) ranging between -1 and 1, where the closer the RBC is to ±1, the stronger the effect. Abbreviations: RBC = rank biserial correlation, orig. = original, pred. = predicted, CSF = cerebrospinal fluid, WM = white matter, GMcort = cortical grey matter, GMsubcat = subcortical grey matter.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="top" rowspan="2">Region</th><th align="center" valign="top" colspan="2">All<sub>orig</sub> VS All<sub>pred</sub></th><th align="center" valign="top" colspan="2">AXI<sub>orig</sub> VS AXI<sub>pred</sub></th><th align="center" valign="top" colspan="2">COR<sub>orig</sub> VS COR<sub>pred</sub></th><th align="center" valign="top" colspan="2">SAG<sub>orig</sub> VS SAG<sub>pred</sub></th><th align="center" valign="top" colspan="2">AXI<sub>pred</sub> VS COR<sub>pred</sub></th><th align="center" valign="top" colspan="2">AXI<sub>pred</sub> VS SAG<sub>pred</sub></th><th align="center" valign="top" colspan="2">COR<sub>pred</sub> VS SAG<sub>pred</sub></th></tr><tr><th align="left" valign="top" style="border-top: solid thin">p-value</th><th align="center" valign="top" style="border-top: solid thin">RBC</th><th align="center" valign="top" style="border-top: solid thin">p-value</th><th align="center" valign="top" style="border-top: solid thin">RBC</th><th align="center" valign="top" style="border-top: solid thin">p-value</th><th align="center" valign="top" style="border-top: solid thin">RBC</th><th align="center" valign="top" style="border-top: solid thin">p-value</th><th align="center" valign="top" style="border-top: solid thin">RBC</th><th align="center" valign="top" style="border-top: solid thin">p-value</th><th align="center" valign="top" style="border-top: solid thin">RBC</th><th align="center" valign="top" style="border-top: solid thin">p-value</th><th align="center" valign="top" style="border-top: solid thin">RBC</th><th align="center" valign="top" style="border-top: solid thin">p-value</th><th align="center" valign="top" style="border-top: solid thin">RBC</th></tr></thead><tbody><tr><td align="left" valign="top">CSF</td><td align="center" valign="top">&lt;.01*</td><td align="center" valign="top">0.95</td><td align="center" valign="top">&lt;.01*</td><td align="center" valign="top">0.76</td><td align="center" valign="top">&lt;.01*</td><td align="center" valign="top">1.0</td><td align="center" valign="top">&lt;.01*</td><td align="center" valign="top">1.0</td><td align="center" valign="top">.47</td><td align="center" valign="top">0.02</td><td align="center" valign="top">&lt;.00l*</td><td align="center" valign="top">0.88</td><td align="center" valign="top">&lt;.01*</td><td align="center" valign="top">0.92</td></tr><tr><td align="left" valign="top">WM</td><td align="center" valign="top">&lt;.00l*</td><td align="center" valign="top">0.61</td><td align="center" valign="top">.93</td><td align="center" valign="top">-0.35</td><td align="center" valign="top">&lt;.00l*</td><td align="center" valign="top">0.85</td><td align="center" valign="top">&lt;.00l*</td><td align="center" valign="top">0.91</td><td align="center" valign="top">.14</td><td align="center" valign="top">0.27</td><td align="center" valign="top">&lt;.00l*</td><td align="center" valign="top">0.96</td><td align="center" valign="top">&lt;.00l*</td><td align="center" valign="top">0.91</td></tr><tr><td align="left" valign="top">GM<sub>cort</sub></td><td align="center" valign="top">&lt;.01*</td><td align="center" valign="top">0.75</td><td align="center" valign="top">.27</td><td align="center" valign="top">0.15</td><td align="center" valign="top">&lt;.00l*</td><td align="center" valign="top">0.91</td><td align="center" valign="top">&lt;.00l*</td><td align="center" valign="top">0.95</td><td align="center" valign="top">.36</td><td align="center" valign="top">0.09</td><td align="center" valign="top">&lt;.01*</td><td align="center" valign="top">0.97</td><td align="center" valign="top">&lt;.00l*</td><td align="center" valign="top">0.99</td></tr><tr><td align="left" valign="top">GM<sub>subcort</sub></td><td align="center" valign="top">&lt;.00l*</td><td align="center" valign="top">0.52</td><td align="center" valign="top">.91</td><td align="center" valign="top">-0.33</td><td align="center" valign="top">.0013</td><td align="center" valign="top">0.71</td><td align="center" valign="top">&lt;.00l*</td><td align="center" valign="top">0.87</td><td align="center" valign="top">.24</td><td align="center" valign="top">0.18</td><td align="center" valign="top">&lt;.01*</td><td align="center" valign="top">0.79</td><td align="center" valign="top">.0013</td><td align="center" valign="top">0.71</td></tr><tr style="border-top: solid thin"><td align="left" valign="top">Thalamus</td><td align="center" valign="top">.66</td><td align="center" valign="top">-0.06</td><td align="center" valign="top">.64</td><td align="center" valign="top">-0.08</td><td align="center" valign="top">.46</td><td align="center" valign="top">0.02</td><td align="center" valign="top">.70</td><td align="center" valign="top">-0.12</td><td align="center" valign="top">.34</td><td align="center" valign="top">0.11</td><td align="center" valign="top">.0037*</td><td align="center" valign="top">0.64</td><td align="center" valign="top">.015</td><td align="center" valign="top">0.53</td></tr><tr><td align="left" valign="top">Amygdala</td><td align="center" valign="top">.062</td><td align="center" valign="top">0.22</td><td align="center" valign="top">.14</td><td align="center" valign="top">0.27</td><td align="center" valign="top">.45</td><td align="center" valign="top">0.04</td><td align="center" valign="top">.10</td><td align="center" valign="top">0.32</td><td align="center" valign="top">.15</td><td align="center" valign="top">0.26</td><td align="center" valign="top">.073</td><td align="center" valign="top">.36</td><td align="center" valign="top">.10</td><td align="center" valign="top">0.31</td></tr><tr><td align="left" valign="top">Accumbens</td><td align="center" valign="top">.0036*</td><td align="center" valign="top">0.38</td><td align="center" valign="top">.88</td><td align="center" valign="top">-0.29</td><td align="center" valign="top">&lt;.00l*</td><td align="center" valign="top">0.74</td><td align="center" valign="top">.078</td><td align="center" valign="top">0.35</td><td align="center" valign="top">.10</td><td align="center" valign="top">0.31</td><td align="center" valign="top">.0033*</td><td align="center" valign="top">0.64</td><td align="center" valign="top">.032</td><td align="center" valign="top">0.45</td></tr><tr><td align="left" valign="top">Hippocampus</td><td align="center" valign="top">.028</td><td align="center" valign="top">0.27</td><td align="center" valign="top">.22</td><td align="center" valign="top">0.19</td><td align="center" valign="top">.73</td><td align="center" valign="top">-0.15</td><td align="center" valign="top">.0018*</td><td align="center" valign="top">0.68</td><td align="center" valign="top">.44</td><td align="center" valign="top">0.04</td><td align="center" valign="top">.032</td><td align="center" valign="top">0.45</td><td align="center" valign="top">.010</td><td align="center" valign="top">0.56</td></tr><tr><td align="left" valign="top">Putamen</td><td align="center" valign="top">&lt;.01*</td><td align="center" valign="top">0.55</td><td align="center" valign="top">.95</td><td align="center" valign="top">-0.42</td><td align="center" valign="top">&lt;.01*</td><td align="center" valign="top">0.94</td><td align="center" valign="top">&lt;.00l*</td><td align="center" valign="top">0.91</td><td align="center" valign="top">.33</td><td align="center" valign="top">0.11</td><td align="center" valign="top">.0078</td><td align="center" valign="top">0.58</td><td align="center" valign="top">.010</td><td align="center" valign="top">0.56</td></tr><tr><td align="left" valign="top">Pallidum</td><td align="center" valign="top">.016</td><td align="center" valign="top">0.30</td><td align="center" valign="top">.81</td><td align="center" valign="top">-0.21</td><td align="center" valign="top">.0042*</td><td align="center" valign="top">0.63</td><td align="center" valign="top">.11</td><td align="center" valign="top">0.30</td><td align="center" valign="top">.24</td><td align="center" valign="top">0.18</td><td align="center" valign="top">.0021*</td><td align="center" valign="top">0.68</td><td align="center" valign="top">.053</td><td align="center" valign="top">0.40</td></tr><tr><td align="left" valign="top">Caudate</td><td align="center" valign="top">&lt;.01*</td><td align="center" valign="top">0.59</td><td align="center" valign="top">.98</td><td align="center" valign="top">-0.52</td><td align="center" valign="top">&lt;.00l*</td><td align="center" valign="top">0.95</td><td align="center" valign="top">&lt;.00l*</td><td align="center" valign="top">0.85</td><td align="center" valign="top">.10</td><td align="center" valign="top">0.32</td><td align="center" valign="top">.0021*</td><td align="center" valign="top">0.68</td><td align="center" valign="top">.010</td><td align="center" valign="top">0.50</td></tr><tr><td align="left" valign="top">Diencephalon</td><td align="center" valign="top">.089</td><td align="center" valign="top">0.19</td><td align="center" valign="top">.51</td><td align="center" valign="top">0.00</td><td align="center" valign="top">.15</td><td align="center" valign="top">0.26</td><td align="center" valign="top">.073</td><td align="center" valign="top">0.36</td><td align="center" valign="top">.34</td><td align="center" valign="top">0.11</td><td align="center" valign="top">.016</td><td align="center" valign="top">0.52</td><td align="center" valign="top">.049</td><td align="center" valign="top">0.41</td></tr></tbody></table></table-wrap><table-wrap id="T3" position="float" orientation="portrait"><label>Table 3</label><caption><title>Volume correlations for 4 global tissue types per orientation.</title><p>Volume correlations between MRR and ULF, MRR and UNet volumes and differences between them (Δ Pearson’s r and Δ CCC).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" rowspan="3">Region</th><th align="center" valign="top" colspan="2">AX</th><th align="center" valign="top" colspan="2">COR</th><th align="center" valign="top" colspan="2">SAG</th></tr><tr style="border-top: solid thin"><th align="center" valign="top">Pearson’s r</th><th align="center" valign="top">CCC</th><th align="center" valign="top">Pearson’s r</th><th align="center" valign="top">CCC</th><th align="center" valign="top">Pearson’s r</th><th align="center" valign="top">CCC</th></tr><tr style="border-top: solid thin"><th align="center" valign="top" colspan="6">Volume correlation between MRR and ULF</th></tr></thead><tbody><tr><td align="left" valign="top">CSF</td><td align="center" valign="top">0.97</td><td align="center" valign="top">0.95</td><td align="center" valign="top">0.98</td><td align="center" valign="top">0.87</td><td align="center" valign="top">0.98</td><td align="center" valign="top">0.96</td></tr><tr><td align="left" valign="top">WM</td><td align="center" valign="top">0.98</td><td align="center" valign="top">0.98</td><td align="center" valign="top">0.95</td><td align="center" valign="top">0.89</td><td align="center" valign="top">0.98</td><td align="center" valign="top">0.94</td></tr><tr><td align="left" valign="top">GM<sub>cort</sub></td><td align="center" valign="top">0.98</td><td align="center" valign="top">0.92</td><td align="center" valign="top">0.93</td><td align="center" valign="top">0.92</td><td align="center" valign="top">0.98</td><td align="center" valign="top">0.98</td></tr><tr><td align="left" valign="top">GM<sub>subcort</sub></td><td align="center" valign="top">0.80</td><td align="center" valign="top">0.72</td><td align="center" valign="top">0.83</td><td align="center" valign="top">0.54</td><td align="center" valign="top">0.83</td><td align="center" valign="top">0.55</td></tr><tr style="border-top: solid thin"><td align="center" valign="top"/><td align="center" valign="top" colspan="6">Volume correlation between MRR and UNet</td></tr><tr style="border-top: solid thin"><td align="left" valign="top">CSF</td><td align="center" valign="top">0.98</td><td align="center" valign="top">0.93</td><td align="center" valign="top">0.97</td><td align="center" valign="top">0.96</td><td align="center" valign="top">0.97</td><td align="center" valign="top">0.91</td></tr><tr><td align="left" valign="top">WM</td><td align="center" valign="top">0.98</td><td align="center" valign="top">0.97</td><td align="center" valign="top">0.97</td><td align="center" valign="top">0.95</td><td align="center" valign="top">0.97</td><td align="center" valign="top">0.94</td></tr><tr><td align="left" valign="top">GM<sub>cort</sub></td><td align="center" valign="top">0.96</td><td align="center" valign="top">0.92</td><td align="center" valign="top">0.99</td><td align="center" valign="top">0.94</td><td align="center" valign="top">0.98</td><td align="center" valign="top">0.86</td></tr><tr><td align="left" valign="top">GM<sub>subcort</sub></td><td align="center" valign="top">0.86</td><td align="center" valign="top">0.79</td><td align="center" valign="top">0.85</td><td align="center" valign="top">0.81</td><td align="center" valign="top">0.81</td><td align="center" valign="top">0.75</td></tr><tr style="border-top: solid thin"><td align="center" valign="top"/><td align="center" valign="top" colspan="6">Δ volume correlation (Ml 1R vs UNet - MRR vs ULF)</td></tr><tr style="border-top: solid thin"><td align="left" valign="top">CSF</td><td align="center" valign="top">0.01</td><td align="center" valign="top">-0.02</td><td align="center" valign="top">-0.01</td><td align="center" valign="top">0.09</td><td align="center" valign="top">-0.01</td><td align="center" valign="top">-0.05</td></tr><tr><td align="left" valign="top">WM</td><td align="center" valign="top">0</td><td align="center" valign="top">-0.01</td><td align="center" valign="top">0.02</td><td align="center" valign="top">0.06</td><td align="center" valign="top">-0.01</td><td align="center" valign="top">0.0</td></tr><tr><td align="left" valign="top">GM<sub>cort</sub></td><td align="center" valign="top">-0.02</td><td align="center" valign="top">0.0</td><td align="center" valign="top">0.06</td><td align="center" valign="top">0.02</td><td align="center" valign="top">0</td><td align="center" valign="top">-0.12</td></tr><tr><td align="left" valign="top">GM<sub>subcort</sub></td><td align="center" valign="top">0.06</td><td align="center" valign="top">0.07</td><td align="center" valign="top">0.02</td><td align="center" valign="top">0.27</td><td align="center" valign="top">-0.02</td><td align="center" valign="top">0.20</td></tr></tbody></table></table-wrap><table-wrap id="T4" position="float" orientation="portrait"><label>Table 4</label><caption><title>Volume correlations for 8 subcortical regions per orientation.</title><p>Volume correlations between MRR and ULF, MRR and UNet and differences between them (Δ Pearson’s r and Δ CCC).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" rowspan="3">Region</th><th align="center" valign="top" colspan="2">AXI</th><th align="center" valign="top" colspan="2">COR</th><th align="center" valign="top" colspan="2">SAG</th></tr><tr style="border-top: solid thin"><th align="center" valign="top">Pearson’s r</th><th align="center" valign="top">CCC</th><th align="center" valign="top">Pearson’s r</th><th align="center" valign="top">CCC</th><th align="center" valign="top">Pearson’s r</th><th align="center" valign="top">CCC</th></tr><tr style="border-top: solid thin"><th align="center" valign="top" colspan="6">Volume correlation between MRR and ULF</th></tr></thead><tbody><tr><td align="left" valign="top">Thalamus</td><td align="center" valign="top">0.80</td><td align="center" valign="top">0.66</td><td align="center" valign="top">0.84</td><td align="center" valign="top">0.70</td><td align="center" valign="top">0.84</td><td align="center" valign="top">0.74</td></tr><tr><td align="left" valign="top">Amygdala</td><td align="center" valign="top">0.59</td><td align="center" valign="top">0.34</td><td align="center" valign="top">0.69</td><td align="center" valign="top">0.45</td><td align="center" valign="top">0.42</td><td align="center" valign="top">0.20</td></tr><tr><td align="left" valign="top">Accumbens</td><td align="center" valign="top">0.77</td><td align="center" valign="top">0.76</td><td align="center" valign="top">0.90</td><td align="center" valign="top">0.88</td><td align="center" valign="top">0.65</td><td align="center" valign="top">0.61</td></tr><tr><td align="left" valign="top">Hippocampus</td><td align="center" valign="top">0.27</td><td align="center" valign="top">0.22</td><td align="center" valign="top">0.56</td><td align="center" valign="top">0.41</td><td align="center" valign="top">0.50</td><td align="center" valign="top">0.21</td></tr><tr><td align="left" valign="top">Putamen</td><td align="center" valign="top">0.82</td><td align="center" valign="top">0.80</td><td align="center" valign="top">0.75</td><td align="center" valign="top">0.49</td><td align="center" valign="top">0.70</td><td align="center" valign="top">0.45</td></tr><tr><td align="left" valign="top">Pallidum</td><td align="center" valign="top">0.79</td><td align="center" valign="top">0.66</td><td align="center" valign="top">0.66</td><td align="center" valign="top">0.37</td><td align="center" valign="top">0.70</td><td align="center" valign="top">0.35</td></tr><tr><td align="left" valign="top">Caudate</td><td align="center" valign="top">0.93</td><td align="center" valign="top">0.87</td><td align="center" valign="top">0.91</td><td align="center" valign="top">0.80</td><td align="center" valign="top">0.85</td><td align="center" valign="top">0.80</td></tr><tr><td align="left" valign="top">Diencephalon</td><td align="center" valign="top">0.59</td><td align="center" valign="top">0.51</td><td align="center" valign="top">0.70</td><td align="center" valign="top">0.35</td><td align="center" valign="top">0.73</td><td align="center" valign="top">0.60</td></tr><tr style="border-top: solid thin"><td align="left" valign="top"/><td align="center" valign="top" colspan="6">Volume correlation between MRR and UNet</td></tr><tr style="border-top: solid thin"><td align="left" valign="top">Thalamus</td><td align="center" valign="top">0.85</td><td align="center" valign="top">0.84</td><td align="center" valign="top">0.86</td><td align="center" valign="top">0.83</td><td align="center" valign="top">0.80</td><td align="center" valign="top">0.79</td></tr><tr><td align="left" valign="top">Amygdala</td><td align="center" valign="top">0.69</td><td align="center" valign="top">0.48</td><td align="center" valign="top">0.74</td><td align="center" valign="top">0.53</td><td align="center" valign="top">0.49</td><td align="center" valign="top">0.28</td></tr><tr><td align="left" valign="top">Accumbens</td><td align="center" valign="top">0.61</td><td align="center" valign="top">0.60</td><td align="center" valign="top">0.71</td><td align="center" valign="top">0.68</td><td align="center" valign="top">0.63</td><td align="center" valign="top">0.55</td></tr><tr><td align="left" valign="top">Hippocampus</td><td align="center" valign="top">0.47</td><td align="center" valign="top">0.32</td><td align="center" valign="top">0.80</td><td align="center" valign="top">0.75</td><td align="center" valign="top">0.29</td><td align="center" valign="top">0.19</td></tr><tr><td align="left" valign="top">Putamen</td><td align="center" valign="top">0.71</td><td align="center" valign="top">0.68</td><td align="center" valign="top">0.79</td><td align="center" valign="top">0.76</td><td align="center" valign="top">0.73</td><td align="center" valign="top">0.73</td></tr><tr><td align="left" valign="top">Pallidum</td><td align="center" valign="top">0.67</td><td align="center" valign="top">0.51</td><td align="center" valign="top">0.69</td><td align="center" valign="top">0.46</td><td align="center" valign="top">0.50</td><td align="center" valign="top">0.38</td></tr><tr><td align="left" valign="top">Caudate</td><td align="center" valign="top">0.89</td><td align="center" valign="top">0.89</td><td align="center" valign="top">0.90</td><td align="center" valign="top">0.89</td><td align="center" valign="top">0.92</td><td align="center" valign="top">0.90</td></tr><tr><td align="left" valign="top">Diencephalon</td><td align="center" valign="top">0.83</td><td align="center" valign="top">0.77</td><td align="center" valign="top">0.65</td><td align="center" valign="top">0.57</td><td align="center" valign="top">0.58</td><td align="center" valign="top">0.50</td></tr><tr style="border-top: solid thin"><td align="center" valign="top"/><td align="center" valign="top" colspan="6">Δ volume correlation (MRR vs UNet - MRR vs ULF)</td></tr><tr style="border-top: solid thin"><td align="left" valign="top">Thalamus</td><td align="center" valign="top">0.05</td><td align="center" valign="top">0.18</td><td align="center" valign="top">0.02</td><td align="center" valign="top">0.13</td><td align="center" valign="top">-0.04</td><td align="center" valign="top">0.05</td></tr><tr><td align="left" valign="top">Amygdala</td><td align="center" valign="top">0.1</td><td align="center" valign="top">0.14</td><td align="center" valign="top">0.05</td><td align="center" valign="top">0.08</td><td align="center" valign="top">0.07</td><td align="center" valign="top">0.08</td></tr><tr><td align="left" valign="top">Accumbens</td><td align="center" valign="top">-0.16</td><td align="center" valign="top">0.16</td><td align="center" valign="top">-0.19</td><td align="center" valign="top">-0.20</td><td align="center" valign="top">-0.02</td><td align="center" valign="top">-0.06</td></tr><tr><td align="left" valign="top">Hippocampus</td><td align="center" valign="top">0.2</td><td align="center" valign="top">0.10</td><td align="center" valign="top">0.24</td><td align="center" valign="top">0.34</td><td align="center" valign="top">-0.21</td><td align="center" valign="top">-0.02</td></tr><tr><td align="left" valign="top">Putamen</td><td align="center" valign="top">-0.11</td><td align="center" valign="top">-0.12</td><td align="center" valign="top">0.04</td><td align="center" valign="top">0.27</td><td align="center" valign="top">0.03</td><td align="center" valign="top">0.28</td></tr><tr><td align="left" valign="top">Pallidum</td><td align="center" valign="top">-0.12</td><td align="center" valign="top">-0.15</td><td align="center" valign="top">0.03</td><td align="center" valign="top">0.06</td><td align="center" valign="top">-0.2</td><td align="center" valign="top">0.03</td></tr><tr><td align="left" valign="top">Caudate</td><td align="center" valign="top">-0.4</td><td align="center" valign="top">0.02</td><td align="center" valign="top">-0.01</td><td align="center" valign="top">0.09</td><td align="center" valign="top">0.07</td><td align="center" valign="top">0.10</td></tr><tr><td align="left" valign="top">Diencephalon</td><td align="center" valign="top">0.24</td><td align="center" valign="top">0.26</td><td align="center" valign="top">-0.05</td><td align="center" valign="top">0.22</td><td align="center" valign="top">-0.15</td><td align="center" valign="top">-0.10</td></tr></tbody></table></table-wrap></floats-group></article>