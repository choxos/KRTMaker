<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS202327</article-id><article-id pub-id-type="doi">10.1101/2024.12.20.629717</article-id><article-id pub-id-type="archive">PPR958146</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">2</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>An improved neural network model enables worm tracking in challenging conditions and increases signal-to-noise ratio in phenotypic screens</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Weheliye</surname><given-names>Weheliye H</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Rodriguez</surname><given-names>Javier</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Feriani</surname><given-names>Luigi</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="fn" rid="FN1">*</xref></contrib><contrib contrib-type="author"><name><surname>Javer</surname><given-names>Avelino</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="fn" rid="FN2">#</xref></contrib><contrib contrib-type="author"><name><surname>Uhlmann</surname><given-names>Virginie</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Brown</surname><given-names>André EX</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib></contrib-group><aff id="A1"><label>1</label>MRC Laboratory of Medical Sciences, London, UK</aff><aff id="A2"><label>2</label>Institute of Clinical Sciences, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/041kmwe10</institution-id><institution>Imperial College London</institution></institution-wrap>, <city>London</city>, <country country="GB">UK</country></aff><aff id="A3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02catss52</institution-id><institution>European Bioinformatics Institute (EMBL-EBI)</institution></institution-wrap>, <addr-line>Wellcome Genome Campus</addr-line>, <city>Cambridge</city>, <country country="GB">UK</country></aff><author-notes><corresp id="CR1">Correspondence to: <email>andre.brown@imperial.ac.uk</email>
</corresp><fn fn-type="present-address" id="FN1"><label>*</label><p id="P1">Present address: Australian Synchrotron - ANSTO, Melbourne, Australia</p></fn><fn id="FN2"><label>#</label><p id="P2">Apheris, Berlin, Germany</p></fn></author-notes><pub-date pub-type="nihms-submitted"><day>17</day><month>01</month><year>2025</year></pub-date><pub-date pub-type="preprint"><day>21</day><month>12</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P3">High-resolution posture tracking of <italic>C. elegans</italic> has applications in genetics, neuroscience, and drug screening. While classic methods can reliably track isolated worms on uniform backgrounds, they fail when worms overlap, coil, or move in complex environments. Model-based tracking and deep learning approaches have addressed these issues to an extent, but there is still significant room for improvement in tracking crawling worms. Here we train a version of the DeepTangle algorithm developed for swimming worms using a combination of data derived from Tierpsy tracker and hand-annotated data for more difficult cases. DeepTangleCrawl (DTC) outperforms existing methods, reducing failure rates and producing more continuous, gap-free worm trajectories that are less likely to be interrupted by collisions between worms or self-intersecting postures (coils). We show that DTC enables the analysis of previously inaccessible behaviours and increases the signal-to-noise ratio in phenotypic screens, even for data that was specifically collected to be compatible with legacy trackers including low worm density and thin bacterial lawns. DTC broadens the applicability of high-throughput worm imaging to more complex behaviours that involve worm-worm interactions and more naturalistic environments including thicker bacterial lawns.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P4">High-resolution tracking that measures both position and posture has long been applied to the nematode <italic>C. elegans</italic><sup><xref ref-type="bibr" rid="R1">1</xref>–<xref ref-type="bibr" rid="R12">12</xref></sup> and has enabled applications in genetics, neuroscience, and drug screening among others. One reason that tracking postural details in <italic>C. elegans</italic> has been accessible since the turn of the century is that worms have a simple body morphology and can be imaged with high contrast while confined to two dimensions on the surface of an agar plate. High contrast imaging of sparsely distributed animals with a simple morphology is well suited to classic computer vision approaches for segmentation and skeletonisation. Despite these successes in worm tracking, most methods developed for pose estimation were brittle and simple overlaps between animals or self-intersection during coiling remained challenging for longer. Alternative methods were developed for pose estimation in overlapping worms<sup><xref ref-type="bibr" rid="R13">13</xref>,<xref ref-type="bibr" rid="R14">14</xref></sup> or individual coiling worms<sup><xref ref-type="bibr" rid="R15">15</xref>,<xref ref-type="bibr" rid="R16">16</xref></sup> but most of these methods are computationally intensive and less appropriate for high-throughput screening with large numbers of worms.</p><p id="P5">More recently, advances in deep learning have extended the applicability of pose estimation to more challenging applications including non-uniform backgrounds and animals with complex body plans<sup><xref ref-type="bibr" rid="R17">17</xref>–<xref ref-type="bibr" rid="R19">19</xref></sup>. Several instance segmentation methods have been applied to or developed for segmenting worms specifically<sup><xref ref-type="bibr" rid="R20">20</xref>–<xref ref-type="bibr" rid="R22">22</xref></sup>. A model called DeepTangle was recently developed to track swimming worms which takes into account temporal information and explicitly learns a worm shape model to produce skeletons directly from videos<sup><xref ref-type="bibr" rid="R23">23</xref></sup>. Swimming worms adopt simpler lower-dimensional postures than crawling worms, rarely coil, and tend to have simpler overlaps. Here we extend DeepTangle to the more complex postures and long overlaps of crawling worms using a new training dataset and show that the new model, DeepTangleCrawl (DTC), can track coiling and overlapping worms, achieving a new state-of-the-art on challenging worm tracking data compared to two instance segmentation approaches. The resulting data is accurate, and tracks are longer and more complete (fewer gaps). The improved tracking enables the analysis of behaviour in previously inaccessible conditions and improves the signal-to-noise ratio in phenotypic screens.</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>Data and model training</title><p id="P6">All the data we used for training and evaluation were collected using megapixel camera arrays with a resolution of 12.4 µm/pixel and a frame rate of 25 frames/second<sup><xref ref-type="bibr" rid="R24">24</xref></sup>. We selected subsets of data from diverse experiments including data from disease models<sup><xref ref-type="bibr" rid="R25">25</xref></sup>, worms treated with pharmaceuticals<sup><xref ref-type="bibr" rid="R24">24</xref></sup>, and worms treated with insecticides and nematicides<sup><xref ref-type="bibr" rid="R26">26</xref></sup>. The data were collected by different operators over several years. The goal was to have a training dataset that included variation in worm behaviour and morphology as well as technical variation, day-to-day variation, and seasonal variation. The current training data does not include data from different imaging setups or different labs.</p><p id="P7">The original DeepTangle model was trained on synthetic data. Simulating swimming worms is simpler than crawling worms for biological and technical reasons. First, worm behaviour during swimming is simpler than during crawling<sup><xref ref-type="bibr" rid="R5">5</xref>,<xref ref-type="bibr" rid="R27">27</xref>,<xref ref-type="bibr" rid="R28">28</xref></sup>. Second, imaging worms in liquid tends to result in a more uniform background than worms crawling on the surface of an agar plate seeded with a lawn of bacteria. Third, the challenging tracking examples we are most interested in solving—such as worms in thick bacterial lawns or tightly overlapping worms with correlated locomotion and an interface that is complicated optically—are the cases where producing reliable synthetic images would be the most challenging. Given these considerations, we constructed our training data from recordings of actual worms.</p><p id="P8">DeepTangle uses information from adjacent frames to improve segmentation. Therefore, all annotations were done on short clips consisting of 11 frames with all worms in the middle three frames annotated manually (<xref ref-type="fig" rid="F1">Figure 1A</xref>). All 11 frames from each clip are input to the model during training. We first compiled an extensive training data set consisting of non-coiling non-overlapping worms that were accurately tracked using our baseline algorithm Tierpsy (<xref ref-type="fig" rid="F1">Figure 1B</xref>). We then created a synthetic dataset by overlaying pre-segmented and skeletonised worms with random orientations to create training data with overlaps. These simple cases and synthetic overlaps were then supplemented with manual annotations from more challenging cases (<xref ref-type="fig" rid="F1">Figure 1C</xref>). Most instance segmentation methods require only single annotated frames. To train these models we simply used the annotated frames individually.</p><p id="P9">Compared to the original DeepTangle model, we increased the dimension of the late space representing worm shapes from 12 to 72. Because crawling behaviour has diverse timescales ranging from fast locomotion to long periods of dwelling, the 11-frame clips in the training data were uniformly sampled from 2, 4, and 8 second recordings so that the model could learn postural changes across several time scales.</p><p id="P10">Before both training and inference, we subtracted the background from recordings using singular value decomposition. Specifically, we performed SVD on videos that were subsampled 1:400 in time and used the single highest energy mode as a background image that was then subtracted from all video frames. This first mode consistently corresponds to the background without any worms. This step made all of the data, including the synthetic overlapping worm clips, directly comparable without a background outside of the worms.</p></sec><sec id="S4"><title>Pose estimation accuracy</title><p id="P11">We evaluated DTC on a set of held out data that were sampled across the same experiments as the training data but that were not seen by any model during training. The median root mean squared deviation between the predicted pose and the manually annotated pose (or Tierpsy pose in the case of simple non-overlapping shapes) is 2.2 pixels (<xref ref-type="fig" rid="F2">Figure 2A</xref>). At the resolution used here, this corresponds to an RMSD of approximately half a worm body width. To compare the performance of DTC to the current state of the art, we used the same data to train Omnipose and a landmark-based tracker similar to SLEAP<sup><xref ref-type="bibr" rid="R29">29</xref></sup> we refer to here as PAF for part affinity field<sup><xref ref-type="bibr" rid="R30">30</xref></sup>. PAF and Omnipose have a lower modal RMSD than DTC for the cases where a prediction is made, and all of the models have a lower modal RMSD than the inter-annotator RMSD of greater than 4 pixels. However, there are also cases where no prediction is made and this failure rate is different across models (<xref ref-type="fig" rid="F2">Figure 2B</xref>). On this measure, DTC performs the best, making no predictions less frequently than other models.</p><p id="P12">Not only are the rates of failure different, the types of cases each model fails on are different. For example, Omnipose takes worm width as a parameter and is good at separating parallel worms that are in contact along their length but tends to over-segment coiled worms or crossing worms (<xref ref-type="fig" rid="F2">Figure 2C</xref>, top row). PAF makes some similar errors on coiled and crossing worms and struggles to separate parallel worms (<xref ref-type="fig" rid="F2">Figure 2C</xref>, middle row). DTC performs better on coiled and crossing worms, most likely because it is able to take earlier and later frames into account to resolve the most likely poses. DTC does still fail on worms that form long-lasting overlaps or tight coils with little motion (<xref ref-type="fig" rid="F2">Figure 2C</xref>, bottom row). Because the worms do not move, DTC is unable to take advantage of temporal information to resolve the coiled shape. When these slow-moving coilers do straighten within a clip available to DTC, it is often able to estimate the pose in the coiled frames. All models currently fail in cases of complex overlaps with multiple worms (<xref ref-type="fig" rid="F2">Figure 2C</xref>, bottom row, right).</p><p id="P13">In addition to their accuracy, computation time is an important metric to consider for any model that will be used for large scale phenotypic screens. We compared the inference time for each model separated into CPU and GPU time (<xref ref-type="fig" rid="F2">Figure 2D</xref>). We only included the time to generate skeletons, not the time to connect them into tracks because the tracking algorithm could be varied and is not a core feature of each approach. The baseline model Tierpsy is run only on CPUs while PAF and DTC run only on GPUs. Omnipose outputs a mask for each worm and so requires a second skeletonization step. Here we apply Tierpsy’s skeletonization algorithm and do this step on CPUs. The neural network methods process all pixels and so processing time does not depend on the number of worms. Tierpsy skeletonises each segmented object separately and so processing time increases with worm number. To speed up inference in cases where there are a small number of worms, we down-sampled videos 1:3 in time and then used a three-dimensional smoothing spline to interpolate the missing data. This reduced total computation time with only a small decrease in performance, but the time advantage is modest with 15 worms per well (<xref ref-type="supplementary-material" rid="SD1">Figure S1</xref>).</p><p id="P14">Given its state-of-the-art performance in number of worms skeletonised and its acceptable accuracy and computation time, we focus on characterising DTC for the remainder of the paper. The results below were calculated using the DTC+spline algorithm but the results are broadly similar with DTC applied to each frame.</p></sec><sec id="S5"><title>DTC improves tracking in challenging conditions</title><p id="P15">The data presented so far were collected in conditions with relatively sparse worms on thin food lawns that were optimised for tracking using Tierpsy. Tierpsy is more sensitive than DTC to thicker food lawns, the presence of eggs or other background objects, as well as more frequent overlaps or coiling. The difference in performance in more challenging conditions is therefore larger. When there is a higher density of worms and eggs on the lawn, DTC is still able to skeletonise the majority of worms accurately (<xref ref-type="fig" rid="F3">Figure 3A</xref> and Supplementary Movie 1) whereas Tierpsy frequently fails (Supplementary Movie 2). Because skeletonization is maintained for overlapping worms, DTC is also able to accurately maintain worm identity through collisions (<xref ref-type="fig" rid="F3">Figure 3B</xref>).</p><p id="P16">When worms perform sharp turns, their self-intersecting poses cannot be resolved by Tierpsy, leading to gaps in data. For strong coiler mutants or worms treated with drugs that cause coiling, these gaps can be long and the poses that are not resolved are precisely those that are most characteristic so missing them limits accurate phenotyping. DTC is often able to resolve these shapes if there is some worm motion (<xref ref-type="fig" rid="F3">Figure 3C</xref> and Supplementary Movie 3) although tight coils with little to no motion remain challenging, as mentioned previously (<xref ref-type="fig" rid="F2">Figure 2C</xref>). Quantitatively, we see an increase in both trajectory duration and the number of skeletons per frame between Tierpsy and DTC for both 3 and 15 worms per well (<xref ref-type="fig" rid="F3">Figure 3D</xref>).</p></sec><sec id="S6"><title>Improved signal to noise ratio in phenotypic screens</title><p id="P17">To determine whether improved tracking quality leads to improved sensitivity in the context of a large phenotypic screen, we reanalysed a previously published dataset consisting of videos of worms treated with diverse insecticides and nematicides<sup><xref ref-type="bibr" rid="R26">26</xref></sup>. Using Tierpsy tracker, we have previously shown that we could detect the effects of insecticides and also predict their mode of action based only on their behavioural effects. As noted in the previous section, some of these treatments led to worms that adopted self-intersecting postures that were not tracked by Tierpsy leading to significant missing data. We extracted the same behavioural features from videos tracked using Tierpsy and DTC.</p><p id="P18">As expected for relatively clean data that was collected specifically to work with Tierpsy, the results are broadly comparable using both tracking methods. For example, the median midbody speed calculated using skeletons from Tierpsy is highly correlated to the speed calculated using skeletons from DTC (<xref ref-type="fig" rid="F4">Figure 4A</xref>). Looking across the Tierpsy256 features, the modal correlation coefficient is 0.71 (<xref ref-type="fig" rid="F4">Figure 4B</xref>, left). Some features show little correlation between the two tracking methods. Many of these poorly correlated features also have low F-statistic calculated across drug classes suggesting that they are simply noisy features in this dataset (<xref ref-type="fig" rid="F4">Figure 4B</xref>, right).</p><p id="P19">We next focussed specifically on a spiroindoline drug (SY1713) that causes strong coiling where we expect a difference between Tierpsy and DTC. For each dose, we find a higher curvature estimate from DTC than for Tierpsy, consistent with the expectation that DTC skeletonises more frames with coiling (<xref ref-type="fig" rid="F3">Figure 3C</xref>). The distribution of curvature values is also narrower for DTC-calculated curvatures, corresponding to a larger signal-to-noise ratio comparing drug treatments to DMSO controls for feature data derived from DTC tracking. Based on this observation we quantified the effect size for all features and all samples using features derived from Tierpsy and DTC tracking (<xref ref-type="fig" rid="F4">Figure 4D</xref>). Effect sizes are correlated but are larger (below the diagonal in <xref ref-type="fig" rid="F4">Figure 4D</xref>) for DTC-derived features.</p></sec></sec><sec id="S7" sec-type="discussion"><title>Discussion</title><p id="P20">DTC tracks more worms in challenging conditions including complex backgrounds and overlaps than the other tested models. It also has acceptable accuracy, with an RMSD that is better than the variation between human annotators, although the current version is less accurate than Omnipose and PAF for the cases where these models make any prediction. DTC therefore opens the door to experiments that were previously difficult or impossible. For example, thicker bacterial lawns that are closer to natural worm habitats obscure worm outlines and lead to more worm-like tracks that are difficult for some algorithms to distinguish from worms themselves. Perhaps more interestingly, a quantitative study of worm-worm interactions including their dependence on other behavioural variables like posture and speed is within reach. Although we do not expect to be able to track worms in large dense aggregates from brightfield imaging alone, studying the early stages of aggregate formation involving two- and three-worm interactions may already yield insight into aggregate formation. Because this analysis is compatible with large-scale imaging, it will be possible to look for mutants and natural variants that affect the early interactions and may reveal novel genes involved in worm behaviour.</p><p id="P21">Even in conditions optimized for classic segmentation and skeletonization algorithms, DTC increases the fraction of frames that yield reliable posture data, minimising gaps and preserving identity through overlaps and coils. This improvement directly translates into more data points per imaging run and increased effect sizes in phenotypic screens. The increased power comes from a better ability to detect subtle differences in behaviour as well as the ability to detect more extreme phenotypes such as the persistent high curvature seen in <xref ref-type="fig" rid="F3">Figure 3C</xref> which is effectively never observed in control worms.</p><p id="P22">Despite these advances, DTC still has limitations. For example, the long-lived close interactions that are characteristic of mating remain a challenge for DTC. We expect that a large-scale study of mating would become possible with more mating-specific training data. More generally, a significant increase in training data will likely be required to make a model that generalises to data collected on other trackers. We have included code for an annotator that makes generating new training data relatively straightforward and would welcome a collaborative approach to worm annotation to generate multi-lab training data. A complementary approach would be to generate better fully synthetic training sets that capture complex behaviours like mating.</p><p id="P23">For a modest increase in GPU time, DTC increases the value of data that is expensive to generate in terms of reagents (for drug screens) and labour.</p></sec><sec id="S8" sec-type="materials | methods"><title>Materials and Methods</title><sec id="S9"><title>Training data</title><p id="P24">Initially, isolated worm clips from Tierpsy were organized into sequences of 2, 4, or 8 seconds with the background subtracted. Each training clip consisted of a stack of 11 frames, each frame being 512x512 pixels. In total, we created 50,000 such clips. To simulate overlapping, we paired and stacked two clips on top of each other. Importantly, for each epoch, the dataset was randomly shuffled so that at every epoch, different pairs of clips were stacked together. This approach introduced variability and diversity to the training data. As a result, the effective number of unique clips used for training was 25,000 after stacking. The Tierpsy-derived data were supplemented with manual annotations for challenging cases included coils, complex overlaps, and non-uniform backgrounds. A total of 1000 clips were annotated using a custom graphical user interface (GUI) (<ext-link ext-link-type="uri" xlink:href="https://github.com/WeheliyeHashi/Worm_annotation">https://github.com/WeheliyeHashi/Worm_annotation</ext-link>). To create new manually annotated training data, we have also included a script that converts videos to clips that can be loaded and annotated using the GUI. For testing, we used 2000 skeletons, which are available on Zenodo (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.14517883">https://doi.org/10.5281/zenodo.14517883</ext-link>).</p></sec><sec id="S10"><title>DTC training and hyperparameters</title><p id="P25">Training code was adapted from ref. <sup><xref ref-type="bibr" rid="R23">23</xref></sup>. Training was conducted on an NVIDIA RTX A6000 with 48 GB of RAM. The model was implemented using JAX, with the Adam optimization algorithm, a learning rate of 0.001, and a modified ResNet (average pooling instead of max pooling in the first layer to break translational invariance) as the backbone architecture. The latent dimension was set to 8, and the maximum number of skeletons per grid was also set to 8. A cutoff distance of 48 pixels was applied during training, and the Principal Component Analysis dimension was set to 72.</p><p id="P26">The PCA transformation matrix was constructed using skeletons from both the training clips and supplemented with single-frame annotation data. As the model is based on a YOLO algorithm, it produces skeleton predictions, confidence scores, and latent space outputs. The output shape for the skeleton predictions was (32, 32, 8, 3, 72), corresponding to a 32x32 grid, with each cell measuring 16x16 pixels. Each cell contains 8 predicted skeletons across three central frames of the input stack, along with 72 PCA components. To convert these PCA components into real-space coordinates, we multiplied them by the PCA transformation matrix derived from the training data and single-frame annotations. The output shape for confidence scores was (32, 32, 8), and for latent space outputs, it was (32, 32, 8, 8). For inference, input images were padded so that the image resolution was a multiple of 16.</p></sec><sec id="S11"><title>Fitting 3D splines</title><p id="P27">For small numbers of worms, we found that it was faster and only slightly less accurate (<xref ref-type="supplementary-material" rid="SD1">Figure S1</xref>) to down-sample videos 1:3 in time before applying DTC and then using a 3D smoothing spline to interpolate over missing frames<sup><xref ref-type="bibr" rid="R31">31</xref></sup>. The comparisons shown in <xref ref-type="fig" rid="F3">Figures 3</xref> and <xref ref-type="fig" rid="F4">4</xref> were done using this down-sampling and interpolation procedure.</p></sec><sec id="S12"><title>Omnipose training and hyperparameters</title><p id="P28">The Omnipose code was adapted from ref. <sup><xref ref-type="bibr" rid="R22">22</xref></sup>. Training was conducted on an NVIDIA RTX A6000 GPU with 48 GB of memory. The model was implemented using PyTorch, with the Adam optimization algorithm, a learning rate of 0.1, and UNET as the backbone architecture. During training, we used the average cell diameter method as input, providing the estimated diameter for each worm. However, during inference, the actual worm diameter in pixels was used to enhance prediction accuracy. The training data, the trained model, and the inference code are available on Zenodo (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.14517883">https://doi.org/10.5281/zenodo.14517883</ext-link>).</p></sec><sec id="S13"><title>PAF training and hyperparameters</title><p id="P29">We implemented the ‘Part Affinity Fields’ (PAF) model, based on ref. <sup><xref ref-type="bibr" rid="R30">30</xref></sup>. The PAF model uses vector fields to represent connections between body parts. We adopted a bottom-up approach, where all body parts are detected simultaneously in the image, unlike top-down methods that first detect individuals and then estimate their poses. The model predicts the location of landmark points along the worm body and PAFs to determine the connections between them. To link two landmarks, the algorithm calculates a confidence score by integrating the PAF values along the line segment connecting the two points. Higher scores indicate stronger connections, allowing the model to accurately link the correct body parts and construct worm skeletons. This method considers both the spatial positions and the relationships between the body parts to ensure coherence. The implementation was carried out in PyTorch, using the Adam optimizer with a learning rate of 0.001 and a UNET-based backbone architecture. To reduce under-segmentation of closely aligned worms, the spreads of both the part affinity fields (edges) and confidence maps were set to match the diameter of the worms.</p></sec><sec id="S14"><title>RMSD calculation and failed predictions</title><p id="P30">To calculate the Root Mean Square Deviation (RMSD), we measure the minimum distances between the predicted skeleton points and the labelled points. Before calculating the RMSD, we ensure that the first point of the skeleton data corresponds to the head and the last point corresponds to the tail. This alignment is achieved using the head-tail algorithm in Tierpsy<sup><xref ref-type="bibr" rid="R12">12</xref></sup>. Points are interpolated to be equally spaced before calculating the RMSD.</p><p id="P31">‘Failures’ occur when no prediction is made. For Tierpsy, this decision is based on a series of heuristics. For example, the segmented area to be skeletonised must have exactly two points of curvature above a threshold (the head and tail) and the maximum and minimum width of the worm after skeletonization must not differ more than a preset threshold (helps to detect coiled shapes with an incorrect skeleton). For DTC, failure occurs when the model does not predict any skeleton above a confidence score of 0.5. For PAF, we check the number of detected landmark points. If a skeleton has fewer than 14 points (the expected number for a complete skeleton), we classify it as a half or broken skeleton and call it a failed prediction. For Omnipose, we adopt the same approach as Tierpsy since we use the Tierpsy skeletonisation algorithm to go from segmented objects to skeletons.</p></sec><sec id="S15"><title>Phenotypic screen analysis</title><p id="P32">We used the Pearson correlation coefficient to evaluate the linear association between the features obtained by Tierpsy and DTC. We used the F-statistic to quantify the signal in a given feature, calculated across all drugs and doses. Hedge’s d was used to quantify the standardized difference in effect sizes between the two methods. Hedge’s d is an adjusted version of Cohen’s d designed to correct for small sample bias. These values were converted to absolute values, disregarding the direction of the effect, and then averaged across all drugs and doses for each feature.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Figure S1</label><caption><title>Skipping frames and interpolating skeletons with a spline achieves similar accuracy and faster computational time.</title><p id="P33">(A) Histograms of root mean square deviation between held-out skeletons and predicted skeletons for each tested model, here including ‘DTC + spline’ in which only every third frame is tracked using DTC and skeletons for intermediate frames are interpolated using a 3D smoothing spline. The vertical dashed line shows the RMSD between three manual annotators. Skeletons above the histogram are examples that illustrate the corresponding RMSD visually. (B) Bar chart showing the number of cases in the held-out test data where a model fails to make a prediction (e.g. Tierpsy fails on coiled worms or a neural network model does not identify any worms above a confidence threshold). (C) Computation time per input frame for the different models as a function of worm number. Tierpsy only uses CPU computation while Omnipose uses GPU and CPU because we use Tierpsy’s skeletonization algorithm to convert segmented regions to skeletons. DTC + spline uses both GPUs for predicting skeletons on the subsampled frames and CPUs for fitting smoothing splines to interpolate the missing frames. The advantage is modest for videos with 15 worms/well (which is nominally 240 worms/video since each camera records 16 wells on a 96 well plate).</p></caption><media xlink:href="EMS202327-supplement-Figure_S1.jpg" mimetype="image" mime-subtype="jpeg" id="d62aAcEbC" position="anchor"/></supplementary-material></sec></body><back><ack id="S16"><title>Acknowledgements</title><p>We thank Bonnie Evans, Eleanor Warren, Riju Balachandran, and Tom O’Brien for help annotating data. This work was supported by the Medical Research Council through grant MC-A658-5TY30.</p></ack><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yemini</surname><given-names>E</given-names></name><name><surname>Jucikas</surname><given-names>T</given-names></name><name><surname>Grundy</surname><given-names>LJ</given-names></name><name><surname>Brown</surname><given-names>AEX</given-names></name><name><surname>Schafer</surname><given-names>WR</given-names></name></person-group><article-title>A database of Caenorhabditis elegans behavioral phenotypes</article-title><source>Nat Methods</source><year>2013</year><volume>10</volume><fpage>877</fpage><lpage>879</lpage><pub-id pub-id-type="pmcid">PMC3962822</pub-id><pub-id pub-id-type="pmid">23852451</pub-id><pub-id pub-id-type="doi">10.1038/nmeth.2560</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Restif</surname><given-names>C</given-names></name><etal/></person-group><article-title>CeleST: Computer Vision Software for Quantitative Analysis of C. elegans Swim Behavior Reveals Novel Features of Locomotion</article-title><source>PLoS Comput Biol</source><year>2014</year><volume>10</volume><elocation-id>e1003702</elocation-id><pub-id pub-id-type="pmcid">PMC4102393</pub-id><pub-id pub-id-type="pmid">25033081</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003702</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nagy</surname><given-names>S</given-names></name><name><surname>Goessling</surname><given-names>M</given-names></name><name><surname>Amit</surname><given-names>Y</given-names></name><name><surname>Biron</surname><given-names>D</given-names></name></person-group><article-title>A Generative Statistical Algorithm for Automatic Detection of Complex Postures</article-title><source>PLOS Comput Biol</source><year>2015</year><volume>11</volume><elocation-id>e1004517</elocation-id><pub-id pub-id-type="pmcid">PMC4595081</pub-id><pub-id pub-id-type="pmid">26439258</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004517</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Swierczek</surname><given-names>NA</given-names></name><name><surname>Giles</surname><given-names>AC</given-names></name><name><surname>Rankin</surname><given-names>CH</given-names></name><name><surname>Kerr</surname><given-names>RA</given-names></name></person-group><article-title>High-throughput behavioral analysis in C. elegans</article-title><source>Nat Methods</source><year>2011</year><volume>8</volume><fpage>592</fpage><lpage>598</lpage><pub-id pub-id-type="pmcid">PMC3128206</pub-id><pub-id pub-id-type="pmid">21642964</pub-id><pub-id pub-id-type="doi">10.1038/nmeth.1625</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pierce-Shimomura</surname><given-names>JT</given-names></name><etal/></person-group><article-title>Genetic analysis of crawling and swimming locomotory patterns in C. elegans</article-title><source>Proc Natl Acad Sci</source><year>2008</year><volume>105</volume><fpage>20982</fpage><lpage>20987</lpage><pub-id pub-id-type="pmcid">PMC2634943</pub-id><pub-id pub-id-type="pmid">19074276</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0810359105</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baek</surname><given-names>J-H</given-names></name><name><surname>Cosman</surname><given-names>P</given-names></name><name><surname>Feng</surname><given-names>Z</given-names></name><name><surname>Silver</surname><given-names>J</given-names></name><name><surname>Schafer</surname><given-names>WR</given-names></name></person-group><article-title>Using machine vision to analyze and classify Caenorhabditis elegans behavioral phenotypes quantitatively</article-title><source>J Neurosci Methods</source><year>2002</year><volume>118</volume><fpage>9</fpage><lpage>21</lpage><pub-id pub-id-type="pmid">12191753</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geng</surname><given-names>W</given-names></name><name><surname>Cosman</surname><given-names>P</given-names></name><name><surname>Baek</surname><given-names>J-H</given-names></name><name><surname>Berry</surname><given-names>CC</given-names></name><name><surname>Schafer</surname><given-names>WR</given-names></name></person-group><article-title>Quantitative classification and natural clustering of Caenorhabditis elegans behavioral phenotypes</article-title><source>Genetics</source><year>2003</year><volume>165</volume><fpage>1117</fpage><lpage>1126</lpage><pub-id pub-id-type="pmcid">PMC1462821</pub-id><pub-id pub-id-type="pmid">14668369</pub-id><pub-id pub-id-type="doi">10.1093/genetics/165.3.1117</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feng</surname><given-names>Z</given-names></name><name><surname>Cronin</surname><given-names>CJ</given-names></name><name><surname>Wittig</surname><given-names>JH</given-names><suffix>Jr</suffix></name><name><surname>Sternberg</surname><given-names>PW</given-names></name><name><surname>Schafer</surname><given-names>WR</given-names></name></person-group><article-title>An imaging system for standardized quantitative analysis of C. elegans behavior</article-title><source>BMC Bioinformatics</source><year>2004</year><volume>5</volume><fpage>115</fpage><pub-id pub-id-type="pmcid">PMC517925</pub-id><pub-id pub-id-type="pmid">15331023</pub-id><pub-id pub-id-type="doi">10.1186/1471-2105-5-115</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stephens</surname><given-names>GJ</given-names></name><name><surname>Johnson-Kerner</surname><given-names>B</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name><name><surname>Ryu</surname><given-names>WS</given-names></name></person-group><article-title>Dimensionality and Dynamics in the Behavior of C. elegans</article-title><source>PLoS Comput Biol</source><year>2008</year><volume>4</volume><elocation-id>e1000028</elocation-id><pub-id pub-id-type="pmcid">PMC2276863</pub-id><pub-id pub-id-type="pmid">18389066</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000028</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsibidis</surname><given-names>GD</given-names></name><name><surname>Tavernarakis</surname><given-names>N</given-names></name></person-group><article-title>Nemo: a computational tool for analyzing nematode locomotion</article-title><source>BMC Neurosci</source><year>2007</year><volume>8</volume><fpage>86</fpage><pub-id pub-id-type="pmcid">PMC2148042</pub-id><pub-id pub-id-type="pmid">17941975</pub-id><pub-id pub-id-type="doi">10.1186/1471-2202-8-86</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sznitman</surname><given-names>R</given-names></name><name><surname>Gupta</surname><given-names>M</given-names></name><name><surname>Hager</surname><given-names>GD</given-names></name><name><surname>Arratia</surname><given-names>PE</given-names></name><name><surname>Sznitman</surname><given-names>J</given-names></name></person-group><article-title>Multi-Environment Model Estimation for Motility Analysis of Caenorhabditis elegans</article-title><source>PLoS ONE</source><year>2010</year><volume>5</volume><elocation-id>e11631</elocation-id><pub-id pub-id-type="pmcid">PMC2908547</pub-id><pub-id pub-id-type="pmid">20661478</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0011631</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Javer</surname><given-names>A</given-names></name><etal/></person-group><article-title>An open-source platform for analyzing and sharing worm-behavior data</article-title><source>Nat Methods</source><year>2018</year><volume>15</volume><fpage>645</fpage><lpage>646</lpage><pub-id pub-id-type="pmcid">PMC6284784</pub-id><pub-id pub-id-type="pmid">30171234</pub-id><pub-id pub-id-type="doi">10.1038/s41592-018-0112-1</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fontaine</surname><given-names>E</given-names></name><name><surname>Burdick</surname><given-names>J</given-names></name><name><surname>Barr</surname><given-names>A</given-names></name></person-group><source>Automated Tracking of Multiple C. Elegans</source><publisher-name>IEEE</publisher-name><year>2006</year><fpage>3716</fpage><lpage>3719</lpage><pub-id pub-id-type="pmid">17945791</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roussel</surname><given-names>N</given-names></name><name><surname>Morton</surname><given-names>CA</given-names></name><name><surname>Finger</surname><given-names>FP</given-names></name><name><surname>Roysam</surname><given-names>B</given-names></name></person-group><article-title>A computational model for C. elegans locomotory behavior: application to multiworm tracking</article-title><source>IEEE Trans Biomed Eng</source><year>2007</year><volume>54</volume><fpage>1786</fpage><lpage>1797</lpage><pub-id pub-id-type="pmid">17926677</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Broekmans</surname><given-names>OD</given-names></name><name><surname>Rodgers</surname><given-names>JB</given-names></name><name><surname>Ryu</surname><given-names>WS</given-names></name><name><surname>Stephens</surname><given-names>GJ</given-names></name></person-group><article-title>Resolving coiled shapes reveals new reorientation behaviors in <italic>C. elegans</italic></article-title><source>eLife</source><year>2016</year><volume>5</volume><pub-id pub-id-type="pmcid">PMC5030097</pub-id><pub-id pub-id-type="pmid">27644113</pub-id><pub-id pub-id-type="doi">10.7554/eLife.17227</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebert</surname><given-names>L</given-names></name><name><surname>Ahamed</surname><given-names>T</given-names></name><name><surname>Costa</surname><given-names>AC</given-names></name><name><surname>O’Shaughnessy</surname><given-names>L</given-names></name><name><surname>Stephens</surname><given-names>GJ</given-names></name></person-group><article-title>WormPose: Image synthesis and convolutional networks for pose estimation in C. elegans</article-title><source>PLOS Comput Biol</source><year>2021</year><volume>17</volume><elocation-id>e1008914</elocation-id><pub-id pub-id-type="pmcid">PMC8078761</pub-id><pub-id pub-id-type="pmid">33905413</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008914</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname><given-names>A</given-names></name><etal/></person-group><article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title><source>Nat Neurosci</source><year>2018</year><volume>21</volume><fpage>1281</fpage><lpage>1289</lpage><pub-id pub-id-type="pmid">30127430</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graving</surname><given-names>JM</given-names></name><etal/></person-group><article-title>DeepPoseKit, a software toolkit for fast and robust animal pose estimation using deep learning</article-title><source>eLife</source><year>2019</year><volume>8</volume><elocation-id>e47994</elocation-id><pub-id pub-id-type="pmcid">PMC6897514</pub-id><pub-id pub-id-type="pmid">31570119</pub-id><pub-id pub-id-type="doi">10.7554/eLife.47994</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>TD</given-names></name><etal/></person-group><article-title>Fast animal pose estimation using deep neural networks</article-title><source>Nat Methods</source><year>2019</year><volume>16</volume><fpage>117</fpage><lpage>125</lpage><pub-id pub-id-type="pmcid">PMC6899221</pub-id><pub-id pub-id-type="pmid">30573820</pub-id><pub-id pub-id-type="doi">10.1038/s41592-018-0234-5</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deserno</surname><given-names>M</given-names></name><name><surname>Bozek</surname><given-names>K</given-names></name></person-group><article-title>WormSwin: Instance segmentation of C. elegans using vision transformer</article-title><source>Sci Rep</source><year>2023</year><volume>13</volume><elocation-id>11021</elocation-id><pub-id pub-id-type="pmcid">PMC10328995</pub-id><pub-id pub-id-type="pmid">37419938</pub-id><pub-id pub-id-type="doi">10.1038/s41598-023-38213-7</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Banerjee</surname><given-names>SC</given-names></name><name><surname>Khan</surname><given-names>KA</given-names></name><name><surname>Sharma</surname><given-names>R</given-names></name></person-group><article-title>Deep-worm-tracker: Deep learning methods for accurate detection and tracking for behavioral studies in C. elegans</article-title><source>Appl Anim Behav Sci</source><year>2023</year><volume>266</volume><elocation-id>106024</elocation-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cutler</surname><given-names>KJ</given-names></name><etal/></person-group><article-title>Omnipose: a high-precision morphology-independent solution for bacterial cell segmentation</article-title><source>Nat Methods</source><year>2022</year><volume>19</volume><fpage>1438</fpage><lpage>1448</lpage><pub-id pub-id-type="pmcid">PMC9636021</pub-id><pub-id pub-id-type="pmid">36253643</pub-id><pub-id pub-id-type="doi">10.1038/s41592-022-01639-4</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alonso</surname><given-names>A</given-names></name><name><surname>Kirkegaard</surname><given-names>JB</given-names></name></person-group><article-title>Fast detection of slender bodies in high density microscopy data</article-title><source>Commun Biol</source><year>2023</year><volume>6</volume><fpage>754</fpage><pub-id pub-id-type="pmcid">PMC10356847</pub-id><pub-id pub-id-type="pmid">37468539</pub-id><pub-id pub-id-type="doi">10.1038/s42003-023-05098-1</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barlow</surname><given-names>IL</given-names></name><etal/></person-group><article-title>Megapixel camera arrays enable high-resolution animal tracking in multiwell plates</article-title><source>Commun Biol</source><year>2022</year><volume>5</volume><fpage>253</fpage><pub-id pub-id-type="pmcid">PMC8943053</pub-id><pub-id pub-id-type="pmid">35322206</pub-id><pub-id pub-id-type="doi">10.1038/s42003-022-03206-1</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Brien</surname><given-names>TJ</given-names></name><name><surname>Barlow</surname><given-names>IL</given-names></name><name><surname>Feriani</surname><given-names>L</given-names></name><name><surname>Brown</surname><given-names>AE</given-names></name></person-group><article-title>Systematic creation and phenotyping of Mendelian disease models in C. elegans: towards large-scale drug repurposing</article-title><year>2024</year><pub-id pub-id-type="doi">10.7554/eLife.92491.2</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McDermott-Rouse</surname><given-names>A</given-names></name><etal/></person-group><article-title>Behavioral fingerprints predict insecticide and anthelmintic mode of action</article-title><source>Mol Syst Biol</source><year>2021</year><volume>17</volume><pub-id pub-id-type="pmcid">PMC8144879</pub-id><pub-id pub-id-type="pmid">34031985</pub-id><pub-id pub-id-type="doi">10.15252/msb.202110267</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berri</surname><given-names>S</given-names></name><name><surname>Boyle</surname><given-names>JH</given-names></name><name><surname>Tassieri</surname><given-names>M</given-names></name><name><surname>Hope</surname><given-names>IA</given-names></name><name><surname>Cohen</surname><given-names>N</given-names></name></person-group><article-title>Forward locomotion of the nematode <italic>C. elegans</italic> is achieved through modulation of a single gait</article-title><source>HFSP J</source><year>2009</year><volume>3</volume><fpage>186</fpage><lpage>193</lpage><pub-id pub-id-type="pmcid">PMC2714959</pub-id><pub-id pub-id-type="pmid">19639043</pub-id><pub-id pub-id-type="doi">10.2976/1.3082260</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hassinan</surname><given-names>CW</given-names></name><etal/></person-group><article-title>Dimensionality of locomotor behaviors in developing C. elegans</article-title><source>PLOS Comput Biol</source><year>2024</year><volume>20</volume><elocation-id>e1011906</elocation-id><pub-id pub-id-type="pmcid">PMC10939432</pub-id><pub-id pub-id-type="pmid">38437243</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1011906</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>TD</given-names></name><etal/></person-group><article-title>SLEAP: A deep learning system for multi-animal pose tracking</article-title><source>Nat Methods</source><year>2022</year><volume>19</volume><fpage>486</fpage><lpage>495</lpage><pub-id pub-id-type="pmcid">PMC9007740</pub-id><pub-id pub-id-type="pmid">35379947</pub-id><pub-id pub-id-type="doi">10.1038/s41592-022-01426-1</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cao</surname><given-names>Z</given-names></name><name><surname>Simon</surname><given-names>T</given-names></name><name><surname>Wei</surname><given-names>S-E</given-names></name><name><surname>Sheikh</surname><given-names>Y</given-names></name></person-group><source>Realtime Multi-person 2D Pose Estimation Using Part Affinity Fields</source><conf-name>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><conf-sponsor>IEEE</conf-sponsor><conf-loc>Honolulu, HI</conf-loc><year>2017</year><fpage>1302</fpage><lpage>1310</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2017.143</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>A</given-names></name><name><surname>Uhlmann</surname><given-names>V</given-names></name><name><surname>Fageot</surname><given-names>J</given-names></name><name><surname>Unser</surname><given-names>M</given-names></name></person-group><article-title>Dictionary Learning for Two-Dimensional Kendall Shapes</article-title><source>SIAM J Imaging Sci</source><year>2020</year><volume>13</volume><fpage>141</fpage><lpage>175</lpage></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Training data is a mix of straightforward and challenging cases.</title><p>(A) DeepTangle’s input consists of clips with central frames annotated. Other models were simply trained on individual annotated frames. (B) Isolated non-intersecting worms can be tracked using Tierpsy’s existing algorithm. (C) There are several categories of more challenging cases where simple skeletonization algorithms fail including self-intersecting worms, multiple overlapping worms, and worms with complex non-uniform backgrounds. Clips for these cases were manually annotated.</p></caption><graphic xlink:href="EMS202327-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Pose estimation accuracy.</title><p>(A) Histograms of root mean square deviation between held-out skeletons and predicted skeletons for each tested model. The vertical dashed line shows the RMSD between three manual annotators. Skeletons above the histogram are examples that illustrate the corresponding RMSD visually. (B) Bar chart showing the number of cases in the held-out test data where a model fails to make a prediction (e.g. Tierpsy fails on coiled worms or a neural network model does not identify any worms above a confidence threshold). (C) Examples of the kinds of errors each model makes. PAF and Omnipose often over-segment worms. DTC fails when worms are persistently tightly coiled or in tight parallel contact for an extended time. (D) Computation time per input frame for the different models as a function of worm number/well. Each camera records 16 wells in a 96-well plate so these correspond to 48 and 240 worms per video. Tierpsy only uses CPU computation while Omnipose uses GPU and CPU because we use Tierpsy’s skeletonization algorithm to convert segmented regions to skeletons.</p></caption><graphic xlink:href="EMS202327-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Tracking in challenging conditions.</title><p>(A) Multiple overlapping worms with high density of eggs. Inset images show higher magnification images of two sets of overlapping worms where all individuals are successfully skeletonised. (B) Examples of continuous tracks that preserve worm identify during and through collisions. (C) For highly curved worms that form long-lived coiling shapes, long gaps in the data can be present in Tierpsy-derived data. Here, there is a long gap with a high curvature that is recovered using DTC. (D) Improved skeletonization leads to longer tracks from DTC compared to Tierpsy. Note the log scale. The duration is longer for videos with 3 and 15 worms per well. (E) The number of skeletons/frame averaged over a video. DTC tracking produces numbers closer to the nominal number of worms per well.</p></caption><graphic xlink:href="EMS202327-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>DTC improves the signal to noise ratio in a phenotypic screen. All data from ref.</title><p><sup><xref ref-type="bibr" rid="R26">26</xref></sup>. (A) Speed calculated from Tierpsy skeletons and from DTC skeletons for a random sample of 290 wells from a previously published drug screen. The dashed line is y = x. (B) Correlation coefficients for each of the Tierpsy 256 behavioural features for the data from ref. <sup><xref ref-type="bibr" rid="R26">26</xref></sup> (left). The red line indicates the modal value of 0.71. Correlation coefficient plotted against the F-statistic for each feature calculated over the entire dataset. (C) Tail curvature as a function of dose for worms treated with a spiroindoline known to cause coiling. (D) A comparison of the mean absolute value of Hedge’s d effect size calculated using features derived from Tierpsy and DTC tracking data. Each point is the mean Hedge’s d across all doses of a drug compared to DMSO controls for a single feature. Any feature with ‘curvature’ in the name except for time derivatives of curvature are shown in blue. All other features are shown in yellow.</p></caption><graphic xlink:href="EMS202327-f004"/></fig></floats-group></article>