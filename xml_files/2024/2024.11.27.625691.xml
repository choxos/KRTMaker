<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS201451</article-id><article-id pub-id-type="doi">10.1101/2024.11.27.625691</article-id><article-id pub-id-type="archive">PPR946416</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">2</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Dynamic consensus-building between neocortical areas via long-range connections</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Javadzadeh</surname><given-names>Mitra</given-names></name><xref ref-type="fn" rid="FN1">*</xref><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="corresp" rid="CR1">@</xref></contrib><contrib contrib-type="author"><name><surname>Schimel</surname><given-names>Marine</given-names></name><xref ref-type="fn" rid="FN1">*</xref><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A4">4</xref><xref ref-type="corresp" rid="CR1">@</xref></contrib><contrib contrib-type="author"><name><surname>Hofer</surname><given-names>Sonja B.</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Ahmadian</surname><given-names>Yashar</given-names></name><xref ref-type="fn" rid="FN2">†</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Hennequin</surname><given-names>Guillaume</given-names></name><xref ref-type="fn" rid="FN2">†</xref><xref ref-type="aff" rid="A1">1</xref></contrib></contrib-group><aff id="A1"><label>1</label>Computational and Biological Learning Lab, Department of Engineering, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>University of Cambridge</institution></institution-wrap>, <city>Cambridge</city>, <country country="GB">U.K.</country></aff><aff id="A2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02qz8b764</institution-id><institution>Cold Spring Harbor Laboratory</institution></institution-wrap>, <city>NY</city>, <country country="US">USA</country></aff><aff id="A3"><label>3</label>SainsburyWellcome Centre, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap>, <city>London</city>, <country country="GB">U.K.</country></aff><aff id="A4"><label>4</label>Wu Tsai Neurosciences Institute, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Stanford University</institution></institution-wrap>, <city>Stanford</city>, <state>CA</state>, <country country="US">USA</country></aff><author-notes><corresp id="CR1">
<label>@</label><email>javadzadeh@cshl.edu</email>, <email>mschimel@stanford.edu</email></corresp><fn id="FN1" fn-type="equal"><label>*</label><p id="P1">Equal contribution</p></fn><fn id="FN2" fn-type="equal"><label>†</label><p id="P2">Equal contribution</p></fn></author-notes><pub-date pub-type="nihms-submitted"><day>29</day><month>11</month><year>2024</year></pub-date><pub-date pub-type="preprint"><day>27</day><month>11</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P3">The neocortex is organized into functionally specialized areas. While the functions and underlying neural circuitry of individual neocortical areas are well studied, it is unclear how these regions operate collectively to form percepts and implement cognitive processes. In particular, it remains unknown how distributed, potentially conflicting computations can be reconciled. Here we show that the reciprocal excitatory connections between cortical areas orchestrate neural dynamics to facilitate the gradual emergence of a ‘consensus’ across areas. We investigated the joint neural dynamics of primary (V1) and higher-order lateromedial (LM) visual areas in mice, using simultaneous multi-area electrophysiological recordings along with focal optogenetic perturbations to causally manipulate neural activity. We combined mechanistic circuit modeling with state-of-the-art data-driven nonlinear system identification, to construct biologically-constrained latent circuit models of the data that we could further interrogate. This approach revealed that long-range, reciprocal excitatory connections between V1 and LM implement an approximate line attractor in their joint dynamics, which promotes activity patterns encoding the presence of the stimulus consistently across the two areas. Further theoretical analyses revealed that the emergence of line attractor dynamics is a signature of a more general principle governing multi-area network dynamics: reciprocal inter-area excitatory connections reshape the dynamical landscape of the network, specifically slowing down the decay of activity patterns that encode stimulus features congruently across areas, while accelerating the decay of inconsistent patterns. This selective dynamic amplification leads to the emergence of multi-dimensional consensus between cortical areas about various stimulus features. Our analytical framework further predicted the timescales of specific activity patterns across areas, which we directly verified in our data. Therefore, by linking the anatomical organization of inter-area connections to the features they reconcile across areas, our work introduces a general theory of multi-area computation.</p></abstract></article-meta></front><body><p id="P4">The neocortex is segregated into distinct areas that are specialized for specific functions. This organization allows for de-composing complex problems into simpler sub-computations, such as the extraction of low-level features from intricate visual scenes. However, cognition arises from the holistic integration of these processes, making it essential that the different areas work in concert and remain consistent with each other. It is unclear how such coordination is achieved, and in particular how any conflict that might arise between local subunits can be globally resolved.</p><p id="P5">Anatomically, cortical areas are densely interconnected through reciprocal long-range inter-area connections [<xref ref-type="bibr" rid="R1">Felleman and Van Essen, 1991</xref>], whose organization is markedly distinct from that of local circuits within a cortical area. For instance, both excitatory and inhibitory neurons have local innervation, while only excitatory neurons have long-range projections that may target other areas [<xref ref-type="bibr" rid="R2">Douglas and Martin, 2004</xref>, <xref ref-type="bibr" rid="R3">Markram et al., 2004</xref>, <xref ref-type="bibr" rid="R4">Harris and Shepherd, 2015</xref>]. The functional role of these distinct connectivity rules is not clear; it remains unknown how excitatory inter-area connections coordinate cortical activity and unify local sub-units into coherent global computations. To address this, we combined mechanistic modelling of cortical circuits with data-driven inference of circuit dynamics. This approach allowed us to build models of cortical activity that not only explained neural responses quantitatively, but also captured the causal effects of optogenetic perturbations and had biologically interpretable components – including local and long-range connections – whose functional significance we could interrogate.</p><p id="P6">We focused on the joint activity dynamics of the primary (V1) and higher-order (LM) visual areas in mice during visual processing. We used simultaneous multi-channel recordings from V1 and LM performed while mice were presented with a 500 ms-long visual stimulus – one of two stationary gratings oriented at 45° or -45°(<xref ref-type="fig" rid="F1">Figure 1A-B</xref>). Mice were trained to perform a go/no-go task, discriminating the two stimuli. In some trials, neural activity in either V1 or LM (varying across animals) was perturbed in brief 150 ms time windows, using optogenetic activation of inhibitory parvalbumin-expressing (PV+) interneurons expressing channelrhodopsin-2 (ChR2) (<xref ref-type="fig" rid="F1">Figure 1C</xref>) [<xref ref-type="bibr" rid="R5">Javadzadeh and Hofer, 2022</xref>].</p><p id="P7">We built circuit models that explicitly incorporated known aspects of cortical circuit organization, in particular the excitatory nature of long-range connections between areas and local excitation-inhibition dynamics. In these models, the time course of spiking activity in V1 and LM was explained by the recurrent dynamics of the latent circuit (<xref ref-type="fig" rid="F1">Figure 1D</xref>). These dynamics were driven by time-varying inputs that we inferred for each trial, reflecting any unobserved signals external to the V1-LM circuit such as sensory or optogenetic stimuli.</p><p id="P8">Specifically, the latent circuit’s activity <bold><italic>z</italic></bold>(<italic>t</italic>) evolved according to <disp-formula id="FD1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:mi>τ</mml:mi><mml:mover accent="true"><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.2em"/><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mo>Φ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula> where <italic>τ</italic> = 20 ms is the characteristic neuronal membrane time constant, <bold><italic>W</italic></bold> is the latent circuit connectivity, <bold>Φ</bold>(·) is a soft-rectified nonlinear activation function, and <bold><italic>u</italic></bold> is a set of trial-specific external input signals that enter the dynamics through the input matrix <bold><italic>B</italic></bold> (<xref ref-type="sec" rid="S13">Methods</xref>). The activity of this latent circuit was used to describe firing rate fluctuations in the observed V1 and LM neurons, according to <disp-formula id="FD2"><label>(2)</label><mml:math id="M2"><mml:mrow><mml:mtext mathvariant="bold-italic">r</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo/><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula> where <italic><bold>C</bold></italic> is a readout matrix specifying the way in which each recorded neuron relates to the latent units, and <italic><bold>d</bold></italic> is a vector of constant offsets. Action potentials were modelled as Poisson processes given these time-varying firing rates.</p><p id="P9">The latent circuit was partitioned into two areas, which mapped onto V1 and LM neurons respectively. Moreover, the recurrent connectivity matrix <bold><italic>W</italic></bold> was constrained so that each area was composed of separate populations of excitatory and inhibitory units, and long-range connections between the two areas originated exclusively from the excitatory units (<xref ref-type="sec" rid="S13">Methods</xref>). Although we did not know the E/I identities of most of the recorded neurons, we used a specific sparsity penalty on <italic><bold>C</bold></italic> to discourage any nonsensical, simultaneous association of a neuron with both E and I latents subpopulations (<xref ref-type="sec" rid="S13">Methods</xref>). This soft constraint encouraged the model to learn to label each neuron as <italic>either</italic> E <italic>or</italic> I.</p><p id="P10">To fit the model, we used iLQR-VAE [<xref ref-type="bibr" rid="R6">Schimel et al., 2022</xref>], a method ideally suited to learning the dynamics of a circuit when the detailed time course of external inputs is unknown and must therefore be inferred in each trial. Importantly, here we did have some knowledge of <italic>what</italic> input signals might have driven the circuit in a given condition and <italic>when</italic>. iLQR-VAE lets us incorporate such information in the form of condition-specific, time-varying statistical priors over the input <italic><bold>u</bold></italic>(<italic>t</italic>) in <xref ref-type="disp-formula" rid="FD1">Equation 1</xref>. Thus, we used three input channels reflecting the two visual stimuli and the optogenetic perturbation events. The mapping from inputs to latents, <bold><italic>B</italic></bold>, was constrained such that the input channel with the optogenetic perturbation prior could only target the inhibitory latents of the stimulated area for each animal. For each channel, we learned two prior variances: the higher variance was used during the time the corresponding stimulus was on, and the lower one outside those epochs (<xref ref-type="fig" rid="F1">Figure 1D</xref>, dashed lines). This encouraged the model to use larger inputs when the stimuli were present, while retaining flexibility with respect to their exact time course. iLQR-VAE then inferred this time course on a single trial basis, by computing a posterior distribution over the input signals in each channel conditioned on the observed neural data (<xref ref-type="fig" rid="F1">Figure 1D and E</xref>, solid lines).</p><p id="P11">The parameters of the model (<italic><bold>W, B, C, d</bold></italic> and the prior variances) were obtained by maximizing the likelihood of the observed spike trains. For each animal, we performed multiple fits starting from random initializations (<xref ref-type="sec" rid="S13">Methods</xref>), and found that each fit robustly attributed a definite E or I identity to each observed neuron (<xref ref-type="fig" rid="F1">Figure 1F</xref>). For most fits (78.24%), the model correctly labelled all of the directly photo-stimulated neurons (known to be PV+ inhibitory cells) as inhibitory (<xref ref-type="fig" rid="F1">Figure 1F</xref>, cyan mark); we rejected the few models where these cells were mislabelled. Finally, for each animal we selected the model with the best goodness of fit on held-out data (see below, and <xref ref-type="sec" rid="S13">Methods</xref>).</p><sec id="S1"><title>The model captures trial-by-trial variability</title><p id="P12">We first characterized how well the learned models captured single trial activity in our recorded neurons. For each trial, we could leave one neuron out, and let the model infer the time course of its firing rate given the activity of the other neurons (<xref ref-type="fig" rid="F2">Figure 2A</xref>). Based on this single-trial firing rate, the model then attributed a (Poisson) likelihood to each spike for that neuron. On average, this single-trial likelihood was greater than that predicted by the PSTH of the same cell obtained by averaging over the other trials in the same condition (<xref ref-type="fig" rid="F2">Figure 2B</xref>, ‘residual likelihood’). In other words, our latent circuits captured the spatio-temporal structure of our recordings beyond condition averages. Accordingly, our models also captured the structure of pairwise covariances in neural activity (<xref ref-type="supplementary-material" rid="SD1">Figure S3</xref>). Importantly, the models did not significantly suffer from the circuit constraints we imposed; they explained the single-trial data just as well as fully unconstrained models (<xref ref-type="fig" rid="F2">Figure 2A-B</xref>, gray; <xref ref-type="sec" rid="S13">Methods</xref>).</p></sec><sec id="S2"><title>The model infers circuit dynamics that are consistent across animals and captures key aspects of V1-LM cortical physiology</title><p id="P13">That constrained and unconstrained models explain the data equally well, despite being entirely different families of dynamical systems, raises a concern: have our constrained models learned dynamics that really capture the mechanics of the underlying V1-LM circuit?</p><p id="P14">A first indication of faithful dynamics reconstruction is the consistency of the learned solutions across animals. We evaluated the distance between the inferred latent flow fields between pairs of animals, accounting for an arbitrary rotation of the latent state space for each animal (<xref ref-type="sec" rid="S13">Methods</xref>). This analysis revealed that the dynamics learned by our constrained latent circuits were broadly consistent across animals – indeed more consistent than in unconstrained models (<xref ref-type="fig" rid="F2">Figure 2C</xref>).</p><p id="P15">As a second, stronger test of accurate dynamics reconstruction, we probed the responses of our models to <italic>internal</italic> perturbations of the inhibitory cells, and compared those to the responses observed in the V1-LM data during optogenetic perturbations. Specifically, we simulated single perturbation trials by directly providing positive input to the inhibitory latent units of the relevant area for the entire duration of the photo-stimulation, whilst replaying the external inputs inferred on a control trial (no photo-stimulation) for the relevant condition (go vs. no-go). We adjusted the amplitude of each perturbation input (four parameters per animal) in order to match the trial-averaged norm of the responses of the known PV cells in the stimulated area. We then evaluated the model-predicted change in firing rates in the other neurons (per-condition averages). These predictions were positively correlated with the corresponding firing rate changes observed in the data (<xref ref-type="fig" rid="F2">Figure 2D</xref>; average Pearson <italic>ρ</italic> = 0.34). In contrast, models trained using only no-perturbation trials failed to capture the sensitivity of the V1-LM circuit to photoinhibition (<xref ref-type="fig" rid="F2">Figure 2D</xref>, gray; average Pearson <italic>ρ</italic> = 0.12), highlighting the importance of optogenetic manipulations for accurate neural system identification.</p><p id="P16">As a third indication that our model has learned the correct circuit structure, we looked at the excitatory/inhibitory identity that it assigned to each neuron in our recordings. Whilst we used the assigned identity of the known PV cells in the photo-stimulated area as a criterion for model selection, we could study the identity assigned to the other recorded neurons. Inhibitory neurons in the cortex are known to exhibit a bimodal distribution of spike widths: fast-spiking (PV) interneurons exhibit narrow spike waveforms, whilst other (non-PV) neurons have slower action potentials similar to that of excitatory neurons [<xref ref-type="bibr" rid="R7">Rudy et al., 2011</xref>]. Consistent with this known aspect of cortical electrophysiology, we found that the neurons which the model deemed inhibitory had a bimodal distribution of spike widths (<xref ref-type="fig" rid="F2">Figure 2E</xref>). The mode of the histogram corresponding to broader spikes aligned well with the distribution of spike widths in the neurons classified as excitatory by the model.</p><p id="P17">Finally, the dynamics inferred by the model are consistent with previous studies of the mammalian visual cortex. In particular, the network operates in the inhibition-stabilized regime (<xref ref-type="fig" rid="F2">Figure 2F</xref>; Ozeki et al., 2009, Ahmadian and Miller, 2021), whereby the excitatory subnetwork is unstable on its own but stabilized by feedback inhibition. In fact, the network is inhibition-stabilized even in the absence of visual stimulation, as previously shown in mouse V1 [<xref ref-type="bibr" rid="R10">Sanzeni et al., 2020</xref>]. Moreover, noise correlations in the latent circuit reflect the strength of excitatory connectivity between pairs of latent units (<xref ref-type="fig" rid="F2">Figure 2G</xref>), as observed in mouse visual cortex [<xref ref-type="bibr" rid="R11">Ko et al., 2011</xref>]. Importantly, this relationship was not present at initialization, but arose after fitting the model to the data.</p></sec><sec id="S3"><title>Contribution of external and recurrent inputs in shaping cortical visual responses</title><p id="P18">Having established the validity of our model fits, we then used the resulting latent circuits to dissect the roles of various structural components of the V1-LM network in shaping its sensory responses. To do this, we focused on several key features of the learned latent circuit connectivity, systematically and individually down-modulated their strengths, and quantified the effect of these modulations on the circuit’s responses to sensory stimuli. Only no-go trials, with no reward or licking-related movement, were used for this analysis (<xref ref-type="fig" rid="F3">Figure 3</xref>)</p><p id="P19">We began by dissociating external and recurrent inputs to the latent circuit. Whilst the average external input to each neuron was mostly transient, i.e. confined to the onset and offset of the sensory stimulus, the corresponding recurrent input remained elevated for the whole stimulus duration (<xref ref-type="fig" rid="F3">Figure 3A</xref>, left), mirroring the period of sustained activity across two areas during the stimulus epoch (recall <xref ref-type="fig" rid="F1">Figure 1G</xref>). Even modest down-scaling of all recurrent weights during the stimulus (<xref ref-type="fig" rid="F3">Figure 3A</xref>, center bottom, black bar) could nearly abolish these sustained responses. Similar down-scaling of recurrent connectivity during stimulus onset (<xref ref-type="fig" rid="F3">Figure 3A</xref>, center bottom, gray bar) had a weaker effect (<xref ref-type="fig" rid="F3">Figure 3A</xref>, right; compare top and bottom green curves). Modulation of the external input weights had a weaker effect still (<xref ref-type="fig" rid="F3">Figure 3A</xref>, center and right), indicating that sustained activity arose primarily from recurrent connections, with external inputs triggering the onset response.</p><p id="P20">Next, we characterized the differential contributions of local vs. long-range connections. While the net local inputs were smaller and negative (inhibition-dominated; <xref ref-type="bibr" rid="R12">Haider et al., 2013</xref>), the long-range inputs were stronger (and positive by design; <xref ref-type="fig" rid="F3">Figure 3B</xref>, left) leading to positive net recurrent inputs. Moreover, modulating local and long-range connection strengths separately revealed that stimulus-epoch sustained activity depended strongly on long-range interactions, but more weakly on within-area interactions. Together, these sensitivity analyses suggest a mechanism for sustained sensory responses in the V1-LM circuit that relies on across-area reverberation of activity, mediated by bidirectional long-range connections.</p></sec><sec id="S4"><title>Sustained sensory responses are maintained by approximate line attractor dynamics across V1 and LM</title><p id="P21">To further characterize the origin and properties of V1-LM reverberation induced by transient inputs (<xref ref-type="fig" rid="F3">Figure 3C</xref>), we analyzed the activity flow field in the latent circuits. In the subspace defined by the two principal components of latent activity, the autonomous flow of the latent circuit’s dynamics (i.e. latent trajectories obtained in the absence of external inputs) primarily converged towards a line of slow dynamics (<xref ref-type="fig" rid="F3">Figure 3D</xref>, one example mouse). Consistently across mice, the latent state trajectories underlying the neural data spent most of the stimulus epoch near this line of weak flow, only briefly leaving this region at stimulus onset and offset in response to transient external inputs (<xref ref-type="fig" rid="F3">Figure 3E</xref>). This picture is highly suggestive of line attractor dynamics [<xref ref-type="bibr" rid="R13">Ganguli et al., 2008</xref>, <xref ref-type="bibr" rid="R14">Mante et al., 2013</xref>, <xref ref-type="bibr" rid="R15">Nair et al., 2023</xref>, <xref ref-type="bibr" rid="R16">Sylwestrak et al., 2022</xref>], a regime characterized by slow decay of activity along a select direction in state space, with all other directions decaying more rapidly. Mathematical analysis of the time constants present in the latent circuits (<xref ref-type="fig" rid="F3">Figure 3F</xref>, <xref ref-type="sec" rid="S13">Methods</xref>) revealed such a gap, with local dynamics around the stimulus-evoked response largely dominated by a slow mode with a timescale of ~ 400 ms, which is 20 times longer than the characteristic time constant of single neurons in our model (20 ms). Although the second longest time constant was also slow (≫ 20 ms) – a point we will return to below (<xref ref-type="fig" rid="F4">Figure 4</xref>) – it was significantly shorter than the slowest, reflected in a high "line attractor score" (<xref ref-type="fig" rid="F3">Figure 3G</xref>; <xref ref-type="sec" rid="S13">Methods</xref>).</p><p id="P22">Importantly, the approximate line attractor we identified in the latent models arose from the constraints we imposed on the structure of the circuit. Indeed, whilst unconstrained model fits did also produce slow dynamics (<xref ref-type="fig" rid="F3">Figure 3G</xref>, orange), they exhibited less consistent line attractor scores, primarily because their slowest modes were occasionally oscillatory (and therefore planar; inset). Moreover, we found that the line attractor arose specifically from the long-range excitatory interactions between V1 and LM. First, the line attractor score was sensitive to modulation of the long range, but not the local, connections (<xref ref-type="fig" rid="F3">Figure 3J</xref>). Second, each area considered separately (i.e. with long-range connections removed) did not exhibit any line attractor (<xref ref-type="fig" rid="F3">Figure 3H</xref>) and had substantially faster dynamics (<xref ref-type="fig" rid="F3">Figure 3I</xref>). Although weaker reverberation of activity in those isolated areas could in principle reflect their smaller sizes, randomly thinning both latent sub-circuits by eliminating half of their units did yield significantly slower dynamics than those of the isolated areas (<xref ref-type="fig" rid="F3">Figure 3I</xref>, gray).</p></sec><sec id="S5"><title>A minimal model of the V1-LM circuit explains the emergence of a line attractor</title><p id="P23">To understand the circuit mechanisms that underlie the emergence of a line attractor across V1 and LM, we considered simplified models of multi-area excitation/inhibition (E-I) networks. As a starting point, we recall a canonical model of cortical E-I circuits, with one E and one I population recurrently connected as shown in <xref ref-type="fig" rid="F4">Figure 4A</xref> (top), with E-I weight parameters <italic>e</italic> and <italic>i</italic>. In these networks, activity can be generically decomposed into two main motifs (<xref ref-type="fig" rid="F4">Figure 4A</xref>, middle): E-I imbalance (with the E population firing more than average, and the I population firing less; dashed boxes) and E-I balance (both populations firing in the same way; solid boxes). In this modal decomposition, the recurrent connectivity is more easily interpreted: it acts to transiently amplify any momentary imbalance in network activity into balanced activity (<xref ref-type="fig" rid="F4">Figure 4A</xref>, “Schur basis”; <xref ref-type="bibr" rid="R17">Murphy and Miller, 2009</xref>). Whilst E-I imbalance is typically short-lived, balanced activity may linger depending on the level of excitatory dominance in the recurrent connectivity (<xref ref-type="fig" rid="F4">Figure 4A</xref>, bottom; <xref ref-type="supplementary-material" rid="SD1">Supplementary Material S2</xref>).</p><p id="P24">Next, we extended the canonical single-area E/I model to two interacting areas, yielding an idealized reduction of our latent circuit models of V1 and LM. In this model, each area is modelled as an E-I circuit as above, and they interact via long-range excitatory connections of strength <italic>ℓ</italic> (<xref ref-type="fig" rid="F4">Figure 4B</xref>). Mathematical analysis of this model revealed a similar kind of feedforward connectivity as for single-area E-I networks, now for two different sets of unbalanced/balanced modes. In the first set, the two areas fluctuate congruently such that their patterns of E-I activities – whether balanced or unbalanced – are aligned (“agree”, green boxes). In the other set, these patterns are anti-aligned across the two areas (“disagree”, purple boxes). Recurrent connectivity now acts separately on each set, with transient amplification of congruent/incongruent E-I imbalance into the corresponding balanced pattern. Notably, long-range excitatory connectivity has an opposite effect on each set of modes: it acts to slow down activity where the two areas agree, and speed up the decay of any disagreement (<xref ref-type="fig" rid="F4">Figure 4E</xref>; <xref ref-type="supplementary-material" rid="SD1">Supplementary Material S2</xref>). This separation of timescales gives rise to approximate line attractor dynamics in the combined circuit (<xref ref-type="fig" rid="F4">Figure 4G</xref>), as observed in the latent circuit models we had obtained from data (recall <xref ref-type="fig" rid="F3">Figure 3D-G</xref>). Moreover, the model clarifies that the line attractor arises specifically from long-range connections as previously shown in <xref ref-type="fig" rid="F3">Figure 3H-J</xref>. In addition, the model confirms that the line attractor score (which depends directly on the timescale separation) should grow with the strength of those long-range connections as in <xref ref-type="fig" rid="F3">Figure 3J</xref>.</p><p id="P25">Notably, this simplified model of V1-LM interactions not only provided a qualitative explanation for the emergence of a line attractor, it also matched the dynamics of our latent circuit models quantitatively. Indeed, with optimally chosen parameters, this 4-dimensional network could account for 65% of the impulse response of the (linearized) 16-dimensional network (<xref ref-type="supplementary-material" rid="SD1">Figure S8</xref>).</p><p id="P26">Importantly, the simplified model of V1-LM interactions out lined above makes a prediction that can be tested independently of our latent circuit model fits. Specifically, V1-LM activity projected along the balanced-agree mode should exhibit slower fluctuations than along the balanced-disagree mode. To verify this prediction experimentally without relying on the latent circuit model, we estimated the contribution of each of these two modes to the momentary activity of the recorded neurons in our dataset. This was done by separately averaging the activity of V1 and LM neurons to estimate local balance in each area, and then taking the sum (agree) and the difference (disagree) of these local averages. As predicted, we found that the empirical ‘balanced-agree’ mode had a longer autocorrelation decay time than its ‘disagree’ counterpart (<xref ref-type="fig" rid="F4">Figure 4H</xref>; <xref ref-type="sec" rid="S13">Methods</xref>), both before (left) and during (right) the presentation of the sensory stimulus.</p><p id="P27">More generally, the model predicts slower dynamics along the balanced-agree mode compared to <italic>any</italic> other mode, including the unbalanced modes. Testing this more general prediction without referring to our latent circuit models is difficult, because estimating momentary E-I imbalance in V1 or LM directly from the neural data requires knowing the E-I identities of all cells. Nevertheless, identifying these modes based on model-predicted cell identities (c.f. <xref ref-type="fig" rid="F1">Figures 1</xref> to <xref ref-type="fig" rid="F3">3</xref>) allowed us to confirm this more general prediction (<xref ref-type="supplementary-material" rid="SD1">Figure S7A</xref>).</p></sec><sec id="S6"><title>Multi-area consensus on stimulus presence and identity via selective long-range interactions</title><p id="P28">The selective slowing down of activity patterns where V1 and LM “agree”, and concurrent quenching of patterns where they disagree, can be seen as a circuit mechanism for consensus building (<xref ref-type="fig" rid="F4">Figure 4G</xref>). We wondered about the generality of this mechanism: whilst the minimal 2-area model of <xref ref-type="fig" rid="F4">Figure 4B</xref> gives rise to consensus regarding whether or not a stimulus is present, a similar mechanism could also underlie consensus about stimulus identity. We hypothesized that this second mode of consensus might also account for the second slowest mode in the learned dynamics (<xref ref-type="fig" rid="F3">Figure 3F</xref>), which the simple reduced model introduced above was unable to explain.</p><p id="P29">To explore this hypothesis, we took a similar modelling approach as above. We constructed a more detailed reduced model of a 2-area network (<xref ref-type="fig" rid="F4">Figure 4C</xref>) that incorporates feature specificity in its connectivity (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Material S2</xref>). Each area was split into two E-I sub-circuits that were differentially driven by two orthogonally oriented stimuli (corresponding to go and no-go stimuli in our experiments). Recurrent E-I connectivity in each area had a degree of specificity that we could vary, i.e. connectivity could be made stronger within, compared to between, the two local sub-circuits with different stimulus preference. Similarly, long range excitatory connection strengths included both a baseline (<italic>ℓ<sub>0</sub></italic>) and a specific (<italic>ℓ<sub>s</sub></italic>) component (<xref ref-type="sec" rid="S13">Methods</xref>).</p><p id="P30">The effect of the connectivity on the dynamics of this circuit could again be understood by considering a modal decomposition similar to <xref ref-type="fig" rid="F4">Figure 4B</xref>, which included (i) patterns of E-I imbalance/balance, in which (ii) the two areas could either agree (green) or disagree (purple), and which (iii) were either stimulus selective (dark) or unselective (light). We found that this circuit would predominantly dwell in two of these activity modes: the ‘balanced-agree-selective’ mode, and the ‘balanced-agree-unselective’ mode, both of which were characterized by long time constants. As before, all ‘unbalanced’ and ‘disagree’ modes were associated with comparatively faster decay times. The slow decay of the ‘balanced-agree-unselective’ mode relied on strong long-range connections regardless of specificity (<italic>ℓ<sub>0</sub></italic> or <italic>ℓ<sub>s</sub></italic>; <xref ref-type="fig" rid="F4">Figure 4F</xref>, left), whilst the slow decay of the ‘balanced-agree-selective’ mode required strong <italic>specific</italic> long-range connections (<italic>ℓ<sub>s</sub></italic>; <xref ref-type="fig" rid="F4">Figure 4F</xref>, right). Thus, this circuit supports the dynamic formation of a consensus across V1 and LM about both the presence of a stimulus and its identity. The presence of a second slow mode made this 8D reduced model an even better quantitative match to the linearized dynamics of the full 16D model (<xref ref-type="supplementary-material" rid="SD1">Figure S9</xref>; 77.3% of variance captured in the impulse response). Additionally, we also verified that the two slowest modes in the dynamics of our latent circuit models aligned well with the unselective and selective balanced-agree modes (<xref ref-type="supplementary-material" rid="SD1">Figure S7F</xref>).</p><p id="P31">We could again articulate the model’s predictions regarding the relative timescales of these different activity modes, and use our neural recordings to test these predictions. In particular, the model predicted that the network’s activity should fluctuate slower along the two main modes of consensus than along the corresponding modes of disagreement. To test this hypothesis independently of our model fits, we estimated the degree of engagement of each neuron in the four ‘agree/disagree-selective/unselective’ modes based on its observed responses, and assessed the slowness of population activity projected onto these modes. Specifically, we first extracted the sensitivity of each recorded population (V1 or LM) to the presence of a stimulus irrespective of its identity by taking the difference of its population activity vector after and before stimulus onset, denoted by <inline-formula><mml:math id="M3"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mtext>V1/LM</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> (<xref ref-type="sec" rid="S13">Methods</xref>). We then defined the ‘agree-unselective’ mode as <inline-formula><mml:math id="M4"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mtext>V1</mml:mtext></mml:mrow></mml:msubsup><mml:mspace width="0.2em"/><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mi>α</mml:mi><mml:msubsup><mml:mi>v</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mtext>LM</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, and the ‘disagree-unselective’ mode as <inline-formula><mml:math id="M5"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mtext>V1</mml:mtext></mml:mrow></mml:msubsup><mml:mspace width="0.2em"/><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:msubsup><mml:mi>v</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mtext>LM</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, where <italic>α</italic> is a scaling factor that accounts for unequal sampling of V1 vs. LM neurons in our recordings (<xref ref-type="sec" rid="S13">Methods</xref>). Similarly, by computing the differences <inline-formula><mml:math id="M6"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mtext>s</mml:mtext><mml:mrow><mml:mtext>V1/LM</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> between population responses to the go and no-go stimuli, we could define the ‘agree-selective’ and ‘disagree-selective’ modes as <inline-formula><mml:math id="M7"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mtext>s</mml:mtext><mml:mrow><mml:mtext>V1</mml:mtext></mml:mrow></mml:msubsup><mml:mspace width="0.2em"/><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mo>±</mml:mo><mml:mi>α</mml:mi><mml:msubsup><mml:mi>v</mml:mi><mml:mtext>s</mml:mtext><mml:mrow><mml:mtext>LM</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> respectively. We then projected the activity of all recorded neurons onto these four modes and computed the autocorrelations of the resulting signals (<xref ref-type="fig" rid="F4">Figure 4I</xref>). As predicted by the model, activity fluctuated slower along both ‘agree’ modes, compared to the corresponding ‘disagree’ modes. This was true both for activity taken before and during the stimulus presentation. The relative slowness of the ‘agree-selective’ mode thus suggests some degree of specificity in the long-range connections between V1 and LM, consistent with experimental findings [<xref ref-type="bibr" rid="R18">Ding et al., 2023</xref>].</p></sec><sec id="S7"><title>Functional consequences of consensual dynamics</title><p id="P32">To explore the functional significance of slow unselective and selective consensual dynamics across V1 and LM, we revisited our minimal selective model (<xref ref-type="fig" rid="F5">Figure 5A</xref>, top), and examined its dynamics along the agree/disagree modes identified earlier. Specifically, we provided the model with a stimulus whose time course captured both the strong transient and weaker sustained characteristics of the input which we had inferred from the recorded spiking data (<xref ref-type="fig" rid="F5">Figure 5A</xref>, middle). By varying the degree to which the stimulus drove (i) each area, as well as (ii) each sub-population therein, we could manipulate the degree of ‘input agreement’ between V1 and LM about (i) the presence of a visual stimulus and (ii) whether it is oriented at +45 or −45 degrees (<xref ref-type="fig" rid="F5">Figure 5B and E</xref>, gray insets). We could then examine any emergent consensus in the network’s response. For example, the stimulus pattern shown in <xref ref-type="fig" rid="F5">Figure 5B</xref> (gray) drives V1 and LM in opposite directions, increasing V1 activity while suppressing LM (evidence for the presence of the stimulus in V1, and against it in LM). Mathematically, this stimulus recruits both the agree- and disagree-unspecific modes, leading to input ambiguity. In the absence of long-range connections between V1 and LM, the network’s response directly reflects this lack of consensus in the input (<xref ref-type="fig" rid="F5">Figure 5B, left; C and D, black</xref>). However, with increasingly strong long-range connections, the network selectively amplifies the input contribution to the agree-unselective mode, while suppressing it for the disagree-unselective mode, thus allowing an inter-area consensus to dynamically emerge on the presence of a stimulus (<xref ref-type="fig" rid="F5">Figure 5B-D</xref>, orange). Importantly, this consensus is contingent on bidirectional inter-area reverberation of activity, and is significantly diminished if the feedback connections from LM to V1 are ablated (<xref ref-type="supplementary-material" rid="SD1">Figure S10</xref>).</p><p id="P33">Likewise, when the input to both areas has the same total magnitude but is conflicted about stimulus orientation (<xref ref-type="fig" rid="F5">Figure 5E</xref>, gray inset), specific long-range connections contribute to the emergence of a consensus about stimulus identity (<xref ref-type="fig" rid="F5">Figure 5E-G</xref>). Importantly, this consensus favors the alternative that is more strongly supported by the input (here, +45°; see <xref ref-type="fig" rid="F5">Figure 5G</xref>, dashed, for the opposite scenario). This is true even when the stimulus contributes more to the disagree-selective than to the agree-selective mode (as in the case shown here).</p></sec><sec id="S8" sec-type="discussion"><title>Discussion</title><p id="P34">Here, we set out to elucidate the role of long-range connectivity in orchestrating dynamics across cortical modules. By combining data-driven and mechanistic modelling, we developed latent circuit models of observed neural activity across mouse V1 and LM, which were constrained by known properties of cortical circuit organization. These models uncovered slow reverberation of activity through long-range connections between the two areas. Further mathematical modelling revealed how this dynamical motif constrains the activity of distributed cortical modules in a way that ensures consistency of computation, or ‘consensus’ between them.</p><sec id="S9"><title>Issues with model identifiability and how to mitigate them</title><p id="P35">Identifying dynamical interactions between brain areas from concurrent observations of their activity is in general an ill-posed problem. Indeed, when trying to account for observed neural activity using a network model, it is difficult to unequivocally tease apart external and recurrent contributions to the input that drives each neuron’s fluctuations [<xref ref-type="bibr" rid="R19">Pandarinath et al., 2018</xref>, <xref ref-type="bibr" rid="R6">Schimel et al., 2022</xref>, <xref ref-type="bibr" rid="R20">Malonis et al., 2021</xref>, <xref ref-type="bibr" rid="R21">Soldado-Magraner et al., 2023</xref>], as neither input is directly observed. In principle, even when using rich single-trial data, no approach is immune to wrongly inferring a mechanism not actually present in the cortical circuit [<xref ref-type="bibr" rid="R22">Qian et al., 2024</xref>, <xref ref-type="bibr" rid="R23">Genkin and Engel, 2020</xref>]. Our approach mitigates this concern in two ways. First, we include responses to optogenetic perturbations in the dataset used to fit the model; thus, the time course of at least some of the external inputs to specific cells is known in at least some of the trials. Indeed, such perturbations apply instantaneous, direct input to known cells, in contrast to e.g. sensory stimuli which enter the circuit of interest after largely unknown spatial and temporal filtering. Second, by introducing biological constraints into the model, we not only restrict the space of possible models that fit the recorded neural activity, but also expose the model to a series of experimentally testable validation criteria. For example, we were able to exclude models that would wrongly label known PV cells as excitatory, and explicitly simulate the effect of cell type-specific photo-activation to predict the corresponding neural responses. Finally, our model ultimately made qualitative predictions about the relative timescales of activity in different cross-area modes, which we were able to verify completely independently of our specific model fits (<xref ref-type="fig" rid="F4">Figure 4</xref>).</p></sec><sec id="S10"><title>Generalizing to other mechanistic models</title><p id="P36">Mechanistic models of cortical circuits have classically focused on capturing the average behaviour of large neuronal populations, and have proven remarkably effective at explaining non-trivial qualitative features such as oscillations, global E/I balance, normalization effects, surround suppression, etc [<xref ref-type="bibr" rid="R24">Rubin et al., 2015</xref>, <xref ref-type="bibr" rid="R25">Kraynyukova and Tchumatchenko, 2018</xref>]. However, it remains unclear how these models should be extended to account for more detailed aspects of a circuit’s behaviour, and how their parameters could be constrained quantitatively using large-scale time series of neural data. Our work outlines a systematic path for distilling detailed recordings of large neuronal populations into the parameters of rich mechanistic models.</p></sec><sec id="S11"><title>Role of long-range connections in sustaining activity in the cortex</title><p id="P37">Our models and analyses make experimentally testable predictions. Specifically, we predict that stimulus-specific external input to the visual cortex is predominantly restricted to stimulus onset and offset, while the sustained cortical responses are supported by long-range cortical connections. Notably, the transient time course of our inferred external input resembles recent recordings from the visual thalamus (dLGN, <xref ref-type="bibr" rid="R26">Siegle et al., 2021</xref>). Paradoxically, despite the transient nature of feedforward thalamic input, intact thalamic activity was shown to be essential for sustained cortical responses: silencing the thalamus via optogenetic activation of the thalamic reticular nucleus (TRN) leads to a rapid decay of activity in V1 [<xref ref-type="bibr" rid="R27">Reinhold et al., 2015</xref>]. At first glance, this appears to also contradict our predictions. However, it is important to consider that TRN activation inhibits not only dLGN but also higher-order thalamic areas (e.g., pulvinar), which are thought to modulate corticocortical interactions [<xref ref-type="bibr" rid="R28">Sherman and Guillery, 2011</xref>, <xref ref-type="bibr" rid="R29">Saalmann and Kastner, 2011</xref>]. This could effectively isolate V1 from other cortical areas. Indeed, the rapid decay of cortical activity observed in <xref ref-type="bibr" rid="R27">Reinhold et al. [2015]</xref> is consistent with the fast decay time constants we identified in the isolated dynamics of our model’s V1 population. More broadly, beyond visual networks, sustained cortical activity in decision making or motor planning has also been shown to rely on multi-area interactions [<xref ref-type="bibr" rid="R30">Li et al., 2016</xref>, <xref ref-type="bibr" rid="R31">Guo et al., 2017</xref>].</p></sec><sec id="S12"><title>Role of long-range connections in consensus building</title><p id="P38">Here, we have found that the coupled dynamics of V1 and LM implement a form of consensus algorithm, whereby the two areas progressively get to reconcile their views about the presence of a stimulus and its coarse orientation. The fairly simple nature of this consensus arguably reflects the simplicity of our experimental go/no-go task. However, we hypothesize that dynamic consensus is a general feature of cortical dynamics that could play out at finer scales and be modulated to meet complex behavioural demands. Importantly, achieving fine-grained consensus would require detailed specificity in long-range connections between cortical areas. Just how such specificity could be achieved and regulated by behavioral context or learning is largely unknown. One possible mechanism would exploit trans-thalamic pathways, which appear to systematically mirror direct cortico-cortical pathways [<xref ref-type="bibr" rid="R32">Halassa and Sherman, 2019</xref>, <xref ref-type="bibr" rid="R33">Shepherd and Yamawaki, 2021</xref>]. Detailed gain modulation of thalamic neurons involved in those pathways (e.g. pulvinar, known to send functionally specific projections to V1; <xref ref-type="bibr" rid="R34">Furutachi et al., 2024</xref>) could provide sufficient flexiblity for regulating multiple modes of consensus between cortical areas. Indeed, <xref ref-type="bibr" rid="R35">Mo et al. [2024]</xref> showed that inhibiting the trans-thalamic pathway between primary and higher-order somatosensory cortices in mice leads to a loss of learning-induced texture selectivity, but no change in overall cell responsiveness to tactile stimuli. Our model of <xref ref-type="fig" rid="F5">Figure 5</xref> would attribute such effects to a decrease in <italic>specific</italic> long-range connectivity affecting consensus in the <italic>selective</italic> mode useful for stimulus discrimination, but not affecting the unselective mode useful for stimulus detection. More generally, richer forms of consensus arising from fine-grained connectivity could serve more complex computations, for example the integration and reconciliation of bottom-up sensory information with top-down prior expectations [<xref ref-type="bibr" rid="R36">Knill and Pouget, 2004</xref>]. By integrating data from large-scale functional connectomics [<xref ref-type="bibr" rid="R37">MICrONS Consortium et al., 2021</xref>] with multi-area neural recordings during more complex tasks, our theoretical approach is ideally positioned to test such hypotheses and uncover the richer dynamics of brain-wide consensus.</p></sec></sec><sec id="S13" sec-type="methods"><title>Methods</title><sec id="S14" sec-type="methods"><title>Experimental procedures</title><p id="P39">No new experimental data were collected for the purposes of this study. The acquisition and pre-processing of data used in this study are described in detail in <xref ref-type="bibr" rid="R5">Javadzadeh and Hofer [2022]</xref>. From the total of 14 mice included in <xref ref-type="bibr" rid="R5">Javadzadeh and Hofer [2022]</xref>, we sub-selected 7 mice for inclusion in this study, based on the criterion that the electrophysiological recordings contained at least one well-isolated single unit that was identified by the optogenetic perturbations as PV+. Models were fit using all trial types, but only trials in which the mice performed the task correctly were included in subsequent analyses, unless specified otherwise. The spiking activity of the recorded neurons was binned at 5ms resolution, and for visualization, smoothed with a running average of 25ms or 5 bins (<xref ref-type="fig" rid="F1">Figures 1B,C,G</xref>,<xref ref-type="fig" rid="F2">2A,D</xref>,<xref ref-type="fig" rid="F3">3C</xref>).</p></sec><sec id="S15"><title>Latent circuit model of V1/LMdata</title><sec id="S16"><title>Latent circuit dynamics</title><p id="P40">We modelled latent circuit dynamics as an input-driven recurrent neural network described by a standard firing rate equation [<xref ref-type="bibr" rid="R38">Dayan and Abbott, 2005</xref>]. Specifically, the circuit’s <italic>n</italic>-dimensional ‘latent state’ <italic>z</italic> evolved according to <disp-formula id="FD3"><label>(3)</label><mml:math id="M8"><mml:mrow><mml:mi>τ</mml:mi><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.2em"/><mml:mo>=</mml:mo><mml:mspace width="0.2em"/><mml:mo>−</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.2em"/><mml:mo>+</mml:mo><mml:mspace width="0.2em"/><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mo>Φ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.2em"/><mml:mo>+</mml:mo><mml:mspace width="0.2em"/><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>τ</italic> = 20ms is a single-neuron characteristic time constant, <bold><italic>W</italic></bold> is a matrix of recurrent connectivity (see below), <bold><italic>B</italic></bold> is a matrix of input weights, and <inline-formula><mml:math id="M9"><mml:mrow><mml:mo>Φ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.2em"/><mml:mo>=</mml:mo><mml:mspace width="0.2em"/><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mspace width="0.2em"/><mml:mo>+</mml:mo><mml:mspace width="0.2em"/><mml:msqrt><mml:mrow><mml:msup><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mn>0.1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula> is a soft rectified-linear activation function. Note the presence of external inputs <italic><bold>u</bold>(t)</italic> described in detail below. The spiking activities of our <italic>N</italic> recorded neurons were then modelled as conditionally independent Poisson processes given the latent circuit’s activity, <italic><bold>z</bold></italic>(<italic>t</italic>), with momentary firing rates <italic><bold>r</bold></italic>(<italic>t</italic>) given by: <disp-formula id="FD4"><label>(4)</label><mml:math id="M10"><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.2em"/><mml:mo>=</mml:mo><mml:mspace width="0.2em"/><mml:mi>exp</mml:mi><mml:mspace width="0.2em"/><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.2em"/><mml:mo>+</mml:mo><mml:mspace width="0.2em"/><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></disp-formula> <disp-formula id="FD5"><label>(5)</label><mml:math id="M11"><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.2em"/><mml:mo>∼</mml:mo><mml:mspace width="0.2em"/><mml:mtext>Poisson</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>Here, <italic><bold>C</bold></italic> is a <italic>N</italic> x <italic>n</italic> matrix of output weights and <italic><bold>d</bold></italic> is an N-dimensional vector of constant offsets. <xref ref-type="disp-formula" rid="FD3">Equation 3</xref> was discretized using a time step <italic>dt</italic> = 5<italic>ms</italic>. All model parameters were optimized to fit the electrophysiological data (see below, ‘<xref ref-type="sec" rid="S21">Network training procedure</xref>’). Critically, <bold><italic>W, C</italic></bold> and <bold><italic>B</italic></bold> were constrained to reflect biophysical properties of the V1-LM network (see below; schematics in <xref ref-type="fig" rid="F1">Figure 1D-F</xref>).</p><p id="P41">Note that <xref ref-type="disp-formula" rid="FD3">Equation 3</xref> does not include a constant input term. We found that including such a bias term caused the model to fall into local minima, consistently learning solutions with worse residual log-likelihoods (see <xref ref-type="supplementary-material" rid="SD1">Figure S1E</xref>).</p></sec><sec id="S17"><title>External inputs</title><p id="P42">Our model captures trial-by-trial variability in neural activity not only via the Poisson sampling step in <xref ref-type="disp-formula" rid="FD5">Equation 5</xref>, but also – and more importantly – through trial-by-trial fluctuations in the external inputs <italic><bold>u</bold></italic>(<italic>t</italic>). These (deterministically) produce variations in latent circuit activity according to <xref ref-type="disp-formula" rid="FD3">Equation 3</xref>, and therefore also in the neurons’ firing rates (<xref ref-type="disp-formula" rid="FD4">Equation 4</xref>). In the language of probabilistic modelling, the external inputs u constitute the model’s latent variables.</p><p id="P43">Simultaneously inferring dynamics <italic>and</italic> external input is a fundamentally ill-posed problem, which our probabilistic model addresses by placing task-informed, non-stationary prior distributions on the latent inputs. Specifically, we used three input channels – i.e. <italic><bold>u</bold></italic>(<italic>t</italic>) = [<italic>u</italic><sub>0</sub>(<italic>t</italic>), <italic>u</italic><sub>1</sub>(<italic>t</italic>), u<sub>2</sub>(<italic>t</italic>)]<sup>⊤</sup>, each entering the latent circuit through input weights given by the corresponding column of the <italic>n</italic> x 3 matrix <italic><bold>B</bold></italic> (<xref ref-type="disp-formula" rid="FD3">Equation 3</xref>). For each input channel <italic>i</italic>, we assumed <italic>u<sub>i</sub></italic>(<italic>t</italic>) to be (a priori) <italic>independently</italic> and normally distributed across time steps – ensuring that any continuous/smooth fluctuations in firing rates could only be accounted for by recurrent dynamics in the latent circuit. Moreover, the variance of this Gaussian prior was given a channel- and trial-specific temporal profile reflecting the known timing of the corresponding stimulus: <disp-formula id="FD6"><label>(6)</label><mml:math id="M12"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.2em"/><mml:mo>∼</mml:mo><mml:mspace width="0.2em"/><mml:mi mathvariant="script">N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:msubsup><mml:mi>∑</mml:mi><mml:mn>0</mml:mn><mml:mi>i</mml:mi></mml:msubsup><mml:mspace width="0.2em"/><mml:mo>+</mml:mo><mml:msup><mml:mi>∑</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula> <disp-formula id="FD7"><label>(7)</label><mml:math id="M13"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.2em"/><mml:mo>=</mml:mo><mml:mspace width="0.2em"/><mml:mn>1</mml:mn><mml:mspace width="0.2em"/><mml:mtext>if</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>laser</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>on,</mml:mtext><mml:mspace width="0.2em"/><mml:mn>0</mml:mn><mml:mspace width="0.2em"/><mml:mtext>otherwise</mml:mtext></mml:mrow></mml:math></disp-formula> <disp-formula id="FD8"><label>(8)</label><mml:math id="M14"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.2em"/><mml:mo>=</mml:mo><mml:mspace width="0.2em"/><mml:mn>1</mml:mn><mml:mspace width="0.2em"/><mml:mtext>if</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>go</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>stimulus</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>on</mml:mtext><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mn>0</mml:mn><mml:mspace width="0.2em"/><mml:mtext>otherwise</mml:mtext></mml:mrow></mml:math></disp-formula> <disp-formula id="FD9"><label>(9)</label><mml:math id="M15"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.2em"/><mml:mo>=</mml:mo><mml:mspace width="0.2em"/><mml:mn>1</mml:mn><mml:mspace width="0.2em"/><mml:mtext>if</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>no-go</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>stimulus</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>on,</mml:mtext><mml:mspace width="0.2em"/><mml:mn>0</mml:mn><mml:mspace width="0.2em"/><mml:mtext>otherwise</mml:mtext></mml:mrow></mml:math></disp-formula> where <italic>∑<sup>i</sup><sub>0</sub></italic> and <italic>∑<sup>i</sup></italic> are two positive variance parameters optimized alongside all other model parameters (see below).</p><p id="P44">Given that the laser input in our experiments had a direct effect only on inhibitory neurons, we constrained the first column of <bold><italic>B</italic></bold> (associated with <italic>u</italic><sub>0</sub>(<italic>t</italic>)) to be zero for all sub-populations except for the inhibitory neurons of the targeted area. Additionally, we ensured that the weights of this column of <bold><italic>B</italic></bold> were all positive. Finally, to eliminate the degeneracy that exists between the scale of the inputs <italic><bold>u</bold></italic>(<italic>t</italic>) (set by<italic>∑<sup>i</sup><sub>0</sub></italic> and ∑<italic><sup>i</sup></italic> as detailed above) and the scale of the matrix <bold><italic>B</italic></bold>, we constrained the norm of each column of <bold><italic>B</italic></bold> to be equal to <inline-formula><mml:math id="M16"><mml:mrow><mml:msqrt><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula> (where <italic>n</italic> is the number of units in the latent circuit, and <italic>m</italic> = 3 is the number of input channels). The <bold><italic>B</italic></bold> matrices learned for all animals are shown in <xref ref-type="supplementary-material" rid="SD1">Figure S2</xref>.</p></sec><sec id="S18"><title>Constraints on the latent circuit connectivity</title><p id="P45">We partitioned the latent circuit’s activity <italic><bold>z</bold></italic>(<italic>t</italic>) into two halves, corresponding to the V1 and LM subcircuits respectively (see <xref ref-type="fig" rid="F1">Figure 1D</xref>). Within each subcircuit, we took the first half of the latent units to be excitatory, and the other half to be inhibitory. This partitioning of the circuit into four sub-populations allowed us to enforce Dale’s law, as well as the purely excitatory nature of long-range projections, by constraining the recurrent weight matrix <bold><italic>W</italic></bold> to have the following structure: <disp-formula id="FD10"><label>(10)</label><mml:math id="M17"><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mtext>EE</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mtext>V</mml:mtext><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mtext>EI</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mtext>V</mml:mtext><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mtext>EE</mml:mtext></mml:mrow><mml:mrow><mml:mtext>LM</mml:mtext><mml:mo>→</mml:mo><mml:mtext>LM</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mtext>IE</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mtext>V</mml:mtext><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mtext>II</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mtext>V</mml:mtext><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mtext>IE</mml:mtext></mml:mrow><mml:mrow><mml:mtext>LM</mml:mtext><mml:mo>→</mml:mo><mml:msub><mml:mtext>V</mml:mtext><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mtext>EE</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mtext>V</mml:mtext><mml:mn>1</mml:mn></mml:msub><mml:mo>→</mml:mo><mml:mtext>LM</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mtext>EE</mml:mtext></mml:mrow><mml:mrow><mml:mtext>LM</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mtext>EI</mml:mtext></mml:mrow><mml:mrow><mml:mtext>LM</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mtext>IE</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>→</mml:mo><mml:mtext>LM</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mtext>IE</mml:mtext></mml:mrow><mml:mrow><mml:mtext>LM</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mtext>II</mml:mtext></mml:mrow><mml:mrow><mml:mtext>LM</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula> with all elements of the various <inline-formula><mml:math id="M18"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mo>•</mml:mo><mml:mo>∘</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> blocks constrained to be positive. We enforced the sign constraints in our model by passing elements of <bold><italic>W</italic></bold> through a positive nonlinearity, and multiplying <bold><italic>W</italic></bold> with a mask matrix containing the sign of each element. We note that, in related work, <xref ref-type="bibr" rid="R39">Jha et al. [2024]</xref> proposed a method to learn linear latent dynamical systems constrained to follow Dale’s law using a constrained quadratic optimization approach.</p></sec><sec id="S19"><title>Structured sparsity constraint on the latents-to-neurons readout</title><p id="P46">The matrix <italic><bold>C</bold></italic> in <xref ref-type="disp-formula" rid="FD4">Equation 4</xref>, which determines how the firing rates of the recorded neurons (corresponding to rows) are assembled from the activity of the latent units (corresponding to columns), was constrained such that the neurons recorded in V1 (resp. LM) would only be associated with V1 (resp. LM) latent units. This was achieved by enforcing the following block structure (see <xref ref-type="fig" rid="F1">Figure 1F</xref>): <disp-formula id="FD11"><mml:math id="M19"><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mover><mml:mover><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mi>E</mml:mi><mml:mrow><mml:mtext>V</mml:mtext><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mi>E</mml:mi><mml:mrow><mml:mtext>V</mml:mtext><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mi>E</mml:mi><mml:mrow><mml:mtext>V</mml:mtext><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mi>E</mml:mi><mml:mrow><mml:mtext>LM</mml:mtext></mml:mrow></mml:msubsup><mml:msubsup><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mi>E</mml:mi><mml:mrow><mml:mtext>LM</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mi>I</mml:mi><mml:mrow><mml:mtext>LM</mml:mtext></mml:mrow></mml:msubsup><mml:msubsup><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mi>I</mml:mi><mml:mrow><mml:mtext>LM</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">︷</mml:mo></mml:mover><mml:mrow><mml:mtext>latent</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>units</mml:mtext></mml:mrow></mml:mover><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mstyle displaystyle="true"><mml:mo>}</mml:mo></mml:mstyle><mml:munder><mml:mrow><mml:mtext>recorded</mml:mtext></mml:mrow><mml:mrow><mml:mtext>neurons</mml:mtext></mml:mrow></mml:munder></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula> where each <inline-formula><mml:math id="M20"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mo>•</mml:mo><mml:mo>∘</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> is an element-wise positive matrix with unit-norm columns, and each corresponding <inline-formula><mml:math id="M21"><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mo>•</mml:mo><mml:mo>∘</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> is a positive scalar. This per-block column-wise normalization of <italic><bold>C</bold></italic> balances the model internally by ensuring that all the latent units within each sub-population have a comparable effect on the activity of the observed neurons. Moreover, the inclusion of separate scale factors <inline-formula><mml:math id="M22"><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mo>•</mml:mo><mml:mo>∘</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> allows the different E/I sub-populations to contribute to different degrees to the neural activity.</p><p id="P47">Importantly, to facilitate interpretability of the latent circuit, we learned the model in such a way that it would unequivocally label each recorded neuron as being excitatory or inhibitory. We achieved this by included in the overall cost function (see below) a structured sparsity penalty on <italic><bold>C</bold></italic> that encourages each recorded neuron to be locally associated either with the excitatory latent units, or with the inhibitory latent units, but not with both types simultaneously. In other words, this penalty promotes parameter solutions in which the rows of <bold><italic>C</italic></bold> are non-zero either within the C<sub>E</sub><sup>◦</sup> block or within the C<sub>I</sub><sup>◦</sup> block (where ◦ denotes the relevant cortical area). This penalty took the following form: <disp-formula id="FD12"><label>(11)</label><mml:math id="M23"><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>sparsity</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:msqrt><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>∈</mml:mo><mml:mtext>neurons</mml:mtext></mml:mrow></mml:munder><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mo>∘</mml:mo><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:msqrt></mml:mrow></mml:math></disp-formula> where <italic>o<sub>n</sub></italic> ∈ {V1, LM} is the cortical area where neuron <italic>n</italic> was recorded, and (C<sub>•</sub><sup>◦</sup>)<italic><sub>n</sub></italic> denotes the <italic>n</italic><sup>th</sup> row of the matrix block C<sub>•</sub><sup>◦</sup>, and ∥ · ∥ denotes the <italic>L</italic><sub>2</sub> norm. The scalar <italic>λ</italic> was set to 10<sup>3</sup> following a hyperparameter search.</p></sec><sec id="S20"><title>Definition of putative excitatory and inhibitory cells</title><p id="P48">For models trained with the above constraints, we were able to assign each neuron a unique excitatory or inhibitory identity based on the learned readout matrix, <bold><italic>C</italic></bold> (see <xref ref-type="fig" rid="F1">Figure 1D</xref>). For each neuron, we calculated the <italic>L</italic><sub>2</sub> norms of the corresponding readout weights originating from the excitatory and inhibitory latent sub-populations <italic>separately</italic>, and labelled the neuron as E or I according to which of the two norms was the largest.</p></sec></sec><sec id="S21"><title>Network training procedure</title><p id="P49">Our latent circuit model, together with the prior distribution over external inputs and the Poisson observation noise model described above (<xref ref-type="disp-formula" rid="FD3">Equations 3</xref>, <xref ref-type="disp-formula" rid="FD5">5</xref> and <xref ref-type="disp-formula" rid="FD6">6</xref>), constitute a probabilistic generative model whose parameters we directly optimized to fit our spiking data. To this end, we used iLQR-VAE [<xref ref-type="bibr" rid="R6">Schimel et al., 2022</xref>], a generic control-based algorithm for learning probabilistic, input-driven latent dynamics from neural population recordings. iLQR-VAE learns model parameters <italic>θ</italic> that maximize a lower bound on the log likelihood of the data, log <italic>p<sub>θ</sub></italic>(<italic><bold>y</bold></italic>). This evidence lower bound (ELBO; <xref ref-type="bibr" rid="R40">Kingma and Welling, 2013</xref>) is a standard objective, used when the true log likelihood cannot be evaluated in closed-form, as is the case in our model. The ELBO, denoted by ℒ, relies on an approximate posterior distribution over inputs, <italic>q<sub>Φ</sub></italic>(<italic><bold>u</bold>|<bold>y</bold></italic>): <disp-formula id="FD13"><label>(12)</label><mml:math id="M24"><mml:mi>ℒ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>𝔼</mml:mi><mml:mrow><mml:mi>q</mml:mi><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mspace width="0.2em"/><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtext>log</mml:mtext><mml:mspace width="0.2em"/><mml:msub><mml:mi>p</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mtext>log</mml:mtext><mml:mspace width="0.2em"/><mml:msub><mml:mi>p</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mtext>log</mml:mtext><mml:mspace width="0.2em"/><mml:msub><mml:mi>q</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mtext>log</mml:mtext><mml:mspace width="0.2em"/><mml:msub><mml:mi>p</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula></p><p id="P50">In iLQR-VAE, <italic>q<sub>ϕ</sub></italic>(<italic><bold>u</bold>|<bold>y</bold></italic>) = 𝒩(<italic>μ</italic>(<italic>y</italic>), ∑) is parametrized as a Gaussian distribution, whose mean <italic>μ</italic><sub>0</sub>(<italic><bold>y</bold></italic>) is defined as the most likely set of inputs given the data and the model parameters. This maximum a posteriori estimate can be efficiently obtained using the iLQR algorithm [<xref ref-type="bibr" rid="R41">Li and Todorov, 2004</xref>]: <disp-formula id="FD14"><label>(14)</label><mml:math id="M25"><mml:msub><mml:mi>μ</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>argmax</mml:mtext></mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:munder><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:mtext>log</mml:mtext><mml:mspace width="0.2em"/><mml:msub><mml:mi>p</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mtext>log</mml:mtext><mml:mspace width="0.2em"/><mml:msub><mml:mi>p</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula> <disp-formula id="FD15"><label>(15)</label><mml:math id="M26"><mml:mo>=</mml:mo><mml:mtext>iLQRsolve</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P51">As in <xref ref-type="bibr" rid="R6">Schimel et al. [2022]</xref>, we defined the covariance <bold>∑</bold> as a trial-independent, separable matrix, i.e as the Kronecker product of a spatial factor <bold>∑</bold><sub>s</sub> and a temporal factor <bold>∑</bold><sub>t</sub>, which were learned throughout training and shared across all training trials.</p><p id="P52">In summary, fitting our latent circuit model to the V1-LM spiking data involved jointly optimizing all model parameters <italic>θ</italic> and the approximate posterior parameters <italic>ϕ</italic> = {<italic>θ</italic>, ∑<sub>s</sub>, ∑<sub>t</sub>} to minimize the following combined objective: <disp-formula id="FD16"><label>(16)</label><mml:math id="M27"><mml:mi>𝒪</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>ℒ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>sparsity</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula></p></sec><sec id="S22"><title>Log-likelihood computations</title><sec id="S23"><title>Computation of cross-validated log-likelihoods</title><p id="P53">To validate the performance of our model, we computed its ability to predict the activity of held-out neurons, given firing rates inferred using the held-in neurons. We held out one neuron at a time. To predict the activity of held-out neuron <italic>j</italic>, we inferred inputs as <inline-formula><mml:math id="M28"><mml:msup><mml:mover accent="true"><mml:mi>μ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mtext>iLQRsolve</mml:mtext><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mtext>¬j</mml:mtext></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></inline-formula> where <inline-formula><mml:math id="M29"><mml:msubsup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mo>¬</mml:mo><mml:mtext>j</mml:mtext></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>×</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is the spike trains of all neurons, excluding neuron <italic>j</italic>, in trial <italic>k</italic> (and <inline-formula><mml:math id="M30"><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:math></inline-formula> are the model parameters with the <italic>j</italic>-th row of <bold><italic>C</italic></bold> and <italic><bold>d</bold></italic> masked out). We then computed the predicted firing rates for all (both held-in and held-out) neurons <inline-formula><mml:math id="M31"><mml:msup><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msup></mml:math></inline-formula> by unrolling the trajectories induced by the inputs <inline-formula><mml:math id="M32"><mml:msup><mml:mover accent="true"><mml:mi>μ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msup></mml:math></inline-formula> (using the full set of parameters <italic>θ</italic>). In turn, this allowed to compute the log-likelihood of the spikes in trial <italic>k</italic> for the held-out neuron <italic>j</italic>, as <disp-formula id="FD17"><label>(17)</label><mml:math id="M33"><mml:mi>L</mml:mi><mml:msubsup><mml:mi>L</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mi>t</mml:mi></mml:munder><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.2em"/><mml:mtext>log</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mtext>log</mml:mtext><mml:mspace width="0.2em"/><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>!</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p></sec><sec id="S24"><title>Computation of the empirical log-likelihood</title><p id="P54">As a baseline to compare the model predictions to, we computed the empirical log-likelihood for a trial <italic>k</italic> by evaluating the predicted activity for every neuron using that neuron’s average activity across all the other trials from the same condition c, leading to a predicted firing rate time course <disp-formula id="FD18"><label>(18)</label><mml:math id="M34"><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mtext>emp</mml:mtext></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo>∈</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>ℓ</mml:mi><mml:mo>≠</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>j</mml:mi><mml:mi>ℓ</mml:mi></mml:msubsup></mml:mrow></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula> where <italic>N<sub>c</sub></italic> is the number of trials in condition <italic>c</italic>. Given these empirical firing rates, we computed the empirical log-likelihood for neuron <italic>j</italic> at trial <italic>k</italic> as <disp-formula id="FD19"><label>(19)</label><mml:math id="M35"><mml:mi>L</mml:mi><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mtext>emp</mml:mtext></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mi>t</mml:mi></mml:munder><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.2em"/><mml:mtext>log</mml:mtext><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mtext>emp</mml:mtext></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mtext>emp</mml:mtext></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mtext>log</mml:mtext><mml:mspace width="0.2em"/><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>!</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p></sec><sec id="S25"><title>Residual log-likelihood</title><p id="P55">We define the residual log-likelihood for a given neuron <italic>j</italic> as <inline-formula><mml:math id="M36"><mml:mi>L</mml:mi><mml:msubsup><mml:mi>L</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:mi>L</mml:mi><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mtext>emp</mml:mtext></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mo>.</mml:mo></mml:math></inline-formula> If this quantity is positive, it means that the prediction of the model for that neuron is more accurate than a prediction based on trial averaging, i.e., that the model is able to capture meaningful single-trial variability in the data. Residual likelihoods were calculated separately for each neuron across 18 different conditions (2 visual stimuli and 9 silencing condition for each visual stimulus), and then averaged across all trials and conditions.</p></sec></sec><sec id="S26"><title>Model selection</title><sec id="S27"><title>Choice of hyperparameters</title><p id="P56">To select the model hyperparameters <italic>n</italic> and <italic>m</italic> (number of latent state variables and input channels, respectively), we used a 3-fold cross-validation approach. For each animal, we split the trials into 3 subsets. Then, for each possible pair of subsets among these three, we trained a model using the data from that pair and subsequently computed the heldout log-likelihoods on the remaining subset. Finally, we averaged the results over the three pairs, over animals, and over neurons.</p><p id="P57">We first selected the optimal value of <italic>n</italic> for the model with three input channels (<italic>m</italic> = 3) corresponding to the visual and optogenetic stimuli, as described above. We explored model sizes ranging from <italic>n</italic> = 8 to <italic>n</italic> = 24 in increments of 4, and selected the minimal value of <italic>n</italic> after which the residual log-likelihood stopped improving (see <xref ref-type="supplementary-material" rid="SD1">Figure S1A</xref>). Having selected and fixed the optimal value for <italic>n</italic>, we checked whether the choice <italic>m</italic> = 3 was optimal, using the same model selection procedure. When varying the number of input channels, we considered both (i) having multiple channels corresponding to each prior variance profile (c.f. <xref ref-type="disp-formula" rid="FD7">Equations 7</xref> to <xref ref-type="disp-formula" rid="FD9">9</xref>), i.e. multiple channels for each external stimulus (<xref ref-type="supplementary-material" rid="SD1">Figure S1B</xref>), and (ii) the addition of channels with temporally unmodulated prior variance (see <xref ref-type="supplementary-material" rid="SD1">Figure S1C</xref>). Neither of those increased model performance relative to using <italic>m</italic> = 3, which is the minimal number of channels allowing to have one input per external stimulus. Note that models with additional input channels could in theory capture timing difference in the visual input to V1 and to LM. However, we found that having one channel per input yielded the best performance on the validation fold.</p><p id="P58">Additionally, we compared our models to models with the same architecture but for which the inputs were not inferred, and were instead fixed to follow the envelope corresponding to each external stimulus. This implied that their time course was constrained to be the same for every trial of a given condition. Those models performed considerably worse than the models with inferred inputs (<xref ref-type="supplementary-material" rid="SD1">Figure S1D</xref>). Other model hyperparameters such as the spectral radius of <bold><italic>W</italic></bold> at initialization and the Adam learning rate were fixed to values that allowed robust training. Final hyperparameter choices are reported in <xref ref-type="table" rid="T1">Table 1</xref>. All trials, irrespective of behavioral outcome, were included for log-likelihood calculation and model selection.</p></sec><sec id="S28"><title>Selection of models for plotting and analysis</title><p id="P59">For the constrained models, having set the hyperparameters as described above, we trained 10 models with different random seeds (i.e. different random initializations of the model) per animal. This was done to reduce the chance of getting stuck in local minima. Moreover, as our conclusions were dependent on the learned values of the long-range weights, and to avoid biasing our models, we varied the value of the long-range weights at initialization. More precisely, we varied the ratio of the norm of long-range weights to local weights at initialization between 1 and 1.6 in steps of 0.2. We discarded models that diverged during training (41 out of 280 models in total). Out of the remaining models, we then picked the best model for each animal, across initialization seeds and long-range weights, for further analyses and plotting. For each animal, the best model was selected by first sub-selecting the models that classified the known PV cells correctly as inhibitory (187 out of 239, i.e. 78.24% of the models; see <xref ref-type="supplementary-material" rid="SD1">Figure S1F</xref>). Among these, we picked the model that yielded the highest cross-validated log-likelihood. Furthermore, we only included active cells (neurons whose spike count during the stimulus in control trials had a signal-to-noise ratio, i.e. mean/std over trials, larger than 1) for log-likelihood calculations.</p><p id="P60">For the unconstrained models, we used 5 random initialization seeds. However, as inhibitory cell identities were not defined in these models, we picked the best model based only on the held-out log-likelihood criterion explained above.</p></sec></sec><sec id="S29"><title>Calculating covariances</title><p id="P61">In <xref ref-type="supplementary-material" rid="SD1">Figure S3</xref>, we calculated <italic>N × N</italic> noise covariance matrices in both data and model-predicted activity as: <disp-formula id="FD20"><label>(20)</label><mml:math id="M37"><mml:mi>∑</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>T</mml:mi><mml:mstyle displaystyle="true"><mml:msub><mml:mi>∑</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>C</mml:mi></mml:munderover><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula> where <italic>c</italic> indexes conditions (2 visual stimuli and 9 silencing condition per stimulus), <inline-formula><mml:math id="M38"><mml:msubsup><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup></mml:math></inline-formula> is a <italic>N</italic> × 1 vector denoting spike count of <italic>N</italic> neurons in 25ms bins, in control condition <italic>c</italic> (no optogenetic stimulation), trial <italic>k</italic>, and time <italic>t</italic> (<italic>K<sub>c</sub>:</italic> number of trials in condition <italic>c,T</italic>: number of time points, <italic>N</italic>: number of neurons). <inline-formula><mml:math id="M39"><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:math></inline-formula> is the trial-average activity in condition <italic>c</italic>. For calculating model covariances, we sampled pseudo-observations <bold><italic>y</italic></bold> from a Poisson distribution whose mean was taken to be the posterior predicted firing rates. All trials, irrespective of behavioral outcome, were used for calculating covariances. Variances in <xref ref-type="supplementary-material" rid="SD1">Figure S3</xref> are the diagonal values of <bold>∑</bold> and cross-covariances are its off-diagonal values.</p></sec><sec id="S30"><title>Linearization of the dynamics</title><p id="P62">Around a (approximate) fixed point <italic>z<sub>f</sub></italic>, the dynamics in <xref ref-type="disp-formula" rid="FD3">Equation 3</xref> can be Taylor-expanded to first order, leading to a linear dynamical system whose dynamics matrix is given by the Jacobian <bold><italic>A</italic></bold>: <disp-formula id="FD21"><label>(21)</label><mml:math id="M40"><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mo>+</mml:mo><mml:munder><mml:munder><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi><mml:msup><mml:mi>Φ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">︸</mml:mo></mml:munder><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:munder></mml:math></disp-formula> Here, <bold><italic>W</italic></bold><sub>eff</sub> can be thought of as a matrix of “effective connectivity”.</p><p id="P63">For a given trial <italic>k</italic>, we defined <inline-formula><mml:math id="M41"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> as the time-averaged activity either before or during stimulus, i.e <inline-formula><mml:math id="M42"><mml:mrow><mml:msup><mml:mover accent="false"><mml:mi>z</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mo>Δ</mml:mo></mml:mfrac><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mo>Δ</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>z</mml:mi><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> with Δ = 400ms, <italic>T<sub>0</sub></italic> = –400ms for the pre-stimulus window and <italic>T<sub>0</sub></italic> = 100ms for the stimulus window (<italic>T</italic><sub>0</sub> is measured relative to visual stimulus onset). This choice was motivated by the fact that the dynamics exhibited very small velocities in these time windows; see <xref ref-type="fig" rid="F3">Figure 3E</xref>. We then defined <inline-formula><mml:math id="M43"><mml:mrow><mml:msub><mml:mtext mathvariant="bold-italic">z</mml:mtext><mml:mi>f</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:msub><mml:mi>∑</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:msup><mml:mover accent="false"><mml:mtext mathvariant="bold-italic">z</mml:mtext><mml:mo>¯</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula></p></sec><sec id="S31"><title>Computation of the dynamics distance</title><p id="P64">In <xref ref-type="fig" rid="F2">Figure 2C</xref>, we computed the similarity between the dynamics of the model for different animals, as a normalized Procrustes distance (see <xref ref-type="bibr" rid="R42">Williams et al., 2021</xref> and <xref ref-type="bibr" rid="R43">Ostrow et al., 2023</xref>) between their linearized dynamics, i.e as : <disp-formula id="FD22"><label>(22)</label><mml:math id="M44"><mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mtext mathvariant="bold-italic">A</mml:mtext><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext mathvariant="bold-italic">A</mml:mtext><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mstyle displaystyle="true"><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:mstyle><mml:mrow><mml:mtext mathvariant="bold-italic">U</mml:mtext><mml:mo>∈</mml:mo><mml:mi>𝒪</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mspace width="0.2em"/><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mtext>Tr</mml:mtext><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold-italic">A</mml:mtext><mml:mi>i</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext mathvariant="bold-italic">U</mml:mtext><mml:mspace width="0.2em"/><mml:msub><mml:mtext mathvariant="bold-italic">A</mml:mtext><mml:mi>j</mml:mi></mml:msub><mml:msup><mml:mtext mathvariant="bold-italic">U</mml:mtext><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msub><mml:mtext mathvariant="bold-italic">A</mml:mtext><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow><mml:mtext>F</mml:mtext></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msub><mml:mtext mathvariant="bold-italic">A</mml:mtext><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow><mml:mtext>F</mml:mtext></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic><bold>A</bold><sub>i</sub></italic> and <italic><bold>A<sub>j</sub></bold></italic> denote the linearized learned dynamics for animals <italic>i</italic> and <italic>j</italic> (obtained as described in <xref ref-type="sec" rid="S30">Methods - Linearization of the dynamics</xref>), ∥ · ∥<sub>F</sub> denotes the Frobenius norm, and <bold><italic>U</italic></bold> is an orthogonal (rotation) matrix (optimization over <bold><italic>U</italic></bold> is necessary in order to account for the fact that the dynamics may be equivalent up to a rotation). We used the average distance <italic>d</italic>(<italic><bold>A</bold><sub>i</sub></italic>, <italic><bold>A</bold><sub>j</sub></italic>) across all pairs of animals (i.e. all (<italic>i, j</italic>) such that <italic>i</italic> &gt; <italic>j</italic>), as our measure of consistency of the learned dynamics across animals.</p><p id="P65">As shown in <xref ref-type="bibr" rid="R43">Ostrow et al. [2023]</xref>, <italic>d</italic>(·,·) is a valid distance metric, bounded between 0 and 1, which computes the similarity of the vector fields of two dynamical systems. While <xref ref-type="bibr" rid="R43">Ostrow et al. [2023]</xref> applied this analysis to dynamical systems identified via delay embedding of the dynamics, we instead apply it directly to the linearized dynamics of our model.</p><p id="P66">To perform the minimization in <xref ref-type="disp-formula" rid="FD22">Equation 22</xref>, we parametrized the orthogonal matrix <italic><bold>U</bold></italic> using a Cayley transformation [<xref ref-type="bibr" rid="R43">Ostrow et al., 2023</xref>]. As pointed out in <xref ref-type="bibr" rid="R43">Ostrow et al. [2023]</xref>, the optimization landscape is disjoint for <italic><bold>U</bold></italic> matrices with det <italic><bold>U</bold></italic> = 1 and det <italic><bold>U</bold></italic> = –1. Thus, for each pair of dynamics matrices, we perform the optimization over matrices <bold><italic>U</italic></bold> such that det <bold><italic>U</italic></bold> = 1 as well as over matrices <bold><italic>U</italic></bold> such that det <bold><italic>U</italic></bold> = –1, and use the minimum distance across those two subsets.</p></sec><sec id="S32"><title>Comparing the model’s ability to capture the effect of optogenetic perturbations</title><p id="P67">To evaluate how well the models captured the effect of simulated optogenetic perturbations (<xref ref-type="fig" rid="F2">Figure 2D</xref>), we first evaluated the inferred input for each no-go, no-laser trial. We then ran the model dynamics forward with those inputs, whilst additionally perturbing the inhibitory model population in either V1 and LM, depending on which area expressed ChR2 in our experiments (different across animals). Note that these perturbations were simulated for each trial, based on the inferred input for that trial. We could then compare the neural responses predicted by the latent circuit model to the corresponding photo-stimulation responses observed in the experiments. Specifically, we used a simulated pulse of optogenetic input modeled as <disp-formula id="FD23"><label>(23)</label><mml:math id="M45"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext><mml:mspace width="0.2em"/><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext>laser</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext>laser</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>Δ</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>otherwise</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula> where <italic>t</italic><sub>laser</sub> is the onset time of laser stimulation in the relevant silencing condition in the experiments, and Δ = 150 ms is the laser duration. We assumed that this input influenced the latent units via a weight vector <italic>B<sub>p</sub></italic>, whose elements were non-zero only for the inhibitory latent units of the stimulated area. We optimized the non-zero elements of <italic>B<sub>p</sub></italic> to maximize the log-likelihood for the spike trains of the known PV cells in the relevant perturbation trials. We then measured the average predicted perturbation-induced change (relative to no-perturbation), <inline-formula><mml:math id="M46"><mml:mrow><mml:mo>Δ</mml:mo><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">r</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">r</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mtext>control</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">r</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mtext>perturbation</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> in the rest of the neurons during the stimulation time window, and compared it to the same quantity, Δ<bold><italic>r</italic></bold>, measured in the data. We report the quality of fit as the Pearson correlation between <inline-formula><mml:math id="M47"><mml:mrow><mml:mo>Δ</mml:mo><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">r</mml:mtext><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> and Δ<italic><bold>r</bold></italic>. This is plotted for one animal in <xref ref-type="fig" rid="F2">Figure 2D</xref> middle, and for the rest of the animals in <xref ref-type="supplementary-material" rid="SD1">Figure S4</xref>.</p><p id="P68">As a comparison (see <xref ref-type="fig" rid="F2">Figure 2D</xref>), we repeated the above for “control-only” models which were trained on control trials without optogenetic perturbation (2/3 of the control trials were used for training). We trained a minimum of 12 models per animal, and chose the best model following the same procedure used for the default models; see <xref ref-type="sec" rid="S28">Selection of models for plotting and analysis</xref>. (For one of the animals, no model —out of 30 models— resulted in correct classification of all PV neurons; for that animal, we only used the log-likelihood criterion for model selection.)</p></sec><sec id="S33"><title>Spike width histograms</title><p id="P69">We extracted the average spike waveforms for each neuron, and the spike width was defined as the width of this waveform at 10% of its full amplitude (<xref ref-type="fig" rid="F2">Figure 2E</xref>).</p></sec><sec id="S34"><title>Analysis of the role of inhibition in the dynamics</title><p id="P70">To evaluate the role of inhibition in stabilizing the dynamics (<xref ref-type="fig" rid="F2">Figure 2F</xref>), we measured the stability of our latent circuit dynamics, in the presence or absence of inhibition. We measured stability before and during stimulus presentation by computing the effective connectivity <bold><italic>W</italic></bold><sub>eff</sub> (see <xref ref-type="disp-formula" rid="FD21">Equation 21</xref> in <xref ref-type="sec" rid="S30">Methods - Linearization of the dynamics</xref>). We then computed the largest real part of the eigenvalues of the effective linear dynamical system, <italic>λ<sub>max</sub></italic> = max<sub><italic>i</italic></sub> (ℜ(<italic>λ<sub>i</sub></italic>)) where <italic>λ<sub>i</sub></italic> are the eigenvalues of <bold><italic>W</italic></bold><sub>eff</sub>. A (linearized) network is said to be "inhibition-stabilized" if <italic>λ</italic><sub>max</sub> &lt; 1 (stable) when computed on the full <italic><bold>W</bold></italic><sub>eff</sub>, but <italic>λ</italic><sub>max</sub> &gt; 1 (unstable) when all the inhibitory weights in <italic><bold>W</bold></italic><sub>eff</sub> are set to zero.</p></sec><sec id="S35"><title>Connectivity strength as a function of the noise correlation</title><p id="P71">In <xref ref-type="fig" rid="F2">Figure 2G</xref>, we computed the noise correlation matrix of the mean-subtracted latent circuit responses of the V1 excitatory subcircuit during control no-go trials, <italic>z<sub>t</sub></italic>, as follows: <disp-formula id="FD24"><label>(24)</label><mml:math id="M48"><mml:mrow><mml:mo mathvariant="bold">Σ</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mtext mathvariant="bold-italic">D</mml:mtext><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>K</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mtext mathvariant="bold-italic">z</mml:mtext><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mtext mathvariant="bold-italic">z</mml:mtext><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mtext mathvariant="bold-italic">z</mml:mtext><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mtext mathvariant="bold-italic">z</mml:mtext><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="0.2em"/><mml:msup><mml:mtext mathvariant="bold-italic">D</mml:mtext><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula> where <italic>t</italic> and <italic>k</italic> denote time bin and trial, respectively, <inline-formula><mml:math id="M49"><mml:mover accent="false"><mml:mrow><mml:msub><mml:mtext mathvariant="bold-italic">z</mml:mtext><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mspace width="0.2em"/><mml:mrow><mml:msub><mml:mtext mathvariant="bold-italic">z</mml:mtext><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <bold><italic>D</italic></bold> is a diagonal matrix of single-neuron variances, i.e. <inline-formula><mml:math id="M50"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>K</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula></p><p id="P72">In <xref ref-type="fig" rid="F2">Figure 2G</xref>, we plot the effective connection weights, <inline-formula><mml:math id="M51"><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold-italic">W</mml:mtext><mml:mrow><mml:mtext>eff</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> (computed based on the stimulus period as described by <xref ref-type="disp-formula" rid="FD21">Equation 21</xref>), for pairs of excitatory latent units (<italic>i</italic> &lt; <italic>j</italic>), as a function of the pairs’ noise correlation <bold>∑</bold><sup><italic>i, j</italic></sup> (more specifically, we binned pairs based on their noise correlation and created box-whisker plots of the effective connection weights for each bin). When comparing with the circuit at initialization (<xref ref-type="fig" rid="F2">Figure 2G</xref>, top), we considered <bold><italic>W</italic></bold><sub>eff</sub> and <italic><bold>z</bold><sub>t</sub></italic> to be the connectivity matrix and latent trajectories at the first iteration of training the model. We repeated the same procedure for go trials, with similar results (<xref ref-type="supplementary-material" rid="SD1">Figure S5</xref>).</p></sec><sec id="S36"><title>Calculating recurrent and external currents</title><p id="P73">For analyses described in <xref ref-type="fig" rid="F3">Figure 3A-B</xref>, we defined external and recurrent currents as <inline-formula><mml:math id="M52"><mml:mtext>ext</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="false"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtext mathvariant="bold-italic">Bu</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="M53"><mml:mtext>rec</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mstyle displaystyle="false"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtext mathvariant="bold-italic">W</mml:mtext><mml:mo>Φ</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext mathvariant="bold-italic">z</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:math></inline-formula> respectively.</p></sec><sec id="S37"><title>Sensitivity of the networks</title><p id="P74">In <xref ref-type="fig" rid="F3">Figure 3A, B, and J</xref>, we evaluated the sensitivity of the latent circuit to changes in the inputs vs. changes in the recurrent weights by running the network dynamics forward, using the inputs inferred from the data for every test trial, but including a gain <italic>γ</italic> that we used to either scale down the input matrix <bold><italic>B</italic></bold> (see <xref ref-type="disp-formula" rid="FD25">Equation 25</xref>), or the connectivity matrix <italic><bold>W</bold></italic> (see <xref ref-type="disp-formula" rid="FD27">Equation 27</xref>): <disp-formula id="FD25"><label>(25)</label><mml:math id="M54"><mml:mi>τ</mml:mi><mml:msup><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">z</mml:mtext><mml:mo>˙</mml:mo></mml:mover><mml:mrow><mml:mi>γ</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mtext mathvariant="bold-italic">z</mml:mtext><mml:mrow><mml:mi>γ</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mtext mathvariant="bold-italic">W</mml:mtext><mml:mo>Φ</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mtext mathvariant="bold-italic">z</mml:mtext><mml:mrow><mml:mi>γ</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mtext mathvariant="bold-italic">Bu</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula> <disp-formula id="FD26"><label>(26)</label><mml:math id="M55"><mml:msup><mml:mtext mathvariant="bold-italic">r</mml:mtext><mml:mrow><mml:mi>γ</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mtext>exp</mml:mtext><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext mathvariant="bold-italic">C</mml:mtext><mml:msup><mml:mtext mathvariant="bold-italic">z</mml:mtext><mml:mrow><mml:mi>γ</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mtext mathvariant="bold-italic">d</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula> vs. <disp-formula id="FD27"><label>(27)</label><mml:math id="M56"><mml:mi>τ</mml:mi><mml:msup><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">z</mml:mtext><mml:mo>˙</mml:mo></mml:mover><mml:mrow><mml:mi>γ</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mtext mathvariant="bold-italic">z</mml:mtext><mml:mrow><mml:mi>γ</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mtext mathvariant="bold-italic">W</mml:mtext><mml:mo>Φ</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mtext mathvariant="bold-italic">z</mml:mtext><mml:mrow><mml:mi>γ</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mtext mathvariant="bold-italic">Bu</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula> <disp-formula id="FD28"><label>(28)</label><mml:math id="M57"><mml:msup><mml:mtext mathvariant="bold-italic">r</mml:mtext><mml:mrow><mml:mi>γ</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mtext>exp</mml:mtext><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext mathvariant="bold-italic">C</mml:mtext><mml:msup><mml:mtext mathvariant="bold-italic">z</mml:mtext><mml:mrow><mml:mi>γ</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mtext mathvariant="bold-italic">d</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula></p><p id="P75">We computed the sensitivity by measuring changes in the total activity in no-go trials, either before or during stimulus onset, and normalizing those to the activity obtained for <italic>γ</italic> = 1, i.e. <inline-formula><mml:math id="M58"><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:msup><mml:mover accent="false"><mml:mtext mathvariant="bold-italic">r</mml:mtext><mml:mo>˜</mml:mo></mml:mover><mml:mi>γ</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo></mml:mrow></mml:mstyle><mml:mstyle displaystyle="false"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">r</mml:mtext><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="M59"><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">r</mml:mtext><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext mathvariant="bold-italic">r</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mtext mathvariant="bold-italic">r</mml:mtext><mml:mrow><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math></inline-formula> with <italic><bold>r</bold><sub>bs</sub></italic> the average baseline (pre-stimulus) activity.</p><p id="P76">We used the same approach to compute the sensitivity separately to either local or long-range weights, which was done by applying the gain to the corresponding local (<italic>W <sup>LM</sup></italic> and <italic>W</italic><sup><italic>V</italic>1</sup>) or long-range blocks (<italic>W</italic><sup><italic>LM→V</italic>1</sup> and <italic>W</italic><sup><italic>V</italic>1→<italic>LM</italic></sup>) of the <bold><italic>W</italic></bold> matrix (see <xref ref-type="disp-formula" rid="FD10">Equation 10</xref>).</p></sec><sec id="S38"><title>Intrinsic flow and velocity</title><p id="P77">In <xref ref-type="fig" rid="F3">Figure 3D</xref>, we plot the velocity field of the intrinsic dynamics (i.e dynamics in the absence of external inputs), projected onto the subspace spanned by the top two principal components (PCs) of the latent trajectories. Note that projections onto the PCs were only used for visualization purposes, and all analyses were performed using the full-dimensional dynamics. We first performed a singular value decomposition on the trial-averaged latent activity in no-go trials <italic>Z</italic> ∈ ℝ<sup><italic>n×T</italic></sup> as <italic>Z</italic> = <italic>U∑V<sup>T</sup></italic>, before defining <inline-formula><mml:math id="M60"><mml:mrow><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">U</mml:mtext><mml:mo>˜</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mtext mathvariant="bold-italic">U</mml:mtext><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext mathvariant="bold-italic">U</mml:mtext><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> as the top 2 PCs. We then computed the projected velocity field at each point in the 2D space, <italic><bold>x</bold></italic> = (<italic>x, y</italic>), as <italic><bold>v</bold></italic>(<italic><bold>x</bold></italic>) ∈ ℝ<sup>2</sup>, where: <disp-formula id="FD29"><label>(29)</label><mml:math id="M61"><mml:mrow><mml:mtext mathvariant="bold-italic">v</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="bold-italic">x</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">U</mml:mtext><mml:mo>˜</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msup><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">z</mml:mtext><mml:mo>˙</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">U</mml:mtext><mml:mo>˜</mml:mo></mml:mover><mml:mtext mathvariant="bold-italic">x</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>and the function <inline-formula><mml:math id="M62"><mml:mrow><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mo>⋅</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is given by: <disp-formula id="FD30"><label>(30)</label><mml:math id="M63"><mml:mrow><mml:mi>τ</mml:mi><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold">ξ</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mtext mathvariant="bold">ξ</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="bold">W</mml:mtext><mml:mi>Φ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold">ξ</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P78">To compute the velocity in <xref ref-type="fig" rid="F3">Figure 3E</xref>, we similarly used <xref ref-type="disp-formula" rid="FD30">Equation 30</xref>, but we used the no-go trial-averaged latent trajectories (without projecting onto PCs) for ξ. In <xref ref-type="fig" rid="F3">Figure 3H</xref>, we followed the same procedure, but using the <bold><italic>Z</italic></bold> and <bold><italic>W</italic></bold> restricted to each area. In this case, the 2 PCs were similarly extracted from the area-restricted latents.</p></sec><sec id="S39"><title>Network time constants and line attractor score</title><p id="P79">For analyses in <xref ref-type="fig" rid="F3">Figure 3F, G, I, and J</xref>, we linearized the dynamics around the average value of the latents across trials and time, either during or before the stimulus, and computed the eigenvalues and eigenmodes of the linearized dynamics <bold><italic>A</italic></bold> (see <xref ref-type="sec" rid="S30">Methods - Linearization of the dynamics</xref>). In this continuous-time linear dynamical system, each eigenmode <italic>j</italic> evolves in time according to <inline-formula><mml:math id="M64"><mml:mrow><mml:mi>e</mml:mi><mml:mfrac><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mi>τ</mml:mi></mml:mfrac><mml:mi>t</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> where <italic>τ</italic> is the single neuron time constant and <italic>λ<sub>j</sub></italic> the eigenvalue of mode <italic>j</italic>. The characteristic decay timescale of each mode is given by <inline-formula><mml:math id="M65"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mi>τ</mml:mi><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mtext>Re</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> Assuming the modes are ordered such that 0 &gt; Re(<italic>λ</italic><sub>0</sub>) ≥…≥ Re(<italic>λ<sub>n</sub></italic>), i.e <italic>τ</italic><sub>0</sub> ≥…≥ <italic>τ<sub>n</sub></italic>, <italic>τ</italic><sub>0</sub> defines the slowest timescale in the dynamics.</p><p id="P80">To calculate the time constants in the V1-only or LM-only networks in <xref ref-type="fig" rid="F3">Figure 3I</xref>, we followed the same procedure but used <bold><italic>W</italic></bold> and <bold><italic>Z</italic></bold> restricted to each individual area to compute the linearized dynamics. When comparing the time constants of these single-area networks to the full network, in order to control for their smaller size, we constructed subnetworks of the size of each individual area (i.e. of size <italic>n</italic>/2), sampled randomly from the full latent network (500 random subsets, and excluding any subselection that would correspond to the V1 or LM network).</p><p id="P81">To quantify the existence of a line attractor in the dynamics, we compute the “line attractor score”, defined as in <xref ref-type="bibr" rid="R15">Nair et al. [2023]</xref> as a log ratio of the slowest to the second-slowest time constant of the network dynamics, i.e. log(<italic>τ</italic><sub>0</sub>/<italic>τ</italic><sub>1</sub>)/ log2. A true line attractor would correspond to an infinite line attractor score. A score of 1 means that the slowest mode is twice as slow as the next mode. A score of 0 means that the first two slowest modes have the same time constant (as happens, e.g., when these two modes define a plane with rotational dynamics, i.e, the imaginary parts of their eigenvalues are non-zero).</p></sec><sec id="S40"><title>Minimal E-I networks</title><p id="P82">Our minimal E/I networks (<xref ref-type="fig" rid="F4">Figures 4</xref> and <xref ref-type="fig" rid="F5">5</xref>) are described as linear rate models, consisting of two areas, where each area’s connectivity is given by: <disp-formula id="FD31"><label>(31)</label><mml:math id="M66"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mtext>local</mml:mtext><mml:mo>=</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>e</mml:mi></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>e</mml:mi></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula> where <italic>e</italic> and <italic>i</italic> are the strength of excitatory and inhibitory connections, respectively. The activity in the full network evolves as: <disp-formula id="FD32"><label>(32)</label><mml:math id="M67"><mml:mrow><mml:mi>τ</mml:mi><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mi>W</mml:mi><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula> where <italic><bold>u</bold></italic>(<italic>t</italic>) is an external input which is zero unless otherwise specified, <disp-formula id="FD33"><label>(33)</label><mml:math id="M68"><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mtext>V</mml:mtext><mml:mn>1</mml:mn></mml:mrow><mml:mtext>E</mml:mtext></mml:msubsup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mtext>V</mml:mtext><mml:mn>1</mml:mn></mml:mrow><mml:mtext>I</mml:mtext></mml:msubsup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mtext>LM</mml:mtext></mml:mrow><mml:mtext>E</mml:mtext></mml:msubsup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mtext>LM</mml:mtext></mml:mrow><mml:mtext>I</mml:mtext></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="1.4em"/><mml:mtext>and</mml:mtext><mml:mspace width="0.8em"/><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>e</mml:mi></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mi>ℓ</mml:mi></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>e</mml:mi></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mi>ℓ</mml:mi></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>ℓ</mml:mi></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mi>e</mml:mi></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>ℓ</mml:mi></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mi>e</mml:mi></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula> and <italic>ℓ</italic> is the strength of long-range excitatory connections. This is the minimal network architecture depicted in <xref ref-type="fig" rid="F4">Figure 4B</xref>.</p><p id="P83">We can show that the orthonormal basis <italic>Q</italic> consisting of vectors <italic>Q</italic> = [<bold><italic>b<sub>a</sub>, u<sub>a</sub>, b<sub>d</sub>, u<sub>d</sub></italic></bold>] (‘<italic>b</italic>’ for ‘balanced’, ‘<italic>u</italic>’ for ‘unbalanced’; ‘<italic>a</italic>’ for ‘agree’, ‘<italic>d</italic>’ for ‘disagree’), where: <disp-formula id="FD34"><label>(34)</label><mml:math id="M69"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="1.4em"/><mml:msub><mml:mi>u</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="1.2em"/><mml:msub><mml:mi>b</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="1.4em"/><mml:msub><mml:mi>u</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula> is a Schur basis of <italic>W</italic> such that <inline-formula><mml:math id="M70"><mml:mrow><mml:mover accent="true"><mml:mi>W</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mi>Q</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>W</mml:mi><mml:mi>Q</mml:mi></mml:mrow></mml:math></inline-formula> is the upper triangular matrix <disp-formula id="FD35"><label>(35)</label><mml:math id="M71"><mml:mrow><mml:mover accent="true"><mml:mi>W</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>e</mml:mi><mml:mo>−</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>ℓ</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>ℓ</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mi>e</mml:mi><mml:mo>−</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mi>ℓ</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mi>ℓ</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula></p><p id="P84">This upper triangular form describes feedforward connectivity in the new basis <italic>Q</italic>, and reveals the existence of two separate functional subnetworks, respectively describing the dynamics of agreement and disagreement between V1 and LM. The dynamics of each functional subnetwork are characterized by balanced amplification due to the feedforward weight from the unbalanced to the balanced mode [<xref ref-type="bibr" rid="R17">Murphy and Miller, 2009</xref>]. See <xref ref-type="supplementary-material" rid="SD1">Supplementary Section S2.1</xref> for more details, including the interpretation of different elements of the Schur form.</p><p id="P85">We also considered a version of the above minimal model that also incorporated a notion of selectivity for go vs. no-go stimuli (<xref ref-type="fig" rid="F4">Figure 4C</xref>). Specifically, we split every E and I population in V1 and LM into two sub-populations, one receiving direct go input, and the other receiving direct no-go input. This resulted in a circuit with 8 units, with recurrent connectivity parameterized as: <disp-formula id="FD36"><label>(36)</label><mml:math id="M72"><mml:mrow><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mi>e</mml:mi></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>ℓ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mi>ℓ</mml:mi></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>e</mml:mi></mml:mtd><mml:mtd><mml:mrow><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mi>ℓ</mml:mi></mml:mtd><mml:mtd><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>ℓ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mi>e</mml:mi></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>ℓ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mi>ℓ</mml:mi></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>e</mml:mi></mml:mtd><mml:mtd><mml:mrow><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mi>ℓ</mml:mi></mml:mtd><mml:mtd><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>ℓ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>ℓ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mi>ℓ</mml:mi></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mi>e</mml:mi></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>ℓ</mml:mi></mml:mtd><mml:mtd><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>ℓ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mi>e</mml:mi></mml:mtd><mml:mtd><mml:mrow><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>ℓ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mi>ℓ</mml:mi></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mi>e</mml:mi></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>ℓ</mml:mi></mml:mtd><mml:mtd><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>ℓ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mi>e</mml:mi></mml:mtd><mml:mtd><mml:mrow><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula> where <italic>e<sub>s</sub></italic>, <italic>i<sub>s</sub></italic>, and <italic>l<sub>s</sub></italic> control the stimulus selectivity of local excitatory and inhibitory, and long-range excitatory connections, respectively. The Schur decomposition of this network, along with its interpretation in terms of time constants, can be found in <xref ref-type="supplementary-material" rid="SD1">Section S2</xref>.</p></sec><sec id="S41"><title>Autocorrelation of neural data</title><p id="P86">The autocorrelation of the agree (<italic>a</italic>) and disagree (<italic>d</italic>) neural activity patterns across V1 and LM are defined, respectively, as the sum and difference of the average empirical spike counts binned at 5 ms, <italic>s</italic>(<italic>t</italic>), within each area, i.e., <disp-formula id="FD37"><label>(37)</label><mml:math id="M73"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>V</mml:mtext><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mtext>V</mml:mtext><mml:mn>1</mml:mn></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>LM</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mtext>LM</mml:mtext></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula> <disp-formula id="FD38"><label>(38)</label><mml:math id="M74"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>V</mml:mtext><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mtext>V</mml:mtext><mml:mn>1</mml:mn></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>LM</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mtext>LM</mml:mtext></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mi>S</mml:mi><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula> <disp-formula id="FD39"><label>(39)</label><mml:math id="M75"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:msup><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:msub></mml:mrow></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula> <disp-formula id="FD40"><label>(40)</label><mml:math id="M76"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:msup><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:msub></mml:mrow></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula></p><p id="P87">We define the autocorrelation of the agree mode as the autocovariance normalized to the overall variance: <disp-formula id="FD41"><label>(41)</label><mml:math id="M77"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mi>k</mml:mi><mml:mi>a</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula> where 〈·〉<sub><italic>t</italic></sub> denotes an average over time bins <italic>t</italic> that are such that both <italic>t</italic> and <italic>t</italic> + <italic>τ</italic> fall within the relevant time window. This time window was [−400, 0] ms (‘pre’) or [100, 500] ms (‘during’) relative to stimulus onset. The autocorrelation of the disagree mode, <inline-formula><mml:math id="M78"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mi>k</mml:mi><mml:mi>d</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> is defined analogously. See <xref ref-type="supplementary-material" rid="SD1">Figure S7B</xref> for a distribution of marginal variances (denominator in <xref ref-type="disp-formula" rid="FD41">Equation 41</xref>) in the agree and disagree modes.</p><p id="P88">In <xref ref-type="fig" rid="F4">Figure 4H and I</xref>, we report the mean autocorrelation and its standard error across all correct control go and no-go trials and all animals. Note that mean subtraction in <xref ref-type="disp-formula" rid="FD39">Equation 39</xref> was done separately per animal/condition for go and no-go trials.</p><p id="P89">Our minimal models of V1-LM dynamics (<xref ref-type="fig" rid="F4">Figure 4B-C</xref>) also make predictions for the decay timescales of balanced vs. unbalanced modes. Estimating the autocorrelation time constant of these modes required estimating the E/I identity of each recorded neuron. For this, we used the identities inferred by the latent circuit models, and computed the momentary contributions of the balanced and unbalanced agree (<italic>a<sub>b</sub></italic>, <italic>a<sub>u</sub></italic>) or disagree (<italic>d<sub>b</sub></italic>, <italic>d<sub>u</sub></italic>) modes to the recorded activity as: <disp-formula id="FD42"><label>(42)</label><mml:math id="M79"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:msup><mml:mn>1</mml:mn><mml:mi>E</mml:mi></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi><mml:msup><mml:mn>1</mml:mn><mml:mi>E</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold-italic">s</mml:mtext><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:msup><mml:mn>1</mml:mn><mml:mi>I</mml:mi></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi><mml:msup><mml:mn>1</mml:mn><mml:mi>I</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold-italic">s</mml:mtext><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:msup><mml:mi>M</mml:mi><mml:mi>E</mml:mi></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>L</mml:mi><mml:msup><mml:mi>M</mml:mi><mml:mi>E</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold-italic">s</mml:mtext><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:msup><mml:mi>M</mml:mi><mml:mi>I</mml:mi></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>L</mml:mi><mml:msup><mml:mi>M</mml:mi><mml:mi>I</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold-italic">s</mml:mtext><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula> <disp-formula id="FD43"><label>(43)</label><mml:math id="M80"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:msup><mml:mn>1</mml:mn><mml:mi>E</mml:mi></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi><mml:msup><mml:mn>1</mml:mn><mml:mi>E</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold-italic">s</mml:mtext><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:msup><mml:mn>1</mml:mn><mml:mi>I</mml:mi></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi><mml:msup><mml:mn>1</mml:mn><mml:mi>I</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold-italic">s</mml:mtext><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:msup><mml:mi>M</mml:mi><mml:mi>E</mml:mi></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>L</mml:mi><mml:msup><mml:mi>M</mml:mi><mml:mi>E</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold-italic">s</mml:mtext><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:msup><mml:mi>M</mml:mi><mml:mi>I</mml:mi></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>L</mml:mi><mml:msup><mml:mi>M</mml:mi><mml:mi>I</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mtext mathvariant="bold-italic">s</mml:mtext><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula> <disp-formula id="FD44"><label>(44)</label><mml:math id="M81"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:msup><mml:mn>1</mml:mn><mml:mi>E</mml:mi></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi><mml:msup><mml:mn>1</mml:mn><mml:mi>E</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mtext mathvariant="bold-italic">s</mml:mtext><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:msup><mml:mn>1</mml:mn><mml:mi>I</mml:mi></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi><mml:msup><mml:mn>1</mml:mn><mml:mi>I</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mtext mathvariant="bold-italic">s</mml:mtext><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:msup><mml:mi>M</mml:mi><mml:mi>E</mml:mi></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mi>L</mml:mi><mml:msup><mml:mi>M</mml:mi><mml:mi>E</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mtext mathvariant="bold-italic">s</mml:mtext><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:msup><mml:mi>M</mml:mi><mml:mi>I</mml:mi></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mi>L</mml:mi><mml:msup><mml:mi>M</mml:mi><mml:mi>I</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mtext mathvariant="bold-italic">s</mml:mtext><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula> <disp-formula id="FD45"><label>(45)</label><mml:math id="M82"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:msup><mml:mn>1</mml:mn><mml:mi>E</mml:mi></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi><mml:msup><mml:mn>1</mml:mn><mml:mi>E</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mtext mathvariant="bold-italic">s</mml:mtext><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:msup><mml:mn>1</mml:mn><mml:mi>I</mml:mi></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi><mml:msup><mml:mn>1</mml:mn><mml:mi>I</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mtext mathvariant="bold-italic">s</mml:mtext><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:msup><mml:mi>M</mml:mi><mml:mi>E</mml:mi></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mi>L</mml:mi><mml:msup><mml:mi>M</mml:mi><mml:mi>E</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mtext mathvariant="bold-italic">s</mml:mtext><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:msup><mml:mi>M</mml:mi><mml:mi>I</mml:mi></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mi>L</mml:mi><mml:msup><mml:mi>M</mml:mi><mml:mi>I</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mtext mathvariant="bold-italic">s</mml:mtext><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p id="P90">Having defined these projections, we follow the same procedure as in <xref ref-type="disp-formula" rid="FD39">Equation 39</xref> - <xref ref-type="disp-formula" rid="FD41">Equation 41</xref> to calculate autocorrelations. Results are shown in <xref ref-type="supplementary-material" rid="SD1">Figure S7A</xref></p></sec><sec id="S42"><title>Quantifying autocorrelation differences</title><p id="P91">To quantify the difference between the autocorrelation functions of neural activity projected onto the agree vs. disagree modes (<xref ref-type="fig" rid="F4">Figure 4H and I</xref>), we first identified the time lag at which the average agree/disagree autocorrelation function reached its maximum (<italic>t</italic><sub>max</sub>). We then quantified the difference, denoted by Δ<sub>max</sub>, between the agree and disagree autocorrelation functions at this time point (see the caption of <xref ref-type="fig" rid="F4">Figure 4</xref>).</p></sec><sec id="S43"><title>Projected autocorrelations for selective/unselective modes</title><p id="P92">In <xref ref-type="fig" rid="F4">Figure 4I</xref>, to estimate the time course of the selective and unselective modes from the neural data, we computed two indices for each neuron. The first index measured ‘unselective’ responsiveness, i.e. how much more each neuron responded to either stimuli (go/no-go), relative to baseline. The second index measured ‘selective’ responsiveness, i.e. how much each neuron preferred the go stimulus over the no-go stimulus. This resulted in two vectors of indices: <disp-formula id="FD46"><label>(46)</label><mml:math id="M83"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mtext>unsel</mml:mtext></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>T</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mi>k</mml:mi></mml:munder><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mtext>go</mml:mtext><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mtext>no</mml:mtext><mml:mo>-</mml:mo><mml:mtext>go</mml:mtext></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>500</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mi>k</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>500</mml:mn></mml:mrow><mml:mn>0</mml:mn></mml:munderover><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mi>k</mml:mi><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula> <disp-formula id="FD47"><label>(47)</label><mml:math id="M84"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mrow><mml:mtext>sel</mml:mtext></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>T</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mi>k</mml:mi></mml:munder><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>40</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>40</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mo>−</mml:mo><mml:mi>g</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p id="P93">We then used these indices to compute weighted averages of the neural activity at each time step, <inline-formula><mml:math id="M85"><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mtext>unsel</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mtext>unsel</mml:mtext></mml:mrow><mml:mtext>T</mml:mtext></mml:msup></mml:mrow></mml:msup><mml:msub><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.2em"/><mml:mtext>and</mml:mtext><mml:mspace width="0.2em"/><mml:msubsup><mml:mi>a</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mtext>sel</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mtext>sel</mml:mtext></mml:mrow><mml:mtext>T</mml:mtext></mml:msup></mml:mrow></mml:msup><mml:msub><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula></p><p id="P94">The distributions of unselective and selective weights (<italic><bold>w</bold></italic><sup>unsel</sup> and <bold><italic>w</italic></bold><sup>sel</sup>) across V1 and LM neurons are shown in <xref ref-type="supplementary-material" rid="SD1">Figure S7D-E</xref>, and their relationship with one another is shown in <xref ref-type="supplementary-material" rid="SD1">Figure S7C</xref>. The elements of <italic><bold>w</bold></italic><sup>unsel</sup> were biased towards positive values (<xref ref-type="supplementary-material" rid="SD1">Figure S7D</xref>), as most recorded neurons responded to visual stimuli by increasing their firing rates. For <bold><italic>w</italic></bold><sup>sel</sup>, in contrast, the distribution was symmetric. This is because we measured responses early during stimulus presentation, i.e. likely before any go-stimulus-related behavior could break the symmetry in the neural responses to the two stimuli (<xref ref-type="supplementary-material" rid="SD1">Figure S7E</xref>). The choice of a small time window to compute <bold><italic>w</italic></bold><sup>sel</sup> also ensured that the selective index for a neuron was not corrupted by its unselective stimulus responsiveness, i.e that <bold><italic>w</italic></bold><sup>sel</sup> were not directly correlated with <bold><italic>w</italic></bold><sup>unsel</sup> (<xref ref-type="supplementary-material" rid="SD1">Figure S7C</xref>). This was important to establish that the slow time constant of the autocorrelation in agree-selective mode was not simply due to the correlation of this mode with the agree-unselective one, but instead depended on the stimulus selectivity of neurons.</p></sec><sec id="S44"><title>Alignment of the latent circuits’ eigenmodes onto agree unselective/selective modes</title><p id="P95">For our latent circuit models, we could ask whether their two agree modes (unselective and selective) bore any relationship with their two slowest eigenmodes. Eigenmodes were computed for the Jacobian of the latent circuit dynamics linearized around the average activity in no-go trials during stimulus presentation (see <xref ref-type="sec" rid="S30">Methods - Linearization of the dynamics</xref>), and were sorted from slowest to fastest according to their associated eigenvalues. Similarly, we could estimate the latent circuit’s unselective and selective agree modes by computing, for each latent unit, similar indices of unselective/selective responsiveness as we had computed for recorded neurons in <xref ref-type="fig" rid="F4">Figure 4I</xref> (c.f. <xref ref-type="disp-formula" rid="FD46">Equations 46</xref> and <xref ref-type="disp-formula" rid="FD47">47</xref>). For the selective indices, activity of the latent circuit during the onset period (0-100ms from the stimulus presentation) was used, as the latent circuit activity was strongly driven by external inputs during this time window (<xref ref-type="fig" rid="F3">Figure 3A</xref>). This yielded two normalized vectors, whose overlaps with the eigenvectors we evaluated, by calculating the absolute value of their dot product. These overlaps are shown in <xref ref-type="supplementary-material" rid="SD1">Figure S7F</xref>.</p></sec><sec id="S45"><title>Details of <xref ref-type="fig" rid="F5">Figure 5</xref></title><p id="P96">In <xref ref-type="fig" rid="F5">Figure 5</xref>, we simulated the linear dynamics of the minimal circuit model of <xref ref-type="fig" rid="F4">Figure 4C</xref>, i.e. <xref ref-type="disp-formula" rid="FD32">Equations 32</xref> and <xref ref-type="disp-formula" rid="FD36">36</xref> with parameters <italic>τ</italic> = 10 ms, <italic>e</italic> = 2, <italic>i</italic> = 2, <italic>e</italic><sub>s</sub> = 0, <italic>i</italic><sub>s</sub> = 0, <italic>ℓ</italic> = 0, and <italic>ℓ</italic><sub>s</sub> taking values in the set {0, 0.5, 0.9}. The input to the network was <italic><bold>u</bold></italic>(<italic>t</italic>) = <italic>α</italic>(<italic>t</italic>/<italic>τ</italic>′)<bold><italic>u</italic></bold><sub>0</sub> with <italic>τ</italic>′ = 15 ms, i.e. it was the product of a scalar temporal envelope (<xref ref-type="fig" rid="F5">Figure 5A</xref>, bottom) <inline-formula><mml:math id="M86"><mml:mrow><mml:mi>α</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext>stim</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (where <italic>H</italic>(·) denotes the Heaviside function) and a spatial input pattern <italic><bold>u</bold></italic><sub>0</sub> which expressed how much each subpopulation was driven by the ‘visual’ stimulus. For <xref ref-type="fig" rid="F5">Figure 5B-D</xref>, we set <italic><bold>u</bold></italic><sub>0</sub> = (1, 0, 1, 0, –0.6, 0, –0.6, 0)<sup><italic>T</italic></sup>, whereas for <xref ref-type="fig" rid="F5">Figure 5E-G</xref> we set <italic><bold>u</bold></italic><sub>0</sub> = (1, 0, 1, 0, 0, 0.6, 0, 0.6)<sup><italic>T</italic></sup> (c.f. gray insets).</p></sec><sec id="S46"><title>Statistics</title><p id="P97">We used two-sided Wilcoxon rank-sum tests for independent group comparisons, and two-sided Wilcoxon signed-rank tests for paired tests, unless otherwise stated.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary material</label><media xlink:href="EMS201451-supplement-Supplementary_material.pdf" mimetype="application" mime-subtype="pdf" id="d8aAcRbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S47"><title>Acknowledgments</title><p>This work was performed using resources provided by the Cambridge Service for Data Driven Discovery (CSD3) operated by the University of Cambridge Research Computing Service (<ext-link ext-link-type="uri" xlink:href="https://www.csd3.cam.ac.uk/">www.csd3.cam.ac.uk</ext-link>), provided by Dell EMC and Intel using Tier-2 funding from the Engineering and Physical Sciences Research Council (capital grant EP/T022159/1), and DiRAC funding from the Science and Technology Facilities Council (www.dirac.ac.uk). MS was funded by an Engineering and Physical Sciences Research Council (EPSRC DTP) studentship (RG94782). SBH and MJ were supported by the Sainsbury Wellcome Centre core grant from the Gatsby Charitable Foundation and the Wellcome Foundation (090843/F/09/Z), and a Wellcome Investigator Award (S.B.H., 219561/Z/19/Z). MJ was additionally supported by the Cold Spring Harbor Laboratory Fellows Program. YA was supported by UKRI Biotechnology and Biological Sciences Research Council research grant BB/X013235/1. We would like to thank Ari Benjamin, Kyle Daruwalla, YoungJu Jo, and Ivan Voitov for feedback on the manuscript.</p></ack><sec id="S48" sec-type="data-availability"><title>Data and code availability</title><p id="P98">Data and code are available from the corresponding authors upon request.</p></sec><fn-group><fn id="FN3" fn-type="con"><p id="P99"><bold>Author contribution</bold></p><p id="P100">MJ, MS, YA, GH conceived the study. MJ, MS, GH performed the analyses with feedback from YA and SBH. MJ, MS, GH, YA wrote the manuscript with feedback from SBH.</p></fn><fn id="FN4" fn-type="conflict"><p id="P101"><bold>Competing interests</bold></p><p id="P102">The authors declare no competing interests.</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Felleman</surname><given-names>Daniel J</given-names></name><name><surname>Van Essen</surname><given-names>David C</given-names></name></person-group><article-title>Distributed hierarchical processing in the primate cerebral cortex</article-title><source>Cerebral cortex (New York, NY: 1991)</source><year>1991</year><volume>1</volume><issue>1</issue><fpage>1</fpage><lpage>47</lpage><pub-id pub-id-type="pmid">1822724</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Douglas</surname><given-names>Rodney J</given-names></name><name><surname>Martin</surname><given-names>Kevan AC</given-names></name></person-group><article-title>Neuronal circuits of the neocortex</article-title><source>Annu Rev Neurosci</source><year>2004</year><volume>27</volume><issue>1</issue><fpage>419</fpage><lpage>451</lpage><pub-id pub-id-type="pmid">15217339</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markram</surname><given-names>Henry</given-names></name><name><surname>Toledo-Rodriguez</surname><given-names>Maria</given-names></name><name><surname>Wang</surname><given-names>Yun</given-names></name><name><surname>Gupta</surname><given-names>Anirudh</given-names></name><name><surname>Silberberg</surname><given-names>Gilad</given-names></name><name><surname>Wu</surname><given-names>Caizhi</given-names></name></person-group><article-title>Interneurons of the neocortical inhibitory system</article-title><source>Nature reviews neuroscience</source><year>2004</year><volume>5</volume><issue>10</issue><fpage>793</fpage><lpage>807</lpage><pub-id pub-id-type="pmid">15378039</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>Kenneth D</given-names></name><name><surname>Shepherd</surname><given-names>Gordon MG</given-names></name></person-group><article-title>The neocortical circuit: themes and variations</article-title><source>Nature neuroscience</source><year>2015</year><volume>18</volume><issue>2</issue><fpage>170</fpage><lpage>181</lpage><pub-id pub-id-type="pmcid">PMC4889215</pub-id><pub-id pub-id-type="pmid">25622573</pub-id><pub-id pub-id-type="doi">10.1038/nn.3917</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Javadzadeh</surname><given-names>Mitra</given-names></name><name><surname>Hofer</surname><given-names>Sonja B</given-names></name></person-group><article-title>Dynamic causal communication channels between neocortical areas</article-title><source>Neuron</source><year>2022</year><pub-id pub-id-type="pmcid">PMC9616801</pub-id><pub-id pub-id-type="pmid">35690063</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2022.05.011</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Schimel</surname><given-names>Marine</given-names></name><name><surname>Kao</surname><given-names>Ta-Chu</given-names></name><name><surname>Jensen</surname><given-names>Kristopher T</given-names></name><name><surname>Hennequin</surname><given-names>Guillaume</given-names></name></person-group><source>iLQR-VAE : control-based learning of input-driven dynamics with applications to neural data</source><conf-name>International Conference on Learning Representations</conf-name><year>2022</year><comment>URL <ext-link ext-link-type="uri" xlink:href="https://openreview.net/forum?id=wRODLDHaAiW">https://openreview.net/forum?id=wRODLDHaAiW</ext-link></comment></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rudy</surname><given-names>Bernardo</given-names></name><name><surname>Fishell</surname><given-names>Gordon</given-names></name><name><surname>Lee</surname><given-names>SooHyun</given-names></name><name><surname>Hjerling-Leffler</surname><given-names>Jens</given-names></name></person-group><article-title>Three groups of interneurons account for nearly 100% of neocortical gabaergic neurons</article-title><source>Developmental neurobiology</source><year>2011</year><volume>71</volume><issue>1</issue><fpage>45</fpage><lpage>61</lpage><pub-id pub-id-type="pmcid">PMC3556905</pub-id><pub-id pub-id-type="pmid">21154909</pub-id><pub-id pub-id-type="doi">10.1002/dneu.20853</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ozeki</surname><given-names>Hirofumi</given-names></name><name><surname>Finn</surname><given-names>Ian M</given-names></name><name><surname>Schaffer</surname><given-names>Evan S</given-names></name><name><surname>Miller</surname><given-names>Kenneth D</given-names></name><name><surname>Ferster</surname><given-names>David</given-names></name></person-group><article-title>Inhibitory stabilization of the cortical network underlies visual surround suppression</article-title><source>Neuron</source><year>2009</year><volume>62</volume><issue>4</issue><fpage>578</fpage><lpage>592</lpage><pub-id pub-id-type="pmcid">PMC2691725</pub-id><pub-id pub-id-type="pmid">19477158</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2009.03.028</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahmadian</surname><given-names>Yashar</given-names></name><name><surname>Miller</surname><given-names>Kenneth D</given-names></name></person-group><article-title>What is the dynamical regime of cerebral cortex?</article-title><source>Neuron</source><year>2021</year><volume>109</volume><issue>21</issue><fpage>3373</fpage><lpage>3391</lpage><pub-id pub-id-type="pmcid">PMC9129095</pub-id><pub-id pub-id-type="pmid">34464597</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2021.07.031</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanzeni</surname><given-names>Alessandro</given-names></name><name><surname>Akitake</surname><given-names>Bradley</given-names></name><name><surname>Goldbach</surname><given-names>Hannah C</given-names></name><name><surname>Leedy</surname><given-names>Caitlin E</given-names></name><name><surname>Brunel</surname><given-names>Nicolas</given-names></name><name><surname>Histed</surname><given-names>Mark H</given-names></name></person-group><article-title>Inhibition stabilization is a widespread property of cortical networks</article-title><source>Elife</source><year>2020</year><volume>9</volume><elocation-id>e54875</elocation-id><pub-id pub-id-type="pmcid">PMC7324160</pub-id><pub-id pub-id-type="pmid">32598278</pub-id><pub-id pub-id-type="doi">10.7554/eLife.54875</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ko</surname><given-names>Ho</given-names></name><name><surname>Hofer</surname><given-names>Sonja B</given-names></name><name><surname>Pichler</surname><given-names>Bruno</given-names></name><name><surname>Buchanan</surname><given-names>Katherine A</given-names></name><name><surname>Sjöström</surname><given-names>P Jesper</given-names></name><name><surname>Mrsic-Flogel</surname><given-names>Thomas D</given-names></name></person-group><article-title>Functional specificity of local synaptic connections in neocortical networks</article-title><source>Nature</source><year>2011</year><volume>473</volume><issue>7345</issue><fpage>87</fpage><lpage>91</lpage><pub-id pub-id-type="pmcid">PMC3089591</pub-id><pub-id pub-id-type="pmid">21478872</pub-id><pub-id pub-id-type="doi">10.1038/nature09880</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haider</surname><given-names>Bilal</given-names></name><name><surname>Häusser</surname><given-names>Michael</given-names></name><name><surname>Carandini</surname><given-names>Matteo</given-names></name></person-group><article-title>Inhibition dominates sensory responses in the awake cortex</article-title><source>Nature</source><year>2013</year><volume>493</volume><issue>7430</issue><fpage>97</fpage><lpage>100</lpage><pub-id pub-id-type="pmcid">PMC3537822</pub-id><pub-id pub-id-type="pmid">23172139</pub-id><pub-id pub-id-type="doi">10.1038/nature11665</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ganguli</surname><given-names>Surya</given-names></name><name><surname>Bisley</surname><given-names>James W</given-names></name><name><surname>Roitman</surname><given-names>Jamie D</given-names></name><name><surname>Shadlen</surname><given-names>Michael N</given-names></name><name><surname>Goldberg</surname><given-names>Michael E</given-names></name><name><surname>Miller</surname><given-names>Kenneth D</given-names></name></person-group><article-title>One-dimensional dynamics of attention and decision making in lip</article-title><source>Neuron</source><year>2008</year><volume>58</volume><issue>1</issue><fpage>15</fpage><lpage>25</lpage><pub-id pub-id-type="pmcid">PMC7204626</pub-id><pub-id pub-id-type="pmid">18400159</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2008.01.038</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mante</surname><given-names>Valerio</given-names></name><name><surname>Sussillo</surname><given-names>David</given-names></name><name><surname>Shenoy</surname><given-names>Krishna V</given-names></name><name><surname>Newsome</surname><given-names>William T</given-names></name></person-group><article-title>Context-dependent computation by recurrent dynamics in prefrontal cortex</article-title><source>Nature</source><year>2013</year><volume>503</volume><issue>7474</issue><fpage>78</fpage><lpage>84</lpage><pub-id pub-id-type="pmcid">PMC4121670</pub-id><pub-id pub-id-type="pmid">24201281</pub-id><pub-id pub-id-type="doi">10.1038/nature12742</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nair</surname><given-names>Aditya</given-names></name><name><surname>Karigo</surname><given-names>Tomomi</given-names></name><name><surname>Yang</surname><given-names>Bin</given-names></name><name><surname>Ganguli</surname><given-names>Surya</given-names></name><name><surname>Schnitzer</surname><given-names>Mark J</given-names></name><name><surname>Linderman</surname><given-names>Scott W</given-names></name><name><surname>Anderson</surname><given-names>David J</given-names></name><name><surname>Kennedy</surname><given-names>Ann</given-names></name></person-group><article-title>An approximate line attractor in the hypothalamus encodes an aggressive state</article-title><source>Cell</source><year>2023</year><volume>186</volume><issue>1</issue><fpage>178</fpage><lpage>193</lpage><pub-id pub-id-type="pmcid">PMC9990527</pub-id><pub-id pub-id-type="pmid">36608653</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2022.11.027</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sylwestrak</surname><given-names>Emily L</given-names></name><name><surname>Jo</surname><given-names>YoungJu</given-names></name><name><surname>Vesuna</surname><given-names>Sam</given-names></name><name><surname>Wang</surname><given-names>Xiao</given-names></name><name><surname>Holcomb</surname><given-names>Blake</given-names></name><name><surname>Tien</surname><given-names>Rebecca H</given-names></name><name><surname>Kim</surname><given-names>Doo Kyung</given-names></name><name><surname>Fenno</surname><given-names>Lief</given-names></name><name><surname>Ramakrishnan</surname><given-names>Charu</given-names></name><name><surname>Allen</surname><given-names>William E</given-names></name><etal/></person-group><article-title>Cell-type-specific population dynamics of diverse reward computations</article-title><source>Cell</source><year>2022</year><volume>185</volume><issue>19</issue><fpage>3568</fpage><lpage>3587</lpage><pub-id pub-id-type="pmcid">PMC10387374</pub-id><pub-id pub-id-type="pmid">36113428</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2022.08.019</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murphy</surname><given-names>Brendan K</given-names></name><name><surname>Miller</surname><given-names>Kenneth D</given-names></name></person-group><article-title>Balanced amplification: a new mechanism of selective amplification of neural activity patterns</article-title><source>Neuron</source><year>2009</year><volume>61</volume><issue>4</issue><fpage>635</fpage><lpage>648</lpage><pub-id pub-id-type="pmcid">PMC2667957</pub-id><pub-id pub-id-type="pmid">19249282</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2009.02.005</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>Zhuokun</given-names></name><name><surname>Fahey</surname><given-names>Paul G</given-names></name><name><surname>Papadopoulos</surname><given-names>Stelios</given-names></name><name><surname>Wang</surname><given-names>Eric Y</given-names></name><name><surname>Celii</surname><given-names>Brendan</given-names></name><name><surname>Papadopoulos</surname><given-names>Christos</given-names></name><name><surname>Kunin</surname><given-names>Alexander B</given-names></name><name><surname>Chang</surname><given-names>Andersen</given-names></name><name><surname>Fu</surname><given-names>Jiakun</given-names></name><name><surname>Ding</surname><given-names>Zhiwei</given-names></name><etal/></person-group><article-title>Functional connectomics reveals general wiring rule in mouse visual cortex</article-title><source>bioRxiv</source><year>2023</year></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pandarinath</surname><given-names>Chethan</given-names></name><name><surname>O’Shea</surname><given-names>Daniel J</given-names></name><name><surname>Collins</surname><given-names>Jasmine</given-names></name><name><surname>Jozefowicz</surname><given-names>Rafal</given-names></name><name><surname>Stavisky</surname><given-names>Sergey D</given-names></name><name><surname>Kao</surname><given-names>Jonathan C</given-names></name><name><surname>Trautmann</surname><given-names>Eric M</given-names></name><name><surname>Kaufman</surname><given-names>Matthew T</given-names></name><name><surname>Ryu</surname><given-names>Stephen I</given-names></name><name><surname>Hochberg</surname><given-names>Leigh R</given-names></name><name><surname>Henderson</surname><given-names>Jaimie M</given-names></name><etal/></person-group><article-title>Inferring single-trial neural population dynamics using sequential auto-encoders</article-title><source>Nature Methods</source><year>2018</year><month>September</month><volume>15</volume><issue>10</issue><fpage>805</fpage><lpage>815</lpage><pub-id pub-id-type="pmcid">PMC6380887</pub-id><pub-id pub-id-type="pmid">30224673</pub-id><pub-id pub-id-type="doi">10.1038/s41592-018-0109-9</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malonis</surname><given-names>Peter J</given-names></name><name><surname>Hatsopoulos</surname><given-names>Nicholas G</given-names></name><name><surname>MacLean</surname><given-names>Jason N</given-names></name><name><surname>Kaufman</surname><given-names>Matthew T</given-names></name></person-group><article-title>M1 dynamics share similar inputs for initiating and correcting movement</article-title><source>bioRxiv</source><year>2021</year><elocation-id>2021-10</elocation-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soldado-Magraner</surname><given-names>Joana</given-names></name><name><surname>Mante</surname><given-names>Valerio</given-names></name><name><surname>Sahani</surname><given-names>Maneesh</given-names></name></person-group><article-title>Inferring context-dependent computations through linear approximations of prefrontal cortex dynamics</article-title><source>bioRxiv</source><year>2023</year><pub-id pub-id-type="pmcid">PMC11654703</pub-id><pub-id pub-id-type="pmid">39693450</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.adl4743</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qian</surname><given-names>William</given-names></name><name><surname>Zavatone-Veth</surname><given-names>Jacob A</given-names></name><name><surname>Ruben</surname><given-names>Benjamin S</given-names></name><name><surname>Pehlevan</surname><given-names>Cengiz</given-names></name></person-group><article-title>Partial observation can induce mechanistic mismatches in data-constrained models of neural dynamics</article-title><source>bioRxiv</source><year>2024</year><elocation-id>2024-05</elocation-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Genkin</surname><given-names>Mikhail</given-names></name><name><surname>Engel</surname><given-names>Tatiana A</given-names></name></person-group><article-title>Moving beyond generalization to accurate interpretation of flexible models</article-title><source>Nature machine intelligence</source><year>2020</year><volume>2</volume><issue>11</issue><fpage>674</fpage><lpage>683</lpage><pub-id pub-id-type="pmcid">PMC9708065</pub-id><pub-id pub-id-type="pmid">36451696</pub-id><pub-id pub-id-type="doi">10.1038/s42256-020-00242-6</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubin</surname><given-names>Daniel B</given-names></name><name><surname>Van Hooser</surname><given-names>Stephen D</given-names></name><name><surname>Miller</surname><given-names>Kenneth D</given-names></name></person-group><article-title>The stabilized supralinear network: a unifying circuit motif underlying multi-input integration in sensory cortex</article-title><source>Neuron</source><year>2015</year><volume>85</volume><issue>2</issue><fpage>402</fpage><lpage>417</lpage><pub-id pub-id-type="pmcid">PMC4344127</pub-id><pub-id pub-id-type="pmid">25611511</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2014.12.026</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kraynyukova</surname><given-names>Nataliya</given-names></name><name><surname>Tchumatchenko</surname><given-names>Tatjana</given-names></name></person-group><article-title>Stabilized supralinear network can give rise to bistable, oscillatory, and persistent activity</article-title><source>Proceedings of the National Academy of Sciences</source><year>2018</year><volume>115</volume><issue>13</issue><fpage>3464</fpage><lpage>3469</lpage><pub-id pub-id-type="pmcid">PMC5879648</pub-id><pub-id pub-id-type="pmid">29531035</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1700080115</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siegle</surname><given-names>Joshua H</given-names></name><name><surname>Xiaoxuan</surname><given-names>Jia</given-names></name><name><surname>Durand</surname><given-names>Séverine</given-names></name><name><surname>Gale</surname><given-names>Sam</given-names></name><name><surname>Bennett</surname><given-names>Corbett</given-names></name><name><surname>Graddis</surname><given-names>Nile</given-names></name><name><surname>Heller</surname><given-names>Greggory</given-names></name><name><surname>Ramirez</surname><given-names>Tamina K</given-names></name><name><surname>Choi</surname><given-names>Hannah</given-names></name><name><surname>Luviano</surname><given-names>Jennifer A</given-names></name><etal/></person-group><article-title>Survey of spiking in the mouse visual system reveals functional hierarchy</article-title><source>Nature</source><year>2021</year><volume>592</volume><issue>7852</issue><fpage>86</fpage><lpage>92</lpage><pub-id pub-id-type="pmcid">PMC10399640</pub-id><pub-id pub-id-type="pmid">33473216</pub-id><pub-id pub-id-type="doi">10.1038/s41586-020-03171-x</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reinhold</surname><given-names>Kimberly</given-names></name><name><surname>Lien</surname><given-names>Anthony D</given-names></name><name><surname>Scanziani</surname><given-names>Massimo</given-names></name></person-group><article-title>Distinct recurrent versus afferent dynamics in cortical visual processing</article-title><source>Nature neuroscience</source><year>2015</year><volume>18</volume><issue>12</issue><fpage>1789</fpage><lpage>1797</lpage><pub-id pub-id-type="pmid">26502263</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sherman</surname><given-names>S Murray</given-names></name><name><surname>Guillery</surname><given-names>RW</given-names></name></person-group><article-title>Distinct functions for direct and transthalamic corticocortical connections</article-title><source>Journal of neurophysiology</source><year>2011</year><volume>106</volume><issue>3</issue><fpage>1068</fpage><lpage>1077</lpage><pub-id pub-id-type="pmid">21676936</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saalmann</surname><given-names>Yuri B</given-names></name><name><surname>Kastner</surname><given-names>Sabine</given-names></name></person-group><article-title>Cognitive and perceptual functions of the visual thalamus</article-title><source>Neuron</source><year>2011</year><volume>71</volume><issue>2</issue><fpage>209</fpage><lpage>223</lpage><pub-id pub-id-type="pmcid">PMC3148184</pub-id><pub-id pub-id-type="pmid">21791281</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2011.06.027</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Nuo</given-names></name><name><surname>Daie</surname><given-names>Kayvon</given-names></name><name><surname>Svoboda</surname><given-names>Karel</given-names></name><name><surname>Druckmann</surname><given-names>Shaul</given-names></name></person-group><article-title>Robust neuronal dynamics in premotor cortex during motor planning</article-title><source>Nature</source><year>2016</year><volume>532</volume><issue>7600</issue><fpage>459</fpage><lpage>464</lpage><pub-id pub-id-type="pmcid">PMC5081260</pub-id><pub-id pub-id-type="pmid">27074502</pub-id><pub-id pub-id-type="doi">10.1038/nature17643</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>Zengcai V</given-names></name><name><surname>Inagaki</surname><given-names>Hidehiko K</given-names></name><name><surname>Daie</surname><given-names>Kayvon</given-names></name><name><surname>Druckmann</surname><given-names>Shaul</given-names></name><name><surname>Gerfen</surname><given-names>Charles R</given-names></name><name><surname>Svoboda</surname><given-names>Karel</given-names></name></person-group><article-title>Mainte-nance of persistent activity in a frontal thalamocortical loop</article-title><source>Nature</source><year>2017</year><volume>545</volume><issue>7653</issue><fpage>181</fpage><lpage>186</lpage><pub-id pub-id-type="pmcid">PMC6431254</pub-id><pub-id pub-id-type="pmid">28467817</pub-id><pub-id pub-id-type="doi">10.1038/nature22324</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Halassa</surname><given-names>Michael M</given-names></name><name><surname>Sherman</surname><given-names>S Murray</given-names></name></person-group><article-title>Thalamocortical circuit motifs: a general framework</article-title><source>Neuron</source><year>2019</year><volume>103</volume><issue>5</issue><fpage>762</fpage><lpage>770</lpage><pub-id pub-id-type="pmcid">PMC6886702</pub-id><pub-id pub-id-type="pmid">31487527</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2019.06.005</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shepherd</surname><given-names>Gordon MG</given-names></name><name><surname>Yamawaki</surname><given-names>Naoki</given-names></name></person-group><article-title>Untangling the cortico-thalamo-cortical loop: cellular pieces of a knotty circuit puzzle</article-title><source>Nature Reviews Neuroscience</source><year>2021</year><volume>22</volume><issue>7</issue><fpage>389</fpage><lpage>406</lpage><pub-id pub-id-type="pmcid">PMC9006917</pub-id><pub-id pub-id-type="pmid">33958775</pub-id><pub-id pub-id-type="doi">10.1038/s41583-021-00459-3</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Furutachi</surname><given-names>Shohei</given-names></name><name><surname>Franklin</surname><given-names>Alexis D</given-names></name><name><surname>Aldea</surname><given-names>Andreea M</given-names></name><name><surname>Mrsic-Flogel</surname><given-names>Thomas D</given-names></name><name><surname>Hofer</surname><given-names>Sonja B</given-names></name></person-group><article-title>Cooperative thalamocortical circuit mechanism for sensory prediction errors</article-title><source>Nature</source><year>2024</year><volume>633</volume><issue>8029</issue><fpage>398</fpage><lpage>406</lpage><pub-id pub-id-type="pmcid">PMC11390482</pub-id><pub-id pub-id-type="pmid">39198646</pub-id><pub-id pub-id-type="doi">10.1038/s41586-024-07851-w</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mo</surname><given-names>Christina</given-names></name><name><surname>McKinnon</surname><given-names>Claire</given-names></name><name><surname>Sherman</surname><given-names>S Murray</given-names></name></person-group><article-title>A transthalamic pathway crucial for perception</article-title><source>Nature Communications</source><year>2024</year><volume>15</volume><issue>1</issue><elocation-id>6300</elocation-id><pub-id pub-id-type="pmcid">PMC11282105</pub-id><pub-id pub-id-type="pmid">39060240</pub-id><pub-id pub-id-type="doi">10.1038/s41467-024-50163-w</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knill</surname><given-names>David C</given-names></name><name><surname>Pouget</surname><given-names>Alexandre</given-names></name></person-group><article-title>The bayesian brain: the role of uncertainty in neural coding and computation</article-title><source>TRENDS in Neurosciences</source><year>2004</year><volume>27</volume><issue>12</issue><fpage>712</fpage><lpage>719</lpage><pub-id pub-id-type="pmid">15541511</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><collab>MICrONS Consortium</collab><name><surname>Bae</surname><given-names>J Alexander</given-names></name><name><surname>Baptiste</surname><given-names>Mahaly</given-names></name><name><surname>Bishop</surname><given-names>Caitlyn A</given-names></name><name><surname>Bodor</surname><given-names>Agnes L</given-names></name><name><surname>Brittain</surname><given-names>Derrick</given-names></name><name><surname>Buchanan</surname><given-names>JoAnn</given-names></name><name><surname>Bumbarger</surname><given-names>Daniel J</given-names></name><name><surname>Castro</surname><given-names>Manuel A</given-names></name><name><surname>Celii</surname><given-names>Brendan</given-names></name><etal/></person-group><article-title>Functional connectomics spanning multiple areas of mouse visual cortex</article-title><source>BioRxiv</source><year>2021</year><elocation-id>2021-07</elocation-id></element-citation></ref><ref id="R38"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dayan</surname><given-names>Peter</given-names></name><name><surname>Abbott</surname><given-names>Laurence F</given-names></name></person-group><source>Theoretical neuroscience: computational and mathematical modeling of neural systems</source><publisher-name>MIT press</publisher-name><year>2005</year></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jha</surname><given-names>Aditi</given-names></name><name><surname>Gupta</surname><given-names>Diksha</given-names></name><name><surname>Brody</surname><given-names>Carlos D</given-names></name><name><surname>Pillow</surname><given-names>Jonathan W</given-names></name></person-group><article-title>Disentangling the roles of distinct cell classes with cell-type dynamical systems</article-title><source>bioRxiv</source><year>2024</year><elocation-id>2024-07</elocation-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>Diederik P</given-names></name><name><surname>Welling</surname><given-names>Max</given-names></name></person-group><article-title>Auto-encoding variational Bayes</article-title><source>arXiv preprint</source><year>2013</year><elocation-id>arXiv: 2322.6224</elocation-id></element-citation></ref><ref id="R41"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Weiwei</given-names></name><name><surname>Todorov</surname><given-names>Emanuel</given-names></name></person-group><source>Iterative linear quadratic regulator design for nonlinear biological movement systems</source><conf-name>First International Conference on Informatics in Control, Automation and Robotics</conf-name><publisher-name>SciTePress</publisher-name><year>2004</year><volume>2</volume><fpage>222</fpage><lpage>229</lpage></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>Alex H</given-names></name><name><surname>Kunz</surname><given-names>Erin</given-names></name><name><surname>Kornblith</surname><given-names>Simon</given-names></name><name><surname>Linderman</surname><given-names>Scott</given-names></name></person-group><article-title>Generalized shape metrics on neural representations</article-title><source>Advances in Neural Information Processing Systems</source><year>2021</year><volume>34</volume><fpage>4738</fpage><lpage>4750</lpage><pub-id pub-id-type="pmcid">PMC10760997</pub-id><pub-id pub-id-type="pmid">38170102</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ostrow</surname><given-names>Mitchell</given-names></name><name><surname>Eisen</surname><given-names>Adam</given-names></name><name><surname>Kozachkov</surname><given-names>Leo</given-names></name><name><surname>Fiete</surname><given-names>Ila</given-names></name></person-group><article-title>Beyond geometry: Comparing the temporal structure of computation in neural circuits with dynamical similarity analysis</article-title><source>arXiv preprint</source><year>2023</year><elocation-id>arXiv: 2306.20268</elocation-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Modeling input-driven dynamics in the V1-LM network during visual processing.</title><p><bold>(A)</bold> Head-fixed, stationary mice were presented with two differently oriented stationary grating stimuli (45° or -45°), only one of which was rewarded. Mice reported the rewarded stimulus by licking a spout, which triggered the delivery of the reward (go/no-go). Paired neural recordings were performed in retinotopically matched regions of V1 and LM with silicon probes in PV-Cre mice. <bold>(B)</bold> Trial- and neuron-averaged spiking activity in no-go trials in V1 (left) and LM (right). A total of 194 neurons in V1 and 228 neurons in LM were recorded in 7 mice across a total of 513±110 (mean ± std) correct trials per mouse across two stimuli. <bold>(C)</bold> Top: In some trials, either V1 or LM was silenced through light-mediated activation of parvalbumin-expressing inhibitory cells expressing ChR2. The light onset was randomly chosen in each trial amongst 8 different times, spanning the duration of the stimulus uniformly (at 65 ms intervals), with a total of 449±98 (mean ± std) silencing trials per mouse. Bottom: Neuron- and trial-averaged spiking activity of the optogenetically stimulated PV+ neurons (top) and all other neurons (bottom) in an example animal, for one laser delay. <bold>(D)</bold> Biologically-constrained latent circuit model of V1-LM, with dynamics driven by 3 external inputs whose time course is inferred on a single trial basis. Dashed lines indicate the time-varying standard deviations of the (zero-mean) prior distributions over these inputs. Solid lines and shaded areas indicate posterior mean and standard deviation respectively, in one example trial, estimated from 100 posterior samples and smoothed with a running average of 25ms for visualization. <bold>(E)</bold> Left: Inferred time course of inputs for three example trials. Each column shows three input channels for one trial (optogenetic perturbation in blue, go stimulus in green, and no-go stimulus in red). The prior standard deviation (dashed line) indicates the presence of each input in the trial: no-go stimulus in the first trial, go stimulus paired with optogenetic perturbation in the second trial, and no-go stimulus paired with optogenetic perturbation in the third trial. Shaded area is the posterior standard deviation. Right: Trial-averaged time course of the three input channels, aligned to the input onset, shown as mean and standard deviation (shaded area) of the posterior mean across all trials. For each input channels, trial averages were calculated from trials where that input was present. All traces were smoothed with a running average of 25ms for visualization. <bold>(F)</bold> Example readout matrix (<italic><bold>C</bold></italic> in <xref ref-type="disp-formula" rid="FD2">Equation 2</xref>) in the fitted model, depicting the mapping from the latent units (columns, divided into two areas, and into excitatory (red) / inhibitory (blue) subpopulations within each area) to the recorded neurons (rows; blue bars mark PV cells identified by optogenetic perturbations in LM). <bold>(G)</bold> Top: Average recorded activity in V1 (left) and LM (right) during the no-go visual stimulus in an example animal. Bottom: Corresponding activity of the excitatory (red) and inhibitory (blue) latent units. Shaded areas around mean traces in B, C, and G denote 95% confidence intervals (±2 s.e.m.).</p></caption><graphic xlink:href="EMS201451-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Circuit-constrained models capture the statistics and underlying mechanisms of neural activity</title><p><bold>(A)</bold> Top: trial-averaged empirical firing rates, smoothed with a running average of 25 ms, for 4 example neurons in one animal. Different colors denote the two visual stimuli (red and green) and one silencing condition (cyan). Middle: corresponding model-predicted firing rates in individual trials (4 trials per condition), given the spikes observed concurrently in all other neurons. Bottom: corresponding spike rasters in the same three conditions, for the same four neurons. <bold>(B)</bold> Residual log-likelihood of the predicted firing rates of active cells (test trials, see <xref ref-type="sec" rid="S13">Methods</xref>) for the constrained (‘C’, black) and unconstrained (‘UC’, orange) models. <italic>n</italic> = 246 neurons across 7 animals, ‘C’ vs. ‘UC’ paired p-value = 6 x 10<sup>–7</sup>, unpaired p-value = 0.23. <bold>(C)</bold> Between-animal similarity in model dynamics, linearized around the stimulus-period activity. Similarity is computed as an average pairwise Procrustes distance (see <xref ref-type="sec" rid="S13">Methods</xref>), calculated separately for constrained (‘C’) and unconstrained (‘UC’) models (paired p-value = 6.7 x 10<sup>–6</sup>). <bold>(D)</bold> Left: trial-averaged activity of two example neurons, obtained either from the data (control no-go trials in black and one perturbation condition in blue), or by running the learned dynamics forward given condition-specific inferred inputs and artificial perturbations (see <xref ref-type="sec" rid="S13">Methods</xref>; control no-go trials in gray and one perturbation condition in cyan). The top cell is directly perturbed by the optogenetic perturbation, while the bottom cell is only indirectly affected. Center: change of neural activity (relative to control trials) predicted by the model in response to a simulated optogenetic perturbation, as a function of the experimentally observed response difference between laser and corresponding control trials. These are shown for all cells in V1 and LM that were not directly perturbed, and in one animal. <italic>r</italic> denotes the Pearson correlation coefficient between the predicted and true change (see <xref ref-type="sec" rid="S13">Methods</xref> and <xref ref-type="supplementary-material" rid="SD1">Figure S4</xref>). Right: distribution of Pearson correlation coefficients (c.f. middle) across animals, for models trained with perturbation data (black) and without perturbation data (gray) (Paired <italic>p</italic> = 0.0003). <bold>(E)</bold> Histograms of spike widths for the neurons labeled by the model as excitatory (top, red) and inhibitory (bottom, blue; known PV cells shown in cyan). Insets show the average spike waveforms (± 2 sem) for those neurons lying around the marked peaks. <bold>(F)</bold> Distribution across animals of the maximum real part of the eigenvalues of the latent circuit dynamics, linearized either before (gray) or during (black) stimulus presentation (pooled across both go and no-go trials). This is shown for the full model (left), and in the absence of inhibition (right; see <xref ref-type="sec" rid="S13">Methods</xref>). <bold>(G)</bold> Effective connectivity (see <xref ref-type="sec" rid="S13">Methods</xref>) plotted against noise correlations in control no-go trials, for pairs of latent circuit units. This is shown at model initialization (top gray), and after training (bottom black).</p></caption><graphic xlink:href="EMS201451-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Maintenance of activity via a slow mode emerging from interacting E/I networks</title><p><bold>(A)</bold> Left top: Schematic showing the external (teal) and recurrent (burnt orange) inputs in the V1-LM circuit. Left bottom : Average external and recurrent inputs, in no-go trials across all animals (shaded area denotes 2 sem). Middle top : Average network activity in no-go trials in an example animal, as we scale down the gain of the external inputs (see <xref ref-type="sec" rid="S13">Methods</xref>). We use gain values of 0.8, 0.9 and 1, ordered from light to dark. The grey and black bars denote the stimulus onset and during the stimulus. Middle bottom : Same as top, for recurrent inputs. Right : Sensitivity, i.e change in the response of the network (see <xref ref-type="sec" rid="S13">Methods</xref>) as we scale down (by <italic>γ</italic>) the external (burnt orange) or recurrent (teal) inputs. This is shown at stimulus onset (top; first 100ms of the stimulus) and during the stimulus presentation (bottom; 100-500 ms after stimulus onset). <bold>(B)</bold> Same as (A), but comparing local and long-range recurrent inputs (see <xref ref-type="sec" rid="S13">Methods</xref>). <bold>(C)</bold> Trial-averaged firing rate of an example neuron during no-go trials, smoothed at 25 ms (top) and the trial-averaged inferred external input to the circuit, averaged over all latent units (bottom). The colors indicate different time segments. <bold>(D)</bold> Flow field of the dynamics for the same animal as (C), projected in the subspace defined by the top 2 PCs of the latent activity (see <xref ref-type="sec" rid="S13">Methods</xref>). The color bar indicates the magnitude of the velocity. The trajectory represents the projection of the trial-averaged inferred latent activity in no-go trials, with time segments color-coded as in (C). <bold>(E)</bold> Velocity of the autonomous latent dynamics (i.e latent dynamics in the absence of external input), averaged either over the pre-stimulus period (-400-0 ms), around stimulus onset (0-100 ms), or during the stimulus (100-500 ms). <bold>(F)</bold> Relaxation time constants (mean ± 2 sem across all animals; see <xref ref-type="sec" rid="S13">Methods</xref>) of the linearized dynamics in the 100-500 ms time window of no-go trials. This is shown for constrained models in black and unconstrained models in orange. The inset shows the absolute value of the imaginary to real ratio of the eigenvalues corresponding to the slowest direction. <bold>(G)</bold> Distribution across animals of the line attractor score (see <xref ref-type="sec" rid="S13">Methods</xref>) of the dynamics, linearized around the mean activity in the 100-500 ms time window of no-go trials, for the constrained (black) and unconstrained (orange) models. <bold>(H)</bold> Flow field of the V1-only (top) or LM-only (bottom) dynamics, for the same example animal and trials as in (D), projected in the subspace defined by the top 2 PCs of the latent trajectories in each area (see <xref ref-type="sec" rid="S13">Methods</xref>). <bold>(I)</bold> Distribution across animals of the lowest relaxation time constant of the dynamics in the full networks (constrained models) and the V1-only or LM-only networks. The gray box corresponds to the distribution of slowest time constants in networks of the size of a single area, randomly sub-selected from the full networks. <bold>(J)</bold> Line attractor score of the V1-LM network, as we scale down the long-range (left) or within-area (right) connections by a factor <italic>γ</italic>.</p></caption><graphic xlink:href="EMS201451-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Network time constants in minimal models of interconnected E/I circuits</title><p><bold>(A)</bold> In a canonical E-I circuit (top), momentary activity can always be expressed as a linear combination of two modes (middle): an "unbalanced" mode (dashed outline) and a "balanced" mode (solid outline). Recurrent E-I connectivity is equivalent to feedforward connectivity from the unbalanced to the balanced mode (bottom; <xref ref-type="bibr" rid="R17">Murphy and Miller, 2009</xref>). The unbalanced mode exhibits fast dynamics, whereas the balanced mode can evolve more slowly depending on the degree of excitatory dominance (c.f. D). <bold>(B)</bold> Minimal E-I model of V1 and LM (top), where connectivity within each area is of the same form as in (A), and each area excites the other via long-range connections of strength <italic>ℓ.</italic> In this model, activity can be decomposed into four modes (middle): a pair of balanced &amp; unbalanced modes in which V1 and LM activities are anti-aligned (purple, ‘disagree’), and a similar pair in which they align (green, ‘agree’). These two pairs of modes are decoupled, and interactions within each pair are effectively feedforward (bottom). The dynamics are slowest along the mode of balanced agreement (c.f. E), resulting in an approximate line attractor. <bold>(C)</bold> This minimal model can be extended to accommodate selectivity to ±45° visual gratings, by splitting each E/I population into two differentially selective subpopulations. All connection types (local E, local I, long-range E) are composed of an unselective baseline and a selective (like-to-like) components (top). This connectivity structure gives rise to two versions (unselective, pale / selective, dark) of each of the four modes in B (middle), and results in slow dynamics in the two modes of balanced agreement (approximate ‘plane attractor’). <bold>(D-F)</bold> Time constants of the balanced mode(s) for each model (colors as in A-C), as a function of key connectivity parameters. In (F), only the two slow modes are shown. <bold>(G)</bold> Flow field of the dynamics of the model in (B) in the activity plane spanned by the two balanced modes, showing convergence onto the ‘agree’ mode. Each line is obtained by integrating the network’s dynamics starting from a different initial condition in that plane. <bold>(H)</bold> Autocorrelation function of the neural data pre- (left half) and during stimulus (right half) projected onto the ‘agree (resp. disagree) balanced’ modes (green resp. purple). These two modes correspond to the sum (resp. difference) of the average V1 and LM spiking activities. Traces are mean ± 95% confidence intervals. pre: Δ<sub>max</sub> = 0.06, <italic>p</italic> &lt; 10<sup>–5</sup>. during: Δ<sub>max</sub> = 0.13, <italic>p</italic> &lt; 10<sup>–5</sup>. <bold>(I)</bold> Same as (H), but for neural activity projected onto the pair of unselective agree/disagree modes (left) and analogous selective versions (right) defined in the main text. unselective, pre: Δ<sub>max</sub> = 0.035, <italic>p</italic> = 0.0006. unselective, during: Δ<sub>max</sub> = 0.11, <italic>p</italic> &lt; 10<sup>–5</sup>. selective, pre: Δ<sub>max</sub> = 0.022, <italic>p</italic> = 0.003. selective, during: Δ<sub>max</sub> = 0.054, <italic>p</italic> &lt; 10<sup>–5</sup>.</p></caption><graphic xlink:href="EMS201451-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Dynamic emergence of consensus via long-range connections in minimal models of interconnected E/I networks.</title><p><bold>(A)</bold> Schematics of the minimal selective model of the V1-LM circuit, driven by an external stimulus that can target either one of the two E/I pairs in each area depending on their orientation preference (<inline-graphic xlink:href="EMS201451-i001.jpg"/> vs. ⦸) with stylized time course shown at the bottom displaying both transient and sustained elements. We allow for a variable degree of input coherence across V1 and LM (‘input consensus’ dial). <bold>(B-D)</bold> Dynamics of V1-LM consensus for stimulus detection. <bold>(B)</bold> Network activity projected onto the ‘agree’ (green) and ‘disagree’ (purple) modes of balanced, unselective activity (green and purple insets; recall Figure 4B), in response to a <inline-graphic xlink:href="EMS201451-i001.jpg"/> stimulus that drives V1 while suppressing LM albeit less strongly (gray inset). Response projections are shown for three values of the specific long-range connection parameter <italic>ℓ</italic><sub>s</sub> (0, 0.5 and 0.9), with diamond marks indicating the point of maximum consensus and triangular marks indicating 200ms after that. <bold>(C)</bold> Same data as in (B), with the projection of momentary, trial-averaged network responses onto the ‘agree’ mode (green line in B) now plotted against its ‘disagree’ counterpart (purple line in B). Diamond and triangular marks as in (B). <bold>(D)</bold> Same data as in (B-C), now showing the projections onto <italic>local</italic> unselective modes (presence vs. absence of stimulus) in V1 and LM against each other. <bold>(E-G)</bold> Same as (B-D), for stimulus discrimination. In this case, V1 is strongly driven by a <inline-graphic xlink:href="EMS201451-i001.jpg"/> stimulus whilst LM is more weakly driven by a ⦸ stimulus (gray inset). The relevant agree/disagree modes are now the selective modes (green and purple insets), corresponding to consensus about the identity of the stimulus, rather than its presence/absence. As for detection, this conflicting stimulus gives rise to the correct consensus (<inline-graphic xlink:href="EMS201451-i001.jpg"/>) especially for large <italic>ℓ</italic><sub>s</sub>. This happens even though the input itself presents more disagreement than agreement (F, black).</p></caption><graphic xlink:href="EMS201451-f005"/></fig><table-wrap id="T1" position="float" orientation="portrait"><label>Table 1</label><caption><title>Model hyperparameters.</title></caption><table frame="box" rules="all"><thead><tr style="background-color:#F2E5D9"><th align="center" valign="middle">symbol</th><th align="center" valign="middle">value</th><th align="center" valign="middle">unit</th><th align="center" valign="middle">description</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-left: hidden"/><td align="center" valign="middle" style="border-right: hidden"/><td align="center" valign="middle" style="border-right: hidden"/><td align="center" valign="middle" style="border-right: hidden"/></tr><tr style="background-color:#DFDFDF"><td align="center" valign="middle"><italic>n</italic></td><td align="center" valign="middle">16</td><td align="center" valign="middle">-</td><td align="center" valign="middle">number of latent units</td></tr><tr><td align="center" valign="middle"><italic>m</italic></td><td align="center" valign="middle">3</td><td align="center" valign="middle">-</td><td align="center" valign="middle">number of latent inputs</td></tr><tr style="background-color:#DFDFDF"><td align="center" valign="middle">τ<italic><sub>E</sub></italic></td><td align="center" valign="middle">20</td><td align="center" valign="middle">ms</td><td align="center" valign="middle">excitatory latents time constant</td></tr><tr><td align="center" valign="middle">τ<italic><sub>I</sub></italic></td><td align="center" valign="middle">20</td><td align="center" valign="middle">ms</td><td align="center" valign="middle">inhibitory latents time constant</td></tr><tr style="background-color:#DFDFDF"><td align="center" valign="middle"><italic>η</italic></td><td align="center" valign="middle">0.004</td><td align="center" valign="middle">-</td><td align="center" valign="middle">learning rate</td></tr><tr><td align="center" valign="middle"><italic>k</italic></td><td align="center" valign="middle">10</td><td align="center" valign="middle">-</td><td align="center" valign="middle">scaling of the optimizer square root decay</td></tr><tr style="background-color:#DFDFDF"><td align="center" valign="middle"><italic>r</italic></td><td align="center" valign="middle">0.6</td><td align="center" valign="middle">-</td><td align="center" valign="middle">spectral radius of <italic><bold>W</bold></italic> at initialization</td></tr><tr><td align="center" valign="middle"><italic>λ</italic></td><td align="center" valign="middle">1000</td><td align="center" valign="middle">-</td><td align="center" valign="middle">scale of the regularization for <italic><bold>C</bold></italic></td></tr><tr style="background-color:#DFDFDF"><td align="center" valign="middle"><italic>F<sub>e</sub></italic></td><td align="center" valign="middle">0.5</td><td align="center" valign="middle">-</td><td align="center" valign="middle">fraction of excitatory neurons</td></tr></tbody></table></table-wrap></floats-group></article>