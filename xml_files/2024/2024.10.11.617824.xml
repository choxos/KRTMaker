<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS199397</article-id><article-id pub-id-type="doi">10.1101/2024.10.11.617824</article-id><article-id pub-id-type="archive">PPR923684</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">3</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Cortical alpha rhythms interpolate occluded motion from natural scene context</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Yeh</surname><given-names>Lu-Chun</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Bardelang</surname><given-names>Max</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Kaiser</surname><given-names>Daniel</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><aff id="A1"><label>1</label>Neural Computation Group, Department of Mathematics and Computer Science, Physics, Geography, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/033eqas34</institution-id><institution>Justus Liebig University Gießen</institution></institution-wrap>, <country country="DE">Germany</country></aff><aff id="A2"><label>2</label>Center for Mind, Brain and Behavior (CMBB), <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01rdrb571</institution-id><institution>Philipps University Marburg</institution></institution-wrap>, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/033eqas34</institution-id><institution>Justus Liebig University Gießen</institution></institution-wrap>, and <institution-wrap><institution-id institution-id-type="ror">https://ror.org/05n911h24</institution-id><institution>Technical University Darmstadt</institution></institution-wrap>, <country country="DE">Germany</country></aff></contrib-group><author-notes><corresp id="CR1">
<label>*</label>Correspondence: <email>luchun.yeh@gmail.com</email>
</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>14</day><month>10</month><year>2024</year></pub-date><pub-date pub-type="preprint"><day>12</day><month>10</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Tracking objects as they dynamically move in and out of sight is critical for parsing our ever-changing real-world surroundings. Here, we explored how the interpolation of occluded object motion in natural scenes is mediated by top-down information flows expressed in cortical alpha rhythms. We recorded EEG while participants viewed videos of a person walking across a scene. We then used multivariate decoding on alpha-band responses to decode the direction of movement across the scene. In trials where the person was temporarily occluded, alpha dynamics interpolated the person’s predicted movement. Critically, they did so in a context-dependent manner: When the scene context required the person to stop in front of an obstacle, alpha dynamics tracked the termination of motion during occlusion. As these effects were obtained with an orthogonal task at fixation, we conclude that alpha rhythms automatically interpolate occluded motion in a context-dependent way.</p></abstract><kwd-group><kwd>Object permanence</kwd><kwd>biological motion perception</kwd><kwd>alpha oscillations</kwd><kwd>multivariate pattern analysis</kwd><kwd>cortical feedback</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">In real life, objects keep moving in and out of occlusion (e.g., pedestrians are occluded by cars passing by). Though out of sight, occluded objects are not out of our minds: an object’s behavior during an occlusion event is routinely interpolated by human observers (<xref ref-type="bibr" rid="R1">1</xref>), allowing for predictions about its future behavior (<xref ref-type="bibr" rid="R2">2</xref>). This ability, referred to as object permanence, can already be observed in infants (<xref ref-type="bibr" rid="R3">3</xref>). Given how fundamental the ability to keep track of occluded objects is, relatively little is known about the underlying neural mechanisms.</p><p id="P3">Previous neuroimaging studies show that the motion trajectory of occluded objects is interpolated in early visual cortex (<xref ref-type="bibr" rid="R4">4</xref>,<xref ref-type="bibr" rid="R5">5</xref>) and sustained during the initial occlusion period (<xref ref-type="bibr" rid="R6">6</xref>,<xref ref-type="bibr" rid="R7">7</xref>). Although these studies suggest a sustained representation of motion during occlusion, they probed occlusion with highly artificial stimuli like basic shapes moving on a blank background.</p><p id="P4">The way information about occluded objects is represented is likely different in natural scenes, where context constrains object movement (e.g., people can only move in certain ways across a scene). Such context-specific interpolation effects in natural scenes should be mediated by top-down predictions in the visual system. Given that previous studies suggest a link between top-down connectivity and alpha rhythms (<xref ref-type="bibr" rid="R8">8</xref>–<xref ref-type="bibr" rid="R10">10</xref>), an intriguing hypothesis is that alpha rhythms are critically involved in the interpolation of occluded motion in natural scenes.</p><p id="P5">To test this hypothesis, we applied multivariate pattern analysis to time-frequency-resolved EEG data recorded while participants viewed videos of a person walking across a scene. Critically, in some trials, the person dynamically moved in and out of occlusion, allowing us to track the interpolation of motion information while the person was out of sight.</p></sec><sec id="S2" sec-type="results"><title>Results</title><p id="P6">Participants (n=48) watched 4-second videos of a person walking across a scene (either left-to-right or right-to-left) while performing an unrelated task at fixation (<xref ref-type="fig" rid="F1">Fig. 1A</xref>). The person could walk across the scene without occlusion (visible condition) or with an occluder present throughout the video, which occluded the walking person from 1.5 to 3 seconds (occluded condition). Additionally, the walking person was shown on a gray background (isolated condition). Performing multivariate decoding analysis on time- and frequency-resolved EEG response patterns, we could test how object motion during occlusion is represented in patterns of alpha power (8-12hz).</p><sec id="S3"><title>Alpha rhythms carry stimulus-specific information during occlusion</title><p id="P7">We trained linear classifiers on time-frequency response patterns to discriminate rightward- and leftward-walking in the isolated condition and tested them on the visible and occluded conditions (and vice versa; <xref ref-type="fig" rid="F1">Fig. 1B</xref>). This cross-decoding approach allowed us to isolate the representations of object motion while removing the contribution of the background.</p><p id="P8">We first performed this analysis in a “moving” condition, where the person walked across the scene in an uninterrupted way (<xref ref-type="fig" rid="F1">Fig. 1C</xref>). Classifiers trained on alpha power patterns successfully discriminated walking direction in the visible condition. Critically, when the person was occluded, decoding extended into the occlusion period (i.e., from 1.5 in the occluded condition, <xref ref-type="fig" rid="F1">Fig. 1C</xref>). This suggests that motion representations in alpha rhythms are dynamically sustained into an occlusion event. However, the effect observed in the occluded time window might simply reflect a carryover of earlier representations of motion when the object was still visible. We address this concern in the following analysis.</p></sec><sec id="S4"><title>Alpha dynamics reflect the inferred behavior of occluded objects</title><p id="P9">To test whether decoding during occlusion was indeed related to an interpolation of motion information, we introduced an additional “stopping” condition where the person stopped in front of a natural obstacle in the scene (e.g., a river; <xref ref-type="fig" rid="F1">Fig. 1D</xref>). If the brain utilized this contextual information, motion information should not be sustained during occlusion, because the person is required to stop right after disappearing behind the occluder. As expected, alpha dynamics tracked the termination of motion in this condition (<xref ref-type="fig" rid="F1">Fig. 1D</xref>). The stopping condition yielded significantly lower decoding during occlusion than the moving condition (<xref ref-type="fig" rid="F1">Fig. 1E</xref>). These results suggest that representations during occlusion are not driven by residual representations of the visible object. Instead, our brain actively interpolates the behavior of the included person given the current scene context.</p></sec><sec id="S5"><title>Only alpha rhythms interpolate occluded information</title><p id="P10">Critically, motion representations during occlusion were absent in theta, beta, and gamma rhythms, as well as in time-locked broadband responses: When comparing averaged decoding performance during the early occlusion time window (i.e., the first half of the occlusion period; <xref ref-type="fig" rid="F1">Fig. 1F</xref>), only alpha rhythms yielded a significant representation of occluded motion in the moving condition (<italic>t</italic>(47) = 4.06, FDR corrected <italic>p</italic> = .001), as well as a significant difference between the moving and stopping conditions (<italic>t</italic>(47) = 2.73, FDR corrected <italic>p</italic> = .027). For detailed results across all frequency bands, see SI appendix. Furthermore, no significant effects were found when decoding was performed on participants’ eye positions recorded during the occlusion time window (<xref ref-type="fig" rid="F1">Fig. 1F</xref>), suggesting that the interpolation effects in alpha rhythms were not related to differences in gaze during occlusion.</p></sec></sec><sec id="S6" sec-type="discussion"><title>Discussion</title><p id="P11">Our findings show that the visual system generates representations of occluded motion based on object behaviors inferred from the current scene context. This interpolated motion information is specifically represented in alpha rhythms, suggesting that alpha routes contextual motion predictions upstream (<xref ref-type="bibr" rid="R8">8</xref>–<xref ref-type="bibr" rid="R10">10</xref>).</p><p id="P12">Scene context effectively constrains the interpolation of object motion. In the stopping condition, context dictated that the walking person had to stop in front of an obstacle. In occlusion trials, the termination of motion happened after the person disappeared behind the occluder, so that the stopping had to be entirely inferred from context. Alpha dynamics tracked this inferred termination of motion, suggesting that top-down predictions accurately track the likely behavior of objects in the current environment – as opposed to a simple interpolation of a linear motion trajectory. This even held true in the absence of a task that requires interpolation.</p><p id="P13">Alpha rhythms, however, did not exhibit a difference between the moving stopping difference conditions without occlusion. One possible explanation is that in the visible condition, the person still faced the walking direction, and thus continued to share some visual features with the isolated condition. Alternatively, alpha may specifically interpolate motion based on context when feedforward signals are unavailable during occlusion, while a continued representation of motion is not needed when the whole scene is constantly present.</p><p id="P14">Consistent with previous MEG results obtained with simple visual stimuli (<xref ref-type="bibr" rid="R6">6</xref>), interpolated motion was not represented throughout the whole occlusion period. This may be due to interpolation becoming less accurate over time or due to representations (and consequently EEG scalp patterns) prominently changing as the person crosses the vertical midline and representations shift across hemispheres.</p><p id="P15">The involvement of alpha in interpolating occluded motion bears a resemblance to other key functions of visual alpha rhythms. First, alpha has been linked to the deployment of spatial attention across the visual field (<xref ref-type="bibr" rid="R11">11</xref>). The deployment of covert spatial attention may indeed play a prominent role in tracking objects during occlusion. However, the relatively broad temporal generalization pattern observed here may point towards representations of motion direction rather than reorientations of attended locations (which should result in narrow temporal generalization pattern). Second, alpha rhythms have been linked to mental imagery, where features of imagined visual contents (e.g., objects and scenes) can be retrieved from alpha activity (<xref ref-type="bibr" rid="R12">12</xref>,<xref ref-type="bibr" rid="R13">13</xref>). To which extent attention- and imagery-related mechanisms play a prominent role in the interpolation of occluded information needs to be explored in future work.</p><p id="P16">While we demonstrate that alpha rhythms carry information about the movement of occluded objects in scenes, some interesting questions remain open. First, we still need to better understand how this information is routed across visual cortex. Are alpha rhythms carrying detailed information about object features to early visual cortex, as suggested by fMRI work on occlusion (<xref ref-type="bibr" rid="R14">14</xref>,<xref ref-type="bibr" rid="R15">15</xref>)? Second, we only investigated the interpolation of object motion. Are other features of occluded objects (e.g., color, shape, or category) also represented in alpha rhythms?</p><p id="P17">Together, our study reveals alpha rhythms actively interpolate missing information in natural scenes in a context-specific way. This supports the assumption that interpolation relies on cortical top-down predictions carried by alpha rhythms.</p></sec><sec id="S7" sec-type="materials | methods"><title>Materials and Methods</title><sec id="S8" sec-type="subjects"><title>Participants</title><p id="P18">Fifty-two volunteers (32 females, mean age = 26.23, SD = 4.60 years) participated in the EEG experiment. All participants had normal or corrected-to-normal vision. The participants gave informed written consent and received 10 euros per hour for their time. The study was approved by the Ethics Committee of the Julius Liebig University Gießen and was in accordance with the 6th Declaration of Helsinki. Four participants were excluded due to the low behavioral accuracy or prominent horizontal eye movements during the task, as indicated by the eye-tracking data.</p></sec><sec id="S9"><title>Stimuli</title><p id="P19">Participants passively viewed 4-second videos (428 x 240 pixels, 11.6*6.6° visual angle, frame rate 30 fps), split into four conditions: (<xref ref-type="bibr" rid="R1">1</xref>) a person walking across a scene, (<xref ref-type="bibr" rid="R2">2</xref>) a person walking across a scene with an occluder present, (<xref ref-type="bibr" rid="R3">3</xref>) a person walking across a scene but stopping in front of a natural obstacle located near the center of the scene, and (<xref ref-type="bibr" rid="R4">4</xref>) a person walking across a scene but stopping in front of a natural obstacle located near the center of the scene with an occluder present. In separate blocks, we showed a person walking across a blank background. For all conditions, in half of the trials, the person walked from left to right, and in the other half of trials, they walked from right to left.</p><p id="P20">The videos were created by combining eight scenes from four categories (street, grassland, snowfield, and desert, each with or without an obstacle; taken from Google Images) with a walking-person animation. The same animation was used for all scenes. The videos were initially created with a leftward walking direction and were horizontally flipped to create versions with a rightward walking direction. This yielded a total of 16 unique videos. These videos were divided into four stimulus sets, which each featured 4 videos. In these 4 videos, the leftward and rightward motion and the presence of an obstacle were uniquely associated with a specific scene category (e.g., in the street scene, the person moved from left to right, with an obstacle present). Each participant only saw one of the stimulus sets. This was done so that the behavior of the walking person could be inferred from the scene context (e.g., on the street scene, participants could predict that the person had to stop due to the obstacle).</p><p id="P21">Critically, half of the scene videos were presented with a gray occluder, which occluded the central half of the scene (and occluded the walking person between 1.5 and 3 seconds after stimulus onset). The occluder remained on the video for the whole presentation duration.</p></sec><sec id="S10"><title>Paradigm</title><p id="P22">Participants comfortably sat in an illuminated room at a distance of 57 cm from the monitor. EEG and eye movement data were recorded while participants performed a fixation task unrelated to the videos, counting how many times the fixation turned purple. The fixation color started at white and changed every 500ms during the video, that is, seven times during each video. It changed to purple between 1 and 4 times each trial. After the videos, a question and two numbers were presented on the screen, and participants had to press the button “F” (mapping to the number on the left side) or “J” (mapping to the number on the right side) to choose their answers.</p><p id="P23">Prior to the experiment, participants performed a calibration and validation routine to set up the eye tracking. Participants were instructed to maintain central fixation in the subsequent experiment. Next, participants practiced for two blocks of eight trials (four trials for the visible condition, followed by four trials for the occluded condition). This practice allowed participants to learn the association between scene categories and the behavior of the walking (walking uninterruptedly vs. stopping in front of an obstacle).</p><p id="P24">During the experiment, participants first performed six blocks of 32 trials with the person walking across a scene. Each block featured 16 trials for the visible and 16 trials for the occluded condition, as well as 16 trials for the moving and 16 trials for the stopping condition. Trials within a block were presented in a random order. Afterward, participants practiced four trials and performed four blocks of 32 trials for the isolated condition.</p><p id="P25">Each trial started with a fixation cross presented at the center of the screen for 500 ms. Then, a four-second video with a fixation changing its color every 500 ms was presented, followed by a response screen with the question and two numbers on the right and left. Participants responded by pressing the F key to choose the number on the left or the J key to choose the number on the right. The intertrial intervals were randomly among 800, 1,000, and 1,200 ms. The full experiment lasted around 70 minutes.</p></sec><sec id="S11"><title>EEG data acquisition and preprocessing</title><p id="P26">Electrophysiological data were recorded using an Easycap system with 64 channels and a Brain Products amplifier with 1000 Hz sampling rate. The electrodes included seven sites in the central line (Fz, FCz, Cz, CPz, Pz, POz, and Oz) and 28 sites over the left and right hemispheres (FP1/FP2, AF3/AF4, AF7/AF8, F1/F2, F3/F4, F5/F6, F7/F8, FC1/FC2, FC3/FC4, FT5/FT6, FT7/FT8, FT9/FT10, C1/C2, C3/C4, C5/C6, T7/T8, CP1/CP2, CP3/CP4, CP5/CP6, TP7/TP8, P1/P2, P3/P4, P5/P6, P7/P8, PO3/PO4, PO7/PO8, PO9/PO10, and O1/O2). AFz served as the ground electrode, and Fz served as a reference electrode. FP1/2 and FT9/10 were re-mounted to vertical and horizontal eye movement recorders and removed from the subsequent EEG analysis. Impedance of all the electrodes was kept below 20 kΩ. Triggers were sent from the presentation computer to the EEG computer via a parallel port.</p><p id="P27">EEG data were preprocessed offline using the Fieldtrip toolbox (<xref ref-type="bibr" rid="R16">16</xref>) in MATLAB (MathWorks). First, noisy channels (1.63 ± 1.01 channels) were removed by visual inspection and repaired by the mean signals of the neighboring channels. Then, artifacts caused by eye movements and blinks were removed using an independent component analysis (ICA) and visual inspection of the resulting components from the data of each participant. After that, EEG data were re-referenced using the average of all electrodes (except EOG, FP1, FP2, FT9, and FT10) and epoched from -0.5 to 4.5 s relative to stimulus onset. Epochs were baseline-corrected from -100 to 0 ms.</p></sec><sec id="S12"><title>EEG time-frequency analysis</title><p id="P28">Time-frequency analysis was performed using the Fieldtrip toolbox. Continuous Morlet wavelet transformation with a 7-cycle length was used for time-frequency decomposition from 0 to 4 seconds in 50-ms steps. Power values in each frequency band (4 - 7 Hz for the theta band, 8 -12 Hz for the alpha band, 13 - 30 Hz for the beta band, and 31 - 70 for the gamma band) were averaged for the following decoding analysis.</p></sec><sec id="S13"><title>EEG decoding analysis</title><p id="P29">To track the neural representation during dynamic occlusion, we conducted a multivariate classification analysis on time-frequency-resolved EEG data using the Amsterdam Decoding and Modeling toolbox (ADAM; 17). Linear discriminant analysis (LDA) classifiers were employed to decode moving direction (right-to-left versus left-to-right). To increase the signal-to-noise-ratio, the time-frequency-resolved data was resampled to 250ms resolution with shape-preserving piecewise cubic interpolation. The classification analysis was carried out from video onset to 4 seconds after onset, with 250-ms resolution. The Area Under the Curve (AUC) served as the measure of decoding sensitivity, where a higher AUC indicates better discrimination between classes.</p><p id="P30">A cross-decoding approach was applied to isolate motion direction information from contributions of the scene background. To this end, we probed the generalization of classifiers from the person walking on a blank background to the person walking across a scene. Specifically, classifiers were trained on discriminating motion directions in videos with gray backgrounds and testing on videos with scenes, or vice versa. Results were averaged across both train/test directions.</p><p id="P31">To assess if the resulting decoding performance exceeded chance performance, we used a cluster-based nonparametric permutation test (<xref ref-type="bibr" rid="R18">18</xref>). The same test was used for comparing decoding performance across conditions. Averaged decoding performance (AUC – 50%) during the early occlusion time window (from 1.5 to 2.25 seconds, i.e., analysis windows centered on 1.75 and 2.0 s) was assessed using one-tailed t-tests against zero with FDR correction. Simple effect analyses were conducted to compare the moving and stopping conditions during occlusion.</p></sec><sec id="S14"><title>Eye Movement Analysis</title><p id="P32">To assess the fixation stability for our paradigm, the eye-tracking data were collected during the task, except for the first two participants. Based on visual inspection, we first excluded participants who showed prominent horizontal eye movements during the task from all analyses. We then calculated the mean and SD of the horizontal eye movement across trials for each timepoint (8ms resolution) and separately for each condition. The mean horizontal eye position was within ± 0.2° from the center of the screen for each condition during the entire video, indicating stable fixation along the horizontal axis. To exclude that the interpolation effects during occlusion found in EEG signals were related to differences in eye positions, we employed Linear discriminant analysis classifiers to decode movement direction (right-to-left or left-to-right) from the eye-tracking data (left and right eye’s horizontal and vertical positions) during the first half of the occlusion period. Akin to the EEG decoding analysis, we performed the decoding on eye data from two time windows of 500ms, centered on 1.75 and 2 seconds after onset, and averaged decoding performance across these two windows (see <xref ref-type="fig" rid="F1">Fig. 1F</xref>).</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Materials</label><media xlink:href="EMS199397-supplement-Supplementary_Materials.pdf" mimetype="application" mime-subtype="pdf" id="d8aAcEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S15"><title>Acknowledgments</title><p>L-C.Y. is supported by the Marie Skłodowska-Curie Actions (MSCA) programme (Grant agreement ID: 101149060). D.K. is supported by the DFG (SFB/TRR135, project number 222641018; KA4683/5-1, project number 518483074, KA4683/6-1, project number 536053998), “The Adaptive Mind”, funded by the Excellence Program of the Hessian Ministry of Higher Education, Science, Research and Art, and an ERC Starting Grant (PEP, ERC-2022-STG 101076057). Views and opinions expressed are those of the authors only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them.</p></ack><fn-group><fn id="FN1" fn-type="con"><p id="P33"><bold>Author Contributions:</bold> L-C.Y. and D.K. designed the research. L-C.Y. and M.B. performed the research and analyzed the data. L-C.Y. and D.K. wrote the paper.</p></fn><fn id="FN2" fn-type="conflict"><p id="P34"><bold>Competing Interest Statement:</bold> The authors declare no competing interest.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kellman</surname><given-names>PJ</given-names></name><name><surname>Shipley</surname><given-names>TF</given-names></name></person-group><article-title>A theory of visual interpolation in object perception</article-title><source>Cognitive Psychology</source><year>1991</year><volume>23</volume><fpage>141</fpage><lpage>221</lpage><pub-id pub-id-type="pmid">2055000</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rauschenberger</surname><given-names>R</given-names></name><name><surname>Yantis</surname><given-names>S</given-names></name></person-group><article-title>Masking unveils pre-amodal completion representation in visual search</article-title><source>Nature</source><year>2001</year><volume>410</volume><fpage>369</fpage><lpage>372</lpage><pub-id pub-id-type="pmid">11268213</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baillargeon</surname><given-names>R</given-names></name><name><surname>DeVos</surname><given-names>J</given-names></name></person-group><article-title>Object permanence in young infants: Further evidence</article-title><source>Child Development</source><year>1991</year><volume>62</volume><fpage>1227</fpage><lpage>1246</lpage><pub-id pub-id-type="pmid">1786712</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shuwairi</surname><given-names>SM</given-names></name><name><surname>Curtis</surname><given-names>CE</given-names></name><name><surname>Johnson</surname><given-names>SP</given-names></name></person-group><article-title>Neural substrates of dynamic object occlusion</article-title><source>Journal of Cognitive Neuroscience</source><year>2007</year><volume>19</volume><fpage>1275</fpage><lpage>1285</lpage><pub-id pub-id-type="pmcid">PMC3133772</pub-id><pub-id pub-id-type="pmid">17651002</pub-id><pub-id pub-id-type="doi">10.1162/jocn.2007.19.8.1275</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zbären</surname><given-names>GA</given-names></name><name><surname>Meissner</surname><given-names>SN</given-names></name><name><surname>Kapur</surname><given-names>M</given-names></name><name><surname>Wenderoth</surname><given-names>N</given-names></name></person-group><article-title>Physical inference of falling objects involves simulation of occluded trajectories in early visual areas</article-title><source>Human Brain Mapping</source><year>2023</year><volume>44</volume><fpage>4183</fpage><lpage>4196</lpage><pub-id pub-id-type="pmcid">PMC10258535</pub-id><pub-id pub-id-type="pmid">37195021</pub-id><pub-id pub-id-type="doi">10.1002/hbm.26338</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teichmann</surname><given-names>L</given-names></name><name><surname>Moerel</surname><given-names>D</given-names></name><name><surname>Rich</surname><given-names>AN</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><article-title>The nature of neural object representations during dynamic occlusion</article-title><source>Cortex</source><year>2022</year><volume>153</volume><fpage>66</fpage><lpage>86</lpage><pub-id pub-id-type="pmcid">PMC9247008</pub-id><pub-id pub-id-type="pmid">35597052</pub-id><pub-id pub-id-type="doi">10.1016/j.cortex.2022.04.009</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robinson</surname><given-names>AK</given-names></name><name><surname>Grootswagers</surname><given-names>T</given-names></name><name><surname>Shatek</surname><given-names>SM</given-names></name><name><surname>Gerboni</surname><given-names>J</given-names></name><name><surname>Holcombe</surname><given-names>A</given-names></name><name><surname>Carlson</surname><given-names>TA</given-names></name></person-group><article-title>Overlapping neural representations for the position of visible and imagined objects</article-title><source>arXiv preprint</source><year>2020</year><elocation-id>arXiv:2010.09932</elocation-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Kerkoerle</surname><given-names>T</given-names></name><name><surname>Self</surname><given-names>MW</given-names></name><name><surname>Dagnino</surname><given-names>B</given-names></name><name><surname>Gariel-Mathis</surname><given-names>MA</given-names></name><name><surname>Poort</surname><given-names>J</given-names></name><name><surname>Van Der Togt</surname><given-names>C</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><article-title>Alpha and gamma oscillations characterize feedback and feedforward processing in monkey visual cortex</article-title><source>Proceedings of the National Academy of Sciences</source><year>2014</year><volume>111</volume><fpage>14332</fpage><lpage>14341</lpage><pub-id pub-id-type="pmcid">PMC4210002</pub-id><pub-id pub-id-type="pmid">25205811</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1402773111</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Kaiser</surname><given-names>D</given-names></name></person-group><article-title>Alpha-frequency feedback to early visual cortex orchestrates coherent naturalistic vision</article-title><source>Science Advances</source><year>2023</year><volume>9</volume><elocation-id>eadi2321</elocation-id><pub-id pub-id-type="pmcid">PMC10637741</pub-id><pub-id pub-id-type="pmid">37948520</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.adi2321</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stecher</surname><given-names>R</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Kaiser</surname><given-names>D</given-names></name></person-group><article-title>Decoding the contents of visual brain rhythms</article-title><year>2024</year><pub-id pub-id-type="doi">10.31234/osf.io/nmf6k</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Samaha</surname><given-names>J</given-names></name><name><surname>Sprague</surname><given-names>TC</given-names></name><name><surname>Postle</surname><given-names>BR</given-names></name></person-group><article-title>Decoding and reconstructing the focus of spatial attention from the topography of alpha-band oscillations</article-title><source>Journal of Cognitive Neuroscience</source><year>2016</year><volume>28</volume><fpage>1090</fpage><lpage>1097</lpage><pub-id pub-id-type="pmcid">PMC5074376</pub-id><pub-id pub-id-type="pmid">27003790</pub-id><pub-id pub-id-type="doi">10.1162/jocn_a_00955</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>S</given-names></name><name><surname>Kaiser</surname><given-names>D</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><article-title>Visual imagery and perception share neural representations in the alpha frequency band</article-title><source>Current Biology</source><year>2020</year><volume>30</volume><fpage>2621</fpage><lpage>2627</lpage><pub-id pub-id-type="pmcid">PMC7416104</pub-id><pub-id pub-id-type="pmid">32750335</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2020.07.023</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stecher</surname><given-names>R</given-names></name><name><surname>Kaiser</surname><given-names>D</given-names></name></person-group><article-title>Representations of imaginary scenes and their properties in cortical alpha activity</article-title><source>Scientific Reports</source><year>2024</year><volume>14</volume><elocation-id>12796</elocation-id><pub-id pub-id-type="pmcid">PMC11150249</pub-id><pub-id pub-id-type="pmid">38834699</pub-id><pub-id pub-id-type="doi">10.1038/s41598-024-63320-4</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>FW</given-names></name><name><surname>Muckli</surname><given-names>L</given-names></name></person-group><article-title>Nonstimulated early visual areas carry information about surrounding context</article-title><source>Proceedings of the National Academy of Sciences</source><year>2010</year><volume>107</volume><fpage>20099</fpage><lpage>20103</lpage><pub-id pub-id-type="pmcid">PMC2993348</pub-id><pub-id pub-id-type="pmid">21041652</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1000233107</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muckli</surname><given-names>L</given-names></name><name><surname>De Martino</surname><given-names>F</given-names></name><name><surname>Vizioli</surname><given-names>L</given-names></name><name><surname>Petro</surname><given-names>LS</given-names></name><name><surname>Smith</surname><given-names>FW</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name><etal/><name><surname>Yacoub</surname><given-names>E</given-names></name></person-group><article-title>Contextual feedback to superficial layers of V1</article-title><source>Current Biology</source><year>2015</year><volume>25</volume><fpage>2690</fpage><lpage>2695</lpage><pub-id pub-id-type="pmcid">PMC4612466</pub-id><pub-id pub-id-type="pmid">26441356</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2015.08.057</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name></person-group><article-title>FieldTrip: open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Computational intelligence and neuroscience</source><year>2011</year><volume>2011</volume><elocation-id>156869</elocation-id><pub-id pub-id-type="pmcid">PMC3021840</pub-id><pub-id pub-id-type="pmid">21253357</pub-id><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fahrenfort</surname><given-names>JJ</given-names></name><name><surname>Van Driel</surname><given-names>J</given-names></name><name><surname>Van Gaal</surname><given-names>S</given-names></name><name><surname>Olivers</surname><given-names>CN</given-names></name></person-group><article-title>From ERPs to MVPA using the Amsterdam decoding and modeling toolbox (ADAM)</article-title><source>Frontiers in neuroscience</source><year>2018</year><volume>12</volume><fpage>368</fpage><pub-id pub-id-type="pmcid">PMC6038716</pub-id><pub-id pub-id-type="pmid">30018529</pub-id><pub-id pub-id-type="doi">10.3389/fnins.2018.00368</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><article-title>Nonparametric statistical testing of EEG-and MEG-data</article-title><source>Journal of neuroscience methods</source><year>2007</year><volume>164</volume><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="pmid">17517438</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><p><bold>(A)</bold> Participants viewed 4-second videos of a person walking left-to-right or right-to-left across a scene while performing an unrelated task at fixation. In half of these trials, the person was visible throughout, while in the other half, it was occluded between 1.5s and 3s. In separate blocks, the person walked across an isolated background, used as a scene-independent benchmark for classification analysis. <bold>(B)</bold> We trained and tested linear classifiers on alpha-power patterns to discriminate rightward- and leftward-walking. Classifiers were trained on the isolated condition and tested on the scene conditions, and vice versa. <bold>(C)</bold> In the moving condition, where the person walked uninterruptedly across the scene, alpha rhythms represented motion direction in both the occluded and non-occluded conditions. The purple dashed frames indicate the occluded period, dark red lines indicate significant decoding (<italic>p</italic> &lt; 0.05, corrected). <bold>(D)</bold> In the stopping condition, where the person stopped in front of a natural obstacle, alpha rhythms tracked the termination of movement although it happened behind the occluder. <bold>(E)</bold> In the occluded trials, the representation of motion was more sustained in the moving than in the stopping condition. Dark red lined indicate significant decoding differences (<italic>p</italic> &lt; 0.05, corrected). <bold>(F)</bold> Results of averaged decoding performance within the early occlusion time window revealed that only alpha activity tracked interpolated motion during occlusion. Asterisks indicate <italic>p</italic> &lt; 0.05. Error bars represent SEM.</p></caption><graphic xlink:href="EMS199397-f001"/></fig></floats-group></article>