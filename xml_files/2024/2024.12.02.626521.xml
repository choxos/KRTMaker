<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS201716</article-id><article-id pub-id-type="doi">10.1101/2024.12.02.626521</article-id><article-id pub-id-type="archive">PPR950006</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A universal of speech timing: Intonation units form low frequency rhythms and balance cross-linguistic syllable rate variability</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Inbar</surname><given-names>Maya</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Grossman</surname><given-names>Eitan</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Landau</surname><given-names>Ayelet N.</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref></contrib></contrib-group><aff id="A1"><label>1</label>Department of Linguistics, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/03qxff017</institution-id><institution>The Hebrew University of Jerusalem</institution></institution-wrap>, <addr-line>Mount Scopus</addr-line>, <postal-code>9190501</postal-code><city>Jerusalem</city>, <country country="IL">Israel</country></aff><aff id="A2"><label>2</label>Department of Psychology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/03qxff017</institution-id><institution>The Hebrew University of Jerusalem</institution></institution-wrap>, <addr-line>Mount Scopus</addr-line>, <postal-code>9190501</postal-code><city>Jerusalem</city>, <country country="IL">Israel</country></aff><aff id="A3"><label>3</label>Department of Cognitive and Brain Sciences, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/03qxff017</institution-id><institution>The Hebrew University of Jerusalem</institution></institution-wrap>, <addr-line>Mount Scopus</addr-line>, <postal-code>9190501</postal-code><city>Jerusalem</city>, <country country="IL">Israel</country></aff><aff id="A4"><label>4</label>Department of Experimental Psychology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap>, <city>London</city><postal-code>WC1H 0AP</postal-code>, <country country="GB">UK</country></aff><author-notes><corresp id="CR1"><label>*</label>Corresponding author: <email>maya.inbar@gmail.com</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>06</day><month>12</month><year>2024</year></pub-date><pub-date pub-type="preprint"><day>04</day><month>12</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Intonation units (IUs) are a universal building-block of human speech. They are found cross-linguistically and are tied to important language functions such as the pacing of information in discourse and swift turn-taking. We study the rate of IUs in 48 languages from every continent and from 27 distinct language families. Using a novel analytic method to annotate natural speech recordings, we identify a low-frequency rate of IUs across the sample, with a peak at 0.6 Hz, and little variation between sexes or across the life span. We find that IU rate is only weakly related to speech rate quantified at the syllable level (SR), and crucially, that cross-linguistic variation in IU rate is smaller than cross-linguistic variation in SR. Since SR was shown to be inversely related to information density quantified at the syllable level, we suggest that across languages, IUs are more balanced than syllables in their information content.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">What are the foundations of the language system and how are they encoded and realized by our biological machinery? Theorizing about language in a way that is sensitive to the vast diversity of the world’s languages is a challenging endeavor (<xref ref-type="bibr" rid="R1">1</xref>), one which raises the question: why have human beings, who share similar brain functions and neurocognitive abilities, developed thousands of distinct languages, each with its unique properties. Research from recent years has introduced exciting ways to tackle this question: first, although the over 7000 distinct languages of the world feature a great degree of diversity, there are robust universal tendencies of linguistic change (<xref ref-type="bibr" rid="R2">2</xref>–<xref ref-type="bibr" rid="R8">8</xref>). Second, a shifting focus to interactive processes reveals hitherto unknown shared mechanisms across languages (<xref ref-type="bibr" rid="R9">9</xref>–<xref ref-type="bibr" rid="R11">11</xref>). And finally, considering the unfolding of speech in time has allowed researchers to reconcile apparent diversities across languages. For example, languages differ greatly in their number of distinct syllables, resulting in substantial variation in the amount of information encoded per syllable. Recently, it has been shown that speakers adjust their syllable delivery rate rather consistently within a language, in inverse relation to the amount of information encoded per syllable (<xref ref-type="bibr" rid="R12">12</xref>). When syllables are less informative, more are packed into a unit of time and vice versa. In such a way an equilibrium of information transmission per unit time is reached across languages.</p><p id="P3">The current study is similarly interested in the structure of language in time, and aims to systematically characterize utterance-level prosodic regularities across a genealogically and geographically diverse sample of the world’s languages. Prosody is the domain of language that has to do with how the qualities of our voice, such as pitch and loudness, unfold over time. The qualities of our voice and how they are perceived by our partners in communication have to do with the physical properties of our speech apparatus and of the sound reaching the other’s perceptual apparatus. As such, they are at least partially independent of language-specific linguistic categories. Prosody plays a central role in communication, and – among its many functions (e.g., conveying stance and emotion, initiating action) – is used for segmenting continuous speech into sequences of prosodic phrases. Cross-linguistic research suggests that prosodic phrasing is fundamental to human language and constitutes a universal property of language (<xref ref-type="bibr" rid="R13">13</xref>–<xref ref-type="bibr" rid="R15">15</xref>). Here we substantiate this claim and characterize the acoustic realization of prosodic phrasing in spontaneous speech in a large and diverse sample of languages. In our work we embrace the close auditory work of generations of speech analysts and phoneticians, and carefully model it acoustically. This body of research has taught us to listen to changes in pitch, delivery rate, and loudness in order to identify units that are termed <italic>intonation units</italic> (henceforth IUs, 13, 16–18), <italic>intonation(al) phrases</italic> (<xref ref-type="bibr" rid="R19">19</xref>–<xref ref-type="bibr" rid="R22">22</xref>), <italic>tone groups</italic> (<xref ref-type="bibr" rid="R23">23</xref>), <italic>elementary discourse units</italic> (<xref ref-type="bibr" rid="R24">24</xref>). A characteristic IU is a “stretch of speech uttered under a single coherent pitch contour” (<xref ref-type="bibr" rid="R17">17</xref>), with an acceleration-deceleration dynamic of syllable delivery rate and an increase-decrease dynamic of loudness throughout the utterance (<xref ref-type="bibr" rid="R14">14</xref>, <xref ref-type="bibr" rid="R25">25</xref>). We touch upon some additional cues that allow us to perceive stretches of speech as separable in later sections.</p><p id="P4">Our focus on these prosodic phrases stems from their multifaceted importance in communication. For example, at a very basic level, IUs in a given language may become conventionalized and acquire meaning, and become language-specific phonological categories. Yet prosodic phrasing assumes many other roles: it is hypothesized to take on the role of pacing new information in the discourse (<xref ref-type="bibr" rid="R26">26</xref>, <xref ref-type="bibr" rid="R27">27</xref>), it allows us to predict in real time when it is our turn to talk (<xref ref-type="bibr" rid="R28">28</xref>–<xref ref-type="bibr" rid="R31">31</xref>) and it plays crucial roles in language development (<xref ref-type="bibr" rid="R32">32</xref>–<xref ref-type="bibr" rid="R36">36</xref>). Testing whether IUs assume these roles universally is a fascinating yet complex research program. To carry it out, an important bottleneck needs to be lifted: the reliance on manual annotations of IUs. Therefore, before studying the structure of IUs in time, we begin by devising an automatic annotation option. We then successfully validate its performance compared to traditional manual annotations across languages and use it on a larger corpus of speech recordings.</p><p id="P5">We study 668 speech recordings of multiple speakers in 48 languages, a sample that represents languages from every continent and from 27 distinct phylogenetic units (i.e., language families). This is one of the largest and most diverse samples to date that tackles questions relating to prosody, and specifically prosodic phrasing. We algorithmically characterize the strength of acoustic boundaries at each and every word in this dataset. This characterization reveals strong correspondences between languages. We show that boundary strength is a dimension clearly dividing the data into two classes, with slight differences between languages. Taking a step further, we show that the resulting classes are directly related to the auditory segmentation of speech into IUs by expert transcribers, for the first time in three languages in addition to English. This validation, we believe, will facilitate and promote studying speech in a manner that is sensitive to its prosodic structure and unfolding over time. And indeed, we rely on our validation work and identify the “acoustic” prosodic phrases across the dataset, which we take as a proxy for IUs.</p><p id="P6">Finally, recently we and others have shown that IUs succeed each other at a tempo of approximately 1 Hz in several mostly unrelated languages (<xref ref-type="bibr" rid="R37">37</xref>, <xref ref-type="bibr" rid="R38">38</xref>). This shared time scale is related to speech production mechanisms, including motor control and biophysical constraints of the speech articulation system (<xref ref-type="bibr" rid="R39">39</xref>). In addition, recent neuroscientific findings suggest that low-frequency neural activity contributes to speech production (<xref ref-type="bibr" rid="R40">40</xref>) and to the emergence of free thoughts and spontaneous actions (<xref ref-type="bibr" rid="R41">41</xref>). In the realm of speech perception and the neural processing involved, low-frequency neural activity is also known to play crucial roles (<xref ref-type="bibr" rid="R42">42</xref>–<xref ref-type="bibr" rid="R46">46</xref>) and recently we directly related some of this activity to IUs (<xref ref-type="bibr" rid="R47">47</xref>). The current study extends the finding that IUs form low-frequency rhythms in spontaneous speech to ever more languages and cultures. Moreover, for the first time, we probe the relationship between syllable rate and IU rate. We show that the two rates are only loosely connected and that languages are more similar to one another in their IU rate compared to syllable rate. This set of results speaks to the question stated at the outset: speakers pace their utterances similarly in time in a wide variety of languages, indeed perhaps universally. The low-frequency temporal structure is a foundation of human language, and it is one that is relatable to the biological mechanisms of our brain and body.</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>Automatically derived IUs show commonalities with manual IU annotations cross-linguistically</title><p id="P7">We adopt and adapt a method for the hierarchical representation and estimation of prosody, which was previously shown to detect prosodic breaks in English news stories read by 6 speakers (<xref ref-type="bibr" rid="R48">48</xref>). In that study, the method reached 85.5% accuracy and 0.73 F1-score relative to expert-made manual annotations of prosodic phrase breaks. Here we applied the method to spontaneous English speech recorded in an informal interview setup (<xref ref-type="bibr" rid="R49">49</xref>), and to spontaneous speech in three additional languages: Russian (<xref ref-type="bibr" rid="R50">50</xref>), Hebrew (<xref ref-type="bibr" rid="R51">51</xref>) and Totoli (<xref ref-type="bibr" rid="R52">52</xref>). <xref ref-type="table" rid="T1">Table 1</xref> summarizes the method’s performance in prosodic boundary detection in each of these languages. Relative to expert-made manual annotations, the method reached an average of 82.26% accuracy and 0.72 F1-score across the different languages. The average scores indicate moderate classification results in each language to the least, and the average Cohen’s <italic>k</italic> across languages (0.60) is comparable to the agreement between multiple pairs of student annotators in a study surveying the ability to auditorily detect prosodic phrases cross-linguistically (0.58; 14). In addition, in all 34 recordings but one (in English), accuracy was significantly higher than the accuracy expected by simply predicting the most common class in the data. Focusing specifically on English, in which we can compare the method’s performance on spontaneous speech to read speech, the method reached an average of 84.24% accuracy and 0.74 F1-score across the different recordings. Thus, we can conclude that the automatic IU identification pipeline is suitable for cross-linguistic application, and to a spontaneous speaking style.</p><p id="P8">We explore similarities and differences between the manually annotated IUs and the automatically derived IUs (<xref ref-type="fig" rid="F1">Fig. 1</xref>). In Hebrew and in Russian, the automatically derived IUs are longer in the number of words per IU (<xref ref-type="fig" rid="F1">Fig. 1A</xref>). The automatically derived IUs are longer in duration in Hebrew, Russian and Totoli (<xref ref-type="fig" rid="F1">Fig. 1B</xref>). Only in Hebrew were intervening pauses longer between the automatically derived IUs compared to the manually annotated IUs (<xref ref-type="fig" rid="F1">Fig. 1C</xref>). An inspection of several acoustic properties of the automatically derived IUs reveals a large degree of similarity compared to the manually annotated IUs (see <xref ref-type="sec" rid="S7">Materials and methods</xref>: <xref ref-type="sec" rid="S16">Acoustic analyses</xref>). The grand average envelope time courses (<xref ref-type="fig" rid="F1">Fig. 1D</xref>), pitch tracks (<xref ref-type="fig" rid="F1">Fig. 1E</xref>) and harmonic ratio time courses (<xref ref-type="fig" rid="F1">Fig. 1F</xref>) do not significantly differ between the methods.</p><p id="P9">With these results in mind, we turn to automatically derive IUs in a large collection of spontaneous speech recordings (<xref ref-type="bibr" rid="R53">53</xref>) in 44 additional languages, which did not include IU annotations to date. Inspection of the resulting time- and scale- normalized envelope, f0 and harmonic ratio time courses (<xref ref-type="fig" rid="F2">Fig. 2A-C</xref>) reveals probable correlates of the pattern that lies at the heart of prosodic segmentation, namely, a reset and declination pattern in intensity and pitch throughout the IU, with no significant deviances across languages (see <xref ref-type="sec" rid="S7">Materials and methods</xref>: <xref ref-type="sec" rid="S23">Statistical analyses</xref>: <xref ref-type="sec" rid="S28">Deviances across languages in acoustics and IU rate</xref>; see <xref ref-type="supplementary-material" rid="SD1">Supplementary materials: Text S2 and Fig. S1</xref> for analyses at the individual language level). We do not know of a direct demonstration of this pattern in the envelope and harmonic ratio measures, but at least two studies report this pattern in the f0 time course. Specifically, Biron and colleagues (<xref ref-type="bibr" rid="R54">54</xref>) show this pitch declination in manually and automatically identified IUs, and directly compare the average f0 values in the time windows 0.15-0.25 and 0.85-0.95. Recently, Ozaki and colleagues (<xref ref-type="bibr" rid="R55">55</xref>) similarly demonstrated a notable descending f0 trend in utterances in spoken descriptions across 55 languages. Cohen’s <italic>d</italic> statistics for the difference between the early and late time windows as defined by Biron and colleagues (<xref ref-type="bibr" rid="R54">54</xref>) indicate small to medium yet statistically significant average effect sizes across the sample of languages, in each of the measures (envelope: Cohen’s <italic>d</italic>=0.29±0.21 (M±SD), CI [0.23,0.35], p&lt;0.001; pitch: <italic>d</italic>=0.4±0.24 (M±SD), CI [0.33,0.47], p&lt;0.001; harmonic ratio: <italic>d</italic>=0.28±0.16 (M±SD), CI [0.24,0.33], p&lt;0.001). The reset and declination pattern in intensity and pitch is typical of manually annotated IUs (<xref ref-type="bibr" rid="R13">13</xref>, <xref ref-type="bibr" rid="R14">14</xref>, <xref ref-type="bibr" rid="R21">21</xref>, <xref ref-type="bibr" rid="R22">22</xref>), and therefore we interpret this result as further validation that the automatically derived IUs can be considered a proxy for IUs, cross-linguistically.</p></sec><sec id="S4"><title>Sequences of IU form low-frequency rhythms cross-linguistically</title><p id="P10">In a previous study, looking at manually annotated IUs in six languages from around the world, we found that sequences of IUs in spontaneous speech succeed each other at a tempo of approximately 1 unit per second (<xref ref-type="bibr" rid="R37">37</xref>). We set out to establish the extent of this effect in spontaneous speech in a broader sample of languages. First, we relied on our original analysis pipeline, which draws on spike-field synchronization studies (<xref ref-type="bibr" rid="R56">56</xref>). In our analysis we assume that the speech envelope includes slow periodic modulations. We use these periodic components to estimate the phases of IU onsets, and subsequently examine whether the consistency among phases is higher than expected under the null hypothesis that the slow periodic modulations in the envelope capture word onsets in general, not IU onsets specifically. We find, in all 48 languages, that IU onsets appear at significantly consistent phases of the low-frequency components of the speech envelope, indicating that their rhythms are captured in the speech envelope (<xref ref-type="fig" rid="F3">Fig. 3</xref>). The frequency range of maximal phase consistency in the current sample of languages is 0.6-1 Hz. We next turn to assess whether the spectrum of phase consistency in any language deviates reliably from the grand average across this sample of 48 languages. To this end, we construct a 95% confidence interval around the grand average across languages using a bootstrap procedure, and find that it includes the estimated phase consistency spectra of all languages (<xref ref-type="fig" rid="F2">Fig. 2D</xref>; see <xref ref-type="sec" rid="S7">Materials and methods</xref>: <xref ref-type="sec" rid="S23">Statistical analyses</xref>: <xref ref-type="sec" rid="S28">Deviances across languages in acoustics and IU rate</xref>). Therefore, no language can be said to significantly deviate from the others in terms of the alignment of IUs with the periodic components of the speech envelope.</p><p id="P11">We next sought to determine whether the rhythm of IUs characterized above is rooted in the sequence of IUs or rather stems from the durations of IUs themselves. To this end, we calculated per language the Coefficient of Variation (CoV) of the distribution of IU durations (M±SD = 0.53 ±0.03) and the CoV of the distribution of IU inter-onset intervals (IU-IOI values; M±SD = 0.5 ±0.05; see <xref ref-type="sec" rid="S7">Materials and methods</xref>: <xref ref-type="sec" rid="S23">Statistical analyses</xref>: <xref ref-type="sec" rid="S21">IU rate</xref>). We tested the paired difference of these measures against zero. On average, IU durations are 4% CI [2%-5%] more dispersed around their mean compared to IU-IOI values. Cohen’s <italic>d</italic> statistic for the difference between two paired samples indicates that this is a medium to large effect size, <italic>d</italic>=0.66. Taken together, these results suggest that rather than producing equally long IUs, speakers tend to balance between IU duration and pauses and in such a way the sequence of IUs gives rise to a rhythm.</p></sec><sec id="S5"><title>Relation of IU rhythm to syllable-level speech rate</title><p id="P12">The primary focus of the current paper is the organization of speech into IUs and the temporal structure thereof. However, a broad literature exists that studies speech rate from the perspective of the syllable. As previously mentioned, there is substantial variation between languages in speech rate quantified at the syllable level, and a recent study has found that speakers systematically balance their average syllable delivery rate with language-specific informativeness properties of syllables (<xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R57">57</xref>). We sought to connect the two measures of speech rate -- at the IU and syllable levels. First, under the working hypothesis that IUs are related to cognitive mechanisms of attention and memory, we hypothesized that variation across languages in IU rate will be lesser than the systematic variation across languages in syllable delivery rate. In addition, we wished to investigate the possibility that our IU rate results are mediated by effects at the syllable level. To address these points, we explicitly modeled the rate of acoustic landmarks (<xref ref-type="bibr" rid="R58">58</xref>, <xref ref-type="bibr" rid="R59">59</xref>), which reflect the onsets of the vocalic nuclei of syllables (henceforth syl-IOI rate; see <xref ref-type="sec" rid="S7">Materials and methods</xref>: <xref ref-type="sec" rid="S20">Rate analyses</xref>: <xref ref-type="sec" rid="S22">Syllable rate</xref>), and the rate of IUs (henceforth IU-IOI rate). In both cases, we used a linear mixed-effects approach to account for possible dependencies been measurements, and utilized a model selection procedure to identify the best fitting model.</p><p id="P13">We found that syl-IOI rate is centered on a mean of 6.77 Hz. This rate is remarkably similar to the mean syllable rate found in previous studies that compared syllable rate in different languages (17 languages in (<xref ref-type="bibr" rid="R12">12</xref>), and 55 languages in (<xref ref-type="bibr" rid="R55">55</xref>)). Moreover, we replicated a previous finding whereby speaker sex is significantly related to the syl-IOI rate, in the same direction (<xref ref-type="bibr" rid="R12">12</xref>). Men have a significantly faster syl-IOI rate of 6.90 Hz. The variation across recordings in mean syl-IOI rate is estimated at 0.35 Hz, and the variation across languages at 0.36 Hz. Nevertheless, the model that includes speaker sex, recording identity, and the language spoken accounts for only a small portion of the overall variance in syl-IOI rate (4%), suggesting that we are missing important information for modeling syl-IOI rate.</p><p id="P14">Next, we found that IU-IOI rate is centered on a mean of 0.6 Hz, identical to our results based on spectral decomposition (see <xref ref-type="sec" rid="S2">Results</xref>: <italic>Sequences of IUs form low-frequency rhythms cross-linguistically</italic>). The IU-IOI rate is significantly dependent on the local syl-IOI rate, where a faster local syl-IOI rate corresponds to a faster IU-IOI rate. In this model, the total explanatory power is moderate (accounting for 15% of the overall variance in IU-IOI rate), yet the local syl-IOI rate effect accounts for at most 0.8% of the variance in IU-IOI rate. Therefore, it is unlikely that IU rate can be explained away by effects at the syllable level. The variation across recordings in mean IU-IOI rate is estimated at 0.07 Hz, and the variation across languages at 0.12 Hz.</p><p id="P15">In line with our hypothesis, the models above suggest that variation across languages in mean IU-IOI rate (0.12 Hz) is lower than the variation across languages in mean syl-IOI rate (0.36 Hz). To systematically compare the two, we computed several divergence metrics between IU-IOI estimates of pairs of languages, as well as the same divergence metrics between syl-IOI estimates of pairs of languages (<xref ref-type="fig" rid="F4">Fig. 4</xref>; see <xref ref-type="sec" rid="S7">Materials and methods</xref>: <xref ref-type="sec" rid="S23">Statistical analyses</xref>: <xref ref-type="sec" rid="S29">IU rate and syllable rate</xref>). Subsequently, paired permutation t-tests revealed that languages are significantly more like each other in IU-IOI rate in comparison to syl-IOI rate, in all metrics (all p ≤ 0.001).</p></sec></sec><sec id="S6" sec-type="discussion"><title>Discussion</title><p id="P16">In this study, we have investigated prosodic regularities in a diverse sample of 48 languages. We sought to characterize the temporal structure of intonation unit (IU) sequences in spontaneous speech, and to relate our findings to another measure of speech rate. We first validated the performance of an algorithm for automatically identifying IUs cross-linguistically. Then, we revealed a low-frequency rate of IUs across the sample, with a peak at 0.6 Hz, corresponding to an IU beginning every 1.6 seconds. While this rate of IUs varies somewhat between individual recordings and languages, it varies little across speaker sex and age. Finally, we found that syllable-level speech rate (SR) accounts for significant though small variation in IU rate, and more importantly, that the variance across languages in IU rate is significantly smaller than the variance across languages in SR.</p><p id="P17">Our results support the <italic>universal phonetic-IP hypothesis</italic> (<xref ref-type="bibr" rid="R14">14</xref>) by characterizing units with a similar prosodic profile in all the languages of the sample. Language universals are few (<xref ref-type="bibr" rid="R1">1</xref>), yet here, through the lens of spontaneous spoken language, we are faced with one. This study therefore demonstrates the importance of considering the temporal unfolding of spontaneous behavior to the study of language and human cognition in general. In future studies, it would be important to investigate the relation of this universal unit to mechanisms of speech production, to bodily rhythms such as breathing, heart rate and eye movements, to mechanisms of attention, temporal prediction, and memory, and to vocalizations in other species.</p><p id="P18">Recently it has been shown that speakers systematically balance their syllable delivery rate with language-specific informativeness properties of syllables, leading to substantial variation between languages in average SR (<xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R57">57</xref>). We show that only a little of this variation affects the delivery rate of a unit hierarchically above the syllable – IUs. This finding suggests that IUs fulfill a role in the pacing of language that is different from the role fulfilled by syllables. Syllables are low-level building blocks that are highly affected by the idiosyncrasies of articulation, while IUs are “planning units”, whose temporal structure relies on mechanisms of attention and memory.</p><p id="P19">Our findings are consistent with a recent influential neuroscientific model positing that temporally structured brain activity parses and processes incoming speech signals in a hierarchical fashion (<xref ref-type="bibr" rid="R60">60</xref>–<xref ref-type="bibr" rid="R62">62</xref>). The temporal structure of syllables is relatively well-characterized across the languages of the world, with 4-8 syllables per second (<xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R55">55</xref>, <xref ref-type="bibr" rid="R57">57</xref>, <xref ref-type="bibr" rid="R58">58</xref>, <xref ref-type="bibr" rid="R63">63</xref>–<xref ref-type="bibr" rid="R65">65</xref>). Here we shed light on a temporal structure at a level higher in the hierarchy, at the level of IUs, which begin every 1.6 seconds. In the brain, activity in this time scale has been especially implicated in studies that are concerned with the question of higher-level speech comprehension, beyond the processing that occurs when we are, for example, listening to backward-played speech, or word-lists (<xref ref-type="bibr" rid="R42">42</xref>–<xref ref-type="bibr" rid="R44">44</xref>, <xref ref-type="bibr" rid="R66">66</xref>). This suggests that the corresponding linguistic time scale of IUs is especially crucial for speech comprehension and the processing of information. From the perspective of speech production, we know even less about neural mechanisms that might be related to IUs and their temporal structure. A few studies have shown that low-frequency neural activity also contributes to speech production (<xref ref-type="bibr" rid="R40">40</xref>) and to the emergence of free thoughts and spontaneous actions (<xref ref-type="bibr" rid="R41">41</xref>).</p><p id="P20">The identification of IUs has been for a long time a laborious manual task, which has limited the breadth of investigations pertaining to IUs, in terms of the languages studied, in the amount of data in each, and across disciplines. We introduce and validate an algorithm to annotate IUs cross-linguistically. We rely on a previous automatic pipeline validated for English (<xref ref-type="bibr" rid="R48">48</xref>), and use the same parameters for all studied languages. Here, our sample included only four languages with IU annotations made by expert transcribers, and therefore our ability to tailor parameters to individual languages and investigate the influence of parameter fitting was limited. Future studies might shed light on this matter, and improve the ability to characterize language-specific or even speaker-specific profiles of prosodic phrasing. We see this study as an important step towards studying speech in a manner that is sensitive to its prosodic structure and its unfolding over time. Some of our results are similarly reliant on the automatic identification of syllables cross-linguistically. Future studies should validate whether this method is successful in capturing acoustic landmarks that correspond to the onsets of vowels in languages other than English and Chinese (<xref ref-type="bibr" rid="R58">58</xref>), as we assume.</p><p id="P21">Our study draws a general picture of the temporal structure of IUs in spontaneous production across the world’s languages. The data we analyzed limits our ability to characterize individual variability at the speaker level (the majority of recorded individuals were recorded only once). We are curious as to whether such variability could be traced and explained. Until then, IUs appear to be a universal audio-motor gestalt in human speech, with IUs beginning approximately every 1.6 seconds and with less variance across languages than SR.</p></sec><sec id="S7" sec-type="materials | methods"><title>Materials and methods</title><sec id="S8"><title>Data</title><sec id="S9"><title>Unannotated data</title><p id="P22">We analyzed 634 speech recordings from 44 languages, a sample that represents languages from every continent and from 27 distinct phylogenetic units. This sample was drawn from the Language Documentation Reference Corpus version 1.2 (DoReCo; 53), a database that compiles spoken language corpora originating in documentation efforts of mainly small and endangered languages.</p><p id="P23">Most of the recordings in the DoReCo database include narratives, either personal or traditional, as well as stimulus retellings, and are recounted by a single speaker at a time. To simplify matters related to acoustic analysis in multi-speaker setting, we limited our sample to the single-speaker recordings (which resulted in the exclusion of data from 3 languages without such recordings). We further excluded data from 4 additional languages whose audio recordings could not be retrieved. While in some cases a single speaker contributed more than one recording, this was not the case for most speakers, and therefore the systematic assessment of individual speaker effects lies beyond the scope of the current study. In addition to audio files, we extracted from the database time-aligned annotations at the word level for each recording. According to the DoReCo annotation conventions, silent pauses were annotated as words, and so we excluded these from our analyses. <xref ref-type="supplementary-material" rid="SD1">Table S1</xref> in the Supplementary materials references the specific language corpora used, as well as metadata information about the recordings analyzed in each.</p></sec><sec id="S10"><title>Annotated data</title><p id="P24">For testing and validating the automatic IU identification pipeline, we analyzed 34 speech recordings in 4 languages for which we had expert-made IU transcriptions. <xref ref-type="supplementary-material" rid="SD1">Table S2</xref> in the Supplementary materials references the specific language corpora used, as well as metadata information about the recordings analyzed in each. Recordings in three of the languages (Hebrew, Russian, Totoli) had time-aligned annotations at both word- and IU-level, and recordings in one language (English) had no time-aligned annotations whatsoever. We added the levels of time-aligned annotations in English (see <xref ref-type="supplementary-material" rid="SD1">Supplementary materials: Text S1</xref>).</p></sec></sec><sec id="S11"><title>Automatic prosodic phrase break identification</title><sec id="S12"><title>Acoustic boundary strength scores (BS)</title><p id="P25">Each word in each recording was given a score indicating the strength of acoustic boundary cues at its offset. Scores were calculated using a published algorithm, the Wavelet Prosody Toolkit (<xref ref-type="bibr" rid="R48">48</xref>). This algorithm derives fundamental frequency and intensity information from the speech audio signal, and duration information from time-stamped word annotations. As such, the algorithm operates on the signals that correspond to the main cues for an IU boundary. The sum of these signals is decomposed using a continuous wavelet transform in several frequency bands. The algorithm then finds peaks and troughs in the output of the continuous wavelet transform. Based on these peaks and troughs, the algorithm defines for each annotated word a prominence value and a boundary strength value, of which we only used the boundary strength value. In general, the prominence value of a word is defined as the strongest peak within the word. The boundary strength value of a word is defined as the strongest trough between two peaks, namely, between the strongest peak within the word and the strongest peak within the next word. We provided as input to the algorithm the speech audio files and the time-stamped word annotations and applied the algorithm with configurations optimized by Suni and colleagues for detecting prosodic phrase boundaries in English. We set the range of f0 values extracted by the algorithm separately for male (50-350 Hz) and female (100-400 Hz) speakers.</p></sec><sec id="S13"><title>Clustering</title><p id="P26">We utilized the BS scores for the purpose of classifying each word in the data to one of two categories: those that have strong boundary cues and those that have weak boundary cues. The k-means method partitions the BS scores into two groups in a way that minimizes the sum of squares from the datapoints to their assigned cluster centers. We classified words into two groups using the <italic>kmeans</italic> function in the R package <italic>stats</italic> (<xref ref-type="bibr" rid="R67">67</xref>). We used the default Hartigan-Wong algorithm with ten starting points.</p><p id="P27">In compliance with the goal of classifying words into two categories according to their BS scores, we observed that the distribution of BS scores across words tended to be bi-modal (<xref ref-type="fig" rid="F5">Fig. 5</xref>). For the analyses reported below, we classified words into two classes while pooling all words from all speakers across all languages. When considering the entire dataset in this way, the cluster centers were found to be 0.14 and 1.22 (<xref ref-type="fig" rid="F5">Fig. 5</xref>; see black triangles). Based on the identified clusters, we calculated silhouette indices per language to evaluate how similar each point is relative to its own cluster compared to the other cluster (68; Silhouette indices range between ±1, with higher values indicating better separation between clusters and compactness within clusters). The average silhouette width per language ranged between 0.7 to 0.8, which is by convention interpreted as strong clustering. Note that this is the case even though clustering was done while pooling data from all languages in the dataset. An alternative approach would be to classify words into two classes within each language separately (<xref ref-type="fig" rid="F5">Fig. 5</xref>; see pink triangles). The clustering of BS scores into two classes opens several questions for future research regarding cross-linguistic variability. For the current investigation, it lifts the bottleneck of annotating large amounts of data (over 300,000 words). We capitalize on this advance and turn to validate that the pipeline successfully identifies prosodic boundaries cross-linguistically.</p></sec><sec id="S14"><title>Validation</title><p id="P28">To assess the validity of the automatic IU identification pipeline, we compared the classification results with expert-made annotations in 4 languages (see <xref ref-type="sec" rid="S7">Materials and methods</xref>: <xref ref-type="sec" rid="S8">Data</xref>: <xref ref-type="sec" rid="S10">Annotated data</xref>). To this end, we used the <italic>confusionMatrix</italic> function in the R package caret (<xref ref-type="bibr" rid="R69">69</xref>) to compute several metrics: precision (<italic>p</italic>), recall (<italic>r</italic>), specificity, F1-score, and overall accuracy. These measures are derived from true positive (TP), true negative (TN), false positive (FP) and false negative (FN) classification values of IU-final words, and are defined as follows: precision: TP/(TP+FP), recall: TP/(TP+FN), specificity: TN/(FN+TN), F1-score: 2⋅p⋅r/(p+r), and overall accuracy: (TP+TN)/(TP+FP+FN+TN). In addition, we computed Cohen’s <italic>k</italic> (<xref ref-type="bibr" rid="R70">70</xref>), a metric for the quantification of agreement over and above chance agreement. To compute these metrics per language we first computed them per recording and then averaged the results across recordings within each language. In addition, per recording, we used a one-sided binomial test with a confidence level of 0.95 to test if the accuracy rate was better than the “no information rate”, the expected accuracy rate when always predicting the most common class in the data.</p></sec><sec id="S15"><title>Segmentation</title><p id="P29">The clustering of BS scores yields a classification of words as either ending an IU or not. From this result we derive a classification of words as either opening an IU or not: we regard words that immediately succeed IU-ending words as beginning a new IU. In this way, and while relying on the annotated onset and offset times of words, we obtain the onset and offset times of IUs.</p></sec></sec><sec id="S16"><title>Acoustic analyses</title><p id="P30">We calculated three acoustic signals per recording following the procedures described in the following subsections – the speech envelope, f0 and the harmonic ratio. Subsequently, we epoched each of the three acoustic signals from the onset to the offset of each manually annotated IU in the annotated data (34 recordings in 4 languages), and of each automatically derived IU in the entire sample of recordings (668 recordings in 48 languages). To compare epochs of equal scale and duration we standardized their values relative to the mean and standard deviation of per epoch, and resampled all epochs at 50 equally-spaced time points. Then, per acoustic signal, we computed the average in each recording and the grand average across recordings within a given language.</p><sec id="S17"><title>Envelope</title><p id="P31">We computed the amplitude envelope for each audio file along the following steps: audio files that were recorded in stereo were converted to mono by averaging the channels. Audio files with a sampling rate above 20 kHz were downsampled to 20 kHz. Using the Chimera toolbox, audio files were then band-pass filtered into 9 bands between 100 Hz and half the audio file’s sampling frequency, with cut-off frequencies designed to be equidistant on the human cochlear map (<xref ref-type="bibr" rid="R71">71</xref>). Band-pass filtering was performed by using a sixth order Butterworth filter between each two successive cutoff frequencies. Amplitude envelopes for each band (the narrowband envelopes) were computed as absolute values of the Hilbert transform. These narrowband envelopes were downsampled to 1000 Hz and subsequently averaged, yielding the wideband envelope. The wideband envelope was smoothed using a 50 ms sliding Gaussian filter and divided by its maximal value to be on a scale of 0-1.</p></sec><sec id="S18"><title>Fundamental frequency (f0)</title><p id="P32">We computed the pitch track for each audio file using MATLAB’s <italic>HelperPitchTracker</italic> function from the Audio Toolbox, designed to improve pitch estimation of noisy speech signals. Before applying the function, audio files were converted to mono by averaging the channels and downsampled to 8 kHz. The function estimates pitch every 10 ms using partially overlapping windows such that the resulting pitch tracks have a sampling rate of 100 Hz. Multiple pitch candidates are estimated using different pitch detection algorithms, octave-smoothing is applied, and a Hidden Markov Model selects among the candidates based on their confidence and the probability of a certain pitch value moving from one state to another across time in the range 50-400 Hz (we used the pre-trained probability matrix provided by the algorithm). Finally, the function applies a moving median filter with a window length of 30 ms. We set a threshold at 3/4 of the maximal harmonic ratio in the recording (following the conventions in the algorithm) to detect regions that do not include voiced speech and replaced them with NaNs. Finally, we transformed each pitch track to semitones relative to 100 Hz.</p></sec><sec id="S19"><title>Harmonic ratio</title><p id="P33">The harmonic ratio indicates the ratio of energy in the harmonic portion of an audio segment to the total energy in that segment. The ability to extract the fundamental frequency depends on whether periodicities can be reliably extracted from the signal, and this is why the MATLAB function <italic>HelperPitchTracker</italic> relies on the harmonic ratio. However, the dynamics of voicing during speech are interesting in their own right. The function <italic>harmonicRatio</italic> from MATLAB’s Audio Toolbox is called from within <italic>HelperPitchTracker</italic>, and we record its output to inspect its dynamics relative to IU boundaries in addition to the dynamics of pitch.</p></sec></sec><sec id="S20"><title>Rate analyses</title><sec id="S21"><title>IU rate</title><p id="P34">We analyzed the relation between IU onsets and the speech envelope using a point-field synchronization measure. In this analysis, the rhythmicity of IU sequences is measured through the phase consistency of their onsets with respect to the periodic components of the speech envelope. Using this method, we have previously shown that IU onsets appear at significantly consistent phases of the low-frequency components of the speech envelope in six languages (<xref ref-type="bibr" rid="R37">37</xref>).</p><p id="P35">We extracted 5 second windows of the speech envelope centered on each IU onset, and decomposed them between 0-15 Hz using Fast Fourier Transform (FFT) with a single Hann window, no padding or smoothing across frequencies, and following demeaning. This yielded phase estimations for frequency components at a resolution of 0.2 Hz. We then measured the consistency in phase of each FFT frequency component across speech segments using the pairwise-phase consistency metric (PPC; 56), yielding a consistency spectrum. We computed a PPC spectrum per recording and language, and averaged the spectra across recordings within a language. This method is adopted from the study of rhythmic synchronization of neural spiking activity and Local Field Potentials.</p><p id="P36">As an additional measure of IU rate, we calculated the intervals between IU onsets (inter onset interval, and henceforth, IU-IOI) in each recording and language, from which we calculate the IU-IOI rate. The unit of IOI is seconds, and IOI rate is the reciprocal of IOI and is measured in Hertz. To aid the convergence of models in subsequent statistical analyses we excluded IU-IOI rates that were above or below thresholds calculated per individual recording. Given IQR, the range between the first (Q1) and the third (Q3) quartiles of IU-IOI measurements in a recording, the lower threshold was defined as Q1-3·IQR and the upper threshold was defined as Q3+3·IQR. To this end, we used the <italic>identify_outliers</italic> function in the R package <italic>rstatix</italic> (<xref ref-type="bibr" rid="R72">72</xref>) and the <italic>anti_join</italic> function in the package <italic>dplyr</italic> (<xref ref-type="bibr" rid="R73">73</xref>). This resulted in excluding 1.04% of the 91540 IU-IOI rates.</p></sec><sec id="S22"><title>Syllable rate</title><p id="P37">While the preceding section dealt with the rate of IUs, in the following we turn to discuss our methods for estimating speech rate from the perspective of the syllable, with the aim of studying its relation to our main findings on the rate of IUs.</p><p id="P38">A precise estimation of speech rate at the syllable level requires fine-grained transcription and detailed knowledge of each language’s phonological system (see <xref ref-type="supplementary-material" rid="SD1">Text S2</xref> in 12, 55). An alternative is to estimate speech rate through automatically identifying syllable nuclei (usually vowels) via acoustic analysis. However, we are somewhat lacking in evidence on the robustness of automatic methods in estimating speech rate, if not the contrary – that they might systematically err in performance in different languages. For example, Coupé and colleagues (<xref ref-type="bibr" rid="R12">12</xref>) considered such an algorithm yet deemed it not sufficiently reliable, as did additional studies that considered large samples of languages (<xref ref-type="bibr" rid="R55">55</xref>, <xref ref-type="bibr" rid="R74">74</xref>). Since then, however, another approach for detecting vowel onsets automatically was reintroduced in the literature (<xref ref-type="bibr" rid="R59">59</xref>), and in a recent comprehensive assessment it was found to outperform other detection methods in two unrelated languages and diverse speaking contexts (<xref ref-type="bibr" rid="R58">58</xref>), even if not perfectly replicating gold-standard inter-vowel time series. Of course, much work is still required to assess the robustness of this method across languages, yet we chose nonetheless to rely on it here for considerations of time and the scope of this study.</p><p id="P39">We utilized the scripts published by MacIntyre and colleagues (<xref ref-type="bibr" rid="R58">58</xref>) to estimate syllable-level speech rate in our dataset (henceforth, SR). These scripts consider 5 different procedures to extract envelopes along with 5 event detection methods (such as peaks in the envelope or peaks in the envelope’s first derivative). Of the 25 possible acoustic landmarks, we a-priori chose the one method that was found by MacIntyre and colleagues (<xref ref-type="bibr" rid="R58">58</xref>) to be the most robust, termed “envelope3/peaks in the first derivative”. This is the precise acoustic landmark used by Oganian &amp; Chang (<xref ref-type="bibr" rid="R59">59</xref>), and which they found to be specifically encoded by a defined region in the Superior Temporal Gyrus during speech perception. Unlike MacIntyre and colleagues (<xref ref-type="bibr" rid="R58">58</xref>), we applied the algorithm to the continuous speech recordings (unsegmented in any way) rather than to windowed data, to avoid conflating in unknown ways the acoustic landmark detection with the segmentation of spontaneous speech currently under discussion. However, to ensure that the resulting SR will not be calculated over periods of silence, we subsequently used the onset and offset times of IUs (see <xref ref-type="sec" rid="S7">Materials and methods</xref>: <xref ref-type="sec" rid="S11">Automatic prosodic phrase break identification</xref>: <xref ref-type="sec" rid="S15">Segmentation</xref>) to epoch the continuous time course of acoustic landmarks, thus excluding any non-speech acoustic landmarks that might have been calculated by mistake. We then calculated the intervals between acoustic landmarks within each epoch (an epoch with <italic>n</italic> detected acoustic landmarks contributed to the calculation of <italic>n-1</italic> intervals, meaning that an epoch had to contain at least two acoustic landmarks in order for an interval to be calculated). For simplicity, we refer to these measurements as syl-IOI, yet we remind that acoustic landmarks more precisely reflect the onsets of the vocalic nuclei of syllables. From the syl-IOI we calculate the syl-IOI rate. The unit of IOI is seconds, and IOI rate is the reciprocal of IOI and is measured in Hertz. To aid the convergence of models in subsequent statistical analyses we checked if any syl-IOI rates were above Q3+3·IQR and below Q1-3·IQR (calculating quartiles per individual recording), with the aid of the <italic>identify_outliers</italic> function in the R package <italic>rstatix</italic> (<xref ref-type="bibr" rid="R72">72</xref>) and the <italic>anti_join</italic> function in the package <italic>dplyr</italic> (<xref ref-type="bibr" rid="R73">73</xref>). None of the syl-IOI rates exceeded these criteria. Finally, to reach an IU-by-IU measure of SR, we either averaged the syl-IOI rates within each IU epoch, simply recorded the syl-IOI rate if there was only one, or recorded NA if no syl-IOI was detected in it.</p></sec></sec><sec id="S23"><title>Statistical analyses</title><sec id="S24"><title>Validation</title><sec id="S25"><title>Number of words, duration and intervening pauses</title><p id="P40">To compare these measures in the automatically derived IUs and in the manually annotated IUs we conducted a series of nonparametric statistical tests per measure and language. The test statistic was the estimate for the difference between the two methods in the measured variable. The estimate was obtained in a mixed-effect general linear model of the measure against the segmentation method, with a by-file random intercept (<xref ref-type="bibr" rid="R75">75</xref>). We created a permutation distribution for each statistic by shuffling segmentation method labels 100 times. This permutation distribution represents the expected difference between the segmentation methods under the null hypothesis, as it breaks the association between measure and method. This procedure created 3(measures) x 4(languages) = 12 permutation distributions which we compared to 12 observed test statistics, namely, the estimated difference in each of the measures and in each of the languages between the manual and automatic IUs. As a p-value for each test, we calculated the proportion of times that the permutation procedure resulted in an estimate larger than the observed estimate for the difference between the manually and automatically derived IUs.</p></sec><sec id="S26"><title>Envelope, f0 and harmonic ratio</title><p id="P41">To compare these measures in the automatically derived IUs and in the manually annotated IUs we computed confidence intervals around the grand average time course in each measure and language. We first computed the mean and standard error, and then multiplied the standard error by the critical t value. The critical t value was calculated with degrees of freedom according to the number of non-NaN observations in each timepoint, and for an alpha level of 0.025/50=0.0005, to correct for multiple comparisons across the different timepoints via Bonferroni correction. The intervals overlapped in all measures and languages.</p></sec></sec><sec id="S27"><title>IU rate</title><p id="P42">We assessed the statistical significance of peaks in the average PPC spectra using a randomization procedure. Per language, we created 100 sets of surrogate average spectra. These surrogate spectra were calculated using the speech envelope as before, but with temporally permuted onsets that maintained the association with word onsets (i.e., we sampled onsets from among all words in each recording without replacement). Each iteration resulted in an average phase consistency value at each frequency. However, we generated a single randomization distribution from this procedure by selecting, per iteration, the maximal consistency value across frequencies. We then calculated, for each frequency, whether the observed consistency value exceeds the 95% percentile of the randomization distribution of maximal statistics. This approach corrects for multiple comparisons over the different frequencies (<xref ref-type="bibr" rid="R76">76</xref>).</p><p id="P43">In an additional analysis, we tested whether the Coefficient of Variation (CoV) of the distribution of inter IU onset intervals (IU-IOI) was different from the CoV of the distribution of IU durations. To compute the CoV of each distribution per language we first excluded observations (IU-IOI and IU duration) exceeding ±2.5 standard deviations from the mean within each language. We additionally excluded the entire data from two languages, Russian and American English (from the validation set), as this data comes from conversational corpora and therefore a substantial number of IU-IOIs are conflated by the occurrence of speaker change. We then subtracted per language the CoV of the IU-IOI distribution from the CoV of the duration distribution, and averaged across the 46 remaining languages to obtain the observed test statistic. We used a nonparametric bootstrap procedure with 100,000 iterations to construct a confidence interval around this statistic. On each iteration, we sampled with replacement 46 CoV differences and averaged the sample. This creates a distribution of average CoV differences that could have been obtained had the sample of languages been composed slightly differently. We computed the limits of the confidence interval as the 2.5% and 97.5% percentiles of this distribution. If this interval does not include 0, we can conclude that 0 is an unlikely value for the difference between the CoV of the IU-IOI distribution and the CoV of the duration distribution.</p></sec><sec id="S28"><title>Deviances across languages in acoustics and IU rate</title><p id="P44">We assessed whether individual languages differed in their envelope, f0 and harmonic ratio time courses and PPC spectra from the grand average time course (or spectrum, in the case of the PPC measure) across languages. To this end, we constructed a confidence interval using a nonparametric bootstrap procedure. On each iteration, we sampled with replacement time courses (or spectra) per language, according to the number of languages in our sample. Each iteration resulted in an average value per timepoint (or frequency) and language. However, we generated a single distribution per timepoint (or frequency) by selecting the maximal and minimal values across individual languages. We computed the limits of the confidence interval per timepoint (or frequency) as the 2.5% percentile of the distribution of minimal values in that timepoint and the 97.5% percentile of the distribution of maximal values. We then calculated, for each language, whether the observed value in a given timepoint (or frequency) exceeds this interval. This approach corrects for multiple comparisons across the different languages (<xref ref-type="bibr" rid="R76">76</xref>). We do not additionally correct for multiple comparisons across time bins as there were no significant effects following the comparison across languages.</p></sec><sec id="S29"><title>IU rate and syllable rate</title><p id="P45">We followed the approach in Coupé and colleagues (<xref ref-type="bibr" rid="R12">12</xref>) and conducted several statistical analyses: <list list-type="simple" id="L1"><list-item><p id="P46">(1) We modeled SR using the automatically derived syl-IOI rate.</p></list-item><list-item><p id="P47">(2) We modeled the IU rate using the IU-IOI rate; in both these cases, we used a linear mixed-effects approach, to account for possible dependencies been intervals originating from the same recording, language, language family, area, age and sex. In the modeling of IU-IOI rate we additionally considered the local, IU-level SR as a potential source of variance in the distribution of IU-IOI rates.</p></list-item><list-item><p id="P48">(3) We compare the divergence between languages in IU rate and in SR, and show that it is significantly lower in the case of the IU rate.</p></list-item></list></p><p id="P49">As a preliminary analysis, we fitted all unique combinations of the predictor variables mentioned above (127 combinations), separately for the dependent variables syl-IOI rate and IU-IOI rate. We used the BIC to select the best fitting model, thus balancing the likelihood with the number of model parameters to avoid overfitting.</p><p id="P50">In both cases, the selected models included the same 2/4 random intercepts we considered (recording and language, but not family and area). Unlike the case in Coupé and colleagues (<xref ref-type="bibr" rid="R12">12</xref>), our speech material does not include multiple texts of the same kind (each recording featured unique material) and does not include multiple recordings for most speakers. Therefore, we could not test these effects, but we used random intercepts per recording instead. In addition, our sample usually does not include more than one language per family, and this is probably a source of divergence in our selected syl-IOI rate model compared to Coupé and colleagues (12; they too note that family had a very small effect beyond language). Finally, while the diverse data at hand allowed us to estimate random intercepts per linguistic area, the models including them had a lower BIC compared to the selected model. The selected syl-IOI rate model included only a fixed effect for speaker sex, identical to Coupé and colleagues (<xref ref-type="bibr" rid="R12">12</xref>). The selected IU-IOI rate model included only a fixed effect for IU-level SR (see <xref ref-type="sec" rid="S7">Materials and methods</xref>: Main analyses: Speech rate).</p><p id="P51">We fitted the models using the R package <italic>lme4</italic> (<xref ref-type="bibr" rid="R75">75</xref>), and tested the significance of the fixed effects using the package <italic>lmerTest</italic> (<xref ref-type="bibr" rid="R77">77</xref>). Speaker age and IU-level SR variables were standardized, such that their estimated coefficients could be interpreted as the expected change in IOI for a standard deviation change in age or IU-level SR, and the intercept as the expected IOI for an average age and IU-level SR. The speaker sex variable was coded as a sum contrast so that the estimated intercept could be interpreted as the average across sexes.</p><p id="P52">Inspecting the results of the selected models, the total explanatory power of the syl-IOI rate model is weak (conditional <italic>R<sup>2</sup></italic> = 0.04) and the part related to the fixed effects alone (marginal <italic>R<sup>2</sup></italic>) is 0.0026. The intercept of the model, corresponding to the average syl-IOI is at 6.77 (95% CI [6.66, 6.88]). Speaker sex is significantly related to the syl-IOI rate, with men having significantly higher syl-IOI rates (<italic>β</italic> = 0.13, 95% CI [0.10, 0.17], t(572937) = 7.80, p &lt; .001).</p><p id="P53">The total explanatory power of the IU-IOI rate model is moderate (conditional <italic>R<sup>2</sup></italic> = 0.15) and the part related to the fixed effects alone (marginal <italic>R<sup>2</sup></italic>) is 0.0085. The intercept of the model, corresponding to the average IU-IOI rate with average syl-IOI rate, is at 0.60 (95% CI [0.57, 0.64]). IU-level syl-IOI rate is significantly related to the IU-IOI rate, with faster rates of syl-IOI associated with faster rates of IU-IOI (<italic>β</italic> = 0.03, 95% CI [0.03, 0.04], t(86299) = 27.43, p &lt; .001).</p><p id="P54">Finally, we extracted from the selected models the estimated syl-IOI and IU-IOI rates per recording for the following final analysis. In this analysis, we followed precisely the steps and code in Coupé and colleagues (<xref ref-type="bibr" rid="R12">12</xref>) for inspecting pairwise distances between languages, except we consider the divergence between languages in syl-IOI and IU-IOI rates. For each of the two variables, we computed all the possible 48·(48–1)/2 = 1128 unique pairs of languages, and compared the two languages’ distributions of a given IOI rate variable. As in (<xref ref-type="bibr" rid="R12">12</xref>), we used five methods for a robust comparison (Hellinger distance, Jensen-Shannon divergence, Kolmogorov-Smirnov distance, Kullback-Leibler divergence, and chi-square divergence) with the aid of the <italic>distance</italic> function from the R package <italic>philentropy</italic> (78; all divergence methods provided equivalent conclusions). We subsequently tested paired differences between corresponding distance metrics for syl-IOI rate and IU-IOI rate across the sample of languages using permutation t tests with 1000 permutations.</p></sec></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Materials</label><media xlink:href="EMS201716-supplement-Supplementary_Materials.pdf" mimetype="application" mime-subtype="pdf" id="d227aAcEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S30"><title>Acknowledgments</title><p>We thank Anat Perry for access to the Hebrew data we report on, Shlomi Frige for help with its annotation, and Nadav Matalon, Amit Avigdor, Hagai Berenson, and members of the Brain, Attention and Time lab for discussion. We especially thank Frank Seifart and Nikolaus Himmelmann for the resources and thoughts they shared with us. Our work would not have been possible without them and without the substantial efforts of language documenters and the people they record all over the world.</p><sec id="S31"><title>Funding</title><p>The Jack, Joseph, and Morton Mandel School for Advanced Studies in the Humanities (MI)</p><p>The Azrieli Graduate Studies Fellowship (MI)</p><p>Israel Science Foundation Grant 2765/21 (EG)</p><p>James McDonnell Scholar Award in Understanding Human Cognition (ANL)</p><p>Israel Science Foundation Grant 958/16 (ANL)</p><p>Israel Science Foundation Grant 1899/21 (ANL)</p><p>HORIZON EUROPE European Research Council Grant 852387 (ANL)</p></sec></ack><sec id="S32" sec-type="data-availability"><title>Data and materials availability</title><p id="P55">Code producing the data, analyses and figures we report on will be made available online upon publication. Individual language raw annotations and audio files can be retrieved from the corpora cited in <xref ref-type="supplementary-material" rid="SD1">Tables S1-S2</xref>.</p></sec><fn-group><fn id="FN1" fn-type="con"><p id="P56"><bold>Author contributions</bold></p><p id="P57">Conceptualization: MI, EG, ANL</p><p id="P58">Funding acquisition: MI, EG, ANL</p><p id="P59">Data curation: MI</p><p id="P60">Methodology: MI, ANL</p><p id="P61">Visualization: MI, ANL</p><p id="P62">Investigation: MI, EG, ANL</p><p id="P63">Supervision: EG, ANL</p><p id="P64">Writing—original draft: MI</p><p id="P65">Writing—review &amp; editing: MI, EG, ANL</p></fn><fn id="FN2" fn-type="conflict"><p id="P66"><bold>Competing interests</bold></p><p id="P67">The authors declare that they have no competing interests.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Evans</surname><given-names>N</given-names></name><name><surname>Levinson</surname><given-names>SC</given-names></name></person-group><article-title>The myth of language universals: Language diversity and its importance for cognitive science</article-title><source>Behav Brain Sci</source><year>2009</year><volume>32</volume><fpage>429</fpage><lpage>448</lpage><pub-id pub-id-type="pmid">19857320</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bickel</surname><given-names>B</given-names></name></person-group><chapter-title>Distributional Typology: statistical inquiries into the dynamics of linguistic diversity</chapter-title><person-group person-group-type="editor"><name><surname>Heine</surname><given-names>B</given-names></name><name><surname>Narrog</surname><given-names>H</given-names></name></person-group><source>Oxford Handbook of Linguistic Analysis</source><publisher-name>Oxford University Press</publisher-name><year>2013</year><fpage>901</fpage><lpage>923</lpage></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tal</surname><given-names>S</given-names></name><name><surname>Smith</surname><given-names>K</given-names></name><name><surname>Culbertson</surname><given-names>J</given-names></name><name><surname>Grossman</surname><given-names>E</given-names></name><name><surname>Arnon</surname><given-names>I</given-names></name></person-group><article-title>The Impact of Information Structure on the Emergence of Differential Object Marking: An Experimental Study</article-title><source>Cogn Sci</source><year>2022</year><volume>46</volume><pub-id pub-id-type="pmcid">PMC9286624</pub-id><pub-id pub-id-type="pmid">35297091</pub-id><pub-id pub-id-type="doi">10.1111/cogs.13119</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Givón</surname><given-names>T</given-names></name></person-group><source>The Diachrony of Grammar</source><publisher-name>John Benjamins Publishing Company</publisher-name><year>2015</year></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bybee</surname><given-names>JL</given-names></name></person-group><article-title>Formal Universals as Emergent Phenomena: The Origins of Structure Preservation</article-title><source>Linguist Universals Lang Chang</source><year>2008</year><pub-id pub-id-type="doi">10.1093/acprof:oso/9780199298495.003.0005</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Croft</surname><given-names>W</given-names></name></person-group><chapter-title>Diachronic typology</chapter-title><source>Typology and Universals</source><publisher-name>Cambridge University Press</publisher-name><year>2002</year><fpage>232</fpage><lpage>279</lpage></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Greenberg</surname><given-names>JH</given-names></name></person-group><chapter-title>Diachrony, synchrony and language universals</chapter-title><person-group person-group-type="editor"><name><surname>Greenberg</surname><given-names>JH</given-names></name><name><surname>Ferguson</surname><given-names>CA</given-names></name><name><surname>Moravcsik</surname><given-names>EA</given-names></name></person-group><source>Universals of Human Language I: Method and Theory</source><publisher-name>Stanford University Press</publisher-name><year>1978</year><fpage>61</fpage><lpage>92</lpage></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hopper</surname><given-names>PJ</given-names></name><name><surname>Traugott</surname><given-names>EC</given-names></name></person-group><source>Grammaticalization</source><publisher-name>Cambridge University Press</publisher-name><year>1993</year></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dingemanse</surname><given-names>M</given-names></name><etal/></person-group><article-title>Universal principles in the repair of communication problems</article-title><source>PLoS One</source><year>2015</year><volume>10</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="pmcid">PMC4573759</pub-id><pub-id pub-id-type="pmid">26375483</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0136100</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stivers</surname><given-names>T</given-names></name><etal/></person-group><article-title>Universals and cultural variation in turn-taking in conversation</article-title><source>Proc Natl Acad Sci U S A</source><year>2009</year><volume>106</volume><fpage>10587</fpage><lpage>10592</lpage><pub-id pub-id-type="pmcid">PMC2705608</pub-id><pub-id pub-id-type="pmid">19553212</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0903616106</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Auer</surname><given-names>P</given-names></name><name><surname>Maschler</surname><given-names>Y</given-names></name></person-group><chapter-title>The family of NU and NÅ across the languages of Europe and beyond: Structure, function, and history</chapter-title><person-group person-group-type="editor"><name><surname>Auer</surname><given-names>P</given-names></name><name><surname>Maschler</surname><given-names>Y</given-names></name></person-group><source>NU / NÅ: A Family of Discourse Markers Across the Languages of Europe and Beyond</source><publisher-name>De Gruyter</publisher-name><year>2016</year><fpage>1</fpage><lpage>47</lpage></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coupé</surname><given-names>C</given-names></name><name><surname>Oh</surname><given-names>YM</given-names></name><name><surname>Dediu</surname><given-names>D</given-names></name><name><surname>Pellegrino</surname><given-names>F</given-names></name></person-group><article-title>Different languages, similar encoding efficiency: Comparable information rates across the human communicative niche</article-title><source>Sci Adv</source><year>2019</year><volume>5</volume><pub-id pub-id-type="pmcid">PMC6984970</pub-id><pub-id pub-id-type="pmid">32047854</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.aaw2594</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chafe</surname><given-names>W</given-names></name></person-group><source>Discourse, Consciousness and Time: The Flow and Displacement of Conscious Experience in Speaking and Writing</source><publisher-name>University of Chicago Press</publisher-name><year>1994</year></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Himmelmann</surname><given-names>NP</given-names></name><name><surname>Sandler</surname><given-names>M</given-names></name><name><surname>Strunk</surname><given-names>J</given-names></name><name><surname>Unterladstetter</surname><given-names>V</given-names></name></person-group><article-title>On the universality of intonational phrases: A cross-linguistic interrater study</article-title><source>Phonology</source><year>2018</year><volume>35</volume><fpage>207</fpage><lpage>245</lpage></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jun</surname><given-names>S</given-names></name></person-group><chapter-title>Prosodic typology</chapter-title><source>Prosodic Typology: The Phonology of Intonation and Phrasing</source><publisher-name>Oxford University Press</publisher-name><year>2005</year><fpage>430</fpage><lpage>458</lpage></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chafe</surname><given-names>W</given-names></name></person-group><chapter-title>The Flow of Thought and the Flow of Language</chapter-title><person-group person-group-type="editor"><name><surname>Givon</surname><given-names>T</given-names></name></person-group><source>Discourse and Syntax</source><publisher-name>Academic Press, Inc</publisher-name><year>1979</year><fpage>159</fpage><lpage>181</lpage></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Du Bois</surname><given-names>JW</given-names></name><name><surname>Cumming</surname><given-names>S</given-names></name><name><surname>Schuetze-Coburn</surname><given-names>S</given-names></name><name><surname>Paolino</surname><given-names>D</given-names></name></person-group><source>Discourse Transcription</source><person-group person-group-type="editor"><name><surname>Du Bois</surname><given-names>JW</given-names></name><name><surname>Cumming</surname><given-names>S</given-names></name><name><surname>Schuetze-Coburn</surname><given-names>S</given-names></name><name><surname>Paolino</surname><given-names>D</given-names></name></person-group><publisher-name>University of California</publisher-name><publisher-loc>Santa Barbara</publisher-loc><year>1992</year></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schuetze-Coburn</surname><given-names>S</given-names></name><name><surname>Shapley</surname><given-names>M</given-names></name><name><surname>Weber</surname><given-names>EG</given-names></name></person-group><article-title>Units of Intonation in Discourse: A Comparison of Acoustic and Auditory Analyses</article-title><source>Lang Speech</source><year>1991</year><volume>34</volume><fpage>207</fpage><lpage>234</lpage><pub-id pub-id-type="pmid">1843524</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Couper-Kuhlen</surname><given-names>E</given-names></name><name><surname>Barth-Weingarten</surname><given-names>D</given-names></name></person-group><article-title>A system for transcribing talk-in-interaction: GAT 2. English translation and adaptation of Selting, Margret et al. (2009)</article-title><source>Gesprächsforsch - Online-Zeitschrift zur verbalen Interaktion</source><year>2011</year><fpage>1</fpage><lpage>51</lpage></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Couper-Kuhlen</surname><given-names>E</given-names></name></person-group><source>English Speech Rhythm: Form and Function in Everyday Verbal Interaction</source><publisher-name>John Benjamins Publishing Company</publisher-name><year>1993</year></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cruttenden</surname><given-names>A</given-names></name></person-group><source>Intonation</source><edition>2nd Ed</edition><publisher-name>Cambridge University Press</publisher-name><year>1997</year></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shattuck-Hufnagel</surname><given-names>S</given-names></name><name><surname>Turk</surname><given-names>AE</given-names></name></person-group><article-title>A prosody tutorial for investigators of auditory sentence processing</article-title><source>J Psycholinguist Res</source><year>1996</year><volume>25</volume><fpage>193</fpage><lpage>246</lpage><pub-id pub-id-type="pmid">8667297</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Halliday</surname><given-names>MAK</given-names></name></person-group><source>Intonation and Grammar in British English</source><year>1967</year></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kibrik</surname><given-names>AA</given-names></name></person-group><article-title>Upper Kuskokwim (Athabaskan, Alaska)</article-title><source>Anthropol Linguist</source><year>2019</year><volume>61</volume><fpage>141</fpage><lpage>182</lpage></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Inbar</surname><given-names>M</given-names></name><name><surname>Landau</surname><given-names>AN</given-names></name><name><surname>Grossman</surname><given-names>E</given-names></name></person-group><chapter-title>Intonation Units: Prosodic Regularity in Spontaneous Speech as a Window onto Cognitive Dynamics</chapter-title><person-group person-group-type="editor"><name><surname>Meyer</surname><given-names>L</given-names></name><name><surname>Strauss</surname><given-names>A</given-names></name></person-group><source>Rhythms of Speech and Language</source><publisher-name>Cambridge University Press</publisher-name><year>2025</year></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chafe</surname><given-names>W</given-names></name></person-group><chapter-title>Cognitive constraints on information flow</chapter-title><person-group person-group-type="editor"><name><surname>Tomlin</surname><given-names>RS</given-names></name></person-group><source>Coherence and Grounding in Discourse</source><publisher-name>John Benjamins Publishing Company</publisher-name><year>1987</year><fpage>21</fpage><lpage>51</lpage></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turk</surname><given-names>A</given-names></name></person-group><article-title>Does prosodic constituency signal relative predictability? A Smooth Signal Redundancy hypothesis</article-title><source>Lab Phonol</source><year>2010</year><volume>1</volume><fpage>227</fpage><lpage>262</lpage></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ford</surname><given-names>CE</given-names></name><name><surname>Thompson</surname><given-names>SA</given-names></name></person-group><chapter-title>Interactional units in conversation: syntactic, intonational, and pragmatic resources for the management of turns</chapter-title><person-group person-group-type="editor"><name><surname>Ochs</surname><given-names>E</given-names></name><name><surname>Schegloff</surname><given-names>EA</given-names></name><name><surname>Thompson</surname><given-names>SA</given-names></name></person-group><source>Interaction and Grammar</source><publisher-name>Cambridge University Press</publisher-name><year>1996</year><fpage>134</fpage><lpage>184</lpage></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Selting</surname><given-names>M</given-names></name></person-group><article-title>On the interplay of syntax and prosody in the constitution of turn-constructional units and turns in conversation</article-title><source>Pragmatics</source><year>1996</year><volume>6</volume><fpage>357</fpage><lpage>388</lpage></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bögels</surname><given-names>S</given-names></name><name><surname>Torreira</surname><given-names>F</given-names></name></person-group><article-title>Turn-end Estimation in Conversational Turn-taking: The Roles of Context and Prosody</article-title><source>Discourse Process</source><year>2021</year><volume>58</volume><fpage>903</fpage><lpage>924</lpage></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bögels</surname><given-names>S</given-names></name><name><surname>Torreira</surname><given-names>F</given-names></name></person-group><article-title>Listeners use intonational phrase boundaries to project turn ends in spoken interaction</article-title><source>J Phon</source><year>2015</year><volume>52</volume><fpage>46</fpage><lpage>57</lpage></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Menn</surname><given-names>KH</given-names></name><name><surname>Männel</surname><given-names>C</given-names></name><name><surname>Meyer</surname><given-names>L</given-names></name></person-group><article-title>Phonological acquisition depends on the timing of speech sounds: Deconvolution EEG modeling across the first five years</article-title><source>Sci Adv</source><year>2023</year><volume>9</volume><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="pmcid">PMC10619930</pub-id><pub-id pub-id-type="pmid">37910625</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.adh2560</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Menn</surname><given-names>KH</given-names></name><name><surname>Michel</surname><given-names>C</given-names></name><name><surname>Meyer</surname><given-names>L</given-names></name><name><surname>Hoehl</surname><given-names>S</given-names></name><name><surname>Männel</surname><given-names>C</given-names></name></person-group><article-title>Natural infant-directed speech facilitates neural tracking of prosody</article-title><source>Neuroimage</source><year>2022</year><volume>251</volume><elocation-id>118991</elocation-id><pub-id pub-id-type="pmid">35158023</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holzgrefe-Lang</surname><given-names>J</given-names></name><name><surname>Wellmann</surname><given-names>C</given-names></name><name><surname>Höhle</surname><given-names>B</given-names></name><name><surname>Wartenburger</surname><given-names>I</given-names></name></person-group><article-title>Infants’ Processing of Prosodic Cues: Electrophysiological Evidence for Boundary Perception beyond Pause Detection</article-title><source>Lang Speech</source><year>2018</year><volume>61</volume><fpage>153</fpage><lpage>169</lpage><pub-id pub-id-type="pmid">28937300</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keitel</surname><given-names>A</given-names></name><name><surname>Prinz</surname><given-names>W</given-names></name><name><surname>Friederici</surname><given-names>AD</given-names></name><name><surname>von Hofsten</surname><given-names>C</given-names></name><name><surname>Daum</surname><given-names>MM</given-names></name></person-group><article-title>Perception of conversations: The importance of semantics and intonation in children’s development</article-title><source>J Exp Child Psychol</source><year>2013</year><volume>116</volume><fpage>264</fpage><lpage>277</lpage><pub-id pub-id-type="pmid">23876388</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Slone</surname><given-names>LK</given-names></name><name><surname>Abney</surname><given-names>DH</given-names></name><name><surname>Smith</surname><given-names>LB</given-names></name><name><surname>Yu</surname><given-names>C</given-names></name></person-group><article-title>The temporal structure of parent talk to toddlers about objects</article-title><source>Cognition</source><year>2023</year><volume>230</volume><elocation-id>105266</elocation-id><pub-id pub-id-type="pmid">36116401</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Inbar</surname><given-names>M</given-names></name><name><surname>Grossman</surname><given-names>E</given-names></name><name><surname>Landau</surname><given-names>AN</given-names></name></person-group><article-title>Sequences of Intonation Units form a ∼ 1 Hz rhythm</article-title><source>Sci Rep</source><year>2020</year><volume>10</volume><elocation-id>15846</elocation-id><pub-id pub-id-type="pmcid">PMC7522717</pub-id><pub-id pub-id-type="pmid">32985572</pub-id><pub-id pub-id-type="doi">10.1038/s41598-020-72739-4</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stehwien</surname><given-names>S</given-names></name><name><surname>Meyer</surname><given-names>L</given-names></name></person-group><article-title>Short-Term Periodicity of Prosodic Phrasing: Corpus-based Evidence</article-title><source>Speech Prosody</source><year>2022</year><volume>2022</volume><fpage>693</fpage><lpage>698</lpage><pub-id pub-id-type="doi">10.21437/speechprosody.2022-141</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turk</surname><given-names>A</given-names></name><name><surname>Shattuck-Hufnagel</surname><given-names>S</given-names></name></person-group><article-title>Timing in talking: What is it used for, and how is it controlled?</article-title><source>Philos Trans R Soc B Biol Sci</source><year>2014</year><volume>369</volume><pub-id pub-id-type="pmcid">PMC4240962</pub-id><pub-id pub-id-type="pmid">25385773</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2013.0395</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piai</surname><given-names>V</given-names></name><etal/></person-group><article-title>Direct brain recordings reveal hippocampal rhythm underpinnings of language processing</article-title><source>Proc Natl Acad Sci</source><year>2016</year><volume>113</volume><fpage>11366</fpage><lpage>11371</lpage><pub-id pub-id-type="pmcid">PMC5056038</pub-id><pub-id pub-id-type="pmid">27647880</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1603312113</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Norman</surname><given-names>Y</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name></person-group><chapter-title>What Can iEEG Inform Us About Mechanisms of Spontaneous Behavior?</chapter-title><person-group person-group-type="editor"><name><surname>Axmacher</surname><given-names>N</given-names></name></person-group><source>Intracranial EEG: A Guide for Cognitive Neuroscientists</source><publisher-name>Springer</publisher-name><year>2023</year><fpage>331</fpage><lpage>350</lpage></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>H</given-names></name><name><surname>Ince</surname><given-names>RAA</given-names></name><name><surname>Schyns</surname><given-names>PG</given-names></name><name><surname>Thut</surname><given-names>G</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name></person-group><article-title>Frontal top-down signals increase coupling of auditory low-frequency oscillations to continuous speech in human listeners</article-title><source>Curr Biol</source><year>2015</year><volume>25</volume><fpage>1649</fpage><lpage>1653</lpage><pub-id pub-id-type="pmcid">PMC4503802</pub-id><pub-id pub-id-type="pmid">26028433</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2015.04.049</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>N</given-names></name><name><surname>Melloni</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Tian</surname><given-names>X</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>Cortical tracking of hierarchical linguistic structures in connected speech</article-title><source>Nat Neurosci</source><year>2016</year><volume>19</volume><fpage>158</fpage><lpage>164</lpage><pub-id pub-id-type="pmcid">PMC4809195</pub-id><pub-id pub-id-type="pmid">26642090</pub-id><pub-id pub-id-type="doi">10.1038/nn.4186</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonhage</surname><given-names>CE</given-names></name><name><surname>Meyer</surname><given-names>L</given-names></name><name><surname>Gruber</surname><given-names>T</given-names></name><name><surname>Friederici</surname><given-names>AD</given-names></name><name><surname>Mueller</surname><given-names>JL</given-names></name></person-group><article-title>Oscillatory EEG dynamics underlying automatic chunking during sentence processing</article-title><source>Neuroimage</source><year>2017</year><volume>152</volume><fpage>647</fpage><lpage>657</lpage><pub-id pub-id-type="pmid">28288909</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nelson</surname><given-names>MJ</given-names></name><etal/></person-group><article-title>Neurophysiological dynamics of phrase-structure building during sentence processing</article-title><source>Proc Natl Acad Sci</source><year>2017</year><volume>114</volume><fpage>E3669</fpage><lpage>E3678</lpage><pub-id pub-id-type="pmcid">PMC5422821</pub-id><pub-id pub-id-type="pmid">28416691</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1701590114</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaufeld</surname><given-names>G</given-names></name><etal/></person-group><article-title>Linguistic structure and meaning organize neural oscillations into a content-specific hierarchy</article-title><source>J Neurosci</source><year>2020</year><volume>40</volume><fpage>9467</fpage><lpage>9475</lpage><pub-id pub-id-type="pmcid">PMC7724143</pub-id><pub-id pub-id-type="pmid">33097640</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0302-20.2020</pub-id></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Inbar</surname><given-names>M</given-names></name><name><surname>Genzer</surname><given-names>S</given-names></name><name><surname>Perry</surname><given-names>A</given-names></name><name><surname>Grossman</surname><given-names>E</given-names></name><name><surname>Landau</surname><given-names>AN</given-names></name></person-group><article-title>Intonation Units in spontaneous speech evoke a neural response</article-title><source>J Neurosci</source><year>2023</year><volume>43</volume><fpage>8189</fpage><lpage>8200</lpage><pub-id pub-id-type="pmcid">PMC10697392</pub-id><pub-id pub-id-type="pmid">37793909</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0235-23.2023</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suni</surname><given-names>A</given-names></name><name><surname>Šimko</surname><given-names>J</given-names></name><name><surname>Aalto</surname><given-names>D</given-names></name><name><surname>Vainio</surname><given-names>M</given-names></name></person-group><article-title>Hierarchical representation and estimation of prosody using continuous wavelet transform</article-title><source>Comput Speech Lang</source><year>2017</year><volume>45</volume><fpage>123</fpage><lpage>136</lpage></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Englebretson</surname><given-names>R</given-names></name><name><surname>Kemmer</surname><given-names>S</given-names></name><name><surname>Niedzielski</surname><given-names>N</given-names></name></person-group><source>HONOR (Harvey Oral Narratives on Record): A Corpus of Interviews from Hurricane Harvey (Version 1)</source><year>2020</year></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Kibrik</surname><given-names>А</given-names></name><etal/></person-group><source>Russian Multichannel Discourse</source><year>2018</year><comment>Available at: <ext-link ext-link-type="uri" xlink:href="https://multidiscourse.ru/main/?en=1">https://multidiscourse.ru/main/?en=1</ext-link></comment></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jospe</surname><given-names>K</given-names></name><etal/></person-group><article-title>The contribution of linguistic and visual cues to physiological synchrony and empathic accuracy</article-title><source>Cortex</source><year>2020</year><volume>132</volume><fpage>296</fpage><lpage>308</lpage><pub-id pub-id-type="pmid">33010739</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bardají</surname><given-names>M</given-names></name><etal/></person-group><chapter-title>Totoli DoReCo data set</chapter-title><person-group person-group-type="editor"><name><surname>Seifart</surname><given-names>F</given-names></name><name><surname>Paschen</surname><given-names>L</given-names></name><name><surname>Stave</surname><given-names>M</given-names></name></person-group><source>Language Documentation Reference Corpus (DoReCo) 1.3</source><publisher-name>Laboratoire Dynamique Du Langage (UMR5596, CNRS &amp; Université Lyon</publisher-name><year>2024</year><volume>2</volume></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Seifart</surname><given-names>F</given-names></name><name><surname>Paschen</surname><given-names>L</given-names></name><name><surname>Stave</surname><given-names>M</given-names></name></person-group><source>Language Documentation Reference Corpus (DoReCo) 1.2</source><person-group person-group-type="editor"><name><surname>Seifart</surname><given-names>F</given-names></name><name><surname>Paschen</surname><given-names>L</given-names></name><name><surname>Stave</surname><given-names>M</given-names></name></person-group><publisher-name>Leibniz-Zentrum Allgemeine Sprachwissenschaft &amp; laboratoire Dynamique Du Langage (UMR5596, CNRS &amp; Université Lyon 2)</publisher-name><year>2022</year></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biron</surname><given-names>T</given-names></name><etal/></person-group><article-title>Automatic detection of prosodic boundaries in spontaneous speech</article-title><source>PLoS One</source><year>2021</year><volume>16</volume><fpage>1</fpage><lpage>21</lpage><pub-id pub-id-type="pmcid">PMC8092678</pub-id><pub-id pub-id-type="pmid">33939754</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0250969</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ozaki</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Globally, songs and instrumental melodies are slower and higher and use more stable pitches than speech: A Registered Report</article-title><source>Sci Adv</source><year>2024</year><volume>10</volume><pub-id pub-id-type="pmcid">PMC11095461</pub-id><pub-id pub-id-type="pmid">38748798</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.adm9797</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vinck</surname><given-names>M</given-names></name><name><surname>van Wingerden</surname><given-names>M</given-names></name><name><surname>Womelsdorf</surname><given-names>T</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Pennartz</surname><given-names>CMA</given-names></name></person-group><article-title>The pairwise phase consistency: A bias-free measure of rhythmic neuronal synchronization</article-title><source>Neuroimage</source><year>2010</year><volume>51</volume><fpage>112</fpage><lpage>122</lpage><pub-id pub-id-type="pmid">20114076</pub-id></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pellegrino</surname><given-names>F</given-names></name><name><surname>Coupé</surname><given-names>C</given-names></name><name><surname>Marisco</surname><given-names>E</given-names></name></person-group><article-title>A Cross-Language Perspecitve on Speech Information Rate</article-title><source>Language (Baltim)</source><year>2011</year><volume>87</volume><fpage>539</fpage><lpage>558</lpage></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacIntyre</surname><given-names>AD</given-names></name><name><surname>Cai</surname><given-names>CQ</given-names></name><name><surname>Scott</surname><given-names>SK</given-names></name></person-group><article-title>Pushing the envelope: Evaluating speech rhythm with different envelope extraction techniques</article-title><source>J Acoust Soc Am</source><year>2022</year><volume>151</volume><fpage>2002</fpage><lpage>2026</lpage><pub-id pub-id-type="pmid">35364952</pub-id></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oganian</surname><given-names>Y</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name></person-group><article-title>A speech envelope landmark for syllable encoding in human superior temporal gyrus</article-title><source>Sci Adv</source><year>2019</year><volume>5</volume><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="pmcid">PMC6957234</pub-id><pub-id pub-id-type="pmid">31976369</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.aay6279</pub-id></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghitza</surname><given-names>O</given-names></name></person-group><article-title>Linking speech perception and neurophysiology: Speech decoding guided by cascaded oscillators locked to the input rhythm</article-title><source>Front Psychol</source><year>2011</year><volume>2</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="pmcid">PMC3127251</pub-id><pub-id pub-id-type="pmid">21743809</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2011.00130</pub-id></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giraud</surname><given-names>A-L</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>Cortical oscillations and speech processing: emerging computational principles and operations</article-title><source>Nat Neurosci</source><year>2012</year><volume>15</volume><fpage>511</fpage><lpage>517</lpage><pub-id pub-id-type="pmcid">PMC4461038</pub-id><pub-id pub-id-type="pmid">22426255</pub-id><pub-id pub-id-type="doi">10.1038/nn.3063</pub-id></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zoefel</surname><given-names>B</given-names></name><name><surname>Kösem</surname><given-names>A</given-names></name></person-group><article-title>Neural tracking of continuous acoustics: properties, speech-specificity and open questions</article-title><source>Eur J Neurosci</source><year>2023</year><fpage>394</fpage><lpage>414</lpage><pub-id pub-id-type="pmid">38151889</pub-id></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chandrasekaran</surname><given-names>C</given-names></name><name><surname>Trubanova</surname><given-names>A</given-names></name><name><surname>Stillittano</surname><given-names>S</given-names></name><name><surname>Caplier</surname><given-names>A</given-names></name><name><surname>Ghazanfar</surname><given-names>AA</given-names></name></person-group><article-title>The natural statistics of audiovisual speech</article-title><source>PLoS Comput Biol</source><year>2009</year><volume>5</volume><elocation-id>e1000436</elocation-id><pub-id pub-id-type="pmcid">PMC2700967</pub-id><pub-id pub-id-type="pmid">19609344</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000436</pub-id></element-citation></ref><ref id="R64"><label>64</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>N</given-names></name><etal/></person-group><article-title>Temporal modulations in speech and music</article-title><source>Neurosci Biobehav Rev</source><year>2017</year><pub-id pub-id-type="pmid">28212857</pub-id></element-citation></ref><ref id="R65"><label>65</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greenberg</surname><given-names>S</given-names></name><name><surname>Carvey</surname><given-names>H</given-names></name><name><surname>Hitchcock</surname><given-names>L</given-names></name><name><surname>Chang</surname><given-names>S</given-names></name></person-group><article-title>Temporal properties of spontaneous speech - A syllable-centric perspective</article-title><source>J Phon</source><year>2003</year><volume>31</volume><fpage>465</fpage><lpage>485</lpage></element-citation></ref><ref id="R66"><label>66</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keitel</surname><given-names>A</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name></person-group><article-title>Perceptually relevant speech tracking in auditory and motor cortex reflects distinct linguistic features</article-title><source>PLOS Biol</source><year>2018</year><volume>16</volume><elocation-id>e2004473</elocation-id><pub-id pub-id-type="pmcid">PMC5864086</pub-id><pub-id pub-id-type="pmid">29529019</pub-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2004473</pub-id></element-citation></ref><ref id="R67"><label>67</label><element-citation publication-type="web"><collab>R Core Team</collab><source>R: A Language and Environment for Statistical Computing</source><year>2022</year><comment>Available at: <ext-link ext-link-type="uri" xlink:href="https://www.r-project.org/">https://www.r-project.org/</ext-link></comment></element-citation></ref><ref id="R68"><label>68</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rousseeuw</surname><given-names>PJ</given-names></name></person-group><article-title>Silhouettes: A graphical aid to the interpretation and validation of cluster analysis</article-title><source>J Comput Appl Math</source><year>1987</year><volume>20</volume><fpage>53</fpage><lpage>65</lpage></element-citation></ref><ref id="R69"><label>69</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Kuhn</surname><given-names>M</given-names></name></person-group><source>caret: Classification and Regression Training</source><year>2022</year><comment>Available at: <ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/web/packages/caret/index.html">https://cran.r-project.org/package=caret</ext-link></comment></element-citation></ref><ref id="R70"><label>70</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>J</given-names></name></person-group><article-title>A Coefficient of Agreement for Nominal Scales</article-title><source>Educ Psychol Meas</source><year>1960</year><volume>20</volume><fpage>37</fpage><lpage>46</lpage></element-citation></ref><ref id="R71"><label>71</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>ZM</given-names></name><name><surname>Delgutte</surname><given-names>B</given-names></name><name><surname>Oxenham</surname><given-names>AJ</given-names></name></person-group><article-title>Chimaeric sounds reveal dichotomies in auditory perception</article-title><source>Nature</source><year>2002</year><volume>416</volume><fpage>87</fpage><lpage>90</lpage><pub-id pub-id-type="pmcid">PMC2268248</pub-id><pub-id pub-id-type="pmid">11882898</pub-id><pub-id pub-id-type="doi">10.1038/416087a</pub-id></element-citation></ref><ref id="R72"><label>72</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Kassambara</surname><given-names>A</given-names></name></person-group><source>rstatix: Pipe-Friendly Framework for Basic Statistical Tests</source><year>2023</year><comment>Available at: <ext-link ext-link-type="uri" xlink:href="https://rpkgs.datanovia.com/rstatix/">https://rpkgs.datanovia.com/rstatix/</ext-link></comment></element-citation></ref><ref id="R73"><label>73</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Wickham</surname><given-names>H</given-names></name><name><surname>François</surname><given-names>R</given-names></name><name><surname>Henry</surname><given-names>L</given-names></name><name><surname>Müller</surname><given-names>K</given-names></name><name><surname>Vaughan</surname><given-names>D</given-names></name></person-group><source>dplyr: A Grammar of Data Manipulation</source><year>2023</year><comment>Available at: <ext-link ext-link-type="uri" xlink:href="https://dplyr.tidyverse.org/">https://dplyr.tidyverse.org</ext-link></comment></element-citation></ref><ref id="R74"><label>74</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jadoul</surname><given-names>Y</given-names></name><name><surname>Ravignani</surname><given-names>A</given-names></name><name><surname>Thompson</surname><given-names>B</given-names></name><name><surname>Filippi</surname><given-names>P</given-names></name><name><surname>de Boer</surname><given-names>B</given-names></name></person-group><article-title>Seeking temporal predictability in speech: Comparing statistical approaches on 18 world languages</article-title><source>Front Hum Neurosci</source><year>2016</year><volume>10</volume><pub-id pub-id-type="pmcid">PMC5133256</pub-id><pub-id pub-id-type="pmid">27994544</pub-id><pub-id pub-id-type="doi">10.3389/fnhum.2016.00586</pub-id></element-citation></ref><ref id="R75"><label>75</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bates</surname><given-names>D</given-names></name><name><surname>Maechler</surname><given-names>M</given-names></name><name><surname>Bolker</surname><given-names>B</given-names></name><name><surname>Walker</surname><given-names>S</given-names></name></person-group><article-title>Fitting Linear Mixed-Effects Models Using lme4</article-title><source>J Stat Softw</source><year>2015</year><volume>67</volume><fpage>1</fpage><lpage>48</lpage></element-citation></ref><ref id="R76"><label>76</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nichols</surname><given-names>T</given-names></name><name><surname>Holmes</surname><given-names>A</given-names></name></person-group><article-title>Nonparametric Permutation Tests for Functional Neuroimaging</article-title><source>Hum Brain Funct Second Ed</source><year>2003</year><volume>25</volume><fpage>887</fpage><lpage>910</lpage><pub-id pub-id-type="pmcid">PMC6871862</pub-id><pub-id pub-id-type="pmid">11747097</pub-id><pub-id pub-id-type="doi">10.1002/hbm.1058</pub-id></element-citation></ref><ref id="R77"><label>77</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuznetsova</surname><given-names>A</given-names></name><name><surname>Brockhoff</surname><given-names>PB</given-names></name><name><surname>Christensen</surname><given-names>RHB</given-names></name></person-group><article-title>{lmerTest} Package: Tests in Linear Mixed Effects Models</article-title><source>J Stat Softw</source><year>2017</year><volume>82</volume><fpage>1</fpage><lpage>26</lpage></element-citation></ref><ref id="R78"><label>78</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drost</surname><given-names>H</given-names></name></person-group><article-title>Philentropy: Information Theory and Distance Quantification with R</article-title><source>J Open Source Softw</source><year>2018</year><volume>3</volume><fpage>765</fpage></element-citation></ref><ref id="R79"><label>79</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pratap</surname><given-names>V</given-names></name><etal/></person-group><article-title>Scaling Speech Technology to 1,000+ Languages</article-title><year>2023</year></element-citation></ref><ref id="R80"><label>80</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bořil</surname><given-names>T</given-names></name><name><surname>Skarnitzl</surname><given-names>R</given-names></name></person-group><source>Tools rPraat and mPraat</source><person-group person-group-type="editor"><name><surname>Sojka</surname><given-names>P</given-names></name><name><surname>Horák</surname><given-names>A</given-names></name><name><surname>Kopeček</surname><given-names>I</given-names></name><name><surname>Pala</surname><given-names>K</given-names></name></person-group><conf-name>Text, Speech, and Dialogue: 19th International Conference, {TSD} 2016, {B}rno, {C}zech Republic, September 12-16, 2016, Proceedings</conf-name><conf-sponsor>Springer International Publishing</conf-sponsor><year>2016</year><fpage>367</fpage><lpage>374</lpage></element-citation></ref><ref id="R81"><label>81</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Boersma</surname><given-names>P</given-names></name><name><surname>Weenink</surname><given-names>D</given-names></name></person-group><source>PRAAT: doing phonetics by computer</source><year>2022</year><comment>Available at: praat.org</comment></element-citation></ref><ref id="R82"><label>82</label><element-citation publication-type="web"><collab>ELAN</collab><year>2022</year><comment>Available at: <ext-link ext-link-type="uri" xlink:href="https://archive.mpi.nl/tla/elan">https://archive.mpi.nl/tla/elan</ext-link></comment></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Fig. 1</label><caption><title>Similarities and differences between the manually annotated IUs and the automatically derived IUs.</title><p>In each measure, we visualize the data of the automatically derived IUs in color, and in transparent gray the data of the manually annotated IUs. (<bold>A-C</bold>) Probability distributions of the number of words per IU (<bold>A</bold>), of IU duration (<bold>B</bold>), and of pause durations between IUs (<bold>C</bold>). Histogram bins span 0.1 seconds. Wherever a significant difference exists between the two distributions, a p-value is provided in the respective figure. (<bold>D-F</bold>) Time- and scale-normalized time courses of the speech envelope (<bold>D</bold>) f0 (<bold>E</bold>) and harmonic ratio (<bold>F</bold>). Shaded ribbons correspond to the 95% confidence interval corrected for multiple comparisons across timepoints.</p></caption><graphic xlink:href="EMS201716-f001"/></fig><fig id="F2" position="float"><label>Fig. 2</label><caption><title>Grand average envelope (A), f0 (B) and harmonic ratio (C) time courses, and phase consistency spectra (D; see next section) of automatically derived IUs.</title><p>Grand averages across 48 languages appear in color. Grand averages across recordings within a language appear in thin gray lines. Shaded ribbons correspond to 95% bootstrapped confidence intervals that are corrected for multiple comparisons across languages.</p></caption><graphic xlink:href="EMS201716-f002"/></fig><fig id="F3" position="float"><label>Fig. 3</label><caption><title>Phase consistency spectra (left) and probability distributions of IU duration (pink), pause duration (green), and IU-IOI values (gray) in 48 languages.</title><p>Colored regions under the phase consistency spectra correspond to portions of the spectrum that are significantly higher than expected under the null hypothesis, after correction for multiple comparisons across frequencies (see <xref ref-type="sec" rid="S7">Materials and methods</xref>: <xref ref-type="sec" rid="S23">Statistical analyses</xref>: <xref ref-type="sec" rid="S21">IU rate</xref>). Languages are sorted in ascending order by their maximal consistency value. The y-axis range of the phase consistency spectra lies between 0-0.63, the maximal consistency value obtained in our sample - in Warlpiri. The y-axis range for the duration and IU-IOI probability distributions lies between 0-0.2 and the histogram bins span 0.2 seconds. The y-axis range for the pause duration probability distribution lies between 0-0.8 and the histogram bins span 0.1 seconds. Coefficient of Variation scores for the duration and IU-IOI measurements appear in the top-right corner of the respective histograms.</p></caption><graphic xlink:href="EMS201716-f003"/></fig><fig id="F4" position="float"><label>Fig. 4</label><caption><title>Distances between language pairs in syl-IOI rate and IU-IOI rate, computed using Jensen-Shannon divergence.</title><p>For each rate measure, green bars indicate individual observations, the black spike indicates the distribution mode and the gray-shaded density plot shows the 50% (dark) and 95% (mid) highest density intervals of the distribution.</p></caption><graphic xlink:href="EMS201716-f004"/></fig><fig id="F5" position="float"><label>Fig. 5</label><caption><title>Probability distributions of boundary strength scores.</title><p>Histogram bins span 0.083 units of boundary strength in the range 0.001 and 2.5, and the y-axis range is identical across languages. The highest histogram bin, in Gorwaa, reaches 0.233. For visualization purposes only, we omitted from the histograms the boundary strength scores that equaled 0, as these were by far the most common (in all languages) and obscured the shape of the histogram. Triangles denote the identified cluster centers, based on the entire pooled data (in black; lower cluster center: 0.14, higher cluster center: 1.22) or at the individual language level (in pink). Average silhouette indices for the clustering based on the pooled data appear below the language label of each histogram.</p></caption><graphic xlink:href="EMS201716-f005"/></fig><table-wrap id="T1" position="float" orientation="portrait"><label>Table 1</label><caption><title>Classification results.</title><p>Automatic prosodic phrase break detection relative to expert-made manual annotations in four languages.</p></caption><table frame="void" rules="groups"><thead><tr><th align="left" valign="middle"/><th align="left" valign="middle">English</th><th align="left" valign="middle">Hebrew</th><th align="left" valign="middle">Russian</th><th align="left" valign="middle">Totoli</th></tr></thead><tbody><tr><td align="left" valign="middle">Precision</td><td align="left" valign="middle">0.76</td><td align="left" valign="middle">0.90</td><td align="left" valign="middle">0.71</td><td align="left" valign="middle">0.90</td></tr><tr><td align="left" valign="middle">Recall</td><td align="left" valign="middle">0.73</td><td align="left" valign="middle">0.47</td><td align="left" valign="middle">0.59</td><td align="left" valign="middle">0.89</td></tr><tr style="border-bottom: solid thin"><td align="left" valign="middle">Specificity</td><td align="left" valign="middle">0.89</td><td align="left" valign="middle">0.95</td><td align="left" valign="middle">0.87</td><td align="left" valign="middle">0.96</td></tr><tr style="border-bottom: solid thin"><td align="left" valign="middle"><bold>Accuracy</bold></td><td align="left" valign="middle">84.24%</td><td align="left" valign="middle">73.00%</td><td align="left" valign="middle">77.55%</td><td align="left" valign="middle">94.25%</td></tr><tr style="border-bottom: solid thin"><td align="left" valign="middle"><bold>F1-score</bold></td><td align="left" valign="middle">0.74</td><td align="left" valign="middle">0.61</td><td align="left" valign="middle">0.64</td><td align="left" valign="middle">0.89</td></tr><tr><td align="left" valign="middle"><bold>Cohen’s k</bold></td><td align="left" valign="middle">0.62</td><td align="left" valign="middle">0.44</td><td align="left" valign="middle">0.48</td><td align="left" valign="middle">0.85</td></tr></tbody></table></table-wrap></floats-group></article>