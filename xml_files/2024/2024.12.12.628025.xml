<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS202298</article-id><article-id pub-id-type="doi">10.1101/2024.12.12.628025</article-id><article-id pub-id-type="archive">PPR957597</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>FeatureForest: the power of foundation models, the usability of random forests</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Seifi</surname><given-names>Mehdi</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Nogare</surname><given-names>Damian Dalle</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Battagliotti</surname><given-names>Juan</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Galinova</surname><given-names>Vera</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Rao</surname><given-names>Ananya Kedige</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><collab>AI4Life Horizon Europe Programme Consortium</collab><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Decelle</surname><given-names>Johan</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Jug</surname><given-names>Florian</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Deschamps</surname><given-names>Joran</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>Computational Biology Research Center, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/029gmnc79</institution-id><institution>Human Technopole</institution></institution-wrap>, <city>Milan</city>, <country country="IT">Italy</country></aff><aff id="A2"><label>2</label>Bioimage Analysis Unit, National Facility for Data Handling and Analysis, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/029gmnc79</institution-id><institution>Human Technopole</institution></institution-wrap>, <city>Milan</city>, <country country="IT">Italy</country></aff><aff id="A3"><label>3</label>Cell and Plant Physiology Laboratory, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02feahw73</institution-id><institution>CNRS</institution></institution-wrap>, CEA, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/003vg9w96</institution-id><institution>INRAE</institution></institution-wrap>, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/00byxdz40</institution-id><institution>IRIG</institution></institution-wrap>, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02rx3b187</institution-id><institution>Université Grenoble Alpes</institution></institution-wrap>, <city>Grenoble</city>, <country country="FR">France</country></aff><aff id="A4"><label>4</label>Fynn Beuttenmueller, Joran Deschamps, Mariana G. Ferreira, Caterina Fuster-Barcelo, Vera Galinova, Carlos Garcia-Lopez-de-Haro, Estibaliz Gómez-de-Mariscal, Matthew Hartley, Ricardo Henriques, Ivan Hidalgo-Cenalmor, Florian Jug, Anna Kreshuk, Emma Lundberg, Nils Mechtel, Arrate Muñoz-Barrutia, Wei Ouyang, Constantin Pape, Craig T. Russell, MehdiSeifi, Beatriz Serrano-Solano, Tomaz Vieira, Teresa Zulueta-Coarasa</aff><author-notes><corresp id="CR1"><label>*</label><email>joran.deschamps@fht.org</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>17</day><month>01</month><year>2025</year></pub-date><pub-date pub-type="preprint"><day>16</day><month>12</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Once the work at the microscope is done, biological discoveries rely heavily on proper downstream analysis. This often amounts to first segmenting the biological objects of interest in the image before performing a quantitative analysis. Deep-learning (DL) is nowadays ubiquitous in such segmentation tasks. However, DL can be cumbersome to apply, as it often requires large amount of manual labeling to produce ground-truth data, and expert knowledge to train the models from scratch. Nonetheless, the performance of large foundation models, although trained on natural images, are improving on scientific images with every new model released. They, however, require either manual prompting or tedious post-processing to selectively segment the biological objects of interest. Classical machine learning algorithms, such as random forest classifiers, on the other hand, are well-established, easy to train, and often yield results of sufficient quality for downstream processing tasks, hence their continued popularity. Unfortunately, they are limited to objects with distinct, well-defined textures compared to their environment. This generally limits their usefulness to structures easy to recognize. Here, we present FeatureForest, an open-source tool that leverages the feature embeddings of large foundation models to train a random forest classifier, thereby providing users with a rapid way of semantically segmenting complex images using only a few labeling strokes. We demonstrate the improvement in performance over a variety of datasets, including large and complex volumetric electron microscopy stacks. Our implementation is available in napari, currently integrates four foundation models, and can easily be extended to any new model once they become available.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><label>1</label><title>Introduction</title><p id="P2">Segmentation is an ubiquitous task in microscopy image analysis, as it enables downstream processing and quantification of objects of interest. Researchers have at their disposal a wide array of algorithms, among which machine learning approaches have long been the methods of choice. In particular, random forest pixel classification is a well-established algorithm, at the heart of several popular software tools for bioimage analysis [<xref ref-type="bibr" rid="R1">1</xref>, <xref ref-type="bibr" rid="R2">2</xref>, <xref ref-type="bibr" rid="R3">3</xref>, <xref ref-type="bibr" rid="R4">4</xref>]. It uses common image filters to extract a feature vector representation of hand-labeled pixels in order to train decision trees to best match the given input labels. Because the image filters can be 2D or 3D, random forest pixel classifiers can natively perform 3D segmentation. Moreover, they are compatible with multiclass pixel classification. These algorithms owe their popularity to the simple iterative process by which users draw small scribbles to assign a class to a subset of pixels, rapidly train a random forest, and predict results over many images. This swift training procedure allows the correction of mistakes by adding new labels to the training set and training anew. While random forest pixel classification algorithms have a wide application range covering all types of images and modalities, they are limited in their predictive power, and easily confuse different object types that have similar textures [<xref ref-type="bibr" rid="R3">3</xref>].</p><p id="P3">In recent years, deep-learning has emerged as the most powerful approach for image segmentation. Such approaches are most often trained in a supervised fashion, that is to say with a large dataset of manually segmented images as reference. The likes of StarDist [<xref ref-type="bibr" rid="R5">5</xref>] or CellPose [<xref ref-type="bibr" rid="R6">6</xref>] are go-to tools for image analysts wishing to perform image segmentation. Once trained, these methods often outperform random forest pixel classification [<xref ref-type="bibr" rid="R7">7</xref>, <xref ref-type="bibr" rid="R8">8</xref>]. In addition, both methods are compatible with 3D segmentation. Furthermore, CellPose2 [<xref ref-type="bibr" rid="R9">9</xref>] introduced user-friendly fine-tuning of models by providing a user interface to correct errors and retrain the selected model, similar to the way random forest classifiers are used. Base models were trained on datasets consisting of various imaging modalities and diverse samples, and are capable of segmenting objects of similar size in a wide range of images. It does not, however, segment multiple classes, and can struggle to effectively segment objects with various shapes and sizes simultaneously.</p><p id="P4">With more compute power and more data being available, much larger networks are now being trained with astounding results. For instance, Segment Anything Model (SAM) [<xref ref-type="bibr" rid="R10">10</xref>], is capable of accurately segmenting biological objects in 2D in both electron and light microscopy images, all the while being trained on a dataset overwhelmingly composed of natural (i.e. every-day) images. To push the boundary of its capabilities, fine-tuning this model with scientific images is being explored [<xref ref-type="bibr" rid="R11">11</xref>].</p><p id="P5">SAM does not natively segment whole images, but rather expects user annotations - also called prompts - in the form of bounding boxes or points as inputs, and returns segmented instances of the annotated objects. While this is a powerful way to enable interactivity, scientific segmentation pipelines preferentially require automated processing of large datasets. Without an accurate and automated way of producing the prompts, SAM applications in bioimage analysis are limited to direct and time-consuming user interactions for each object in the dataset.</p><p id="P6">Another fruitful research avenue is the use of rich latent spaces as basis for segmentation. Rather than segmenting pixels directly, other approaches, such as MAESTER [<xref ref-type="bibr" rid="R12">12</xref>] or DINOv2 [<xref ref-type="bibr" rid="R13">13</xref>, <xref ref-type="bibr" rid="R14">14</xref>], train a large network on a different task (e.g. reconstructing masked areas of the image) in order to produce rich feature embedding of the image. These features can then be used to cluster the pixels based on their proximity in this latent space, and identify object classes with these clusters. While enticing, cluster-based features are often limited by the lack of knowledge of how many classes are expected in a given image, and whether these classes cluster meaningfully in the feature space. Moreover, the application of such approaches are so far limited to deep-learning experts due to the complexity of the training process, and success in segmenting scientific images different from those in the training set is not ensured.</p><p id="P7">In the context of electron microscopy, for instance, certain imaging methods lead to high contrast images with a high density of objects. These datasets are particularly interesting to the community as they potentially enable the observation of unseen biology or the quantification with high resolution of many biological structures. However, segmenting these structures remains an unsolved challenge owing to the complexity and size of the images. De-novo training of supervised deep-learning models is out of reach for many researchers, and few groups have been capable of systematically spending the time and effort to label large volumes [<xref ref-type="bibr" rid="R15">15</xref>, <xref ref-type="bibr" rid="R16">16</xref>]. Large deep-learning networks such as SAM are capable of segmenting objects with high accuracy, provided that users generate the correct prompts. Even though SAM requires annotations that are much easier to produce than whole-object masks, the sheer size of the typical electron microscopy volume prevents annotating every object in the dataset.</p><p id="P8">Here, we present FeatureForest, a method that combines the power of large deep-learning models with the simplicity and user-guidance provided by random forest classification algorithms. With FeatureForest, manual labeling can be a matter of minutes, and user-guidance allows segmenting complex objects throughout entire datasets without requiring re-training large deep learning networks. We showcase how FeatureForest fills a gap in the segmentation of large electron microscopy datasets, enabling researchers to segment challenging images. More specifically, FeatureForest uses large foundation models to extract feature vectors corresponding to user-labeled pixels in order to train a random forest algorithm. In this manuscript, we demonstrate the usefulness of FeatureForest over various scientific datasets for which no straightforward or user-friendly algorithm exist, and the improvements it yields over classical random forest classification. We provide an implementation of FeatureForest in an open napari [<xref ref-type="bibr" rid="R17">17</xref>] plugin, as well as example scripts and notebooks to perform prediction outside napari (e.g. on clusters).</p></sec><sec id="S2"><label>2</label><title>Principle</title><p id="P9">FeatureForest replaces the classical filters of a random forest classifier with large deep-learning models (see <xref ref-type="fig" rid="F1">Fig. 1a</xref>), and extracts the feature vectors used during random forest training from the embeddings that are computed within those networks. The overall iterative training process remains otherwise similar, with users requiring a few iterations of labeling and training before obtaining desired results. FeatureForest currently includes several foundation models: SAM [<xref ref-type="bibr" rid="R10">10</xref>], MobileSAM [<xref ref-type="bibr" rid="R18">18</xref>], SAM2 [<xref ref-type="bibr" rid="R19">19</xref>] and DINOv2 [<xref ref-type="bibr" rid="R13">13</xref>] (see <xref ref-type="sec" rid="S7">Methods</xref> for a description of the feature vectors extraction process). Advanced users can extend this list and adapt the model of their choice for use in FeatureForest (see <xref ref-type="sec" rid="S7">Methods</xref>).</p><p id="P10">In <xref ref-type="fig" rid="F1">Fig. 1b</xref>, we describe the FeatureForest pipeline as available to users via the napari plugin we provide. In a first step, using the <italic>Feature Extraction</italic> widget, users extract the feature vectors corresponding to all pixels in a set of images loaded in napari from the model of their choice. The feature vectors corresponding to individual pixels are stored in an HDF5 file to allow random access during the later stages. The feature vectors are large (from 320 to 1536 features per pixel depending on the model) and their extraction slow. For instance, with SAM2 (768 features per pixel), a 512 × 512 slice requires 900 MB of storage space, and required on our system 38 seconds to be generated. Therefore, storing them once for the whole training dataset enables faster iterations when training the random forest. Then, the <italic>Segmentation Widget</italic> is used to train iteratively a random forest on the data subset, as well as perform the final segmentation. First, users select a napari layer containing their data, point to the feature vector file that was exported using the <italic>Feature Extraction</italic> widget, and select their labeling layer. Next, using the napari’s built-in labeling tools, they label a small representative set of pixels before training a random forest on these labeled pixels. Once the training done, users can segment the currently selected slice or full stack. The results can be improved by iteratively adding new labeled pixels where the trained classifier performed poorly. The training process allows rapid iteration between labeling, training and prediction. At any point, users can save the trained random forest classifier. After a few iterations, users can predict on a dataset saved on the disk, e.g. a much larger stack. Furthermore, FeatureForest includes post-processing (see <xref ref-type="sec" rid="S7">Methods</xref>), such as smoothing steps and filtering connected components based on size. Additional post-processing tools leverage SAM2 in two different ways: (i) by generating bounding boxes around instances obtained from performing watershed on the output of FeatureForest and using them as prompts for SAM2 (see <xref ref-type="fig" rid="F1">Fig. 1c</xref>), or (ii) by using the SAM2 auto-segmentation feature in which a grid of points over the image is passed to the model as prompts, and the final masks are selected by thresholding the intersection over union (IoU) between instances obtained from SAM2 and watershed-processed FeatureForest results. Using SAM2 in the post-processing step typically results in object segmentations with smoother boundaries (see <xref ref-type="fig" rid="F1">Fig. 1c</xref> insets).</p></sec><sec id="S3" sec-type="results"><label>3</label><title>Results</title><sec id="S4"><label>3.1</label><title>FeatureForest on various microscopy modalities</title><p id="P11">We applied FeatureForest to various datasets from three different imaging modalities: focused ion beam scanning electron microscopy (FIB-SEM), brightfield microscopy, and H&amp;E staining. For each dataset, we trained a classical random forest classifier using Labkit [<xref ref-type="bibr" rid="R3">3</xref>] and FeatureForest on the same training images. Additionally, we applied the SAM2 bounding-box post-processing available within FeatureForest (see <xref ref-type="sec" rid="S7">Methods</xref>). In order to quantify the segmentation performance, we computed the Dice coefficient between the resulting segmentation and the ground-truth provided in the public datasets.</p><p id="P12">FIB-SEM data typically has high contrast and dense structures, while being too large to manually label and too complex to segment using random forest pixel classifiers. <xref ref-type="fig" rid="F2">Fig. 2a</xref> shows a single slice of a fly brain imaged by FIB-SEM as well as the ground-truth masks of mitochondria and the segmentations obtained with Labkit and FeatureForest. The mitochondria appear as dark and round objects of varying intensity. While the random forest classifier is able to classify most pixels from inside the mitochondria, it also creates a high number of false positive and misses their outer membrane. In contrast to the random forest classifier, FeatureForest produces a segmentation with high coverage of the mitochondria and few false positive pixels, which is quantitatively confirmed by a Dice score of 0.56 for the random forest and 0.89 for FeatureForest. Post-processing the segmentation from FeatureForest using the bounding boxes generation and SAM2 (see <xref ref-type="fig" rid="F1">Fig. 1c</xref>) yields smoother segmentation masks and an improved Dice score of 0.93. Similar results are obtained throughout the dataset (see various slices in <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. S1</xref>) and computing the Dice score over the entire dataset shows that FeatureForest performs much better than the classical random forest (see <xref ref-type="fig" rid="F2">Fig. 2b</xref>), with mean and standard deviations of 0.87 ± 0.04 (FeatureForest), 0.91 ± 0.03 (FeatureForest + post-processing), and 0.61 ± 0.07 (random forest). In addition to the higher mean Dice score, FeatureForest also results in lower variability and less sensitivity to varying image quality.</p><p id="P13">The example dataset of <xref ref-type="fig" rid="F2">Fig. 2a</xref> is a relatively easy segmentation challenge as the mitochondrial texture is sufficiently different from the rest of the image to be well captured by classical image filters. Classical image analysis can further improve the segmentation obtained with the random forest classifier, for instance by filtering connected components by size and applying smoothing or morphological operations. In <xref ref-type="fig" rid="F2">Fig. 2c</xref>, we use another FIB-SEM dataset (human breast cancer spheroid) in which the mitochondria have similar texture to their surrounding and can only be segmented by considering their larger context and shape. Such a situation is exactly where random forest classifiers typically fail, and indeed the classical approach applied to this dataset resulted in a poor quality segmentation (Dice score of 0.20). In comparison, FeatureForest leads to the correct segmentation of the mitochondria with few spurious segmented pixels (Dice score 0.81). As before, the results can be further improved by using our post-processing (Dice score 0.87). The distribution of Dice scores over the whole dataset (500 slices) further shows that FeatureForest enables segmenting the stack with high fidelity while the random forest classifier leads to poor quality results (see <xref ref-type="fig" rid="F2">Fig. 2d</xref>), with mean and standard deviations of 0.67 ± 0.09 (FeatureForest), 0.72 ± 0.11 (FeatureForest + post-processing), and 0.13 ± 0.04 (random forest classifier). The Dice score for FeatureForest decreases throughout the stack (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. S2</xref>), mostly due to false positive segmentation within the nucleus, which could easily be removed by independently segmenting the nucleus itself and subtracting it from the mitochondria segmentation.</p><p id="P14">Next, we compared segmentation performance on data from a different imaging modality and sample type. <xref ref-type="fig" rid="F2">Fig. 2e</xref> showcases the output of Labkit and FeatureForest on an H&amp;E stained human kidney tissue. This data contains specific blood vessel structures called glomeruli. In the example from <xref ref-type="fig" rid="F2">Fig. 2e</xref>, the glomeruli are slightly darker than their surrounding and, most importantly, display a wide variety of textures. The random forest classifier is capable of approximately segmenting many glomerulus instances, but misses several of them and produces many spurious groups of segmented pixels (Dice score 0.51). Here again, FeatureForest correctly segments all structures, and its post-processing leads to smooth and complete segmented objects. The dataset was created by tiling a larger image, and some tiles are shown in <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. S3</xref>, including the recomposed image, showcasing the performance of FeatureForest. Computing the Dice score for each tile (<xref ref-type="fig" rid="F2">Fig. 2f</xref>) leads to mean and standard deviations of 0.81 ± 0.04 (FeatureForest), 0.87 ± 0.04 (FeatureForest + post-processing), and 0.52 ± 0.08 (random forest).</p><p id="P15">Because FeatureForest uses a random forest as classifier on top of the foundational model features, FeatureForest can segment multiple classes at a time. To demonstrate this, in <xref ref-type="fig" rid="F2">Fig. 2g</xref>, we segment a mouse embryo imaged in brightfield microscopy. While the cells at the center of the embryo have a vastly different texture from the rest of the image, the extraembryonic membrane of the embryo has spatially varying intensity due to shadowing and is closer to the uniform background texture. The random forest classifier performs well on the cell mass (Dice score 0.91), but is subpar on the extraembryonic membrane (0.70), leading to incomplete segmentation of the latter. Once again, the classical random forest approach also erroneously segments other structures in the image. FeatureForest produces an almost perfect segmentation with high Dice scores (0.99 for the cells, and 0.90 for the extraembryonic membrane). This is the case throughout all the test images (see <xref ref-type="fig" rid="F2">Fig. 2h</xref>), with mean and standard deviations of 0.90 ± 0.01 (FeatureForest), 0.93 ± 0.01 (FeatureForest + post-processing), and 0.66 ± 0.05 (random forest classifier). To further showcase multiclass segmentation, we also segmented a FIB-SEM dataset distinguishing 6 classes (endoplasmic reticulum, golgi, mitochondria, lysosomes, lipid droplets and nuclear envelope). FeatureForest correctly segments most objects in the images (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. S4</xref>), across a wide range of texture and shapes.</p></sec><sec id="S5"><label>3.2</label><title>FeatureForest enables biological discoveries</title><p id="P16">As we have seen, for complex datasets, the performance of classical random forest pixel classification can lead to unusable segmentation, such as the one shown in <xref ref-type="fig" rid="F2">Fig. 2c</xref>. When training deep learning networks is not possible due the ground-truth label generation requirement, FeatureForest provides a useful alternative to perform the segmentation.</p><p id="P17">This was exemplified in a recent study [<xref ref-type="bibr" rid="R20">20</xref>], in which FeatureForest was used to segment organelles in a complex symbiotic interaction between eukaryotic cells. The data consisted of large resin-embedded FIB-SEM stacks representing a dinoflagellate cell (referred to as the host). This dinoflagellate species is known to acquire and hijack organelles from its algal prey (microalga <italic>Phaeocystis antarctica</italic>), including nucleus, plastids and mitochondria, and retain them over several months.</p><p id="P18">In <xref ref-type="fig" rid="F3">Fig. 3a</xref>, we compare the manual segmentation of three classes (algal plastids, algal mitochondria and host mitochondria) with the results from FeatureForest on three different slices of a single FIB-SEM stack from [<xref ref-type="bibr" rid="R20">20</xref>] (original stack of size 3598 × 4455 × 3944 pixels, which was binned with a factor 4). The mitochondria of both the host (orange) and the algal prey (red) were segmented in two different classes in one FeatureForest model, while we trained FeatureForest again separately for the algal plastids (blue). In all cases, FeatureForest led to high quality segmentation. In particular, the plastids are accurately segmented throughout the stack. To quantify this, we manually segmented 7 test slices distributed over the whole range of the stack. We then computed the Dice score between the manual segmentation and FeatureForest + post-processing on these test slices, confirming the visual impression, with mean and standard deviations of 0.58 ± 0.06 (algal prey mitochondria), 0.64 ± 0.03 (host mitochondria), and 0.88 ± 0.02 (algal plastids) (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. S5</xref> for the distributions). Here, manually annotating 7 slices for quantification purposes was a slow process. In contrast, the trained FeatureForest classifier does not require additional inputs to segment the three classes in the 3598 slices of the entire stack. Segmentation of these organelles throughout such a large stack is essential to visualize and quantify morphological changes (e.g. changes in volume and surface of stolen organelles). The segmentation provided by FeatureForest allows building a 3D model of the distribution of organelles in space (see <xref ref-type="fig" rid="F3">Fig. 3c</xref>), a necessary step in measuring the morphometrics of the various organelles. More details on the findings of the study are available in Rao <italic>et al</italic> [<xref ref-type="bibr" rid="R20">20</xref>].</p></sec></sec><sec id="S6" sec-type="discussion"><label>4</label><title>Discussion</title><p id="P19">In this manuscript, we introduce FeatureForest, an approach leveraging foundation models to generate high-quality feature representations of pixels which are then used to train a random forest. Via our napari plugin implementation, FeatureForest provides a simple, intuitive and straightforward segmentation pipeline, combining the power of large deep learning image segmentation models with the ease of use of random forests. Crucially, these models can be applied even by researchers with no knowledge of deep learning. FeatureForest fills a gap in the landscape of segmentation tools, in particular for large and complex datasets such as electron microscopy volumes, for which the annotation effort required to assemble ground-truth for deep-learning is considerable. We provide several different foundation models for feature generation, including SAM2, the current state of the art large foundation model for segmentation, as well as the possibility for advanced users to add their own model adapter to FeatureForest. Moreoever, we also designed post-processing steps allowing further improvement in the results of FeatureForest by using its segmentation output to directly generate specific SAM2 predictions.</p><p id="P20">We benchmarked FeatureForest on multiple publicly available datasets that were published with ground-truth (or for which we could generate our own ground truth), including FIB-SEM, H&amp;E stainings and brightfield images, both for single and multi-class segmentation. We showed that not only does FeatureForest significantly improve segmentation performance on these datasets compared to a classical random forest pixel classifier, but that it also produces high quality segmentation for complex datasets for which the random forest classifier pixel results are unusable.</p><p id="P21">In our experiments, FeatureForest post-processing improved the results by leading to smoother masks. Overall, the output of SAM and SAM2 tend to follow the same image features than the segmentation obtained by FeatureForest, albeit with more complete connected components. In certain cases, post-processing with bounding box generation can lead to oversegmentation when object instances are difficult to separate using watershed in the FeatureForest segmentation result, and even in rare cases to a mask covering the entire image. In such cases, users might need to post-process these images separately with different parameters (e.g. smaller or higher number of smoothing steps), or implement their own post-processing.</p><p id="P22">Our method has several limitations that are inherent to the large deep-learning models we are using to extract feature vectors. Firstly, SAM2, SAM and DINOv2 are trained on natural images (e.g. scenes of everyday life, often RGB images) and the feature vectors they produce might not be optimized to separate the biological objects of interest. To address this, fine-tuning these models on microscopy images is an exciting perspective [<xref ref-type="bibr" rid="R11">11</xref>].</p><p id="P23">A further limitation concerns the storage and generation of feature vectors. In order to be time-efficient, feature extraction should preferentially be performed on a graphical processing unit (GPU). Without access to a GPU, users should expect the feature extraction, and the segmentation of full stacks for which features were not pre-exported, to take from minutes to hours depending on the stack size. As this is the most time consuming step, we separated the feature extraction and training steps in the napari plugins. Once FeatureForest is trained, the features are computed on the fly while segmenting an entire dataset. We therefore advise users to train on a representative substack of the image in order to minimize the footprint on disk and generation time of the feature vectors, and segment on the larger stack once they are satisfied with the results on the training stack. In addition, the more complex models have a larger memory footprint as they consist of a much larger number of parameters. For users with limited GPU memory, we also provide lighter models (e.g. MobileSAM [<xref ref-type="bibr" rid="R18">18</xref>]) that nonetheless perform well. Future updates will include further optimization of memory usage.</p><p id="P24">During the development of FeatureForest, similar approaches have been demonstrated, highlighting the usefulness of the method [<xref ref-type="bibr" rid="R21">21</xref>, <xref ref-type="bibr" rid="R22">22</xref>, <xref ref-type="bibr" rid="R23">23</xref>, <xref ref-type="bibr" rid="R24">24</xref>]. Compared to these variants, we use state of the art foundation models to generate the feature vectors (e.g. SAM, SAM2), rather than simpler and older networks such as VGG16 [<xref ref-type="bibr" rid="R25">25</xref>] or custom networks. Furthermore, we showcase FeatureForest on large and complex datasets, with focus on microscopy imaging, for which available tools for segmentation are limited.</p><p id="P25">In the future, we will continue to optimize FeatureForest in order to improve user experience, in particular with respect to speed and memory efficiency, and add newer models for feature extraction or post-processing. The source code for the napari plugin is freely and openly available on Github [<xref ref-type="bibr" rid="R26">26</xref>], and can be installed through PyPi. We also provide documentation on how to use FeatureForest, as well as scripts and notebooks examples for running FeatureForest outside napari (e.g. on high performance computing (HPC) systems). We believe that FeatureForest constitutes a promising tool for many studies that deal with complex images and for which either knowledge or time prevents researchers from training their own deep-learning segmentation algorithms.</p></sec><sec id="S7" sec-type="methods"><title>Methods</title><sec id="S8"><label>4.1</label><title>FeatureForest</title><p id="P26">FeatureForest is a Python software package and consists of convenience functions and a napari plugin. All code and documentation is accessible on Github (juglab/featureforest). The FeatureForest napari plugin contains two different widgets: <italic>Feature Extraction</italic> and <italic>Segmentation widget.</italic> The first plugin extracts feature vectors for each pixels in a selected napari layer and stores them in a HDF5 container to allow random access. The second widget allows training the random forest classifier using the previously exported feature vectors, as well as perform post-processing and segmentation of the entire dataset.</p><sec id="S9"><label>4.1.1</label><title>Models</title><p id="P27">The embeddings of deep-learning networks are often of different dimension that those of the input images. In order to obtain per image pixel features, we use patches of smaller size than the expected input size to the network and scale them up. Finally, the resulting feature vectors have smaller spatial dimensions than the original patches and are themselves also up-scaled. This forces the features to have better spatial resolution than without patching.</p><p id="P28">FeatureForest includes the following models: <italic>SAM2_Large</italic> [<xref ref-type="bibr" rid="R19">19</xref>], <italic>SAM2_Base</italic> [<xref ref-type="bibr" rid="R19">19</xref>], <italic>SAM</italic> [<xref ref-type="bibr" rid="R10">10</xref>], <italic>MobileSAM</italic> [<xref ref-type="bibr" rid="R18">18</xref>], and <italic>DINOv2</italic> [<xref ref-type="bibr" rid="R13">13</xref>]. All models are implemented by extending the <italic>BaseModelAdapter</italic> class, which allows setting patch size compatible with the specific model, as well as extracting feature vectors for each pixel provided to the model. Each model has its own implementation, as they have different input requirements and architectures.</p><p id="P29">More specifically, <italic>SAM2_Large</italic> uses “<italic>sam2.1_hiera_large.pt</italic>” as weights, while <italic>SAM2_Base</italic> corresponds to the lighter “<italic>sam2.1_hiera_base_plus.pt</italic>” (see facebookresearch/sam2 on Github). To extract SAM2 embeddings, the images are patched in overlapping patches of size equal to a power of two, and an overlap of half the patch size. The patches are then scaled to 1024x1024 before being used as inputs to SAM2. The feature vectors are computed as the concatenation of the image encoder output, leading to 768 features per pixel. The image encoder output has a spatial component, and the tensors are cropped to the non-overlapping regions and scaled back to their original size. <italic>SAM</italic> model uses “<italic>sam_vit_h_4b8939.pth</italic>”. The model differs from SAM2 by the number of output features (1536 per pixel), which are concatenated from both the encoder output and the patch embedding layer. For both SAM and SAM2, the RGB input is simply a concatenation of the same gray-level microscopy image input.</p><p id="P30"><italic>MobileSAM</italic> model uses a modified version of the <italic>TinyVIT</italic> model architecture that give access to the internal embeddings computed by the encoder. We use “<italic>mobile_sam.pt</italic>” (see ChaoningZhang/MobileSAM on Github) as weights to our modified visual transformer architecture. <italic>MobileSAM</italic> leads to the 320 features per pixel as <italic>SAM</italic>.</p><p id="P31">Finally, we use “<italic>dinov2_vits14_reg</italic>” from the PyTorch Hub for <italic>DINOv2.</italic> DINOv2 input patches of size divisible by 14. To obtain per pixel output, we create patches of size 70x70 with overlaps 28x28. The number of output features for each pixel is 384, and is the output of the model itself.</p></sec><sec id="S10"><label>4.1.2</label><title>Training</title><p id="P32">FeatureForest trains a random forest classifier using the feature vectors extracted from one of its adapted models. For each labeled pixel in the labeling layer in napari, the corresponding feature vectors are extracted, and fed along with the label number to the random forest classifier [<xref ref-type="bibr" rid="R27">27</xref>]. By default, we use 450 trees of maximum depth 9. The trained classifier can then be used to predict pixel label class for each pixels in the image or slice currently displayed in napari, or predict on the whole stack.</p></sec><sec id="S11"><label>4.1.3</label><title>Post-processing</title><p id="P33">As part of FeatureForest pipeline, we provide several post-processing options that leverage the large deep learning network used for feature generation. In any case, the first step employs mean curvature smoothing, an iterative edge-preserving smoothing method that fill small holes, and filters out small connected components. Users can change the number of smoothing iterations and the threshold used to filter out connected components by area (absolute or relative). By default, we use 25 smoothing iterations, and an absolute threshold of 50 pixels.</p><p id="P34">Subsequently, users can use either of two additional steps: <italic>SAM2ImagePredictor</italic> and <italic>SAM2AutomaticMaskGenerator.</italic> In the former, we use a watershed algorithm to separate the mask into instances. Bounding boxes are then generated around each instance, and used as prompts for SAM2. The output instances are merged into a single mask and added into napari as a layer. <italic>SamAutomaticMaskGenerator</italic> generates a evenly-spaced grid of points as prompts to SAM2, which outputs a large number of masks. We retain only instances with an intersection over union with respect to the closest connected component from the random forest segmentation larger than a user-set threshold (by default 0.35).</p></sec></sec><sec id="S12"><label>4.2</label><title>Data and analysis</title><p id="P35">For each experiment, FeatureForest was run from the commit <italic>97d880d,</italic> with the codebase being available on Github (juglab/featureforest). Unless otherwise indicated, the training and post-processing were carried out with defaults parameters. Labkit [<xref ref-type="bibr" rid="R3">3</xref>] was used as the random forest classifier. All training, analysis, and plotting were performed in Python, using the GPU conda environment provided in the source code repository, on a Linux virtual machine with access to a NVIDIA A40-16Q (16 GB) GPU.</p><p id="P36">The fly brain stack (<xref ref-type="fig" rid="F2">Fig. 2a</xref> and <xref ref-type="supplementary-material" rid="SD1">supplementary Fig.S1</xref>) is available as part of the <italic>EMPIAR-10982</italic> dataset, and consist of a stack of size 256 × 255 × 255 and an isotropic pixel size 12 nm. We use every 16 frames, starting from the first one, as training set, while prediction was performed on the whole dataset. In the figures, only images that were not used for training and are as far as possible from neighboring training slices are shown. In <xref ref-type="fig" rid="F2">Fig. 2a</xref>, slice number 72 is shown. Dice coefficients in <xref ref-type="fig" rid="F2">Fig. 2b</xref> are computed over the whole stack.</p><p id="P37">The human breast cancer spheroid stack (<xref ref-type="fig" rid="F2">Fig. 2c</xref> and <xref ref-type="supplementary-material" rid="SD1">supplementary Fig.S2</xref>) is extracted from <italic>EMPIAR-11380</italic> (sample <italic>F059</italic>_<italic>bin2</italic>) [<xref ref-type="bibr" rid="R28">28</xref>]. The stack has dimensions 1446 × 1683 × 1928 and an isotropic pixel size of 20 nm. We cropped the data to size 500 × 512 × 1024 from the top-left coordinate (390, 800, 150). Training was performed using every 30th frame, starting from the first, while prediction was performed on the whole dataset. In the figures, only images that were not used for training are shown, selecting specifically slices that are as far as possible in z from the training slices. For <xref ref-type="fig" rid="F2">Fig. 2c</xref>, we cropped the slice (slice number 435) to a square region. Dice coefficients in <xref ref-type="fig" rid="F2">Fig. 2d</xref> are computed over the whole stack.</p><p id="P38">The human kidney tissue example (<xref ref-type="fig" rid="F2">Fig. 2e</xref> and <xref ref-type="supplementary-material" rid="SD1">supplementary Fig. S3</xref>) is part of a dataset that was compiled from the Human Biomolecular Atlas Program (HuBMAP) and publicly released as part of a Kaggle challenge (<ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/c7hubmap-kidney-segmentation/data">https://www.kaggle.com/c7hubmap-kidney-segmentation/data</ext-link>). Specifically, we selected the <italic>1e2425f28</italic> sample, and used the fourth series (resolution 4027 × 3347), and cropped it to 1024 × 3072 (top-left coordinates (486, 1532)), before tiling it into a set of 512 × 512 images (<italic>N</italic> = 12). The masks were provided as instances in a json file and were converted into a binary image, before being cropped and tiled as the raw image. We trained FeatureForest on the first four frames, and predicted on the whole tile stack. We only show in the figures tiles from the range 5 – 12 (<xref ref-type="fig" rid="F2">Fig. 2e</xref> and <xref ref-type="supplementary-material" rid="SD1">supplementary Fig. S3a</xref>), and the whole crop in <xref ref-type="supplementary-material" rid="SD1">supplementary Fig. S3b</xref>. Dice coefficients in <xref ref-type="fig" rid="F2">Fig. 2f</xref> are computed over the whole tile stack.</p><p id="P39">The mouse embryo dataset (<xref ref-type="fig" rid="F2">Fig. 2g</xref>) is publicly available on the Broad Bioimage Benchmark Collection with access number <italic>BBBC003.</italic> It consists of 5 slices of a 3D brightfield stack of size 640 × 480 and pixel size 420 nm. As the initial ground truth only included the segmentation of the embryo as a single class, we manually labeled the extraembryonic membrane as a second class to generate two-label ground truth. Training was performed on the first slice, and prediction on the whole stack. The fourth slice is shown in <xref ref-type="fig" rid="F2">Fig. 2g</xref>, and is cropped to a square image. Dice coefficients in <xref ref-type="fig" rid="F2">Fig. 2h</xref> are computed over the whole stack.</p><p id="P40">The U2OS FIB-SEM dataset (<xref ref-type="supplementary-material" rid="SD1">Fig. S4</xref>) is publicly available as <italic>EMPIAR-11746</italic> [<xref ref-type="bibr" rid="R29">29</xref>], and consists of a 1168 × 3394 × 1385 stack with pixel size 2.5 nm in × and Y, and 0.5 nm in Z. We down-scaled the whole stack to a width of 1200, and used every 40 images from slice 500 as training dataset, and predicted on every 30 slice from slice 501 (test dataset). We used 6 out of the 8 classes available in the dataset ground-truth. Slices in <xref ref-type="supplementary-material" rid="SD1">Fig. S4</xref> were slightly cropped to exclude white border without information.</p><p id="P41">The dinoflagellate FIB-SEM dataset (<xref ref-type="fig" rid="F3">Fig. 3</xref>) is part of a recent publication [<xref ref-type="bibr" rid="R20">20</xref>]. It was high-pressure frozen and resin-embedded before imaging, and has dimensions 3598 × 4455 × 3944 pixels. More details about sample preparation are available in Rao <italic>et al.</italic> We binned the stack with a factor 4 (3598 × 1113 × 986 pixels) to work on a smaller stack. To obtain the segmentation of the three classes (host mitochondria, algal mitochondria and algal plastids), we trained two different FeatureForest classifiers: one to segment the two types of mitochondria, and one to segment the plastids. In both cases, we use slices 50, 275, 462, 752, 1024, 1375, 1721, 2015, 2310, 2813, and 3067 for training and predicted on every 3 slices (total number of 1200 slices). To allow for quantification, we manually labeled 7 slices (370, 650, 900, 1550, 1850, 2175, 2550) with the three classes using the SAMJ ImageJ plugin. The Dice scores were computed over this test stack. In order to visualize the segmentation, we performed segmentation and post-processing of the three organelles using the classifiers trained for <xref ref-type="fig" rid="F3">Fig. 3a</xref> on the 1200 prediction slices. The final post-processed stack was curated to re-segment with different post-processing parameters (smoothing steps 23 or 28 instead of default 25) slices 263, 626, 628, 631, 690, 839, 859, 860 and 864 (in the substack with 1200 slices) to avoid whole-slice masks. The 3D reconstruction was performed in Blender 4.2 using the microscopy nodes (Github, oanegros/MicroscopyNodes).</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary figures</label><media xlink:href="EMS202298-supplement-Supplementary_figures.pdf" mimetype="application" mime-subtype="pdf" id="d152aAcFbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S13"><title>Acknowledgements</title><p>We thank Noan Deschamps-Chevyreva for fruitful discussions and advice on data visualization. AKR and JDecelle were supported by the ANR EPHEMER and the ERC consolidator grant SymbiOCEAN (101088661). Data acquisition was possible thanks to AtlaSymbio. AtlaSymbio is funded by the Gordon and Betty Moore Foundation (grant ID GBMF11532, <ext-link ext-link-type="uri" xlink:href="https://www.moore.org/grant-detail?grantId=GBMF11532">10.37807/GBMF11532</ext-link>). MS and FJ were supported by AI4Life, and the work presented here was performed as part of the AI4Life Open Calls. AI4Life receives funding from the European Union’s Horizon Europe research and innovation programme under grant agreement number 101057970. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them.</p></ack><sec id="S14" sec-type="data-availability"><title>Data availability</title><p id="P42">All datasets are publicly available on scientific databases (EMPIAR, BBBC) or challenge platforms (Kaggle). The dataset of <xref ref-type="fig" rid="F3">Fig. 3</xref> is currently being uploaded to a public database.</p></sec><sec id="S15" sec-type="data-availability"><title>Code availability</title><p id="P43">All source code is released under BSD-3-Clause license and freely available on Github [<xref ref-type="bibr" rid="R26">26</xref>].</p></sec><fn-group><fn id="FN1" fn-type="con"><p id="P44"><bold>Authors contributions</bold></p><p id="P45">MS, FJ and JD developed the method; MS, VG and JD developed the code; AKR and JDecelle acquired data; MS, DDN, JB, and JD analysed the data; DDN, FJ and JD wrote the manuscript; FJ and JD supervised the project.</p></fn><fn id="FN2" fn-type="conflict"><p id="P46"><bold>Competing financial interests</bold></p><p id="P47">The authors declare that they have no conflict of interest.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hall</surname><given-names>M</given-names></name><etal/></person-group><article-title>The weka data mining software: an update</article-title><source>ACM SIGKDD explorations newsletter</source><year>2009</year><volume>11</volume><fpage>10</fpage><lpage>18</lpage></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berg</surname><given-names>S</given-names></name><etal/></person-group><article-title>Ilastik: interactive machine learning for (bio) image analysis</article-title><source>Nat methods</source><year>2019</year><volume>16</volume><fpage>1226</fpage><lpage>1232</lpage><pub-id pub-id-type="pmid">31570887</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arzt</surname><given-names>M</given-names></name><etal/></person-group><article-title>Labkit: labeling and segmentation toolkit for big image data</article-title><source>Front computer science</source><year>2022</year><volume>4</volume><fpage>10</fpage></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bankhead</surname><given-names>P</given-names></name><etal/></person-group><article-title>Qupath: Open source software for digital pathology image analysis</article-title><source>Sci reports</source><year>2017</year><volume>7</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="pmcid">PMC5715110</pub-id><pub-id pub-id-type="pmid">29203879</pub-id><pub-id pub-id-type="doi">10.1038/s41598-017-17204-5</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Schmidt</surname><given-names>U</given-names></name><name><surname>Weigert</surname><given-names>M</given-names></name><name><surname>Broaddus</surname><given-names>C</given-names></name><name><surname>Myers</surname><given-names>G</given-names></name></person-group><source>Cell detection with star-convex polygons</source><conf-name>Medical Image Computing and Computer Assisted Intervention–MICCAI 2018: 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part II11</conf-name><conf-sponsor>Springer</conf-sponsor><year>2018</year><fpage>265</fpage><lpage>273</lpage></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Wang</surname><given-names>T</given-names></name><name><surname>Michaelos</surname><given-names>M</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name></person-group><article-title>Cellpose: a generalist algorithm for cellular segmentation</article-title><source>Nat methods</source><year>2021</year><volume>18</volume><fpage>100</fpage><lpage>106</lpage><pub-id pub-id-type="pmid">33318659</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Baltissen</surname><given-names>D</given-names></name><etal/></person-group><source>Comparison of segmentation methods for tissue microscopy images of glioblastoma cells</source><conf-name>2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)</conf-name><conf-sponsor>IEEE</conf-sponsor><year>2018</year><fpage>396</fpage><lpage>399</lpage></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aswath</surname><given-names>A</given-names></name><name><surname>Alsahaf</surname><given-names>A</given-names></name><name><surname>Giepmans</surname><given-names>BN</given-names></name><name><surname>Azzopardi</surname><given-names>G</given-names></name></person-group><article-title>Segmentation in large-scale cellular electron microscopy with deep learning: A literature survey</article-title><source>Med image analysis</source><year>2023</year><elocation-id>102920</elocation-id><pub-id pub-id-type="pmid">37572414</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Stringer</surname><given-names>C</given-names></name></person-group><article-title>Cellpose 2.0: how to train your own model</article-title><source>Nat methods</source><year>2022</year><volume>19</volume><fpage>1634</fpage><lpage>1641</lpage><pub-id pub-id-type="pmcid">PMC9718665</pub-id><pub-id pub-id-type="pmid">36344832</pub-id><pub-id pub-id-type="doi">10.1038/s41592-022-01663-4</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kirillov</surname><given-names>A</given-names></name><etal/></person-group><source>Segment anything</source><conf-name>Proceedings of the IEEE/CVF International Conference on Computer Vision</conf-name><year>2023</year><fpage>4015</fpage><lpage>4026</lpage></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Archit</surname><given-names>A</given-names></name><etal/></person-group><article-title>Segment anything for microscopy</article-title><source>bioRxiv</source><year>2023</year><fpage>2023</fpage><lpage>08</lpage></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>R</given-names></name><name><surname>Pang</surname><given-names>K</given-names></name><name><surname>Bader</surname><given-names>GD</given-names></name><name><surname>Wang</surname><given-names>B</given-names></name></person-group><source>Maester: masked autoencoder guided segmentation at pixel resolution for accurate, self-supervised subcellular structure recognition</source><conf-name>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name><year>2023</year><fpage>3292</fpage><lpage>3301</lpage></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oquab</surname><given-names>M</given-names></name><etal/></person-group><source>Dinov2: Learning robust visual features without supervision</source><year>2024</year><elocation-id>2304.07193</elocation-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koch</surname><given-names>V</given-names></name><etal/></person-group><article-title>Dinobloom: A foundation model for generalizable cell embeddings in hematology</article-title><source>arXiv preprint arXiv:2404 05022</source><year>2024</year></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heinrich</surname><given-names>L</given-names></name><etal/></person-group><article-title>Whole-cell organelle segmentation in volume electron microscopy</article-title><source>Nature</source><year>2021</year><volume>599</volume><fpage>141</fpage><lpage>146</lpage><pub-id pub-id-type="pmid">34616042</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bharathan</surname><given-names>NK</given-names></name><etal/></person-group><article-title>Architecture and dynamics of a desmosome–endoplasmic reticulum complex</article-title><source>Nat cell biology</source><year>2023</year><volume>25</volume><fpage>823</fpage><lpage>835</lpage><pub-id pub-id-type="pmcid">PMC10960982</pub-id><pub-id pub-id-type="pmid">37291267</pub-id><pub-id pub-id-type="doi">10.1038/s41556-023-01154-4</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sofroniew</surname><given-names>N</given-names></name><etal/></person-group><article-title>napari: a multi-dimensional image viewer for Python</article-title><year>2024</year><pub-id pub-id-type="doi">10.5281/zenodo.13863809</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>C</given-names></name><etal/></person-group><article-title>Faster segment anything: Towards lightweight sam for mobile applications</article-title><source>arXiv preprint</source><year>2023</year><elocation-id>arXiv:2306 14289</elocation-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ravi</surname><given-names>N</given-names></name><etal/></person-group><article-title>Sam 2: Segment anything in images and videos</article-title><source>arXiv preprint</source><year>2024</year><elocation-id>arXiv:2408 00714</elocation-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>AK</given-names></name><etal/></person-group><article-title>Hijacking and integration of algal plastids and mitochondria in a polar planktonic host</article-title><source>bioRxiv</source><year>2024</year><fpage>2024</fpage><lpage>10</lpage></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xi</surname><given-names>E</given-names></name></person-group><article-title>Image classification and recognition based on deep learning and random forest algorithm</article-title><source>Wirel Commun Mob Comput</source><year>2022</year><year>2022</year><elocation-id>2013181</elocation-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qamar</surname><given-names>S</given-names></name><name><surname>Öberg</surname><given-names>R</given-names></name><name><surname>Malyshev</surname><given-names>D</given-names></name><name><surname>Andersson</surname><given-names>M</given-names></name></person-group><article-title>A hybrid cnn-random forest algorithm for bacterial spore segmentation and classification in tem images</article-title><source>Sci Reports</source><year>2023</year><volume>13</volume><elocation-id>18758</elocation-id><pub-id pub-id-type="pmcid">PMC10618482</pub-id><pub-id pub-id-type="pmid">37907463</pub-id><pub-id pub-id-type="doi">10.1038/s41598-023-44212-5</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Faska</surname><given-names>Z</given-names></name><name><surname>Khrissi</surname><given-names>L</given-names></name><name><surname>Haddouch</surname><given-names>K</given-names></name><name><surname>El Akkad</surname><given-names>N</given-names></name></person-group><source>Random forest for semantic segmentation using pre trained cnn (vgg16) features</source><conf-name>International conference on digital technologies and applications</conf-name><conf-sponsor>Springer</conf-sponsor><year>2023</year><fpage>510</fpage><lpage>520</lpage></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hinderling</surname><given-names>L</given-names></name><etal/></person-group><article-title>Convpaint - interactive pixel classification using pretrained neural networks</article-title><source>bioRxiv</source><year>2024</year><comment><ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/early/2024/09/14/2024.09.12.610926.full.pdf">https://www.biorxiv.org/content/early/2024/09/14/2024.09.12.610926.full.pdf</ext-link></comment><pub-id pub-id-type="doi">10.1101/2024.09.12.610926</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K</given-names></name></person-group><article-title>Very deep convolutional networks for large-scale image recognition</article-title><source>arXivpreprint</source><year>2014</year><elocation-id>arXiv:1409 1556</elocation-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Seifi</surname><given-names>M</given-names></name><name><surname>Galinova</surname><given-names>V</given-names></name><name><surname>Jug</surname><given-names>F</given-names></name><name><surname>Deschamps</surname><given-names>J</given-names></name></person-group><source>FeatureForest</source><year>2024</year><date-in-citation>accessed 5-December-2024</date-in-citation><comment>Online; <ext-link ext-link-type="uri" xlink:href="https://github.com/juglab/featureforest">https://github.com/juglab/featureforest</ext-link></comment></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><etal/></person-group><article-title>Scikit-learn: Machine learning in Python</article-title><source>J Mach Learn Res</source><year>2011</year><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>D’Imprima</surname><given-names>E</given-names></name><etal/></person-group><article-title>Light and electron microscopy continuum-resolution imaging of 3d cell cultures</article-title><source>Dev Cell</source><year>2023</year><volume>58</volume><fpage>616</fpage><lpage>632</lpage><pub-id pub-id-type="pmcid">PMC10114294</pub-id><pub-id pub-id-type="pmid">36990090</pub-id><pub-id pub-id-type="doi">10.1016/j.devcel.2023.03.001</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Czymmek</surname><given-names>KJ</given-names></name><etal/></person-group><article-title>Accelerating data sharing and reuse in volume electron microscopy</article-title><source>nature cell biology</source><year>2024</year><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="pmid">38609529</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>FeatureForest principle.</title><p>(a) A subset of the data is used to train the random forest model. First, feature vectors from a large deep-learning model corresponding to each pixel in the data are extracted. Users provides both pixel labels and feature vectors to a random forest classifier in order to train the random forest model. Once trained, the classifier can segment slices in the data. An iterative labeling, training and segmenting process is followed to improve the classifier until satisfaction. (b) Overview of the napari widgets and the various steps followed by users. (c) Post-processing with SAM2 is performed by generating bounding boxes around the connected components in the segmentation, and using them as prompts for SAM2. This results in multiple masks that are merged into a semantic segmentation mask.</p></caption><graphic xlink:href="EMS202298-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>FeatureForest outperforms classical random forest classifiers on complex datasets.</title><p>(a) FIB-SEM image of a fly brain, overlaid with mitochondria ground-truth mask, and with corresponding segmentation obtained with a random forest classifier, FeatureForest, and after post-processing the results from FeatureForest, from left to right respectively. Dice score with respect to the ground-truth for the specific slice and algorithm is indicated in the top right corner. Scale bar 500 nm. (b) Dice score distribution over the whole dataset (256 slices) presented in (a) for the random forest classifier (blue), FeatureForest (orange) and post-processed FeatureForest (green). (c) FIB-SEM image of a human breast cancer spheroid, overlaid with mitochondria ground-truth mask, and with results from a random forest classifier, FeatureForest, and FeatureForest with post-processing, as in (a). The arrows indicate incomplete segmentation of a mitochondria instance in the ground-truth that is correctly segmented by FeatureForest. Scale bar 1 μm. (d) Distribution of the Dice score corresponding to (c) over the whole dataset (500 slices). (e) H&amp;E staining of a human kidney tissue slice, overlaid with glomerulus ground-truth, and with results from a random forest classifier, FeatureForest, and FeatureForest with post-processing, as in (a). Scale bar 500 μm. (f) Distribution of the Dice score corresponding to (e) over the whole stack (12 tiles). (g) Brightfield image of a mouse embryo, overlaid with cells (orange) and extraembryonic membrane (red) ground-truth masks, and with results from a random forest classifier, FeatureForest, and FeatureForest with post-processing, as in (a). Dice score is indicated for each class. Scale bar 50 μm. (h) Distribution of the Dice score corresponding to the chorion class (red) in (g) over the whole stack (5 slices).</p></caption><graphic xlink:href="EMS202298-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Segmentation of plankton organelles from a FIB-SEM stack using Featureforest.</title><p>(a) Three different slices (out of 3598) of a dinoflagellate cell imaged in FIB-SEM, overlaid with manual segmentation, and post-processed FeatureForest. The segmentation masks consist of three classes: algal plastids (blue), algal mitochondria (red) and host mitochondria (orange). Dice score between the ground truth and FeatureForest + post-processing is indicated on the top right corner for each class. Scale bar 4 μm. (b) 3D reconstruction of the three classes - algal plastids, algal mitochondria, and host mitochondria - of (a) throughout the entire dataset.</p></caption><graphic xlink:href="EMS202298-f003"/></fig></floats-group></article>