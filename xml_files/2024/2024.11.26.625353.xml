<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS201300</article-id><article-id pub-id-type="doi">10.1101/2024.11.26.625353</article-id><article-id pub-id-type="archive">PPR945810</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Modeling speech adaptation to altered sensory feedback through continuous learning of internal sensory predictions</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Elie</surname><given-names>Benjamin</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Šimko</surname><given-names>Juraj</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Turk</surname><given-names>Alice</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib></contrib-group><aff id="A1"><label>1</label>Linguistics &amp; English Language, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01nrxwf90</institution-id><institution>The University of Edinburgh</institution></institution-wrap>, <city>Edinburgh</city>, <state>Scotland</state>, <country country="GB">United Kingdom</country></aff><aff id="A2"><label>2</label>Faculty of Arts, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/040af2s02</institution-id><institution>University of Helsinki</institution></institution-wrap>, <city>Helsinki</city>, <country country="FI">Finland</country></aff><author-notes><corresp id="CR1">
<label>*</label>corresponding author: <email>benjamin.elie@ed.ac.uk</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>28</day><month>11</month><year>2024</year></pub-date><pub-date pub-type="preprint"><day>26</day><month>11</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">The paper presents a version of an optimization-based model of speech production that reproduces key acoustic and articulatory features of motorsensory adaptation to altered sensory feedback. In the presented approach, the mechanism of motorsensory adaptation is based on regular updates, based on the sensory feedback perceived by the speaker, of two of the speakers’ internal models used for computing (near)-optimal articulation. These internal models, modeled as separate Artificial Neural Networks, are 1) a model that predicts the acoustic consequences of motor (articulatory commands) and 2) a model that predicts the somatosensory sensations from given motor commands. The paper presents simulations of adaptation experiments that successfully reproduce key acoustic and articulatory features of motorsensory adaptation of speech to altered sensory feedback. These include gradual and incomplete motorsensory adaptation when the auditory (or the somatosensory) feedback is suddenly altered (F1-shifted for the altered auditory feedback, forced jaw movement for altered somatosensory feedback). The presented simulations also show that the rate and magnitude of adaptation behavior depend on a small number of parameters. Variation in the values of these parameters can potentially explain inter-speaker differences in terms of adaptation behavior, including sensory preference.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Turk &amp; Shattuck-Hufnagel [<xref ref-type="bibr" rid="R1">1</xref>, <xref ref-type="bibr" rid="R2">2</xref>] recently proposed an XT/3C (Phonology-Extrinsic-Timing-Based, Three-Component) approach to modeling speech production based on symbolic phonological representations and phonology-extrinsic timing. It has 3 processing Components: 1) Phonological Planning, 2) Phonetic Planning, and 3) Motor-Sensory Implementation. We have recently begun developing a computational implementation of the Phonetic Planning Component, which assumes an Optimal Control based process that finds the lowest cost movements that accomplish phonological goals (cf. [<xref ref-type="bibr" rid="R3">3</xref>–<xref ref-type="bibr" rid="R5">5</xref>]). We have shown that this approach can account for various aspects of speech, including durational and spatial correlates of prominence, including reduction and centralization of unstressed vowels and lenition of consonants [<xref ref-type="bibr" rid="R6">6</xref>], different levels of hypo and hyper-articulation [<xref ref-type="bibr" rid="R6">6</xref>–<xref ref-type="bibr" rid="R8">8</xref>], and articulatory and acoustic effects of Lombard speech [<xref ref-type="bibr" rid="R9">9</xref>].</p><p id="P3">In the current paper, our goal is to model sensorimotor compensation and adaptation within this framework. When exposed to perturbed auditory feedback, speakers often compensate on the next production by modifying their speech production in the direction opposite to the perturbations. Compensation is usually incomplete, and takes a number of trials before it stabilizes. Compensatory behavior has been experimentally observed when the feedback is formant-shifted [<xref ref-type="bibr" rid="R10">10</xref>–<xref ref-type="bibr" rid="R24">24</xref>], F0-shifted [<xref ref-type="bibr" rid="R25">25</xref>–<xref ref-type="bibr" rid="R31">31</xref>], or temporally altered [<xref ref-type="bibr" rid="R32">32</xref>]. After alterations to auditory feedback are turned off, speakers’ productions do not return to pre-perturbation patterns immediately, but instead do so gradually. Articulatory compensation has also been observed when somatosensory feedback is altered [<xref ref-type="bibr" rid="R13">13</xref>, <xref ref-type="bibr" rid="R33">33</xref>–<xref ref-type="bibr" rid="R35">35</xref>]. These findings show that sensory feedback plays a role in speech production and speech articulatory planning.</p><p id="P4">This study will allow us to begin to address the third processing component of XT/3C [<xref ref-type="bibr" rid="R1">1</xref>, <xref ref-type="bibr" rid="R2">2</xref>], namely the Motor-Sensory Implementation Component, and to further develop the Phonetic Planning component to account for adaptation behavior. The Motor-Sensory Implementation Component is envisaged as the stage when productions actually occur, and are monitored and adapted to ensure that as far as possible, the goals for the utterance are met. Compensatory behavior occurs in reaction to sensory effects of produced speech, and must involve mechanisms in the Motor-Sensory Implementation Component that monitor the (altered) feedback. Experience with this feedback in turn affects the plans for subsequent productions, namely plans that are made in the hypothesized Phonetic Planning component in XT/3C [<xref ref-type="bibr" rid="R1">1</xref>, <xref ref-type="bibr" rid="R2">2</xref>].</p><p id="P5">We begin with a brief review of existing approaches to modeling compensation and adaptation behavior, which broadly speaking, can be classified as either correction-based, or based on adaptation of internal models of relationships among articulation, sensory consequences, and phonemic goals.</p><p id="P6">We then present a more detailed view of the compensation and adaptation behavior that we propose to model, followed by our adaptation-based account of this behavior. We provide simulations that show that XT/3C can provide an account using an architecture that is independently motivated by XT/3C’s assumption that speakers intend to signal acoustic cues to phonological categories, and by XT/3C’s optimization approach to Phonetic Planning. However, the internal models that we previously used to account for phonetic behavior in [<xref ref-type="bibr" rid="R6">6</xref>–<xref ref-type="bibr" rid="R9">9</xref>] must be supplemented with internal models that relate articulation to somatosensation (as e.g., in [<xref ref-type="bibr" rid="R36">36</xref>–<xref ref-type="bibr" rid="R38">38</xref>]), as well as somatosensation to phoneme recognition likelihood [<xref ref-type="bibr" rid="R36">36</xref>, <xref ref-type="bibr" rid="R37">37</xref>].</p><p id="P7">Finally, we compare fits of our model predictions to experimental data with those of SimpleDIVA [<xref ref-type="bibr" rid="R39">39</xref>], which provides a correction-based approach. We show that results of the two approaches are very similar.</p><sec id="S2"><title>Brief review of existing approaches</title><p id="P8">Several existing models are able to reproduce observed compensation and adaptation behavior. These models provide hypotheses about the mechanisms that speakers use in producing this behavior. Available models include SimpleDIVA [<xref ref-type="bibr" rid="R39">39</xref>], derived from the DIVA model [<xref ref-type="bibr" rid="R40">40</xref>–<xref ref-type="bibr" rid="R42">42</xref>], Bayesian GEPPETO [<xref ref-type="bibr" rid="R36">36</xref>, <xref ref-type="bibr" rid="R37">37</xref>, <xref ref-type="bibr" rid="R43">43</xref>], derived from the GEPPETO model [<xref ref-type="bibr" rid="R44">44</xref>], and FACTS [<xref ref-type="bibr" rid="R38">38</xref>, <xref ref-type="bibr" rid="R45">45</xref>].</p><p id="P9">Both DIVA and FACTS provide <italic>correction-based</italic> mechanisms: they use auditory and somatosensory feedback to monitor and update feedforward commands, where the update is based on a comparison between the target sensory feedback (either auditory or somatosensory) and received sensory feedback. Feedforward commands are thus corrected (updated) when discrepancies are detected between expected sensory (somatosensory and auditory) targets and sensory feedback. The correction is proportional to the difference between the target (or predicted) sensory feedback and the actual sensory feedback received by the speaker. FACTS [<xref ref-type="bibr" rid="R38">38</xref>, <xref ref-type="bibr" rid="R45">45</xref>] is similar to DIVA [<xref ref-type="bibr" rid="R40">40</xref>–<xref ref-type="bibr" rid="R42">42</xref>] in that it uses an error detection mechanism to update feedforward commands, but because it uses an internal model of the relationship between an efference copy of motor commands and their sensory consequences, it additionally updates the somatosensory and auditory prediction models when an error is detected. The correction gain is based on the prediction error.</p><p id="P10">Bayesian GEPPETO [<xref ref-type="bibr" rid="R36">36</xref>, <xref ref-type="bibr" rid="R37">37</xref>, <xref ref-type="bibr" rid="R43">43</xref>] provides a mechanism that is solely based on <italic>adaptation.</italic> Bayesian GEPPETO is a probabilistic generative model that generates feedforward motor commands by sampling from a distribution of motor commands associated with the auditory and somatosensory characterization of target phonemes. The probability distribution from which the feedforward motor commands are sampled is the product of several probability distributions, including the probability of auditory and somatosensory consequences given motor commands, the probability of acoustic and somatosoensory characteristics given the target phonemes, and matching constraint functions that are used to compare the sensory-motor predictions and the sensory characterization of phonemes. In order to model adaptation behavior using Bayesian GEPPETO, the authors in [<xref ref-type="bibr" rid="R36">36</xref>, <xref ref-type="bibr" rid="R37">37</xref>] assumed that the probability distributions of the auditory and somatosensory consequences given a set of motor commands are updated based on discrepancies between predicted and perceived sensory feedback. As a consequence, this affects the probability distribution of feedforward motor commands associated with each phoneme, yielding new acoustic realizations of target phonemes. Here, adaptation occurs because motor commands chosen from the updated probability distribution are more likely to generate auditory sensations that correspond to the characterization of the target phoneme in the auditory space. This is because the internal motor command to auditory (or somatosensory) sensation has been updated and has learned the new (artificial) articulatory to sensory mapping the sensory perturbation.</p></sec><sec id="S3"><title>Modeling framework</title><p id="P11">In this paper, we present an adaptation-based account of compensation and adaptation behavior, that does not involve explicit detection of a discrepancy between predicted and actual sensory feedback to update feedforward commands and internal models of sensory predictions in order to correct for the discrepancy. Instead, our approach is a purely <italic>adaptation-based</italic> approach, involving continuous, permanent, and automatic learning and adaptation to an evolving environment, which is conceptually similar to the framework provided in the Bayesian GEPPETO papers [<xref ref-type="bibr" rid="R36">36</xref>, <xref ref-type="bibr" rid="R37">37</xref>]. The main difference between our approach and GEPPETO is that the adaptation model we propose is based on Optimal Control Theory (OCT) [<xref ref-type="bibr" rid="R46">46</xref>, <xref ref-type="bibr" rid="R47">47</xref>]: We assume that speakers plan articulatory movements that satisfy conflicting linguistic and extra-linguistic requirements [<xref ref-type="bibr" rid="R6">6</xref>–<xref ref-type="bibr" rid="R9">9</xref>], that is, that speakers minimize a cost function that includes weighted costs of effort and intelligibility<sup><xref ref-type="fn" rid="FN1">1</xref></sup>.</p><p id="P12">The version of our optimization approach presented here includes an internal forward model that predicts the somatosensory consequences of articulatory commands, in addition to the internal forward model predicting the acoustic consequences of articulatory commands included in the previous versions. We show that compensation and adaptation behavior can be accounted for through automatic updating of the assumed internal models of the relationship between articulation and sensory consequences, both acoustic and somatosensory. This type of internal model is independently motivated within our optimization approach; internal models which predict sensory consequences of movement are required because planned articulations are evaluated according to intelligibility costs related to the prediction of recognition of target phonemes which in turn are based on the prediction of acoustics and somatosensory sensations of the candidate motor commands. As such, for both the auditory and somatosensory branches, we assume a 2-part mapping: 1) between motor command and the corresponding sensory consequences (acoustics for the auditory branch, somatosensation for the somatosensory branch), and 2) between the corresponding sensory consequences and predicted probability of recognition of target phonemes. We assume these two types of internal models because they may be learned at different times of life, possibly at different rates, and possibly based on different information sources. For example, the models that relate motor commands to sensory consequences can start developing as soon as a baby begins to vocalize, and are based exclusively on the speaker’s own experience; on the other hand, the model of the relationship between acoustics and the probability of recognition might only start developing once the infant starts forming sound categories, and could potentially include information from other talkers (e.g. analogous to an exemplar-based, hybrid model of speech perception [<xref ref-type="bibr" rid="R51">51</xref>–<xref ref-type="bibr" rid="R53">53</xref>]).</p><p id="P13">The proposed overall cost function <italic>J</italic> to be minimized is as follows: <disp-formula id="FD1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:mi>J</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>x</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>x</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mtext>x</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mtext>x</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P14">where <italic>E</italic>, <italic>R<sub>A</sub></italic> and <italic>R<sub>S</sub></italic> are the articulatory effort cost, and the predicted recognition costs associated with acoustics and somatosensory sensations, respectively. All are functions of the vector x that contains articulatory information, and the optimization task is to find the vector x that minimizes overall cost <italic>J</italic>(x).</p><p id="P15">The relative importance of the often mutually conflicting requirements enforced by these cost components is specified by the weights <italic>α<sub>E</sub></italic>, <italic>α<sub>A</sub>,</italic> and <italic>α<sub>S</sub></italic>, assigned to the least effort and the maximal auditory and somatosensory based intelligibility requirements, respectively. The recognition costs <italic>R<sub>A</sub></italic>(<italic>x</italic>) and <italic>R<sub>S</sub></italic>(<italic>x</italic>) in the cost function J(x) capturing a <italic>non</italic>-intelligibility of the utterance are computed using, firstly, internal forward models that predict the acoustic (for <italic>R<sub>A</sub></italic>) and somatosensory (for <italic>R<sub>S</sub></italic>) consequences of motor commands (included in x), and secondly, probabilistic models that associate a probability of recognition of the target phoneme given the predicted acoustic (for <italic>R<sub>A</sub></italic>) and somatosensory (for <italic>R<sub>S</sub></italic>) vectors.</p><p id="P16">In order to model sensorimotor adaptation of speech to altered auditory feedback, the optimization-based approach needs to change either the weights assigned to the different costs used in <xref ref-type="disp-formula" rid="FD1">Eq. (1)</xref> and/or modify the models used to compute <italic>E</italic>(<italic>x</italic>), <italic>R<sub>A</sub></italic>(<italic>x</italic>), and <italic>R<sub>S</sub></italic>(<italic>x</italic>). This is because, without modification of <xref ref-type="disp-formula" rid="FD1">Eq. (1)</xref> or its parameter values, and assuming that the speaker has no information about the new relationship between produced acoustic (or somatosensory) and altered sensory feedback during the adaptation phase in sensorimotor adaptation experiments, the solution that minimizes <xref ref-type="disp-formula" rid="FD1">Eq. (1)</xref> would remain the same, that is, with no adaptation behavior. In this paper, we propose an approach which is based on automatic updates of the speaker’s internal forward models, namely those used to compute the cost function in <xref ref-type="disp-formula" rid="FD1">Eq. (1)</xref>, based on the speaker’s sensory feedback. In this approach, updating the model of the internal relationship between articulation and sensory characteristics would mean that candidate motor commands would have different Recognition costs; optimal articulations are therefore likely to change.</p></sec><sec id="S4"><title>Modeling objectives</title><p id="P17">The features of sensorimotor adaptation that we attempt to reproduce with our approach are detailed in this section along with possible accounts of these phenomena. These include gradual adaptation (Objective A), incomplete compensation (Objective B), interspeaker variability (Objective C), changes to unaltered formants during adaptation (Objective D), and adaptation to somatosensory perturbations (Objective E). Gradual adaptation and incomplete compensation are illustrated in <xref ref-type="fig" rid="F1">Figure 1</xref>.</p></sec><sec id="S5"><title>Objective A: Gradual adaptation</title><p id="P18">The process of motor-sensory adaptation is generally not immediate. As shown in the left panel of <xref ref-type="fig" rid="F1">Figure 1</xref>, which presents experimental data from [<xref ref-type="bibr" rid="R21">21</xref>], once sensory feedback is altered it takes several trials for subjects to reach their maximal adaptation response. The maximal adaptation response is the production that generates altered sensory feedback closest to the sensory feedback produced before sensory perturbation was applied. In <xref ref-type="fig" rid="F1">Figure 1</xref>, maximal adaptation occurs around trial 70. For speech, gradual adaptation is observed, for instance, in auditory perturbation experiments in [<xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R14">14</xref>, <xref ref-type="bibr" rid="R16">16</xref>, <xref ref-type="bibr" rid="R21">21</xref>, <xref ref-type="bibr" rid="R24">24</xref>, <xref ref-type="bibr" rid="R54">54</xref>]. The left panel of <xref ref-type="fig" rid="F1">Figure 1</xref> shows that it took around 10 trials (10 target word productions) after the beginning of the auditory perturbation phase for speakers to produce an adaptation response close to the maximal adaptation response.</p><p id="P19">This gradual adaptation in auditory perturbation based experiments has been reproduced by FACTS [<xref ref-type="bibr" rid="R45">45</xref>] and SimpleDIVA [<xref ref-type="bibr" rid="R39">39</xref>]. In SimpleDIVA, the speed at which speakers tend toward their maximal adaptation response is directly included in the model and specified by the learning rate parameter. This learning rate parameter corresponds to the proportion, assumed to be constant throughout the experiments, of the feedback-based corrective command to be added to the new feedforward command. In FACTS [<xref ref-type="bibr" rid="R45">45</xref>], realistic adaptation responses were achieved by using adaptive Kalman filters to control and monitor the gain to apply to the task state correction, i.e. the correction applied to the estimate of the current state of the articulators, when sufficiently large discrepancies (higher than a specific threshold) between the predicted and the actual received sensory feedback were detected. Similarly to the role of the learning rate parameter in SimpleDIVA [<xref ref-type="bibr" rid="R39">39</xref>], the adaptive Kalman filter gains impacts the adaptation rate: faster adaptation rates can be achieved in FACTS with larger adaptive Kalman filter gains, simply because more correction is applied to the updated task state. Note that unlike SimpleDIVA, the modified version of FACTS in [<xref ref-type="bibr" rid="R45">45</xref>] allows feedback-based correction gains to vary during the experiments: in FACTS, the gain can be increased when the discrepancy between predicted and received sensory feedback is higher than a pre-defined threshold. This increase in feedback-based correction gains allows larger and faster update in case of large and obvious detected prediction “error”.</p><p id="P20">In this paper, we propose an approach that does not require a comparison between predicted and actual sensory feedback. Our approach uses an automatic and permanent update of the speaker’s internal models that predict the acoustic and articulatory consequences of motor commands. Our assumption is that building these internal models is part experience with vocalization, including speech: these internal models are continuously trained and updated during the speaker’s lifetime based on production and sensory feedback, regardless of whether the model predictions are accurate. Consequently, during adaptation to altered auditory feedback, the motor command to acoustics internal model learns the new (artificial) relationship between motor commands and the acoustic consequences. We hypothesize that the completion of the learning process may take several trials to complete, resulting in gradual adaptation. Similarly, when the feedback perturbation is removed, we expect our approach to account for a gradual return to the baseline.</p></sec><sec id="S6"><title>Objective B: Incomplete compensation</title><p id="P21">Experimental studies reported that compensation to feedback perturbations is usually incomplete (see e.g. [<xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R19">19</xref>, <xref ref-type="bibr" rid="R21">21</xref>, <xref ref-type="bibr" rid="R23">23</xref>, <xref ref-type="bibr" rid="R54">54</xref>]). Kitchen <italic>et al.</italic> [<xref ref-type="bibr" rid="R54">54</xref>] reported compensation magnitudes that range from 20-40% of the auditory perturbation. In the particular example presented in the left panel of <xref ref-type="fig" rid="F1">Figure 1</xref>, using data from [<xref ref-type="bibr" rid="R21">21</xref>], the compensation ratio (defined as the ratio between compensation magnitude and perturbation magnitude) is 37.7%.</p><p id="P22">In FACTS, as in [<xref ref-type="bibr" rid="R45">45</xref>], incomplete compensation is explained by convergence between actual and predicted auditory output. More precisely, the (altered) auditory feedback and the predicted auditory output converge during the adaptation phase because 1) the altered auditory feedback goes towards the baseline because of the speaker’s compensation and 2) internal models used to predict the auditory output from motor commands are updated so that new predictions go in the same direction as the perturbation, leading to an eventual convergence. For instance, for a raised-F1 type of perturbation, new predictions from the updated internal model will estimate higher F1 than before perturbation, because the earlier predictions were evaluated as erroneously too low. Subsequently, the feedforward command will be updated in order to produce lower F1 to compensate for the unexpectedly high F1 perceived under altered auditory feedback. As a result, the F1 perceived altered auditory feedback will be lower on a subsequent trial. This gradually increasing predicted F1 and the gradually decreasing perceived F1 eventually converge and meet somewhere in the F1 space between the baseline and the altered baseline. This convergence nullifies the discrepancies between predicted and actual auditory feedback before full compensation is reached, removing the need for further adaptation by the speaker and yielding incomplete compensation.</p><p id="P23">Since our approach does not use a direct comparison between sensory prediction and received sensory feedback, this convergence mechanism does not apply. Instead, we rely on updates of internal models that relate motor commands to sensory consequences. This approach requires models that relate both 1) motor commands to acoustics, and 2) motor commands to somatosensation. Because our previous model had a predicted recognition cost that determined the probability of phoneme recognition based only on its acoustics consequences, as in [<xref ref-type="bibr" rid="R6">6</xref>–<xref ref-type="bibr" rid="R9">9</xref>], there would be no limit to compensation, as the auditory internal model would learn the artificially modified articulatory to acoustic mapping and eventually find the solution that minimizes the cost function with a predicted acoustic output that corresponds to the baseline. This purely auditory-based kind of recognition optimization is analogous to the purely auditory planning process in Bayesian GEPPETO [<xref ref-type="bibr" rid="R37">37</xref>], which also predicts full compensation because of the absence of any constraint on the somatosensory characterization of target phonemes. In order to obtain incomplete adaptation, we propose to include somatosensory information in the cost function. In doing so, we predict that the solution that minimizes the cost function will be a trade-off between full acoustic compensation (satisfying the auditory objective) and remaining close to the somatosensory sensations associated to the target phonemes (satisfying the somatosensory objective). This solution is inspired by Bayesian GEPPETO [<xref ref-type="bibr" rid="R37">37</xref>], in which incomplete compensation is achieved using an extension of the purely auditory planning process, namely the fusion planning process, which uses both the auditory and somatosensory characterization of target phonemes. That is, incomplete adaptation is explained by a balance of somatosensory and auditory adaptation: the articulatory solution for complete adaptation would generate somatosensory feedback too far from the expected one. In this paper, we propose to add an extra cost in the cost function that accounts for the speaker’s somatosensory characterization of phonemes, as in <xref ref-type="disp-formula" rid="FD1">Eq. (1)</xref>.</p></sec><sec id="S7"><title>Objective C: Inter-speaker variability</title><p id="P24">Sensorimotor adaptation abilities have been shown to be strongly speaker dependent, even among neurotypical speakers with similar cognitive characteristics and language backgrounds [<xref ref-type="bibr" rid="R23">23</xref>, <xref ref-type="bibr" rid="R24">24</xref>, <xref ref-type="bibr" rid="R54">54</xref>]. Speakers also tend to compensate more or less depending on the nature of the sensory perturbation. This has been called <italic>sensory preference,</italic> reported in [<xref ref-type="bibr" rid="R13">13</xref>]: Some speakers compensate more for the auditory perturbations while others compensate more for somatosensory perturbations. In addition to these inter-speaker sources of variability, many studies have found that adaptation behaviors can differ across groups of speakers, based on age [<xref ref-type="bibr" rid="R24">24</xref>], auditory acuity [<xref ref-type="bibr" rid="R19">19</xref>, <xref ref-type="bibr" rid="R23">23</xref>], language (L1 or L2 speakers) [<xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R23">23</xref>], cognitive ability [<xref ref-type="bibr" rid="R23">23</xref>], speech disorders (e.g. stuttering [<xref ref-type="bibr" rid="R21">21</xref>] and speech sound disorder [<xref ref-type="bibr" rid="R24">24</xref>]), or Parkinson’s disease [<xref ref-type="bibr" rid="R14">14</xref>].</p><p id="P25">Two variants of Bayesian GEPPETO have been proposed in [<xref ref-type="bibr" rid="R37">37</xref>] to model sensory preference. One approach specifies sensory preference by modulating the relative precision of the characterization of speech motor goals in both the auditory and the somatosensory spaces, namely by narrowing or widening the target regions of phonemes in their respective sensory spaces. More precisely, if the target regions are, let say, much narrower in the auditory space, compared to the target regions in the somatosensory space, perturbations in the auditory space will result in larger involvement of the auditory pathway in the planning process, and, conversely, less involvement of the somatosensory pathway. This is because large distributions (such as in the somatosensory space, in this case) allow for greater deviation from the center of the distribution. The other approach specifies sensory preference by modulating the matching constraints of the auditory and somatosensory pathways when comparing motor-sensory predictions and sensory characterization of phonemes. More precisely, the modulation of these matching constraints as for effect to allow more or less deviation from the center of the distribution of the target phoneme in the considered sensory space. For instance, if the matching constraints allows larger deviations in somatosensory space than in auditory space, it is expected than the speaker will compensate more for auditory perturbations than for somatosensory perturbations. Both approaches lead to very similar results.</p><p id="P26">Since SimpleDIVA has been designed to fit experimental data (and their variability), it naturally accounts for inter-speaker variability. Its use on experimental data showed that inter-speaker variability is explained in the SimpleDIVA framework by variability in feedback-based correction gains applied to the auditory and the somatosensory subsystems [<xref ref-type="bibr" rid="R24">24</xref>, <xref ref-type="bibr" rid="R39">39</xref>].</p><p id="P27">As shown in our previous papers [<xref ref-type="bibr" rid="R6">6</xref>–<xref ref-type="bibr" rid="R9">9</xref>], modifying the weights of the cost function, as in <xref ref-type="disp-formula" rid="FD1">Eq. (1)</xref>, results in different characteristics of the output speech. A first source of inter-speaker variability in terms of adaptation behavior may then stem from different weights encoded in each speaker’s speech system. In addition, we hypothesize that characteristics of the internal sensory prediction models are speaker-dependent and explain the observed inter-speaker variability. These potentially different characteristics include the architecture and sizes of the Artificial Neural Networks that define the internal models, the amount of training data (exposition to language), accuracy of the perception of sensory feedback (e.g., poor auditory or somatosensory acuity lead to poor performance of the internal model because of inaccurate training data), as well as the model’s learning rate.</p></sec><sec id="S8"><title>Objective D: Changes in unaltered formants during adaptation</title><p id="P28">Most experimental studies about adaptation to formant-shifted auditory feedback only analyzed the modification of the production of the altered formant or formants, and ignored the behavior of unaltered formants. That is, most studies have reported variations across trials of only F1 for F1-shifted auditory feedback. The small number of studies which have also measured the behavior of other formants have reported that these other formants were also impacted during adaptation [<xref ref-type="bibr" rid="R11">11</xref>, <xref ref-type="bibr" rid="R14">14</xref>, <xref ref-type="bibr" rid="R16">16</xref>, <xref ref-type="bibr" rid="R29">29</xref>]. Basically, F1-raised auditory feedback induced both F1 lowering and F2 raising. However, changes in unaltered formants are not always observed: no systematic change in F2 was observed in [<xref ref-type="bibr" rid="R55">55</xref>] for F1-raised auditory feedback among 18 participants that adapted to altered F1 auditory feedback.</p><p id="P29">These observations cannot be explained by purely acoustic target based models if based on single formant targets, such as in SimpleDIVA [<xref ref-type="bibr" rid="R39">39</xref>] or the forward/inverse model in [<xref ref-type="bibr" rid="R56">56</xref>]. This is because these models assume that the formant targets which are not altered remain the same during the adaptation process. As a result, they predict that planned articulatory commands will compensate for formants which have been altered, but not for other, unaltered, formants.</p><p id="P30">This aspect is not explicitly addressed in the studies using Bayesian GEPPETO, such as in [<xref ref-type="bibr" rid="R36">36</xref>, <xref ref-type="bibr" rid="R37">37</xref>]. However, the results of the simulations presented in [<xref ref-type="bibr" rid="R37">37</xref>] suggest that, in the presence of F1-shifted auditory feedback, the predicted acoustic output of the fusion model exhibits a change in both F1 and F2. However, the authors do not discuss this question. FACTS [<xref ref-type="bibr" rid="R45">45</xref>] is able to successfully reproduce similar changes in F1 and F2 to the adaptation behavior experimentally observed in [<xref ref-type="bibr" rid="R14">14</xref>]. When discussing these results, the authors provide possible explanations for mechanisms that could affect non-altered formants during adaptation. One possible explanation discussed by the authors is that targets, in FACTS [<xref ref-type="bibr" rid="R38">38</xref>], are defined in the vocal tract constriction location and degree task space (namely as tract variables defined by the gestural score). Updating the articulatory constriction tasks during adaptation yield simultaneous changes in all formants because modifying articulatory configurations affects all formants simultaneously. In addition, there are biomechanical constraints on articulatory configurations that could prevent speakers from modifying a single formant independently of the others.</p><p id="P31">Our approach uses symbolic phonological representations as goals: Articulatory movements are planned to satisfy conflicting tasks of phoneme recognition and low articulatory effort. Formant-shift based perturbation of auditory feedback will result in modification of the probability of recognition of the target phonemes (as internalized by the speaker). As such, formants are not fully specified as targets: we expect compensation involves finding new articulatory configurations that generate target phonemes (as heard by the speaker) which are as likely to be recognized as before the perturbation and which still satisfy the trade-off between intelligibility and low articulatory effort. Consequently, there is no hard constraint on the unaltered formant frequencies and these can potentially be modified during the compensation process.</p></sec><sec id="S9"><title>Objective E: Adaptation to somatosensory perturbations</title><p id="P32">One objective of this paper is also to show that our model is also able to simulate the kind of sensimotor adaptation observed in [<xref ref-type="bibr" rid="R13">13</xref>] using the same architecture as for the adaptation to altered auditory feedback. Since our approach assumes symmetric auditory and somatosensory subsystems, simulation of adaptation to either (or both) auditory and somatosensory feedback perturbations should be very similar.</p></sec><sec id="S10"><title>An overview of our optimization-based model of speech production</title><p id="P33">The proposed approach is based on the assumption of a phonetic planning process that derives articulatory patterns that balance requirements of minimizing movement costs (articulatory effort in this paper, but time could also be included) while maximizing intelligibility of the produced utterance. During the optimization process, intelligibility is estimated using internal predictions of both acoustic and somatosensory consequences of a multitude of candidate, not yet realized articulatory actions. The presence of internal models that estimate recognition likelihood by a listener is thus a necessary consequence of the assumption of an online optimization process that forms the basis of the Phonetic Planning component in XT/3C.</p><p id="P34">The acoustic and somatosensory consequences of articulatory actions are subsequently used to predict probabilities of recognition of the planned phonemes by a listener. The mapping between sensory characteristics and probability of recognition is assumed to be fixed in the present version. In our current approach, the internal models mapping articulation to acoustic and somatosensory sensations are updated after each produced utterance based on the actual percepts of the speaker. This is similar to the approach presented in FACTS [<xref ref-type="bibr" rid="R45">45</xref>], where the definitions of task dynamics are updated when there is a discrepancy between predicted and actual output. However, unlike FACTS, we do not assume any specific error detection mechanism. Instead, the prediction models relating motor commands to sensation are updated after each production. Any alteration of perceptual (acoustic or somatosensory) feedback will thus result in a change of the relevant model, leading to a different output of the optimization process in subsequent steps. When, for example, speech output is modified by an experimenter by artificially shifting a formant upwards, the speaker will gradually internalize this change, and will compensate for the modification in subsequent utterances. So, unlike SimpleDIVA and FACTS models that account for adaptation phenomena through explicit modification of actual feedforward commands, our approach uses updates of internal models that are used to derive the optimal articulatory realizations.</p><p id="P35">Like GEPPETO, we define two internal forward models that map motor commands onto sensory spaces, namely an acoustic (auditory) model and a somatosensory model. The acoustic model, denoted <inline-formula><mml:math id="M2"><mml:mi mathvariant="script">A</mml:mi></mml:math></inline-formula> is used to predict the acoustic consequences of motor commands. The somatosensory model, denoted <inline-formula><mml:math id="M3"><mml:mi mathvariant="script">S</mml:mi></mml:math></inline-formula>, is used to predict the somatosensory sensations that result from motor commands. This yields the following relationships: <disp-formula id="FD2"><label>(2)</label><mml:math id="M4"><mml:mrow><mml:msub><mml:mtext>a</mml:mtext><mml:mrow><mml:mtext>pred</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="script">A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>x</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> <disp-formula id="FD3"><label>(3)</label><mml:math id="M5"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>s</mml:mi></mml:mstyle><mml:mrow><mml:mtext>pred </mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="script">S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>x</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P36">where a<sub>pred</sub> and s<sub>pred</sub> are the predicted acoustic and somatosensory vectors, respectively, given the vector of articulatory commands x. In our approach, the predicted sensory vectors a<sub>pred</sub> and s<sub>pred</sub> are subsequently used to estimate the probability of recognition of target phonemes.</p><p id="P37">For the sake of simplicity, we assume in this paper that the acoustic consequences are formant frequencies and that the somatosensory sensations are the tract variables. Discussing the choice of acoustic and somatosensory representations is left for future work. In this paper, both <inline-formula><mml:math id="M6"><mml:mi mathvariant="script">A</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="M7"><mml:mi mathvariant="script">S</mml:mi></mml:math></inline-formula> are implemented in the form of MultiLayer Perceptrons (MLPs), namely feedforward fully connected Artificial Neural Networks (ANNs). Updating these internal models after each trial consists of updating weights of the corresponding MLP via fine-tuning through a few epochs using a motor command and sensory output pair which resulted from the previous trial. For instance, the input/output pair used to update <inline-formula><mml:math id="M8"><mml:mrow><mml:msup><mml:mi mathvariant="script">A</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> after trial <italic>n</italic> consists of the vector of motor command x<sup>(<italic>n</italic>)</sup> provided by the optimization process at trial n and the vector of formants <inline-formula><mml:math id="M9"><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>a</mml:mi></mml:mstyle><mml:mrow><mml:mtext>perc</mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> perceived by the (virtual) speaker at trial n (i.e., its auditory feedback, which is potentially perturbed during the simulated adaptation experiment). Similarly, the input/output pair used to update the somatosensory model <inline-formula><mml:math id="M10"><mml:mrow><mml:msup><mml:mi mathvariant="script">S</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> after trial <italic>n</italic> consists of <italic>x</italic><sup>(<italic>n</italic>)</sup> and the somatosensory vector <inline-formula><mml:math id="M11"><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>s</mml:mi></mml:mstyle><mml:mrow><mml:mtext>perc</mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> perceived by the speaker at trial n.</p><p id="P38">To generate the acoustic and somatosensory representations, <italic>i.e.</italic> the formant frequency and tract variables respectively, we use the Maeda model [<xref ref-type="bibr" rid="R57">57</xref>] in this paper. The Maeda model generates midsagittal shapes of the vocal tract using seven independent articulatory parameters, corresponding to the principal components that explain most of the observed variance in articulatory data. These are expressed in terms of standard deviations above or below the mean value, where the mean value (i.e. 0) corresponds to the parameter’s value in a neutral vocal tract position. The tract variables are computed following the technique detailed in [<xref ref-type="bibr" rid="R58">58</xref>], to which we add vocal tract length (VTL), computed as the length of the vocal tract midline from the larynx to the end of the lips. We used the Maedeep python library [<xref ref-type="bibr" rid="R59">59</xref>] implementation, which is a python version of the original VTCalcs module, written in C [<xref ref-type="bibr" rid="R60">60</xref>] to compute the formant frequencies associated with the vector x of Maeda parameters. For the sake of simplicity, we used the Maeda parameters (i.e., the seven independent articulatory parameters introduced above) as our vector of motor commands. Consequently, <inline-formula><mml:math id="M12"><mml:mi mathvariant="script">A</mml:mi></mml:math></inline-formula> learns the mapping between Maeda parameter values and associated formant frequencies, while <inline-formula><mml:math id="M13"><mml:mi mathvariant="script">S</mml:mi></mml:math></inline-formula> learns the mapping between Maeda parameter values and tract variable values.</p><p id="P39">The architecture of our model is represented in <xref ref-type="fig" rid="F2">Figure 2</xref>. At each trial n, the optimization block plans a vector of motor commands <italic>x</italic><sub>n</sub> that minimizes the cost function, using the internal models <inline-formula><mml:math id="M14"><mml:msup><mml:mi mathvariant="script">A</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="M15"><mml:msup><mml:mi mathvariant="script">S</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula>, for a given target phoneme <italic>p.</italic> The optimization block uses the vector of motor commands of the previous trial x<sub><italic>n</italic>-1</sub> as the initial solution. The plant (here the Maeda model) is used to compute the produced acoustic vector <inline-formula><mml:math id="M16"><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>a</mml:mi></mml:mstyle><mml:mrow><mml:mtext>pred</mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and the produced somatosensory vector <inline-formula><mml:math id="M17"><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>s</mml:mi></mml:mstyle><mml:mrow><mml:mtext>pred</mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>. Depending on the nature and on the amount of the sensory perturbation, these vectors are modified (or not if no perturbation is applied) to generate the sensory feedback vectors <inline-formula><mml:math id="M18"><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>a</mml:mi></mml:mstyle><mml:mrow><mml:mtext>perc</mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M19"><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>s</mml:mi></mml:mstyle><mml:mrow><mml:mtext>perc</mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>. These sensory feedback vectors are used to update the weights of the internal models <inline-formula><mml:math id="M20"><mml:mrow><mml:msup><mml:mi mathvariant="script">A</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M21"><mml:mrow><mml:msup><mml:mi mathvariant="script">S</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, which are then used to compute the solution of the optimization block at the next trial <italic>n</italic> + 1.</p></sec><sec id="S11"><title>The cost function</title><p id="P40">As introduced in Section Modeling framework, following [<xref ref-type="bibr" rid="R7">7</xref>–<xref ref-type="bibr" rid="R9">9</xref>], we assume a cost function that accounts for minimal articulatory effort and maximal intelligibility. The minimal model used in this paper generates static configurations. As a consequence, intelligibility is based on single frames of produced speech<sup><xref ref-type="fn" rid="FN2">2</xref></sup>. In the earlier versions of our OCT-based model [<xref ref-type="bibr" rid="R7">7</xref>–<xref ref-type="bibr" rid="R9">9</xref>], the intelligibility cost component was based on purely acoustic (auditory) consequences of articulatory movements. That is, the intelligibility cost component was assumed to be the speaker’s estimate of how intelligible a phoneme would be to a listener, given the acoustics only. As discussed in the introduction, we propose a new cost function that accounts for both the somatosensory and the auditory characterizations of phonemes. This translates into two independent costs related to an estimation of the probability that a phoneme will be recognized by a listener, namely one related to intelligibility given the acoustics, and one related to intelligibility given the somatosensory characterization of the target phoneme, as in <xref ref-type="disp-formula" rid="FD1">Eq. (1)</xref>. This is based on evidence that the speaker’s plan for articulatory movement is also based on their somatosensory characterization of phonemes. For instance, the phenomenon of <italic>covert contrast</italic> [<xref ref-type="bibr" rid="R62">62</xref>, <xref ref-type="bibr" rid="R63">63</xref>] suggests that speakers with phonological disorder may use two distinct articulatory targets to distinguish a pair of phonemes (for instance alveolar and velar stop consonants) without making them acoustically distinguishable: their articulatory plan to distinguish these pairs of phonemes is predominantly based on distinct somatosensory output, as opposed to distinct acoustic consequences. The somatosensory characterization of phonemes in articulatory planning is taken into account in some models of speech production by two feedback loops, corresponding to auditory and somatosensory sensations, which are used to update feedforward commands, such as in DIVA [<xref ref-type="bibr" rid="R39">39</xref>–<xref ref-type="bibr" rid="R42">42</xref>], or in FACTS [<xref ref-type="bibr" rid="R38">38</xref>, <xref ref-type="bibr" rid="R45">45</xref>]. In Bayesian GEPPETO [<xref ref-type="bibr" rid="R36">36</xref>, <xref ref-type="bibr" rid="R37">37</xref>, <xref ref-type="bibr" rid="R43">43</xref>], auditory and somatosensory characterizations of phonemes are modeled by a dual branch-based system of motor planning, including a subsystem that accounts for auditory goals, and another subsystem that accounts for somatosensory goals.</p><p id="P41">Still following [<xref ref-type="bibr" rid="R7">7</xref>–<xref ref-type="bibr" rid="R9">9</xref>], we assume that the cost of articulatory effort is related to the distance of articulators from their neutral position. Using Maeda’s articulatory model [<xref ref-type="bibr" rid="R57">57</xref>], this corresponds to computing the <italic>l</italic><sub>2</sub> – norm of the vector x of Maeda parameters: <disp-formula id="FD4"><label>(4)</label><mml:math id="M22"><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>x</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mtext>x</mml:mtext><mml:mo>‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P42">where <italic>E</italic><sub>max</sub> = 63 is used as a normalization factor to yield an effort cost with values ranging between 0 and 1 (63 being the maximal value of <italic>E</italic>(x), obtained when all elements in x are either -3 or +3). This normalization ensures that effort costs are in the same range of values as the recognition costs, whose values range from 0 to 1.</p></sec><sec id="S12"><title>Probabilistic models for computing intelligibility scores</title><p id="P43">The (predicted) recognition score <italic>R</italic> is the cost of not being intelligible, where intelligibility is modeled as the recognition probability of the target phoneme <italic>p</italic> given the articulatory vector x, hence:<disp-formula id="FD5"><label>(5)</label><mml:math id="M23"><mml:mrow><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>x</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mtext>P</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>|</mml:mo><mml:mtext>x</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P44">We consider two probabilistic models, denoted <italic>P</italic><sub>A</sub> and <italic>P</italic><sub>S</sub>, which return the probability of the target phoneme given the predicted auditory and somatosensory sensations, respectively. These two probabilistic models are used to compute two distinct recognition costs <italic>R<sub>A</sub></italic> and <italic>R<sub>S</sub></italic>, corresponding to the cost of recognition in the auditory and the somatosensory space, respectively, namely: <disp-formula id="FD6"><label>(6)</label><mml:math id="M24"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mtext>x</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mtext>P</mml:mtext><mml:mi>A</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mtext>a</mml:mtext><mml:mrow><mml:mtext>pred</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mtext>P</mml:mtext><mml:mi>A</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>∣</mml:mo><mml:mi mathvariant="script">A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>x</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="FD7"><label>(7)</label><mml:math id="M25"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mtext>x</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mtext>P</mml:mtext><mml:mi>S</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>s</mml:mi></mml:mstyle><mml:mrow><mml:mtext>pred</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mtext>P</mml:mtext><mml:mi>S</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>∣</mml:mo><mml:mi mathvariant="script">S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>x</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p></sec><sec id="S13"><title>Simulations</title><p id="P45">This section presents the simulations performed for this study, as well as fits to real experimental data.</p></sec><sec id="S14"><title>Training the internal forward models</title><p id="P46">In this paper, we used pre-trained internal forward models, denoted <inline-formula><mml:math id="M26"><mml:msup><mml:mi mathvariant="script">A</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula> for the acoustic model and <inline-formula><mml:math id="M27"><mml:msup><mml:mi mathvariant="script">S</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula> for the somatosensory model. They were trained using random generation of articulatory vectors x and with computation of associated formant frequency vectors f (for the acoustic model) and tract variable vectors s (for the somatosensory model) using the Maeda model [<xref ref-type="bibr" rid="R57">57</xref>]. We generated <italic>N</italic> articulatory vectors following a uniform random distribution between -3 and +3. We kept 90% for training and 10% for validation. Note that the associated formant vector of closed configurations (closed vocal tract) was set to the null vector (all formants at 0). We hypothesize that changing the architecture of internal models and/or the training data can modify the model’s flexibility to adapt and extrapolate to new observed data (auditory feedback), especially when these new observed data are far from what would be expected. That is, we believe that the size of the model (<italic>i.e.</italic>, the number and size of the hidden layers) has a direct influence on the ability of the speaker to compensate for altered auditory feedback. Similarly, the amount of data used for training the internal models, corresponding to the past history of the speaker, can potentially affect the characteristics of the adaptation response, such as the maximal compensation or the number of trials that are necessary to reach that maximal compensation. In order to investigate the potential effect of the internal models’ characteristics on the adaptation behavior, we trained, for both internal models, various MLP networks with different number <italic>M</italic> of hidden layers of different sizes, and with a tanh activation function, during a maximal number of 100 epochs. Training was stopped when the validation loss was not improved for 5 successive epochs. For the simulations, we define nominal internal models as MLPs having the characteristics detailed in <xref ref-type="table" rid="T1">Table 1</xref>. Unless specifically mentioned, simulations are performed using these nominal internal forward models.</p></sec><sec id="S15"><title>Training the internal probabilistic models</title><p id="P47">Similarly to the probabilistic model presented in [<xref ref-type="bibr" rid="R9">9</xref>], we used a Quadratic Discriminant Analysis (QDA) model trained on real formant values to predict the probability of vowels given a vector a containing the first 4 formant frequencies. In this paper, we trained the QDA model on formants of American-English vowels extracted from the Vocal Tract Resonance (VTR) database [<xref ref-type="bibr" rid="R64">64</xref>]. We did the same to train the somatosensory-vector-to-probability model <italic>P<sub>S</sub></italic>, except for the input data being the vectors of tract variables s.</p></sec><sec id="S16"><title>Experimental data</title><p id="P48">In this paper, our simulations are based on participant data from the experimental study by Kim and Max [<xref ref-type="bibr" rid="R21">21</xref>]<sup><xref ref-type="fn" rid="FN3">3</xref></sup>. This is because we also aim to compare the results of our simulations with real experimental data from adaptation-based experiments. In this experimental study [<xref ref-type="bibr" rid="R21">21</xref>], speakers were asked to produce the American-English phoneme /ε/ in one of either <italic>bed</italic> or <italic>pet</italic> in sequences of 160 trials. During the first 20 trials, auditory feedback was not altered. For the next 120 trials, auditory feedback of speakers’ productions was altered by applying an upward shift of 400 cents (around 26%) to F1. Then, the alteration was removed for the last 20 trials. Our simulations are designed to simulate this experiment, namely producing the American-English phoneme /ε/ in sequences of 160 trials, with a perturbation function applied to F1 which is the same as the one used in [<xref ref-type="bibr" rid="R21">21</xref>]. This perturbation function is shown in the right panel of <xref ref-type="fig" rid="F1">Figure 1</xref>.</p></sec></sec><sec id="S17" sec-type="methods"><title>Methods</title><p id="P49">The presented simulations were conducted to investigate the influence of different aspects of our model on the adaptation process. For that purpose, we simulated several altered feedback experiments. These simulated experiments consisted of simulating the production of the English vowel /ε/ for 160 trials with a first formant frequency F1 altered as shown in <xref ref-type="fig" rid="F1">Figure 1</xref>. Several parameters were modified for each simulation in order to investigate the effect of the weights of the cost function, as defined in <xref ref-type="disp-formula" rid="FD1">Eq. (1)</xref>, resulting in different degrees of hypo-/hyper- articulation, and also the effect of various characteristics of the pre-trained internal models, such as the learning rate, the number of training samples, the number, and sizes of hidden layers. Note that for optimization-based problems, the number of required weights is one unit less than the number of costs. This is because multiplying all weights by the same factor does not change the minimal solution. Consequently, only two weights are required. In this paper, we chose to normalize the auditory and somatosensory weights <italic>α<sub>A</sub></italic> and <italic>α<sub>S</sub></italic> such that they always sum as 1, namely <italic>α<sub>A</sub></italic> + <italic>α<sub>s</sub></italic> = 1.</p><p id="P50">Each simulation was performed as follows. We first chose specific pre-trained internal forward models, denoted <inline-formula><mml:math id="M28"><mml:msup><mml:mi mathvariant="script">A</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="M29"><mml:msup><mml:mi mathvariant="script">S</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula>, with specific learning rates <inline-formula><mml:math id="M30"><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi mathvariant="script">A</mml:mi></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="M31"><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi mathvariant="script">A</mml:mi></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula>, and a number of epochs <italic>e<sub>A</sub></italic> = <italic>e<sub>S</sub></italic> = 10 during updates, and fixed weights <italic>α<sub>E</sub></italic>, <italic>α<sub>A</sub>,</italic> and <italic>α<sub>s</sub></italic> = 1 – <italic>α<sub>A</sub>.</italic> We used the Nelder-Mead algorithm [<xref ref-type="bibr" rid="R65">65</xref>] to run the initial optimization and estimate the optimal articulatory vector x<sup>(0)</sup> that minimizes <italic>J</italic> in <xref ref-type="disp-formula" rid="FD1">Eq. (1)</xref>. For that first iteration, we used the pre-trained internal models <inline-formula><mml:math id="M32"><mml:msup><mml:mi mathvariant="script">A</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="M33"><mml:msup><mml:mi mathvariant="script">S</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula> to compute both intelligibility scores based on auditory and somatosensory sensations. The estimated articulatory vector x<sup>(0)</sup> was used to compute the actually produced formant vector <inline-formula><mml:math id="M34"><mml:msubsup><mml:mtext>a</mml:mtext><mml:mrow><mml:mtext>prod </mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> and produced somatosensory vectors <inline-formula><mml:math id="M35"><mml:msubsup><mml:mtext>s</mml:mtext><mml:mrow><mml:mtext>prod </mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>. If there is no altered sensory feedback, the formant vector perceived by the speaker, denoted <inline-formula><mml:math id="M36"><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>a</mml:mi></mml:mstyle><mml:mrow><mml:mtext>perc </mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>, is simply <inline-formula><mml:math id="M37"><mml:msubsup><mml:mtext>a</mml:mtext><mml:mrow><mml:mtext>prod </mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>. Similarly, the somatosensory vector perceived by the speaker, denoted <inline-formula><mml:math id="M38"><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>a</mml:mi></mml:mstyle><mml:mrow><mml:mtext>perc </mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>, is simply the produced somatosensory vector <inline-formula><mml:math id="M39"><mml:msubsup><mml:mtext>a</mml:mtext><mml:mrow><mml:mtext>prod </mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>. In that case, both x<sup>(0)</sup> and <inline-formula><mml:math id="M40"><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>a</mml:mi></mml:mstyle><mml:mrow><mml:mtext>perc </mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> are used to fine-tune the internal model <inline-formula><mml:math id="M41"><mml:msup><mml:mi mathvariant="script">A</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> with <italic>e<sub>A</sub></italic> epochs run over this single sample. The fine-tuned version of the model is then <inline-formula><mml:math id="M42"><mml:msup><mml:mi mathvariant="script">A</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula>, and is used to estimate the new optimal articulatory vector x<sup>(1)</sup>, similarly to x<sup>(0)</sup>, and so forth until end of the simulations. Updating <inline-formula><mml:math id="M43"><mml:msup><mml:mi mathvariant="script">S</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> is done in similar fashion, except that the input/output pair used for fine-tuning is x<sup>(0)</sup> and <inline-formula><mml:math id="M44"><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>s</mml:mi></mml:mstyle><mml:mrow><mml:mtext>perc </mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>. At the end of each iteration <italic>n</italic>, the internal models <inline-formula><mml:math id="M45"><mml:msup><mml:mi mathvariant="script">A</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="M46"><mml:msup><mml:mi mathvariant="script">S</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> are obtained by updating the weight <inline-formula><mml:math id="M47"><mml:msup><mml:mi mathvariant="script">A</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="M48"><mml:msup><mml:mi mathvariant="script">S</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> over <italic>e<sub>A</sub></italic> and <italic>e<sub>S</sub></italic> epochs, respectively, using only the samples x<sup>(<italic>n</italic>)</sup> and <inline-formula><mml:math id="M49"><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>a</mml:mi></mml:mstyle><mml:mrow><mml:mtext>perc </mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> (respectively <inline-formula><mml:math id="M50"><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>s</mml:mi></mml:mstyle><mml:mrow><mml:mtext>perc </mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>). However, in the case of altered auditory feedback, the perceived formant vector at iteration <italic>n</italic> <inline-formula><mml:math id="M51"><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>a</mml:mi></mml:mstyle><mml:mrow><mml:mtext>perc</mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> is an altered version of the actually produced formant vector <inline-formula><mml:math id="M52"><mml:msubsup><mml:mtext>a</mml:mtext><mml:mrow><mml:mtext>prod </mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>, as:<disp-formula id="FD8"><label>(8)</label><mml:math id="M53"><mml:mrow><mml:msubsup><mml:mtext>a</mml:mtext><mml:mrow><mml:mtext>perc </mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mtext>a</mml:mtext><mml:mrow><mml:mtext>prod </mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>·</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo>Δ</mml:mo><mml:mi>F</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P51">where Δ<italic>F</italic> as a function of the trial <italic>n</italic> is shown in <xref ref-type="fig" rid="F1">Figure 1</xref>. That is, the alteration concerns only the first formant, the other formants being perceived as they are actually produced by the speaker.</p><p id="P52">Additionally, we compare the results of our simulations with those of the SimpleDIVA model. We chose SimpleDIVA as the baseline model for comparison because this model has been designed to fit experimental data. For that purpose, we fit our model and a 2-parameter version of SimpleDIVA to the real experimental data presented in [<xref ref-type="bibr" rid="R21">21</xref>]. That is, for a given speaker, we found the pair of parameters <inline-formula><mml:math id="M54"><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> and <italic>α<sub>A</sub></italic> for which our model returned a F1 trajectory that minimized the RMSE with respect to the recorded F1 trajectory.</p><sec id="S18"><title>Results of simulations with altered auditory feedback</title><p id="P53">This section presents the results of our simulations when the perturbation is applied to auditory feedback. These results show the influence of different parameters of our model, as well as the influence of the characteristics of the internal forward models, on the adaptation behavior.</p></sec><sec id="S19"><title>Illustration of a typical simulation</title><p id="P54"><xref ref-type="fig" rid="F3">Figure 3</xref> shows a typical example of the adaptation behavior returned by our simulations, with perturbations applied only to auditory feedback. The acoustic output visualized in <xref ref-type="fig" rid="F3">Figure 3 (A)</xref> reproduces typical adaptation behavior to altered auditory feedback with perceived F1 shifted upwards between trials 20 and 140. The produced F1 (the blue solid line in <xref ref-type="fig" rid="F3">Figure 3 (A)</xref>) moves downwards in the direction opposite to the perturbation. As observed experimentally in previous studies [<xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R14">14</xref>, <xref ref-type="bibr" rid="R16">16</xref>, <xref ref-type="bibr" rid="R21">21</xref>, <xref ref-type="bibr" rid="R24">24</xref>, <xref ref-type="bibr" rid="R54">54</xref>], the adaptation is gradual, as it takes several trials to reach an altered auditory feedback (F1 value) close to the maximal compensation (defined as the lowest altered F1 value).</p><p id="P55">Additionally, the compensation is not complete, as the minimal heard F1 (auditory feedback, in red) during the perturbation phase is higher than the baseline (gray dotted line), i.e., the F1 perceived before the perturbation was applied. This type of incomplete adaptation is also a feature commonly observed experimentally in adaptation studies (e.g., in [<xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R19">19</xref>, <xref ref-type="bibr" rid="R21">21</xref>, <xref ref-type="bibr" rid="R23">23</xref>, <xref ref-type="bibr" rid="R54">54</xref>]).</p><p id="P56"><xref ref-type="fig" rid="F3">Figure 3 (A)</xref> also shows the acoustic output as predicted by the auditory internal model for each subsequent trial, based on the performed articulatory action. Within our optimization-based approach, it is this predicted acoustic output–rather than the actual “heard” realization–that is evaluated as the optimal realization of the speaker’s intention, i.e., that minimizes the cost function <italic>J</italic> in <xref ref-type="disp-formula" rid="FD1">Eq. (1)</xref>. In this example, the simulations updated the internal model predicting the acoustic consequence very early in the adaptation phase, around trial 25. While the predicted value of the acoustic consequence of the articulation largely stabilizes after this trial, these relatively constant predictions correspond to different, downward shifting actual productions. The internal model keeps being updated and drives behavioral adaptation until the gap between predicted and perceived acoustic consequences (dash-dotted red and dashed green lines) disappears at around trial 60. This gradual adaptation of the internal model of articulatory consequences of articulatory action thus gives rise to gradual behavioral adaptation.</p><p id="P57">Our simulations also predict that, in addition to F1, formants F2 and F3 are also modified during the adaptation phase; F2 and F3 both shift upward. The maximal upward shift of F2 is around 5% of the baseline value, which is in agreement with the F2 shift observed in [<xref ref-type="bibr" rid="R14">14</xref>]. During the adaptation phase, the simulated example in <xref ref-type="fig" rid="F3">Figure 3</xref> predicts that the speaker would raise their tongue and move it frontward, making the original /ε/ higher and more front, namely tending towards /e/ and /i/ (see <xref ref-type="fig" rid="F3">Figure 3 (B)</xref>). This articulatory shift would result in lower F1 (because of adaptation to the altered F1), but would also increase F2.</p><p id="P58">This typical example of simulation illustrates that some of our modeling objectives presented in the introduction are fulfilled. This is because it shows that our approach is able to reproduce gradual (Objective A) and incomplete (Objective B) adaptation to altered auditory feedback by moving the altered formants in the direction opposite to the perturbation. In addition, the adaptation also modified other formants, namely F2 and F3, although they were not modified in the auditory feedback (Objective D).</p></sec><sec id="S20"><title>The influence of the somatosensory intelligibility preference on compensation</title><p id="P59"><xref ref-type="fig" rid="F4">Figure 4</xref> shows the results of our simulations for the American-English vowel /ε/ under formant-shifted perturbations shown in <xref ref-type="fig" rid="F1">Figure 1</xref>, for different values of <italic>α<sub>S</sub></italic>, the relative weight assigned to the somatosensory intelligibility. We remind the reader that the weight assigned to acoustic intelligibility is <italic>α<sub>A</sub></italic> = 1 – <italic>α<sub>S</sub></italic>. This means that when <italic>α<sub>S</sub></italic> = 0 (left plot in <xref ref-type="fig" rid="F4">Figure 4</xref>), only acoustic intelligibility is optimized, while when <italic>α<sub>S</sub></italic> = 1 (right plot in <xref ref-type="fig" rid="F4">Figure 4</xref>), only somatosensory intelligibility is optimized, as <italic>α<sub>A</sub></italic> = 0. In these experiments, the weight assigned to the effort cost was 0. This was done to make sure that the observed effect was due to the different values of <italic>α<sub>A</sub></italic> and <italic>α<sub>S</sub></italic>, and not due to a different ratio between one of those weights and the effort weight <italic>α<sub>E</sub></italic>.</p><p id="P60">Our simulations show that the balance between acoustic and somatosensory intelligibility optimization has an effect on the level of adaptation to altered auditory feedback: the greater the priority assigned to acoustic intelligibility, the larger the compensation. With maximal priority assigned to acoustic intelligibility (<italic>α<sub>S</sub></italic> = 0, <italic>α<sub>A</sub></italic> = 1), our model predicts full compensation, as the perceived first formant frequency during the auditory feedback altered trials is equal to the first formant frequency during the non-perturbed trials. With increasing <italic>α<sub>S</sub></italic>, the compensation degree gradually decreases. Finally, when <italic>α<sub>S</sub></italic> = 1 (and <italic>α<sub>S</sub></italic> = 0, <italic>i.e.</italic> acoustic intelligibility is not optimized), there is no adaptation at all.</p><p id="P61"><xref ref-type="fig" rid="F4">Figure 4</xref> also shows simulated adaptation behavior for two different values of the learning rate <inline-formula><mml:math id="M55"><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> of the auditory internal model, <italic>i.e.</italic> the amount of correction to apply to the internal auditory forward model’s weights during the update at each epoch. It shows that the learning rate controls the speed at which the speaker reaches its maximal adaptation. For a higher learning rate (here <inline-formula><mml:math id="M56"><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>), it takes fewer trials for the model output to reach maximal compensation than for a lower learning rate (here <inline-formula><mml:math id="M57"><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>). There is a small impact of learning rate on compensation, but this depends systematically on the relative weighting of acoustic and somatosensory intelligibility. For <italic>α<sub>S</sub></italic> &lt; 0.5, a higher learning rate yields slightly more compensation, while for <italic>α<sub>S</sub></italic> &gt; 0.5, a lower learning rate yields slightly more compensation. Combination of the dual requirement of predicting and continuous adapting of the internal model of both acoustic and somatosensory consequences of articulatory action thus answers Objective B of incomplete compensation as well as potentially the Objective C of inter-speaker variability. This is because it is possible that values of the weights <italic>α<sub>A</sub></italic> and <italic>α<sub>S</sub></italic> are speaker-dependent. That is, speakers adapt differently to altered sensory feedback because they have different values of <italic>α<sub>A</sub></italic> and <italic>α<sub>S</sub></italic>. Similarly, speakers adapt more or less quickly to altered sensory feedback because their internal forward models have different learning rates.</p></sec><sec id="S21"><title>The influence of the weights assigned to the effort cost</title><p id="P62"><xref ref-type="fig" rid="F5">Figure 5</xref> shows the results of our simulations for the American-English vowel /ε/ under formant-shifted perturbations shown in <xref ref-type="fig" rid="F1">Figure 1</xref>, for different values of <italic>a<sub>E</sub></italic>, the weight assigned to the articulatory effort cost. This parameter is directly related to the level of planned hyper- and hypo-articulation [<xref ref-type="bibr" rid="R7">7</xref>, <xref ref-type="bibr" rid="R8">8</xref>]: the lower the weight assigned to the effort cost, the greater the predicted hyperarticulation.</p><p id="P63">Our simulations predict that the degree of planned hyper-articulation slightly changes the adaptation response of the speaker. This is evidenced by the left panel of <xref ref-type="fig" rid="F5">Figure 5</xref>, which shows produced F1 for different values of <italic>α<sub>E</sub></italic>. Note that increasing <italic>α<sub>E</sub></italic> yields a lower F1 baseline (measured as the produced F1 averaged across trials 11 to 20. This is because the baseline solution with high α<italic><sub>E</sub></italic> yields a more centralized /ε/, namely it is closer to the neutral solution x = 0, which generates <italic>F</italic>1 = 417 Hz. The compensation ratio computed from the simulations shown in <xref ref-type="fig" rid="F5">Figure 5</xref> are 80%, 71%, 73%, and 72%, for <italic>α<sub>E</sub></italic> = 0.1, <italic>α<sub>E</sub></italic> = 0.5, <italic>α<sub>E</sub></italic> = 1, and <italic>α<sub>E</sub></italic> = 2, respectively. These different compensation ratios indicate that, although the degree of planned hyper-articulation changes the degree of compensation, there is no direct rank correlation between these two variables. The right panel of <xref ref-type="fig" rid="F5">Figure 5</xref> shows the effect on F2. It shows that hyperarticulation (<italic>i.e.</italic> low <italic>α<sub>E</sub></italic>) favors changes in F2: <italic>α<sub>E</sub></italic> = 0.1 yields a rise of F2 of about 5%, while it is slightly raised before going down towards the baseline for higher values of <italic>α<sub>E</sub></italic>. This influence of the degree of hypo- and hyper-articulation on unaltered formants during adaptation, as predicted by our approach, might explain variability which has been observed in previous experiments. Indeed, some studies reported modifications of F2 during F1-altered feedback experiments [<xref ref-type="bibr" rid="R11">11</xref>, <xref ref-type="bibr" rid="R14">14</xref>, <xref ref-type="bibr" rid="R16">16</xref>, <xref ref-type="bibr" rid="R29">29</xref>], while another study did not report significant changes [<xref ref-type="bibr" rid="R55">55</xref>]. This could be due to inter-speaker variability of hyper-articulation across these different experiments. In that case, our approach can also simulate inter-speaker (or intra-speaker) variability of adaptation behavior (which is our Objective C) by changing the weight assigned to the effort cost, <italic>i.e.</italic> by varying the degree of planned hyper-articulation.</p></sec><sec id="S22"><title>The influence of the architecture and training of the internal models</title><p id="P64">In our approach, speech adaptation behavior to altered auditory feedback depends on the ability of the internal models to learn a new relationship between motor commands and acoustics during the auditory feedback perturbation phase. This can be seen in <xref ref-type="fig" rid="F4">Figure 4</xref>, which shows the effect of the learning rate of the auditory internal model on adaptation behavior. Another possible variable that may impact adaptation behavior is the amount of data on which internal models have been trained, and also the general architecture (number and size of hidden layers). In order to investigate this potential effect, we ran simulations of adaptation experiments using various pre-trained internal forward models <inline-formula><mml:math id="M58"><mml:msup><mml:mi mathvariant="script">A</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="M59"><mml:msup><mml:mi mathvariant="script">S</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula>, each differing according to the size of the dataset used to train them, and also on the number and size of hidden layers.</p><p id="P65"><xref ref-type="fig" rid="F6">Figure 6</xref> shows the results of these experiments for these different pre-trained internal models, with fixed <italic>α<sub>E</sub></italic> = 0.5, <italic>α<sub>S</sub></italic> = 0.4 and <italic>α<sub>A</sub></italic> = 0.6, and fixed learning rate of 10<sup>–5</sup>. Results show that the architecture of the internal models and how internal models have been trained have an influence on adaptation behavior. For instance, for fixed effort and auditory weights, and fixed learning rates, the number of training samples affects the amount of compensation (speakers compensate more with models trained on more data), in addition to the adaptation rate (the speaker reaches their maximal compensation faster with internal models trained on less data). In other words, our approach predicts that speakers with less experience in speaking compensate less, but faster, than speakers with more speaking experience. However, similarly to the effect of the degree of planned hyper-articulation, there is no direct rank correlation between size of training data and amount of compensation, as the compensation ratio obtained with models pre-trained on 1e4 samples is larger than the compensation ratio obtained with models pre-trained on more data (5e4 and 1e5 samples). Regarding the architecture of the internal models, larger internal models (more hidden layers and larger hidden layers) yield faster adaptation behavior with higher compensation amplitude than smaller internal models.</p><p id="P66">These results show that different characteristics of internal forward models can be a source of inter-speaker variability and, consequently, these observations answer Objective C. However, the characteristics of internal forward models only impact the compensation ratio and the compensation rate of speakers, but not the general compensation patterns. These different pre-trained internal forward models did not generate unusual adaptation behavior, such as no compensation or compensation in a different direction (For example, F1 that goes in the same direction as the perturbation). In addition, it is worth noting that the compensation ratio and the compensation rate can be adjusted or modified by other means, such as by the degree of planned hyper-articulation (defined by the ratio between weights assigned to effort and recognition costs), the learning rate, or by adjusting the relative contributions of auditory and somatosensory recognition costs in the global cost function.</p></sec><sec id="S23"><title>Results of simulations with altered somatosensory feedback</title><p id="P67">Modeling adaptation to altered somatosensory feedback using our approach is relatively similar. The major difference is that perturbation of articulatory movements, e.g., perturbation of jaw movements, as proposed in [<xref ref-type="bibr" rid="R13">13</xref>], also impacts produced acoustics. As a consequence, both auditory and somatosensory feedback received by the speaker differ from predicted sensory feedback. However, since our approach does not require the detection or the evaluation of discrepancies between predicted and actual sensory feedback, our model deals with altered somatosensory feedback in exactly the same way as with altered auditory feedback.</p><p id="P68">In this section, we present simulations which are inspired by Lametti <italic>et al.</italic> [<xref ref-type="bibr" rid="R13">13</xref>]. The aim of this section is to show that, accordingly to our Objective E, our approach is also able to reproduce adaptation to altered somatosensory feedback. We model somatosensory perturbation by artificially pulling the jaw outward, horizontally, and by artificially increasing the lip protrusion. This is done by modifying the <italic>x</italic>-coordinates of the upper and lower contours of the vocal tract given by the Maeda model from the jaw to the end of the lips: the horizontal distance <italic>d</italic> of the jaw from a reference point (here the center of the polar grid used to compute the area function, located at <italic>x</italic> = 10) is multiplied by the perturbation factor <italic>k</italic>, and lip protrusion is also multiplied by <italic>k</italic>. <xref ref-type="fig" rid="F7">Figure 7</xref> shows an example of such geometry perturbation for <italic>k</italic> = 1.26. The series of perturbations is similar to the one used for the altered auditory feedback simulations, namely as shown in the right panel of <xref ref-type="fig" rid="F7">Figure 7</xref>.</p><p id="P69"><xref ref-type="fig" rid="F8">Figure 8</xref> shows the results of our simulations for three values of the weights assigned to auditory and somatosensory recognition costs. In the purely-auditory planning case (<italic>α<sub>S</sub></italic> = 0, <italic>α<sub>A</sub></italic> = 1), represented in the left panels in <xref ref-type="fig" rid="F8">Fig. 8</xref>, the change in F1 induced by the articulatory perturbation is fully compensated: the lowering of F1 induced by the jaw pulling (which lengthens the vocal tract, hence reduce F1) is compensated as the produced F1 goes back to the value before perturbation around trial 60. The lip protrusion perturbation is, however, not fully compensated, as shown in the bottom left plot in <xref ref-type="fig" rid="F8">Fig. 8</xref>: at trial 60, when the produced F1 is back to the value before perturbation, the optimal chosen motor command is such that the produced lip protrusion is still higher than before perturbation (around 1.1 cm, while it is 1 cm before perturbation). After the perturbation is applied, namely after trial 140, we also observe an acoustic adaptation. In the absence of articulatory perturbation, the produced F1 is now too high, but the produced F1 goes back towards the baseline in subsequent trials (not quickly enough to reach full compensation before the end of the simulated experiments). Similarly, the chosen optimal motor commands are such that the lip protrusion is increased such that lip protrusion tends towards the value of 1 cm, namely that before the first perturbations. In the purely somatosensory-based planning case (<italic>α<sub>S</sub></italic> = 1, <italic>α<sub>A</sub></italic> = 0), which is represented in the right panels in <xref ref-type="fig" rid="F8">Fig. 8</xref>, the change in F1 induced by the articulatory perturbation is not fully compensated. However, lip protrusion is fully compensated: during the adaptation phase, the optimal motor commands are chosen such that the perturbed lip protrusion goes back to the baseline value. In the intermediate case (<italic>α<sub>S</sub></italic> = 0.5, <italic>α<sub>A</sub></italic> = 0.5), as shown in the central panels in <xref ref-type="fig" rid="F8">Fig. 8</xref>, our model predicts incomplete compensation for both F1 and lip protrusion.</p></sec><sec id="S24"><title>Fit to experimental data</title><p id="P70">The previous section showed that our approach was able to reproduce several observed aspects of adaption to altered auditory feedback qualitatively, including gradual and incomplete compensation, and inter- and intra-speaker variability. This section investigates the ability of our approach to reproduce these behaviors quantitatively. In other words, how well can the acoustic output generated by our model fit observed experimental data? For that purpose, for each of the F1 trajectories recorded from the 24 speakers of the Kim and Max experiments [<xref ref-type="bibr" rid="R21">21</xref>], we estimated some parameters of our model for which the simulated F1 output best fits the experimental F1 trajectories. We chose to estimate only two parameters, namely the auditory weight <italic>α<sub>A</sub></italic> and the learning rate of the internal auditory model <inline-formula><mml:math id="M60"><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula>, because our simulations with different weights <italic>α<sub>E</sub></italic> showed that this parameter had a small impact on characteristics (compensation ratio and compensation rate, as shown in <xref ref-type="fig" rid="F5">Fig. 5</xref>) that can also be adjusted via modifications of <italic>α<sub>A</sub></italic> and <inline-formula><mml:math id="M61"><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula>, as shown in <xref ref-type="fig" rid="F4">Fig. 4</xref>.</p><p id="P71">The fit is done by minimizing an objective function which is the Normalized Mean Root Square Error (NRMSE) between the fit (the F1 trajectory generated by the model) and the experimental data (F1 produced by the speaker, as reported in [<xref ref-type="bibr" rid="R21">21</xref>]), where the fit and the experimental data are each normalized by their baseline values (taken as the mean between trials 11 and 20). For each speaker, the minimization is done running 100 instances of the Nelder-Mead algorithm [<xref ref-type="bibr" rid="R65">65</xref>] with random initialization of <italic>α<sub>A</sub></italic> and <inline-formula><mml:math id="M62"><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula>. For each speaker, the final estimate of <italic>α<sub>A</sub></italic> and <inline-formula><mml:math id="M63"><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> is the solution of the Nelder-Mead minimization which returns the lowest NRMSE across the 100 runs. In addition, we compare the results of our fit with the fit obtained with SimpleDIVA [<xref ref-type="bibr" rid="R39">39</xref>], because it is the only adaptation model which has been used to fit and analyze experimental data.</p></sec><sec id="S25"><title>A 2-parameter version of SimpleDIVA</title><p id="P72">The SimpleDIVA model, as presented in [<xref ref-type="bibr" rid="R39">39</xref>], is a simple 3-parameter model based on the DIVA model [<xref ref-type="bibr" rid="R40">40</xref>–<xref ref-type="bibr" rid="R42">42</xref>]. It is used to model feedback and feedforward control mechanisms involved in the production of formant trajectories. It has been developed to model and fit data from adaptation to altered auditory feedback experiments. The model simply assumes that the formant pattern produced by the speaker at a given trial <italic>n</italic> is the sum of the feedforward command and a sensory feedback-based correction, hence: <disp-formula id="FD9"><label>(9)</label><mml:math id="M64"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mtext>prod </mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>Δ</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P73">where <italic>F</italic><sub>prod</sub>(<italic>n</italic>) is the produced formant at trial <italic>n</italic>, <italic>F<sub>FF</sub></italic>(<italic>n</italic>) is the feedforward command, and <italic>ΔF<sub>FB</sub></italic> (<italic>n</italic>) is the sensory feedback-based correction. The feedback-based correction is a weighted sum of the contributions of both auditory and somatosensory feedback, as: <disp-formula id="FD10"><label>(10)</label><mml:math id="M65"><mml:mrow><mml:mo>Δ</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P74">where <italic>c<sub>A</sub></italic> and <italic>c<sub>S</sub></italic> are coefficients that specify the contributions of auditory and somatosensory feedback, respectively, in the feedback-based correction, <italic>FT</italic> is the vector containing the target formants, and <italic>F<sub>AF</sub></italic> (<italic>n</italic>), and <italic>F<sub>SF</sub></italic>(<italic>n</italic>) are formants corresponding to the auditory and somatosensory feedback received at trial n, respectively. SimpleDIVA assumes that the feedforward command <italic>F<sub>FF</sub></italic> is regularly updated based on the feedback received on the preceding trial. The equation for updating <italic>F<sub>FF</sub></italic> is <disp-formula id="FD11"><label>(11)</label><mml:math id="M66"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mo>Δ</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P75">where <italic>λ</italic> is a parameter which controls the rate at which adaptation is performed by the speaker.</p><p id="P76">The three parameters of the SimpleDIVA model are then <italic>c<sub>A</sub>, c<sub>S</sub></italic>, and λ. Optimization procedures can be used to fit the model to real experimental data [<xref ref-type="bibr" rid="R39">39</xref>]. These consist of finding values for the three parameters of the SimpleDIVA model for which the generated formant trajectories best fit the formant trajectories extracted from the experimental recordings. This has been done, for instance, to compare the values of these parameters between different populations, such as adults and children, and children with complex speech sound disorders [<xref ref-type="bibr" rid="R24">24</xref>].</p><p id="P77">From <xref ref-type="disp-formula" rid="FD10">Eqs. (10)</xref> and <xref ref-type="disp-formula" rid="FD11">(11)</xref>, one can see that the <italic>λ</italic> parameter multiplies the coefficients <italic>c<sub>A</sub></italic> and <italic>c<sub>S</sub></italic> by a common factor, which makes the λ parameter mathematically redundant. That is, the SimpleDIVA model can be reduced to a simple 2-parameter model by setting <italic>λ</italic> =1. Doing so reduces the complexity of the fit and ensures the uniqueness of the solution.</p></sec></sec><sec id="S26"><title>Results of the fits</title><p id="P78"><xref ref-type="table" rid="T2">Table 2</xref> presents the results of the fits for both our approach (XT/3C) and SimpleDIVA [<xref ref-type="bibr" rid="R39">39</xref>], and <xref ref-type="fig" rid="F9">Figure 9</xref> shows the comparison between the generated F1 trajectories obtained from the fit with our approach and SimpleDIVA, and the experimental F1 trajectories from [<xref ref-type="bibr" rid="R21">21</xref>]. The fit error (column NRMSE) shows very similar values for both XT/3C (mean NRMSE is 0.0442) and SimpleDIVA (mean NRMSE is 0.0436), as well as similar Pearson correlation coefficient (mean <italic>r</italic> is 0.40 for XT/3C, 0.43 for SimpleDIVA) for all speakers. This similar ability of XT/3C and SimpleDIVA to fit experimental trajectories is illustrated in <xref ref-type="fig" rid="F9">Figure 9</xref>, as the F1 trajectories generated by both models are very close to each other. This shows that our approach can quantitatively reproduce adaptation behavior with performance that is comparable to that of SimpleDIVA.</p><p id="P79">Interestingly, we found strong correlations between the estimated parameters of our model and the estimated parameters of SimpleDIVA, as shown in <xref ref-type="fig" rid="F10">Figure 10</xref>. The weight <italic>α<sub>A</sub></italic> assigned to the auditory recognition cost in our approach (relatively to <italic>α<sub>A</sub></italic> + <italic>α<sub>S</sub></italic>) and <italic>c<sub>A</sub></italic> the auditory gain in of SimpleDIVA (relatively to <italic>c<sub>A</sub></italic> + <italic>c<sub>S</sub></italic>) are correlated with a Pearson correlation coefficient of 0.65 (<italic>p</italic> &lt; 0.001). The somatosensory weight <italic>α<sub>S</sub></italic> in our approach and the somatosensory gain <italic>c<sub>S</sub></italic> of SimpleDIVA are similarly correlated. This is interesting given that despite some obvious architectural similarities, the two models are based on different principles. The key difference being that SimpleDIVA updates the feedforward command by a compensation factor proportional to the difference between the feedback and the target, while our approach updates the feedforward command because the estimation of the optimal solution of the cost function changes due to the update of the speaker’s internal forward models.</p></sec><sec id="S27" sec-type="conclusions"><title>Conclusion</title><p id="P80">This paper has presented a development of the optimization-based model of speech production, derived from [<xref ref-type="bibr" rid="R6">6</xref>–<xref ref-type="bibr" rid="R9">9</xref>], that forms the Phonetic Planning component of Turk and Shattuck-Hufnagel’s XT/3C model [<xref ref-type="bibr" rid="R1">1</xref>, <xref ref-type="bibr" rid="R2">2</xref>]. This new version is an extension of the original model, where extensions include adding a somatosensory-based loop of intelligibility prediction and a continuous update of internal sensory predictions based on sensory feedback. We have shown that our new model accounts for the key acoustic and articulatory features of motorsensory adaptation of speech to altered sensory feedback. More precisely, these key features include gradual (Objective A) and incomplete adaption (objective B), inter-speaker variability (Objective C), adaptive modification in unaltered formants (Objective D), and articulatory adaptation to altered somatosensory feedback (Objective E).</p><p id="P81">In our approach, the mechanism of motorsensory adaptation is based on regular and continuous updates of the speaker’s internal models that drive the Motor-Sensory Implementation stage of speech production, based on sensory feedback perceived by the speaker. The updated internal models are used during the optimization of the cost function that occurs during Phonetic Planning for any subsequent production. The internal models that require updates to account for compensation and adaptation behavior are 1) a model that predicts the acoustic consequences of motor (articulatory commands) and 2) a model that predicts the somatosensory sensations (here tract variables) from given motor commands. The idea of continuous updates of internal models is compatible with the suggestions of [<xref ref-type="bibr" rid="R66">66</xref>] and provides appropriate mechanisms for flexible and continuous learning of speech production. Internal models of the relationship between articulation and their sensory consequences are proposed to develop from infancy; flexible and continuous updates of these models are required to account for adjustments to speech articulation in reaction to changes in vocal tract anatomy (during growth, after surgery, or during temporary speech disorders), as well as in reaction to a changing external environment (e.g. environmental noise, or altered feedback in experimental situations).</p><p id="P82">We have modeled these internal forward models as Artificial Neural networks (ANN). The weights of these ANN internal forward models are updated at each iteration, using the motor commands passed to the plant and the feedback that result from these motor commands. Phonetic planning (cost-function minimization) that makes use of these internal models successfully reproduces gradual motor-sensory adaptation to altered feedback, both when auditory feedback is altered (F1 shifted), and when somatosensory feedback is altered (forced jaw and lip protrusions).</p><p id="P83">When only the auditory feedback was perturbed, the optimization-based model of speech production returned articulatory output for which the first formant frequency moved gradually in the direction opposite to the perturbation, fulfilling Objective A. When jaw and lip protrusion were perturbed (artificially increased), the model predicted a compensation in the motor commands that resulted in perturbed jaw and lip protrusion at the end of the perturbation phase which were closer to the baseline than at the beginning of the perturbation phase. This shows that adaptation to altered somatosensory feedback could also be modeled using our approach, as required by Objective E. Our simulations also show that the rate at which the adaptation was made could be controlled by adjusting the learning rate of the internal models. Basically, the higher the learning rate, the faster the adaptation. Regarding inter-speaker variability, which is Objective C, this shows that different adaptation behavior can be controlled and modeled with our approach via modifications of meaningful model’s parameters.</p><p id="P84">The simulation results with and without a somatosensory intelligibility criterion showed the necessity of having both auditory and somatosensory intelligibility costs in our model. More specifically, using a single auditory-based intelligibility criterion incorrectly results in complete (or almost complete) adaptation, whereas, observations in the literature consistently show incomplete adaptation (e.g., in [<xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R19">19</xref>, <xref ref-type="bibr" rid="R21">21</xref>, <xref ref-type="bibr" rid="R23">23</xref>, <xref ref-type="bibr" rid="R54">54</xref>]). We therefore introduced an intelligibility cost based on somatosensory characteristics, to be used alongside the intelligibility cost based on acoustic (or auditory) characteristics, which we had used in our previous studies [<xref ref-type="bibr" rid="R6">6</xref>–<xref ref-type="bibr" rid="R9">9</xref>]. This extension was inspired by feedback-based models of speech production, which generally use two sensory feedback subsystems, to account for auditory and somatosensory feedback [<xref ref-type="bibr" rid="R36">36</xref>, <xref ref-type="bibr" rid="R37">37</xref>, <xref ref-type="bibr" rid="R39">39</xref>, <xref ref-type="bibr" rid="R41">41</xref>–<xref ref-type="bibr" rid="R43">43</xref>, <xref ref-type="bibr" rid="R45">45</xref>], and is conceptually most similar to Bayesian GEPPETO [<xref ref-type="bibr" rid="R36">36</xref>, <xref ref-type="bibr" rid="R37">37</xref>, <xref ref-type="bibr" rid="R43">43</xref>].</p><p id="P85">Our simulations showed that the introduction of this additional intelligibility score in the cost function successfully reproduced the phenomenon of incomplete compensation, which fulfills Objective B. The compensation rate could be modified by adjusting the relative weights assigned to both intelligibility functions, which provides another source of inter-speaker variability. Additionally, our simulations highlighted another source of variability, namely the ratio between the weights assigned to effort vs. recognition costs, which result in different degrees of hyper- vs. hypo- articulation. Indeed, our model predicts that modifying the weight assigned to the effort cost, which is directly related to patterns of hyper-/hypo- articulation, changes adaptation behavior. To the best of our knowledge, this is the first study of the impact of planned hyper-articulation on adaptation behavior. Consequently, this observation requires further experimental investigation for validation. These results show that our model can successfully simulate inter-speaker variability of adaptation behavior, as required by Objective C. This is because the characteristics of this adaptation behavior can be modified by adjusting the parameters of our model: the learning rate of the internal models impacts adaptation speed and the weights assigned to the auditory and somatosensory intelligibility tasks in the cost function impact the amount of compensation during adaptation.</p><p id="P86">Finally, we presented an evaluation of the approach in terms of model fits to experimentally observed adaptation behavior to altered auditory feedback. This consisted of estimating the model parameters for which our model output fit best experimental data taken from [<xref ref-type="bibr" rid="R21">21</xref>]. The results of this evaluation shows that our approach can accurately fit real F1 trajectories recorded during adaptation experiments. The fit error is very similar to the fit error obtained with SimpleDIVA [<xref ref-type="bibr" rid="R39">39</xref>], the only adaptation model which has been used to fit and analyze experimental data. Our results also showed that the parameters of our model were correlated with the parameters of SimpleDIVA.</p><p id="P87">The weights assigned to auditory and somatosensory intelligibility tasks are correlated with the gains of the auditory and somatosensory feedback control subsystems, respectively. Similarly to SimpleDIVA [<xref ref-type="bibr" rid="R39">39</xref>], this inversion method (estimating the input parameters of a forward model from observations of its outputs) presents the interest of interpreting the results of adaptation experiments in terms of speakers’ sensory preference and learning rate, that is with a few number of meaningful and interpretable parameters. In addition, unlike SimpleDIVA, which only predicts acoustic consequences of altered acoustic feedback during adaptation experiments, our approach offers the possibility of investigating the articulatory consequences of altered motorsensory feedback. It would be interesting in the near future to compare our predictions of articulatory compensation with real data taken from perturbation experiments.</p></sec></body><back><ack id="S28"><title>Acknowledgements</title><p>This research has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (Grant agreement No. 101019847).</p></ack><fn-group><fn id="FN1"><label>1</label><p id="P88">Note that we could also include a duration cost to account for the brevity requirement, as proposed in [<xref ref-type="bibr" rid="R5">5</xref>, <xref ref-type="bibr" rid="R6">6</xref>, <xref ref-type="bibr" rid="R48">48</xref>–<xref ref-type="bibr" rid="R50">50</xref>]. For the sake of simplicity, this is not included in this paper, mainly because we will simulate static production of vowels</p></fn><fn id="FN2"><label>2</label><p id="P89">Note that we recently showed in [<xref ref-type="bibr" rid="R61">61</xref>] that our model can potentially accept intelligibility based on a wider stretch of speech.</p></fn><fn id="FN3"><label>3</label><p id="P90">The experimental data have been extracted from the authors’ repository at <ext-link ext-link-type="uri" xlink:href="https://osf.io/s84df/">https://osf.io/s84df/</ext-link>.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Turk</surname><given-names>A</given-names></name><name><surname>Shattuck-Hufnagel</surname><given-names>S</given-names></name></person-group><chapter-title>How do timing mechanisms work?</chapter-title><source>Speech timing: Implications for theories of phonology, speech production, and speech motor control</source><publisher-name>Oxford University Press</publisher-name><publisher-loc>USA</publisher-loc><year>2020</year><volume>5</volume><fpage>238</fpage><lpage>263</lpage></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turk</surname><given-names>A</given-names></name><name><surname>Shattuck-Hufnagel</surname><given-names>S</given-names></name></person-group><article-title>Timing evidence for symbolic phonological representations and phonology-extrinsic timing in speech production</article-title><source>Frontiers in Psychology</source><year>2020</year><volume>10</volume><fpage>2952</fpage><pub-id pub-id-type="pmcid">PMC6993048</pub-id><pub-id pub-id-type="pmid">32038364</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2019.02952</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nelson</surname><given-names>WL</given-names></name></person-group><article-title>Physical principles for economies of skilled movements</article-title><source>Biological cybernetics</source><year>1983</year><volume>46</volume><issue>2</issue><fpage>135</fpage><lpage>147</lpage><pub-id pub-id-type="pmid">6838914</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lindblom</surname><given-names>B</given-names></name></person-group><chapter-title>Explaining phonetic variation: A sketch of the H&amp;H theory</chapter-title><source>Speech production and speech modelling</source><publisher-name>Springer</publisher-name><year>1990</year><fpage>403</fpage><lpage>439</lpage></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simko</surname><given-names>J</given-names></name><name><surname>Cummins</surname><given-names>F</given-names></name></person-group><article-title>Embodied task dynamics</article-title><source>Psychological review</source><year>2010</year><volume>117</volume><issue>4</issue><elocation-id>1229â€”–1246</elocation-id><pub-id pub-id-type="pmid">21038977</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elie</surname><given-names>B</given-names></name><name><surname>Šimko</surname><given-names>J</given-names></name><name><surname>Turk</surname><given-names>A</given-names></name></person-group><article-title>Optimization-based planning of speech articulation using general Tau Theory</article-title><source>Speech Communication</source><year>2024</year><volume>160</volume><elocation-id>103083</elocation-id><pub-id pub-id-type="doi">10.1016/j.specom.2024.103083</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Elie</surname><given-names>B</given-names></name><name><surname>Šimko</surname><given-names>J</given-names></name><name><surname>Turk</surname><given-names>A</given-names></name></person-group><source>Optimal Control Theory of speech production using probabilistic articulatory-acoustic models</source><conf-name>20th International Conference of Phonetic Sciences (ICPhS)</conf-name><year>2023</year></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Elie</surname><given-names>B</given-names></name><name><surname>Šimko</surname><given-names>J</given-names></name><name><surname>Turk</surname><given-names>A</given-names></name></person-group><source>Optimal control of speech with context-dependent articulatory targets</source><conf-name>Interspeech 2023</conf-name><conf-loc>Dublin</conf-loc><year>2023</year></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elie</surname><given-names>B</given-names></name><name><surname>Åimko</surname><given-names>J</given-names></name><name><surname>Turk</surname><given-names>A</given-names></name></person-group><article-title>Optimization-based modeling of Lombard speech articulation: Supraglottal characteristics</article-title><source>JASA Express Letters</source><year>2024</year><volume>4</volume><issue>1</issue><elocation-id>015204</elocation-id><pub-id pub-id-type="pmid">38206126</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shiller</surname><given-names>DM</given-names></name><name><surname>Sato</surname><given-names>M</given-names></name><name><surname>Gracco</surname><given-names>VL</given-names></name><name><surname>Baum</surname><given-names>SR</given-names></name></person-group><article-title>Perceptual recalibration of speech sounds following speech motor learning</article-title><source>The Journal of the Acoustical Society of America</source><year>2009</year><volume>125</volume><issue>2</issue><fpage>1103</fpage><lpage>1113</lpage><pub-id pub-id-type="pmid">19206885</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rochet-Capellan</surname><given-names>A</given-names></name><name><surname>Ostry</surname><given-names>DJ</given-names></name></person-group><article-title>Simultaneous acquisition of multiple auditory–motor transformations in speech</article-title><source>Journal of Neuroscience</source><year>2011</year><volume>31</volume><issue>7</issue><fpage>2657</fpage><lpage>2662</lpage><pub-id pub-id-type="pmcid">PMC3079285</pub-id><pub-id pub-id-type="pmid">21325534</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6020-10.2011</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mitsuya</surname><given-names>T</given-names></name><name><surname>MacDonald</surname><given-names>EN</given-names></name><name><surname>Purcell</surname><given-names>DW</given-names></name><name><surname>Munhall</surname><given-names>KG</given-names></name></person-group><article-title>A cross-language study of compensation in response to real-time formant perturbation</article-title><source>The Journal of the Acoustical Society of America</source><year>2011</year><volume>130</volume><issue>5</issue><fpage>2978</fpage><lpage>2986</lpage><pub-id pub-id-type="pmcid">PMC3261647</pub-id><pub-id pub-id-type="pmid">22087926</pub-id><pub-id pub-id-type="doi">10.1121/1.3643826</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lametti</surname><given-names>DR</given-names></name><name><surname>Nasir</surname><given-names>SM</given-names></name><name><surname>Ostry</surname><given-names>DJ</given-names></name></person-group><article-title>Sensory preference in speech production revealed by simultaneous alteration of auditory and somatosensory feedback</article-title><source>Journal of Neuroscience</source><year>2012</year><volume>32</volume><issue>27</issue><fpage>9351</fpage><lpage>9358</lpage><pub-id pub-id-type="pmcid">PMC3404292</pub-id><pub-id pub-id-type="pmid">22764242</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0404-12.2012</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mollaei</surname><given-names>F</given-names></name><name><surname>Shiller</surname><given-names>DM</given-names></name><name><surname>Gracco</surname><given-names>VL</given-names></name></person-group><article-title>Sensorimotor adaptation of speech in Parkinson’s disease</article-title><source>Movement Disorders</source><year>2013</year><volume>28</volume><issue>12</issue><fpage>1668</fpage><lpage>1674</lpage><pub-id pub-id-type="pmcid">PMC3812368</pub-id><pub-id pub-id-type="pmid">23861349</pub-id><pub-id pub-id-type="doi">10.1002/mds.25588</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lametti</surname><given-names>DR</given-names></name><name><surname>Rochet-Capellan</surname><given-names>A</given-names></name><name><surname>Neufeld</surname><given-names>E</given-names></name><name><surname>Shiller</surname><given-names>DM</given-names></name><name><surname>Ostry</surname><given-names>DJ</given-names></name></person-group><article-title>Plasticity in the human speech motor system drives changes in speech perception</article-title><source>Journal of Neuroscience</source><year>2014</year><volume>34</volume><issue>31</issue><fpage>10339</fpage><lpage>10346</lpage><pub-id pub-id-type="pmcid">PMC4115140</pub-id><pub-id pub-id-type="pmid">25080594</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0108-14.2014</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lametti</surname><given-names>DR</given-names></name><name><surname>Smith</surname><given-names>HJ</given-names></name><name><surname>Freidin</surname><given-names>PF</given-names></name><name><surname>Watkins</surname><given-names>KE</given-names></name></person-group><article-title>Cortico-cerebellar networks drive sensorimotor learning in speech</article-title><source>Journal of cognitive neuroscience</source><year>2018</year><volume>30</volume><issue>4</issue><fpage>540</fpage><lpage>551</lpage><pub-id pub-id-type="pmid">29211651</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daliri</surname><given-names>A</given-names></name><name><surname>Wieland</surname><given-names>EA</given-names></name><name><surname>Cai</surname><given-names>S</given-names></name><name><surname>Guenther</surname><given-names>FH</given-names></name><name><surname>Chang</surname><given-names>SE</given-names></name></person-group><article-title>Auditory-motor adaptation is reduced in adults who stutter but not in children who stutter</article-title><source>Developmental science</source><year>2016</year><volume>21</volume><issue>2</issue><elocation-id>e12521</elocation-id><pub-id pub-id-type="pmcid">PMC5581739</pub-id><pub-id pub-id-type="pmid">28256029</pub-id><pub-id pub-id-type="doi">10.1111/desc.12521</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ballard</surname><given-names>KJ</given-names></name><name><surname>Halaki</surname><given-names>M</given-names></name><name><surname>Sowman</surname><given-names>P</given-names></name><name><surname>Kha</surname><given-names>A</given-names></name><name><surname>Daliri</surname><given-names>A</given-names></name><name><surname>Robin</surname><given-names>DA</given-names></name><etal/></person-group><article-title>An investigation of compensation and adaptation to auditory perturbations in individuals with acquired apraxia of speech</article-title><source>Frontiers in Human Neuroscience</source><year>2018</year><volume>12</volume><fpage>510</fpage><pub-id pub-id-type="pmcid">PMC6305734</pub-id><pub-id pub-id-type="pmid">30618687</pub-id><pub-id pub-id-type="doi">10.3389/fnhum.2018.00510</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname><given-names>CD</given-names></name><name><surname>Niziolek</surname><given-names>CA</given-names></name><name><surname>Duñabeitia</surname><given-names>JA</given-names></name><name><surname>Perez</surname><given-names>A</given-names></name><name><surname>Hernandez</surname><given-names>D</given-names></name><name><surname>Carreiras</surname><given-names>M</given-names></name><etal/></person-group><article-title>Online adaptation to altered auditory feedback is predicted by auditory acuity and not by domain-general executive control resources</article-title><source>Frontiers in Human Neuroscience</source><year>2018</year><volume>12</volume><fpage>91</fpage><pub-id pub-id-type="pmcid">PMC5857594</pub-id><pub-id pub-id-type="pmid">29593516</pub-id><pub-id pub-id-type="doi">10.3389/fnhum.2018.00091</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kothare</surname><given-names>H</given-names></name><name><surname>Raharjo</surname><given-names>I</given-names></name><name><surname>Ramanarayanan</surname><given-names>V</given-names></name><name><surname>Ranasinghe</surname><given-names>K</given-names></name><name><surname>Parrell</surname><given-names>B</given-names></name><name><surname>Johnson</surname><given-names>K</given-names></name><etal/></person-group><article-title>Sensorimotor adaptation of speech depends on the direction of auditory feedback alteration</article-title><source>The Journal of the Acoustical Society of America</source><year>2020</year><volume>148</volume><issue>6</issue><fpage>3682</fpage><lpage>3697</lpage><pub-id pub-id-type="pmcid">PMC7738200</pub-id><pub-id pub-id-type="pmid">33379892</pub-id><pub-id pub-id-type="doi">10.1121/10.0002876</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>KS</given-names></name><name><surname>Max</surname><given-names>L</given-names></name></person-group><article-title>Speech auditory-motor adaptation to formant-shifted feedback lacks an explicit component: Reduced adaptation in adults who stutter reflects limitations in implicit sensorimotor learning</article-title><source>European Journal of Neuroscience</source><year>2021</year><volume>53</volume><issue>9</issue><fpage>3093</fpage><lpage>3108</lpage><pub-id pub-id-type="pmcid">PMC8259784</pub-id><pub-id pub-id-type="pmid">33675539</pub-id><pub-id pub-id-type="doi">10.1111/ejn.15175</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parrell</surname><given-names>B</given-names></name><name><surname>Niziolek</surname><given-names>CA</given-names></name></person-group><article-title>Increased speech contrast induced by sensorimotor adaptation to a nonuniform auditory perturbation</article-title><source>Journal of Neurophysiology</source><year>2021</year><volume>125</volume><issue>2</issue><fpage>638</fpage><lpage>647</lpage><pub-id pub-id-type="pmcid">PMC7948141</pub-id><pub-id pub-id-type="pmid">33356887</pub-id><pub-id pub-id-type="doi">10.1152/jn.00466.2020</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname><given-names>X</given-names></name><name><surname>Ouyang</surname><given-names>M</given-names></name><name><surname>Yin</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>Q</given-names></name></person-group><article-title>Sensorimotor Adaptation to Formant-Shifted Auditory Feedback Is Predicted by Language-Specific Factors in L1 and L2 Speech Production</article-title><source>Language and Speech</source><year>2023</year><elocation-id>00238309231202503</elocation-id><pub-id pub-id-type="pmid">37830332</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Terband</surname><given-names>H</given-names></name><name><surname>van Brenk</surname><given-names>F</given-names></name></person-group><article-title>Modeling Responses to Auditory Feedback Perturbations in Adults, Children, and Children With Complex Speech Sound Disorders: Evidence for Impaired Auditory Self-Monitoring?</article-title><source>Journal of speech, language, and hearing research</source><year>2023</year><volume>66</volume><issue>5</issue><fpage>1563</fpage><lpage>1587</lpage><pub-id pub-id-type="pmid">37071803</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patel</surname><given-names>R</given-names></name><name><surname>Niziolek</surname><given-names>C</given-names></name><name><surname>Reilly</surname><given-names>K</given-names></name><name><surname>Guenther</surname><given-names>FH</given-names></name></person-group><article-title>Prosodic Adaptations to Pitch Perturbation in Running Speech</article-title><source>Journal of Speech, Language, and Hearing Research</source><year>2011</year><volume>54</volume><issue>4</issue><fpage>1051</fpage><lpage>1059</lpage><pub-id pub-id-type="pmcid">PMC3352853</pub-id><pub-id pub-id-type="pmid">21173388</pub-id><pub-id pub-id-type="doi">10.1044/1092-4388(2010/10-0162)</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keough</surname><given-names>D</given-names></name><name><surname>Hawco</surname><given-names>C</given-names></name><name><surname>Jones</surname><given-names>JA</given-names></name></person-group><article-title>Auditory-motor adaptation to frequency-altered auditory feedback occurs when participants ignore feedback</article-title><source>BMC neuroscience</source><year>2013</year><volume>14</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="pmcid">PMC3602002</pub-id><pub-id pub-id-type="pmid">23497238</pub-id><pub-id pub-id-type="doi">10.1186/1471-2202-14-25</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feng</surname><given-names>Y</given-names></name><name><surname>Xiao</surname><given-names>Y</given-names></name><name><surname>Yan</surname><given-names>Y</given-names></name><name><surname>Max</surname><given-names>L</given-names></name></person-group><article-title>Adaptation in Mandarin tone production with pitch-shifted auditory feedback: Influence of tonal contrast requirements</article-title><source>Language, cognition and neuroscience</source><year>2018</year><volume>33</volume><issue>6</issue><fpage>734</fpage><lpage>749</lpage><pub-id pub-id-type="pmcid">PMC6097622</pub-id><pub-id pub-id-type="pmid">30128314</pub-id><pub-id pub-id-type="doi">10.1080/23273798.2017.1421317</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alemi</surname><given-names>R</given-names></name><name><surname>Lehmann</surname><given-names>A</given-names></name><name><surname>Deroche</surname><given-names>ML</given-names></name></person-group><article-title>Adaptation to pitch-altered feedback is independent of oneâ€™s own voice pitch sensitivity</article-title><source>Scientific reports</source><year>2020</year><volume>10</volume><issue>1</issue><elocation-id>16860</elocation-id><pub-id pub-id-type="pmcid">PMC7544828</pub-id><pub-id pub-id-type="pmid">33033324</pub-id><pub-id pub-id-type="doi">10.1038/s41598-020-73932-1</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>DL</given-names></name><name><surname>McDaniel</surname><given-names>A</given-names></name><name><surname>Watkins</surname><given-names>KE</given-names></name></person-group><article-title>Disruption of speech motor adaptation with repetitive transcranial magnetic stimulation of the articulatory representation in primary motor cortex</article-title><source>cortex</source><year>2021</year><volume>145</volume><fpage>115</fpage><lpage>130</lpage><pub-id pub-id-type="pmcid">PMC8650828</pub-id><pub-id pub-id-type="pmid">34717269</pub-id><pub-id pub-id-type="doi">10.1016/j.cortex.2021.09.008</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alemi</surname><given-names>R</given-names></name><name><surname>Lehmann</surname><given-names>A</given-names></name><name><surname>Deroche</surname><given-names>ML</given-names></name></person-group><article-title>Changes in spoken and sung productions following adaptation to Pitch-shifted auditory feedback</article-title><source>Journal of Voice</source><year>2023</year><volume>37</volume><issue>3</issue><elocation-id>466-e1</elocation-id><pub-id pub-id-type="pmid">33745802</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dahl</surname><given-names>KL</given-names></name><name><surname>Cádiz</surname><given-names>MD</given-names></name><name><surname>Zuk</surname><given-names>J</given-names></name><name><surname>Guenther</surname><given-names>FH</given-names></name><name><surname>Stepp</surname><given-names>CE</given-names></name></person-group><article-title>Controlling pitch for prosody: Sensorimotor adaptation in linguistically meaningful contexts</article-title><source>Journal of Speech, Language, and Hearing Research</source><year>2024</year><volume>67</volume><issue>2</issue><fpage>440</fpage><lpage>454</lpage><pub-id pub-id-type="pmcid">PMC11000799</pub-id><pub-id pub-id-type="pmid">38241671</pub-id><pub-id pub-id-type="doi">10.1044/2023_JSLHR-23-00460</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karlin</surname><given-names>R</given-names></name><name><surname>Naber</surname><given-names>C</given-names></name><name><surname>Parrell</surname><given-names>B</given-names></name></person-group><article-title>Auditory feedback is used for adaptation and compensation in speech timing</article-title><source>Journal of Speech, Language, and Hearing Research</source><year>2021</year><volume>64</volume><issue>9</issue><fpage>3361</fpage><lpage>3381</lpage><pub-id pub-id-type="pmcid">PMC8642089</pub-id><pub-id pub-id-type="pmid">34310188</pub-id><pub-id pub-id-type="doi">10.1044/2021_JSLHR-21-00021</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gay</surname><given-names>T</given-names></name><name><surname>Lindblom</surname><given-names>B</given-names></name><name><surname>Lubker</surname><given-names>J</given-names></name></person-group><article-title>Production of bite-block vowels: Acoustic equivalence by selective compensation</article-title><source>The Journal of the Acoustical Society of America</source><year>1981</year><volume>69</volume><issue>3</issue><fpage>802</fpage><lpage>810</lpage><pub-id pub-id-type="pmid">7240561</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Savariaux</surname><given-names>C</given-names></name><name><surname>Perrier</surname><given-names>P</given-names></name><name><surname>Orliaguet</surname><given-names>JP</given-names></name><name><surname>Schwartz</surname><given-names>JL</given-names></name></person-group><article-title>Compensation strategies for the perturbation of French [u] using a lip tube. II. Perceptual analysis</article-title><source>The Journal of the Acoustical Society of America</source><year>1999</year><volume>106</volume><issue>1</issue><fpage>381</fpage><lpage>393</lpage><pub-id pub-id-type="pmid">10420629</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tremblay</surname><given-names>S</given-names></name><name><surname>Shiller</surname><given-names>DM</given-names></name><name><surname>Ostry</surname><given-names>DJ</given-names></name></person-group><article-title>Somatosensory basis of speech production</article-title><source>Nature</source><year>2003</year><volume>423</volume><issue>6942</issue><fpage>866</fpage><lpage>869</lpage><pub-id pub-id-type="pmid">12815431</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patri</surname><given-names>JF</given-names></name><name><surname>Perrier</surname><given-names>P</given-names></name><name><surname>Schwartz</surname><given-names>JL</given-names></name><name><surname>Diard</surname><given-names>J</given-names></name></person-group><article-title>What drives the perceptual change resulting from speech motor adaptation? Evaluation of hypotheses in a Bayesian modeling framework</article-title><source>PLoS computational biology</source><year>2018</year><volume>14</volume><issue>1</issue><elocation-id>e1005942</elocation-id><pub-id pub-id-type="pmcid">PMC5794199</pub-id><pub-id pub-id-type="pmid">29357357</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005942</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patri</surname><given-names>JF</given-names></name><name><surname>Diard</surname><given-names>J</given-names></name><name><surname>Perrier</surname><given-names>P</given-names></name></person-group><article-title>Modeling sensory preference in speech motor planning: a Bayesian modeling framework</article-title><source>Frontiers in Psychology</source><year>2019</year><volume>10</volume><elocation-id>469247</elocation-id><pub-id pub-id-type="pmcid">PMC6824204</pub-id><pub-id pub-id-type="pmid">31708828</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2019.02339</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parrell</surname><given-names>B</given-names></name><name><surname>Ramanarayanan</surname><given-names>V</given-names></name><name><surname>Nagarajan</surname><given-names>S</given-names></name><name><surname>Houde</surname><given-names>J</given-names></name></person-group><article-title>The FACTS model of speech motor control: Fusing state estimation and task-based control</article-title><source>PLoS computational biology</source><year>2019</year><volume>15</volume><issue>9</issue><elocation-id>e1007321</elocation-id><pub-id pub-id-type="pmcid">PMC6743785</pub-id><pub-id pub-id-type="pmid">31479444</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007321</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kearney</surname><given-names>E</given-names></name><name><surname>Nieto-Castañón</surname><given-names>A</given-names></name><name><surname>Weerathunge</surname><given-names>HR</given-names></name><name><surname>Falsini</surname><given-names>R</given-names></name><name><surname>Daliri</surname><given-names>A</given-names></name><name><surname>Abur</surname><given-names>D</given-names></name><etal/></person-group><article-title>A simple 3-parameter model for examining adaptation in speech and voice production</article-title><source>Frontiers in psychology</source><year>2020</year><volume>10</volume><elocation-id>468734</elocation-id><pub-id pub-id-type="pmcid">PMC6985569</pub-id><pub-id pub-id-type="pmid">32038381</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2019.02995</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guenther</surname><given-names>FH</given-names></name></person-group><article-title>Speech sound acquisition, coarticulation, and rate effects in a neural network model of speech production</article-title><source>Psychological review</source><year>1995</year><volume>102</volume><issue>3</issue><fpage>594</fpage><pub-id pub-id-type="pmid">7624456</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Guenther</surname><given-names>FH</given-names></name></person-group><chapter-title>Neural control of speech movements</chapter-title><person-group person-group-type="editor"><name><surname>Schiller</surname><given-names>NO</given-names></name><name><surname>Meyer</surname><given-names>AS</given-names></name></person-group><source>Phonetics and phonology in language comprehension and production: Differences and similarities</source><publisher-name>Mouton de Gruyter</publisher-name><publisher-loc>Berlin</publisher-loc><year>2003</year><fpage>209</fpage><lpage>239</lpage></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Guenther</surname><given-names>FH</given-names></name></person-group><source>Neural control of speech</source><publisher-name>Mit Press</publisher-name><year>2016</year></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patri</surname><given-names>JF</given-names></name><name><surname>Diard</surname><given-names>J</given-names></name><name><surname>Perrier</surname><given-names>P</given-names></name></person-group><article-title>Optimal speech motor control and token-to-token variability: a Bayesian modeling approach</article-title><source>Biological cybernetics</source><year>2015</year><volume>109</volume><fpage>611</fpage><lpage>626</lpage><pub-id pub-id-type="pmid">26497359</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Perrier</surname><given-names>P</given-names></name><name><surname>Ma</surname><given-names>L</given-names></name><name><surname>Payan</surname><given-names>Y</given-names></name></person-group><source>Modeling the production of VCV sequences via the inversion of a biomechanical model of the tongue</source><conf-name>Proceedings of the 9th European Conference on Speech Communication and Technology. InterSpeech’2005 Editor</conf-name><year>2005</year><fpage>1041</fpage><lpage>1044</lpage><comment>ISSN 1018-4074</comment></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>KS</given-names></name><name><surname>Gaines</surname><given-names>JL</given-names></name><name><surname>Parrell</surname><given-names>B</given-names></name><name><surname>Ramanarayanan</surname><given-names>V</given-names></name><name><surname>Nagarajan</surname><given-names>SS</given-names></name><name><surname>Houde</surname><given-names>JF</given-names></name></person-group><article-title>Mechanisms of sensorimotor adaptation in a hierarchical state feedback control model of speech</article-title><source>PLoS Computational Biology</source><year>2023</year><volume>19</volume><issue>7</issue><elocation-id>e1011244</elocation-id><pub-id pub-id-type="pmcid">PMC10434967</pub-id><pub-id pub-id-type="pmid">37506120</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1011244</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Todorov</surname><given-names>E</given-names></name><name><surname>Jordan</surname><given-names>MI</given-names></name></person-group><article-title>Optimal feedback control as a theory of motor coordination</article-title><source>Nature neuroscience</source><year>2002</year><volume>5</volume><issue>11</issue><fpage>1226</fpage><lpage>1235</lpage><pub-id pub-id-type="pmid">12404008</pub-id></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Todorov</surname><given-names>E</given-names></name></person-group><chapter-title>Optimal control theory</chapter-title><person-group person-group-type="editor"><name><surname>K</surname><given-names>D</given-names></name></person-group><source>Bayesian brain: probabilistic approaches to neural coding</source><publisher-name>MIT press</publisher-name><publisher-loc>Cambridge, MA</publisher-loc><year>2006</year><fpage>268</fpage><lpage>298</lpage></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Šimko</surname><given-names>J</given-names></name><name><surname>O’Dell</surname><given-names>M</given-names></name><name><surname>Vainio</surname><given-names>M</given-names></name></person-group><article-title>Emergent consonantal quantity contrast and context-dependence of gestural phasing</article-title><source>Journal of Phonetics</source><year>2014</year><volume>44</volume><fpage>130</fpage><lpage>151</lpage></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Šimko</surname><given-names>J</given-names></name><name><surname>Beňuš</surname><given-names>Š</given-names></name><name><surname>Vainio</surname><given-names>M</given-names></name></person-group><article-title>Hyperarticulation in Lombard speech: Global coordination of the jaw, lips and the tongue</article-title><source>The Journal of the Acoustical Society of America</source><year>2016</year><volume>139</volume><issue>1</issue><fpage>151</fpage><lpage>162</lpage><pub-id pub-id-type="pmid">26827013</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beňuš</surname><given-names>Š</given-names></name><name><surname>Šimko</surname><given-names>J</given-names></name></person-group><article-title>Emergence of prosodic boundary: continuous effects of temporal affordance on inter-gestural timing</article-title><source>Journal of Phonetics</source><year>2014</year><volume>44</volume><fpage>110</fpage><lpage>129</lpage></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pierrehumbert</surname><given-names>J</given-names></name></person-group><source>Word-specific phonetics</source><publisher-name>Laboratory Phonology VII/Mouton de Gruyter</publisher-name><year>2002</year></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pierrehumbert</surname><given-names>JB</given-names></name></person-group><article-title>Phonological representation: Beyond abstract versus episodic</article-title><source>Annual Review of Linguistics</source><year>2016</year><volume>2</volume><issue>1</issue><fpage>33</fpage><lpage>52</lpage></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McQueen</surname><given-names>JM</given-names></name><name><surname>Cutler</surname><given-names>A</given-names></name><name><surname>Norris</surname><given-names>D</given-names></name></person-group><article-title>Phonological abstraction in the mental lexicon</article-title><source>Cognitive science</source><year>2006</year><volume>30</volume><issue>6</issue><fpage>1113</fpage><lpage>1126</lpage><pub-id pub-id-type="pmid">21702849</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kitchen</surname><given-names>NM</given-names></name><name><surname>Kim</surname><given-names>KS</given-names></name><name><surname>Wang</surname><given-names>PZ</given-names></name><name><surname>Hermosillo</surname><given-names>RJ</given-names></name><name><surname>Max</surname><given-names>L</given-names></name></person-group><article-title>Individual sensorimotor adaptation characteristics are independent across orofacial speech movements and limb reaching movements</article-title><source>Journal of Neurophysiology</source><year>2022</year><volume>128</volume><issue>3</issue><fpage>696</fpage><lpage>710</lpage><pub-id pub-id-type="pmcid">PMC9484989</pub-id><pub-id pub-id-type="pmid">35946809</pub-id><pub-id pub-id-type="doi">10.1152/jn.00167.2022</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scott</surname><given-names>TL</given-names></name><name><surname>Haenchen</surname><given-names>L</given-names></name><name><surname>Daliri</surname><given-names>A</given-names></name><name><surname>Chartove</surname><given-names>J</given-names></name><name><surname>Guenther</surname><given-names>FH</given-names></name><name><surname>Perrachione</surname><given-names>TK</given-names></name></person-group><article-title>Noninvasive neurostimulation of left ventral motor cortex enhances sensorimotor adaptation in speech production</article-title><source>Brain and Language</source><year>2020</year><volume>209</volume><elocation-id>104840</elocation-id><pub-id pub-id-type="pmcid">PMC7484095</pub-id><pub-id pub-id-type="pmid">32738502</pub-id><pub-id pub-id-type="doi">10.1016/j.bandl.2020.104840</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>T</given-names></name><name><surname>Lammert</surname><given-names>AC</given-names></name><name><surname>Parrell</surname><given-names>B</given-names></name></person-group><source>Modeling Sensorimotor Adaptation in Speech Through Alterations to Forward and Inverse Models</source><conf-name>Interspeech</conf-name><year>2021</year><fpage>3201</fpage><lpage>3205</lpage></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Maeda</surname><given-names>S</given-names></name></person-group><chapter-title>Compensatory articulation during speech: Evidence from the analysis and synthesis of vocal-tract shapes using an articulatory model</chapter-title><source>Speech production and speech modelling</source><publisher-name>Springer</publisher-name><year>1990</year><fpage>131</fpage><lpage>149</lpage></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gaines</surname><given-names>JL</given-names></name><name><surname>Kim</surname><given-names>KS</given-names></name><name><surname>Parrell</surname><given-names>B</given-names></name><name><surname>Ramanarayanan</surname><given-names>V</given-names></name><name><surname>Nagarajan</surname><given-names>SS</given-names></name><name><surname>Houde</surname><given-names>JF</given-names></name></person-group><article-title>Discrete constriction locations describe a comprehensive range of vocal tract shapes in the Maeda model</article-title><source>JASA express letters</source><year>2021</year><volume>1</volume><issue>12</issue><elocation-id>124402</elocation-id><pub-id pub-id-type="pmcid">PMC8715799</pub-id><pub-id pub-id-type="pmid">35005711</pub-id><pub-id pub-id-type="doi">10.1121/10.0009058</pub-id></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Elie</surname><given-names>B</given-names></name></person-group><source>Maedeep</source><year>2023</year><comment><ext-link ext-link-type="uri" xlink:href="https://git.ecdf.ed.ac.uk/belie/maedeep">https://git.ecdf.ed.ac.uk/belie/maedeep</ext-link></comment></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Ghosh</surname><given-names>S</given-names></name></person-group><source>VocalTractModels</source><year>2013</year><comment><ext-link ext-link-type="uri" xlink:href="https://github.com/sensein/VocalTractModels">https://github.com/sensein/VocalTractModels</ext-link></comment></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Elie</surname><given-names>B</given-names></name><name><surname>Simko</surname><given-names>J</given-names></name><name><surname>Turk</surname><given-names>A</given-names></name></person-group><source>A data-driven model of acoustic speech intelligibility for optimization-based models of speech production</source><conf-name>Interspeech</conf-name><year>2024</year><volume>2024</volume><fpage>3610</fpage><lpage>3614</lpage></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gibbon</surname><given-names>F</given-names></name></person-group><article-title>Lingual activity in two speech-disordered children’s attempts to produce velar and alveolar stop consonants: evidence from electropalatographic (EPG) data</article-title><source>International Journal of Language &amp; Communication Disorders</source><year>1990</year><volume>25</volume><issue>3</issue><fpage>329</fpage><lpage>340</lpage><pub-id pub-id-type="pmid">2095839</pub-id></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gibbon</surname><given-names>F</given-names></name><name><surname>Stewart</surname><given-names>F</given-names></name><name><surname>Hardcastle</surname><given-names>WJ</given-names></name><name><surname>Crampin</surname><given-names>L</given-names></name></person-group><article-title>Widening access to electropalatography for children with persistent sound system disorders</article-title><source>American Journal of Speech-Language Pathology</source><year>1999</year><volume>8</volume><issue>4</issue><fpage>319</fpage><lpage>334</lpage></element-citation></ref><ref id="R64"><label>64</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>L</given-names></name><name><surname>Cui</surname><given-names>X</given-names></name><name><surname>Pruvenok</surname><given-names>R</given-names></name><name><surname>Huang</surname><given-names>J</given-names></name><name><surname>Momen</surname><given-names>S</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><etal/></person-group><source>A database of vocal tract resonance trajectories for research in speech processing</source><conf-name>2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings</conf-name><conf-sponsor>IEEE</conf-sponsor><year>2006</year><fpage>369</fpage><lpage>372</lpage></element-citation></ref><ref id="R65"><label>65</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nelder</surname><given-names>JA</given-names></name><name><surname>Mead</surname><given-names>R</given-names></name></person-group><article-title>A simplex method for function minimization</article-title><source>The computer journal</source><year>1965</year><volume>7</volume><issue>4</issue><fpage>308</fpage><lpage>313</lpage></element-citation></ref><ref id="R66"><label>66</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perkell</surname><given-names>JS</given-names></name><name><surname>Guenther</surname><given-names>FH</given-names></name><name><surname>Lane</surname><given-names>H</given-names></name><name><surname>Matthies</surname><given-names>ML</given-names></name><name><surname>Perrier</surname><given-names>P</given-names></name><name><surname>Vick</surname><given-names>J</given-names></name><etal/></person-group><article-title>A theory of speech motor control and supporting data from speakers with normal hearing and with profound hearing loss</article-title><source>Journal of phonetics</source><year>2000</year><volume>28</volume><issue>3</issue><fpage>233</fpage><lpage>272</lpage></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Fig. 1</label><caption><title>Example of experimental data that show adaptation to altered auditory feedback.</title><p>Experimental data are taken from Kim and Max [<xref ref-type="bibr" rid="R21">21</xref>]. The experiments used proportional perturbations of F1, where the (upward) shift in % is shown on the right panel. The left panel shows the evolution of the recorded F1 (blue squares), and the auditory feedback received by the speaker (green circles), namely the first formant frequency F1 (altered during the perturbation phase). Solid lines represent a smoothed (filtered) version of the recorded and heard F1. The baseline corresponds to the mean value of the produced F1 before the perturbation phase (here, trials 11 to 20, with a baseline value of 530 Hz). The altered baseline corresponds to the value in Hz of the baseline when the perturbation is applied. The perturbation magnitude is the difference, in Hz, between the altered baseline and the baseline value. The compensation magnitude is the difference in Hz between the value of the auditory feedback received during the adaptation phase that is closest to the baseline, and the altered baseline.</p></caption><graphic xlink:href="EMS201300-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>The architecture of our adaptation model.</title><p>The top panel represents the optimization block. The bottom plot shows the general architecture of the model.</p></caption><graphic xlink:href="EMS201300-f002"/></fig><fig id="F3" position="float"><label>Fig. 3</label><caption><title>A typical example of simulated adaptation to altered auditory feedback.</title><p>A) shows simulated acoustic output in separate panels for formant frequencies F1, F2, and F3, including produced acoustics (blue solid line), auditory feedback (dash-dotted red line), and acoustics predicted by the auditory internal model (dashed green line). The bottom plot B) shows the mean simulated shape of the vocal tract as returned by the Maeda model, averaged across the baseline phase (without perturbation, trials 11 to 20), the early adaptation phase (trials 21 to 80), the late adaptation phase (trials 81 to 140), and the end phase (after-effects, trials 151 to 160).</p></caption><graphic xlink:href="EMS201300-f003"/></fig><fig id="F4" position="float"><label>Fig. 4</label><caption><p>Top panel: The evolution of produced first formant frequency (solid lines) during simulated trials for different values of <italic>α<sub>S</sub></italic>, the weight assigned to the somatosensory intelligibility and <inline-formula><mml:math id="M67"><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula>, the learning rate of the auditory internal model. From left to right: <italic>α<sub>S</sub></italic> =0 (only acoustic intelligibility is taken into account), <italic>α<sub>S</sub> =</italic> 0.4, <italic>α<sub>S</sub> =</italic> 0.7, and <italic>α<sub>S</sub></italic> = 1 (only somatosensory intelligibility is taken into account). The dash-dotted lines represents auditory feedback, and the dashed black line represents the baseline value of F1. The dotted black line corresponds to an example of experimentally recorded adaptation behavior from Kim and Max [<xref ref-type="bibr" rid="R21">21</xref>]. The bottom panel shows the compensation ratio, expressed in %, returned by the simulations as a function of <italic>α<sub>S</sub></italic> for two values of the learning rate, <inline-formula><mml:math id="M68"><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="M69"><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>.</p></caption><graphic xlink:href="EMS201300-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><p>Evolution of the simulated produced formant frequencies as a function of trials for different values of <italic>α<sub>E</sub></italic>, at fixed values of <italic>α<sub>A</sub></italic> = 0.6 and <italic>α<sub>S</sub></italic> = 0.4. The left panel shows F1 and the right panel shows F2.</p></caption><graphic xlink:href="EMS201300-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><p>The evolution of the simulated produced first formant frequency F1 as a function of trial number for different internal models. The left panel shows the effect of the number of samples used for training internal models with nominal architectures (100,000 samples are the nominal models, as presented in <xref ref-type="table" rid="T1">Table 1</xref>). The central panel shows the effect of the number of hidden layers (<italic>N</italic> = 3 are nominal models) and the right panel shows the effect of the size of hidden layers (<italic>L</italic> = 256 are nominal models).</p></caption><graphic xlink:href="EMS201300-f006"/></fig><fig id="F7" position="float"><label>Figure 7</label><caption><title>Example of jaw perturbation with <italic>k</italic> = 1.26 (left panel) and perturbation shift <italic>k</italic> as a function of trials used for the experiment (right panel).</title></caption><graphic xlink:href="EMS201300-f007"/></fig><fig id="F8" position="float"><label>Figure 8</label><caption><p>Simulated adaptation to altered somatosensory feedback for three values of <italic>α<sub>S</sub></italic>, namely <italic>α<sub>S</sub></italic> = 0 (left panels), <italic>α<sub>S</sub></italic> = 0.5 (central panels), and <italic>α<sub>S</sub></italic> = 1 (right panels). The top panels show simulated acoustic output (first formant frequency F1), including produced acoustics (blue solid line) and acoustics predicted by the auditory internal model (dashed green line). The bottom panels show simulated tract variable lip protrusion (LP), including virtual lip protrusion (without perturbation, solid blue line), received somatosensory feedback (after perturbation, dotted-dashed red line), and lip perturbation predicted by the somatosensory internal model (dashed green line).</p></caption><graphic xlink:href="EMS201300-f008"/></fig><fig id="F9" position="float"><label>Figure 9</label><caption><title>Results of best fits obtained with SimpleDIVA (red dashed line) and XT/3C (blue solid line) for the 24 speakers.</title></caption><graphic xlink:href="EMS201300-f009"/></fig><fig id="F10" position="float"><label>Figure 10</label><caption><p>Estimated parameters of our model (<italic>α<sub>E</sub></italic>, <italic>α<sub>A</sub></italic> and <inline-formula><mml:math id="M70"><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula>) as a function of the estimated SimpleDIVA parameters <italic>c<sub>A</sub></italic> and <italic>c<sub>S</sub></italic> for the 24 experimental F1 trajectories taken from [<xref ref-type="bibr" rid="R21">21</xref>].</p></caption><graphic xlink:href="EMS201300-f010"/></fig><table-wrap id="T1" position="float" orientation="portrait"><label>Table 1</label><caption><title>Architecture of the MLPs used for the nominal internal forward models</title></caption><table frame="hsides" rules="groups"><thead><tr style="border-top:hidden"><th align="left" valign="top" style="border-right: solid thin">Characteristics</th><th align="center" valign="middle" style="border-right: solid thin"><inline-formula><mml:math id="M71"><mml:mi mathvariant="script">A</mml:mi></mml:math></inline-formula></th><th align="center" valign="middle"><inline-formula><mml:math id="M72"><mml:mi mathvariant="script">S</mml:mi></mml:math></inline-formula></th></tr></thead><tbody><tr><td align="left" valign="top" style="border-right: solid thin">Training set sample size</td><td align="center" valign="middle" style="border-right: solid thin">10<sup>5</sup></td><td align="center" valign="middle">10<sup>5</sup></td></tr><tr><td align="left" valign="top" style="border-right: solid thin">Size of hidden layers</td><td align="center" valign="middle" style="border-right: solid thin">(40, 80, 40)</td><td align="center" valign="middle">(256, 256, 256, 256)</td></tr><tr style="border-bottom: hidden"><td align="left" valign="top" style="border-right: solid thin">Activation function</td><td align="center" valign="middle" style="border-right: solid thin">tanh</td><td align="center" valign="middle">tanh</td></tr></tbody></table></table-wrap><table-wrap id="T2" position="float" orientation="portrait"><label>Table 2</label><caption><title>Results of fits per speakers and method</title></caption><table frame="hsides" rules="cols"><thead><tr style="border-top: hidden; border-bottom: solid thin"><th align="center" valign="middle" rowspan="2">Speakers</th><th align="center" valign="middle" colspan="4">XT/3C</th><th align="center" valign="middle" colspan="4">SimpleDIVA</th></tr><tr style="border-bottom: solid thin"><th align="center" valign="middle">αA</th><th align="center" valign="middle"><inline-formula><mml:math id="M73"><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula></th><th align="center" valign="middle">NRMSE</th><th align="center" valign="middle"><italic>r</italic></th><th align="center" valign="middle"><italic>C<sub>A</sub></italic></th><th align="center" valign="middle"><italic>c<sub>S</sub></italic></th><th align="center" valign="middle">NRMSE</th><th align="center" valign="middle"><italic>r</italic></th></tr></thead><tbody><tr><td align="left" valign="top">S01</td><td align="center" valign="middle">0.84</td><td align="center" valign="middle">1.2e-05</td><td align="center" valign="middle">6.06e-02</td><td align="center" valign="middle">0.26</td><td align="center" valign="middle">5.78e-02</td><td align="center" valign="middle">2.16e-02</td><td align="center" valign="middle">5.90e-02</td><td align="center" valign="middle">0.28</td></tr><tr><td align="left" valign="top">S02</td><td align="center" valign="middle">0.77</td><td align="center" valign="middle">7.6e-04</td><td align="center" valign="middle">5.77e-02</td><td align="center" valign="middle">0.52</td><td align="center" valign="middle">1.72e-01</td><td align="center" valign="middle">9.57e-02</td><td align="center" valign="middle">5.25e-02</td><td align="center" valign="middle">0.61</td></tr><tr><td align="left" valign="top">S03</td><td align="center" valign="middle">0.50</td><td align="center" valign="middle">5.5e-05</td><td align="center" valign="middle">4.19e-02</td><td align="center" valign="middle">0.65</td><td align="center" valign="middle">1.32e-01</td><td align="center" valign="middle">1.82e-01</td><td align="center" valign="middle">3.84e-02</td><td align="center" valign="middle">0.72</td></tr><tr><td align="left" valign="top">S04</td><td align="center" valign="middle">0.50</td><td align="center" valign="middle">1.0e-05</td><td align="center" valign="middle">3.75e-02</td><td align="center" valign="middle">0.55</td><td align="center" valign="middle">1.04e-02</td><td align="center" valign="middle">1.42e-02</td><td align="center" valign="middle">3.63e-02</td><td align="center" valign="middle">0.58</td></tr><tr><td align="left" valign="top">S05</td><td align="center" valign="middle">0.51</td><td align="center" valign="middle">1.0e-03</td><td align="center" valign="middle">3.70e-02</td><td align="center" valign="middle">0.15</td><td align="center" valign="middle">2.80e-03</td><td align="center" valign="middle">2.70e-01</td><td align="center" valign="middle">3.73e-02</td><td align="center" valign="middle">0.13</td></tr><tr><td align="left" valign="top">S06</td><td align="center" valign="middle">0.58</td><td align="center" valign="middle">7.1e-05</td><td align="center" valign="middle">3.56e-02</td><td align="center" valign="middle">0.76</td><td align="center" valign="middle">3.57e-02</td><td align="center" valign="middle">3.37e-02</td><td align="center" valign="middle">3.68e-02</td><td align="center" valign="middle">0.77</td></tr><tr><td align="left" valign="top">S07</td><td align="center" valign="middle">0.56</td><td align="center" valign="middle">9.1e-05</td><td align="center" valign="middle">3.13e-02</td><td align="center" valign="middle">0.68</td><td align="center" valign="middle">5.51e-02</td><td align="center" valign="middle">3.39e-02</td><td align="center" valign="middle">2.97e-02</td><td align="center" valign="middle">0.69</td></tr><tr><td align="left" valign="top">S08</td><td align="center" valign="middle">0.57</td><td align="center" valign="middle">7.4e-05</td><td align="center" valign="middle">4.75e-02</td><td align="center" valign="middle">0.51</td><td align="center" valign="middle">5.68e-02</td><td align="center" valign="middle">3.56e-02</td><td align="center" valign="middle">4.70e-02</td><td align="center" valign="middle">0.52</td></tr><tr><td align="left" valign="top">S09</td><td align="center" valign="middle">0.62</td><td align="center" valign="middle">1.0e-03</td><td align="center" valign="middle">3.45e-02</td><td align="center" valign="middle">0.30</td><td align="center" valign="middle">1.61e-02</td><td align="center" valign="middle">5.79e-02</td><td align="center" valign="middle">3.48e-02</td><td align="center" valign="middle">0.27</td></tr><tr><td align="left" valign="top">S10</td><td align="center" valign="middle">0.71</td><td align="center" valign="middle">1.0e-05</td><td align="center" valign="middle">3.53e-02</td><td align="center" valign="middle">0.62</td><td align="center" valign="middle">3.88e-02</td><td align="center" valign="middle">2.02e-02</td><td align="center" valign="middle">3.49e-02</td><td align="center" valign="middle">0.62</td></tr><tr><td align="left" valign="top">S11</td><td align="center" valign="middle">0.53</td><td align="center" valign="middle">1.0e-05</td><td align="center" valign="middle">4.71e-02</td><td align="center" valign="middle">0.39</td><td align="center" valign="middle">2.34e-02</td><td align="center" valign="middle">3.03e-02</td><td align="center" valign="middle">4.70e-02</td><td align="center" valign="middle">0.39</td></tr><tr><td align="left" valign="top">S12</td><td align="center" valign="middle">0.46</td><td align="center" valign="middle">4.3e-05</td><td align="center" valign="middle">4.58e-02</td><td align="center" valign="middle">0.44</td><td align="center" valign="middle">3.72e-02</td><td align="center" valign="middle">1.06e-01</td><td align="center" valign="middle">4.60e-02</td><td align="center" valign="middle">0.44</td></tr><tr><td align="left" valign="top">S13</td><td align="center" valign="middle">0.15</td><td align="center" valign="middle">1.3e-05</td><td align="center" valign="middle">3.60e-02</td><td align="center" valign="middle">0.14</td><td align="center" valign="middle">3.33e-03</td><td align="center" valign="middle">2.95e-02</td><td align="center" valign="middle">3.48e-02</td><td align="center" valign="middle">0.33</td></tr><tr><td align="left" valign="top">S14</td><td align="center" valign="middle">0.48</td><td align="center" valign="middle">9.1e-04</td><td align="center" valign="middle">5.16e-02</td><td align="center" valign="middle">0.29</td><td align="center" valign="middle">1.09e-01</td><td align="center" valign="middle">1.45e+00</td><td align="center" valign="middle">5.13e-02</td><td align="center" valign="middle">0.31</td></tr><tr><td align="left" valign="top">S15</td><td align="center" valign="middle">0.15</td><td align="center" valign="middle">1.1e-05</td><td align="center" valign="middle">6.41e-02</td><td align="center" valign="middle">0.22</td><td align="center" valign="middle">7.76e-04</td><td align="center" valign="middle">3.23e-03</td><td align="center" valign="middle">6.40e-02</td><td align="center" valign="middle">0.14</td></tr><tr><td align="left" valign="top">S16</td><td align="center" valign="middle">0.79</td><td align="center" valign="middle">9.7e-04</td><td align="center" valign="middle">4.78e-02</td><td align="center" valign="middle">0.38</td><td align="center" valign="middle">2.25e-02</td><td align="center" valign="middle">3.33e-02</td><td align="center" valign="middle">4.85e-02</td><td align="center" valign="middle">0.35</td></tr><tr><td align="left" valign="top">S17</td><td align="center" valign="middle">0.33</td><td align="center" valign="middle">1.0e-05</td><td align="center" valign="middle">3.99e-02</td><td align="center" valign="middle">0.04</td><td align="center" valign="middle">1.33e-03</td><td align="center" valign="middle">-6.04e-03</td><td align="center" valign="middle">3.77e-02</td><td align="center" valign="middle">0.33</td></tr><tr><td align="left" valign="top">S18</td><td align="center" valign="middle">0.55</td><td align="center" valign="middle">2.5e-04</td><td align="center" valign="middle">4.29e-02</td><td align="center" valign="middle">0.46</td><td align="center" valign="middle">3.55e-02</td><td align="center" valign="middle">5.04e-02</td><td align="center" valign="middle">4.28e-02</td><td align="center" valign="middle">0.46</td></tr><tr><td align="left" valign="top">S20</td><td align="center" valign="middle">0.40</td><td align="center" valign="middle">8.8e-05</td><td align="center" valign="middle">3.68e-02</td><td align="center" valign="middle">0.32</td><td align="center" valign="middle">1.49e-02</td><td align="center" valign="middle">6.40e-02</td><td align="center" valign="middle">3.67e-02</td><td align="center" valign="middle">0.32</td></tr><tr><td align="left" valign="top">S21</td><td align="center" valign="middle">0.37</td><td align="center" valign="middle">5.2e-04</td><td align="center" valign="middle">4.61e-02</td><td align="center" valign="middle">0.23</td><td align="center" valign="middle">9.01e-03</td><td align="center" valign="middle">7.75e-02</td><td align="center" valign="middle">4.62e-02</td><td align="center" valign="middle">0.24</td></tr><tr><td align="left" valign="top">S22</td><td align="center" valign="middle">0.80</td><td align="center" valign="middle">4.5e-04</td><td align="center" valign="middle">8.08e-02</td><td align="center" valign="middle">0.23</td><td align="center" valign="middle">2.63e-02</td><td align="center" valign="middle">1.93e-02</td><td align="center" valign="middle">8.10e-02</td><td align="center" valign="middle">0.24</td></tr><tr><td align="left" valign="top">S23</td><td align="center" valign="middle">0.39</td><td align="center" valign="middle">5.9e-05</td><td align="center" valign="middle">2.66e-02</td><td align="center" valign="middle">0.57</td><td align="center" valign="middle">2.57e-02</td><td align="center" valign="middle">1.54e-01</td><td align="center" valign="middle">2.68e-02</td><td align="center" valign="middle">0.57</td></tr><tr style="border-bottom:solid thin"><td align="left" valign="top">S24</td><td align="center" valign="middle">0.64</td><td align="center" valign="middle">9.9e-04</td><td align="center" valign="middle">3.27e-02</td><td align="center" valign="middle">0.47</td><td align="center" valign="middle">1.44e-02</td><td align="center" valign="middle">4.25e-02</td><td align="center" valign="middle">3.23e-02</td><td align="center" valign="middle">0.49</td></tr><tr style="border-bottom: hidden"><td align="left" valign="top">Mean</td><td align="center" valign="middle">0.53</td><td align="center" valign="middle">3.2e-04</td><td align="center" valign="middle">4.42e-02</td><td align="center" valign="middle">0.40</td><td align="center" valign="middle">3.75e-02</td><td align="center" valign="middle">1.17e-01</td><td align="center" valign="middle">4.36e-02</td><td align="center" valign="middle">0.43</td></tr></tbody></table></table-wrap></floats-group></article>