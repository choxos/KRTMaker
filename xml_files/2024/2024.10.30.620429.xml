<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS199824</article-id><article-id pub-id-type="doi">10.1101/2024.10.30.620429</article-id><article-id pub-id-type="archive">PPR933906</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Touch to text: Spatiotemporal evolution of braille letter representations in blind readers</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Teng</surname><given-names>Santani</given-names></name><xref ref-type="corresp" rid="CR1">*</xref><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Cichy</surname><given-names>Radoslaw</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Pantazis</surname><given-names>Dimitrios</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Oliva</surname><given-names>Aude</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib></contrib-group><aff id="A1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05783y657</institution-id><institution>The Smith-Kettlewell Eye Research Institute</institution></institution-wrap></aff><aff id="A2"><label>2</label>Computer Science and Artificial Intelligence Laboratory, MIT</aff><aff id="A3"><label>3</label>Department of Education and Psychology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/046ak2485</institution-id><institution>Freie Universität Berlin</institution></institution-wrap></aff><aff id="A4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05ymca674</institution-id><institution>McGovern Institute for Brain Research</institution></institution-wrap>, MIT</aff><author-notes><corresp id="CR1"><label>*</label>Correspondence: <email>santani@ski.org</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>03</day><month>11</month><year>2024</year></pub-date><pub-date pub-type="preprint"><day>31</day><month>10</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Visual deprivation does not silence the visual cortex, which is responsive to auditory, tactile, and other nonvisual tasks in blind persons. However, the underlying functional dynamics of the neural networks mediating such crossmodal responses remain unclear. Here, using braille reading as a model framework to investigate these networks, we presented sighted (N=13) and blind (N=12) readers with individual visual print and tactile braille alphabetic letters, respectively, during MEG recording. Using time-resolved multivariate pattern analysis and representational similarity analysis, we traced the alphabetic letter processing cascade in both groups of participants. We found that letter representations unfolded more slowly in blind than in sighted brains, with decoding peak latencies ~200 ms later in braille readers. Focusing on the blind group, we found that the format of neural letter representations transformed within the first 500 ms after stimulus onset from a low-level structure consistent with peripheral nerve afferent coding to high-level format reflecting pairwise letter embeddings in a text corpus. The spatiotemporal dynamics of the transformation suggest that the processing cascade proceeds from a starting point in somatosensory cortex to early visual cortex and then to inferotemporal cortex. Together our results give insight into the neural mechanisms underlying braille reading in blind persons and the dynamics of functional reorganization in sensory deprivation.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">In the absence of vision, cortical areas traditionally characterized as visual in sighted people are recruited for a range of nonvisual tasks (<xref ref-type="bibr" rid="R65">Pascual-Leone et al., 2005</xref>; <xref ref-type="bibr" rid="R60">Merabet and Pascual-Leone, 2010</xref>; <xref ref-type="bibr" rid="R53">Kupers and Ptito, 2014</xref>; <xref ref-type="bibr" rid="R4">Amedi et al., 2017</xref>; <xref ref-type="bibr" rid="R5">Bedny, 2017</xref>), suggesting a functional reorganization in response to sensory deprivation. For decades, such crossmodal plasticity in blindness has been investigated using the cornerstone model of braille, a text system of raised dots designed to be read haptically with the fingerpad (<xref ref-type="bibr" rid="R61">Millar, 1997</xref>; <xref ref-type="bibr" rid="R29">Englebretson et al., 2023</xref>). This work has revealed a cortical network of braille-sensitive regions including the left-lateralized “Visual” Word Form Area (VWFA) (<xref ref-type="bibr" rid="R22">Dehaene and Cohen, 2011</xref>) representing multimodal orthographic formats and visual print (<xref ref-type="bibr" rid="R69">Reich et al., 2011</xref>; <xref ref-type="bibr" rid="R75">Striem-Amit et al., 2012</xref>; <xref ref-type="bibr" rid="R48">Kim et al., 2017</xref>; <xref ref-type="bibr" rid="R68">Rączy et al., 2019</xref>), and even early visual cortex (EVC), the starting point of the standard cortical visual processing hierarchy (<xref ref-type="bibr" rid="R72">Sadato et al., 1996</xref>; <xref ref-type="bibr" rid="R69">Reich et al., 2011</xref>). Notably, EVC activity is not merely epiphenomenal, but functionally important to braille reading: disruption, whether by stroke or magnetic stimulation, selectively impairs letter recognition while sparing basic somatosensation (<xref ref-type="bibr" rid="R18">Cohen et al., 1997</xref>; <xref ref-type="bibr" rid="R39">Hamilton and Pascual-Leone, 1998</xref>; <xref ref-type="bibr" rid="R40">Hamilton et al., 2000</xref>). However, the extent of functional reorganization in, and thus the functional role of, regions like EVC in the brains of congenitally blind persons remains a subject of ongoing debate (<xref ref-type="bibr" rid="R4">Amedi et al., 2017</xref>; <xref ref-type="bibr" rid="R5">Bedny, 2017</xref>; <xref ref-type="bibr" rid="R57">Makin and Krakauer, 2023</xref>; <xref ref-type="bibr" rid="R74">Seydell-Greenwald et al., 2023</xref>), in part becausethe temporal and representational dynamics of the braille processing network remain poorly understood.</p><p id="P3">A larger body of analogous work has revealed how, in the sighted brain, print letters as a special class of visual object stimulus (<xref ref-type="bibr" rid="R16">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="R7">Bi et al., 2016</xref>) are represented as patterns of retinal stimulation, sub-letter features, and distinct and abstract letter identities in high-level ventral visual cortex (<xref ref-type="bibr" rid="R36">Grainger et al., 2008</xref>; <xref ref-type="bibr" rid="R56">Madec et al., 2012</xref>; <xref ref-type="bibr" rid="R77">Thesen et al., 2012</xref>; <xref ref-type="bibr" rid="R47">Isik et al., 2014</xref>; <xref ref-type="bibr" rid="R31">Fischer-Baum et al., 2017</xref>). Similarly elucidating the dynamics of the braille text perception network in blind readers would clarify both the building blocks of the braille reading process as well as structure of the larger sensory processing hierarchy in a functionally reorganized brain. This requires describing not only the anatomical loci of relevant brain activity (<xref ref-type="bibr" rid="R69">Reich et al. 2011</xref>; Liu, Rapp, and Bedny 2023), but the temporal and representational dynamics of the process. In analogy to print characters, our working hypothesis was that braille characters must likewise develop in ordered fashion from dot patterns impinging on the fingerpad into meaningful components of written language, mediated by a cortical network including EVC (<xref ref-type="bibr" rid="R39">Hamilton and Pascual-Leone, 1998</xref>; <xref ref-type="bibr" rid="R46">Ioannides et al., 2013</xref>; <xref ref-type="bibr" rid="R41">Haupt et al., 2024</xref>).</p><p id="P4">To test this hypothesis and reveal the underlying neural networks, we presented blind participants with braille letters and sighted participants with visual print letters while recording brain responses with MEG. We then used multivariate pattern classification (<xref ref-type="bibr" rid="R42">Haynes and Rees, 2006</xref>; <xref ref-type="bibr" rid="R11">Carlson et al., 2013</xref>; <xref ref-type="bibr" rid="R16">Cichy et al., 2014</xref>) and model comparison using representational similarity analysis (RSA) (<xref ref-type="bibr" rid="R51">Kriegeskorte et al., 2008</xref>; <xref ref-type="bibr" rid="R50">Kriegeskorte and Kievit, 2013</xref>) to low- and high-level models of alphabetic letter representation to characterize the processing hierarchy in both sighted and blind readers.</p></sec><sec id="S2" sec-type="results"><title>Results</title><p id="P5">We presented congenitally or early-blind (N=12) and typically sighted (N=13) participants with lowercase alphabetic letters while recording brain activity with magnetoencephalography (MEG). We used analogous experimental designs and analyses of each group: both subject groups were presented with a common set of 10 different consonants, in tactile braille and in visual printed formats respectively (<xref ref-type="fig" rid="F1">Fig. 1a</xref>; purple eye and green hand icons indicate procedures and results for Sighted and Blind participant groups, respectively, hereafter). Blind participants were all braille-proficient since early childhood and had no history of vision better than nonspatial light perception, and thus had no visual experience of the printed alphabet (see <xref ref-type="table" rid="T1">Table 1</xref> for blind participant details). They were presented with braille letters via a refreshable tactile braille cell to the stationary fingerpad of their preferred index finger (10 left, 2 right). Sighted subjects had normal or corrected-to-normal vision, no experience with reading braille visually, and were presented with lowercase print letters in standard font centrally on a computer screen rear-projected into the MEG chamber.</p><p id="P6">On each trial, participants were presented with a letter for 500ms, followed by a 600 – 800 ms interstimulus interval (ISI). Trials were presented in random order, interspersed with vigilance trials on which participants responded to a target character (either an ‘e’ or an ‘o’ for blind, and ‘omega’ (Ω) for sighted) with a button press. Vigilance trials were excluded from further analysis.</p><p id="P7">Both blind and sighted participants identified letters easily, as indicated by informal pretesting and high performance on the vigilance task (mean hit rate 90%, <italic>d</italic>’ 3.63 for blind; hit rate 96%, <italic>d</italic>’ 4.83 for sighted; see <xref ref-type="fig" rid="F1">Fig. 1b</xref>). Notably, blind participants performed highly despite receiving the stimulus on a static finger, rather than making sweeping motions typical of naturalistic braille text reading. This allowed us to precisely mark the experimenter-defined stimulus onset in both blind and sighted participants.</p><sec id="S3"><title>The temporal dynamics of braille and visual letter identity</title><p id="P8">To determine the temporal dynamics with which letter representations emerge in sighted and blind brains, we subjected both data sets to an analogous analysis pipeline (<xref ref-type="fig" rid="F1">Fig. 1c</xref>). We first extracted trial epochs spanning -200 ms to +1000 ms relative to stimulus onset. We then used time-resolved multivariate pattern analysis (MVPA) (<xref ref-type="bibr" rid="R11">Carlson et al., 2013</xref>; <xref ref-type="bibr" rid="R16">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="R38">Guggenmos et al., 2018</xref>) to classify the 10 letter conditions pairwise from MEG data for every millisecond in the epoch. We saved the classification results (i.e. decoding accuracy) in a 10 x 10 decoding accuracy matrix (symmetric across the diagonal, and the diagonal undefined) indexed in rows and columns by the conditions classified.</p><p id="P9">Averaging across decoding accuracy matrices at each point, we obtained grand-average time courses indicating the dynamics with which letter representations emerge in blind and sighted subjects (<xref ref-type="fig" rid="F1">Fig. 1c</xref>, <italic>right</italic> panel). We assessed significance using non-parametric sign-permutation tests and controlled for multiple comparisons by cluster correction (<xref ref-type="bibr" rid="R58">Maris and Oostenveld, 2007</xref>) (cluster definition threshold p &lt; 0.05, cluster threshold p &lt; 0.05). We report peak latencies as a measure of the time at which representations are best discriminated indicating untangling (<xref ref-type="bibr" rid="R26">DiCarlo and Cox, 2007</xref>), as well as onset latencies of the first significant cluster indicating the beginning of distinct letter processing (both with bootstrapped 95% confidence intervals in brackets).</p><p id="P10">We found that letter stimuli were significantly discriminated in both blind braille readers and sighted visual readers (<xref ref-type="fig" rid="F1">Fig. 1d</xref>). As expected, in the sighted group, we observed the standard curve shape for classifying single visual stimuli with an early onset at 32 ms (22–75 ms), precipitating a sharp rise to a peak at 120 ms (113–127 ms), followed by a gradual decline. In contrast, in the blind group we observed a different shape of the classification time course: a less steep rise with a comparably early onset at 82 ms (50–125 ms), led to a peak within a wide plateau at 329 ms (160–460 ms). The qualitative differences and similarities were ascertained statistically (<xref ref-type="supplementary-material" rid="SD1">Supplementary Table S1</xref>), with a significantly later peak by 209 ms (54–390 ms) in the blind compared to the sighted subjects (p &lt; 0.002, determined by bootstrapping) and no significant difference in onset latency (p = 0.08).</p><p id="P11">Together, these results establish the feasibility of assessing braille letter representations from MEG data, determine the time course with which braille letter representations emerge in the blind brain, and provide a first comparative characterization of the neural dynamics of braille letter representations in relation to visual letter representations in the sighted brain.</p></sec><sec id="S4"><title>The representational format of neural responses to visual and braille letters</title><p id="P12">The rationale in using RSA to relate the models to human brain activity (<xref ref-type="fig" rid="F2">Fig. 2b</xref>) is to abstract from incommensurate signal spaces, such as MEG sensors and computational models, to a common space defined by pairwise dissimilarities between experimental conditions. The pairwise dissimilarities are aggregated in <italic>representational dissimilarity matrices</italic> (RDMs) indexed in rows and columns by the conditions compared. Interpreting decoding accuracy as a dissimilarity measure (i.e., higher accuracy indexes greater representational differences (<xref ref-type="bibr" rid="R51">Kriegeskorte et al., 2008</xref>)), we used the decoding accuracy matrices from the MVPA analysis as MEG RDMs. To compute the <italic>visual</italic> and <italic>tactile</italic> model RDMs, we presented the experimental stimuli to the models, extracted the resulting activation values into pattern vectors, and calculated pairwise dissimilarity between pattern vectors using 1–Spearman correlation distance. The <italic>bigram</italic> model RDM was populated by the inverted average co-occurrence frequency of each letter pair. We then related the time-resolved MEG RDMs to the model RDMs by computing semipartial Spearman correlations between them, resulting in two correlation time courses for each participant group (<xref ref-type="fig" rid="F2">Fig. 2b</xref>, <italic>right</italic>).</p><p id="P13">As expected, in the sighted group (<xref ref-type="fig" rid="F2">Fig. 2c</xref>), visual letter representations exhibited a strong and early correspondence with the visual low-level model (<xref ref-type="bibr" rid="R15">Cichy et al., 2016a</xref>), beginning at 63 ms (38–132 ms) and peaking at 102 ms (75–155 ms) relative to stimulus onset. However, we did not find significant evidence for correspondence to the high-level bigram model. This pattern of results confirms the contribution of low-level feature representations to individual visual letter processing, while remaining silent about the format of high-level visual letter representations. In contrast, in the blind group (<xref ref-type="fig" rid="F2">Fig. 2d</xref>), both the low-level tactile model and the high-level bigram model corresponded significantly with braille-elicited brain responses. Importantly, the bigram model correlation emerged significantly later than the tactile model correlation for both onset (414 ms; p=0.03) and peak latencies (465; p=0.0046); see <xref ref-type="supplementary-material" rid="SD1">Supplementary Table 2</xref> for details. These results characterize the evolving format of letter neural representations in blind brains, revealing a transformation from a low-level tactile to a high-level linguistic format.</p></sec><sec id="S5"><title>The spatial distribution of letter representations</title><p id="P14">We next aimed to refine the account of letter representations in sighted and blind brains by assessing their spatial distribution. For this, we decoded source-localized MEG activity in three key regions of interest (ROIs) relevant to visual and braille letter perception (<xref ref-type="fig" rid="F3">Fig. 3a</xref>): 1) Sensorimotor Cortex (<italic>SM</italic>), the cortical point of entry for tactile stimulation (<xref ref-type="bibr" rid="R39">Hamilton and Pascual-Leone, 1998</xref>; <xref ref-type="bibr" rid="R21">de Haan and Dijkerman, 2020</xref>); 2) Early Visual Cortex (<italic>EVC</italic>), the cortical point of entry for visual stimulation (<xref ref-type="bibr" rid="R45">Hubel and Wiesel, 1962</xref>; <xref ref-type="bibr" rid="R30">Felleman and Van Essen, 1991</xref>); and 3) left-lateralized Fusiform/Inferotemporal Cortex (<italic>IT</italic>), a high-level part of the ventral visual stream comprising the VWFA and other letter-sensitive regions (<xref ref-type="bibr" rid="R36">Grainger et al., 2008</xref>; <xref ref-type="bibr" rid="R22">Dehaene and Cohen, 2011</xref>; <xref ref-type="bibr" rid="R69">Reich et al., 2011</xref>; <xref ref-type="bibr" rid="R75">Striem-Amit et al., 2012</xref>; <xref ref-type="bibr" rid="R54">Lochy et al., 2018</xref>). We conducted the time-resolved MVPA analogous to the whole-brain analysis outlined above, but separately for each ROI.</p><p id="P15">ROI-wise decoding results are shown in <xref ref-type="fig" rid="F3">Fig. 3b</xref>. As expected, in the sighted group we observed strong decoding in EVC and IT, the entrance and late processing stage of visual processing in the brain. We also observe weaker but significant decoding in SM with a similar curve shape as for EVC and IT, likely a result of signal leakage. Comparing decoding onset and peak latencies across ROIs for the sighted group, we found only one significant difference, an earlier onset in EVC than in SM (-37 ms (-3 – 72 ms), p = 0.04; all other p &gt; 0.2).</p><p id="P16">In contrast, in the blind group, we observed strong and comparable braille decoding effects in SM, EVC and IT, suggesting that all three regions are involved in processing tactile letter representations. We qualitatively observed a systematic increase in onset latency, with shortest latency in SM 63 (28 ms–114 ms), followed by EVC 94 ms (67–149 ms) and finally IT 143 ms (80–399 ms), suggesting a processing cascade. Pairwise comparison of onset latencies across ROIs partially supported this observation, with significant SM–EVC and SM–IT effects (ps&lt;0.05), but not between EVC and IT (p=0.12). Peak latencies did not differ significantly (all p&gt;0.1).</p><p id="P17">These results clarify the distribution of letter representations in sighted and blind participants, tentatively suggesting a processing cascade for tactile letter representations in the blind from SM over EVC to IT.</p></sec><sec id="S6"><title>The format of letter representations in SM, EVC and IT</title><p id="P18">Based on the spatially-resolved MVPA analysis, we next repeated the above analysis linking computational models to neural responses, but separately for each ROI rather than across the whole-brain MEG data as above. In the sighted group (<xref ref-type="fig" rid="F3">Fig. 3c</xref>), for the low-level visual model we observed the expected pattern of results (<xref ref-type="bibr" rid="R36">Grainger et al., 2008</xref>; <xref ref-type="bibr" rid="R27">DiCarlo et al., 2012</xref>; <xref ref-type="bibr" rid="R16">Cichy et al., 2014</xref>): the model predicted activity in EVC and IT as bookends of the feedforward ventral visual stream, but not in sensorimotor cortex (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Tables S4</xref> and <xref ref-type="supplementary-material" rid="SD1">S5</xref>). The high-level bigram model did not predict activity in any ROI. This shows that low-level representations of letters localize to EVC and IT within the regions we examined.</p><p id="P19">In the blind group (<xref ref-type="fig" rid="F3">Fig. 3d</xref>), both the low-level tactile and the high-level bigram models correlated significantly with MEG decoding patterns in all three ROIs (details see <xref ref-type="supplementary-material" rid="SD1">Table S4</xref>). Consistent with the outcome of the MVPA analysis, this establishes that all ROIs process braille letters in both low tactile- and high linguistic-level in the blind brain.</p><p id="P20">Beyond this we detected two further patterns in the results. First, we qualitatively observed that the onset latencies of the correlation time course for the tactile model were earlier than for the bigram model in all three ROIs, consistent with the results pattern observed for the sensor-level analysis above. This observation was statistically ascertained for onset latencies in SM (latency difference 185 ms, p=0.0214), marginally significant in EVC (370 ms, p=0.078), but not supported in IT (p=0.14). This strengthens the view that tactile letter representations transform from a low-level tactile to a high-level linguistic format across time. Second, onset latencies for both models qualitatively followed the same order as for the analysis decoding Braille letters from ROIs: onset latencies were shortest in SM, followed by EVC and then finally IT (respectively: 148, 189, 354 ms for <italic>tactile</italic>; 333, 559, 630 ms for <italic>bigrams</italic>). However, this observation was supported statistically only for the delay between SM and IT (206 ms, p&lt;0.002; see <xref ref-type="supplementary-material" rid="SD1">Supplementary Table S5</xref> for details).</p><p id="P21">Together, our results show that SM, EVC and IT all contribute to processing braille letter representations in the blind brain, and suggests that a spatial hierarchy starting with SM and ending in IT mediates the transformation of earlier low-level tactile into later high-level linguistic representations.</p></sec><sec id="S7"><title>The inter-region dynamics of letter representations</title><p id="P22">The above analyses capture representational dynamics for each region separately. However, in a functional network, regions interact and stimulus information is communicated across regions over time, during which we would expect representational formats to be shared between the neural dynamics of the communicating ROIs. Thus, in a final step we interrogated the evolution of letter representations by assessing shared representations across the 3 ROI pairs (i.e. SM and EVC, EVC and IT, and SM and IT) for each group. For this we used a model-based commonality analysis based on variance partitioning (<xref ref-type="bibr" rid="R73">Seibold and McPhee, 1979</xref>; <xref ref-type="bibr" rid="R43">Hebart et al., 2018</xref>). In brief (<xref ref-type="fig" rid="F4">Fig. 4a,b</xref>), we determined the shared variance between MEG RDMs in a given region pair (e.g. EVC and IT) and either the low- or the high-level model, while discarding the effect of the other model. The analysis yields a commonality coefficient <italic>R</italic><sup>2</sup> indicating each model’s unique contribution to the shared variance at all time-point combinations between ROIs, producing a 2D matrix of <italic>R</italic><sup>2</sup> values for each model and ROI pair, with respective ROI time points indexing the row and column axes (<xref ref-type="fig" rid="F4">Fig. 4b</xref>). We statistically assessed the <italic>R</italic><sup>2</sup> for each model against the other, i.e. the difference score between low- and high-level models and vice versa. The relative onsets (edges) and centroids of the resultant two-dimensional patches of unique model contributions were compared within this space (<xref ref-type="fig" rid="F4">Fig. 4c</xref>; see <xref ref-type="sec" rid="S15">Methods</xref> for details).</p><p id="P23">In the sighted group, we found significant effects for the low-level <italic>visual</italic> model only for EVC and IT (<xref ref-type="fig" rid="F4">Fig. 4d</xref>, left column) between ~100–400 ms. In contrast, we found significant effects for the high-level <italic>bigram</italic> model for all region pairs emerging (<xref ref-type="fig" rid="F4">Fig. 4d</xref>, right column) between ~500–900 ms. For the EVC-IT pair, where both models fit the neural data, shared low-level visual model contributions reliably preceded high-level bigram fits in both EVC (onset asynchrony 453 ms, p = 0.002; centroid asynchrony 711 ms, p &lt; 0.002) and IT (onset asynchrony 464 ms, p = 0.044; centroid asynchrony 667 ms, p &lt; 0.002). This suggests that EVC, IT and VWFA partake in representing low-level visual representations, whereas only EVC and IT partake in representing both low-level visual letter representations before high-level bigram representations emerge.</p><p id="P24">For the blind group (<xref ref-type="fig" rid="F4">Fig. 4e</xref>), we find significant effects for all region pairs, for both the low-level tactile as well as the high-level bigram model. Qualitative inspection suggested that effects for the low-level tactile model emerged prior to the high-level bigram model, but this was not substantiated by statistical analysis (all p &gt; 0.14). Together, the result indicates that EVC, IT and VWFA share low- and high-level braille letter representations.</p></sec></sec><sec id="S8" sec-type="discussion"><title>Discussion</title><sec id="S9"><title>Summary</title><p id="P25">In the present study, we observed the time course of MEG responses to alphabetic braille characters presented to the fingerpads of early-blind participants, and visual printed letters presented foveally to sighted participants. This allowed us to establish the specifics of braille letter representations in blind readers, compared to the visual processing route in sighted subjects. Our main findings are twofold. First, we establish the differential temporal dynamics with which alphabetic letter representations emerge in blind and sighted brains. Second, we detail how letter representations are transformed from a low-level sensory to high-level linguistic format across processing time and cortical regions.</p></sec><sec id="S10"><title>The temporal dynamics of braille letter representations in blind brains</title><p id="P26">Previous work characterizing the temporal dynamics of neural response patterns has helped to reveal the functional architecture of visual (<xref ref-type="bibr" rid="R26">DiCarlo and Cox, 2007</xref>; <xref ref-type="bibr" rid="R16">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="R80">Yamins and DiCarlo, 2016</xref>; <xref ref-type="bibr" rid="R14">Cichy and Kaiser, 2019</xref>; <xref ref-type="bibr" rid="R37">Graumann et al., 2022</xref>) and auditory (<xref ref-type="bibr" rid="R9">Brodbeck et al., 2018</xref>; <xref ref-type="bibr" rid="R64">Ogg et al., 2020</xref>; <xref ref-type="bibr" rid="R55">Lowe et al., 2021</xref>) processing cascades. Here we thus assessed the temporal dynamics of neural responses to alphabetic responses in braille and print format in blind and sighted brains respectively. Our results revealed different dynamics of letter perception in blind compared to sighted readers, with slower dynamics in the blind group. This difference in processing speed might contribute to the generally slower reading speed for braille compared to print character text (<xref ref-type="bibr" rid="R78">Wetzel and Knowlton, 2000</xref>). More generally, our results demonstrate the feasibility of decoding rich single-letter braille representations from MEG, despite suboptimal reading conditions compared to more ecological behavior. A supplementary temporal generalization analysis (<xref ref-type="bibr" rid="R49">King and Dehaene, 2014</xref>) further refines this view, showing that both persistent and transient neural representations underlie the observed time courses in both groups and exhibit different dynamics across ROIs (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figs. S1</xref>, <xref ref-type="supplementary-material" rid="SD1">S2</xref>). Together these findings invite future research to harvest the power and sensitivity of multivariate methods for research on brain plasticity using braille reading as a model system.</p></sec><sec id="S11"><title>The format of braille letter representations in blind brains</title><p id="P27">We found that the brains of blind participants transform representations of braille letters from tactile to linguistic format reflecting the statistics of their pairwise embedding in written text, although each letter was only ever presented as an individual stimulus. This pattern is reminiscent of the gradual abstraction of relevant features from sensory input representations in, e.g., vision (<xref ref-type="bibr" rid="R11">Carlson et al., 2013</xref>; <xref ref-type="bibr" rid="R16">Cichy et al., 2014</xref>, <xref ref-type="bibr" rid="R17">2016b</xref>) and audition (<xref ref-type="bibr" rid="R23">de Heer et al., 2017</xref>; <xref ref-type="bibr" rid="R64">Ogg et al., 2020</xref>; <xref ref-type="bibr" rid="R55">Lowe et al., 2021</xref>; <xref ref-type="bibr" rid="R35">Giordano et al., 2023</xref>). Our results are consistent with previous RSA-based modeling of visual word reading in fMRI (<xref ref-type="bibr" rid="R31">Fischer-Baum et al., 2017</xref>), which suggests dissociable visual, orthographic, and semantic processing in ventral occipitotemporal and angular gyrus ROIs. While abstract/semantic representations of single letters are less straightforward than those of whole words (<xref ref-type="bibr" rid="R67">Popham et al., 2021</xref>), phrases (<xref ref-type="bibr" rid="R34">Fyshe et al., 2016</xref>; <xref ref-type="bibr" rid="R33">Fyshe, 2020</xref>), or continuous language (<xref ref-type="bibr" rid="R12">Caucheteux et al., 2023</xref>), sublexical processing is tied to visual reading performance in development (<xref ref-type="bibr" rid="R70">Ritchey and Speece, 2006</xref>; <xref ref-type="bibr" rid="R1">Acha et al., 2024</xref>) as well as adult braille reading (<xref ref-type="bibr" rid="R79">Wilson et al., 2024</xref>). Extending earlier work by resolving text-elicited brain responses at the single-letter and millisecond level, we reveal a dynamic and brain-wide network rapidly transforming letter representations through persistent, overlapping representational stages.</p><p id="P28">Counterintuitively, we found that the <italic>bigram</italic> model contributes to visual print representations in sighted participants, but only in the commonality analysis. Given that the co-occurrence statistics of this model were derived from a visual print corpus, what would explain the relative scarcity of a <italic>bigram</italic>-related signature in sighted versus blind letter responses? The commonality analysis focuses on signals shared across ROI pairs and the model, effectively eliminating non-shared variance. This suggests that aspects of representations reflecting bigram contributions are shared between ROIs, whereas other aspects of the representations are not. These non-shared aspects are arguably strong contributors to representations: for example, coding for low-level visual features in EVC would have been driven by 500 ms of continuous input from a high-contrast uncrowded letter image of typical size for visual neuroscientific studies, but an order of magnitude larger than the 0.2° (or less) subtended by standard printed text (<xref ref-type="bibr" rid="R6">Beier and Oderkerk, 2019</xref>). By contrast, braille-reading participants were presented with letters at the same scale they would be typically read. While this design maximized the visual neural signal available for decoding, the proportional imbalance might have caused the visual input to overwhelm higher-level, more abstract signals for individual ROI or whole-brain analyses.</p><p id="P29">Our approach presents fertile ground for further elucidating the cortical representations activated during braille reading. This could incorporate more and different models, including those derived from behavioral judgments (<xref ref-type="bibr" rid="R41">Haupt et al., 2024</xref>) and richer representations elicited by using full words or sentences as stimuli. In addition, while braille students typically learn alphabetic letters first, modern braille text is often rendered in contracted form (<xref ref-type="bibr" rid="R20">D’Andrea et al., 2014</xref>); braille-specific models of high-level text representations should account for this when using enriched stimuli. The RSA-based modeling approach could also illuminate evolving representations in braille learners as they improve their proficiency, e.g. by tracking changes in the compositionality of letter combinations (<xref ref-type="bibr" rid="R3">Agrawal et al., 2019</xref>, <xref ref-type="bibr" rid="R2">2020</xref>).</p></sec><sec id="S12"><title>A processing cascade for tactile letter representations in the blind from SM over EVC to IT?</title><p id="P30">Crossmodal plasticity in early- and congenitally blind braille readers has been a source of debate for decades (<xref ref-type="bibr" rid="R72">Sadato et al., 1996</xref>; <xref ref-type="bibr" rid="R10">Büchel, 2003</xref>; <xref ref-type="bibr" rid="R4">Amedi et al., 2017</xref>; <xref ref-type="bibr" rid="R5">Bedny, 2017</xref>; <xref ref-type="bibr" rid="R57">Makin and Krakauer, 2023</xref>). Our results inform this debate by suggesting a processing cascade in which tactile braille afferents enter SM and are communicated cortico-cortically to EVC before continuing to IT. This suggests that EVC plays a mediating role between SM and IT, rather than being an epiphenomenal activation. Furthermore, a somatosensory-EVC pathway has been implicated in detecting and identifying tactile/braille stimuli (<xref ref-type="bibr" rid="R39">Hamilton and Pascual-Leone, 1998</xref>; <xref ref-type="bibr" rid="R66">Pascual-Leone et al., 1999</xref>; <xref ref-type="bibr" rid="R46">Ioannides et al., 2013</xref>), but has not been shown directly. Here we provide empirical evidence consistent with this prediction.</p><p id="P31">While further work is needed to detail the spatiotemporal dynamics of braille reading, our work makes clear predictions about the nature and spatiotemporal profile of braille letter processing that could be experimentally probed with active methods such as TMS (<xref ref-type="bibr" rid="R18">Cohen et al., 1997</xref>; <xref ref-type="bibr" rid="R39">Hamilton and Pascual-Leone, 1998</xref>). The spatial characterization of these functional networks may also be pursued with fMRI, and link spatially and temporally resolved representations in M/EEG for a spatiotemporally integrated view (<xref ref-type="bibr" rid="R41">Haupt et al., 2024</xref>).</p></sec><sec id="S13"><title>Limitations</title><p id="P32">Our experimental framework for assessing braille letter perception must be viewed in light of several key limitations in ecological and external validity to real-world context. First, the braille reading conditions encountered by our blind participants were atypical. To precisely control stimulus presentation timing, minimize muscle artifacts, and maximize the signal-to-noise ratio of the MEG recordings via repeated presentations, we presented an alphabetic subset of single letters to our braille readers’ static fingers via refreshable braille display, asking them to omit the active sweeping movements typically observed in braille readers. Still, they identified letters with relative ease (<xref ref-type="fig" rid="F1">Fig. 1b</xref>), suggesting that our results could generalize to braille processing in real-world situations. Second, our setup does not assess higher-level lexical content characteristic of real-world braille text reading as captured in words, sentences and stories. However, the model analyses revealed neural sensitivity to both the letters’ low-level features and their embedding statistics in written text, a convincing signature of literacy even at the single-letter level. Finally, naturalistic reading in proficient braille readers often involves two hands (<xref ref-type="bibr" rid="R59">Martiniello and Wittich, 2022</xref>), a cross-hands integration step explored in similar recent single-letter work (<xref ref-type="bibr" rid="R41">Haupt et al., 2024</xref>) but not addressed here. Solving the technical challenges of brain measurements in active reading would address all these limitations to reveal the sensorimotor dynamics of continuous text processing.</p></sec><sec id="S14" sec-type="conclusions"><title>Conclusions</title><p id="P33">In sum, the present study reveals the spatiotemporal and representational dynamics of alphabetic letter processing in blind readers: it involves a transformation from low- to high-level representations that is realized in a cortical network suggestive of a cascade from sensorimotor cortex, over EVC to IT.</p></sec></sec><sec id="S15" sec-type="methods"><title>Methods</title><p id="P34">Twelve congenitally or early-blind (7 females; mean age 28.8 y, 5.7 SD) volunteers participated in the study (see <xref ref-type="table" rid="T1">Table 1</xref> for details). The maximum age of blindness onset was 3 years. Participants were self-reported fluent daily braille readers and native English speakers. They were either totally blind or had nonspatial light perception; none had had any experience with printed letter reading or visual form generally.. Additionally, 13 sighted volunteers (8 females; mean age 27.1 y, 4.9 SD) with normal or corrected vision were recruited as the visual control group. All participants were compensated for their participation and gave written informed consent in accordance with the guidelines of MIT’s Committee on the Use of Humans as Experimental Subjects.</p><sec id="S16"><title>Stimuli, task and procedure</title><p id="P35">Braille stimuli for blind readers comprised 12 single lowercase alphabetic letters (b, c, d, e, l, m, n, o, v, x, y, z), presented via a custom-built single-cell refreshable braille display (model P16; Metec AG, Stuttgart, Germany). The subset was chosen to cover a wide range of letter positions in the alphabet while maximizing stimulus repetitions, and thus signal, within experimental time constraints. During testing, participants rested their index finger on the braille display cell, which presented the stimulus directly to their stationary fingerpads. To reliably peg stimulus onset times to the display’s piezoelectric rods, we affixed a small accelerometer to each participant’s fingernail, registering each impact as a z-axis “spike” synchronized to the MEG recording. Visual stimuli for the sighted group comprised the full alphabet of 26 lowercase alphabetic print letters, rendered in Myriad Pro Bold font using Adobe Illustrator (Adobe, Inc., San Jose, CA) and presented foveally to subtend ~2° on a rear-projection screen. From this set, the subset of 10 letters corresponding to the braille stimuli was extracted for equivalent analysis to the braille conditions. For both groups, letters were presented in pseudorandom order using custom code and the Psychophysics Toolbox (<xref ref-type="bibr" rid="R8">Brainard, 1997</xref>) for Matlab (The MathWorks, Natick, MA). Stimulus duration was 500 ms, with SOA jittered between 1100 and 1300 ms. The experimental sessions comprised approximately 10 runs, resulting in an average of ~100 presentations per letter condition. Participants performed a vigilance letter-detection task by responding via button press to targets (either an ‘e’ or an ‘o’ for blind, and ‘omega’ (Ω) for sighted) which appeared every 3 to 5 trials. All target trials were excluded from further analyses reported here.</p></sec><sec id="S17"><title>MEG data acquisition</title><p id="P36">We scanned participants in an Elekta Neuromag TRIUX MEG scanner (Elekta, Stockholm, Sweden), with continuous whole-brain data acquisition at 1 kHz from 306 sensors (204 planar gradiometers; 102 magnetometers), filtered online between 0.3 and 330 Hz. Head motion was tracked at 330 Hz using five head-position indicator coils affixed to each subject’s scalp, whose location was digitized prior to scanning along with three fiducials and other location markers.</p></sec><sec id="S18"><title>Data preprocessing</title><p id="P37">Data were motion-compensated and spatiotemporally filtered offline using Maxfilter software (Elekta, Stockholm, Sweden). All further analysis was conducted using a combination of Brainstorm software (<xref ref-type="bibr" rid="R76">Tadel et al., 2011</xref>) and custom analysis scripts, both Matlab-based. From the preprocessed MEG data, we extracted trial epochs for each letter presentation with a prestimulus baseline of 200 ms and 1000 ms post-stimulus onset. For each epoch, the baseline mean was removed and a 30 Hz low-pass filter applied.</p></sec><sec id="S19"><title>Multivariate analysis of MEG data</title><p id="P38">We decoded letter identity from trial epochs using a linear support vector machine (SVM; <xref ref-type="bibr" rid="R13">Chang and Lin, 2011</xref> <ext-link ext-link-type="uri" xlink:href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/">http://www.csie.ntu.edu.tw/~cjlin/libsvm/</ext-link>). For each time point in the epoch, the following analysis was performed: The MEG data yielded a 306-dimensional pattern vector for each of N trials per condition (<xref ref-type="fig" rid="F1">Fig. 1b</xref>). The single-trial pattern vectors were whitened and reduced using PCA to improve SNR, then randomly sub-averaged to yield N<sub>10</sub>=N/10 subaverages per condition, which were then used in a leave-one-out cross-validation approach to train the SVM classifier for every pairwise condition comparison (<xref ref-type="bibr" rid="R16">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="R38">Guggenmos et al., 2018</xref>). This process was repeated 100 times, each time with randomized sub-averaging and assignment of training and testing sets. The overall measure of decoding accuracy for a given pairwise comparison and time point was the mean of those 100 permutations. Decoding each pair of conditions (45 pairs for 10 conditions) produced a symmetric 10 x 10 decoding matrix of pairwise decoding accuracies, with the diagonal undefined, for each time point. Thus, analysis of each trial epoch resulted in 1201 such matrices. Interpreting decoding accuracy as a measure of dissimilarity, the decoding matrix is a <italic>representational dissimilarity matrix</italic> (RDM; <xref ref-type="bibr" rid="R51">Kriegeskorte et al., 2008</xref>). To generate the grand average decoding time course for the epoch (<xref ref-type="fig" rid="F1">Fig. 1c</xref>), a mean accuracy was computed from the individual pairwise accuracies of the RDM for each time point.</p></sec><sec id="S20"><title>MEG source estimation and ROI selection</title><p id="P39">We estimated cortical generators of the MEG signal using Brainstorm's minimum norm estimate (MNE) method, normalized by noise estimates using a dynamical Statistical Parametric Mapping (dSPM) approach (<xref ref-type="bibr" rid="R19">Dale et al., 2000</xref>). For 8 of 12 blind participants, individual T1 structural MRI scans were used to create personalized cortical models; for all remaining subjects, we used Brainstorm's default cortical model based on the MNI152 template (<xref ref-type="bibr" rid="R32">Fonov et al., 2009</xref>). Cortical source estimates were computed on a grid of approximately 15,000 vertices spanning the cortical surface. The forward model was constructed using the overlapping spheres approach (<xref ref-type="bibr" rid="R44">Huang et al., 1999</xref>), and MEG signals were mapped onto the cortex using dSPM. From the resulting cortical activity map, we defined ROIs by selecting vertices corresponding to anatomical regions defined in the Desikan-Killiany anatomical atlas (<xref ref-type="bibr" rid="R25">Desikan et al., 2006</xref>). For the <italic>Sensorimotor</italic> (SM) ROI, we selected the pre- and postcentral gyrus atlas regions contralateral to the braille-stimulated finger in the blind group (R hemisphere for 9 of 12 subjects); for the sighted group, lacking an analogous selection principle, we used right-hemisphere regions. Our <italic>Early Visual Cortex</italic> (EVC) ROI combines bilateral pericalcarine, lingual, lateral occipital, and cuneus atlas regions. Finally, our <italic>Inferotemporal</italic> (IT) ROI combines left-lateralized inferior temporal and fusiform atlas regions, comprising several left-lateralized regions known to encode higher-level linguistic representations of written text (<xref ref-type="bibr" rid="R69">Reich et al., 2011</xref>; <xref ref-type="bibr" rid="R77">Thesen et al., 2012</xref>; <xref ref-type="bibr" rid="R31">Fischer-Baum et al., 2017</xref>; <xref ref-type="bibr" rid="R41">Haupt et al., 2024</xref>). The estimated activity in those regions was subjected to the same sensor-space MVPA decoding analysis described above.</p></sec><sec id="S21"><title>RDM representational models</title><p id="P40">To interrogate the representational content of the MEG responses, we computed RDMs corresponding to predicted responses at different levels of analysis. Specifically, we hypothesized sensory-specific low-level response patterns for each group, and a common higher-level abstracted representation for both groups. To model low-level visual representations in the sighted group (“<italic>Visual</italic>” model), we extracted activations elicited by the visual letter images in the first convolutional layer of a deep neural network, AlexNet, pretrained on ImageNet to categorize visual objects (<xref ref-type="bibr" rid="R24">Deng et al., 2009</xref>; <xref ref-type="bibr" rid="R52">Krizhevsky et al., 2012</xref>; <xref ref-type="bibr" rid="R15">Cichy et al., 2016a</xref>). The analogous “<italic>Tactile</italic>” braille model for the blind group computes simulated afferent nerve responses to the spatiotemporal braille indentation pattern on the distal index fingerpad using the Matlab-based touchSIM toolbox (<xref ref-type="bibr" rid="R71">Saal et al., 2017</xref>). We set the toolbox stimulation parameters to closely resemble the physical experiment: pins of radius 0.7 mm spaced at 2.5 mm; 0.7 mm indentation in the center of the distal index fingerpad (region D2d) for 500 ms with 20 ms linear ramp; all available innervating fibers (SA1, RA, PC) simulated at full density. From the resulting 945-afferent output, the time-averaged rate vector was extracted to represent the response to each letter. The low-level model RDMs in both cases comprised the pairwise correlation distances (1 – Spearman’s ρ) between the extracted vectors for each stimulus condition. Finally, because individual letters are semantically impoverished compared to words or longer text, our higher-level model leveraged the statistics of pairwise letter co-occurrences (“<italic>Bigrams</italic>”) over a Google Books English-language text corpus comprising approximately 2.8 trillion exemplars (<xref ref-type="bibr" rid="R63">Norvig, 2013</xref>). Bigram co-occurrence frequencies were normalized to the corpus, averaged to be order-independent (e.g. ‘BL’ and ‘LB’ frequencies were averaged), then inverted to index dissimilarity, thus matching the pairwise format of our neural decoding approach. A neural correspondence to the <italic>Bigrams</italic> model would imply high-level sensitivity to letter distributions in written language, rather than to physical letter similarities: more frequent occurrences of a particular bigram reflect lower pairwise dissimilarity in the model and lower MEG decoding accuracy for that letter pair, and vice versa. For all models, Model-MEG correspondence was computed via rank-order semipartial correlation (Spearman’s ρ) between each computational model RDM and the empirical RDM of the MEG pairwise decoding matrix at each time point, controlling for the effect of the other model (<xref ref-type="bibr" rid="R43">Hebart et al., 2018</xref>; <xref ref-type="bibr" rid="R28">Dobs et al., 2019</xref>).</p></sec><sec id="S22"><title>Statistical analysis</title><p id="P41">The decoding time courses, temporal generalization results, MEG-model correlations, and model-based commonality analyses were tested for significance via permutation tests for cluster-size inference and bootstrap tests for peak and onset time confidence intervals (<xref ref-type="bibr" rid="R62">Nichols and Holmes, 2002</xref>; <xref ref-type="bibr" rid="R58">Maris and Oostenveld, 2007</xref>; <xref ref-type="bibr" rid="R16">Cichy et al., 2014</xref>). The null hypothesis against which correlations or decoding accuracies were tested was 50% (chance) decoding accuracy or 0 correlation between MEG and model RDMs. To generate an empirical null distribution, we created 1000 permutation samples in which each participant’s MEG response was randomly multiplied by +1 or -1, thus allowing the conversion of the data into p-values. We then corrected for multiple comparisons across time points via cluster-size inference (cluster-definition threshold = p &lt; 0.05), using cluster size as the statistic of interest, an analysis more sensitive to temporally extended weak effects than brief strong effects. Clusters were reported as significant if they contained more time points than 95% of the maximal cluster size distribution (cluster-size threshold = p &lt; 0.05). Note that while each individual cluster-based analysis corrects for multiple comparisons across time points, we did not correct the inter-ROI, inter-group, or inter-model analyses for multiple comparisons. Finally, for peak latency and cluster onset distributions, we bootstrapped the participant sample (with replacement) 500 times, repeating the above analysis for each iteration. This produced an empirical distribution of onsets and peak/centroid latencies from which we estimated 95% confidence intervals. For our directional hypotheses (e.g. low-to-high-level representation onset asynchrony between two ROIs), we applied 1-tailed tests.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplemental Data and Tables</label><media xlink:href="EMS199824-supplement-Supplemental_Data_and_Tables.pdf" mimetype="application" mime-subtype="pdf" id="d74aAcEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S23"><title>Acknowledgments</title><p>This work was supported by the Vannevar Bush Faculty Fellowship program sponsored by the Basic Research Office of the Assistant Secretary of Defense for Research and Engineering and funded by the Office of Naval Research Grant N00014-16-1-3116 (to A.O.) and the McGovern Institute Neurotechnology Program (to A.O. and D.P.). S.T. was supported by Smith-Kettlewell Institute and the National Eye Institute (Training Grant 5T32EY025201). R.M.C is supported by the Deutsche Forschungsgemeinschaft (DFG; CI241/3-1, CI241/3-3, CI241/3-7 and INST 272/297-1) and by a European Research Council (ERC) starting grant (ERC-2018-STG 803370). We thank K. Ramakrishnan for assistance with early DNN modeling of visual features.</p></ack><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Acha</surname><given-names>J</given-names></name><name><surname>Ibaibarriaga</surname><given-names>G</given-names></name><name><surname>Rodríguez</surname><given-names>N</given-names></name><name><surname>Perea</surname><given-names>M</given-names></name></person-group><article-title>Lexical and sublexical skills in children’s literacy</article-title><source>J Lit Res</source><year>2024</year><volume>56</volume><fpage>6</fpage><lpage>26</lpage></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Agrawal</surname><given-names>A</given-names></name><name><surname>Hari</surname><given-names>K</given-names></name><name><surname>Arun</surname><given-names>SP</given-names></name></person-group><article-title>A compositional neural code in high-level visual cortex can explain jumbled word reading</article-title><source>Elife</source><year>2020</year><volume>9</volume><pub-id pub-id-type="pmcid">PMC7272193</pub-id><pub-id pub-id-type="pmid">32369017</pub-id><pub-id pub-id-type="doi">10.7554/eLife.54846</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Agrawal</surname><given-names>A</given-names></name><name><surname>Hari</surname><given-names>KVS</given-names></name><name><surname>Arun</surname><given-names>SP</given-names></name></person-group><article-title>Reading Increases the Compositionality of Visual Word Representations</article-title><source>Psychol Sci</source><year>2019</year><volume>30</volume><fpage>1707</fpage><lpage>1723</lpage><pub-id pub-id-type="pmcid">PMC6912929</pub-id><pub-id pub-id-type="pmid">31697615</pub-id><pub-id pub-id-type="doi">10.1177/0956797619881134</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amedi</surname><given-names>A</given-names></name><name><surname>Hofstetter</surname><given-names>S</given-names></name><name><surname>Maidenbaum</surname><given-names>S</given-names></name><name><surname>Heimler</surname><given-names>B</given-names></name></person-group><article-title>Task Selectivity as a Comprehensive Principle for Brain Organization</article-title><source>Trends Cogn Sci</source><year>2017</year><volume>21</volume><fpage>307</fpage><lpage>310</lpage><pub-id pub-id-type="pmid">28385460</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bedny</surname><given-names>M</given-names></name></person-group><article-title>Evidence from Blindness for a Cognitively Pluripotent Cortex</article-title><source>Trends Cogn Sci</source><year>2017</year><volume>21</volume><fpage>637</fpage><lpage>648</lpage><pub-id pub-id-type="pmid">28821345</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beier</surname><given-names>S</given-names></name><name><surname>Oderkerk</surname><given-names>CAT</given-names></name></person-group><article-title>Smaller visual angles show greater benefit of letter boldness than larger visual angles</article-title><source>Acta Psychol (Amst)</source><year>2019</year><volume>199</volume><elocation-id>102904</elocation-id><pub-id pub-id-type="pmid">31421483</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bi</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Caramazza</surname><given-names>A</given-names></name></person-group><article-title>Object Domain and Modality in the Ventral Visual Pathway</article-title><source>Trends Cogn Sci</source><year>2016</year><volume>20</volume><fpage>282</fpage><lpage>290</lpage><pub-id pub-id-type="pmid">26944219</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><article-title>The Psychophysics Toolbox</article-title><source>Spat Vis</source><year>1997</year><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Hong</surname><given-names>LE</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><article-title>Rapid Transformation from Auditory to Linguistic Representations of Continuous Speech</article-title><source>Curr Biol</source><year>2018</year><volume>28</volume><fpage>3976</fpage><lpage>3983</lpage><elocation-id>e5</elocation-id><pub-id pub-id-type="pmcid">PMC6339854</pub-id><pub-id pub-id-type="pmid">30503620</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2018.10.042</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Büchel</surname><given-names>C</given-names></name></person-group><article-title>Cortical hierarchy turned on its head</article-title><source>Nat Neurosci</source><year>2003</year><volume>6</volume><fpage>657</fpage><lpage>658</lpage><pub-id pub-id-type="pmid">12830152</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlson</surname><given-names>T</given-names></name><name><surname>Tovar</surname><given-names>DA</given-names></name><name><surname>Alink</surname><given-names>A</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Representational dynamics of object vision: the first 1000 ms</article-title><source>J Vis</source><year>2013</year><volume>13</volume><pub-id pub-id-type="pmid">23908380</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caucheteux</surname><given-names>C</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>King</surname><given-names>J-R</given-names></name></person-group><article-title>Evidence of a predictive coding hierarchy in the human brain listening to speech</article-title><source>Nat Hum Behav</source><year>2023</year><volume>7</volume><fpage>430</fpage><lpage>441</lpage><pub-id pub-id-type="pmcid">PMC10038805</pub-id><pub-id pub-id-type="pmid">36864133</pub-id><pub-id pub-id-type="doi">10.1038/s41562-022-01516-2</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>C-C</given-names></name><name><surname>Lin</surname><given-names>C-J</given-names></name></person-group><article-title>LIBSVM: A library for support vector machines</article-title><source>ACM Trans Intell Syst Technol</source><year>2011</year><volume>2</volume><fpage>1</fpage><lpage>27</lpage></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Kaiser</surname><given-names>D</given-names></name></person-group><article-title>Deep Neural Networks as Scientific Models</article-title><source>Trends Cogn Sci</source><year>2019</year><volume>23</volume><fpage>305</fpage><lpage>317</lpage><pub-id pub-id-type="pmid">30795896</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Khosla</surname><given-names>A</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Torralba</surname><given-names>A</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><article-title>Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence</article-title><source>Sci Rep</source><year>2016a</year><volume>6</volume><elocation-id>27755</elocation-id><pub-id pub-id-type="pmcid">PMC4901271</pub-id><pub-id pub-id-type="pmid">27282108</pub-id><pub-id pub-id-type="doi">10.1038/srep27755</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><article-title>Resolving human object recognition in space and time</article-title><source>Nat Neurosci</source><year>2014</year><volume>17</volume><fpage>455</fpage><lpage>462</lpage><pub-id pub-id-type="pmcid">PMC4261693</pub-id><pub-id pub-id-type="pmid">24464044</pub-id><pub-id pub-id-type="doi">10.1038/nn.3635</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><article-title>Similarity-Based Fusion of MEG and fMRI Reveals Spatio-Temporal Dynamics in Human Cortex During Visual Object Recognition</article-title><source>Cereb Cortex</source><year>2016b</year><volume>26</volume><fpage>3563</fpage><lpage>3579</lpage><pub-id pub-id-type="pmcid">PMC4961022</pub-id><pub-id pub-id-type="pmid">27235099</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhw135</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>LG</given-names></name><name><surname>Celnik</surname><given-names>P</given-names></name><name><surname>Pascual-Leone</surname><given-names>A</given-names></name><name><surname>Corwell</surname><given-names>B</given-names></name><name><surname>Falz</surname><given-names>L</given-names></name><name><surname>Dambrosia</surname><given-names>J</given-names></name><name><surname>Honda</surname><given-names>M</given-names></name><name><surname>Sadato</surname><given-names>N</given-names></name><name><surname>Gerloff</surname><given-names>C</given-names></name><name><surname>Catalá</surname><given-names>MD</given-names></name><name><surname>Hallett</surname><given-names>M</given-names></name></person-group><article-title>Functional relevance of cross-modal plasticity in blind humans</article-title><source>Nature</source><year>1997</year><volume>389</volume><fpage>180</fpage><lpage>183</lpage><pub-id pub-id-type="pmid">9296495</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Liu</surname><given-names>AK</given-names></name><name><surname>Fischl</surname><given-names>BR</given-names></name><name><surname>Buckner</surname><given-names>RL</given-names></name><name><surname>Belliveau</surname><given-names>JW</given-names></name><name><surname>Lewine</surname><given-names>JD</given-names></name><name><surname>Halgren</surname><given-names>E</given-names></name></person-group><article-title>Dynamic statistical parametric mapping: combining fMRI and MEG for high-resolution imaging of cortical activity</article-title><source>Neuron</source><year>2000</year><volume>26</volume><fpage>55</fpage><lpage>67</lpage><pub-id pub-id-type="pmid">10798392</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>D’Andrea</surname><given-names>FM</given-names></name><name><surname>Wormsley</surname><given-names>DP</given-names></name><name><surname>Savaiano</surname><given-names>ME</given-names></name></person-group><chapter-title>Chapter Five - Unified English Braille in the United States: A Research Agenda for Transition and Instruction</chapter-title><person-group person-group-type="editor"><name><surname>Hatton</surname><given-names>DD</given-names></name></person-group><source>International Review of Research in Developmental Disabilities</source><publisher-name>Academic Press</publisher-name><year>2014</year><fpage>145</fpage><lpage>175</lpage></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Haan</surname><given-names>EHF</given-names></name><name><surname>Dijkerman</surname><given-names>HC</given-names></name></person-group><article-title>Somatosensation in the brain: A theoretical re-evaluation and a new model</article-title><source>Trends Cogn Sci</source><year>2020</year><volume>24</volume><fpage>529</fpage><lpage>541</lpage><pub-id pub-id-type="pmid">32430229</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Cohen</surname><given-names>L</given-names></name></person-group><article-title>The unique role of the visual word form area in reading</article-title><source>Trends Cogn Sci</source><year>2011</year><volume>15</volume><fpage>254</fpage><lpage>262</lpage><pub-id pub-id-type="pmid">21592844</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Heer</surname><given-names>WA</given-names></name><name><surname>Huth</surname><given-names>AG</given-names></name><name><surname>Griffiths</surname><given-names>TL</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name><name><surname>Theunissen</surname><given-names>FE</given-names></name></person-group><article-title>The Hierarchical Cortical Organization of Human Speech Processing</article-title><source>J Neurosci</source><year>2017</year><volume>37</volume><fpage>6539</fpage><lpage>6557</lpage><pub-id pub-id-type="pmcid">PMC5511884</pub-id><pub-id pub-id-type="pmid">28588065</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3267-16.2017</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>J</given-names></name><name><surname>Dong</surname><given-names>W</given-names></name><name><surname>Socher</surname><given-names>R</given-names></name><name><surname>Li</surname><given-names>L-J</given-names></name><name><surname>Li</surname><given-names>K</given-names></name><name><surname>Fei-Fei</surname><given-names>L</given-names></name></person-group><source>ImageNet: A large-scale hierarchical image database</source><conf-name>2009 IEEE Conference on Computer Vision and Pattern Recognition</conf-name><conf-sponsor>IEEE</conf-sponsor><year>2009</year><fpage>248</fpage><lpage>255</lpage></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desikan</surname><given-names>RS</given-names></name><name><surname>Ségonne</surname><given-names>F</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Quinn</surname><given-names>BT</given-names></name><name><surname>Dickerson</surname><given-names>BC</given-names></name><name><surname>Blacker</surname><given-names>D</given-names></name><name><surname>Buckner</surname><given-names>RL</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Maguire</surname><given-names>RP</given-names></name><name><surname>Hyman</surname><given-names>BT</given-names></name><name><surname>Albert</surname><given-names>MS</given-names></name><etal/></person-group><article-title>An automated labeling system for subdividing the human cerebral cortex on MRI scans into gyral based regions of interest</article-title><source>Neuroimage</source><year>2006</year><volume>31</volume><fpage>968</fpage><lpage>980</lpage><pub-id pub-id-type="pmid">16530430</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Cox</surname><given-names>DD</given-names></name></person-group><article-title>Untangling invariant object recognition</article-title><source>Trends Cogn Sci</source><year>2007</year><volume>11</volume><fpage>333</fpage><lpage>341</lpage><pub-id pub-id-type="pmid">17631409</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name><name><surname>Rust</surname><given-names>NC</given-names></name></person-group><article-title>How does the brain solve visual object recognition?</article-title><source>Neuron</source><year>2012</year><volume>73</volume><fpage>415</fpage><lpage>434</lpage><pub-id pub-id-type="pmcid">PMC3306444</pub-id><pub-id pub-id-type="pmid">22325196</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2012.01.010</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dobs</surname><given-names>K</given-names></name><name><surname>Isik</surname><given-names>L</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><article-title>How face perception unfolds over time</article-title><source>Nat Commun</source><year>2019</year><volume>10</volume><elocation-id>1258</elocation-id><pub-id pub-id-type="pmcid">PMC6425020</pub-id><pub-id pub-id-type="pmid">30890707</pub-id><pub-id pub-id-type="doi">10.1038/s41467-019-09239-1</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Englebretson</surname><given-names>R</given-names></name><name><surname>Cay Holbrook</surname><given-names>M</given-names></name><name><surname>Fischer-Baum</surname><given-names>S</given-names></name></person-group><article-title>A position paper on researching braille in the cognitive sciences: decentering the sighted norm</article-title><source>Appl Psycholinguist</source><year>2023</year><volume>44</volume><fpage>400</fpage><lpage>415</lpage></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Felleman</surname><given-names>DJ</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><article-title>Distributed hierarchical processing in the primate cerebral cortex</article-title><source>Cereb Cortex</source><year>1991</year><volume>1</volume><fpage>1</fpage><lpage>47</lpage><pub-id pub-id-type="pmid">1822724</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischer-Baum</surname><given-names>S</given-names></name><name><surname>Bruggemann</surname><given-names>D</given-names></name><name><surname>Gallego</surname><given-names>IF</given-names></name><name><surname>Li</surname><given-names>DSP</given-names></name><name><surname>Tamez</surname><given-names>ER</given-names></name></person-group><article-title>Decoding levels of representation in reading: A representational similarity approach</article-title><source>Cortex</source><year>2017</year><volume>90</volume><fpage>88</fpage><lpage>102</lpage><pub-id pub-id-type="pmid">28384482</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fonov</surname><given-names>VS</given-names></name><name><surname>Evans</surname><given-names>AC</given-names></name><name><surname>McKinstry</surname><given-names>RC</given-names></name><name><surname>Almli</surname><given-names>CR</given-names></name><name><surname>Collins</surname><given-names>DL</given-names></name></person-group><article-title>Unbiased nonlinear average age-appropriate brain templates from birth to adulthood</article-title><source>Neuroimage</source><year>2009</year><volume>47</volume><elocation-id>S102</elocation-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fyshe</surname><given-names>A</given-names></name></person-group><article-title>Studying language in context using the temporal generalization method</article-title><source>Philos Trans R Soc Lond B Biol Sci</source><year>2020</year><volume>375</volume><elocation-id>20180531</elocation-id><pub-id pub-id-type="pmcid">PMC6939359</pub-id><pub-id pub-id-type="pmid">31840577</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2018.0531</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fyshe</surname><given-names>A</given-names></name><name><surname>Sudre</surname><given-names>G</given-names></name><name><surname>Wehbe</surname><given-names>L</given-names></name><name><surname>Rafidi</surname><given-names>N</given-names></name><name><surname>Mitchell</surname><given-names>TM</given-names></name></person-group><article-title>The semantics of adjective noun phrases in the human brain</article-title><source>bioRxiv</source><year>2016</year><pub-id pub-id-type="pmcid">PMC6865843</pub-id><pub-id pub-id-type="pmid">31313467</pub-id><pub-id pub-id-type="doi">10.1002/hbm.24714</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giordano</surname><given-names>BL</given-names></name><name><surname>Esposito</surname><given-names>M</given-names></name><name><surname>Valente</surname><given-names>G</given-names></name><name><surname>Formisano</surname><given-names>E</given-names></name></person-group><article-title>Intermediate acoustic-to-semantic representations link behavioral and neural responses to natural sounds</article-title><source>Nat Neurosci</source><year>2023</year><volume>26</volume><fpage>664</fpage><lpage>672</lpage><pub-id pub-id-type="pmcid">PMC10076214</pub-id><pub-id pub-id-type="pmid">36928634</pub-id><pub-id pub-id-type="doi">10.1038/s41593-023-01285-9</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grainger</surname><given-names>J</given-names></name><name><surname>Rey</surname><given-names>A</given-names></name><name><surname>Dufau</surname><given-names>S</given-names></name></person-group><article-title>Letter perception: from pixels to pandemonium</article-title><source>Trends Cogn Sci</source><year>2008</year><volume>12</volume><fpage>381</fpage><lpage>387</lpage><pub-id pub-id-type="pmid">18760658</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graumann</surname><given-names>M</given-names></name><name><surname>Ciuffi</surname><given-names>C</given-names></name><name><surname>Dwivedi</surname><given-names>K</given-names></name><name><surname>Roig</surname><given-names>G</given-names></name><name><surname>Cichy</surname><given-names>R</given-names></name></person-group><article-title>The spatiotemporal neural dynamics of object location representations in the human brain</article-title><source>Nat Hum Behav</source><year>2022</year><volume>6</volume><fpage>796</fpage><lpage>811</lpage><pub-id pub-id-type="pmcid">PMC9225954</pub-id><pub-id pub-id-type="pmid">35210593</pub-id><pub-id pub-id-type="doi">10.1038/s41562-022-01302-0</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guggenmos</surname><given-names>M</given-names></name><name><surname>Sterzer</surname><given-names>P</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><article-title>Multivariate pattern analysis for MEG: A comparison of dissimilarity measures</article-title><source>Neuroimage</source><year>2018</year><volume>173</volume><fpage>434</fpage><lpage>447</lpage><pub-id pub-id-type="pmid">29499313</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamilton</surname><given-names>RH</given-names></name><name><surname>Pascual-Leone</surname><given-names>A</given-names></name></person-group><article-title>Cortical plasticity associated with Braille learning</article-title><source>Trends Cogn Sci</source><year>1998</year><volume>2</volume><fpage>168</fpage><lpage>174</lpage><pub-id pub-id-type="pmid">21227151</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamilton</surname><given-names>R</given-names></name><name><surname>Keenan</surname><given-names>JP</given-names></name><name><surname>Catala</surname><given-names>M</given-names></name><name><surname>Pascual-Leone</surname><given-names>A</given-names></name></person-group><article-title>Alexia for Braille following bilateral occipital stroke in an early blind woman</article-title><source>Neuroreport</source><year>2000</year><volume>11</volume><fpage>237</fpage><lpage>240</lpage><pub-id pub-id-type="pmid">10674462</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haupt</surname><given-names>M</given-names></name><name><surname>Graumann</surname><given-names>M</given-names></name><name><surname>Teng</surname><given-names>S</given-names></name><name><surname>Kaltenbach</surname><given-names>C</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><article-title>The transformation of sensory to perceptual braille letter representations in the visually deprived brain</article-title><source>bioRxiv</source><year>2024</year><elocation-id>2024.02.12.579923</elocation-id><comment>Available at: <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/biorxiv/early/2024/03/22/2024.02.12.579923">https://www.biorxiv.org/content/biorxiv/early/2024/03/22/2024.02.12.579923</ext-link></comment><date-in-citation>Accessed April 2, 2024</date-in-citation></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haynes</surname><given-names>J-D</given-names></name><name><surname>Rees</surname><given-names>G</given-names></name></person-group><article-title>Decoding mental states from brain activity in humans</article-title><year>2006</year><volume>7</volume><fpage>523</fpage><lpage>534</lpage><pub-id pub-id-type="pmid">16791142</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Bankson</surname><given-names>BB</given-names></name><name><surname>Harel</surname><given-names>A</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><article-title>The representational dynamics of task and object processing in humans</article-title><source>Elife</source><year>2018</year><volume>7</volume><pub-id pub-id-type="pmcid">PMC5811210</pub-id><pub-id pub-id-type="pmid">29384473</pub-id><pub-id pub-id-type="doi">10.7554/eLife.32816</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>MX</given-names></name><name><surname>Mosher</surname><given-names>JC</given-names></name><name><surname>Leahy</surname><given-names>RM</given-names></name></person-group><article-title>A sensor-weighted overlapping-sphere head model and exhaustive head model comparison for MEG</article-title><source>Phys Med Biol</source><year>1999</year><volume>44</volume><fpage>423</fpage><lpage>440</lpage><pub-id pub-id-type="pmid">10070792</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname><given-names>DH</given-names></name><name><surname>Wiesel</surname><given-names>TN</given-names></name></person-group><article-title>Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex</article-title><source>J Physiol</source><year>1962</year><volume>160</volume><fpage>106</fpage><lpage>154</lpage><pub-id pub-id-type="pmcid">PMC1359523</pub-id><pub-id pub-id-type="pmid">14449617</pub-id><pub-id pub-id-type="doi">10.1113/jphysiol.1962.sp006837</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ioannides</surname><given-names>A</given-names></name><name><surname>Liu</surname><given-names>L</given-names></name><name><surname>Poghosyan</surname><given-names>V</given-names></name><name><surname>Saridis</surname><given-names>G</given-names></name><name><surname>Gjedde</surname><given-names>A</given-names></name><name><surname>Ptito</surname><given-names>M</given-names></name><name><surname>Kupers</surname><given-names>R</given-names></name></person-group><article-title>MEG reveals a fast pathway from somatosensory cortex to occipital areas via posterior parietal cortex in a blind subject</article-title><source>Front Hum Neurosci</source><year>2013</year><volume>7</volume><fpage>429</fpage><pub-id pub-id-type="pmcid">PMC3733019</pub-id><pub-id pub-id-type="pmid">23935576</pub-id><pub-id pub-id-type="doi">10.3389/fnhum.2013.00429</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isik</surname><given-names>L</given-names></name><name><surname>Meyers</surname><given-names>EM</given-names></name><name><surname>Leibo</surname><given-names>JZ</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name></person-group><article-title>The dynamics of invariant object recognition in the human visual system</article-title><source>J Neurophysiol</source><year>2014</year><volume>111</volume><fpage>91</fpage><lpage>102</lpage><pub-id pub-id-type="pmcid">PMC4280161</pub-id><pub-id pub-id-type="pmid">24089402</pub-id><pub-id pub-id-type="doi">10.1152/jn.00394.2013</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>JS</given-names></name><name><surname>Kanjlia</surname><given-names>S</given-names></name><name><surname>Merabet</surname><given-names>LB</given-names></name><name><surname>Bedny</surname><given-names>M</given-names></name></person-group><article-title>Development of the Visual Word Form Area Requires Visual Experience: Evidence from Blind Braille Readers</article-title><source>J Neurosci</source><year>2017</year><volume>37</volume><fpage>11495</fpage><lpage>11504</lpage><pub-id pub-id-type="pmcid">PMC5700429</pub-id><pub-id pub-id-type="pmid">29061700</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0997-17.2017</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname><given-names>J-R</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><article-title>Characterizing the dynamics of mental representations: the temporal generalization method</article-title><source>Trends Cogn Sci</source><year>2014</year><volume>18</volume><fpage>203</fpage><lpage>210</lpage><pub-id pub-id-type="pmcid">PMC5635958</pub-id><pub-id pub-id-type="pmid">24593982</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2014.01.002</pub-id></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Kievit</surname><given-names>RA</given-names></name></person-group><article-title>Representational geometry: integrating cognition, computation, and the brain</article-title><source>Trends Cogn Sci</source><year>2013</year><volume>17</volume><fpage>401</fpage><lpage>412</lpage><pub-id pub-id-type="pmcid">PMC3730178</pub-id><pub-id pub-id-type="pmid">23876494</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2013.06.007</pub-id></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Bandettini</surname><given-names>P</given-names></name></person-group><article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title><source>Front Syst Neurosci</source><year>2008</year><volume>2</volume><fpage>4</fpage><pub-id pub-id-type="pmcid">PMC2605405</pub-id><pub-id pub-id-type="pmid">19104670</pub-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><article-title>Imagenet classification with deep convolutional neural networks</article-title><source>Adv Neural Inf Process Syst</source><year>2012</year><volume>25</volume><fpage>1097</fpage><lpage>1105</lpage></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kupers</surname><given-names>R</given-names></name><name><surname>Ptito</surname><given-names>M</given-names></name></person-group><article-title>Compensatory plasticity and cross-modal reorganization following early visual deprivation</article-title><source>Neurosci Biobehav Rev</source><year>2014</year><volume>41</volume><fpage>36</fpage><lpage>52</lpage><pub-id pub-id-type="pmid">23954750</pub-id></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lochy</surname><given-names>A</given-names></name><name><surname>Jacques</surname><given-names>C</given-names></name><name><surname>Maillard</surname><given-names>L</given-names></name><name><surname>Colnat-Coulbois</surname><given-names>S</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name><name><surname>Jonas</surname><given-names>J</given-names></name></person-group><article-title>Selective visual representation of letters and words in the left ventral occipito-temporal cortex with intracerebral recordings</article-title><source>Proc Natl Acad Sci USA</source><year>2018</year><volume>115</volume><pub-id pub-id-type="pmcid">PMC6094145</pub-id><pub-id pub-id-type="pmid">30038000</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1718987115</pub-id></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lowe</surname><given-names>MX</given-names></name><name><surname>Mohsenzadeh</surname><given-names>Y</given-names></name><name><surname>Lahner</surname><given-names>B</given-names></name><name><surname>Charest</surname><given-names>I</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name><name><surname>Teng</surname><given-names>S</given-names></name></person-group><article-title>Cochlea to categories: The spatiotemporal dynamics of semantic auditory representations</article-title><source>Cogn Neuropsychol</source><year>2021</year><volume>38</volume><fpage>468</fpage><lpage>489</lpage><pub-id pub-id-type="pmcid">PMC10589059</pub-id><pub-id pub-id-type="pmid">35729704</pub-id><pub-id pub-id-type="doi">10.1080/02643294.2022.2085085</pub-id></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Madec</surname><given-names>S</given-names></name><name><surname>Rey</surname><given-names>A</given-names></name><name><surname>Dufau</surname><given-names>S</given-names></name><name><surname>Klein</surname><given-names>M</given-names></name><name><surname>Grainger</surname><given-names>J</given-names></name></person-group><article-title>The time course of visual letter perception</article-title><source>J Cogn Neurosci</source><year>2012</year><volume>24</volume><fpage>1645</fpage><lpage>1655</lpage><pub-id pub-id-type="pmid">22185493</pub-id></element-citation></ref><ref id="R57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Makin</surname><given-names>TR</given-names></name><name><surname>Krakauer</surname><given-names>JW</given-names></name></person-group><article-title>Against cortical reorganisation</article-title><source>Elife</source><year>2023</year><volume>12</volume><pub-id pub-id-type="pmcid">PMC10662956</pub-id><pub-id pub-id-type="pmid">37986628</pub-id><pub-id pub-id-type="doi">10.7554/eLife.84716</pub-id></element-citation></ref><ref id="R58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title><source>J Neurosci Methods</source><year>2007</year><volume>164</volume><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="pmid">17517438</pub-id></element-citation></ref><ref id="R59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martiniello</surname><given-names>N</given-names></name><name><surname>Wittich</surname><given-names>W</given-names></name></person-group><article-title>The association between tactile, motor and cognitive capacities and braille reading performance: a scoping review of primary evidence to advance research on braille and aging</article-title><source>Disabil Rehabil</source><year>2022</year><volume>44</volume><fpage>2515</fpage><lpage>2536</lpage><pub-id pub-id-type="pmid">33147427</pub-id></element-citation></ref><ref id="R60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Merabet</surname><given-names>LB</given-names></name><name><surname>Pascual-Leone</surname><given-names>A</given-names></name></person-group><article-title>Neural reorganization following sensory loss: the opportunity of change</article-title><source>Nat Rev Neurosci</source><year>2010</year><volume>11</volume><fpage>44</fpage><lpage>52</lpage><pub-id pub-id-type="pmcid">PMC3898172</pub-id><pub-id pub-id-type="pmid">19935836</pub-id><pub-id pub-id-type="doi">10.1038/nrn2758</pub-id></element-citation></ref><ref id="R61"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Millar</surname><given-names>S</given-names></name></person-group><source>Reading by touch</source><year>1997</year><comment><ext-link ext-link-type="uri" xlink:href="https://psycnet.apa.org">https://psycnet.apa.org</ext-link> › record<ext-link ext-link-type="uri" xlink:href="https://psycnet.apa.org">https://psycnet.apa.org</ext-link> › record 337 Available at: <ext-link ext-link-type="uri" xlink:href="https://psycnet.apa.org/fulltext/1997-05059-000.pdf">https://psycnet.apa.org/fulltext/1997-05059-000.pdf</ext-link></comment></element-citation></ref><ref id="R62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nichols</surname><given-names>TE</given-names></name><name><surname>Holmes</surname><given-names>AP</given-names></name></person-group><article-title>Nonparametric permutation tests for functional neuroimaging: a primer with examples</article-title><source>Hum Brain Mapp</source><year>2002</year><volume>15</volume><fpage>1</fpage><lpage>25</lpage><pub-id pub-id-type="pmcid">PMC6871862</pub-id><pub-id pub-id-type="pmid">11747097</pub-id><pub-id pub-id-type="doi">10.1002/hbm.1058</pub-id></element-citation></ref><ref id="R63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norvig</surname><given-names>P</given-names></name></person-group><article-title>English Letter Frequency Counts: Mayzner Revisited</article-title><source>Peter Norvig</source><year>2013</year><date-in-citation>Accessed March 4, 2024</date-in-citation><comment>Available at: <ext-link ext-link-type="uri" xlink:href="http://norvig.com/mayzner.html">http://norvig.com/mayzner.html</ext-link></comment></element-citation></ref><ref id="R64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ogg</surname><given-names>M</given-names></name><name><surname>Carlson</surname><given-names>TA</given-names></name><name><surname>Slevc</surname><given-names>LR</given-names></name></person-group><article-title>The Rapid Emergence of Auditory Object Representations in Cortex Reflect Central Acoustic Attributes</article-title><source>J Cogn Neurosci</source><year>2020</year><volume>32</volume><fpage>111</fpage><lpage>123</lpage><pub-id pub-id-type="pmid">31560265</pub-id></element-citation></ref><ref id="R65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pascual-Leone</surname><given-names>A</given-names></name><name><surname>Amedi</surname><given-names>A</given-names></name><name><surname>Fregni</surname><given-names>F</given-names></name><name><surname>Merabet</surname><given-names>LB</given-names></name></person-group><article-title>The plastic human brain cortex</article-title><source>Annu Rev Neurosci</source><year>2005</year><volume>28</volume><fpage>377</fpage><lpage>401</lpage><pub-id pub-id-type="pmid">16022601</pub-id></element-citation></ref><ref id="R66"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pascual-Leone</surname><given-names>A</given-names></name><name><surname>Hamilton</surname><given-names>R</given-names></name><name><surname>Tormos</surname><given-names>JM</given-names></name><name><surname>Keenan</surname><given-names>JP</given-names></name><name><surname>Catalá</surname><given-names>MD</given-names></name></person-group><chapter-title>Neuroplasticity in the Adjustment to Blindness</chapter-title><source>Neuronal Plasticity: Building a Bridge from the Laboratory to the Clinic</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin Heidelberg</publisher-loc><year>1999</year><fpage>93</fpage><lpage>108</lpage></element-citation></ref><ref id="R67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Popham</surname><given-names>SF</given-names></name><name><surname>Huth</surname><given-names>AG</given-names></name><name><surname>Bilenko</surname><given-names>NY</given-names></name><name><surname>Deniz</surname><given-names>F</given-names></name><name><surname>Gao</surname><given-names>JS</given-names></name><name><surname>Nunez-Elizalde</surname><given-names>AO</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><article-title>Visual and linguistic semantic representations are aligned at the border of human visual cortex</article-title><source>Nat Neurosci</source><year>2021</year><volume>24</volume><fpage>1628</fpage><lpage>1636</lpage><pub-id pub-id-type="pmid">34711960</pub-id></element-citation></ref><ref id="R68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rączy</surname><given-names>K</given-names></name><name><surname>Urbańczyk</surname><given-names>A</given-names></name><name><surname>Korczyk</surname><given-names>M</given-names></name><name><surname>Szewczyk</surname><given-names>JM</given-names></name><name><surname>Sumera</surname><given-names>E</given-names></name><name><surname>Szwed</surname><given-names>M</given-names></name></person-group><article-title>Orthographic Priming in Braille Reading as Evidence for Task-specific Reorganization in the Ventral Visual Cortex of the Congenitally Blind</article-title><source>J Cogn Neurosci</source><year>2019</year><volume>31</volume><fpage>1065</fpage><lpage>1078</lpage><pub-id pub-id-type="pmid">30938589</pub-id></element-citation></ref><ref id="R69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reich</surname><given-names>L</given-names></name><name><surname>Szwed</surname><given-names>M</given-names></name><name><surname>Cohen</surname><given-names>L</given-names></name><name><surname>Amedi</surname><given-names>A</given-names></name></person-group><article-title>A ventral visual stream reading center independent of visual experience</article-title><source>Curr Biol</source><year>2011</year><volume>21</volume><fpage>363</fpage><lpage>368</lpage><pub-id pub-id-type="pmid">21333539</pub-id></element-citation></ref><ref id="R70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ritchey</surname><given-names>KD</given-names></name><name><surname>Speece</surname><given-names>DL</given-names></name></person-group><article-title>From letter names to word reading: The nascent role of sublexical fluency</article-title><source>Contemp Educ Psychol</source><year>2006</year><volume>31</volume><fpage>301</fpage><lpage>327</lpage></element-citation></ref><ref id="R71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saal</surname><given-names>HP</given-names></name><name><surname>Delhaye</surname><given-names>BP</given-names></name><name><surname>Rayhaun</surname><given-names>BC</given-names></name><name><surname>Bensmaia</surname><given-names>SJ</given-names></name></person-group><article-title>Simulating tactile signals from the whole hand with millisecond precision</article-title><source>Proc Natl Acad Sci U S A</source><year>2017</year><volume>114</volume><fpage>E5693</fpage><lpage>E5702</lpage><pub-id pub-id-type="pmcid">PMC5514748</pub-id><pub-id pub-id-type="pmid">28652360</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1704856114</pub-id></element-citation></ref><ref id="R72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sadato</surname><given-names>N</given-names></name><name><surname>Pascual-Leone</surname><given-names>A</given-names></name><name><surname>Grafman</surname><given-names>J</given-names></name><name><surname>Ibañez</surname><given-names>V</given-names></name><name><surname>Deiber</surname><given-names>M-P</given-names></name><name><surname>Dold</surname><given-names>G</given-names></name><name><surname>Hallett</surname><given-names>M</given-names></name></person-group><article-title>Activation of the primary visual cortex by Braille reading in blind subjects</article-title><source>Nature</source><year>1996</year><volume>380</volume><fpage>526</fpage><lpage>528</lpage><pub-id pub-id-type="pmid">8606771</pub-id></element-citation></ref><ref id="R73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seibold</surname><given-names>DR</given-names></name><name><surname>McPhee</surname><given-names>RD</given-names></name></person-group><article-title>Commonality analysis: A method for decomposing explained variance in multiple regression analyses</article-title><source>Hum Commun Res</source><year>1979</year><volume>5</volume><fpage>355</fpage><lpage>365</lpage></element-citation></ref><ref id="R74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seydell-Greenwald</surname><given-names>A</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Newport</surname><given-names>EL</given-names></name><name><surname>Bi</surname><given-names>Y</given-names></name><name><surname>Striem-Amit</surname><given-names>E</given-names></name></person-group><article-title>Spoken language processing activates the primary visual cortex</article-title><source>PLoS One</source><year>2023</year><volume>18</volume><elocation-id>e0289671</elocation-id><pub-id pub-id-type="pmcid">PMC10420367</pub-id><pub-id pub-id-type="pmid">37566582</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0289671</pub-id></element-citation></ref><ref id="R75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Striem-Amit</surname><given-names>E</given-names></name><name><surname>Cohen</surname><given-names>L</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Amedi</surname><given-names>A</given-names></name></person-group><article-title>Reading with sounds: sensory substitution selectively activates the visual word form area in the blind</article-title><source>Neuron</source><year>2012</year><volume>76</volume><fpage>640</fpage><lpage>652</lpage><pub-id pub-id-type="pmid">23141074</pub-id></element-citation></ref><ref id="R76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tadel</surname><given-names>F</given-names></name><name><surname>Baillet</surname><given-names>S</given-names></name><name><surname>Mosher</surname><given-names>JC</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Leahy</surname><given-names>RM</given-names></name></person-group><article-title>Brainstorm: a user-friendly application for MEG/EEG analysis</article-title><source>Comput Intell Neurosci</source><year>2011</year><volume>2011</volume><elocation-id>879716</elocation-id><pub-id pub-id-type="pmcid">PMC3090754</pub-id><pub-id pub-id-type="pmid">21584256</pub-id><pub-id pub-id-type="doi">10.1155/2011/879716</pub-id></element-citation></ref><ref id="R77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thesen</surname><given-names>T</given-names></name><name><surname>McDonald</surname><given-names>CR</given-names></name><name><surname>Carlson</surname><given-names>C</given-names></name><name><surname>Doyle</surname><given-names>W</given-names></name><name><surname>Cash</surname><given-names>S</given-names></name><name><surname>Sherfey</surname><given-names>J</given-names></name><name><surname>Felsovalyi</surname><given-names>O</given-names></name><name><surname>Girard</surname><given-names>H</given-names></name><name><surname>Barr</surname><given-names>W</given-names></name><name><surname>Devinsky</surname><given-names>O</given-names></name><name><surname>Kuzniecky</surname><given-names>R</given-names></name><etal/></person-group><article-title>Sequential then interactive processing of letters and words in the left fusiform gyrus</article-title><source>Nat Commun</source><year>2012</year><volume>3</volume><elocation-id>1284</elocation-id><pub-id pub-id-type="pmcid">PMC4407686</pub-id><pub-id pub-id-type="pmid">23250414</pub-id><pub-id pub-id-type="doi">10.1038/ncomms2220</pub-id></element-citation></ref><ref id="R78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wetzel</surname><given-names>R</given-names></name><name><surname>Knowlton</surname><given-names>M</given-names></name></person-group><article-title>A comparison of print and braille reading rates on three reading tasks</article-title><source>J Vis Impair Blind</source><year>2000</year><volume>94</volume><fpage>146</fpage><lpage>154</lpage></element-citation></ref><ref id="R79"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>C</given-names></name><name><surname>Mc Clinton</surname><given-names>Z</given-names></name><name><surname>Silvano</surname><given-names>E</given-names></name><name><surname>Alfonso</surname><given-names>C</given-names></name><name><surname>Jaicaman</surname><given-names>LP</given-names></name><name><surname>Yun</surname><given-names>H</given-names></name><name><surname>Bedny</surname><given-names>M</given-names></name></person-group><source>Contextual, lexical, and sublexical effects on braille reading</source><year>2024</year><comment>Available at: <ext-link ext-link-type="uri" xlink:href="https://hsp2024.github.io/abstracts/submission_201.pdf">https://hsp2024.github.io/abstracts/submission_201.pdf</ext-link></comment></element-citation></ref><ref id="R80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><article-title>Using goal-driven deep learning models to understand sensory cortex</article-title><source>Nat Neurosci</source><year>2016</year><volume>19</volume><fpage>356</fpage><lpage>365</lpage><pub-id pub-id-type="pmid">26906502</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Experimental setup, analysis, and general MEG decoding results.</title><p>(<bold>a</bold>) Stimulus presentation and trial schematic. Single alphabetical letters were presented visually at central fixation to sighted subjects, or to the stationary index fingerpad via braille display to blind subjects. Procedures and results for the Sighted and Blind groups are color-coded and denoted with purple eye and green hand icons, respectively. (<bold>b</bold>) Both groups readily perceived and responded to occasional vigilance targets. Color-coded scatter plots indicate <italic>d</italic>’ for individuals; boxplot shows median and interquartile range. (<bold>c</bold>) Time-resolved multivariate pattern analysis. For each time point <italic>t</italic> and trial separately, we arranged MEG activity across the 306 sensors into MEG pattern vectors. Single-trial pattern vectors were then whitened, and randomly assigned to bins that were averaged to create <italic>M</italic> pseudo-trial vectors for increased signal-to-noise ratio. We classified experimental conditions (i.e. the 10 letters) pairwise from the pseudotrials using a linear support vector machine (SVM) classifier. The assignment of raw trials to pseudotrials and classification procedure was repeated 100 times; the resulting mean decoding accuracy for each stimulus pair populated a decoding matrix indexed in rows and conditions by the letters classified, for each <italic>t</italic> The decoding matrix — a representational dissimilarity matrix — formed the basis of further analyses. For example, averaging across the matrix yielded a grand average time course indicating the dynamics with which letter representations emerge. (<bold>d</bold>) Grand-average classification time courses for letters for blind (green) and sighted (purple) participants. Color-coded bars beneath plots indicate clusters of significant decoding; arrows indicate peaks, with thin horizontal bars indicating 95% confidence intervals (CIs). Shaded region from 0–500 ms indicates stimulus duration period. Significance was assessed via permutation-based cluster-size inference (p&lt;0.05 cluster-definition threshold, p&lt;0.05 cluster threshold, one-sided, 500 permutations).</p></caption><graphic xlink:href="EMS199824-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>The evolving format of letter representations.</title><p>(<bold>a</bold>) Models used to test hypotheses about representational format. Low-level (<italic>visual, tactile</italic>) models were tested specific to group/modality; high-level <italic>bigram</italic> model capturing text embeddings was tested for both groups. (<bold>b</bold>) Schematic of model comparison between MEG data and models. Comparisons are performed using partial rank-order (Spearman) correlation. (<bold>c,d</bold>) Whole-brain model comparison time courses for sighted and blind participants. Significance testing, significant time point clusters, peaks, and 95% CIs as in <xref ref-type="fig" rid="F1">Fig. 1</xref>. Significance onset times for each model shown at bottom of each plot.</p></caption><graphic xlink:href="EMS199824-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Spatiotemporal distribution of letter representations and formats in key ROIs.</title><p>(<bold>a</bold>) Key anatomical regions of interest (ROIs) for MEG source localization. <italic>Sensorimotor</italic> (<italic>SM, top</italic>) comprises pre- and postcentral gyri on the right (sighted) or stimulation-contralateral (blind) hemisphere. Early visual cortex (<italic>EVC, middle</italic>) comprises pericalcarine, lateral occipital, cuneus, and lingual gyrus ROIs. Inferotemporal (<italic>IT, bottom</italic>) comprises left-lateralized inferotemporal and fusiform anatomical ROIs. (<bold>b</bold>) ROI-specific MEG decoding of letters color-coded by group, in rows as specified in (<bold>a</bold>). Triangles and thin horizontal bars indicate peaks with 95% CIs; thick bars below curves indicate significant clusters as in previous figures. (<bold>c,d</bold>) ROI-specific MEG-model correlation time courses for group-specific low-level (<italic>visual</italic>, <bold>c</bold>; <italic>tactile</italic>, <bold>d</bold>) and common high-level (<italic>bigrams</italic>) representational models. Peaks, significant clusters, significance onsets, and CIs color-coded by model. Significance was assessed as in whole-brain analyses, via permutation-based cluster-size inference (p &lt; 0.05 cluster-definition threshold, p &lt; 0.05 cluster threshold, one-sided, 500 permutations).</p></caption><graphic xlink:href="EMS199824-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Interregional dynamics of shared letter representations.</title><p>(<bold>a</bold>) Schematic of commonality analysis identifying shared variance between pairs of ROI-specific MEG data and models. (<bold>b</bold>) Computing the commonality R<sup>2</sup> for both models across all pairs of time points between MEG ROIs yields a temporal generalization map of uniquely shared variance for each model, expressed as an R<sup>2</sup> difference score for low (<italic>left</italic>) or high-level (<italic>right</italic>) representations. (<bold>c</bold>) The geometry of the resulting “patches” of significance indicates inter-model differences in temporal dynamics (<italic>left</italic>) or inter-ROI temporal differences (<italic>right</italic>) suggestive of information flow. (<bold>d,e</bold>) Commonality results for sighted and blind groups, respectively, with lowand high-level representations plotted separately for clarity. White dotted lines indicate stimulus onset. White contours outline significant clusters, assessed by 2-d permutation-based cluster-size inference with parameters as in other analyses (p &lt; 0.05 cluster-definition threshold, p &lt; 0.05 cluster threshold, one-sided, 500 permutations).</p></caption><graphic xlink:href="EMS199824-f004"/></fig><table-wrap id="T1" position="float" orientation="portrait"><label>Table 1</label><caption><title>Blind participant demographics and characteristics.</title></caption><table frame="box" rules="all"><thead><tr><th valign="top" align="left">ID</th><th valign="top" align="left">M/F</th><th valign="top" align="left">Age at test (years)</th><th valign="top" align="left">Blindness level</th><th valign="top" align="left">Blindness onset age (years)</th></tr></thead><tbody><tr><td valign="top" align="left">1</td><td valign="top" align="left">F</td><td valign="top" align="left">34</td><td valign="top" align="left">Total</td><td valign="top" align="left">0</td></tr><tr><td valign="top" align="left">2</td><td valign="top" align="left">F</td><td valign="top" align="left">24</td><td valign="top" align="left">LP</td><td valign="top" align="left">0</td></tr><tr><td valign="top" align="left">3</td><td valign="top" align="left">M</td><td valign="top" align="left">28</td><td valign="top" align="left">Total</td><td valign="top" align="left">0</td></tr><tr><td valign="top" align="left">4</td><td valign="top" align="left">F</td><td valign="top" align="left">29</td><td valign="top" align="left">Total</td><td valign="top" align="left">0</td></tr><tr><td valign="top" align="left">5</td><td valign="top" align="left">M</td><td valign="top" align="left">27</td><td valign="top" align="left">LP</td><td valign="top" align="left">0</td></tr><tr><td valign="top" align="left">6</td><td valign="top" align="left">F</td><td valign="top" align="left">26</td><td valign="top" align="left">LP</td><td valign="top" align="left">0</td></tr><tr><td valign="top" align="left">7</td><td valign="top" align="left">F</td><td valign="top" align="left">29</td><td valign="top" align="left">Total</td><td valign="top" align="left">0</td></tr><tr><td valign="top" align="left">8</td><td valign="top" align="left">F</td><td valign="top" align="left">32</td><td valign="top" align="left">Total</td><td valign="top" align="left">3</td></tr><tr><td valign="top" align="left">9</td><td valign="top" align="left">M</td><td valign="top" align="left">39</td><td valign="top" align="left">LP</td><td valign="top" align="left">0</td></tr><tr><td valign="top" align="left">10</td><td valign="top" align="left">M</td><td valign="top" align="left">24</td><td valign="top" align="left">LP</td><td valign="top" align="left">0</td></tr><tr><td valign="top" align="left">11</td><td valign="top" align="left">F</td><td valign="top" align="left">35</td><td valign="top" align="left">LP</td><td valign="top" align="left">3</td></tr><tr><td valign="top" align="left">12</td><td valign="top" align="left">M</td><td valign="top" align="left">18</td><td valign="top" align="left">LP</td><td valign="top" align="left">&lt;1</td></tr></tbody></table><table-wrap-foot><fn id="TFN1"><p id="P42"><italic>Total</italic> = total blindness, no light perception in either eye. <italic>LP</italic> = light perception, no spatial perception.</p></fn></table-wrap-foot></table-wrap></floats-group></article>