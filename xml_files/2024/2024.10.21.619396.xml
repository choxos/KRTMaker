<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS199634</article-id><article-id pub-id-type="doi">10.1101/2024.10.21.619396</article-id><article-id pub-id-type="archive">PPR929512</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Functional and causal neural mechanisms of human voice perception in noisy situations</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-0638-3981</contrib-id><name><surname>Ceravolo</surname><given-names>Leonardo</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Jaussi</surname><given-names>Elisa Scariati</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Frühholz</surname><given-names>Sascha</given-names></name><xref ref-type="aff" rid="A4">4</xref><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Van De Ville</surname><given-names>Dimitri</given-names></name><xref ref-type="aff" rid="A6">6</xref><xref ref-type="aff" rid="A7">7</xref></contrib><contrib contrib-type="author"><name><surname>Grandjean</surname><given-names>Didier</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib></contrib-group><aff id="A1"><label>1</label>Neuroscience of Emotion and Affective Dynamics Lab, Department of Psychology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01swzsf04</institution-id><institution>University of Geneva</institution></institution-wrap>, <city>Geneva</city>, <country country="CH">Switzerland</country></aff><aff id="A2"><label>2</label>Swiss Center for Affective Sciences, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01swzsf04</institution-id><institution>University of Geneva</institution></institution-wrap>, <city>Geneva</city>, <country country="CH">Switzerland</country></aff><aff id="A3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01m1pv723</institution-id><institution>Geneva University Hospital</institution></institution-wrap>, <city>Geneva</city>, <country country="CH">Switzerland</country></aff><aff id="A4"><label>4</label>Department of Psychology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02crff812</institution-id><institution>University of Zurich</institution></institution-wrap>, <city>Zurich</city>, <country country="CH">Switzerland</country></aff><aff id="A5"><label>5</label>Department of Psychology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01xtthb56</institution-id><institution>University of Oslo</institution></institution-wrap>, <city>Oslo</city>, <country country="NO">Norway</country></aff><aff id="A6"><label>6</label>Department of Radiology and Medical Informatics, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01swzsf04</institution-id><institution>University of Geneva</institution></institution-wrap>, <city>Geneva</city>, <country country="CH">Switzerland</country></aff><aff id="A7"><label>7</label>Institute of Bioengineering, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02s376052</institution-id><institution>Ecole Polytechnique Fédérale de Lausanne</institution></institution-wrap>, <city>Lausanne</city>, <country country="CH">Switzerland</country></aff><author-notes><corresp id="CR1">
<label>*</label><bold>Corresponding author:</bold> Leonardo Ceravolo, University of Geneva &amp; Swiss Center for Affective Sciences, Unimail, office 5133, Boulevard Pont-d’Arve 50, CH-1205 Geneva, Switzerland, <email>leonardo.ceravolo@unige.ch</email> Phone: +41 22 379 92 73</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>25</day><month>10</month><year>2024</year></pub-date><pub-date pub-type="preprint"><day>22</day><month>10</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Human communication entails an efficient way of simultaneously processing voice and reducing the impact of environmental noise. By manipulating background noise, we aimed at clarifying the neural mechanisms allowing voice comprehension in noisy situations. Our results point to spatial and temporal coexistence of lateral and medial temporal cortex networks when voice is easily detected in highly noisy conditions, revealing the necessary neural underpinnings of human communication in realistic situations.</p></abstract></article-meta></front><body><p id="P2">Among the most recent evolutionary changes that made <italic>Homo sapiens</italic> today’s anatomically modern human<sup><xref ref-type="bibr" rid="R1">1</xref></sup>, language and vocal communication were of the highest importance and have therefore been extensively studied by the neuroscientific community<sup><xref ref-type="bibr" rid="R2">2</xref></sup>. Voice processing—independently of semantics—was however in the limelight, especially in the context of cognitive neuroscience, until voice-sensitive areas were identified along the bilateral human temporal lobe<sup><xref ref-type="bibr" rid="R3">3</xref></sup>. These voice-sensitive areas include large portions of the middle and superior temporal lobe or superior temporal cortex (STC) and underlie vocal processing in anterior, mid and posterior subparts of the STC<sup><xref ref-type="bibr" rid="R4">4</xref></sup>. Notably, parts of the posterior STC have been shown to process information about voices, faces and face-voice integration in humans<sup><xref ref-type="bibr" rid="R5">5</xref></sup>, raising the question of whether voice-sensitive brain regions are species-specific or species-independent. Species-specific, voice-sensitive/vocalization-specific areas have been highlighted in studies on the STC of humans<sup><xref ref-type="bibr" rid="R6">6</xref></sup> and macaques<sup><xref ref-type="bibr" rid="R7">7</xref></sup>, notably in the anterior temporal sulcus for the former as well as in the posterior ‘belt’ and ‘parabelt’ regions of the auditory cortex for the latter. In humans, subparts of these species-specific, voice-sensitive areas—in particular, the mid and posterior parts of the STC—respond strongly to general emotional voice content<sup><xref ref-type="bibr" rid="R8">8</xref>–<xref ref-type="bibr" rid="R10">10</xref></sup> and more specifically to angry vocal signals<sup><xref ref-type="bibr" rid="R8">8</xref>,<xref ref-type="bibr" rid="R9">9</xref>,<xref ref-type="bibr" rid="R11">11</xref>–<xref ref-type="bibr" rid="R16">16</xref></sup>, extending previously mentioned findings that focused mainly on non-emotional voice perception. The posterior STC was also highlighted as an important bilateral region involved in recognizing environmental sounds<sup><xref ref-type="bibr" rid="R17">17</xref></sup> and voice-in-noise perception in adults<sup><xref ref-type="bibr" rid="R18">18</xref></sup>, in addition to the bilateral posterior middle temporal gyrus (MTG)<sup><xref ref-type="bibr" rid="R19">19</xref></sup>, emphasizing distinct roles among temporal lobe subregions usually taken as a whole and referred to as ‘voice-sensitive’.</p><p id="P3">One can however question whether voice-related processing mainly relies on the bilateral STC—and its subparts—as specialized hubs for processing vocal signals, or whether these areas instead work in a coordinated way with other relevant cortical and/or subcortical brain regions known to be involved in (emotional) vocal processing and/or environmental sound recognition. This aspect is of particular interest since our brain should be able at any time to filter noise—at least partially—to favor vocal communication, especially in the context of social interactions. Relevant areas include the inferior frontal cortex<sup><xref ref-type="bibr" rid="R15">15</xref>,<xref ref-type="bibr" rid="R17">17</xref>,<xref ref-type="bibr" rid="R19">19</xref></sup> and subregions of the medial temporal lobe (MTL; i.e., the bilateral hippocampus, amygdala and parahippocampal gyrus) involved for instance in vocal threat perception<sup><xref ref-type="bibr" rid="R20">20</xref></sup>. In fact, subregions of the amygdala were shown to underlie the automatic (bilateral superficial complex) and explicit (right laterobasal complex) focus of attention on vocal threat<sup><xref ref-type="bibr" rid="R14">14</xref>,<xref ref-type="bibr" rid="R20">20</xref></sup>. Moreover, the role of the hippocampus in vocal and musical affect processing was emphasized through both direct connections to the primary auditory cortex and indirect connections mediated by the parahippocampal gyrus<sup><xref ref-type="bibr" rid="R21">21</xref></sup>. The amygdala, parahippocampal gyrus and hippocampus were also shown to underlie the processing of familiar as opposed to non-familiar voices and noises presented both visually and auditorily<sup><xref ref-type="bibr" rid="R22">22</xref></sup>, adding even more weight to the inclusion of MTL regions in the functional neural framework of voice perception in realistic—often noisy—situations.</p><p id="P4">While environmental noise is, in the context of the present studies, related to voice perception and may seem almost trivial in comparison to the duties and challenges of everyday life as humans, being exposed to noise may actually play a significant role in several health conditions and cognitive impairments. In fact, recent literature abounded for a role of noise in the induction of stress hormones, oxidative stress and ultimately vascular dysfunctions<sup><xref ref-type="bibr" rid="R23">23</xref></sup>—as recently reviewed for transportation noise pollution<sup><xref ref-type="bibr" rid="R24">24</xref></sup>—as well as brain impairment and cognitive decline in mice<sup><xref ref-type="bibr" rid="R25">25</xref></sup> when exposed to chronic traffic noise. The same goes for humans<sup><xref ref-type="bibr" rid="R26">26</xref></sup>, together with potential psychological disorders. Therefore, noise can be categorized as a prevalent and important cause of stress, including above-mentioned risks on health. In the context of our studies—and even though we emphasized the importance and causal relation between an exposition to noise and stress, we specifically challenged the ability of human participants to perceive voice and environmental noise both separately and concurrently. We assumed, based on the literature and on known models of voice processing<sup><xref ref-type="bibr" rid="R27">27</xref></sup> that parallel brain networks would be involved, while their specific locations and the involved brain areas still remain unclear. Therefore, an integrated brain-level understanding of voice processing in humans in realistic, noisy situations should question and try to pinpoint the potential existence of such parallelization. This investigation should have as a starting point the different subparts of voice-sensitive areas. In fact, these subparts could act in parallel and/or together with relevant cortical and subcortical brain regions such as the MTL or the lateral/inferior frontal cortex. Identifying and testing such brain network could therefore have the potential to clarify the neural mechanisms allowing noise-free vocal communication. Such questions can be addressed scientifically by the use of static<sup><xref ref-type="bibr" rid="R28">28</xref></sup> and dynamic or causal<sup><xref ref-type="bibr" rid="R29">29</xref>,<xref ref-type="bibr" rid="R30">30</xref></sup> connectivity measures at the brain level. The literature on structural<sup><xref ref-type="bibr" rid="R31">31</xref></sup> and functional<sup><xref ref-type="bibr" rid="R4">4</xref>,<xref ref-type="bibr" rid="R15">15</xref>,<xref ref-type="bibr" rid="R32">32</xref></sup> connectivity of voice-sensitive brain areas is however scarce and the lack of evidence for a distributed brain network underlying the full neural spectrum of vocal processing, especially in realistic noisy situations<sup><xref ref-type="bibr" rid="R33">33</xref></sup>, is needed. The abovementioned research question led us to several assumptions and hypotheses that were operationalized through two independent functional MRI studies and one behavioral study. When voice and noise were presented separately (Study 1, N=98), we hypothesized: (a) enhanced activity and functional and effective connectivity in voice-sensitive areas (STC)—as a replication of existing data3— and MTL<sup><xref ref-type="bibr" rid="R21">21</xref></sup>, (b) an involvement of MTL and posterior STC when processing environmental sounds and noise<sup><xref ref-type="bibr" rid="R19">19</xref></sup>. When voice and noise were presented concurrently at various intensities (Study 2, N=18), we predicted: (c) response patterns pertaining to an ease of perceiving a vocal signal shifting towards chance when noise was the highest, (d) with slower reaction times due to an increase of task difficulty (high noise levels). For the same paradigm using fMRI (Study 3, N=20), we hypothesized: (e) a replication of the patterns of behavioral responses found in Study 2, (f) enhanced activity in and functional connectivity between posterior STC and the MTL—specifically the amygdala, parahippocampal gyrus and hippocampus—when processing and assessing voices while background noise was present, especially when noise is at higher acoustic intensities than voices and finally (g) a causal implication of posterior STC, especially the posterior middle temporal gyrus (pMTG) and subregions of the MTL (amygdala, parahippocampal gyrus and hippocampus) for successful vocal communication in noisy situations. The operationalization of our hypotheses should therefore reveal how specific, potentially parallel brain networks may underlie successful and reliable voice processing and comprehension in noisy environments and bring new insights to voice perception and communication in humans. This approach is meant to point out a novel framework targeting the general, coordinated and distributed neural substrates underlying vocal processing rather than focusing solely on temporal voice areas (TVAs) or on auditory brain regions.</p><sec id="S1" sec-type="results"><title>Results</title><p id="P5">The three studies presented in this manuscript were planned as a grouped effort to approach a better understanding of voice perception in realistic situations—i.e., when environmental noise is omnipresent. The final outcome is illustrated by a general model of voice processing in everyday situations. Study 1 aimed at replicating previous findings<sup><xref ref-type="bibr" rid="R3">3</xref></sup> involving ‘voice-sensitive areas’ and also extending our knowledge of how humans process both voice and noise signals, separately. Study 2 was implemented to assess the ability of human participants to process and perceive voice when background noise is present—at varying intensity levels. It was also the starting point to allow an implementation of such paradigm in the MRI scanner (Study 3). Study 3 was therefore acquired to better understand brain networks—both using wholebrain, functional and effective connectivity analyses—allowing a reliable perception of voice signals in noisy situations and the subsequent decision(s) based on such prior processing, which parallels our everyday experience as humans considering social interactions and communication.</p><sec id="S2"><title>Study 1: voice-sensitive areas and ‘non-vocal’, environmental noise processing</title><p id="P6">We started with whole-brain analyses in order to replicate the findings of Belin and colleagues<sup><xref ref-type="bibr" rid="R3">3</xref></sup> and, of more importance, to have a strong statistical basis to extract specific, hypothesis- and contrast-driven functional ROIs recruited when processing voice and noise separately (see <xref ref-type="table" rid="T1">Table 1</xref> for an overview of the 26 ROIs extracted). We therefore focused on voice processing by contrasting vocal with non-vocal blocks—the latter blocks containing about 50% of environmental noise stimuli—and also performed the inverse contrast to focus on non-vocal, noise processing. These ROIs were then included in our functional and effective connectivity matrices as well as used as a mask in connectivity multi-voxel pattern analyses (fc-MVPA, see below).</p><sec id="S3"><title>Whole-brain neuroimaging results</title><p id="P7">Voice processing—contrasting Voice to Non-Voice blocks—recruited large and distributed brain regions, notably along the temporal cortices bilaterally (<xref ref-type="fig" rid="F1">Fig.1a-c</xref>). These expected enhanced temporal lobe activations included large portions of the anterior, mid and posterior superior temporal gyrus (STG) and the MTG, including most of the superior temporal sulcus as well (<xref ref-type="fig" rid="F1">Fig.1a,c</xref>). These regions are known as the TVAs and our data reliably replicated the results of the princeps study<sup><xref ref-type="bibr" rid="R3">3</xref></sup>. We also found stronger activity in the bilateral prefrontal cortex, namely in the right inferior frontal gyrus (IFG), bilateral amygdala, putamen, medial dorsal nucleus and medial geniculate body of the thalamus (see <xref ref-type="table" rid="T2">Table 2</xref>). We extracted 19 ROIs from significantly enhanced voxels resulting from the Voice &gt; Non-voice contrast (<xref ref-type="table" rid="T1">Table 1</xref>).</p><p id="P8">The inverse contrast, i.e., Non-voice &gt; Voice, yielded to enhanced BOLD signal in large portions of the bilateral prefrontal cortex, most notably in the IFG (<xref ref-type="fig" rid="F1">Fig.1d,f</xref>), as well as in the anterior and posterior cingulate cortex (<xref ref-type="fig" rid="F1">Fig.1g,h</xref>) and in the bilateral parahippocampal gyri (<xref ref-type="fig" rid="F1">Fig.1e</xref>). Seven ROIs to be included in the connectivity analyses—and masked fc-MVPA analyses—were extracted from this second contrast (<xref ref-type="table" rid="T1">Table 1</xref>).</p></sec><sec id="S4"><title>Connectivity results: Masked fc-MVPA</title><p id="P9">So far, we used mass-univariate statistical methods and here, we used multivariate analyses, taking the brain as a whole instead of running as many regressions as the number of existing voxels. Contrasting Voice to Non-voice trials led to enhanced multivariate connectivity in the left pMTG (posterior MTG), right IFGtri (IFG <italic>pars triangularis</italic>, <xref ref-type="fig" rid="F2">Fig.2a</xref>) as well as in the posterior division of the bilateral parahippocampal gyrus (<xref ref-type="fig" rid="F2">Fig.2b</xref>). These results allow for a clearer understanding of whole-brain data presented above, showing how connectivity patterns link lateral temporal, frontal and parahippocampal—medial temporal lobe—regions for voice processing.</p></sec><sec id="S5"><title>Seed-to-seed effective connectivity (generalized Psychophysiological Interactions using bivariate regression)</title><p id="P10">The second step of our connectivity analyses for Study1 was the comparison of seed-to-seed (or ROI-to-ROI) effective—and functional, see <xref ref-type="supplementary-material" rid="SD1">Fig.S1</xref>—connectivity for Voice against Non-voice blocks. We extracted a total of 26 ROIs on the basis of our two whole-brain contrasts of interest (19 for Voice &gt; Non-voice and 7 for Non-voice &gt; Voice; see <xref ref-type="table" rid="T1">Table 1</xref>). The general effective relations between these seed regions can be viewed in <xref ref-type="fig" rid="F2">Fig.2c</xref> for the Voice &gt; Non-voice contrast. Of note is that this contrast includes seed regions from both Voice &gt; Non-voice and Non-voice &gt; Voice whole-brain results (N=26 ROIs, as mentioned above). Coupling involved several connections in the lateral temporal cortex, mainly between the anterior, middle and posterior STG, bilaterally, and between the middle STG and the frontal cortex (IFG, dorsolateral prefrontal cortex: DLPFC), again bilaterally (<xref ref-type="fig" rid="F2">Fig.2c</xref>). Anti-coupling involved a large network of ROIs, including again the IFG, DLPFC and more interestingly the anterior as well as posterior parahippocampal gyrus, the posterior STG and MTG, bilaterally (<xref ref-type="fig" rid="F2">Fig.2c</xref>). ROI-to-ROI effective connectivity therefore highlights a coupled lateral temporal-frontal network while in parallel, an anti-coupled, medial temporal to lateral temporal cortex network was observed. See <xref ref-type="table" rid="T3">Table 3</xref> for connectivity values.</p><p id="P11">Taken together, our effective connectivity results for Study 1 hence make it clear that voice processing not only involves the TVAs or lateral temporal cortex regions, but it also crucially relies on medial temporal connected areas such as the parahippocampal gyrus.</p></sec></sec><sec id="S6"><title>Study 2 &amp; 3: voice processing in noisy situations (behavioral study and neuroimaging)</title><p id="P12">Following the first study and the possibility that noise would be processed by the medial temporal lobe and the posterior MTG <italic>during</italic> voice processing, we raised the following question: are the MTL and pMTG working together to allow clear voice perception through—possibly parallel—noise reduction? Such question led to the necessity of directly manipulating background noise while concurrently perceiving voice signals, which was implemented in Study 2 as a behavioral pilot study and in Study 3 as an fMRI study. The paradigm of Study 3 was hence identical to the one used in Study 2—and included sparse-sampling MRI data acquisition to allow for silent periods while stimuli were presented and therefore improve general task sensitivity. In these studies, the participants had to specify whether the voice was easy or rather difficult to perceive in varying noise levels (<xref ref-type="fig" rid="F3">Fig.3b</xref>). See the methods for more details, <xref ref-type="fig" rid="F3">Fig.3a</xref> below for task design and <xref ref-type="fig" rid="F3">Fig.3b</xref> for conditions as well as stimulus characteristics and manipulation.</p><sec id="S7"><title>Probability of perceiving voice signals in varying levels of noise (behavior for Study 2 &amp; 3)</title><p id="P13">Behavioral results of both studies 2 &amp; 3 illustrate the probability of perceiving a voice in a noisy background and were computed using mixed effects, logistic regression analyses (see <xref ref-type="sec" rid="S12">Methods</xref> for details). The dependent variable was the response of the participant, the fixed effect was the level of noise and random effects included the identity of the participant, its gender and age as well as the identity and order of each stimulus presented. Response probability shows a clear degradation of the perception of voice signals in noise when noise level is High to Highest (voice-to-noise dB ratio=1/8:1 &amp; 1/16:1, respectively) in both independent samples (<xref ref-type="fig" rid="F4">Fig.4a</xref>, left panel: Study 2, N=18; right panel: Study 3, N=20). Individual data values and violin distribution curves for reaction times data are reported in <xref ref-type="supplementary-material" rid="SD1">Fig.S2</xref>.</p><p id="P14">In Study 2, we observed a significant impact of noise levels on the probability of perceiving a voice signal (χ<sup>2</sup>(4)=283.93, <italic>p</italic>&lt;.001). Variance explained by this model was 31.68% for the fixed factor only (R<sup>2</sup>m) and 61.75% when including both fixed and random effects (R<sup>2</sup>c). Using a confidence interval of 97.5% and a Bonferroni correction for multiple comparisons, we computed paired contrasts of interest between each level of noise (<xref ref-type="fig" rid="F4">Fig.4a</xref>, right panel). Results show no difference between Lowest and Low noise (χ<sup>2</sup>(1)=0.06, <italic>p</italic>=.81) and between Low noise and Equal voice and noise levels (χ<sup>2</sup>(1)=0.83, <italic>p</italic>=.36), significant differences between Equal and High noise (χ<sup>2</sup>(1)=47.81, <italic>p</italic>&lt;.001) as well as between High and Highest noise levels (χ<sup>2</sup>(1)=52.12, <italic>p</italic>&lt;.001), interestingly. Using a parametric contrast including all conditions and targeting voice perception in noisy as opposed to less noisy situations—with a stronger difference between Highest vs. High and Lowest vs. Low values—led to a significant effect (χ<sup>2</sup>(1)=208.28, <italic>p</italic>&lt;.001). The vector of this contrast was the following: [2(Highest noise) 1(High noise) 0(Equal) -1(Low noise) -2(Lowest noise)]. Therefore, the probability of reporting the voice was ‘easy’ as opposed to ‘difficult’ to perceive was modulated by the level of background noise, especially when noise was Highest and High compared to when it was Low or Lowest.</p><p id="P15">Concerning reaction times—even though of no particular interest here, we observed a significant effect of noise level on response probability (χ<sup>2</sup>(4)=16.03, <italic>p</italic>&lt;.01), triggered by a difference between the Highest and the High noise conditions (χ<sup>2</sup>(1)=8.45, <italic>p</italic>&lt;.01). No other contrast reached significance (Lowest &gt; Low noise: χ<sup>2</sup>(1)=0.04, <italic>p</italic> =.84; Low noise &gt; Equal: χ<sup>2</sup>(1)=0.85, <italic>p</italic>=.36; Equal &gt; High noise: χ<sup>2</sup>(1)=0.57, <italic>p</italic>=.45). We again used the same parametric contrast as above, including all conditions and targeting voice perception in noisy as opposed to less noisy situations—with a stronger difference between Highest vs. High and Lowest vs. Low values—and it led to a significant effect (χ<sup>2</sup>(1)=8.45, <italic>p</italic>&lt;.01). This result highlights slower reaction times between high noise situations, which seems to indicate higher task difficulty as well. Again, the confidence interval was 97.5% and a Bonferroni correction was used. Variance explained by this model was 0.39% for the fixed factor only (R<sup>2</sup>m) and 6.22% when including both fixed and random effects (R<sup>2</sup>c).</p><p id="P16">In fMRI Study 3, we observed—as in behavioral Study 2—a significant impact of noise levels on the probability of perceiving a voice signal (χ<sup>2</sup>(4)=375.65, <italic>p</italic>&lt;.001). Variance explained by this model was 24.08% for the fixed factor only (R<sup>2</sup>m) and 71.43% when including both fixed and random effects (R<sup>2</sup>c). Using a confidence interval of 97.5% and a Bonferroni correction for multiple comparisons, we computed paired contrasts of interest between each level of noise (<xref ref-type="fig" rid="F4">Fig.4b</xref>, right panel). Results show no difference between Lowest and Low noise (χ<sup>2</sup>(1)=0.57, <italic>p</italic>=.45), a tendency between Low noise and Equal voice and noise levels (χ<sup>2</sup>(1)=3.69, <italic>p</italic>=.055) and significant differences between Equal and High noise (χ<sup>2</sup>(1)=67.44, <italic>p</italic>&lt;.001) as well as between High and Highest noise levels (χ<sup>2</sup>(1)=77.67, <italic>p</italic>&lt;.001). Using the same parametric contrast as in Study 2, we observed again a significant effect (χ<sup>2</sup>(1)=314.01, <italic>p</italic>&lt;.001). Therefore, we can again say here that the probability of reporting the voice was ‘easy’ as opposed to ‘difficult’ to perceive was modulated by the level of background noise, especially when noise was High and Highest compared to when it was Low or Lowest.</p><p id="P17">Concerning reaction times—and even though they were still of no interest in Study 3, we observed a significant effect of noise level on response probability (χ<sup>2</sup>(4)=18.59, <italic>p</italic>&lt;.001), triggered by a difference between an Equal ratio of voice-to-noise and the High noise condition (χ<sup>2</sup>(1)=8.59, <italic>p</italic>&lt;.01). No other contrast reached significance (Lowest &gt; Low noise: χ<sup>2</sup>(1)=0.02, <italic>p</italic>=.89; Low noise &gt; Equal: χ<sup>2</sup>(1)=0.015, <italic>p</italic>=.90; High &gt; Highest noise: χ<sup>2</sup>(1)=0.002, <italic>p</italic>=.97). We again used the same parametric contrast as above, and it led to a significant effect (χ<sup>2</sup>(1)=12.95, <italic>p</italic>&lt;.001). This result highlights slower reaction times for High—but not Highest—noise situations, which again seems to indicate higher task difficulty as well. Again, the confidence interval was 97.5% and a Bonferroni correction was used. Variance explained by this model was 0.42% for the fixed factor only (R<sup>2</sup>m) and 15.24% when including both fixed and random effects (R<sup>2</sup>c).</p></sec><sec id="S8"><title>Whole-brain neuroimaging results (Study 3)</title><p id="P18">Neuroimaging results were obtained using the sample of Study 3, since Study 2 was only a behavioral study. Here, we were specifically interested in using High- as opposed to Low-noise situations. Therefore, and to be consistent with behavioral analyses, we used the same parametric contrast as above. It included all conditions and targeted voice perception in noisy as opposed to less noisy situations—with a stronger difference between Highest vs. High and Lowest vs. Low trials. The vector of this contrast was identical to the one used for behavior (<xref ref-type="fig" rid="F4">Fig.4f</xref>), namely: [2(Highest noise) 1(High noise) 0(Equal) -1(Low noise) -2(Lowest noise)]. Trials in which the participants indicated they easily perceived the voice signals were used, while those ‘difficult’ trials were concatenated into a single column of no-interest, since we wanted to understand ‘successful’ voice processing when noise is present. The data emphasize a strong role of the right posterior MTG (<xref ref-type="fig" rid="F4">Fig.4d</xref>), anterior and posterior bilateral PHG (<xref ref-type="fig" rid="F4">Fig.4c,e,g</xref>), right posterior, mid and anterior STS (<xref ref-type="fig" rid="F4">Fig.4d</xref>) and bilateral insula (<xref ref-type="fig" rid="F4">Fig.4b,d</xref>). All above-threshold clusters are reported in <xref ref-type="table" rid="T4">Table 4</xref>. The inverse contrast yielded to vast prefrontal, orbitofrontal and inferior activity, but posterior MTG or any of the PHGs did not show any above-threshold enhanced activity (<xref ref-type="supplementary-material" rid="SD1">Fig.S3</xref>). Among the regions of the main contrast of interest, it is important to highlight that many of them overlap with both Voice-specific and Non-voice-specific brain areas observed in Study 1 (see <xref ref-type="supplementary-material" rid="SD1">Fig.S4</xref> and <xref ref-type="supplementary-material" rid="SD1">Fig.S5</xref>, respectively), therefore adding more weight to the assumption of their parallel role for processing voice in noise. The F contrast including all trials (‘easy’ and ‘difficult’ voice perception as assessed by our participants’ responses) is reported in <xref ref-type="supplementary-material" rid="SD1">Fig.S6</xref>, while the F contrast of solely the ‘easy’ perception trials is reported in <xref ref-type="supplementary-material" rid="SD1">Fig.S7</xref>.</p></sec><sec id="S9"><title>Connectivity results (Study 3): Masked fc-MVPA and seed-to-voxel functional and effective connectivity (generalized Psychophysiological Interactions)</title><p id="P19">In order to grasp the full picture of voice processing in noisy situations, we then turned to undirected—this section, <xref ref-type="fig" rid="F5">Fig.5</xref>—and directed connectivity analyses—dynamic causal modeling, next section (<xref ref-type="fig" rid="F6">Fig.6</xref>) using the same integrative contrast as before ([2(Highest noise) 1(High noise) 0(Equal) -1(Low noise) - 2(Lowest noise)]). We started by computing connectivity MVPA and this multivariate method yielded to a single region, namely the right posterior MTG (<xref ref-type="fig" rid="F5">Fig.5a</xref>). This region of course greatly overlaps with the one observed in whole-brain results of this study (<xref ref-type="fig" rid="F4">Fig.4d</xref>). Taking this right pMTG as seed for further seed-to-voxel effective connectivity analyses using bivariate regressions, results show a direct negative relation with the left PHG, more specifically its posterior division (<xref ref-type="fig" rid="F5">Fig.5b</xref>). To close the circle, we then used this right posterior PHG as seed. We also included the left pPHG observed in Study 1 (specific to Non-voice) to better disentangle a potential dissociation between hemispheres in the PHG. Both functional—using bivariate correlation—and effective—using bivariate regression—connectivity analyses were performed here. The left pPHG showed functional anti-coupling back with the right pMTG (<xref ref-type="fig" rid="F5">Fig.5c</xref>) while the right pPHG showed effective, direct anti-coupling with the right planum temporale (<xref ref-type="fig" rid="F5">Fig.5d</xref>). Taken together with the results of Study 1 as well as those mentioned above of Study 2 and of this study, these results give even more strength to the concept of a medial temporal network working in concert with the TVA and voice-processing areas for an improved voice perception in noisy situations.</p></sec><sec id="S10"><title>Dynamic causal modelling, fixed effects (Study 3)</title><p id="P20">We computed one last analysis for this study, with the aim of constraining our data even more to understand how the relations between the lateral and medial temporal cortex would constitute a biological mechanism recruited for voice perception in noise. To do so, we carefully designed nine credible models—in addition to one for defining intrinsic connectivity—decomposed into three families based on their given input (see <xref ref-type="supplementary-material" rid="SD1">Fig.S8</xref>). The winning family—involving High and Highest noise conditions as input to the right pMTG and bilateral pPHG—contained the winning model with condition input to the right pMTG, the bilateral pPHG as well as input to the directed effective relation from the right pMTG to the bilateral pPHG (<xref ref-type="fig" rid="F6">Fig.6a,b</xref>). Because we were interested in within-participant rather than between-participant differences, we computed a fixed-effect analysis type. For deciding the winning family, the Bayesian model averaging posterior probability was above pp&gt;.99, same goes for Bayesian model selection allowing model 8 to be the selected as the best model (pp&gt;.99, see <xref ref-type="fig" rid="F6">Fig.6b</xref> and <xref ref-type="supplementary-material" rid="SD1">Fig.S8</xref> for more details). The winning model therefore had inputs of High and Highest noise conditions on all three regions of interest as well as on the positive directed link from the right pMTG to the right pPHG and on the negative directed link from the right pMTG to the left pPHG (<xref ref-type="fig" rid="F6">Fig.6a</xref>). Finally, it is important to note that in this analysis, the ROIs were the actual clusters found in the previous results sections mentioned above, not a sphere or a square average based on coordinates (see <xref ref-type="sec" rid="S12">Methods</xref>).</p></sec></sec></sec><sec id="S11" sec-type="discussion"><title>Discussion</title><p id="P21">The present studies aimed at a better understanding of voice processing at the neural level with the general goal of shedding new light on auditory and language pathways used by humans in social interactions and in everyday life communication. We used a combination of tasks involving the processing of voice and noise3 separately (Study 1)—as well as voice <italic>in</italic> noise (Study 2 &amp; 3), by modulating the background environment of auditory scenes. In Study 1, we observed two distinct neural networks underlying the processing of vocal and non-vocal auditory material—the anterior, mid, posterior superior temporal cortex and the IFG for voice processing; the posterior STC and the bilateral parahippocampal gyrus for the processing of non-vocal stimuli. The latter represents a novel anti-coupled subnetwork, originating from the posterior part of the parahippocampal gyrus (bilaterally) to the most posterior part of the bilateral temporal lobe (middle and superior portions). Are these brain networks working hand in hand to allow clear voice perception in noise? Is this latter network playing any role in the boosting of voice processing in noisy situations? Is it responsible for noise reduction?</p><p id="P22">To clarify these aspects—that could not be addressed by the design of Study 1, a second task was programmed and performed on independent samples both behaviorally and in an fMRI scanner (Study 2 &amp; 3, respectively; sparse-sampling fMRI in Study 3), involving voice perception in varying levels of realistic background noise. The results revealed expected behavioral effects—the more background noise, the more difficult it becomes for the participants to distinguish vocal signals, in addition to neural effects greatly overlapping with those observed in Study 1: voice perception in high levels of environmental noise triggered enhanced brain activity in the posterior temporal lobe as well as in the bilateral parahippocampal gyrus—named throughout this section lateral (TC<sub>lat</sub>) and medial (TC<sub>med</sub>) temporal cortex, respectively. The commonalities between our fMRI studies led us to the articulation of our findings as a specific model of voice processing in noisy situations, which is summarized in <xref ref-type="fig" rid="F7">Fig.7</xref> below. We will iterate this model throughout this section and discuss in further detail the implications of our results for social neuroscience and mention future perspectives to consider.</p><p id="P23">Among the evolutionary changes of modern humans, verbal communication was—and still is—of the highest importance. The vector of human verbal communication being the vocal signal, the study of the human voice appears therefore to be a crucial research topic and this argument represents the main motivation behind the present body of work. In addition to vocal verbal communication, suprasegmental aspects (i.e., pitch, voice quality, intonation) of the human voice contain information not only about the emotional state of the speaker<sup><xref ref-type="bibr" rid="R34">34</xref>,<xref ref-type="bibr" rid="R35">35</xref></sup>, but also about its identity<sup><xref ref-type="bibr" rid="R36">36</xref></sup>, age and gender<sup><xref ref-type="bibr" rid="R37">37</xref></sup>, and even personality<sup><xref ref-type="bibr" rid="R38">38</xref>,<xref ref-type="bibr" rid="R39">39</xref></sup>. In parallel to these highly informative behavioral aspects, neuroimaging highlighted the role of the temporal lobe (i.e., TVAs)<sup><xref ref-type="bibr" rid="R3">3</xref></sup> in voice processing and our results are in line with these <italic>princeps</italic> findings<sup><xref ref-type="bibr" rid="R34">34</xref></sup>. More specifically, three sub-clusters of the TVAs were observed in our data, namely the posterior, mid and anterior regions, and these TVAs subparts were positively correlated (<xref ref-type="fig" rid="F2">Fig.2c</xref>), a result that was already reported previously<sup><xref ref-type="bibr" rid="R4">4</xref></sup>. However, our data revealed that the most posterior part of the STC correlated with the anterior and mid parts of the STC in a weaker manner, especially the left posterior STG. Since the posterior auditory cortex or posterior STC was shown to gate novel sounds to consciousness<sup><xref ref-type="bibr" rid="R40">40</xref></sup>, to underlie the recognition of environmental sounds<sup><xref ref-type="bibr" rid="R19">19</xref></sup> and to process voice as part of the TVAs, why would this region be less connected to the rest of the temporal lobe during voice processing? More importantly, the bilateral posterior STC—including the MTG—was also part of an anti-coupled network together with the parahippocampal gyrus, bilaterally. The parahippocampal gyrus was reported to mediate the indirect processing of vocal threat<sup><xref ref-type="bibr" rid="R21">21</xref></sup>, as well as the perception and identification of environmental sounds<sup><xref ref-type="bibr" rid="R41">41</xref></sup>, but its role in general voice perception has, to the best of our knowledge, neither been revealed nor explored. Therefore, we must understand the role of the parahippocampal gyrus for voice perception—since we know its involvement in noise perception, most crucially in relation to the posterior temporal cortex. A specific investigation of voice perception in noise therefore had to be assessed—since we think noise could be reduced or even filtered out thanks to this anti-coupled TC<sub>lat</sub>-TC<sub>med</sub> network, and such aspect was not manipulated at all in Study 1.</p><p id="P24">The assumption that a network of regions including the parahippocampal gyrus (TC<sub>med</sub>) and the posterior STC and MTG (TC<sub>lat</sub>) might play a role in voice processing in noise should therefore yield to enhanced activity and/or connectivity when voices are processed and well perceived in high vs low noise situations. This is exactly what we observed in Study 3 both for wholebrain and connectivity analyses (functional, effective—both for directed and undirected measures). Our results together with existing literature on the perception of voice<sup><xref ref-type="bibr" rid="R3">3</xref>,<xref ref-type="bibr" rid="R4">4</xref>,<xref ref-type="bibr" rid="R9">9</xref>,<xref ref-type="bibr" rid="R14">14</xref>–<xref ref-type="bibr" rid="R16">16</xref>,<xref ref-type="bibr" rid="R20">20</xref>,<xref ref-type="bibr" rid="R21">21</xref>,<xref ref-type="bibr" rid="R27">27</xref>,<xref ref-type="bibr" rid="R36">36</xref></sup>, noise<sup><xref ref-type="bibr" rid="R17">17</xref>,<xref ref-type="bibr" rid="R19">19</xref>,<xref ref-type="bibr" rid="R23">23</xref>,<xref ref-type="bibr" rid="R25">25</xref>,<xref ref-type="bibr" rid="R26">26</xref>,<xref ref-type="bibr" rid="R41">41</xref></sup> and voice-in-noise<sup><xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R33">33</xref>,<xref ref-type="bibr" rid="R42">42</xref>–<xref ref-type="bibr" rid="R45">45</xref></sup> processing led us to drafting a three-step neural model of voice processing in noise (<xref ref-type="fig" rid="F7">Fig.7</xref>). As mentioned above, we refer to the bilateral TVAs and especially the STC and MTG as the ‘lateral temporal cortex’ (TC<sub>lat</sub>) while we regroup the subregions of the parahippocampal gyrus as the ‘medial temporal cortex’ (TC<sub>med</sub>). Inferior frontal cortex subregions <italic>(pars orbitalis</italic>, <italic>opercularis</italic> and <italic>triangularis</italic>) are also regrouped under the general term ‘IFC’. Three steps would unfold to allow for efficient and accurate voice processing in noise and potentially for any type of social interactions involving speech. These steps include: i) the processing of the auditory scene, involving ambient sounds, noises and vocal signals and triggering enhanced brain activations and coupled connectivity in the primary auditory cortex as well as in more secondary areas such as the bilateral TC<sub>lat</sub> (<xref ref-type="fig" rid="F7">Fig.7a</xref>); ii) to focus more specifically on voice signals and therefore clarify communication, the TC<sub>lat</sub> regions would still be active together with a potentially parallel, anti-coupled network including the posterior TC<sub>lat</sub> as well as the TC<sub>med</sub> that would reduce the impact of ambient noise (<xref ref-type="fig" rid="F7">Fig.7b</xref>); iii) finally, ‘noise-free’ or ‘noise-reduced’ voice processing would allow for enhanced activity and coupled connectivity between the TC<sub>lat</sub>—especially the mid and anterior subparts—and the IFC (<xref ref-type="fig" rid="F7">Fig.7c</xref>), especially the <italic>pars triangularis</italic> to allow for a cognitive evaluation of the voice as well as a decision to be made about this vocal signal—depending on context and task demands. The first step of this model is supported by many previous studies by us<sup><xref ref-type="bibr" rid="R8">8</xref>,<xref ref-type="bibr" rid="R11">11</xref>,<xref ref-type="bibr" rid="R12">12</xref>,<xref ref-type="bibr" rid="R14">14</xref>,<xref ref-type="bibr" rid="R16">16</xref>,<xref ref-type="bibr" rid="R46">46</xref></sup> and others<sup><xref ref-type="bibr" rid="R3">3</xref>,<xref ref-type="bibr" rid="R4">4</xref>,<xref ref-type="bibr" rid="R6">6</xref>,<xref ref-type="bibr" rid="R13">13</xref>,<xref ref-type="bibr" rid="R21">21</xref>,<xref ref-type="bibr" rid="R27">27</xref>,<xref ref-type="bibr" rid="R32">32</xref>,<xref ref-type="bibr" rid="R35">35</xref>,<xref ref-type="bibr" rid="R36">36</xref></sup>, and the same is true for the third step of this model in which the IFC is recruited for decisional processes of auditory vocal signals<sup><xref ref-type="bibr" rid="R14">14</xref>,<xref ref-type="bibr" rid="R15">15</xref>,<xref ref-type="bibr" rid="R46">46</xref>–<xref ref-type="bibr" rid="R48">48</xref></sup>. The second step however still represents a grey area, and the present work aimed at clarifying the role of TC<sub>lat</sub> and TC<sub>med</sub>—and especially of the bilateral posterior parahippocampal gyrus—in favoring voice processing in noise. These medial regions are indeed at the crossroads of vocal and non-vocal sound processing, with evidence for their distinct involvement in noise<sup><xref ref-type="bibr" rid="R17">17</xref>,<xref ref-type="bibr" rid="R19">19</xref>,<xref ref-type="bibr" rid="R26">26</xref>,<xref ref-type="bibr" rid="R40">40</xref>,<xref ref-type="bibr" rid="R41">41</xref></sup> as well as in voice or music processing, for instance when emotional content is included<sup><xref ref-type="bibr" rid="R21">21</xref></sup>. So far, existing literature of voice or speech perception in noise revealed the role of the Medial Olivocochlear Efferent System<sup><xref ref-type="bibr" rid="R2">2</xref></sup>, brainstem<sup><xref ref-type="bibr" rid="R43">43</xref>,<xref ref-type="bibr" rid="R45">45</xref></sup>, and primary and secondary auditory cortices, which were also reported as core brain regions for voice-in-noise neural development from childhood to adulthood<sup><xref ref-type="bibr" rid="R18">18</xref></sup>. A recent study pointed to the role of a limbic-cortical network as a basis for separating emotional voices from noise in an auditory scene, interestingly involving subparts of the medial temporal cortex, namely the amygdala<sup><xref ref-type="bibr" rid="R33">33</xref></sup>. While emotional voices might represent a special case of voice stimuli, these results are still in line with what we observed here and also add more weight to the second step of our model of voice processing in noise. The amygdala and the parahippocampal gyrus—two regions that are next to each other anatomically—could therefore share a role in helping voice perception in noise for emotional and non-emotional content, respectively. This assumption could reinforce the TC<sub>med</sub>-TC<sub>lat</sub> network hypothesis for noise reduction, but it should be tested scientifically in a dedicated study by using a mix of emotional and non-emotional voice stimuli. Noteworthy is the fact that no study to date highlighted the specific role of the parahippocampal gyrus in voice perception in noise although its involvement in environmental noise processing was previously reported<sup><xref ref-type="bibr" rid="R49">49</xref></sup>. Interestingly, the parahippocampal gyrus was also repeatedly reported in tinnitus<sup><xref ref-type="bibr" rid="R50">50</xref>–<xref ref-type="bibr" rid="R53">53</xref></sup>, a condition in which patients hear ringing, roaring or buzzing sounds, that are allegedly extremely close to what we would categorize as ‘noise’. The parahippocampal gyrus might therefore play a crucial— and causal—role in the perception, processing and reduction of auditory noise.</p><p id="P25">At least one open question remains concerning our model and the way it illustrates the results, especially about the mentioned brain networks in step 2 and 3: are these brain networks sequential or parallel? In fact, one could assume that these processes are dealt with in parallel by the brain, since it seems to be a way of functioning especially true for Humans compared to other animals<sup><xref ref-type="bibr" rid="R54">54</xref></sup>. Our results cannot give a clear answer to this point and while no relevant extrapolation can be made here, we can only say that our results do not exclude either sequential or parallel processing and neural functioning. Future studies should therefore focus on the brain dynamics of voice perception in noise to address this aspect as reported for emotional prosody processing in the past<sup><xref ref-type="bibr" rid="R34">34</xref></sup>, by the use of electroencephalography or magnetoencephalography for instance. The time-course of this process may also help characterize specific biological markers of the impact of noise exposition on health<sup><xref ref-type="bibr" rid="R23">23</xref>–<xref ref-type="bibr" rid="R25">25</xref></sup>, and this aspect goes beyond communication and is of course of the highest importance for health care.</p><p id="P26">Several methodological and conceptual limitations should be considered to put our results into perspective and point out what remains to be investigated in the future. First, sample size of Study 3 could have been improved—even though our simulation power calculations gave us optimism concerning effect size in our regions of interest compared to a sample of N=35 or even N=50. Second, ecological validity was not optimal since laboratory studies do not represent real-world experience, especially for concrete topics such as voice-in-noise perception. Other brain networks or regions could therefore be responsible for processing voice signals in noise but their implication could have been attenuated or even annihilated in the laboratory. Third, a direct characterization of the specific acoustical parameters potentially impacting voice perception in noise should be performed, especially using the Mel-Frequency Cepstral Coefficients (MFCC) as a measure of acoustic stimulus organization (‘acoustic entropy’). Fourth, even though we designed Study 3 with task-triggered sparse-sampling fMRI to allow for silent periods during stimulus presentation, we cannot exclude that scanner noise still impacted perception in a differed manner. Fifth, the low temporal resolution of fMRI—in our case 1 second—prevents any reliable testing of sequential or parallel functioning of the observed brain networks. Finally, 3T fMRI does not allow for any assumption or observation of layer-level mechanisms potentially adding up to our results of voice processing in noisy situations. This is especially true for the auditory cortex and/or the parahippocampal gyrus. Future studies should therefore investigate layer-level wholebrain or connectivity data in these regions of interest using for instance high field fMRI at 7T. By doing so, the resulting granularity of research data in this topic would be improved, yielding to more representative brain mechanisms of realistic voice perception.</p><p id="P27">In conclusion, vocal communication requires humans to filter out—at least partially—sources of noise that would otherwise alter perception and potentially negatively impact social interactions and ultimately communication. We directly manipulated background noise in auditory scenes in which voice signals were presented. High compared to low noise situations altered voice perception and causally relied on a network of regions including the lateral and medial temporal cortex (TC<sub>lat</sub>, TC<sub>med</sub>). Based on our data, we propose a three-step model of voice perception in noise—(1) auditory scene processing, (2) noise reduction, (3) final voice processing and decision. Further research should address the potential parallel functioning of these brain networks and assess whether other regions may be involved in order to draw a clearer picture of functional brain organization related to everyday life communication, especially in the case of voice processing in noisy situations.</p></sec><sec id="S12" sec-type="methods"><title>Methods</title><sec id="S13"><title>Study 1</title><sec id="S14" sec-type="subjects"><title>Participants</title><p id="P28">Ninety-eight right-handed, healthy, either native or highly proficient French-speaking participants (45 male, 53 female, mean age 24.82 years, SD 5.45) were included in this functional magnetic resonance study. All participants were naive to the experimental design and study, had normal or corrected-to-normal vision, normal hearing and no history of psychiatric or neurologic incidents. Participants gave written informed consent for their participation in accordance with ethical and data security guidelines of the University of Geneva. The study was approved by the Ethical Committee of the University of Geneva and was conducted according to the Declaration of Helsinki.</p><p id="P29">For this study, we did not determine sample size <italic>a priori</italic> since we had a rather large sample already at hand. In the princeps study by Belin and colleagues, reliable and replicated<sup><xref ref-type="bibr" rid="R11">11</xref>,<xref ref-type="bibr" rid="R14">14</xref>,<xref ref-type="bibr" rid="R16">16</xref>,<xref ref-type="bibr" rid="R27">27</xref></sup> results were observed with only fourteen participants. We have here in Study 1 about seven times more participants, and this sample size and results are therefore allowing for a very low type I error.</p></sec><sec id="S15"><title>Stimuli</title><p id="P30">Auditory stimuli consisted of sounds from a variety of sources. Vocal stimuli were obtained from 47 speakers: 7 babies, 12 adults, 23 children and 5 older adults. Stimuli included 20 blocks of vocal sounds and 20 blocks of non-vocal sounds. Vocal stimuli within a block could be either speech (33%: words, non-words, foreign language) or non-speech (67%: laughs, sighs, various onomatopoeia). Non-vocal stimuli consisted of natural sounds (14%: wind, streams), animals (29%: cries, gallops), the human environment (37%: cars, telephones, airplanes) or musical instruments (20%: bells, harp, instrumental orchestra). The paradigm, design and stimuli were obtained through the Voice Neurocognition Laboratory website (<ext-link ext-link-type="uri" xlink:href="http://vnl.psy.gla.ac.uk/resources.php">http://vnl.psy.gla.ac.uk/resources.php</ext-link>). Stimuli were presented through headphones (model S14, Sensimetrics Corporation, Gloucester, MA, USA) at an intensity that was kept constant throughout the experiment (70 dB sound-pressure level). Participants were instructed to actively listen to the sounds. The silent interblock interval was 8 s long.</p></sec><sec id="S16"><title>Image acquisition</title><p id="P31">Structural and functional brain imaging data were acquired by using a 3T scanner (Siemens Trio, Erlangen, Germany) with a 32-channel coil. A magnetization-prepared rapid acquisition gradient echo sequence was used to acquire high-resolution (1 × 1 × 1 mm3) T1-weighted structural images (TR = 1,900 ms, TE = 2.27 ms, TI = 900 ms). Functional images were acquired by using a multislice echo planar imaging sequence (36 transversal slices in descending order, slice thickness 3.2 mm, TR = 2,100 ms, TE = 30 ms, field of view = 205 × 205 mm2, 64 × 64 matrix, flip angle = 90°, bandwidth 1562 Hz/Px).</p></sec><sec id="S17"><title>Image analysis</title><sec id="S18"><title>Whole-brain analyses</title><p id="P32">Functional images were analyzed with Statistical Parametric Mapping software (SPM12, Wellcome Trust Centre for Neuroimaging, London, UK). Preprocessing steps included realignment to the first volume of the time series, slice timing, normalization into the Montreal Neurological Institute<sup><xref ref-type="bibr" rid="R55">55</xref></sup> (MNI) space using the DARTEL toolbox<sup><xref ref-type="bibr" rid="R56">56</xref></sup> and spatial smoothing with an isotropic Gaussian filter of 8 mm full width at half maximum. To remove low-frequency components, we used a high-pass filter with a cutoff frequency of 128 s. Anatomical locations were defined with a standardized coordinate database (Talairach Client, <ext-link ext-link-type="uri" xlink:href="http://www.talairach.org/client.html">http://www.talairach.org/client.html</ext-link>) by transforming MNI coordinates to match the Talairach space and transforming it back into MNI for display purposes.</p><p id="P33">A general linear model was used to compute first-level statistics, in which each block was modeled by using a block function and was convolved with the hemodynamic response function, time-locked to the onset of each block. Separate regressors were created for each condition (vocal and non-vocal; “condition” factor). Finally, six motion parameters were included as regressors of no interest to account for movement in the data. The condition regressors were used to compute simple contrasts for each participant, leading to a main effect of vocal and non-vocal at the first-level of analysis ([1 0] for vocal, [0 1] for non-vocal). These simple contrasts were then taken to a flexible factorial second-level analysis in which there were two factors: the “participants” factor (independence set to “yes”, variance set to unequal) and the “condition” factor (independence set to “no”, variance set to unequal). All neuroimaging activations were thresholded in SPM12 by using a voxel-wise family-wise error (FWE) correction at p&lt;0.05. Whole-brain, second-level analysis results were used to extract and delineate 26 ROIs from the main contrasts (Voice &gt; Non-voice: 19 ROIs; non-vocal&gt;vocal: 7 ROIs) as separate masks (higher accuracy) rather than as spheres around the peak coordinates.</p></sec></sec><sec id="S19"><title>Connectivity analyses</title><sec id="S20"><title>Seed-to-voxel and seed-to-seed functional and effective connectivity</title><p id="P34">Functional and effective connectivity analyses were computed by using CONN<sup><xref ref-type="bibr" rid="R30">30</xref></sup> (version 22.a) implemented in Matlab 9.0 (The MathWorks, Inc., Natick, MA, USA). Functional and anatomical data were first imported with the SPM import option (automatically loading data from the ‘SPM.mat’ file created in the whole-brain first-level analyses). Following this step, the 26 ROI were entered in the model and seed-to-voxel and seed-to-seed analyses were enabled by using parametric statistics. The following step was denoising, during which data are by default band-pass filtered and detrended and finally artifact corrected (removal of white matter, cerebrospinal fluid, motion, outlier regressors). The last analysis steps are first-level (with SPM regressors, i.e., movement parameters, entered as covariates of no interest) and second-level statistical analyses during which single-subject and group-level analyses are computed, respectively. For each participant and for each condition (vocal; non-vocal), the correlational measures were bivariate and one coefficient was calculated for each seed and target ROI (seed-to-seed analyses) and for each seed and voxels (seed-to-voxel analyses). All analysis steps described were computed by using an explicit mask of the brain to remove any non-brain tissue and the analysis space was kept from the input files on SPM.</p><p id="P35">Seed-to-voxel analyses were computed with the aim of understanding functional connectivity originating from our ROIs and expanding by increased/reduced functional connectivity with voxels in the whole brain for our contrast of interest (Voice &gt; Non-voice). For seed-to-voxel analysis, all activations are reported at a voxel-wise false-discovery rate (FDR) corrected threshold of p&lt;.01, two-tailed.</p><p id="P36">On the other hand, seed-to-seed analyses were computed in order to characterize the direct links (increased/reduced functional connectivity) between our extracted ROIs for the contrast of interest (Voice &gt; Non-voice). Results for seed-to-seed analyses are reported at an analysis-level FDR correction for multiple comparisons with a threshold of p&lt;.01, two-tailed.</p><p id="P37">Finally, seed-to-voxel analyses were computed by using atlas-based labeling implemented in CONN to accurately localize certain subregions of our ROIs, notably to identify which part(s) of the parahippocampal gyri were involved in a negatively connected network, as found in both seed-to-voxel and seed-to-seed analyses mentioned earlier for the Voice &gt; Non-voice contrast. This analysis was computed for several regions of the MTL, notably the amygdalae, hippocampi and parahippocampal gyri. For these seed-to-voxel atlas-based analyses, results are reported at FWE voxel-level correction of p&lt;.01, two-tailed.</p></sec></sec></sec><sec id="S21"><title>Study 2 &amp; 3</title><sec id="S22" sec-type="subjects"><title>Participants</title><p id="P38">Eighteen (Study 2: 9 male, 9 female, mean age 21.17 years, SD 4.30) and twenty (Study 3: 11 male, 9 female, mean age 22.90 years, SD 3.64) right-handed, healthy, either native or highly proficient French-speaking participants were included in the analyses. The samples of the two studies were completely independent. All participants were naive to the experimental design and study, had normal or corrected-to-normal vision, normal hearing and no history of psychiatric or neurologic incidents. Participants gave written informed consent for their participation in accordance with ethical and data security guidelines of the University of Geneva. The study was approved by the Ethical Committee of the University of Geneva and was conducted according to the Declaration of Helsinki.</p><p id="P39">Concerning power of Study 2 &amp; 3, we aimed at 80% of power. Using G*Power<sup><xref ref-type="bibr" rid="R57">57</xref></sup> software (version 3.1.9.7), we therefore determined that for a medium effect size of 0.5, an alpha of .05 for a two-tailed test, we needed twenty participants to obtain such power. Two participants had to be excluded from Study 2 due to corrupted logfiles while the whole sample was acquired for Study 3. Therefore, after taking this subtraction into account we finally obtained a post-hoc power of 74% instead of 80% for Study 2.</p><p id="P40">For Study 3, we additionally used the ‘neuRosim’ R package<sup><xref ref-type="bibr" rid="R58">58</xref></sup> to simulate nifti format fMRI data according to the specific design of the task—including: the timing of each trial (event-related), the trial sequence (onsets, durations) for each condition in a pseudo-randomized order (no more than three times the same condition in a row). This procedure allowed us to precisely assess fMRI power with N=20. MRI sequence properties were also specified: repetition timing (1 sec), number of scans per session (N=444), field of view, final smoothing of the data (8mm), the specified ROIs (N=10)—including the bilateral posterior STG, MTG, amygdala and parahippocampal gyrus (PHG) with iteratively varying (±3mm) peak coordinates around a given peak voxel—‘runif’ function. Region-specific effect size was also included—ranging from 0.25 (amygdala) to 0.75 (STG/MTG and PHG)—with alpha=.05, power=95%. Noise mixture of type ‘rician’ was also added to better simulate the data. Simulations were computed using non-parametric data generation using 10’000 iterations for a total of 35 simulated participants, exported in 3D nifti format (‘oro.nifti’ R package<sup><xref ref-type="bibr" rid="R59">59</xref></sup>) and then preprocessed and analyzed in SPM12 with one regressor per condition—conditions included: Highest noise, High noise, Equal, Low noise, Lowest noise—in addition to six movement parameters (Design matrix column number: five conditions + six movement parameters = 11 columns). The results show very good ROI activations at the highest statistical thresholding usually used in publications (p &lt; .05 Family-Wise Error corrected for multiple comparisons) for N=35, N=30 and N=20 (<xref ref-type="fig" rid="F8">Fig.8c, 8b and 8a</xref>, respectively) when contrasting factor modalities at the lowest hierarchical level of the factorial architecture, such as in the comparison between High and Low noise conditions. Simulation methods are state-of-the-art and therefore make for a strong statistical basis for this last study. Twenty participants were therefore included in Study 3, representing the maximum number we could recruit due to financial limitations at this point while still maintaining strong statistical power, as shown by this conservative simulation procedure.</p></sec><sec id="S23"><title>Stimuli</title><p id="P41">Auditory stimuli consisted of a subset of those used in Study 1, namely stimuli from the Voice Neurocognition Laboratory website (<ext-link ext-link-type="uri" xlink:href="http://vnl.psy.gla.ac.uk/resources.php">http://vnl.psy.gla.ac.uk/resources.php</ext-link>). Vocal stimuli (47 speakers: 7 babies, 12 adults, 23 children and 5 older adults) were selected to include only those with vocal cords usage, excluding coughs, sighs and throat clearing sounds. Vocal stimuli could therefore be either speech (60%: words, non-words, foreign language) or non-speech (40%: laughs, various onomatopoeia). Nonvocal stimuli consisted of natural sounds (30%: wind, streams) and the human environment (70%: cars, telephones, airplanes). To effectively manipulate the level of background noise we mixed the stimuli by pairs—i.e., each vocal stimulus with each non-vocal stimulus—using Matlab (The Mathworks Inc., Natick, MA, USA) at various ratios of voice-vs-noise intensity (VNI; in dB) creating five conditions (N<sub>trials</sub>=45 each): Highest noise (VNI: 1/16:1), High noise (VNI: 1/8:1), Equal (VNI: 1:1), Low noise (VNI: 1:1/8), Lowest noise (VNI: 1:1/16). Mixed stimuli were saved individually offline and then presented during the task in pseudo-random order (see <italic>Procedure</italic> below) at an intensity that was kept constant throughout the experiment (70 dB sound-pressure level; no statistical difference concerning sound energy between the five conditions, all Ps&gt;.5, <xref ref-type="fig" rid="F3">Fig.3b</xref>). Average stimulus duration was 1.5s (±0.5s). Five different lists of mixed stimuli were created, allowing for four rotations within our sample of participants for both studies. Every aspect related to the stimuli that we described in this section is identical for Study 2 &amp; 3. Details about the stimuli are illustrated in <xref ref-type="fig" rid="F3">Fig.3b</xref>, including an example of a stimulus across the five different conditions including waveform, spectrogram, intensity and fundamental frequency plots.</p></sec><sec id="S24" sec-type="methods"><title>Procedure</title><p id="P42">The procedure for Study 2 &amp; 3 was identical (See <xref ref-type="fig" rid="F3">Fig.3</xref>). Participants were presented with a first instruction screen, synthesizing the experimenter’s instructions given beforehand, namely they were instructed to “Listen carefully to each sound and evaluate how easy or difficult it is to clearly perceive the voice (a voice is present in each trial)”. Each trial included a first light grey screen with a black fixation cross during which the stimulus was presented through headphones (Study 2: Sennheiser HD-25 II, Sennheiser electronic SE &amp; Co. KG, Germany; Study 3: model S14, Sensimetrics Corporation, Gloucester, MA, USA) for an average duration of 1.5s (±0.5s), followed by a second light grey screen (duration of 2s) with the fixation cross turning white to indicate to the participant that she/he had to respond using either the ‘1’ (easy to perceive the voice signal) or ‘2’ (difficult to perceive the voice signal) button on the response box (fORP 8 button bimanual curved, Cortech Solutions, Inc., Wilmington, NC, USA ; button mapping counter-balanced between participants). A blank screen of average duration 500msec (±200msec) ended the trial sequence. During stimulus presentation, the MRI scanner was silent to allow for clear perception of the sounds, while brain volumes were acquired during the response screen (see the section on MRI data analysis below for more details about this aspect). The task included a total of 225 trials for a total task duration of about 15 minutes. Details about the procedure are illustrated in <xref ref-type="fig" rid="F3">Fig.3a</xref>.</p></sec><sec id="S25"><title>Behavioral data analysis</title><p id="P43">As mentioned in the Results section, behavioral results of both studies 2 &amp; 3 illustrate the probability of perceiving a voice in a noisy background and were computed using mixed effects, logistic regression analyses for the response (of-interest) and for the associated reaction times (of-no-interest) in R studio<sup><xref ref-type="bibr" rid="R60">60</xref></sup>.</p><p id="P44">For the ‘response’ dependent variable, we used a logistic regression using the lme4 ‘glmer’ package<sup><xref ref-type="bibr" rid="R61">61</xref></sup> with ‘conditions’ as fixed effect and participant identity (ID), gender and age as random factors, in addition to stimulus identity (ID) and order. The model formula was the following:
<disp-formula id="FD1"><mml:math id="M1"><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mo>&lt;</mml:mo><mml:mo>−</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>∣</mml:mo><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>∣</mml:mo><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/><mml:mi>G</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="8em"/><mml:mrow><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>∣</mml:mo><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/><mml:mi>A</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>∣</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.2em"/><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>∣</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.2em"/><mml:mi>O</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p id="P45">For the ‘reaction times’ dependent variable (RTs), we used a linear regression using the lme4 ‘lmer’ package<sup><xref ref-type="bibr" rid="R1">1</xref></sup> with ‘conditions’ as fixed effect and participant identity (ID), gender and age as random factors, in addition to stimulus identity (ID) and order. The model formula was the following:
<disp-formula id="FD2"><mml:math id="M2"><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>s</mml:mi><mml:mo>&lt;</mml:mo><mml:mo>−</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>∣</mml:mo><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>∣</mml:mo><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/><mml:mi>G</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="10em"/><mml:mrow><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>∣</mml:mo><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/><mml:mi>A</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>∣</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.2em"/><mml:mi>I</mml:mi><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>∣</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.2em"/><mml:mi>O</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p id="P46">For our planned comparisons, we used a confidence interval of 97.5% and a Bonferroni correction for multiple comparisons. Specific contrasts were computed using the ‘phia’ package<sup><xref ref-type="bibr" rid="R62">62</xref></sup>. The vector of the main contrast of interest for both dependent variables was the following: [2(Highest noise) 1(High noise) 0(Equal) -1(Low noise) -2(Lowest noise)]. The analyses were identical for Study 2 &amp; 3.</p></sec><sec id="S26"><title>Image acquisition</title><p id="P47">Structural and functional brain imaging data were acquired by using a 3T scanner (Siemens Trio, Erlangen, Germany) with a 32-channel coil. A magnetization-prepared rapid acquisition gradient echo sequence was used to acquire high-resolution (1 × 1 × 1mm3) T1-weighted structural images (TR = 1,900 ms, TE = 2.27 ms, TI = 900 ms). Functional images were acquired using two consecutive 1s volumes post stimulus presentation (during the response screen) in a sparse manner by using a multislice echo planar imaging sequence (56 transversal slices in descending order with 2.5mm isotropic voxels, TR = 1000 ms, TE = 30 ms, field of view = 205 × 205 mm2, 84 × 84 matrix, flip angle = 64°). A total of 444 volumes were acquired for each participant of Study 3.</p></sec><sec id="S27"><title>Image analysis</title><sec id="S28"><title>Whole-brain analyses</title><p id="P48">Functional images were analyzed with Statistical Parametric Mapping software (SPM12, Wellcome Trust Centre for Neuroimaging, London, UK). Preprocessing steps included realignment to the first volume of the time series, slice timing, normalization into the Montreal Neurological Institute<sup><xref ref-type="bibr" rid="R55">55</xref></sup> (MNI) space and spatial smoothing with an isotropic Gaussian filter of 8mm full width at half maximum. To remove low-frequency components, we used a high-pass filter with a cutoff frequency of 128s. Autocorrelation estimation was set to ‘FAST’ since our MRI repetition time was short (1s).</p><p id="P49">A general linear model was used to compute first-level statistics, in which each event was modeled by using a convolution with the ‘finite impulse response’ function (fixed duration of 2s), time-locked to the onset of each event. Separate regressors were created for each condition (Highest noise, High noise, Equal, Low noise, Lowest noise; “condition” factor). Finally, six motion parameters were included as regressors of no interest to account for movement in the data. The condition regressors were used to compute simple contrasts for each participant, leading to a main effect of each condition. These simple contrasts were then taken to a flexible factorial second-level analysis in which there were two factors: the “participants” factor (independence set to “yes”, variance set to unequal) and the “condition” factor (independence set to “no”, variance set to unequal). All neuroimaging activations were thresholded in SPM12 by using a voxel-wise correction for multiple comparisons of p&lt;.05 FDR.</p></sec><sec id="S29"><title>Functional and effective connectivity analyses</title><p id="P50">Functional and effective connectivity analyses (except for dynamic causal modelling, see specific section below) were computed by using the CONN toolbox<sup><xref ref-type="bibr" rid="R30">30</xref></sup> (version 22.a) implemented in Matlab 9.0 (The MathWorks, Inc., Natick, MA, USA). Analyses included: seed-to-voxel functional and effective connectivity, functional connectivity multivariate pattern analysis (fc-MVPA).</p><p id="P51">Functional and anatomical data were first imported with the SPM import option. Following this step, seed-to-voxel analyses were enabled by using parametric statistics. The following step was denoising, during which data are by default band-pass filtered and detrended and finally artifact corrected (removal of white matter, cerebrospinal fluid, motion, outlier regressors). The last analysis steps are first-level (with SPM regressors, i.e., movement parameters, entered as covariates of no interest) and second-level statistical analyses during which single-subject and group-level analyses are computed, respectively.</p></sec><sec id="S30"><title>Seed-to-voxel and seed-to-seed functional and effective connectivity analyses</title><p id="P52">For each participant and for each condition, measures were bivariate (correlation for functional connectivity, regression for effective connectivity) and one coefficient was calculated for each seed (seed-to-seed analyses) and each voxel (seed-to-voxel analyses). All analysis steps described were computed by using an explicit mask of the brain to remove any non-brain tissue and the analysis space was kept from the input files on SPM.</p><p id="P53">These analyses were computed using generalized psychophysiological interaction (gPPI) maps. These maps represent the level of task-modulated ‘effective’ connectivity between a seed and another seed (seed-to-seed) or every voxels in the brain (seed-to-voxel)—namely, changes in functional association strength covarying with an experimental factor. gPPI is computed using a separate multiple regression model for each target seed/voxel timeseries. For these analyses, seeds were labelled by using the atlas implemented in the CONN toolbox, with the addition of bilateral pMTG and PHG ROIs extracted from Study 1 (see <xref ref-type="fig" rid="F1">Fig.1d-f</xref>) as binary masks. We chose to include these ROIs to investigate the level of similarity the connectivity patterns of processing noise alone (Study 1) as opposed to processing voice with background noise (Study 3). For these analyses, all activations are reported at a voxel-wise corrected threshold of p&lt;.05 FDR, two-tailed.</p></sec><sec id="S31"><title>Voxel-level network analyses: fc-MVPA</title><p id="P54">Multivariate correlation maps represent for each voxel the number of most salient spatial features of the seed-based connectivity maps seeded at this very same voxel. They are defined by using a Singular Value Decomposition<sup><xref ref-type="bibr" rid="R63">63</xref>,<xref ref-type="bibr" rid="R64">64</xref></sup> separately for each seed voxel, of the patterns of seed-based correlations across all participants. The motivation behind the use of this additional connectivity measure was to extend and <italic>a minima</italic> compare univariate to multivariate connectivity results. A homogeneity between these two distinct types of analyses gives even more strength to the data and provides valuable coherence. All activations are reported at a voxel-wise corrected threshold of p&lt;.05 FDR, two-tailed.</p></sec><sec id="S32"><title>Dynamic causal modelling (DCM)</title><p id="P55">DCM—a directed, effective connectivity measure—was performed by following the steps recommended by Friston<sup><xref ref-type="bibr" rid="R28">28</xref>,<xref ref-type="bibr" rid="R29">29</xref></sup>. Volumes of interest were first extracted in SPM12 as actual activated clusters—not spheres or squares, including: the right posterior middle temporal gyrus obtained by running the fc-MVPA analysis (<xref ref-type="fig" rid="F5">Fig.5a</xref>) mentioned above (peak MNI <italic>xyz</italic> 62,-16,-12); the right parahippocampal gyrus activity resulting from contrasting non-vocal to vocal stimuli (<xref ref-type="fig" rid="F1">Fig.1e</xref>, Study 1, peak MNI <italic>xyz</italic> 26,-30,-18) overlapping with the same region in Study 3 (<xref ref-type="fig" rid="F4">Fig.4c</xref>); the left posterior parahippocampal gyrus observed in Study 3 gPPIs (<xref ref-type="fig" rid="F5">Fig.5b</xref>) when contrasting high noise to low noise vocal discrimination (peak MNI <italic>xyz</italic> -22,-26,-12). First level analyses were then computed per participant by creating three families of models to be tested for the two High noise conditions: the ‘MTG’ models family (Family 1), in which connections originated from the right pMTG to the parahippocampus (<xref ref-type="supplementary-material" rid="SD1">Fig.S8b</xref>); the ‘PHG inputs’ models family (Family 2), in which the connections don’t originate from the pMTG but enter directly the PHGs or their inter-connectivity (<xref ref-type="supplementary-material" rid="SD1">FigS8c</xref>); the last family includes ‘MTG and PHG input’ models (Family 3), in which the connections enter either between the MTG and PHGs or both the MTG and PHGs and their connections (<xref ref-type="supplementary-material" rid="SD1">Fig.S8d</xref>). These families and models were computed in relation with relevant literature<sup><xref ref-type="bibr" rid="R4">4</xref>,<xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R19">19</xref>,<xref ref-type="bibr" rid="R33">33</xref>,<xref ref-type="bibr" rid="R41">41</xref>,<xref ref-type="bibr" rid="R44">44</xref>,<xref ref-type="bibr" rid="R45">45</xref></sup> and our task results (Study 3, <xref ref-type="fig" rid="F4">Fig.4</xref> and <xref ref-type="fig" rid="F5">Fig.5</xref>). Next, fixed effects (FFX), second-level Bayesian model selection (BMS) analyses were computed for our sample of participants. We chose FFX because we were interested in patterns of directed effective connectivity common to our participants, not to differences between them. These analyses revealed Family 3 as the winning family with model 8 (inputs to the right pMTG, both PHGs and to the directed connections from the pMTG to the PHGs) as the overall winning model (posterior probability &gt;.99, <xref ref-type="fig" rid="F6">Fig.6</xref> and <xref ref-type="supplementary-material" rid="SD1">Fig.S8d</xref>), see the results section.</p></sec></sec></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary figures</label><media xlink:href="EMS199634-supplement-Supplementary_figures.pdf" mimetype="application" mime-subtype="pdf" id="d11aAcGbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S33"><title>Acknowledgements</title><p>The authors acknowledge the Swiss National Science Foundation (SNSF project no 105314_124572/1) and the Swiss National Competence Center in Research in affective sciences (51NF40-104897 – DG). The authors also thank Bruno Bonnet, Valérie Milesi-Sterck and Julie Péron, who helped acquire the neuroimaging data as well as Pascal Belin for his insights into this work. LC designed and created the task of Study 2 &amp; 3, programmed all tasks, collected the behavioral and neuroimaging data, analyzed the data, created and edited the figures and wrote and edited the manuscript. ES and DVD helped in the specific design and computation of the connectivity analyses. SF acquired part of the data and helped design the dynamic causal modeling analyses. DG designed the studies, especially Study 2 &amp; 3 and funded all studies. All authors reviewed and edited the manuscript.</p></ack><sec id="S34" sec-type="data-availability"><title>Data statement</title><p id="P56">The data and codes will be made available on the open YARETA repository once the article is published.</p></sec><fn-group><fn id="FN1" fn-type="conflict"><p id="P57"><bold>Author statement</bold></p><p id="P58">The authors declare no competing interests whatsoever.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Nitecki</surname><given-names>DV</given-names></name><name><surname>Nitecki</surname><given-names>MH</given-names></name></person-group><source>Origins of anatomically modern humans</source><publisher-name>Springer Science &amp; Business Media</publisher-name><year>2013</year></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arbib</surname><given-names>MA</given-names></name></person-group><article-title>From monkey-like action recognition to human language: An evolutionary framework for neurolinguistics</article-title><source>Behavioral and brain sciences</source><year>2005</year><volume>28</volume><fpage>105</fpage><lpage>124</lpage><pub-id pub-id-type="pmid">16201457</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Belin</surname><given-names>P</given-names></name><name><surname>Zatorre</surname><given-names>RJ</given-names></name><name><surname>Lafaille</surname><given-names>P</given-names></name><name><surname>Ahad</surname><given-names>P</given-names></name><name><surname>Pike</surname><given-names>B</given-names></name></person-group><article-title>Voice-selective areas in human auditory cortex</article-title><source>Nature</source><year>2000</year><volume>403</volume><fpage>309</fpage><lpage>312</lpage><pub-id pub-id-type="pmid">10659849</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pernet</surname><given-names>CR</given-names></name><etal/></person-group><article-title>The human voice areas: Spatial organization and inter-individual variability in temporal and extra-temporal cortices</article-title><source>Neuroimage</source><year>2015</year><volume>119</volume><fpage>164</fpage><lpage>174</lpage><pub-id pub-id-type="pmcid">PMC4768083</pub-id><pub-id pub-id-type="pmid">26116964</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.06.050</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ethofer</surname><given-names>T</given-names></name><etal/></person-group><article-title>Functional responses and structural connections of cortical areas for processing faces and voices in the superior temporal sulcus</article-title><source>Neuroimage</source><year>2013</year><volume>76</volume><fpage>45</fpage><lpage>56</lpage><pub-id pub-id-type="pmid">23507387</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fecteau</surname><given-names>S</given-names></name><name><surname>Armony</surname><given-names>JL</given-names></name><name><surname>Joanette</surname><given-names>Y</given-names></name><name><surname>Belin</surname><given-names>P</given-names></name></person-group><article-title>Is voice processing species-specific in human auditory cortex? An fMRI study</article-title><source>Neuroimage</source><year>2004</year><volume>23</volume><fpage>840</fpage><lpage>848</lpage><pub-id pub-id-type="pmid">15528084</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petkov</surname><given-names>CI</given-names></name><etal/></person-group><article-title>A voice region in the monkey brain</article-title><source>Nature neuroscience</source><year>2008</year><volume>11</volume><fpage>367</fpage><lpage>374</lpage><pub-id pub-id-type="pmid">18264095</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ceravolo</surname><given-names>L</given-names></name><name><surname>Frühholz</surname><given-names>S</given-names></name><name><surname>Pierce</surname><given-names>J</given-names></name><name><surname>Grandjean</surname><given-names>D</given-names></name><name><surname>Péron</surname><given-names>J</given-names></name></person-group><article-title>Basal ganglia and cerebellum contributions to vocal emotion processing as revealed by high-resolution fMRI</article-title><source>Scientific reports</source><year>2021</year><volume>11</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="pmcid">PMC8138027</pub-id><pub-id pub-id-type="pmid">34017050</pub-id><pub-id pub-id-type="doi">10.1038/s41598-021-90222-6</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frühholz</surname><given-names>S</given-names></name><name><surname>Ceravolo</surname><given-names>L</given-names></name></person-group><article-title>The neural network underlying the processing of affective vocalizations</article-title><source>The Oxford handbook of voice perception</source><year>2018</year><fpage>431</fpage></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kreifelts</surname><given-names>B</given-names></name><name><surname>Ethofer</surname><given-names>T</given-names></name><name><surname>Grodd</surname><given-names>W</given-names></name><name><surname>Erb</surname><given-names>M</given-names></name><name><surname>Wildgruber</surname><given-names>D</given-names></name></person-group><article-title>Audiovisual integration of emotional signals in voice and face: an event-related fMRI study</article-title><source>Neuroimage</source><year>2007</year><volume>37</volume><fpage>1445</fpage><lpage>1456</lpage><pub-id pub-id-type="pmid">17659885</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ceravolo</surname><given-names>L</given-names></name><name><surname>Frühholz</surname><given-names>S</given-names></name><name><surname>Grandjean</surname><given-names>D</given-names></name></person-group><article-title>Proximal vocal threat recruits the right voice-sensitive auditory cortex</article-title><source>Social cognitive and affective neuroscience</source><year>2016</year><volume>11</volume><fpage>793</fpage><lpage>802</lpage><pub-id pub-id-type="pmcid">PMC4847700</pub-id><pub-id pub-id-type="pmid">26746180</pub-id><pub-id pub-id-type="doi">10.1093/scan/nsw004</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ceravolo</surname><given-names>L</given-names></name><name><surname>Frühholz</surname><given-names>S</given-names></name><name><surname>Grandjean</surname><given-names>D</given-names></name></person-group><article-title>Modulation of auditory spatial attention by angry prosody: an fMRI auditory dot-probe study</article-title><source>Frontiers in neuroscience</source><year>2016</year><volume>10</volume><fpage>216</fpage><pub-id pub-id-type="pmcid">PMC4864064</pub-id><pub-id pub-id-type="pmid">27242420</pub-id><pub-id pub-id-type="doi">10.3389/fnins.2016.00216</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ethofer</surname><given-names>T</given-names></name><name><surname>Van De Ville</surname><given-names>D</given-names></name><name><surname>Scherer</surname><given-names>K</given-names></name><name><surname>Vuilleumier</surname><given-names>P</given-names></name></person-group><article-title>Decoding of emotional information in voice-sensitive cortices</article-title><source>Current Biology</source><year>2009</year><volume>19</volume><fpage>1028</fpage><lpage>1033</lpage><pub-id pub-id-type="pmid">19446457</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frühholz</surname><given-names>S</given-names></name><name><surname>Ceravolo</surname><given-names>L</given-names></name><name><surname>Grandjean</surname><given-names>D</given-names></name></person-group><article-title>Specific brain networks during explicit and implicit decoding of emotional prosody</article-title><source>Cerebral Cortex</source><year>2011</year><elocation-id>bhr184</elocation-id><pub-id pub-id-type="pmid">21750247</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frühholz</surname><given-names>S</given-names></name><name><surname>Grandjean</surname><given-names>D</given-names></name></person-group><article-title>Towards a fronto-temporal neural network for the decoding of angry vocal expressions</article-title><source>Neuroimage</source><year>2012</year><volume>62</volume><fpage>1658</fpage><lpage>1666</lpage><pub-id pub-id-type="pmid">22721630</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grandjean</surname><given-names>D</given-names></name><etal/></person-group><article-title>The voices of wrath: brain responses to angry prosody in meaningless speech</article-title><source>Nature neuroscience</source><year>2005</year><volume>8</volume><fpage>145</fpage><lpage>146</lpage><pub-id pub-id-type="pmid">15665880</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schnider</surname><given-names>A</given-names></name><name><surname>Benson</surname><given-names>DF</given-names></name><name><surname>Alexander</surname><given-names>DN</given-names></name><name><surname>Schnider-Klaus</surname><given-names>A</given-names></name></person-group><article-title>Non-verbal environmental sound recognition after unilateral hemispheric stroke</article-title><source>Brain</source><year>1994</year><volume>117</volume><fpage>281</fpage><lpage>287</lpage><pub-id pub-id-type="pmid">8186955</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vander Ghinst</surname><given-names>M</given-names></name><etal/></person-group><article-title>Cortical tracking of speech-in-noise develops from childhood to adulthood</article-title><source>Journal of Neuroscience</source><year>2019</year><volume>39</volume><fpage>2938</fpage><lpage>2950</lpage><pub-id pub-id-type="pmcid">PMC6462442</pub-id><pub-id pub-id-type="pmid">30745419</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1732-18.2019</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewis</surname><given-names>JW</given-names></name><etal/></person-group><article-title>Human brain regions involved in recognizing environmental sounds</article-title><source>Cerebral cortex</source><year>2004</year><volume>14</volume><fpage>1008</fpage><lpage>1021</lpage><pub-id pub-id-type="pmid">15166097</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frühholz</surname><given-names>S</given-names></name><name><surname>Grandjean</surname><given-names>D</given-names></name></person-group><article-title>Amygdala subregions differentially respond and rapidly adapt to threatening voices</article-title><source>Cortex</source><year>2013</year><volume>49</volume><fpage>1394</fpage><lpage>1403</lpage><pub-id pub-id-type="pmid">22938844</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frühholz</surname><given-names>S</given-names></name><name><surname>Trost</surname><given-names>W</given-names></name><name><surname>Grandjean</surname><given-names>D</given-names></name></person-group><article-title>The role of the medial temporal limbic system in processing emotions in voice and music</article-title><source>Progress in neurobiology</source><year>2014</year><volume>123</volume><fpage>1</fpage><lpage>17</lpage><pub-id pub-id-type="pmid">25291405</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Von Kriegstein</surname><given-names>K</given-names></name><name><surname>Kleinschmidt</surname><given-names>A</given-names></name><name><surname>Sterzer</surname><given-names>P</given-names></name><name><surname>Giraud</surname><given-names>A-L</given-names></name></person-group><article-title>Interaction of face and voice areas during speaker recognition</article-title><source>Journal of Cognitive Neuroscience</source><year>2005</year><volume>17</volume><fpage>367</fpage><lpage>376</lpage><pub-id pub-id-type="pmid">15813998</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daiber</surname><given-names>A</given-names></name><etal/></person-group><article-title>Environmental noise induces the release of stress hormones and inflammatory signaling molecules leading to oxidative stress and vascular dysfunction—Signatures of the internal exposome</article-title><source>Biofactors</source><year>2019</year><volume>45</volume><fpage>495</fpage><lpage>506</lpage><pub-id pub-id-type="pmid">30937979</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Münzel</surname><given-names>T</given-names></name><name><surname>Sørensen</surname><given-names>M</given-names></name><name><surname>Daiber</surname><given-names>A</given-names></name></person-group><article-title>Transportation noise pollution and cardiovascular disease</article-title><source>Nature Reviews Cardiology</source><year>2021</year><volume>18</volume><fpage>619</fpage><lpage>636</lpage><pub-id pub-id-type="pmid">33790462</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jafari</surname><given-names>Z</given-names></name><name><surname>Kolb</surname><given-names>BE</given-names></name><name><surname>Mohajerani</surname><given-names>MH</given-names></name></person-group><article-title>Chronic traffic noise stress accelerates brain impairment and cognitive decline in mice</article-title><source>Experimental neurology</source><year>2018</year><volume>308</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="pmid">29936225</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arjunan</surname><given-names>A</given-names></name><name><surname>Rajan</surname><given-names>R</given-names></name></person-group><article-title>Noise and brain</article-title><source>Physiology &amp; Behavior</source><year>2020</year><volume>227</volume><elocation-id>113136</elocation-id><pub-id pub-id-type="pmid">32798569</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Belin</surname><given-names>P</given-names></name><name><surname>Fecteau</surname><given-names>S</given-names></name><name><surname>Bedard</surname><given-names>C</given-names></name></person-group><article-title>Thinking the voice: neural correlates of voice perception</article-title><source>Trends in cognitive sciences</source><year>2004</year><volume>8</volume><fpage>129</fpage><lpage>135</lpage><pub-id pub-id-type="pmid">15301753</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K</given-names></name><etal/></person-group><article-title>Psychophysiological and modulatory interactions in neuroimaging</article-title><source>Neuroimage</source><year>1997</year><volume>6</volume><fpage>218</fpage><lpage>229</lpage><pub-id pub-id-type="pmid">9344826</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Harrison</surname><given-names>L</given-names></name><name><surname>Penny</surname><given-names>W</given-names></name></person-group><article-title>Dynamic causal modelling</article-title><source>Neuroimage</source><year>2003</year><volume>19</volume><fpage>1273</fpage><lpage>1302</lpage><pub-id pub-id-type="pmid">12948688</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whitfield-Gabrieli</surname><given-names>S</given-names></name><name><surname>Nieto-Castanon</surname><given-names>A</given-names></name></person-group><article-title>Conn: a functional connectivity toolbox for correlated and anticorrelated brain networks</article-title><source>Brain connectivity</source><year>2012</year><volume>2</volume><fpage>125</fpage><lpage>141</lpage><pub-id pub-id-type="pmid">22642651</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ethofer</surname><given-names>T</given-names></name><etal/></person-group><article-title>Emotional voice areas: anatomic location, functional properties, and structural connections revealed by combined fMRI/DTI</article-title><source>Cerebral Cortex</source><year>2011</year><elocation-id>bhr113</elocation-id><pub-id pub-id-type="pmid">21625012</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegstein</surname><given-names>KV</given-names></name><name><surname>Giraud</surname><given-names>A-L</given-names></name></person-group><article-title>Distinct functional substrates along the right superior temporal sulcus for the processing of voices</article-title><source>Neuroimage</source><year>2004</year><volume>22</volume><fpage>948</fpage><lpage>955</lpage><pub-id pub-id-type="pmid">15193626</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Swanborough</surname><given-names>H</given-names></name><name><surname>Staib</surname><given-names>M</given-names></name><name><surname>Frühholz</surname><given-names>S</given-names></name></person-group><article-title>Neurocognitive dynamics of near-threshold voice signal detection and affective voice evaluation</article-title><source>Science advances</source><year>2020</year><volume>6</volume><elocation-id>eabb3884</elocation-id><pub-id pub-id-type="pmcid">PMC7732184</pub-id><pub-id pub-id-type="pmid">33310844</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.abb3884</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schirmer</surname><given-names>A</given-names></name><name><surname>Kotz</surname><given-names>SA</given-names></name></person-group><article-title>Beyond the right hemisphere: brain mechanisms mediating vocal emotional processing</article-title><source>Trends in cognitive sciences</source><year>2006</year><volume>10</volume><fpage>24</fpage><lpage>30</lpage><pub-id pub-id-type="pmid">16321562</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wildgruber</surname><given-names>D</given-names></name><name><surname>Pihan</surname><given-names>H</given-names></name><name><surname>Ackermann</surname><given-names>H</given-names></name><name><surname>Erb</surname><given-names>M</given-names></name><name><surname>Grodd</surname><given-names>W</given-names></name></person-group><article-title>Dynamic brain activation during processing of emotional intonation: influence of acoustic parameters, emotional valence, and sex</article-title><source>Neuroimage</source><year>2002</year><volume>15</volume><fpage>856</fpage><lpage>869</lpage><pub-id pub-id-type="pmid">11906226</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Latinus</surname><given-names>M</given-names></name><name><surname>Belin</surname><given-names>P</given-names></name></person-group><article-title>Human voice perception</article-title><source>Current Biology</source><year>2011</year><volume>21</volume><fpage>R143</fpage><lpage>R145</lpage><pub-id pub-id-type="pmid">21334289</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iseli</surname><given-names>M</given-names></name><name><surname>Shue</surname><given-names>Y-L</given-names></name><name><surname>Alwan</surname><given-names>A</given-names></name></person-group><article-title>Age, sex, and vowel dependencies of acoustic measures related to the voice sourcea)</article-title><source>The Journal of the Acoustical Society of America</source><year>2007</year><volume>121</volume><fpage>2283</fpage><lpage>2295</lpage><pub-id pub-id-type="pmid">17471742</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hall</surname><given-names>JA</given-names></name><name><surname>Gunnery</surname><given-names>SD</given-names></name><name><surname>Letzring</surname><given-names>T</given-names></name><name><surname>Carney</surname><given-names>DR</given-names></name><name><surname>Colvin</surname><given-names>CR</given-names></name></person-group><article-title>Accuracy of Judging Affect and Accuracy of Judging Personality: How and When Are They Related?</article-title><source>Journal of personality</source><year>2016</year><pub-id pub-id-type="pmid">27237702</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McAleer</surname><given-names>P</given-names></name><name><surname>Todorov</surname><given-names>A</given-names></name><name><surname>Belin</surname><given-names>P</given-names></name></person-group><article-title>How do you say ‘Hello’? Personality impressions from brief novel voices</article-title><source>PloS one</source><year>2014</year><volume>9</volume><elocation-id>e90779</elocation-id><pub-id pub-id-type="pmcid">PMC3951273</pub-id><pub-id pub-id-type="pmid">24622283</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0090779</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jääskeläinen</surname><given-names>IP</given-names></name><etal/></person-group><article-title>Human posterior auditory cortex gates novel sounds to consciousness</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2004</year><volume>101</volume><fpage>6809</fpage><lpage>6814</lpage><pub-id pub-id-type="pmcid">PMC404127</pub-id><pub-id pub-id-type="pmid">15096618</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0303760101</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tomasino</surname><given-names>B</given-names></name><etal/></person-group><article-title>Identifying environmental sounds: a multimodal mapping study</article-title><source>Frontiers in human neuroscience</source><year>2015</year><volume>9</volume><pub-id pub-id-type="pmcid">PMC4612670</pub-id><pub-id pub-id-type="pmid">26539096</pub-id><pub-id pub-id-type="doi">10.3389/fnhum.2015.00567</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mishra</surname><given-names>SK</given-names></name><name><surname>Lutman</surname><given-names>ME</given-names></name></person-group><article-title>Top-down influences of the medial olivocochlear efferent system in speech perception in noise</article-title><source>PloS one</source><year>2014</year><volume>9</volume><elocation-id>e85756</elocation-id><pub-id pub-id-type="pmcid">PMC3896402</pub-id><pub-id pub-id-type="pmid">24465686</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0085756</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parbery-Clark</surname><given-names>A</given-names></name><name><surname>Strait</surname><given-names>D</given-names></name><name><surname>Kraus</surname><given-names>N</given-names></name></person-group><article-title>Context-dependent encoding in the auditory brainstem subserves enhanced speech-in-noise perception in musicians</article-title><source>Neuropsychologia</source><year>2011</year><volume>49</volume><fpage>3338</fpage><lpage>3345</lpage><pub-id pub-id-type="pmcid">PMC3445334</pub-id><pub-id pub-id-type="pmid">21864552</pub-id><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2011.08.007</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scott</surname><given-names>SK</given-names></name><name><surname>Blank</surname><given-names>CC</given-names></name><name><surname>Rosen</surname><given-names>S</given-names></name><name><surname>Wise</surname><given-names>RJ</given-names></name></person-group><article-title>Identification of a pathway for intelligible speech in the left temporal lobe</article-title><source>Brain</source><year>2000</year><volume>123</volume><fpage>2400</fpage><lpage>2406</lpage><pub-id pub-id-type="pmcid">PMC5630088</pub-id><pub-id pub-id-type="pmid">11099443</pub-id><pub-id pub-id-type="doi">10.1093/brain/123.12.2400</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>JH</given-names></name><name><surname>Skoe</surname><given-names>E</given-names></name><name><surname>Banai</surname><given-names>K</given-names></name><name><surname>Kraus</surname><given-names>N</given-names></name></person-group><article-title>Perception of speech in noise: neural correlates</article-title><source>Journal of cognitive neuroscience</source><year>2011</year><volume>23</volume><fpage>2268</fpage><lpage>2279</lpage><pub-id pub-id-type="pmcid">PMC3253852</pub-id><pub-id pub-id-type="pmid">20681749</pub-id><pub-id pub-id-type="doi">10.1162/jocn.2010.21556</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grandjean</surname><given-names>D</given-names></name></person-group><article-title>Brain networks of emotional prosody processing</article-title><source>Emotion Review</source><year>2021</year><volume>13</volume><fpage>34</fpage><lpage>43</lpage></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ceravolo</surname><given-names>L</given-names></name><name><surname>Debracque</surname><given-names>C</given-names></name><name><surname>Pool</surname><given-names>E</given-names></name><name><surname>Gruber</surname><given-names>T</given-names></name><name><surname>Grandjean</surname><given-names>D</given-names></name></person-group><article-title>Frontal mechanisms underlying primate calls recognition by humans</article-title><source>Cerebral Cortex Communications</source><year>2023</year><volume>4</volume><elocation-id>tgad019</elocation-id><pub-id pub-id-type="pmcid">PMC10661312</pub-id><pub-id pub-id-type="pmid">38025828</pub-id><pub-id pub-id-type="doi">10.1093/texcom/tgad019</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dricu</surname><given-names>M</given-names></name><name><surname>Ceravolo</surname><given-names>L</given-names></name><name><surname>Grandjean</surname><given-names>D</given-names></name><name><surname>Frühholz</surname><given-names>S</given-names></name></person-group><article-title>Biased and unbiased perceptual decision-making on vocal emotions</article-title><source>Scientific reports</source><year>2017</year><volume>7</volume><elocation-id>16274</elocation-id><pub-id pub-id-type="pmcid">PMC5701116</pub-id><pub-id pub-id-type="pmid">29176612</pub-id><pub-id pub-id-type="doi">10.1038/s41598-017-16594-w</pub-id></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Milne</surname><given-names>JL</given-names></name><name><surname>Arnott</surname><given-names>SR</given-names></name><name><surname>Kish</surname><given-names>D</given-names></name><name><surname>Goodale</surname><given-names>MA</given-names></name><name><surname>Thaler</surname><given-names>L</given-names></name></person-group><article-title>Parahippocampal cortex is involved in material processing via echoes in blind echolocation experts</article-title><source>Vision research</source><year>2015</year><volume>109</volume><fpage>139</fpage><lpage>148</lpage><pub-id pub-id-type="pmid">25086210</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>J</given-names></name><etal/></person-group><article-title>Sensorineural hearing loss affects functional connectivity of the auditory Cortex, parahippocampal gyrus and Inferior prefrontal gyrus in tinnitus patients</article-title><source>Frontiers in Neuroscience</source><year>2022</year><volume>16</volume><elocation-id>816712</elocation-id><pub-id pub-id-type="pmcid">PMC9011051</pub-id><pub-id pub-id-type="pmid">35431781</pub-id><pub-id pub-id-type="doi">10.3389/fnins.2022.816712</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Ridder</surname><given-names>D</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name><name><surname>Sedley</surname><given-names>W</given-names></name><name><surname>Vanneste</surname><given-names>S</given-names></name></person-group><article-title>A parahippocampal-sensory Bayesian vicious circle generates pain or tinnitus: a source-localized EEG study</article-title><source>Brain communications</source><year>2023</year><volume>5</volume><elocation-id>fcad132</elocation-id><pub-id pub-id-type="pmcid">PMC10202557</pub-id><pub-id pub-id-type="pmid">37223127</pub-id><pub-id pub-id-type="doi">10.1093/braincomms/fcad132</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Ridder</surname><given-names>D</given-names></name><name><surname>Vanneste</surname><given-names>S</given-names></name></person-group><article-title>Targeting the parahippocampal area by auditory cortex stimulation in tinnitus</article-title><source>Brain stimulation</source><year>2014</year><volume>7</volume><fpage>709</fpage><lpage>717</lpage><pub-id pub-id-type="pmid">25129400</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elgoyhen</surname><given-names>AB</given-names></name><name><surname>Langguth</surname><given-names>B</given-names></name><name><surname>De Ridder</surname><given-names>D</given-names></name><name><surname>Vanneste</surname><given-names>S</given-names></name></person-group><article-title>Tinnitus: perspectives from human neuroimaging</article-title><source>Nature Reviews Neuroscience</source><year>2015</year><volume>16</volume><fpage>632</fpage><lpage>642</lpage><pub-id pub-id-type="pmid">26373470</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Griffa</surname><given-names>A</given-names></name><etal/></person-group><article-title>Evidence for increased parallel information transmission in human brain networks compared to macaques and male mice</article-title><source>Nature Communications</source><year>2023</year><volume>14</volume><elocation-id>8216</elocation-id><pub-id pub-id-type="pmcid">PMC10713651</pub-id><pub-id pub-id-type="pmid">38081838</pub-id><pub-id pub-id-type="doi">10.1038/s41467-023-43971-z</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname><given-names>DL</given-names></name><name><surname>Neelin</surname><given-names>P</given-names></name><name><surname>Peters</surname><given-names>TM</given-names></name><name><surname>Evans</surname><given-names>AC</given-names></name></person-group><article-title>Automatic 3D intersubject registration of MR volumetric data in standardized Talairach space</article-title><source>Journal of computer assisted tomography</source><year>1994</year><volume>18</volume><fpage>192</fpage><lpage>205</lpage><pub-id pub-id-type="pmid">8126267</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashburner</surname><given-names>J</given-names></name></person-group><article-title>A fast diffeomorphic image registration algorithm</article-title><source>Neuroimage</source><year>2007</year><volume>38</volume><fpage>95</fpage><lpage>113</lpage><pub-id pub-id-type="pmid">17761438</pub-id></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kang</surname><given-names>H</given-names></name></person-group><article-title>Sample size determination and power analysis using the G* Power software</article-title><source>Journal of educational evaluation for health professions</source><year>2021</year><volume>18</volume><pub-id pub-id-type="pmcid">PMC8441096</pub-id><pub-id pub-id-type="pmid">34325496</pub-id><pub-id pub-id-type="doi">10.3352/jeehp.2021.18.17</pub-id></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Welvaert</surname><given-names>M</given-names></name><name><surname>Durnez</surname><given-names>J</given-names></name><name><surname>Moerkerke</surname><given-names>B</given-names></name><name><surname>Verdoolaege</surname><given-names>G</given-names></name><name><surname>Rosseel</surname><given-names>Y</given-names></name></person-group><article-title>neuRosim: An R package for generating fMRI data</article-title><source>Journal of Statistical Software</source><year>2011</year><volume>44</volume><fpage>1</fpage><lpage>18</lpage></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whitcher</surname><given-names>B</given-names></name><name><surname>Schmid</surname><given-names>VJ</given-names></name><name><surname>Thorton</surname><given-names>A</given-names></name></person-group><article-title>Working with the DICOM and NIfTI Data Standards in R</article-title><source>Journal of Statistical Software</source><year>2011</year><volume>44</volume><fpage>1</fpage><lpage>29</lpage></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="book"><collab>RStudio T</collab><publisher-name>PBC Boston</publisher-name><publisher-loc>MA, USA</publisher-loc><year>2020</year></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Team</surname><given-names>RC</given-names></name></person-group><source>R Foundation for Statistical Computing</source><comment><ext-link ext-link-type="uri" xlink:href="https://www.R-project.org">https://www.R-project.org</ext-link></comment><year>2018</year></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Rosario-Martinez</surname><given-names>H</given-names></name><name><surname>Fox</surname><given-names>J</given-names></name><collab>Team RC</collab><name><surname>De Rosario-Martinez</surname><given-names>MH</given-names></name></person-group><article-title>Package ‘phia’</article-title><source>CRAN Repos</source><year>2015</year><volume>1</volume><fpage>2015</fpage><comment>Retrieved</comment></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strang</surname><given-names>G</given-names></name></person-group><article-title>The fundamental theorem of linear algebra</article-title><source>The American Mathematical Monthly</source><year>1993</year><volume>100</volume><fpage>848</fpage><lpage>855</lpage></element-citation></ref><ref id="R64"><label>64</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wall</surname><given-names>ME</given-names></name><name><surname>Rechtsteiner</surname><given-names>A</given-names></name><name><surname>Rocha</surname><given-names>LM</given-names></name></person-group><source>A practical approach to microarray data analysis</source><publisher-name>Springer</publisher-name><year>2003</year><fpage>91</fpage><lpage>109</lpage></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Fig. 1</label><caption><title>Enhanced brain activations specific to voice or noise processing.</title><p>(<bold>a-c</bold>) Voice-specific processing in the bilateral middle and superior temporal cortex, planum temporale, inferior frontal gyrus and insula. (<bold>d-h</bold>) Non-voice-specific—including 50% of environmental noise stimuli—processing in the bilateral middle temporal gyrus, parahippocampal gyrus, inferior frontal gyrus, anterior &amp; posterior cingulate cortices. All clusters and their coordinates are reported in <xref ref-type="table" rid="T2">Table 2</xref>. Statistically significant clusters are displayed on a normalized template at a threshold of <italic>p</italic>&lt;.05, corrected for multiple comparisons at the voxel level (False Discovery Rate; FDR). The color bars illustrate the ‘t’ statistical values. A1: primary auditory cortex; IFG: inferior frontal gyrus; INS: insula; STG: superior temporal gyrus; STS: superior temporal sulcus; PHG: parahippocampal gyrus; AMY: amygdala; preSMA: pre-supplementary motor area; preM: premotor cortex; M1: primary motor cortex; S1: primary somatosensory cortex; SMG: supramarginal gyrus; OFC: orbitofrontal cortex. Suffixes: orb, pars orbitalis; tri, pars triangularis; op, pars opercularis; sup, superior; mid, middle; med, medial.</p></caption><graphic xlink:href="EMS199634-f001"/></fig><fig id="F2" position="float"><label>Fig. 2</label><caption><title>Effective connectivity underlying voice processing.</title><p>(<bold>a,b</bold>) Connectivity multi-voxel pattern analysis for voice-specific processing. (<bold>c</bold>) Voice-specific, ROI-to-ROI effective connectivity—bivariate regression between ROIs, including both coupling (red lines) and anti-coupling (blue lines) between ROIs. The pink outlined circle in the posterior TC<sub>lat</sub> regions indicates the posterior MTG (pMTG), the white outlined circle the posterior STG (pSTG). Effective connectivity was computed through generalized Psychophysiological Interaction analyses (gPPI). Statistically significant clusters are displayed on a normalized template at a threshold of <italic>p</italic>&lt;.05, corrected for multiple comparisons at the voxel level (False Discovery Rate; FDR). The color bar illustrates the ‘t’ statistical values. IFGtri: inferior frontal gyrus, <italic>pars triangularis</italic>; pMTG: posterior middle temporal gyrus; pPHG: posterior parahippocampal gyrus; TC<sub>lat</sub>: lateral temporal cortex, including STG and STS regions; IFG: inferior frontal gyrus; DLPFC: dorsolateral prefrontal cortex; Put: putamen; PHG: parahippocampal gyrus; ant: anterior; post: posterior; mid: middle; L: left; R: right. K: number of voxels for a given cluster.</p></caption><graphic xlink:href="EMS199634-f002"/></fig><fig id="F3" position="float"><label>Fig. 3</label><caption><title>Task design and stimulus description for Study 2 and 3.</title><p>(<bold>a</bold>) Task design for Study 2 &amp; 3, illustrating task instruction and one trial. Stimulus was presented for 1.5sec in average and response time was fixed to 2sec, with a 2 Alternate-Forced Choice response (buttons counter-balanced between participants). Participants had to be as accurate as possible, but not as fast as possible since precision was the goal here. Study 2 was behavioral only while Study 3 included MRI scanning in a script-triggered, sparse-sampling fashion to avoid additional sources of noise during stimulus perception. Two full volumes of 1sec each were therefore acquired during the response slide, for 2sec in total. (<bold>b</bold>) Stimuli consisted of a combination of voice and environmental noise, carefully selected from the Non-voice stimuli of the database of Study 1<sup><xref ref-type="bibr" rid="R1">1</xref></sup>. There were five conditions (Highest noise, High noise, Equal, Low noise, Lowest noise), each including 45 trials (total N<sub>trials</sub>=225) and a specific ratio of voice-to-noise intensity (difference between conditions concerning energy showed no significant effect with all Ps &gt; .5, see <xref ref-type="sec" rid="S12">Methods</xref>). fMRI: functional magnetic resonance imaging; RT: MRI repetition time; dB: decibel; F0: voice fundamental frequency; kHz: kilohertz (1Hz*1000=1kHz).</p></caption><graphic xlink:href="EMS199634-f003"/></fig><fig id="F4" position="float"><label>Fig. 4</label><caption><title>Perception of voice signals in varying levels of noise, behavior and neuroimaging (Study 2 &amp; 3).</title><p>(<bold>a</bold>) Behavioral results of both studies illustrating the probability of perceiving a voice in a noisy background (logistic regression analysis, in blue, circles indicate individual values, half-violin curves the distribution of the data) as well as reaction times (in black, squares, individual values and half-violin curves in <xref ref-type="supplementary-material" rid="SD1">Fig.S2</xref>), <italic>y</italic> axis. Response probability shows a clear degradation of the perception of voice in noise when noise level is High to Highest in both independent samples (left panel: Study 2, N=18; right panel: Study 3, N=20). X axis: conditions with voice-to-noise ratio, in decibel (dB). Error bars represent the standard error of the mean. Chance level is 50%. n.s.: not significant; <bold>·</bold>: statistical tendency (.05&lt;p&lt;.1); ***<italic>p</italic>&lt;.001; **<italic>p</italic>&lt;.01. Probab.: probability; Resp.: response. (<bold>b-e, g</bold>) Whole-brain neuroimaging results of voice perception in noisy situations highlighting posterior lateral temporal cortex (<bold>d,e,g</bold>) and bilateral parahippocampal activity (<bold>c</bold>) as specified by (<bold>f</bold>) the parametric contrast: [2(Highest noise) 1(High noise) 0(Equal) -1(Low noise) -2(Lowest noise)]. All clusters and their coordinates are reported in <xref ref-type="table" rid="T4">Table 4</xref>. Statistically significant clusters are displayed on a normalized template at a threshold of <italic>p</italic>&lt;.05, corrected for multiple comparisons at the voxel level (False Discovery Rate; FDR). The color bars illustrate the ‘t’ statistical values. A1: primary auditory cortex; IFG: inferior frontal gyrus; INS: insula; MTG: middle temporal gyrus; STG: superior temporal gyrus; STS: superior temporal sulcus; PHG: parahippocampal gyrus; AMY: amygdala; preSMA: pre-supplementary motor area; preM: premotor cortex; M1: primary motor cortex; S1: primary somatosensory cortex; SMG: supramarginal gyrus; OFC: orbitofrontal cortex; TVAs: temporal voice areas. Suffixes: orb, pars orbitalis; tri, pars triangularis; op, pars opercularis; sup, superior; mid, middle; med, medial.</p></caption><graphic xlink:href="EMS199634-f004"/></fig><fig id="F5" position="float"><label>Fig. 5</label><caption><title>Effective and functional connectivity underlying voice processing in noise.</title><p>(<bold>a</bold>) Connectivity multi-voxel pattern analysis (MVPA) resulting in right posterior middle temporal gyrus (pMTG) used as seed for (<bold>b</bold>) effective connectivity analysis leading to a direct anti-coupling with the left posterior parahippocampal gyrus (pPHG), itself functionally anti-coupled back with the right pMTG (<bold>c</bold>). (<bold>d</bold>) When taking the right pPHG—observed in study 1 for the Non-voice-specific contrast, as seed, we found direct anti-coupling with the right planum temporale (PT). Effective connectivity used bivariate regression while functional connectivity used bivariate correlation, both computed through generalized Psychophysiological Interaction analyses (gPPI). Statistically significant clusters are displayed on a normalized template at a threshold of <italic>p</italic> &lt;.05, corrected for multiple comparisons at the voxel level (False Discovery Rate; FDR). The color bars illustrate the ‘t’ statistical values. L: left; R: right.</p></caption><graphic xlink:href="EMS199634-f005"/></fig><fig id="F6" position="float"><label>Fig. 6</label><caption><title>Effective, directed fixed-effect group-level connectivity underlying voice processing in noise using dynamic causal modelling (DCM).</title><p>DCM analysis was used on nine models and three families, all containing the right posterior middle temporal gyrus (pMTG) and the bilateral posterior parahippocampal gyrus (pPHG). Families and models are detailed in <xref ref-type="supplementary-material" rid="SD1">Fig.S8</xref>. (<bold>a</bold>) The input was represented by High and Highest noise conditions (red triangle), and (<bold>b</bold>) direct coupling was observed from the right pMTG to the right pPHG (red arrow) while direct anti-coupling was observed from the right pMTG to the left pPHG (blue arrow). Bayesian model averaging and Bayesian model selection criteria both surpassed a posterior probability of p&gt;.99. L: left; R: right.</p></caption><graphic xlink:href="EMS199634-f006"/></fig><fig id="F7" position="float"><label>Fig. 7</label><caption><title>Proposed model of voice processing in noisy situations.</title><p>Based on the results obtained in the present set of studies (Study 1-3), we postulate an updated model of voice processing, more specifically addressing the processing and possible parallel diminution of background noise. Although fMRI does not have a temporal resolution dynamic enough to focus on the time-course of voice processing in noise, we present three steps, happening in a supposed combination of serial and parallel steps: (<bold>a</bold>) First, auditory processing happens in the auditory cortex, namely the lateral temporal cortex (TC<sub>lat</sub>), both in a bilateral and ipsilateral fashion through coupling (red outline). (<bold>b</bold>) Second, background noise is reduced thanks to the anti-coupling (blue outline) between voice-specific TC<sub>lat</sub> regions and noise-specific medial temporal cortex (TC<sub>med</sub>) regions. (<bold>c</bold>) Lastly, voice processing ‘free of noise’—or at least with the main sources of noise diminished enough to allow for clear voice perception—resumes and/or continues in parallel and decision processes can be triggered thanks to a communication between TC<sub>lat</sub> and the inferior frontal cortex (IFC). This step may well occur partially in parallel to step 2, and step 2 may well occur partially in parallel to step 1.</p></caption><graphic xlink:href="EMS199634-f007"/></fig><fig id="F8" position="float"><label>Fig. 8</label><caption><title>Power analysis using simulated fMRI data, task-specific.</title><p>The ‘neuRosim’ R package was used to simulate nifti volumes according to the specific parameters, conditions and trial timings of the task of Study 3. Analyses were then performed in SPM12 at the first- and second-level and thresholded at p&lt;.05 Family-Wise Error (FWE) corrected for samples with (<bold>a</bold>) N=20, (<bold>b</bold>) N=30 and (<bold>c</bold>) N=35. Due to financial limitations and because high statistical power was obtained already with N=20, we included twenty participants in Study 3 (<bold>a</bold>, black rounded square). The colorbars represent the t-value, fixed to the highest level, namely the highest t-value obtained in the global maxima of the N=35 sample (T<sub>(34)</sub>=18). N<sub>simul</sub>: simulate sample size; AMY: amygdala; PHG: parahippocampal gyrus; pMTG: posterior division of the middle temporal gyrus; pSTG: posterior division of the superior temporal gyrus; L: left hemisphere; R: right hemisphere.</p></caption><graphic xlink:href="EMS199634-f008"/></fig><table-wrap id="T1" position="float" orientation="portrait"><label>Table 1</label><caption><title>Regions of interest (ROIs) extracted for each whole-brain contrast of interest used in functional connectivity analyses.</title></caption><table frame="box" rules="cols"><thead><tr><th align="left" valign="top">ROI extracted from Voice &gt; Non-voice contrast (<italic>p</italic>&lt;.05 FWE)</th><th align="center" valign="middle">Hemisphere (L/R/medial)</th></tr></thead><tbody><tr style="border-top: solid thin"><td align="left" valign="top">Anterior STG</td><td align="center" valign="top">L+R</td></tr><tr><td align="left" valign="top">Mid STG</td><td align="center" valign="top">L+R</td></tr><tr><td align="left" valign="top">Posterior STG</td><td align="center" valign="top">L+R</td></tr><tr><td align="left" valign="top">Medial dorsal nucleus</td><td align="center" valign="top">L+R</td></tr><tr><td align="left" valign="top">Medial geniculate body</td><td align="center" valign="top">L+R</td></tr><tr><td align="left" valign="top">Amygdala</td><td align="center" valign="top">L+R</td></tr><tr><td align="left" valign="top">Putamen</td><td align="center" valign="top">L+R</td></tr><tr><td align="left" valign="top">Inferior frontal gyrus</td><td align="center" valign="top">L+R</td></tr><tr><td align="left" valign="top">Medial superior frontal gyrus</td><td align="center" valign="top">Medial</td></tr><tr><td align="left" valign="top">Dorsolateral prefrontal cortex</td><td align="center" valign="top">L+R</td></tr><tr style="border-top: solid thin"><td align="left" valign="top"><bold>ROI extracted from Non-voice &gt; Voice contrast <italic>(p</italic>&lt;.05 FWE)</bold></td><td align="center" valign="top" style="border-right: hidden"/></tr><tr style="border-top: solid thin"><td align="left" valign="top">Caudate head</td><td align="center" valign="top">L</td></tr><tr><td align="left" valign="top">Parahippocampal gyrus</td><td align="center" valign="top">L+R</td></tr><tr><td align="left" valign="top">Dorsolateral prefrontal cortex</td><td align="center" valign="top">R</td></tr><tr><td align="left" valign="top">Anterior cingulate gyrus</td><td align="center" valign="top">R</td></tr><tr><td align="left" valign="top">Posterior MTG</td><td align="center" valign="top">L</td></tr><tr><td align="left" valign="top">Posterior cingulate gyrus</td><td align="center" valign="top">Medial</td></tr></tbody></table><table-wrap-foot><fn id="TFN1"><p id="P59">FWE: family-wise error; L: left; R: right; L+R: bilateral; STG: superior temporal gyrus; MTG: middle temporal gyrus.</p></fn></table-wrap-foot></table-wrap><table-wrap id="T2" position="float" orientation="portrait"><label>Table 2</label><caption><title>Clusters and coordinates for Vocal &gt; Non-vocal and Non-vocal &gt; Vocal contrasts, Study 1, p&lt;.05 FDR.</title></caption><table frame="box" rules="all"><thead><tr><th align="left" valign="top"/><th align="right" valign="top">Cluster size</th><th align="right" valign="top">T value</th><th align="right" valign="top">MNI X</th><th align="right" valign="top">MNI Y</th><th align="right" valign="top">MNI Z</th><th align="left" valign="top">Region</th><th align="left" valign="top">Hemisphere</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="12"><bold>Vocal &gt; Non-vocal</bold></td><td align="right" valign="top" rowspan="5">18128</td><td align="right" valign="top">20,49</td><td align="right" valign="top">-62</td><td align="right" valign="top">-18</td><td align="right" valign="top">2</td><td align="left" valign="top">STG</td><td align="left" valign="top">L</td></tr><tr><td align="right" valign="top">20,13</td><td align="right" valign="top">60</td><td align="right" valign="top">-12</td><td align="right" valign="top">-2</td><td align="left" valign="top">STG</td><td align="left" valign="top">R</td></tr><tr><td align="right" valign="top">19,78</td><td align="right" valign="top">60</td><td align="right" valign="top">-2</td><td align="right" valign="top">-4</td><td align="left" valign="top">STG</td><td align="left" valign="top">R</td></tr><tr><td align="right" valign="top">19,62</td><td align="right" valign="top">-58</td><td align="right" valign="top">-6</td><td align="right" valign="top">-2</td><td align="left" valign="top">STG</td><td align="left" valign="top">L</td></tr><tr><td align="right" valign="top">16,56</td><td align="right" valign="top">-60</td><td align="right" valign="top">-30</td><td align="right" valign="top">6</td><td align="left" valign="top">MTG</td><td align="left" valign="top">L</td></tr><tr><td align="right" valign="top">605</td><td align="right" valign="top">9,39</td><td align="right" valign="top">-50</td><td align="right" valign="top">-6</td><td align="right" valign="top">46</td><td align="left" valign="top">preM</td><td align="left" valign="top">L</td></tr><tr><td align="right" valign="top">524</td><td align="right" valign="top">8,60</td><td align="right" valign="top">2</td><td align="right" valign="top">2</td><td align="right" valign="top">60</td><td align="left" valign="top">preSMA</td><td align="left" valign="top">R</td></tr><tr><td align="right" valign="top">156</td><td align="right" valign="top">6,41</td><td align="right" valign="top">-24</td><td align="right" valign="top">-62</td><td align="right" valign="top">-20</td><td align="left" valign="top">Cer VI</td><td align="left" valign="top">L</td></tr><tr><td align="right" valign="top">703</td><td align="right" valign="top">5,30</td><td align="right" valign="top">-4</td><td align="right" valign="top">56</td><td align="right" valign="top">28</td><td align="left" valign="top">ACC</td><td align="left" valign="top">L</td></tr><tr><td align="right" valign="top">556</td><td align="right" valign="top">4,75</td><td align="right" valign="top">4</td><td align="right" valign="top">-58</td><td align="right" valign="top">32</td><td align="left" valign="top">Precuneus</td><td align="left" valign="top">R</td></tr><tr><td align="right" valign="top">84</td><td align="right" valign="top">4,28</td><td align="right" valign="top">22</td><td align="right" valign="top">-60</td><td align="right" valign="top">-18</td><td align="left" valign="top">Cer VI</td><td align="left" valign="top">R</td></tr><tr><td align="right" valign="top">58</td><td align="right" valign="top">3,46</td><td align="right" valign="top">-6</td><td align="right" valign="top">-14</td><td align="right" valign="top">42</td><td align="left" valign="top">CCmid</td><td align="left" valign="top">L</td></tr><tr><td align="left" valign="middle" rowspan="32"><bold>Non-vocal &gt; Vocal</bold></td><td align="right" valign="top" colspan="7"> </td></tr><tr><td align="right" valign="top" rowspan="4">6159</td><td align="right" valign="top">9,39</td><td align="right" valign="top">26</td><td align="right" valign="top">-30</td><td align="right" valign="top">-18</td><td align="left" valign="top">FFC</td><td align="left" valign="top">R</td></tr><tr><td align="right" valign="top">8,93</td><td align="right" valign="top">-26</td><td align="right" valign="top">-38</td><td align="right" valign="top">-16</td><td align="left" valign="top">FFC</td><td align="left" valign="top">L</td></tr><tr><td align="right" valign="top">8,75</td><td align="right" valign="top">-32</td><td align="right" valign="top">-32</td><td align="right" valign="top">-14</td><td align="left" valign="top">PHG</td><td align="left" valign="top">L</td></tr><tr><td align="right" valign="top">7,38</td><td align="right" valign="top">14</td><td align="right" valign="top">-50</td><td align="right" valign="top">10</td><td align="left" valign="top">Calcarine</td><td align="left" valign="top">R</td></tr><tr><td align="right" valign="top" rowspan="5">3091</td><td align="right" valign="top">6,76</td><td align="right" valign="top">6</td><td align="right" valign="top">46</td><td align="right" valign="top">-4</td><td align="left" valign="top">OFCmed</td><td align="left" valign="top">R</td></tr><tr><td align="right" valign="top">5,07</td><td align="right" valign="top">2</td><td align="right" valign="top">18</td><td align="right" valign="top">-6</td><td align="left" valign="top">Olfactory</td><td align="left" valign="top">R</td></tr><tr><td align="right" valign="top">4,98</td><td align="right" valign="top">-16</td><td align="right" valign="top">24</td><td align="right" valign="top">2</td><td align="left" valign="top">Caudate</td><td align="left" valign="top">L</td></tr><tr><td align="right" valign="top">4,61</td><td align="right" valign="top">-34</td><td align="right" valign="top">58</td><td align="right" valign="top">4</td><td align="left" valign="top">VLPFC</td><td align="left" valign="top">L</td></tr><tr><td align="right" valign="top">4,54</td><td align="right" valign="top">-12</td><td align="right" valign="top">16</td><td align="right" valign="top">12</td><td align="left" valign="top">Caudate</td><td align="left" valign="top">L</td></tr><tr><td align="right" valign="top" rowspan="3">765</td><td align="right" valign="top">5,80</td><td align="right" valign="top">6</td><td align="right" valign="top">-28</td><td align="right" valign="top">40</td><td align="left" valign="top">CCmid</td><td align="left" valign="top">L</td></tr><tr><td align="right" valign="top">4,46</td><td align="right" valign="top">-6</td><td align="right" valign="top">-38</td><td align="right" valign="top">42</td><td align="left" valign="top">CCmid</td><td align="left" valign="top">L</td></tr><tr><td align="right" valign="top">3,83</td><td align="right" valign="top">4</td><td align="right" valign="top">-40</td><td align="right" valign="top">52</td><td align="left" valign="top">Precuneus</td><td align="left" valign="top">L</td></tr><tr><td align="right" valign="top" rowspan="4">938</td><td align="right" valign="top">5,49</td><td align="right" valign="top">40</td><td align="right" valign="top">-68</td><td align="right" valign="top">12</td><td align="left" valign="top">MTG</td><td align="left" valign="top">R</td></tr><tr><td align="right" valign="top">4,16</td><td align="right" valign="top">42</td><td align="right" valign="top">-76</td><td align="right" valign="top">28</td><td align="left" valign="top">LOCmid</td><td align="left" valign="top">R</td></tr><tr><td align="right" valign="top">3,10</td><td align="right" valign="top">54</td><td align="right" valign="top">-62</td><td align="right" valign="top">6</td><td align="left" valign="top">MTG</td><td align="left" valign="top">R</td></tr><tr><td align="right" valign="top">2,90</td><td align="right" valign="top">30</td><td align="right" valign="top">-84</td><td align="right" valign="top">32</td><td align="left" valign="top">LOCmid</td><td align="left" valign="top">R</td></tr><tr><td align="right" valign="top" rowspan="4">2381</td><td align="right" valign="top">5,36</td><td align="right" valign="top">38</td><td align="right" valign="top">28</td><td align="right" valign="top">38</td><td align="left" valign="top">DLPFC</td><td align="left" valign="top">R</td></tr><tr><td align="right" valign="top">5,35</td><td align="right" valign="top">32</td><td align="right" valign="top">52</td><td align="right" valign="top">2</td><td align="left" valign="top">DLPFC</td><td align="left" valign="top">R</td></tr><tr><td align="right" valign="top">5,30</td><td align="right" valign="top">40</td><td align="right" valign="top">50</td><td align="right" valign="top">10</td><td align="left" valign="top">DLPFC</td><td align="left" valign="top">R</td></tr><tr><td align="right" valign="top">5,29</td><td align="right" valign="top">30</td><td align="right" valign="top">36</td><td align="right" valign="top">34</td><td align="left" valign="top">DLPFC</td><td align="left" valign="top">R</td></tr><tr><td align="right" valign="top" rowspan="5">857</td><td align="right" valign="top">4,71</td><td align="right" valign="top">52</td><td align="right" valign="top">-48</td><td align="right" valign="top">50</td><td align="left" valign="top">IPL</td><td align="left" valign="top">R</td></tr><tr><td align="right" valign="top">4,45</td><td align="right" valign="top">56</td><td align="right" valign="top">-46</td><td align="right" valign="top">40</td><td align="left" valign="top">IPL</td><td align="left" valign="top">R</td></tr><tr><td align="right" valign="top">4,21</td><td align="right" valign="top">44</td><td align="right" valign="top">-48</td><td align="right" valign="top">34</td><td align="left" valign="top">AG</td><td align="left" valign="top">R</td></tr><tr><td align="right" valign="top">4,14</td><td align="right" valign="top">50</td><td align="right" valign="top">-54</td><td align="right" valign="top">44</td><td align="left" valign="top">IPL</td><td align="left" valign="top">R</td></tr><tr><td align="right" valign="top">2,95</td><td align="right" valign="top">42</td><td align="right" valign="top">-42</td><td align="right" valign="top">42</td><td align="left" valign="top">SMG</td><td align="left" valign="top">R</td></tr><tr><td align="right" valign="top">634</td><td align="right" valign="top">4,67</td><td align="right" valign="top">-40</td><td align="right" valign="top">30</td><td align="right" valign="top">38</td><td align="left" valign="top">DLPFC</td><td align="left" valign="top">L</td></tr><tr><td align="right" valign="top" rowspan="2">382</td><td align="right" valign="top">4,47</td><td align="right" valign="top">-54</td><td align="right" valign="top">-48</td><td align="right" valign="top">36</td><td align="left" valign="top">IPL</td><td align="left" valign="top">L</td></tr><tr><td align="right" valign="top">3,81</td><td align="right" valign="top">-44</td><td align="right" valign="top">-54</td><td align="right" valign="top">38</td><td align="left" valign="top">IPL</td><td align="left" valign="top">L</td></tr><tr><td align="right" valign="top">43</td><td align="right" valign="top">4,26</td><td align="right" valign="top">52</td><td align="right" valign="top">-22</td><td align="right" valign="top">26</td><td align="left" valign="top">SMG</td><td align="left" valign="top">R</td></tr><tr><td align="right" valign="top">141</td><td align="right" valign="top">4,05</td><td align="right" valign="top">10</td><td align="right" valign="top">-80</td><td align="right" valign="top">-8</td><td align="left" valign="top">Lingual</td><td align="left" valign="top">R</td></tr></tbody></table><table-wrap-foot><fn id="TFN2"><p id="P60">STG: superior temporal gyrus; MTG: middle temporal gyrus; preM: premotor cortex; preSMA: pre-supplementary motor area; Cer: cerebellar lobule; ACC: anterior cingulate cortex; CC: cingulate cortex; FFC: fusiform cortex; PHG: parahippocampal gyrus; OFC: orbitofrontal cortex; VLPFC: ventrolateral prefrontal cortex; LOC: lateral occipital cortex; DLPFC: dorsolateral prefrontal cortex; IPL: inferior parietal lobule; AG: angular gyrus; SMG: supramarginal gyrus. Suffixes: mid, mid part.</p></fn></table-wrap-foot></table-wrap><table-wrap id="T3" position="float" orientation="portrait"><label>Table 3</label><caption><title>Matrix of connectivity values for seed-to-seed functional connectivity between all seed regions thresholded at <italic>p</italic>&lt;.01 (analysis-wise false discovery rate, two-tailed).</title></caption><table frame="box" rules="all"><thead><tr><th align="center" valign="middle">Analysis Unit</th><th align="center" valign="middle">Statistic</th><th align="center" valign="middle">Intensity within network</th><th align="center" valign="middle">Number of connections</th></tr></thead><tbody><tr><td align="left" valign="middle">Seed mSTGleft</td><td align="center" valign="top">F(20, 78) = 4.25</td><td align="center" valign="top">18.68</td><td align="center" valign="top">4</td></tr><tr><td align="center" valign="middle">mSTGleft-aSTGright</td><td align="center" valign="top">T(97) = 6.13</td><td align="center" valign="top" rowspan="4" colspan="2"/></tr><tr><td align="center" valign="middle">mSTGleft-aSTGleft</td><td align="center" valign="top">T(97) = 4.38</td></tr><tr><td align="center" valign="middle">mSTGleft-mSTGright</td><td align="center" valign="top">T(97) = 4.18</td></tr><tr><td align="center" valign="middle">mSTGleft-pSTGright</td><td align="center" valign="top">T(97) = 3.99</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="top"> </td><td align="center" valign="top"> </td><td align="center" valign="top"> </td></tr><tr><td align="left" valign="middle">Seed mSTGright</td><td align="center" valign="top">F(20, 78) = 4.24</td><td align="center" valign="top">16.03</td><td align="center" valign="top">4</td></tr><tr><td align="center" valign="middle">mSTGright-aSTGleft</td><td align="center" valign="top">T(97) = 4.42</td><td align="center" valign="top" rowspan="4" colspan="2"/></tr><tr><td align="center" valign="middle">mSTGright-mSTGleft</td><td align="center" valign="top">T(97) = 4.18</td></tr><tr><td align="center" valign="middle">mSTGright-aSTGright</td><td align="center" valign="top">T(97) = 3.97</td></tr><tr><td align="center" valign="middle">mSTGright-pSTGright</td><td align="center" valign="top">T(97) = 3.46</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="top"> </td><td align="center" valign="middle"/><td align="center" valign="top"> </td></tr><tr><td align="left" valign="middle">Seed aSTGright</td><td align="center" valign="top">F(20, 78) = 3.46</td><td align="center" valign="top">13.24</td><td align="center" valign="top">3</td></tr><tr><td align="center" valign="middle">aSTGright-mSTGleft</td><td align="center" valign="top">T(97) = 6.13</td><td align="center" valign="top" colspan="2" rowspan="3"/></tr><tr><td align="center" valign="middle">aSTGright-mSTGright</td><td align="center" valign="top">T(97) = 3.97</td></tr><tr><td align="center" valign="middle">aSTGright-aSTGleft</td><td align="center" valign="top">T(97) = 3.13</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="top"> </td><td align="center" valign="middle"/><td align="center" valign="top"> </td></tr><tr><td align="left" valign="middle">Seed pSTGleft</td><td align="center" valign="top">F(20, 78) = 3.36</td><td align="center" valign="top">13.77</td><td align="center" valign="top">4</td></tr><tr><td align="center" valign="middle">pSTGleft-leftPHG_NV</td><td align="center" valign="top">T(97) = -4.30</td><td align="center" valign="top" colspan="2" rowspan="4"/></tr><tr><td align="center" valign="middle">pSTGleft-rightPHG_NV</td><td align="center" valign="top">T(97) = -3.87</td></tr><tr><td align="center" valign="middle">pSTGleft-pMTGleft_NV</td><td align="center" valign="top">T(97) = -2.89</td></tr><tr><td align="center" valign="middle">pSTGleft-MedSFC</td><td align="center" valign="top">T(97) = 2.71</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="top"> </td><td align="center" valign="middle"/><td align="center" valign="top"> </td></tr><tr><td align="left" valign="middle">Seed pSTGright</td><td align="center" valign="top">F(20, 78) = 3.28</td><td align="center" valign="top">33.5</td><td align="center" valign="top">10</td></tr><tr><td align="center" valign="middle">pSTGright-rightPHG_NV</td><td align="center" valign="top">T(97) = -4.13</td><td align="center" valign="top" colspan="2" rowspan="10"/></tr><tr><td align="center" valign="middle">pSTGright-mSTGleft</td><td align="center" valign="top">T(97) = 3.99</td></tr><tr><td align="center" valign="middle">pSTGright-aSTGleft</td><td align="center" valign="top">T(97) = 3.98</td></tr><tr><td align="center" valign="middle">pSTGright-leftPHG_NV</td><td align="center" valign="top">T(97) = -3.58</td></tr><tr><td align="center" valign="middle">pSTGright-leftIFG</td><td align="center" valign="top">T(97) = 3.50</td></tr><tr><td align="center" valign="middle">pSTGright-mSTGright</td><td align="center" valign="top">T(97) = 3.46</td></tr><tr><td align="center" valign="middle">pSTGright-rightIFG</td><td align="center" valign="top">T(97) = 3.20</td></tr><tr><td align="center" valign="middle">pSTGright-rightDLPFC</td><td align="center" valign="top">T(97) = 2.76</td></tr><tr><td align="center" valign="middle">pSTGright-MedSFC</td><td align="center" valign="top">T(97) = 2.48</td></tr><tr><td align="center" valign="middle">pSTGright-leftDLPFC</td><td align="center" valign="top">T(97) = 2.43</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="top"> </td><td align="center" valign="middle"/><td align="center" valign="top"> </td></tr><tr><td align="left" valign="middle">Seed rightDLPFC</td><td align="center" valign="top">F(20, 78) = 3.10</td><td align="center" valign="top">24.71</td><td align="center" valign="top">8</td></tr><tr><td align="center" valign="middle">rightDLPFC-leftPHG_NV</td><td align="center" valign="top">T(97) = -3.72</td><td align="center" valign="top" colspan="2" rowspan="8"/></tr><tr><td align="center" valign="middle">rightDLPFC-MedSFC</td><td align="center" valign="top">T(97) = 3.25</td></tr><tr><td align="center" valign="middle">rightDLPFC-rightDLPFC NV</td><td align="center" valign="top">T(97) = -3.23</td></tr><tr><td align="center" valign="middle">rightDLPFC-leftIFG</td><td align="center" valign="top">T(97) = 3.22</td></tr><tr><td align="center" valign="middle">rightDLPFC-pMTGleft_NV</td><td align="center" valign="top">T(97) = -3.07</td></tr><tr><td align="center" valign="middle">rightDLPFC-rightPHG_NV</td><td align="center" valign="top">T(97) = -2.86</td></tr><tr><td align="center" valign="middle">rightDLPFC-pSTGright</td><td align="center" valign="top">T(97) = 2.76</td></tr><tr><td align="center" valign="middle">rightDLPFC-aSTGleft</td><td align="center" valign="top">T(97) = 2.61</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="top"> </td><td align="center" valign="middle"/><td align="center" valign="top"> </td></tr><tr><td align="left" valign="middle">Seed aSTGleft</td><td align="center" valign="top">F(20, 78) = 2.51</td><td align="center" valign="top">21.84</td><td align="center" valign="top">6</td></tr><tr><td align="center" valign="middle">aSTGleft-mSTGright</td><td align="center" valign="top">T(97) = 4.42</td><td align="center" valign="top" rowspan="6" colspan="2"/></tr><tr><td align="center" valign="middle">aSTGleft-mSTGleft</td><td align="center" valign="top">T(97) = 4.38</td></tr><tr><td align="center" valign="middle">aSTGleft-pSTGright</td><td align="center" valign="top">T(97) = 3.98</td></tr><tr><td align="center" valign="middle">aSTGleft-rightIFG</td><td align="center" valign="top">T(97) = 3.33</td></tr><tr><td align="center" valign="middle">aSTGleft-aSTGright</td><td align="center" valign="top">T(97) = 3.13</td></tr><tr><td align="center" valign="middle">aSTGleft-rightDLPFC</td><td align="center" valign="top">T(97) = 2.61</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="top"> </td><td align="center" valign="middle"/><td align="center" valign="top"> </td></tr><tr><td align="left" valign="middle">Seed rightIFG</td><td align="center" valign="top">F(20, 78) = 2.28</td><td align="center" valign="top">13.06</td><td align="center" valign="top">4</td></tr><tr><td align="center" valign="middle">rightIFG-rightDLPFC_NV</td><td align="center" valign="top">T(97) = -3.53</td><td align="center" valign="top" rowspan="4" colspan="2"/></tr><tr><td align="center" valign="middle">rightIFG-aSTGleft</td><td align="center" valign="top">T(97) = 3.33</td></tr><tr><td align="center" valign="middle">rightIFG-pSTGright</td><td align="center" valign="top">T(97) = 3.20</td></tr><tr><td align="center" valign="middle">rightIFG-pMTGleft_NV</td><td align="center" valign="top">T(97) = -2.99</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="top"> </td><td align="center" valign="middle"/><td align="center" valign="top"> </td></tr><tr><td align="left" valign="middle">Seed rightDLPFC_NV</td><td align="center" valign="top">F(20, 78) = 2.25</td><td align="center" valign="top">10.07</td><td align="center" valign="top">3</td></tr><tr><td align="center" valign="middle">rightDLPFC_NV-rightIFG</td><td align="center" valign="top">T(97) = -3.53</td><td align="center" valign="top" rowspan="3" colspan="2"/></tr><tr><td align="center" valign="middle">rightDLPFC_NV-leftPHG NV</td><td align="center" valign="top">T(97) = 3.31</td></tr><tr><td align="center" valign="middle">rightDLPFC_NV-rightDLPFC</td><td align="center" valign="top">T(97) = -3.23</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="top"> </td><td align="center" valign="middle"/><td align="center" valign="top"> </td></tr><tr><td align="left" valign="middle">Seed leftPHG NV</td><td align="center" valign="top">F(20, 78) = 2.00</td><td align="center" valign="top">31.67</td><td align="center" valign="top">10</td></tr><tr><td align="center" valign="middle">leftPHG_NV-pSTGleft</td><td align="center" valign="top">T(97) = -4.30</td><td align="center" valign="top" rowspan="10" colspan="2"/></tr><tr><td align="center" valign="middle">leftPHG_NV-rightACC_NV</td><td align="center" valign="top">T(97) = 4.02</td></tr><tr><td align="center" valign="middle">leftPHG_NV-rightDLPFC</td><td align="center" valign="top">T(97) = -3.72</td></tr><tr><td align="center" valign="middle">leftPHG_NV-pSTGright</td><td align="center" valign="top">T(97) = -3.58</td></tr><tr><td align="center" valign="middle">leftPHG_NV-rightDLPFC NV</td><td align="center" valign="top">T(97) = 3.31</td></tr><tr><td align="center" valign="middle">leftPHG_NV-pCG_NV</td><td align="center" valign="top">T(97) = 2.70</td></tr><tr><td align="center" valign="middle">leftPHG_NV-leftAMY</td><td align="center" valign="top">T(97) = -2.62</td></tr><tr><td align="center" valign="middle">leftPHG_NV-mSTGright</td><td align="center" valign="top">T(97) = -2.55</td></tr><tr><td align="center" valign="middle">leftPHG_NV-mSTGleft</td><td align="center" valign="top">T(97) = -2.51</td></tr><tr><td align="center" valign="middle">leftPHG_NV-rightIFG</td><td align="center" valign="top">T(97) = -2.37</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="top"> </td><td align="center" valign="middle"/><td align="center" valign="top"> </td></tr><tr><td align="left" valign="middle">Seed rightACC_NV</td><td align="center" valign="top">F(20, 78) = 1.93</td><td align="center" valign="top">4.02</td><td align="center" valign="top">1</td></tr><tr><td align="left" valign="middle">rightACC_NV-leftPHG_NV</td><td align="center" valign="top">T(97) = 4.02</td><td align="center" valign="top" colspan="2"/></tr><tr><td align="center" valign="middle"/><td align="center" valign="top"> </td><td align="center" valign="middle"/><td align="center" valign="top"> </td></tr><tr><td align="left" valign="middle">Seed rightPHG_NV</td><td align="center" valign="top">F(20, 78) = 1.82</td><td align="center" valign="top">10.86</td><td align="center" valign="top">3</td></tr><tr><td align="center" valign="middle">rightPHG_NV -pSTGright</td><td align="center" valign="top">T(97) = -4.13</td><td align="center" valign="top" rowspan="3" colspan="2"/></tr><tr><td align="center" valign="middle">rightPHG_NV-pSTGleft</td><td align="center" valign="top">T(97) = -3.87</td></tr><tr><td align="center" valign="middle">rightPHG_NV-rightDLPFC</td><td align="center" valign="top">T(97) = -2.86</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="top"> </td><td align="center" valign="middle"/><td align="center" valign="top"> </td></tr><tr><td align="left" valign="middle">Seed MedSFC</td><td align="center" valign="top">F(20, 78) = 1.71</td><td align="center" valign="top">3.25</td><td align="center" valign="top">1</td></tr><tr><td align="center" valign="middle">MedSFC-rightDLPFC</td><td align="center" valign="top">T(97) = 3.25</td><td align="center" valign="top" colspan="2"/></tr><tr><td align="center" valign="middle"/><td align="center" valign="top"> </td><td align="center" valign="middle"/><td align="center" valign="top"> </td></tr><tr><td align="left" valign="middle">Seed pMTGleft_NV</td><td align="center" valign="top">F(20, 78) = 1.58</td><td align="center" valign="top">8.95</td><td align="center" valign="top">3</td></tr><tr><td align="left" valign="middle">pMTGleft_NV-rightDLPFC</td><td align="center" valign="top">T(97) = -3.07</td><td align="center" valign="top" rowspan="3" colspan="2"/></tr><tr><td align="center" valign="middle">pMTGleft_NV-rightIFG</td><td align="center" valign="top">T(97) = -2.99</td></tr><tr><td align="center" valign="middle">pMTGleft_NV-pSTGleft</td><td align="center" valign="top">T(97) = -2.89</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="top"> </td><td align="center" valign="middle"/><td align="center" valign="top"> </td></tr><tr><td align="left" valign="middle">Seed leftIFG</td><td align="center" valign="top">F(20, 78) = 1.49</td><td align="center" valign="top">6.71</td><td align="center" valign="top">2</td></tr><tr><td align="center" valign="middle">leftIFG-pSTGright</td><td align="center" valign="top">T(97) = 3.50</td><td align="center" valign="top" rowspan="2" colspan="2"/></tr><tr><td align="center" valign="middle">leftIFG-rightDLPFC</td><td align="center" valign="top">T(97) = 3.22</td></tr></tbody></table><table-wrap-foot><fn id="TFN3"><p id="P61">N.B: suffix “_NV” is used to notify when the region was originally extracted from the non-vocal&gt;vocal whole-brain contrast.</p></fn></table-wrap-foot></table-wrap><table-wrap id="T4" position="float" orientation="portrait"><label>Table 4</label><caption><title>Clusters and coordinates for Voices perceived in High compared to Low noise situations, Study 3, p&lt;.05 FDR.</title></caption><table frame="box" rules="all"><thead><tr><th align="right" valign="top">Cluster size</th><th align="right" valign="top">T value</th><th align="right" valign="top">MNI X</th><th align="right" valign="top">MNI Y</th><th align="right" valign="top">MNI Z</th><th align="left" valign="top">Region</th><th align="left" valign="top">Hemisphere</th></tr></thead><tbody><tr><td align="right" valign="top" rowspan="5">21795</td><td align="right" valign="top">4,91</td><td align="right" valign="top">26</td><td align="right" valign="top">-60</td><td align="right" valign="top">-10</td><td align="left" valign="top">Lingual</td><td align="left" valign="top">R</td></tr><tr><td align="right" valign="top">4,80</td><td align="right" valign="top">50</td><td align="right" valign="top">-66</td><td align="right" valign="top">-8</td><td align="left" valign="top">ITG</td><td align="left" valign="top">R</td></tr><tr><td align="right" valign="top">4,69</td><td align="right" valign="top">-12</td><td align="right" valign="top">-46</td><td align="right" valign="top">4</td><td align="left" valign="top">Calcarine</td><td align="left" valign="top">L</td></tr><tr><td align="right" valign="top">4,58</td><td align="right" valign="top">-16</td><td align="right" valign="top">-56</td><td align="right" valign="top">6</td><td align="left" valign="top">Calcarine</td><td align="left" valign="top">L</td></tr><tr><td align="right" valign="top">4,58</td><td align="right" valign="top">42</td><td align="right" valign="top">-72</td><td align="right" valign="top">-10</td><td align="left" valign="top">FFC</td><td align="left" valign="top">R</td></tr><tr><td align="right" valign="top" rowspan="7">957</td><td align="right" valign="top">4,82</td><td align="right" valign="top">14</td><td align="right" valign="top">-28</td><td align="right" valign="top">40</td><td align="left" valign="top">CCmid</td><td align="left" valign="top">R</td></tr><tr><td align="right" valign="top">3,96</td><td align="right" valign="top">4</td><td align="right" valign="top">-56</td><td align="right" valign="top">52</td><td align="left" valign="top">Precuneus</td><td align="left" valign="top">R</td></tr><tr><td align="right" valign="top">3,95</td><td align="right" valign="top">-16</td><td align="right" valign="top">-36</td><td align="right" valign="top">-14</td><td align="left" valign="top">PHG</td><td align="left" valign="top">L</td></tr><tr><td align="right" valign="top">3,72</td><td align="right" valign="top">14</td><td align="right" valign="top">-84</td><td align="right" valign="top">46</td><td align="left" valign="top">Cuneus</td><td align="left" valign="top">R</td></tr><tr><td align="right" valign="top">3,65</td><td align="right" valign="top">-10</td><td align="right" valign="top">-30</td><td align="right" valign="top">40</td><td align="left" valign="top">CCmid</td><td align="left" valign="top">L</td></tr><tr><td align="right" valign="top">3,57</td><td align="right" valign="top">22</td><td align="right" valign="top">-34</td><td align="right" valign="top">-12</td><td align="left" valign="top">PHG</td><td align="left" valign="top">R</td></tr><tr><td align="right" valign="top">3,45</td><td align="right" valign="top">-4</td><td align="right" valign="top">-54</td><td align="right" valign="top">44</td><td align="left" valign="top">Precuneus</td><td align="left" valign="top">L</td></tr><tr><td align="right" valign="top">153</td><td align="right" valign="top">4,24</td><td align="right" valign="top">40</td><td align="right" valign="top">-78</td><td align="right" valign="top">34</td><td align="left" valign="top">LOCmid</td><td align="left" valign="top">R</td></tr><tr><td align="right" valign="top" rowspan="3">469</td><td align="right" valign="top">4,03</td><td align="right" valign="top">-46</td><td align="right" valign="top">-10</td><td align="right" valign="top">-2</td><td align="left" valign="top">STG</td><td align="left" valign="top">L</td></tr><tr><td align="right" valign="top">3,55</td><td align="right" valign="top">-36</td><td align="right" valign="top">-4</td><td align="right" valign="top">8</td><td align="left" valign="top">Insula</td><td align="left" valign="top">L</td></tr><tr><td align="right" valign="top">3,49</td><td align="right" valign="top">-40</td><td align="right" valign="top">-20</td><td align="right" valign="top">-2</td><td align="left" valign="top">STG</td><td align="left" valign="top">L</td></tr><tr><td align="right" valign="top">39</td><td align="right" valign="top">3,69</td><td align="right" valign="top">4</td><td align="right" valign="top">-6</td><td align="right" valign="top">38</td><td align="left" valign="top">CCmid</td><td align="left" valign="top">R</td></tr><tr><td align="right" valign="top">34</td><td align="right" valign="top">3,16</td><td align="right" valign="top">-46</td><td align="right" valign="top">8</td><td align="right" valign="top">-20</td><td align="left" valign="top">STG</td><td align="left" valign="top">L</td></tr><tr><td align="right" valign="top">33</td><td align="right" valign="top">3,12</td><td align="right" valign="top">26</td><td align="right" valign="top">-94</td><td align="right" valign="top">16</td><td align="left" valign="top">LOCsup</td><td align="left" valign="top">R</td></tr></tbody></table><table-wrap-foot><fn id="TFN4"><p id="P62">STG: superior temporal gyrus; CC: cingulate cortex; FFC: fusiform cortex; PHG: parahippocampal gyrus; LOC: lateral occipital cortex; ITG: inferior temporal gyrus. Suffixes: mid, mid part; sup, superior part.</p></fn></table-wrap-foot></table-wrap></floats-group></article>